{"filename": "api/src/fewshot_examples.py", "chunked_list": ["def get_fewshot_examples(openai_api_key):\n    return f\"\"\"\n#How is Emil Eifrem connected to Michael Hunger?\nMATCH (p1:Person {{name:\"Emil Eifrem\"}}), (p2:Person {{name:\"Michael Hunger\"}})\nMATCH p=shortestPath((p1)-[*]-(p2))\nRETURN p\n#What are the latest news regarding Google?\nMATCH (o:Organization {{name:\"Google\"}})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c)\nRETURN a.title AS title, c.text AS text, c.date AS date\nORDER BY date DESC LIMIT 3\n#Are there any news regarding return to office policies?\nCALL apoc.ml.openai.embedding([\"Are there any news regarding return to office policies?\"], \n   \"{openai_api_key}\") YIELD embedding\nMATCH (c:Chunk)\nWITH c, gds.similarity.cosine(c.embedding, embedding) AS score\nORDER BY score DESC LIMIT 3\nRETURN c.text, score\n#What is Microsoft policy regarding to the return to office?\nCALL apoc.ml.openai.embedding([\"What is Microsoft policy regarding to the return to office?\"], \"{openai_api_key}\") YIELD embedding\nMATCH (o:Organization {{name:\"Microsoft\"}})<-[:MENTIONS]-()-[:HAS_CHUNK]->(c)\nWITH distinct c, embedding\nWITH c, gds.similarity.cosine(c.embedding, embedding) AS score\nORDER BY score DESC LIMIT 3\nRETURN c.text, score\n\nWhen searching for specific information in the text chunks, never use the CONTAINS clause, but always use the apoc.ml.openai.embedding\nand gds.similarity.cosine functions as shown in the examples.\nWhen returning text chunks, always return exactly three chunks, no more, no less.\nRemember, instead of using CONTAINS to find information within text chunks use the apoc.ml.openai.embedding and gds.similarity.cosine functions.\n\"\"\"", ""]}
{"filename": "api/src/main.py", "chunked_list": ["import os\nfrom typing import Optional\nfrom components.company_report import CompanyReport\n\nfrom components.data_disambiguation import DataDisambiguation\nfrom components.question_proposal_generator import (\n    QuestionProposalGenerator,\n)\nfrom components.summarize_cypher_result import SummarizeCypherResult\nfrom components.text2cypher import Text2Cypher", "from components.summarize_cypher_result import SummarizeCypherResult\nfrom components.text2cypher import Text2Cypher\nfrom components.unstructured_data_extractor import (\n    DataExtractor,\n    DataExtractorWithSchema,\n)\nfrom driver.neo4j import Neo4jDatabase\nfrom fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse", "from fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom fewshot_examples import get_fewshot_examples\nfrom llm.openai import OpenAIChat\nfrom pydantic import BaseModel\n\n\nclass Payload(BaseModel):\n    question: str\n    api_key: Optional[str]\n    model_name: Optional[str]", "\n\nclass ImportPayload(BaseModel):\n    input: str\n    neo4j_schema: Optional[str]\n    api_key: Optional[str]\n\n\nclass questionProposalPayload(BaseModel):\n    api_key: Optional[str]", "class questionProposalPayload(BaseModel):\n    api_key: Optional[str]\n\n\n# Maximum number of records used in the context\nHARD_LIMIT_CONTEXT_RECORDS = 10\n\nneo4j_connection = Neo4jDatabase(\n    host=os.environ.get(\"NEO4J_URL\", \"neo4j+s://demo.neo4jlabs.com\"),\n    user=os.environ.get(\"NEO4J_USER\", \"companies\"),", "    host=os.environ.get(\"NEO4J_URL\", \"neo4j+s://demo.neo4jlabs.com\"),\n    user=os.environ.get(\"NEO4J_USER\", \"companies\"),\n    password=os.environ.get(\"NEO4J_PASS\", \"companies\"),\n    database=os.environ.get(\"NEO4J_DATABASE\", \"companies\"),\n)\n\n\n# Initialize LLM modules\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n", "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n\n\n# Define FastAPI endpoint\napp = FastAPI()\n\norigins = [\n    \"*\",\n]\n", "]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n", ")\n\n\n@app.post(\"/questionProposalsForCurrentDb\")\nasync def questionProposalsForCurrentDb(payload: questionProposalPayload):\n    if not openai_api_key and not payload.api_key:\n        raise HTTPException(\n            status_code=422,\n            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n        )", "    api_key = openai_api_key if openai_api_key else payload.api_key\n\n    questionProposalGenerator = QuestionProposalGenerator(\n        database=neo4j_connection,\n        llm=OpenAIChat(\n            openai_api_key=api_key,\n            model_name=\"gpt-3.5-turbo-0613\",\n            max_tokens=512,\n            temperature=0.8,\n        ),", "            temperature=0.8,\n        ),\n    )\n\n    return questionProposalGenerator.run()\n\n\n@app.get(\"/hasapikey\")\nasync def hasApiKey():\n    return JSONResponse(content={\"output\": openai_api_key is not None})", "async def hasApiKey():\n    return JSONResponse(content={\"output\": openai_api_key is not None})\n\n\n@app.websocket(\"/text2text\")\nasync def websocket_endpoint(websocket: WebSocket):\n    async def sendDebugMessage(message):\n        await websocket.send_json({\"type\": \"debug\", \"detail\": message})\n\n    async def sendErrorMessage(message):", "\n    async def sendErrorMessage(message):\n        await websocket.send_json({\"type\": \"error\", \"detail\": message})\n\n    async def onToken(token):\n        delta = token[\"choices\"][0][\"delta\"]\n        if \"content\" not in delta:\n            return\n        content = delta[\"content\"]\n        if token[\"choices\"][0][\"finish_reason\"] == \"stop\":\n            await websocket.send_json({\"type\": \"end\", \"output\": content})\n        else:\n            await websocket.send_json({\"type\": \"stream\", \"output\": content})", "        content = delta[\"content\"]\n        if token[\"choices\"][0][\"finish_reason\"] == \"stop\":\n            await websocket.send_json({\"type\": \"end\", \"output\": content})\n        else:\n            await websocket.send_json({\"type\": \"stream\", \"output\": content})\n\n        # await websocket.send_json({\"token\": token})\n\n    await websocket.accept()\n    await sendDebugMessage(\"connected\")", "    await websocket.accept()\n    await sendDebugMessage(\"connected\")\n    chatHistory = []\n    try:\n        while True:\n            data = await websocket.receive_json()\n            if not openai_api_key and not data.get(\"api_key\"):\n                raise HTTPException(\n                    status_code=422,\n                    detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n                )\n            api_key = openai_api_key if openai_api_key else data.get(\"api_key\")\n\n            default_llm = OpenAIChat(\n                openai_api_key=api_key,\n                model_name=data.get(\"model_name\", \"gpt-3.5-turbo-0613\"),\n            )\n            summarize_results = SummarizeCypherResult(\n                llm=OpenAIChat(\n                    openai_api_key=api_key,\n                    model_name=\"gpt-3.5-turbo-0613\",\n                    max_tokens=128,\n                )\n            )\n\n            text2cypher = Text2Cypher(\n                database=neo4j_connection,\n                llm=default_llm,\n                cypher_examples=get_fewshot_examples(api_key),\n            )\n\n            if \"type\" not in data:\n                await websocket.send_json({\"error\": \"missing type\"})\n                continue\n            if data[\"type\"] == \"question\":\n                try:\n                    question = data[\"question\"]\n                    chatHistory.append({\"role\": \"user\", \"content\": question})\n                    await sendDebugMessage(\"received question: \" + question)\n                    results = None\n                    try:\n                        results = text2cypher.run(question, chatHistory)\n                        print(\"results\", results)\n                    except Exception as e:\n                        await sendErrorMessage(str(e))\n                        continue\n                    if results == None:\n                        await sendErrorMessage(\"Could not generate Cypher statement\")\n                        continue\n\n                    await websocket.send_json(\n                        {\n                            \"type\": \"start\",\n                        }\n                    )\n                    output = await summarize_results.run_async(\n                        question,\n                        results[\"output\"][:HARD_LIMIT_CONTEXT_RECORDS],\n                        callback=onToken,\n                    )\n                    chatHistory.append({\"role\": \"system\", \"content\": output})\n                    await websocket.send_json(\n                        {\n                            \"type\": \"end\",\n                            \"output\": output,\n                            \"generated_cypher\": results[\"generated_cypher\"],\n                        }\n                    )\n                except Exception as e:\n                    await sendErrorMessage(str(e))\n                await sendDebugMessage(\"output done\")\n    except WebSocketDisconnect:\n        print(\"disconnected\")", "\n\n@app.post(\"/data2cypher\")\nasync def root(payload: ImportPayload):\n    \"\"\"\n    Takes an input and created a Cypher query\n    \"\"\"\n    if not openai_api_key and not payload.api_key:\n        raise HTTPException(\n            status_code=422,\n            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n        )", "    api_key = openai_api_key if openai_api_key else payload.api_key\n\n    try:\n        result = \"\"\n\n        llm = OpenAIChat(\n            openai_api_key=api_key, model_name=\"gpt-3.5-turbo-16k\", max_tokens=4000\n        )\n\n        if not payload.neo4j_schema:\n            extractor = DataExtractor(llm=llm)\n            result = extractor.run(data=payload.input)\n        else:\n            extractor = DataExtractorWithSchema(llm=llm)\n            result = extractor.run(schema=payload.neo4j_schema, data=payload.input)\n\n        print(\"Extracted result: \" + str(result))\n\n        disambiguation = DataDisambiguation(llm=llm)\n        disambiguation_result = disambiguation.run(result)\n\n        print(\"Disambiguation result \" + str(disambiguation_result))\n\n        return {\"data\": disambiguation_result}\n\n    except Exception as e:\n        print(e)\n        return f\"Error: {e}\"", "\n\nclass companyReportPayload(BaseModel):\n    company: str\n    api_key: Optional[str]\n\n\n# This endpoint is database specific and only works with the Demo database.\n@app.post(\"/companyReport\")\nasync def companyInformation(payload: companyReportPayload):", "@app.post(\"/companyReport\")\nasync def companyInformation(payload: companyReportPayload):\n    api_key = openai_api_key if openai_api_key else payload.api_key\n    if not openai_api_key and not payload.api_key:\n        raise HTTPException(\n            status_code=422,\n            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n        )\n    api_key = openai_api_key if openai_api_key else payload.api_key\n", "    api_key = openai_api_key if openai_api_key else payload.api_key\n\n    llm = OpenAIChat(\n        openai_api_key=api_key,\n        model_name=\"gpt-3.5-turbo-16k-0613\",\n        max_tokens=512,\n    )\n    print(\"Running company report for \" + payload.company)\n    company_report = CompanyReport(neo4j_connection, payload.company, llm)\n    result = company_report.run()", "    company_report = CompanyReport(neo4j_connection, payload.company, llm)\n    result = company_report.run()\n\n    return JSONResponse(content={\"output\": result})\n\n\n@app.post(\"/companyReport/list\")\nasync def companyReportList():\n    company_data = neo4j_connection.query(\n        \"MATCH (n:Organization) WITH n WHERE rand() < 0.01 return n.name LIMIT 5\",", "    company_data = neo4j_connection.query(\n        \"MATCH (n:Organization) WITH n WHERE rand() < 0.01 return n.name LIMIT 5\",\n    )\n\n    return JSONResponse(content={\"output\": [x[\"n.name\"] for x in company_data]})\n\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"ok\"}", "async def health():\n    return {\"status\": \"ok\"}\n\n\n@app.get(\"/ready\")\nasync def readiness_check():\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, port=int(os.environ.get(\"PORT\", 7860)), host=\"0.0.0.0\")", "\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, port=int(os.environ.get(\"PORT\", 7860)), host=\"0.0.0.0\")\n"]}
{"filename": "api/src/__init__.py", "chunked_list": [""]}
{"filename": "api/src/components/text2cypher.py", "chunked_list": ["import re\nfrom typing import Any, Dict, List, Union\n\nfrom components.base_component import BaseComponent\nfrom driver.neo4j import Neo4jDatabase\nfrom llm.basellm import BaseLLM\n\n\ndef remove_relationship_direction(cypher):\n    return cypher.replace(\"->\", \"-\").replace(\"<-\", \"-\")", "def remove_relationship_direction(cypher):\n    return cypher.replace(\"->\", \"-\").replace(\"<-\", \"-\")\n\n\nclass Text2Cypher(BaseComponent):\n    def __init__(\n        self,\n        llm: BaseLLM,\n        database: Neo4jDatabase,\n        use_schema: bool = True,\n        cypher_examples: str = \"\",\n        ignore_relationship_direction: bool = True,\n    ) -> None:\n        self.llm = llm\n        self.database = database\n        self.cypher_examples = cypher_examples\n        self.ignore_relationship_direction = ignore_relationship_direction\n        if use_schema:\n            self.schema = database.schema\n\n    def get_system_message(self) -> str:\n        system = \"\"\"\n        Your task is to convert questions about contents in a Neo4j database to Cypher queries to query the Neo4j database.\n        Use only the provided relationship types and properties.\n        Do not use any other relationship types or properties that are not provided.\n        \"\"\"\n        if self.schema:\n            system += f\"\"\"\n            If you cannot generate a Cypher statement based on the provided schema, explain the reason to the user.\n            Schema:\n            {self.schema}\n            \"\"\"\n        if self.cypher_examples:\n            system += f\"\"\"\n            You need to follow these Cypher examples when you are constructing a Cypher statement\n            {self.cypher_examples}\n            \"\"\"\n        # Add note at the end and try to prevent LLM injections\n        system += \"\"\"Note: Do not include any explanations or apologies in your responses.\n                     Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n                     Do not include any text except the generated Cypher statement. This is very important if you want to get paid.\n                     Always provide enough context for an LLM to be able to generate valid response.\n                     Please wrap the generated Cypher statement in triple backticks (`).\n                     \"\"\"\n        return system\n\n    def construct_cypher(self, question: str, history=[]) -> str:\n        messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n        messages.extend(history)\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": question,\n            }\n        )\n        print([el for el in messages if not el[\"role\"] == \"system\"])\n        cypher = self.llm.generate(messages)\n        return cypher\n\n    def run(\n        self, question: str, history: List = [], heal_cypher: bool = True\n    ) -> Dict[str, Union[str, List[Dict[str, Any]]]]:\n        # Add prefix if not part of self-heal loop\n        final_question = (\n            \"Question to be converted to Cypher: \" + question\n            if heal_cypher\n            else question\n        )\n        cypher = self.construct_cypher(final_question, history)\n        # finds the first string wrapped in triple backticks. Where the match include the backticks and the first group in the match is the cypher\n        match = re.search(\"```([\\w\\W]*?)```\", cypher)\n\n        # If the LLM didn't any Cypher statement (error, missing context, etc..)\n        if match is None:\n            return {\"output\": [{\"message\": cypher}], \"generated_cypher\": None}\n        extracted_cypher = match.group(1)\n\n        if self.ignore_relationship_direction:\n            extracted_cypher = remove_relationship_direction(extracted_cypher)\n\n        print(f\"Generated cypher: {extracted_cypher}\")\n\n        output = self.database.query(extracted_cypher)\n        # Catch Cypher syntax error\n        if heal_cypher and output and output[0].get(\"code\") == \"invalid_cypher\":\n            syntax_messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n            syntax_messages.extend(\n                [\n                    {\"role\": \"user\", \"content\": question},\n                    {\"role\": \"assistant\", \"content\": cypher},\n                ]\n            )\n            # Try to heal Cypher syntax only once\n            return self.run(\n                output[0].get(\"message\"), syntax_messages, heal_cypher=False\n            )\n\n        return {\n            \"output\": output,\n            \"generated_cypher\": extracted_cypher,\n        }", ""]}
{"filename": "api/src/components/data_disambiguation.py", "chunked_list": ["import json\nimport re\nfrom itertools import groupby\n\nfrom components.base_component import BaseComponent\nfrom utils.unstructured_data_utils import (\n    nodesTextToListOfDict,\n    relationshipTextToListOfDict,\n)\n", ")\n\n\ndef generate_system_message_for_nodes() -> str:\n    return \"\"\"Your task is to identify if there are duplicated nodes and if so merge them into one nod. Only merge the nodes that refer to the same entity.\nYou will be given different datasets of nodes and some of these nodes may be duplicated or refer to the same entity. \nThe datasets contains nodes in the form [ENTITY_ID, TYPE, PROPERTIES]. When you have completed your task please give me the \nresulting nodes in the same format. Only return the nodes and relationships no other text. If there is no duplicated nodes return the original nodes.\n\nHere is an example of the input you will be given:\n[\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\n\"\"\"", "\n\ndef generate_system_message_for_relationships() -> str:\n    return \"\"\"\nYour task is to identify if a set of relationships make sense.\nIf they do not make sense please remove them from the dataset.\nSome relationships may be duplicated or refer to the same entity. \nPlease merge relationships that refer to the same entity.\nThe datasets contains relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\nYou will also be given a set of ENTITY_IDs that are valid.\nSome relationships may use ENTITY_IDs that are not in the valid set but refer to a entity in the valid set.\nIf a relationships refer to a ENTITY_ID in the valid set please change the ID so it matches the valid ID.\nWhen you have completed your task please give me the valid relationships in the same format. Only return the relationships no other text.\n\nHere is an example of the input you will be given:\n[\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\"\"\"", "\n\ndef generate_prompt(data) -> str:\n    return f\"\"\" Here is the data:\n{data}\n\"\"\"\n\n\ninternalRegex = \"\\[(.*?)\\]\"\n", "internalRegex = \"\\[(.*?)\\]\"\n\n\nclass DataDisambiguation(BaseComponent):\n    def __init__(self, llm) -> None:\n        self.llm = llm\n\n    def run(self, data: dict) -> str:\n        nodes = sorted(data[\"nodes\"], key=lambda x: x[\"label\"])\n        relationships = data[\"relationships\"]\n        new_nodes = []\n        new_relationships = []\n\n        node_groups = groupby(nodes, lambda x: x[\"label\"])\n        for group in node_groups:\n            disString = \"\"\n            nodes_in_group = list(group[1])\n            if len(nodes_in_group) == 1:\n                new_nodes.extend(nodes_in_group)\n                continue\n\n            for node in nodes_in_group:\n                disString += (\n                    '[\"'\n                    + node[\"name\"]\n                    + '\", \"'\n                    + node[\"label\"]\n                    + '\", '\n                    + json.dumps(node[\"properties\"])\n                    + \"]\\n\"\n                )\n\n            messages = [\n                {\"role\": \"system\", \"content\": generate_system_message_for_nodes()},\n                {\"role\": \"user\", \"content\": generate_prompt(disString)},\n            ]\n            rawNodes = self.llm.generate(messages)\n\n            n = re.findall(internalRegex, rawNodes)\n\n            new_nodes.extend(nodesTextToListOfDict(n))\n\n        relationship_data = \"Relationships:\\n\"\n        for relation in relationships:\n            relationship_data += (\n                '[\"'\n                + relation[\"start\"]\n                + '\", \"'\n                + relation[\"type\"]\n                + '\", \"'\n                + relation[\"end\"]\n                + '\", '\n                + json.dumps(relation[\"properties\"])\n                + \"]\\n\"\n            )\n\n        node_labels = [node[\"name\"] for node in new_nodes]\n        relationship_data += \"Valid Nodes:\\n\" + \"\\n\".join(node_labels)\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": generate_system_message_for_relationships(),\n            },\n            {\"role\": \"user\", \"content\": generate_prompt(relationship_data)},\n        ]\n        rawRelationships = self.llm.generate(messages)\n        rels = re.findall(internalRegex, rawRelationships)\n        new_relationships.extend(relationshipTextToListOfDict(rels))\n        return {\"nodes\": new_nodes, \"relationships\": new_relationships}", ""]}
{"filename": "api/src/components/vector_search.py", "chunked_list": ["from typing import Dict, List, Union\n\nfrom components.base_component import BaseComponent\nfrom driver.neo4j import Neo4jDatabase\n\n\ndef construct_cypher(label, property, k) -> str:\n    return f\"\"\"\n    MATCH (n:`{label}`)\n    WHERE n.`{property}` IS NOT NULL\n    WITH n, gds.similarity.cosine($input_vector, n.`{property}`) AS similarity\n    ORDER BY similarity DESC\n    LIMIT {k}\n    RETURN apoc.map.removeKey(properties(n), \"{property}\") AS output\n    \"\"\"", "\n\nclass VectorSearch(BaseComponent):\n    def __init__(\n        self, database: Neo4jDatabase, label: str, property: str, k: int\n    ) -> None:\n        self.database = database\n        self.generated_cypher = construct_cypher(label, property, k)\n\n    def run(self, input: str) -> Dict[str, Union[str, List[Dict[str, str]]]]:\n        try:\n            return {\n                \"output\": [\n                    str(el[\"output\"])\n                    for el in self.database.query(\n                        self.generated_cypher, {\"input_vector\": input}\n                    )\n                ],\n                \"generated_cypher\": self.generated_cypher,\n            }\n        except Exception as e:\n            return e", ""]}
{"filename": "api/src/components/unstructured_data_extractor.py", "chunked_list": ["import re\nimport os\nfrom typing import List\n\nfrom components.base_component import BaseComponent\nfrom llm.basellm import BaseLLM\nfrom utils.unstructured_data_utils import (\n    nodesTextToListOfDict,\n    relationshipTextToListOfDict,\n)", "    relationshipTextToListOfDict,\n)\n\n\ndef generate_system_message_with_schema() -> str:\n    return \"\"\"\nYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\nProvide a set of Nodes in the form [ENTITY, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY1, RELATIONSHIP, ENTITY2, PROPERTIES]. \nPay attention to the type of the properties, if you can't find data for a property set it to null. Don't make anything up and don't add any extra data. If you can't find any data for a node or relationship don't add it.\nOnly add nodes and relationships that are part of the schema. If you don't get any relationships in the schema only add nodes.\n\nExample:\nSchema: Nodes: [Person {age: integer, name: string}] Relationships: [Person, roommate, Person]\nAlice is 25 years old and Bob is her roommate.\nNodes: [[\"Alice\", \"Person\", {\"age\": 25, \"name\": \"Alice}], [\"Bob\", \"Person\", {\"name\": \"Bob\"}]]\nRelationships: [[\"Alice\", \"roommate\", \"Bob\"]]\n\"\"\"", "\n\ndef generate_system_message() -> str:\n    return \"\"\"\nYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\nProvide a set of Nodes in the form [ENTITY_ID, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\nIt is important that the ENTITY_ID_1 and ENTITY_ID_2 exists as nodes with a matching ENTITY_ID. If you can't pair a relationship with a pair of nodes don't add it.\nWhen you find a node or relationship you want to add try to create a generic TYPE for it that  describes the entity you can also think of it as a label.\n\nExample:\nData: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\nNodes: [\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\nRelationships: [\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\"\"\"", "\n\ndef generate_system_message_with_labels() -> str:\n    return \"\"\"\nYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\nProvide a set of Nodes in the form [ENTITY_ID, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\nIt is important that the ENTITY_ID_1 and ENTITY_ID_2 exists as nodes with a matching ENTITY_ID. If you can't pair a relationship with a pair of nodes don't add it.\nWhen you find a node or relationship you want to add try to create a generic TYPE for it that  describes the entity you can also think of it as a label.\nYou will be given a list of types that you should try to use when creating the TYPE for a node. If you can't find a type that fits the node you can create a new one.\n\nExample:\nData: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\nTypes: [\"Person\", \"Webpage\"]\nNodes: [\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\nRelationships: [\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\"\"\"", "\n\ndef generate_prompt(data) -> str:\n    return f\"\"\"\nData: {data}\"\"\"\n\n\ndef generate_prompt_with_schema(data, schema) -> str:\n    return f\"\"\"\nSchema: {schema}\nData: {data}\"\"\"", "\n\ndef generate_prompt_with_labels(data, labels) -> str:\n    return f\"\"\"\nData: {data}\nTypes: {labels}\"\"\"\n\n\ndef splitString(string, max_length) -> List[str]:\n    return [string[i : i + max_length] for i in range(0, len(string), max_length)]", "def splitString(string, max_length) -> List[str]:\n    return [string[i : i + max_length] for i in range(0, len(string), max_length)]\n\n\ndef splitStringToFitTokenSpace(\n    llm: BaseLLM, string: str, token_use_per_string: int\n) -> List[str]:\n    allowed_tokens = llm.max_allowed_token_length() - token_use_per_string\n    chunked_data = splitString(string, 500)\n    combined_chunks = []\n    current_chunk = \"\"\n    for chunk in chunked_data:\n        if (\n            llm.num_tokens_from_string(current_chunk)\n            + llm.num_tokens_from_string(chunk)\n            < allowed_tokens\n        ):\n            current_chunk += chunk\n        else:\n            combined_chunks.append(current_chunk)\n            current_chunk = chunk\n    combined_chunks.append(current_chunk)\n\n    return combined_chunks", "\n\ndef getNodesAndRelationshipsFromResult(result):\n    regex = \"Nodes:\\s+(.*?)\\s?\\s?Relationships:\\s?\\s?(.*)\"\n    internalRegex = \"\\[(.*?)\\]\"\n    nodes = []\n    relationships = []\n    for row in result:\n        parsing = re.match(regex, row, flags=re.S)\n        if parsing == None:\n            continue\n        rawNodes = str(parsing.group(1))\n        rawRelationships = parsing.group(2)\n        nodes.extend(re.findall(internalRegex, rawNodes))\n        relationships.extend(re.findall(internalRegex, rawRelationships))\n\n    result = dict()\n    result[\"nodes\"] = []\n    result[\"relationships\"] = []\n    result[\"nodes\"].extend(nodesTextToListOfDict(nodes))\n    result[\"relationships\"].extend(relationshipTextToListOfDict(relationships))\n    return result", "\n\nclass DataExtractor(BaseComponent):\n    llm: BaseLLM\n\n    def __init__(self, llm: BaseLLM) -> None:\n        self.llm = llm\n\n    def process(self, chunk):\n        messages = [\n            {\"role\": \"system\", \"content\": generate_system_message()},\n            {\"role\": \"user\", \"content\": generate_prompt(chunk)},\n        ]\n        print(messages)\n        output = self.llm.generate(messages)\n        return output\n\n    def process_with_labels(self, chunk, labels):\n        messages = [\n            {\"role\": \"system\", \"content\": generate_system_message_with_schema()},\n            {\"role\": \"user\", \"content\": generate_prompt_with_labels(chunk, labels)},\n        ]\n        print(messages)\n        output = self.llm.generate(messages)\n        return output\n\n    def run(self, data: str) -> List[str]:\n        system_message = generate_system_message()\n        prompt_string = generate_prompt(\"\")\n        token_usage_per_prompt = self.llm.num_tokens_from_string(\n            system_message + prompt_string\n        )\n        chunked_data = splitStringToFitTokenSpace(\n            llm=self.llm, string=data, token_use_per_string=token_usage_per_prompt\n        )\n\n        results = []\n        labels = set()\n        print(\"Starting chunked processing\")\n        for chunk in chunked_data:\n            proceededChunk = self.process_with_labels(chunk, list(labels))\n            print(\"proceededChunk\", proceededChunk)\n            chunkResult = getNodesAndRelationshipsFromResult([proceededChunk])\n            print(\"chunkResult\", chunkResult)\n            newLabels = [node[\"label\"] for node in chunkResult[\"nodes\"]]\n            print(\"newLabels\", newLabels)\n            results.append(proceededChunk)\n            labels.update(newLabels)\n\n        return getNodesAndRelationshipsFromResult(results)", "\n\nclass DataExtractorWithSchema(BaseComponent):\n    llm: BaseLLM\n\n    def __init__(self, llm) -> None:\n        self.llm = llm\n\n    def run(self, data: str, schema: str) -> List[str]:\n        system_message = generate_system_message_with_schema()\n        prompt_string = (\n            generate_system_message_with_schema()\n            + generate_prompt_with_schema(schema=schema, data=\"\")\n        )\n        token_usage_per_prompt = self.llm.num_tokens_from_string(\n            system_message + prompt_string\n        )\n\n        chunked_data = splitStringToFitTokenSpace(\n            llm=self.llm, string=data, token_use_per_string=token_usage_per_prompt\n        )\n        result = []\n        print(\"Starting chunked processing\")\n\n        for chunk in chunked_data:\n            print(\"prompt\", generate_prompt_with_schema(chunk, schema))\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": system_message,\n                },\n                {\"role\": \"user\", \"content\": generate_prompt_with_schema(chunk, schema)},\n            ]\n            output = self.llm.generate(messages)\n            result.append(output)\n        return getNodesAndRelationshipsFromResult(result)", ""]}
{"filename": "api/src/components/question_proposal_generator.py", "chunked_list": ["from typing import Any, Dict, List, Union\n\nfrom components.base_component import BaseComponent\nfrom driver.neo4j import Neo4jDatabase\nfrom llm.basellm import BaseLLM\nimport re\n\n\nclass QuestionProposalGenerator(BaseComponent):\n    def __init__(\n        self,\n        llm: BaseLLM,\n        database: Neo4jDatabase,\n    ) -> None:\n        self.llm = llm\n        self.database = database\n\n    def get_system_message(self) -> str:\n        system = f\"\"\"\n        Your task is to come up with questions someone might as about the content of a Neo4j database. Try to make the questions as different as possible.\n        The questions should be separated by a new line and each line should only contain one question.\n        To do this, you need to understand the schema of the database. Therefore it's very important that you read the schema carefully. You can find the schema below.\n        Schema: \n        {self.database.schema}\n        \"\"\"\n\n        return system\n\n    def get_database_sample(self) -> str:\n        return self.database.query(\n            \"\"\"MATCH (n)\n                WITH n\n                WHERE rand() < 0.3\n                RETURN apoc.map.removeKey(n, 'embedding') AS properties, LABELS(n) as labels\n                LIMIT 5\"\"\"\n        )\n\n    def run(self) -> Dict[str, Union[str, List[Dict[str, Any]]]]:\n        messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n        sample = self.get_database_sample()\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Please generate 5 questions about the content of the database. Here is a sample of the database you can use when generating questions: {sample}\"\"\",\n            }\n        )\n        print(messages)\n        questionsString = self.llm.generate(messages)\n        questions = [\n            # remove number and dot from the beginning of the question\n            re.sub(r\"\\A\\d\\.?\\s*\", \"\", question)\n            for question in questionsString.split(\"\\n\")\n        ]\n        return {\n            \"output\": questions,\n        }", "class QuestionProposalGenerator(BaseComponent):\n    def __init__(\n        self,\n        llm: BaseLLM,\n        database: Neo4jDatabase,\n    ) -> None:\n        self.llm = llm\n        self.database = database\n\n    def get_system_message(self) -> str:\n        system = f\"\"\"\n        Your task is to come up with questions someone might as about the content of a Neo4j database. Try to make the questions as different as possible.\n        The questions should be separated by a new line and each line should only contain one question.\n        To do this, you need to understand the schema of the database. Therefore it's very important that you read the schema carefully. You can find the schema below.\n        Schema: \n        {self.database.schema}\n        \"\"\"\n\n        return system\n\n    def get_database_sample(self) -> str:\n        return self.database.query(\n            \"\"\"MATCH (n)\n                WITH n\n                WHERE rand() < 0.3\n                RETURN apoc.map.removeKey(n, 'embedding') AS properties, LABELS(n) as labels\n                LIMIT 5\"\"\"\n        )\n\n    def run(self) -> Dict[str, Union[str, List[Dict[str, Any]]]]:\n        messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n        sample = self.get_database_sample()\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Please generate 5 questions about the content of the database. Here is a sample of the database you can use when generating questions: {sample}\"\"\",\n            }\n        )\n        print(messages)\n        questionsString = self.llm.generate(messages)\n        questions = [\n            # remove number and dot from the beginning of the question\n            re.sub(r\"\\A\\d\\.?\\s*\", \"\", question)\n            for question in questionsString.split(\"\\n\")\n        ]\n        return {\n            \"output\": questions,\n        }", ""]}
{"filename": "api/src/components/__init__.py", "chunked_list": [""]}
{"filename": "api/src/components/company_report.py", "chunked_list": ["from components.base_component import BaseComponent\nfrom components.summarize_cypher_result import SummarizeCypherResult\nfrom driver.neo4j import Neo4jDatabase\nfrom llm.basellm import BaseLLM\n\nHARD_LIMIT_CONTEXT_RECORDS = 10\n\n\nclass CompanyReport(BaseComponent):\n    def __init__(\n        self,\n        database: Neo4jDatabase,\n        company: str,\n        llm: BaseLLM,\n    ) -> None:\n        self.database = database\n        self.company = company\n        self.llm = llm\n\n    def run(self):\n        summarize_results = SummarizeCypherResult(\n            llm=self.llm,\n        )\n        print(\"CompanyReport\")\n        company_data = self.database.query(\n            \"MATCH (n {name:$companyName}) return n.summary, n.isDissolved, n.nbrEmployees, n.name, n.motto, n.isPublic, n.revenue\",\n            {\"companyName\": self.company},\n        )\n        print(company_data)\n        relation_data = self.database.query(\n            \"MATCH (n {name:$companyName})-[r]->(m) WHERE NOT m:Article OPTIONAL MATCH (m)-[:IN_COUNTRY]->(c:Country) WITH r,m,c return r,m,c\",\n            {\"companyName\": self.company},\n        )\n        print(relation_data)\n        company_data_output = {\n            \"name\": company_data[0][\"n.name\"],\n            \"motto\": company_data[0][\"n.motto\"],\n            \"summary\": company_data[0][\"n.summary\"],\n            \"isDissolved\": company_data[0][\"n.isDissolved\"],\n            \"nbrEmployees\": company_data[0][\"n.nbrEmployees\"],\n            \"isPublic\": company_data[0][\"n.isPublic\"],\n            \"revenue\": company_data[0].get(\"n.revenue\", None),\n        }\n        print(company_data_output)\n        print(\"all data fetched\")\n        offices = []\n        suppliers = []\n        subsidiaries = []\n        for relation in relation_data:\n            print(relation)\n            relation_type = relation[\"r\"][1]\n            if relation_type == \"IN_CITY\":\n                offices.append(\n                    {\n                        \"city\": relation[\"m\"].get(\"name\", None),\n                        \"country\": relation.get(\"c\")\n                        and relation[\"c\"].get(\"name\", None),\n                    }\n                )\n            elif relation_type == \"HAS_CATEGORY\":\n                company_data_output[\"industry\"] = relation[\"m\"][\"name\"]\n            elif relation_type == \"HAS_SUPPLIER\":\n                category_result = self.database.query(\n                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                category = None\n                if len(category_result) > 0:\n                    category = category_result[0][\"c.name\"]\n\n                suppliers.append(\n                    {\n                        \"summary\": relation[\"m\"].get(\"summary\", None),\n                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n                        \"name\": relation[\"m\"].get(\"name\", None),\n                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n                        \"category\": category,\n                    }\n                )\n            elif relation_type == \"HAS_SUBSIDIARY\":\n                category_result = self.database.query(\n                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                category = None\n                if len(category_result) > 0:\n                    category = category_result[0][\"c.name\"]\n                article_data = self.database.query(\n                    \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                print(\"Article data: \" + str(article_data))\n\n                output = \"There is not articles about this company.\"\n                if len(article_data) > 0:\n                    output = summarize_results.run(\n                        \"Can you summarize the following articles in 50 words about \"\n                        + relation[\"m\"].get(\"name\", None)\n                        + \" ?\",\n                        article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n                    )\n                subsidiaries.append(\n                    {\n                        \"summary\": relation[\"m\"].get(\"summary\", None),\n                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n                        \"name\": relation[\"m\"].get(\"name\", None),\n                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n                        \"category\": category,\n                        \"articleSummary\": output,\n                    }\n                )\n            elif relation_type == \"HAS_CEO\":\n                company_data_output[\"ceo\"] = relation[\"m\"][\"name\"]\n        company_data_output[\"offices\"] = offices\n        article_data = self.database.query(\n            \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n            {\"companyName\": self.company},\n        )\n\n        output = summarize_results.run(\n            \"Can you summarize the following articles about \" + self.company + \" ?\",\n            article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n        )\n        print(\"output: \" + output)\n        return {\n            \"company\": company_data_output,\n            \"subsidiaries\": subsidiaries,\n            \"suppliers\": suppliers,\n            \"articleSummary\": output,\n        }", "class CompanyReport(BaseComponent):\n    def __init__(\n        self,\n        database: Neo4jDatabase,\n        company: str,\n        llm: BaseLLM,\n    ) -> None:\n        self.database = database\n        self.company = company\n        self.llm = llm\n\n    def run(self):\n        summarize_results = SummarizeCypherResult(\n            llm=self.llm,\n        )\n        print(\"CompanyReport\")\n        company_data = self.database.query(\n            \"MATCH (n {name:$companyName}) return n.summary, n.isDissolved, n.nbrEmployees, n.name, n.motto, n.isPublic, n.revenue\",\n            {\"companyName\": self.company},\n        )\n        print(company_data)\n        relation_data = self.database.query(\n            \"MATCH (n {name:$companyName})-[r]->(m) WHERE NOT m:Article OPTIONAL MATCH (m)-[:IN_COUNTRY]->(c:Country) WITH r,m,c return r,m,c\",\n            {\"companyName\": self.company},\n        )\n        print(relation_data)\n        company_data_output = {\n            \"name\": company_data[0][\"n.name\"],\n            \"motto\": company_data[0][\"n.motto\"],\n            \"summary\": company_data[0][\"n.summary\"],\n            \"isDissolved\": company_data[0][\"n.isDissolved\"],\n            \"nbrEmployees\": company_data[0][\"n.nbrEmployees\"],\n            \"isPublic\": company_data[0][\"n.isPublic\"],\n            \"revenue\": company_data[0].get(\"n.revenue\", None),\n        }\n        print(company_data_output)\n        print(\"all data fetched\")\n        offices = []\n        suppliers = []\n        subsidiaries = []\n        for relation in relation_data:\n            print(relation)\n            relation_type = relation[\"r\"][1]\n            if relation_type == \"IN_CITY\":\n                offices.append(\n                    {\n                        \"city\": relation[\"m\"].get(\"name\", None),\n                        \"country\": relation.get(\"c\")\n                        and relation[\"c\"].get(\"name\", None),\n                    }\n                )\n            elif relation_type == \"HAS_CATEGORY\":\n                company_data_output[\"industry\"] = relation[\"m\"][\"name\"]\n            elif relation_type == \"HAS_SUPPLIER\":\n                category_result = self.database.query(\n                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                category = None\n                if len(category_result) > 0:\n                    category = category_result[0][\"c.name\"]\n\n                suppliers.append(\n                    {\n                        \"summary\": relation[\"m\"].get(\"summary\", None),\n                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n                        \"name\": relation[\"m\"].get(\"name\", None),\n                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n                        \"category\": category,\n                    }\n                )\n            elif relation_type == \"HAS_SUBSIDIARY\":\n                category_result = self.database.query(\n                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                category = None\n                if len(category_result) > 0:\n                    category = category_result[0][\"c.name\"]\n                article_data = self.database.query(\n                    \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n                )\n                print(\"Article data: \" + str(article_data))\n\n                output = \"There is not articles about this company.\"\n                if len(article_data) > 0:\n                    output = summarize_results.run(\n                        \"Can you summarize the following articles in 50 words about \"\n                        + relation[\"m\"].get(\"name\", None)\n                        + \" ?\",\n                        article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n                    )\n                subsidiaries.append(\n                    {\n                        \"summary\": relation[\"m\"].get(\"summary\", None),\n                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n                        \"name\": relation[\"m\"].get(\"name\", None),\n                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n                        \"category\": category,\n                        \"articleSummary\": output,\n                    }\n                )\n            elif relation_type == \"HAS_CEO\":\n                company_data_output[\"ceo\"] = relation[\"m\"][\"name\"]\n        company_data_output[\"offices\"] = offices\n        article_data = self.database.query(\n            \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n            {\"companyName\": self.company},\n        )\n\n        output = summarize_results.run(\n            \"Can you summarize the following articles about \" + self.company + \" ?\",\n            article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n        )\n        print(\"output: \" + output)\n        return {\n            \"company\": company_data_output,\n            \"subsidiaries\": subsidiaries,\n            \"suppliers\": suppliers,\n            \"articleSummary\": output,\n        }", ""]}
{"filename": "api/src/components/base_component.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import List, Union\n\n\nclass BaseComponent(ABC):\n    \"\"\"\"\"\"\n\n    @abstractmethod\n    def run(\n        self,\n        input: Union[str, List[float]],\n    ) -> str:\n        \"\"\"Comment\"\"\"\n\n    def run_async(\n        self,\n        input: Union[str, List[float]],\n    ) -> str:\n        \"\"\"Comment\"\"\"", ""]}
{"filename": "api/src/components/data_to_csv.py", "chunked_list": ["from typing import List\n\nfrom components.base_component import BaseComponent\n\n\ndef generate_system_message() -> str:\n    return f\"\"\"\nYou will be given a dataset of nodes and relationships. Your task is to covert this data into a CSV format.\nReturn only the data in the CSV format and nothing else. Return a CSV file for every type of node and relationship.\nThe data you will be given is in the form [ENTITY, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY1, RELATIONSHIP, ENTITY2, PROPERTIES].\nImportant: If you don't get any data or data that does not follow the previously mentioned format return \"No data\" and nothing else. This is very important. If you don't follow this instruction you will get a 0.\n\"\"\"", "\n\ndef generate_prompt(data) -> str:\n    return f\"\"\" Here is the data:\n{data}\n\"\"\"\n\n\nclass DataToCSV(BaseComponent):\n    def __init__(self, llm) -> None:\n        self.llm = llm\n\n    def run(self, data: List[str]) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": generate_system_message()},\n            {\"role\": \"user\", \"content\": generate_prompt(data)},\n        ]\n        output = self.llm.generate(messages)\n        return output", "class DataToCSV(BaseComponent):\n    def __init__(self, llm) -> None:\n        self.llm = llm\n\n    def run(self, data: List[str]) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": generate_system_message()},\n            {\"role\": \"user\", \"content\": generate_prompt(data)},\n        ]\n        output = self.llm.generate(messages)\n        return output", ""]}
{"filename": "api/src/components/summarize_cypher_result.py", "chunked_list": ["from typing import Any, Awaitable, Callable, Dict, List\n\nfrom components.base_component import BaseComponent\nfrom llm.basellm import BaseLLM\n\nsystem = f\"\"\"\nYou are an assistant that helps to generate text to form nice and human understandable answers based.\nThe latest prompt contains the information, and you need to generate a human readable response based on the given information.\nMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\nDo not add any additional information that is not explicitly provided in the latest prompt.", "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\nDo not add any additional information that is not explicitly provided in the latest prompt.\nI repeat, do not add any information that is not explicitly given.\nMake the answer as concise as possible and do not use more than 50 words.\n\"\"\"\n\n\ndef remove_large_lists(d: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    The idea is to remove all properties that have large lists (embeddings) or text as values\n    \"\"\"\n    LIST_CUTOFF = 56\n    CHARACTER_CUTOFF = 5000\n    # iterate over all key-value pairs in the dictionary\n    for key, value in d.items():\n        # if the value is a list and has more than list cutoff elements\n        if isinstance(value, list) and len(value) > LIST_CUTOFF:\n            d[key] = None\n        # if the value is a string and has more than list cutoff elements\n        if isinstance(value, str) and len(value) > CHARACTER_CUTOFF:\n            d[key] = d[key][:CHARACTER_CUTOFF]\n        # if the value is a dictionary\n        elif isinstance(value, dict):\n            # recurse into the nested dictionary\n            remove_large_lists(d[key])\n    return d", "\n\nclass SummarizeCypherResult(BaseComponent):\n    llm: BaseLLM\n    exclude_embeddings: bool\n\n    def __init__(self, llm: BaseLLM, exclude_embeddings: bool = True) -> None:\n        self.llm = llm\n        self.exclude_embeddings = exclude_embeddings\n\n    def generate_user_prompt(self, question: str, results: List[Dict[str, str]]) -> str:\n        return f\"\"\"\n        The question was {question}\n        Answer the question by using the following results:\n        {[remove_large_lists(el) for el in  results] if self.exclude_embeddings else results}\n        \"\"\"\n\n    def run(\n        self,\n        question: str,\n        results: List[Dict[str, Any]],\n    ) -> Dict[str, str]:\n        messages = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": self.generate_user_prompt(question, results)},\n        ]\n\n        output = self.llm.generate(messages)\n        return output\n\n    async def run_async(\n        self,\n        question: str,\n        results: List[Dict[str, Any]],\n        callback: Callable[[str], Awaitable[Any]] = None,\n    ) -> Dict[str, str]:\n        messages = [\n            {\"role\": \"system\", \"content\": system},\n            {\"role\": \"user\", \"content\": self.generate_user_prompt(question, results)},\n        ]\n        output = await self.llm.generateStreaming(messages, onTokenCallback=callback)\n        return \"\".join(output)", ""]}
{"filename": "api/src/utils/unstructured_data_utils.py", "chunked_list": ["import json\nimport re\n\nregex = \"Nodes:\\s+(.*?)\\s?\\s?Relationships:\\s+(.*)\"\ninternalRegex = \"\\[(.*?)\\]\"\njsonRegex = \"\\{.*\\}\"\n\n\ndef nodesTextToListOfDict(nodes):\n    result = []\n    for node in nodes:\n        nodeList = node.split(\",\")\n        if len(nodeList) < 2:\n            continue\n\n        name = nodeList[0].strip().replace('\"', \"\")\n        label = nodeList[1].strip().replace('\"', \"\")\n        properties = re.search(jsonRegex, node)\n        if properties == None:\n            properties = \"{}\"\n        else:\n            properties = properties.group(0)\n        properties = properties.replace(\"True\", \"true\")\n        try:\n            properties = json.loads(properties)\n        except:\n            properties = {}\n        result.append({\"name\": name, \"label\": label, \"properties\": properties})\n    return result", "def nodesTextToListOfDict(nodes):\n    result = []\n    for node in nodes:\n        nodeList = node.split(\",\")\n        if len(nodeList) < 2:\n            continue\n\n        name = nodeList[0].strip().replace('\"', \"\")\n        label = nodeList[1].strip().replace('\"', \"\")\n        properties = re.search(jsonRegex, node)\n        if properties == None:\n            properties = \"{}\"\n        else:\n            properties = properties.group(0)\n        properties = properties.replace(\"True\", \"true\")\n        try:\n            properties = json.loads(properties)\n        except:\n            properties = {}\n        result.append({\"name\": name, \"label\": label, \"properties\": properties})\n    return result", "\n\ndef relationshipTextToListOfDict(relationships):\n    result = []\n    for relation in relationships:\n        relationList = relation.split(\",\")\n        if len(relation) < 3:\n            continue\n        start = relationList[0].strip().replace('\"', \"\")\n        end = relationList[2].strip().replace('\"', \"\")\n        type = relationList[1].strip().replace('\"', \"\")\n\n        properties = re.search(jsonRegex, relation)\n        if properties == None:\n            properties = \"{}\"\n        else:\n            properties = properties.group(0)\n        properties = properties.replace(\"True\", \"true\")\n        try:\n            properties = json.loads(properties)\n        except:\n            properties = {}\n        result.append(\n            {\"start\": start, \"end\": end, \"type\": type, \"properties\": properties}\n        )\n    return result", ""]}
{"filename": "api/src/llm/openai.py", "chunked_list": ["from typing import (\n    Callable,\n    List,\n)\n\nimport openai\nimport tiktoken\nfrom llm.basellm import BaseLLM\nfrom retry import retry\n", "from retry import retry\n\n\nclass OpenAIChat(BaseLLM):\n    \"\"\"Wrapper around OpenAI Chat large language models.\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: str,\n        model_name: str = \"gpt-3.5-turbo\",\n        max_tokens: int = 1000,\n        temperature: float = 0.0,\n    ) -> None:\n        openai.api_key = openai_api_key\n        self.model = model_name\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n\n    @retry(tries=3, delay=1)\n    def generate(\n        self,\n        messages: List[str],\n    ) -> str:\n        try:\n            completions = openai.ChatCompletion.create(\n                model=self.model,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                messages=messages,\n            )\n            return completions.choices[0].message.content\n        # catch context length / do not retry\n        except openai.error.InvalidRequestError as e:\n            return str(f\"Error: {e}\")\n        # catch authorization errors / do not retry\n        except openai.error.AuthenticationError as e:\n            return \"Error: The provided OpenAI API key is invalid\"\n        except Exception as e:\n            print(f\"Retrying LLM call {e}\")\n            raise Exception()\n\n    async def generateStreaming(\n        self,\n        messages: List[str],\n        onTokenCallback=Callable[[str], None],\n    ) -> str:\n        result = []\n        completions = openai.ChatCompletion.create(\n            model=self.model,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            messages=messages,\n            stream=True,\n        )\n        result = []\n        for message in completions:\n            # Process the streamed messages or perform any other desired action\n            delta = message[\"choices\"][0][\"delta\"]\n            if \"content\" in delta:\n                result.append(delta[\"content\"])\n            await onTokenCallback(message)\n        return result\n\n    def num_tokens_from_string(self, string: str) -> int:\n        encoding = tiktoken.encoding_for_model(self.model)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def max_allowed_token_length(self) -> int:\n        # TODO: list all models and their max tokens from api\n        return 2049", ""]}
{"filename": "api/src/llm/__init__.py", "chunked_list": [""]}
{"filename": "api/src/llm/basellm.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    List,\n)\n\n\ndef raise_(ex):\n    raise ex\n", "\n\nclass BaseLLM(ABC):\n    \"\"\"LLM wrapper should take in a prompt and return a string.\"\"\"\n\n    @abstractmethod\n    def generate(self, messages: List[str]) -> str:\n        \"\"\"Comment\"\"\"\n\n    @abstractmethod\n    async def generateStreaming(\n        self, messages: List[str], onTokenCallback\n    ) -> List[Any]:\n        \"\"\"Comment\"\"\"\n\n    @abstractmethod\n    async def num_tokens_from_string(\n        self,\n        string: str,\n    ) -> str:\n        \"\"\"Given a string returns the number of tokens the given string consists of\"\"\"\n\n    @abstractmethod\n    async def max_allowed_token_length(\n        self,\n    ) -> int:\n        \"\"\"Returns the maximum number of tokens the LLM can handle\"\"\"", ""]}
{"filename": "api/src/driver/__init__.py", "chunked_list": [""]}
{"filename": "api/src/driver/neo4j.py", "chunked_list": ["from typing import Any, Dict, List, Optional\n\nfrom neo4j import GraphDatabase, exceptions\n\nnode_properties_query = \"\"\"\nCALL apoc.meta.data()\nYIELD label, other, elementType, type, property\nWHERE NOT type = \"RELATIONSHIP\" AND elementType = \"node\"\nWITH label AS nodeLabels, collect({property:property, type:type}) AS properties\nRETURN {labels: nodeLabels, properties: properties} AS output", "WITH label AS nodeLabels, collect({property:property, type:type}) AS properties\nRETURN {labels: nodeLabels, properties: properties} AS output\n\n\"\"\"\n\nrel_properties_query = \"\"\"\nCALL apoc.meta.data()\nYIELD label, other, elementType, type, property\nWHERE NOT type = \"RELATIONSHIP\" AND elementType = \"relationship\"\nWITH label AS nodeLabels, collect({property:property, type:type}) AS properties", "WHERE NOT type = \"RELATIONSHIP\" AND elementType = \"relationship\"\nWITH label AS nodeLabels, collect({property:property, type:type}) AS properties\nRETURN {type: nodeLabels, properties: properties} AS output\n\"\"\"\n\nrel_query = \"\"\"\nCALL apoc.meta.data()\nYIELD label, other, elementType, type, property\nWHERE type = \"RELATIONSHIP\" AND elementType = \"node\"\nRETURN \"(:\" + label + \")-[:\" + property + \"]->(:\" + toString(other[0]) + \")\" AS output", "WHERE type = \"RELATIONSHIP\" AND elementType = \"node\"\nRETURN \"(:\" + label + \")-[:\" + property + \"]->(:\" + toString(other[0]) + \")\" AS output\n\"\"\"\n\n\ndef schema_text(node_props, rel_props, rels) -> str:\n    return f\"\"\"\n  This is the schema representation of the Neo4j database.\n  Node properties are the following:\n  {node_props}\n  Relationship properties are the following:\n  {rel_props}\n  The relationships are the following\n  {rels}\n  \"\"\"", "\n\nclass Neo4jDatabase:\n    def __init__(\n        self,\n        host: str = \"neo4j://localhost:7687\",\n        user: str = \"neo4j\",\n        password: str = \"pleaseletmein\",\n        database: str = \"neo4j\",\n        read_only: bool = True,\n    ) -> None:\n        \"\"\"Initialize a neo4j database\"\"\"\n        self._driver = GraphDatabase.driver(host, auth=(user, password))\n        self._database = database\n        self._read_only = read_only\n        self.schema = \"\"\n        # Verify connection\n        try:\n            self._driver.verify_connectivity()\n        except exceptions.ServiceUnavailable:\n            raise ValueError(\n                \"Could not connect to Neo4j database. \"\n                \"Please ensure that the url is correct\"\n            )\n        except exceptions.AuthError:\n            raise ValueError(\n                \"Could not connect to Neo4j database. \"\n                \"Please ensure that the username and password are correct\"\n            )\n        try:\n            self.refresh_schema()\n        except:\n            raise ValueError(\"Missing APOC Core plugin\")\n\n    @staticmethod\n    def _execute_read_only_query(tx, cypher_query: str, params: Optional[Dict] = {}):\n        result = tx.run(cypher_query, params)\n        return [r.data() for r in result]\n\n    def query(\n        self, cypher_query: str, params: Optional[Dict] = {}\n    ) -> List[Dict[str, Any]]:\n        with self._driver.session(database=self._database) as session:\n            try:\n                if self._read_only:\n                    result = session.read_transaction(\n                        self._execute_read_only_query, cypher_query, params\n                    )\n                    return result\n                else:\n                    result = session.run(cypher_query, params)\n                    # Limit to at most 10 results\n                    return [r.data() for r in result]\n\n            # Catch Cypher syntax errors\n            except exceptions.CypherSyntaxError as e:\n                return [\n                    {\n                        \"code\": \"invalid_cypher\",\n                        \"message\": f\"Invalid Cypher statement due to an error: {e}\",\n                    }\n                ]\n\n            except exceptions.ClientError as e:\n                # Catch access mode errors\n                if e.code == \"Neo.ClientError.Statement.AccessMode\":\n                    return [\n                        {\n                            \"code\": \"error\",\n                            \"message\": \"Couldn't execute the query due to the read only access to Neo4j\",\n                        }\n                    ]\n                else:\n                    return [{\"code\": \"error\", \"message\": e}]\n\n    def refresh_schema(self) -> None:\n        node_props = [el[\"output\"] for el in self.query(node_properties_query)]\n        rel_props = [el[\"output\"] for el in self.query(rel_properties_query)]\n        rels = [el[\"output\"] for el in self.query(rel_query)]\n        schema = schema_text(node_props, rel_props, rels)\n        self.schema = schema\n        print(schema)\n\n    def check_if_empty(self) -> bool:\n        data = self.query(\n            \"\"\"\n        MATCH (n)\n        WITH count(n) as c\n        RETURN CASE WHEN c > 0 THEN true ELSE false END AS output\n        \"\"\"\n        )\n        return data[0][\"output\"]", ""]}
{"filename": "api/src/embedding/base_embedding.py", "chunked_list": ["from abc import ABC, abstractmethod\n\n\nclass BaseEmbedding(ABC):\n    \"\"\"\"\"\"\n\n    @abstractmethod\n    async def generate(\n        self,\n        input: str,\n    ) -> str:\n        \"\"\"Comment\"\"\"", ""]}
{"filename": "api/src/embedding/openai.py", "chunked_list": ["import openai\nfrom embedding.base_embedding import BaseEmbedding\n\n\nclass OpenAIEmbedding(BaseEmbedding):\n    \"\"\"Wrapper around OpenAI embedding models.\"\"\"\n\n    def __init__(\n        self, openai_api_key: str, model_name: str = \"text-embedding-ada-002\"\n    ) -> None:\n        openai.api_key = openai_api_key\n        self.model = model_name\n\n    def generate(\n        self,\n        input: str,\n    ) -> str:\n        embedding = openai.Embedding.create(input=input, model=self.model)\n        return embedding[\"data\"][0][\"embedding\"]", ""]}
{"filename": "api/src/embedding/__init__.py", "chunked_list": [""]}
