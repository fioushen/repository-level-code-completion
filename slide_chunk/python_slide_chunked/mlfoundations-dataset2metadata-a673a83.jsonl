{"filename": "setup.py", "chunked_list": ["# modified from https://github.com/rom1504/img2dataset/blob/main/setup.py\nfrom setuptools import setup, find_packages\nfrom pathlib import Path\nimport os\n\nif __name__ == \"__main__\":\n    with Path(Path(__file__).parent, \"README.md\").open(encoding=\"utf-8\") as file:\n        long_description = file.read()\n\n    def _read_reqs(relpath):\n        fullpath = os.path.join(os.path.dirname(__file__), relpath)\n        with open(fullpath) as f:\n            return [\n                s.strip()\n                for s in f.readlines()\n                if (s.strip() and not s.startswith(\"#\"))\n            ]\n\n    REQUIREMENTS = _read_reqs(\"requirements.txt\")\n\n    setup(\n        name=\"dataset2metadata\",\n        packages=find_packages(),\n        include_package_data=True,\n        version=\"0.1.0\",\n        license=\"MIT\",\n        description=\"Generate infered metadata for a dataset\",\n        long_description=long_description,\n        long_description_content_type=\"text/markdown\",\n        entry_points={\n            \"console_scripts\": [\"dataset2metadata = dataset2metadata.main:main\"]\n        },\n        author=\"Samir Gadre\",\n        author_email=\"syagadre@gmail.com\",\n        url=\"https://github.com/mlfoundations/dataset2metadata\",\n        data_files=[(\".\", [\"README.md\"])],\n        keywords=[\n            \"machine learning\",\n            \"computer vision\",\n            \"download\",\n            \"image\",\n            \"dataset\",\n        ],\n        install_requires=REQUIREMENTS,\n        classifiers=[\n            \"Development Status :: 4 - Beta\",\n            \"Intended Audience :: Developers\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"License :: OSI Approved :: MIT License\",\n            \"Programming Language :: Python :: 3.8\",\n        ],\n    )", ""]}
{"filename": "tests/test_main.py", "chunked_list": ["import pytest\nfrom dataset2metadata.process import process\n\ndef test_braceexpand():\n    try:\n        process('./tests/ymls/test_braceexpand.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False", "\n# def test_custom():\n#     try:\n#         process('./custom/blip2.yml')\n#         assert True\n#     except Exception as e:\n#         print(str(e))\n#         assert False\n\ndef test_local():\n    try:\n        process('./tests/ymls/test_local.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False", "\ndef test_local():\n    try:\n        process('./tests/ymls/test_local.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False\n\ndef test_s3():\n    try:\n        process('./tests/ymls/test_s3.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False", "\ndef test_s3():\n    try:\n        process('./tests/ymls/test_s3.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False\n\ndef test_cache():\n    try:\n        process('./tests/ymls/test_cache.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False", "\ndef test_cache():\n    try:\n        process('./tests/ymls/test_cache.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False\n\ndef test_datacomp_names():\n    try:\n        process('./tests/ymls/test_local_datacomp_names.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False", "\ndef test_datacomp_names():\n    try:\n        process('./tests/ymls/test_local_datacomp_names.yml')\n        assert True\n    except Exception as e:\n        print(str(e))\n        assert False"]}
{"filename": "dataset2metadata/writer.py", "chunked_list": ["import logging\nimport os\nfrom typing import List\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport fsspec\n\nlogging.getLogger().setLevel(logging.INFO)", "\nlogging.getLogger().setLevel(logging.INFO)\nfrom dataset2metadata.registry import d2m_to_datacomp_keys\n\nclass Writer(object):\n\n    def __init__(\n            self,\n            name: str,\n            feature_fields: List[str],\n            parquet_fields: List[str],\n            use_datacomp_keys: bool,\n        ) -> None:\n        self.name = name\n\n        # store things like CLIP features, ultimately in an npz\n        self.feature_store = {e: [] for e in feature_fields}\n\n        # store other metadata like image height, ultimately in a parquet\n        self.parquet_store = {e: [] for e in parquet_fields}\n\n        # if true, then we will remap keys using d2m_to_datacomp_keys for some fields\n        self.use_datacomp_keys = use_datacomp_keys\n\n    def update_feature_store(self, k, v):\n        if self.use_datacomp_keys:\n            if k in d2m_to_datacomp_keys:\n                self.feature_store[d2m_to_datacomp_keys[k]].append(v)\n            else:\n                self.feature_store[k].append(v)\n        else:\n            self.feature_store[k].append(v)\n\n    def update_parquet_store(self, k, v):\n        if self.use_datacomp_keys:\n            if k in d2m_to_datacomp_keys:\n                self.parquet_store[d2m_to_datacomp_keys[k]].append(v)\n            else:\n                self.parquet_store[k].append(v)\n        else:\n            self.parquet_store[k].append(v)\n\n    def write(self, out_dir_path):\n        try:\n            for k in self.feature_store:\n                self.feature_store[k] = self._flatten_helper(self.feature_store[k], to_npy=True)\n\n            for k in self.parquet_store:\n                self.parquet_store[k] = self._flatten_helper(self.parquet_store[k])\n\n            if len(self.parquet_store):\n                df = pd.DataFrame.from_dict(self.parquet_store)\n                df.to_parquet(os.path.join(out_dir_path, f'{self.name}.parquet'), engine='pyarrow')\n                logging.info(f'saved metadata: {f\"{self.name}.parquet\"}')\n\n            if len(self.feature_store):\n                fs, output_path = fsspec.core.url_to_fs(os.path.join(out_dir_path, f'{self.name}.npz'))\n                with fs.open(output_path, \"wb\") as f:\n                    np.savez_compressed(f, **self.feature_store)\n                    logging.info(f'saved features: {f\"{self.name}.npz\"}')\n\n                return True\n\n        except Exception as e:\n            logging.exception(e)\n            logging.error(f'failed to write metadata for shard: {self.name}')\n            return False\n\n    def _flatten_helper(self, l, to_npy=False):\n        if len(l):\n            if torch.is_tensor(l[0]):\n                if to_npy:\n                    return torch.cat(l, dim=0).float().numpy()\n                return torch.cat(l, dim=0).float().tolist()\n            else:\n                l_flat = []\n                for e in l:\n                    l_flat.extend(e)\n\n                return l_flat\n        return l"]}
{"filename": "dataset2metadata/registry.py", "chunked_list": ["import inspect\nfrom functools import partial\n\nimport dataset2metadata.models as models\nimport dataset2metadata.postprocessors as post\nimport dataset2metadata.preprocessors as pre\n\n# Models\nmodel_lookup = {\n    cls.name: cls", "model_lookup = {\n    cls.name: cls\n    for _, cls in models.__dict__.items()\n    if hasattr(cls, \"name\") and inspect.isclass(cls)\n}\n\n# Preprocessors\npreprocessor_lookup = {\n    \"clip-aug\": pre.oai_clip_image,\n    \"clip-tokens\": pre.oai_clip_text,", "    \"clip-aug\": pre.oai_clip_image,\n    \"clip-tokens\": pre.oai_clip_text,\n    \"identity\": pre.identity,\n    \"dedup-aug\": pre.dedup,\n    \"faces-aug\": pre.faces_scrfd,\n}\n\n# Postprocessors\npostprocess_parquet_lookup = {\n    \"oai-clip-vit-b32-score\": partial(", "postprocess_parquet_lookup = {\n    \"oai-clip-vit-b32-score\": partial(\n        post.batched_dot_product, model=\"oai-clip-vit-b32\"\n    ),\n    \"oai-clip-vit-l14-score\": partial(\n        post.batched_dot_product, model=\"oai-clip-vit-l14\"\n    ),\n    \"nsfw-detoxify-score\": partial(post.identity, model=\"nsfw-detoxify\"),\n    \"nsfw-image-score\": partial(post.identity, model=\"nsfw-image-oai-clip-vit-l-14\"),\n    \"dedup-isc-ft-v107-score\": partial(post.select, model=\"dedup-isc-ft-v107\", index=1),", "    \"nsfw-image-score\": partial(post.identity, model=\"nsfw-image-oai-clip-vit-l-14\"),\n    \"dedup-isc-ft-v107-score\": partial(post.select, model=\"dedup-isc-ft-v107\", index=1),\n    \"json-transpose\": partial(post.transpose_list, model=\"json\"),\n    \"face-boxes\": partial(post.identity, model=\"faces-scrfd10g\", to_cpu=False),\n}\n\npostprocess_feature_lookup = {\n    \"oai-clip-vit-b32-image\": partial(post.select, model=\"oai-clip-vit-b32\", index=0),\n    \"oai-clip-vit-b32-text\": partial(post.select, model=\"oai-clip-vit-b32\", index=1),\n    \"oai-clip-vit-l14-image\": partial(post.select, model=\"oai-clip-vit-l14\", index=0),", "    \"oai-clip-vit-b32-text\": partial(post.select, model=\"oai-clip-vit-b32\", index=1),\n    \"oai-clip-vit-l14-image\": partial(post.select, model=\"oai-clip-vit-l14\", index=0),\n    \"oai-clip-vit-l14-text\": partial(post.select, model=\"oai-clip-vit-l14\", index=1),\n    \"dedup-isc-ft-v107-image\": partial(post.select, model=\"dedup-isc-ft-v107\", index=0),\n}\n\nd2m_to_datacomp_keys = {\n    \"oai-clip-vit-b32-score\": \"clip_b32_similarity_score\",\n    \"oai-clip-vit-l14-score\": \"clip_l14_similarity_score\",\n    \"face-boxes\": \"face_bboxes\",", "    \"oai-clip-vit-l14-score\": \"clip_l14_similarity_score\",\n    \"face-boxes\": \"face_bboxes\",\n    \"oai-clip-vit-b32-image\": \"b32_img\",\n    \"oai-clip-vit-b32-text\": \"b32_txt\",\n    \"oai-clip-vit-l14-image\": \"l14_img\",\n    \"oai-clip-vit-l14-text\": \"l14_txt\",\n    \"dedup-isc-ft-v107-image\": \"dedup\",\n}\n\n# update functions\ndef update_registry(module):\n    global model_lookup\n    global preprocessor_lookup\n    global postprocess_parquet_lookup\n    global postprocess_feature_lookup\n\n    model_lookup = {\n        **model_lookup,\n        **module.model_lookup,\n    }\n\n    preprocessor_lookup = {**preprocessor_lookup, **module.preprocessor_lookup}\n\n    postprocess_parquet_lookup = {\n        **postprocess_parquet_lookup,\n        **module.postprocess_parquet_lookup,\n    }\n\n    postprocess_feature_lookup = {\n        **postprocess_feature_lookup,\n        **module.postprocess_feature_lookup,\n    }", "\n# update functions\ndef update_registry(module):\n    global model_lookup\n    global preprocessor_lookup\n    global postprocess_parquet_lookup\n    global postprocess_feature_lookup\n\n    model_lookup = {\n        **model_lookup,\n        **module.model_lookup,\n    }\n\n    preprocessor_lookup = {**preprocessor_lookup, **module.preprocessor_lookup}\n\n    postprocess_parquet_lookup = {\n        **postprocess_parquet_lookup,\n        **module.postprocess_parquet_lookup,\n    }\n\n    postprocess_feature_lookup = {\n        **postprocess_feature_lookup,\n        **module.postprocess_feature_lookup,\n    }", ""]}
{"filename": "dataset2metadata/process.py", "chunked_list": ["import os\nimport pathlib\nfrom importlib.machinery import SourceFileLoader\nfrom typing import List\n\nimport torch\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport hashlib", "\nimport hashlib\nimport logging\nfrom pathlib import Path\n\nimport fsspec\nimport yaml\nfrom PIL import ImageFile\n\nfrom dataset2metadata.dataloaders import create_loader", "\nfrom dataset2metadata.dataloaders import create_loader\nfrom dataset2metadata.registry import update_registry\nfrom dataset2metadata.utils import topsort\nfrom dataset2metadata.writer import Writer\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nlogging.getLogger().setLevel(logging.INFO)\n\ndef check_yml(yml):\n\n    # manditory fields in the yml\n    yml_fields = [\n        'models',\n        'nworkers',\n        'batch_size',\n        'device',\n        'input_tars',\n        'output_metadata_dir',\n    ]\n\n    yml_optional_fields = [\n        'postprocess_columns',\n        'postprocess_features',\n        'additional_fields',\n        'custom_pypath',\n        'reprocess',\n        'use_datacomp_keys'\n    ]\n\n    for f in yml_fields:\n        if f not in yml:\n            raise ValueError(f'missing required yml field: {f}')\n\n    for f in yml:\n        if f not in yml_fields + yml_optional_fields:\n            raise ValueError(f'unknown field: {f}')", "\ndef check_yml(yml):\n\n    # manditory fields in the yml\n    yml_fields = [\n        'models',\n        'nworkers',\n        'batch_size',\n        'device',\n        'input_tars',\n        'output_metadata_dir',\n    ]\n\n    yml_optional_fields = [\n        'postprocess_columns',\n        'postprocess_features',\n        'additional_fields',\n        'custom_pypath',\n        'reprocess',\n        'use_datacomp_keys'\n    ]\n\n    for f in yml_fields:\n        if f not in yml:\n            raise ValueError(f'missing required yml field: {f}')\n\n    for f in yml:\n        if f not in yml_fields + yml_optional_fields:\n            raise ValueError(f'unknown field: {f}')", "\ndef process(\n    yml,\n):\n    if type(yml) is str:\n        # parse yml and check resulting dict\n        yml = yaml.safe_load(Path(yml).read_text())\n\n    check_yml(yml)\n\n    # if local out dir does not exist make it\n    fs, output_path = fsspec.core.url_to_fs(yml['output_metadata_dir'])\n    fs.makedirs(output_path, exist_ok=True)\n\n    # assign a name to the group of shards being processed\n    name = hashlib.md5(str(yml['input_tars']).encode()).hexdigest()\n\n    # cache if result already there and user does not want to reprocess\n    if 'reprocess' not in yml or not yml['reprocess']:\n        # cache\n        completed = fs.ls(output_path)\n        completed_parquets = [p for p in completed if 'parquet' in p]\n        if name in set([Path(s).stem for s in completed_parquets]):\n            logging.info(f'found cached result: {name}')\n            return\n\n    # if the user specifies specific custom implementaion of their own update the registry\n    if 'custom_pypath' in yml and yml['custom_pypath'] is not None:\n        custom = SourceFileLoader(\n            pathlib.Path(yml['custom_pypath']).stem,\n            yml['custom_pypath']\n        ).load_module()\n\n        update_registry(custom)\n\n    # import from registry here after we have updated\n    from dataset2metadata.registry import (model_lookup,\n                                           postprocess_feature_lookup,\n                                           postprocess_parquet_lookup)\n\n    # create dataloader based on user input\n    dataloader, input_map = create_loader(\n        yml['input_tars'],\n        yml['models'],\n        yml['additional_fields'],\n        yml['nworkers'],\n        yml['batch_size'],\n    )\n\n    # initializing models\n    models = {m_str: model_lookup[m_str](yml['device']) for m_str in yml['models']}\n\n    # deciding order to run them in based on dependencies\n    topsort_order = topsort(\n        {m_str: model_lookup[m_str].dependencies for m_str in yml['models']}\n    )\n\n    logging.info(f'topsort model evaluation order: {topsort_order}')\n\n    # initialize the writer that stores results and dumps them to store\n    # TODO: fix the name here\n    feature_fields = []\n    parquet_fields = []\n    if 'postprocess_features' in yml:\n        feature_fields = yml['postprocess_features']\n    if 'postprocess_columns' in yml:\n        parquet_fields.extend(yml['postprocess_columns'])\n    if 'additional_fields' in yml:\n        parquet_fields.extend(yml['additional_fields'])\n\n    writer = Writer(\n        name,\n        feature_fields,\n        parquet_fields,\n        yml['use_datacomp_keys'] if 'use_datacomp_keys' in yml else False\n    )\n\n    for sample in dataloader:\n        model_outputs = {}\n\n        # eval all models sequentially in a top sort order\n        for m_str in topsort_order:\n\n            model_input = []\n            cache = {}\n\n            # fill the model input\n            for i in input_map[m_str]:\n\n                if isinstance(i, int):\n                    if models[m_str].to_device and i not in cache:\n                        if isinstance(sample[i], List):\n                            # if list needs to be moved to device transpose and move it\n                            sample[i] = list(zip(*sample[i]))\n                            for j in range(len(sample[i])):\n                                sample[i][j] = torch.cat(sample[i][j]).to(yml['device'])\n                            cache[i] = sample[i]\n                        else:\n                            cache[i] = sample[i].to(yml['device'])\n                    else:\n                        cache[i] = sample[i]\n\n                    model_input.append(cache[i])\n                else:\n                    # use previously computed outputs and new inputs\n                    # NOTE: assume downstream model consumes on same device as upstream\n                    assert i in model_outputs\n                    model_input.append(model_outputs[i])\n\n            with torch.no_grad():\n                model_outputs[m_str] = models[m_str](*model_input)\n\n            # TODO: make this more general, right now assumes last entry is json fields\n            if len(yml['additional_fields']):\n                model_outputs['json'] = sample[-1]\n\n        if 'postprocess_features' in yml:\n            for k in yml['postprocess_features']:\n                writer.update_feature_store(k, postprocess_feature_lookup[k](model_outputs))\n\n        if 'postprocess_columns' in yml:\n            for k in yml['postprocess_columns']:\n                writer.update_parquet_store(k, postprocess_parquet_lookup[k](model_outputs))\n\n        # if additional fields from json need to be saved, add those to the store\n        if 'additional_fields' in yml and len(yml['additional_fields']):\n            transposed_additional_fields = postprocess_parquet_lookup['json-transpose'](model_outputs)\n            assert len(transposed_additional_fields) == len(yml['additional_fields'])\n            for i, v in enumerate(transposed_additional_fields):\n                writer.update_parquet_store(yml['additional_fields'][i], v)\n\n    writer.write(yml['output_metadata_dir'])", ""]}
{"filename": "dataset2metadata/preprocessors.py", "chunked_list": ["from functools import partial\n\nimport torchvision.transforms as T\nfrom dataset2metadata.augmentations import SquarePadResizeNorm\nfrom clip import clip\nimport simdjson\nfrom typing import List\n\nCLIP_SIZE = 224\nFACES_SIZE = 224", "CLIP_SIZE = 224\nFACES_SIZE = 224\nDEDUP_SIZE = 512\n\nCLIP_IMAGE_TRANFORM = clip._transform(n_px=CLIP_SIZE)\nCLIP_TEXT_TOKENIZER = partial(clip.tokenize, truncate=True)\nFACES_IMAGE_TRANFORM = SquarePadResizeNorm(img_size=FACES_SIZE)\nDEDUP_IMAGE_TRANFORM = T.Compose(\n    [\n        T.Resize((DEDUP_SIZE, DEDUP_SIZE)),", "    [\n        T.Resize((DEDUP_SIZE, DEDUP_SIZE)),\n        T.ToTensor(),\n        T.Normalize(\n            mean=(0.5, 0.5, 0.5),\n            std=(0.5, 0.5, 0.5),\n        ),\n    ]\n)\nJSON_PARSER = simdjson.Parser()", ")\nJSON_PARSER = simdjson.Parser()\n\n\ndef oai_clip_image(x):\n    return CLIP_IMAGE_TRANFORM(x)\n\n\ndef oai_clip_text(t):\n    return CLIP_TEXT_TOKENIZER(t)[0]", "def oai_clip_text(t):\n    return CLIP_TEXT_TOKENIZER(t)[0]\n\n\ndef faces_scrfd(x):\n    return FACES_IMAGE_TRANFORM(x)\n\n\ndef dedup(x):\n    return DEDUP_IMAGE_TRANFORM(x)", "def dedup(x):\n    return DEDUP_IMAGE_TRANFORM(x)\n\n\ndef json_decoder(key, value, json_keys):\n    if not key.endswith(\"json\") or json_keys is None:\n        return None\n\n    json_dict = JSON_PARSER.parse(value).as_dict()\n\n    return [json_dict[k] for k in json_keys]", "\n\ndef identity(a):\n    return a\n"]}
{"filename": "dataset2metadata/main.py", "chunked_list": ["import fire\nfrom dataset2metadata.process import process\n\n\ndef main():\n    fire.Fire(process)\n\n\nif __name__ == \"__main__\":\n    main()", "if __name__ == \"__main__\":\n    main()\n"]}
{"filename": "dataset2metadata/models.py", "chunked_list": ["import abc\nimport logging\nimport os\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom clip import clip\nfrom detoxify import Detoxify\nfrom isc_feature_extractor import create_model", "from detoxify import Detoxify\nfrom isc_feature_extractor import create_model\nfrom dataset2metadata.utils import download\nfrom dataset2metadata.face_detection.scrfd_wrapper import FaceDetector\n\nlogging.getLogger().setLevel(logging.INFO)\n\n\nclass WrapperMixin(metaclass=abc.ABCMeta):\n    pass", "class WrapperMixin(metaclass=abc.ABCMeta):\n    pass\n\n\nclass OaiClipWrapper(nn.Module, WrapperMixin, metaclass=abc.ABCMeta):\n    def __init__(self, model_name: str, device: str) -> None:\n        super().__init__()\n        self.model, _ = clip.load(model_name, device=device)\n        self.model.eval()\n\n    def forward(self, x, t):\n        image_feature = self.model.encode_image(x)\n        text_feature = self.model.encode_text(t)\n\n        # normalized features\n        image_feature = image_feature / image_feature.norm(dim=1, keepdim=True)\n        text_feature = text_feature / text_feature.norm(dim=1, keepdim=True)\n\n        return image_feature, text_feature", "\n\nclass OaiClipVitB32Wrapper(OaiClipWrapper):\n    name = \"oai-clip-vit-b32\"\n    raw_inputs = [\"image\", \"text\"]\n    preprocessors = [\"clip-aug\", \"clip-tokens\"]\n    dependencies = []\n    to_device = True\n\n    def __init__(self, device) -> None:\n        super().__init__(\"ViT-B/32\", device)\n        logging.info(f\"instantiated {self.name} on {device}\")", "\n\nclass OaiClipVitL14Wrapper(OaiClipWrapper):\n    name = \"oai-clip-vit-l14\"\n    raw_inputs = [\"image\", \"text\"]\n    preprocessors = [\"clip-aug\", \"clip-tokens\"]\n    dependencies = []\n    to_device = True\n\n    def __init__(self, device) -> None:\n        super().__init__(\"ViT-L/14\", device)\n        logging.info(f\"instantiated {self.name} on {device}\")", "\n\nclass DetoxifyWrapper(nn.Module, WrapperMixin):\n    name = \"nsfw-detoxify\"\n    raw_inputs = [\n        \"text\",\n    ]\n    preprocessors = [\n        \"identity\",\n    ]\n    dependencies = []\n    to_device = False\n\n    def __init__(self, device) -> None:\n        super().__init__()\n        self.model = Detoxify(\"multilingual\", device=f\"cuda:{device}\")\n        logging.info(f\"instantiated {self.name} on {device}\")\n\n    def forward(self, t):\n        scores = []\n        preds = self.model.predict(t)\n        for _, v in preds.items():\n            scores.append(v)\n\n        # column-wise max score\n        maxi, _ = torch.tensor(scores).transpose(0, 1).max(axis=1)\n\n        return maxi", "\n\nclass NsfwImageWrapper(nn.Module, WrapperMixin):\n    name = \"nsfw-image-oai-clip-vit-l-14\"\n    raw_inputs = []\n    preprocessors = []\n    dependencies = [\"oai-clip-vit-l14\"]\n    to_device = True\n\n    def __init__(self, device) -> None:\n        super().__init__()\n        self.model = self.load(download(\"nsfw-image\"))\n        self.model.to(device).eval()\n        logging.info(f\"instantiated {self.name} on {device}\")\n\n    def load(self, classifier_path):\n        names = [\n            \"norm\",\n            \"dense\",\n            \"relu\",\n            \"dense1\",\n            \"relu1\",\n            \"dense2\",\n            \"relu2\",\n            \"dense3\",\n            \"sigmoid\",\n        ]\n\n        model = nn.Sequential(\n            OrderedDict(\n                [\n                    (names[0], nn.BatchNorm1d(768, eps=0.0)),\n                    (names[1], nn.Linear(768, 64)),\n                    (names[2], nn.ReLU()),\n                    (names[3], nn.Linear(64, 512)),\n                    (names[4], nn.ReLU()),\n                    (names[5], nn.Linear(512, 256)),\n                    (names[6], nn.ReLU()),\n                    (names[7], nn.Linear(256, 1)),\n                    (names[8], nn.Sigmoid()),\n                ]\n            )\n        )\n\n        state_dict = torch.load(classifier_path, map_location=\"cpu\")\n        model.load_state_dict(state_dict)\n\n        return model\n\n    def forward(self, z):\n        # use only the image feature\n        return self.model(z[0].float()).squeeze()", "\n\nclass IscFtV107Wrapper(nn.Module, WrapperMixin):\n    name = \"dedup-isc-ft-v107\"\n    raw_inputs = [\n        \"image\",\n    ]\n    preprocessors = [\n        \"dedup-aug\",\n    ]\n    dependencies = []\n    to_device = True\n\n    def __init__(self, device) -> None:\n        super().__init__()\n        self.model, _ = create_model(weight_name=\"isc_ft_v107\", device=device)\n        self.model.eval()\n        self.reference_embeddings = (\n            torch.load(download(\"dedup-embeddings\")).to(device).t()\n        )\n        self.reference_embeddings.requires_grad = False\n        logging.info(f\"instantiated {self.name} on {device}\")\n\n    def forward(self, x):\n        z = self.model(x)\n        z /= z.norm(dim=-1, keepdim=True)\n        scores = z @ self.reference_embeddings  # (b,c) @ (c,n) = (b,n)\n        max_scores, _ = scores.max(axis=1)\n\n        return z, max_scores", "\n\nclass Scrfd10GWrapper(nn.Module, WrapperMixin):\n    name = \"faces-scrfd10g\"\n    raw_inputs = [\n        \"image\",\n    ]\n    preprocessors = [\n        \"faces-aug\",\n    ]\n    dependencies = []\n    to_device = True\n\n    def __init__(self, device) -> None:\n        super().__init__()\n        self.model = FaceDetector(download(\"faces-scrfd10g\"), device)\n        logging.info(f\"instantiated {self.name} on {device}\")\n\n    def forward(self, x):\n        return self.model.detect_faces(images=x[0], paddings=x[1])", ""]}
{"filename": "dataset2metadata/dataloaders.py", "chunked_list": ["from functools import partial\n\nimport webdataset as wds\nfrom dataset2metadata.preprocessors import json_decoder\n\n\ndef get_to_tuple_directives(models, additional_fields):\n\n    # import here as registry may have updated\n    from dataset2metadata.registry import model_lookup\n\n    wrapper_classes = [model_lookup[m] for m in models]\n\n    input_map = {}\n\n    # get unique preprocessor directive, which is a raw_input, preprocessor pair\n    unique_derectives = []\n\n    for i, model_class in enumerate(wrapper_classes):\n        assert len(model_class.preprocessors) == len(model_class.raw_inputs)\n\n        preprocess_directives = [\n            (model_class.raw_inputs[k], model_class.preprocessors[k]) for k in range(len(model_class.preprocessors))\n        ]\n\n        input_map[models[i]] = []\n\n        for j in range(len(preprocess_directives)):\n            if preprocess_directives[j] not in unique_derectives:\n                input_map[models[i]].append(len(unique_derectives))\n                unique_derectives.append(preprocess_directives[j])\n            else:\n                input_map[models[i]].append(unique_derectives.index(preprocess_directives[j]))\n\n        if len(model_class.dependencies):\n            # non-numeric, nameded dependencies, i.e., the outputs of other models\n            input_map[models[i]].extend(model_class.dependencies)\n\n    # add directives to include data from the tars into the webdataset\n    if additional_fields is not None and len(additional_fields):\n        # NOTE: currently no support for these additional fields being taken as inputs to models\n        input_map['json'] = [len(unique_derectives), ]\n        unique_derectives.append(('json', 'identity'))\n\n    return unique_derectives, input_map", "\ndef create_loader(input_shards, models, additional_fields, nworkers, batch_size):\n\n    # import here as registry may have updated\n    from dataset2metadata.registry import preprocessor_lookup\n\n    (\n        unique_derectives,\n        input_map,\n    ) = get_to_tuple_directives(models, additional_fields)\n\n    tuple_fields = [e[0] for e in unique_derectives]\n    unique_preprocessors = [\n        preprocessor_lookup[e[-1]] for e in unique_derectives\n    ]\n\n    pipeline = [wds.SimpleShardList(input_shards), ]\n\n    pipeline.extend([\n        wds.split_by_worker,\n        wds.tarfile_to_samples(handler=wds.warn_and_continue),\n        wds.decode(\n            'pilrgb',\n            partial(json_decoder, json_keys=additional_fields),\n            handler=wds.warn_and_continue),\n        wds.rename(image='jpg;png;jpeg;webp', text='txt'),\n        wds.to_tuple(*tuple_fields),\n        wds.map_tuple(*unique_preprocessors),\n        wds.batched(batch_size, partial=True),\n    ])\n\n    loader = wds.WebLoader(\n        wds.DataPipeline(*pipeline),\n        batch_size=None,\n        shuffle=False,\n        num_workers=nworkers,\n        persistent_workers=True,\n    )\n\n    return loader, input_map"]}
{"filename": "dataset2metadata/__init__.py", "chunked_list": ["__version__ = \"0.1.0\"\n__author__ = \"Samir Gadre\"\n__credits__ = \"DataComp team\"\n"]}
{"filename": "dataset2metadata/utils.py", "chunked_list": ["import hashlib\nimport os\nimport random\nimport urllib\nimport warnings\nfrom typing import Dict, List, Set\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm", "import torch\nfrom tqdm import tqdm\n\n\ndef random_seed(seed: int = 0) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef topsort(graph: Dict[str, List[str]]) -> List[str]:\n    # from: https://stackoverflow.com/questions/52432988/python-dict-key-order-based-on-values-recursive-solution\n    result: List[str] = []\n    seen: Set[str] = set()\n\n    def recursive_helper(node: str) -> None:\n        for neighbor in graph.get(node, []):\n            if neighbor not in seen:\n                seen.add(neighbor)\n                recursive_helper(neighbor)\n        if node not in result:\n            result.append(node)\n\n    for key in graph.keys():\n        recursive_helper(key)\n\n    return result", "\n\ndef topsort(graph: Dict[str, List[str]]) -> List[str]:\n    # from: https://stackoverflow.com/questions/52432988/python-dict-key-order-based-on-values-recursive-solution\n    result: List[str] = []\n    seen: Set[str] = set()\n\n    def recursive_helper(node: str) -> None:\n        for neighbor in graph.get(node, []):\n            if neighbor not in seen:\n                seen.add(neighbor)\n                recursive_helper(neighbor)\n        if node not in result:\n            result.append(node)\n\n    for key in graph.keys():\n        recursive_helper(key)\n\n    return result", "\n\ndef download(name: str, root: str = None):\n    # modified from oai _download clip function\n\n    if root is None:\n        root = os.path.expanduser(\"~/.cache/dataset2metadata\")\n\n    cloud_checkpoints = {\n        \"nsfw-image\": {\n            # 'url': 'file://' + os.path.abspath('./assets/nsfw_torch.pt'),\n            \"url\": \"https://github.com/mlfoundations/dataset2metadata/releases/download/v0.1.0-alpha/nsfw_torch.pt\",\n            \"sha256\": \"3c97d5478477c181bfa29a33e6933f710c8ec587e3c3551ff855e293acdaf390\",\n        },\n        \"faces-scrfd10g\": {\n            # 'url': 'file://' + os.path.abspath('./assets/scrfd_10g.pt'),\n            \"url\": \"https://github.com/mlfoundations/dataset2metadata/releases/download/v0.1.0-alpha/scrfd_10g.pt\",\n            \"sha256\": \"963570df5e0ebf6bb313239d0f9f3f0c096c1ff6937e8e28e45abad4d8b1d5c7\",\n        },\n        \"dedup-embeddings\": {\n            # 'url': 'file://' + os.path.abspath('./assets/eval_dedup_embeddings.pt'),\n            \"url\": \"https://github.com/mlfoundations/dataset2metadata/releases/download/v0.1.0-alpha/eval_dedup_embeddings.pt\",\n            \"sha256\": \"18aeb08e1d53638761e05ac95254a2ef49653bfdb59a631c9304117ceef23d14\",\n        },\n    }\n\n    if name not in cloud_checkpoints:\n        raise ValueError(\n            f\"unsupported cloud checkpoint: {name}. currently we only support: {list(cloud_checkpoints.keys())}\"\n        )\n\n    os.makedirs(root, exist_ok=True)\n\n    expected_sha256 = cloud_checkpoints[name][\"sha256\"]\n    download_target = os.path.join(root, f\"{name}.pt\")\n    url = cloud_checkpoints[name][\"url\"]\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if (\n            hashlib.sha256(open(download_target, \"rb\").read()).hexdigest()\n            == expected_sha256\n        ):\n            return download_target\n        else:\n            warnings.warn(\n                f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\"\n            )\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(\n            total=int(source.info().get(\"Content-Length\")),\n            ncols=80,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if (\n        hashlib.sha256(open(download_target, \"rb\").read()).hexdigest()\n        != expected_sha256\n    ):\n        raise RuntimeError(\n            \"Model has been downloaded but the SHA256 checksum does not not match\"\n        )\n\n    return download_target", "\n\ndef download_all():\n    for k in [\"nsfw-image\", \"faces-scrfd10g\", \"dedup-embeddings\"]:\n        download(k)\n"]}
{"filename": "dataset2metadata/augmentations.py", "chunked_list": ["import torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nimport PIL\nimport numpy as np\n\n\nclass SquarePadResizeNorm:\n    \"\"\"Pad an image to be square, and resize to given shape.\n\n    Credit to Sunding Wei:\n    https://discuss.pytorch.org/t/how-to-resize-and-pad-in-a-torchvision-transforms-compose/71850/5\n    \"\"\"\n\n    def __init__(self, img_size, norm_mean=(0.5, 0.5, 0.5), norm_std=(0.5, 0.5, 0.5)):\n        self.img_size = img_size\n        self.resize = T.Resize(\n            img_size, interpolation=T.InterpolationMode.BICUBIC\n        )  # Image will already be square\n        self.normalize = T.Normalize(\n            mean=norm_mean,\n            std=norm_std,\n        )\n\n    def __call__(self, image):\n        if isinstance(image, torch.Tensor):\n            h, w = image.shape[-2:]\n            pad_value = 1.0\n\n        elif isinstance(image, PIL.Image.Image):\n            w, h = image.size\n            pad_value = 255\n\n        else:\n            raise TypeError(\"Unsupported image type.\")\n\n        max_wh = np.max([w, h])\n        hp = int((max_wh - w) / 2)\n        vp = int((max_wh - h) / 2)\n        padding = [hp, vp, max_wh - w - hp, max_wh - h - vp]\n\n        padded_img = F.pad(image, padding, pad_value, \"constant\")\n\n        # Calculate offsets with respect to the original image size.\n        padding = torch.from_numpy(np.asarray(padding, dtype=np.float32) / max_wh)\n\n        resized_img = self.resize(padded_img)\n\n        if isinstance(resized_img, PIL.Image.Image):\n            resized_img = F.to_tensor(resized_img)\n\n        return self.normalize(resized_img).unsqueeze(0), padding.unsqueeze(0)", ""]}
{"filename": "dataset2metadata/postprocessors.py", "chunked_list": ["import torch\nfrom typing import Dict, Any\n\n\ndef _cpu_helper(entry: Any, to_cpu: bool):\n    if to_cpu:\n        return entry.cpu()\n    return entry\n\n\ndef batched_dot_product_index(\n    cache: Dict, model: str, i: int, j: int, to_cpu: bool = True\n):\n    return _cpu_helper(\n        torch.einsum(\"bn,bn->b\", cache[model][i], cache[model][j]), to_cpu\n    )", "\n\ndef batched_dot_product_index(\n    cache: Dict, model: str, i: int, j: int, to_cpu: bool = True\n):\n    return _cpu_helper(\n        torch.einsum(\"bn,bn->b\", cache[model][i], cache[model][j]), to_cpu\n    )\n\n\ndef batched_dot_product(cache: Dict, model: str, to_cpu: bool = True):\n    return batched_dot_product_index(cache, model, 0, 1, to_cpu=to_cpu)", "\n\ndef batched_dot_product(cache: Dict, model: str, to_cpu: bool = True):\n    return batched_dot_product_index(cache, model, 0, 1, to_cpu=to_cpu)\n\n\ndef select(cache: Dict, model: str, index: int, to_cpu: bool = True):\n    return _cpu_helper(cache[model][index], to_cpu)\n\n\ndef identity(cache: Dict, model: str, to_cpu: bool = True):\n    return _cpu_helper(cache[model], to_cpu)", "\n\ndef identity(cache: Dict, model: str, to_cpu: bool = True):\n    return _cpu_helper(cache[model], to_cpu)\n\n\ndef transpose_list(cache: Dict, model: str):\n    return list(map(list, zip(*cache[model])))\n", ""]}
{"filename": "dataset2metadata/face_detection/backbone.py", "chunked_list": ["# contributed by George Smyrnis\n\nimport torch.nn as nn\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\n# Backbone\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        dilation=1,\n        downsample=None,\n        style=\"pytorch\",\n        with_cp=False,\n        conv_cfg=None,\n        norm_cfg=dict(type=\"BN\"),\n    ):\n        super(BasicBlock, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        # print('init basic block:', inplanes, planes)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False,\n        )\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        out = self.relu(out)\n\n        return out", "class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        dilation=1,\n        downsample=None,\n        style=\"pytorch\",\n        with_cp=False,\n        conv_cfg=None,\n        norm_cfg=dict(type=\"BN\"),\n    ):\n        super(BasicBlock, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        # print('init basic block:', inplanes, planes)\n\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False,\n        )\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        out = self.relu(out)\n\n        return out", "\n\nclass ResNet(nn.Module):\n    \"\"\"ResNet backbone.\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        stem_channels (int | None): Number of stem channels. If not specified,\n            it will be the same as `base_channels`. Default: None.\n        base_channels (int): Number of base channels of res layer. Default: 64.\n        in_channels (int): Number of input image channels. Default: 3.\n        num_stages (int): Resnet stages. Default: 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        plugins (list[dict]): List of plugins for stages, each dict contains:\n            - cfg (dict, required): Cfg dict to build plugin.\n            - position (str, required): Position inside block to insert\n              plugin, options are 'after_conv1', 'after_conv2', 'after_conv3'.\n            - stages (tuple[bool], optional): Stages to apply plugin, length\n              should be same as 'num_stages'.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): Whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    Example:\n        >>> from mmdet.models import ResNet\n        >>> import torch\n        >>> self = ResNet(depth=18)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 64, 8, 8)\n        (1, 128, 4, 4)\n        (1, 256, 2, 2)\n        (1, 512, 1, 1)\n    \"\"\"\n\n    arch_settings = {\n        0: (BasicBlock, (2, 2, 2, 2)),\n        18: (BasicBlock, (2, 2, 2, 2)),\n        19: (BasicBlock, (2, 4, 4, 1)),\n        20: (BasicBlock, (2, 3, 2, 2)),\n        22: (BasicBlock, (2, 4, 3, 1)),\n        24: (BasicBlock, (2, 4, 4, 1)),\n        26: (BasicBlock, (2, 4, 4, 2)),\n        28: (BasicBlock, (2, 5, 4, 2)),\n        29: (BasicBlock, (2, 6, 3, 2)),\n        30: (BasicBlock, (2, 5, 5, 2)),\n        32: (BasicBlock, (2, 6, 5, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        35: (BasicBlock, (3, 6, 4, 3)),\n        38: (BasicBlock, (3, 8, 4, 3)),\n        40: (BasicBlock, (3, 8, 5, 3)),\n    }\n\n    def __init__(\n        self,\n        depth,\n        in_channels=3,\n        stem_channels=None,\n        base_channels=64,\n        num_stages=4,\n        block_cfg=None,\n        strides=(1, 2, 2, 2),\n        dilations=(1, 1, 1, 1),\n        out_indices=(0, 1, 2, 3),\n        style=\"pytorch\",\n        deep_stem=False,\n        avg_down=False,\n        no_pool33=False,\n        frozen_stages=-1,\n        conv_cfg=None,\n        norm_cfg=dict(type=\"BN\", requires_grad=True),\n        norm_eval=True,\n        dcn=None,\n        stage_with_dcn=(False, False, False, False),\n        plugins=None,\n        with_cp=False,\n        zero_init_residual=True,\n    ):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(f\"invalid depth {depth} for resnet\")\n        self.depth = depth\n        if stem_channels is None:\n            stem_channels = base_channels\n        self.stem_channels = stem_channels\n        self.base_channels = base_channels\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.deep_stem = deep_stem\n        self.avg_down = avg_down\n        self.no_pool33 = no_pool33\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.plugins = plugins\n        self.zero_init_residual = zero_init_residual\n        if block_cfg is None:\n            self.block, stage_blocks = self.arch_settings[depth]\n        else:\n            self.block = BasicBlock\n            stage_blocks = block_cfg[\"stage_blocks\"]\n            assert len(stage_blocks) >= num_stages\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = stem_channels\n\n        self._make_stem_layer(in_channels, stem_channels)\n        if block_cfg is not None and \"stage_planes\" in block_cfg:\n            stage_planes = block_cfg[\"stage_planes\"]\n        else:\n            stage_planes = [base_channels * 2**i for i in range(num_stages)]\n\n        # print('resnet cfg:', stage_blocks, stage_planes)\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            if plugins is not None:\n                stage_plugins = self.make_stage_plugins(plugins, i)\n            else:\n                stage_plugins = None\n            # planes = base_channels * 2**i\n            planes = stage_planes[i]\n            # print('block detail:', i, self.inplanes, planes, stride)\n            res_layer = self.make_res_layer(\n                block=self.block,\n                inplanes=self.inplanes,\n                planes=planes,\n                num_blocks=num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                avg_down=self.avg_down,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n            )\n            self.inplanes = planes * self.block.expansion\n            layer_name = f\"layer{i + 1}\"\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = (\n            self.block.expansion * base_channels * 2 ** (len(self.stage_blocks) - 1)\n        )\n\n    def make_stage_plugins(self, plugins, stage_idx):\n        \"\"\"Make plugins for ResNet ``stage_idx`` th stage.\n        Currently we support to insert ``context_block``,\n        ``empirical_attention_block``, ``nonlocal_block`` into the backbone\n        like ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of\n        Bottleneck.\n        An example of plugins format could be:\n        Examples:\n            >>> plugins=[\n            ...     dict(cfg=dict(type='xxx', arg1='xxx'),\n            ...          stages=(False, True, True, True),\n            ...          position='after_conv2'),\n            ...     dict(cfg=dict(type='yyy'),\n            ...          stages=(True, True, True, True),\n            ...          position='after_conv3'),\n            ...     dict(cfg=dict(type='zzz', postfix='1'),\n            ...          stages=(True, True, True, True),\n            ...          position='after_conv3'),\n            ...     dict(cfg=dict(type='zzz', postfix='2'),\n            ...          stages=(True, True, True, True),\n            ...          position='after_conv3')\n            ... ]\n            >>> self = ResNet(depth=18)\n            >>> stage_plugins = self.make_stage_plugins(plugins, 0)\n            >>> assert len(stage_plugins) == 3\n        Suppose ``stage_idx=0``, the structure of blocks in the stage would be:\n        .. code-block:: none\n            conv1-> conv2->conv3->yyy->zzz1->zzz2\n        Suppose 'stage_idx=1', the structure of blocks in the stage would be:\n        .. code-block:: none\n            conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2\n        If stages is missing, the plugin would be applied to all stages.\n        Args:\n            plugins (list[dict]): List of plugins cfg to build. The postfix is\n                required if multiple same type plugins are inserted.\n            stage_idx (int): Index of stage to build\n        Returns:\n            list[dict]: Plugins for current stage\n        \"\"\"\n        stage_plugins = []\n        for plugin in plugins:\n            plugin = plugin.copy()\n            stages = plugin.pop(\"stages\", None)\n            assert stages is None or len(stages) == self.num_stages\n            # whether to insert plugin into current stage\n            if stages is None or stages[stage_idx]:\n                stage_plugins.append(plugin)\n\n        return stage_plugins\n\n    def make_res_layer(self, **kwargs):\n        \"\"\"Pack all blocks in a stage into a ``ResLayer``.\"\"\"\n        return ResLayer(**kwargs)\n\n    @property\n    def norm1(self):\n        \"\"\"nn.Module: the normalization layer named \"norm1\" \"\"\"\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self, in_channels, stem_channels):\n        if self.deep_stem:\n            self.stem = nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(stem_channels // 2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    stem_channels // 2,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(stem_channels // 2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(\n                    stem_channels // 2,\n                    stem_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(stem_channels),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            self.conv1 = nn.Conv2d(\n                in_channels,\n                stem_channels,\n                kernel_size=7,\n                stride=2,\n                padding=3,\n                bias=False,\n            )\n            self.bn1 = nn.BatchNorm2d(stem_channels)\n            self.relu = nn.ReLU(inplace=True)\n        if self.no_pool33:\n            assert self.deep_stem\n            self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        else:\n            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            if self.deep_stem:\n                self.stem.eval()\n                for param in self.stem.parameters():\n                    param.requires_grad = False\n            else:\n                self.norm1.eval()\n                for m in [self.conv1, self.norm1]:\n                    for param in m.parameters():\n                        param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, f\"layer{i}\")\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        if self.deep_stem:\n            x = self.stem(x)\n        else:\n            x = self.conv1(x)\n            x = self.norm1(x)\n            x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return tuple(outs)\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep normalization layer\n        freezed.\"\"\"\n        super(ResNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()", "\n\nclass ResNetV1e(ResNet):\n    r\"\"\"ResNetV1d variant described in `Bag of Tricks\n    <https://arxiv.org/pdf/1812.01187.pdf>`_.\n    Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv in\n    the input stem with three 3x3 convs. And in the downsampling block, a 2x2\n    avg_pool with stride 2 is added before conv, whose stride is changed to 1.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super(ResNetV1e, self).__init__(\n            deep_stem=True, avg_down=True, no_pool33=True, **kwargs\n        )", "\n\nclass ResLayer(nn.Sequential):\n    \"\"\"ResLayer to build ResNet style backbone.\n    Args:\n        block (nn.Module): block used to build ResLayer.\n        inplanes (int): inplanes of block.\n        planes (int): planes of block.\n        num_blocks (int): number of blocks.\n        stride (int): stride of the first block. Default: 1\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck. Default: False\n        conv_cfg (dict): dictionary to construct and config conv layer.\n            Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='BN')\n        downsample_first (bool): Downsample at the first block or last block.\n            False for Hourglass, True for ResNet. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        block,\n        inplanes,\n        planes,\n        num_blocks,\n        stride=1,\n        avg_down=False,\n        conv_cfg=None,\n        norm_cfg=dict(type=\"BN\"),\n        downsample_first=True,\n        **kwargs,\n    ):\n        self.block = block\n\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = []\n            conv_stride = stride\n            if avg_down:\n                conv_stride = 1\n                downsample.append(\n                    nn.AvgPool2d(\n                        kernel_size=stride,\n                        stride=stride,\n                        ceil_mode=True,\n                        count_include_pad=False,\n                    )\n                )\n            downsample.extend(\n                [\n                    nn.Conv2d(\n                        inplanes,\n                        planes * block.expansion,\n                        kernel_size=1,\n                        stride=conv_stride,\n                        bias=False,\n                    ),\n                    nn.BatchNorm2d(planes * block.expansion),\n                ]\n            )\n            downsample = nn.Sequential(*downsample)\n\n        layers = []\n        if downsample_first:\n            layers.append(\n                block(\n                    inplanes=inplanes,\n                    planes=planes,\n                    stride=stride,\n                    downsample=downsample,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    **kwargs,\n                )\n            )\n            inplanes = planes * block.expansion\n            for _ in range(1, num_blocks):\n                layers.append(\n                    block(\n                        inplanes=inplanes,\n                        planes=planes,\n                        stride=1,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        **kwargs,\n                    )\n                )\n\n        else:  # downsample_first=False is for HourglassModule\n            for _ in range(num_blocks - 1):\n                layers.append(\n                    block(\n                        inplanes=inplanes,\n                        planes=inplanes,\n                        stride=1,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        **kwargs,\n                    )\n                )\n            layers.append(\n                block(\n                    inplanes=inplanes,\n                    planes=planes,\n                    stride=stride,\n                    downsample=downsample,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    **kwargs,\n                )\n            )\n        super(ResLayer, self).__init__(*layers)", ""]}
{"filename": "dataset2metadata/face_detection/head.py", "chunked_list": ["# contributed by George Smyrnis\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nfrom dataset2metadata.face_detection.neck import ConvModule\nfrom dataset2metadata.face_detection.utils import (\n    DIoULoss,\n    QualityFocalLoss,\n    anchor_inside_flags,\n    bbox2delta,\n    bbox2distance,\n    bbox_overlaps,", "    bbox2distance,\n    bbox_overlaps,\n    delta2bbox,\n    distance2bbox,\n    images_to_levels,\n    multi_apply,\n    multiclass_nms,\n    reduce_mean,\n    unmap,\n    weighted_loss,", "    unmap,\n    weighted_loss,\n)\n\n# Head\n\n\nclass BaseDenseHead(nn.Module, metaclass=ABCMeta):\n    \"\"\"Base class for DenseHeads.\"\"\"\n\n    def __init__(self):\n        super(BaseDenseHead, self).__init__()\n\n    @abstractmethod\n    def loss(self, **kwargs):\n        \"\"\"Compute losses of the head.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_bboxes(self, **kwargs):\n        \"\"\"Transform network output for a batch into bbox predictions.\"\"\"\n        pass\n\n    def forward_train(\n        self,\n        x,\n        img_metas,\n        gt_bboxes,\n        gt_labels=None,\n        gt_bboxes_ignore=None,\n        proposal_cfg=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            x (list[Tensor]): Features from FPN.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used\n        Returns:\n            tuple:\n                losses: (dict[str, Tensor]): A dictionary of loss components.\n                proposal_list (list[Tensor]): Proposals of each image.\n        \"\"\"\n        outs = self(x)\n        if gt_labels is None:\n            loss_inputs = outs + (gt_bboxes, img_metas)\n        else:\n            loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)\n        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        if proposal_cfg is None:\n            return losses\n        else:\n            proposal_list = self.get_bboxes(*outs, img_metas, cfg=proposal_cfg)\n            return losses, proposal_list", "\n\nclass DeltaXYWHBBoxCoder(object):\n    \"\"\"Delta XYWH BBox coder.\n    Following the practice in `R-CNN <https://arxiv.org/abs/1311.2524>`_,\n    this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and\n    decodes delta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2).\n    Args:\n        target_means (Sequence[float]): Denormalizing means of target for\n            delta coordinates\n        target_stds (Sequence[float]): Denormalizing standard deviation of\n            target for delta coordinates\n        clip_border (bool, optional): Whether clip the objects outside the\n            border of the image. Defaults to True.\n        add_ctr_clamp (bool): Whether to add center clamp, when added, the\n            predicted box is clamped is its center is too far away from\n            the original anchor's center. Only used by YOLOF. Default False.\n        ctr_clamp (int): the maximum pixel shift to clamp. Only used by YOLOF.\n            Default 32.\n    \"\"\"\n\n    def __init__(\n        self,\n        target_means=(0.0, 0.0, 0.0, 0.0),\n        target_stds=(1.0, 1.0, 1.0, 1.0),\n        clip_border=True,\n        add_ctr_clamp=False,\n        ctr_clamp=32,\n    ):\n        self.means = target_means\n        self.stds = target_stds\n        self.clip_border = clip_border\n        self.add_ctr_clamp = add_ctr_clamp\n        self.ctr_clamp = ctr_clamp\n\n    def encode(self, bboxes, gt_bboxes):\n        \"\"\"Get box regression transformation deltas that can be used to\n        transform the ``bboxes`` into the ``gt_bboxes``.\n        Args:\n            bboxes (torch.Tensor): Source boxes, e.g., object proposals.\n            gt_bboxes (torch.Tensor): Target of the transformation, e.g.,\n                ground-truth boxes.\n        Returns:\n            torch.Tensor: Box transformation deltas\n        \"\"\"\n\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = bbox2delta(bboxes, gt_bboxes, self.means, self.stds)\n        return encoded_bboxes\n\n    def decode(self, bboxes, pred_bboxes, max_shape=None, wh_ratio_clip=16 / 1000):\n        \"\"\"Apply transformation `pred_bboxes` to `boxes`.\n        Args:\n            bboxes (torch.Tensor): Basic boxes. Shape (B, N, 4) or (N, 4)\n            pred_bboxes (Tensor): Encoded offsets with respect to each roi.\n               Has shape (B, N, num_classes * 4) or (B, N, 4) or\n               (N, num_classes * 4) or (N, 4). Note N = num_anchors * W * H\n               when rois is a grid of anchors.Offset encoding follows [1]_.\n            max_shape (Sequence[int] or torch.Tensor or Sequence[\n               Sequence[int]],optional): Maximum bounds for boxes, specifies\n               (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then\n               the max_shape should be a Sequence[Sequence[int]]\n               and the length of max_shape should also be B.\n            wh_ratio_clip (float, optional): The allowed ratio between\n                width and height.\n        Returns:\n            torch.Tensor: Decoded boxes.\n        \"\"\"\n\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        if pred_bboxes.ndim == 3:\n            assert pred_bboxes.size(1) == bboxes.size(1)\n\n        if pred_bboxes.ndim == 2 and not torch.onnx.is_in_onnx_export():\n            # single image decode\n            decoded_bboxes = delta2bbox(\n                bboxes,\n                pred_bboxes,\n                self.means,\n                self.stds,\n                max_shape,\n                wh_ratio_clip,\n                self.clip_border,\n                self.add_ctr_clamp,\n                self.ctr_clamp,\n            )\n        else:\n            if pred_bboxes.ndim == 3 and not torch.onnx.is_in_onnx_export():\n                warnings.warn(\n                    \"DeprecationWarning: onnx_delta2bbox is deprecated \"\n                    \"in the case of batch decoding and non-ONNX, \"\n                    \"please use \u201cdelta2bbox\u201d instead. In order to improve \"\n                    \"the decoding speed, the batch function will no \"\n                    \"longer be supported. \"\n                )\n            raise NotImplementedError(\"Currently not supported.\")\n\n        return decoded_bboxes", "\n\nclass AnchorGenerator:\n    \"\"\"Standard anchor generator for 2D anchor-based detectors.\n    Args:\n        strides (list[int] | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels in order (w, h).\n        ratios (list[float]): The list of ratios between the height and width\n            of anchors in a single level.\n        scales (list[int] | None): Anchor scales for anchors in a single level.\n            It cannot be set at the same time if `octave_base_scale` and\n            `scales_per_octave` are set.\n        base_sizes (list[int] | None): The basic sizes\n            of anchors in multiple levels.\n            If None is given, strides will be used as base_sizes.\n            (If strides are non square, the shortest stride is taken.)\n        scale_major (bool): Whether to multiply scales first when generating\n            base anchors. If true, the anchors in the same row will have the\n            same scales. By default it is True in V2.0\n        octave_base_scale (int): The base scale of octave.\n        scales_per_octave (int): Number of scales for each octave.\n            `octave_base_scale` and `scales_per_octave` are usually used in\n            retinanet and the `scales` should be None when they are set.\n        centers (list[tuple[float, float]] | None): The centers of the anchor\n            relative to the feature grid center in multiple feature levels.\n            By default it is set to be None and not used. If a list of tuple of\n            float is given, they will be used to shift the centers of anchors.\n        center_offset (float): The offset of center in proportion to anchors'\n            width and height. By default it is 0 in V2.0.\n    Examples:\n        >>> from mmdet.core import AnchorGenerator\n        >>> self = AnchorGenerator([16], [1.], [1.], [9])\n        >>> all_anchors = self.grid_priors([(2, 2)], device='cpu')\n        >>> print(all_anchors)\n        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n                [11.5000, -4.5000, 20.5000,  4.5000],\n                [-4.5000, 11.5000,  4.5000, 20.5000],\n                [11.5000, 11.5000, 20.5000, 20.5000]])]\n        >>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18])\n        >>> all_anchors = self.grid_priors([(2, 2), (1, 1)], device='cpu')\n        >>> print(all_anchors)\n        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n                [11.5000, -4.5000, 20.5000,  4.5000],\n                [-4.5000, 11.5000,  4.5000, 20.5000],\n                [11.5000, 11.5000, 20.5000, 20.5000]]), \\\n        tensor([[-9., -9., 9., 9.]])]\n    \"\"\"\n\n    def __init__(\n        self,\n        strides,\n        ratios,\n        scales=None,\n        base_sizes=None,\n        scale_major=True,\n        octave_base_scale=None,\n        scales_per_octave=None,\n        centers=None,\n        center_offset=0.0,\n    ):\n        # check center and center_offset\n        if center_offset != 0:\n            assert centers is None, (\n                \"center cannot be set when center_offset\" f\"!=0, {centers} is given.\"\n            )\n        if not (0 <= center_offset <= 1):\n            raise ValueError(\n                \"center_offset should be in range [0, 1], \" f\"{center_offset} is given.\"\n            )\n        if centers is not None:\n            assert len(centers) == len(strides), (\n                \"The number of strides should be the same as centers, got \"\n                f\"{strides} and {centers}\"\n            )\n\n        # calculate base sizes of anchors\n        self.strides = [nn.modules.utils._pair(stride) for stride in strides]\n        self.base_sizes = (\n            [min(stride) for stride in self.strides]\n            if base_sizes is None\n            else base_sizes\n        )\n        assert len(self.base_sizes) == len(self.strides), (\n            \"The number of strides should be the same as base sizes, got \"\n            f\"{self.strides} and {self.base_sizes}\"\n        )\n\n        # calculate scales of anchors\n        assert (octave_base_scale is not None and scales_per_octave is not None) ^ (\n            scales is not None\n        ), (\n            \"scales and octave_base_scale with scales_per_octave cannot\"\n            \" be set at the same time\"\n        )\n        if scales is not None:\n            self.scales = torch.Tensor(scales)\n        elif octave_base_scale is not None and scales_per_octave is not None:\n            octave_scales = np.array(\n                [2 ** (i / scales_per_octave) for i in range(scales_per_octave)]\n            )\n            scales = octave_scales * octave_base_scale\n            self.scales = torch.Tensor(scales)\n        else:\n            raise ValueError(\n                \"Either scales or octave_base_scale with \"\n                \"scales_per_octave should be set\"\n            )\n\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.ratios = torch.Tensor(ratios)\n        self.scale_major = scale_major\n        self.centers = centers\n        self.center_offset = center_offset\n        self.base_anchors = self.gen_base_anchors()\n\n    @property\n    def num_base_anchors(self):\n        \"\"\"list[int]: total number of base anchors in a feature grid\"\"\"\n        return self.num_base_priors\n\n    @property\n    def num_base_priors(self):\n        \"\"\"list[int]: The number of priors (anchors) at a point\n        on the feature grid\"\"\"\n        return [base_anchors.size(0) for base_anchors in self.base_anchors]\n\n    @property\n    def num_levels(self):\n        \"\"\"int: number of feature levels that the generator will be applied\"\"\"\n        return len(self.strides)\n\n    def gen_base_anchors(self):\n        \"\"\"Generate base anchors.\n        Returns:\n            list(torch.Tensor): Base anchors of a feature grid in multiple \\\n                feature levels.\n        \"\"\"\n        multi_level_base_anchors = []\n        for i, base_size in enumerate(self.base_sizes):\n            center = None\n            if self.centers is not None:\n                center = self.centers[i]\n            multi_level_base_anchors.append(\n                self.gen_single_level_base_anchors(\n                    base_size, scales=self.scales, ratios=self.ratios, center=center\n                )\n            )\n        return multi_level_base_anchors\n\n    def gen_single_level_base_anchors(self, base_size, scales, ratios, center=None):\n        \"\"\"Generate base anchors of a single level.\n        Args:\n            base_size (int | float): Basic size of an anchor.\n            scales (torch.Tensor): Scales of the anchor.\n            ratios (torch.Tensor): The ratio between between the height\n                and width of anchors in a single level.\n            center (tuple[float], optional): The center of the base anchor\n                related to a single feature grid. Defaults to None.\n        Returns:\n            torch.Tensor: Anchors in a single-level feature maps.\n        \"\"\"\n        w = base_size\n        h = base_size\n        if center is None:\n            x_center = self.center_offset * w\n            y_center = self.center_offset * h\n        else:\n            x_center, y_center = center\n\n        h_ratios = torch.sqrt(ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * scales[None, :]).view(-1)\n        else:\n            ws = (w * scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * scales[:, None] * h_ratios[None, :]).view(-1)\n\n        # use float anchor and the anchor's center is aligned with the\n        # pixel center\n        base_anchors = [\n            x_center - 0.5 * ws,\n            y_center - 0.5 * hs,\n            x_center + 0.5 * ws,\n            y_center + 0.5 * hs,\n        ]\n        base_anchors = torch.stack(base_anchors, dim=-1)\n\n        return base_anchors\n\n    def _meshgrid(self, x, y, row_major=True):\n        \"\"\"Generate mesh grid of x and y.\n        Args:\n            x (torch.Tensor): Grids of x dimension.\n            y (torch.Tensor): Grids of y dimension.\n            row_major (bool, optional): Whether to return y grids first.\n                Defaults to True.\n        Returns:\n            tuple[torch.Tensor]: The mesh grids of x and y.\n        \"\"\"\n        # use shape instead of len to keep tracing while exporting to onnx\n        xx = x.repeat(y.shape[0])\n        yy = y.view(-1, 1).repeat(1, x.shape[0]).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_priors(self, featmap_sizes, dtype=torch.float32, device=\"cuda\"):\n        \"\"\"Generate grid anchors in multiple feature levels.\n        Args:\n            featmap_sizes (list[tuple]): List of feature map sizes in\n                multiple feature levels.\n            dtype (:obj:`torch.dtype`): Dtype of priors.\n                Default: torch.float32.\n            device (str): The device where the anchors will be put on.\n        Return:\n            list[torch.Tensor]: Anchors in multiple feature levels. \\\n                The sizes of each tensor should be [N, 4], where \\\n                N = width * height * num_base_anchors, width and height \\\n                are the sizes of the corresponding feature level, \\\n                num_base_anchors is the number of anchors for that level.\n        \"\"\"\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_anchors = []\n        for i in range(self.num_levels):\n            anchors = self.single_level_grid_priors(\n                featmap_sizes[i], level_idx=i, dtype=dtype, device=device\n            )\n            multi_level_anchors.append(anchors)\n        return multi_level_anchors\n\n    def single_level_grid_priors(\n        self, featmap_size, level_idx, dtype=torch.float32, device=\"cuda\"\n    ):\n        \"\"\"Generate grid anchors of a single level.\n        Note:\n            This function is usually called by method ``self.grid_priors``.\n        Args:\n            featmap_size (tuple[int]): Size of the feature maps.\n            level_idx (int): The index of corresponding feature map level.\n            dtype (obj:`torch.dtype`): Date type of points.Defaults to\n                ``torch.float32``.\n            device (str, optional): The device the tensor will be put on.\n                Defaults to 'cuda'.\n        Returns:\n            torch.Tensor: Anchors in the overall feature maps.\n        \"\"\"\n\n        base_anchors = self.base_anchors[level_idx].to(device).to(dtype)\n        feat_h, feat_w = featmap_size\n        stride_w, stride_h = self.strides[level_idx]\n        # First create Range with the default dtype, than convert to\n        # target `dtype` for onnx exporting.\n        shift_x = torch.arange(0, feat_w, device=device).to(dtype) * stride_w\n        shift_y = torch.arange(0, feat_h, device=device).to(dtype) * stride_h\n\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def sparse_priors(\n        self, prior_idxs, featmap_size, level_idx, dtype=torch.float32, device=\"cuda\"\n    ):\n        \"\"\"Generate sparse anchors according to the ``prior_idxs``.\n        Args:\n            prior_idxs (Tensor): The index of corresponding anchors\n                in the feature map.\n            featmap_size (tuple[int]): feature map size arrange as (h, w).\n            level_idx (int): The level index of corresponding feature\n                map.\n            dtype (obj:`torch.dtype`): Date type of points.Defaults to\n                ``torch.float32``.\n            device (obj:`torch.device`): The device where the points is\n                located.\n        Returns:\n            Tensor: Anchor with shape (N, 4), N should be equal to\n                the length of ``prior_idxs``.\n        \"\"\"\n\n        height, width = featmap_size\n        num_base_anchors = self.num_base_anchors[level_idx]\n        base_anchor_id = prior_idxs % num_base_anchors\n        x = (prior_idxs // num_base_anchors) % width * self.strides[level_idx][0]\n        y = (\n            (prior_idxs // width // num_base_anchors)\n            % height\n            * self.strides[level_idx][1]\n        )\n        priors = torch.stack([x, y, x, y], 1).to(dtype).to(device) + self.base_anchors[\n            level_idx\n        ][base_anchor_id, :].to(device)\n\n        return priors\n\n    def grid_anchors(self, featmap_sizes, device=\"cuda\"):\n        \"\"\"Generate grid anchors in multiple feature levels.\n        Args:\n            featmap_sizes (list[tuple]): List of feature map sizes in\n                multiple feature levels.\n            device (str): Device where the anchors will be put on.\n        Return:\n            list[torch.Tensor]: Anchors in multiple feature levels. \\\n                The sizes of each tensor should be [N, 4], where \\\n                N = width * height * num_base_anchors, width and height \\\n                are the sizes of the corresponding feature level, \\\n                num_base_anchors is the number of anchors for that level.\n        \"\"\"\n        # warnings.warn('``grid_anchors`` would be deprecated soon. '\n        #               'Please use ``grid_priors`` ')\n\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_anchors = []\n        for i in range(self.num_levels):\n            anchors = self.single_level_grid_anchors(\n                self.base_anchors[i].to(device),\n                featmap_sizes[i],\n                self.strides[i],\n                device=device,\n            )\n            multi_level_anchors.append(anchors)\n        return multi_level_anchors\n\n    def single_level_grid_anchors(\n        self, base_anchors, featmap_size, stride=(16, 16), device=\"cuda\"\n    ):\n        \"\"\"Generate grid anchors of a single level.\n        Note:\n            This function is usually called by method ``self.grid_anchors``.\n        Args:\n            base_anchors (torch.Tensor): The base anchors of a feature grid.\n            featmap_size (tuple[int]): Size of the feature maps.\n            stride (tuple[int], optional): Stride of the feature map in order\n                (w, h). Defaults to (16, 16).\n            device (str, optional): Device the tensor will be put on.\n                Defaults to 'cuda'.\n        Returns:\n            torch.Tensor: Anchors in the overall feature maps.\n        \"\"\"\n\n        # warnings.warn(\n        #     '``single_level_grid_anchors`` would be deprecated soon. '\n        #     'Please use ``single_level_grid_priors`` ')\n\n        # keep featmap_size as Tensor instead of int, so that we\n        # can convert to ONNX correctly\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0, feat_w, device=device) * stride[0]\n        shift_y = torch.arange(0, feat_h, device=device) * stride[1]\n\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        shifts = shifts.type_as(base_anchors)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def valid_flags(self, featmap_sizes, pad_shape, device=\"cuda\"):\n        \"\"\"Generate valid flags of anchors in multiple feature levels.\n        Args:\n            featmap_sizes (list(tuple)): List of feature map sizes in\n                multiple feature levels.\n            pad_shape (tuple): The padded shape of the image.\n            device (str): Device where the anchors will be put on.\n        Return:\n            list(torch.Tensor): Valid flags of anchors in multiple levels.\n        \"\"\"\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_flags = []\n        for i in range(self.num_levels):\n            anchor_stride = self.strides[i]\n            feat_h, feat_w = featmap_sizes[i]\n            h, w = pad_shape[:2]\n            valid_feat_h = min(int(np.ceil(h / anchor_stride[1])), feat_h)\n            valid_feat_w = min(int(np.ceil(w / anchor_stride[0])), feat_w)\n            flags = self.single_level_valid_flags(\n                (feat_h, feat_w),\n                (valid_feat_h, valid_feat_w),\n                self.num_base_anchors[i],\n                device=device,\n            )\n            multi_level_flags.append(flags)\n        return multi_level_flags\n\n    def single_level_valid_flags(\n        self, featmap_size, valid_size, num_base_anchors, device=\"cuda\"\n    ):\n        \"\"\"Generate the valid flags of anchor in a single feature map.\n        Args:\n            featmap_size (tuple[int]): The size of feature maps, arrange\n                as (h, w).\n            valid_size (tuple[int]): The valid size of the feature maps.\n            num_base_anchors (int): The number of base anchors.\n            device (str, optional): Device where the flags will be put on.\n                Defaults to 'cuda'.\n        Returns:\n            torch.Tensor: The valid flags of each anchor in a single level \\\n                feature map.\n        \"\"\"\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.bool, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.bool, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        valid = (\n            valid[:, None].expand(valid.size(0), num_base_anchors).contiguous().view(-1)\n        )\n        return valid\n\n    def __repr__(self):\n        \"\"\"str: a string that describes the module\"\"\"\n        indent_str = \"    \"\n        repr_str = self.__class__.__name__ + \"(\\n\"\n        repr_str += f\"{indent_str}strides={self.strides},\\n\"\n        repr_str += f\"{indent_str}ratios={self.ratios},\\n\"\n        repr_str += f\"{indent_str}scales={self.scales},\\n\"\n        repr_str += f\"{indent_str}base_sizes={self.base_sizes},\\n\"\n        repr_str += f\"{indent_str}scale_major={self.scale_major},\\n\"\n        repr_str += f\"{indent_str}octave_base_scale=\"\n        repr_str += f\"{self.octave_base_scale},\\n\"\n        repr_str += f\"{indent_str}scales_per_octave=\"\n        repr_str += f\"{self.scales_per_octave},\\n\"\n        repr_str += f\"{indent_str}num_levels={self.num_levels}\\n\"\n        repr_str += f\"{indent_str}centers={self.centers},\\n\"\n        repr_str += f\"{indent_str}center_offset={self.center_offset})\"\n        return repr_str", "\n\nclass AnchorHead(BaseDenseHead):\n    \"\"\"Anchor-based head (RPN, RetinaNet, SSD, etc.).\n    Args:\n        num_classes (int): Number of categories excluding the background\n            category.\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of hidden channels. Used in child classes.\n        anchor_generator (dict): Config dict for anchor generator\n        bbox_coder (dict): Config of bounding box coder.\n        reg_decoded_bbox (bool): If true, the regression loss would be\n            applied on decoded bounding boxes. Default: False\n        loss_cls (dict): Config of classification loss.\n        loss_bbox (dict): Config of localization loss.\n        train_cfg (dict): Training config of anchor head.\n        test_cfg (dict): Testing config of anchor head.\n    \"\"\"  # noqa: W605\n\n    def __init__(\n        self,\n        num_classes,\n        in_channels,\n        feat_channels=256,\n        anchor_generator=dict(\n            type=\"AnchorGenerator\",\n            scales=[8, 16, 32],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64],\n        ),\n        bbox_coder=dict(\n            type=\"DeltaXYWHBBoxCoder\",\n            clip_border=True,\n            target_means=(0.0, 0.0, 0.0, 0.0),\n            target_stds=(1.0, 1.0, 1.0, 1.0),\n        ),\n        reg_decoded_bbox=False,\n        loss_cls=dict(type=\"CrossEntropyLoss\", use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type=\"SmoothL1Loss\", beta=1.0 / 9.0, loss_weight=1.0),\n        train_cfg=None,\n        test_cfg=None,\n    ):\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.use_sigmoid_cls = loss_cls.get(\"use_sigmoid\", False)\n        # TODO better way to determine whether sample or not\n        self.sampling = loss_cls[\"type\"] not in [\n            \"FocalLoss\",\n            \"GHMC\",\n            \"QualityFocalLoss\",\n        ]\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = num_classes\n        else:\n            self.cls_out_channels = num_classes + 1\n\n        if self.cls_out_channels <= 0:\n            raise ValueError(f\"num_classes={num_classes} is too small\")\n        self.reg_decoded_bbox = reg_decoded_bbox\n\n        self.bbox_coder = DeltaXYWHBBoxCoder(\n            clip_border=True,\n            target_means=(0.0, 0.0, 0.0, 0.0),\n            target_stds=(1.0, 1.0, 1.0, 1.0),\n        )\n        self.loss_cls = QualityFocalLoss()\n        self.loss_bbox = DIoULoss(loss_weight=2.0)\n        # Disable training\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.fp16_enabled = False\n\n        self.anchor_generator = AnchorGenerator(\n            ratios=[1.0], scales=[1, 2], base_sizes=[16, 64, 256], strides=[8, 16, 32]\n        )\n        # usually the numbers of anchors for each level are the same\n        # except SSD detectors\n        self.num_anchors = self.anchor_generator.num_base_anchors[0]\n        self._init_layers()\n\n    def _init_layers(self):\n        \"\"\"Initialize layers of the head.\"\"\"\n        self.conv_cls = nn.Conv2d(\n            self.in_channels, self.num_anchors * self.cls_out_channels, 1\n        )\n        self.conv_reg = nn.Conv2d(self.in_channels, self.num_anchors * 4, 1)\n\n    def forward_single(self, x):\n        \"\"\"Forward feature of a single scale level.\n        Args:\n            x (Tensor): Features of a single scale level.\n        Returns:\n            tuple:\n                cls_score (Tensor): Cls scores for a single scale level \\\n                    the channels number is num_anchors * num_classes.\n                bbox_pred (Tensor): Box energies / deltas for a single scale \\\n                    level, the channels number is num_anchors * 4.\n        \"\"\"\n        cls_score = self.conv_cls(x)\n        bbox_pred = self.conv_reg(x)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        \"\"\"Forward features from the upstream network.\n        Args:\n            feats (tuple[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n        Returns:\n            tuple: A tuple of classification scores and bbox prediction.\n                - cls_scores (list[Tensor]): Classification scores for all \\\n                    scale levels, each is a 4D-tensor, the channels number \\\n                    is num_anchors * num_classes.\n                - bbox_preds (list[Tensor]): Box energies / deltas for all \\\n                    scale levels, each is a 4D-tensor, the channels number \\\n                    is num_anchors * 4.\n        \"\"\"\n        return multi_apply(self.forward_single, feats)\n\n    def get_anchors(self, featmap_sizes, img_metas, device=\"cuda\"):\n        \"\"\"Get anchors according to feature map sizes.\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n            device (torch.device | str): Device for returned tensors\n        Returns:\n            tuple:\n                anchor_list (list[Tensor]): Anchors of each image.\n                valid_flag_list (list[Tensor]): Valid flags of each image.\n        \"\"\"\n        num_imgs = len(img_metas)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = self.anchor_generator.grid_anchors(featmap_sizes, device)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = self.anchor_generator.valid_flags(\n                featmap_sizes, img_meta[\"pad_shape\"], device\n            )\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def _get_targets_single(\n        self,\n        flat_anchors,\n        valid_flags,\n        gt_bboxes,\n        gt_bboxes_ignore,\n        gt_labels,\n        img_meta,\n        label_channels=1,\n        unmap_outputs=True,\n    ):\n        \"\"\"Compute regression and classification targets for anchors in a\n        single image.\n        Args:\n            flat_anchors (Tensor): Multi-level anchors of the image, which are\n                concatenated into a single tensor of shape (num_anchors ,4)\n            valid_flags (Tensor): Multi level valid flags of the image,\n                which are concatenated into a single tensor of\n                    shape (num_anchors,).\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            img_meta (dict): Meta info of the image.\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            img_meta (dict): Meta info of the image.\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n        Returns:\n            tuple:\n                labels_list (list[Tensor]): Labels of each level\n                label_weights_list (list[Tensor]): Label weights of each level\n                bbox_targets_list (list[Tensor]): BBox targets of each level\n                bbox_weights_list (list[Tensor]): BBox weights of each level\n                num_total_pos (int): Number of positive samples in all images\n                num_total_neg (int): Number of negative samples in all images\n        \"\"\"\n        inside_flags = anchor_inside_flags(\n            flat_anchors,\n            valid_flags,\n            img_meta[\"img_shape\"][:2],\n            self.train_cfg.allowed_border,\n        )\n        if not inside_flags.any():\n            return (None,) * 7\n        # assign gt and sample anchors\n        anchors = flat_anchors[inside_flags, :]\n\n        assign_result = self.assigner.assign(\n            anchors, gt_bboxes, gt_bboxes_ignore, None if self.sampling else gt_labels\n        )\n        sampling_result = self.sampler.sample(assign_result, anchors, gt_bboxes)\n\n        num_valid_anchors = anchors.shape[0]\n        bbox_targets = torch.zeros_like(anchors)\n        bbox_weights = torch.zeros_like(anchors)\n        labels = anchors.new_full(\n            (num_valid_anchors,), self.num_classes, dtype=torch.long\n        )\n        label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            if not self.reg_decoded_bbox:\n                pos_bbox_targets = self.bbox_coder.encode(\n                    sampling_result.pos_bboxes, sampling_result.pos_gt_bboxes\n                )\n            else:\n                pos_bbox_targets = sampling_result.pos_gt_bboxes\n            bbox_targets[pos_inds, :] = pos_bbox_targets\n            bbox_weights[pos_inds, :] = 1.0\n            if gt_labels is None:\n                # Only rpn gives gt_labels as None\n                # Foreground is the first class since v2.5.0\n                labels[pos_inds] = 0\n            else:\n                labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n            if self.train_cfg.pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = self.train_cfg.pos_weight\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_anchors.size(0)\n            labels = unmap(\n                labels, num_total_anchors, inside_flags, fill=self.num_classes\n            )  # fill bg label\n            label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n            bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n        return (\n            labels,\n            label_weights,\n            bbox_targets,\n            bbox_weights,\n            pos_inds,\n            neg_inds,\n            sampling_result,\n        )\n\n    def get_targets(\n        self,\n        anchor_list,\n        valid_flag_list,\n        gt_bboxes_list,\n        img_metas,\n        gt_bboxes_ignore_list=None,\n        gt_labels_list=None,\n        label_channels=1,\n        unmap_outputs=True,\n        return_sampling_results=False,\n    ):\n        \"\"\"Compute regression and classification targets for anchors in\n        multiple images.\n        Args:\n            anchor_list (list[list[Tensor]]): Multi level anchors of each\n                image. The outer list indicates images, and the inner list\n                corresponds to feature levels of the image. Each element of\n                the inner list is a tensor of shape (num_anchors, 4).\n            valid_flag_list (list[list[Tensor]]): Multi level valid flags of\n                each image. The outer list indicates images, and the inner list\n                corresponds to feature levels of the image. Each element of\n                the inner list is a tensor of shape (num_anchors, )\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n            img_metas (list[dict]): Meta info of each image.\n            gt_bboxes_ignore_list (list[Tensor]): Ground truth bboxes to be\n                ignored.\n            gt_labels_list (list[Tensor]): Ground truth labels of each box.\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n        Returns:\n            tuple: Usually returns a tuple containing learning targets.\n                - labels_list (list[Tensor]): Labels of each level.\n                - label_weights_list (list[Tensor]): Label weights of each \\\n                    level.\n                - bbox_targets_list (list[Tensor]): BBox targets of each level.\n                - bbox_weights_list (list[Tensor]): BBox weights of each level.\n                - num_total_pos (int): Number of positive samples in all \\\n                    images.\n                - num_total_neg (int): Number of negative samples in all \\\n                    images.\n            additional_returns: This function enables user-defined returns from\n                `self._get_targets_single`. These returns are currently refined\n                to properties at each feature map (i.e. having HxW dimension).\n                The results will be concatenated after the end\n        \"\"\"\n        num_imgs = len(img_metas)\n        assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors to a single tensor\n        concat_anchor_list = []\n        concat_valid_flag_list = []\n        for i in range(num_imgs):\n            assert len(anchor_list[i]) == len(valid_flag_list[i])\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n            concat_valid_flag_list.append(torch.cat(valid_flag_list[i]))\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        if gt_labels_list is None:\n            gt_labels_list = [None for _ in range(num_imgs)]\n        results = multi_apply(\n            self._get_targets_single,\n            concat_anchor_list,\n            concat_valid_flag_list,\n            gt_bboxes_list,\n            gt_bboxes_ignore_list,\n            gt_labels_list,\n            img_metas,\n            label_channels=label_channels,\n            unmap_outputs=unmap_outputs,\n        )\n        (\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            pos_inds_list,\n            neg_inds_list,\n            sampling_results_list,\n        ) = results[:7]\n        rest_results = list(results[7:])  # user-added return values\n        # no valid anchors\n        if any([labels is None for labels in all_labels]):\n            return None\n        # sampled anchors of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        # split targets to a list w.r.t. multiple levels\n        labels_list = images_to_levels(all_labels, num_level_anchors)\n        label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n        bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n        bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n        res = (\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_pos,\n            num_total_neg,\n        )\n        if return_sampling_results:\n            res = res + (sampling_results_list,)\n        for i, r in enumerate(rest_results):  # user-added return values\n            rest_results[i] = images_to_levels(r, num_level_anchors)\n\n        return res + tuple(rest_results)\n\n    def loss_single(\n        self,\n        cls_score,\n        bbox_pred,\n        anchors,\n        labels,\n        label_weights,\n        bbox_targets,\n        bbox_weights,\n        num_total_samples,\n    ):\n        \"\"\"Compute loss of a single scale level.\n        Args:\n            cls_score (Tensor): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W).\n            bbox_pred (Tensor): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W).\n            anchors (Tensor): Box reference for each scale level with shape\n                (N, num_total_anchors, 4).\n            labels (Tensor): Labels of each anchors with shape\n                (N, num_total_anchors).\n            label_weights (Tensor): Label weights of each anchor with shape\n                (N, num_total_anchors)\n            bbox_targets (Tensor): BBox regression targets of each anchor wight\n                shape (N, num_total_anchors, 4).\n            bbox_weights (Tensor): BBox regression loss weights of each anchor\n                with shape (N, num_total_anchors, 4).\n            num_total_samples (int): If sampling, num total samples equal to\n                the number of total anchors; Otherwise, it is the number of\n                positive anchors.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        # classification loss\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels)\n        loss_cls = self.loss_cls(\n            cls_score, labels, label_weights, avg_factor=num_total_samples\n        )\n        # regression loss\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        bbox_weights = bbox_weights.reshape(-1, 4)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        if self.reg_decoded_bbox:\n            anchors = anchors.reshape(-1, 4)\n            bbox_pred = self.bbox_coder.decode(anchors, bbox_pred)\n        loss_bbox = self.loss_bbox(\n            bbox_pred, bbox_targets, bbox_weights, avg_factor=num_total_samples\n        )\n        return loss_cls, loss_bbox\n\n    def loss(\n        self,\n        cls_scores,\n        bbox_preds,\n        gt_bboxes,\n        gt_labels,\n        img_metas,\n        gt_bboxes_ignore=None,\n    ):\n        \"\"\"Compute losses of the head.\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W)\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): class indices corresponding to each box\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss. Default: None\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device\n        )\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n        )\n        if cls_reg_targets is None:\n            return None\n        (\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_pos,\n            num_total_neg,\n        ) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos\n        )\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors and flags to a single tensor\n        concat_anchor_list = []\n        for i in range(len(anchor_list)):\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n        all_anchor_list = images_to_levels(concat_anchor_list, num_level_anchors)\n\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            all_anchor_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples,\n        )\n        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n\n    def get_bboxes(\n        self, cls_scores, bbox_preds, img_metas, cfg=None, rescale=False, with_nms=True\n    ):\n        \"\"\"Transform network output for a batch into bbox predictions.\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W)\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            cfg (mmcv.Config | None): Test / postprocessing configuration,\n                if None, test_cfg would be used\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n        Returns:\n            list[tuple[Tensor, Tensor]]: Each item in result_list is 2-tuple.\n                The first item is an (n, 5) tensor, where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1. The second item is a\n                (n,) tensor where each item is the predicted class labelof the\n                corresponding box.\n        Example:\n            >>> import mmcv\n            >>> self = AnchorHead(\n            >>>     num_classes=9,\n            >>>     in_channels=1,\n            >>>     anchor_generator=dict(\n            >>>         type='AnchorGenerator',\n            >>>         scales=[8],\n            >>>         ratios=[0.5, 1.0, 2.0],\n            >>>         strides=[4,]))\n            >>> img_metas = [{'img_shape': (32, 32, 3), 'scale_factor': 1}]\n            >>> cfg = mmcv.Config(dict(\n            >>>     score_thr=0.00,\n            >>>     nms=dict(type='nms', iou_thr=1.0),\n            >>>     max_per_img=10))\n            >>> feat = torch.rand(1, 1, 3, 3)\n            >>> cls_score, bbox_pred = self.forward_single(feat)\n            >>> # note the input lists are over different levels, not images\n            >>> cls_scores, bbox_preds = [cls_score], [bbox_pred]\n            >>> result_list = self.get_bboxes(cls_scores, bbox_preds,\n            >>>                               img_metas, cfg)\n            >>> det_bboxes, det_labels = result_list[0]\n            >>> assert len(result_list) == 1\n            >>> assert det_bboxes.shape[1] == 5\n            >>> assert len(det_bboxes) == len(det_labels) == cfg.max_per_img\n        \"\"\"\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        device = cls_scores[0].device\n        featmap_sizes = [cls_scores[i].shape[-2:] for i in range(num_levels)]\n        mlvl_anchors = self.anchor_generator.grid_anchors(featmap_sizes, device=device)\n\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n            bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n            img_shape = img_metas[img_id][\"img_shape\"]\n            scale_factor = img_metas[img_id][\"scale_factor\"]\n            if with_nms:\n                # some heads don't support with_nms argument\n                proposals = self._get_bboxes_single(\n                    cls_score_list,\n                    bbox_pred_list,\n                    mlvl_anchors,\n                    img_shape,\n                    scale_factor,\n                    cfg,\n                    rescale,\n                )\n            else:\n                proposals = self._get_bboxes_single(\n                    cls_score_list,\n                    bbox_pred_list,\n                    mlvl_anchors,\n                    img_shape,\n                    scale_factor,\n                    cfg,\n                    rescale,\n                    with_nms,\n                )\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(\n        self,\n        cls_score_list,\n        bbox_pred_list,\n        mlvl_anchors,\n        img_shape,\n        scale_factor,\n        cfg,\n        rescale=False,\n        with_nms=True,\n    ):\n        \"\"\"Transform outputs for a single batch item into bbox predictions.\n        Args:\n            cls_score_list (list[Tensor]): Box scores for a single scale level\n                Has shape (num_anchors * num_classes, H, W).\n            bbox_pred_list (list[Tensor]): Box energies / deltas for a single\n                scale level with shape (num_anchors * 4, H, W).\n            mlvl_anchors (list[Tensor]): Box reference for a single scale level\n                with shape (num_total_anchors, 4).\n            img_shape (tuple[int]): Shape of the input image,\n                (height, width, 3).\n            scale_factor (ndarray): Scale factor of the image arange as\n                (w_scale, h_scale, w_scale, h_scale).\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n        Returns:\n            Tensor: Labeled boxes in shape (n, 5), where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1.\n        \"\"\"\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_score_list) == len(bbox_pred_list) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors in zip(\n            cls_score_list, bbox_pred_list, mlvl_anchors\n        ):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\"nms_pre\", -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                # Get maximum scores for foreground classes.\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    # remind that we set FG labels to [0, num_class-1]\n                    # since mmdet v2.0\n                    # BG cat_id: num_class\n                    max_scores, _ = scores[:, :-1].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = self.bbox_coder.decode(anchors, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            # Add a dummy background class to the backend when using sigmoid\n            # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n            # BG cat_id: num_class\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n\n        if with_nms:\n            det_bboxes, det_labels = multiclass_nms(\n                mlvl_bboxes, mlvl_scores, cfg.score_thr, cfg.nms, cfg.max_per_img\n            )\n            return det_bboxes, det_labels\n        else:\n            return mlvl_bboxes, mlvl_scores\n\n    def aug_test(self, feats, img_metas, rescale=False):\n        \"\"\"Test function with test time augmentation.\n        Args:\n            feats (list[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains features for all images in the batch.\n            img_metas (list[list[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch. each dict has image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n        Returns:\n            list[ndarray]: bbox results of each class\n        \"\"\"\n        return self.aug_test_bboxes(feats, img_metas, rescale=rescale)", "\n\nclass Integral(nn.Module):\n    \"\"\"A fixed layer for calculating integral result from distribution.\n    This layer calculates the target location by :math: `sum{P(y_i) * y_i}`,\n    P(y_i) denotes the softmax vector that represents the discrete distribution\n    y_i denotes the discrete set, usually {0, 1, 2, ..., reg_max}\n    Args:\n        reg_max (int): The maximal value of the discrete set. Default: 16. You\n            may want to reset it according to your new dataset or related\n            settings.\n    \"\"\"\n\n    def __init__(self, reg_max=16):\n        super(Integral, self).__init__()\n        self.reg_max = reg_max\n        self.register_buffer(\n            \"project\", torch.linspace(0, self.reg_max, self.reg_max + 1)\n        )\n\n    def forward(self, x):\n        \"\"\"Forward feature from the regression head to get integral result of\n        bounding box location.\n        Args:\n            x (Tensor): Features of the regression head, shape (N, 4*(n+1)),\n                n is self.reg_max.\n        Returns:\n            x (Tensor): Integral result of box locations, i.e., distance\n                offsets from the box center in four directions, shape (N, 4).\n        \"\"\"\n        x = F.softmax(x.reshape(-1, self.reg_max + 1), dim=1)\n        x = F.linear(x, self.project.type_as(x)).reshape(-1, 4)\n        return x", "\n\n@weighted_loss\ndef smooth_l1_loss(pred, target, beta=1.0):\n    \"\"\"Smooth L1 loss.\n    Args:\n        pred (torch.Tensor): The prediction.\n        target (torch.Tensor): The learning target of the prediction.\n        beta (float, optional): The threshold in the piecewise function.\n            Defaults to 1.0.\n    Returns:\n        torch.Tensor: Calculated loss\n    \"\"\"\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n    diff = torch.abs(pred - target)\n    loss = torch.where(diff < beta, 0.5 * diff * diff / beta, diff - 0.5 * beta)\n    return loss", "\n\n@weighted_loss\ndef l1_loss(pred, target):\n    \"\"\"L1 loss.\n    Args:\n        pred (torch.Tensor): The prediction.\n        target (torch.Tensor): The learning target of the prediction.\n    Returns:\n        torch.Tensor: Calculated loss\n    \"\"\"\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss", "\n\nclass SmoothL1Loss(nn.Module):\n    \"\"\"Smooth L1 loss.\n    Args:\n        beta (float, optional): The threshold in the piecewise function.\n            Defaults to 1.0.\n        reduction (str, optional): The method to reduce the loss.\n            Options are \"none\", \"mean\" and \"sum\". Defaults to \"mean\".\n        loss_weight (float, optional): The weight of loss.\n    \"\"\"\n\n    def __init__(self, beta=1.0, reduction=\"mean\", loss_weight=1.0):\n        super(SmoothL1Loss, self).__init__()\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(\n        self,\n        pred,\n        target,\n        weight=None,\n        avg_factor=None,\n        reduction_override=None,\n        **kwargs,\n    ):\n        \"\"\"Forward function.\n        Args:\n            pred (torch.Tensor): The prediction.\n            target (torch.Tensor): The learning target of the prediction.\n            weight (torch.Tensor, optional): The weight of loss for each\n                prediction. Defaults to None.\n            avg_factor (int, optional): Average factor that is used to average\n                the loss. Defaults to None.\n            reduction_override (str, optional): The reduction method used to\n                override the original reduction method of the loss.\n                Defaults to None.\n        \"\"\"\n        assert reduction_override in (None, \"none\", \"mean\", \"sum\")\n        reduction = reduction_override if reduction_override else self.reduction\n        loss_bbox = self.loss_weight * smooth_l1_loss(\n            pred,\n            target,\n            weight,\n            beta=self.beta,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs,\n        )\n        return loss_bbox", "\n\nclass Scale(nn.Module):\n    \"\"\"A learnable scale parameter.\n    This layer scales the input by a learnable factor. It multiplies a\n    learnable scale parameter of shape (1,) with input of any shape.\n    Args:\n        scale (float): Initial value of scale factor. Default: 1.0\n    \"\"\"\n\n    def __init__(self, scale: float = 1.0):\n        super().__init__()\n        self.scale = nn.Parameter(torch.tensor(scale, dtype=torch.float))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x * self.scale", "\n\nclass SCRFDHead(AnchorHead):\n    \"\"\"Generalized Focal Loss: Learning Qualified and Distributed Bounding\n    Boxes for Dense Object Detection.\n    GFL head structure is similar with ATSS, however GFL uses\n    1) joint representation for classification and localization quality, and\n    2) flexible General distribution for bounding box locations,\n    which are supervised by\n    Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), respectively\n    https://arxiv.org/abs/2006.04388\n    Args:\n        num_classes (int): Number of categories excluding the background\n            category.\n        in_channels (int): Number of channels in the input feature map.\n        stacked_convs (int): Number of conv layers in cls and reg tower.\n            Default: 4.\n        conv_cfg (dict): dictionary to construct and config conv layer.\n            Default: None.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type='GN', num_groups=32, requires_grad=True).\n        loss_qfl (dict): Config of Quality Focal Loss (QFL).\n        reg_max (int): Max value of integral set :math: `{0, ..., reg_max}`\n            in QFL setting. Default: 16.\n    Example:\n        >>> self = GFLHead(11, 7)\n        >>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n        >>> cls_quality_score, bbox_pred = self.forward(feats)\n        >>> assert len(cls_quality_score) == len(self.scales)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes,\n        in_channels,\n        stacked_convs=4,\n        feat_mults=None,\n        conv_cfg=None,\n        norm_cfg=dict(type=\"GN\", num_groups=32, requires_grad=True),\n        loss_dfl=None,\n        reg_max=8,\n        cls_reg_share=False,\n        strides_share=True,\n        scale_mode=1,\n        dw_conv=False,\n        use_kps=False,\n        loss_kps=dict(type=\"SmoothL1Loss\", beta=1.0 / 9.0, loss_weight=0.1),\n        # loss_kps=dict(type='SmoothL1Loss', beta=1.0, loss_weight=0.3),\n        **kwargs,\n    ):\n        self.stacked_convs = stacked_convs\n        self.feat_mults = feat_mults\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.reg_max = reg_max\n        self.cls_reg_share = cls_reg_share\n        self.strides_share = strides_share\n        self.scale_mode = scale_mode\n        self.use_dfl = True\n        self.dw_conv = dw_conv\n        self.NK = 5\n        self.extra_flops = 0.0\n        if loss_dfl is None or not loss_dfl:\n            self.use_dfl = False\n        self.use_scale = False\n        self.use_kps = use_kps\n        if self.scale_mode > 0 and (self.strides_share or self.scale_mode == 2):\n            self.use_scale = True\n        super(SCRFDHead, self).__init__(num_classes, in_channels, **kwargs)\n\n        self.sampling = False\n\n        self.integral = Integral(self.reg_max)\n        if self.use_dfl:\n            raise NotImplementedError(\"Unneeded.\")\n        self.loss_kps = SmoothL1Loss(beta=1.0 / 9.0, loss_weight=0.1)\n        self.loss_kps_std = 1.0\n        self.train_step = 0\n        self.pos_count = {}\n        self.gtgroup_count = {}\n        for stride in self.anchor_generator.strides:\n            self.pos_count[stride[0]] = 0\n\n    def _get_conv_module(self, in_channel, out_channel):\n        if not self.dw_conv:\n            conv = ConvModule(\n                in_channel,\n                out_channel,\n                3,\n                stride=1,\n                padding=1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg,\n            )\n        else:\n            raise NotImplementedError(\"Removed due to being unneeded.\")\n        return conv\n\n    def _init_layers(self):\n        \"\"\"Initialize layers of the head.\"\"\"\n        self.relu = nn.ReLU(inplace=True)\n        conv_strides = [0] if self.strides_share else self.anchor_generator.strides\n        self.cls_stride_convs = nn.ModuleDict()\n        self.reg_stride_convs = nn.ModuleDict()\n        self.stride_cls = nn.ModuleDict()\n        self.stride_reg = nn.ModuleDict()\n        if self.use_kps:\n            self.stride_kps = nn.ModuleDict()\n        for stride_idx, conv_stride in enumerate(conv_strides):\n            key = str(conv_stride)\n            cls_convs = nn.ModuleList()\n            reg_convs = nn.ModuleList()\n            stacked_convs = (\n                self.stacked_convs[stride_idx]\n                if isinstance(self.stacked_convs, (list, tuple))\n                else self.stacked_convs\n            )\n            feat_mult = (\n                self.feat_mults[stride_idx] if self.feat_mults is not None else 1\n            )\n            feat_ch = int(self.feat_channels * feat_mult)\n            for i in range(stacked_convs):\n                chn = self.in_channels if i == 0 else last_feat_ch\n                cls_convs.append(self._get_conv_module(chn, feat_ch))\n                if not self.cls_reg_share:\n                    reg_convs.append(self._get_conv_module(chn, feat_ch))\n                last_feat_ch = feat_ch\n            self.cls_stride_convs[key] = cls_convs\n            self.reg_stride_convs[key] = reg_convs\n            self.stride_cls[key] = nn.Conv2d(\n                feat_ch, self.cls_out_channels * self.num_anchors, 3, padding=1\n            )\n            if not self.use_dfl:\n                self.stride_reg[key] = nn.Conv2d(\n                    feat_ch, 4 * self.num_anchors, 3, padding=1\n                )\n            else:\n                self.stride_reg[key] = nn.Conv2d(\n                    feat_ch, 4 * (self.reg_max + 1) * self.num_anchors, 3, padding=1\n                )\n            if self.use_kps:\n                self.stride_kps[key] = nn.Conv2d(\n                    feat_ch, self.NK * 2 * self.num_anchors, 3, padding=1\n                )\n\n        if self.use_scale:\n            self.scales = nn.ModuleList(\n                [Scale(1.0) for _ in self.anchor_generator.strides]\n            )\n        else:\n            self.scales = [None for _ in self.anchor_generator.strides]\n\n    def forward(self, feats):\n        \"\"\"Forward features from the upstream network.\n        Args:\n            feats (tuple[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n        Returns:\n            tuple: Usually a tuple of classification scores and bbox prediction\n                cls_scores (list[Tensor]): Classification and quality (IoU)\n                    joint scores for all scale levels, each is a 4D-tensor,\n                    the channel number is num_classes.\n                bbox_preds (list[Tensor]): Box distribution logits for all\n                    scale levels, each is a 4D-tensor, the channel number is\n                    4*(n+1), n is max value of integral set.\n        \"\"\"\n        return multi_apply(\n            self.forward_single, feats, self.scales, self.anchor_generator.strides\n        )\n\n    def forward_single(self, x, scale, stride):\n        \"\"\"Forward feature of a single scale level.\n        Args:\n            x (Tensor): Features of a single scale level.\n            scale (:obj: `mmcv.cnn.Scale`): Learnable scale module to resize\n                the bbox prediction.\n        Returns:\n            tuple:\n                cls_score (Tensor): Cls and quality joint scores for a single\n                    scale level the channel number is num_classes.\n                bbox_pred (Tensor): Box distribution logits for a single scale\n                    level, the channel number is 4*(n+1), n is max value of\n                    integral set.\n        \"\"\"\n        cls_feat = x\n        reg_feat = x\n\n        cls_convs = (\n            self.cls_stride_convs[\"0\"]\n            if self.strides_share\n            else self.cls_stride_convs[str(stride)]\n        )\n        for cls_conv in cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        if not self.cls_reg_share:\n            reg_convs = (\n                self.reg_stride_convs[\"0\"]\n                if self.strides_share\n                else self.reg_stride_convs[str(stride)]\n            )\n            for reg_conv in reg_convs:\n                reg_feat = reg_conv(reg_feat)\n        else:\n            reg_feat = cls_feat\n        cls_pred_module = (\n            self.stride_cls[\"0\"] if self.strides_share else self.stride_cls[str(stride)]\n        )\n        cls_score = cls_pred_module(cls_feat)\n        reg_pred_module = (\n            self.stride_reg[\"0\"] if self.strides_share else self.stride_reg[str(stride)]\n        )\n        _bbox_pred = reg_pred_module(reg_feat)\n        if self.use_scale:\n            bbox_pred = scale(_bbox_pred)\n        else:\n            bbox_pred = _bbox_pred\n        if self.use_kps:\n            kps_pred_module = (\n                self.stride_kps[\"0\"]\n                if self.strides_share\n                else self.stride_kps[str(stride)]\n            )\n            kps_pred = kps_pred_module(reg_feat)\n        else:\n            kps_pred = bbox_pred.new_zeros(\n                (\n                    bbox_pred.shape[0],\n                    self.NK * 2,\n                    bbox_pred.shape[2],\n                    bbox_pred.shape[3],\n                )\n            )\n        if torch.onnx.is_in_onnx_export():\n            assert not self.use_dfl\n            print(\"in-onnx-export\", cls_score.shape, bbox_pred.shape)\n\n            # Add output batch dim, based on pull request #1593\n            batch_size = cls_score.shape[0]\n            cls_score = (\n                cls_score.permute(0, 2, 3, 1)\n                .reshape(batch_size, -1, self.cls_out_channels)\n                .sigmoid()\n            )\n            bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(batch_size, -1, 4)\n            kps_pred = kps_pred.permute(0, 2, 3, 1).reshape(batch_size, -1, 10)\n\n        return cls_score, bbox_pred, kps_pred\n\n    def forward_train(\n        self,\n        x,\n        img_metas,\n        gt_bboxes,\n        gt_labels=None,\n        gt_keypointss=None,\n        gt_bboxes_ignore=None,\n        proposal_cfg=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            x (list[Tensor]): Features from FPN.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used\n        Returns:\n            tuple:\n                losses: (dict[str, Tensor]): A dictionary of loss components.\n                proposal_list (list[Tensor]): Proposals of each image.\n        \"\"\"\n        outs = self(x)\n        if gt_labels is None:\n            loss_inputs = outs + (gt_bboxes, img_metas)\n        else:\n            loss_inputs = outs + (gt_bboxes, gt_labels, gt_keypointss, img_metas)\n\n        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        if proposal_cfg is None:\n            return losses\n        else:\n            proposal_list = self.get_bboxes(*outs, img_metas, cfg=proposal_cfg)\n            return losses, proposal_list\n\n    def get_anchors(self, featmap_sizes, img_metas, device=\"cuda\"):\n        \"\"\"Get anchors according to feature map sizes.\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n            device (torch.device | str): Device for returned tensors\n        Returns:\n            tuple:\n                anchor_list (list[Tensor]): Anchors of each image.\n                valid_flag_list (list[Tensor]): Valid flags of each image.\n        \"\"\"\n        num_imgs = len(img_metas)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = self.anchor_generator.grid_anchors(featmap_sizes, device)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = self.anchor_generator.valid_flags(\n                featmap_sizes, img_meta[\"pad_shape\"], device\n            )\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def anchor_center(self, anchors):\n        \"\"\"Get anchor centers from anchors.\n        Args:\n            anchors (Tensor): Anchor list with shape (N, 4), \"xyxy\" format.\n        Returns:\n            Tensor: Anchor centers with shape (N, 2), \"xy\" format.\n        \"\"\"\n        anchors_cx = (anchors[:, 2] + anchors[:, 0]) / 2\n        anchors_cy = (anchors[:, 3] + anchors[:, 1]) / 2\n        return torch.stack([anchors_cx, anchors_cy], dim=-1)\n\n    def loss_single(\n        self,\n        anchors,\n        cls_score,\n        bbox_pred,\n        kps_pred,\n        labels,\n        label_weights,\n        bbox_targets,\n        kps_targets,\n        kps_weights,\n        stride,\n        num_total_samples,\n    ):\n        \"\"\"Compute loss of a single scale level.\n        Args:\n            anchors (Tensor): Box reference for each scale level with shape\n                (N, num_total_anchors, 4).\n            cls_score (Tensor): Cls and quality joint scores for each scale\n                level has shape (N, num_classes, H, W).\n            bbox_pred (Tensor): Box distribution logits for each scale\n                level with shape (N, 4*(n+1), H, W), n is max value of integral\n                set.\n            labels (Tensor): Labels of each anchors with shape\n                (N, num_total_anchors).\n            label_weights (Tensor): Label weights of each anchor with shape\n                (N, num_total_anchors)\n            bbox_targets (Tensor): BBox regression targets of each anchor wight\n                shape (N, num_total_anchors, 4).\n            stride (tuple): Stride in this scale level.\n            num_total_samples (int): Number of positive samples that is\n                reduced over all GPUs.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        assert stride[0] == stride[1], \"h stride is not equal to w stride!\"\n        use_qscore = True\n        anchors = anchors.reshape(-1, 4)\n        cls_score = cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels)\n        if not self.use_dfl:\n            bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        else:\n            bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(\n                -1, 4 * (self.reg_max + 1)\n            )\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n\n        if self.use_kps:\n            kps_pred = kps_pred.permute(0, 2, 3, 1).reshape(-1, self.NK * 2)\n            kps_targets = kps_targets.reshape((-1, self.NK * 2))\n            kps_weights = kps_weights.reshape((-1, self.NK * 2))\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = self.num_classes\n        pos_inds = ((labels >= 0) & (labels < bg_class_ind)).nonzero().squeeze(1)\n        score = label_weights.new_zeros(labels.shape)\n\n        if len(pos_inds) > 0:\n            pos_bbox_targets = bbox_targets[pos_inds]\n            pos_bbox_pred = bbox_pred[pos_inds]\n            pos_anchors = anchors[pos_inds]\n            pos_anchor_centers = self.anchor_center(pos_anchors) / stride[0]\n\n            weight_targets = cls_score.detach().sigmoid()\n            weight_targets = weight_targets.max(dim=1)[0][pos_inds]\n            pos_decode_bbox_targets = pos_bbox_targets / stride[0]\n\n            if self.use_dfl:\n                pos_bbox_pred_corners = self.integral(pos_bbox_pred)\n                pos_decode_bbox_pred = distance2bbox(\n                    pos_anchor_centers, pos_bbox_pred_corners\n                )\n            else:\n                pos_decode_bbox_pred = distance2bbox(pos_anchor_centers, pos_bbox_pred)\n            if self.use_kps:\n                raise NotImplementedError(\"Currently not supported.\")\n\n            if use_qscore:\n                score[pos_inds] = bbox_overlaps(\n                    pos_decode_bbox_pred.detach(), pos_decode_bbox_targets\n                )\n            else:\n                score[pos_inds] = 1.0\n\n            # regression loss\n            loss_bbox = self.loss_bbox(\n                pos_decode_bbox_pred,\n                pos_decode_bbox_targets,\n                weight=weight_targets,\n                avg_factor=1.0,\n            )\n\n            if self.use_kps:\n                raise NotImplementedError(\"Currently not supported.\")\n            else:\n                loss_kps = kps_pred.sum() * 0\n\n            # dfl loss\n            if self.use_dfl:\n                pred_corners = pos_bbox_pred.reshape(-1, self.reg_max + 1)\n                target_corners = bbox2distance(\n                    pos_anchor_centers, pos_decode_bbox_targets, self.reg_max\n                ).reshape(-1)\n                loss_dfl = self.loss_dfl(\n                    pred_corners,\n                    target_corners,\n                    weight=weight_targets[:, None].expand(-1, 4).reshape(-1),\n                    avg_factor=4.0,\n                )\n            else:\n                loss_dfl = bbox_pred.sum() * 0\n        else:\n            loss_bbox = bbox_pred.sum() * 0\n            loss_dfl = bbox_pred.sum() * 0\n            loss_kps = kps_pred.sum() * 0\n            weight_targets = torch.tensor(0).cuda()\n\n        loss_cls = self.loss_cls(\n            cls_score,\n            (labels, score),\n            weight=label_weights,\n            avg_factor=num_total_samples,\n        )\n\n        return loss_cls, loss_bbox, loss_dfl, loss_kps, weight_targets.sum()\n\n    def loss(\n        self,\n        cls_scores,\n        bbox_preds,\n        kps_preds,\n        gt_bboxes,\n        gt_labels,\n        gt_keypointss,\n        img_metas,\n        gt_bboxes_ignore=None,\n    ):\n        \"\"\"Compute losses of the head.\n        Args:\n            cls_scores (list[Tensor]): Cls and quality scores for each scale\n                level has shape (N, num_classes, H, W).\n            bbox_preds (list[Tensor]): Box distribution logits for each scale\n                level with shape (N, 4*(n+1), H, W), n is max value of integral\n                set.\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): class indices corresponding to each box\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (list[Tensor] | None): specify which bounding\n                boxes can be ignored when computing the loss.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device\n        )\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            gt_keypointss,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n        )\n        if cls_reg_targets is None:\n            return None\n\n        (\n            anchor_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            keypoints_targets_list,\n            keypoints_weights_list,\n            num_total_pos,\n            num_total_neg,\n        ) = cls_reg_targets\n\n        num_total_samples = reduce_mean(\n            torch.tensor(num_total_pos, dtype=torch.float, device=device)\n        ).item()\n        num_total_samples = max(num_total_samples, 1.0)\n\n        losses_cls, losses_bbox, losses_dfl, losses_kps, avg_factor = multi_apply(\n            self.loss_single,\n            anchor_list,\n            cls_scores,\n            bbox_preds,\n            kps_preds,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            keypoints_targets_list,\n            keypoints_weights_list,\n            self.anchor_generator.strides,\n            num_total_samples=num_total_samples,\n        )\n\n        avg_factor = sum(avg_factor)\n        avg_factor = reduce_mean(avg_factor).item()\n        losses_bbox = list(map(lambda x: x / avg_factor, losses_bbox))\n        losses = dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n        if self.use_kps:\n            losses_kps = list(map(lambda x: x / avg_factor, losses_kps))\n            losses[\"loss_kps\"] = losses_kps\n        if self.use_dfl:\n            losses_dfl = list(map(lambda x: x / avg_factor, losses_dfl))\n            losses[\"loss_dfl\"] = losses_dfl\n        return losses\n\n    def get_bboxes(\n        self,\n        cls_scores,\n        bbox_preds,\n        kps_preds,\n        img_metas,\n        cfg=None,\n        rescale=False,\n        with_nms=True,\n    ):\n        \"\"\"Transform network output for a batch into bbox predictions.\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W)\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            cfg (mmcv.Config | None): Test / postprocessing configuration,\n                if None, test_cfg would be used\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n        Returns:\n            list[tuple[Tensor, Tensor]]: Each item in result_list is 2-tuple.\n                The first item is an (n, 5) tensor, where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1. The second item is a\n                (n,) tensor where each item is the predicted class labelof the\n                corresponding box.\n        Example:\n            >>> import mmcv\n            >>> self = AnchorHead(\n            >>>     num_classes=9,\n            >>>     in_channels=1,\n            >>>     anchor_generator=dict(\n            >>>         type='AnchorGenerator',\n            >>>         scales=[8],\n            >>>         ratios=[0.5, 1.0, 2.0],\n            >>>         strides=[4,]))\n            >>> img_metas = [{'img_shape': (32, 32, 3), 'scale_factor': 1}]\n            >>> cfg = mmcv.Config(dict(\n            >>>     score_thr=0.00,\n            >>>     nms=dict(type='nms', iou_thr=1.0),\n            >>>     max_per_img=10))\n            >>> feat = torch.rand(1, 1, 3, 3)\n            >>> cls_score, bbox_pred = self.forward_single(feat)\n            >>> # note the input lists are over different levels, not images\n            >>> cls_scores, bbox_preds = [cls_score], [bbox_pred]\n            >>> result_list = self.get_bboxes(cls_scores, bbox_preds,\n            >>>                               img_metas, cfg)\n            >>> det_bboxes, det_labels = result_list[0]\n            >>> assert len(result_list) == 1\n            >>> assert det_bboxes.shape[1] == 5\n            >>> assert len(det_bboxes) == len(det_labels) == cfg.max_per_img\n        \"\"\"\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        device = cls_scores[0].device\n        featmap_sizes = [cls_scores[i].shape[-2:] for i in range(num_levels)]\n        mlvl_anchors = self.anchor_generator.grid_anchors(featmap_sizes, device=device)\n\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [cls_scores[i][img_id].detach() for i in range(num_levels)]\n            bbox_pred_list = [bbox_preds[i][img_id].detach() for i in range(num_levels)]\n            img_shape = img_metas[img_id][\"img_shape\"]\n            scale_factor = img_metas[img_id][\"scale_factor\"]\n            if with_nms:\n                # some heads don't support with_nms argument\n                proposals = self._get_bboxes_single(\n                    cls_score_list,\n                    bbox_pred_list,\n                    mlvl_anchors,\n                    img_shape,\n                    scale_factor,\n                    cfg,\n                    rescale,\n                )\n            else:\n                proposals = self._get_bboxes_single(\n                    cls_score_list,\n                    bbox_pred_list,\n                    mlvl_anchors,\n                    img_shape,\n                    scale_factor,\n                    cfg,\n                    rescale,\n                    with_nms,\n                )\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(\n        self,\n        cls_scores,\n        bbox_preds,\n        mlvl_anchors,\n        img_shape,\n        scale_factor,\n        cfg,\n        rescale=False,\n        with_nms=True,\n    ):\n        \"\"\"Transform outputs for a single batch item into labeled boxes.\n        Args:\n            cls_scores (list[Tensor]): Box scores for a single scale level\n                has shape (num_classes, H, W).\n            bbox_preds (list[Tensor]): Box distribution logits for a single\n                scale level with shape (4*(n+1), H, W), n is max value of\n                integral set.\n            mlvl_anchors (list[Tensor]): Box reference for a single scale level\n                with shape (num_total_anchors, 4).\n            img_shape (tuple[int]): Shape of the input image,\n                (height, width, 3).\n            scale_factor (ndarray): Scale factor of the image arange as\n                (w_scale, h_scale, w_scale, h_scale).\n            cfg (mmcv.Config | None): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n        Returns:\n            tuple(Tensor):\n                det_bboxes (Tensor): Bbox predictions in shape (N, 5), where\n                    the first 4 columns are bounding box positions\n                    (tl_x, tl_y, br_x, br_y) and the 5-th column is a score\n                    between 0 and 1.\n                det_labels (Tensor): A (N,) tensor where each item is the\n                    predicted class label of the corresponding box.\n        \"\"\"\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, stride, anchors in zip(\n            cls_scores, bbox_preds, self.anchor_generator.strides, mlvl_anchors\n        ):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            assert stride[0] == stride[1]\n\n            scores = (\n                cls_score.permute(1, 2, 0).reshape(-1, self.cls_out_channels).sigmoid()\n            )\n            bbox_pred = bbox_pred.permute(1, 2, 0)\n            if self.use_dfl:\n                bbox_pred = self.integral(bbox_pred) * stride[0]\n            else:\n                bbox_pred = bbox_pred.reshape((-1, 4)) * stride[0]\n\n            nms_pre = cfg.get(\"nms_pre\", -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                max_scores, _ = scores.max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n\n            bboxes = distance2bbox(\n                self.anchor_center(anchors), bbox_pred, max_shape=img_shape\n            )\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n\n        mlvl_scores = torch.cat(mlvl_scores)\n        # Add a dummy background class to the backend when using sigmoid\n        # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n        # BG cat_id: num_class\n        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n        mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n\n        if with_nms:\n            det_bboxes, det_labels = multiclass_nms(\n                mlvl_bboxes,\n                mlvl_scores,\n                cfg.get(\"score_thr\"),\n                cfg.get(\"nms\"),\n                cfg.get(\"max_per_img\"),\n            )\n            return det_bboxes, det_labels\n        else:\n            return mlvl_bboxes, mlvl_scores\n\n    def get_targets(\n        self,\n        anchor_list,\n        valid_flag_list,\n        gt_bboxes_list,\n        gt_keypointss_list,\n        img_metas,\n        gt_bboxes_ignore_list=None,\n        gt_labels_list=None,\n        label_channels=1,\n        unmap_outputs=True,\n    ):\n        \"\"\"Get targets for GFL head.\n        This method is almost the same as `AnchorHead.get_targets()`. Besides\n        returning the targets as the parent method does, it also returns the\n        anchors as the first element of the returned tuple.\n        \"\"\"\n        num_imgs = len(img_metas)\n        assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        num_level_anchors_list = [num_level_anchors] * num_imgs\n\n        # concat all level anchors and flags to a single tensor\n        for i in range(num_imgs):\n            assert len(anchor_list[i]) == len(valid_flag_list[i])\n            anchor_list[i] = torch.cat(anchor_list[i])\n            valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        if gt_labels_list is None:\n            gt_labels_list = [None for _ in range(num_imgs)]\n        if gt_keypointss_list is None:\n            gt_keypointss_list = [None for _ in range(num_imgs)]\n\n        (\n            all_anchors,\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            all_keypoints_targets,\n            all_keypoints_weights,\n            pos_inds_list,\n            neg_inds_list,\n        ) = multi_apply(\n            self._get_target_single,\n            anchor_list,\n            valid_flag_list,\n            num_level_anchors_list,\n            gt_bboxes_list,\n            gt_bboxes_ignore_list,\n            gt_labels_list,\n            gt_keypointss_list,\n            img_metas,\n            label_channels=label_channels,\n            unmap_outputs=unmap_outputs,\n        )\n        # no valid anchors\n        if any([labels is None for labels in all_labels]):\n            return None\n        # sampled anchors of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        # split targets to a list w.r.t. multiple levels\n        anchors_list = images_to_levels(all_anchors, num_level_anchors)\n        labels_list = images_to_levels(all_labels, num_level_anchors)\n        label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n        bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n        bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n        keypoints_targets_list = images_to_levels(\n            all_keypoints_targets, num_level_anchors\n        )\n        keypoints_weights_list = images_to_levels(\n            all_keypoints_weights, num_level_anchors\n        )\n        return (\n            anchors_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            keypoints_targets_list,\n            keypoints_weights_list,\n            num_total_pos,\n            num_total_neg,\n        )\n\n    def _get_target_single(\n        self,\n        flat_anchors,\n        valid_flags,\n        num_level_anchors,\n        gt_bboxes,\n        gt_bboxes_ignore,\n        gt_labels,\n        gt_keypointss,\n        img_meta,\n        label_channels=1,\n        unmap_outputs=True,\n    ):\n        \"\"\"Compute regression, classification targets for anchors in a single\n        image.\n        Args:\n            flat_anchors (Tensor): Multi-level anchors of the image, which are\n                concatenated into a single tensor of shape (num_anchors, 4)\n            valid_flags (Tensor): Multi level valid flags of the image,\n                which are concatenated into a single tensor of\n                    shape (num_anchors,).\n            num_level_anchors Tensor): Number of anchors of each scale level.\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            img_meta (dict): Meta info of the image.\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n        Returns:\n            tuple: N is the number of total anchors in the image.\n                anchors (Tensor): All anchors in the image with shape (N, 4).\n                labels (Tensor): Labels of all anchors in the image with shape\n                    (N,).\n                label_weights (Tensor): Label weights of all anchor in the\n                    image with shape (N,).\n                bbox_targets (Tensor): BBox targets of all anchors in the\n                    image with shape (N, 4).\n                bbox_weights (Tensor): BBox weights of all anchors in the\n                    image with shape (N, 4).\n                pos_inds (Tensor): Indices of postive anchor with shape\n                    (num_pos,).\n                neg_inds (Tensor): Indices of negative anchor with shape\n                    (num_neg,).\n        \"\"\"\n        inside_flags = anchor_inside_flags(\n            flat_anchors,\n            valid_flags,\n            img_meta[\"img_shape\"][:2],\n            self.train_cfg.allowed_border,\n        )\n        if not inside_flags.any():\n            return (None,) * 7\n        # assign gt and sample anchors\n        anchors = flat_anchors[inside_flags, :]\n\n        num_level_anchors_inside = self.get_num_level_anchors_inside(\n            num_level_anchors, inside_flags\n        )\n        if self.assigner.__class__.__name__ == \"ATSSAssigner\":\n            assign_result = self.assigner.assign(\n                anchors,\n                num_level_anchors_inside,\n                gt_bboxes,\n                gt_bboxes_ignore,\n                gt_labels,\n            )\n        else:\n            assign_result = self.assigner.assign(\n                anchors, gt_bboxes, gt_bboxes_ignore, gt_labels\n            )\n\n        sampling_result = self.sampler.sample(assign_result, anchors, gt_bboxes)\n\n        num_valid_anchors = anchors.shape[0]\n        bbox_targets = torch.zeros_like(anchors)\n        bbox_weights = torch.zeros_like(anchors)\n        kps_targets = anchors.new_zeros(size=(anchors.shape[0], self.NK * 2))\n        kps_weights = anchors.new_zeros(size=(anchors.shape[0], self.NK * 2))\n        labels = anchors.new_full(\n            (num_valid_anchors,), self.num_classes, dtype=torch.long\n        )\n        label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            pos_bbox_targets = sampling_result.pos_gt_bboxes\n            bbox_targets[pos_inds, :] = pos_bbox_targets\n            bbox_weights[pos_inds, :] = 1.0\n            if self.use_kps:\n                pos_assigned_gt_inds = sampling_result.pos_assigned_gt_inds\n                kps_targets[pos_inds, :] = gt_keypointss[\n                    pos_assigned_gt_inds, :, :2\n                ].reshape((-1, self.NK * 2))\n                kps_weights[pos_inds, :] = torch.mean(\n                    gt_keypointss[pos_assigned_gt_inds, :, 2], dim=1, keepdims=True\n                )\n            if gt_labels is None:\n                # Only rpn gives gt_labels as None\n                # Foreground is the first class\n                labels[pos_inds] = 0\n            else:\n                labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n            if self.train_cfg.pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = self.train_cfg.pos_weight\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_anchors.size(0)\n            anchors = unmap(anchors, num_total_anchors, inside_flags)\n            labels = unmap(\n                labels, num_total_anchors, inside_flags, fill=self.num_classes\n            )\n            label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n            bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n            if self.use_kps:\n                kps_targets = unmap(kps_targets, num_total_anchors, inside_flags)\n                kps_weights = unmap(kps_weights, num_total_anchors, inside_flags)\n\n        return (\n            anchors,\n            labels,\n            label_weights,\n            bbox_targets,\n            bbox_weights,\n            kps_targets,\n            kps_weights,\n            pos_inds,\n            neg_inds,\n        )\n\n    def get_num_level_anchors_inside(self, num_level_anchors, inside_flags):\n        split_inside_flags = torch.split(inside_flags, num_level_anchors)\n        num_level_anchors_inside = [int(flags.sum()) for flags in split_inside_flags]\n        return num_level_anchors_inside\n\n    def aug_test(self, feats, img_metas, rescale=False):\n        \"\"\"Test function with test time augmentation.\n        Args:\n            feats (list[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains features for all images in the batch.\n            img_metas (list[list[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch. each dict has image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n        Returns:\n            list[ndarray]: bbox results of each class\n        \"\"\"\n        return self.aug_test_bboxes(feats, img_metas, rescale=rescale)", ""]}
{"filename": "dataset2metadata/face_detection/scrfd_wrapper.py", "chunked_list": ["# contributed by George Smyrnis\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import OrderedDict\nfrom typing import Tuple\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\n", "import torch.nn as nn\n\nfrom dataset2metadata.face_detection.backbone import ResNetV1e\nfrom dataset2metadata.face_detection.head import SCRFDHead\nfrom dataset2metadata.face_detection.neck import PAFPN\nfrom dataset2metadata.face_detection.utils import bbox2result\n\n\nclass BaseDetector(nn.Module, metaclass=ABCMeta):\n    \"\"\"Base class for detectors.\"\"\"\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n        self.fp16_enabled = False\n\n    @property\n    def with_neck(self):\n        \"\"\"bool: whether the detector has a neck\"\"\"\n        return hasattr(self, \"neck\") and self.neck is not None\n\n    # TODO: these properties need to be carefully handled\n    # for both single stage & two stage detectors\n    @property\n    def with_shared_head(self):\n        \"\"\"bool: whether the detector has a shared head in the RoI Head\"\"\"\n        return hasattr(self, \"roi_head\") and self.roi_head.with_shared_head\n\n    @property\n    def with_bbox(self):\n        \"\"\"bool: whether the detector has a bbox head\"\"\"\n        return (hasattr(self, \"roi_head\") and self.roi_head.with_bbox) or (\n            hasattr(self, \"bbox_head\") and self.bbox_head is not None\n        )\n\n    @property\n    def with_mask(self):\n        \"\"\"bool: whether the detector has a mask head\"\"\"\n        return (hasattr(self, \"roi_head\") and self.roi_head.with_mask) or (\n            hasattr(self, \"mask_head\") and self.mask_head is not None\n        )\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        \"\"\"Extract features from images.\"\"\"\n        pass\n\n    def extract_feats(self, imgs):\n        \"\"\"Extract features from multiple images.\n        Args:\n            imgs (list[torch.Tensor]): A list of images. The images are\n                augmented from the same image but in different ways.\n        Returns:\n            list[torch.Tensor]: Features of different images\n        \"\"\"\n        assert isinstance(imgs, list)\n        return [self.extract_feat(img) for img in imgs]\n\n    def forward_train(self, imgs, img_metas, **kwargs):\n        \"\"\"\n        Args:\n            img (list[Tensor]): List of tensors of shape (1, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys, see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            kwargs (keyword arguments): Specific to concrete implementation.\n        \"\"\"\n        # NOTE the batched image size information may be useful, e.g.\n        # in DETR, this is needed for the construction of masks, which is\n        # then used for the transformer_head.\n        batch_input_shape = tuple(imgs[0].size()[-2:])\n        for img_meta in img_metas:\n            img_meta[\"batch_input_shape\"] = batch_input_shape\n\n    async def async_simple_test(self, img, img_metas, **kwargs):\n        raise NotImplementedError\n\n    @abstractmethod\n    def simple_test(self, img, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        \"\"\"Test function with test time augmentation.\"\"\"\n        pass\n\n    async def aforward_test(self, *, img, img_metas, **kwargs):\n        for var, name in [(img, \"img\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(f\"{name} must be a list, but got {type(var)}\")\n\n        num_augs = len(img)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                f\"num of augmentations ({len(img)}) \"\n                f\"!= num of image metas ({len(img_metas)})\"\n            )\n        # TODO: remove the restriction of samples_per_gpu == 1 when prepared\n        samples_per_gpu = img[0].size(0)\n        assert samples_per_gpu == 1\n\n        if num_augs == 1:\n            return await self.async_simple_test(img[0], img_metas[0], **kwargs)\n        else:\n            raise NotImplementedError\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        \"\"\"\n        Args:\n            imgs (List[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (List[List[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch.\n        \"\"\"\n        for var, name in [(imgs, \"imgs\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(f\"{name} must be a list, but got {type(var)}\")\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                f\"num of augmentations ({len(imgs)}) \"\n                f\"!= num of image meta ({len(img_metas)})\"\n            )\n\n        # NOTE the batched image size information may be useful, e.g.\n        # in DETR, this is needed for the construction of masks, which is\n        # then used for the transformer_head.\n        for img, img_meta in zip(imgs, img_metas):\n            batch_size = len(img_meta)\n            for img_id in range(batch_size):\n                img_meta[img_id][\"batch_input_shape\"] = tuple(img.size()[-2:])\n\n        if num_augs == 1:\n            # proposals (List[List[Tensor]]): the outer list indicates\n            # test-time augs (multiscale, flip, etc.) and the inner list\n            # indicates images in a batch.\n            # The Tensor should have a shape Px4, where P is the number of\n            # proposals.\n            if \"proposals\" in kwargs:\n                kwargs[\"proposals\"] = kwargs[\"proposals\"][0]\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            assert imgs[0].size(0) == 1, (\n                \"aug test does not support \"\n                \"inference with batch size \"\n                f\"{imgs[0].size(0)}\"\n            )\n            # TODO: support test augmentation for predefined proposals\n            assert \"proposals\" not in kwargs\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    def forward(self, img, img_metas, return_loss=True, **kwargs):\n        \"\"\"Calls either :func:`forward_train` or :func:`forward_test` depending\n        on whether ``return_loss`` is ``True``.\n        Note this setting will change the expected inputs. When\n        ``return_loss=True``, img and img_meta are single-nested (i.e. Tensor\n        and List[dict]), and when ``resturn_loss=False``, img and img_meta\n        should be double nested (i.e.  List[Tensor], List[List[dict]]), with\n        the outer list indicating test time augmentations.\n        \"\"\"\n        if return_loss:\n            return self.forward_train(img, img_metas, **kwargs)\n        else:\n            return self.forward_test(img, img_metas, **kwargs)\n\n    def _parse_losses(self, losses):\n        \"\"\"Parse the raw outputs (losses) of the network.\n        Args:\n            losses (dict): Raw output of the network, which usually contain\n                losses and other necessary infomation.\n        Returns:\n            tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor \\\n                which may be a weighted sum of all losses, log_vars contains \\\n                all the variables to be sent to the logger.\n        \"\"\"\n        log_vars = OrderedDict()\n        for loss_name, loss_value in losses.items():\n            if isinstance(loss_value, torch.Tensor):\n                log_vars[loss_name] = loss_value.mean()\n            elif isinstance(loss_value, list):\n                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n            else:\n                raise TypeError(f\"{loss_name} is not a tensor or list of tensors\")\n\n        loss = sum(_value for _key, _value in log_vars.items() if \"loss\" in _key)\n\n        log_vars[\"loss\"] = loss\n        for loss_name, loss_value in log_vars.items():\n            # reduce loss when distributed training\n            if dist.is_available() and dist.is_initialized():\n                loss_value = loss_value.data.clone()\n                dist.all_reduce(loss_value.div_(dist.get_world_size()))\n            log_vars[loss_name] = loss_value.item()\n\n        return loss, log_vars\n\n    def train_step(self, data, optimizer):\n        \"\"\"The iteration step during training.\n        This method defines an iteration step during training, except for the\n        back propagation and optimizer updating, which are done in an optimizer\n        hook. Note that in some complicated cases or models, the whole process\n        including back propagation and optimizer updating is also defined in\n        this method, such as GAN.\n        Args:\n            data (dict): The output of dataloader.\n            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n                runner is passed to ``train_step()``. This argument is unused\n                and reserved.\n        Returns:\n            dict: It should contain at least 3 keys: ``loss``, ``log_vars``, \\\n                ``num_samples``.\n                - ``loss`` is a tensor for back propagation, which can be a \\\n                weighted sum of multiple losses.\n                - ``log_vars`` contains all the variables to be sent to the\n                logger.\n                - ``num_samples`` indicates the batch size (when the model is \\\n                DDP, it means the batch size on each GPU), which is used for \\\n                averaging the logs.\n        \"\"\"\n        losses = self(**data)\n        loss, log_vars = self._parse_losses(losses)\n\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[\"img_metas\"]))\n\n        return outputs\n\n    def val_step(self, data, optimizer):\n        \"\"\"The iteration step during validation.\n        This method shares the same signature as :func:`train_step`, but used\n        during val epochs. Note that the evaluation after training epochs is\n        not implemented with this method, but an evaluation hook.\n        \"\"\"\n        losses = self(**data)\n        loss, log_vars = self._parse_losses(losses)\n\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[\"img_metas\"]))\n\n        return outputs", "class BaseDetector(nn.Module, metaclass=ABCMeta):\n    \"\"\"Base class for detectors.\"\"\"\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n        self.fp16_enabled = False\n\n    @property\n    def with_neck(self):\n        \"\"\"bool: whether the detector has a neck\"\"\"\n        return hasattr(self, \"neck\") and self.neck is not None\n\n    # TODO: these properties need to be carefully handled\n    # for both single stage & two stage detectors\n    @property\n    def with_shared_head(self):\n        \"\"\"bool: whether the detector has a shared head in the RoI Head\"\"\"\n        return hasattr(self, \"roi_head\") and self.roi_head.with_shared_head\n\n    @property\n    def with_bbox(self):\n        \"\"\"bool: whether the detector has a bbox head\"\"\"\n        return (hasattr(self, \"roi_head\") and self.roi_head.with_bbox) or (\n            hasattr(self, \"bbox_head\") and self.bbox_head is not None\n        )\n\n    @property\n    def with_mask(self):\n        \"\"\"bool: whether the detector has a mask head\"\"\"\n        return (hasattr(self, \"roi_head\") and self.roi_head.with_mask) or (\n            hasattr(self, \"mask_head\") and self.mask_head is not None\n        )\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        \"\"\"Extract features from images.\"\"\"\n        pass\n\n    def extract_feats(self, imgs):\n        \"\"\"Extract features from multiple images.\n        Args:\n            imgs (list[torch.Tensor]): A list of images. The images are\n                augmented from the same image but in different ways.\n        Returns:\n            list[torch.Tensor]: Features of different images\n        \"\"\"\n        assert isinstance(imgs, list)\n        return [self.extract_feat(img) for img in imgs]\n\n    def forward_train(self, imgs, img_metas, **kwargs):\n        \"\"\"\n        Args:\n            img (list[Tensor]): List of tensors of shape (1, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys, see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            kwargs (keyword arguments): Specific to concrete implementation.\n        \"\"\"\n        # NOTE the batched image size information may be useful, e.g.\n        # in DETR, this is needed for the construction of masks, which is\n        # then used for the transformer_head.\n        batch_input_shape = tuple(imgs[0].size()[-2:])\n        for img_meta in img_metas:\n            img_meta[\"batch_input_shape\"] = batch_input_shape\n\n    async def async_simple_test(self, img, img_metas, **kwargs):\n        raise NotImplementedError\n\n    @abstractmethod\n    def simple_test(self, img, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        \"\"\"Test function with test time augmentation.\"\"\"\n        pass\n\n    async def aforward_test(self, *, img, img_metas, **kwargs):\n        for var, name in [(img, \"img\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(f\"{name} must be a list, but got {type(var)}\")\n\n        num_augs = len(img)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                f\"num of augmentations ({len(img)}) \"\n                f\"!= num of image metas ({len(img_metas)})\"\n            )\n        # TODO: remove the restriction of samples_per_gpu == 1 when prepared\n        samples_per_gpu = img[0].size(0)\n        assert samples_per_gpu == 1\n\n        if num_augs == 1:\n            return await self.async_simple_test(img[0], img_metas[0], **kwargs)\n        else:\n            raise NotImplementedError\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        \"\"\"\n        Args:\n            imgs (List[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (List[List[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch.\n        \"\"\"\n        for var, name in [(imgs, \"imgs\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(f\"{name} must be a list, but got {type(var)}\")\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                f\"num of augmentations ({len(imgs)}) \"\n                f\"!= num of image meta ({len(img_metas)})\"\n            )\n\n        # NOTE the batched image size information may be useful, e.g.\n        # in DETR, this is needed for the construction of masks, which is\n        # then used for the transformer_head.\n        for img, img_meta in zip(imgs, img_metas):\n            batch_size = len(img_meta)\n            for img_id in range(batch_size):\n                img_meta[img_id][\"batch_input_shape\"] = tuple(img.size()[-2:])\n\n        if num_augs == 1:\n            # proposals (List[List[Tensor]]): the outer list indicates\n            # test-time augs (multiscale, flip, etc.) and the inner list\n            # indicates images in a batch.\n            # The Tensor should have a shape Px4, where P is the number of\n            # proposals.\n            if \"proposals\" in kwargs:\n                kwargs[\"proposals\"] = kwargs[\"proposals\"][0]\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            assert imgs[0].size(0) == 1, (\n                \"aug test does not support \"\n                \"inference with batch size \"\n                f\"{imgs[0].size(0)}\"\n            )\n            # TODO: support test augmentation for predefined proposals\n            assert \"proposals\" not in kwargs\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    def forward(self, img, img_metas, return_loss=True, **kwargs):\n        \"\"\"Calls either :func:`forward_train` or :func:`forward_test` depending\n        on whether ``return_loss`` is ``True``.\n        Note this setting will change the expected inputs. When\n        ``return_loss=True``, img and img_meta are single-nested (i.e. Tensor\n        and List[dict]), and when ``resturn_loss=False``, img and img_meta\n        should be double nested (i.e.  List[Tensor], List[List[dict]]), with\n        the outer list indicating test time augmentations.\n        \"\"\"\n        if return_loss:\n            return self.forward_train(img, img_metas, **kwargs)\n        else:\n            return self.forward_test(img, img_metas, **kwargs)\n\n    def _parse_losses(self, losses):\n        \"\"\"Parse the raw outputs (losses) of the network.\n        Args:\n            losses (dict): Raw output of the network, which usually contain\n                losses and other necessary infomation.\n        Returns:\n            tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor \\\n                which may be a weighted sum of all losses, log_vars contains \\\n                all the variables to be sent to the logger.\n        \"\"\"\n        log_vars = OrderedDict()\n        for loss_name, loss_value in losses.items():\n            if isinstance(loss_value, torch.Tensor):\n                log_vars[loss_name] = loss_value.mean()\n            elif isinstance(loss_value, list):\n                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n            else:\n                raise TypeError(f\"{loss_name} is not a tensor or list of tensors\")\n\n        loss = sum(_value for _key, _value in log_vars.items() if \"loss\" in _key)\n\n        log_vars[\"loss\"] = loss\n        for loss_name, loss_value in log_vars.items():\n            # reduce loss when distributed training\n            if dist.is_available() and dist.is_initialized():\n                loss_value = loss_value.data.clone()\n                dist.all_reduce(loss_value.div_(dist.get_world_size()))\n            log_vars[loss_name] = loss_value.item()\n\n        return loss, log_vars\n\n    def train_step(self, data, optimizer):\n        \"\"\"The iteration step during training.\n        This method defines an iteration step during training, except for the\n        back propagation and optimizer updating, which are done in an optimizer\n        hook. Note that in some complicated cases or models, the whole process\n        including back propagation and optimizer updating is also defined in\n        this method, such as GAN.\n        Args:\n            data (dict): The output of dataloader.\n            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of\n                runner is passed to ``train_step()``. This argument is unused\n                and reserved.\n        Returns:\n            dict: It should contain at least 3 keys: ``loss``, ``log_vars``, \\\n                ``num_samples``.\n                - ``loss`` is a tensor for back propagation, which can be a \\\n                weighted sum of multiple losses.\n                - ``log_vars`` contains all the variables to be sent to the\n                logger.\n                - ``num_samples`` indicates the batch size (when the model is \\\n                DDP, it means the batch size on each GPU), which is used for \\\n                averaging the logs.\n        \"\"\"\n        losses = self(**data)\n        loss, log_vars = self._parse_losses(losses)\n\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[\"img_metas\"]))\n\n        return outputs\n\n    def val_step(self, data, optimizer):\n        \"\"\"The iteration step during validation.\n        This method shares the same signature as :func:`train_step`, but used\n        during val epochs. Note that the evaluation after training epochs is\n        not implemented with this method, but an evaluation hook.\n        \"\"\"\n        losses = self(**data)\n        loss, log_vars = self._parse_losses(losses)\n\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[\"img_metas\"]))\n\n        return outputs", "\n\nclass SingleStageDetector(BaseDetector):\n    \"\"\"Base class for single-stage detectors.\n    Single-stage detectors directly and densely predict bounding boxes on the\n    output features of the backbone+neck.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone,\n        neck=None,\n        bbox_head={},\n        train_cfg=None,\n        test_cfg=None,\n        pretrained=None,\n    ):\n        super(SingleStageDetector, self).__init__()\n        self.backbone = ResNetV1e(**backbone)\n        if neck is not None:\n            self.neck = PAFPN(**neck)\n        bbox_head.update(train_cfg=train_cfg)\n        bbox_head.update(test_cfg=test_cfg)\n        self.bbox_head = SCRFDHead(**bbox_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n    def extract_feat(self, img):\n        \"\"\"Directly extract features from the backbone+neck.\"\"\"\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        \"\"\"Used for computing network flops.\n        See `mmdetection/tools/get_flops.py`\n        \"\"\"\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        return outs\n\n    def forward_train(\n        self, img, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore=None\n    ):\n        \"\"\"\n        Args:\n            img (Tensor): Input images of shape (N, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): A List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            gt_bboxes (list[Tensor]): Each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        super(SingleStageDetector, self).forward_train(img, img_metas)\n        x = self.extract_feat(img)\n        losses = self.bbox_head.forward_train(\n            x, img_metas, gt_bboxes, gt_labels, gt_bboxes_ignore\n        )\n        return losses\n\n    def simple_test(self, img, img_metas, rescale=False):\n        \"\"\"Test function without test time augmentation.\n        Args:\n            imgs (list[torch.Tensor]): List of multiple images\n            img_metas (list[dict]): List of image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n        Returns:\n            list[list[np.ndarray]]: BBox results of each image and classes.\n                The outer list corresponds to each image. The inner list\n                corresponds to each class.\n        \"\"\"\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        if torch.onnx.is_in_onnx_export():\n            print(\"single_stage.py in-onnx-export\")\n            print(outs.__class__)\n            cls_score, bbox_pred = outs\n            for c in cls_score:\n                print(c.shape)\n            for c in bbox_pred:\n                print(c.shape)\n            return outs\n        bbox_list = self.bbox_head.get_bboxes(*outs, img_metas, rescale=rescale)\n        # skip post-processing when exporting to ONNX\n        if torch.onnx.is_in_onnx_export():\n            return bbox_list\n\n        bbox_results = [\n            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n            for det_bboxes, det_labels in bbox_list\n        ]\n        return bbox_results\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        \"\"\"Test function with test time augmentation.\n        Args:\n            imgs (list[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (list[list[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch. each dict has image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n        Returns:\n            list[list[np.ndarray]]: BBox results of each image and classes.\n                The outer list corresponds to each image. The inner list\n                corresponds to each class.\n        \"\"\"\n        assert hasattr(self.bbox_head, \"aug_test\"), (\n            f\"{self.bbox_head.__class__.__name__}\"\n            \" does not support test-time augmentation\"\n        )\n        print(\"aug-test:\", len(imgs))\n        feats = self.extract_feats(imgs)\n        return [self.bbox_head.aug_test(feats, img_metas, rescale=rescale)]", "\n\nclass SCRFD(SingleStageDetector):\n    def __init__(\n        self, backbone, neck, bbox_head, train_cfg=None, test_cfg=None, pretrained=None\n    ):\n        super(SCRFD, self).__init__(\n            backbone, neck, bbox_head, train_cfg, test_cfg, pretrained\n        )\n\n    def forward_train(\n        self,\n        img,\n        img_metas,\n        gt_bboxes,\n        gt_labels,\n        gt_keypointss=None,\n        gt_bboxes_ignore=None,\n    ):\n        \"\"\"\n        Args:\n            img (Tensor): Input images of shape (N, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): A List of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            gt_bboxes (list[Tensor]): Each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        super(SingleStageDetector, self).forward_train(img, img_metas)\n        x = self.extract_feat(img)\n        losses = self.bbox_head.forward_train(\n            x, img_metas, gt_bboxes, gt_labels, gt_keypointss, gt_bboxes_ignore\n        )\n        return losses\n\n    def simple_test(self, img, img_metas, rescale=False):\n        \"\"\"Test function without test time augmentation.\n        Args:\n            imgs (list[torch.Tensor]): List of multiple images\n            img_metas (list[dict]): List of image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n        Returns:\n            list[list[np.ndarray]]: BBox results of each image and classes.\n                The outer list corresponds to each image. The inner list\n                corresponds to each class.\n        \"\"\"\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        if torch.onnx.is_in_onnx_export():\n            print(\"single_stage.py in-onnx-export\")\n            print(outs.__class__)\n            cls_score, bbox_pred, kps_pred = outs\n            for c in cls_score:\n                print(c.shape)\n            for c in bbox_pred:\n                print(c.shape)\n            if self.bbox_head.use_kps:\n                for c in kps_pred:\n                    print(c.shape)\n                return (cls_score, bbox_pred, kps_pred)\n            else:\n                return (cls_score, bbox_pred)\n        bbox_list = self.bbox_head.get_bboxes(*outs, img_metas, rescale=rescale)\n\n        bbox_results = [\n            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n            for det_bboxes, det_labels in bbox_list\n        ]\n        return bbox_results\n\n    def feature_test(self, img):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        return outs", "\n\nMODEL_CONFIG = dict(\n    type=\"SCRFD\",\n    backbone=dict(\n        depth=0,\n        block_cfg=dict(\n            block=\"BasicBlock\",\n            stage_blocks=(3, 4, 2, 3),\n            stage_planes=[56, 88, 88, 224],", "            stage_blocks=(3, 4, 2, 3),\n            stage_planes=[56, 88, 88, 224],\n        ),\n        base_channels=56,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        norm_cfg=dict(type=\"BN\", requires_grad=True),\n        norm_eval=False,\n        style=\"pytorch\",\n    ),", "        style=\"pytorch\",\n    ),\n    neck=dict(\n        in_channels=[56, 88, 88, 224],\n        out_channels=56,\n        start_level=1,\n        add_extra_convs=\"on_output\",\n        num_outs=3,\n    ),\n    bbox_head=dict(", "    ),\n    bbox_head=dict(\n        num_classes=1,\n        in_channels=56,\n        stacked_convs=3,\n        feat_channels=80,\n        # norm_cfg=dict(type='BN', requires_grad=True),\n        norm_cfg=dict(type=\"GN\", num_groups=16, requires_grad=True),\n        cls_reg_share=True,\n        strides_share=True,", "        cls_reg_share=True,\n        strides_share=True,\n        scale_mode=2,\n        anchor_generator=dict(\n            type=\"AnchorGenerator\",\n            ratios=[1.0],\n            scales=[1, 2],\n            base_sizes=[16, 64, 256],\n            strides=[8, 16, 32],\n        ),", "            strides=[8, 16, 32],\n        ),\n        loss_cls=dict(\n            type=\"QualityFocalLoss\", use_sigmoid=True, beta=2.0, loss_weight=1.0\n        ),\n        loss_dfl=False,\n        reg_max=8,\n        loss_bbox=dict(type=\"DIoULoss\", loss_weight=2.0),\n        use_kps=False,\n        loss_kps=dict(type=\"SmoothL1Loss\", beta=0.1111111111111111, loss_weight=0.1),", "        use_kps=False,\n        loss_kps=dict(type=\"SmoothL1Loss\", beta=0.1111111111111111, loss_weight=0.1),\n        train_cfg=dict(\n            assigner=dict(type=\"ATSSAssigner\", topk=9),\n            allowed_border=-1,\n            pos_weight=-1,\n            debug=False,\n        ),\n        test_cfg=dict(\n            nms_pre=-1,", "        test_cfg=dict(\n            nms_pre=-1,\n            min_bbox_size=0,\n            score_thr=0.02,\n            nms=dict(type=\"nms\", iou_threshold=0.45),\n            max_per_img=-1,\n        ),\n    ),\n)\n", ")\n\nTEST_CFG = dict(\n    nms_pre=-1,\n    min_bbox_size=0,\n    score_thr=0.02,\n    nms=dict(type=\"nms\", iou_threshold=0.45),\n    max_per_img=-1,\n)\n", ")\n\n\ndef scrfd_wrapper(checkpoint_path):\n    model = SCRFD(\n        MODEL_CONFIG[\"backbone\"],\n        MODEL_CONFIG[\"neck\"],\n        MODEL_CONFIG[\"bbox_head\"],\n        test_cfg=TEST_CFG,\n    )\n    state_dict = torch.load(checkpoint_path)[\"state_dict\"]\n    model.load_state_dict(state_dict)\n\n    return model", "\n\nclass FaceDetector(object):\n    def __init__(self, checkpoint_path: str, device: int = 0) -> None:\n        if device < 0:\n            self.device = \"cpu\"\n        else:\n            if not torch.cuda.is_available():\n                raise RuntimeError(\"CUDA is not available. Consider setting gpu = -1.\")\n            else:\n                self.device = f\"cuda:{device}\"\n\n        self.model = scrfd_wrapper(checkpoint_path).eval().to(self.device)\n\n    def _apply_threshold(self, bbox_result_image, thr: float) -> torch.Tensor:\n        \"\"\"Apply threshold to predicted bboxes.\n\n        Args:\n            bbox_result_image: Results on bboxes as outputted by the model\n                bbox_head. A tuple of tensors, with the first being the tensor\n                containing possible face detections and scores.\n            thr: Threshold to apply on scores.\n\n        Returns:\n            A (F,4) tensor, of type torch.float32, containing the indices of the\n                bounding box corners, in the format [x_top_left, y_top_left,\n                x_bottom_right, y_bottom_right]. The indices are scaled from\n                0 to 1 (corresponding to a relative point from the top left\n                part of the image).\n        \"\"\"\n        bboxes_with_score = bbox_result_image[0]\n        scores = bboxes_with_score[:, -1]\n        ind = scores > thr\n        predicted_bboxes = bboxes_with_score[ind, :-1]\n        return predicted_bboxes\n\n    def _rescale(\n        self,\n        bbox: torch.Tensor,\n        paddings: torch.Tensor,\n        img_shape: Tuple[int, int],\n    ) -> torch.Tensor:\n        \"\"\"Rescale bounding box to [0,1].\n\n        Args:\n            bbox: Bounding box to rescale.\n            img_shape: Dimensions (H,W) of image.\n            paddings: The pads in each border of the image.\n\n        Returns:\n            A torch.Tensor corresponding to the bounding box of the image, with\n                elements in [0,1] (relative positions with respect to the top\n                left corner of the original image).\n        \"\"\"\n        H, W = img_shape\n        result = torch.zeros_like(bbox).type(torch.float32)\n\n        pad_left, pad_top, pad_right, pad_bottom = paddings\n\n        result[:, 0] = torch.clamp(\n            (bbox[:, 0] - pad_left * W) / ((1.0 - pad_left - pad_right) * W), 0, 1\n        )\n        result[:, 1] = torch.clamp(\n            (bbox[:, 1] - pad_top * H) / ((1.0 - pad_top - pad_bottom) * H), 0, 1\n        )\n        result[:, 2] = torch.clamp(\n            (bbox[:, 2] - pad_left * W) / ((1.0 - pad_left - pad_right) * W), 0, 1\n        )\n        result[:, 3] = torch.clamp(\n            (bbox[:, 3] - pad_top * H) / ((1.0 - pad_top - pad_bottom) * H), 0, 1\n        )\n\n        return result\n\n    def detect_faces(\n        self,\n        images: torch.Tensor,\n        paddings: torch.Tensor,\n        score_threshold: float = 0.3,\n    ):\n        \"\"\"Detect faces.\n\n        Args:\n            images: Batch of images, of shape (N, C, H, W).\n            paddings: Pads in each image, of shape (N, 4).\n            score_threshold: Confidence cutoff for bounding boxes, in [0,1].\n                Default = 0.3\n\n        Returns:\n            A list of list of float, with each element corresponding to a single\n                image. For each element there are F inner lists with 4 elements,\n                where F is the number of faces detected in the image with format\n                [x_top_left, y_top_left, x_bottom_right, y_bottom_right].\n        \"\"\"\n        with torch.no_grad():\n            if len(images.shape) != 4 or images.shape[1] != 3:\n                raise ValueError(\n                    \"Images should be provided as a tensor of shape (N,3,H,W).\"\n                )\n\n            if images.dtype == torch.uint8:\n                images = images.float() / 255.0\n\n            images = images.to(self.device)\n\n            N, C, H, W = images.shape\n\n            img_metas = [{\"img_shape\": (H, W, C), \"scale_factor\": 1}] * N\n\n            result = self.model.feature_test(images)  # type: ignore\n            bboxes = self.model.bbox_head.get_bboxes(  # type: ignore\n                *result, img_metas=img_metas\n            )\n\n            # Threshold the bounding boxes.\n            bboxes = list(\n                map(lambda x: self._apply_threshold(x, score_threshold), bboxes)\n            )\n\n            # Make the bounding boxes relative to image.\n            bboxes = list(\n                map(\n                    lambda x, paddings: self._rescale(x, paddings, (H, W)),\n                    bboxes,\n                    paddings.unbind(),\n                )\n            )\n\n            # Make the result be a list of lists.\n            bboxes = list(map(lambda x: x.tolist(), bboxes))\n\n            return bboxes", ""]}
{"filename": "dataset2metadata/face_detection/__init__.py", "chunked_list": [""]}
{"filename": "dataset2metadata/face_detection/utils.py", "chunked_list": ["# contributed by George Smyrnis\n\nimport functools\nfrom enum import Enum\nfrom functools import partial\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist", "import torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nimport torchvision\n\n\ndef reduce_mean(tensor):\n    \"\"\" \"Obtain the mean of tensor on different GPUs.\"\"\"\n    if not (dist.is_available() and dist.is_initialized()):\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor.div_(dist.get_world_size()), op=dist.ReduceOp.SUM)\n    return tensor", "\n\ndef bbox2result(bboxes, labels, num_classes):\n    \"\"\"Convert detection results to a list of numpy arrays.\n    Args:\n        bboxes (torch.Tensor | np.ndarray): shape (n, 5)\n        labels (torch.Tensor | np.ndarray): shape (n, )\n        num_classes (int): class number, including background class\n    Returns:\n        list(ndarray): bbox results of each class\n    \"\"\"\n    if bboxes.shape[0] == 0:\n        return [np.zeros((0, 5), dtype=np.float32) for i in range(num_classes)]\n    else:\n        if isinstance(bboxes, torch.Tensor):\n            bboxes = bboxes.detach().cpu().numpy()\n            labels = labels.detach().cpu().numpy()\n        return [bboxes[labels == i, :] for i in range(num_classes)]", "\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\"iou\", eps=1e-6, use_legacy_coordinate=False):\n    \"\"\"Calculate the ious between each bbox of bboxes1 and bboxes2.\n    Args:\n        bboxes1 (ndarray): Shape (n, 4)\n        bboxes2 (ndarray): Shape (k, 4)\n        mode (str): IOU (intersection over union) or IOF (intersection\n            over foreground)\n        use_legacy_coordinate (bool): Whether to use coordinate system in\n            mmdet v1.x. which means width, height should be\n            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.\n            Note when function is used in `VOCDataset`, it should be\n            True to align with the official implementation\n            `http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar`\n            Default: False.\n    Returns:\n        ious (ndarray): Shape (n, k)\n    \"\"\"\n\n    assert mode in [\"iou\", \"iof\"]\n    if not use_legacy_coordinate:\n        extra_length = 0.0\n    else:\n        extra_length = 1.0\n    bboxes1 = bboxes1.astype(np.float32)\n    bboxes2 = bboxes2.astype(np.float32)\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = np.zeros((rows, cols), dtype=np.float32)\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = np.zeros((cols, rows), dtype=np.float32)\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + extra_length) * (\n        bboxes1[:, 3] - bboxes1[:, 1] + extra_length\n    )\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + extra_length) * (\n        bboxes2[:, 3] - bboxes2[:, 1] + extra_length\n    )\n    for i in range(bboxes1.shape[0]):\n        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n        overlap = np.maximum(x_end - x_start + extra_length, 0) * np.maximum(\n            y_end - y_start + extra_length, 0\n        )\n        if mode == \"iou\":\n            union = area1[i] + area2 - overlap\n        else:\n            union = area1[i] if not exchange else area2\n        union = np.maximum(union, eps)\n        ious[i, :] = overlap / union\n    if exchange:\n        ious = ious.T\n    return ious", "\n\ndef distance2bbox(points, distance, max_shape=None):\n    \"\"\"Decode distance prediction to bounding box.\n    Args:\n        points (Tensor): Shape (B, N, 2) or (N, 2).\n        distance (Tensor): Distance from the given point to 4\n            boundaries (left, top, right, bottom). Shape (B, N, 4) or (N, 4)\n        max_shape (Sequence[int] or torch.Tensor or Sequence[\n            Sequence[int]],optional): Maximum bounds for boxes, specifies\n            (H, W, C) or (H, W). If priors shape is (B, N, 4), then\n            the max_shape should be a Sequence[Sequence[int]]\n            and the length of max_shape should also be B.\n    Returns:\n        Tensor: Boxes with shape (N, 4) or (B, N, 4)\n    \"\"\"\n\n    x1 = points[..., 0] - distance[..., 0]\n    y1 = points[..., 1] - distance[..., 1]\n    x2 = points[..., 0] + distance[..., 2]\n    y2 = points[..., 1] + distance[..., 3]\n\n    bboxes = torch.stack([x1, y1, x2, y2], -1)\n\n    if max_shape is not None:\n        if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():\n            # speed up\n            bboxes[:, 0::2].clamp_(min=0, max=max_shape[1])\n            bboxes[:, 1::2].clamp_(min=0, max=max_shape[0])\n            return bboxes\n\n        # clip bboxes with dynamic `min` and `max` for onnx\n        if torch.onnx.is_in_onnx_export():\n            raise NotImplementedError(\"Removed since unneeded\")\n        if not isinstance(max_shape, torch.Tensor):\n            max_shape = x1.new_tensor(max_shape)\n        max_shape = max_shape[..., :2].type_as(x1)\n        if max_shape.ndim == 2:\n            assert bboxes.ndim == 3\n            assert max_shape.size(0) == bboxes.size(0)\n\n        min_xy = x1.new_tensor(0)\n        max_xy = torch.cat([max_shape, max_shape], dim=-1).flip(-1).unsqueeze(-2)\n        bboxes = torch.where(bboxes < min_xy, min_xy, bboxes)\n        bboxes = torch.where(bboxes > max_xy, max_xy, bboxes)\n\n    return bboxes", "\n\ndef bbox2distance(points, bbox, max_dis=None, eps=0.1):\n    \"\"\"Decode bounding box based on distances.\n    Args:\n        points (Tensor): Shape (n, 2), [x, y].\n        bbox (Tensor): Shape (n, 4), \"xyxy\" format\n        max_dis (float): Upper bound of the distance.\n        eps (float): a small value to ensure target < max_dis, instead <=\n    Returns:\n        Tensor: Decoded distances.\n    \"\"\"\n    left = points[:, 0] - bbox[:, 0]\n    top = points[:, 1] - bbox[:, 1]\n    right = bbox[:, 2] - points[:, 0]\n    bottom = bbox[:, 3] - points[:, 1]\n    if max_dis is not None:\n        left = left.clamp(min=0, max=max_dis - eps)\n        top = top.clamp(min=0, max=max_dis - eps)\n        right = right.clamp(min=0, max=max_dis - eps)\n        bottom = bottom.clamp(min=0, max=max_dis - eps)\n    return torch.stack([left, top, right, bottom], -1)", "\n\ndef batched_nms(\n    boxes: torch.Tensor,\n    scores: torch.Tensor,\n    idxs: torch.Tensor,\n    nms_cfg: Optional[Dict],\n    class_agnostic: bool = False,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Performs non-maximum suppression in a batched fashion.\n    Modified from `torchvision/ops/boxes.py#L39\n    <https://github.com/pytorch/vision/blob/\n    505cd6957711af790211896d32b40291bea1bc21/torchvision/ops/boxes.py#L39>`_.\n    In order to perform NMS independently per class, we add an offset to all\n    the boxes. The offset is dependent only on the class idx, and is large\n    enough so that boxes from different classes do not overlap.\n    Note:\n        In v1.4.1 and later, ``batched_nms`` supports skipping the NMS and\n        returns sorted raw results when `nms_cfg` is None.\n    Args:\n        boxes (torch.Tensor): boxes in shape (N, 4) or (N, 5).\n        scores (torch.Tensor): scores in shape (N, ).\n        idxs (torch.Tensor): each index value correspond to a bbox cluster,\n            and NMS will not be applied between elements of different idxs,\n            shape (N, ).\n        nms_cfg (dict | optional): Supports skipping the nms when `nms_cfg`\n            is None, otherwise it should specify nms type and other\n            parameters like `iou_thr`. Possible keys includes the following.\n            - iou_threshold (float): IoU threshold used for NMS.\n            - split_thr (float): threshold number of boxes. In some cases the\n              number of boxes is large (e.g., 200k). To avoid OOM during\n              training, the users could set `split_thr` to a small value.\n              If the number of boxes is greater than the threshold, it will\n              perform NMS on each group of boxes separately and sequentially.\n              Defaults to 10000.\n        class_agnostic (bool): if true, nms is class agnostic,\n            i.e. IoU thresholding happens over all boxes,\n            regardless of the predicted class. Defaults to False.\n    Returns:\n        tuple: kept dets and indice.\n        - boxes (Tensor): Bboxes with score after nms, has shape\n          (num_bboxes, 5). last dimension 5 arrange as\n          (x1, y1, x2, y2, score)\n        - keep (Tensor): The indices of remaining boxes in input\n          boxes.\n    \"\"\"\n    # skip nms when nms_cfg is None\n    if nms_cfg is None:\n        scores, inds = scores.sort(descending=True)\n        boxes = boxes[inds]\n        return torch.cat([boxes, scores[:, None]], -1), inds\n\n    nms_cfg_ = nms_cfg.copy()\n    class_agnostic = nms_cfg_.pop(\"class_agnostic\", class_agnostic)\n    if class_agnostic:\n        boxes_for_nms = boxes\n    else:\n        # When using rotated boxes, only apply offsets on center.\n        if boxes.size(-1) == 5:\n            # Strictly, the maximum coordinates of the rotating box\n            # (x,y,w,h,a) should be calculated by polygon coordinates.\n            # But the conversion from rotated box to polygon will\n            # slow down the speed.\n            # So we use max(x,y) + max(w,h) as max coordinate\n            # which is larger than polygon max coordinate\n            # max(x1, y1, x2, y2,x3, y3, x4, y4)\n            max_coordinate = boxes[..., :2].max() + boxes[..., 2:4].max()\n            offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(1).to(boxes))\n            boxes_ctr_for_nms = boxes[..., :2] + offsets[:, None]\n            boxes_for_nms = torch.cat([boxes_ctr_for_nms, boxes[..., 2:5]], dim=-1)\n        else:\n            max_coordinate = boxes.max()\n            offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(1).to(boxes))\n            boxes_for_nms = boxes + offsets[:, None]\n\n    nms_op = torchvision.ops.nms\n\n    split_thr = nms_cfg_.pop(\"split_thr\", 10000)\n    # Won't split to multiple nms nodes when exporting to onnx\n    if boxes_for_nms.shape[0] < split_thr or torch.onnx.is_in_onnx_export():\n        keep = nms_op(boxes_for_nms, scores, iou_threshold=nms_cfg_[\"iou_threshold\"])\n        boxes = boxes[keep]\n        scores = scores[keep]\n\n    else:\n        max_num = nms_cfg_.pop(\"max_num\", -1)\n        total_mask = scores.new_zeros(scores.size(), dtype=torch.bool)\n        # Some type of nms would reweight the score, such as SoftNMS\n        scores_after_nms = scores.new_zeros(scores.size())\n        for id in torch.unique(idxs):\n            mask = (idxs == id).nonzero(as_tuple=False).view(-1)\n            keep = nms_op(\n                boxes_for_nms[mask],\n                scores[mask],\n                iou_threshold=nms_cfg_[\"iou_threshold\"],\n            )\n            total_mask[mask[keep]] = True\n            scores_after_nms[mask[keep]] = scores[mask[keep]]\n        keep = total_mask.nonzero(as_tuple=False).view(-1)\n\n        scores, inds = scores_after_nms[keep].sort(descending=True)\n        keep = keep[inds]\n        boxes = boxes[keep]\n\n        if max_num > 0:\n            keep = keep[:max_num]\n            boxes = boxes[:max_num]\n            scores = scores[:max_num]\n\n    boxes = torch.cat([boxes, scores[:, None]], -1)\n    return boxes, keep", "\n\ndef multiclass_nms(\n    multi_bboxes,\n    multi_scores,\n    score_thr,\n    nms_cfg,\n    max_num=-1,\n    score_factors=None,\n    return_inds=False,\n):\n    \"\"\"NMS for multi-class bboxes.\n    Args:\n        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)\n        multi_scores (Tensor): shape (n, #class), where the last column\n            contains scores of the background class, but this will be ignored.\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_cfg (dict): a dict that contains the arguments of nms operations\n        max_num (int, optional): if there are more than max_num bboxes after\n            NMS, only top max_num will be kept. Default to -1.\n        score_factors (Tensor, optional): The factors multiplied to scores\n            before applying NMS. Default to None.\n        return_inds (bool, optional): Whether return the indices of kept\n            bboxes. Default to False.\n    Returns:\n        tuple: (dets, labels, indices (optional)), tensors of shape (k, 5),\n            (k), and (k). Dets are boxes with scores. Labels are 0-based.\n    \"\"\"\n    num_classes = multi_scores.size(1) - 1\n    # exclude background category\n    if multi_bboxes.shape[1] > 4:\n        bboxes = multi_bboxes.view(multi_scores.size(0), -1, 4)\n    else:\n        bboxes = multi_bboxes[:, None].expand(multi_scores.size(0), num_classes, 4)\n\n    scores = multi_scores[:, :-1]\n\n    labels = torch.arange(num_classes, dtype=torch.long, device=scores.device)\n    labels = labels.view(1, -1).expand_as(scores)\n\n    bboxes = bboxes.reshape(-1, 4)\n    scores = scores.reshape(-1)\n    labels = labels.reshape(-1)\n\n    if not torch.onnx.is_in_onnx_export():\n        # NonZero not supported  in TensorRT\n        # remove low scoring boxes\n        valid_mask = scores > score_thr\n    # multiply score_factor after threshold to preserve more bboxes, improve\n    # mAP by 1% for YOLOv3\n    if score_factors is not None:\n        # expand the shape to match original shape of score\n        score_factors = score_factors.view(-1, 1).expand(\n            multi_scores.size(0), num_classes\n        )\n        score_factors = score_factors.reshape(-1)\n        scores = scores * score_factors\n\n    if not torch.onnx.is_in_onnx_export():\n        # NonZero not supported  in TensorRT\n        inds = valid_mask.nonzero(as_tuple=False).squeeze(1)\n        bboxes, scores, labels = bboxes[inds], scores[inds], labels[inds]\n    else:\n        # TensorRT NMS plugin has invalid output filled with -1\n        # add dummy data to make detection output correct.\n        bboxes = torch.cat([bboxes, bboxes.new_zeros(1, 4)], dim=0)\n        scores = torch.cat([scores, scores.new_zeros(1)], dim=0)\n        labels = torch.cat([labels, labels.new_zeros(1)], dim=0)\n\n    if bboxes.numel() == 0:\n        if torch.onnx.is_in_onnx_export():\n            raise RuntimeError(\n                \"[ONNX Error] Can not record NMS \"\n                \"as it has not been executed this time\"\n            )\n        dets = torch.cat([bboxes, scores[:, None]], -1)\n        if return_inds:\n            return dets, labels, inds\n        else:\n            return dets, labels\n\n    dets, keep = batched_nms(bboxes, scores, labels, nms_cfg)\n\n    if max_num > 0:\n        dets = dets[:max_num]\n        keep = keep[:max_num]\n\n    if return_inds:\n        return dets, labels[keep], inds[keep]\n    else:\n        return dets, labels[keep]", "\n\ndef images_to_levels(target, num_levels):\n    \"\"\"Convert targets by image to targets by feature level.\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    \"\"\"\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_levels:\n        end = start + n\n        # level_targets.append(target[:, start:end].squeeze(0))\n        level_targets.append(target[:, start:end])\n        start = end\n    return level_targets", "\n\ndef unmap(data, count, inds, fill=0):\n    \"\"\"Unmap a subset of item (data) back to the original set of items (of size\n    count)\"\"\"\n    if data.dim() == 1:\n        ret = data.new_full((count,), fill)\n        ret[inds.type(torch.bool)] = data\n    else:\n        new_size = (count,) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds.type(torch.bool), :] = data\n    return ret", "\n\ndef anchor_inside_flags(flat_anchors, valid_flags, img_shape, allowed_border=0):\n    \"\"\"Check whether the anchors are inside the border.\n    Args:\n        flat_anchors (torch.Tensor): Flatten anchors, shape (n, 4).\n        valid_flags (torch.Tensor): An existing valid flags of anchors.\n        img_shape (tuple(int)): Shape of current image.\n        allowed_border (int, optional): The border to allow the valid anchor.\n            Defaults to 0.\n    Returns:\n        torch.Tensor: Flags indicating whether the anchors are inside a \\\n            valid range.\n    \"\"\"\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = (\n            valid_flags\n            & (flat_anchors[:, 0] >= -allowed_border)\n            & (flat_anchors[:, 1] >= -allowed_border)\n            & (flat_anchors[:, 2] < img_w + allowed_border)\n            & (flat_anchors[:, 3] < img_h + allowed_border)\n        )\n    else:\n        inside_flags = valid_flags\n    return inside_flags", "\n\ndef multi_apply(func, *args, **kwargs):\n    \"\"\"Apply function to a list of arguments.\n    Note:\n        This function applies the ``func`` to multiple inputs and\n        map the multiple outputs of the ``func`` into different\n        list. Each list contains the same type of outputs corresponding\n        to different inputs.\n    Args:\n        func (Function): A function that will be applied to a list of\n            arguments\n    Returns:\n        tuple(list): A tuple containing multiple list, each list contains \\\n            a kind of returned results by the function\n    \"\"\"\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))", "\n\ndef reduce_loss(loss, reduction):\n    \"\"\"Reduce loss as specified.\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n    Return:\n        Tensor: Reduced loss tensor.\n    \"\"\"\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, elementwise_mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.mean()\n    elif reduction_enum == 2:\n        return loss.sum()", "\n\ndef weight_reduce_loss(loss, weight=None, reduction=\"mean\", avg_factor=None):\n    \"\"\"Apply element-wise weight and reduce loss.\n    Args:\n        loss (Tensor): Element-wise loss.\n        weight (Tensor): Element-wise weights.\n        reduction (str): Same as built-in losses of PyTorch.\n        avg_factor (float): Avarage factor when computing the mean of losses.\n    Returns:\n        Tensor: Processed loss values.\n    \"\"\"\n    # if weight is specified, apply element-wise weight\n    if weight is not None:\n        loss = loss * weight\n\n    # if avg_factor is not specified, just reduce the loss\n    if avg_factor is None:\n        loss = reduce_loss(loss, reduction)\n    else:\n        # if reduction is mean, then average the loss by avg_factor\n        if reduction == \"mean\":\n            loss = loss.sum() / avg_factor\n        # if reduction is 'none', then do nothing, otherwise raise an error\n        elif reduction != \"none\":\n            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n    return loss", "\n\ndef weighted_loss(loss_func):\n    \"\"\"Create a weighted version of a given loss function.\n    To use this decorator, the loss function must have the signature like\n    `loss_func(pred, target, **kwargs)`. The function only needs to compute\n    element-wise loss without any reduction. This decorator will add weight\n    and reduction arguments to the function. The decorated function will have\n    the signature like `loss_func(pred, target, weight=None, reduction='mean',\n    avg_factor=None, **kwargs)`.\n    :Example:\n    >>> import torch\n    >>> @weighted_loss\n    >>> def l1_loss(pred, target):\n    >>>     return (pred - target).abs()\n    >>> pred = torch.Tensor([0, 2, 3])\n    >>> target = torch.Tensor([1, 1, 1])\n    >>> weight = torch.Tensor([1, 0, 1])\n    >>> l1_loss(pred, target)\n    tensor(1.3333)\n    >>> l1_loss(pred, target, weight)\n    tensor(1.)\n    >>> l1_loss(pred, target, reduction='none')\n    tensor([1., 1., 2.])\n    >>> l1_loss(pred, target, weight, avg_factor=2)\n    tensor(1.5000)\n    \"\"\"\n\n    @functools.wraps(loss_func)\n    def wrapper(pred, target, weight=None, reduction=\"mean\", avg_factor=None, **kwargs):\n        # get element-wise loss\n        loss = loss_func(pred, target, **kwargs)\n        # print('LLL', pred.shape, target.shape, loss.shape, weight.shape)\n        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n        return loss\n\n    return wrapper", "\n\ndef bbox2delta(proposals, gt, means=(0.0, 0.0, 0.0, 0.0), stds=(1.0, 1.0, 1.0, 1.0)):\n    \"\"\"Compute deltas of proposals w.r.t. gt.\n    We usually compute the deltas of x, y, w, h of proposals w.r.t ground\n    truth bboxes to get regression target.\n    This is the inverse function of :func:`delta2bbox`.\n    Args:\n        proposals (Tensor): Boxes to be transformed, shape (N, ..., 4)\n        gt (Tensor): Gt bboxes to be used as base, shape (N, ..., 4)\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n    Returns:\n        Tensor: deltas with shape (N, 4), where columns represent dx, dy,\n            dw, dh.\n    \"\"\"\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0]\n    ph = proposals[..., 3] - proposals[..., 1]\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0]\n    gh = gt[..., 3] - gt[..., 1]\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas", "\n\ndef delta2bbox(\n    rois,\n    deltas,\n    means=(0.0, 0.0, 0.0, 0.0),\n    stds=(1.0, 1.0, 1.0, 1.0),\n    max_shape=None,\n    wh_ratio_clip=16 / 1000,\n    clip_border=True,\n    add_ctr_clamp=False,\n    ctr_clamp=32,\n):\n    \"\"\"Apply deltas to shift/scale base boxes.\n    Typically the rois are anchor or proposed bounding boxes and the deltas are\n    network outputs used to shift/scale those boxes.\n    This is the inverse function of :func:`bbox2delta`.\n    Args:\n        rois (Tensor): Boxes to be transformed. Has shape (N, 4).\n        deltas (Tensor): Encoded offsets relative to each roi.\n            Has shape (N, num_classes * 4) or (N, 4). Note\n            N = num_base_anchors * W * H, when rois is a grid of\n            anchors. Offset encoding follows [1]_.\n        means (Sequence[float]): Denormalizing means for delta coordinates.\n            Default (0., 0., 0., 0.).\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates. Default (1., 1., 1., 1.).\n        max_shape (tuple[int, int]): Maximum bounds for boxes, specifies\n           (H, W). Default None.\n        wh_ratio_clip (float): Maximum aspect ratio for boxes. Default\n            16 / 1000.\n        clip_border (bool, optional): Whether clip the objects outside the\n            border of the image. Default True.\n        add_ctr_clamp (bool): Whether to add center clamp. When set to True,\n            the center of the prediction bounding box will be clamped to\n            avoid being too far away from the center of the anchor.\n            Only used by YOLOF. Default False.\n        ctr_clamp (int): the maximum pixel shift to clamp. Only used by YOLOF.\n            Default 32.\n    Returns:\n        Tensor: Boxes with shape (N, num_classes * 4) or (N, 4), where 4\n           represent tl_x, tl_y, br_x, br_y.\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n    Example:\n        >>> rois = torch.Tensor([[ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 5.,  5.,  5.,  5.]])\n        >>> deltas = torch.Tensor([[  0.,   0.,   0.,   0.],\n        >>>                        [  1.,   1.,   1.,   1.],\n        >>>                        [  0.,   0.,   2.,  -1.],\n        >>>                        [ 0.7, -1.9, -0.5,  0.3]])\n        >>> delta2bbox(rois, deltas, max_shape=(32, 32, 3))\n        tensor([[0.0000, 0.0000, 1.0000, 1.0000],\n                [0.1409, 0.1409, 2.8591, 2.8591],\n                [0.0000, 0.3161, 4.1945, 0.6839],\n                [5.0000, 5.0000, 5.0000, 5.0000]])\n    \"\"\"\n    num_bboxes, num_classes = deltas.size(0), deltas.size(1) // 4\n    if num_bboxes == 0:\n        return deltas\n\n    deltas = deltas.reshape(-1, 4)\n\n    means = deltas.new_tensor(means).view(1, -1)\n    stds = deltas.new_tensor(stds).view(1, -1)\n    denorm_deltas = deltas * stds + means\n\n    dxy = denorm_deltas[:, :2]\n    dwh = denorm_deltas[:, 2:]\n\n    # Compute width/height of each roi\n    rois_ = rois.repeat(1, num_classes).reshape(-1, 4)\n    pxy = (rois_[:, :2] + rois_[:, 2:]) * 0.5\n    pwh = rois_[:, 2:] - rois_[:, :2]\n\n    dxy_wh = pwh * dxy\n\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    if add_ctr_clamp:\n        dxy_wh = torch.clamp(dxy_wh, max=ctr_clamp, min=-ctr_clamp)\n        dwh = torch.clamp(dwh, max=max_ratio)\n    else:\n        dwh = dwh.clamp(min=-max_ratio, max=max_ratio)\n\n    gxy = pxy + dxy_wh\n    gwh = pwh * dwh.exp()\n    x1y1 = gxy - (gwh * 0.5)\n    x2y2 = gxy + (gwh * 0.5)\n    bboxes = torch.cat([x1y1, x2y2], dim=-1)\n    if clip_border and max_shape is not None:\n        bboxes[..., 0::2].clamp_(min=0, max=max_shape[1])\n        bboxes[..., 1::2].clamp_(min=0, max=max_shape[0])\n    bboxes = bboxes.reshape(num_bboxes, -1)\n    return bboxes", "\n\n@weighted_loss\ndef quality_focal_loss(pred, target, beta=2.0):\n    r\"\"\"Quality Focal Loss (QFL) is from `Generalized Focal Loss: Learning\n    Qualified and Distributed Bounding Boxes for Dense Object Detection\n    <https://arxiv.org/abs/2006.04388>`_.\n    Args:\n        pred (torch.Tensor): Predicted joint representation of classification\n            and quality (IoU) estimation with shape (N, C), C is the number of\n            classes.\n        target (tuple([torch.Tensor])): Target category label with shape (N,)\n            and target quality label with shape (N,).\n        beta (float): The beta parameter for calculating the modulating factor.\n            Defaults to 2.0.\n    Returns:\n        torch.Tensor: Loss tensor with shape (N,).\n    \"\"\"\n    assert (\n        len(target) == 2\n    ), \"\"\"target for QFL must be a tuple of two elements,\n        including category label and quality label, respectively\"\"\"\n    # label denotes the category id, score denotes the quality score\n    label, score = target\n\n    # negatives are supervised by 0 quality score\n    pred_sigmoid = pred.sigmoid()\n    scale_factor = pred_sigmoid\n    zerolabel = scale_factor.new_zeros(pred.shape)\n    loss = F.binary_cross_entropy_with_logits(\n        pred, zerolabel, reduction=\"none\"\n    ) * scale_factor.pow(beta)\n\n    # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n    bg_class_ind = pred.size(1)\n    pos = ((label >= 0) & (label < bg_class_ind)).nonzero().squeeze(1)\n    pos_label = label[pos].long()\n    # positives are supervised by bbox quality (IoU) score\n    scale_factor = score[pos] - pred_sigmoid[pos, pos_label]\n    loss[pos, pos_label] = F.binary_cross_entropy_with_logits(\n        pred[pos, pos_label], score[pos], reduction=\"none\"\n    ) * scale_factor.abs().pow(beta)\n\n    loss = loss.sum(dim=1, keepdim=False)\n    return loss", "\n\nclass QualityFocalLoss(torch.nn.Module):\n    r\"\"\"Quality Focal Loss (QFL) is a variant of `Generalized Focal Loss:\n    Learning Qualified and Distributed Bounding Boxes for Dense Object\n    Detection <https://arxiv.org/abs/2006.04388>`_.\n    Args:\n        use_sigmoid (bool): Whether sigmoid operation is conducted in QFL.\n            Defaults to True.\n        beta (float): The beta parameter for calculating the modulating factor.\n            Defaults to 2.0.\n        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n        loss_weight (float): Loss weight of current loss.\n    \"\"\"\n\n    def __init__(self, use_sigmoid=True, beta=2.0, reduction=\"mean\", loss_weight=1.0):\n        super(QualityFocalLoss, self).__init__()\n        assert use_sigmoid is True, \"Only sigmoid in QFL supported now.\"\n        self.use_sigmoid = use_sigmoid\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(\n        self, pred, target, weight=None, avg_factor=None, reduction_override=None\n    ):\n        \"\"\"Forward function.\n        Args:\n            pred (torch.Tensor): Predicted joint representation of\n                classification and quality (IoU) estimation with shape (N, C),\n                C is the number of classes.\n            target (tuple([torch.Tensor])): Target category label with shape\n                (N,) and target quality label with shape (N,).\n            weight (torch.Tensor, optional): The weight of loss for each\n                prediction. Defaults to None.\n            avg_factor (int, optional): Average factor that is used to average\n                the loss. Defaults to None.\n            reduction_override (str, optional): The reduction method used to\n                override the original reduction method of the loss.\n                Defaults to None.\n        \"\"\"\n        assert reduction_override in (None, \"none\", \"mean\", \"sum\")\n        reduction = reduction_override if reduction_override else self.reduction\n        if self.use_sigmoid:\n            loss_cls = self.loss_weight * quality_focal_loss(\n                pred,\n                target,\n                weight,\n                beta=self.beta,\n                reduction=reduction,\n                avg_factor=avg_factor,\n            )\n        else:\n            raise NotImplementedError\n        return loss_cls", "\n\n@weighted_loss\ndef diou_loss(pred, target, eps=1e-7):\n    r\"\"\"`Implementation of Distance-IoU Loss: Faster and Better\n    Learning for Bounding Box Regression, https://arxiv.org/abs/1911.08287`_.\n    Code is modified from https://github.com/Zzh-tju/DIoU.\n    Args:\n        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),\n            shape (n, 4).\n        target (Tensor): Corresponding gt bboxes, shape (n, 4).\n        eps (float): Eps to avoid log(0).\n    Return:\n        Tensor: Loss tensor.\n    \"\"\"\n    # overlap\n    lt = torch.max(pred[:, :2], target[:, :2])\n    rb = torch.min(pred[:, 2:], target[:, 2:])\n    wh = (rb - lt).clamp(min=0)\n    overlap = wh[:, 0] * wh[:, 1]\n\n    # union\n    ap = (pred[:, 2] - pred[:, 0]) * (pred[:, 3] - pred[:, 1])\n    ag = (target[:, 2] - target[:, 0]) * (target[:, 3] - target[:, 1])\n    union = ap + ag - overlap + eps\n\n    # IoU\n    ious = overlap / union\n\n    # enclose area\n    enclose_x1y1 = torch.min(pred[:, :2], target[:, :2])\n    enclose_x2y2 = torch.max(pred[:, 2:], target[:, 2:])\n    enclose_wh = (enclose_x2y2 - enclose_x1y1).clamp(min=0)\n\n    cw = enclose_wh[:, 0]\n    ch = enclose_wh[:, 1]\n\n    c2 = cw**2 + ch**2 + eps\n\n    b1_x1, b1_y1 = pred[:, 0], pred[:, 1]\n    b1_x2, b1_y2 = pred[:, 2], pred[:, 3]\n    b2_x1, b2_y1 = target[:, 0], target[:, 1]\n    b2_x2, b2_y2 = target[:, 2], target[:, 3]\n\n    left = ((b2_x1 + b2_x2) - (b1_x1 + b1_x2)) ** 2 / 4\n    right = ((b2_y1 + b2_y2) - (b1_y1 + b1_y2)) ** 2 / 4\n    rho2 = left + right\n\n    # DIoU\n    dious = ious - rho2 / c2\n    loss = 1 - dious\n    return loss", "\n\nclass DIoULoss(torch.nn.Module):\n    def __init__(self, eps=1e-6, reduction=\"mean\", loss_weight=1.0):\n        super(DIoULoss, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(\n        self,\n        pred,\n        target,\n        weight=None,\n        avg_factor=None,\n        reduction_override=None,\n        **kwargs\n    ):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \"none\", \"mean\", \"sum\")\n        reduction = reduction_override if reduction_override else self.reduction\n        if weight is not None and weight.dim() > 1:\n            # TODO: remove this in the future\n            # reduce the weight of shape (n, 4) to (n,) to match the\n            # giou_loss of shape (n,)\n            assert weight.shape == pred.shape\n            weight = weight.mean(-1)\n        loss = self.loss_weight * diou_loss(\n            pred,\n            target,\n            weight,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs\n        )\n        return loss", ""]}
{"filename": "dataset2metadata/face_detection/neck.py", "chunked_list": ["# contributed by George Smyrnis\n\nfrom typing import Union, Tuple, Dict, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport warnings\n", "import warnings\n\n\nclass ConvModule(nn.Module):\n    \"\"\"A conv block that bundles conv/norm/activation layers.\n    This block simplifies the usage of convolution layers, which are commonly\n    used with a norm layer (e.g., BatchNorm) and activation layer (e.g., ReLU).\n    It is based upon three build methods: `build_conv_layer()`,\n    `build_norm_layer()` and `build_activation_layer()`.\n    Besides, we add some additional features in this module.\n    1. Automatically set `bias` of the conv layer.\n    2. Spectral norm is supported.\n    3. More padding modes are supported. Before PyTorch 1.5, nn.Conv2d only\n    supports zero and circular padding, and we add \"reflect\" padding mode.\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n            Same as that in ``nn._ConvNd``.\n        out_channels (int): Number of channels produced by the convolution.\n            Same as that in ``nn._ConvNd``.\n        kernel_size (int | tuple[int]): Size of the convolving kernel.\n            Same as that in ``nn._ConvNd``.\n        stride (int | tuple[int]): Stride of the convolution.\n            Same as that in ``nn._ConvNd``.\n        padding (int | tuple[int]): Zero-padding added to both sides of\n            the input. Same as that in ``nn._ConvNd``.\n        dilation (int | tuple[int]): Spacing between kernel elements.\n            Same as that in ``nn._ConvNd``.\n        groups (int): Number of blocked connections from input channels to\n            output channels. Same as that in ``nn._ConvNd``.\n        bias (bool | str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if `norm_cfg` is None, otherwise\n            False. Default: \"auto\".\n        conv_cfg (dict): Config dict for convolution layer. Default: None,\n            which means using conv2d.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (dict): Config dict for activation layer.\n            Default: dict(type='ReLU').\n        inplace (bool): Whether to use inplace mode for activation.\n            Default: True.\n        with_spectral_norm (bool): Whether use spectral norm in conv module.\n            Default: False.\n        padding_mode (str): If the `padding_mode` has not been supported by\n            current `Conv2d` in PyTorch, we will use our own padding layer\n            instead. Currently, we support ['zeros', 'circular'] with official\n            implementation and ['reflect'] with our own implementation.\n            Default: 'zeros'.\n        order (tuple[str]): The order of conv/norm/activation layers. It is a\n            sequence of \"conv\", \"norm\" and \"act\". Common examples are\n            (\"conv\", \"norm\", \"act\") and (\"act\", \"conv\", \"norm\").\n            Default: ('conv', 'norm', 'act').\n    \"\"\"\n\n    _abbr_ = \"conv_block\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, Tuple[int, int]],\n        stride: Union[int, Tuple[int, int]] = 1,\n        padding: Union[int, Tuple[int, int]] = 0,\n        dilation: Union[int, Tuple[int, int]] = 1,\n        groups: int = 1,\n        bias: Union[bool, str] = \"auto\",\n        conv_cfg: Optional[Dict] = None,\n        norm_cfg: Optional[Dict] = None,\n        act_cfg: Optional[Dict] = dict(type=\"ReLU\"),\n        inplace: bool = True,\n        with_spectral_norm: bool = False,\n        order: tuple = (\"conv\", \"norm\", \"act\"),\n    ):\n        super().__init__()\n        assert conv_cfg is None or isinstance(conv_cfg, dict)\n        assert norm_cfg is None or isinstance(norm_cfg, dict)\n        assert act_cfg is None or isinstance(act_cfg, dict)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.act_cfg = act_cfg\n        self.inplace = inplace\n        self.with_spectral_norm = with_spectral_norm\n        self.order = order\n        assert isinstance(self.order, tuple) and len(self.order) == 3\n        assert set(order) == {\"conv\", \"norm\", \"act\"}\n\n        self.with_norm = norm_cfg is not None\n        self.with_activation = act_cfg is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == \"auto\":\n            bias = not self.with_norm\n        self.with_bias = bias\n\n        # reset padding to 0 for conv module\n        conv_padding = padding\n        # build convolution layer\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=conv_padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n        )\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        if self.with_spectral_norm:\n            self.conv = nn.utils.spectral_norm(self.conv)\n\n        # build normalization layers\n        if self.with_norm:\n            # norm layer is after conv layer\n            if order.index(\"norm\") > order.index(\"conv\"):\n                norm_channels = out_channels\n            else:\n                norm_channels = in_channels\n            if norm_cfg[\"type\"] == \"BN\":\n                self.bn = nn.BatchNorm2d(norm_channels)\n                self.norm_name = \"bn\"\n            elif norm_cfg[\"type\"] == \"GN\":\n                self.gn = nn.GroupNorm(\n                    num_groups=norm_cfg[\"num_groups\"], num_channels=norm_channels\n                )\n                self.norm_name = \"gn\"\n        else:\n            self.norm_name = None  # type: ignore\n\n        # build activation layer\n        if self.with_activation:\n            self.activate = nn.ReLU()\n\n    @property\n    def norm(self):\n        if self.norm_name:\n            return getattr(self, self.norm_name)\n        else:\n            return None\n\n    def forward(\n        self, x: torch.Tensor, activate: bool = True, norm: bool = True\n    ) -> torch.Tensor:\n        for layer in self.order:\n            if layer == \"conv\":\n                x = self.conv(x)\n            elif layer == \"norm\" and norm and self.with_norm:\n                x = self.norm(x)\n            elif layer == \"act\" and activate and self.with_activation:\n                x = self.activate(x)\n        return x", "\n\nclass FPN(nn.Module):\n    r\"\"\"Feature Pyramid Network.\n    This is an implementation of paper `Feature Pyramid Networks for Object\n    Detection <https://arxiv.org/abs/1612.03144>`_.\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool | str): If bool, it decides whether to add conv\n            layers on top of the original feature maps. Default to False.\n            If True, its actual mode is specified by `extra_convs_on_inputs`.\n            If str, it specifies the source feature map of the extra convs.\n            Only the following options are allowed\n            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n            - 'on_lateral':  Last feature map after lateral convs.\n            - 'on_output': The last output feature map after fpn convs.\n        extra_convs_on_inputs (bool, deprecated): Whether to apply extra convs\n            on the original feature from the backbone. If True,\n            it is equivalent to `add_extra_convs='on_input'`. If False, it is\n            equivalent to set `add_extra_convs='on_output'`. Default to True.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n        upsample_cfg (dict): Config dict for interpolate layer.\n            Default: `dict(mode='nearest')`\n    Example:\n        >>> import torch\n        >>> in_channels = [2, 3, 5, 7]\n        >>> scales = [340, 170, 84, 43]\n        >>> inputs = [torch.rand(1, c, s, s)\n        ...           for c, s in zip(in_channels, scales)]\n        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n        >>> outputs = self.forward(inputs)\n        >>> for i in range(len(outputs)):\n        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n        outputs[0].shape = torch.Size([1, 11, 340, 340])\n        outputs[1].shape = torch.Size([1, 11, 170, 170])\n        outputs[2].shape = torch.Size([1, 11, 84, 84])\n        outputs[3].shape = torch.Size([1, 11, 43, 43])\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_outs,\n        start_level=0,\n        end_level=-1,\n        add_extra_convs=False,\n        extra_convs_on_inputs=True,\n        relu_before_extra_convs=False,\n        no_norm_on_lateral=False,\n        conv_cfg=None,\n        norm_cfg=None,\n        act_cfg=None,\n        upsample_cfg=dict(mode=\"nearest\"),\n    ):\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n        self.upsample_cfg = upsample_cfg.copy()\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        assert isinstance(add_extra_convs, (str, bool))\n        if isinstance(add_extra_convs, str):\n            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n            assert add_extra_convs in (\"on_input\", \"on_lateral\", \"on_output\")\n        elif add_extra_convs:  # True\n            if extra_convs_on_inputs:\n                # TODO: deprecate `extra_convs_on_inputs`\n                warnings.simplefilter(\"once\")\n                warnings.warn(\n                    '\"extra_convs_on_inputs\" will be deprecated in v2.9.0,'\n                    'Please use \"add_extra_convs\"',\n                    DeprecationWarning,\n                )\n                self.add_extra_convs = \"on_input\"\n            else:\n                self.add_extra_convs = \"on_output\"\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if self.add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.add_extra_convs == \"on_input\":\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                )\n                self.fpn_convs.append(extra_fpn_conv)\n\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n            #  it cannot co-exist with `size` in `F.interpolate`.\n            if \"scale_factor\" in self.upsample_cfg:\n                laterals[i - 1] += F.interpolate(laterals[i], **self.upsample_cfg)\n            else:\n                prev_shape = laterals[i - 1].shape[2:]\n                laterals[i - 1] += F.interpolate(\n                    laterals[i], size=prev_shape, **self.upsample_cfg\n                )\n\n        # build outputs\n        # part 1: from original levels\n        outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.add_extra_convs == \"on_input\":\n                    extra_source = inputs[self.backbone_end_level - 1]\n                elif self.add_extra_convs == \"on_lateral\":\n                    extra_source = laterals[-1]\n                elif self.add_extra_convs == \"on_output\":\n                    extra_source = outs[-1]\n                else:\n                    raise NotImplementedError\n                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)", "\n\nclass PAFPN(FPN):\n    \"\"\"Path Aggregation Network for Instance Segmentation.\n    This is an implementation of the `PAFPN in Path Aggregation Network\n    <https://arxiv.org/abs/1803.01534>`_.\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool): Whether to add conv layers on top of the\n            original feature maps. Default: False.\n        extra_convs_on_inputs (bool): Whether to apply extra conv on\n            the original feature from the backbone. Default: False.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_outs,\n        start_level=0,\n        end_level=-1,\n        add_extra_convs=False,\n        extra_convs_on_inputs=True,\n        relu_before_extra_convs=False,\n        no_norm_on_lateral=False,\n        conv_cfg=None,\n        norm_cfg=None,\n        act_cfg=None,\n    ):\n        super(PAFPN, self).__init__(\n            in_channels,\n            out_channels,\n            num_outs,\n            start_level,\n            end_level,\n            add_extra_convs,\n            extra_convs_on_inputs,\n            relu_before_extra_convs,\n            no_norm_on_lateral,\n            conv_cfg,\n            norm_cfg,\n            act_cfg,\n        )\n        # add extra bottom up pathway\n        self.downsample_convs = nn.ModuleList()\n        self.pafpn_convs = nn.ModuleList()\n        for i in range(self.start_level + 1, self.backbone_end_level):\n            d_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                stride=2,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n            pafpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n            self.downsample_convs.append(d_conv)\n            self.pafpn_convs.append(pafpn_conv)\n\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] += F.interpolate(\n                laterals[i], size=prev_shape, mode=\"nearest\"\n            )\n\n        # build outputs\n        # part 1: from original levels\n        inter_outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n\n        # part 2: add bottom-up path\n        for i in range(0, used_backbone_levels - 1):\n            inter_outs[i + 1] += self.downsample_convs[i](inter_outs[i])\n\n        outs = []\n        outs.append(inter_outs[0])\n        outs.extend(\n            [\n                self.pafpn_convs[i - 1](inter_outs[i])\n                for i in range(1, used_backbone_levels)\n            ]\n        )\n\n        # part 3: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.add_extra_convs == \"on_input\":\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[used_backbone_levels](orig))\n                elif self.add_extra_convs == \"on_lateral\":\n                    outs.append(self.fpn_convs[used_backbone_levels](laterals[-1]))\n                elif self.add_extra_convs == \"on_output\":\n                    outs.append(self.fpn_convs[used_backbone_levels](outs[-1]))\n                else:\n                    raise NotImplementedError\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)", ""]}
{"filename": "examples/slurm/prepare_jobs.py", "chunked_list": ["import argparse\nimport os\nfrom pathlib import Path\n\nimport fsspec\nimport yaml\nfrom tqdm import tqdm\nfrom dataset2metadata.utils import download_all\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--yml_template\", type=str, required=True, help=\"path to a templete yml\"\n    )\n\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        required=True,\n        help=\"path to a cache dir to save jobs that will be distributed across slurm cluster\",\n    )\n\n    parser.add_argument(\n        \"--shard_dir\",\n        type=str,\n        required=True,\n        help=\"creates jobs out of all shards in this dir\",\n    )\n\n    parser.add_argument(\n        \"--num_tars_per_wds\",\n        type=int,\n        required=False,\n        default=160,\n        help=\"how many tars to group together for 1 job\",\n    )\n\n    parser.add_argument(\n        \"--subdirs\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"parse subdirs within the shard_dir for parquets\",\n    )\n\n    args = parser.parse_args()\n\n    yml_template = yaml.safe_load(Path(args.yml_template).read_text())\n    jobs_dir_path = args.cache_dir\n\n    if not os.path.exists(jobs_dir_path):\n        os.mkdir(jobs_dir_path)\n\n    shards = None\n\n    fs, output_path = fsspec.core.url_to_fs(args.shard_dir)\n\n    if args.subdirs:\n        all_dirs = [d for d in fs.ls(output_path) if \".\" not in d.split(\"/\")[-1]]\n        shards = []\n        for dir in tqdm(all_dirs):\n            shard_part = fs.glob(os.path.join(dir, \"*.tar\"))\n            shards.extend(shard_part)\n        shards = sorted(shards)\n    else:\n        shards = sorted(fs.glob(os.path.join(output_path, \"*.tar\")))\n\n    if args.shard_dir.startswith(\"s3\"):\n        shards = [f\"pipe:aws s3 cp s3://{s} -\" for s in shards]\n\n    groups = [\n        shards[i : i + args.num_tars_per_wds]\n        for i in range(0, len(shards), args.num_tars_per_wds)\n    ]\n    print(f\"num shards: {len(shards)}\")\n    print(f\"num jobs: {len(groups)}\")\n\n    for i, g in enumerate(groups):\n        yml_template[\"input_tars\"] = g\n\n        with open(os.path.join(jobs_dir_path, f\"{i}.yml\"), \"w\") as f:\n            for k in yml_template:\n                f.write(f\"{k}:\")\n                if not isinstance(yml_template[k], list):\n                    f.write(f\" {yml_template[k]}\\n\")\n                else:\n                    f.write(\"\\n\")\n                    for v in yml_template[k]:\n                        f.write(f'  - \"{v}\"\\n')\n\n    print(f\"Saved {len(groups)} jobs to {jobs_dir_path}\")\n\n    download_all()\n    print(\"Downloaded all default dataset2metadata checkpoints\")\n\n    print(\"Done.\")", "\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--yml_template\", type=str, required=True, help=\"path to a templete yml\"\n    )\n\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        required=True,\n        help=\"path to a cache dir to save jobs that will be distributed across slurm cluster\",\n    )\n\n    parser.add_argument(\n        \"--shard_dir\",\n        type=str,\n        required=True,\n        help=\"creates jobs out of all shards in this dir\",\n    )\n\n    parser.add_argument(\n        \"--num_tars_per_wds\",\n        type=int,\n        required=False,\n        default=160,\n        help=\"how many tars to group together for 1 job\",\n    )\n\n    parser.add_argument(\n        \"--subdirs\",\n        action=\"store_true\",\n        required=False,\n        default=False,\n        help=\"parse subdirs within the shard_dir for parquets\",\n    )\n\n    args = parser.parse_args()\n\n    yml_template = yaml.safe_load(Path(args.yml_template).read_text())\n    jobs_dir_path = args.cache_dir\n\n    if not os.path.exists(jobs_dir_path):\n        os.mkdir(jobs_dir_path)\n\n    shards = None\n\n    fs, output_path = fsspec.core.url_to_fs(args.shard_dir)\n\n    if args.subdirs:\n        all_dirs = [d for d in fs.ls(output_path) if \".\" not in d.split(\"/\")[-1]]\n        shards = []\n        for dir in tqdm(all_dirs):\n            shard_part = fs.glob(os.path.join(dir, \"*.tar\"))\n            shards.extend(shard_part)\n        shards = sorted(shards)\n    else:\n        shards = sorted(fs.glob(os.path.join(output_path, \"*.tar\")))\n\n    if args.shard_dir.startswith(\"s3\"):\n        shards = [f\"pipe:aws s3 cp s3://{s} -\" for s in shards]\n\n    groups = [\n        shards[i : i + args.num_tars_per_wds]\n        for i in range(0, len(shards), args.num_tars_per_wds)\n    ]\n    print(f\"num shards: {len(shards)}\")\n    print(f\"num jobs: {len(groups)}\")\n\n    for i, g in enumerate(groups):\n        yml_template[\"input_tars\"] = g\n\n        with open(os.path.join(jobs_dir_path, f\"{i}.yml\"), \"w\") as f:\n            for k in yml_template:\n                f.write(f\"{k}:\")\n                if not isinstance(yml_template[k], list):\n                    f.write(f\" {yml_template[k]}\\n\")\n                else:\n                    f.write(\"\\n\")\n                    for v in yml_template[k]:\n                        f.write(f'  - \"{v}\"\\n')\n\n    print(f\"Saved {len(groups)} jobs to {jobs_dir_path}\")\n\n    download_all()\n    print(\"Downloaded all default dataset2metadata checkpoints\")\n\n    print(\"Done.\")", ""]}
