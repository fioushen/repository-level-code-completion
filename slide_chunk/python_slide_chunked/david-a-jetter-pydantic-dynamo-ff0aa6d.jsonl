{"filename": "dodo.py", "chunked_list": ["from doit.action import CmdAction\n\n\ndef python_version():\n    return \"python --version\"\n\n\ndef poetry_version():\n    return \"poetry --version\"\n", "\n\ndef poetry_env_info():\n    return \"poetry env info\"\n\n\ninfo = [CmdAction(python_version), CmdAction(poetry_version), CmdAction(poetry_env_info)]\n\n\ndef install_deps():\n    return \"poetry install\"", "\ndef install_deps():\n    return \"poetry install\"\n\n\ndef task_black():\n    return {\n        \"actions\": [\n            *info,\n            CmdAction(install_deps),\n            CmdAction(\"poetry run black pydantic_dynamo\"),\n            CmdAction(\"poetry run black tests\"),\n        ]\n    }", "\n\ndef task_test():\n    return {\n        \"actions\": [\n            *info,\n            CmdAction(install_deps),\n            CmdAction(\n                \"poetry run python -m pytest tests/test_unit \"\n                \"--cov=pydantic_dynamo --cov-report xml:unit-coverage.xml\"\n            ),\n            CmdAction(\"poetry run black --check pydantic_dynamo\"),\n            CmdAction(\"poetry run black --check tests\"),\n            CmdAction(\"poetry run mypy pydantic_dynamo tests\"),\n            CmdAction(\"poetry run flake8 pydantic_dynamo --ignore=E203,W503\"),\n            CmdAction(\"poetry run flake8 tests --ignore=E203,W503\"),\n            CmdAction('poetry run python -m pytest tests/test_integration -k \"not long_running\"'),\n            CmdAction('poetry run python -m pytest tests/test_integration -k \"long_running\"'),\n        ],\n        \"verbosity\": 2,\n    }", "\n\ndef task_build():\n    return {\n        \"actions\": [CmdAction(\"poetry build\")],\n        \"task_dep\": [\"test\"],\n        \"verbosity\": 2,\n    }\n\n\ndef task_publish():\n    return {\n        \"actions\": [CmdAction(\"poetry publish\")],\n        \"task_dep\": [\"build\"],\n        \"verbosity\": 2,\n    }", "\n\ndef task_publish():\n    return {\n        \"actions\": [CmdAction(\"poetry publish\")],\n        \"task_dep\": [\"build\"],\n        \"verbosity\": 2,\n    }\n", ""]}
{"filename": "tests/models.py", "chunked_list": ["from datetime import date, time, datetime\nfrom enum import Enum\nfrom typing import Optional, Dict, Any, List, Set\n\nfrom pydantic import BaseModel\n\n\nclass ExtraModel(BaseModel):\n    class Config:\n        extra = \"allow\"", "\n\nclass FieldModel(BaseModel):\n    test_field: str\n    failures: Optional[int]\n\n\nclass ComposedFieldModel(BaseModel):\n    composed: FieldModel\n    test_field: str\n    failures: Optional[int]", "\n\nclass CountEnum(Enum):\n    One = \"one\"\n    Two = \"two\"\n    Three = \"three\"\n\n\nclass Example(BaseModel):\n    dict_field: Dict[str, Any]\n    model_field: FieldModel\n    list_field: List[Any]\n    set_field: Set[Any]\n    date_field: date\n    time_field: time\n    datetime_field: datetime\n    enum_field: CountEnum\n    int_field: int\n    optional_field: Optional[int]", "class Example(BaseModel):\n    dict_field: Dict[str, Any]\n    model_field: FieldModel\n    list_field: List[Any]\n    set_field: Set[Any]\n    date_field: date\n    time_field: time\n    datetime_field: datetime\n    enum_field: CountEnum\n    int_field: int\n    optional_field: Optional[int]", ""]}
{"filename": "tests/factories.py", "chunked_list": ["import random\nfrom datetime import date, time, timezone\nfrom typing import Iterable, TypeVar, Dict\n\nimport factory\nfrom boto3.dynamodb.conditions import Attr\n\nfrom pydantic_dynamo.models import UpdateCommand, PartitionedContent\nfrom faker import Faker\n", "from faker import Faker\n\nfrom pydantic_dynamo.utils import UpdateItemArguments\nfrom tests.models import Example, FieldModel, CountEnum\n\nfake = Faker()\n\nT = TypeVar(\"T\")\n\n\ndef random_element(items: Iterable[T]) -> T:\n    return random.choice([el for el in items])", "\n\ndef random_element(items: Iterable[T]) -> T:\n    return random.choice([el for el in items])\n\n\ndef boto_exception(code: str) -> Exception:\n    response = {\"Error\": {\"Code\": code}}\n    ex = Exception()\n    ex.response = response  # type: ignore[attr-defined]\n    return ex", "\n\nclass UpdateCommandFactory(factory.Factory):\n    class Meta:\n        model = UpdateCommand\n\n    set_commands = factory.Faker(\"pydict\")\n    increment_attrs = factory.LazyFunction(lambda: {fake.bs(): _ for _ in range(5)})\n\n\nclass UpdateItemArgumentsFactory(factory.Factory):\n    class Meta:\n        model = UpdateItemArguments\n\n    update_expression = factory.Faker(\"bs\")\n    condition_expression = factory.LazyFunction(lambda: Attr(fake.bothify()).eq(fake.bothify()))\n    attribute_names = factory.LazyFunction(\n        lambda: {fake.bothify(): fake.bothify() for _ in range(3)}\n    )\n    attribute_values = factory.LazyFunction(lambda: fake.pydict())", "\n\nclass UpdateItemArgumentsFactory(factory.Factory):\n    class Meta:\n        model = UpdateItemArguments\n\n    update_expression = factory.Faker(\"bs\")\n    condition_expression = factory.LazyFunction(lambda: Attr(fake.bothify()).eq(fake.bothify()))\n    attribute_names = factory.LazyFunction(\n        lambda: {fake.bothify(): fake.bothify() for _ in range(3)}\n    )\n    attribute_values = factory.LazyFunction(lambda: fake.pydict())", "\n\nclass FieldModelFactory(factory.Factory):\n    class Meta:\n        model = FieldModel\n\n    test_field = factory.Faker(\"bs\")\n    failures = factory.Faker(\"pyint\")\n\n\nclass ExampleFactory(factory.Factory):\n    class Meta:\n        model = Example\n\n    dict_field = factory.LazyFunction(lambda: {fake.bothify(): FieldModel(test_field=fake.bs())})\n    model_field = factory.SubFactory(FieldModelFactory)\n    list_field = factory.List((fake.bs() for _ in range(5)))\n    set_field = factory.LazyFunction(lambda: {fake.bs() for _ in range(5)})\n    date_field = factory.LazyFunction(lambda: date.fromisoformat(fake.date()))\n    time_field = factory.LazyFunction(lambda: time.fromisoformat(fake.time()))\n    datetime_field = factory.Faker(\"date_time\", tzinfo=timezone.utc)\n    enum_field = factory.LazyFunction(lambda: random_element(CountEnum))\n    int_field = factory.Faker(\"pyint\")", "\n\nclass ExampleFactory(factory.Factory):\n    class Meta:\n        model = Example\n\n    dict_field = factory.LazyFunction(lambda: {fake.bothify(): FieldModel(test_field=fake.bs())})\n    model_field = factory.SubFactory(FieldModelFactory)\n    list_field = factory.List((fake.bs() for _ in range(5)))\n    set_field = factory.LazyFunction(lambda: {fake.bs() for _ in range(5)})\n    date_field = factory.LazyFunction(lambda: date.fromisoformat(fake.date()))\n    time_field = factory.LazyFunction(lambda: time.fromisoformat(fake.time()))\n    datetime_field = factory.Faker(\"date_time\", tzinfo=timezone.utc)\n    enum_field = factory.LazyFunction(lambda: random_element(CountEnum))\n    int_field = factory.Faker(\"pyint\")", "\n\nclass ExamplePartitionedContentFactory(factory.Factory):\n    class Meta:\n        model = PartitionedContent[Example]\n\n    partition_ids = factory.List((fake.bothify() for _ in range(2)))\n    content_ids = factory.List((fake.bothify() for _ in range(2)))\n    item = factory.SubFactory(ExampleFactory)\n", "\n\ndef example_content_to_db_item(\n    partition_key: str,\n    partition_prefix: str,\n    partition_name: str,\n    sort_key: str,\n    content_type: str,\n    example: PartitionedContent[Example],\n) -> Dict:\n    db_item = {\n        partition_key: \"#\".join((partition_prefix, partition_name, *example.partition_ids)),\n        sort_key: \"#\".join((content_type, *example.content_ids)),\n        \"_object_version\": example.current_version,\n        **example.item.dict(),\n    }\n\n    if expiry := example.expiry:\n        db_item[\"_ttl\"] = int(expiry.timestamp())\n\n    return db_item", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_unit/test_repository.py", "chunked_list": ["import random\nfrom datetime import datetime, timezone\nfrom unittest.mock import patch, MagicMock\n\nfrom faker import Faker\n\nfrom pydantic_dynamo.models import PartitionedContent, UpdateCommand, FilterCommand\nfrom pydantic_dynamo.repository import (\n    DynamoRepository,\n)", "    DynamoRepository,\n)\nfrom pydantic_dynamo.utils import clean_dict\nfrom tests.models import ExtraModel, FieldModel, ComposedFieldModel, CountEnum\nfrom tests.factories import UpdateItemArgumentsFactory\n\nfake = Faker()\n\n\ndef _random_enum():\n    return random.choice([s for s in CountEnum])", "\ndef _random_enum():\n    return random.choice([s for s in CountEnum])\n\n\n@patch(\"pydantic_dynamo.repository.Session\")\ndef test_dynamo_repo_build(session_cls):\n    table = MagicMock()\n    session = MagicMock()\n    session_cls.return_value = session\n    resource = MagicMock()\n    session.resource.return_value = resource\n    resource.Table.return_value = table\n    table_name = fake.bs()\n    item_class = ExtraModel\n    partition = fake.bs()\n    partition_key = fake.bs()\n    content_type = fake.bs()\n\n    repo = DynamoRepository[ExtraModel].build(\n        table_name, item_class, partition, partition_key, content_type\n    )\n\n    assert repo._item_class == item_class\n    assert repo._partition_prefix == partition\n    assert repo._partition_name == partition_key\n    assert repo._content_type == content_type\n    assert repo._table == table\n\n    assert session.resource.call_args[0] == (\"dynamodb\",)\n    assert resource.Table.call_args[0] == (f\"{table_name}\",)", "\n\n@patch(\"pydantic_dynamo.repository.internal_timestamp\")\ndef test_dynamo_repo_put(internal_timestamp):\n    now = datetime.now(tz=timezone.utc)\n    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\n    partition = fake.bs()\n    content_type = fake.bs()\n    partition_ids = [fake.bs()]\n    partition_type = fake.bs()\n    content_ids = [fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_type,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n\n    item_dict = fake.pydict()\n    item = ExtraModel(**item_dict)\n    content = PartitionedContent(partition_ids=partition_ids, content_ids=content_ids, item=item)\n    repo.put(content)\n\n    assert table.put_item.call_args[1] == {\n        \"Item\": {\n            partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n            sort_key: f\"{content_type}#{content_ids[0]}\",\n            \"_object_version\": 1,\n            \"_timestamp\": now.isoformat(),\n            **clean_dict(item_dict),\n        }\n    }", "\n\n@patch(\"pydantic_dynamo.repository.internal_timestamp\")\ndef test_dynamo_repo_put_batch(internal_timestamp):\n    now = datetime.now(tz=timezone.utc)\n    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\n    partition = fake.bs()\n    content_type = fake.bs()\n    partition_ids = [fake.bs()]\n    partition_type = fake.bs()\n    content_ids = [fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    writer = MagicMock()\n    table.batch_writer.return_value.__enter__.return_value = writer\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_type,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n\n    item_dict1 = fake.pydict()\n    item_dict2 = fake.pydict()\n    contents = (\n        PartitionedContent(\n            partition_ids=partition_ids, content_ids=content_ids, item=ExtraModel(**item_dict1)\n        ),\n        PartitionedContent(\n            partition_ids=partition_ids, content_ids=content_ids, item=ExtraModel(**item_dict2)\n        ),\n    )\n    repo.put_batch(contents)\n\n    assert writer.put_item.call_args_list == [\n        (\n            (),\n            {\n                \"Item\": {\n                    partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n                    sort_key: f\"{content_type}#{content_ids[0]}\",\n                    \"_object_version\": 1,\n                    \"_timestamp\": now.isoformat(),\n                    **clean_dict(item_dict1),\n                }\n            },\n        ),\n        (\n            (),\n            {\n                \"Item\": {\n                    partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n                    sort_key: f\"{content_type}#{content_ids[0]}\",\n                    \"_object_version\": 1,\n                    \"_timestamp\": now.isoformat(),\n                    **clean_dict(item_dict2),\n                }\n            },\n        ),\n    ]", "\n\ndef test_dynamo_repo_get():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.get_item.return_value = {\"Item\": item_dict}\n\n    partition_id = [fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = repo.get(partition_id, content_id)\n\n    assert actual == ExtraModel(**item_dict)\n    assert table.get_item.call_args == (\n        (),\n        {\n            \"Key\": {\n                partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n            }\n        },\n    )", "\n\ndef test_dynamo_repo_get_batch():\n    partition = fake.bothify()\n    partition_name = fake.bothify()\n    content_type = fake.bothify()\n    table_name = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    resource = MagicMock()\n    item_1 = fake.pydict()\n    item_2 = fake.pydict()\n    item_3 = fake.pydict()\n\n    unprocessed = [{fake.bothify(): fake.bothify()}]\n\n    resource.batch_get_item.side_effect = [\n        {\"Responses\": {table_name: [item_1]}, \"UnprocessedKeys\": unprocessed},\n        {\"Responses\": {table_name: [item_2]}},\n        {\"Responses\": {table_name: [item_3]}},\n    ]\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=table_name,\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=MagicMock(),\n        resource=resource,\n    )\n\n    request_ids = [([fake.bothify()], [fake.bothify()]) for _ in range(120)]\n    items = repo.get_batch(request_ids)\n\n    assert items == [ExtraModel(**item_1), ExtraModel(**item_2), ExtraModel(**item_3)]\n    assert resource.batch_get_item.call_args_list == [\n        (\n            (),\n            {\n                \"RequestItems\": {\n                    table_name: {\n                        \"Keys\": [\n                            {\n                                partition_key: f\"{partition}#{partition_name}#{rid[0][0]}\",\n                                sort_key: f\"{content_type}#{rid[1][0]}\",\n                            }\n                            for rid in request_ids[:100]\n                        ]\n                    }\n                }\n            },\n        ),\n        ((), {\"RequestItems\": {table_name: {\"Keys\": unprocessed}}}),\n        (\n            (),\n            {\n                \"RequestItems\": {\n                    table_name: {\n                        \"Keys\": [\n                            {\n                                partition_key: f\"{partition}#{partition_name}#{rid[0][0]}\",\n                                sort_key: f\"{content_type}#{rid[1][0]}\",\n                            }\n                            for rid in request_ids[100:]\n                        ]\n                    }\n                }\n            },\n        ),\n    ]", "\n\ndef test_dynamo_repo_get_none_inputs():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.get_item.return_value = {\"Item\": item_dict}\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = repo.get(None, None)\n\n    assert actual == ExtraModel(**item_dict)\n    assert table.get_item.call_args == (\n        (),\n        {\n            \"Key\": {\n                partition_key: f\"{partition}#{partition_name}#\",\n                sort_key: f\"{content_type}#\",\n            }\n        },\n    )", "\n\ndef test_dynamo_repo_list():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = list(repo.list(partition_id, content_id, ascending, limit))\n\n    assert actual == [ExtraModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]\n        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"", "\n\ndef test_dynamo_repo_list_none_inputs():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = list(repo.list(None, None, ascending, limit))\n\n    assert actual == [ExtraModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"", "\n\ndef test_dynamo_repo_list_no_ids():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = list(repo.list([], [], ascending, limit))\n\n    assert actual == [ExtraModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"", "\n\ndef test_dynamo_repo_list_with_filter():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = {\"test_field\": fake.bs()}\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    partition_id = [fake.bs()]\n    content_id = [fake.bs()]\n\n    repo = DynamoRepository[FieldModel](\n        item_class=FieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    filters = FilterCommand(not_exists={\"failures\"})\n    actual = list(repo.list(partition_id, content_id, ascending, limit, filters))\n\n    assert actual == [FieldModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert \"Limit\" not in kwargs\n    filter_expression = kwargs[\"FilterExpression\"]\n    assert filter_expression.expression_operator == \"attribute_not_exists\"\n    assert filter_expression._values[0].name == \"failures\"\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#{partition_id[0]}\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}\"", "\n\ndef test_dynamo_repo_list_between():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_start = [fake.bs(), fake.bs()]\n    content_end = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = list(repo.list_between(partition_id, content_start, content_end))\n\n    assert actual == [ExtraModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]\n        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"BETWEEN\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert (\n        expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}#{content_start[1]}\"\n    )\n    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}#{content_end[1]}\"", "\n\ndef test_dynamo_repo_list_between_none_inputs():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = fake.pydict()\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = list(repo.list_between(None, None, None))\n\n    assert actual == [ExtraModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"", "\n\ndef test_dynamo_repo_list_between_with_filter():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    item_dict = {\"test_field\": fake.bs()}\n    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\n    partition_id = [fake.bs()]\n    content_start = [fake.bs()]\n    content_end = [fake.bs()]\n\n    repo = DynamoRepository[FieldModel](\n        item_class=FieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    filters = FilterCommand(not_exists={\"failures\"})\n    actual = list(\n        repo.list_between(partition_id, content_start, content_end, ascending, limit, filters)\n    )\n\n    assert actual == [FieldModel(**item_dict)]\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert \"Limit\" not in kwargs\n    filter_expression = kwargs[\"FilterExpression\"]\n    assert filter_expression.expression_operator == \"attribute_not_exists\"\n    assert filter_expression._values[0].name == \"failures\"\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#{partition_id[0]}\"\n    assert expression._values[1].expression_operator == \"BETWEEN\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}\"\n    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}\"", "\n\ndef test_content_get_repo_no_items():\n    table = MagicMock()\n    table.get_item.return_value = {\"Not_Items\": []}\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=ExtraModel,\n        partition_prefix=fake.bs(),\n        partition_name=fake.bs(),\n        content_type=fake.bs(),\n        table_name=fake.bs(),\n        partition_key=fake.bs(),\n        sort_key=fake.bs(),\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = repo.get(fake.bs(), fake.bs())\n\n    assert actual is None", "\n\n@patch(\"pydantic_dynamo.repository.build_update_args_for_command\")\ndef test_dynamo_repo_update(build_update_args):\n    update_args = UpdateItemArgumentsFactory()\n    build_update_args.return_value = update_args\n    partition = fake.bothify()\n    partition_name = fake.bothify()\n    content_type = fake.bothify()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    repo = DynamoRepository[ComposedFieldModel](\n        item_class=ComposedFieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify(), fake.bothify()]\n    current_version = fake.pyint()\n    command = UpdateCommand(\n        set_commands={\n            \"test_field\": fake.bs(),\n            \"composed\": {\"test_field\": fake.bs(), \"failures\": None},\n        },\n        increment_attrs={\"failures\": 1},\n        current_version=current_version,\n    )\n\n    repo.update(partition_id, content_id, command)\n\n    update_a, update_k = table.update_item.call_args\n\n    assert update_k.pop(\"Key\") == {\n        partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n        sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n    }\n\n    assert update_k.pop(\"ConditionExpression\") == update_args.condition_expression\n    assert update_k.pop(\"UpdateExpression\") == update_args.update_expression\n    assert update_k.pop(\"ExpressionAttributeNames\") == update_args.attribute_names\n    assert update_k.pop(\"ExpressionAttributeValues\") == update_args.attribute_values\n    assert len(update_k) == 0", "\n\ndef test_dynamo_repo_delete():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    items = [\n        ExtraModel(**{partition_key: fake.bs(), sort_key: fake.bs()}).dict() for _ in range(11)\n    ]\n    table.query.return_value = {\"Items\": items, \"Count\": fake.pyint()}\n    writer = MagicMock()\n    table.batch_writer.return_value.__enter__.return_value = writer\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=FieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    repo.delete(partition_id, content_id)\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]\n        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n    assert writer.delete_item.call_args_list == [\n        (\n            (),\n            {\n                \"Key\": {\n                    partition_key: item[partition_key],\n                    sort_key: item[sort_key],\n                }\n            },\n        )\n        for item in items\n    ]", "\n\ndef test_dynamo_repo_delete_none_inputs():\n    partition = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    items = [\n        ExtraModel(**{partition_key: fake.bs(), sort_key: fake.bs()}).dict() for _ in range(11)\n    ]\n    table.query.return_value = {\"Items\": items, \"Count\": fake.pyint()}\n    writer = MagicMock()\n    table.batch_writer.return_value.__enter__.return_value = writer\n\n    repo = DynamoRepository[ExtraModel](\n        item_class=FieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    repo.delete(None, None)\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"\n    assert writer.delete_item.call_args_list == [\n        (\n            (),\n            {\n                \"Key\": {\n                    partition_key: item[partition_key],\n                    sort_key: item[sort_key],\n                }\n            },\n        )\n        for item in items\n    ]", ""]}
{"filename": "tests/test_unit/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_unit/test_utils.py", "chunked_list": ["import random\nfrom copy import deepcopy\nfrom datetime import datetime, timezone, date, time\nfrom typing import Dict\nfrom unittest.mock import patch, MagicMock, AsyncMock\n\nimport pytest\nfrom faker import Faker\n\nfrom pydantic_dynamo.exceptions import RequestObjectStateError", "\nfrom pydantic_dynamo.exceptions import RequestObjectStateError\nfrom pydantic_dynamo.models import UpdateCommand, FilterCommand\nfrom pydantic_dynamo.utils import (\n    chunks,\n    utc_now,\n    get_error_code,\n    clean_dict,\n    build_update_args_for_command,\n    internal_timestamp,", "    build_update_args_for_command,\n    internal_timestamp,\n    execute_update_item,\n    validate_command_for_schema,\n    validate_filters_for_schema,\n)\nfrom tests.factories import (\n    UpdateCommandFactory,\n    UpdateItemArgumentsFactory,\n    ExampleFactory,", "    UpdateItemArgumentsFactory,\n    ExampleFactory,\n    FieldModelFactory,\n)\nfrom tests.models import ExtraModel, CountEnum, Example, FieldModel\n\nfake = Faker()\n\n\ndef _random_enum():\n    return random.choice([s for s in CountEnum])", "\ndef _random_enum():\n    return random.choice([s for s in CountEnum])\n\n\n@patch(\"pydantic_dynamo.utils.datetime\")\ndef test_utc_now(dt_mock):\n    now = datetime.now()\n    dt_mock.now.return_value = now\n    actual = utc_now()\n\n    assert dt_mock.now.call_args == ((), {\"tz\": timezone.utc})\n    assert actual == now", "\n\n@patch(\"pydantic_dynamo.utils.utc_now\")\ndef test_internal_timestamp(utc_now):\n    now = fake.date_time()\n    utc_now.return_value = now\n\n    actual = internal_timestamp()\n    assert actual == {\"_timestamp\": now.isoformat()}\n", "\n\ndef test_get_error_code():\n    ex = RuntimeError()\n    code = fake.bs()\n    ex.response = {\"Error\": {\"Code\": code}}\n\n    actual = get_error_code(ex)\n    assert actual == code\n", "\n\ndef test_get_error_code_no_code():\n    ex = RuntimeError()\n    code = fake.bs()\n    ex.response = {\"Error\": {\"Zode\": code}}\n\n    actual = get_error_code(ex)\n    assert actual is None\n", "\n\ndef test_get_error_code_no_error():\n    ex = RuntimeError()\n    code = fake.bs()\n    ex.response = {\"Zerror\": {\"Zode\": code}}\n\n    actual = get_error_code(ex)\n    assert actual is None\n", "\n\ndef test_get_error_code_no_response():\n    ex = RuntimeError()\n    code = fake.bs()\n    ex.zesponse = {\"Error\": {\"Code\": code}}\n\n    actual = get_error_code(ex)\n    assert actual is None\n", "\n\ndef test_chunks_lt_size():\n    items = [fake.bothify() for _ in range(2)]\n    chunked = list(chunks(items, 3))\n    assert len(chunked) == 1\n    assert len(chunked[0]) == 2\n\n\ndef test_chunks_gt_size():\n    items = [fake.bothify() for _ in range(2)]\n    chunked = list(chunks(items, 1))\n    assert len(chunked) == 2\n    for chunk in chunked:\n        assert len(chunk) == 1", "\ndef test_chunks_gt_size():\n    items = [fake.bothify() for _ in range(2)]\n    chunked = list(chunks(items, 1))\n    assert len(chunked) == 2\n    for chunk in chunked:\n        assert len(chunk) == 1\n\n\ndef test_chunks_gt_size_partial():\n    items = [fake.bothify() for _ in range(25)]\n    chunked = list(chunks(items, 10))\n    assert len(chunked) == 3\n    assert len(chunked[0]) == 10\n    assert len(chunked[1]) == 10\n    assert len(chunked[2]) == 5", "\ndef test_chunks_gt_size_partial():\n    items = [fake.bothify() for _ in range(25)]\n    chunked = list(chunks(items, 10))\n    assert len(chunked) == 3\n    assert len(chunked[0]) == 10\n    assert len(chunked[1]) == 10\n    assert len(chunked[2]) == 5\n\n", "\n\nasync def test_execute_update_item():\n    table = AsyncMock()\n    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n    args = UpdateItemArgumentsFactory()\n\n    await execute_update_item(table, key, args)\n\n    assert table.update_item.call_args == (", "\n    assert table.update_item.call_args == (\n        (),\n        {\n            \"Key\": key,\n            \"UpdateExpression\": args.update_expression,\n            \"ExpressionAttributeNames\": args.attribute_names,\n            \"ExpressionAttributeValues\": args.attribute_values,\n            \"ConditionExpression\": args.condition_expression,\n        },", "            \"ConditionExpression\": args.condition_expression,\n        },\n    )\n\n\nasync def test_execute_update_item_condition_none():\n    table = AsyncMock()\n    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n    args = UpdateItemArgumentsFactory(condition_expression=None)\n", "    args = UpdateItemArgumentsFactory(condition_expression=None)\n\n    await execute_update_item(table, key, args)\n\n    assert table.update_item.call_args == (\n        (),\n        {\n            \"Key\": key,\n            \"UpdateExpression\": args.update_expression,\n            \"ExpressionAttributeNames\": args.attribute_names,", "            \"UpdateExpression\": args.update_expression,\n            \"ExpressionAttributeNames\": args.attribute_names,\n            \"ExpressionAttributeValues\": args.attribute_values,\n        },\n    )\n\n\n@patch(\"pydantic_dynamo.utils.get_error_code\")\nasync def test_execute_update_conditional_check_failed(get_error_code):\n    error_code = \"ConditionalCheckFailedException\"", "async def test_execute_update_conditional_check_failed(get_error_code):\n    error_code = \"ConditionalCheckFailedException\"\n    get_error_code.return_value = error_code\n    table = MagicMock()\n    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n    args = UpdateItemArgumentsFactory()\n\n    db_ex = Exception(fake.bs())\n    table.update_item.side_effect = db_ex\n    with pytest.raises(RequestObjectStateError) as ex:\n        await execute_update_item(table, key, args)", "    table.update_item.side_effect = db_ex\n    with pytest.raises(RequestObjectStateError) as ex:\n        await execute_update_item(table, key, args)\n\n    assert str(key) in str(ex)\n    assert get_error_code.call_args == ((db_ex,), {})\n\n\n@patch(\"pydantic_dynamo.utils.get_error_code\")\nasync def test_execute_update_some_other_exception(get_error_code):", "@patch(\"pydantic_dynamo.utils.get_error_code\")\nasync def test_execute_update_some_other_exception(get_error_code):\n    error_code = fake.bs()\n    get_error_code.return_value = error_code\n    table = MagicMock()\n    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n    args = UpdateItemArgumentsFactory()\n\n    db_ex = Exception(fake.bs())\n    table.update_item.side_effect = db_ex\n    with pytest.raises(Exception) as ex:\n        await execute_update_item(table, key, args)", "    db_ex = Exception(fake.bs())\n    table.update_item.side_effect = db_ex\n    with pytest.raises(Exception) as ex:\n        await execute_update_item(table, key, args)\n\n    assert ex.value == db_ex\n\n\ndef test_clean_dict():\n    nested_composed = ExampleFactory()\n    composed = ExampleFactory()\n    dict_input = {\n        \"nested\": {\n            \"nested_dict\": fake.pydict(),\n            \"nested_enum\": _random_enum(),\n            \"nested_list_of_dict\": [fake.pydict() for _ in range(3)],\n            \"nested_datetime\": datetime.now(tz=timezone.utc),\n            \"nested_list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n            \"nested_list_of_date\": [date.today() for _ in range(3)],\n            \"nested_list_of_time\": [time() for _ in range(3)],\n            \"nested_composed\": nested_composed,\n        },\n        \"dict\": fake.pydict(),\n        \"enum\": _random_enum(),\n        \"list_of_dict\": [fake.pydict() for _ in range(3)],\n        \"datetime\": datetime.now(tz=timezone.utc),\n        \"list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n        \"list_of_date\": [date.today() for _ in range(3)],\n        \"list_of_time\": [time() for _ in range(3)],\n        \"composed\": composed,\n    }\n    og = deepcopy(dict_input)\n    cleaned = clean_dict(dict_input)\n    assert cleaned[\"nested\"][\"nested_dict\"].keys() == og[\"nested\"][\"nested_dict\"].keys()\n    assert cleaned[\"nested\"][\"nested_enum\"] == og[\"nested\"][\"nested_enum\"].value\n    assert len(cleaned[\"nested\"][\"nested_list_of_dict\"]) == len(og[\"nested\"][\"nested_list_of_dict\"])\n    assert cleaned[\"nested\"][\"nested_datetime\"] == og[\"nested\"][\"nested_datetime\"].isoformat()\n    assert cleaned[\"nested\"][\"nested_list_of_dt\"] == [\n        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_dt\"]\n    ]\n    assert cleaned[\"nested\"][\"nested_list_of_date\"] == [\n        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_date\"]\n    ]\n    assert cleaned[\"nested\"][\"nested_list_of_time\"] == [\n        t.isoformat() for t in og[\"nested\"][\"nested_list_of_time\"]\n    ]\n    nested_composed_expected = _example_to_expected(nested_composed)\n    assert sorted(cleaned[\"nested\"][\"nested_composed\"].pop(\"set_field\")) == sorted(\n        nested_composed_expected.pop(\"set_field\")\n    )\n    assert cleaned[\"nested\"][\"nested_composed\"] == nested_composed_expected\n    assert cleaned[\"dict\"].keys() == og[\"dict\"].keys()\n    assert cleaned[\"enum\"] == og[\"enum\"].value\n    assert len(cleaned[\"list_of_dict\"]) == len(og[\"list_of_dict\"])\n    assert cleaned[\"datetime\"] == og[\"datetime\"].isoformat()\n    assert cleaned[\"list_of_dt\"] == [dt.isoformat() for dt in og[\"list_of_dt\"]]\n    assert cleaned[\"list_of_date\"] == [dt.isoformat() for dt in og[\"list_of_date\"]]\n    assert cleaned[\"list_of_time\"] == [t.isoformat() for t in og[\"list_of_time\"]]\n\n    # Need to explicitly assert on set_field because it's not reliably sorted\n    expected = _example_to_expected(composed)\n    assert sorted(cleaned[\"composed\"].pop(\"set_field\")) == sorted(expected.pop(\"set_field\"))\n    assert cleaned[\"composed\"] == expected", "def test_clean_dict():\n    nested_composed = ExampleFactory()\n    composed = ExampleFactory()\n    dict_input = {\n        \"nested\": {\n            \"nested_dict\": fake.pydict(),\n            \"nested_enum\": _random_enum(),\n            \"nested_list_of_dict\": [fake.pydict() for _ in range(3)],\n            \"nested_datetime\": datetime.now(tz=timezone.utc),\n            \"nested_list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n            \"nested_list_of_date\": [date.today() for _ in range(3)],\n            \"nested_list_of_time\": [time() for _ in range(3)],\n            \"nested_composed\": nested_composed,\n        },\n        \"dict\": fake.pydict(),\n        \"enum\": _random_enum(),\n        \"list_of_dict\": [fake.pydict() for _ in range(3)],\n        \"datetime\": datetime.now(tz=timezone.utc),\n        \"list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n        \"list_of_date\": [date.today() for _ in range(3)],\n        \"list_of_time\": [time() for _ in range(3)],\n        \"composed\": composed,\n    }\n    og = deepcopy(dict_input)\n    cleaned = clean_dict(dict_input)\n    assert cleaned[\"nested\"][\"nested_dict\"].keys() == og[\"nested\"][\"nested_dict\"].keys()\n    assert cleaned[\"nested\"][\"nested_enum\"] == og[\"nested\"][\"nested_enum\"].value\n    assert len(cleaned[\"nested\"][\"nested_list_of_dict\"]) == len(og[\"nested\"][\"nested_list_of_dict\"])\n    assert cleaned[\"nested\"][\"nested_datetime\"] == og[\"nested\"][\"nested_datetime\"].isoformat()\n    assert cleaned[\"nested\"][\"nested_list_of_dt\"] == [\n        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_dt\"]\n    ]\n    assert cleaned[\"nested\"][\"nested_list_of_date\"] == [\n        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_date\"]\n    ]\n    assert cleaned[\"nested\"][\"nested_list_of_time\"] == [\n        t.isoformat() for t in og[\"nested\"][\"nested_list_of_time\"]\n    ]\n    nested_composed_expected = _example_to_expected(nested_composed)\n    assert sorted(cleaned[\"nested\"][\"nested_composed\"].pop(\"set_field\")) == sorted(\n        nested_composed_expected.pop(\"set_field\")\n    )\n    assert cleaned[\"nested\"][\"nested_composed\"] == nested_composed_expected\n    assert cleaned[\"dict\"].keys() == og[\"dict\"].keys()\n    assert cleaned[\"enum\"] == og[\"enum\"].value\n    assert len(cleaned[\"list_of_dict\"]) == len(og[\"list_of_dict\"])\n    assert cleaned[\"datetime\"] == og[\"datetime\"].isoformat()\n    assert cleaned[\"list_of_dt\"] == [dt.isoformat() for dt in og[\"list_of_dt\"]]\n    assert cleaned[\"list_of_date\"] == [dt.isoformat() for dt in og[\"list_of_date\"]]\n    assert cleaned[\"list_of_time\"] == [t.isoformat() for t in og[\"list_of_time\"]]\n\n    # Need to explicitly assert on set_field because it's not reliably sorted\n    expected = _example_to_expected(composed)\n    assert sorted(cleaned[\"composed\"].pop(\"set_field\")) == sorted(expected.pop(\"set_field\"))\n    assert cleaned[\"composed\"] == expected", "\n\n# dict_field only supports strings or instances of FieldModel\ndef _example_to_expected(example: Example) -> Dict:\n    return {\n        \"dict_field\": {\n            k: v.dict() if isinstance(v, FieldModel) else v for k, v in example.dict_field.items()\n        },\n        \"model_field\": {\n            \"test_field\": example.model_field.test_field,\n            \"failures\": example.model_field.failures,\n        },\n        \"list_field\": [v.isoformat() if isinstance(v, datetime) else v for v in example.list_field],\n        \"set_field\": [v.isoformat() if isinstance(v, datetime) else v for v in example.set_field],\n        \"date_field\": example.date_field.isoformat(),\n        \"time_field\": example.time_field.isoformat(),\n        \"datetime_field\": example.datetime_field.isoformat(),\n        \"enum_field\": example.enum_field.value,\n        \"int_field\": example.int_field,\n        \"optional_field\": example.optional_field,\n    }", "\n\ndef test_clean_dict_pydantic_base_model():\n    unclean = {\n        \"items\": [ExtraModel(a=\"a\", b=\"b\"), ExtraModel(c=ExtraModel(c=\"c\"), d=[ExtraModel(d=\"d\")])]\n    }\n    cleaned = clean_dict(unclean)\n    assert cleaned[\"items\"] == [{\"a\": \"a\", \"b\": \"b\"}, {\"c\": {\"c\": \"c\"}, \"d\": [{\"d\": \"d\"}]}]\n\n", "\n\n@patch(\"pydantic_dynamo.utils.utc_now\")\ndef test_build_update_item_arguments(utc_now):\n    now = datetime.now(tz=timezone.utc)\n    utc_now.return_value = now\n    some_string = fake.bs()\n    some_enum = _random_enum()\n    some_example = ExampleFactory()\n    incr_attr = fake.bs()\n    incr_attr2 = fake.bs()\n    incr = fake.pyint()\n    current_version = fake.pyint()\n    el1 = fake.bs()\n    el2 = fake.bs()\n    el3 = fake.bs()\n    expiry = datetime.now(tz=timezone.utc)\n    command = UpdateCommand(\n        current_version=current_version,\n        set_commands={\n            \"some_attr\": some_string,\n            \"some_enum\": some_enum,\n            \"some_object\": some_example.dict(),\n        },\n        increment_attrs={incr_attr: incr, incr_attr2: 1},\n        append_attrs={\"some_list\": [el1, el2], \"some_list_2\": [el3]},\n        expiry=expiry,\n    )\n    key = {\"partition_key\": fake.bs(), \"sort_key\": fake.bs()}\n\n    update_args = build_update_args_for_command(command, key=key)\n\n    actual_condition = update_args.condition_expression\n    assert actual_condition.expression_operator == \"AND\"\n    assert actual_condition._values[1]._values[0].name == \"_object_version\"\n    assert actual_condition._values[1]._values[1] == current_version\n    assert actual_condition._values[0]._values[1].expression_operator == \"=\"\n    assert actual_condition._values[0]._values[1]._values[0].name == \"sort_key\"\n    assert actual_condition._values[0]._values[1]._values[1] == key[\"sort_key\"]\n    assert actual_condition._values[0]._values[0].expression_operator == \"=\"\n    assert actual_condition._values[0]._values[0]._values[0].name == \"partition_key\"\n    assert actual_condition._values[0]._values[0]._values[1] == key[\"partition_key\"]\n\n    assert (\n        update_args.update_expression == \"SET #att0 = :val0, #att1 = :val1, \"\n        \"#att2.#att3 = :val2, #att2.#att4 = :val3, \"\n        \"#att2.#att5 = :val4, #att2.#att6 = :val5, \"\n        \"#att2.#att7 = :val6, #att2.#att8 = :val7, \"\n        \"#att2.#att9 = :val8, #att2.#att10 = :val9, \"\n        \"#att2.#att11 = :val10, #att2.#att12 = :val11, \"\n        \"#att13 = :val12, #att14 = :val13, \"\n        \"#att15 = if_not_exists(#att15, :zero) + :val14, \"\n        \"#att16 = if_not_exists(#att16, :zero) + :val15, \"\n        \"#att17 = if_not_exists(#att17, :zero) + :val16, \"\n        \"#att18 = list_append(if_not_exists(#att18, :empty_list), :val17), \"\n        \"#att19 = list_append(if_not_exists(#att19, :empty_list), :val18)\"\n    )\n    assert update_args.attribute_names == {\n        \"#att0\": \"some_attr\",\n        \"#att1\": \"some_enum\",\n        \"#att2\": \"some_object\",\n        \"#att3\": \"dict_field\",\n        \"#att4\": \"model_field\",\n        \"#att5\": \"list_field\",\n        \"#att6\": \"set_field\",\n        \"#att7\": \"date_field\",\n        \"#att8\": \"time_field\",\n        \"#att9\": \"datetime_field\",\n        \"#att10\": \"enum_field\",\n        \"#att11\": \"int_field\",\n        \"#att12\": \"optional_field\",\n        \"#att13\": \"_timestamp\",\n        \"#att14\": \"_ttl\",\n        \"#att15\": incr_attr,\n        \"#att16\": incr_attr2,\n        \"#att17\": \"_object_version\",\n        \"#att18\": \"some_list\",\n        \"#att19\": \"some_list_2\",\n    }\n    assert sorted(update_args.attribute_values.pop(\":val5\")) == sorted(list(some_example.set_field))\n    assert update_args.attribute_values == {\n        \":zero\": 0,\n        \":empty_list\": [],\n        \":val0\": some_string,\n        \":val1\": some_enum.value,\n        \":val2\": {k: v.dict() for k, v in some_example.dict_field.items()},\n        \":val3\": some_example.model_field.dict(),\n        \":val4\": some_example.list_field,\n        \":val6\": some_example.date_field.isoformat(),\n        \":val7\": some_example.time_field.isoformat(),\n        \":val8\": some_example.datetime_field.isoformat(),\n        \":val9\": some_example.enum_field.value,\n        \":val10\": some_example.int_field,\n        \":val11\": None,\n        \":val12\": now.isoformat(),\n        \":val13\": int(expiry.timestamp()),\n        \":val14\": incr,\n        \":val15\": 1,\n        \":val16\": 1,\n        \":val17\": [el1, el2],\n        \":val18\": [el3],\n    }", "\n\ndef test_build_update_item_arguments_null_version():\n    command = UpdateCommandFactory(current_version=None)\n\n    update_args = build_update_args_for_command(command)\n\n    assert update_args.condition_expression is None\n\n", "\n\n# TODO: Fix this flaky test\n# def test_build_filter_expression():\n#     filters = FilterCommand(\n#         not_exists={\"ne_a\", \"ne_b\"},\n#         equals={\"eq_a\": True, \"eq_b\": 1},\n#         not_equals={\"dne_a\": \"dne_b\", \"dne_c\": False},\n#     )\n#     expression = _build_filter_expression(filters)", "#     )\n#     expression = _build_filter_expression(filters)\n#     assert expression == (\n#         Attr(\"ne_a\").not_exists()\n#         & Attr(\"ne_b\").not_exists()\n#         & Attr(\"eq_a\").eq(True)\n#         & Attr(\"eq_b\").eq(1)\n#         & Attr(\"dne_a\").ne(\"dne_b\")\n#         & Attr(\"dne_c\").ne(False)\n#     )", "#         & Attr(\"dne_c\").ne(False)\n#     )\n\n\ndef test_validate_command_for_schema_happy_path():\n    validate_command_for_schema(\n        Example.schema(),\n        UpdateCommand(\n            set_commands={\n                \"dict_field\": fake.pydict(),\n                \"model_field\": FieldModelFactory().dict(),\n                \"set_field\": set((fake.bothify() for _ in range(2))),\n            },\n            increment_attrs={\"int_field\": fake.pyint()},\n            append_attrs={\"list_field\": [fake.bs()]},\n        ),\n    )", "\n\ndef test_validate_command_for_schema_invalid_set_command():\n    command = UpdateCommandFactory()\n\n    with pytest.raises(ValueError) as ex:\n        validate_command_for_schema(FieldModel.schema(), command)\n\n    exception_value = str(ex.value)\n    assert \"FieldModel\" in exception_value\n    for attr in command.set_commands.keys():\n        assert attr in exception_value", "\n\ndef test_validate_command_for_schema_non_dict_field_to_dict():\n    command = UpdateCommand(set_commands={\"date_field\": {fake.bs(): fake.bs()}})\n\n    with pytest.raises(ValueError) as ex:\n        validate_command_for_schema(Example.schema(), command)\n\n    exception_value = str(ex.value)\n    assert \"Example\" in exception_value\n    assert \"date_field\" in exception_value", "\n\ndef test_validate_command_for_schema_invalid_nested_model_field():\n    bs_key = fake.bs()\n    command = UpdateCommand(set_commands={\"model_field\": {bs_key: fake.bs()}})\n\n    with pytest.raises(ValueError) as ex:\n        validate_command_for_schema(Example.schema(), command)\n\n    exception_value = str(ex.value)\n    assert \"Example\" in exception_value\n    assert bs_key in exception_value", "\n\ndef test_validate_command_for_schema_invalid_incr_command():\n    command = UpdateCommandFactory(set_commands={})\n\n    with pytest.raises(ValueError) as ex:\n        validate_command_for_schema(FieldModel.schema(), command)\n\n    exception_value = str(ex.value)\n    assert \"FieldModel\" in exception_value\n    for attr in command.increment_attrs:\n        assert attr in exception_value", "\n\ndef test_validate_command_for_schema_non_integer_incr_command():\n    command = UpdateCommand(increment_attrs={\"test_field\": 1})\n\n    with pytest.raises(ValueError) as ex:\n        validate_command_for_schema(FieldModel.schema(), command)\n\n    exception_value = str(ex.value)\n    assert \"FieldModel\" in exception_value\n    for attr in command.increment_attrs:\n        assert attr in exception_value", "\n\ndef test_validate_filters_for_schema_invalid_filter():\n    filters = FilterCommand(not_exists={fake.bs()})\n\n    with pytest.raises(ValueError) as ex:\n        validate_filters_for_schema(FieldModel.schema(), filters)\n\n    exception_value = str(ex.value)\n    assert \"FieldModel\" in exception_value\n    for attr in filters.not_exists:\n        assert attr in exception_value", ""]}
{"filename": "tests/test_unit/test_v2/test_repository.py", "chunked_list": ["import asyncio\nimport random\nfrom datetime import datetime, timezone\nfrom unittest.mock import patch, MagicMock, AsyncMock\n\nfrom faker import Faker\n\nfrom pydantic_dynamo.models import PartitionedContent, UpdateCommand, FilterCommand\nfrom pydantic_dynamo.v2.repository import (\n    DynamoRepository,", "from pydantic_dynamo.v2.repository import (\n    DynamoRepository,\n)\nfrom pydantic_dynamo.utils import clean_dict\nfrom tests.models import FieldModel, ComposedFieldModel, Example\nfrom tests.factories import (\n    UpdateItemArgumentsFactory,\n    ExamplePartitionedContentFactory,\n    example_content_to_db_item,\n)", "    example_content_to_db_item,\n)\nfrom pydantic_dynamo.v2.models import GetResponse\n\nfake = Faker()\n\n\nasync def test_context_manager():\n    partition = fake.bs()\n    content_type = fake.bs()", "    partition = fake.bs()\n    content_type = fake.bs()\n    partition_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    async with DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition,\n        partition_name=partition_type,", "        partition_prefix=partition,\n        partition_name=partition_type,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    ) as repo:\n        assert isinstance(repo, DynamoRepository)", "    ) as repo:\n        assert isinstance(repo, DynamoRepository)\n\n\n@patch(\"pydantic_dynamo.v2.repository.internal_timestamp\")\nasync def test_dynamo_repo_put(internal_timestamp):\n    now = datetime.now(tz=timezone.utc)\n    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\n    partition = fake.bs()", "\n    partition = fake.bs()\n    content_type = fake.bs()\n    partition_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n\n    repo = DynamoRepository[Example](\n        item_class=Example,", "    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition,\n        partition_name=partition_type,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),", "        table=table,\n        resource=MagicMock(),\n    )\n\n    expiry = fake.date_time()\n    content = ExamplePartitionedContentFactory(expiry=expiry)\n    await repo.put(content)\n\n    assert table.put_item.call_args[1] == {\n        \"Item\": {", "    assert table.put_item.call_args[1] == {\n        \"Item\": {\n            partition_key: f\"{partition}#{partition_type}#\" + \"#\".join(content.partition_ids),\n            sort_key: f\"{content_type}#\" + \"#\".join(content.content_ids),\n            \"_object_version\": 1,\n            \"_timestamp\": now.isoformat(),\n            \"_ttl\": int(expiry.timestamp()),\n            **clean_dict(content.item.dict()),\n        }\n    }", "        }\n    }\n\n\n@patch(\"pydantic_dynamo.v2.repository.internal_timestamp\")\nasync def test_dynamo_repo_put_batch(internal_timestamp):\n    now = datetime.now(tz=timezone.utc)\n    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\n    partition = fake.bs()", "\n    partition = fake.bs()\n    content_type = fake.bs()\n    partition_ids = [fake.bs()]\n    partition_type = fake.bs()\n    content_ids = [fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    writer = AsyncMock()", "    table = MagicMock()\n    writer = AsyncMock()\n    table.batch_writer.return_value.__aenter__.return_value = writer\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition,\n        partition_name=partition_type,\n        content_type=content_type,\n        table_name=fake.bs(),", "        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    expiry = fake.date_time()\n    contents = [\n        ExamplePartitionedContentFactory(", "    contents = [\n        ExamplePartitionedContentFactory(\n            partition_ids=partition_ids, content_ids=content_ids, expiry=expiry\n        ),\n        ExamplePartitionedContentFactory(\n            partition_ids=partition_ids, content_ids=content_ids, expiry=expiry\n        ),\n    ]\n    await repo.put_batch(contents)\n", "    await repo.put_batch(contents)\n\n    assert writer.put_item.call_args_list == [\n        (\n            (),\n            {\n                \"Item\": {\n                    partition_key: f\"{partition}#{partition_type}#\"\n                    + \"#\".join(contents[0].partition_ids),\n                    sort_key: f\"{content_type}#\" + \"#\".join(contents[0].content_ids),", "                    + \"#\".join(contents[0].partition_ids),\n                    sort_key: f\"{content_type}#\" + \"#\".join(contents[0].content_ids),\n                    \"_object_version\": 1,\n                    \"_timestamp\": now.isoformat(),\n                    \"_ttl\": int(expiry.timestamp()),\n                    **clean_dict(contents[0].item.dict()),\n                }\n            },\n        ),\n        (", "        ),\n        (\n            (),\n            {\n                \"Item\": {\n                    partition_key: f\"{partition}#{partition_type}#\"\n                    + \"#\".join(contents[1].partition_ids),\n                    sort_key: f\"{content_type}#\" + \"#\".join(contents[1].content_ids),\n                    \"_object_version\": 1,\n                    \"_timestamp\": now.isoformat(),", "                    \"_object_version\": 1,\n                    \"_timestamp\": now.isoformat(),\n                    \"_ttl\": int(expiry.timestamp()),\n                    **clean_dict(contents[1].item.dict()),\n                }\n            },\n        ),\n    ]\n\n", "\n\nasync def test_dynamo_repo_get():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    table = AsyncMock()\n\n    partition_id = [fake.bs()]\n    content_id = [fake.bs(), fake.bs()]", "    partition_id = [fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    content = ExamplePartitionedContentFactory()\n    table.get_item.return_value = {\n        \"Item\": example_content_to_db_item(\n            partition_key, partition_prefix, partition_name, sort_key, content_type, content\n        )\n    }", "        )\n    }\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,", "        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n        consistent_reads=True,\n    )\n    actual = await repo.get(partition_id, content_id)\n\n    assert actual == GetResponse(content=content)", "\n    assert actual == GetResponse(content=content)\n    assert table.get_item.call_args == (\n        (),\n        {\n            \"Key\": {\n                partition_key: f\"{partition_prefix}#{partition_name}#{partition_id[0]}\",\n                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n            },\n            \"ConsistentRead\": True,", "            },\n            \"ConsistentRead\": True,\n        },\n    )\n\n\nasync def test_dynamo_repo_get_no_object_version():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()", "    partition_name = fake.bs()\n    content_type = fake.bs()\n    table = AsyncMock()\n\n    partition_id = [fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    content = ExamplePartitionedContentFactory(item__object_version=1)\n    response_item = example_content_to_db_item(", "    content = ExamplePartitionedContentFactory(item__object_version=1)\n    response_item = example_content_to_db_item(\n        partition_key, partition_prefix, partition_name, sort_key, content_type, content\n    )\n    response_item.pop(\"_object_version\")\n    table.get_item.return_value = {\"Item\": response_item}\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,", "        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n        consistent_reads=True,", "        resource=MagicMock(),\n        consistent_reads=True,\n    )\n    actual = await repo.get(partition_id, content_id)\n\n    assert actual == GetResponse(content=content)\n    assert table.get_item.call_args == (\n        (),\n        {\n            \"Key\": {", "        {\n            \"Key\": {\n                partition_key: f\"{partition_prefix}#{partition_name}#{partition_id[0]}\",\n                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n            },\n            \"ConsistentRead\": True,\n        },\n    )\n\n", "\n\nasync def test_dynamo_repo_get_none_inputs():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    content = ExamplePartitionedContentFactory()\n    table = AsyncMock()", "    content = ExamplePartitionedContentFactory()\n    table = AsyncMock()\n    table.get_item.return_value = {\n        \"Item\": example_content_to_db_item(\n            partition_key, partition_prefix, partition_name, sort_key, content_type, content\n        )\n    }\n\n    repo = DynamoRepository[Example](\n        item_class=Example,", "    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),", "        table=table,\n        resource=MagicMock(),\n    )\n    actual = await repo.get(None, None)\n\n    assert actual == GetResponse(content=content)\n    assert table.get_item.call_args == (\n        (),\n        {\n            \"Key\": {", "        {\n            \"Key\": {\n                partition_key: f\"{partition_prefix}#{partition_name}#\",\n                sort_key: f\"{content_type}#\",\n            },\n            \"ConsistentRead\": False,\n        },\n    )\n\n", "\n\nasync def test_content_get_repo_no_items():\n    table = AsyncMock()\n    table.get_item.return_value = {\"Not_Items\": []}\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=fake.bs(),\n        partition_name=fake.bs(),", "        partition_prefix=fake.bs(),\n        partition_name=fake.bs(),\n        content_type=fake.bs(),\n        table_name=fake.bs(),\n        partition_key=fake.bs(),\n        sort_key=fake.bs(),\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = await repo.get(fake.bs(), fake.bs())", "    )\n    actual = await repo.get(fake.bs(), fake.bs())\n\n    assert actual == GetResponse(content=None)\n\n\nasync def test_dynamo_repo_get_batch():\n    partition_prefix = fake.bothify()\n    partition_name = fake.bothify()\n    content_type = fake.bothify()", "    partition_name = fake.bothify()\n    content_type = fake.bothify()\n    table_name = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    resource = AsyncMock()\n    items = [\n        example_content_to_db_item(\n            partition_key,\n            partition_prefix,", "            partition_key,\n            partition_prefix,\n            partition_name,\n            sort_key,\n            content_type,\n            ExamplePartitionedContentFactory(partition_ids=[str(i)], content_ids=[str(i)]),\n        )\n        for i in range(5)\n    ]\n", "    ]\n\n    unprocessed = [{fake.bothify(): fake.bothify()}]\n\n    resource.batch_get_item.side_effect = [\n        {\n            \"Responses\": {table_name: items[:2]},\n            \"UnprocessedKeys\": unprocessed,\n        },\n        {\"Responses\": {table_name: items[2:4]}},", "        },\n        {\"Responses\": {table_name: items[2:4]}},\n        {\"Responses\": {table_name: [items[-1]]}},\n    ]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,", "        partition_name=partition_name,\n        content_type=content_type,\n        table_name=table_name,\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=MagicMock(),\n        resource=resource,\n    )\n\n    # Max key count for each request is 100", "\n    # Max key count for each request is 100\n    request_ids = [([fake.bothify()], [fake.bothify()]) for _ in range(120)]\n    actual = [\n        content async for response in repo.get_batch(request_ids) for content in response.contents\n    ]\n\n    expected = [\n        PartitionedContent[Example](\n            partition_ids=[str(i)], content_ids=[str(i)], item=Example(**item)", "        PartitionedContent[Example](\n            partition_ids=[str(i)], content_ids=[str(i)], item=Example(**item)\n        )\n        for i, item in enumerate(items)\n    ]\n\n    assert actual == expected\n    assert resource.batch_get_item.call_args_list == [\n        (\n            (),", "        (\n            (),\n            {\n                \"RequestItems\": {\n                    table_name: {\n                        \"Keys\": [\n                            {\n                                partition_key: f\"{partition_prefix}#{partition_name}#{rid[0][0]}\",\n                                sort_key: f\"{content_type}#{rid[1][0]}\",\n                            }", "                                sort_key: f\"{content_type}#{rid[1][0]}\",\n                            }\n                            for rid in request_ids[:100]\n                        ],\n                        \"ConsistentRead\": False,\n                    }\n                }\n            },\n        ),\n        (", "        ),\n        (\n            (),\n            {\n                \"RequestItems\": {\n                    table_name: {\n                        \"Keys\": unprocessed,\n                        \"ConsistentRead\": False,\n                    }\n                }", "                    }\n                }\n            },\n        ),\n        (\n            (),\n            {\n                \"RequestItems\": {\n                    table_name: {\n                        \"Keys\": [", "                    table_name: {\n                        \"Keys\": [\n                            {\n                                partition_key: f\"{partition_prefix}#{partition_name}#{rid[0][0]}\",\n                                sort_key: f\"{content_type}#{rid[1][0]}\",\n                            }\n                            for rid in request_ids[100:]\n                        ],\n                        \"ConsistentRead\": False,\n                    }", "                        \"ConsistentRead\": False,\n                    }\n                }\n            },\n        ),\n    ]\n\n\nasync def test_dynamo_repo_list():\n    partition_prefix = fake.bs()", "async def test_dynamo_repo_list():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [", "    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }\n", "    }\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,", "        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()", "    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = [\n        content\n        async for response in repo.list(partition_id, content_id, ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n", "    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ConsistentRead\"] is False\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key", "    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]\n        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n", "    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n\n\nasync def test_dynamo_repo_list_last_evaluated_under_limit():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()", "    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(4)\n    start_key = fake.bothify()\n    table.query.side_effect = [\n        {\n            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )", "                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[:3]\n            ],\n            \"LastEvaluatedKey\": start_key,\n            \"Count\": 3,\n        },\n        {\n            \"Items\": [\n                example_content_to_db_item(", "            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[3:4]\n            ],\n            \"Count\": 1,\n        },\n    ]\n", "    ]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,", "        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    ascending = random.choice((True, False))\n    limit = 5\n    actual = [", "    limit = 5\n    actual = [\n        content\n        async for response in repo.list(partition_id, content_id, ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n    assert len(table.query.call_args_list) == 2\n    _, kwargs2 = table.query.call_args_list[1]", "    assert len(table.query.call_args_list) == 2\n    _, kwargs2 = table.query.call_args_list[1]\n    assert kwargs2[\"ExclusiveStartKey\"] == start_key\n\n\nasync def test_dynamo_repo_list_last_evaluated_over_limit():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()", "    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(6)\n    start_key = fake.bothify()\n    table.query.side_effect = [\n        {\n            \"Items\": [\n                example_content_to_db_item(", "            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[:3]\n            ],\n            \"LastEvaluatedKey\": start_key,\n            \"Count\": 3,\n        },\n        {", "        },\n        {\n            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[3:6]\n            ],\n            \"Count\": 3,\n        },", "            \"Count\": 3,\n        },\n    ]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),", "        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    ascending = random.choice((True, False))", "    content_id = [fake.bs(), fake.bs()]\n    ascending = random.choice((True, False))\n    limit = 5\n    actual = [\n        content\n        async for response in repo.list(partition_id, content_id, ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)", "\n    assert sorted(actual) == sorted(contents)\n    assert len(table.query.call_args_list) == 2\n    _, kwargs2 = table.query.call_args_list[1]\n    assert kwargs2[\"ExclusiveStartKey\"] == start_key\n\n\nasync def test_dynamo_repo_list_last_evaluated_over_limit_after_evaluated_key():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()", "    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(8)\n    start_key1 = fake.bothify()\n    start_key2 = fake.bothify()\n    table.query.side_effect = [", "    start_key2 = fake.bothify()\n    table.query.side_effect = [\n        {\n            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[:3]\n            ],\n            \"LastEvaluatedKey\": start_key1,", "            ],\n            \"LastEvaluatedKey\": start_key1,\n            \"Count\": 3,\n        },\n        {\n            \"Items\": [\n                example_content_to_db_item(\n                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n                )\n                for content in contents[3:6]", "                )\n                for content in contents[3:6]\n            ],\n            \"LastEvaluatedKey\": start_key2,\n            \"Count\": 3,\n        },\n    ]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,", "    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),", "        table=table,\n        resource=MagicMock(),\n    )\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n    ascending = random.choice((True, False))\n    limit = 5\n    actual = [\n        content\n        async for response in repo.list(partition_id, content_id, ascending, limit)", "        content\n        async for response in repo.list(partition_id, content_id, ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents[:6])\n    assert len(table.query.call_args_list) == 2\n    _, kwargs2 = table.query.call_args_list[1]\n    assert kwargs2[\"ExclusiveStartKey\"] == start_key1\n", "    assert kwargs2[\"ExclusiveStartKey\"] == start_key1\n\n\nasync def test_dynamo_repo_list_none_inputs():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()", "    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents\n        ],", "            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,", "        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n        consistent_reads=True,\n    )\n    ascending = random.choice((True, False))", "    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = [\n        content\n        async for response in repo.list(None, None, ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)", "\n    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ConsistentRead\"] is True\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"", "    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\n\nasync def test_dynamo_repo_list_no_ids():", "\nasync def test_dynamo_repo_list_no_ids():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {", "    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }", "        \"Count\": fake.pyint(),\n    }\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,", "        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    actual = [\n        content", "    actual = [\n        content\n        async for response in repo.list([], [], ascending, limit)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending", "    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert kwargs[\"Limit\"] == limit\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key", "    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\n\nasync def test_dynamo_repo_list_with_filter():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()", "    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )", "                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }\n\n    partition_id = [fake.bs()]\n    content_id = [fake.bs()]\n", "    content_id = [fake.bs()]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,", "        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    filters = FilterCommand(not_exists={\"optional_field\"})\n    actual = [\n        content", "    actual = [\n        content\n        async for response in repo.list(partition_id, content_id, ascending, limit, filters)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending", "    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert \"Limit\" not in kwargs\n    filter_expression = kwargs[\"FilterExpression\"]\n    assert filter_expression.expression_operator == \"attribute_not_exists\"\n    assert filter_expression._values[0].name == \"optional_field\"\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key", "    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#{partition_id[0]}\"\n    )\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}\"\n\n", "\n\nasync def test_dynamo_repo_list_between():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)", "    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),", "        ],\n        \"Count\": fake.pyint(),\n    }\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_start = [fake.bs(), fake.bs()]\n    content_end = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,", "    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),", "        table=table,\n        resource=MagicMock(),\n    )\n    actual = [\n        content\n        async for response in repo.list_between(partition_id, content_start, content_end)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)", "\n    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]", "    assert (\n        expression._values[0]._values[1]\n        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"BETWEEN\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert (\n        expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}#{content_start[1]}\"\n    )\n    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}#{content_end[1]}\"", "    )\n    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}#{content_end[1]}\"\n\n\nasync def test_dynamo_repo_list_between_none_inputs():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()", "    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents", "            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,", "        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    actual = [", "    )\n    actual = [\n        content\n        async for response in repo.list_between(None, None, None)\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args", "\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"", "    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\n\nasync def test_dynamo_repo_list_between_with_filter():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()", "    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    contents = ExamplePartitionedContentFactory.build_batch(3)\n    table.query.return_value = {\n        \"Items\": [\n            example_content_to_db_item(\n                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n            )\n            for content in contents", "            )\n            for content in contents\n        ],\n        \"Count\": fake.pyint(),\n    }\n\n    partition_id = [fake.bs()]\n    content_start = [fake.bs()]\n    content_end = [fake.bs()]\n", "    content_end = [fake.bs()]\n\n    repo = DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,", "        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    ascending = random.choice((True, False))\n    limit = fake.pyint()\n    filters = FilterCommand(not_exists={\"optional_field\"})\n    actual = [\n        content", "    actual = [\n        content\n        async for response in repo.list_between(\n            partition_id, content_start, content_end, ascending, limit, filters\n        )\n        for content in response.contents\n    ]\n\n    assert sorted(actual) == sorted(contents)\n", "    assert sorted(actual) == sorted(contents)\n\n    args, kwargs = table.query.call_args\n    assert kwargs[\"ScanIndexForward\"] == ascending\n    assert \"Limit\" not in kwargs\n    filter_expression = kwargs[\"FilterExpression\"]\n    assert filter_expression.expression_operator == \"attribute_not_exists\"\n    assert filter_expression._values[0].name == \"optional_field\"\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"", "    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#{partition_id[0]}\"\n    )\n    assert expression._values[1].expression_operator == \"BETWEEN\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}\"", "    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}\"\n    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}\"\n\n\n@patch(\"pydantic_dynamo.v2.repository.build_update_args_for_command\")\nasync def test_dynamo_repo_update(build_update_args):\n    update_args = UpdateItemArgumentsFactory()\n    build_update_args.return_value = update_args\n    partition = fake.bothify()", "    build_update_args.return_value = update_args\n    partition = fake.bothify()\n    partition_name = fake.bothify()\n    content_type = fake.bothify()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = AsyncMock()\n    repo = DynamoRepository[ComposedFieldModel](\n        item_class=ComposedFieldModel,\n        partition_prefix=partition,", "        item_class=ComposedFieldModel,\n        partition_prefix=partition,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )", "        resource=MagicMock(),\n    )\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify(), fake.bothify()]\n    current_version = fake.pyint()\n    command = UpdateCommand(\n        set_commands={\n            \"test_field\": fake.bs(),\n            \"composed\": {\"test_field\": fake.bs(), \"failures\": None},\n        },", "            \"composed\": {\"test_field\": fake.bs(), \"failures\": None},\n        },\n        increment_attrs={\"failures\": 1},\n        current_version=current_version,\n    )\n\n    await repo.update(partition_id, content_id, command)\n\n    update_a, update_k = table.update_item.call_args\n", "    update_a, update_k = table.update_item.call_args\n\n    assert update_k.pop(\"Key\") == {\n        partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n        sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n    }\n\n    assert update_k.pop(\"ConditionExpression\") == update_args.condition_expression\n    assert update_k.pop(\"UpdateExpression\") == update_args.update_expression\n    assert update_k.pop(\"ExpressionAttributeNames\") == update_args.attribute_names", "    assert update_k.pop(\"UpdateExpression\") == update_args.update_expression\n    assert update_k.pop(\"ExpressionAttributeNames\") == update_args.attribute_names\n    assert update_k.pop(\"ExpressionAttributeValues\") == update_args.attribute_values\n    assert len(update_k) == 0\n\n\nasync def test_dynamo_repo_delete():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()", "    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    contents = ExamplePartitionedContentFactory.build_batch(11)\n    items = [\n        example_content_to_db_item(\n            partition_key, partition_prefix, partition_name, sort_key, content_type, c\n        )", "            partition_key, partition_prefix, partition_name, sort_key, content_type, c\n        )\n        for c in contents\n    ]\n    f = asyncio.Future()\n    f.set_result({\"Items\": items, \"Count\": fake.pyint()})\n    table.query.return_value = f\n    writer = AsyncMock()\n    table.batch_writer.return_value.__aenter__.return_value = writer\n", "    table.batch_writer.return_value.__aenter__.return_value = writer\n\n    partition_id = [fake.bs(), fake.bs()]\n    content_id = [fake.bs(), fake.bs()]\n\n    repo = DynamoRepository[Example](\n        item_class=FieldModel,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,", "        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),\n    )\n    await repo.delete(partition_id, content_id)\n", "    await repo.delete(partition_id, content_id)\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert (\n        expression._values[0]._values[1]\n        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"", "        expression._values[0]._values[1]\n        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n    )\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n    assert writer.delete_item.call_args_list == [\n        (\n            (),\n            {", "            (),\n            {\n                \"Key\": {\n                    partition_key: item[partition_key],\n                    sort_key: item[sort_key],\n                }\n            },\n        )\n        for item in items\n    ]", "        for item in items\n    ]\n\n\nasync def test_dynamo_repo_delete_none_inputs():\n    partition_prefix = fake.bs()\n    partition_name = fake.bs()\n    content_type = fake.bs()\n    partition_key = fake.bs()\n    sort_key = fake.bs()", "    partition_key = fake.bs()\n    sort_key = fake.bs()\n    table = MagicMock()\n    contents = ExamplePartitionedContentFactory.build_batch(11)\n    items = [\n        example_content_to_db_item(\n            partition_key, partition_prefix, partition_name, sort_key, content_type, c\n        )\n        for c in contents\n    ]", "        for c in contents\n    ]\n    f = asyncio.Future()\n    f.set_result({\"Items\": items, \"Count\": fake.pyint()})\n    table.query.return_value = f\n    writer = AsyncMock()\n    table.batch_writer.return_value.__aenter__.return_value = writer\n\n    repo = DynamoRepository[Example](\n        item_class=FieldModel,", "    repo = DynamoRepository[Example](\n        item_class=FieldModel,\n        partition_prefix=partition_prefix,\n        partition_name=partition_name,\n        content_type=content_type,\n        table_name=fake.bs(),\n        partition_key=partition_key,\n        sort_key=sort_key,\n        table=table,\n        resource=MagicMock(),", "        table=table,\n        resource=MagicMock(),\n    )\n    await repo.delete(None, None)\n\n    args, kwargs = table.query.call_args\n    expression = kwargs[\"KeyConditionExpression\"]\n    assert expression.expression_operator == \"AND\"\n    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key", "    assert expression._values[0].expression_operator == \"=\"\n    assert expression._values[0]._values[0].name == partition_key\n    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n    assert expression._values[1].expression_operator == \"begins_with\"\n    assert expression._values[1]._values[0].name == sort_key\n    assert expression._values[1]._values[1] == f\"{content_type}#\"\n    assert writer.delete_item.call_args_list == [\n        (\n            (),\n            {", "            (),\n            {\n                \"Key\": {\n                    partition_key: item[partition_key],\n                    sort_key: item[sort_key],\n                }\n            },\n        )\n        for item in items\n    ]", "        for item in items\n    ]\n"]}
{"filename": "tests/test_unit/test_v2/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_unit/test_v2/test_sync_repository.py", "chunked_list": ["from unittest.mock import MagicMock\n\nfrom tests.models import Example\nfrom pydantic_dynamo.v2.sync_repository import SyncDynamoRepository\n\n\ndef test_context_manager():\n    with SyncDynamoRepository[Example](async_repo=MagicMock()) as repo:\n        assert isinstance(repo, SyncDynamoRepository)\n", ""]}
{"filename": "tests/test_unit/test_v2/test_write_once.py", "chunked_list": ["from typing import AsyncIterable, List\nfrom unittest.mock import AsyncMock, MagicMock\n\nfrom faker import Faker\n\nfrom tests.factories import ExamplePartitionedContentFactory\nfrom pydantic_dynamo.v2.models import BatchResponse\nfrom pydantic_dynamo.v2.write_once import WriteOnceRepository\n\n", "\n\nfake = Faker()\n\n\nasync def test_write_once_empty_input():\n    core = MagicMock()\n    repo = WriteOnceRepository(core)\n\n    new = await repo.write([])", "\n    new = await repo.write([])\n    assert new == []\n    assert core.list_between.call_count == 0\n    assert core.put_batch.call_count == 0\n\n\nasync def test_write_once_no_existing():\n    core = MagicMock()\n    core.list_between.return_value.__aiter__.return_value = []", "    core = MagicMock()\n    core.list_between.return_value.__aiter__.return_value = []\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    repo = WriteOnceRepository(core)\n    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids", "        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        for c in content_ids\n    ]\n    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]", "        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n    sorted_expected = sorted(data)\n    assert sorted(actual_put) == sorted_expected\n    assert sorted(new) == sorted_expected\n\n\nasync def test_write_once_async_iter_no_existing():\n    core = MagicMock()", "async def test_write_once_async_iter_no_existing():\n    core = MagicMock()\n    core.list_between.return_value.__aiter__.return_value = []\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    repo = WriteOnceRepository(core)\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids", "        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        async for c in _async_content_ids()\n    ]\n    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]", "        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n    sorted_expected = sorted(data)\n    assert sorted(actual_put) == sorted_expected\n    assert sorted(new) == sorted_expected\n\n\nasync def test_write_once_some_existing():\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]", "async def test_write_once_some_existing():\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        for c in content_ids\n    ]\n    core = MagicMock()\n    response_one = MagicMock()", "    core = MagicMock()\n    response_one = MagicMock()\n    response_one.__aiter__.return_value = [BatchResponse(contents=[data[0]])]\n    response_two = MagicMock()\n    response_two.__aiter__.return_value = [BatchResponse(contents=[data[-1]])]\n    core.list_between.side_effect = [response_one, response_two]\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    repo = WriteOnceRepository(core)\n    new = await repo.write(data)", "    repo = WriteOnceRepository(core)\n    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n    sorted_expected = sorted(data[i] for i in range(8) if i not in (0, 7))\n    assert sorted(actual_put) == sorted_expected", "    sorted_expected = sorted(data[i] for i in range(8) if i not in (0, 7))\n    assert sorted(actual_put) == sorted_expected\n    assert sorted(new) == sorted_expected\n\n\nasync def test_write_once_async_iter_some_existing():\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids", "        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        async for c in _async_content_ids()\n    ]\n    core = MagicMock()\n    response_one = MagicMock()\n    response_one.__aiter__.return_value = [BatchResponse(contents=[data[0]])]\n    response_two = MagicMock()\n    response_two.__aiter__.return_value = [BatchResponse(contents=[data[-1]])]\n    core.list_between.side_effect = [response_one, response_two]", "    response_two.__aiter__.return_value = [BatchResponse(contents=[data[-1]])]\n    core.list_between.side_effect = [response_one, response_two]\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    repo = WriteOnceRepository(core)\n    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),", "        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n    sorted_expected = sorted(data[i] for i in range(8) if i not in (0, 7))\n    assert sorted(actual_put) == sorted_expected\n    assert sorted(new) == sorted_expected\n\n\nasync def test_write_once_all_existing():", "\nasync def test_write_once_all_existing():\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        for c in content_ids\n    ]\n    core = MagicMock()", "    ]\n    core = MagicMock()\n    response_one = MagicMock()\n    response_one.__aiter__.return_value = [\n        BatchResponse(contents=data[0:2]),\n        BatchResponse(contents=data[2:4]),\n    ]\n    response_two = MagicMock()\n    response_two.__aiter__.return_value = [\n        BatchResponse(contents=data[4:6]),", "    response_two.__aiter__.return_value = [\n        BatchResponse(contents=data[4:6]),\n        BatchResponse(contents=data[6:8]),\n    ]\n    core.list_between.side_effect = [response_one, response_two]\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    repo = WriteOnceRepository(core)\n    new = await repo.write(data)\n", "    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    assert core.put_batch.call_count == 0\n    assert new == []\n\n", "\n\nasync def test_write_once_async_iter_all_existing():\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        async for c in _async_content_ids()\n    ]\n    core = MagicMock()", "    ]\n    core = MagicMock()\n    response_one = MagicMock()\n    response_one.__aiter__.return_value = [\n        BatchResponse(contents=data[0:2]),\n        BatchResponse(contents=data[2:4]),\n    ]\n    response_two = MagicMock()\n    response_two.__aiter__.return_value = [\n        BatchResponse(contents=data[4:6]),", "    response_two.__aiter__.return_value = [\n        BatchResponse(contents=data[4:6]),\n        BatchResponse(contents=data[6:8]),\n    ]\n    core.list_between.side_effect = [response_one, response_two]\n    put_batch = AsyncMock()\n    core.put_batch = put_batch\n    repo = WriteOnceRepository(core)\n    new = await repo.write(data)\n", "    new = await repo.write(data)\n\n    assert core.list_between.call_args_list == [\n        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n    ]\n    assert core.put_batch.call_count == 0\n    assert new == []\n\n", "\n\nasync def _async_content_ids() -> AsyncIterable[List[str]]:\n    yield [\"Z\"]\n    yield [\"B\"]\n    yield [\"A\"]\n    yield [\"D\"]\n"]}
{"filename": "tests/test_integration/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_integration/conftest.py", "chunked_list": ["from uuid import uuid4\n\nimport pytest\nimport pytest_asyncio\nfrom aioboto3 import Session\nfrom testcontainers.core.container import DockerContainer\n\nfrom pydantic_dynamo.v2.repository import DynamoRepository\nfrom pydantic_dynamo.v2.sync_repository import SyncDynamoRepository\nfrom tests.models import Example", "from pydantic_dynamo.v2.sync_repository import SyncDynamoRepository\nfrom tests.models import Example\n\nPARTITION_KEY = \"_table_item_id\"\nSORT_KEY = \"_table_content_id\"\n\n\n@pytest.fixture(scope=\"session\")\ndef local_db():\n    with DockerContainer(\"amazon/dynamodb-local\").with_bind_ports(8000, 8000).with_command(\n        \"-jar DynamoDBLocal.jar\"\n    ).with_exposed_ports(8080) as local_db:\n        yield local_db", "def local_db():\n    with DockerContainer(\"amazon/dynamodb-local\").with_bind_ports(8000, 8000).with_command(\n        \"-jar DynamoDBLocal.jar\"\n    ).with_exposed_ports(8080) as local_db:\n        yield local_db\n\n\n@pytest_asyncio.fixture\nasync def local_db_resource(local_db):\n    session = Session()", "async def local_db_resource(local_db):\n    session = Session()\n\n    boto3_kwargs = {\n        \"service_name\": \"dynamodb\",\n        \"endpoint_url\": \"http://localhost:8000\",\n        \"region_name\": \"local\",\n        \"aws_access_key_id\": \"key\",\n        \"aws_secret_access_key\": \"secret\",\n    }", "        \"aws_secret_access_key\": \"secret\",\n    }\n    table_name = str(uuid4())\n    async with session.resource(**boto3_kwargs) as resource:\n        await resource.create_table(\n            TableName=table_name,\n            KeySchema=[\n                {\"AttributeName\": PARTITION_KEY, \"KeyType\": \"HASH\"},\n                {\"AttributeName\": SORT_KEY, \"KeyType\": \"RANGE\"},\n            ],", "                {\"AttributeName\": SORT_KEY, \"KeyType\": \"RANGE\"},\n            ],\n            AttributeDefinitions=[\n                {\"AttributeName\": PARTITION_KEY, \"AttributeType\": \"S\"},\n                {\"AttributeName\": SORT_KEY, \"AttributeType\": \"S\"},\n            ],\n            ProvisionedThroughput={\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10},\n        )\n\n        yield resource, table_name", "\n        yield resource, table_name\n        table = await resource.Table(table_name)\n        await table.delete()\n\n\n@pytest_asyncio.fixture\nasync def v2_example_repo(local_db_resource):\n    resource, table_name = local_db_resource\n    yield DynamoRepository[Example](", "    resource, table_name = local_db_resource\n    yield DynamoRepository[Example](\n        item_class=Example,\n        partition_prefix=\"test\",\n        partition_name=\"integration\",\n        content_type=\"example\",\n        table_name=table_name,\n        partition_key=PARTITION_KEY,\n        sort_key=SORT_KEY,\n        table=await resource.Table(table_name),", "        sort_key=SORT_KEY,\n        table=await resource.Table(table_name),\n        resource=resource,\n    )\n\n\n@pytest.fixture\ndef sync_v2_example_repo(v2_example_repo):\n    yield SyncDynamoRepository[Example](async_repo=v2_example_repo)\n", ""]}
{"filename": "tests/test_integration/test_v2/test_repository.py", "chunked_list": ["from datetime import datetime, timezone, timedelta\nfrom zoneinfo import ZoneInfo\n\nimport pytest\nfrom faker import Faker\n\nfrom pydantic_dynamo.exceptions import RequestObjectStateError\nfrom pydantic_dynamo.models import FilterCommand, UpdateCommand, PartitionedContent\nfrom pydantic_dynamo.v2.models import GetResponse\nfrom tests.factories import ExamplePartitionedContentFactory, ExampleFactory", "from pydantic_dynamo.v2.models import GetResponse\nfrom tests.factories import ExamplePartitionedContentFactory, ExampleFactory\nfrom tests.models import CountEnum, Example, FieldModel\n\nfake = Faker()\n\n\nasync def test_get_none(v2_example_repo):\n    actual = await v2_example_repo.get(None, None)\n    expected = GetResponse()", "    actual = await v2_example_repo.get(None, None)\n    expected = GetResponse()\n    assert actual == expected\n\n\ndef test_sync_get_none(sync_v2_example_repo):\n    actual = sync_v2_example_repo.get(None, None)\n    expected = GetResponse()\n    assert actual == expected\n", "\n\nasync def test_put_then_get_item(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n    await v2_example_repo.put(content)\n    actual = await v2_example_repo.get(partition_ids, content_ids)\n    expected = GetResponse(content=content)\n    assert actual == expected", "    expected = GetResponse(content=content)\n    assert actual == expected\n\n\ndef test_sync_put_then_get_item(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n    sync_v2_example_repo.put(content)\n    actual = sync_v2_example_repo.get(partition_ids, content_ids)\n    expected = GetResponse(content=content)\n    assert actual == expected", "\n\nasync def test_put_batch_then_get_batch(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(125)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)", "    ]\n    await v2_example_repo.put_batch(contents)\n    get_ids = [(partition_ids, content_ids) for content_ids in content_ids_list]\n    actual = [\n        content\n        async for response in v2_example_repo.get_batch(get_ids)\n        for content in response.contents\n    ]\n    actual.sort()\n    contents.sort()", "    actual.sort()\n    contents.sort()\n    assert actual == contents\n\n\ndef test_sync_put_batch_then_get_batch(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(125)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    get_ids = [(partition_ids, content_ids) for content_ids in content_ids_list]\n    actual = [\n        content\n        for response in sync_v2_example_repo.get_batch(get_ids)\n        for content in response.contents\n    ]\n    actual.sort()\n    contents.sort()\n    assert actual == contents", "\n\nasync def test_put_batch_then_list_with_prefix_filter_and_sorting(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)", "    ]\n    await v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        async for response in v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=True\n        )\n        for content in response.contents\n    ]\n    expected = contents[:100]", "    ]\n    expected = contents[:100]\n    expected.sort()\n    assert ascending_actual == expected\n\n    descending_actual = [\n        content\n        async for response in v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n        )", "            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n        )\n        for content in response.contents\n    ]\n    expected.sort(reverse=True)\n    assert descending_actual == expected\n\n\ndef test_sync_put_batch_then_list_with_prefix_filter_and_sorting(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=True\n        )\n        for content in response.contents\n    ]\n    expected = contents[:100]\n    expected.sort()\n    assert ascending_actual == expected\n\n    descending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n        )\n        for content in response.contents\n    ]\n    expected.sort(reverse=True)\n    assert descending_actual == expected", "def test_sync_put_batch_then_list_with_prefix_filter_and_sorting(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=True\n        )\n        for content in response.contents\n    ]\n    expected = contents[:100]\n    expected.sort()\n    assert ascending_actual == expected\n\n    descending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n        )\n        for content in response.contents\n    ]\n    expected.sort(reverse=True)\n    assert descending_actual == expected", "\n\nasync def test_put_batch_then_list_limit(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)", "    ]\n    await v2_example_repo.put_batch(contents)\n    actual = [\n        content\n        async for response in v2_example_repo.list(partition_ids, content_prefix=None, limit=2)\n        for content in response.contents\n    ]\n    expected = contents[:2]\n    assert actual == expected\n", "    assert actual == expected\n\n\ndef test_sync_put_batch_then_list_limit(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    actual = [\n        content\n        for response in sync_v2_example_repo.list(partition_ids, content_prefix=None, limit=2)\n        for content in response.contents\n    ]\n    expected = contents[:2]\n    assert actual == expected", "\n\nasync def test_put_batch_then_list_filter(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)", "    ]\n    await v2_example_repo.put_batch(contents)\n    actual = [\n        content\n        async for response in v2_example_repo.list(\n            partition_ids,\n            content_prefix=None,\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents", "        )\n        for content in response.contents\n    ]\n    actual.sort()\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents))\n    expected.sort()\n    assert actual == expected\n\n\ndef test_sync_put_batch_then_list_filter(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids,\n            content_prefix=None,\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents\n    ]\n    actual.sort()\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents))\n    expected.sort()\n    assert actual == expected", "\ndef test_sync_put_batch_then_list_filter(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids,\n            content_prefix=None,\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents\n    ]\n    actual.sort()\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents))\n    expected.sort()\n    assert actual == expected", "\n\nasync def test_put_batch_then_list_between_sorting(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]", "        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        async for response in v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            sort_ascending=True,", "            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            sort_ascending=True,\n        )\n        for content in response.contents\n    ]\n    expected = contents[1:9]\n    assert ascending_actual == expected\n    descending_actual = [\n        content\n        async for response in v2_example_repo.list_between(", "        content\n        async for response in v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            sort_ascending=False,\n        )\n        for content in response.contents\n    ]\n    expected.sort(key=lambda c: c.content_ids[0], reverse=True)", "    ]\n    expected.sort(key=lambda c: c.content_ids[0], reverse=True)\n    assert descending_actual == expected\n\n\ndef test_sync_put_batch_then_list_between_sorting(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            sort_ascending=True,\n        )\n        for content in response.contents\n    ]\n    expected = contents[1:9]\n    assert ascending_actual == expected\n    descending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            sort_ascending=False,\n        )\n        for content in response.contents\n    ]\n    expected.sort(key=lambda c: c.content_ids[0], reverse=True)\n    assert descending_actual == expected", "\n\nasync def test_put_batch_then_list_between_limit(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]", "        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        async for response in v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            limit=5,", "            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            limit=5,\n            sort_ascending=True,\n        )\n        for content in response.contents\n    ]\n    expected = contents[1:6]\n    assert ascending_actual == expected\n    descending_actual = [\n        content", "    descending_actual = [\n        content\n        async for response in v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            limit=5,\n            sort_ascending=False,\n        )\n        for content in response.contents", "        )\n        for content in response.contents\n    ]\n    descending_expected = sorted(contents, key=lambda c: c.content_ids[0], reverse=True)[1:6]\n    assert descending_actual == descending_expected\n\n\ndef test_sync_put_batch_then_list_between_limit(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            limit=5,\n            sort_ascending=True,\n        )\n        for content in response.contents\n    ]\n    expected = contents[1:6]\n    assert ascending_actual == expected\n    descending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            limit=5,\n            sort_ascending=False,\n        )\n        for content in response.contents\n    ]\n    descending_expected = sorted(contents, key=lambda c: c.content_ids[0], reverse=True)[1:6]\n    assert descending_actual == descending_expected", "\n\nasync def test_put_batch_then_list_between_filter(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]", "        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        async for response in v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),", "            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents\n    ]\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents[1:9]))\n    assert ascending_actual == expected\n\n\ndef test_sync_put_batch_then_list_between_filter(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents\n    ]\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents[1:9]))\n    assert ascending_actual == expected", "\ndef test_sync_put_batch_then_list_between_filter(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    now = datetime.now(tz=timezone.utc)\n    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list_between(\n            partition_id=partition_ids,\n            content_start=[(now + timedelta(seconds=1)).isoformat()],\n            content_end=[(now + timedelta(seconds=8)).isoformat()],\n            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n        )\n        for content in response.contents\n    ]\n    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents[1:9]))\n    assert ascending_actual == expected", "\n\nasync def test_update_happy_path(v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n    await v2_example_repo.put(content)\n\n    new_dt = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n    new_str = fake.bs()", "    new_dt = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n    new_str = fake.bs()\n\n    # TTL stored as integer where we lose microsecond granularity\n    new_expiry = (new_dt + timedelta(days=99)).replace(microsecond=0)\n    await v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(\n            current_version=1,", "        UpdateCommand(\n            current_version=1,\n            set_commands={\"datetime_field\": new_dt},\n            increment_attrs={\"int_field\": 2},\n            append_attrs={\"list_field\": [new_str]},\n            expiry=new_expiry,\n        ),\n    )\n\n    updated = await v2_example_repo.get(partition_id, content_id)", "\n    updated = await v2_example_repo.get(partition_id, content_id)\n\n    og_dict = content.item.dict()\n    og_dict.pop(\"datetime_field\")\n    expected = PartitionedContent[Example](\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=Example(\n            datetime_field=new_dt,", "        item=Example(\n            datetime_field=new_dt,\n            int_field=og_dict.pop(\"int_field\") + 2,\n            list_field=og_dict.pop(\"list_field\") + [new_str],\n            **og_dict\n        ),\n        current_version=2,\n        expiry=new_expiry,\n    )\n", "    )\n\n    assert updated.content == expected\n\n\ndef test_sync_update_happy_path(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n    sync_v2_example_repo.put(content)\n\n    new_dt = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n    new_str = fake.bs()\n\n    # TTL stored as integer where we lose microsecond granularity\n    new_expiry = (new_dt + timedelta(days=99)).replace(microsecond=0)\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(\n            current_version=1,\n            set_commands={\"datetime_field\": new_dt},\n            increment_attrs={\"int_field\": 2},\n            append_attrs={\"list_field\": [new_str]},\n            expiry=new_expiry,\n        ),\n    )\n\n    updated = sync_v2_example_repo.get(partition_id, content_id)\n\n    og_dict = content.item.dict()\n    og_dict.pop(\"datetime_field\")\n    expected = PartitionedContent[Example](\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=Example(\n            datetime_field=new_dt,\n            int_field=og_dict.pop(\"int_field\") + 2,\n            list_field=og_dict.pop(\"list_field\") + [new_str],\n            **og_dict\n        ),\n        current_version=2,\n        expiry=new_expiry,\n    )\n\n    assert updated.content == expected", "\n\nasync def test_update_nested_model_field(v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),", "        content_ids=content_id,\n        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),\n    )\n    await v2_example_repo.put(content)\n    await v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"model_field\": {\"failures\": 2}}),\n    )\n    updated = await v2_example_repo.get(partition_id, content_id)", "    )\n    updated = await v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                model_field=FieldModel(test_field=init_value, failures=2),\n                **content.item.dict(exclude={\"model_field\"})\n            ),", "                **content.item.dict(exclude={\"model_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected\n\n\ndef test_sync_update_nested_model_field(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),\n    )\n    sync_v2_example_repo.put(content)\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"model_field\": {\"failures\": 2}}),\n    )\n    updated = sync_v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                model_field=FieldModel(test_field=init_value, failures=2),\n                **content.item.dict(exclude={\"model_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected", "def test_sync_update_nested_model_field(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),\n    )\n    sync_v2_example_repo.put(content)\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"model_field\": {\"failures\": 2}}),\n    )\n    updated = sync_v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                model_field=FieldModel(test_field=init_value, failures=2),\n                **content.item.dict(exclude={\"model_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected", "\n\nasync def test_update_nested_dict_field(v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),", "        content_ids=content_id,\n        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),\n    )\n    await v2_example_repo.put(content)\n    new_value = fake.bs()\n    await v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n    )", "        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n    )\n    updated = await v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                dict_field={\"one\": init_value, \"two\": new_value},\n                **content.item.dict(exclude={\"dict_field\"})", "                dict_field={\"one\": init_value, \"two\": new_value},\n                **content.item.dict(exclude={\"dict_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected\n\n\ndef test_sync_update_nested_dict_field(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),\n    )\n    sync_v2_example_repo.put(content)\n    new_value = fake.bs()\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n    )\n    updated = sync_v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                dict_field={\"one\": init_value, \"two\": new_value},\n                **content.item.dict(exclude={\"dict_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected", "\ndef test_sync_update_nested_dict_field(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    init_value = fake.bs()\n    content = ExamplePartitionedContentFactory(\n        partition_ids=partition_id,\n        content_ids=content_id,\n        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),\n    )\n    sync_v2_example_repo.put(content)\n    new_value = fake.bs()\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n    )\n    updated = sync_v2_example_repo.get(partition_id, content_id)\n    expected = GetResponse(\n        content=PartitionedContent[Example](\n            partition_ids=partition_id,\n            content_ids=content_id,\n            item=Example(\n                dict_field={\"one\": init_value, \"two\": new_value},\n                **content.item.dict(exclude={\"dict_field\"})\n            ),\n            current_version=2,\n        )\n    )\n    assert updated == expected", "\n\nasync def test_update_requires_exists_but_doesnt(v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    with pytest.raises(RequestObjectStateError) as ex:\n        await v2_example_repo.update(\n            partition_id=partition_id, content_id=content_id, command=UpdateCommand()\n        )\n", "\n    assert partition_id[0] in str(ex)\n    assert content_id[0] in str(ex)\n\n\ndef test_sync_update_requires_exists_but_doesnt(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    with pytest.raises(RequestObjectStateError) as ex:\n        sync_v2_example_repo.update(\n            partition_id=partition_id, content_id=content_id, command=UpdateCommand()\n        )\n\n    assert partition_id[0] in str(ex)\n    assert content_id[0] in str(ex)", "\n\nasync def test_update_mismatched_version(v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n    await v2_example_repo.put(content)\n    new_expiry = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\n    # First update should succeed and increment version to 2", "\n    # First update should succeed and increment version to 2\n    await v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(\n            current_version=1,\n            expiry=new_expiry,\n        ),\n    )", "        ),\n    )\n\n    # Second update should fail because db has current_version of 2\n    with pytest.raises(RequestObjectStateError) as ex:\n        await v2_example_repo.update(\n            partition_id,\n            content_id,\n            UpdateCommand(\n                current_version=1,\n                expiry=new_expiry,\n            ),\n        )", "\n    assert partition_id[0] in str(ex)\n    assert content_id[0] in str(ex)\n\n\ndef test_sync_update_mismatched_version(sync_v2_example_repo):\n    partition_id = [fake.bothify()]\n    content_id = [fake.bothify()]\n    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n    sync_v2_example_repo.put(content)\n    new_expiry = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\n    # First update should succeed and increment version to 2\n    sync_v2_example_repo.update(\n        partition_id,\n        content_id,\n        UpdateCommand(\n            current_version=1,\n            expiry=new_expiry,\n        ),\n    )\n\n    # Second update should fail because db has current_version of 2\n    with pytest.raises(RequestObjectStateError) as ex:\n        sync_v2_example_repo.update(\n            partition_id,\n            content_id,\n            UpdateCommand(\n                current_version=1,\n                expiry=new_expiry,\n            ),\n        )\n\n    assert partition_id[0] in str(ex)\n    assert content_id[0] in str(ex)", "\n\nasync def test_put_batch_then_delete(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    # 25 seems to be the boto3 default size for batch writes, 100 is the limit for batch gets\n    # 137 ensures we span both of those limits and do so with partial size batches\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list", "        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)\n    await v2_example_repo.delete(partition_ids, [\"0\"])\n\n    remaining = [\n        content\n        async for response in v2_example_repo.list(\n            partition_ids,", "        async for response in v2_example_repo.list(\n            partition_ids,\n            None,\n        )\n        for content in response.contents\n    ]\n\n    assert remaining == contents[100:]\n\n\ndef test_sync_put_batch_then_delete(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    # 25 seems to be the boto3 default size for batch writes, 100 is the limit for batch gets\n    # 137 ensures we span both of those limits and do so with partial size batches\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    sync_v2_example_repo.delete(partition_ids, [\"0\"])\n\n    remaining = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids,\n            None,\n        )\n        for content in response.contents\n    ]\n\n    assert remaining == contents[100:]", "\n\ndef test_sync_put_batch_then_delete(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    # 25 seems to be the boto3 default size for batch writes, 100 is the limit for batch gets\n    # 137 ensures we span both of those limits and do so with partial size batches\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    sync_v2_example_repo.delete(partition_ids, [\"0\"])\n\n    remaining = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids,\n            None,\n        )\n        for content in response.contents\n    ]\n\n    assert remaining == contents[100:]", ""]}
{"filename": "tests/test_integration/test_v2/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_integration/test_v2/test_repository_long_running.py", "chunked_list": ["from faker import Faker\n\nfrom tests.factories import ExamplePartitionedContentFactory\n\nfake = Faker()\n\n\nasync def test_put_batch_then_list_no_prefix_filter_with_sorting(v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(2250)]", "    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(2250)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    await v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        async for response in v2_example_repo.list(", "        content\n        async for response in v2_example_repo.list(\n            partition_ids, content_prefix=None, sort_ascending=True\n        )\n        for content in response.contents\n    ]\n    contents.sort()\n    assert ascending_actual == contents\n\n    descending_actual = [", "\n    descending_actual = [\n        content\n        async for response in v2_example_repo.list(\n            partition_ids, content_prefix=None, sort_ascending=False\n        )\n        for content in response.contents\n    ]\n    contents.sort(reverse=True)\n    assert descending_actual == contents", "    contents.sort(reverse=True)\n    assert descending_actual == contents\n\n\ndef test_sync_put_batch_then_list_no_prefix_filter_with_sorting(sync_v2_example_repo):\n    partition_ids = [fake.bothify()]\n    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(2250)]\n    contents = [\n        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n        for content_ids in content_ids_list\n    ]\n    sync_v2_example_repo.put_batch(contents)\n    ascending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=None, sort_ascending=True\n        )\n        for content in response.contents\n    ]\n    contents.sort()\n    assert ascending_actual == contents\n\n    descending_actual = [\n        content\n        for response in sync_v2_example_repo.list(\n            partition_ids, content_prefix=None, sort_ascending=False\n        )\n        for content in response.contents\n    ]\n    contents.sort(reverse=True)\n    assert descending_actual == contents", ""]}
{"filename": "tests/test_integration/test_v2/test_write_once.py", "chunked_list": ["from faker import Faker\n\nfrom tests.factories import ExamplePartitionedContentFactory\nfrom tests.models import Example\nfrom pydantic_dynamo.v2.repository import DynamoRepository\nfrom pydantic_dynamo.v2.write_once import WriteOnceRepository\n\nfake = Faker()\n\n", "\n\nasync def test_write_once_repo(v2_example_repo: DynamoRepository[Example]):\n    repo = WriteOnceRepository[Example](async_repo=v2_example_repo)\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        for c in content_ids", "        for p in partition_ids\n        for c in content_ids\n    ]\n\n    written = await repo.write(data)\n\n    actual = [\n        c\n        for partition_id in partition_ids\n        async for response in v2_example_repo.list(partition_id, None)", "        for partition_id in partition_ids\n        async for response in v2_example_repo.list(partition_id, None)\n        for c in response.contents\n    ]\n\n    assert sorted(actual) == sorted(data)\n    assert sorted(actual) == sorted(written)\n\n    written_again = await repo.write(data)\n", "    written_again = await repo.write(data)\n\n    assert len(written_again) == 0\n\n\nasync def test_write_once_repo_one_item(v2_example_repo: DynamoRepository[Example]):\n    repo = WriteOnceRepository[Example](async_repo=v2_example_repo)\n    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(1)]\n    content_ids = [[\"A\"]]\n    data = [", "    content_ids = [[\"A\"]]\n    data = [\n        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n        for p in partition_ids\n        for c in content_ids\n    ]\n\n    written = await repo.write(data)\n\n    actual = [", "\n    actual = [\n        c\n        for partition_id in partition_ids\n        async for response in v2_example_repo.list(partition_id, None)\n        for c in response.contents\n    ]\n\n    assert sorted(actual) == sorted(data)\n    assert sorted(actual) == sorted(written)", "    assert sorted(actual) == sorted(data)\n    assert sorted(actual) == sorted(written)\n\n    written_again = await repo.write(data)\n\n    assert len(written_again) == 0\n"]}
{"filename": "pydantic_dynamo/models.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom typing import (\n    Generic,\n    TypeVar,\n    List,\n    Optional,\n    Dict,\n    Any,\n    Union,", "    Any,\n    Union,\n    Set,\n    Sequence,\n    Tuple,\n    Iterator,\n    Iterable,\n)\n\nfrom pydantic import BaseModel", "\nfrom pydantic import BaseModel\nfrom pydantic.generics import GenericModel\n\nObjT = TypeVar(\"ObjT\", bound=BaseModel)\n\n\nclass PartitionedContent(GenericModel, Generic[ObjT]):\n    partition_ids: List[str]\n    content_ids: List[str]\n    item: ObjT\n    current_version: int = 1\n    expiry: Optional[datetime]\n\n    def __lt__(self, other):\n        return (self.partition_ids + self.content_ids) < (other.partition_ids + other.content_ids)\n\n    def __gt__(self, other):\n        return (self.partition_ids + self.content_ids) > (other.partition_ids + other.content_ids)\n\n    def __le__(self, other):\n        return (self.partition_ids + self.content_ids) <= (other.partition_ids + other.content_ids)\n\n    def __ge__(self, other):\n        return (self.partition_ids + self.content_ids) >= (other.partition_ids + other.content_ids)", "\n\nclass UpdateCommand(BaseModel):\n    current_version: Optional[int]\n    set_commands: Dict[str, Any] = {}\n    increment_attrs: Dict[str, int] = {}\n    append_attrs: Dict[str, Optional[List[Union[str, Dict]]]] = {}\n    expiry: Optional[datetime]\n\n\nclass FilterCommand(BaseModel):\n    not_exists: Set[str] = set()\n    equals: Dict[str, Any] = {}\n    not_equals: Dict[str, Any] = {}", "\n\nclass FilterCommand(BaseModel):\n    not_exists: Set[str] = set()\n    equals: Dict[str, Any] = {}\n    not_equals: Dict[str, Any] = {}\n\n\nclass ReadOnlyAbstractRepository(ABC, Generic[ObjT]):\n    @abstractmethod\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> Optional[ObjT]:\n        pass\n\n    @abstractmethod\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> List[ObjT]:\n        pass\n\n    @abstractmethod\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        pass\n\n    @abstractmethod\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        pass", "class ReadOnlyAbstractRepository(ABC, Generic[ObjT]):\n    @abstractmethod\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> Optional[ObjT]:\n        pass\n\n    @abstractmethod\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> List[ObjT]:\n        pass\n\n    @abstractmethod\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        pass\n\n    @abstractmethod\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        pass", "\n\nclass AbstractRepository(ReadOnlyAbstractRepository[ObjT], ABC):\n    @abstractmethod\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        pass\n\n    @abstractmethod\n    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n        pass\n\n    @abstractmethod\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        pass", ""]}
{"filename": "pydantic_dynamo/__init__.py", "chunked_list": [""]}
{"filename": "pydantic_dynamo/utils.py", "chunked_list": ["import logging\nfrom datetime import datetime, timezone, date, time\nfrom enum import Enum\nfrom io import StringIO\nfrom typing import Optional, Sequence, Iterable, List, Dict, Any, Set\n\nfrom boto3.dynamodb.conditions import ConditionBase, Attr\nfrom pydantic import BaseModel\n\nfrom pydantic_dynamo.constants import INTERNAL_TIMESTAMP_KEY, INTERNAL_TTL, INTERNAL_OBJECT_VERSION", "\nfrom pydantic_dynamo.constants import INTERNAL_TIMESTAMP_KEY, INTERNAL_TTL, INTERNAL_OBJECT_VERSION\nfrom pydantic_dynamo.exceptions import RequestObjectStateError\nfrom pydantic_dynamo.models import UpdateCommand, FilterCommand\n\nlogger = logging.getLogger(__name__)\n\n\ndef utc_now() -> datetime:\n    return datetime.now(tz=timezone.utc)", "def utc_now() -> datetime:\n    return datetime.now(tz=timezone.utc)\n\n\ndef internal_timestamp() -> Dict[str, str]:\n    return {INTERNAL_TIMESTAMP_KEY: utc_now().isoformat()}\n\n\ndef get_error_code(ex: Exception) -> Optional[str]:\n    if hasattr(ex, \"response\"):\n        return ex.response.get(\"Error\", {}).get(\"Code\")  # type: ignore[no-any-return,attr-defined]\n    return None", "def get_error_code(ex: Exception) -> Optional[str]:\n    if hasattr(ex, \"response\"):\n        return ex.response.get(\"Error\", {}).get(\"Code\")  # type: ignore[no-any-return,attr-defined]\n    return None\n\n\ndef chunks(items: Sequence, size: int) -> Iterable[Sequence]:\n    \"\"\"Yield successive n-sized chunks from items.\"\"\"\n    for i in range(0, len(items), size):\n        yield items[i : (i + size)]", "\n\nclass UpdateItemArguments(BaseModel):\n    class Config:\n        arbitrary_types_allowed = True\n\n    update_expression: str\n    condition_expression: Optional[ConditionBase]\n    attribute_names: Dict[str, str]\n    attribute_values: Dict[str, Any]", "\n\nasync def execute_update_item(table, key: Dict, args: UpdateItemArguments) -> None:\n    try:\n        update_kwargs = {\n            \"Key\": key,\n            \"UpdateExpression\": args.update_expression,\n            \"ExpressionAttributeNames\": args.attribute_names,\n            \"ExpressionAttributeValues\": args.attribute_values,\n        }\n        if args.condition_expression:\n            update_kwargs[\"ConditionExpression\"] = args.condition_expression\n        await table.update_item(**update_kwargs)\n    except Exception as ex:\n        code = get_error_code(ex)\n        if code == \"ConditionalCheckFailedException\":\n            raise RequestObjectStateError(\n                f\"Object version condition failed for: {str(key)}\"\n            ) from ex\n        raise", "\n\ndef clean_dict(item_dict: Dict) -> Dict:\n    dicts = [item_dict]\n\n    while len(dicts) > 0:\n        current_dict = dicts.pop()\n        for k, v in current_dict.items():\n            if isinstance(v, Dict):\n                dicts.append(v)\n            elif isinstance(v, BaseModel):\n                current_dict[k] = clean_dict(v.dict())\n            elif isinstance(v, (List, Set)):\n                if len(v) > 0:\n                    first = next(iter(v))\n                    if isinstance(first, Dict):\n                        for obj in v:\n                            dicts.append(obj)\n                    elif isinstance(first, BaseModel):\n                        current_dict[k] = [clean_dict(obj.dict()) for obj in v]\n                    else:\n                        current_dict[k] = [clean_value(el) for el in v]\n            else:\n                current_dict[k] = clean_value(v)\n    return item_dict", "\n\ndef clean_value(value: Any) -> Any:\n    if isinstance(value, (date, time, datetime)):\n        return value.isoformat()\n    elif isinstance(value, Enum):\n        return value.value\n    elif isinstance(value, BaseModel):\n        return value.dict()\n    else:\n        return value", "\n\ndef build_update_args_for_command(\n    command: UpdateCommand,\n    key: Optional[Dict[str, str]] = None,\n) -> UpdateItemArguments:\n    set_attrs = command.set_commands\n    set_attrs[INTERNAL_TIMESTAMP_KEY] = utc_now()\n    if expiry_dt := command.expiry:\n        set_attrs[INTERNAL_TTL] = int(expiry_dt.timestamp())\n    set_attrs.pop(INTERNAL_OBJECT_VERSION, None)\n\n    update_expression = StringIO()\n    update_expression.write(\"SET \")\n    names = {}\n    values: Dict[str, Any] = {\":zero\": 0}\n\n    attr_count = 0\n    val_count = 0\n    for k, v in set_attrs.items():\n        base_attr_id = f\"#att{attr_count}\"\n        names[base_attr_id] = k\n\n        if isinstance(v, Dict):\n            for set_k, set_v in v.items():\n                if attr_count > 0:\n                    update_expression.write(\", \")\n                attr_count += 1\n                prop_attr_id = f\"#att{attr_count}\"\n                attr_id = f\"{base_attr_id}.{prop_attr_id}\"\n                val_id = f\":val{val_count}\"\n                update_expression.write(f\"{attr_id} = {val_id}\")\n\n                names[prop_attr_id] = set_k\n                values[val_id] = set_v\n\n                val_count += 1\n            attr_count += 1\n        else:\n            val_id = f\":val{val_count}\"\n\n            if attr_count > 0:\n                update_expression.write(\", \")\n            update_expression.write(f\"{base_attr_id} = {val_id}\")\n            values[val_id] = v\n\n            val_count += 1\n            attr_count += 1\n\n    add_attrs = command.increment_attrs\n    add_attrs.update(**{INTERNAL_OBJECT_VERSION: 1})\n    add_attrs.pop(INTERNAL_TIMESTAMP_KEY, None)\n\n    for k, v in add_attrs.items():\n        attr_id = f\"#att{attr_count}\"\n        val_id = f\":val{val_count}\"\n        if attr_count > 0:\n            update_expression.write(\", \")\n\n        update_expression.write(f\"{attr_id} = if_not_exists({attr_id}, :zero) + {val_id}\")\n        names[attr_id] = k\n        values[val_id] = v\n\n        attr_count += 1\n        val_count += 1\n\n    append_attrs = command.append_attrs\n    append_attrs.pop(INTERNAL_TIMESTAMP_KEY, None)\n    append_attrs.pop(INTERNAL_OBJECT_VERSION, None)\n\n    if len(append_attrs) > 0:\n        values[\":empty_list\"] = []\n    for k, v in append_attrs.items():\n        attr_id = f\"#att{attr_count}\"\n        val_id = f\":val{val_count}\"\n        if attr_count > 0:\n            update_expression.write(\", \")\n        update_expression.write(\n            f\"{attr_id} = list_append(if_not_exists({attr_id}, :empty_list), {val_id})\"\n        )\n        names[attr_id] = k\n        values[val_id] = 1 if v is None else v\n\n        attr_count += 1\n        val_count += 1\n\n    condition = build_update_condition(command, key)\n    clean_values = clean_dict(values)\n    arguments = UpdateItemArguments(\n        update_expression=update_expression.getvalue(),\n        condition_expression=condition,\n        attribute_names=names,\n        attribute_values=clean_values,\n    )\n\n    logger.info(\n        \"Generated update item argument expression\",\n        extra={\"expression\": arguments.update_expression},\n    )\n\n    return arguments", "\n\ndef build_update_condition(\n    command: UpdateCommand,\n    key: Optional[Dict[str, str]] = None,\n) -> Optional[ConditionBase]:\n    condition = None\n    if key:\n        for k, v in key.items():\n            key_condition = Attr(k).eq(v)\n            if condition:\n                condition = condition & key_condition\n            else:\n                condition = key_condition\n    if command.current_version:\n        version_condition = Attr(INTERNAL_OBJECT_VERSION).eq(command.current_version)\n        if condition:\n            condition = condition & version_condition\n        else:\n            condition = version_condition\n\n    return condition", "\n\ndef build_filter_expression(filters: FilterCommand) -> Optional[ConditionBase]:\n    condition: Optional[ConditionBase] = None\n    for attr in filters.not_exists:\n        new_condition = Attr(attr).not_exists()\n        if condition is None:\n            condition = new_condition\n        else:\n            condition = condition & new_condition\n    clean_equals = clean_dict(filters.equals)\n    for k, v in clean_equals.items():\n        new_condition = Attr(k).eq(v)\n        if condition is None:\n            condition = new_condition\n        else:\n            condition = condition & new_condition\n    clean_not_equals = clean_dict(filters.not_equals)\n    for k, v in clean_not_equals.items():\n        new_condition = Attr(k).ne(v)\n        if condition is None:\n            condition = new_condition\n        else:\n            condition = condition & new_condition\n\n    return condition", "\n\ndef validate_command_for_schema(schema: Dict, command: UpdateCommand) -> None:\n    schema_props: Dict = schema.get(\"properties\")  # type: ignore[assignment]\n\n    set_error_keys = []\n    for k, v in command.set_commands.items():\n        schema_prop = schema_props.get(k)\n        if not schema_prop:\n            set_error_keys.append(k)\n            continue\n        if isinstance(v, Dict):\n            if schema_prop_ref := schema_prop.get(\"$ref\"):\n                schema_prop_key = schema_prop_ref.split(\"/\")[-1]\n                nested_schema = schema[\"definitions\"][schema_prop_key]\n                nested_props: Dict = nested_schema.get(\"properties\")  # type: ignore[assignment]\n                for nested_k, nested_v in v.items():\n                    if nested_k not in nested_props:\n                        set_error_keys.append(f\"{k}.{nested_k}\")\n            elif schema_prop[\"type\"] != \"object\":\n                set_error_keys.append(k)\n\n    if len(set_error_keys) > 0:\n        raise ValueError(\n            f\"command contains set attrs not found in {schema.get('title')} type: \"\n            f\"{','.join(set_error_keys)}\"\n        )\n\n    increment_attrs = command.increment_attrs.keys()\n    validate_attrs_in_schema(\n        schema,\n        (\n            *increment_attrs,\n            *command.append_attrs.keys(),\n        ),\n    )\n\n    incr_error_keys = []\n    for k in increment_attrs:\n        prop = schema_props[k]\n        if prop.get(\"type\") != \"integer\":\n            incr_error_keys.append(k)\n    if len(incr_error_keys) > 0:\n        raise ValueError(\n            \"command contains increment_attrs that are not integers in \"\n            f\"{schema.get('title')} type: \"\n            f\"{','.join(incr_error_keys)}\"\n        )", "\n\ndef validate_filters_for_schema(schema: Dict, filters: FilterCommand) -> None:\n    validate_attrs_in_schema(\n        schema,\n        (*filters.not_exists, *filters.equals.keys(), *filters.not_equals.keys()),\n    )\n\n\ndef validate_attrs_in_schema(schema: Dict, attrs: Iterable[str]) -> None:\n    schema_props = schema[\"properties\"]\n    invalid_keys = [a for a in attrs if a not in schema_props]\n\n    if len(invalid_keys) > 0:\n        raise ValueError(\n            f\"command contains attrs not found in {schema.get('title')} type: \"\n            f\"{','.join(invalid_keys)}\"\n        )", "\ndef validate_attrs_in_schema(schema: Dict, attrs: Iterable[str]) -> None:\n    schema_props = schema[\"properties\"]\n    invalid_keys = [a for a in attrs if a not in schema_props]\n\n    if len(invalid_keys) > 0:\n        raise ValueError(\n            f\"command contains attrs not found in {schema.get('title')} type: \"\n            f\"{','.join(invalid_keys)}\"\n        )", ""]}
{"filename": "pydantic_dynamo/constants.py", "chunked_list": ["from typing import List\n\nLAST_EVALUATED_KEY = \"LastEvaluatedKey\"\nFILTER_EXPRESSION = \"FilterExpression\"\nINTERNAL_TIMESTAMP_KEY = \"_timestamp\"\nINTERNAL_OBJECT_VERSION = \"_object_version\"\nINTERNAL_TTL = \"_ttl\"\nEMPTY_LIST: List[str] = []\n", ""]}
{"filename": "pydantic_dynamo/repository.py", "chunked_list": ["from __future__ import annotations\n\nimport logging\n\nfrom boto3 import Session\nfrom boto3.dynamodb.conditions import Key\nfrom typing import (\n    Optional,\n    Dict,\n    Iterable,", "    Dict,\n    Iterable,\n    Type,\n    List,\n    Any,\n    Union,\n    Iterator,\n    Tuple,\n    Sequence,\n)", "    Sequence,\n)\n\nfrom pydantic_dynamo.constants import (\n    EMPTY_LIST,\n    INTERNAL_OBJECT_VERSION,\n    INTERNAL_TTL,\n    FILTER_EXPRESSION,\n    LAST_EVALUATED_KEY,\n)", "    LAST_EVALUATED_KEY,\n)\nfrom pydantic_dynamo.exceptions import RequestObjectStateError\nfrom pydantic_dynamo.utils import (\n    chunks,\n    get_error_code,\n    internal_timestamp,\n    validate_filters_for_schema,\n    build_filter_expression,\n    validate_command_for_schema,", "    build_filter_expression,\n    validate_command_for_schema,\n    clean_dict,\n    build_update_args_for_command,\n)\nfrom pydantic_dynamo.models import (\n    UpdateCommand,\n    FilterCommand,\n    ObjT,\n    PartitionedContent,", "    ObjT,\n    PartitionedContent,\n    AbstractRepository,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass DynamoRepository(AbstractRepository[ObjT]):\n    @classmethod\n    def build(\n        cls,\n        table_name: str,\n        item_class: Type[ObjT],\n        partition_prefix: str,\n        partition_name: str,\n        content_type: str,\n    ) -> DynamoRepository[ObjT]:\n        resource = Session().resource(\"dynamodb\")\n        table = resource.Table(table_name)\n        return cls(\n            item_class=item_class,\n            partition_prefix=partition_prefix,\n            partition_name=partition_name,\n            content_type=content_type,\n            table_name=table_name,\n            partition_key=\"_table_item_id\",\n            sort_key=\"_table_content_id\",\n            table=table,\n            resource=resource,\n        )\n\n    def __init__(\n        self,\n        *,\n        item_class: Type[ObjT],\n        partition_prefix: str,\n        partition_name: str,\n        content_type: str,\n        table_name: str,\n        partition_key: str,\n        sort_key: str,\n        table,\n        resource,\n    ):\n        self._item_class = item_class\n        self._item_schema = self._item_class.schema()\n        self._partition_prefix = partition_prefix\n        self._partition_name = partition_name\n        self._content_type = content_type\n        self._table_name = table_name\n        self._partition_key = partition_key\n        self._sort_key = sort_key\n        self._table = table\n        self._resource = resource\n\n    @property\n    def context(self) -> Dict[str, str]:\n        return {\n            \"item_class\": self._item_class.__name__,\n            \"partition_prefix\": self._partition_id(EMPTY_LIST),\n            \"content_prefix\": self._content_id(EMPTY_LIST),\n        }\n\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        log_context: Dict[str, Any] = {\n            \"partition_id\": content.partition_ids,\n            \"content_id\": content.content_ids,\n            **self.context,\n        }\n        logger.info(\"Putting single content\", extra=log_context)\n        self._put_content(self._table, content)\n        logger.info(\"Put single content\", extra=log_context)\n\n    def put_batch(self, batch: Iterable[PartitionedContent[ObjT]]) -> None:\n        logger.info(\"Putting batch content\", extra=self.context)\n        count = 0\n        with self._table.batch_writer() as writer:\n            for content in batch:\n                self._put_content(writer, content)\n                count += 1\n        logger.info(\"Finished putting batch content\", extra={\"count\": count, **self.context})\n\n    def _put_content(self, table, content: PartitionedContent[ObjT]) -> None:\n        item_dict = clean_dict(content.item.dict())\n        item_dict[INTERNAL_OBJECT_VERSION] = 1\n        item_dict.update(**internal_timestamp())\n        put_item: Dict[str, Union[str, int]] = {\n            self._partition_key: self._partition_id(content.partition_ids),\n            self._sort_key: self._content_id(content.content_ids),\n            **item_dict,\n        }\n        if expiry := content.expiry:\n            put_item[INTERNAL_TTL] = int(expiry.timestamp())\n        table.put_item(Item=put_item)\n\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        validate_command_for_schema(self._item_schema, command)\n        key = {\n            self._partition_key: self._partition_id(partition_id),\n            self._sort_key: self._content_id(content_id),\n        }\n        build_kwargs: Dict[str, Any] = {\n            \"command\": command,\n        }\n        if require_exists:\n            build_kwargs[\"key\"] = key\n        args = build_update_args_for_command(**build_kwargs)  # type: ignore\n        try:\n            update_kwargs = {\n                \"Key\": key,\n                \"UpdateExpression\": args.update_expression,\n                \"ExpressionAttributeNames\": args.attribute_names,\n                \"ExpressionAttributeValues\": args.attribute_values,\n            }\n            if args.condition_expression:\n                update_kwargs[\"ConditionExpression\"] = args.condition_expression\n            self._table.update_item(**update_kwargs)\n            logger.info(\"Finished updating item\")\n        except Exception as ex:\n            code = get_error_code(ex)\n            if code == \"ConditionalCheckFailedException\":\n                raise RequestObjectStateError(\n                    f\"Object existence or version condition failed for: {str(key)}\"\n                ) from ex\n            raise\n\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> Optional[ObjT]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_id is None:\n            content_id = EMPTY_LIST\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_id\": content_id,\n            **self.context,\n        }\n        logger.info(\"Getting item from table by key\", extra=log_context)\n        response = self._table.get_item(\n            Key={\n                self._partition_key: self._partition_id(partition_id),\n                self._sort_key: self._content_id(content_id),\n            }\n        )\n        db_item = response.get(\"Item\")\n        if db_item:\n            logger.info(\"Found item from table by key\", extra=log_context)\n            item = self._item_class(**db_item)\n        else:\n            logger.info(\"No item found in table by key\", extra=log_context)\n            item = None\n        return item\n\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> List[ObjT]:\n        records: List[ObjT] = []\n        batch_number = 0\n        for request_id_batch in chunks(request_ids, size=100):\n            batch_number += 1\n            logger.info(\n                \"Starting batch get items request\",\n                extra={\n                    \"batch_number\": batch_number,\n                    \"batch_size\": len(request_id_batch),\n                },\n            )\n            batch_keys = [\n                {\n                    self._partition_key: self._partition_id(partition_id),\n                    self._sort_key: self._content_id(content_id),\n                }\n                for partition_id, content_id in request_id_batch\n            ]\n            batch_response: Dict[str, Any] = self._extend_batch(batch_keys, records)\n            while unprocessed_keys := batch_response.get(\"UnprocessedKeys\"):\n                logger.info(\n                    \"Getting unprocessed keys\",\n                    extra={\n                        \"batch_number\": batch_number,\n                        \"batch_size\": len(unprocessed_keys),\n                    },\n                )\n                batch_response = self._extend_batch(unprocessed_keys, records)\n        return records\n\n    def _extend_batch(self, request_keys: List[Dict[str, str]], records: List[ObjT]):\n        batch_response = self._resource.batch_get_item(\n            RequestItems={self._table_name: {\"Keys\": request_keys}}\n        )\n        records.extend(\n            (self._item_class(**i) for i in batch_response[\"Responses\"].get(self._table_name, []))\n        )\n        return batch_response\n\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content with prefix query\")\n        items = self._query_all_data(condition, sort_ascending, limit, filters)\n        yield from (self._item_class(**item) for item in items)\n\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_start\": content_start,\n            \"content_end\": content_end,\n            **self.context,\n        }\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_start is None:\n            content_start = EMPTY_LIST\n        if content_end is None:\n            content_end = EMPTY_LIST\n\n        if content_start == content_end:\n            logger.info(\n                \"Content start and end filters are equal. Deferring to list\",\n                extra=log_context,\n            )\n            yield from self.list(partition_id, content_start)\n        else:\n            partition_id = self._partition_id(partition_id)\n            sort_start = self._content_id(content_start)\n            sort_end = self._content_id(content_end)\n            condition = Key(self._partition_key).eq(partition_id) & Key(self._sort_key).between(\n                low_value=sort_start, high_value=sort_end\n            )\n            logger.info(\n                \"Starting query for content in range query\",\n                extra={\n                    \"partition_id\": partition_id,\n                    \"low_value\": sort_start,\n                    \"high_value\": sort_end,\n                    **log_context,\n                },\n            )\n            items = self._query_all_data(condition, sort_ascending, limit, filters)\n            yield from (self._item_class(**item) for item in items)\n\n    def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_prefix\": content_prefix,\n            **self.context,\n        }\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content to delete with prefix query\", extra=log_context)\n        items = self._query_all_data(condition)\n        with self._table.batch_writer() as writer:\n            for item in items:\n                writer.delete_item(\n                    Key={\n                        self._partition_key: item[self._partition_key],\n                        self._sort_key: item[self._sort_key],\n                    }\n                )\n        logger.info(\"Finished deleting content from prefix query\", extra=log_context)\n\n    def _partition_id(self, partition_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if partition_ids is None:\n            partition_ids = EMPTY_LIST\n        if isinstance(partition_ids, str):\n            return f\"{self._partition_prefix}#{self._partition_name}#{partition_ids}\"\n        else:\n            return f\"{self._partition_prefix}#{self._partition_name}#{'#'.join(partition_ids)}\"\n\n    def _content_id(self, content_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if content_ids is None:\n            content_ids = EMPTY_LIST\n        if isinstance(content_ids, str):\n            return f\"{self._content_type}#{content_ids}\"\n        else:\n            return f\"{self._content_type}#{'#'.join(content_ids)}\"\n\n    def _query_all_data(\n        self,\n        key_condition_expression: Key,\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterable[Dict]:\n        query_kwargs = {\n            \"KeyConditionExpression\": key_condition_expression,\n            \"ScanIndexForward\": sort_ascending,\n        }\n        if filters:\n            validate_filters_for_schema(self._item_schema, filters)\n            if filter_expression := build_filter_expression(filters):\n                query_kwargs[FILTER_EXPRESSION] = filter_expression\n        if limit and FILTER_EXPRESSION not in query_kwargs:\n            query_kwargs[\"Limit\"] = limit\n\n        response = self._table.query(**query_kwargs)\n        total_count = response[\"Count\"]\n        items = response.get(\"Items\", [])\n\n        yield from items\n\n        while last_evaluated_key := response.get(LAST_EVALUATED_KEY):\n            # TODO: Add tests for limit\n            if limit and total_count >= limit:\n                return\n            logger.info(\n                \"Getting next batch of items from content table\",\n                extra={\n                    \"last_evaluated_key\": last_evaluated_key,\n                },\n            )\n            query_kwargs[\"ExclusiveStartKey\"] = last_evaluated_key\n            response = self._table.query(**query_kwargs)\n            total_count += response[\"Count\"]\n            items = response.get(\"Items\", [])\n            yield from items", "class DynamoRepository(AbstractRepository[ObjT]):\n    @classmethod\n    def build(\n        cls,\n        table_name: str,\n        item_class: Type[ObjT],\n        partition_prefix: str,\n        partition_name: str,\n        content_type: str,\n    ) -> DynamoRepository[ObjT]:\n        resource = Session().resource(\"dynamodb\")\n        table = resource.Table(table_name)\n        return cls(\n            item_class=item_class,\n            partition_prefix=partition_prefix,\n            partition_name=partition_name,\n            content_type=content_type,\n            table_name=table_name,\n            partition_key=\"_table_item_id\",\n            sort_key=\"_table_content_id\",\n            table=table,\n            resource=resource,\n        )\n\n    def __init__(\n        self,\n        *,\n        item_class: Type[ObjT],\n        partition_prefix: str,\n        partition_name: str,\n        content_type: str,\n        table_name: str,\n        partition_key: str,\n        sort_key: str,\n        table,\n        resource,\n    ):\n        self._item_class = item_class\n        self._item_schema = self._item_class.schema()\n        self._partition_prefix = partition_prefix\n        self._partition_name = partition_name\n        self._content_type = content_type\n        self._table_name = table_name\n        self._partition_key = partition_key\n        self._sort_key = sort_key\n        self._table = table\n        self._resource = resource\n\n    @property\n    def context(self) -> Dict[str, str]:\n        return {\n            \"item_class\": self._item_class.__name__,\n            \"partition_prefix\": self._partition_id(EMPTY_LIST),\n            \"content_prefix\": self._content_id(EMPTY_LIST),\n        }\n\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        log_context: Dict[str, Any] = {\n            \"partition_id\": content.partition_ids,\n            \"content_id\": content.content_ids,\n            **self.context,\n        }\n        logger.info(\"Putting single content\", extra=log_context)\n        self._put_content(self._table, content)\n        logger.info(\"Put single content\", extra=log_context)\n\n    def put_batch(self, batch: Iterable[PartitionedContent[ObjT]]) -> None:\n        logger.info(\"Putting batch content\", extra=self.context)\n        count = 0\n        with self._table.batch_writer() as writer:\n            for content in batch:\n                self._put_content(writer, content)\n                count += 1\n        logger.info(\"Finished putting batch content\", extra={\"count\": count, **self.context})\n\n    def _put_content(self, table, content: PartitionedContent[ObjT]) -> None:\n        item_dict = clean_dict(content.item.dict())\n        item_dict[INTERNAL_OBJECT_VERSION] = 1\n        item_dict.update(**internal_timestamp())\n        put_item: Dict[str, Union[str, int]] = {\n            self._partition_key: self._partition_id(content.partition_ids),\n            self._sort_key: self._content_id(content.content_ids),\n            **item_dict,\n        }\n        if expiry := content.expiry:\n            put_item[INTERNAL_TTL] = int(expiry.timestamp())\n        table.put_item(Item=put_item)\n\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        validate_command_for_schema(self._item_schema, command)\n        key = {\n            self._partition_key: self._partition_id(partition_id),\n            self._sort_key: self._content_id(content_id),\n        }\n        build_kwargs: Dict[str, Any] = {\n            \"command\": command,\n        }\n        if require_exists:\n            build_kwargs[\"key\"] = key\n        args = build_update_args_for_command(**build_kwargs)  # type: ignore\n        try:\n            update_kwargs = {\n                \"Key\": key,\n                \"UpdateExpression\": args.update_expression,\n                \"ExpressionAttributeNames\": args.attribute_names,\n                \"ExpressionAttributeValues\": args.attribute_values,\n            }\n            if args.condition_expression:\n                update_kwargs[\"ConditionExpression\"] = args.condition_expression\n            self._table.update_item(**update_kwargs)\n            logger.info(\"Finished updating item\")\n        except Exception as ex:\n            code = get_error_code(ex)\n            if code == \"ConditionalCheckFailedException\":\n                raise RequestObjectStateError(\n                    f\"Object existence or version condition failed for: {str(key)}\"\n                ) from ex\n            raise\n\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> Optional[ObjT]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_id is None:\n            content_id = EMPTY_LIST\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_id\": content_id,\n            **self.context,\n        }\n        logger.info(\"Getting item from table by key\", extra=log_context)\n        response = self._table.get_item(\n            Key={\n                self._partition_key: self._partition_id(partition_id),\n                self._sort_key: self._content_id(content_id),\n            }\n        )\n        db_item = response.get(\"Item\")\n        if db_item:\n            logger.info(\"Found item from table by key\", extra=log_context)\n            item = self._item_class(**db_item)\n        else:\n            logger.info(\"No item found in table by key\", extra=log_context)\n            item = None\n        return item\n\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> List[ObjT]:\n        records: List[ObjT] = []\n        batch_number = 0\n        for request_id_batch in chunks(request_ids, size=100):\n            batch_number += 1\n            logger.info(\n                \"Starting batch get items request\",\n                extra={\n                    \"batch_number\": batch_number,\n                    \"batch_size\": len(request_id_batch),\n                },\n            )\n            batch_keys = [\n                {\n                    self._partition_key: self._partition_id(partition_id),\n                    self._sort_key: self._content_id(content_id),\n                }\n                for partition_id, content_id in request_id_batch\n            ]\n            batch_response: Dict[str, Any] = self._extend_batch(batch_keys, records)\n            while unprocessed_keys := batch_response.get(\"UnprocessedKeys\"):\n                logger.info(\n                    \"Getting unprocessed keys\",\n                    extra={\n                        \"batch_number\": batch_number,\n                        \"batch_size\": len(unprocessed_keys),\n                    },\n                )\n                batch_response = self._extend_batch(unprocessed_keys, records)\n        return records\n\n    def _extend_batch(self, request_keys: List[Dict[str, str]], records: List[ObjT]):\n        batch_response = self._resource.batch_get_item(\n            RequestItems={self._table_name: {\"Keys\": request_keys}}\n        )\n        records.extend(\n            (self._item_class(**i) for i in batch_response[\"Responses\"].get(self._table_name, []))\n        )\n        return batch_response\n\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content with prefix query\")\n        items = self._query_all_data(condition, sort_ascending, limit, filters)\n        yield from (self._item_class(**item) for item in items)\n\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[ObjT]:\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_start\": content_start,\n            \"content_end\": content_end,\n            **self.context,\n        }\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_start is None:\n            content_start = EMPTY_LIST\n        if content_end is None:\n            content_end = EMPTY_LIST\n\n        if content_start == content_end:\n            logger.info(\n                \"Content start and end filters are equal. Deferring to list\",\n                extra=log_context,\n            )\n            yield from self.list(partition_id, content_start)\n        else:\n            partition_id = self._partition_id(partition_id)\n            sort_start = self._content_id(content_start)\n            sort_end = self._content_id(content_end)\n            condition = Key(self._partition_key).eq(partition_id) & Key(self._sort_key).between(\n                low_value=sort_start, high_value=sort_end\n            )\n            logger.info(\n                \"Starting query for content in range query\",\n                extra={\n                    \"partition_id\": partition_id,\n                    \"low_value\": sort_start,\n                    \"high_value\": sort_end,\n                    **log_context,\n                },\n            )\n            items = self._query_all_data(condition, sort_ascending, limit, filters)\n            yield from (self._item_class(**item) for item in items)\n\n    def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_prefix\": content_prefix,\n            **self.context,\n        }\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content to delete with prefix query\", extra=log_context)\n        items = self._query_all_data(condition)\n        with self._table.batch_writer() as writer:\n            for item in items:\n                writer.delete_item(\n                    Key={\n                        self._partition_key: item[self._partition_key],\n                        self._sort_key: item[self._sort_key],\n                    }\n                )\n        logger.info(\"Finished deleting content from prefix query\", extra=log_context)\n\n    def _partition_id(self, partition_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if partition_ids is None:\n            partition_ids = EMPTY_LIST\n        if isinstance(partition_ids, str):\n            return f\"{self._partition_prefix}#{self._partition_name}#{partition_ids}\"\n        else:\n            return f\"{self._partition_prefix}#{self._partition_name}#{'#'.join(partition_ids)}\"\n\n    def _content_id(self, content_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if content_ids is None:\n            content_ids = EMPTY_LIST\n        if isinstance(content_ids, str):\n            return f\"{self._content_type}#{content_ids}\"\n        else:\n            return f\"{self._content_type}#{'#'.join(content_ids)}\"\n\n    def _query_all_data(\n        self,\n        key_condition_expression: Key,\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterable[Dict]:\n        query_kwargs = {\n            \"KeyConditionExpression\": key_condition_expression,\n            \"ScanIndexForward\": sort_ascending,\n        }\n        if filters:\n            validate_filters_for_schema(self._item_schema, filters)\n            if filter_expression := build_filter_expression(filters):\n                query_kwargs[FILTER_EXPRESSION] = filter_expression\n        if limit and FILTER_EXPRESSION not in query_kwargs:\n            query_kwargs[\"Limit\"] = limit\n\n        response = self._table.query(**query_kwargs)\n        total_count = response[\"Count\"]\n        items = response.get(\"Items\", [])\n\n        yield from items\n\n        while last_evaluated_key := response.get(LAST_EVALUATED_KEY):\n            # TODO: Add tests for limit\n            if limit and total_count >= limit:\n                return\n            logger.info(\n                \"Getting next batch of items from content table\",\n                extra={\n                    \"last_evaluated_key\": last_evaluated_key,\n                },\n            )\n            query_kwargs[\"ExclusiveStartKey\"] = last_evaluated_key\n            response = self._table.query(**query_kwargs)\n            total_count += response[\"Count\"]\n            items = response.get(\"Items\", [])\n            yield from items", ""]}
{"filename": "pydantic_dynamo/exceptions.py", "chunked_list": ["class RequestObjectStateError(Exception):\n    pass\n"]}
{"filename": "pydantic_dynamo/v2/models.py", "chunked_list": ["from abc import abstractmethod\nfrom contextlib import AbstractContextManager, AbstractAsyncContextManager\nfrom typing import (\n    Generic,\n    Optional,\n    Iterable,\n    Sequence,\n    Tuple,\n    AsyncIterator,\n    Iterator,", "    AsyncIterator,\n    Iterator,\n)\n\nfrom pydantic.generics import GenericModel\n\nfrom pydantic_dynamo.models import ObjT, PartitionedContent, FilterCommand, UpdateCommand\n\n\nclass GetResponse(GenericModel, Generic[ObjT]):\n    content: Optional[PartitionedContent[ObjT]]", "\nclass GetResponse(GenericModel, Generic[ObjT]):\n    content: Optional[PartitionedContent[ObjT]]\n\n\nclass BatchResponse(GenericModel, Generic[ObjT]):\n    contents: Iterable[PartitionedContent[ObjT]]\n\n\nclass ReadOnlyAbstractRepository(AbstractAsyncContextManager, Generic[ObjT]):\n    @abstractmethod\n    async def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        pass\n\n    @abstractmethod\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass", "\nclass ReadOnlyAbstractRepository(AbstractAsyncContextManager, Generic[ObjT]):\n    @abstractmethod\n    async def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        pass\n\n    @abstractmethod\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        pass", "\n\nclass AbstractRepository(ReadOnlyAbstractRepository[ObjT]):\n    @abstractmethod\n    async def put(self, content: PartitionedContent[ObjT]) -> None:\n        pass\n\n    @abstractmethod\n    async def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n        pass\n\n    @abstractmethod\n    async def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    async def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        pass", "\n\nclass SyncReadOnlyAbstractRepository(AbstractContextManager, Generic[ObjT]):\n    @abstractmethod\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        pass\n\n    @abstractmethod\n    def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> Iterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        pass\n\n    @abstractmethod\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        pass", "\n\nclass SyncAbstractRepository(SyncReadOnlyAbstractRepository[ObjT]):\n    @abstractmethod\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        pass\n\n    @abstractmethod\n    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n        pass\n\n    @abstractmethod\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        pass", ""]}
{"filename": "pydantic_dynamo/v2/__init__.py", "chunked_list": [""]}
{"filename": "pydantic_dynamo/v2/write_once.py", "chunked_list": ["import logging\nfrom collections import defaultdict\nfrom typing import Generic, Iterable, List, Dict, Sequence, AsyncIterable, Union\n\nfrom pydantic_dynamo.models import ObjT, PartitionedContent\nfrom pydantic_dynamo.v2.models import AbstractRepository\n\nlogger = logging.getLogger(__name__)\n\n\nclass WriteOnceRepository(Generic[ObjT]):\n    def __init__(self, async_repo: AbstractRepository[ObjT]):\n        self._async_repo = async_repo\n\n    async def write(\n        self,\n        input_contents: Union[\n            Iterable[PartitionedContent[ObjT]], AsyncIterable[PartitionedContent[ObjT]]\n        ],\n    ) -> List[PartitionedContent[ObjT]]:\n        \"\"\"\n        :param input_contents: contents to write if, they do not exist\n            or are not identical to existing key's content\n        :return: list of contents actually written after checking existing data\n        \"\"\"\n        partitioned_lists: Dict[Sequence[str], List[PartitionedContent[ObjT]]] = defaultdict(list)\n        if isinstance(input_contents, AsyncIterable):\n            async for input_content in input_contents:\n                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n        else:\n            for input_content in input_contents:\n                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n\n        if len(partitioned_lists) == 0:\n            logger.info(\"Empty input content to save\")\n            return []\n\n        new_contents: List[PartitionedContent[ObjT]] = []\n        for partition_key, contents in partitioned_lists.items():\n            contents.sort()\n            content_start = contents[0]\n            content_end = contents[-1]\n            existing_map = {\n                tuple(existing.content_ids): existing\n                async for response in self._async_repo.list_between(\n                    list(partition_key), content_start.content_ids, content_end.content_ids\n                )\n                for existing in response.contents\n            }\n            for input_content in contents:\n                existing_item = existing_map.get(tuple(input_content.content_ids))\n                if existing_item is None or existing_item != input_content:\n                    new_contents.append(input_content)\n        if new_contents:\n            logger.info(\n                \"New contents found to save\",\n                extra={\n                    \"new_count\": len(new_contents),\n                },\n            )\n            await self._async_repo.put_batch(new_contents)\n        else:\n            logger.info(\"No new input content found to save\")\n        return new_contents", "\n\nclass WriteOnceRepository(Generic[ObjT]):\n    def __init__(self, async_repo: AbstractRepository[ObjT]):\n        self._async_repo = async_repo\n\n    async def write(\n        self,\n        input_contents: Union[\n            Iterable[PartitionedContent[ObjT]], AsyncIterable[PartitionedContent[ObjT]]\n        ],\n    ) -> List[PartitionedContent[ObjT]]:\n        \"\"\"\n        :param input_contents: contents to write if, they do not exist\n            or are not identical to existing key's content\n        :return: list of contents actually written after checking existing data\n        \"\"\"\n        partitioned_lists: Dict[Sequence[str], List[PartitionedContent[ObjT]]] = defaultdict(list)\n        if isinstance(input_contents, AsyncIterable):\n            async for input_content in input_contents:\n                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n        else:\n            for input_content in input_contents:\n                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n\n        if len(partitioned_lists) == 0:\n            logger.info(\"Empty input content to save\")\n            return []\n\n        new_contents: List[PartitionedContent[ObjT]] = []\n        for partition_key, contents in partitioned_lists.items():\n            contents.sort()\n            content_start = contents[0]\n            content_end = contents[-1]\n            existing_map = {\n                tuple(existing.content_ids): existing\n                async for response in self._async_repo.list_between(\n                    list(partition_key), content_start.content_ids, content_end.content_ids\n                )\n                for existing in response.contents\n            }\n            for input_content in contents:\n                existing_item = existing_map.get(tuple(input_content.content_ids))\n                if existing_item is None or existing_item != input_content:\n                    new_contents.append(input_content)\n        if new_contents:\n            logger.info(\n                \"New contents found to save\",\n                extra={\n                    \"new_count\": len(new_contents),\n                },\n            )\n            await self._async_repo.put_batch(new_contents)\n        else:\n            logger.info(\"No new input content found to save\")\n        return new_contents", ""]}
{"filename": "pydantic_dynamo/v2/repository.py", "chunked_list": ["from __future__ import annotations\n\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import (\n    Type,\n    Dict,\n    List,\n    Any,\n    Iterable,", "    Any,\n    Iterable,\n    Union,\n    Optional,\n    Sequence,\n    Tuple,\n    AsyncGenerator,\n    AsyncIterator,\n)\n", ")\n\nfrom boto3.dynamodb.conditions import Key\n\nfrom pydantic_dynamo.constants import (\n    EMPTY_LIST,\n    INTERNAL_OBJECT_VERSION,\n    INTERNAL_TTL,\n    FILTER_EXPRESSION,\n    LAST_EVALUATED_KEY,", "    FILTER_EXPRESSION,\n    LAST_EVALUATED_KEY,\n)\nfrom pydantic_dynamo.models import ObjT, PartitionedContent, UpdateCommand, FilterCommand\nfrom pydantic_dynamo.utils import (\n    clean_dict,\n    internal_timestamp,\n    validate_command_for_schema,\n    chunks,\n    validate_filters_for_schema,", "    chunks,\n    validate_filters_for_schema,\n    build_filter_expression,\n    build_update_args_for_command,\n    execute_update_item,\n)\nfrom pydantic_dynamo.v2.models import AbstractRepository, GetResponse, BatchResponse\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass DynamoRepository(AbstractRepository[ObjT]):\n    def __init__(\n        self,\n        *,\n        item_class: Type[ObjT],\n        partition_prefix: str,\n        partition_name: str,\n        content_type: str,\n        table_name: str,\n        partition_key: str,\n        sort_key: str,\n        table,\n        resource,\n        consistent_reads: bool = False,\n    ):\n        \"\"\"\n\n        :param item_class: pydantic model class reference representing a single\n            database record. This should match ObjT\n        :param partition_prefix: first part of the concatenated value used in the\n            partition key value.\n            Shared amongst all records saved and read by this repository.\n        :param partition_name: second part of the concatenated value used in the\n            partition key value.\n            Shared amongst all records saved and read by this repository.\n        :param content_type: first part of the concatenated value used in the sort key value.\n            Shared amongst all records saved and read by this repository.\n        :param table_name: the region-specific name of the DynamoDB table\n        :param partition_key: the name of the attribute defined as the table's partition key\n        :param sort_key: the name of the attribute defined as the table's sort key\n        :param table: instance of the Table service object\n            from `await resource.Table(table_name)`\n        :param resource: instance of the resource service object\n            from `async with aioboto3.session.Session().resource(**kwargs)`\n        :param consistent_reads: boolean to determine if read operations should be performed\n            with strong consistency. Default is False and uses eventual consistency\n        \"\"\"\n        self._item_class = item_class\n        self._item_schema = self._item_class.schema()\n        self._partition_prefix = partition_prefix\n        self._partition_name = partition_name\n        self._content_type = content_type\n        self._table_name = table_name\n        self._partition_key = partition_key\n        self._sort_key = sort_key\n        self._table = table\n        self._resource = resource\n        self._consistent_reads = consistent_reads\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    @property\n    def context(self) -> Dict[str, str]:\n        return {\n            \"item_class\": self._item_class.__name__,\n            \"partition_prefix\": self._partition_id(EMPTY_LIST),\n            \"content_prefix\": self._content_id(EMPTY_LIST),\n        }\n\n    async def put(self, content: PartitionedContent[ObjT]) -> None:\n        log_context: Dict[str, Any] = {\n            \"partition_id\": content.partition_ids,\n            \"content_id\": content.content_ids,\n            **self.context,\n        }\n        logger.info(\"Putting single content\", extra=log_context)\n        await self._put_content(self._table, content)\n        logger.info(\"Put single content\", extra=log_context)\n\n    async def put_batch(self, batch: Iterable[PartitionedContent[ObjT]]) -> None:\n        logger.info(\"Putting batch content\", extra=self.context)\n        count = 0\n        async with self._table.batch_writer() as writer:\n            for content in batch:\n                await self._put_content(writer, content)\n                count += 1\n        logger.info(\"Finished putting batch content\", extra={\"count\": count, **self.context})\n\n    async def _put_content(self, table, content: PartitionedContent[ObjT]) -> None:\n        item_dict = clean_dict(content.item.dict())\n        item_dict[INTERNAL_OBJECT_VERSION] = content.current_version\n        item_dict.update(**internal_timestamp())\n        put_item: Dict[str, Union[str, int]] = {\n            self._partition_key: self._partition_id(content.partition_ids),\n            self._sort_key: self._content_id(content.content_ids),\n            **item_dict,\n        }\n        if expiry := content.expiry:\n            put_item[INTERNAL_TTL] = int(expiry.timestamp())\n        await table.put_item(Item=put_item)\n\n    async def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        validate_command_for_schema(self._item_schema, command)\n        key = {\n            self._partition_key: self._partition_id(partition_id),\n            self._sort_key: self._content_id(content_id),\n        }\n        build_kwargs: Dict[str, Any] = {\n            \"command\": command,\n        }\n        if require_exists:\n            build_kwargs[\"key\"] = key\n        args = build_update_args_for_command(**build_kwargs)  # type: ignore\n        await execute_update_item(self._table, key, args)\n\n    async def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_id is None:\n            content_id = EMPTY_LIST\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_id\": content_id,\n            **self.context,\n        }\n        logger.info(\"Getting item from table by key\", extra=log_context)\n        response = await self._table.get_item(\n            Key={\n                self._partition_key: self._partition_id(partition_id),\n                self._sort_key: self._content_id(content_id),\n            },\n            ConsistentRead=self._consistent_reads,\n        )\n        db_item = response.get(\"Item\")\n        if db_item:\n            logger.info(\"Found item from table by key\", extra=log_context)\n            content = self._db_item_to_object(db_item)\n        else:\n            logger.info(\"No item found in table by key\", extra=log_context)\n            content = None\n        return GetResponse(content=content)\n\n    async def get_batch(\n        self,\n        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        batch_number = 0\n        for request_id_batch in chunks(request_ids, size=100):\n            batch_number += 1\n            logger.info(\n                \"Starting batch get items request\",\n                extra={\n                    \"batch_number\": batch_number,\n                    \"batch_size\": len(request_id_batch),\n                },\n            )\n            batch_keys = [\n                {\n                    self._partition_key: self._partition_id(partition_id),\n                    self._sort_key: self._content_id(content_id),\n                }\n                for partition_id, content_id in request_id_batch\n            ]\n            batch_response = await self._resource.batch_get_item(\n                RequestItems={\n                    self._table_name: {\"Keys\": batch_keys, \"ConsistentRead\": self._consistent_reads}\n                }\n            )\n            yield BatchResponse(\n                contents=(\n                    self._db_item_to_object(db_item)\n                    for db_item in batch_response[\"Responses\"].get(self._table_name, [])\n                )\n            )\n            while unprocessed_keys := batch_response.get(\"UnprocessedKeys\"):\n                logger.info(\n                    \"Getting unprocessed keys\",\n                    extra={\n                        \"batch_number\": batch_number,\n                        \"batch_size\": len(unprocessed_keys),\n                    },\n                )\n                batch_response = await self._resource.batch_get_item(\n                    RequestItems={\n                        self._table_name: {\n                            \"Keys\": unprocessed_keys,\n                            \"ConsistentRead\": self._consistent_reads,\n                        }\n                    }\n                )\n                yield BatchResponse(\n                    contents=(\n                        self._db_item_to_object(db_item)\n                        for db_item in batch_response[\"Responses\"].get(self._table_name, [])\n                    )\n                )\n\n    async def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content with prefix query\")\n        contents = [\n            self._db_item_to_object(db_item)\n            async for items in self._query_all_data(condition, sort_ascending, limit, filters)\n            for db_item in items\n        ]\n        yield BatchResponse(contents=contents)\n\n    async def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> AsyncIterator[BatchResponse[ObjT]]:\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_start\": content_start,\n            \"content_end\": content_end,\n            **self.context,\n        }\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_start is None:\n            content_start = EMPTY_LIST\n        if content_end is None:\n            content_end = EMPTY_LIST\n\n        if content_start == content_end:\n            logger.info(\n                \"Content start and end filters are equal. Deferring to list\",\n                extra=log_context,\n            )\n            async for response in self.list(partition_id, content_start):\n                yield response\n        else:\n            partition_id = self._partition_id(partition_id)\n            sort_start = self._content_id(content_start)\n            sort_end = self._content_id(content_end)\n            condition = Key(self._partition_key).eq(partition_id) & Key(self._sort_key).between(\n                low_value=sort_start, high_value=sort_end\n            )\n            logger.info(\n                \"Starting query for content in range query\",\n                extra={\n                    \"partition_id\": partition_id,\n                    \"low_value\": sort_start,\n                    \"high_value\": sort_end,\n                    **log_context,\n                },\n            )\n            contents = [\n                self._db_item_to_object(db_item)\n                async for items in self._query_all_data(condition, sort_ascending, limit, filters)\n                for db_item in items\n            ]\n            yield BatchResponse(contents=contents)\n\n    def _db_item_to_object(self, db_item: Dict[str, Any]) -> PartitionedContent[ObjT]:\n        expiry: Optional[datetime] = None\n        if db_expiry := db_item.pop(INTERNAL_TTL, None):\n            expiry = datetime.fromtimestamp(int(db_expiry), tz=timezone.utc)\n\n        item_kwargs = {\n            \"partition_ids\": db_item.pop(self._partition_key)\n            .replace(self._partition_id(EMPTY_LIST), \"\", 1)\n            .split(\"#\"),\n            \"content_ids\": db_item.pop(self._sort_key)\n            .replace(self._content_id(EMPTY_LIST), \"\", 1)\n            .split(\"#\"),\n            \"item\": self._item_class(**db_item),\n            \"expiry\": expiry,\n        }\n        if version := db_item.pop(INTERNAL_OBJECT_VERSION, None):\n            item_kwargs[\"current_version\"] = version\n        return PartitionedContent[self._item_class](**item_kwargs)  # type: ignore[name-defined]\n\n    async def delete(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n    ) -> None:\n        if partition_id is None:\n            partition_id = EMPTY_LIST\n        if content_prefix is None:\n            content_prefix = EMPTY_LIST\n\n        log_context = {\n            \"partition_id\": partition_id,\n            \"content_prefix\": content_prefix,\n            **self.context,\n        }\n        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n            self._sort_key\n        ).begins_with(self._content_id(content_prefix))\n        logger.info(\"Starting query for content to delete with prefix query\", extra=log_context)\n        responses = self._query_all_data(\n            condition, select_fields=(self._partition_key, self._sort_key)\n        )\n        async with self._table.batch_writer() as writer:\n            async for items in responses:\n                for item in items:\n                    await writer.delete_item(\n                        Key={\n                            self._partition_key: item[self._partition_key],\n                            self._sort_key: item[self._sort_key],\n                        }\n                    )\n        logger.info(\"Finished deleting content from prefix query\", extra=log_context)\n\n    def _partition_id(self, partition_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if partition_ids is None:\n            partition_ids = EMPTY_LIST\n        if isinstance(partition_ids, str):\n            return f\"{self._partition_prefix}#{self._partition_name}#{partition_ids}\"\n        else:\n            return f\"{self._partition_prefix}#{self._partition_name}#{'#'.join(partition_ids)}\"\n\n    def _content_id(self, content_ids: Optional[Union[str, Sequence[str]]]) -> str:\n        if content_ids is None:\n            content_ids = EMPTY_LIST\n        if isinstance(content_ids, str):\n            return f\"{self._content_type}#{content_ids}\"\n        else:\n            return f\"{self._content_type}#{'#'.join(content_ids)}\"\n\n    async def _query_all_data(\n        self,\n        key_condition_expression: Key,\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n        select_fields: Optional[Sequence[str]] = None,\n    ) -> AsyncGenerator[List[Dict], None]:\n        query_kwargs = {\n            \"KeyConditionExpression\": key_condition_expression,\n            \"ScanIndexForward\": sort_ascending,\n            \"ConsistentRead\": self._consistent_reads,\n        }\n        if filters:\n            validate_filters_for_schema(self._item_schema, filters)\n            if filter_expression := build_filter_expression(filters):\n                query_kwargs[FILTER_EXPRESSION] = filter_expression\n        if limit and FILTER_EXPRESSION not in query_kwargs:\n            query_kwargs[\"Limit\"] = limit\n        if select_fields:\n            expression_attribute_names = {\n                f\"#att{i}\": field for i, field in enumerate(select_fields)\n            }\n            query_kwargs.update(\n                **{\n                    \"Select\": \"SPECIFIC_ATTRIBUTES\",\n                    \"ProjectionExpression\": \", \".join(expression_attribute_names.keys()),\n                    \"ExpressionAttributeNames\": expression_attribute_names,\n                }\n            )\n\n        response = await self._table.query(**query_kwargs)\n        total_count = response[\"Count\"]\n        items = response.get(\"Items\", [])\n\n        yield items\n\n        while last_evaluated_key := response.get(LAST_EVALUATED_KEY):\n            # TODO: Add tests for limit\n            if limit and total_count >= limit:\n                return\n            logger.info(\n                \"Getting next batch of items from content table\",\n                extra={\n                    \"last_evaluated_key\": last_evaluated_key,\n                },\n            )\n            query_kwargs[\"ExclusiveStartKey\"] = last_evaluated_key\n            response = await self._table.query(**query_kwargs)\n            total_count += response[\"Count\"]\n            items = response.get(\"Items\", [])\n            yield items", ""]}
{"filename": "pydantic_dynamo/v2/sync_repository.py", "chunked_list": ["import asyncio\nfrom asyncio import AbstractEventLoop\nfrom typing import Optional, Sequence, Iterable, Tuple, AsyncIterable, TypeVar, Iterator\n\nfrom pydantic_dynamo.models import ObjT, FilterCommand, UpdateCommand, PartitionedContent\nfrom pydantic_dynamo.v2.models import (\n    SyncAbstractRepository,\n    BatchResponse,\n    GetResponse,\n    AbstractRepository,", "    GetResponse,\n    AbstractRepository,\n)\n\n\nOutput = TypeVar(\"Output\")\n\n\nclass SyncDynamoRepository(SyncAbstractRepository[ObjT]):\n    def __init__(self, async_repo: AbstractRepository[ObjT]):\n        self._async_repo = async_repo\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.put(content))\n\n    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.put_batch(content))\n\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(\n            self._async_repo.update(partition_id, content_id, command, require_exists)\n        )\n\n    def delete(\n        self, partition_id: Optional[Sequence[str]], content_prefix: Optional[Sequence[str]]\n    ) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.delete(partition_id, content_prefix))\n\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(self._async_repo.get(partition_id, content_id))\n\n    def get_batch(\n        self, request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]]\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(self._async_repo.get_batch(request_ids), loop)\n\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(\n            self._async_repo.list(partition_id, content_prefix, sort_ascending, limit, filters),\n            loop,\n        )\n\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(\n            self._async_repo.list_between(\n                partition_id, content_start, content_end, sort_ascending, limit, filters\n            ),\n            loop,\n        )", "class SyncDynamoRepository(SyncAbstractRepository[ObjT]):\n    def __init__(self, async_repo: AbstractRepository[ObjT]):\n        self._async_repo = async_repo\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n    def put(self, content: PartitionedContent[ObjT]) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.put(content))\n\n    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.put_batch(content))\n\n    def update(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_id: Optional[Sequence[str]],\n        command: UpdateCommand,\n        require_exists: bool = True,\n    ) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(\n            self._async_repo.update(partition_id, content_id, command, require_exists)\n        )\n\n    def delete(\n        self, partition_id: Optional[Sequence[str]], content_prefix: Optional[Sequence[str]]\n    ) -> None:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(self._async_repo.delete(partition_id, content_prefix))\n\n    def get(\n        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n    ) -> GetResponse[ObjT]:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(self._async_repo.get(partition_id, content_id))\n\n    def get_batch(\n        self, request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]]\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(self._async_repo.get_batch(request_ids), loop)\n\n    def list(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_prefix: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(\n            self._async_repo.list(partition_id, content_prefix, sort_ascending, limit, filters),\n            loop,\n        )\n\n    def list_between(\n        self,\n        partition_id: Optional[Sequence[str]],\n        content_start: Optional[Sequence[str]],\n        content_end: Optional[Sequence[str]],\n        sort_ascending: bool = True,\n        limit: Optional[int] = None,\n        filters: Optional[FilterCommand] = None,\n    ) -> Iterator[BatchResponse[ObjT]]:\n        loop = asyncio.get_event_loop()\n        return iter_over_async(\n            self._async_repo.list_between(\n                partition_id, content_start, content_end, sort_ascending, limit, filters\n            ),\n            loop,\n        )", "\n\ndef iter_over_async(\n    async_iterable: AsyncIterable[Output], loop: AbstractEventLoop\n) -> Iterator[Output]:\n    async_iterator = async_iterable.__aiter__()\n\n    async def get_next():\n        try:\n            next_obj = await async_iterator.__anext__()\n            return False, next_obj\n        except StopAsyncIteration:\n            return True, None\n\n    while True:\n        done, next_obj = loop.run_until_complete(get_next())\n        if done:\n            break\n        yield next_obj", ""]}
