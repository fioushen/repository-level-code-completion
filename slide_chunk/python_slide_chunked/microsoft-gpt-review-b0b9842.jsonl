{"filename": "tests/test_llama_index.py", "chunked_list": ["\"\"\"Tests for the Llame Index Package.\"\"\"\nimport pytest\n\nfrom gpt_review._ask import _load_azure_openai_context\nfrom gpt_review._llama_index import _query_index\n\n\ndef ask_doc_test() -> None:\n    _load_azure_openai_context()\n    question = \"What is the name of the package?\"\n    files = [\"src/gpt_review/__init__.py\"]\n    _query_index(question, files, fast=True)\n\n    # Try again to use the cached index\n    _query_index(question, files, fast=True)", "\n\ndef test_ask_doc(mock_openai) -> None:\n    \"\"\"Unit Test for the ask_doc function.\"\"\"\n    ask_doc_test()\n\n\n@pytest.mark.integration\ndef test_int_ask_doc() -> None:\n    \"\"\"Integration Test for the ask_doc function.\"\"\"\n    ask_doc_test()", "def test_int_ask_doc() -> None:\n    \"\"\"Integration Test for the ask_doc function.\"\"\"\n    ask_doc_test()\n"]}
{"filename": "tests/test_openai.py", "chunked_list": ["\"\"\"Tests for the Open AI Wrapper.\"\"\"\nimport pytest\nfrom openai.error import RateLimitError\n\nimport gpt_review.constants as C\nfrom gpt_review._openai import _call_gpt, _get_model\nfrom gpt_review.context import _load_azure_openai_context\n\n\ndef get_model_test() -> None:\n    prompt = \"This is a test prompt\"\n\n    context = _load_azure_openai_context()\n\n    model = _get_model(prompt=prompt, max_tokens=1000, fast=True)\n    assert model == context.turbo_llm_model_deployment_id\n\n    model = _get_model(prompt=prompt, max_tokens=5000)\n    assert model == context.smart_llm_model_deployment_id\n\n    model = _get_model(prompt=prompt, max_tokens=9000)\n    assert model == context.large_llm_model_deployment_id", "\ndef get_model_test() -> None:\n    prompt = \"This is a test prompt\"\n\n    context = _load_azure_openai_context()\n\n    model = _get_model(prompt=prompt, max_tokens=1000, fast=True)\n    assert model == context.turbo_llm_model_deployment_id\n\n    model = _get_model(prompt=prompt, max_tokens=5000)\n    assert model == context.smart_llm_model_deployment_id\n\n    model = _get_model(prompt=prompt, max_tokens=9000)\n    assert model == context.large_llm_model_deployment_id", "\n\ndef test_get_model() -> None:\n    get_model_test()\n\n\n@pytest.mark.integration\ndef test_int_get_model() -> None:\n    get_model_test()\n", "\n\ndef rate_limit_test(monkeypatch):\n    def mock_get_model(prompt: str, max_tokens: int, fast: bool = False, large: bool = False):\n        error = RateLimitError(\"Rate Limit Error\")\n        error.headers[\"Retry-After\"] = 10\n        raise error\n\n    monkeypatch.setattr(\"gpt_review._openai._get_model\", mock_get_model)\n    with pytest.raises(RateLimitError):\n        _call_gpt(prompt=\"This is a test prompt\", retry=C.MAX_RETRIES)", "\n\ndef test_rate_limit(monkeypatch) -> None:\n    rate_limit_test(monkeypatch)\n\n\n@pytest.mark.integration\ndef test_int_rate_limit(monkeypatch) -> None:\n    rate_limit_test(monkeypatch)\n", ""]}
{"filename": "tests/test_gpt_cli.py", "chunked_list": ["\"\"\"Pytest for gpt_review/main.py\"\"\"\nimport os\nimport subprocess\nimport sys\nfrom dataclasses import dataclass\n\nimport pytest\n\nimport gpt_review.constants as C\nfrom gpt_review._gpt_cli import cli", "import gpt_review.constants as C\nfrom gpt_review._gpt_cli import cli\n\n\n@dataclass\nclass CLICase:\n    command: str\n    expected_error_message: str = \"\"\n    expected_error_code: int = 0\n", "\n\n@dataclass\nclass CLICase1(CLICase):\n    expected_error_code: int = 1\n\n\n@dataclass\nclass CLICase2(CLICase):\n    expected_error_code: int = 2", "class CLICase2(CLICase):\n    expected_error_code: int = 2\n\n\nSAMPLE_FILE = \"src/gpt_review/__init__.py\"\nQUESTION = \"how are you\"\nWHAT_LANGUAGE = \"'what programming language is this code written in?'\"\nHELP_TEXT = \"\"\"usage: gpt ask [-h] [--verbose] [--debug] [--only-show-errors]\n               [--output {json,jsonc,yaml,yamlc,table,tsv,none}]\n               [--query JMESPATH] [--max-tokens MAX_TOKENS]", "               [--output {json,jsonc,yaml,yamlc,table,tsv,none}]\n               [--query JMESPATH] [--max-tokens MAX_TOKENS]\n               [--temperature TEMPERATURE] [--top-p TOP_P]\n               [--frequency-penalty FREQUENCY_PENALTY]\n               [--presence-penalty PRESENCE_PENALTY]\n               <QUESTION> [<QUESTION> ...]\n\"\"\"\n\nROOT_COMMANDS = [\n    CLICase(\"--version\"),", "ROOT_COMMANDS = [\n    CLICase(\"--version\"),\n    CLICase(\"--help\"),\n]\n\nASK_COMMANDS = [\n    CLICase(\"ask --help\"),\n    CLICase(f\"ask {QUESTION}\"),\n    CLICase(f\"ask --fast {QUESTION}\"),\n    CLICase(", "    CLICase(f\"ask --fast {QUESTION}\"),\n    CLICase(\n        f\"ask {QUESTION} --fast --max-tokens {C.MAX_TOKENS_DEFAULT} --temperature {C.TEMPERATURE_DEFAULT} --top-p {C.TOP_P_DEFAULT} --frequency-penalty {C.FREQUENCY_PENALTY_DEFAULT} --presence-penalty {C.FREQUENCY_PENALTY_MAX}\"\n    ),\n    CLICase1(\n        f\"ask {QUESTION} --fast --max-tokens {C.MAX_TOKENS_MIN-1}\",\n        f\"ERROR: --max-tokens must be a(n) int between {C.MAX_TOKENS_MIN} and {C.MAX_TOKENS_MAX}\\n\",\n    ),\n    CLICase1(\n        f\"ask {QUESTION} --temperature {C.TEMPERATURE_MAX+8}\",", "    CLICase1(\n        f\"ask {QUESTION} --temperature {C.TEMPERATURE_MAX+8}\",\n        f\"ERROR: --temperature must be a(n) float between {C.TEMPERATURE_MIN} and {C.TEMPERATURE_MAX}\\n\",\n    ),\n    CLICase1(\n        f\"ask {QUESTION} --top-p {C.TOP_P_MAX+3.5}\",\n        f\"ERROR: --top-p must be a(n) float between {C.TOP_P_MIN} and {C.TOP_P_MAX}\\n\",\n    ),\n    CLICase1(\n        f\"ask {QUESTION} --frequency-penalty {C.FREQUENCY_PENALTY_MAX+2}\",", "    CLICase1(\n        f\"ask {QUESTION} --frequency-penalty {C.FREQUENCY_PENALTY_MAX+2}\",\n        f\"ERROR: --frequency-penalty must be a(n) float between {C.FREQUENCY_PENALTY_MIN} and {C.FREQUENCY_PENALTY_MAX}\\n\",\n    ),\n    CLICase1(\n        f\"ask {QUESTION} --presence-penalty {C.PRESENCE_PENALTY_MAX+7.7}\",\n        f\"ERROR: --presence-penalty must be a(n) float between {C.PRESENCE_PENALTY_MIN} and {C.PRESENCE_PENALTY_MAX}\\n\",\n    ),\n    CLICase2(\n        f\"ask {QUESTION} --fast --max-tokens\",", "    CLICase2(\n        f\"ask {QUESTION} --fast --max-tokens\",\n        f\"\"\"{HELP_TEXT}\ngpt ask: error: argument --max-tokens: expected one argument\n\"\"\",\n    ),\n    CLICase2(\n        f\"ask {QUESTION} --fast --max-tokens 'test'\",\n        f\"\"\"{HELP_TEXT}\ngpt ask: error: argument --max-tokens: invalid int value: \\\"'test'\\\"", "        f\"\"\"{HELP_TEXT}\ngpt ask: error: argument --max-tokens: invalid int value: \\\"'test'\\\"\n\"\"\",\n    ),\n    CLICase(f\"ask --files {SAMPLE_FILE} --files {SAMPLE_FILE} {WHAT_LANGUAGE} --reset\"),\n    CLICase(f\"ask --fast -f {SAMPLE_FILE} {WHAT_LANGUAGE}\"),\n    CLICase(f\"ask --fast -d src/gpt_review --reset --recursive --hidden --required-exts .py {WHAT_LANGUAGE}\"),\n    CLICase(f\"ask --fast -repo microsoft/gpt-review --branch main {WHAT_LANGUAGE}\"),\n]\n", "]\n\nGITHUB_COMMANDS = [\n    CLICase(\"github review --help\"),\n    CLICase(\"github review\"),\n]\n\nGIT_COMMANDS = [\n    CLICase(\"git commit --help\"),\n    # CLICase(\"git commit\"),", "    CLICase(\"git commit --help\"),\n    # CLICase(\"git commit\"),\n    # CLICase(\"git commit --large\"),\n    # CLICase(\"git commit --gpt4\"),\n    # CLICase(\"git commit --push\"),\n]\n\nREVIEW_COMMANDS = [\n    CLICase(\"review --help\"),\n    CLICase(\"review diff --help\"),", "    CLICase(\"review --help\"),\n    CLICase(\"review diff --help\"),\n    CLICase(\"review diff --diff tests/mock.diff --config tests/config.summary.test.yml\"),\n    CLICase(\"review diff --diff tests/mock.diff --config tests/config.summary.extra.yml\"),\n]\n\nARGS = ROOT_COMMANDS + ASK_COMMANDS + GIT_COMMANDS + GITHUB_COMMANDS + REVIEW_COMMANDS\nARGS_DICT = {arg.command: arg for arg in ARGS}\n\nMODULE_COMMANDS = [", "\nMODULE_COMMANDS = [\n    CLICase(\"python -m gpt --version\"),\n    CLICase(\"python -m gpt_review --version\"),\n]\nMODULE_DICT = {arg.command: arg for arg in MODULE_COMMANDS}\n\n\ndef gpt_cli_test(command: CLICase) -> None:\n    os.environ[\"GPT_ASK_COMMANDS\"] = \"1\"\n\n    sys.argv[1:] = command.command.split(\" \")\n    exit_code = -1\n    try:\n        exit_code = cli()\n    except SystemExit as e:\n        exit_code = e.code\n    finally:\n        assert exit_code == command.expected_error_code", "def gpt_cli_test(command: CLICase) -> None:\n    os.environ[\"GPT_ASK_COMMANDS\"] = \"1\"\n\n    sys.argv[1:] = command.command.split(\" \")\n    exit_code = -1\n    try:\n        exit_code = cli()\n    except SystemExit as e:\n        exit_code = e.code\n    finally:\n        assert exit_code == command.expected_error_code", "\n\ndef cli_test(command, command_array) -> None:\n    result = subprocess.run(\n        command_array,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        check=False,\n    )\n\n    assert result.returncode == command.expected_error_code", "\n\n@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\n@pytest.mark.cli\ndef test_cli_gpt_cli(command: str) -> None:\n    \"\"\"Test gpt commands from installed CLI\"\"\"\n    command_array = f\"gpt {ARGS_DICT[command].command}\".split(\" \")\n\n    cli_test(ARGS_DICT[command], command_array)\n", "\n\n@pytest.mark.parametrize(\"command\", MODULE_DICT.keys())\n@pytest.mark.cli\ndef test_cli_gpt_module(command: str) -> None:\n    \"\"\"Test running cli as module\"\"\"\n    command_array = MODULE_DICT[command].command.split(\" \")\n\n    cli_test(MODULE_DICT[command], command_array)\n", "\n\n@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\ndef test_gpt_cli(command: str, mock_openai: None) -> None:\n    gpt_cli_test(ARGS_DICT[command])\n\n\n@pytest.mark.parametrize(\"command\", ARGS_DICT.keys())\n@pytest.mark.integration\ndef test_int_gpt_cli(command: str) -> None:\n    \"\"\"Test gpt commands from CLI file\"\"\"\n    gpt_cli_test(ARGS_DICT[command])", "@pytest.mark.integration\ndef test_int_gpt_cli(command: str) -> None:\n    \"\"\"Test gpt commands from CLI file\"\"\"\n    gpt_cli_test(ARGS_DICT[command])\n"]}
{"filename": "tests/test_context.py", "chunked_list": ["\"\"\"Tests for the context grabber.\"\"\"\nfrom gpt_review.context import _load_context_file\n\n\ndef test_load_context_file(monkeypatch) -> None:\n    monkeypatch.setenv(\"CONTEXT_FILE\", \"azure.yaml.template\")\n    yaml_context = _load_context_file()\n    assert \"azure_api_type\" in yaml_context\n    assert \"azure_api_base\" in yaml_context\n    assert \"azure_api_version\" in yaml_context", ""]}
{"filename": "tests/test_git.py", "chunked_list": ["\"\"\"Test git functions.\"\"\"\nimport pytest\n\nfrom gpt_review._git import _commit, _find_git_dir\n\n\ndef commit_test() -> None:\n    \"\"\"Test Case for commit wrapper.\"\"\"\n    message = _commit()\n    assert message\n\n    message = _commit(push=True)\n    assert message", "\n\ndef test_commit(mock_openai: None, mock_git_commit: None) -> None:\n    \"\"\"Unit test for commit function.\"\"\"\n    commit_test()\n\n\n@pytest.mark.integration\ndef test_int_commit(mock_git_commit: None) -> None:\n    \"\"\"Integration test for commit function.\"\"\"\n    commit_test()", "def test_int_commit(mock_git_commit: None) -> None:\n    \"\"\"Integration test for commit function.\"\"\"\n    commit_test()\n\n\n@pytest.mark.unit\n@pytest.mark.integration\ndef test_find_git_dir() -> None:\n    _find_git_dir(path=\"tests\")\n\n    with pytest.raises(FileNotFoundError):\n        _find_git_dir(path=\"/\")", ""]}
{"filename": "tests/test_review.py", "chunked_list": ["import pytest\n\nfrom gpt_review.repositories.github import GitHubClient\n\n\ndef test_get_review(mock_openai) -> None:\n    get_review_test()\n\n\n@pytest.mark.integration\ndef test_int_get_review() -> None:\n    get_review_test()", "\n@pytest.mark.integration\ndef test_int_get_review() -> None:\n    get_review_test()\n\n\ndef get_review_test() -> None:\n    \"\"\"Test get_review.\"\"\"\n    # Load test data from moock.diff\n    with open(\"tests/mock.diff\", \"r\") as f:\n        diff = f.read()\n\n        GitHubClient.post_pr_summary(diff)", "\n\ndef test_empty_summary(empty_summary, mock_openai) -> None:\n    get_review_test()\n\n\n@pytest.mark.integration\ndef test_int_empty_summary(empty_summary) -> None:\n    get_review_test()\n", "\n\ndef test_file_summary(mock_openai, file_summary) -> None:\n    get_review_test()\n\n\n@pytest.mark.integration\ndef test_int_file_summary(mock_openai, file_summary) -> None:\n    get_review_test()\n", ""]}
{"filename": "tests/test_github.py", "chunked_list": ["import pytest\n\nfrom gpt_review.repositories.github import GitHubClient\n\n\ndef get_pr_diff_test(starts_with, patch_repo=None, patch_pr=None) -> None:\n    \"\"\"Test the GitHub API call.\"\"\"\n    diff = GitHubClient.get_pr_diff(patch_repo=patch_repo, patch_pr=patch_pr)\n    assert diff.startswith(starts_with)\n", "\n\ndef post_pr_comment_test() -> None:\n    \"\"\"Test the GitHub API call.\"\"\"\n    response = GitHubClient.post_pr_summary(\"test\")\n    assert response\n\n\n@pytest.mark.integration\ndef test_int_pr_diff(mock_github) -> None:\n    \"\"\"Integration Test for GitHub API diff call.\"\"\"\n    get_pr_diff_test(\"diff --git a/README.md b/README.md\", \"microsoft/gpt-review\", 1)", "@pytest.mark.integration\ndef test_int_pr_diff(mock_github) -> None:\n    \"\"\"Integration Test for GitHub API diff call.\"\"\"\n    get_pr_diff_test(\"diff --git a/README.md b/README.md\", \"microsoft/gpt-review\", 1)\n\n\ndef test_pr_diff(mock_openai, mock_github) -> None:\n    \"\"\"Unit Test for GitHub API diff call.\"\"\"\n    get_pr_diff_test(\"diff --git a/README.md b/README.md\")\n", "\n\n@pytest.mark.integration\ndef test_int_pr_comment(mock_github) -> None:\n    \"\"\"Integration Test for GitHub API comment call.\"\"\"\n    post_pr_comment_test()\n\n\n@pytest.mark.integration\ndef test_int_pr_update(mock_github, mock_github_comment) -> None:\n    \"\"\"Integration Test for updating GitHub API comment call.\"\"\"\n    post_pr_comment_test()", "@pytest.mark.integration\ndef test_int_pr_update(mock_github, mock_github_comment) -> None:\n    \"\"\"Integration Test for updating GitHub API comment call.\"\"\"\n    post_pr_comment_test()\n\n\ndef test_pr_comment(mock_openai, mock_github) -> None:\n    \"\"\"Unit Test for GitHub API comment call.\"\"\"\n    post_pr_comment_test()\n", "\n\ndef test_pr_update(mock_openai, mock_github, mock_github_comment) -> None:\n    \"\"\"Unit Test for updating GitHub API comment call.\"\"\"\n    post_pr_comment_test()\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["from collections import namedtuple\n\nimport pytest\nimport yaml\nfrom llama_index import SimpleDirectoryReader\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if \"_int_\" in item.nodeid:\n            item.add_marker(pytest.mark.integration)\n        elif \"_cli_\" in item.nodeid:\n            item.add_marker(pytest.mark.cli)\n        else:\n            item.add_marker(pytest.mark.unit)", "\n\n@pytest.fixture\ndef mock_openai(monkeypatch) -> None:\n    \"\"\"\n    Mock OpenAI Functions with monkeypatch\n    - aopenai.ChatCompletion.create\n    \"\"\"\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"MOCK\")\n    monkeypatch.setenv(\"AZURE_OPENAI_API\", \"MOCK\")\n    monkeypatch.setenv(\"AZURE_OPENAI_API_KEY\", \"MOCK\")\n    monkeypatch.setenv(\"FILE_SUMMARY_FULL\", \"true\")\n\n    class MockResponse:\n        def __init__(self) -> None:\n            self.choices = [namedtuple(\"mockMessage\", \"message\")(*[namedtuple(\"mockContent\", \"content\")(*[[\"test\"]])])]\n\n    class MockQueryResponse:\n        def __init__(self) -> None:\n            self.response = \"test\"\n\n    class MockStorageContext:\n        def persist(self, persist_dir) -> None:\n            pass\n\n    class MockIndex:\n        def __init__(self) -> None:\n            self.storage_context = MockStorageContext()\n\n        def query(self, question: str) -> MockQueryResponse:\n            assert isinstance(question, str)\n            return MockQueryResponse()\n\n        def as_query_engine(self):\n            return self\n\n    def mock_create(\n        model=None,\n        deployment_id=None,\n        messages=None,\n        temperature=0,\n        max_tokens=500,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    ) -> MockResponse:\n        return MockResponse()\n\n    def from_documents(documents, service_context=None) -> MockIndex:\n        return MockIndex()\n\n    def init_mock_reader(self, owner, repo, use_parser) -> None:\n        pass\n\n    def mock_load_data_from_branch(self, branch):\n        return SimpleDirectoryReader(input_dir=\".\").load_data()\n\n    monkeypatch.setattr(\"openai.ChatCompletion.create\", mock_create)\n    monkeypatch.setattr(\"llama_index.GPTVectorStoreIndex.from_documents\", from_documents)\n    monkeypatch.setattr(\"llama_index.GithubRepositoryReader.__init__\", init_mock_reader)\n    monkeypatch.setattr(\"llama_index.GithubRepositoryReader._load_data_from_branch\", mock_load_data_from_branch)\n\n    def mock_query(self, question) -> MockQueryResponse:\n        return MockQueryResponse()\n\n    monkeypatch.setattr(\"llama_index.indices.query.base.BaseQueryEngine.query\", mock_query)", "\n\n@pytest.fixture\ndef mock_github(monkeypatch) -> None:\n    \"\"\"\n    Mock GitHub Functions with monkeypatch\n    - requests.get\n    \"\"\"\n    monkeypatch.setenv(\"LINK\", \"https://github.com/microsoft/gpt-review/pull/1\")\n    monkeypatch.setenv(\"GIT_COMMIT_HASH\", \"a9da0c1e65f1102bc2ae16abed7b6a66400a5bde\")\n\n    class MockResponse:\n        def __init__(self) -> None:\n            self.text = \"diff --git a/README.md b/README.md\"\n\n        def json(self) -> dict:\n            return {\"test\": \"test\"}\n\n    def mock_get(url, headers, timeout) -> MockResponse:\n        return MockResponse()\n\n    def mock_put(url, headers, data, timeout) -> MockResponse:\n        return MockResponse()\n\n    def mock_post(url, headers, data, timeout) -> MockResponse:\n        return MockResponse()\n\n    monkeypatch.setattr(\"requests.get\", mock_get)\n    monkeypatch.setattr(\"requests.put\", mock_put)\n    monkeypatch.setattr(\"requests.post\", mock_post)", "\n\n@pytest.fixture\ndef mock_github_comment(monkeypatch) -> None:\n    class MockCommentResponse:\n        def json(self) -> list:\n            return [\n                {\n                    \"user\": {\"login\": \"github-actions[bot]\"},\n                    \"body\": \"Summary by GPT-4\",\n                    \"id\": 1,\n                }\n            ]\n\n    def mock_get(url, headers, timeout) -> MockCommentResponse:\n        return MockCommentResponse()\n\n    monkeypatch.setattr(\"requests.get\", mock_get)", "\n\n@pytest.fixture\ndef mock_git_commit(monkeypatch) -> None:\n    \"\"\"Mock git.commit with pytest monkey patch\"\"\"\n\n    class MockGit:\n        def __init__(self) -> None:\n            self.git = self\n\n        def commit(self, message, push: bool = False) -> str:\n            return \"test commit response\"\n\n        def diff(self, message, cached) -> str:\n            return \"test diff response\"\n\n        def push(self) -> str:\n            return \"test push response\"\n\n    def mock_init(cls) -> MockGit:\n        return MockGit()\n\n    monkeypatch.setattr(\"git.repo.Repo.init\", mock_init)", "\n\n@pytest.fixture\ndef report_config():\n    \"\"\"Load sample.report.yaml file\"\"\"\n    return load_report_config(\"config.summary.template.yml\")\n\n\ndef load_report_config(file_name):\n    with open(file_name, \"r\") as yaml_file:\n        config = yaml.safe_load(yaml_file)\n        return config[\"report\"]", "def load_report_config(file_name):\n    with open(file_name, \"r\") as yaml_file:\n        config = yaml.safe_load(yaml_file)\n        return config[\"report\"]\n\n\n@pytest.fixture\ndef config_yaml():\n    return \"tests/config.summary.test.yml\"\n", "\n\n@pytest.fixture\ndef git_diff() -> str:\n    \"\"\"Load test.diff file\"\"\"\n    with open(\"tests/mock.diff\", \"r\") as diff_file:\n        diff = diff_file.read()\n    return diff\n\n", "\n\n@pytest.fixture\ndef empty_summary(monkeypatch) -> None:\n    \"\"\"Test empty summary.\"\"\"\n    monkeypatch.setenv(\"FILE_SUMMARY\", \"false\")\n    monkeypatch.setenv(\"TEST_SUMMARY\", \"false\")\n    monkeypatch.setenv(\"BUG_SUMMARY\", \"false\")\n    monkeypatch.setenv(\"RISK_SUMMARY\", \"false\")\n    monkeypatch.setenv(\"FULL_SUMMARY\", \"false\")", "\n\n@pytest.fixture\ndef file_summary(monkeypatch) -> None:\n    \"\"\"Test empty summary.\"\"\"\n    monkeypatch.setenv(\"FILE_SUMMARY_FULL\", \"false\")\n"]}
{"filename": "tests/test_report.py", "chunked_list": ["\"\"\"Tests for customized markdown report generation.\"\"\"\nimport pytest\n\nfrom gpt_review._review import _process_report, _process_yaml\n\n\ndef test_report_generation(git_diff, report_config, mock_openai) -> None:\n    \"\"\"Test report generation with mocks.\"\"\"\n    report_generation_test(git_diff, report_config)\n", "\n\n@pytest.mark.integration\ndef test_int_report_generation(git_diff, report_config) -> None:\n    \"\"\"Test report generation.\"\"\"\n    report_generation_test(git_diff, report_config)\n\n\ndef test_process_yaml(git_diff, config_yaml, mock_openai) -> None:\n    process_yaml_test(git_diff, config_yaml)", "def test_process_yaml(git_diff, config_yaml, mock_openai) -> None:\n    process_yaml_test(git_diff, config_yaml)\n\n\n@pytest.mark.integration\ndef test_int_process_yaml(git_diff, config_yaml) -> None:\n    process_yaml_test(git_diff, config_yaml)\n\n\ndef report_generation_test(git_diff, report_config) -> None:\n    report = _process_report(git_diff, report_config)\n    assert report", "\ndef report_generation_test(git_diff, report_config) -> None:\n    report = _process_report(git_diff, report_config)\n    assert report\n\n\ndef process_yaml_test(git_diff, config_yaml) -> None:\n    \"\"\"Test process_yaml.\"\"\"\n    report = _process_yaml(git_diff, config_yaml)\n    assert report", ""]}
{"filename": "src/gpt_review/_command.py", "chunked_list": ["\"\"\"Interface for GPT CLI command groups.\"\"\"\nfrom knack import CLICommandsLoader\n\n\nclass GPTCommandGroup:\n    \"\"\"Command Group Interface.\"\"\"\n\n    @staticmethod\n    def load_command_table(loader: CLICommandsLoader) -> None:\n        \"\"\"Load the command table.\"\"\"\n\n    @staticmethod\n    def load_arguments(loader: CLICommandsLoader) -> None:\n        \"\"\"Load the arguments for the command group.\"\"\"", ""]}
{"filename": "src/gpt_review/_openai.py", "chunked_list": ["\"\"\"Open AI API Call Wrapper.\"\"\"\nimport logging\nimport os\n\nimport openai\nfrom openai.error import RateLimitError\n\nimport gpt_review.constants as C\nfrom gpt_review.context import _load_azure_openai_context\nfrom gpt_review.utils import _retry_with_exponential_backoff", "from gpt_review.context import _load_azure_openai_context\nfrom gpt_review.utils import _retry_with_exponential_backoff\n\n\ndef _count_tokens(prompt) -> int:\n    \"\"\"\n    Determine number of tokens in prompt.\n\n    Args:\n        prompt (str): The prompt to send to GPT-4.\n\n    Returns:\n        int: The number of tokens in the prompt.\n    \"\"\"\n    return int(len(prompt) / 4 * 3)", "\n\ndef _get_model(prompt: str, max_tokens: int, fast: bool = False, large: bool = False) -> str:\n    \"\"\"\n    Get the OpenAI model based on the prompt length.\n    - when greater then 8k use gpt-4-32k\n    - otherwise use gpt-4\n    - enable fast to use gpt-35-turbo for small prompts\n\n    Args:\n        prompt (str): The prompt to send to GPT-4.\n        max_tokens (int): The maximum number of tokens to generate.\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n\n    Returns:\n        str: The model to use.\n    \"\"\"\n    context = _load_azure_openai_context()\n\n    tokens = _count_tokens(prompt)\n    if large or tokens + max_tokens > 8000:\n        return context.large_llm_model_deployment_id\n    if tokens + max_tokens > 4000:\n        return context.smart_llm_model_deployment_id\n    return context.turbo_llm_model_deployment_id if fast else context.smart_llm_model_deployment_id", "\n\ndef _call_gpt(\n    prompt: str,\n    temperature=0.10,\n    max_tokens=500,\n    top_p=1.0,\n    frequency_penalty=0.5,\n    presence_penalty=0.0,\n    retry=0,\n    messages=None,\n    fast: bool = False,\n    large: bool = False,\n) -> str:\n    \"\"\"\n    Call GPT with the given prompt.\n\n    Args:\n        prompt (str): The prompt to send to GPT-4.\n        temperature (float, optional): The temperature to use. Defaults to 0.10.\n        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 500.\n        top_p (float, optional): The top_p to use. Defaults to 1.\n        frequency_penalty (float, optional): The frequency penalty to use. Defaults to 0.5.\n        presence_penalty (float, optional): The presence penalty to use. Defaults to 0.0.\n        retry (int, optional): The number of times to retry the request. Defaults to 0.\n        messages (List[Dict[str, str]], optional): The messages to send to GPT-4. Defaults to None.\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n\n    Returns:\n        str: The response from GPT.\n    \"\"\"\n    messages = messages or [{\"role\": \"user\", \"content\": prompt}]\n    logging.debug(\"Prompt sent to GPT: %s\\n\", prompt)\n\n    try:\n        model = _get_model(prompt, max_tokens=max_tokens, fast=fast, large=large)\n        logging.debug(\"Model Selected based on prompt size: %s\", model)\n\n        if os.environ.get(\"OPENAI_API_TYPE\", \"\") == C.AZURE_API_TYPE:\n            logging.debug(\"Using Azure Open AI.\")\n            completion = openai.ChatCompletion.create(\n                deployment_id=model,\n                messages=messages,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                top_p=top_p,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n            )\n        else:\n            logging.debug(\"Using Open AI.\")\n            completion = openai.ChatCompletion.create(\n                model=model,\n                messages=messages,\n                max_tokens=max_tokens,\n                temperature=temperature,\n                top_p=top_p,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n            )\n        return completion.choices[0].message.content  # type: ignore\n    except RateLimitError as error:\n        if retry < C.MAX_RETRIES:\n            retry_after = error.headers.get(\"Retry-After\", C.DEFAULT_RETRY_AFTER)\n            _retry_with_exponential_backoff(retry, retry_after)\n\n            return _call_gpt(prompt, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, retry + 1)\n        raise RateLimitError(\"Retry limit exceeded\") from error", ""]}
{"filename": "src/gpt_review/main.py", "chunked_list": ["\"\"\"The GPT CLI entry point.\"\"\"\nimport sys\n\nfrom knack.help_files import helps\n\nfrom gpt_review._gpt_cli import cli\n\n\ndef _help_text(help_type, short_summary) -> str:\n    return f\"\"\"\ntype: {help_type}\nshort-summary: {short_summary}\n\"\"\"", "def _help_text(help_type, short_summary) -> str:\n    return f\"\"\"\ntype: {help_type}\nshort-summary: {short_summary}\n\"\"\"\n\n\nhelps[\"\"] = _help_text(\"group\", \"Easily interact with GPT APIs.\")\nhelps[\"ask\"] = _help_text(\"group\", \"Use GPT to ask questions.\")\nhelps[\"git\"] = _help_text(\"group\", \"Use GPT enchanced git commands.\")", "helps[\"ask\"] = _help_text(\"group\", \"Use GPT to ask questions.\")\nhelps[\"git\"] = _help_text(\"group\", \"Use GPT enchanced git commands.\")\nhelps[\"git commit\"] = _help_text(\"command\", \"Run git commit with a commit message generated by GPT.\")\nhelps[\"github\"] = _help_text(\"group\", \"Use GPT with GitHub Repositories.\")\nhelps[\"github review\"] = _help_text(\"command\", \"Review GitHub PR with Open AI, and post response as a comment.\")\nhelps[\"review\"] = _help_text(\"group\", \"Use GPT to perform customized reviews.\")\nhelps[\"review diff\"] = _help_text(\"command\", \"Review a git diff from file.\")\n\n\nexit_code = cli()", "\nexit_code = cli()\nsys.exit(exit_code)\n"]}
{"filename": "src/gpt_review/_ask.py", "chunked_list": ["\"\"\"Ask GPT a question.\"\"\"\nimport logging\nfrom typing import Dict, List, Optional\n\nfrom knack import CLICommandsLoader\nfrom knack.arguments import ArgumentsContext\nfrom knack.commands import CommandGroup\nfrom knack.util import CLIError\n\nimport gpt_review.constants as C", "\nimport gpt_review.constants as C\nfrom gpt_review._command import GPTCommandGroup\nfrom gpt_review._llama_index import _query_index\nfrom gpt_review._openai import _call_gpt\nfrom gpt_review.context import _load_azure_openai_context\n\n\ndef validate_parameter_range(namespace) -> None:\n    \"\"\"\n    Validate the following parameters:\n    - max_tokens is in [1,4000]\n    - temperature is in [0,1]\n    - top_p is in [0,1]\n    - frequency_penalty is in [0,2]\n    - presence_penalty is in [0,2]\n\n    Args:\n        namespace (argparse.Namespace): The namespace to validate.\n\n    Raises:\n        CLIError: If the parameter is not within the allowed range.\n    \"\"\"\n    _range_validation(namespace.max_tokens, \"max-tokens\", C.MAX_TOKENS_MIN, C.MAX_TOKENS_MAX)\n    _range_validation(namespace.temperature, \"temperature\", C.TEMPERATURE_MIN, C.TEMPERATURE_MAX)\n    _range_validation(namespace.top_p, \"top-p\", C.TOP_P_MIN, C.TOP_P_MAX)\n    _range_validation(\n        namespace.frequency_penalty, \"frequency-penalty\", C.FREQUENCY_PENALTY_MIN, C.FREQUENCY_PENALTY_MAX\n    )\n    _range_validation(namespace.presence_penalty, \"presence-penalty\", C.PRESENCE_PENALTY_MIN, C.PRESENCE_PENALTY_MAX)", "def validate_parameter_range(namespace) -> None:\n    \"\"\"\n    Validate the following parameters:\n    - max_tokens is in [1,4000]\n    - temperature is in [0,1]\n    - top_p is in [0,1]\n    - frequency_penalty is in [0,2]\n    - presence_penalty is in [0,2]\n\n    Args:\n        namespace (argparse.Namespace): The namespace to validate.\n\n    Raises:\n        CLIError: If the parameter is not within the allowed range.\n    \"\"\"\n    _range_validation(namespace.max_tokens, \"max-tokens\", C.MAX_TOKENS_MIN, C.MAX_TOKENS_MAX)\n    _range_validation(namespace.temperature, \"temperature\", C.TEMPERATURE_MIN, C.TEMPERATURE_MAX)\n    _range_validation(namespace.top_p, \"top-p\", C.TOP_P_MIN, C.TOP_P_MAX)\n    _range_validation(\n        namespace.frequency_penalty, \"frequency-penalty\", C.FREQUENCY_PENALTY_MIN, C.FREQUENCY_PENALTY_MAX\n    )\n    _range_validation(namespace.presence_penalty, \"presence-penalty\", C.PRESENCE_PENALTY_MIN, C.PRESENCE_PENALTY_MAX)", "\n\ndef _range_validation(param, name, min_value, max_value) -> None:\n    \"\"\"Validates that the given parameter is within the allowed range\n\n    Args:\n        param (int or float): The parameter value to validate.\n        name (str): The name of the parameter.\n        min_value (int or float): The minimum allowed value for the parameter.\n        max_value (int or float): The maximum allowed value for the parameter.\n\n    Raises:\n        CLIError: If the parameter is not within the allowed range.\n    \"\"\"\n    if param is not None and (param < min_value or param > max_value):\n        raise CLIError(f\"--{name} must be a(n) {type(param).__name__} between {min_value} and {max_value}\")", "\n\ndef _ask(\n    question: List[str],\n    max_tokens: int = C.MAX_TOKENS_DEFAULT,\n    temperature: float = C.TEMPERATURE_DEFAULT,\n    top_p: float = C.TOP_P_DEFAULT,\n    frequency_penalty: float = C.FREQUENCY_PENALTY_DEFAULT,\n    presence_penalty: float = C.PRESENCE_PENALTY_DEFAULT,\n    files: Optional[List[str]] = None,\n    messages=None,\n    fast: bool = False,\n    large: bool = False,\n    directory: Optional[str] = None,\n    reset: bool = False,\n    required_exts: Optional[List[str]] = None,\n    hidden: bool = False,\n    recursive: bool = False,\n    repository: Optional[str] = None,\n    branch: str = \"main\",\n) -> Dict[str, str]:\n    \"\"\"\n    Ask GPT a question.\n\n    Args:\n        question (List[str]): The question to ask GPT.\n        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to C.MAX_TOKENS_DEFAULT.\n        temperature (float, optional): Controls randomness. Defaults to C.TEMPERATURE_DEFAULT.\n        top_p (float, optional): Controls diversity via nucleus sampling. Defaults to C.TOP_P_DEFAULT.\n        frequency_penalty (float, optional): How much to penalize new tokens based on their existing frequency in the\n            text so far. Defaults to C.FREQUENCY_PENALTY_DEFAULT.\n        presence_penalty (float, optional): How much to penalize new tokens based on whether they appear in the text so\n            far. Defaults to C.PRESENCE_PENALTY_DEFAULT.\n        files (Optional[List[str]], optional): The files to search. Defaults to None.\n        messages ([type], optional): [description]. Defaults to None.\n        fast (bool, optional): Use the fast model. Defaults to False.\n        large (bool, optional): Use the large model. Defaults to False.\n        directory (Optional[str], optional): The directory to search. Defaults to None.\n        reset (bool, optional): Whether to reset the index. Defaults to False.\n        required_exts (Optional[List[str]], optional): The required file extensions. Defaults to None.\n        hidden (bool, optional): Include hidden files. Defaults to False.\n        recursive (bool, optional): Recursively search the directory. Defaults to False.\n        repository (Optional[str], optional): The repository to search. Defaults to None.\n\n    Returns:\n            Dict[str, str]: The response from GPT.\n    \"\"\"\n    _load_azure_openai_context()\n\n    prompt = \"\".join(question)\n\n    if files or directory or repository:\n        response = _query_index(\n            prompt,\n            files,\n            input_dir=directory,\n            reset=reset,\n            exclude_hidden=not hidden,\n            recursive=recursive,\n            required_exts=required_exts,\n            repository=repository,\n            branch=branch,\n            fast=fast,\n            large=large,\n        )\n    else:\n        response = _call_gpt(\n            prompt=prompt,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            fast=fast,\n            large=large,\n            messages=messages,\n        )\n    logging.info(response)\n    return {\"response\": response}", "\n\nclass AskCommandGroup(GPTCommandGroup):\n    \"\"\"Ask Command Group.\"\"\"\n\n    @staticmethod\n    def load_command_table(loader: CLICommandsLoader) -> None:\n        with CommandGroup(loader, \"\", \"gpt_review._ask#{}\") as group:\n            group.command(\"ask\", \"_ask\", is_preview=True)\n\n    @staticmethod\n    def load_arguments(loader: CLICommandsLoader) -> None:\n        with ArgumentsContext(loader, \"ask\") as args:\n            args.positional(\"question\", type=str, nargs=\"+\", help=\"Provide a question to ask GPT.\")\n            args.argument(\n                \"fast\",\n                help=\"Use gpt-35-turbo for prompts < 4000 tokens.\",\n                default=False,\n                action=\"store_true\",\n            )\n            args.argument(\n                \"large\",\n                help=\"Use gpt-4-32k for prompts.\",\n                default=False,\n                action=\"store_true\",\n            )\n            args.argument(\n                \"temperature\",\n                type=float,\n                help=\"Sets the level of creativity/randomness.\",\n                validator=validate_parameter_range,\n            )\n            args.argument(\n                \"max_tokens\",\n                type=int,\n                help=\"The maximum number of tokens to generate.\",\n                validator=validate_parameter_range,\n            )\n            args.argument(\n                \"top_p\",\n                type=float,\n                help=\"Also sets the level of creativity/randomness. Adjust temperature or top p but not both.\",\n                validator=validate_parameter_range,\n            )\n            args.argument(\n                \"frequency_penalty\",\n                type=float,\n                help=\"Reduce the chance of repeating a token based on current frequency in the text.\",\n                validator=validate_parameter_range,\n            )\n            args.argument(\n                \"presence_penalty\",\n                type=float,\n                help=\"Reduce the chance of repeating any token that has appeared in the text so far.\",\n                validator=validate_parameter_range,\n            )\n            args.argument(\n                \"files\",\n                type=str,\n                help=\"Ask question about a file. Can be used multiple times.\",\n                default=None,\n                action=\"append\",\n                options_list=(\"--files\", \"-f\"),\n            )\n            args.argument(\n                \"directory\",\n                type=str,\n                help=\"Path of the directory to index. Use --recursive (or -r) to index subdirectories.\",\n                default=None,\n                options_list=(\"--directory\", \"-d\"),\n            )\n            args.argument(\n                \"required_exts\",\n                type=str,\n                help=\"Required extensions when indexing a directory. Requires --directory. Can be used multiple times.\",\n                default=None,\n                action=\"append\",\n            )\n            args.argument(\n                \"hidden\",\n                help=\"Include hidden files when indexing a directory. Requires --directory.\",\n                default=False,\n                action=\"store_true\",\n            )\n            args.argument(\n                \"recursive\",\n                help=\"Recursively index a directory. Requires --directory.\",\n                default=False,\n                action=\"store_true\",\n                options_list=(\"--recursive\", \"-r\"),\n            )\n            args.argument(\n                \"repository\",\n                type=str,\n                help=\"Repository to index. Default: None.\",\n                default=None,\n                options_list=(\"--repository\", \"-repo\"),\n            )\n            args.argument(\n                \"branch\",\n                type=str,\n                help=\"Branch to index. Default: main.\",\n                default=\"main\",\n                options_list=(\"--branch\", \"-b\"),\n            )\n            args.argument(\n                \"reset\",\n                help=\"Reset the index, overwriting the directory. Requires --directory, --files, or --repository.\",\n                default=False,\n                action=\"store_true\",\n            )", ""]}
{"filename": "src/gpt_review/__main__.py", "chunked_list": ["\"\"\"The GPT CLI entry point for python -m gpt\"\"\"\nimport sys\n\nfrom gpt_review._gpt_cli import cli\n\nif __name__ == \"__main__\":\n    exit_code = cli()\n    sys.exit(exit_code)\n", ""]}
{"filename": "src/gpt_review/context.py", "chunked_list": ["\"\"\"Context for the Azure OpenAI API and the models.\"\"\"\nimport os\nfrom dataclasses import dataclass\n\nimport openai\nimport yaml\nfrom azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\n\nimport gpt_review.constants as C", "\nimport gpt_review.constants as C\n\n\n@dataclass\nclass Context:\n    azure_api_base: str\n    azure_api_type: str = C.AZURE_API_TYPE\n    azure_api_version: str = C.AZURE_API_VERSION\n    turbo_llm_model_deployment_id: str = C.AZURE_TURBO_MODEL\n    smart_llm_model_deployment_id: str = C.AZURE_SMART_MODEL\n    large_llm_model_deployment_id: str = C.AZURE_LARGE_MODEL\n    embedding_model_deployment_id: str = C.AZURE_EMBEDDING_MODEL", "\n\ndef _load_context_file():\n    \"\"\"Import from yaml file and return the context.\"\"\"\n    context_file = os.getenv(\"CONTEXT_FILE\", C.AZURE_CONFIG_FILE)\n    with open(context_file, \"r\", encoding=\"utf8\") as file:\n        return yaml.load(file, Loader=yaml.SafeLoader)\n\n\ndef _load_azure_openai_context() -> Context:\n    \"\"\"Load the context from the environment variables or the context file.\n\n    If a config file is available its values will take precedence. Otherwise\n    it will first check for an AZURE_OPENAI_API key, next OPENAI_API_KEY, and\n    lastly the Azure Key Vault.\n\n    Returns:\n        Context: The context for the Azure OpenAI API and the models.\n    \"\"\"\n    azure_config = _load_context_file() if os.path.exists(os.getenv(\"CONTEXT_FILE\", C.AZURE_CONFIG_FILE)) else {}\n\n    if azure_config.get(\"azure_api_type\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = azure_config.get(\"azure_api_type\")\n    elif os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n    elif \"OPENAI_API_TYPE\" in os.environ:\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n\n    if azure_config.get(\"azure_api_version\"):\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = azure_config.get(\"azure_api_version\")\n    elif os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = C.AZURE_API_VERSION\n    elif \"OPENAI_API_VERSION\" in os.environ:\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n\n    if os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_API\") or azure_config.get(\n            \"azure_api_base\"\n        )\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")  # type: ignore\n    elif os.getenv(\"OPENAI_API_KEY\"):\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n    else:\n        kv_client = SecretClient(\n            vault_url=os.getenv(\"AZURE_KEY_VAULT_URL\", C.AZURE_KEY_VAULT),\n            credential=DefaultAzureCredential(additionally_allowed_tenants=[\"*\"]),\n        )\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = kv_client.get_secret(\"azure-open-ai\").value  # type: ignore\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = kv_client.get_secret(\"azure-openai-key\").value  # type: ignore\n\n    return Context(\n        azure_api_base=openai.api_base,\n        azure_api_type=openai.api_type,\n        azure_api_version=openai.api_version,\n        **azure_config.get(\"azure_model_map\", {}),\n    )", "\ndef _load_azure_openai_context() -> Context:\n    \"\"\"Load the context from the environment variables or the context file.\n\n    If a config file is available its values will take precedence. Otherwise\n    it will first check for an AZURE_OPENAI_API key, next OPENAI_API_KEY, and\n    lastly the Azure Key Vault.\n\n    Returns:\n        Context: The context for the Azure OpenAI API and the models.\n    \"\"\"\n    azure_config = _load_context_file() if os.path.exists(os.getenv(\"CONTEXT_FILE\", C.AZURE_CONFIG_FILE)) else {}\n\n    if azure_config.get(\"azure_api_type\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = azure_config.get(\"azure_api_type\")\n    elif os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n    elif \"OPENAI_API_TYPE\" in os.environ:\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n\n    if azure_config.get(\"azure_api_version\"):\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = azure_config.get(\"azure_api_version\")\n    elif os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = C.AZURE_API_VERSION\n    elif \"OPENAI_API_VERSION\" in os.environ:\n        openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n\n    if os.getenv(\"AZURE_OPENAI_API\"):\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_API\") or azure_config.get(\n            \"azure_api_base\"\n        )\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")  # type: ignore\n    elif os.getenv(\"OPENAI_API_KEY\"):\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n    else:\n        kv_client = SecretClient(\n            vault_url=os.getenv(\"AZURE_KEY_VAULT_URL\", C.AZURE_KEY_VAULT),\n            credential=DefaultAzureCredential(additionally_allowed_tenants=[\"*\"]),\n        )\n        openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = C.AZURE_API_TYPE\n        openai.api_base = os.environ[\"OPENAI_API_BASE\"] = kv_client.get_secret(\"azure-open-ai\").value  # type: ignore\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"] = kv_client.get_secret(\"azure-openai-key\").value  # type: ignore\n\n    return Context(\n        azure_api_base=openai.api_base,\n        azure_api_type=openai.api_type,\n        azure_api_version=openai.api_version,\n        **azure_config.get(\"azure_model_map\", {}),\n    )", ""]}
{"filename": "src/gpt_review/_git.py", "chunked_list": ["\"\"\"Basic Shell Commands for Git.\"\"\"\nimport logging\nimport os\nfrom typing import Dict\n\nfrom git.repo import Repo\nfrom knack import CLICommandsLoader\nfrom knack.arguments import ArgumentsContext\nfrom knack.commands import CommandGroup\n", "from knack.commands import CommandGroup\n\nfrom gpt_review._command import GPTCommandGroup\nfrom gpt_review._review import _request_goal\n\n\ndef _find_git_dir(path=\".\") -> str:\n    \"\"\"\n    Find the .git directory.\n\n    Args:\n        path (str): The path to start searching from.\n\n    Returns:\n        path (str): The path to the .git directory.\n    \"\"\"\n    while path != \"/\":\n        if os.path.exists(os.path.join(path, \".git\")):\n            return path\n        path = os.path.abspath(os.path.join(path, os.pardir))\n    raise FileNotFoundError(\".git directory not found\")", "\n\ndef _diff() -> str:\n    \"\"\"\n    Get the diff of the PR\n    - run git commands via python\n\n    Returns:\n        diff (str): The diff of the PR.\n    \"\"\"\n    return Repo.init(_find_git_dir()).git.diff(None, cached=True)", "\n\ndef _commit_message(gpt4: bool = False, large: bool = False) -> str:\n    \"\"\"\n    Create a commit message with GPT.\n\n    Args:\n        gpt4 (bool, optional): Whether to use gpt-4. Defaults to False.\n        large (bool, optional): Whether to use gpt-4-32k. Defaults to False.\n\n    Returns:\n        response (str): The response from GPT-4.\n    \"\"\"\n\n    goal = \"\"\"\nCreate a short, single-line, git commit message for these changes\n\"\"\"\n    diff = _diff()\n    logging.debug(\"Diff: %s\", diff)\n\n    return _request_goal(diff, goal, fast=not gpt4, large=large)", "\n\ndef _push() -> str:\n    \"\"\"Run git push.\"\"\"\n    logging.debug(\"Pushing commit to remote.\")\n    repo = Repo.init(_find_git_dir())\n    return repo.git.push()\n\n\ndef _commit(gpt4: bool = False, large: bool = False, push: bool = False) -> Dict[str, str]:\n    \"\"\"Run git commit with a commit message generated by GPT.\n\n    Args:\n        gpt4 (bool, optional): Whether to use gpt-4. Defaults to False.\n        large (bool, optional): Whether to use gpt-4-32k. Defaults to False.\n        push (bool, optional): Whether to push the commit to the remote. Defaults to False.\n\n    Returns:\n        response (Dict[str, str]): The response from git commit.\n    \"\"\"\n    message = _commit_message(gpt4=gpt4, large=large)\n    logging.debug(\"Commit Message: %s\", message)\n    repo = Repo.init(_find_git_dir())\n    commit = repo.git.commit(message=message)\n    if push:\n        commit += f\"\\n{_push()}\"\n    return {\"response\": commit}", "\ndef _commit(gpt4: bool = False, large: bool = False, push: bool = False) -> Dict[str, str]:\n    \"\"\"Run git commit with a commit message generated by GPT.\n\n    Args:\n        gpt4 (bool, optional): Whether to use gpt-4. Defaults to False.\n        large (bool, optional): Whether to use gpt-4-32k. Defaults to False.\n        push (bool, optional): Whether to push the commit to the remote. Defaults to False.\n\n    Returns:\n        response (Dict[str, str]): The response from git commit.\n    \"\"\"\n    message = _commit_message(gpt4=gpt4, large=large)\n    logging.debug(\"Commit Message: %s\", message)\n    repo = Repo.init(_find_git_dir())\n    commit = repo.git.commit(message=message)\n    if push:\n        commit += f\"\\n{_push()}\"\n    return {\"response\": commit}", "\n\nclass GitCommandGroup(GPTCommandGroup):\n    \"\"\"Ask Command Group.\"\"\"\n\n    @staticmethod\n    def load_command_table(loader: CLICommandsLoader) -> None:\n        with CommandGroup(loader, \"git\", \"gpt_review._git#{}\", is_preview=True) as group:\n            group.command(\"commit\", \"_commit\", is_preview=True)\n\n    @staticmethod\n    def load_arguments(loader: CLICommandsLoader) -> None:\n        with ArgumentsContext(loader, \"git commit\") as args:\n            args.argument(\n                \"gpt4\",\n                help=\"Use gpt-4 for generating commit messages instead of gpt-35-turbo.\",\n                default=False,\n                action=\"store_true\",\n            )\n            args.argument(\n                \"large\",\n                help=\"Use gpt-4-32k model for generating commit messages.\",\n                default=False,\n                action=\"store_true\",\n            )\n            args.argument(\n                \"push\",\n                help=\"Push the commit to the remote.\",\n                default=False,\n                action=\"store_true\",\n            )", ""]}
{"filename": "src/gpt_review/_gpt_cli.py", "chunked_list": ["\"\"\"The GPT CLI configuration and utilities.\"\"\"\nimport os\nimport sys\nfrom collections import OrderedDict\n\nfrom knack import CLI, CLICommandsLoader\n\nfrom gpt_review import __version__\nfrom gpt_review._ask import AskCommandGroup\nfrom gpt_review._git import GitCommandGroup", "from gpt_review._ask import AskCommandGroup\nfrom gpt_review._git import GitCommandGroup\nfrom gpt_review._review import ReviewCommandGroup\nfrom gpt_review.repositories.github import GitHubCommandGroup\n\nCLI_NAME = \"gpt\"\n\n\nclass GPTCLI(CLI):\n    \"\"\"Custom CLI implemntation to set version for the GPT CLI.\"\"\"\n\n    def get_cli_version(self) -> str:\n        return __version__", "class GPTCLI(CLI):\n    \"\"\"Custom CLI implemntation to set version for the GPT CLI.\"\"\"\n\n    def get_cli_version(self) -> str:\n        return __version__\n\n\nclass GPTCommandsLoader(CLICommandsLoader):\n    \"\"\"The GPT CLI Commands Loader.\"\"\"\n\n    _CommandGroups = [AskCommandGroup, GitHubCommandGroup, GitCommandGroup, ReviewCommandGroup]\n\n    def load_command_table(self, args) -> OrderedDict:\n        for command_group in self._CommandGroups:\n            command_group.load_command_table(self)\n        return OrderedDict(self.command_table)\n\n    def load_arguments(self, command) -> None:\n        for argument_group in self._CommandGroups:\n            argument_group.load_arguments(self)\n        super(GPTCommandsLoader, self).load_arguments(command)", "\n\ndef cli() -> int:\n    \"\"\"The GPT CLI entry point.\"\"\"\n    gpt = GPTCLI(\n        cli_name=CLI_NAME,\n        config_dir=os.path.expanduser(os.path.join(\"~\", f\".{CLI_NAME}\")),\n        config_env_var_prefix=CLI_NAME,\n        commands_loader_cls=GPTCommandsLoader,\n    )\n    return gpt.invoke(sys.argv[1:])", ""]}
{"filename": "src/gpt_review/__init__.py", "chunked_list": ["#   -------------------------------------------------------------\n#   Copyright (c) Microsoft Corporation. All rights reserved.\n#   Licensed under the MIT License. See LICENSE in project root for information.\n#   -------------------------------------------------------------\n\"\"\"Easy GPT CLI\"\"\"\nfrom __future__ import annotations\n\n__version__ = \"0.9.5\"\n", ""]}
{"filename": "src/gpt_review/utils.py", "chunked_list": ["\"\"\"Utility functions\"\"\"\nimport logging\nimport time\nfrom typing import Optional\n\nimport gpt_review.constants as C\n\n\ndef _retry_with_exponential_backoff(current_retry: int, retry_after: Optional[str]) -> None:\n    \"\"\"\n    Use exponential backoff to retry a request after specific time while staying under the retry count\n\n    Args:\n        current_retry (int): The current retry count.\n        retry_after (Optional[str]): The time to wait before retrying.\n    \"\"\"\n    logging.warning(\"Call to GPT failed due to rate limit, retry attempt %s of %s\", current_retry, C.MAX_RETRIES)\n\n    multiplication_factor = 2 * (1 + current_retry / C.MAX_RETRIES)\n    wait_time = int(retry_after) * multiplication_factor if retry_after else current_retry * multiplication_factor\n\n    logging.warning(\"Waiting for %s seconds before retrying.\", wait_time)\n\n    time.sleep(wait_time)", "def _retry_with_exponential_backoff(current_retry: int, retry_after: Optional[str]) -> None:\n    \"\"\"\n    Use exponential backoff to retry a request after specific time while staying under the retry count\n\n    Args:\n        current_retry (int): The current retry count.\n        retry_after (Optional[str]): The time to wait before retrying.\n    \"\"\"\n    logging.warning(\"Call to GPT failed due to rate limit, retry attempt %s of %s\", current_retry, C.MAX_RETRIES)\n\n    multiplication_factor = 2 * (1 + current_retry / C.MAX_RETRIES)\n    wait_time = int(retry_after) * multiplication_factor if retry_after else current_retry * multiplication_factor\n\n    logging.warning(\"Waiting for %s seconds before retrying.\", wait_time)\n\n    time.sleep(wait_time)", ""]}
{"filename": "src/gpt_review/constants.py", "chunked_list": ["\"\"\"Contains constants for minimum and maximum values of various parameters used in GPT Review.\"\"\"\nimport os\nimport sys\n\nMAX_TOKENS_DEFAULT = 100\nMAX_TOKENS_MIN = 1\nMAX_TOKENS_MAX = sys.maxsize\n\nTEMPERATURE_DEFAULT = 0.7\nTEMPERATURE_MIN = 0", "TEMPERATURE_DEFAULT = 0.7\nTEMPERATURE_MIN = 0\nTEMPERATURE_MAX = 1\n\nTOP_P_DEFAULT = 0.5\nTOP_P_MIN = 0\nTOP_P_MAX = 1\n\nFREQUENCY_PENALTY_DEFAULT = 0.5\nFREQUENCY_PENALTY_MIN = 0", "FREQUENCY_PENALTY_DEFAULT = 0.5\nFREQUENCY_PENALTY_MIN = 0\nFREQUENCY_PENALTY_MAX = 2\n\nPRESENCE_PENALTY_DEFAULT = 0\nPRESENCE_PENALTY_MIN = 0\nPRESENCE_PENALTY_MAX = 2\n\nMAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", 15))\nDEFAULT_RETRY_AFTER = 30", "MAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", 15))\nDEFAULT_RETRY_AFTER = 30\n\nAZURE_API_TYPE = \"azure\"\nAZURE_API_VERSION = \"2023-03-15-preview\"\nAZURE_CONFIG_FILE = \"azure.yaml\"\nAZURE_TURBO_MODEL = \"gpt-35-turbo\"\nAZURE_SMART_MODEL = \"gpt-4\"\nAZURE_LARGE_MODEL = \"gpt-4-32k\"\nAZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"", "AZURE_LARGE_MODEL = \"gpt-4-32k\"\nAZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"\nAZURE_KEY_VAULT = \"https://dciborow-openai.vault.azure.net/\"\n\nBUG_PROMPT_YAML = \"prompt_bug.yaml\"\nCOVERAGE_PROMPT_YAML = \"prompt_coverage.yaml\"\nSUMMARY_PROMPT_YAML = \"prompt_summary.yaml\"\n"]}
{"filename": "src/gpt_review/_review.py", "chunked_list": ["\"\"\"Basic functions for requesting review based goals from GPT-4.\"\"\"\nimport os\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport yaml\nfrom knack import CLICommandsLoader\nfrom knack.arguments import ArgumentsContext\nfrom knack.commands import CommandGroup\n", "from knack.commands import CommandGroup\n\nfrom gpt_review._ask import _ask\nfrom gpt_review._command import GPTCommandGroup\nfrom gpt_review.prompts._prompt import (\n    load_bug_yaml,\n    load_coverage_yaml,\n    load_summary_yaml,\n)\n", ")\n\n_CHECKS = {\n    \"SUMMARY_CHECKS\": [\n        {\n            \"flag\": \"SUMMARY_SUGGEST\",\n            \"header\": \"Suggestions\",\n            \"goal\": \"\"\"\n            Provide suggestions for improving the changes in this PR.\n            If the PR has no clear issues, mention that no suggestions are needed.", "            Provide suggestions for improving the changes in this PR.\n            If the PR has no clear issues, mention that no suggestions are needed.\n            \"\"\",\n        },\n    ],\n    \"RISK_CHECKS\": [\n        {\n            \"flag\": \"RISK_BREAKING\",\n            \"header\": \"Breaking Changes\",\n            \"goal\": \"\"\"Detect breaking changes in a git diff. Here are some things that can cause a breaking change.", "            \"header\": \"Breaking Changes\",\n            \"goal\": \"\"\"Detect breaking changes in a git diff. Here are some things that can cause a breaking change.\n- new parameters to public functions which are required and have no default value.\n\"\"\",\n        },\n    ],\n}\n\n\n@dataclass\nclass GitFile:\n    \"\"\"A git file with its diff contents.\"\"\"\n\n    file_name: str\n    diff: str", "\n@dataclass\nclass GitFile:\n    \"\"\"A git file with its diff contents.\"\"\"\n\n    file_name: str\n    diff: str\n\n\ndef _request_goal(git_diff, goal, fast: bool = False, large: bool = False, temperature: float = 0) -> str:\n    \"\"\"\n    Request a goal from GPT-4.\n\n    Args:\n        git_diff (str): The git diff to split.\n        goal (str): The goal to request from GPT-4.\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n        temperature (float, optional): The temperature to use. Defaults to 0.\n\n    Returns:\n        response (str): The response from GPT-4.\n    \"\"\"\n    prompt = f\"\"\"\n{goal}\n\n{git_diff}\n\"\"\"\n\n    return _ask([prompt], max_tokens=1500, fast=fast, large=large, temperature=temperature)[\"response\"]", "\ndef _request_goal(git_diff, goal, fast: bool = False, large: bool = False, temperature: float = 0) -> str:\n    \"\"\"\n    Request a goal from GPT-4.\n\n    Args:\n        git_diff (str): The git diff to split.\n        goal (str): The goal to request from GPT-4.\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n        temperature (float, optional): The temperature to use. Defaults to 0.\n\n    Returns:\n        response (str): The response from GPT-4.\n    \"\"\"\n    prompt = f\"\"\"\n{goal}\n\n{git_diff}\n\"\"\"\n\n    return _ask([prompt], max_tokens=1500, fast=fast, large=large, temperature=temperature)[\"response\"]", "\n\ndef _check_goals(git_diff, checks, indent=\"###\") -> str:\n    \"\"\"\n    Check goals.\n\n    Args:\n        git_diff (str): The git diff to check.\n        checks (list): The checks to run.\n\n    Returns:\n        str: The output of the checks.\n    \"\"\"\n    return \"\".join(\n        f\"\"\"\n{indent} {check[\"header\"]}\n\n{_request_goal(git_diff, goal=check[\"goal\"])}\n\"\"\"\n        for check in checks\n        if os.getenv(check[\"flag\"], \"true\").lower() == \"true\"\n    )", "\n\ndef _summarize_pr(git_diff) -> str:\n    \"\"\"\n    Summarize a PR.\n\n    Args:\n        git_diff (str): The git diff to summarize.\n\n    Returns:\n        str: The summary of the PR.\n    \"\"\"\n    text = \"\"\n    if os.getenv(\"FULL_SUMMARY\", \"true\").lower() == \"true\":\n        text += f\"\"\"\n{_request_goal(git_diff, goal=\"\")}\n\"\"\"\n\n        text += _check_goals(git_diff, _CHECKS[\"SUMMARY_CHECKS\"])\n    return text", "\n\ndef _summarize_file(diff) -> str:\n    \"\"\"Summarize a file in a git diff.\n\n    Args:\n        diff (str): The file to summarize.\n\n    Returns:\n        str: The summary of the file.\n    \"\"\"\n    git_file = GitFile(diff.split(\" b/\")[0], diff)\n    question = load_summary_yaml().format(diff=diff)\n\n    response = _ask(question=[question], temperature=0.0)\n    return f\"\"\"\n### {git_file.file_name}\n{response}\n\"\"\"", "\n\ndef _split_diff(git_diff):\n    \"\"\"Split a git diff into a list of files and their diff contents.\n\n    Args:\n        git_diff (str): The git diff to split.\n\n    Returns:\n        list: A list of tuples containing the file name and diff contents.\n    \"\"\"\n    diff = \"diff\"\n    git = \"--git a/\"\n    return git_diff.split(f\"{diff} {git}\")[1:]  # Use formated string to prevent splitting", "\n\ndef _summarize_test_coverage(git_diff) -> str:\n    \"\"\"Summarize the test coverage of a git diff.\n\n    Args:\n        git_diff (str): The git diff to summarize.\n\n    Returns:\n        str: The summary of the test coverage.\n    \"\"\"\n    files = {}\n    for diff in _split_diff(git_diff):\n        path = diff.split(\" b/\")[0]\n        git_file = GitFile(path.split(\"/\")[len(path.split(\"/\")) - 1], diff)\n\n        files[git_file.file_name] = git_file\n\n    question = load_coverage_yaml().format(diff=git_diff)\n\n    return _ask([question], temperature=0.0, max_tokens=1500)[\"response\"]", "\n\ndef _summarize_risk(git_diff) -> str:\n    \"\"\"\n    Summarize potential risks.\n\n    Args:\n        git_diff (str): The git diff to split.\n\n    Returns:\n        response (str): The response from GPT-4.\n    \"\"\"\n    text = \"\"\n    if os.getenv(\"RISK_SUMMARY\", \"true\").lower() == \"true\":\n        text += \"\"\"\n## Potential Risks\n\n\"\"\"\n        text += _check_goals(git_diff, _CHECKS[\"RISK_CHECKS\"])\n    return text", "\n\ndef _summarize_files(git_diff) -> str:\n    \"\"\"Summarize git files.\"\"\"\n    summary = \"\"\"\n# Summary by GPT-4\n\"\"\"\n\n    summary += _summarize_pr(git_diff)\n\n    if os.getenv(\"FILE_SUMMARY\", \"true\").lower() == \"true\":\n        file_summary = \"\"\"\n## Changes\n\n\"\"\"\n        file_summary += \"\".join(_summarize_file(diff) for diff in _split_diff(git_diff))\n        if os.getenv(\"FILE_SUMMARY_FULL\", \"true\").lower() == \"true\":\n            summary += file_summary\n\n        summary += f\"\"\"\n### Summary of File Changes\n{_request_goal(file_summary, goal=\"Summarize the changes to the files.\")}\n\"\"\"\n\n    if os.getenv(\"TEST_SUMMARY\", \"true\").lower() == \"true\":\n        summary += f\"\"\"\n## Test Coverage\n{_summarize_test_coverage(git_diff)}\n\"\"\"\n\n    if os.getenv(\"BUG_SUMMARY\", \"true\").lower() == \"true\":\n        question = load_bug_yaml().format(diff=git_diff)\n        pr_bugs = _ask([question])[\"response\"]\n\n        summary += f\"\"\"\n## Potential Bugs\n{pr_bugs}\n\"\"\"\n\n    summary += _summarize_risk(git_diff)\n\n    return summary", "\n\ndef _review(diff: str = \".diff\", config: str = \"config.summary.yml\") -> Dict[str, str]:\n    \"\"\"Review a git diff from file\n\n    Args:\n        diff (str, optional): The diff to review. Defaults to \".diff\".\n        config (str, optional): The config to use. Defaults to \"config.summary.yml\".\n\n    Returns:\n        Dict[str, str]: The response from GPT-4.\n    \"\"\"\n\n    # If config is a file, use it\n\n    with open(diff, \"r\", encoding=\"utf8\") as file:\n        diff_contents = file.read()\n\n        if os.path.isfile(config):\n            summary = _process_yaml(git_diff=diff_contents, yaml_file=config)\n        else:\n            summary = _summarize_files(diff_contents)\n        return {\"response\": summary}", "\n\ndef _process_yaml(git_diff, yaml_file, headers=True) -> str:\n    \"\"\"\n    Process a yaml file.\n\n    Args:\n        git_diff (str): The diff of the PR.\n        yaml_file (str): The path to the yaml file.\n        headers (bool, optional): Whether to include headers. Defaults to True.\n\n    Returns:\n        str: The report.\n    \"\"\"\n    with open(yaml_file, \"r\", encoding=\"utf8\") as file:\n        yaml_contents = file.read()\n        config = yaml.safe_load(yaml_contents)\n        report = config[\"report\"]\n\n        return _process_report(git_diff, report, headers=headers)", "\n\ndef _process_report(git_diff, report: dict, indent=\"#\", headers=True) -> str:\n    \"\"\"\n    for-each record in report\n    - if record is a string, check_goals\n    - else recursively call process_report\n\n    Args:\n        git_diff (str): The diff of the PR.\n        report (dict): The report to process.\n        indent (str, optional): The indent to use. Defaults to \"#\".\n        headers (bool, optional): Whether to include headers. Defaults to True.\n\n    Returns:\n        str: The report.\n    \"\"\"\n    text = \"\"\n    for key, record in report.items():\n        if isinstance(record, str) or record is None:\n            if headers and key != \"_\":\n                text += f\"\"\"\n{indent} {key}\n\"\"\"\n            text += f\"{_request_goal(git_diff, goal=record)}\"\n\n        else:\n            text += f\"\"\"\n{indent} {key}\n\"\"\"\n            text += _process_report(git_diff, record, indent=f\"{indent}#\", headers=headers)\n\n    return text", "\n\nclass ReviewCommandGroup(GPTCommandGroup):\n    \"\"\"Review Command Group.\"\"\"\n\n    @staticmethod\n    def load_command_table(loader: CLICommandsLoader) -> None:\n        with CommandGroup(loader, \"review\", \"gpt_review._review#{}\", is_preview=True) as group:\n            group.command(\"diff\", \"_review\", is_preview=True)\n\n    @staticmethod\n    def load_arguments(loader: CLICommandsLoader) -> None:\n        \"\"\"Add patch_repo, patch_pr, and access_token arguments.\"\"\"\n        with ArgumentsContext(loader, \"github\") as args:\n            args.argument(\n                \"diff\",\n                type=str,\n                help=\"Git diff to review.\",\n                default=\".diff\",\n            )\n            args.argument(\n                \"config\",\n                type=str,\n                help=\"The config file to use to customize review summary.\",\n                default=\"config.template.yml\",\n            )", ""]}
{"filename": "src/gpt_review/_llama_index.py", "chunked_list": ["\"\"\"Wrapper for Llama Index.\"\"\"\nimport logging\nimport os\nfrom typing import List, Optional\n\nimport openai\nfrom langchain.chat_models import AzureChatOpenAI, ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.llms import AzureOpenAI\nfrom llama_index import (", "from langchain.llms import AzureOpenAI\nfrom llama_index import (\n    Document,\n    GithubRepositoryReader,\n    GPTVectorStoreIndex,\n    LangchainEmbedding,\n    LLMPredictor,\n    ServiceContext,\n    SimpleDirectoryReader,\n    StorageContext,", "    SimpleDirectoryReader,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.indices.base import BaseGPTIndex\nfrom llama_index.storage.storage_context import DEFAULT_PERSIST_DIR\nfrom typing_extensions import override\n\nimport gpt_review.constants as C\nfrom gpt_review.context import _load_azure_openai_context", "import gpt_review.constants as C\nfrom gpt_review.context import _load_azure_openai_context\n\nlogger = logging.getLogger(__name__)\n\n\ndef _query_index(\n    question: str,\n    files: Optional[List[str]] = None,\n    input_dir: Optional[str] = None,\n    exclude_hidden: bool = True,\n    recursive: bool = True,\n    required_exts: Optional[List[str]] = None,\n    repository: Optional[str] = None,\n    branch: str = \"main\",\n    fast: bool = False,\n    large: bool = False,\n    reset: bool = False,\n) -> str:\n    \"\"\"\n    Query a Vector Index with GPT.\n    Args:\n        question (List[str]): The question to ask.\n        files (List[str], optional): The files to search.\n            (Optional; overrides input_dir, exclude)\n        input_dir (str, optional): Path to the directory.\n        exclude_hidden (bool): Whether to exclude hidden files.\n        recursive (bool): Whether to search directory recursively.\n        required_exts (List, optional): The required extensions for files in directory.\n        repository (str): The repository to search. Format: owner/repo\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n        reset (bool, optional): Whether to reset the index. Defaults to False.\n\n    Returns:\n        Dict[str, str]: The response.\n    \"\"\"\n    documents = []\n    if files:\n        documents += SimpleDirectoryReader(input_files=files).load_data()\n    elif input_dir:\n        documents += SimpleDirectoryReader(\n            input_dir=input_dir, exclude_hidden=exclude_hidden, recursive=recursive, required_exts=required_exts\n        ).load_data()\n    if repository:\n        owner, repo = repository.split(\"/\")\n        documents += GithubRepositoryReader(owner=owner, repo=repo, use_parser=False).load_data(branch=branch)\n\n    index = _load_index(documents, fast=fast, large=large, reset=reset)\n\n    return index.as_query_engine().query(question).response  # type: ignore", "\n\ndef _load_index(\n    documents: List[Document],\n    fast: bool = True,\n    large: bool = True,\n    reset: bool = False,\n    persist_dir: str = DEFAULT_PERSIST_DIR,\n) -> BaseGPTIndex:\n    \"\"\"\n    Load or create a document indexer.\n\n    Args:\n        documents (List[Document]): The documents to index.\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n        reset (bool, optional): Whether to reset the index. Defaults to False.\n        persist_dir (str, optional): The directory to persist the index to. Defaults to './storage'.\n\n    Returns:\n        BaseGPTIndex: The document indexer.\n    \"\"\"\n    service_context = _load_service_context(fast, large)\n\n    if os.path.isdir(f\"{persist_dir}\") and not reset:\n        logger.info(\"Loading index from storage\")\n        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n        return load_index_from_storage(service_context=service_context, storage_context=storage_context)\n\n    logger.info(\"Creating index\")\n    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\n    logger.info(\"Saving index to storage\")\n    index.storage_context.persist(persist_dir=persist_dir)\n\n    return index", "\n\ndef _load_service_context(fast: bool = False, large: bool = False) -> ServiceContext:\n    \"\"\"\n    Load the service context.\n\n    Args:\n        fast (bool, optional): Whether to use the fast model. Defaults to False.\n        large (bool, optional): Whether to use the large model. Defaults to False.\n\n    Returns:\n        ServiceContext: The service context.\n    \"\"\"\n\n    context = _load_azure_openai_context()\n    model_name = (\n        context.turbo_llm_model_deployment_id\n        if fast\n        else context.large_llm_model_deployment_id\n        if large\n        else context.smart_llm_model_deployment_id\n    )\n\n    if openai.api_type == C.AZURE_API_TYPE:\n        llm_type = AzureGPT35Turbo if fast else AzureChatOpenAI\n        llm = llm_type(  # type: ignore\n            deployment_name=model_name,\n            model_kwargs={\n                \"api_key\": openai.api_key,\n                \"api_base\": openai.api_base,\n                \"api_type\": openai.api_type,\n                \"api_version\": openai.api_version,\n            },\n            max_retries=C.MAX_RETRIES,\n        )\n    else:\n        llm = ChatOpenAI(\n            model_name=model_name,\n            model_kwargs={\n                \"api_key\": openai.api_key,\n                \"api_base\": openai.api_base,\n                \"api_type\": openai.api_type,\n                \"api_version\": openai.api_version,\n            },\n            max_retries=C.MAX_RETRIES,\n        )\n\n    llm_predictor = LLMPredictor(llm=llm)\n\n    embedding_llm = LangchainEmbedding(\n        OpenAIEmbeddings(\n            model=\"text-embedding-ada-002\",\n        ),  # type: ignore\n        embed_batch_size=1,\n    )\n\n    return ServiceContext.from_defaults(\n        llm_predictor=llm_predictor,\n        embed_model=embedding_llm,\n    )", "\n\nclass AzureGPT35Turbo(AzureOpenAI):\n    \"\"\"Azure OpenAI Chat API.\"\"\"\n\n    @property\n    @override\n    def _default_params(self):\n        \"\"\"\n        Get the default parameters for calling OpenAI API.\n        gpt-35-turbo does not support best_of, logprobs, or echo.\n        \"\"\"\n        normal_params = {\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"presence_penalty\": self.presence_penalty,\n            \"n\": self.n,\n            \"request_timeout\": self.request_timeout,\n            \"logit_bias\": self.logit_bias,\n        }\n        return {**normal_params, **self.model_kwargs}", ""]}
{"filename": "src/gpt_review/prompts/_prompt.py", "chunked_list": ["\"\"\"Interface for a GPT Prompts.\"\"\"\nimport os\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom langchain.prompts import PromptTemplate, load_prompt\n\nimport gpt_review.constants as C\n\nif sys.version_info[:2] <= (3, 10):\n    from typing_extensions import Self\nelse:\n    from typing import Self", "import gpt_review.constants as C\n\nif sys.version_info[:2] <= (3, 10):\n    from typing_extensions import Self\nelse:\n    from typing import Self\n\n\n@dataclass\nclass LangChainPrompt(PromptTemplate):\n    \"\"\"A prompt for the GPT LangChain task.\"\"\"\n\n    prompt_yaml: str\n\n    @classmethod\n    def load(cls, prompt_yaml) -> Self:\n        \"\"\"Load the prompt.\"\"\"\n        return load_prompt(prompt_yaml)", "@dataclass\nclass LangChainPrompt(PromptTemplate):\n    \"\"\"A prompt for the GPT LangChain task.\"\"\"\n\n    prompt_yaml: str\n\n    @classmethod\n    def load(cls, prompt_yaml) -> Self:\n        \"\"\"Load the prompt.\"\"\"\n        return load_prompt(prompt_yaml)", "\n\ndef load_bug_yaml() -> LangChainPrompt:\n    \"\"\"Load the bug yaml.\"\"\"\n    yaml_path = os.getenv(\"PROMPT_BUG\", str(Path(__file__).parents[0].joinpath(C.BUG_PROMPT_YAML)))\n    return LangChainPrompt.load(yaml_path)\n\n\ndef load_coverage_yaml() -> LangChainPrompt:\n    \"\"\"Load the coverage yaml.\"\"\"\n    yaml_path = os.getenv(\"PROMPT_COVERAGE\", str(Path(__file__).parents[0].joinpath(C.COVERAGE_PROMPT_YAML)))\n    return LangChainPrompt.load(yaml_path)", "def load_coverage_yaml() -> LangChainPrompt:\n    \"\"\"Load the coverage yaml.\"\"\"\n    yaml_path = os.getenv(\"PROMPT_COVERAGE\", str(Path(__file__).parents[0].joinpath(C.COVERAGE_PROMPT_YAML)))\n    return LangChainPrompt.load(yaml_path)\n\n\ndef load_summary_yaml() -> LangChainPrompt:\n    \"\"\"Load the summary yaml.\"\"\"\n    yaml_path = os.getenv(\"PROMPT_SUMMARY\", str(Path(__file__).parents[0].joinpath(C.SUMMARY_PROMPT_YAML)))\n    return LangChainPrompt.load(yaml_path)", ""]}
{"filename": "src/gpt_review/prompts/__init__.py", "chunked_list": ["\"\"\"Collection of GPT Prompts.\"\"\"\n"]}
{"filename": "src/gpt_review/repositories/__init__.py", "chunked_list": ["\n"]}
{"filename": "src/gpt_review/repositories/github.py", "chunked_list": ["\"\"\"GitHub API helpers.\"\"\"\nimport json\nimport logging\nimport os\nfrom typing import Dict\n\nimport requests\nfrom knack import CLICommandsLoader\nfrom knack.arguments import ArgumentsContext\nfrom knack.commands import CommandGroup", "from knack.arguments import ArgumentsContext\nfrom knack.commands import CommandGroup\n\nfrom gpt_review._command import GPTCommandGroup\nfrom gpt_review._review import _summarize_files\nfrom gpt_review.repositories._repository import _RepositoryClient\n\n\nclass GitHubClient(_RepositoryClient):\n    \"\"\"GitHub client.\"\"\"\n\n    @staticmethod\n    def get_pr_diff(patch_repo=None, patch_pr=None, access_token=None) -> str:\n        \"\"\"\n        Get the diff of a PR.\n\n        Args:\n            patch_repo (str): The repo.\n            patch_pr (str): The PR.\n            access_token (str): The GitHub access token.\n\n        Returns:\n            str: The diff of the PR.\n        \"\"\"\n        patch_repo = patch_repo or os.getenv(\"PATCH_REPO\")\n        patch_pr = patch_pr or os.getenv(\"PATCH_PR\")\n        access_token = access_token or os.getenv(\"GITHUB_TOKEN\")\n\n        headers = {\n            \"Accept\": \"application/vnd.github.v3.diff\",\n            \"authorization\": f\"Bearer {access_token}\",\n        }\n\n        response = requests.get(\n            f\"https://api.github.com/repos/{patch_repo}/pulls/{patch_pr}\", headers=headers, timeout=10\n        )\n        return response.text\n\n    @staticmethod\n    def _post_pr_comment(review, git_commit_hash: str, link: str, access_token: str) -> requests.Response:\n        \"\"\"\n        Post a comment to a PR.\n\n        Args:\n            review (str): The review.\n            git_commit_hash (str): The git commit hash.\n            link (str): The link to the PR.\n            access_token (str): The GitHub access token.\n\n        Returns:\n            requests.Response: The response.\n        \"\"\"\n        data = {\"body\": review, \"commit_id\": git_commit_hash, \"event\": \"COMMENT\"}\n        data = json.dumps(data)\n\n        owner = link.split(\"/\")[-4]\n        repo = link.split(\"/\")[-3]\n        pr_number = link.split(\"/\")[-1]\n\n        headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"authorization\": f\"Bearer {access_token}\",\n        }\n        response = requests.get(\n            f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\", headers=headers, timeout=10\n        )\n        comments = response.json()\n\n        for comment in comments:\n            if (\n                \"user\" in comment\n                and comment[\"user\"][\"login\"] == \"github-actions[bot]\"\n                and \"body\" in comment\n                and \"Summary by GPT-4\" in comment[\"body\"]\n            ):\n                review_id = comment[\"id\"]\n                data = {\"body\": review}\n                data = json.dumps(data)\n\n                response = requests.put(\n                    f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews/{review_id}\",\n                    headers=headers,\n                    data=data,\n                    timeout=10,\n                )\n                break\n        else:\n            # https://api.github.com/repos/OWNER/REPO/pulls/PULL_NUMBER/reviews\n            response = requests.post(\n                f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\",\n                headers=headers,\n                data=data,\n                timeout=10,\n            )\n        logging.debug(response.json())\n        return response\n\n    @staticmethod\n    def post_pr_summary(diff) -> Dict[str, str]:\n        \"\"\"\n        Get a review of a PR.\n\n        Requires the following environment variables:\n            - LINK: The link to the PR.\n            - GIT_COMMIT_HASH: The git commit hash.\n            - GITHUB_TOKEN: The GitHub access token.\n\n        Args:\n            diff (str): The patch of the PR.\n\n        Returns:\n            Dict[str, str]: The review.\n        \"\"\"\n        review = _summarize_files(diff)\n        logging.debug(review)\n\n        link = os.getenv(\"LINK\")\n        git_commit_hash = os.getenv(\"GIT_COMMIT_HASH\")\n        access_token = os.getenv(\"GITHUB_TOKEN\")\n\n        if link and git_commit_hash and access_token:\n            GitHubClient._post_pr_comment(\n                review=review, git_commit_hash=git_commit_hash, link=link, access_token=access_token\n            )\n            return {\"response\": \"PR posted\"}\n\n        logging.warning(\"No PR to post too\")\n        return {\"response\": \"No PR to post too\"}", "class GitHubClient(_RepositoryClient):\n    \"\"\"GitHub client.\"\"\"\n\n    @staticmethod\n    def get_pr_diff(patch_repo=None, patch_pr=None, access_token=None) -> str:\n        \"\"\"\n        Get the diff of a PR.\n\n        Args:\n            patch_repo (str): The repo.\n            patch_pr (str): The PR.\n            access_token (str): The GitHub access token.\n\n        Returns:\n            str: The diff of the PR.\n        \"\"\"\n        patch_repo = patch_repo or os.getenv(\"PATCH_REPO\")\n        patch_pr = patch_pr or os.getenv(\"PATCH_PR\")\n        access_token = access_token or os.getenv(\"GITHUB_TOKEN\")\n\n        headers = {\n            \"Accept\": \"application/vnd.github.v3.diff\",\n            \"authorization\": f\"Bearer {access_token}\",\n        }\n\n        response = requests.get(\n            f\"https://api.github.com/repos/{patch_repo}/pulls/{patch_pr}\", headers=headers, timeout=10\n        )\n        return response.text\n\n    @staticmethod\n    def _post_pr_comment(review, git_commit_hash: str, link: str, access_token: str) -> requests.Response:\n        \"\"\"\n        Post a comment to a PR.\n\n        Args:\n            review (str): The review.\n            git_commit_hash (str): The git commit hash.\n            link (str): The link to the PR.\n            access_token (str): The GitHub access token.\n\n        Returns:\n            requests.Response: The response.\n        \"\"\"\n        data = {\"body\": review, \"commit_id\": git_commit_hash, \"event\": \"COMMENT\"}\n        data = json.dumps(data)\n\n        owner = link.split(\"/\")[-4]\n        repo = link.split(\"/\")[-3]\n        pr_number = link.split(\"/\")[-1]\n\n        headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"authorization\": f\"Bearer {access_token}\",\n        }\n        response = requests.get(\n            f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\", headers=headers, timeout=10\n        )\n        comments = response.json()\n\n        for comment in comments:\n            if (\n                \"user\" in comment\n                and comment[\"user\"][\"login\"] == \"github-actions[bot]\"\n                and \"body\" in comment\n                and \"Summary by GPT-4\" in comment[\"body\"]\n            ):\n                review_id = comment[\"id\"]\n                data = {\"body\": review}\n                data = json.dumps(data)\n\n                response = requests.put(\n                    f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews/{review_id}\",\n                    headers=headers,\n                    data=data,\n                    timeout=10,\n                )\n                break\n        else:\n            # https://api.github.com/repos/OWNER/REPO/pulls/PULL_NUMBER/reviews\n            response = requests.post(\n                f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/reviews\",\n                headers=headers,\n                data=data,\n                timeout=10,\n            )\n        logging.debug(response.json())\n        return response\n\n    @staticmethod\n    def post_pr_summary(diff) -> Dict[str, str]:\n        \"\"\"\n        Get a review of a PR.\n\n        Requires the following environment variables:\n            - LINK: The link to the PR.\n            - GIT_COMMIT_HASH: The git commit hash.\n            - GITHUB_TOKEN: The GitHub access token.\n\n        Args:\n            diff (str): The patch of the PR.\n\n        Returns:\n            Dict[str, str]: The review.\n        \"\"\"\n        review = _summarize_files(diff)\n        logging.debug(review)\n\n        link = os.getenv(\"LINK\")\n        git_commit_hash = os.getenv(\"GIT_COMMIT_HASH\")\n        access_token = os.getenv(\"GITHUB_TOKEN\")\n\n        if link and git_commit_hash and access_token:\n            GitHubClient._post_pr_comment(\n                review=review, git_commit_hash=git_commit_hash, link=link, access_token=access_token\n            )\n            return {\"response\": \"PR posted\"}\n\n        logging.warning(\"No PR to post too\")\n        return {\"response\": \"No PR to post too\"}", "\n\ndef _review(repository=None, pull_request=None, access_token=None) -> Dict[str, str]:\n    \"\"\"Review GitHub PR with Open AI, and post response as a comment.\n\n    Args:\n        repository (str): The repo of the PR.\n        pull_request (str): The PR number.\n        access_token (str): The GitHub access token.\n\n    Returns:\n        Dict[str, str]: The response.\n    \"\"\"\n    diff = GitHubClient.get_pr_diff(repository, pull_request, access_token)\n    GitHubClient.post_pr_summary(diff)\n    return {\"response\": \"Review posted as a comment.\"}", "\n\ndef _comment(question: str, comment_id: int, diff: str = \".diff\", link=None, access_token=None) -> Dict[str, str]:\n    \"\"\"\"\"\"\n    raise NotImplementedError\n\n\nclass GitHubCommandGroup(GPTCommandGroup):\n    \"\"\"Ask Command Group.\"\"\"\n\n    @staticmethod\n    def load_command_table(loader: CLICommandsLoader) -> None:\n        with CommandGroup(loader, \"github\", \"gpt_review.repositories.github#{}\", is_preview=True) as group:\n            group.command(\"review\", \"_review\", is_preview=True)\n\n    @staticmethod\n    def load_arguments(loader: CLICommandsLoader) -> None:\n        \"\"\"Add patch_repo, patch_pr, and access_token arguments.\"\"\"\n        with ArgumentsContext(loader, \"github\") as args:\n            args.argument(\n                \"access_token\",\n                type=str,\n                help=\"The GitHub access token. Set or use GITHUB_TOKEN environment variable.\",\n                default=None,\n            )\n            args.argument(\n                \"pull_request\",\n                type=str,\n                help=\"The PR number. Set or use PATCH_PR environment variable.\",\n                default=None,\n                options_list=(\"--pull-request\", \"-pr\"),\n            )\n            args.argument(\n                \"repository\",\n                type=str,\n                help=\"The repo of the PR. Set or use PATCH_REPO environment variable.\",\n                default=None,\n                options_list=(\"--repository\", \"-r\"),\n            )", ""]}
{"filename": "src/gpt_review/repositories/_repository.py", "chunked_list": ["\"\"\"Abstract class for a repository client.\"\"\"\nfrom abc import abstractmethod\n\n\nclass _RepositoryClient:\n    \"\"\"Abstract class for a repository client.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def get_pr_diff(patch_repo=None, patch_pr=None, access_token=None) -> str:\n        \"\"\"\n        Get the diff of a PR.\n\n        Args:\n            patch_repo (str): The repo.\n            patch_pr (str): The PR.\n            access_token (str): The GitHub access token.\n\n        Returns:\n            str: The diff of the PR.\n        \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def post_pr_summary(diff) -> None:\n        \"\"\"\n        Post a summary to a PR.\n\n        Args:\n            diff (str): The diff of the PR.\n\n        Returns:\n            str: The review of the PR.\n        \"\"\"", ""]}
{"filename": "src/gpt/__main__.py", "chunked_list": ["\"\"\"The GPT CLI entry point for python -m gpt\"\"\"\nimport sys\n\nfrom gpt_review._gpt_cli import cli\n\nif __name__ == \"__main__\":\n    exit_code = cli()\n    sys.exit(exit_code)\n", ""]}
{"filename": "src/gpt/__init__.py", "chunked_list": [""]}
