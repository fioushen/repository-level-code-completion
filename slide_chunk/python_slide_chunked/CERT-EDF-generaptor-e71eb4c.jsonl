{"filename": "setup.py", "chunked_list": ["\"\"\"Setup script\n\"\"\"\nfrom setuptools import setup\n\nsetup(package_data={'generaptor': ['data/*']})\n"]}
{"filename": "generaptor/main.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"Application\n\"\"\"\nfrom argparse import ArgumentParser\nfrom .command import setup_commands\nfrom .__version__ import version\nfrom .helper.cache import Cache\nfrom .helper.logging import LOGGER\n\n\ndef _parse_args():\n    parser = ArgumentParser(\n        description=\"Generate Velociraptor-based collectors in no time\"\n    )\n    parser.add_argument(\n        '--cache',\n        type=Cache,\n        default=Cache(),\n        help=\"Set cache directory\",\n    )\n    cmd = parser.add_subparsers(dest='cmd')\n    cmd.required = True\n    setup_commands(cmd)\n    return parser.parse_args()", "\n\ndef _parse_args():\n    parser = ArgumentParser(\n        description=\"Generate Velociraptor-based collectors in no time\"\n    )\n    parser.add_argument(\n        '--cache',\n        type=Cache,\n        default=Cache(),\n        help=\"Set cache directory\",\n    )\n    cmd = parser.add_subparsers(dest='cmd')\n    cmd.required = True\n    setup_commands(cmd)\n    return parser.parse_args()", "\n\ndef app():\n    \"\"\"Application entry point\"\"\"\n    LOGGER.info(\"Generaptor v%s\", version)\n    args = _parse_args()\n    args.func(args)\n\n\nif __name__ == '__main__':\n    app()", "\nif __name__ == '__main__':\n    app()\n"]}
{"filename": "generaptor/__init__.py", "chunked_list": ["\"\"\"Generaptor module\n\"\"\"\n"]}
{"filename": "generaptor/data/linux.generate.targets.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"Generate linux.targets.csv from linux.rules.csv\n\"\"\"\nfrom csv import reader, writer, QUOTE_MINIMAL\nfrom json import dumps\nfrom pathlib import Path\nfrom collections import defaultdict\n\nHERE = Path(__file__).resolve().parent\nRULES = HERE / 'linux.rules.csv'", "HERE = Path(__file__).resolve().parent\nRULES = HERE / 'linux.rules.csv'\nTARGETS = HERE / 'linux.targets.csv'\nDELIMITER = ','\nQUOTECHAR = '\"'\n\n\ndef app():\n    # parse rules\n    group_ruleids = defaultdict(set)\n    all_ruleids = set()\n    with RULES.open(newline='') as rules_fp:\n        csv_reader = reader(\n            rules_fp,\n            delimiter=DELIMITER,\n            quotechar=QUOTECHAR,\n        )\n        next(csv_reader)  # skip header\n        for row in csv_reader:\n            rule_id = int(row[0])\n            all_ruleids.add(rule_id)\n            group_ruleids[row[2]].add(rule_id)\n    # write targets\n    with TARGETS.open('w', newline='') as targets_fp:\n        csv_writer = writer(\n            targets_fp,\n            delimiter=DELIMITER,\n            quotechar=QUOTECHAR,\n            quoting=QUOTE_MINIMAL,\n        )\n        csv_writer.writerow(['Group', 'RuleIds'])\n        for group, ruleids in group_ruleids.items():\n            csv_writer.writerow([group, dumps(list(ruleids))])\n        csv_writer.writerow(['LinuxTriage', list(all_ruleids)])", "\n\nif __name__ == '__main__':\n    app()\n"]}
{"filename": "generaptor/command/get_secret.py", "chunked_list": ["\"\"\"get-secret command\n\"\"\"\nfrom pathlib import Path\nfrom ..helper.crypto import RSAPrivateKey, decrypt_secret, load_private_key\nfrom ..helper.logging import LOGGER\nfrom ..helper.collection import collection_metadata\n\n\ndef _print_collection_secret(private_key: RSAPrivateKey, collection: Path):\n    metadata = collection_metadata(collection)\n    if not metadata:\n        LOGGER.error(\"failed to retrieve metadata from collection.\")\n        return\n    for field in ('b64_enc_secret', 'fingerprint_hex'):\n        if field not in metadata:\n            LOGGER.error(\"metadata field not found: %s\", field)\n            return\n    LOGGER.info(\n        \"collection certificate fingerprint: %s\", metadata['fingerprint_hex']\n    )\n    b64_enc_secret = metadata['b64_enc_secret']\n    secret = decrypt_secret(private_key, b64_enc_secret)\n    print(f\"{secret.decode()}:{collection}\")", "def _print_collection_secret(private_key: RSAPrivateKey, collection: Path):\n    metadata = collection_metadata(collection)\n    if not metadata:\n        LOGGER.error(\"failed to retrieve metadata from collection.\")\n        return\n    for field in ('b64_enc_secret', 'fingerprint_hex'):\n        if field not in metadata:\n            LOGGER.error(\"metadata field not found: %s\", field)\n            return\n    LOGGER.info(\n        \"collection certificate fingerprint: %s\", metadata['fingerprint_hex']\n    )\n    b64_enc_secret = metadata['b64_enc_secret']\n    secret = decrypt_secret(private_key, b64_enc_secret)\n    print(f\"{secret.decode()}:{collection}\")", "\n\ndef _get_secret_cmd(args):\n    private_key = load_private_key(args.private_key)\n    if not private_key:\n        return\n    for collection in args.collections:\n        if collection.is_file():\n            _print_collection_secret(private_key, collection)\n            continue\n        if collection.is_dir():\n            for item in collection.glob('Collection_*.zip'):\n                _print_collection_secret(private_key, item)\n            continue\n        LOGGER.warning(\"skipped %s\", collection)", "\n\ndef setup_get_secret(cmd):\n    \"\"\"Setup get-secret command\"\"\"\n    get_secret = cmd.add_parser(\n        'get-secret', help=\"get the collection archive secret\"\n    )\n    get_secret.add_argument(\n        'private_key',\n        type=Path,\n        help=\"private key, given collections must share the same certificate fingerprint\",\n    )\n    get_secret.add_argument(\n        'collections',\n        metavar='collection',\n        nargs='+',\n        type=Path,\n        help=\"collection archives\",\n    )\n    get_secret.set_defaults(func=_get_secret_cmd)", ""]}
{"filename": "generaptor/command/__init__.py", "chunked_list": ["\"\"\"Command module\n\"\"\"\nfrom .refresh import setup_refresh\nfrom .generate import setup_generate\nfrom .get_secret import setup_get_secret\nfrom .get_fingerprint import setup_get_fingerprint\n\n\ndef setup_commands(cmd):\n    \"\"\"Setup commands\"\"\"\n    setup_refresh(cmd)\n    setup_generate(cmd)\n    setup_get_secret(cmd)\n    setup_get_fingerprint(cmd)", "def setup_commands(cmd):\n    \"\"\"Setup commands\"\"\"\n    setup_refresh(cmd)\n    setup_generate(cmd)\n    setup_get_secret(cmd)\n    setup_get_fingerprint(cmd)\n"]}
{"filename": "generaptor/command/get_fingerprint.py", "chunked_list": ["\"\"\"get-fingerprint command\n\"\"\"\nfrom pathlib import Path\nfrom ..helper.logging import LOGGER\nfrom ..helper.collection import collection_metadata\n\n\ndef _print_collection_fingerprint(collection: Path):\n    metadata = collection_metadata(collection)\n    if not metadata:\n        LOGGER.error(\"failed to retrieve metadata from collection.\")\n        return\n    if 'fingerprint_hex' not in metadata:\n        LOGGER.error(\"metadata field not found: fingerprint_hex\")\n        return\n    fingerprint_hex = metadata['fingerprint_hex']\n    print(f\"{fingerprint_hex}:{collection}\")", "\n\ndef _get_fingerprint_cmd(args):\n    for collection in args.collections:\n        if collection.is_file():\n            _print_collection_fingerprint(collection)\n            continue\n        if collection.is_dir():\n            for item in collection.glob('Collection_*.zip'):\n                _print_collection_fingerprint(item)\n            continue\n        LOGGER.warning(\"skipped %s\", collection)", "\n\ndef setup_get_fingerprint(cmd):\n    \"\"\"Setup get-fingerprint command\"\"\"\n    get_fingerprint = cmd.add_parser(\n        'get-fingerprint',\n        help=\"get the collection archive certificate fingerprint\",\n    )\n    get_fingerprint.add_argument(\n        'collections',\n        metavar='collection',\n        nargs='+',\n        type=Path,\n        help=\"collection archives\",\n    )\n    get_fingerprint.set_defaults(func=_get_fingerprint_cmd)", ""]}
{"filename": "generaptor/command/refresh.py", "chunked_list": ["\"\"\"refresh command\n\"\"\"\nfrom ..helper.http import http_set_proxies, http_download\nfrom ..helper.prompt import confirm\nfrom ..helper.github import github_release\nfrom ..helper.logging import LOGGER\nfrom ..helper.distrib import SUPPORTED_DISTRIBUTIONS\n\n\ndef _refresh_cmd(args):\n    if not (args.yes or confirm(\"Refreshing cache will flush current cache.\")):\n        return\n    LOGGER.info(\"refreshing cache...\")\n    args.cache.flush(args.refresh_config, args.do_not_fetch)\n    args.cache.ensure()\n    if args.do_not_fetch:\n        return\n    LOGGER.info(\"downloading %s release...\", args.fetch_tag)\n    if args.proxy_url:\n        http_set_proxies({'https': args.proxy_url})\n    gh_release = github_release('velocidex', 'velociraptor', args.fetch_tag)\n    LOGGER.info(\"velociraptor release matched: %s\", gh_release.tag)\n    downloaded = set()\n    for asset in gh_release.assets:\n        for distrib in SUPPORTED_DISTRIBUTIONS:\n            if distrib in downloaded:\n                continue\n            if not distrib.match_asset_name(asset.name):\n                continue\n            downloaded.add(distrib)\n            LOGGER.info(\n                \"%s matched asset '%s' (size=%d)\",\n                distrib,\n                asset.name,\n                asset.size,\n            )\n            http_download(asset.url, args.cache.path(asset.url.split('/')[-1]))", "\ndef _refresh_cmd(args):\n    if not (args.yes or confirm(\"Refreshing cache will flush current cache.\")):\n        return\n    LOGGER.info(\"refreshing cache...\")\n    args.cache.flush(args.refresh_config, args.do_not_fetch)\n    args.cache.ensure()\n    if args.do_not_fetch:\n        return\n    LOGGER.info(\"downloading %s release...\", args.fetch_tag)\n    if args.proxy_url:\n        http_set_proxies({'https': args.proxy_url})\n    gh_release = github_release('velocidex', 'velociraptor', args.fetch_tag)\n    LOGGER.info(\"velociraptor release matched: %s\", gh_release.tag)\n    downloaded = set()\n    for asset in gh_release.assets:\n        for distrib in SUPPORTED_DISTRIBUTIONS:\n            if distrib in downloaded:\n                continue\n            if not distrib.match_asset_name(asset.name):\n                continue\n            downloaded.add(distrib)\n            LOGGER.info(\n                \"%s matched asset '%s' (size=%d)\",\n                distrib,\n                asset.name,\n                asset.size,\n            )\n            http_download(asset.url, args.cache.path(asset.url.split('/')[-1]))", "\n\ndef setup_refresh(cmd):\n    \"\"\"Setup refresh command\"\"\"\n    refresh = cmd.add_parser('refresh', help=\"refresh environment cache\")\n    refresh.add_argument(\n        '--yes', '-y', action='store_true', help=\"non-interactive confirmation\"\n    )\n    refresh.add_argument(\n        '--refresh-config',\n        action='store_true',\n        help=\"refresh configuration files as well\",\n    )\n    refresh.add_argument(\n        '--do-not-fetch',\n        action='store_true',\n        help=\"do not fetch latest velociraptor release\",\n    )\n    refresh.add_argument(\n        '--fetch-tag',\n        default='v0.6.9',\n        help=(\n            \"fetch this tag, use 'latest' to fetch the latest version, warning:\"\n            \" fecthing another version than the default might break the collector\"\n        ),\n    )\n    refresh.add_argument('--proxy-url', help=\"set proxy url\")\n    refresh.set_defaults(func=_refresh_cmd)", ""]}
{"filename": "generaptor/command/generate.py", "chunked_list": ["\"\"\"generate command\n\"\"\"\nfrom json import loads\nfrom pathlib import Path\nfrom ..helper.crypto import provide_x509_certificate\nfrom ..helper.logging import LOGGER\nfrom ..helper.distrib import Distribution, Architecture, OperatingSystem\nfrom ..helper.validation import check_device\nfrom ..helper.generation import Generator\n", "from ..helper.generation import Generator\n\n\ndef _load_custom_profile_targets(custom_profile):\n    try:\n        custom_profile = loads(custom_profile.read_text())\n        return custom_profile.get('targets')\n    except:\n        LOGGER.exception(\"failed to load custom profile!\")\n    return None", "\n\ndef _select_default_targets(args, default_targets):\n    if args.custom:\n        return None\n    if args.custom_profile and args.custom_profile.is_file():\n        targets = _load_custom_profile_targets(args.custom_profile)\n        if not targets:\n            LOGGER.warning(\n                \"failed to load custom profile, using default targets instead.\"\n            )\n            return default_targets\n        return targets\n    return default_targets", "\n\ndef _generate_linux_cmd(args):\n    LOGGER.info(\"starting linux collector generator...\")\n    default_targets = _select_default_targets(args, ['LinuxTriage'])\n    if not check_device(args.device):\n        return\n    try:\n        certificate = provide_x509_certificate(\n            args.output_directory,\n            args.x509_certificate,\n            args.ask_password,\n        )\n    except KeyboardInterrupt:\n        print()\n        LOGGER.warning(\"operation canceled.\")\n        return\n    artifacts = ['Linux.Collector']\n    Generator(\n        Distribution(OperatingSystem.LINUX, Architecture(args.architecture)),\n        args.cache,\n        certificate,\n        args.output_directory,\n    ).generate(\n        {\n            'device': args.device,\n            'artifacts': ','.join([f'\"{artifact}\"' for artifact in artifacts]),\n        },\n        default_targets,\n    )", "\n\ndef _generate_windows_cmd(args):\n    LOGGER.info(\"starting windows collector generator...\")\n    default_targets = _select_default_targets(args, ['KapeTriage'])\n    if not check_device(args.device):\n        return\n    if args.device and not args.device.endswith(':'):\n        LOGGER.warning(\"assuming device name is '%s:'\", args.device)\n        args.device += ':'\n    try:\n        certificate = provide_x509_certificate(\n            args.output_directory,\n            args.x509_certificate,\n            args.ask_password,\n        )\n    except KeyboardInterrupt:\n        print()\n        LOGGER.warning(\"operation canceled.\")\n        return\n    artifacts = ['Windows.Collector']\n    Generator(\n        Distribution(OperatingSystem.WINDOWS, Architecture(args.architecture)),\n        args.cache,\n        certificate,\n        args.output_directory,\n    ).generate(\n        {\n            'device': args.device,\n            'artifacts': ','.join([f'\"{artifact}\"' for artifact in artifacts]),\n            'use_auto_accessor': 'N' if args.no_auto_accessor else 'Y',\n            'vss_analysis': 'N' if args.no_vss_analysis else 'Y',\n            'dont_be_lazy': 'Y' if args.dont_be_lazy else 'N',\n        },\n        default_targets,\n    )", "\n\ndef setup_generate(cmd):\n    \"\"\"Setup generate command\"\"\"\n    generate = cmd.add_parser('generate', help=\"generate a collector\")\n    generate.add_argument(\n        '--custom',\n        '-c',\n        action='store_true',\n        help=\"enable collector targets customization (interactive)\",\n    )\n    generate.add_argument(\n        '--custom-profile',\n        '--cp',\n        type=Path,\n        help=\"use given customization profile (non interactive)\",\n    )\n    generate.add_argument(\n        '--output-directory',\n        '-o',\n        type=Path,\n        default=Path('output').resolve(),\n        help=\"set output folder\",\n    )\n    generate.add_argument(\n        '--x509-certificate',\n        '-x',\n        type=Path,\n        help=\"bring your own x509 certificate instead of generating one \"\n        \"(must be RSA)\",\n    )\n    generate.add_argument(\n        '--ask-password',\n        '-p',\n        action='store_true',\n        help=\"prompt for private key password instead of generating one or \"\n        \"reading GENERAPTOR_PK_SECRET environment variable (ignored if \"\n        \"--x509 is used)\",\n    )\n    target = generate.add_subparsers(dest='target')\n    target.required = True\n    linux = target.add_parser('linux', help=\"generate linux collector\")\n    linux.set_defaults(func=_generate_linux_cmd)\n    linux.add_argument(\n        '--architecture',\n        '-a',\n        default=Architecture.AMD64.value,\n        choices=[arch.value for arch in Architecture],\n        help=\"set released binary architecture\",\n    )\n    linux.add_argument(\n        '--device',\n        '-d',\n        default='',\n        help=\"set root directory (absolute path), empty means '/'\",\n    )\n    windows = target.add_parser('windows', help=\"Generate windows collector\")\n    windows.set_defaults(func=_generate_windows_cmd)\n    windows.add_argument(\n        '--architecture',\n        '-a',\n        default=Architecture.AMD64.value,\n        choices=[arch.value for arch in Architecture],\n        help=\"set released binary architecture\",\n    )\n    windows.add_argument(\n        '--device',\n        '-d',\n        default='',\n        help=\"set root directory (absolute path), empty means all filesystems\",\n    )\n    windows.add_argument(\n        '--no-auto-accessor',\n        action='store_true',\n        help=\"disable auto accessor (which automatically select fastest collection technique)\",\n    )\n    windows.add_argument(\n        '--no-vss-analysis',\n        action='store_true',\n        help=\"disable windows volume shadow copies analysis\",\n    )\n    windows.add_argument(\n        '--dont-be-lazy',\n        action='store_true',\n        help=\"disable lazy_ntfs accessor (which uses OS api calls to collect files)\",\n    )", ""]}
{"filename": "generaptor/helper/collection.py", "chunked_list": ["\"\"\"Collection archive helper\n\"\"\"\nfrom json import loads\nfrom zipfile import ZipFile\nfrom pathlib import Path\n\n\ndef collection_metadata(archive: Path):\n    \"\"\"Load JSON metadata from archive\"\"\"\n    with ZipFile(archive) as zipf:\n        try:\n            zipinf = zipf.getinfo('metadata.json')\n        except KeyError:\n            return None\n        data = zipf.read(zipinf)\n        metadata_objects = loads(data.decode())\n        return metadata_objects[0]", ""]}
{"filename": "generaptor/helper/distrib.py", "chunked_list": ["\"\"\"Distribution\n\"\"\"\nfrom enum import Enum\nfrom dataclasses import dataclass\n\n\nclass Architecture(Enum):\n    \"\"\"Software architecture\"\"\"\n\n    X86 = '386'\n    AMD64 = 'amd64'\n    AMD64_MUSL = 'amd64-musl'\n    ARM64 = 'arm64'", "\n\nclass OperatingSystem(Enum):\n    \"\"\"Operating system\"\"\"\n\n    WINDOWS = 'windows'\n    DARWIN = 'darwin'\n    LINUX = 'linux'\n\n", "\n\n@dataclass\nclass Distribution:\n    \"\"\"Combination of operating system and architecture\"\"\"\n\n    os: OperatingSystem\n    arch: Architecture\n\n    def __hash__(self):\n        return hash((self.os, self.arch))\n\n    @property\n    def suffix(self):\n        \"\"\"Filename suffix\"\"\"\n        suffix = '-'.join([self.os.value, self.arch.value])\n        suffix += '.exe' if self.os == OperatingSystem.WINDOWS else ''\n        return suffix\n\n    def match_asset_name(self, name: str):\n        \"\"\"Determine if asset name matches this distribution\"\"\"\n        return name.endswith(self.suffix)", "\n\nSUPPORTED_DISTRIBUTIONS = [\n    Distribution(OperatingSystem.LINUX, Architecture.AMD64),\n    Distribution(OperatingSystem.LINUX, Architecture.AMD64_MUSL),\n    Distribution(OperatingSystem.WINDOWS, Architecture.X86),\n    Distribution(OperatingSystem.WINDOWS, Architecture.AMD64),\n]\n", ""]}
{"filename": "generaptor/helper/cache.py", "chunked_list": ["\"\"\"Cache helpers\n\"\"\"\nimport typing as t\nfrom csv import reader\nfrom json import loads\nfrom shutil import copy\nfrom pathlib import Path\nfrom platform import system, architecture\nfrom dataclasses import dataclass\nfrom jinja2 import FileSystemLoader, Environment, Template", "from dataclasses import dataclass\nfrom jinja2 import FileSystemLoader, Environment, Template\nfrom .logging import LOGGER\nfrom .distrib import Distribution, OperatingSystem, Architecture\n\n\nHERE = Path(__file__).resolve()\nPKG_DATA_DIR = HERE.parent.parent / 'data'\nPLATFORM_DISTRIB_MAP = {\n    'Linux': Distribution(OperatingSystem.LINUX, Architecture.AMD64),", "PLATFORM_DISTRIB_MAP = {\n    'Linux': Distribution(OperatingSystem.LINUX, Architecture.AMD64),\n    'Windows': Distribution(OperatingSystem.WINDOWS, Architecture.AMD64),\n}\n\n\ndef _stream_csv(csv_filepath: Path):\n    with csv_filepath.open(newline='') as csv_fp:\n        csv_reader = reader(csv_fp, delimiter=',', quotechar='\"')\n        try:\n            next(csv_reader)  # skip csv header\n        except StopIteration:\n            return\n        for row in csv_reader:\n            yield row", "\n\ndef _copy_pkg_data_to_cache(pattern, cache_dir):\n    for src_path in PKG_DATA_DIR.glob(pattern):\n        dst_path = cache_dir / src_path.name\n        if not dst_path.is_file():\n            copy(src_path, dst_path)\n\n\n@dataclass\nclass Cache:\n    \"\"\"Cache directory\"\"\"\n\n    directory: Path = Path.home() / '.cache' / 'generaptor'\n\n    @property\n    def program(self):\n        \"\"\"Cache program directory\"\"\"\n        return self.directory / 'program'\n\n    def path(self, filename: str) -> Path:\n        \"\"\"Generate program path for filename\"\"\"\n        filepath = (self.program / filename).resolve()\n        if not filepath.is_relative_to(self.program):\n            LOGGER.warning(\"path traversal attempt!\")\n            return None\n        return filepath\n\n    def flush(self, update_config=False, do_not_fetch=False):\n        \"\"\"Flush cached config and/or programs\"\"\"\n        if self.program.is_dir() and not do_not_fetch:\n            for filepath in self.program.iterdir():\n                filepath.unlink()\n        if self.directory.is_dir() and update_config:\n            for filepath in self.directory.iterdir():\n                if not filepath.is_file():\n                    continue\n                filepath.unlink()\n\n    def ensure(self) -> bool:\n        \"\"\"Ensure that the cache directory is valid and mandatory files are present\"\"\"\n        # attempt to create directory anyway\n        self.program.mkdir(parents=True, exist_ok=True)\n        # copy configuration templates\n        _copy_pkg_data_to_cache('*.collector.yml', self.directory)\n        # copy targets datasets\n        _copy_pkg_data_to_cache('*.targets.csv', self.directory)\n        # copy rules datasets\n        _copy_pkg_data_to_cache('*.rules.csv', self.directory)\n        return True\n\n    def load_rules(self, distrib: Distribution):\n        \"\"\"Load rules from cache matching given distribution\"\"\"\n        filepath = self.directory / f'{distrib.os.value}.rules.csv'\n        rules = {int(row[0]): row[1:] for row in _stream_csv(filepath)}\n        LOGGER.info(\"loaded %d rules.\", len(rules))\n        return rules\n\n    def load_targets(self, distrib: Distribution):\n        \"\"\"Load targets from cache matching given distribution\"\"\"\n        filepath = self.directory / f'{distrib.os.value}.targets.csv'\n        targets = {}\n        for row in _stream_csv(filepath):\n            targets[row[0]] = set(loads(row[1]))\n        LOGGER.info(\"loaded %d targets.\", len(targets))\n        return targets\n\n    def template_config(self, distrib: Distribution) -> Template:\n        \"\"\"Load jinja template matching given distribution\"\"\"\n        loader = FileSystemLoader(self.directory)\n        environment = Environment(\n            loader=loader,\n            autoescape=False,  # worst case scenario: we generate invalid YAML\n            trim_blocks=False,\n            lstrip_blocks=False,\n            keep_trailing_newline=True,\n        )\n        return environment.get_template(f'{distrib.os.value}.collector.yml')\n\n    def template_binary(self, distrib: Distribution) -> t.Optional[Path]:\n        \"\"\"Return template binary for distrib\"\"\"\n        try:\n            return next(self.program.glob(f'*{distrib.suffix}'))\n        except StopIteration:\n            LOGGER.critical(\n                \"distribution file not found in cache! Please update the cache.\"\n            )\n            return None\n\n    def platform_binary(self) -> t.Optional[Path]:\n        \"\"\"Platform binary to be used to produce collectors\"\"\"\n        if architecture()[0] != '64bit':\n            LOGGER.critical(\"current machine architecture is not supported!\")\n            return None\n        distrib = PLATFORM_DISTRIB_MAP.get(system())\n        if not distrib:\n            LOGGER.critical(\"current machine distribution is not supported!\")\n            return None\n        return self.template_binary(distrib)", "\n@dataclass\nclass Cache:\n    \"\"\"Cache directory\"\"\"\n\n    directory: Path = Path.home() / '.cache' / 'generaptor'\n\n    @property\n    def program(self):\n        \"\"\"Cache program directory\"\"\"\n        return self.directory / 'program'\n\n    def path(self, filename: str) -> Path:\n        \"\"\"Generate program path for filename\"\"\"\n        filepath = (self.program / filename).resolve()\n        if not filepath.is_relative_to(self.program):\n            LOGGER.warning(\"path traversal attempt!\")\n            return None\n        return filepath\n\n    def flush(self, update_config=False, do_not_fetch=False):\n        \"\"\"Flush cached config and/or programs\"\"\"\n        if self.program.is_dir() and not do_not_fetch:\n            for filepath in self.program.iterdir():\n                filepath.unlink()\n        if self.directory.is_dir() and update_config:\n            for filepath in self.directory.iterdir():\n                if not filepath.is_file():\n                    continue\n                filepath.unlink()\n\n    def ensure(self) -> bool:\n        \"\"\"Ensure that the cache directory is valid and mandatory files are present\"\"\"\n        # attempt to create directory anyway\n        self.program.mkdir(parents=True, exist_ok=True)\n        # copy configuration templates\n        _copy_pkg_data_to_cache('*.collector.yml', self.directory)\n        # copy targets datasets\n        _copy_pkg_data_to_cache('*.targets.csv', self.directory)\n        # copy rules datasets\n        _copy_pkg_data_to_cache('*.rules.csv', self.directory)\n        return True\n\n    def load_rules(self, distrib: Distribution):\n        \"\"\"Load rules from cache matching given distribution\"\"\"\n        filepath = self.directory / f'{distrib.os.value}.rules.csv'\n        rules = {int(row[0]): row[1:] for row in _stream_csv(filepath)}\n        LOGGER.info(\"loaded %d rules.\", len(rules))\n        return rules\n\n    def load_targets(self, distrib: Distribution):\n        \"\"\"Load targets from cache matching given distribution\"\"\"\n        filepath = self.directory / f'{distrib.os.value}.targets.csv'\n        targets = {}\n        for row in _stream_csv(filepath):\n            targets[row[0]] = set(loads(row[1]))\n        LOGGER.info(\"loaded %d targets.\", len(targets))\n        return targets\n\n    def template_config(self, distrib: Distribution) -> Template:\n        \"\"\"Load jinja template matching given distribution\"\"\"\n        loader = FileSystemLoader(self.directory)\n        environment = Environment(\n            loader=loader,\n            autoescape=False,  # worst case scenario: we generate invalid YAML\n            trim_blocks=False,\n            lstrip_blocks=False,\n            keep_trailing_newline=True,\n        )\n        return environment.get_template(f'{distrib.os.value}.collector.yml')\n\n    def template_binary(self, distrib: Distribution) -> t.Optional[Path]:\n        \"\"\"Return template binary for distrib\"\"\"\n        try:\n            return next(self.program.glob(f'*{distrib.suffix}'))\n        except StopIteration:\n            LOGGER.critical(\n                \"distribution file not found in cache! Please update the cache.\"\n            )\n            return None\n\n    def platform_binary(self) -> t.Optional[Path]:\n        \"\"\"Platform binary to be used to produce collectors\"\"\"\n        if architecture()[0] != '64bit':\n            LOGGER.critical(\"current machine architecture is not supported!\")\n            return None\n        distrib = PLATFORM_DISTRIB_MAP.get(system())\n        if not distrib:\n            LOGGER.critical(\"current machine distribution is not supported!\")\n            return None\n        return self.template_binary(distrib)", ""]}
{"filename": "generaptor/helper/__init__.py", "chunked_list": ["\"\"\"Helper package\n\"\"\"\n"]}
{"filename": "generaptor/helper/prompt.py", "chunked_list": ["\"\"\"Prompt helpers\n\"\"\"\nfrom pick import pick\n\n\ndef confirm(warning):\n    \"\"\"Prompt for confirmation\"\"\"\n    print(warning)\n    try:\n        answer = input(\"Do you want to proceed? [yes/NO]: \")\n    except KeyboardInterrupt:\n        return False\n    return answer.lower() == 'yes'", "\n\ndef multiselect(title, options):\n    \"\"\"Pick multiselection wrapper\"\"\"\n    return [\n        option\n        for option, _ in pick(\n            options, title, multiselect=True, min_selection_count=1\n        )\n    ]", ""]}
{"filename": "generaptor/helper/github.py", "chunked_list": ["\"\"\"Github helpers\n\"\"\"\nimport typing as t\nfrom dataclasses import dataclass\nfrom .http import http_get_json\n\n\n@dataclass\nclass GithubAsset:\n    \"\"\"Github asset data\"\"\"\n    name: str\n    size: int\n    url: str", "class GithubAsset:\n    \"\"\"Github asset data\"\"\"\n    name: str\n    size: int\n    url: str\n\n\n@dataclass\nclass GithubRelease:\n    \"\"\"Github release data\"\"\"\n    name: str\n    tag: str\n    assets: t.List[GithubAsset]\n\n    @classmethod\n    def from_dict(cls, dct):\n        return cls(\n            name=dct['name'],\n            tag=dct['tag_name'],\n            assets=[\n                GithubAsset(\n                    name=asset['name'],\n                    size=asset['size'],\n                    url=asset['browser_download_url']\n                )\n                for asset in dct['assets']\n            ],\n        )", "class GithubRelease:\n    \"\"\"Github release data\"\"\"\n    name: str\n    tag: str\n    assets: t.List[GithubAsset]\n\n    @classmethod\n    def from_dict(cls, dct):\n        return cls(\n            name=dct['name'],\n            tag=dct['tag_name'],\n            assets=[\n                GithubAsset(\n                    name=asset['name'],\n                    size=asset['size'],\n                    url=asset['browser_download_url']\n                )\n                for asset in dct['assets']\n            ],\n        )", "\n\ndef github_release(\n    owner: str, repository: str, tag: str = 'latest'\n) -> GithubRelease:\n    \"\"\"Get a summary of the latest release published in a Github repository\"\"\"\n    page = 1\n    while page:\n        url = f'https://api.github.com/repos/{owner}/{repository}/releases?per_page=10&page={page}'\n        releases = http_get_json(url)\n        if not releases:\n            return None\n        for release in releases:\n            if release['draft'] or release['prerelease']:\n                continue\n            if tag == 'latest':\n                return GithubRelease.from_dict(release)\n            if tag == release['tag_name']:\n                return GithubRelease.from_dict(release)\n        page += 1\n    return None", ""]}
{"filename": "generaptor/helper/generation.py", "chunked_list": ["\"\"\"Generation\n\"\"\"\nfrom io import StringIO\nfrom csv import writer, QUOTE_MINIMAL\nfrom pathlib import Path\nfrom platform import system\nfrom datetime import datetime\nfrom subprocess import run\nfrom .cache import Cache\nfrom .crypto import Certificate, fingerprint, pem_string", "from .cache import Cache\nfrom .crypto import Certificate, fingerprint, pem_string\nfrom .prompt import multiselect\nfrom .logging import LOGGER\nfrom .distrib import Distribution\n\n\ndef _dump_file_globs(selected_rules):\n    imstr = StringIO()\n    csv_writer = writer(\n        imstr, delimiter=',', quotechar='\"', quoting=QUOTE_MINIMAL\n    )\n    for rule in selected_rules:\n        csv_writer.writerow(rule[2:4])\n    file_globs = imstr.getvalue()\n    imstr.close()\n    return file_globs", "\n\nclass Generator:\n    \"\"\"Generate configuration file and velociraptor pre-configured binary\"\"\"\n\n    def __init__(\n        self,\n        distrib: Distribution,\n        cache: Cache,\n        certificate: Certificate,\n        output_directory: Path,\n    ):\n        self._distrib = distrib\n        self._cache = cache\n        self._certificate = certificate\n        self._output_directory = output_directory\n\n    def _select_globs(self, default_targets=None):\n        rules = self._cache.load_rules(self._distrib)\n        targets = self._cache.load_targets(self._distrib)\n        title = \"Pick one or more collection targets\"\n        options = list(sorted(targets.keys()))\n        selected_targets = default_targets\n        if not selected_targets:\n            selected_targets = multiselect(title, options)\n        selected_indices = set()\n        LOGGER.info(\"generating for targets:\")\n        for target in selected_targets:\n            indices = targets[target]\n            LOGGER.info(\" * %s (%d rules)\", target, len(indices))\n            selected_indices.update(indices)\n        return _dump_file_globs(rules[index] for index in selected_indices)\n\n    def _generate_config(self, context, output_config):\n        with output_config.open('wb') as fstream:\n            template = self._cache.template_config(self._distrib)\n            stream = template.stream(context)\n            stream.dump(fstream, encoding='utf-8')\n\n    def generate(self, context, default_targets=None):\n        \"\"\"Generate a configuration file and a pre-configured binary\"\"\"\n        # check platform binary availability\n        platform_binary = self._cache.platform_binary()\n        if not platform_binary:\n            LOGGER.critical(\"unsupported platform!\")\n            return\n        if system() == 'Linux':\n            platform_binary.chmod(0o700)\n        try:\n            file_globs = self._select_globs(default_targets)\n        except KeyboardInterrupt:\n            LOGGER.warning(\"operation canceled by user.\")\n            return\n        context.update(\n            {\n                'cert_data_pem_str': pem_string(self._certificate),\n                'cert_fingerprint_hex': fingerprint(self._certificate),\n                'file_globs': file_globs,\n            }\n        )\n        self._output_directory.mkdir(parents=True, exist_ok=True)\n        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n        output_config = self._output_directory / f'collector-{timestamp}.yml'\n        output_binary = (\n            self._output_directory\n            / f'collector-{timestamp}-{self._distrib.suffix}'\n        )\n        template_binary = self._cache.template_binary(self._distrib)\n        LOGGER.info(\"generating configuration...\")\n        self._generate_config(context, output_config)\n        LOGGER.info(\"configuration written to: %s\", output_config)\n        LOGGER.info(\"generating release binary...\")\n        args = [\n            str(platform_binary),\n            'config',\n            'repack',\n            '--exe',\n            str(template_binary),\n            str(output_config),\n            str(output_binary),\n        ]\n        LOGGER.info(\"running command: %s\", args)\n        run(args, check=True)\n        LOGGER.info(\"release binary written to: %s\", output_binary)", ""]}
{"filename": "generaptor/helper/http.py", "chunked_list": ["\"\"\"HTTP helpers\n\"\"\"\nfrom json import load, JSONDecodeError\nfrom shutil import copyfileobj\nfrom pathlib import Path\nfrom urllib.request import (\n    install_opener,\n    build_opener,\n    urlopen,\n    ProxyHandler,", "    urlopen,\n    ProxyHandler,\n)\nfrom rich.progress import wrap_file\nfrom .logging import LOGGER\n\n\ndef http_set_proxies(proxies):\n    \"\"\"Configure proxies\"\"\"\n    LOGGER.info(\"using proxies %s\", proxies)\n    install_opener(build_opener(ProxyHandler(proxies)))", "\n\ndef http_download(url: str, filepath: Path):\n    \"\"\"Download a resource and store it inside a file\"\"\"\n    LOGGER.info(\"downloading from %s\", url)\n    with urlopen(url) as response:\n        size = int(response.headers['Content-Length'])\n        with wrap_file(\n            response, total=size, description=\"Downloading\"\n        ) as wrapped:\n            with filepath.open('wb') as file:\n                copyfileobj(wrapped, file)", "\n\ndef http_get_json(url: str):\n    \"\"\"GET a JSON resource\"\"\"\n    LOGGER.info(\"requesting %s\", url)\n    with urlopen(url) as response:\n        if response.status != 200:\n            LOGGER.error(\"response status %d\", response.status)\n            return None\n        try:\n            return load(response)\n        except JSONDecodeError:\n            LOGGER.error(\"failed to decode json data!\")\n    return None", ""]}
{"filename": "generaptor/helper/crypto.py", "chunked_list": ["\"\"\"Cryptography helper\n\"\"\"\nimport typing as t\nfrom os import getenv\nfrom base64 import b64decode\nfrom pathlib import Path\nfrom getpass import getpass\nfrom secrets import token_urlsafe\nfrom datetime import datetime, timedelta\nfrom cryptography.x509 import (", "from datetime import datetime, timedelta\nfrom cryptography.x509 import (\n    load_pem_x509_certificate,\n    random_serial_number,\n    SubjectAlternativeName,\n    CertificateBuilder,\n    NameAttribute,\n    Certificate,\n    DNSName,\n    Name,", "    DNSName,\n    Name,\n)\nfrom cryptography.x509.oid import NameOID\nfrom cryptography.hazmat.primitives.hashes import SHA256, SHA512\nfrom cryptography.hazmat.primitives.serialization import (\n    load_pem_private_key,\n    BestAvailableEncryption,\n    PrivateFormat,\n    Encoding,", "    PrivateFormat,\n    Encoding,\n)\nfrom cryptography.hazmat.primitives.asymmetric.rsa import (\n    generate_private_key,\n    RSAPrivateKey,\n)\nfrom cryptography.hazmat.primitives.asymmetric.padding import OAEP, MGF1\nfrom .logging import LOGGER\n", "from .logging import LOGGER\n\n\nVALIDITY = timedelta(days=30)\nRSA_KEY_SIZE = 4096\nRSA_PUBLIC_EXPONENT = 65537\n\n\ndef fingerprint(certificate: Certificate):\n    \"\"\"Certificate SHA256 fingerprint\"\"\"\n    return certificate.fingerprint(SHA256()).hex()", "def fingerprint(certificate: Certificate):\n    \"\"\"Certificate SHA256 fingerprint\"\"\"\n    return certificate.fingerprint(SHA256()).hex()\n\n\ndef pem_string(certificate: Certificate):\n    \"\"\"Certificate as a PEM string\"\"\"\n    crt_pem_bytes = certificate.public_bytes(Encoding.PEM)\n    crt_pem_string = crt_pem_bytes.decode()\n    crt_pem_string = crt_pem_string.replace('\\n', '\\\\n')\n    return crt_pem_string", "\n\ndef _provide_private_key_secret(\n    ask_password: bool = False, raise_if_generate: bool = False\n) -> str:\n    # attempt to load the secret from the environment\n    private_key_secret = getenv('GENERAPTOR_PK_SECRET')\n    # interactively ask the user for the secret if necessary\n    if not private_key_secret and ask_password:\n        private_key_secret = getpass(\"private key secret: \")\n    # generate and display the secret if necessary\n    if not private_key_secret:\n        if raise_if_generate:\n            raise ValueError(\"failed to provide private key secret\")\n        private_key_secret = token_urlsafe(16)\n        LOGGER.warning(\"private key secret is %s\", private_key_secret)\n        LOGGER.warning(\"store this secret in a vault please!\")\n    return private_key_secret", "\n\ndef _generate_self_signed_certificate(\n    output_directory: Path, ask_password: bool = False\n) -> Certificate:\n    LOGGER.info(\"generating private key... please wait...\")\n    private_key = generate_private_key(\n        public_exponent=RSA_PUBLIC_EXPONENT,\n        key_size=RSA_KEY_SIZE,\n    )\n    subject_name = issuer_name = Name(\n        [\n            NameAttribute(NameOID.COMMON_NAME, \"generaptor\"),\n        ]\n    )\n    utc_now = datetime.utcnow()\n    LOGGER.info(\"generating certificate...\")\n    certificate = (\n        CertificateBuilder()\n        .subject_name(\n            subject_name,\n        )\n        .issuer_name(\n            issuer_name,\n        )\n        .public_key(\n            private_key.public_key(),\n        )\n        .serial_number(\n            random_serial_number(),\n        )\n        .not_valid_before(\n            utc_now,\n        )\n        .not_valid_after(\n            utc_now + VALIDITY,\n        )\n        .add_extension(\n            SubjectAlternativeName([DNSName(\"generaptor\")]),\n            critical=False,\n        )\n        .sign(private_key, SHA256())\n    )\n    # ensure output directory exists\n    output_directory.mkdir(parents=True, exist_ok=True)\n    # retrieve private key password\n    private_key_secret = _provide_private_key_secret(ask_password=ask_password)\n    # store encrypted private key in a file\n    fingerprint_hex = fingerprint(certificate)\n    (output_directory / f'{fingerprint_hex}.key.pem').write_bytes(\n        private_key.private_bytes(\n            encoding=Encoding.PEM,\n            format=PrivateFormat.TraditionalOpenSSL,\n            encryption_algorithm=BestAvailableEncryption(\n                private_key_secret.encode()\n            ),\n        ),\n    )\n    # store certificate in a file\n    crt_pem_bytes = certificate.public_bytes(Encoding.PEM)\n    (output_directory / f'{fingerprint_hex}.crt.pem').write_bytes(\n        crt_pem_bytes\n    )\n    return certificate", "\n\ndef provide_x509_certificate(\n    output_directory: Path,\n    cert_filepath: t.Optional[Path] = None,\n    ask_password: bool = False,\n) -> str:\n    \"\"\"Provide x509 certificate\"\"\"\n    if cert_filepath and cert_filepath.is_file():\n        crt_pem_bytes = cert_filepath.read_bytes()\n        certificate = load_pem_x509_certificate(crt_pem_bytes)\n        LOGGER.info(\n            \"using certificate %s fingerprint %s\",\n            cert_filepath,\n            fingerprint(certificate),\n        )\n    else:\n        certificate = _generate_self_signed_certificate(\n            output_directory, ask_password\n        )\n    return certificate", "\n\ndef load_private_key(private_key_path: Path) -> t.Optional[RSAPrivateKey]:\n    try:\n        private_key_secret = _provide_private_key_secret(\n            ask_password=True, raise_if_generate=True\n        )\n    except (ValueError, KeyboardInterrupt):\n        private_key_secret = None\n    if not private_key_secret:\n        LOGGER.warning(\"failed to provide private key secret\")\n        return None\n    return load_pem_private_key(\n        private_key_path.read_bytes(), private_key_secret.encode()\n    )", "\n\ndef decrypt_secret(private_key: RSAPrivateKey, b64_enc_secret: str) -> bytes:\n    \"\"\"Decrypt a base64-encoded secret using given private key\"\"\"\n    enc_secret = b64decode(b64_enc_secret)\n    secret = private_key.decrypt(\n        enc_secret,\n        OAEP(mgf=MGF1(algorithm=SHA512()), algorithm=SHA512(), label=None),\n    )\n    return secret", ""]}
{"filename": "generaptor/helper/validation.py", "chunked_list": ["\"\"\"Validation helpers\n\"\"\"\nfrom .logging import LOGGER\n\n\ndef check_device(device: str):\n    \"\"\"Check device name\"\"\"\n    if '\"' in device:\n        LOGGER.critical(\"device name cannot contain '\\\"'.\")\n        return False\n    return True", ""]}
{"filename": "generaptor/helper/logging.py", "chunked_list": ["\"\"\"Logging\n\"\"\"\nfrom logging import basicConfig, getLogger\nfrom rich.console import Console\nfrom rich.logging import RichHandler\n\nbasicConfig(\n    level='INFO',\n    format=\"%(message)s\",\n    datefmt=\"[%Y-%m-%dT%H:%M:%S]\",", "    format=\"%(message)s\",\n    datefmt=\"[%Y-%m-%dT%H:%M:%S]\",\n    handlers=[RichHandler(console=Console(stderr=True))],\n)\n\nLOGGER = getLogger('generaptor')\n"]}
