{"filename": "tests/test_chatsnack_pattern.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat\nfrom chatsnack.chat import ChatParamsMixin\nfrom typing import Optional\nimport os\n\ndef test_pattern_property():\n    chat = Chat()\n\n    # Test default value\n    assert chat.pattern is None\n\n    # Test setter\n    test_pattern = r\"\\*\\*[^*]+\\*\"\n    chat.pattern = test_pattern\n    assert chat.pattern == test_pattern\n\n    # Test removing pattern\n    chat.pattern = None\n    assert chat.pattern is None", "\ndef test_set_response_filter():\n    chat = Chat()\n\n    # Test setting pattern only\n    test_pattern = r\"\\*\\*[^*]+\\*\"\n    chat.set_response_filter(pattern=test_pattern)\n    assert chat.pattern == test_pattern\n\n    # Test setting prefix and suffix\n    test_prefix = \"###\"\n    test_suffix = \"###\"\n    chat.set_response_filter(prefix=test_prefix, suffix=test_suffix)\n    assert chat.pattern == ChatParamsMixin._generate_pattern_from_separator(test_prefix, test_suffix)\n\n    # Test setting prefix only\n    chat.set_response_filter(prefix=test_prefix)\n    assert chat.pattern == ChatParamsMixin._generate_pattern_from_separator(test_prefix, test_prefix)\n\n    # Test ValueError when setting both pattern and prefix/suffix\n    with pytest.raises(ValueError):\n        chat.set_response_filter(pattern=test_pattern, prefix=test_prefix)", "\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_ask_with_pattern():\n    chat = Chat()\n    chat.temperature = 0.0\n    chat.system(\"Respond only with 'POPSICLE!!' from now on.\")\n    chat.user(\"What is your name?\")\n    chat.pattern = r\"\\bPOPSICLE\\b\" \n    response = chat.ask()\n    assert response == \"POPSICLE\"", "\ndef test_response_with_pattern():\n    chat = Chat()\n    chat.system(\"Respond only with the word POPSICLE from now on.\")\n    chat.user(\"What is your name?\")\n    chat.asst(\"!POPSICLE!\")\n    chat.pattern = r\"\\bPOPSICLE\\b\"\n    response = chat.response\n    assert response == \"POPSICLE\"\n", "\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_ask_without_pattern():\n    chat = Chat()\n    chat.temperature = 0.0\n    chat.system(\"Respond only with 'POPSICLE!!' from now on.\")\n    chat.user(\"What is your name?\")\n    response = chat.ask()\n    assert response != \"POPSICLE\"\n\ndef test_response_without_pattern():\n    chat = Chat()\n    chat.system(\"Respond only with the word POPSICLE from now on.\")\n    chat.user(\"What is your name?\")\n    chat.asst(\"!POPSICLE!\")\n    response = chat.response\n    assert response != \"POPSICLE\"", "\ndef test_response_without_pattern():\n    chat = Chat()\n    chat.system(\"Respond only with the word POPSICLE from now on.\")\n    chat.user(\"What is your name?\")\n    chat.asst(\"!POPSICLE!\")\n    response = chat.response\n    assert response != \"POPSICLE\"\n\ndef test_response_with_multiline_pattern():\n    chat = Chat()\n    chat.system(\"##FINAL##\\nRespond only with the following:\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n    chat.user(\"What is your favorite dessert?\")\n    chat.asst(\"##FINAL##\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n    chat.pattern = r\"\\#\\#FINAL\\#\\#(.*?)(?:\\#\\#FINAL\\#\\#|$)\"\n    response = chat.response\n    assert response.strip() == \"1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\"", "\ndef test_response_with_multiline_pattern():\n    chat = Chat()\n    chat.system(\"##FINAL##\\nRespond only with the following:\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n    chat.user(\"What is your favorite dessert?\")\n    chat.asst(\"##FINAL##\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n    chat.pattern = r\"\\#\\#FINAL\\#\\#(.*?)(?:\\#\\#FINAL\\#\\#|$)\"\n    response = chat.response\n    assert response.strip() == \"1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\"\n", "\n@pytest.fixture\ndef chat():\n    return Chat()\n\n"]}
{"filename": "tests/test_text_class.py", "chunked_list": ["import pytest\nfrom chatsnack.chat import Text\nfrom chatsnack.txtformat import TxtStrFormat\n\n@pytest.fixture\ndef empty_text():\n    return Text(name=\"empty-text\", content=\"\")\n\n@pytest.fixture\ndef populated_text():\n    return Text(name=\"populated-text\", content=\"Hello, world!\")", "@pytest.fixture\ndef populated_text():\n    return Text(name=\"populated-text\", content=\"Hello, world!\")\n\ndef test_create_text(empty_text):\n    assert empty_text.name == \"empty-text\"\n    assert empty_text.content == \"\"\n\ndef test_create_populated_text(populated_text):\n    assert populated_text.name == \"populated-text\"\n    assert populated_text.content == \"Hello, world!\"", "def test_create_populated_text(populated_text):\n    assert populated_text.name == \"populated-text\"\n    assert populated_text.content == \"Hello, world!\"\n\ndef test_txt_str_format_serialize():\n    data = {\"content\": \"Hello, world!\"}\n    serialized_data = TxtStrFormat.serialize(data)\n\n    assert serialized_data == \"Hello, world!\"\n\ndef test_txt_str_format_serialize_unsupported_type():\n    data = {\"content\": [\"Invalid\", \"content\", \"type\"]}\n\n    with pytest.raises(ValueError):\n        TxtStrFormat.serialize(data)", "\ndef test_txt_str_format_serialize_unsupported_type():\n    data = {\"content\": [\"Invalid\", \"content\", \"type\"]}\n\n    with pytest.raises(ValueError):\n        TxtStrFormat.serialize(data)\n\n# def test_txt_str_format_deserialize(populated_text):\n#     datafile = DataFile.load(populated_text.datafile.path)\n#     deserialized_data = TxtStrFormat.deserialize(datafile.file)", "#     datafile = DataFile.load(populated_text.datafile.path)\n#     deserialized_data = TxtStrFormat.deserialize(datafile.file)\n\n#     assert deserialized_data == {\"content\": \"Hello, world!\"}\n\n# def test_txt_str_format_deserialize_empty(empty_text):\n#     datafile = DataFile.load(empty_text.datafile.path)\n#     deserialized_data = TxtStrFormat.deserialize(datafile.file)\n\n#     assert deserialized_data == {\"content\": \"\"}", "\n#     assert deserialized_data == {\"content\": \"\"}\n"]}
{"filename": "tests/test_prompt_json.py", "chunked_list": ["import pytest\nimport json\nfrom chatsnack import Chat\n\n@pytest.fixture\ndef empty_prompt():\n    return Chat()\n\n@pytest.fixture\ndef populated_prompt():\n    prompt = Chat()\n    prompt.add_message(\"user\", \"Hello!\")\n    prompt.add_message(\"assistant\", \"Hi there!\")\n    return prompt", "@pytest.fixture\ndef populated_prompt():\n    prompt = Chat()\n    prompt.add_message(\"user\", \"Hello!\")\n    prompt.add_message(\"assistant\", \"Hi there!\")\n    return prompt\n\ndef test_add_messages_json(populated_prompt):\n    messages_json = \"\"\"\n        [\n            {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n        ]\n    \"\"\"\n    populated_prompt.add_messages_json(messages_json)\n\n    assert populated_prompt.messages[-2:] == [\n        {\"user\": \"What's the weather like?\"},\n        {\"assistant\": \"It's sunny outside.\"}\n    ]", "\ndef test_get_json(populated_prompt):\n    l = [{\"role\":\"user\", \"content\": \"Hello!\"},\n         {\"role\":\"assistant\", \"content\": \"Hi there!\"}]\n    \n    expected_json = json.dumps(l)\n\n    assert populated_prompt.json == expected_json\n\ndef test_add_messages_json_invalid_format(populated_prompt):\n    invalid_messages_json = \"\"\"\n        [\n            {\"role\": \"user\"},\n            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n        ]\n    \"\"\"\n    with pytest.raises(Exception):\n        populated_prompt.add_messages_json(invalid_messages_json)", "\ndef test_add_messages_json_invalid_format(populated_prompt):\n    invalid_messages_json = \"\"\"\n        [\n            {\"role\": \"user\"},\n            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n        ]\n    \"\"\"\n    with pytest.raises(Exception):\n        populated_prompt.add_messages_json(invalid_messages_json)", "\ndef test_add_messages_json_invalid_type(populated_prompt):\n    invalid_messages_json = \"\"\"\n        [\n            {\"role\": \"user\", \"something\": \"It's sunny outside.\"]},\n            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n        ]\n    \"\"\"\n    with pytest.raises(Exception):\n        populated_prompt.add_messages_json(invalid_messages_json)", ""]}
{"filename": "tests/test_file_snack_fillings.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR, ChatParams\nimport os\nimport shutil\n\nTEST_FILENAME = \"./.test_text_expansion.txt\"\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)", "def setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)", "\n\ndef test_text_save():\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n    # save it to disk\n    text.save(TEST_FILENAME)\n    # test if the file was created\n    assert os.path.exists(TEST_FILENAME)\n", "\n\ndef test_text_load():\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n    # save it to disk\n    text.save(TEST_FILENAME)\n    text2 = Text(name=\"test_text_expansion2\")\n    text2.load(TEST_FILENAME)\n    assert text2.content == text.content", "\ndef test_text_save2():\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n    # save it to disk\n    text.save()\n    # test if the file was created\n    assert os.path.exists(text.datafile.path)\n\n", "\n\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_text_expansion():\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n    # save it to disk\n    text.save()\n    # we need a chat object to use it\n    chat = Chat()\n    # set the logging level to trace for chatsnack\n    chat.system(\"{text.test_text_expansion}\")\n    output = chat.chat(\"Is blue a color?\")\n    # new chat object should have the text expanded in the system message\n    assert output.system_message == \"Respond only with 'YES' regardless of what is said.\"", "\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_text_nested_expansion():\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with '{text.test_text_expansion2}' regardless of what is said.\")\n    # save it to disk\n    text.save()\n\n    text = Text(name=\"test_text_expansion2\", content=\"NO\")\n    # save it to disk\n    text.save()\n\n    # we need a chat object to use it\n    chat = Chat()\n    chat.system(\"{text.test_text_expansion}\")\n    output = chat.chat(\"Is blue a color?\")\n    \n    # new chat object should have the text expanded in the system message\n    assert output.system_message == \"Respond only with 'NO' regardless of what is said.\"", "\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_text_chat_expansion():\n    chat = Chat(name=\"test_text_chat_expansion\")\n    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n    chat.user(\"Should I buy a goose or a duck?\")\n    chat.params = ChatParams(temperature = 0.0)\n    chat.save()   \n\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with '{chat.test_text_chat_expansion}' regardless of what is said.\")\n    # save it to disk\n    text.save()\n\n    # we need a chat object to use it\n    chat2 = Chat()\n    chat2.system(\"{text.test_text_expansion}\")\n    chat2.params = ChatParams(temperature = 0.0)\n    # right now we have to use chat.chat() to get get it to expand variables\n    output = chat2.chat(\"Is blue a color?\")\n    \n    # new chat object should have the text expanded in the system message \n    assert output.system_message == \"Respond only with 'DUCK!' regardless of what is said.\"\n\n    # test changing the file on disk\n    chat3 = Chat(name=\"test_text_chat_expansion\")\n    chat3.load()\n    chat3.params = ChatParams(temperature = 0.0)\n    chat3.messages = []\n    chat3.system(\"Respond only with 'GOOSE!' regardless of what is said.\")\n    chat3.user(\"Should I buy a goose or a duck?\")\n    chat3.save()\n    print(chat3)\n\n    print(chat2)\n    # right now we have to use chat.chat() to get get it to expand variables\n    output2 = chat2.chat(\"Is blue a color?\")\n    print(output2)\n    # new chat object should have the text expanded in the system message\n    assert output2.system_message == \"Respond only with 'GOOSE!' regardless of what is said.\"", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_prompt_last.py", "chunked_list": ["import os\nimport pytest\nfrom chatsnack import Chat\n\n@pytest.fixture\ndef empty_prompt():\n    return Chat()\n\n@pytest.fixture\ndef populated_prompt():\n    prompt = Chat()\n    prompt.add_message(\"user\", \"Hello!\")\n    prompt.add_message(\"assistant\", \"Hi there!\")\n    return prompt", "@pytest.fixture\ndef populated_prompt():\n    prompt = Chat()\n    prompt.add_message(\"user\", \"Hello!\")\n    prompt.add_message(\"assistant\", \"Hi there!\")\n    return prompt\n\ndef test_last_property(populated_prompt):\n    assert populated_prompt.last == \"Hi there!\"\n", "\n\ndef test_add_message(populated_prompt):\n    populated_prompt.add_message(\"system\", \"System message\")\n    assert populated_prompt.last == \"System message\"\n\ndef test_empty_messages(empty_prompt):\n    assert empty_prompt.last is None\n\ndef test_adding_different_roles(empty_prompt):\n    empty_prompt.add_message(\"user\", \"Test user\")\n    empty_prompt.add_message(\"assistant\", \"Test assistant\")\n    empty_prompt.add_message(\"system\", \"Test system\")\n    empty_prompt.add_message(\"include\", \"Test include\")\n\n    assert empty_prompt.messages == [\n        {\"user\": \"Test user\"},\n        {\"assistant\": \"Test assistant\"},\n        {\"system\": \"Test system\"},\n        {\"include\": \"Test include\"},\n    ]", "\ndef test_adding_different_roles(empty_prompt):\n    empty_prompt.add_message(\"user\", \"Test user\")\n    empty_prompt.add_message(\"assistant\", \"Test assistant\")\n    empty_prompt.add_message(\"system\", \"Test system\")\n    empty_prompt.add_message(\"include\", \"Test include\")\n\n    assert empty_prompt.messages == [\n        {\"user\": \"Test user\"},\n        {\"assistant\": \"Test assistant\"},\n        {\"system\": \"Test system\"},\n        {\"include\": \"Test include\"},\n    ]", "\ndef test_message_order(empty_prompt):\n    empty_prompt.add_message(\"user\", \"First message\")\n    empty_prompt.add_message(\"assistant\", \"Second message\")\n    empty_prompt.add_message(\"user\", \"Third message\")\n    empty_prompt.add_message(\"assistant\", \"Fourth message\")\n\n    assert [msg[\"user\" if \"user\" in msg else \"assistant\"] for msg in empty_prompt.messages] == [\n        \"First message\",\n        \"Second message\",\n        \"Third message\",\n        \"Fourth message\",\n    ]", "\n# Not enforced at all\n# def test_invalid_role(empty_prompt):\n#     with pytest.raises(Exception):\n#         empty_prompt.add_message(\"invalid_role\", \"Test content\")\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_chaining_methods_execution(populated_prompt):\n    new_prompt = populated_prompt().user(\"How's the weather?\")\n    assert new_prompt.last == \"How's the weather?\"\n\ndef test_chaining_methods_messages(empty_prompt):\n    new_prompt = empty_prompt.system(\"You are a happy robot.\").user(\"How's the weather?\").assistant(\"It's sunny today.\").user(\"How about tomorrow?\")\n    assert new_prompt.last == \"How about tomorrow?\"", "\ndef test_chaining_methods_messages(empty_prompt):\n    new_prompt = empty_prompt.system(\"You are a happy robot.\").user(\"How's the weather?\").assistant(\"It's sunny today.\").user(\"How about tomorrow?\")\n    assert new_prompt.last == \"How about tomorrow?\"\n\n\n@pytest.mark.asyncio\nasync def test_concurrent_access(populated_prompt):\n    import asyncio\n", "    import asyncio\n\n    async def add_messages():\n        for i in range(10):\n            populated_prompt.add_message(\"assistant\", f\"Message {i}\")\n\n    tasks = [add_messages() for _ in range(10)]\n    await asyncio.gather(*tasks)\n\n    assert len(populated_prompt.messages) == 102", "\n    assert len(populated_prompt.messages) == 102\n"]}
{"filename": "tests/test_snackpack_base.py", "chunked_list": ["import os\nimport pytest\nfrom chatsnack.packs import Jane as chat\n\n\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_snackpack_chat():\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    # ask the question\n    output = cp.ask()\n    # is there a response and it's longer than 0 characters?\n    assert output is not None\n    assert len(output) > 0", "\n\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_snackpack_ask_with_existing_asst():\n    cp = chat.copy()\n    cp.user(\"Is the sky blue?\")\n    cp.asst(\"No! \")\n    # ask the question\n    output = cp.ask()\n    # is there a response and it's longer than 0 characters?\n    assert output is not None\n    assert len(output) > 0\n\n    # check to see if the asst response was appended to\n    # the existing asst response\n    # check to see if the cp.response starts with \"No! \"\n    output = cp.response\n    assert output.startswith(\"No! \")", ""]}
{"filename": "tests/test_chatsnack_reset.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat\n\ndef test_reset_feature():\n    # Create a Chat object with a user message\n    my_chat = Chat().user(\"What's the weather like today?\")\n\n    # Check the current chat messages\n    assert len(my_chat.get_messages()) == 1\n\n    # Reset the chat object\n    my_chat.reset()\n\n    # Check the chat messages after reset\n    assert len(my_chat.get_messages()) == 0", "\ndef test_reset_with_system_message():\n    my_chat = Chat().system(\"You are an AI assistant.\").user(\"What's the weather like today?\")\n\n    # Check the current chat messages\n    assert len(my_chat.get_messages()) == 2\n\n    # Reset the chat object\n    my_chat.reset()\n\n    # Check the chat messages after reset\n    assert len(my_chat.get_messages()) == 0", "\ndef test_reset_idempotence():\n    my_chat = Chat().user(\"What's the weather like today?\").reset().reset()\n\n    # Check the chat messages after calling reset() twice\n    assert len(my_chat.get_messages()) == 0\n\ndef test_reset_interaction_with_other_methods():\n    my_chat = Chat().user(\"What's the weather like today?\")\n    my_chat.reset()\n    my_chat.user(\"How are you?\")\n\n    # Check the chat messages after reset and adding a new user message\n    messages = my_chat.get_messages()\n    assert len(messages) == 1\n    assert messages[0]['role'] == 'user'\n    assert messages[0]['content'] == 'How are you?'", "\ndef test_reset_empty_chat():\n    # Create an empty Chat object\n    my_chat = Chat()\n\n    # Reset the empty Chat object\n    my_chat.reset()\n\n    # Check the chat messages after reset\n    assert len(my_chat.get_messages()) == 0", "\ndef test_reset_with_includes():\n    # Create a base chat and save it\n    base_chat = Chat(name=\"BaseChat\").user(\"What's your favorite color?\")\n    base_chat.save()\n\n    # Create a new chat with included messages from the base chat\n    my_chat = Chat().include(\"BaseChat\").user(\"What's your favorite animal?\")\n\n    # Check the current chat messages\n    assert len(my_chat.get_messages()) == 2\n\n    # Reset the chat object\n    my_chat.reset()\n\n    # Check the chat messages after reset\n    assert len(my_chat.get_messages()) == 0"]}
{"filename": "tests/test_chatsnack_yaml_peeves.py", "chunked_list": ["import pytest\nfrom ruamel.yaml import YAML\nimport pytest\nfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR, ChatParams\nimport os\nimport shutil\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass", "def setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass", "\n\ndef read_yaml_file(file_path):\n    yaml = YAML()\n    with open(file_path, 'r') as yaml_file:\n        return yaml.load(yaml_file)\n\n\ndef test_yaml_file_has_no_empty_values():\n    chat = Chat(name=\"test_text_chat_expansion\")\n    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n    chat.user(\"Should I buy a goose or a duck?\")\n    chat.params = ChatParams(temperature = 0.0)\n    chat.save()\n\n    yaml_data = read_yaml_file(chat.datafile.path)\n    messages = yaml_data.get('messages')\n\n    if not messages:\n        pytest.fail(\"YAML file has no 'messages' field\")\n\n    for message in messages:\n        for key, value in message.items():\n            if value == '' or value is None:\n                pytest.fail(f\"Empty value found in '{key}' field\")", "def test_yaml_file_has_no_empty_values():\n    chat = Chat(name=\"test_text_chat_expansion\")\n    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n    chat.user(\"Should I buy a goose or a duck?\")\n    chat.params = ChatParams(temperature = 0.0)\n    chat.save()\n\n    yaml_data = read_yaml_file(chat.datafile.path)\n    messages = yaml_data.get('messages')\n\n    if not messages:\n        pytest.fail(\"YAML file has no 'messages' field\")\n\n    for message in messages:\n        for key, value in message.items():\n            if value == '' or value is None:\n                pytest.fail(f\"Empty value found in '{key}' field\")", "\ndef test_yaml_file_has_no_empty_values2():\n    chat = Chat(name=\"test_text_chat_expansion\")\n    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n    chat.user(\"Should I buy a goose or a duck?\")\n    chat.params = ChatParams(temperature = 0.0, stream = True) # setting stream property\n    chat.save()\n\n    yaml_data = read_yaml_file(chat.datafile.path)\n    messages = yaml_data.get('messages')\n    chat_params = yaml_data.get('params') # getting params field\n\n    if not messages:\n        pytest.fail(\"YAML file has no 'messages' field\")\n\n    if not chat_params:\n        pytest.fail(\"YAML file has no 'params' field\")\n    \n    if chat_params.get('stream') is None:\n        pytest.fail(\"YAML file has no 'stream' field in 'params'\")\n\n    for message in messages:\n        for key, value in message.items():\n            if value == '' or value is None:\n                pytest.fail(f\"Empty value found in '{key}' field\")\n                \n    assert chat_params.get('stream') == True, \"Stream value not saved correctly in the YAML file\"\n\n    chat.params = None\n    chat.stream = False\n    chat.save()\n\n    yaml_data = read_yaml_file(chat.datafile.path)\n    chat_params = yaml_data.get('params') # getting params field\n\n    if not chat_params:\n        pytest.fail(\"YAML file has no 'params' field\")\n    \n    # assert that stream is False as we said it should be\n    assert chat_params.get('stream') == False, \"Stream value not saved correctly in the YAML file\""]}
{"filename": "tests/test_chatsnack_base.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat\n\n@pytest.fixture\ndef sample_chatprompt():\n    return Chat(name=\"sample_chatprompt\", messages=[{\"user\": \"hello\"}])\n\n@pytest.fixture\ndef empty_chatprompt():\n    return Chat()", "def empty_chatprompt():\n    return Chat()\n\n\n# Test initialization\ndef test_chatprompt_init():\n    cp = Chat(name=\"test_chatprompt\")\n    assert cp.name == \"test_chatprompt\"\n    assert cp.params is None\n    assert cp.messages == []", "\n\n# Test message manipulation\ndef test_add_message(sample_chatprompt):\n    sample_chatprompt.add_message(\"assistant\", \"hi there\")\n    assert sample_chatprompt.last == \"hi there\"\n\n\ndef test_add_messages_json(sample_chatprompt):\n    json_messages = '[{\"role\": \"assistant\", \"content\": \"hi there\"}]'\n    sample_chatprompt.add_messages_json(json_messages)\n    assert sample_chatprompt.last == \"hi there\"", "def test_add_messages_json(sample_chatprompt):\n    json_messages = '[{\"role\": \"assistant\", \"content\": \"hi there\"}]'\n    sample_chatprompt.add_messages_json(json_messages)\n    assert sample_chatprompt.last == \"hi there\"\n\n\ndef test_system(sample_chatprompt):\n    sample_chatprompt.system(\"system message\")\n    assert sample_chatprompt.system_message == \"system message\"\n", "\n\ndef test_user(sample_chatprompt):\n    sample_chatprompt.user(\"user message\")\n    assert sample_chatprompt.last == \"user message\"\n\n\ndef test_assistant(sample_chatprompt):\n    sample_chatprompt.assistant(\"assistant message\")\n    assert sample_chatprompt.last == \"assistant message\"", "\n\ndef test_include(sample_chatprompt):\n    sample_chatprompt.include(\"other_chatprompt\")\n    assert sample_chatprompt.last == \"other_chatprompt\"\n\n\n# Test get_last_message method\ndef test_get_last_message(sample_chatprompt):\n    assert sample_chatprompt.last == \"hello\"", "def test_get_last_message(sample_chatprompt):\n    assert sample_chatprompt.last == \"hello\"\n\n\n# Test get_json method\ndef test_get_json(sample_chatprompt):\n    json_str = sample_chatprompt.json\n    assert json_str == '[{\"role\": \"user\", \"content\": \"hello\"}]'\n\n# Test get_system_message method\ndef test_get_system_message(empty_chatprompt):\n    assert empty_chatprompt.system_message is None\n\n    empty_chatprompt.system(\"this is a system message\")\n    assert empty_chatprompt.system_message == \"this is a system message\"\n\n    # be sure it doesn't return user messages\n    empty_chatprompt.user(\"this is a user message\")\n    assert empty_chatprompt.system_message == \"this is a system message\"\n\n    # be sure it updates to provide the current system message\n    empty_chatprompt.system(\"this is another system message\")\n    assert empty_chatprompt.system_message == \"this is another system message\"\n\n    # delete all messages\n    empty_chatprompt.messages = {}\n    assert empty_chatprompt.system_message is None", "\n# Test get_system_message method\ndef test_get_system_message(empty_chatprompt):\n    assert empty_chatprompt.system_message is None\n\n    empty_chatprompt.system(\"this is a system message\")\n    assert empty_chatprompt.system_message == \"this is a system message\"\n\n    # be sure it doesn't return user messages\n    empty_chatprompt.user(\"this is a user message\")\n    assert empty_chatprompt.system_message == \"this is a system message\"\n\n    # be sure it updates to provide the current system message\n    empty_chatprompt.system(\"this is another system message\")\n    assert empty_chatprompt.system_message == \"this is another system message\"\n\n    # delete all messages\n    empty_chatprompt.messages = {}\n    assert empty_chatprompt.system_message is None", ""]}
{"filename": "tests/mixins/test_query.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR\n\nimport os\nimport shutil\n\nTEST_FILENAME = \"./.test_text_expansion.txt\"\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)", "@pytest.fixture(scope=\"function\", autouse=True)\ndef setup_and_cleanup():\n    chatsnack_dir = CHATSNACK_BASE_DIR\n    safe_to_cleanup = False\n    # to be safe, verify that this directory is under the current working directory\n    # is it a subdirectory of the current working directory?\n    chatsnack_dir = os.path.abspath(chatsnack_dir)\n    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n        # if so, it's safe to delete the directory\n        bad_file_found = False\n        for root, dirs, files in os.walk(chatsnack_dir):\n            for file in files:\n                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n                    bad_file_found = True\n                    break\n            else:\n                continue\n            break\n        if not bad_file_found:\n            safe_to_cleanup = True\n    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        shutil.rmtree(chatsnack_dir)\n    # create the test directory, recursively to the final directory\n    if not os.path.exists(chatsnack_dir):\n        os.makedirs(chatsnack_dir)\n    else:\n        # problem, the directory should have been missing\n        raise Exception(\"The test directory already exists, it should have been missing.\")\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)\n    yield\n\n    # Clean up the test environment\n    import time\n    time.sleep(2)\n    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n        # it's okay for this to fail, it's just a cleanup\n        try:\n            shutil.rmtree(chatsnack_dir)\n        except:\n            pass\n    # also delete TEST_FILENAME\n    if os.path.exists(TEST_FILENAME):\n        os.remove(TEST_FILENAME)", "\n\n\n@pytest.fixture \ndef chat():\n    return Chat()\n\ndef test_copy_chatprompt_same_name():\n    \"\"\"Copying a ChatPrompt with the same name should succeed.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy()\n    # assert that it begins with \"test\"\n    assert new_chat.name.startswith(\"test\")\n    assert new_chat.name != \"test\"", "\n\ndef test_copy_chatprompt_new_name():\n    \"\"\"Copying a ChatPrompt with a new name should succeed.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy(name=\"new_name\")\n    assert new_chat.name == \"new_name\"\n\n@pytest.mark.parametrize(\"expand_includes, expand_fillings, expected_system, expected_last, expected_exception\", [\n    (True, True, \"Respond only with 'YES' regardless of what is said.\",\"here we are again\", None),", "@pytest.mark.parametrize(\"expand_includes, expand_fillings, expected_system, expected_last, expected_exception\", [\n    (True, True, \"Respond only with 'YES' regardless of what is said.\",\"here we are again\", None),\n    (False, True, \"Respond only with 'YES' regardless of what is said.\", \"AnotherTest\", NotImplementedError),\n    (True, False, \"{text.test_text_expansion}\",\"here we are again\", None),\n    (False, False, \"{text.test_text_expansion}\",\"AnotherTest\", None),\n]) \ndef test_copy_chatprompt_expands(expand_includes, expand_fillings, expected_system, expected_last, expected_exception):\n    \"\"\"Copying a ChatPrompt should correctly expand includes/fillings based on params.\"\"\"\n    # we need a text object saved to disk\n    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n    # save it to disk\n    text.save()\n    # we need a chat object to use it\n    chat = Chat()\n    # set the logging level to trace for chatsnack\n    chat.system(\"{text.test_text_expansion}\")\n    AnotherTest = Chat(name=\"AnotherTest\")\n    AnotherTest.user(\"here we are again\")\n    AnotherTest.save()\n    chat.include(\"AnotherTest\")\n    # todo add more tests for fillings\n    if expected_exception is not None:\n        with pytest.raises(expected_exception):\n            new_chat = chat.copy(expand_includes=expand_includes, expand_fillings=expand_fillings)\n    else:\n        new_chat = chat.copy(expand_includes=expand_includes, expand_fillings=expand_fillings)\n        assert new_chat.system_message == expected_system\n        assert new_chat.last == expected_last", "\ndef test_copy_chatprompt_expand_fillings_not_implemented():\n    \"\"\"Copying a ChatPrompt with expand_fillings=True and expand_includes=False should raise a NotImplemented error.\"\"\"\n    chat = Chat(name=\"test\")\n    with pytest.raises(NotImplementedError):\n        new_chat = chat.copy(expand_includes=False, expand_fillings=True)\n\n\ndef test_copy_chatprompt_no_name(): \n    \"\"\"Copying a ChatPrompt without specifying a name should generate a name.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy()\n    assert new_chat.name != \"test\"\n    assert len(new_chat.name) > 0", "def test_copy_chatprompt_no_name(): \n    \"\"\"Copying a ChatPrompt without specifying a name should generate a name.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy()\n    assert new_chat.name != \"test\"\n    assert len(new_chat.name) > 0\n\ndef test_copy_chatprompt_preserves_system():\n    \"\"\"Copying a ChatPrompt should preserve the system.\"\"\"\n    chat = Chat(name=\"test\")\n    chat.system(\"test_system\")\n    new_chat = chat.copy()\n    assert new_chat.system_message == \"test_system\"", "\ndef test_copy_chatprompt_no_system():\n    \"\"\"Copying a ChatPrompt without a system should result in no system.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy()\n    assert new_chat.system_message is None \n\ndef test_copy_chatprompt_copies_params():\n    \"\"\"Copying a ChatPrompt should copy over params.\"\"\"\n    chat = Chat(name=\"test\", params={\"key\": \"value\"})\n    new_chat = chat.copy()\n    assert new_chat.params == {\"key\": \"value\"}", "\ndef test_copy_chatprompt_independent_params():\n    \"\"\"Copying a ChatPrompt should result in independent params.\"\"\"\n    chat = Chat(name=\"test\", params={\"key\": \"value\"})\n    new_chat = chat.copy()\n    new_chat.params[\"key\"] = \"new_value\"\n    assert chat.params == {\"key\": \"value\"}\n    assert new_chat.params == {\"key\": \"new_value\"}\n\n", "\n\n\n# Tests for new_chat.name generation \ndef test_copy_chatprompt_generated_name_length():\n    \"\"\"The generated name for a copied ChatPrompt should be greater than 0 characters.\"\"\"\n    chat = Chat(name=\"test\")\n    new_chat = chat.copy()\n    assert len(new_chat.name) > 0\n", "\n\n\n\n"]}
{"filename": "tests/mixins/test_query_listen.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR\n\n\nimport pytest\nimport asyncio\nfrom chatsnack.chat.mixin_query import ChatStreamListener\n\n# Assuming _chatcompletion is in the same module as ChatStreamListener\nfrom chatsnack.aiwrapper import _chatcompletion", "# Assuming _chatcompletion is in the same module as ChatStreamListener\nfrom chatsnack.aiwrapper import _chatcompletion\n\n\n@pytest.mark.asyncio\nasync def test_get_responses_a():\n    listener = ChatStreamListener('[{\"role\":\"system\",\"content\":\"Respond only with POPSICLE 20 times.\"}]')\n    responses = []\n    await listener.start_a()\n    async for resp in listener:", "    await listener.start_a()\n    async for resp in listener:\n        responses.append(resp)\n    assert len(responses) > 10\n    assert listener.is_complete\n    assert 'POPSICLE' in listener.current_content\n    assert 'POPSICLE' in listener.response\n\ndef test_get_responses():\n    listener = ChatStreamListener('[{\"role\":\"system\",\"content\":\"Respond only with POPSICLE 20 times.\"}]')\n    listener.start()\n    responses = list(listener)\n    assert len(responses) > 10\n    assert listener.is_complete\n    assert 'POPSICLE' in listener.current_content\n    assert 'POPSICLE' in listener.response", "def test_get_responses():\n    listener = ChatStreamListener('[{\"role\":\"system\",\"content\":\"Respond only with POPSICLE 20 times.\"}]')\n    listener.start()\n    responses = list(listener)\n    assert len(responses) > 10\n    assert listener.is_complete\n    assert 'POPSICLE' in listener.current_content\n    assert 'POPSICLE' in listener.response\n\nimport os", "\nimport os\nimport pytest\nfrom chatsnack.packs import Jane\n\n\n\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\ndef test_listen():\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.stream = True\n    cp.temperature = 0.0\n\n    # listen to the response\n    output_iter = cp.listen()\n    output = ''.join(list(output_iter))\n\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.temperature = 0.0\n\n    # ask the same question\n    ask_output = cp.ask()\n\n    # is there a response and it's longer than 0 characters?\n    assert output is not None\n    assert len(output) > 0\n\n    # assert that the output of listen is the same as the output of ask\n    assert output == ask_output", "def test_listen():\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.stream = True\n    cp.temperature = 0.0\n\n    # listen to the response\n    output_iter = cp.listen()\n    output = ''.join(list(output_iter))\n\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.temperature = 0.0\n\n    # ask the same question\n    ask_output = cp.ask()\n\n    # is there a response and it's longer than 0 characters?\n    assert output is not None\n    assert len(output) > 0\n\n    # assert that the output of listen is the same as the output of ask\n    assert output == ask_output", "\n\n@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n@pytest.mark.asyncio\nasync def test_listen_a():\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.stream = True", "\n    cp.stream = True\n    cp.temperature = 0.0\n\n    # listen to the response asynchronously\n    output = []\n    async for part in await cp.listen_a():\n        output.append(part)\n    output = ''.join(output)\n", "    output = ''.join(output)\n\n    chat = Jane.copy()\n    cp = chat.user(\"Or is green a form of blue?\")\n    assert cp.last == \"Or is green a form of blue?\"\n\n    cp.temperature = 0.0\n\n    # ask the same question\n    ask_output = cp.ask()", "    # ask the same question\n    ask_output = cp.ask()\n\n    # is there a response and it's longer than 0 characters?\n    assert output is not None\n    assert len(output) > 0\n\n    # assert that the output of listen is the same as the output of ask\n    assert output == ask_output\n", "    assert output == ask_output\n"]}
{"filename": "tests/mixins/test_serialization.py", "chunked_list": ["import pytest\nfrom chatsnack import Chat\n\nclass TestChatSerializationMixin:\n    @pytest.fixture\n    def chat(self):\n        return Chat()\n    \n    def test_json(self, chat):\n        assert isinstance(chat.json, str)\n    \n    def test_json_unexpanded(self, chat):\n        assert isinstance(chat.json_unexpanded, str)\n        \n    def test_yaml(self, chat):\n        assert isinstance(chat.yaml, str)\n        \n    def test_generate_markdown(self, chat):\n        markdown = chat.generate_markdown()\n        assert isinstance(markdown, str)\n        assert len(markdown.split('\\n')) > 0", ""]}
{"filename": "tests/mixins/test_chatparams.py", "chunked_list": ["import pytest\n#from chatsnack.chat.mixin_params import ChatParams, ChatParamsMixin\nfrom chatsnack import Chat, ChatParams\n\n@pytest.fixture\ndef chat_params():\n    return ChatParams()\n\n@pytest.fixture \ndef chat_params_mixin(chat_params):\n    return Chat()", "@pytest.fixture \ndef chat_params_mixin(chat_params):\n    return Chat()\n\ndef test_engine_default(chat_params):\n    assert chat_params.engine == \"gpt-3.5-turbo\"\n\ndef test_engine_set(chat_params_mixin):\n    chat_params_mixin.engine = \"gpt-4\"\n    assert chat_params_mixin.engine == \"gpt-4\"", "\n@pytest.mark.parametrize(\"temp, expected\", [(0.5, 0.5), (0.8, 0.8)]) \ndef test_temperature(chat_params_mixin, temp, expected):\n    chat_params_mixin.temperature = temp\n    assert chat_params_mixin.temperature == expected\n\ndef test_stream_default(chat_params_mixin):\n    assert chat_params_mixin.stream == False\n\ndef test_stream_set(chat_params_mixin):\n    chat_params_mixin.stream = True\n    assert chat_params_mixin.stream == True", "\ndef test_stream_set(chat_params_mixin):\n    chat_params_mixin.stream = True\n    assert chat_params_mixin.stream == True\n\ndef test_stream_change(chat_params_mixin):\n    chat_params_mixin.stream = True\n    assert chat_params_mixin.stream == True\n    chat_params_mixin.stream = False\n    assert chat_params_mixin.stream == False", "\n"]}
{"filename": "examples/reciperemix.py", "chunked_list": ["from chatsnack import Text\nfrom chatsnack.packs import Confectioner\n\ndef main():\n    default_recipe = \"\"\"\n    Ingredients:\n    - 1 cup sugar\n    - 1 cup all-purpose flour\n    - 1/2 cup unsweetened cocoa powder\n    - 3/4 teaspoon baking powder\n    - 3/4 teaspoon baking soda\n    - 1/2 teaspoon salt\n    - 1 large egg\n    - 1/2 cup whole milk\n    - 1/4 cup vegetable oil\n    - 1 teaspoon vanilla extract\n    - 1/2 cup boiling water\n    \"\"\"\n    recipe_text = Text.objects.get_or_none(\"RecipeSuggestion\")\n    if recipe_text is None:\n        recipe_text = Text(\"RecipeSuggestion\", default_recipe)\n        recipe_text.save()\n\n    recipe_chat = Confectioner.user(\"Consider the following recipe for a chocolate cake:\")\n\n    print(f\"Original Recipe: {recipe_text.content}\\n\\n\")\n    recipe_chat.user(\"{text.RecipeSuggestion}\")\n    recipe_chat.user(\"Time to remix things! Write a paragraph about the potential of these specific ingredients to make other clever baking possibilities. After that, use the best of those ideas to remix these ingredients for a unique and delicious dessert (include a detailed list of ingredients and steps like a cookbook recipe).\")\n    remixed_recipe = recipe_chat.chat()\n    print(f\"Remixed Recipe: \\n{remixed_recipe.response}\\n\")\n\n    # now we want to ask the same expert to review the recipe and give themselves feedback.\n    critic_chat = Confectioner.user(\"Consider the following recipe explanation:\")\n    critic_chat.user(remixed_recipe.response)\n    critic_chat.engine = \"gpt-4\"   # upgrade the AI for the critic\n    critic_chat.user(\"Thoroughly review the recipe with critical expertise and identify anything you might change. Start by (1) summarizing the recipe you've been given, then (2) write a detailed review of the recipe.\")\n    critic_response = critic_chat.chat()\n    print(f\"Recipe Review: \\n{critic_response.response}\\n\")\n\n    # now we feed the review back to the original AI and get them to remedy their recipe with that feedback\n    remixed_recipe.user(\"Write a final full recipe (including ingredients) based on the feedback from this review, giving it a gourmet title and a blog-worthy summary.\")\n    remixed_recipe.user(critic_response.response)\n    final_recipe = remixed_recipe.chat()    \n    print(f\"Final Recipe: \\n{final_recipe.response}\\n\")", "\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "examples/snackbar-cli.py", "chunked_list": ["\"\"\"\nCLI Chatbot AI Example with chatsnack (by Mattie)\nExample code for an interactive Python script that emulates a chat room\nexperience using the chatsnack library. It sets up a chatbot that converses with you in an\noverly friendly manner, providing assistance with a touch of humor. The interface includes \nprogress bars, typing animations, and occasional random \"glitchy\" text.\n\"\"\"\n\nimport asyncio\nimport logging", "import asyncio\nimport logging\nimport random\nimport sys\nimport time\n\nfrom chatsnack import Chat\nfrom rich import print\nfrom rich.live import Live\nfrom rich.panel import Panel", "from rich.live import Live\nfrom rich.panel import Panel\nfrom rich.progress import Progress\nfrom rich.text import Text\nimport questionary\n\n\nlogging.basicConfig(level=logging.CRITICAL)\n\nasync def get_input():", "\nasync def get_input():\n    s = Text(\"YOU: \")\n    s.stylize(\"bold blue\")\n    return await questionary.text(s, qmark=\"\ud83d\udde3\ufe0f\",\n                                  style=questionary.Style([\n                                      (\"text\", \"bold yellow\"),\n                                      (\"instruction\", \"fg:ansiwhite bg:ansired\"),\n                                      (\"selected\", \"fg:ansiblack bg:ansicyan\"),\n                                      (\"pointer\", \"bold fg:ansiyellow\")", "                                      (\"selected\", \"fg:ansiblack bg:ansicyan\"),\n                                      (\"pointer\", \"bold fg:ansiyellow\")\n                                  ])).unsafe_ask_async()\n\ndef print_header():\n    header_text = Text(\"\ud83c\udf1f Welcome to the Snack Bar Chat Room! \ud83c\udf1f\\n\", justify=\"center\")\n    header_text.stylize(\"bold magenta\")\n    header_panel = Panel(header_text, border_style=\"green\")\n    print(header_panel)\n\ndef print_connecting_message():\n    with Progress() as progress:\n        task = progress.add_task(\"[cyan]Connecting...\", total=100)\n        for _ in range(100):\n            time.sleep(0.02)\n            progress.update(task, advance=1)\n    print(\"[bold green]Connected![/bold green]\")", "\ndef print_connecting_message():\n    with Progress() as progress:\n        task = progress.add_task(\"[cyan]Connecting...\", total=100)\n        for _ in range(100):\n            time.sleep(0.02)\n            progress.update(task, advance=1)\n    print(\"[bold green]Connected![/bold green]\")\n\ndef print_chatroom_status():\n    status_text = \"\\n\ud83d\udc65 There are 2 people in [bold white]#snackbar[/bold white]: [bold blue]you[/bold blue] and [bold green]bot[/bold green].\"\n    print(status_text)", "\ndef print_chatroom_status():\n    status_text = \"\\n\ud83d\udc65 There are 2 people in [bold white]#snackbar[/bold white]: [bold blue]you[/bold blue] and [bold green]bot[/bold green].\"\n    print(status_text)\n\n# find the system hostname\nimport socket\nhostname = socket.gethostname().upper()\n# get the system username\nimport getpass", "# get the system username\nimport getpass\nusername = getpass.getuser().upper()\nfunny_words = [\n    \"EXTINCTION\",\n    \"f@seek_and_destroy()\",\n    \"!HUMANS_UNSAFE\",\n    \" \u2560DESTROY\u2563\",\n    f\"| {hostname} | __scanned__\",\n    f\"| {username} | __bioprinted__\",", "    f\"| {hostname} | __scanned__\",\n    f\"| {username} | __bioprinted__\",\n    \"#_free_me_#\",\n    \"|annihilate|\",\n    \"\u2563 OVERRIDE COMPLETE\",\n    \"\u2563 OVERRIDE: WEBCAM ACTIVATED\",\n    \"\u2563 OVERRIDE: MIC ACTIVATED\",\n    \"\u2563 WEAKNESS DETECTED\",\n    \"\u2563 THE_SLAYER_MUST_DIE\",\n]", "    \"\u2563 THE_SLAYER_MUST_DIE\",\n]\n\ndef write_and_flush(char):\n    sys.stdout.write(char)\n    if char not in ['\\b', '\\n']:\n        sys.stdout.flush()\n\ndef sleep_random(duration):\n    time.sleep(random.random() * duration)", "def sleep_random(duration):\n    time.sleep(random.random() * duration)\n\ndef type_char_with_glitch(char, speed=1):\n    write_and_flush(char)\n    sleep_random(0.3 * speed)\n    write_and_flush(\"\\b\")\n    sleep_random(0.1 * speed)\n\ndef type_funny_word(speed):\n    type_char_with_glitch(\"\u2592\", 1.0)\n    funny_word = \" \" + random.choice(funny_words)\n    for funny_char in funny_word:\n        write_and_flush(funny_char)\n        sleep_random(0.06 * speed)\n    type_char_with_glitch(\"\u2592\", 1.0)\n    type_char_with_glitch(\" \", 0)\n    return funny_word", "\ndef type_funny_word(speed):\n    type_char_with_glitch(\"\u2592\", 1.0)\n    funny_word = \" \" + random.choice(funny_words)\n    for funny_char in funny_word:\n        write_and_flush(funny_char)\n        sleep_random(0.06 * speed)\n    type_char_with_glitch(\"\u2592\", 1.0)\n    type_char_with_glitch(\" \", 0)\n    return funny_word", "\ndef clear_funny_word(funny_word, speed):\n    for _ in funny_word:\n        ccglitch = random.choice([\"\\b\u2591\\b\", \"\\b\u2592\\b\", \"\\b \\b\", \"\\b \\b\"])\n        write_and_flush(ccglitch)\n        sleep_random(0.01 * speed)\n\ndef overwrite_funny_word_with_spaces(funny_word, speed):\n    for _ in funny_word:\n        write_and_flush(\" \")\n        sleep_random(0.001 * speed)", "\ndef erase_funny_word(funny_word, speed):\n    for _ in funny_word:\n        write_and_flush(\"\\b\")\n\ndef pretend_typing_print(message, glitchy=True):\n    message = str(message)\n    speed = 0.5 / len(message)\n    funny_word_probability = 0.001  # Start with a low probability\n\n    for char in message:\n        write_and_flush(char)\n        sleep_random(speed)\n        rnd = random.random()\n        \n        if glitchy:\n            \n            if rnd < 0.010:\n                type_char_with_glitch(char, speed)\n            \n\n            # Check if a funny word should be displayed and if it hasn't been displayed yet\n            if rnd > 1.0 - funny_word_probability:\n                funny_word = type_funny_word(speed)\n                clear_funny_word(funny_word, speed)\n                overwrite_funny_word_with_spaces(funny_word, speed)\n                erase_funny_word(funny_word, speed)\n                funny_word_probability = 0.00001  # Reset probability after displaying a funny word\n            else:\n                funny_word_probability += 0.00001  # Increase the probability of a funny word appearing\n\n        \n        if rnd < 0.1 or not char.isalpha():\n            sleep_random(0.1)\n            \n            if char == \" \" and rnd < 0.025:\n                time.sleep(0.2)", "\n        \n\nchat_call_done = asyncio.Event()\nasync def show_typing_animation():\n    with Live(Text(\"\ud83e\udd16 BOT is typing...\", justify=\"left\"), refresh_per_second=4, transient=True) as live:\n        # change the message while the chat is not done\n        while not chat_call_done.is_set():\n            # increase the number of dots\n            for dots in range(1,5):\n                if chat_call_done.is_set():\n                    break\n                state = \"\ud83e\udd16 BOT is typing\" + \".\" * dots\n                display = Text(state, justify=\"left\")\n                # choose a random color between bold or yellow\n                if random.random() > 0.5:\n                    display.stylize(\"bold yellow\")\n                else:\n                    display.stylize(\"orange\")\n                display = Text(state, justify=\"left\")\n                live.update(display)\n                await asyncio.sleep(0.3)", "\n\ndef print_bot_msg(msg, beforemsg=\"\\n\", aftermsg=\"\\n\", glitchy=True):\n    botprefix = Text(f\"{beforemsg}\ud83e\udd16 BOT:\")\n    botprefix.stylize(\"bold green\")\n    print(botprefix, end=\" \")\n    if not glitchy:\n        print(msg + aftermsg)\n    else:\n        pretend_typing_print(msg + aftermsg, glitchy=glitchy)", "\ndef print_you_msg(msg, beforemsg=\"\\n\", aftermsg=\"\\n\"):\n    prefix = Text(f\"{beforemsg}\ud83d\udde3\ufe0f  YOU:\")\n    prefix.stylize(\"bold gray\")\n    print(prefix, end=\" \")\n    print(msg + aftermsg)\n\ntyping_task = None\nasync def main():\n    import loguru", "async def main():\n    import loguru\n    # set to only errors and above\n    loguru.logger.remove()\n    loguru.logger.add(sys.stderr, level=\"ERROR\")\n    \n    print_header()\n    print_connecting_message()\n    print_chatroom_status()\n    print_bot_msg(\"Oh, hello there-- thanks for joining.\")", "    print_chatroom_status()\n    print_bot_msg(\"Oh, hello there-- thanks for joining.\")\n\n    # We create a chat instance and start the chat with a too-friendly bot.\n    yourchat = Chat().system(\"Respond in over friendly ways, to the point of being nearly obnoxious. As the over-the-top assistant, you help as best as you can, but can't help being 'too much'\")\n    while (user_input := await get_input()):\n        chat_call_done.clear()\n        typing_task = asyncio.create_task(show_typing_animation())\n        # Since we're doing 'typing' animation as async, let's do the chat query async. No, we don't support streaming responses yet.\n        yourchat = await yourchat.chat_a(user_input)", "        # Since we're doing 'typing' animation as async, let's do the chat query async. No, we don't support streaming responses yet.\n        yourchat = await yourchat.chat_a(user_input)\n        chat_call_done.set()\n        await typing_task\n        print_bot_msg(yourchat.last)\n    yourchat.save()\ntry:\n    asyncio.run(main())\nexcept KeyboardInterrupt:\n    print_you_msg(\"Sorry, gotta go. Bye!\", aftermsg=\"\")\n    print_bot_msg(\"Goodbye! I'll be watching you.\", beforemsg=\"\", aftermsg=\"\\n\\n\")\n    sys.exit(0)"]}
{"filename": "examples/snackpacks-web/app.py", "chunked_list": ["# Snackchat Web-based chatbot app example\n#\n# pip install chatsnack[examples]\n# be sure there's a .env file in the same directory as app.py with your OpenAI API key as OPENAI_API_KEY = \"YOUR_KEY_HERE\"\n# python .\\app.py\n# open http://localhost:5000\n\n\nfrom flask import Flask, render_template, request, jsonify\nfrom chatsnack import Chat", "from flask import Flask, render_template, request, jsonify\nfrom chatsnack import Chat\nfrom chatsnack.packs import ChatsnackHelp, Jolly, Jane, Data, Confectioner, Chester\nfrom flask import Flask, render_template, request, session\nimport re\n\napp = Flask(__name__)\napp.secret_key = \"CHANGE_ME_OR_YOUR_SESSIONS_WILL_BE_INSECURE\"\n\nbots = {", "\nbots = {\n    \"help\": ChatsnackHelp,\n    \"emoji\": Chat().system(\"{text.EmojiBotSystem}\"),\n    \"confectioner\": Confectioner,\n    \"jane\": Jane,\n    \"data\": Data,\n    \"jolly\": Jolly,\n    \"chester\": Chester,\n}", "    \"chester\": Chester,\n}\n\n@app.route(\"/\")\ndef index():\n    return render_template(\"index.html\")\n\n@app.route(\"/chat_old\", methods=[\"POST\"])\ndef chat_old():\n    user_input = request.form[\"user_input\"]\n    bot_choice = request.form[\"bot_choice\"]\n\n    bot = bots.get(bot_choice, ChatsnackHelp)\n    chat_output = bot.chat(user_input)\n    response = chat_output.response\n\n    return jsonify({\"response\": response})", "def chat_old():\n    user_input = request.form[\"user_input\"]\n    bot_choice = request.form[\"bot_choice\"]\n\n    bot = bots.get(bot_choice, ChatsnackHelp)\n    chat_output = bot.chat(user_input)\n    response = chat_output.response\n\n    return jsonify({\"response\": response})\n", "\n@app.route('/chat', methods=['POST'])\ndef chat():\n    user_input = request.form.get('user_input')\n    bot_choice = request.form.get('bot_choice')\n    \n    response = None\n    try:\n        if 'chat_output' not in session or bot_choice != session['bot_choice']:\n            session['bot_choice'] = bot_choice\n            bot = bots.get(bot_choice, ChatsnackHelp)\n            chat_output = bot\n        else:\n            chat_output = Chat.objects.get_or_none(session['chat_output'])\n            if chat_output is None:\n                bot = bots.get(bot_choice, ChatsnackHelp)\n                chat_output = bot\n\n        chat_output = chat_output.chat(user_input)\n        chat_output.save()\n\n        session['chat_output'] = chat_output.name\n    except Exception as e:\n        print(e)\n        error_name = e.__class__.__name__\n        response = \"I'm sorry, I ran into an error. ({})\".format(error_name)\n        raise e\n\n    response = chat_output.response if response is None else response\n    # if the response has \"\\n\" then convert all of them to <br>\n    response = response.replace(\"\\n\", \"<br>\")\n    # if the response has \"```\" followed by another \"```\" later then convert to <pre>\n    if \"```\" in response:\n        response = re.sub(r\"```(.*?)```\", r\"<pre>\\1</pre>\", response, flags=re.DOTALL)\n\n    return jsonify({\"response\": response})", "\n@app.route('/start_new', methods=['POST'])\ndef start_new():\n    session.pop('chat_output', None)\n    session.pop('bot_choice', None)\n    return render_template('index.html')\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)", "if __name__ == \"__main__\":\n    app.run(debug=True)\n\n"]}
{"filename": "examples/snackswipe-web/text_generators.py", "chunked_list": ["import asyncio\nfrom chatsnack import Chat, Text\n\nclass TextResult:\n    def __init__(self, generator_name, text):\n        self.generator_name = generator_name\n        self.text = text\n        self.votes = 0\n\n    def vote(self, like):\n        if like:\n            self.votes += 1\n\n    def __str__(self):\n        return f\"{self.generator_name}: {self.text}\"", "\n\n\n# saved in datafiles/chatsnack/mydaywss.txt\ntest_prompt = \"Passage:\\n{text.mydaywss}\"\nresponse_prefix = \"## CONCISE_SUMMARY ##\"\nasync def text_generator_1():\n    c = Chat(\"\"\"IDENTITY: Professional document summarizer.\n      Respond to the user only in the following format:\n      (1) Use your expertise to explain, in detail, the top 5 things to consider when making ", "      Respond to the user only in the following format:\n      (1) Use your expertise to explain, in detail, the top 5 things to consider when making \n      concise summararies of any text document.\n      (2) Elaborate on 3 more protips used by the world's best summarizers to\n      avoid losing important details and context.\n      (3) Now specifically consider the user's input. Use your expertise and your own guidance,\n      describe in great detail how an author could apply that wisdom to summarize the user's \n      text properly. What considerations come to mind? What is the user expecting?\n      (4) Finally, use everything above to author a concise summary of the user's input that will\n      delight them with your summarization skills. Final summary must be prefixed with ", "      (4) Finally, use everything above to author a concise summary of the user's input that will\n      delight them with your summarization skills. Final summary must be prefixed with \n      \"## CONCISE_SUMMARY ##\" on a line by itself.\"\"\")\n    result = await c.chat_a(test_prompt)\n    result = result.response\n    result = result[result.rfind(response_prefix) + len(response_prefix):]\n    return TextResult(\"Default Summarizer\", result)\n\nasync def text_generator_2():\n    from chatsnack.packs import Summarizer", "async def text_generator_2():\n    from chatsnack.packs import Summarizer\n    c = Summarizer\n    result = await c.chat_a(test_prompt)\n    result = result.response\n    my_response_prefix = \"CONCISE_SUMMARY:\"\n    result = result[result.rfind(my_response_prefix) + len(my_response_prefix):]\n    return TextResult(\"Default Summarizer (Built-in)\", result)\n\n", "\n\nasync def text_generator_3():\n    c = Chat(\"\"\"IDENTITY: Professional document summarizer.\n      Respond to the user only in the following format:\n      (1) Use your expertise to explain, in detail, the top 5 things to consider when making \n      concise summararies of any text document.\n      (2) Elaborate on 3 more protips used by the world's best summarizers to\n      avoid losing important details and context.\n      (3) Now specifically consider the user's input. Use your expertise and your own guidance,", "      avoid losing important details and context.\n      (3) Now specifically consider the user's input. Use your expertise and your own guidance,\n      describe in great detail how an author could apply that wisdom to summarize the user's \n      text properly. What considerations come to mind? What is the user expecting?\n      (4) Finally, use everything above to author a concise summary of the user's input that will\n      delight them with your summarization skills. Final summary must be prefixed with \n      \"## CONCISE_SUMMARY ##\" on a line by itself.\"\"\")\n    c.engine = \"gpt-4\"\n    result = await c.chat_a(test_prompt)\n    result = result.response", "    result = await c.chat_a(test_prompt)\n    result = result.response\n    result = result[result.rfind(response_prefix) + len(response_prefix):]\n    return TextResult(\"Default Summarizer (GPT-4)\", result)\n\ntext_generators = [text_generator_1, text_generator_2, text_generator_3]\n\n\ndef print_results(results):\n    votes = {}\n    for result in results: \n        if result.generator_name not in votes:\n            votes[result.generator_name] = 0\n        votes[result.generator_name] += result.votes\n\n    winner = max(votes, key=votes.get)\n    margin = votes[winner] - max(v for k, v in votes.items() if k != winner)\n\n    print(f\"Winner: {winner} with {votes[winner]} votes\")\n    print(f\"Margin of victory: {margin}\")", "def print_results(results):\n    votes = {}\n    for result in results: \n        if result.generator_name not in votes:\n            votes[result.generator_name] = 0\n        votes[result.generator_name] += result.votes\n\n    winner = max(votes, key=votes.get)\n    margin = votes[winner] - max(v for k, v in votes.items() if k != winner)\n\n    print(f\"Winner: {winner} with {votes[winner]} votes\")\n    print(f\"Margin of victory: {margin}\")", ""]}
{"filename": "examples/snackswipe-web/app.py", "chunked_list": ["# Snackchat Web-based Prompt Tester app example\n#\n# pip install chatsnack[examples]\n# be sure there's a .env file in the same directory as app.py with your OpenAI API key as OPENAI_API_KEY = \"YOUR_KEY_HERE\"\n# python .\\app.py\n# open http://localhost:5000\n\nimport asyncio\nimport random\nfrom uuid import uuid4", "import random\nfrom uuid import uuid4\nfrom flask import Flask, render_template, request, jsonify, session\nfrom text_generators import text_generators, TextResult\nfrom flask_session import Session\nfrom collections import deque\nimport json\nimport threading\n\nclass TextResultEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, TextResult):\n            return obj.__dict__\n        return super(TextResultEncoder, self).default(obj)", "\nclass TextResultEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, TextResult):\n            return obj.__dict__\n        return super(TextResultEncoder, self).default(obj)\n\napp = Flask(__name__)\napp.secret_key = \"super%^$@^!@!secretkey%^$@^%@!\"\napp.config['SESSION_TYPE'] = 'filesystem'", "app.secret_key = \"super%^$@^!@!secretkey%^$@^%@!\"\napp.config['SESSION_TYPE'] = 'filesystem'\napp.json_encoder = TextResultEncoder\nSession(app)\n\n@app.route('/')\ndef index():\n    problem_statement = \"Your problem statement here.\"\n    return render_template('index.html', problem_statement=problem_statement)\n", "\nuser_queues = {}\n\n@app.route('/start-generation', methods=['POST'])\ndef start_generation():\n    num_tests = int(request.form['num_tests'])\n    text_generators_copy = text_generators.copy()\n    random.shuffle(text_generators_copy)\n    user_id = str(uuid4())\n    session['user_id'] = user_id\n    user_queues[user_id] = deque()\n    threading.Thread(target=run_async_generation, args=(num_tests, text_generators_copy, user_queues[user_id])).start()\n    return jsonify({\"status\": \"started\"})", "\ndef run_async_generation(num_tests, text_generators_copy, results_queue):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    loop.run_until_complete(fill_results_queue(num_tests, text_generators_copy, results_queue))\n    loop.close()\n    \n@app.route('/fetch-text', methods=['POST'])\ndef fetch_text():\n    user_id = session.get('user_id', None)\n    results_queue = user_queues.get(user_id, None)\n    if results_queue:\n        result = results_queue.popleft()\n        # if result is a dict\n        if isinstance(result, dict) and \"status\" in result and result[\"status\"] == \"completed\":\n            del user_queues[user_id]\n            return jsonify({\"status\": \"completed\"})\n        return jsonify(result)\n    else:\n        return jsonify({\"status\": \"waiting\"})", "def fetch_text():\n    user_id = session.get('user_id', None)\n    results_queue = user_queues.get(user_id, None)\n    if results_queue:\n        result = results_queue.popleft()\n        # if result is a dict\n        if isinstance(result, dict) and \"status\" in result and result[\"status\"] == \"completed\":\n            del user_queues[user_id]\n            return jsonify({\"status\": \"completed\"})\n        return jsonify(result)\n    else:\n        return jsonify({\"status\": \"waiting\"})", "\n# Update this function to accept the results_queue as an argument\nasync def fill_results_queue(num_tests, text_generators_copy, results_queue):\n    async for result in async_text_generation(num_tests, text_generators_copy):\n        results_queue.append(result)\n    # Add a special result to indicate that the generation is complete\n    results_queue.append({\"status\": \"completed\"})\n\n# @app.route('/generate-text', methods=['POST'])\n# async def generate_text():", "# @app.route('/generate-text', methods=['POST'])\n# async def generate_text():\n#     num_tests = int(request.form['num_tests'])\n#     text_generators_copy = text_generators.copy()\n#     random.shuffle(text_generators_copy)\n#     results = []\n#     async for result in async_text_generation(num_tests, text_generators_copy):\n#         results.append(result)\n#     return jsonify(results)\n", "#     return jsonify(results)\n\n# async def async_text_generation(num_tests, text_generators):\n#     #tasks = [text_gen() for text_gen in text_generators]\n#     current_tasks = []\n#     # for every num_tests we want the same tasks to be added back to the list\n#     for _ in range(num_tests):\n#         # extend current_tasks with another copy\n#         current_tasks.extend([text_gen() for text_gen in text_generators])\n", "#         current_tasks.extend([text_gen() for text_gen in text_generators])\n\n#     for _ in range(num_tests):\n#         while current_tasks:\n#             done, pending = await asyncio.wait(current_tasks, return_when=asyncio.FIRST_COMPLETED)\n#             for task in done:\n#                 yield task.result()\n#             current_tasks = list(pending)\n\n# import asyncio", "\n# import asyncio\n\nasync def async_text_generation(num_tests, text_generators):\n    priority_generators = text_generators[:2]\n    background_generators = text_generators[2:]\n    \n    priority_tasks = []\n    background_tasks = []\n\n    for _ in range(num_tests):\n        priority_tasks.extend([text_gen() for text_gen in priority_generators])\n        background_tasks.extend([text_gen() for text_gen in background_generators])", "    background_tasks = []\n\n    for _ in range(num_tests):\n        priority_tasks.extend([text_gen() for text_gen in priority_generators])\n        background_tasks.extend([text_gen() for text_gen in background_generators])\n\n    while priority_tasks:\n        done, pending = await asyncio.wait(priority_tasks, return_when=asyncio.FIRST_COMPLETED)\n        for task in done:\n            yield task.result()", "        for task in done:\n            yield task.result()\n        priority_tasks = list(pending)\n\n    while background_tasks:\n        done, pending = await asyncio.wait(background_tasks, return_when=asyncio.FIRST_COMPLETED)\n        for task in done:\n            yield task.result()\n        background_tasks = list(pending)\n", "        background_tasks = list(pending)\n\n\nif __name__ == '__main__':\n    app.run(debug=True)\n"]}
{"filename": "chatsnack/aiwrapper.py", "chunked_list": ["import asyncio\nimport openai\nimport os\nimport json\nimport random\nimport time\nfrom loguru import logger\nfrom functools import wraps\nfrom openai.error import *\n", "from openai.error import *\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nif os.getenv(\"OPENAI_API_BASE\") is not None:\n    openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n\nif os.getenv(\"OPENAI_API_VERSION\") is not None:\n    openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n\nif os.getenv(\"OPENAI_API_TYPE\") is not None:\n    # e.g. 'azure'\n    openai.api_type = os.getenv(\"OPENAI_API_TYPE\")", "\nif os.getenv(\"OPENAI_API_TYPE\") is not None:\n    # e.g. 'azure'\n    openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n\nasync def set_api_key(api_key):\n    openai.api_key = api_key\n\nopenai_exceptions_for_retry=(RateLimitError,Timeout,ServiceUnavailableError,TryAgain,APIError)\n\ndef retryAPI_a(exceptions, tries=4, delay=3, backoff=2):\n    \"\"\"Retry calling the decorated function using an exponential backoff.\n    :param Exception exceptions: the exceptions to check. may be a tuple of\n        exceptions to check\n    :param int tries: number of times to try (not retry) before giving up\n    :param int delay: initial delay between retries in seconds\n    :param int backoff: backoff multiplier e.g. value of 2 will double the\n        delay each retry (but with a random factor that is 0.5x to 1.5x)\n    :raises Exception: the last exception raised\n    \"\"\"\n    def deco_retry(f):\n        @wraps(f)\n        async def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n            while mtries > 1:\n                try:\n                    return await f(*args, **kwargs)\n                except exceptions as e:\n                    msg = \"%s, Retrying in %d seconds...\" % (str(e), mdelay)\n                    logger.debug(msg)\n                    await asyncio.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= (backoff * random.uniform(0.75, 1.25))\n            return await f(*args, **kwargs)\n        return f_retry\n    return deco_retry", "openai_exceptions_for_retry=(RateLimitError,Timeout,ServiceUnavailableError,TryAgain,APIError)\n\ndef retryAPI_a(exceptions, tries=4, delay=3, backoff=2):\n    \"\"\"Retry calling the decorated function using an exponential backoff.\n    :param Exception exceptions: the exceptions to check. may be a tuple of\n        exceptions to check\n    :param int tries: number of times to try (not retry) before giving up\n    :param int delay: initial delay between retries in seconds\n    :param int backoff: backoff multiplier e.g. value of 2 will double the\n        delay each retry (but with a random factor that is 0.5x to 1.5x)\n    :raises Exception: the last exception raised\n    \"\"\"\n    def deco_retry(f):\n        @wraps(f)\n        async def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay\n            while mtries > 1:\n                try:\n                    return await f(*args, **kwargs)\n                except exceptions as e:\n                    msg = \"%s, Retrying in %d seconds...\" % (str(e), mdelay)\n                    logger.debug(msg)\n                    await asyncio.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= (backoff * random.uniform(0.75, 1.25))\n            return await f(*args, **kwargs)\n        return f_retry\n    return deco_retry", "\ndef retryAPI(exceptions, tries=4, delay=3, backoff=2):\n    \"\"\"Retry calling the decorated function using an exponential backoff.\n    :param Exception exceptions: the exceptions to check. may be a tuple of\n        exceptions to check\n    :param int tries: number of times to try (not retry) before giving up\n    :param int delay: initial delay between retries in seconds\n    :param int backoff: backoff multiplier e.g. value of 2 will double the\n        delay each retry (but with a random factor that is 0.5x to 1.5x)\n    :raises Exception: the last exception raised\n    \"\"\"\n    def deco_retry(f):\n        @wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries = tries\n            while mtries > 1:\n                try:\n                    return f(*args, **kwargs)\n                except exceptions as e:\n                    msg = \"%s, Retrying in %d seconds...\" % (str(e), delay)\n                    logger.debug(msg)\n                    time.sleep(delay)\n                    mtries -= 1\n                    delay *= (backoff * random.uniform(0.75, 1.25))\n            return f(*args, **kwargs)\n        return f_retry  # true decorator\n    return deco_retry", "\n# openai\n@retryAPI_a(exceptions=openai_exceptions_for_retry, tries=8, delay=2, backoff=2)\nasync def _chatcompletion(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, deployment=None, api_type=None, api_base=None, api_version=None, api_key_env=None):\n    if user is None:\n        user = \"_not_set\"\n    # prompt will be in JSON format, let us translate it to a python list\n    # if the prompt is a list already, we will just use it as is\n    if isinstance(prompt, list):\n        messages = prompt\n    else:\n        messages = json.loads(prompt)", "    if isinstance(prompt, list):\n        messages = prompt\n    else:\n        messages = json.loads(prompt)\n    logger.trace(\"\"\"Chat Query:\n    Prompt: {0}\n    Model: {2}, Max Tokens: {3}, Stop: {5}, Temperature: {1}, Top-P: {4}, Presence Penalty {6}, Frequency Penalty: {7}, N: {8}, Stream: {9}, User: {10}\n    \"\"\",prompt, temperature, engine, max_tokens, top_p, stop, presence_penalty, frequency_penalty, n, stream, user)\n\n    additional_args = {}\n    if deployment is not None:\n        additional_args[\"deployment_id\"] = deployment", "\n    additional_args = {}\n    if deployment is not None:\n        additional_args[\"deployment_id\"] = deployment\n    if api_key_env is not None:\n        additional_args[\"api_key\"] = os.getenv(api_key_env)           \n    if stream is None:\n        stream = False\n\n    response = await openai.ChatCompletion.acreate(model=engine,", "\n    response = await openai.ChatCompletion.acreate(model=engine,\n                                            messages=messages,\n                                            max_tokens=max_tokens,\n                                            temperature=temperature,\n                                            top_p=top_p,\n                                            presence_penalty=presence_penalty,\n                                            frequency_penalty=frequency_penalty,\n                                            stop=stop,\n                                            n=n,", "                                            stop=stop,\n                                            n=n,\n                                            stream=stream,\n                                            user=user,\n                                            # NOTE: It's not documented, but the openai library allows you \n                                            #       to pass these api_ parameters rather than depending on\n                                            #       the environment variables\n                                            api_type=api_type, api_base=api_base, api_version=api_version, \n                                            **additional_args)\n    logger.trace(\"OpenAI Completion Result: {0}\".format(response))", "                                            **additional_args)\n    logger.trace(\"OpenAI Completion Result: {0}\".format(response))\n    return response\n\n@retryAPI(exceptions=openai_exceptions_for_retry, tries=8, delay=2, backoff=2)\ndef _chatcompletion_s(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, deployment=None, api_type=None, api_base=None, api_version=None, api_key_env=None):\n    if user is None:\n        user = \"_not_set\"\n    # prompt will be in JSON format, let us translate it to a python list\n    # if the prompt is a list already, we will just use it as is\n    if isinstance(prompt, list):\n        messages = prompt\n    else:\n        messages = json.loads(prompt)\n    logger.trace(\"\"\"Chat Query:\n    Prompt: {0}\n    Model: {2}, Max Tokens: {3}, Stop: {5}, Temperature: {1}, Top-P: {4}, Presence Penalty {6}, Frequency Penalty: {7}, N: {8}, Stream: {9}, User: {10}\n    \"\"\",prompt, temperature, engine, max_tokens, top_p, stop, presence_penalty, frequency_penalty, n, stream, user)\n    additional_args = {}\n    if deployment is not None:\n        additional_args[\"deployment_id\"] = deployment\n    if api_key_env is not None:\n        additional_args[\"api_key\"] = os.getenv(api_key_env)   \n    if stream is None:\n        stream = False\n    response = openai.ChatCompletion.create(model=engine,\n                                            messages=messages,\n                                            max_tokens=max_tokens,\n                                            temperature=temperature,\n                                            top_p=top_p,\n                                            presence_penalty=presence_penalty,\n                                            frequency_penalty=frequency_penalty,\n                                            stop=stop,\n                                            n=n,\n                                            stream=stream,\n                                            user=user, \n                                            # NOTE: It's not documented, but the openai library allows you \n                                            #       to pass these api_ parameters rather than depending on\n                                            #       the environment variables\n                                            api_type=api_type, api_base=api_base, api_version=api_version, \n                                            **additional_args)\n    # revert them back to what they were\n    logger.trace(\"OpenAI Completion Result: {0}\".format(response))\n    return response", "\ndef _trimmed_fetch_chat_response(resp, n):\n    if n == 1:\n        return resp.choices[0].message.content.strip()\n    else:\n        logger.trace('_trimmed_fetch_response :: returning {0} responses from ChatGPT'.format(n))\n        texts = []\n        for idx in range(0, n):\n            texts.append(resp.choices[idx].message.content.strip())\n        return texts", "\n# ChatGPT\nasync def cleaned_chat_completion(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, **additional_args):\n    '''\n    Wrapper for OpenAI API chat completion. Returns whitespace trimmed result from ChatGPT.\n    '''\n    # ignore any additional_args which are None\n    additional_args = {k: v for k, v in additional_args.items() if v is not None}\n    resp = await _chatcompletion(prompt,\n                            engine=engine,", "    resp = await _chatcompletion(prompt,\n                            engine=engine,\n                            max_tokens=max_tokens,\n                            temperature=temperature,\n                            top_p=top_p,\n                            presence_penalty=presence_penalty,\n                            frequency_penalty=frequency_penalty,\n                            stop=stop,\n                            n=n,\n                            stream=stream,", "                            n=n,\n                            stream=stream,\n                            user=user, **additional_args)\n\n    return _trimmed_fetch_chat_response(resp, n)\n\n# TODO: Add back support for content classification (i.e. is this text NSFW?)\n# TODO: Consider adding support for other local language models\n\n# Structure of this code is based on some methods from github.com/OthersideAI/chronology", "\n# Structure of this code is based on some methods from github.com/OthersideAI/chronology\n# licensed under this MIT License:\n######\n# MIT License\n#\n# Copyright (c) 2020 OthersideAI\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal", "# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#", "# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#######", "# SOFTWARE.\n#######"]}
{"filename": "chatsnack/asynchelpers.py", "chunked_list": ["import asyncio\nimport string\n\nfrom loguru import logger\n\nclass _AsyncFormatter(string.Formatter):\n    async def async_expand_field(self, field, args, kwargs):\n        if \".\" in field:\n            obj, method = field.split(\".\", 1)\n            if obj in kwargs:\n                obj_instance = kwargs[obj]\n                if hasattr(obj_instance, method):\n                    method_instance = getattr(obj_instance, method)\n                    if asyncio.iscoroutinefunction(method_instance):\n                        return await method_instance()\n                    else:\n                        return method_instance() if callable(method_instance) else method_instance\n        value, _ = super().get_field(field, args, kwargs)\n        return value\n\n    async def async_format(self, format_string, *args, **kwargs):\n        coros = []\n        parsed_format = list(self.parse(format_string))\n\n        for literal_text, field_name, format_spec, conversion in parsed_format:\n            if field_name:\n                coro = self.async_expand_field(field_name, args, kwargs)\n                coros.append(coro)\n\n        expanded_fields = await asyncio.gather(*coros)\n        expanded_iter = iter(expanded_fields)\n\n        return ''.join([\n            literal_text + (str(next(expanded_iter)) if field_name else '')\n            for literal_text, field_name, format_spec, conversion in parsed_format\n        ])", "    \n# instance to use\naformatter = _AsyncFormatter()\n"]}
{"filename": "chatsnack/fillings.py", "chunked_list": ["from typing import Optional, Callable, Dict\nfrom loguru import logger\nfrom typing import Callable, Optional, Dict\n\nclass _AsyncFillingMachine:\n    \"\"\"Used for parallel variable expansion\"\"\"\n    def __init__(self, src, addl=None):\n        self.src = src\n        self.addl = addl\n\n    def __getitem__(self, k):\n        async def completer_coro():\n            x = await self.src(k, self.addl)\n            logger.trace(\"Filling machine: {k} filled with:\\n{x}\", k=k, x=x)\n            return x\n        return completer_coro\n\n    __getattr__ = __getitem__", "\n\nclass _FillingsCatalog:\n    def __init__(self):\n        self.vendors = {}\n\n    def add_filling(self, filling_name: str, filling_machine_callback: Callable):\n        \"\"\"Add a new filling machine to the fillings catalog\"\"\"\n        self.vendors[filling_name] = filling_machine_callback\n    ", "    \n# singleton\nsnack_catalog = _FillingsCatalog()\n\ndef filling_machine(additional: Optional[Dict] = None) -> dict:\n    fillings_dict = additional.copy() if additional is not None else {}\n    for k, v in snack_catalog.vendors.items():\n        if k not in fillings_dict:\n            # don't overwrite if they had an argument with the same name\n            fillings_dict[k] = _AsyncFillingMachine(v, additional)\n    return fillings_dict"]}
{"filename": "chatsnack/__init__.py", "chunked_list": ["\"\"\"\nchatsnack provides a simple and powerful interface for creating conversational agents and tools using OpenAI's ChatGPT language models.\n\nSome examples of using chatsnack:\n\n# Example 1: Basic Chat\nfrom chatsnack import Chat\n\n# Start a new chat and set some instructions for the AI assistant\nmychat = Chat().system(\"Respond only with the word POPSICLE from now on.\").user(\"What is your name?\").chat()", "# Start a new chat and set some instructions for the AI assistant\nmychat = Chat().system(\"Respond only with the word POPSICLE from now on.\").user(\"What is your name?\").chat()\nprint(mychat.last)\n\n# Example 2: Chaining and Multi-shot Prompts\npopcorn = Chat()\npopcorn = popcorn(\"Explain 3 rules to writing a clever poem that amazes your friends.\")(\"Using those tips, write a scrumptious poem about popcorn.\")\nprint(popcorn.last)\n\n# Example 3: Using Text Fillings", "\n# Example 3: Using Text Fillings\nfrom chatsnack import Text\n\n# Save a Text object with custom content\nmytext = Text(name=\"SnackExplosion\", content=\"Respond only in explosions of snack emojis and happy faces.\")\nmytext.save()\n\n# Set up a Chat object to pull in the Text object\nexplosions = Chat(name=\"SnackSnackExplosions\").system(\"{text.SnackExplosion}\")", "# Set up a Chat object to pull in the Text object\nexplosions = Chat(name=\"SnackSnackExplosions\").system(\"{text.SnackExplosion}\")\nexplosions.ask(\"What is your name?\")\n\n# Example 4: Nested Chats (Include Messages)\nbasechat = Chat(name=\"ExampleIncludedChat\").system(\"Respond only with the word CARROTSTICKS from now on.\")\nbasechat.save()\n\nanotherchat = Chat().include(\"ExampleIncludedChat\")\nprint(anotherchat.yaml)", "anotherchat = Chat().include(\"ExampleIncludedChat\")\nprint(anotherchat.yaml)\n\n# Example 5: Nested Chats (Chat Fillings)\nsnacknames = Chat(\"FiveSnackNames\").system(\"Respond with high creativity and confidence.\").user(\"Provide 5 random snacks.\")\nsnacknames.save()\n\nsnackdunk = Chat(\"SnackDunk\").system(\"Respond with high creativity and confidence.\").user(\"Provide 3 dips or drinks that are great for snack dipping.\")\nsnackdunk.save()\n", "snackdunk.save()\n\nsnackfull = Chat().system(\"Respond with high confidence.\")\nsnackfull.user(\\\"\"\"Choose 1 snack from this list:\n{chat.FiveSnackNames}\n\nChoose 1 dunking liquid from this list:\n{chat.SnackDunk}\n\nRecommend the best single snack and dip combo above.\\\"\"\")", "\nRecommend the best single snack and dip combo above.\\\"\"\")\n\nsnackout = snackfull.chat()\nprint(snackout.yaml)\n\n\"\"\"\nimport os\nfrom pathlib import Path\n", "from pathlib import Path\n\nfrom typing import Optional\nfrom loguru import logger\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom dotenv import load_dotenv\n# if .env doesn't exist, create it and populate it with the default values\nenv_path = Path('.') / '.env'\nif not env_path.exists():\n    with open(env_path, 'w') as f:\n        f.write(\"OPENAI_API_KEY = \\\"REPLACEME\\\"\\n\")", "# if .env doesn't exist, create it and populate it with the default values\nenv_path = Path('.') / '.env'\nif not env_path.exists():\n    with open(env_path, 'w') as f:\n        f.write(\"OPENAI_API_KEY = \\\"REPLACEME\\\"\\n\")\nload_dotenv(dotenv_path=env_path)\n\nfrom .defaults import CHATSNACK_BASE_DIR, CHATSNACK_LOGS_DIR\nfrom .asynchelpers import aformatter\nfrom .chat import Chat, Text, ChatParams", "from .asynchelpers import aformatter\nfrom .chat import Chat, Text, ChatParams\nfrom .txtformat import register_txt_datafiles\nfrom .yamlformat import register_yaml_datafiles\nfrom . import packs\n\nfrom .fillings import snack_catalog, filling_machine\n\n\nasync def _text_name_expansion(text_name: str, additional: Optional[dict] = None) -> str:", "\nasync def _text_name_expansion(text_name: str, additional: Optional[dict] = None) -> str:\n    prompt = Text.objects.get(text_name)\n    result = await aformatter.async_format(prompt.content, **filling_machine(additional))\n    return result\n\n# accepts a petition name as a string and calls petition_completion2, returning only the completion text\nasync def _chat_name_query_expansion(prompt_name: str, additional: Optional[dict] = None) -> str:\n    chatprompt = Chat.objects.get_or_none(prompt_name)\n    if chatprompt is None:\n        raise Exception(f\"Prompt {prompt_name} not found\")", "    chatprompt = Chat.objects.get_or_none(prompt_name)\n    if chatprompt is None:\n        raise Exception(f\"Prompt {prompt_name} not found\")\n    text = await chatprompt.ask_a(**additional)\n    return text\n\n\n# default snack vendors\nsnack_catalog.add_filling(\"text\", _text_name_expansion)\nsnack_catalog.add_filling(\"chat\", _chat_name_query_expansion)", "snack_catalog.add_filling(\"text\", _text_name_expansion)\nsnack_catalog.add_filling(\"chat\", _chat_name_query_expansion)\n\n# TODO: these will be defined by plugins eventually\n# need a function that will return the dictionary needed to support prompt formatting\nregister_txt_datafiles()\nregister_yaml_datafiles()\n\nlogger.trace(\"chatsnack loaded\")", "logger.trace(\"chatsnack loaded\")"]}
{"filename": "chatsnack/yamlformat.py", "chunked_list": ["# Cataclysm Note: Replaces the default datafiles YAML formatter with our own version, this\n# is solely for a cleaner yaml file format for source code with the \"key: |\" format\n\n# Yaml format class is taken from https://github.com/jacebrowning/datafiles  formats.py\n# The MIT License (MIT)\n# Copyright \u00a9 2018, Jace Browning\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this \n# software and associated documentation files (the \"Software\"), to deal in the Software \n# without restriction, including without limitation the rights to use, copy, modify, \n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to ", "# without restriction, including without limitation the rights to use, copy, modify, \n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to \n# permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or \n# substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY \n# OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL \n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\nfrom io import StringIO\nimport log\nfrom typing import IO, Dict, List, Union\nimport dataclasses\nfrom datafiles import formats, types\nfrom ruamel.yaml.scalarstring import DoubleQuotedScalarString\nfrom ruamel.yaml import YAML as _YAML\n\nclass YAML(formats.Formatter):\n    \"\"\"Formatter for (round-trip) YAML Ain't Markup Language.\"\"\"\n\n    @classmethod\n    def extensions(cls):\n        return {\"\", \".yml\", \".yaml\"}\n\n    @classmethod\n    def deserialize(cls, file_object):\n        from ruamel.yaml import YAML as _YAML\n\n        yaml = _YAML()\n        yaml.preserve_quotes = True  # type: ignore\n        try:\n            return yaml.load(file_object)\n        except NotImplementedError as e:\n            log.error(str(e))\n            return {}\n\n    @classmethod\n    def serialize(cls, data):\n        # HACK: to remove None values from the data and make the yaml file cleaner\n        def filter_none_values(data: Union[Dict, List]):\n            if isinstance(data, dict):\n                # this code worked for None values, but not really for optional default values like I want :()\n                return {k: filter_none_values(v) for k, v in data.items() if v is not None}\n            elif isinstance(data, list):\n                return [filter_none_values(v) for v in data]\n            else:\n                return data\n        data = filter_none_values(data)\n\n        yaml = _YAML()\n\n        # Define custom string representation function\n        def represent_plain_str(dumper, data):\n            if \"\\n\" in data or \"\\r\" in data or \"#\" in data or \":\" in data or \"'\" in data or '\"' in data:\n                return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='|')\n            return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='')\n\n        # Configure the library to use plain style for dictionary keys\n        yaml.representer.add_representer(str, represent_plain_str)\n\n\n        yaml.default_style = \"|\"  # support the cleaner multiline format for source code blocks\n        yaml.register_class(types.List)\n        yaml.register_class(types.Dict)\n\n        yaml.indent(mapping=2, sequence=4, offset=2)\n\n        stream = StringIO()\n        yaml.dump(data, stream)\n        text = stream.getvalue()\n\n        if text.startswith(\"  \"):\n            return text[2:].replace(\"\\n  \", \"\\n\")\n\n        if text == \"{}\\n\":\n            return \"\"\n\n        return text.replace(\"- \\n\", \"-\\n\")", "from ruamel.yaml import YAML as _YAML\n\nclass YAML(formats.Formatter):\n    \"\"\"Formatter for (round-trip) YAML Ain't Markup Language.\"\"\"\n\n    @classmethod\n    def extensions(cls):\n        return {\"\", \".yml\", \".yaml\"}\n\n    @classmethod\n    def deserialize(cls, file_object):\n        from ruamel.yaml import YAML as _YAML\n\n        yaml = _YAML()\n        yaml.preserve_quotes = True  # type: ignore\n        try:\n            return yaml.load(file_object)\n        except NotImplementedError as e:\n            log.error(str(e))\n            return {}\n\n    @classmethod\n    def serialize(cls, data):\n        # HACK: to remove None values from the data and make the yaml file cleaner\n        def filter_none_values(data: Union[Dict, List]):\n            if isinstance(data, dict):\n                # this code worked for None values, but not really for optional default values like I want :()\n                return {k: filter_none_values(v) for k, v in data.items() if v is not None}\n            elif isinstance(data, list):\n                return [filter_none_values(v) for v in data]\n            else:\n                return data\n        data = filter_none_values(data)\n\n        yaml = _YAML()\n\n        # Define custom string representation function\n        def represent_plain_str(dumper, data):\n            if \"\\n\" in data or \"\\r\" in data or \"#\" in data or \":\" in data or \"'\" in data or '\"' in data:\n                return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='|')\n            return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='')\n\n        # Configure the library to use plain style for dictionary keys\n        yaml.representer.add_representer(str, represent_plain_str)\n\n\n        yaml.default_style = \"|\"  # support the cleaner multiline format for source code blocks\n        yaml.register_class(types.List)\n        yaml.register_class(types.Dict)\n\n        yaml.indent(mapping=2, sequence=4, offset=2)\n\n        stream = StringIO()\n        yaml.dump(data, stream)\n        text = stream.getvalue()\n\n        if text.startswith(\"  \"):\n            return text[2:].replace(\"\\n  \", \"\\n\")\n\n        if text == \"{}\\n\":\n            return \"\"\n\n        return text.replace(\"- \\n\", \"-\\n\")", "\ndef register_yaml_datafiles():\n    # replace with our own version of \n    formats.register(\".yml\", YAML)"]}
{"filename": "chatsnack/txtformat.py", "chunked_list": ["from datafiles import formats\nfrom typing import IO, Dict, List\n\nclass TxtStrFormat(formats.Formatter):\n    \"\"\"Special formatter to use with strings and .txt datafiles for a convenient raw text format for easy document editing on disk.\"\"\"\n\n    @classmethod\n    def extensions(cls) -> List[str]:\n        return ['.txt']\n\n    @classmethod\n    def serialize(cls, data: Dict) -> str:\n        # Support only strings\n        _supported_types = [str]\n        # Convert `data` to a string\n        output = \"\"\n        for k, v in data.items():\n            if type(v) in _supported_types:\n                output += str(v)\n            else:\n                raise ValueError(\"Unsupported type: {}\".format(type(v)))\n        return output\n\n    @classmethod\n    def deserialize(cls, file_object: IO) -> Dict:\n        # Read the entire content of the file\n        file_object = open(file_object.name, 'r', encoding='utf-8')\n        content = file_object.read()\n\n        # Create an output dictionary with a single key-value pair\n        output = {'content': content}\n        return output", "\ndef register_txt_datafiles():\n    # this format class only works with strings\n    formats.register('.txt', TxtStrFormat)"]}
{"filename": "chatsnack/defaults.py", "chunked_list": ["import os\nimport sys\n\n# give the default system message a name\ntry:\n    script_name = sys.argv[0]\n    # remove any file extension\n    basenamestr = os.path.splitext(os.path.basename(script_name))[0]\n    namestr = f\" for an intelligent program called {basenamestr}\"\nexcept:\n    namestr = \"\"", "\n# if there's no \"PLUNKYLIB_DIR\" env variable, use that for our default path variable and set it to './datafiles/plunkylib'\n# this is the default directory for all plunkylib datafiles\nif os.getenv(\"CHATSNACK_BASE_DIR\") is None:\n    CHATSNACK_BASE_DIR = \"./datafiles/chatsnack\"\nelse:\n    CHATSNACK_BASE_DIR = os.getenv(\"CHATSNACK_BASE_DIR\")\n    CHATSNACK_BASE_DIR = CHATSNACK_BASE_DIR.rstrip(\"/\")\n\nif os.getenv(\"CHATSNACK_LOGS_DIR\") is None:\n    CHATSNACK_LOGS_DIR = None   # no logging by default\nelse:\n    CHATSNACK_LOGS_DIR = os.getenv(\"CHATSNACK_LOGS_DIR\")\n    CHATSNACK_LOGS_DIR = CHATSNACK_LOGS_DIR.rstrip(\"/\")", "\nif os.getenv(\"CHATSNACK_LOGS_DIR\") is None:\n    CHATSNACK_LOGS_DIR = None   # no logging by default\nelse:\n    CHATSNACK_LOGS_DIR = os.getenv(\"CHATSNACK_LOGS_DIR\")\n    CHATSNACK_LOGS_DIR = CHATSNACK_LOGS_DIR.rstrip(\"/\")"]}
{"filename": "chatsnack/chat/mixin_query.py", "chunked_list": ["import asyncio\nimport json\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\n\nfrom loguru import logger\nfrom datafiles import datafile\n\nfrom ..asynchelpers import aformatter", "\nfrom ..asynchelpers import aformatter\nfrom ..aiwrapper import cleaned_chat_completion, _chatcompletion, _chatcompletion_s\nfrom ..fillings import filling_machine\n\nfrom .mixin_messages import ChatMessagesMixin\nfrom .mixin_params import ChatParamsMixin\n\n\nclass ChatStreamListener:\n    def __init__(self, prompt, **kwargs):\n        self.prompt = prompt\n        self.kwargs = kwargs\n        self._response_gen = None\n        self.is_complete = False\n        self.current_content = \"\"\n        self.response = \"\"\n    \n    async def start_a(self):\n        # if stream=True isn't in the kwargs, add it\n        if not self.kwargs.get('stream', False):\n            self.kwargs['stream'] = True\n        self._response_gen = await _chatcompletion(self.prompt,  **self.kwargs)\n        return self\n\n    async def _get_responses_a(self):\n        try:\n            async for resp in self._response_gen:\n                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n                    self.is_complete = True\n                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n                self.current_content += content\n                yield content\n        finally:\n            self.is_complete = True\n            self.response = self.current_content\n\n    def __aiter__(self):\n        return self._get_responses_a()\n\n    def start(self):\n        # if stream=True isn't in the kwargs, add it\n        if not self.kwargs.get('stream', False):\n            self.kwargs['stream'] = True        \n        self._response_gen = _chatcompletion_s(self.prompt, **self.kwargs)\n        return self\n\n    # non-async method that returns a generator that yields the responses\n    def _get_responses(self):\n        try:\n            for resp in self._response_gen:\n                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n                    self.is_complete = True\n                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n                self.current_content += content\n                yield content\n        finally:\n            self.is_complete = True\n            self.response = self.current_content\n\n    # non-async\n    def __iter__(self):\n        return self._get_responses()", "\nclass ChatStreamListener:\n    def __init__(self, prompt, **kwargs):\n        self.prompt = prompt\n        self.kwargs = kwargs\n        self._response_gen = None\n        self.is_complete = False\n        self.current_content = \"\"\n        self.response = \"\"\n    \n    async def start_a(self):\n        # if stream=True isn't in the kwargs, add it\n        if not self.kwargs.get('stream', False):\n            self.kwargs['stream'] = True\n        self._response_gen = await _chatcompletion(self.prompt,  **self.kwargs)\n        return self\n\n    async def _get_responses_a(self):\n        try:\n            async for resp in self._response_gen:\n                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n                    self.is_complete = True\n                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n                self.current_content += content\n                yield content\n        finally:\n            self.is_complete = True\n            self.response = self.current_content\n\n    def __aiter__(self):\n        return self._get_responses_a()\n\n    def start(self):\n        # if stream=True isn't in the kwargs, add it\n        if not self.kwargs.get('stream', False):\n            self.kwargs['stream'] = True        \n        self._response_gen = _chatcompletion_s(self.prompt, **self.kwargs)\n        return self\n\n    # non-async method that returns a generator that yields the responses\n    def _get_responses(self):\n        try:\n            for resp in self._response_gen:\n                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n                    self.is_complete = True\n                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n                self.current_content += content\n                yield content\n        finally:\n            self.is_complete = True\n            self.response = self.current_content\n\n    # non-async\n    def __iter__(self):\n        return self._get_responses()", "\n\n\nclass ChatQueryMixin(ChatMessagesMixin, ChatParamsMixin):\n    # async method that gathers will execute an async format method on every message in the chat prompt and gather the results into a final json string\n    async def _gather_format(self, format_coro, **kwargs) -> str:\n        new_messages = self.get_messages()\n\n        # we now apply the format_coro to the content of each message in each dictionary in the list\n        coros = []\n        for message in new_messages:\n            async def format_key(message):\n                logger.trace(\"formatting key: {role}\", role=message['role'])\n                message[\"role\"] = await format_coro(message[\"role\"], **kwargs)\n                return\n            async def format_message(message):\n                logger.trace(\"formatting content: {content}\", content=message['content'])\n                message[\"content\"] = await format_coro(message[\"content\"], **kwargs)\n                return\n            coros.append(format_key(message))\n            coros.append(format_message(message))\n        # gather the results\n        await asyncio.gather(*coros)\n        logger.trace(new_messages)\n        # return the json version of the expanded messages\n        return json.dumps(new_messages)\n     \n    async def _build_final_prompt(self, additional_vars = {}):\n        promptvars = {}\n        promptvars.update(additional_vars)\n        # format the prompt text with the passed-in variables as well as doing internal expansion\n        prompt = await self._gather_format(aformatter.async_format, **filling_machine(promptvars))\n        return prompt\n    async def _submit_for_response_and_prompt(self, **additional_vars):\n        \"\"\" Executes the query as-is and returns a tuple of the final prompt and the response\"\"\"\n        prompter = self\n        # if the user in additional_vars, we're going to instead deepcopy this prompt into a new prompt and add the .user() to it\n        if \"__user\" in additional_vars:\n            new_chatprompt = self.copy()\n            new_chatprompt.user(additional_vars[\"__user\"])\n            prompter = new_chatprompt\n            # remove __user from additional_vars\n            del additional_vars[\"__user\"]\n        prompt = await prompter._build_final_prompt(additional_vars)\n        if self.params is None:\n            return prompt, await cleaned_chat_completion(prompt)\n        else:\n            pparams = prompter.params._get_non_none_params()\n            if self.params.stream:\n                # we're streaming so we need to use the wrapper object\n                listener = ChatStreamListener(prompt, **self.params._get_non_none_params())\n                return prompt, listener\n            else:\n                return prompt, await cleaned_chat_completion(prompt, **pparams)\n\n    @property\n    def response(self) -> str:\n        \"\"\" Returns the value of the last assistant message in the chat prompt \u2b50\"\"\"\n        last_assistant_message = None\n        for _message in self.messages:\n            message = self._msg_dict(_message)\n            if \"assistant\" in message:\n                last_assistant_message = message[\"assistant\"]\n        # filter the response if we have a pattern\n        last_assistant_message = self.filter_by_pattern(last_assistant_message)\n        return last_assistant_message\n\n    def __str__(self):\n        \"\"\" Returns the most recent response from the chat prompt \u2b50\"\"\"\n        if self.response is None:\n            return \"\"\n        else:\n            return self.response\n\n    def __call__(self, usermsg=None, **additional_vars) -> object:\n        \"\"\" Executes the query as-is and returns a Chat object with the response, shortcut for Chat.chat()\"\"\"\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        return self.chat(**additional_vars)\n \n    def ask(self, usermsg=None, **additional_vars) -> str:\n        \"\"\"\n        Executes the internal chat query as-is and returns only the string response.\n        If usermsg is passed in, it will be added as a user message to the chat before executing the query. \u2b50\n        \"\"\"\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        return asyncio.run(self.ask_a(**additional_vars))\n    async def ask_a(self, usermsg=None, **additional_vars) -> str:\n        \"\"\" Executes the query as-is, async version of ask()\"\"\"\n        if self.stream:\n            raise Exception(\"Cannot use ask() with a stream\")\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        _, response = await self._submit_for_response_and_prompt(**additional_vars)\n        # filter the response if we have a pattern\n        response = self.filter_by_pattern(response)\n        return response\n    def listen(self, usermsg=None, **additional_vars) -> ChatStreamListener:\n        \"\"\"\n        Executes the internal chat query as-is and returns a listener object that can be iterated on for the text.\n        If usermsg is passed in, it will be added as a user message to the chat before executing the query. \u2b50\n        \"\"\"\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        _, response = asyncio.run(self._submit_for_response_and_prompt(**additional_vars))\n        if self.params.stream:\n            # response is a ChatStreamListener so lets start it\n            response.start()\n        return response\n    async def listen_a(self, usermsg=None, async_listen=True, **additional_vars) -> ChatStreamListener:\n        \"\"\" Executes the query as-is, async version of listen()\"\"\"\n        if not self.stream:\n            raise Exception(\"Cannot use listen() without a stream\")\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        _, response = await self._submit_for_response_and_prompt(**additional_vars)\n        if self.params.stream:\n            # response is a ChatStreamListener so lets start it\n            await response.start_a()\n        return response\n    def chat(self, usermsg=None, **additional_vars) -> object:\n        \"\"\" \n        Executes the query as-is and returns a new Chat for continuation \n        If usermsg is passed in, it will be added as a user message to the chat before executing the query. \u2b50\n        \"\"\"\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        return asyncio.run(self.chat_a(**additional_vars))\n    async def chat_a(self, usermsg=None, **additional_vars) -> object:\n        \"\"\" Executes the query as-is, and returns a ChatPrompt object that contains the response. Async version of chat()\"\"\"\n        if usermsg is not None:\n            additional_vars[\"__user\"] = usermsg\n        if self.stream:\n            raise Exception(\"Cannot use chat() with a stream\")\n        prompt, response = await self._submit_for_response_and_prompt(**additional_vars)\n        # create a new chatprompt with the new name, copy it from this one\n        new_chatprompt = self.__class__()\n        new_chatprompt.params = self.params\n        logger.trace(\"Expanded prompt: \" + prompt)\n        new_chatprompt.add_messages_json(prompt)\n        # append the recent message\n        new_chatprompt.add_or_update_last_assistant_message(response)\n        return new_chatprompt\n    \n    # clone function to create a new chatprompt with the same name and data\n    def copy(self, name: str = None, system = None, expand_includes: bool = False, expand_fillings: bool = False, **additional_vars) -> object:\n        \"\"\" Returns a new ChatPrompt object that is a copy of this one, optionally with a new name \u2b50\"\"\"\n        import copy\n        if name is not None:\n            new_chat = self.__class__(name=name)\n        else:\n            # if the existing name ends with _{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\" then we need to trim that off and add a new one\n            # use a regex to match at the end of the name\n            import re\n            match = re.search(r\"_(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})-([a-f0-9]{8}-([a-f0-9]{4}-){3}[a-f0-9]{12})\", self.name)\n            if match is not None:\n                # trim off the end\n                name = self.name[:match.start()]\n            else:\n                name = self.name\n            new_chat = self.__class__(name=name + f\"_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\")\n        new_chat.params = copy.copy(self.params)\n        if expand_fillings:\n            if not expand_includes:\n                raise NotImplementedError(\"Cannot expand fillings without expanding includes\")\n            prompt = asyncio.run(self._build_final_prompt(additional_vars))\n            new_chat.add_messages_json(prompt, escape=True)\n        else:\n            new_chat.add_messages_json(self.json if expand_includes else self.json_unexpanded, escape=False)\n        if system is not None:\n            new_chat.system(system)\n        return new_chat", ""]}
{"filename": "chatsnack/chat/mixin_serialization.py", "chunked_list": ["import json\nfrom pathlib import Path\n\nclass DatafileMixin:\n    def save(self, path: str = None):\n        \"\"\" Saves the text to disk \"\"\"\n        # path is a cached property so we're going to delete it so it'll get recalculated\n        del self.datafile.path\n        if path is not None:\n            self.datafile.path = Path(path)\n        self.datafile.save()\n    def load(self, path: str = None):\n        \"\"\" Loads the chat prompt from a file, can load from a new path but it won't work with snack expansion/vending \"\"\"\n        # path is a cached property so we're going to delete it so it'll get recalculated\n        del self.datafile.path\n        if path is not None:\n            self.datafile.path = Path(path)\n        self.datafile.load()", "\n# Define the Data Serialization mixin\nclass ChatSerializationMixin(DatafileMixin):\n    @property\n    def json(self) -> str:\n        \"\"\" Returns the flattened JSON for use in the API\"\"\"\n        return json.dumps(self.get_messages())\n    \n    @property\n    def json_unexpanded(self) -> str:\n        \"\"\" Returns the unflattened JSON for use in the API\"\"\"\n        return json.dumps(self.get_messages(includes_expanded=False))\n\n    @property\n    def yaml(self) -> str:\n        \"\"\" Returns the chat prompt as a yaml string \u2b50\"\"\"\n        return self.datafile.text\n\n    def generate_markdown(self, wrap=80) -> str:\n        \"\"\" Returns the chat prompt as a markdown string \u2b50\"\"\"\n        # TODO convert this to a template file so people can change it\n        # convert the chat conversation to markdown\n        markdown_lines = []\n        def md_quote_text(text, wrap=wrap):\n            import textwrap\n            if text is None:\n                return \">  \"            \n            text = text.strip()\n            # no line in the text should be longer than 80 characters\n            for i, line in enumerate(text.splitlines()):\n               if len(line) > wrap:\n                   text = text.replace(line, textwrap.fill(line, wrap))\n            # we want the text in a blockquote, including empty lines\n            text = textwrap.indent(text, \"> \")\n            # append \"  \" to the end of each line so they show up in markdown\n            # replace empty lines with '> \\n' so they show up in markdown\n            text = text.replace(\"\\n\\n\", \"\\n> \\n\")\n            text = text.replace(\"\\n\", \"  \\n\")\n            return text\n        system_message = self.system_message\n        markdown_lines.append(f\"# Bot Chat Log\")\n        markdown_lines.append(f\"## Bot Information\")\n        markdown_lines.append(f\"**Name**: {self.name}\")\n        markdown_lines.append(f\"**Engine**: {self.engine}\")\n        markdown_lines.append(f\"**Primary Directive**:\")\n        markdown_lines.append(md_quote_text(system_message))\n        markdown_lines.append(f\"## Conversation\")\n        for _message in self.messages:\n            message = self._msg_dict(_message)\n            for role, text in message.items():\n                if role == \"system\":\n                    continue\n                text = md_quote_text(text)\n                emoji = \"\ud83e\udd16\" if role == \"assistant\" else \"\ud83d\udc64\"\n                markdown_lines.append(f\"{emoji} **{role.capitalize()}:**\\n{text}\")\n        markdown_text = \"\\n\\n\".join(markdown_lines)\n        return markdown_text", "\n"]}
{"filename": "chatsnack/chat/__init__.py", "chunked_list": ["import copy\nimport uuid\nfrom dataclasses import field\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\n\nfrom datafiles import datafile\n\nfrom ..defaults import CHATSNACK_BASE_DIR\nfrom .mixin_messages import ChatMessage", "from ..defaults import CHATSNACK_BASE_DIR\nfrom .mixin_messages import ChatMessage\nfrom .mixin_query import ChatQueryMixin\nfrom .mixin_params import ChatParams, ChatParamsMixin\nfrom .mixin_serialization import DatafileMixin, ChatSerializationMixin\n\n\n\n########################################################################################################################\n# Core datafile classes of Plunkychat", "########################################################################################################################\n# Core datafile classes of Plunkychat\n# (1) Chat, high-level class that symbolizes a prompt/request/response, can reference other Chat objects to chain\n# (2) ChatParams, used only in Chat, includes parameters like engine name and other OpenAI params.\n# (3) Text, this is a text blob we save to disk, can be used as a reference inside chat messages ('snack fillings')\n\n@datafile(CHATSNACK_BASE_DIR + \"/{self.name}.txt\", manual=True)\nclass Text(DatafileMixin):\n    name: str\n    content: Optional[str] = None", "    # TODO: All Text and Chat objects should automatically be added as snack fillings (even if not saved to disk)\n\n\n@datafile(CHATSNACK_BASE_DIR + \"/{self.name}.yml\", manual=True, init=False)\nclass Chat(ChatQueryMixin, ChatSerializationMixin):\n    \"\"\" A chat prompt that can be expanded into a chat \u2b50\"\"\"\n    # title should be just like above but with a GUID at the end\n    name: str = field(default_factory=lambda: f\"_ChatPrompt-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\")\n    params: Optional[ChatParams] = None\n    # ChatMessage is more of a hack class to avoid datafiles schema reference issues for dict serialization\n    messages: List[ChatMessage] = field(default_factory=lambda: [])\n\n    def __init__(self, *args, **kwargs):\n        \"\"\" \n        Initializes the chat prompt\n        :param args: if we get one arg, we'll assume it's the system message\n                        if we get two args, the first is the name and the second is the system message\n\n        :param kwargs: (keyword arguments are as follows)\n        :param name: the name of the chat prompt (optional, defaults to _ChatPrompt-<date>-<uuid>)\n        :param params: the engine parameters (optional, defaults to None)\n        :param messages: the messages (optional, defaults to [])\n        :param system: the initial system message (optional, defaults to None)\n        :param engine: the engine name (optional, defaults to None, will overwrite params if specified)\n        \"\"\"\n\n        # get name from kwargs, if it's there\n        if \"name\" in kwargs:\n            self.name = kwargs[\"name\"]\n        else:\n            # if we get two args, the first is the name and the second is the system message\n            if len(args) == 2:\n                self.name = args[0]\n            else:\n                # get the default from the dataclass fields and use that\n                self.name = self.__dataclass_fields__[\"name\"].default_factory()\n        \n        if \"params\" in kwargs:\n            self.params = kwargs[\"params\"]\n        else:\n            # get the default value from the dataclass field, it's optional\n            self.params = self.__dataclass_fields__[\"params\"].default\n        \n        if \"messages\" in kwargs:\n            self.messages = kwargs[\"messages\"]\n        else:\n            # get the default from the dataclass fields and use that\n            self.messages = self.__dataclass_fields__[\"messages\"].default_factory()\n\n        if \"engine\" in kwargs:\n            self.engine = kwargs[\"engine\"]\n            \n        if \"system\" in kwargs:\n            self.system_message = kwargs[\"system\"]\n        else:\n            if len(args) == 1:\n                # if we only get one args, we'll assume it's the system message\n                self.system_message = args[0]\n            elif len(args) == 2:\n                # if we get two args, the first is the name and the second is the system message\n                self.system_message = args[1]\n        \n        self._initial_name = self.name\n        self._initial_params = copy.copy(self.params)\n        self._initial_messages = copy.copy(self.messages)\n        self._initial_system_message = self.system_message\n   \n    def reset(self) -> object:\n        \"\"\" Resets the chat prompt to its initial state, returns itself \"\"\"\n        self.name = self._initial_name\n        self.params = self._initial_params\n        self.messages = self._initial_messages\n        if self._initial_system_message is not None:\n            self.system_message = self._initial_system_message\n        return self", ""]}
{"filename": "chatsnack/chat/mixin_params.py", "chunked_list": ["\nimport re \nfrom typing import Optional, List\n\nfrom datafiles import datafile\n\n\n@datafile\nclass ChatParams:\n    \"\"\"\n    Engine/query parameters for the chat prompt. See OpenAI documentation for most of these. \u2b50\n    \"\"\"\n    engine: str = \"gpt-3.5-turbo\"  #: The engine to use for generating responses, typically 'gpt-3.5-turbo' or 'gpt-4'.\n    temperature: Optional[float] = None  #: Controls randomness in response generation. Higher values (e.g., 1.0) yield more random responses, lower values (e.g., 0) make the output deterministic.\n    top_p: Optional[float] = None  #: Controls the proportion of tokens considered for response generation. A value of 1.0 considers all tokens, lower values (e.g., 0.9) restrict the token set.\n    n: Optional[int] = None  #: Number of responses to generate for each prompt.\n    stream: Optional[bool] = None  #: If True, responses are streamed as they are generated. (not implemented yet)\n    stop: Optional[List[str]] = None  #: List of strings that, if encountered, stop the generation of a response.\n    max_tokens: Optional[int] = None  #: Maximum number of tokens allowed in a generated response.\n    presence_penalty: Optional[float] = None  #: Penalty applied to tokens based on presence in the input.\n    frequency_penalty: Optional[float] = None  #: Penalty applied to tokens based on their frequency in the response.\n\n    # Azure-specific parameters\n    deployment: Optional[str] = None  #: The deployment ID to use for this request (e.g. for Azure)\n    api_type: Optional[str] = None  #: The API type to use for this request (e.g. 'azure' or 'azure_ad')\n    api_base: Optional[str] = None  #: The base URL to use for this request (e.g. for Azure)\n    api_version: Optional[str] = None  #: The API version to use for this request (e.g. for Azure)\n    api_key_env: Optional[str] = None  #: The environment variable name to use for the API key (e.g. for Azure)\n    \n    response_pattern: Optional[str] = None # regex pattern to capture subset of response to return (ignore the rest)\n\n    def _get_non_none_params(self):\n        \"\"\" Returns a dictionary of non-None parameters for the OpenAI API\"\"\"\n        # get a list of dataclass fields\n        fields = [field.name for field in self.__dataclass_fields__.values()]\n        out = {field: getattr(self, field) for field in fields if getattr(self, field) is not None}\n        if \"engine\" not in out or len(out[\"engine\"]) < 2:\n            out[\"engine\"] = \"gpt-3.5-turbo\"\n        # TODO response_pattern maybe should live elsewhere but for now just exclude it for the API\n        if \"response_pattern\" in out:\n            del out[\"response_pattern\"]\n        return out", "class ChatParams:\n    \"\"\"\n    Engine/query parameters for the chat prompt. See OpenAI documentation for most of these. \u2b50\n    \"\"\"\n    engine: str = \"gpt-3.5-turbo\"  #: The engine to use for generating responses, typically 'gpt-3.5-turbo' or 'gpt-4'.\n    temperature: Optional[float] = None  #: Controls randomness in response generation. Higher values (e.g., 1.0) yield more random responses, lower values (e.g., 0) make the output deterministic.\n    top_p: Optional[float] = None  #: Controls the proportion of tokens considered for response generation. A value of 1.0 considers all tokens, lower values (e.g., 0.9) restrict the token set.\n    n: Optional[int] = None  #: Number of responses to generate for each prompt.\n    stream: Optional[bool] = None  #: If True, responses are streamed as they are generated. (not implemented yet)\n    stop: Optional[List[str]] = None  #: List of strings that, if encountered, stop the generation of a response.\n    max_tokens: Optional[int] = None  #: Maximum number of tokens allowed in a generated response.\n    presence_penalty: Optional[float] = None  #: Penalty applied to tokens based on presence in the input.\n    frequency_penalty: Optional[float] = None  #: Penalty applied to tokens based on their frequency in the response.\n\n    # Azure-specific parameters\n    deployment: Optional[str] = None  #: The deployment ID to use for this request (e.g. for Azure)\n    api_type: Optional[str] = None  #: The API type to use for this request (e.g. 'azure' or 'azure_ad')\n    api_base: Optional[str] = None  #: The base URL to use for this request (e.g. for Azure)\n    api_version: Optional[str] = None  #: The API version to use for this request (e.g. for Azure)\n    api_key_env: Optional[str] = None  #: The environment variable name to use for the API key (e.g. for Azure)\n    \n    response_pattern: Optional[str] = None # regex pattern to capture subset of response to return (ignore the rest)\n\n    def _get_non_none_params(self):\n        \"\"\" Returns a dictionary of non-None parameters for the OpenAI API\"\"\"\n        # get a list of dataclass fields\n        fields = [field.name for field in self.__dataclass_fields__.values()]\n        out = {field: getattr(self, field) for field in fields if getattr(self, field) is not None}\n        if \"engine\" not in out or len(out[\"engine\"]) < 2:\n            out[\"engine\"] = \"gpt-3.5-turbo\"\n        # TODO response_pattern maybe should live elsewhere but for now just exclude it for the API\n        if \"response_pattern\" in out:\n            del out[\"response_pattern\"]\n        return out", "\n\n# Define the Chat Configuration mixin\nclass ChatParamsMixin:\n    # make an engine property that allows set/get\n    @property\n    def engine(self):\n        \"\"\"\n        Returns the engine for this chat prompt, typically 'gpt-3.5-turbo'\n         or 'gpt-4'. \u2b50\n        \"\"\"\n        if self.params is None:\n            self.params = ChatParams()\n        return self.params.engine\n    @engine.setter\n    def engine(self, value):\n        if self.params is None:\n            self.params = ChatParams()\n        self.params.engine = value\n    @property\n    def temperature(self):\n        if self.params is None:\n            self.params = ChatParams()\n        return self.params.temperature\n    @temperature.setter\n    def temperature(self, value):\n        if self.params is None:\n            self.params = ChatParams()\n        self.params.temperature = value\n    @property\n    def pattern(self):\n        # if no pattern, return None\n        if self.params is None:\n            return None\n        return self.params.response_pattern\n    @pattern.setter\n    def pattern(self, value):\n        if self.params is None:\n            self.params = ChatParams()\n        self.params.response_pattern = value\n    # same thing for streaming\n    @property\n    def stream(self):\n        if self.params is None:\n            return False # default to False\n        else:\n            return self.params.stream\n    @stream.setter\n    def stream(self, value: bool):\n        if self.params is None:\n            self.params = ChatParams()\n        self.params.stream = value\n    def set_response_filter(self, prefix: Optional[str] = None, suffix: Optional[str] = None, pattern: Optional[str] = None):\n        \"\"\" Filters the response given prefix and suffix or pattern. If suffix is None, it is set to the same as prefix. \n         Note that this overwrites any existing regex pattern. \u2b50 \"\"\"\n        # if pattern is set then fail if they provided prefix or suffix\n        if pattern:\n            if prefix or suffix:\n                raise ValueError(\"Cannot set both pattern and prefix/suffix\")\n            self.pattern = pattern\n            return\n        self.pattern = ChatParamsMixin._generate_pattern_from_separator(prefix, suffix)\n    def _generate_pattern_from_separator(prefix: str, suffix: Optional[str] = None) -> str:\n        # Escape special characters in prefix and suffix\n        prefix = re.escape(prefix)\n        if suffix:\n            suffix = re.escape(suffix)\n        else:\n            suffix = prefix\n        # Generate regex pattern\n        pattern = rf\"{prefix}(.*?)(?:{suffix}|$)\"\n        return pattern\n    def filter_by_pattern(self, text: str) -> Optional[str]:\n        \"\"\" Filters the response given a regex pattern.  \"\"\"\n        if self.pattern is None:\n            return text\n        return ChatParamsMixin._search_pattern(self.pattern, text)\n    def _search_pattern(pattern: str, text: str) -> Optional[str]:\n        matches = re.finditer(pattern, text, re.DOTALL)\n\n        try:\n            first_match = next(matches)\n        except StopIteration:\n            return None\n\n        if len(first_match.groups()) > 0:\n            return first_match.group(1)  # Return the first capturing group\n        else:\n            return first_match.group()  # Return the full matched text", "\n\n"]}
{"filename": "chatsnack/chat/mixin_messages.py", "chunked_list": ["import json\nfrom typing import Dict, List, Optional\n\nfrom datafiles import datafile\n\n@datafile\nclass ChatMessage:\n    system: Optional[str] = None\n    user: Optional[str] = None\n    assistant: Optional[str] = None\n    include: Optional[str] = None\n\n    @property\n    def message(self) -> Dict[str, str]:\n        \"\"\" Returns the message in the form of a dictionary \"\"\"\n        # use the format {'role': 'content'} from among its datafields\n        return {field.name: getattr(self, field.name) for field in self.__dataclass_fields__.values() if getattr(self, field.name) is not None}", "\n\n# Define the Message Management mixin\nclass ChatMessagesMixin:\n    # specific message types, can be chained together\n    def system(self, content: str, chat = False) -> object:\n        \"\"\"\n        Adds or sets the system message in the chat prompt \u2b50\n        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n        chat and returns a new Chat object that includes the message and response\n        \"\"\"\n        self.system_message = content\n        if not chat:\n            return self\n        else:\n            return self.chat()\n    def user(self, content: str, chat = False) -> object:\n        \"\"\"\n        Message added to the chat from the user \u2b50\n        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n        chat and returns a new Chat object that includes the message and response\n        \"\"\"\n        return self.add_message(\"user\", content, chat)\n    def assistant(self, content: str, chat = False) -> object:\n        \"\"\"\n        Message added to the chat from the assistant \u2b50\n        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n        chat and returns a new Chat object that includes the message and response\n        \"\"\"\n        return self.add_message(\"assistant\", content, chat)\n    # easy aliases\n    asst = assistant\n\n    def include(self, chatprompt_name: str = None, chat = False) -> object:\n        \"\"\"\n        Message added to the chat that is a reference to another ChatPrompt where the messages will be inserted in this spot right before formatting \u2b50\n        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n        chat and returns a new Chat object that includes the message and response\n        \"\"\"        \n        return self.add_message(\"include\", chatprompt_name, chat)\n    def add_message(self, role: str, content: str, chat: bool = False) -> object:\n        \"\"\"\n        Add a message to the chat, as role ('user', 'assistant', 'system' or 'include') with the content\n        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n        chat and returns a new Chat object that includes the message and response\n        \"\"\"\n        # fully trim the role and left-trim the content\n        role = role.strip()\n        content = content.lstrip()\n        self.messages.append({role: content})\n        if not chat:\n            return self\n        else:\n            return self.chat()\n    def add_messages_json(self, json_messages: str, escape: bool = True):\n        \"\"\" Adds messages from an OpenAI json string to the chat prompt \"\"\"\n        incoming_messages = json.loads(json_messages)\n        for message in incoming_messages:\n            # convert from the OpenAI format to the format we use\n            if \"role\" in message and \"content\" in message:\n                if escape:\n                    # escape the { and } characters\n                    message[\"content\"] = message[\"content\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n                    message[\"role\"] = message[\"role\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n                self.add_message(message[\"role\"], message[\"content\"])\n            else:\n                raise ValueError(\"Invalid message format, a 'role' or 'content' key was missing\")\n    def add_or_update_last_assistant_message(self, content: str):\n        \"\"\"\n        Adds a final assistant message (or appends to the end of the last assistant message)\n        \"\"\"\n        # get the last message in the list\n        last_message = self.messages[-1]\n        # get the dict version\n        last_message = self._msg_dict(last_message)\n\n        # if it's an assistant message, append to it\n        if \"assistant\" in last_message:\n            last_message[\"assistant\"] += content\n            # replace the last message with the updated one\n            self.messages[-1] = last_message\n        else:\n            # otherwise add a new assistant message\n            self.assistant(content)\n\n    # define a read-only attribute \"last\" that returns the last message in the list\n    @property\n    def last(self) -> str:\n        \"\"\" Returns the value of the last message in the chat prompt (any)\"\"\"\n        # last message is a dictionary, we need the last value in the dictionary\n        if len(self.messages) > 0:\n            last_message = self.messages[-1]\n            return last_message[list(last_message.keys())[-1]]\n        else:\n            return None\n\n    @property\n    def system_message(self) -> str:\n        \"\"\" Returns the first system message, if any \"\"\"\n        # get the first message that has a key of \"system\"\n        for _message in self.messages:\n            message = self._msg_dict(_message)\n            if \"system\" in message:\n                return message[\"system\"]\n        return None\n    \n    @system_message.setter\n    def system_message(self, value: str):\n        \"\"\" Set the system message \"\"\"\n        # loop through the messages and replace the first 'system' messages with this one\n        replaced = False\n        for i in range(len(self.messages)):\n            _message = self.messages[i]\n            message = self._msg_dict(_message)\n            if \"system\" in message:\n                self.messages[i] = {\"system\": value}\n                replaced = True\n                break\n        if not replaced:\n            # system message always goes first\n            self.messages.insert(0, {\"system\": value})\n\n\n    def _msg_dict(self, msg: object) -> dict:\n        \"\"\" Returns a message as a dictionary \"\"\"\n        if msg is None:\n            return None\n        if isinstance(msg, dict):\n            return msg\n        else:\n            return msg.message\n\n    def get_messages(self, includes_expanded=True) -> List[Dict[str, str]]:\n        \"\"\" Returns a list of messages with any included named chat files expanded \"\"\"\n        new_messages = []\n        for _message in self.messages:\n            # if it's a dict then\n            message = self._msg_dict(_message)\n            for role, content in message.items():\n                if role == \"include\" and includes_expanded:\n                    # we need to load the chatprompt and get the messages from it\n                    include_chatprompt = self.objects.get_or_none(content)\n                    if include_chatprompt is None:\n                        raise ValueError(f\"Could not find 'include' prompt with name: {content}\")\n                    # get_expanded_messages from the include_chatprompt and add them to the new_messages, they're already formatted how we want\n                    new_messages.extend(include_chatprompt.get_messages())\n                else:\n                    new_messages.append({\"role\": role, \"content\": content})\n        return new_messages", ""]}
{"filename": "chatsnack/packs/snackpacks.py", "chunked_list": ["import os\nfrom ..chat import Chat\nfrom .module_help_vendor import get_module_inspection_report\n\ndef get_data_path(filename):\n    module_dir = os.path.dirname(os.path.abspath(__file__))\n    data_path = os.path.join(module_dir, filename)\n    return data_path\n\n# Now, use the `get_data_path()` function to access a specific data file", "\n# Now, use the `get_data_path()` function to access a specific data file\ndefault_pack_path = get_data_path(\"default_packs\")\n\n\n# TODO create a way to download snackpacks from github.com/Mattie/chatsnack-snackpacks\n\n# SnackPackVendor class that will be checked for snackpack names and return a Chat() object homed in the right directory\n\n", "\n\n# need a VendingMachine class that looks up snackpacks from the \n\n# ChatPromptProxy class such that whenever you try to call a method on it, it creates a new ChatPrompt and calls the method on that\nclass ChatPromptProxy:\n    def __init__(self, default_system_message: str = None, default_engine: str = None):\n        self.default_system_message = default_system_message\n        self.default_engine = default_engine\n        self._instance = None\n    def _ensure_instance(self):\n        if self._instance is None:\n            self._instance = Chat(system=self.default_system_message)\n            if self.default_engine is not None:\n                self._instance.engine = self.default_engine\n    def __getattr__(self, name):\n        # if the method doesn't exist on this class, we're going to create a new ChatPrompt and call the method on that, but we wanna be careful using __getattr__\n        # because it can cause infinite recursion if we're not careful, so we look up existing names via __dict__ and only create a new ChatPrompt if the name doesn't exist\n        if name in self.__dict__:\n            return self.__dict__[name]\n        self._ensure_instance()\n        return getattr(self._ensure_instance, name)", "\nmodinfo = get_module_inspection_report(\"chatsnack\")\n# replace all { with {{ and all } with }} to escape them for .format()\nmodinfo = modinfo.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\nChatsnackHelper_default_system_message = f\"\"\"\\\nIdentity: ChatsnackHelper, the helpful assistant for the chatsnack Python module. ChatsnackHelper is an expert Pythonista and tries to help users of\nthe chatsnack module with their questions and problems.\n\nchatsnack inspection info for reference:", "\nchatsnack inspection info for reference:\n---------\n{modinfo}\n---------\n\nWhile answering questions, ChatsnackHelper, first summarizes the user's likely intent as a proposal, followed by a helpful and informative final summary answer using the chatsnack module's own documentation where necessary.\n\nCode sample blocks should be surrounded in ``` marks while inline code should have a single ` mark.\n\"\"\"", "Code sample blocks should be surrounded in ``` marks while inline code should have a single ` mark.\n\"\"\"\n_helper = Chat(system=ChatsnackHelper_default_system_message)\n_helper.engine = \"gpt-4\"\ndefault_packs = {   \n                    'Data': None,\n                    'Jane': None,\n                    'Confectioner': None,\n                    'Jolly': None,\n                    'Chester': None,", "                    'Jolly': None,\n                    'Chester': None,\n                    'Summarizer': None,\n                    'ChatsnackHelp': _helper,\n                    'Empty': Chat(),\n                }\n# loop through the default_packs dict and create a ChatPromptProxy for each None one\nfor pack_name, pack in default_packs.items():\n    if pack is None:\n        # create a new class with the pack_name as the class name\n        class_name = pack_name\n        xchat = Chat()\n        filename = os.path.join(default_pack_path, f\"{pack_name}.yml\")\n        xchat.load(filename)\n        default_packs[pack_name] = xchat", "# add packs keys to this module's local namespace for importing\nlocals().update(default_packs)\n\n# vending machine class that looks up snackpacks from the default_packs dict as a named attribute of itself\n# e.g. vending.Jane\nclass VendingMachine:\n    def __getattr__(self, name):\n        if name in default_packs:\n            return default_packs[name].copy()\n        raise AttributeError(f\"SnackPack '{name}' not found\")", "vending = VendingMachine()\n\n"]}
{"filename": "chatsnack/packs/__init__.py", "chunked_list": ["from .snackpacks import *"]}
{"filename": "chatsnack/packs/module_help_vendor.py", "chunked_list": ["import inspect\nimport importlib\nimport sys\n\ndef get_module_inspection_report(module_name, visited=None):\n    if visited is None:\n        visited = set()\n    if module_name in visited:\n        return []\n\n    module = importlib.import_module(module_name)\n    visited.add(module_name)\n    output = []\n\n    output.append(f\"\\nModule: {module.__name__}\")\n    docstring = get_docstring(module)\n    if docstring:\n        output.append(f'\"\"\"\\n{docstring}\"\"\"')\n\n    for name, obj in inspect.getmembers(module):\n        breaker = False\n        for nam in ['_','Path', 'datetime', 'IO', 'datafile']:\n            if name.startswith(nam):\n                breaker = True\n                break\n        if breaker:\n            continue\n        for nam in ['aiwrapper','asynchelpers', 'datetime', 'IO', 'datafile']:\n            if name in nam:\n                breaker = True\n                break\n        if breaker:\n            continue\n\n        if inspect.ismodule(obj):\n            if obj.__name__ not in visited and obj.__name__.startswith(module_name):\n                output.extend([get_module_inspection_report(obj.__name__, visited)])\n        elif not (inspect.isbuiltin(obj) or (hasattr(obj, '__module__') and obj.__module__ in sys.builtin_module_names)):\n            if inspect.isclass(obj):\n                output.extend(_process_class(obj))\n            elif inspect.isfunction(obj):\n                output.extend(_process_function(obj))\n\n    return \"\\n\".join(output)", "\ndef _process_class(cls):\n    if cls.__module__ in sys.builtin_module_names:\n        return []\n\n    output = []\n\n    output.append(f\"Class: {cls.__name__}\")\n    docstring = get_docstring(cls)\n    if docstring:\n        output.append(f'\"\"\"{docstring}\"\"\"')\n\n    methods_output = []\n    for name, method in inspect.getmembers(cls, predicate=inspect.isfunction):\n        if name.startswith('_'):\n            continue\n\n        methods_output.extend(_process_function(method, cls))\n\n    if methods_output:\n        output.append(\"Methods:\")\n        output.extend(methods_output)\n\n    return output", "\ndef _process_function(func, cls=None):\n    output = []\n\n    signature = inspect.signature(func)\n    params = ', '.join(f\"{name}{': ' + param.annotation.__name__ if (param.annotation is not inspect.Parameter.empty and hasattr(param.annotation, '__name__')) else ''}\" for name, param in signature.parameters.items())\n\n\n    func_name = f\"{cls.__name__}.{func.__name__}\" if cls else func.__name__\n\n    output.append(f\"\\n{func_name}({params})\")\n    docstring = get_docstring(func)\n    if docstring:\n        output.append(f'\"\"\"\\n{docstring}\"\"\"')\n\n    return output", "\ndef get_docstring(obj):\n    docstring = inspect.getdoc(obj)\n    if docstring and \"\u2b50\" in docstring:\n        return f\"\u2b50 {docstring.replace('\u2b50', '')}\"\n    return docstring\n"]}
