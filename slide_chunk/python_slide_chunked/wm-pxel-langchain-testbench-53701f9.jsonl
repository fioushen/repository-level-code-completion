{"filename": "setup.py", "chunked_list": ["from setuptools import setup\n\nsetup(\n    name='testbench',\n    version='0.1.0',\n    py_modules=['testbench'],\n    install_requires=[\n        'Click',\n    ],\n    entry_points={", "    ],\n    entry_points={\n        'console_scripts': [\n            'testbench = cli:cli',\n        ],        \n    },\n)"]}
{"filename": "cli.py", "chunked_list": ["\nimport click\nfrom lib.model.chain_revision import ChainRevision, find_ancestor_ids\nfrom lib.model.chain import Chain\nfrom lib.model.chain_spec import LLMSpec\nfrom lib.model.lang_chain_context import LangChainContext\nfrom lib.db import chain_revision_repository, chain_repository, result_repository\nfrom bson import ObjectId\nfrom collections import defaultdict\nfrom typing import Optional, Dict, Any", "from collections import defaultdict\nfrom typing import Optional, Dict, Any\nimport lib.chain_service as chain_service\nimport csv\nimport dotenv\nimport json\nimport re\nimport sys\n\ndotenv.load_dotenv()", "\ndotenv.load_dotenv()\n\n@click.group()\ndef cli():\n  pass\n\n\n#####################\n# Chain revisions", "#####################\n# Chain revisions\n#####################\n\n\n@click.group()\ndef revision():\n  \"\"\"Save and load chain revisions.\"\"\"\n  pass\n", "\ncli.add_command(revision)\n\n\n@click.command()\n@click.argument('chain-name')\n@click.argument(\"file\", default=\"-\", type=click.File(\"rb\"))\ndef save(chain_name, file):\n    \"\"\"Save a new chain revision.\n    The revision is read as json from stdin or from the given file.\n    If chain-name is unknown, a new chain is created. Otherwise,\n    This revision will be saved with the chain's current revision as\n    its parent, and the chain's reference will be updated to this\n    revision.\n    \"\"\"\n    buffer = b\"\"\n    for f in file:\n      buffer += f\n\n    revision_json = buffer.decode('utf-8')\n    revision = ChainRevision.parse_raw(revision_json)\n\n    revision_id = chain_service.save_revision(chain_name, revision)\n\n    print(\"Saved revision\", revision.id, \"for chain\", chain_name)", "\nrevision.add_command(save)\n\n\n@click.command()\n@click.argument('chain-name')\ndef load(chain_name):\n  \"\"\"Load a chain revision from the database.\n  The revision will be loaded and then output as json to stdout.\n  \"\"\"\n  revision = chain_service.load_by_chain_name(chain_name)\n  print(revision.json(indent=2))", "\nrevision.add_command(load)\n\n\n@click.command()\n@click.argument('chain-name')\ndef history(chain_name):\n  \"\"\"Output the ids of all revisions of the chain.\n  The ids will be output to stdout.\n  \"\"\"\n  ancestor_ids = chain_service.history_by_chain_name(chain_name)\n  for id in ancestor_ids:\n    print(id)", "\nrevision.add_command(history)\n\n\n@click.command()\n@click.argument('chain-name')\ndef patch(chain_name):\n  \"\"\"Replace a single chain spec component to create a new chain.\n  The patch must have the same chain_id as the chain spec to be replaced.\n  The new chain will be saved as a new revision and the chain name will be\n  updated to refrence it.\n  \"\"\"\n  buffer = b\"\"\n  for f in sys.stdin:\n    buffer += f.encode('utf-8')\n  \n  patch_dict = json.loads(buffer.decode('utf-8'))\n\n  # junk revision lets us correctly deserialize the patch\n  junk_revision = ChainRevision(**{'chain': patch_dict, 'llms':{}})\n  patch = junk_revision.chain\n\n  revision_id = chain_service.save_patch(chain_name, patch)\n  print(\"Saved revision\", revision_id, \"for chain\", chain_name)", "\nrevision.add_command(patch)\n\n\n@click.command()\n@click.argument('chain-name')\n@click.argument('chain-id')\ndef patch_prompt(chain_name, chain_id):\n  \"\"\"Replace the prompt text for an LLMSpec of the given revision.\n  The prompt text will be provided on stdin.\n  The new chain will be saved as a new revision and the chain name will be\n  updated to refrence it.\n  \"\"\"\n  revision = chain_service.load_by_chain_name(chain_name)\n  patch = revision.chain.find_by_chain_id(int(chain_id)).copy(deep=True)\n  \n  # error if spec is not an LLMSpec\n  if not isinstance(patch, LLMSpec):\n    print(f\"Spec {chain_id} is not an LLMSpec\")\n    sys.exit(1)\n\n  buffer = b\"\"\n  for f in sys.stdin:\n    buffer += f.encode('utf-8')\n  \n  patch.prompt = buffer.decode('utf-8')\n\n  revision_id = chain_service.save_patch(chain_name, patch)\n  print(\"Saved revision\", revision_id, \"for chain\", chain_name)", "\nrevision.add_command(patch_prompt)\n\n\n#####################\n# Chain names\n#####################\n\n\n@click.group()\ndef chain():\n  \"\"\"Branch and reset chain names.\"\"\"\n  pass", "\n@click.group()\ndef chain():\n  \"\"\"Branch and reset chain names.\"\"\"\n  pass\n\ncli.add_command(chain)\n\n\n@click.command()", "\n@click.command()\n@click.argument('chain-name')\n@click.argument(\"branch-name\")\ndef branch(chain_name, branch_name):\n  \"\"\"Branch a chain revision.\n  This will create a new chain that points to the same revision\n  as the provided chain. The new chain may be then revised independently.\n  \"\"\"\n  new_chain_id = chain_service.branch(chain_name, branch_name)\n  print(f\"Created branch {branch_name} at {new_chain_id}\")", "\nchain.add_command(branch)\n\n\n@click.command()\n@click.argument('chain-name')\n@click.argument(\"revision\")\ndef reset(chain_name, revision):\n  \"\"\"Reset a chain to a another revision.\n  This will update the chain to point to the given revision.\n  \"\"\"\n  chain_service.reset(chain_name, revision)\n  print(f\"Reset {chain_name} to revision {revision}\")", "\nchain.add_command(reset)\n\n\n@click.command()\ndef list():\n  \"\"\"List all chains.\"\"\"\n  for chain in chain_service.list_chains().items():\n    print(f\"{chain[0]}: {chain[1]}\")\n", "\nchain.add_command(list)\n\n\n#####################\n# Running\n#####################\n\n\n@click.group()\ndef run():\n  \"\"\"Run chains once or interactively.\"\"\"\n  pass", "\n@click.group()\ndef run():\n  \"\"\"Run chains once or interactively.\"\"\"\n  pass\n\ncli.add_command(run)\n\n\ndef save_results(ctx: LangChainContext, revision_id: str):\n  results = ctx.results(revision_id)\n  for result in results:\n    result_repository.save(result)\n\n  ctx.reset_results()", "\ndef save_results(ctx: LangChainContext, revision_id: str):\n  results = ctx.results(revision_id)\n  for result in results:\n    result_repository.save(result)\n\n  ctx.reset_results()\n\n\n@click.command()", "\n@click.command()\n@click.argument('chain-name')\n@click.argument(\"input\", default=\"-\", type=click.File(\"rb\"))\n@click.option(\"--record\", is_flag=True, help = \"Record the inputs and outputs of LLM chains to the database.\")\ndef once(chain_name, input, record):\n  \"\"\"Run a chain revision.\n  The revision is read from the database and fed input from stdin or the given file.\n  The results are ouput as json to stdout.\n  \"\"\"\n  buffer = b\"\"\n  for f in input:\n    buffer += f\n\n  input_json = buffer.decode('utf-8')\n  input = json.loads(input_json)\n\n  chain_service.initialize_services()\n\n  output = chain_service.run_once(chain_name, input, record)\n\n  print(json.dumps(output, indent=2))", "\nrun.add_command(once)\n\n\n@click.command()\n@click.argument('chain-name')\n@click.option(\"--record\", is_flag=True, help = \"Record the inputs and outputs of LLM chains to the database.\")\ndef interactive(chain_name, record):\n  \"\"\"Interact with a chain from the database on the command line.  \n  The chain must either have an input key called 'input' or have exactly one \n  input key that is not also an output key. The user's input will be fed to this key.\n\n  The chain must either have an output key called 'output' or have exactly one\n  output key that is not also an input key. The output of this key will be\n  displayed to the user for each iteration.\n\n  Outputs can be fed as inputs to subsequent iterations by adding '_in' to the\n  input name and '_out' to the output name. \n  \n  For example, if the inputs are 'subject' and 'memory_in', and the outputs are\n  'output' and 'memory_out', then the user will be prompted for 'subject',\n  the output of 'output' will be displayed, and the output of 'memory_out' will\n  be fed to 'memory_in' in the next iteration.\n\n  All other inputs will get the empty string as input, and all other outputs\n  will be ignored.\n  \"\"\"\n  revision = chain_service.load_by_chain_name(chain_name)\n \n  ctx = LangChainContext(llms=revision.llms, recording=True)\n  lang_chain = revision.chain.to_lang_chain(ctx)\n\n  input_keys = set(lang_chain.input_keys)\n  output_keys = set(lang_chain.output_keys)\n  output_mapping = {re.sub(r\"_out$\", \"_in\", key): key\n    for key in output_keys\n    if re.search(r\"_out$\", key) and re.sub(r\"_out$\", \"_in\", key) in input_keys}\n\n  if \"input\" in input_keys:\n    user_input_key = \"input\"\n  elif len(input_keys - output_keys) == 1:\n    keys = input_keys - output_keys\n    user_input_key = next(iter(keys))\n  else:\n    print(\"error inp key\", input_keys)\n    raise Exception(\"Chain must have exactly one input key that is not also an output key or an input key called 'input'\")\n  \n  if \"output\" in output_keys:\n    user_output_key = \"output\"\n  elif len(output_keys - input_keys) == 1:\n    user_output_key = list(output_keys - input_keys)[0]\n  else:\n    raise Exception(\"Chain must have exactly one output key that is not also an input key or an output key called 'output'\")\n  \n  chain_service.initialize_services()\n\n  inputs = {key: \"\" for key in input_keys if key != user_input_key}\n  while True:\n    inputs[user_input_key] = input(f\"{user_input_key}> \")\n    outputs = chain_service.run_once(chain_name, inputs, record)\n\n    print(outputs[user_output_key])\n    print()\n\n    inputs = {key: outputs[output_mapping[key]] if key in output_mapping else \"\" for key in input_keys}", "\n\nrun.add_command(interactive)\n\n\n#####################\n# Results\n#####################\n\n", "\n\n@click.group()\ndef results():\n  \"\"\"Show results.\"\"\"\n  pass\n\ncli.add_command(results)\n\n\ndef dict_to_csv_column(d: dict):\n  return \"\\n\".join([f\"{key}: {value}\" for key, value in d.items()])", "\n\ndef dict_to_csv_column(d: dict):\n  return \"\\n\".join([f\"{key}: {value}\" for key, value in d.items()])\n\n@click.command()\n@click.option(\"--chain-name\", default=None, help=\"Find results for the current revision of named chain.\")\n@click.option(\"--revision\", default=None, help=\"Find results for the given revision id.\")\n@click.option(\"--ancestors\", is_flag=True, help=\"Include results for ancestors of specified revision.\")\n@click.option(\"--csv-format\", is_flag=True, help=\"Return results as csv instead of json.\")", "@click.option(\"--ancestors\", is_flag=True, help=\"Include results for ancestors of specified revision.\")\n@click.option(\"--csv-format\", is_flag=True, help=\"Return results as csv instead of json.\")\n@click.argument(\"chain-id\")\ndef show(chain_name: str, revision: str, ancestors: bool, csv_format: bool, chain_id: str):\n  \"\"\"Find results for a prompt in for one or more chain revisions.\n  One of chain-name or revision must be specified.\n  If the ancestors flag is set, results for all ancestors of the specified revision are included.\n  Results are output as json or csv (depending on the --csv-format flag) to stdout.\n  \"\"\"\n  if chain_name is not None:\n    revision = chain_service.load_by_chain_name(chain_name)\n  elif revision is not None:\n    revision = chain_service.load_by_id(revision)\n  else:\n    raise Exception(\"Must specify chain name, revision id, or chain id\")  \n\n  results = chain_service.results(revision.id, ancestors, chain_id)\n  if csv_format:\n    csv_out = csv.writer(sys.stdout)\n    csv_out.writerow([\"chain_id\", \"revision\", \"input\", \"output\"])\n    for result in results:\n      csv_out.writerow([\n        result.chain_id, \n        result.revision, \n        dict_to_csv_column(result.input), \n        result.output,\n      ])\n  else:\n    print('[')\n    for result in results:\n      print(json.dumps(result.dict(), indent=2, default=lambda o: str(o)), end=',\\n')\n    print(']')", "\nresults.add_command(show)\n\ndef add_result(results: Dict, key: str, value: Any):\n  if key not in results:\n    results[key] = []\n  results[key].append(value)\n\n@click.command()\n@click.argument('chain-name1')", "@click.command()\n@click.argument('chain-name1')\n@click.argument('chain-name2')\n@click.option(\"-c\", \"--chain-id\", help=\"limit diff to a specific chain id\", required=False, type=int)\ndef diff(chain_name1, chain_name2, chain_id: Optional[int] = None):\n  revision1 = chain_service.load_by_chain_name(chain_name1)\n  revision2 = chain_service.load_by_chain_name(chain_name2)\n  \n  query = {'chain_id': chain_id} if chain_id is not None else {}\n\n  grouped_results = {}\n\n  results1 = chain_service.results(revision1.id, False, chain_id)\n  for result in results1:\n    add_result(grouped_results, json.dumps(result.input), result)\n\n  results2 = chain_service.results(revision2.id, False, chain_id)\n  for result in results2:\n    add_result(grouped_results, json.dumps(result.input), result)\n\n  csv_out = csv.writer(sys.stdout)\n  csv_out.writerow([\"input\", \"revision 1\", \"revision 2\"])\n  for input, results in grouped_results.items():\n    formatted_input = dict_to_csv_column(json.loads(input))\n    if len(results) == 1:\n      if results[0].revision == revision1.id:\n        csv_out.writerow([formatted_input, results[0].output, \"\"])\n      else:\n        csv_out.writerow([formatted_input, \"\", results[0].output])\n    else:\n      csv_out.writerow([formatted_input, results[0].output, results[1].output])", "\nresults.add_command(diff)"]}
{"filename": "scripts/pickle_to_json.py", "chunked_list": ["import pandas as pd\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-k\", \"--api_key\", type=str, help=\"API Key for Pinecone\")\nparser.add_argument(\"-emb\", \"--embedding_file\", help=\"File location for embeddings (must be .pkl file)\")\n\nargs = vars(parser.parse_args())\n\n## Parse pickle file to JSON for further processing\ndef main(args):\n    embedding_filepath = args[\"embedding_file\"]\n\n    df_emb = pd.read_pickle(embedding_filepath)\n\n    print(df_emb.to_json())", "\n## Parse pickle file to JSON for further processing\ndef main(args):\n    embedding_filepath = args[\"embedding_file\"]\n\n    df_emb = pd.read_pickle(embedding_filepath)\n\n    print(df_emb.to_json())\n\nmain(args)", "\nmain(args)"]}
{"filename": "scripts/generate_openai_embeddings.py", "chunked_list": ["import uuid\nimport dotenv\nimport sys\nimport json\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\ndotenv.load_dotenv()\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-m\", \"--model\", default=\"text-embedding-ada-002\", help=\"OpenAI Model for embedding\")", "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-m\", \"--model\", default=\"text-embedding-ada-002\", help=\"OpenAI Model for embedding\")\n\nargs = vars(parser.parse_args())\n\ndef createUUID():\n    return str(uuid.uuid4())\n\n## Main Function ##\ndef main(args):\n    from langchain.embeddings import OpenAIEmbeddings\n\n    documents = json.load(sys.stdin)\n    embeddings_model = OpenAIEmbeddings(model=args['model'])\n    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\n    docs = [{\n        'text': doc['text'],\n        'meta': doc['meta'],\n        'embedding': embedding,\n    } for doc, embedding in zip(documents, embeddings)]\n    json.dump(docs, sys.stdout, indent=2)", "## Main Function ##\ndef main(args):\n    from langchain.embeddings import OpenAIEmbeddings\n\n    documents = json.load(sys.stdin)\n    embeddings_model = OpenAIEmbeddings(model=args['model'])\n    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\n    docs = [{\n        'text': doc['text'],\n        'meta': doc['meta'],\n        'embedding': embedding,\n    } for doc, embedding in zip(documents, embeddings)]\n    json.dump(docs, sys.stdout, indent=2)", "\nmain(args)"]}
{"filename": "scripts/run_vector_search_chain.py", "chunked_list": ["from lib.chains.vector_search_chain import VectorSearchChain\n\nchain = VectorSearchChain(\n    query=\"{input}\",\n    embedding_engine='huggingface',\n    embedding_params={},\n    database = 'pinecone',\n    database_params = {\n      'index': 'test-mpnet-v2',\n      'namespace': 'test1',", "      'index': 'test-mpnet-v2',\n      'namespace': 'test1',\n    },\n    num_results=10,\n    min_score=0.0,\n    input_variables=['input'],\n    output_key='results',\n)\n\nchain._call({'input': 'How do I open a can of paint?'})", "\nchain._call({'input': 'How do I open a can of paint?'})\nchain._call({'input': 'What is the capitol of the US?'})\nchain._call({'input': 'Which country is Kingston in?'})\nchain._call({'input': 'Which country is Kingston the capitol of?'})\nchain._call({'input': 'Kingston is the capitol of Jamaica.'})\nchain._call({'input': \"Which city is Jamaica's capitol?\"})\nchain._call({'input': \"Which cities are capitols of Caribbean countries?\"})\nchain._call({'input': \"Which cities are capitols of European countries?\"})\nchain._call({'input': \"Which cities are capitols of Asian countries?\"})", "chain._call({'input': \"Which cities are capitols of European countries?\"})\nchain._call({'input': \"Which cities are capitols of Asian countries?\"})\nchain._call({'input': \"Which cities are capitols of South American countries?\"})\nchain._call({'input': \"Which cities are capitols of North American countries?\"})\nchain._call({'input': \"Which cities are capitols of North African countries?\"})\nchain._call({'input': \"blah blah blah\"})"]}
{"filename": "scripts/generate_sentencetransformers_embeddings.py", "chunked_list": ["import uuid\nimport dotenv\nimport sys\nimport json\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\ndotenv.load_dotenv()\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-m\", \"--model\", default=\"all-mpnet-base-v2\", help=\"Huggingface Hub model for embedding\")", "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-m\", \"--model\", default=\"all-mpnet-base-v2\", help=\"Huggingface Hub model for embedding\")\n\nargs = vars(parser.parse_args())\n\ndef createUUID():\n    return str(uuid.uuid4())\n\n## Main Function ##\ndef main(args):\n    from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n\n    documents = json.load(sys.stdin)\n    embeddings_model = HuggingFaceEmbeddings(model_name=args['model'])\n    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\n    docs = [{**doc,'embedding': embedding} for doc, embedding in zip(documents, embeddings)]\n    json.dump(docs, sys.stdout, indent=2)", "## Main Function ##\ndef main(args):\n    from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n\n    documents = json.load(sys.stdin)\n    embeddings_model = HuggingFaceEmbeddings(model_name=args['model'])\n    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\n    docs = [{**doc,'embedding': embedding} for doc, embedding in zip(documents, embeddings)]\n    json.dump(docs, sys.stdout, indent=2)", "\nmain(args)"]}
{"filename": "scripts/coverage.py", "chunked_list": ["import sys\nimport requests\nimport json\nimport re\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain, SequentialChain, TransformChain\nfrom langchain.chains.base import Chain\nfrom langchain.input import get_colored_text\n", "from langchain.input import get_colored_text\n\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Union\nfrom pydantic import root_validator\n\nllm_decider = OpenAI(temperature=0.0)\nllm_creative = OpenAI(temperature=0.7)\nproduct_prompt = \"What is a good name for a company that makes {product}?\"\n\n\nclass LazyPrompt(PromptTemplate):\n  def format(self, **kwargs: Any) -> str:\n    prompt_template = kwargs[self.template]\n    template_kwargs = dict(kwargs)\n    template_kwargs.pop(self.template)\n    return prompt_template.format(**template_kwargs)\n  \n  @root_validator()\n  def template_is_valid(cls, values: Dict) -> Dict:\n    return values", "\n\nclass LazyPrompt(PromptTemplate):\n  def format(self, **kwargs: Any) -> str:\n    prompt_template = kwargs[self.template]\n    template_kwargs = dict(kwargs)\n    template_kwargs.pop(self.template)\n    return prompt_template.format(**template_kwargs)\n  \n  @root_validator()\n  def template_is_valid(cls, values: Dict) -> Dict:\n    return values", "\n\nclass CategorizationConditional(Chain):\n  categorization_input: str\n  subchains: Dict[str, Chain]\n  default_chain: Chain\n  output_variables: List[str] = [\"text\"]\n\n  # TODO: validator requires the union of subchain inputs\n  # TODO: validator requires all subchains have all output keys that are not also input keys\n\n  @property\n  def input_keys(self) -> List[str]:    \n    return self.default_chain.input_keys + [key for subchain in self.subchains.values() for key in subchain.input_keys]\n\n  @property\n  def output_keys(self) -> List[str]:\n    return self.output_variables\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    categorization = inputs[self.categorization_input].strip()\n\n    _colored_text = get_colored_text(categorization, \"yellow\")\n    _text = \"Categorization input:\\n\" + _colored_text\n    self.callback_manager.on_text(_text, end=\"\\n\", verbose=self.verbose)\n\n    subchain = self.subchains[categorization] if categorization in self.subchains else self.default_chain\n \n    known_values = inputs.copy()\n    outputs = subchain(known_values, return_only_outputs=True)\n    known_values.update(outputs)\n    return {k: known_values[k] for k in self.output_variables}    ", "\n\nclass ESearchChain(Chain):\n  output_variable: str = \"text\"\n  input_variable: str = \"text\"\n\n  @property\n  def input_keys(self) -> List[str]:    \n    return [self.input_variable]\n\n  @property\n  def output_keys(self) -> List[str]:\n    return [self.output_variable]\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    search = inputs[self.input_variable]\n    search = 'x-ray'\n    url = f\"http://localhost:9200/coverages/_search?q={requests.utils.quote(search)}\"\n    response = requests.get(url)\n    decoded_response = json.loads(response.content)\n\n    return {self.output_variable: decoded_response['hits']['hits']}", "\n\nformat_hit = lambda i, h: f\"{i+1}: ${h['rate']} for {h['name']} {h['description']} (CPT code: {h['billing_code']}).\"\n\n\nsearch_format_chain = TransformChain(\n  input_variables=['hits'],\n  output_variables=['search_results'],\n  transform = lambda inputs: {'search_results': '\\n'.join([format_hit(i, h['_source']) for (i, h) in enumerate(inputs['hits'])])}\n)", "  transform = lambda inputs: {'search_results': '\\n'.join([format_hit(i, h['_source']) for (i, h) in enumerate(inputs['hits'])])}\n)\n\ncoverage_question_chain = SequentialChain(\n  input_variables=[\"disambiguation_prompt\", \"response_prompt\", \"response_prompt\", \"identity\", \"context\", \"response_prefix\", \"user_input\"],\n  chains=[\n    LLMChain(llm=llm_decider, prompt=LazyPrompt(input_variables=[\"disambiguation_prompt\", \"identity\", \"context\", \"user_input\"], template=\"disambiguation_prompt\"), output_key=\"search\", verbose=True),\n    ESearchChain(input_variable=\"search\", output_variable=\"hits\"),\n    search_format_chain,\n    LLMChain(llm=llm_creative, prompt=LazyPrompt(input_variables=[\"search\", \"response_prompt\", \"search_results\", \"context\", \"identity\", \"response_prefix\"], template=\"response_prompt\"), output_key=\"response\", verbose=True),", "    search_format_chain,\n    LLMChain(llm=llm_creative, prompt=LazyPrompt(input_variables=[\"search\", \"response_prompt\", \"search_results\", \"context\", \"identity\", \"response_prefix\"], template=\"response_prompt\"), output_key=\"response\", verbose=True),\n    TransformChain(\n      input_variables=[\"response_prefix\", \"response\"], output_variables=[\"text\"], \n      transform = lambda inputs: {'text': inputs['response_prefix'] + inputs['response']}\n    )\n  ]\n)\n\nchain = SequentialChain(", "\nchain = SequentialChain(\n  input_variables=[\"categorization_prompt\", \"disambiguation_prompt\", \"response_prompt\", \"default_prompt\", \"response_prefix\", \"identity\", \"context\", \"user_input\"],\n  chains=[\n    LLMChain(llm=llm_decider, prompt=LazyPrompt(input_variables=[\"categorization_prompt\", \"context\", \"user_input\"], template=\"categorization_prompt\"), output_key=\"categorization\", verbose=True),\n    CategorizationConditional(\n      input_variables=[\"disambiguation_prompt\", \"response_prompt\", \"default_prompt\", \"response_prefix\", \"identity\", \"context\", \"user_input\"],\n      categorization_input=\"categorization\",\n      verbose=True,\n      subchains={", "      verbose=True,\n      subchains={\n        \"Questions about whether insurance will cover a medical procedure or service\": coverage_question_chain,\n        \"Questions about how much insurance will cover for a medical procedure or service\": coverage_question_chain,\n      },\n      default_chain=LLMChain(llm=llm_creative, prompt=LazyPrompt(input_variables=[\"default_prompt\", \"identity\", \"context\", \"user_input\"], template=\"default_prompt\"), verbose=True),\n    ),\n  ]\n)\n", ")\n\nidentity_txt = '''You are a very friendly, positive and helpful representative of a health insurance company that is a good listener. \nThe customer is enrolled in the company's insurance plan.'''\n\ncategorization_template = '''Context: ${context}\\n The following is a list of categories\nthat insurance customer questions fall into:\n\nQuestions about whether insurance will cover a medical procedure or service, Questions about how much insurance will cover for a medical procedure or service, Other statements or questions.\n", "Questions about whether insurance will cover a medical procedure or service, Questions about how much insurance will cover for a medical procedure or service, Other statements or questions.\n\n{user_input}\n\nCategory:'''\n\ndisambiguation_template = '''{identity}\\n{context}\\nThe customer asks:\\n\\n{user_input}\\n\nWhat medical service, procedure or device is the customer asking about? Be concise.\\n\\nThe customer is asking about'''\n\nresponse_template = '''Context: {context} Customer has insurance from a company that covers medical services at the following rates:\\n", "\nresponse_template = '''Context: {context} Customer has insurance from a company that covers medical services at the following rates:\\n\n{search_results}\n{identity} You will only answer about the services listed above.\\n\nRespond with normal capitalization. Use full words rather than abbrieviations. Do not provide CPT or medical codes in your responses. \nDo not respond with different coverage rates. \nAsk the customer to be more specific.\n\\n##\\nCustomer: Tell me how much money my insurance covers for {search} and describe that service.\\nAgent: {response_prefix}'''\n\ndefault_template = '''{identity}\\n{context}\\n", "\ndefault_template = '''{identity}\\n{context}\\n\nYou will only answer questions about health insurance.\n\\n##\\nCustomer: ${user_input}\\nAgent:'''\n\ndef prompt_from_template(template) -> PromptTemplate:\n    keys = re.findall(r'{([^{}]*)}', template)\n    return PromptTemplate(\n      input_variables=keys,\n      template = template\n    )", "\nsystem_inputs = {\n  \"identity\": identity_txt,\n  \"context\": \"\",\n  \"response_prefix\": \"Your insurance will\",\n  \"categorization_prompt\": prompt_from_template(categorization_template),\n  \"disambiguation_prompt\": prompt_from_template(disambiguation_template),\n  \"response_prompt\": prompt_from_template(response_template),\n  \"default_prompt\": prompt_from_template(default_template),\n}", "  \"default_prompt\": prompt_from_template(default_template),\n}\n\nuser_inputs = {\n  \"user_input\": sys.argv[1],\n}\n\n# Run the chain only specifying the input variable.\nprint(chain.run(dict(**system_inputs, **user_inputs)))", "print(chain.run(dict(**system_inputs, **user_inputs)))"]}
{"filename": "scripts/print_chain_spec.py", "chunked_list": ["from langchain.llms import OpenAI\nfrom lib.model.chain_revision import ChainRevision\nfrom lib.model.chain_spec import LLMSpec, SequentialSpec\nimport dotenv\n\ndotenv.load_dotenv()\n\n\nllm = OpenAI(temperature=0.8)\n", "llm = OpenAI(temperature=0.8)\n\nchain = SequentialSpec(\n  chain_id = 0,\n  input_keys = [\"input\", \"memory_in\"],\n  output_keys = [\"output\", \"memory_out\"],\n  chain_type = \"sequential_spec\",\n  chains = [\n    LLMSpec(\n      chain_id = 1,", "    LLMSpec(\n      chain_id = 1,\n      input_keys = [\"input\", \"memory_in\"],\n      output_key = \"output\",\n      prompt = \"Context: {memory_in}\\n\\nYou are a witty but kind professor. Respond in a single paragraph to the student's question or statement.\\n\\nStudent: {input}\\n\\nResponse:\",\n      llm_key = \"llm\",\n      chain_type = \"llm_spec\",\n    ),\n    LLMSpec(\n      chain_id = 2,", "    LLMSpec(\n      chain_id = 2,\n      input_keys = [\"input\", \"output\", \"memory_in\"],\n      output_key = \"memory_out\",\n      prompt = \"You are a witty but kind professor. Summarize in a single paragraph the conversation up to this point including the context.\\n\\nContext: {memory_in}\\n\\nStudent: {input}\\n\\nProfessor: {output}\\n\\nSummary:\",\n      llm_key = \"llm\",\n      chain_type = \"llm_spec\",\n    ),\n  ]\n)", "  ]\n)\n\nchain_revision = ChainRevision(\n  llms = {\"llm\": llm},\n  chain = chain\n)\n\nprint(chain_revision.json())", "print(chain_revision.json())"]}
{"filename": "scripts/send_to_pinecone.py", "chunked_list": ["import pinecone\nimport uuid\nimport json\nimport os\nimport sys\nimport dotenv\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\ndotenv.load_dotenv()\n", "dotenv.load_dotenv()\n\nBATCH_SIZE = 200\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"-i\", \"--index\", help=\"Name of Pinecone index to insert into\")\nparser.add_argument(\"-n\", \"--namespace\", help=\"Namespace for Pinecone index\")\n\nargs = vars(parser.parse_args())\n\ndef createUUID():\n    return str(uuid.uuid4())", "args = vars(parser.parse_args())\n\ndef createUUID():\n    return str(uuid.uuid4())\n\ndef main(args):\n    pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENVIRONMENT'])\n\n    embeddings = json.load(sys.stdin)\n    pinecone_embeddings = [(createUUID(), doc['embedding'], {**doc['meta'], 'text': doc['text']}) for doc in embeddings]\n  \n    index = pinecone.Index(args['index'])\n    batch = pinecone_embeddings[:BATCH_SIZE]\n    pinecone_embeddings = pinecone_embeddings[BATCH_SIZE:]\n    while len(batch) > 0:\n      index.upsert(vectors=batch, namespace=args['namespace'])\n      batch = pinecone_embeddings[:BATCH_SIZE]\n      pinecone_embeddings = pinecone_embeddings[BATCH_SIZE:]\n\n    print(json.dumps(index.describe_index_stats().to_dict(), indent=2))", "\nmain(args)"]}
{"filename": "scripts/test.py", "chunked_list": ["from pprint import pprint\nfrom lib.model.chain_revision import ChainRevision\nfrom lib.db import chain_repository, chain_revision_repository\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom lib.model.chain_spec import LLMSpec, SequentialSpec\nfrom lib.model.chain import Chain\n\ndef pretty_print(clas, indent=0):\n  print(' ' * indent +  type(clas).__name__ +  ':')\n  indent += 4\n  for k,v in clas.__dict__.items():\n    if '__dict__' in dir(v):\n      pretty_print(v,indent)\n    else:\n      print(' ' * indent +  k + ': ' + str(v))", "def pretty_print(clas, indent=0):\n  print(' ' * indent +  type(clas).__name__ +  ':')\n  indent += 4\n  for k,v in clas.__dict__.items():\n    if '__dict__' in dir(v):\n      pretty_print(v,indent)\n    else:\n      print(' ' * indent +  k + ': ' + str(v))\n\ncategorization_template = '''Context: ${context}\\n The following is a list of categories", "\ncategorization_template = '''Context: ${context}\\n The following is a list of categories\nthat insurance customer questions fall into:\n\nQuestions about whether insurance will cover a medical procedure or service, Questions about how much insurance will cover for a medical procedure or service, Other statements or questions.\n\n{user_input}\n\nCategory:'''\n", "Category:'''\n\ntemplate2 = \"Sumarize this response:\\n\\n{category}\\n\\nSummary:\"\n\n# llm = OpenAI(temperature=0.7)\n\nchain1 = LLMSpec(prompt=categorization_template, chain_id=0, llm_key=\"llm\", input_keys=[], output_key=\"out\", chain_type='llm_spec')\nchain2 = LLMSpec(prompt=template2, chain_id=0, llm_key=\"llm\", input_keys=[], output_key=\"out\", chain_type='llm_spec')\nchain = SequentialSpec(\n  chains=[chain1, chain2],", "chain = SequentialSpec(\n  chains=[chain1, chain2],\n  chain_id=1,\n  input_keys=[],\n  output_keys=[\"out\"],\n  chain_type='sequential_spec'\n)\n\nrevision = ChainRevision(name=\"main\", major=0, minor=0, chain=chain, llms={\"llm\": OpenAI(temperature=0.7)})\nchain_revision_repository.save(revision)", "revision = ChainRevision(name=\"main\", major=0, minor=0, chain=chain, llms={\"llm\": OpenAI(temperature=0.7)})\nchain_revision_repository.save(revision)\n\ntest_chain = Chain(name=\"test\", revision=revision.id)\nchain_repository.save(test_chain)\n\n\nrecords = chain_revision_repository.find_by({})\nrevisions = [x for x in records]\n", "revisions = [x for x in records]\n\npretty_print(revisions[0], indent=2)\n\nfor revision in revisions:\n  print(chain_revision_repository.delete(revision))\n\nrecords = chain_repository.find_by({})\nchains = [x for x in records]\nprint(chains[0])", "chains = [x for x in records]\nprint(chains[0])\n\nfor c in chains:\n  print(chain_repository.delete(c))"]}
{"filename": "scripts/csv_to_json.py", "chunked_list": ["import csv\nimport sys\nimport json\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n\nargs = vars(parser.parse_args())\n\n# Parse CSV to JSON", "\n# Parse CSV to JSON\n# Input: CSV file with header containing 'sentences' and 'section_time_stamp'\n# Output: JSON file with sentences in 'text' field and timestamp in 'meta.section' field.\ndef main(args):\n  data = csv.DictReader(sys.stdin)\n  records = [{\n    'text': row['sentences'],\n    'meta': {\n      'section': row['section_time_stamp'],\n    }\n  } for row in data]\n  json.dump(records, sys.stdout)", "\nmain(args)"]}
{"filename": "lib/db_migration.py", "chunked_list": ["import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\n\nchain_spec_conversion = {\n      \"llm_spec\": \"llm_chain_spec\",\n      \"sequential_spec\": \"sequential_chain_spec\",\n      \"case_spec\": \"case_chain_spec\",\n      \"reformat_spec\": \"reformat_chain_spec\",\n      \"transform_spec\": \"transform_chain_spec\",", "      \"reformat_spec\": \"reformat_chain_spec\",\n      \"transform_spec\": \"transform_chain_spec\",\n      \"api_spec\": \"api_chain_spec\",\n      \"vector_search_spec\": \"vector_search_chain_spec\"\n}\n\n\ndef updateChainType(chain, prefix, update_fields):\n  # make sure chain_type is properly formatted\n  try:\n    chain_type = chain.get(\"chain_type\")\n\n    if chain_type and chain_type in chain_spec_conversion:\n      update_fields[f\"{prefix}.chain_type\"] = chain_spec_conversion[chain_type]\n\n  except Exception as e:\n    print(e, chain, flush=True)\n\n  # recursively handle sequential chains\n  if \"chains\" in chain:\n    if type(chain[\"chains\"]) is list:\n      for index, child_chain in enumerate(chain[\"chains\"]):\n        updateChainType(child_chain, f\"{prefix}.chains.{index}\", update_fields)\n    else:\n      for key, child_chain in chain[\"chains\"].items():\n        updateChainType(child_chain, f\"{prefix}.chains.{key}\", update_fields)\n\n  # recursively handle case chains\n  if \"cases\" in chain:\n    for key, case in chain[\"cases\"].items():\n      updateChainType(case, f\"{prefix}.cases.{key}\", update_fields)\n    updateChainType(chain[\"default_case\"], f\"{prefix}.default_case\", update_fields)", "\n\nload_dotenv()\n\nclient = MongoClient(host=[\"localhost:27017\"],\n                     username=\"mongoadmin\",\n                     password=os.environ[\"MONGO_ROOT_PASSWORD\"])\ndatabase = client[os.environ[\"MONGODB_DATABASE\"]]\n\nchain_revisions = database[\"chain_revisions\"]", "\nchain_revisions = database[\"chain_revisions\"]\n\nfor revision in chain_revisions.find({}):\n  update_fields = {}\n  unset_fields = {}\n\n  # Handle the recursive case when chains have children\n  updateChainType(revision[\"chain\"], \"chain\", update_fields)\n\n  for key, llm in revision.get(\"llms\", {}).items():\n    if \"_type\" in llm and llm[\"_type\"] is not None:\n      llm_type = llm[\"_type\"].replace(\"_llm\", \"\")\n      update_fields[f\"llms.{key}.llm_type\"] = llm_type\n      unset_fields[f\"llms.{key}._type\"] = \"\"\n\n  if update_fields or unset_fields:\n    try:\n      updates = {}\n      if update_fields:\n        updates[\"$set\"] = update_fields\n      if unset_fields:\n        updates[\"$unset\"] = unset_fields\n      chain_revisions.update_one({\"_id\": revision[\"_id\"]}, updates)\n    except Exception as e:\n      print(f\"Error updating document with _id={revision['_id']}: {e}\")", "\nclient.close()\n"]}
{"filename": "lib/db.py", "chunked_list": ["import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\nfrom lib.model.chain_revision import ChainRevisionRepository\nfrom lib.model.chain import ChainRepository\nfrom lib.model.result import ResultRepository\n\nload_dotenv()\n\nclient = MongoClient(os.environ[\"MONGODB_URL\"])", "\nclient = MongoClient(os.environ[\"MONGODB_URL\"])\ndatabase = client[os.environ[\"MONGODB_DATABASE\"]]\n\nchain_revision_repository = ChainRevisionRepository(database=database)\nchain_repository = ChainRepository(database=database)\nresult_repository = ResultRepository(database=database)\n"]}
{"filename": "lib/chain_service.py", "chunked_list": ["import logging\n\nimport traceback\nfrom types import MappingProxyType\nfrom typing import Optional, Dict\nimport json\n\nfrom huggingface_hub.inference_api import InferenceApi\nfrom langchain import HuggingFaceHub, OpenAI\nfrom lib.model.chain_revision import ChainRevision, find_ancestor_ids", "from langchain import HuggingFaceHub, OpenAI\nfrom lib.model.chain_revision import ChainRevision, find_ancestor_ids\nfrom lib.model.chain import Chain\nfrom lib.model.lang_chain_context import LangChainContext\nfrom lib.db import chain_revision_repository, chain_repository, result_repository\nfrom bson import ObjectId\nimport dotenv\nimport os\nimport pinecone\n", "import pinecone\n\nlogging.basicConfig(level=logging.INFO)\n\ndotenv.load_dotenv()\n\n\ndef initialize_services():\n  if os.getenv(\"PINECONE_API_KEY\") is not None:\n    pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENVIRONMENT\"))", "\n\ndef save_revision(chain_name, revision) -> str:\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n\n  if chain is not None:\n    revision.parent = chain.revision\n    chain.revision = revision.id\n    chain_revision_repository.save(revision)\n\n    chain.revision = revision.id\n    chain_repository.save(chain)\n  else:\n    chain_revision_repository.save(revision)\n\n    chain = Chain(name=chain_name, revision=revision.id)\n    chain_repository.save(chain)\n\n  return revision.id", "\n\ndef load_by_chain_name(chain_name):\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  return chain_revision_repository.find_one_by_id(chain.revision)\n\n\ndef load_by_id(revision_id):\n  return chain_revision_repository.find_one_by_id(revision_id)\n", "\n\ndef history_by_chain_name(chain_name):\n  \"\"\"return the ids of all revisions of the chain.\n  \"\"\"\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  revision = chain_revision_repository.find_one_by_id(chain.revision)\n  return [revision.id] + find_ancestor_ids(revision.id, chain_revision_repository)\n\n\ndef save_patch(chain_name, patch) -> str:\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  revision = chain_revision_repository.find_one_by_id(chain.revision)\n  new_chain = revision.chain.copy_replace(lambda spec: patch if spec.chain_id == patch.chain_id else spec)\n\n  try:\n    ctx = LangChainContext(llms=revision.llms)\n    revision.chain.to_lang_chain(ctx)\n  except Exception as e:\n    raise ValueError(\"Could not realize patched chain:\", e)\n\n  new_revision = revision.copy()\n  new_revision.id = None\n  new_revision.chain = new_chain\n  new_revision.parent = revision.id\n\n  chain_revision_repository.save(new_revision)\n  chain.revision = new_revision.id\n  chain_repository.save(chain)\n\n  return revision.id", "\n\ndef save_patch(chain_name, patch) -> str:\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  revision = chain_revision_repository.find_one_by_id(chain.revision)\n  new_chain = revision.chain.copy_replace(lambda spec: patch if spec.chain_id == patch.chain_id else spec)\n\n  try:\n    ctx = LangChainContext(llms=revision.llms)\n    revision.chain.to_lang_chain(ctx)\n  except Exception as e:\n    raise ValueError(\"Could not realize patched chain:\", e)\n\n  new_revision = revision.copy()\n  new_revision.id = None\n  new_revision.chain = new_chain\n  new_revision.parent = revision.id\n\n  chain_revision_repository.save(new_revision)\n  chain.revision = new_revision.id\n  chain_repository.save(chain)\n\n  return revision.id", "\n\ndef branch(chain_name, branch_name):\n  \"\"\"Branch a chain revision.\n  This will create a new chain that points to the same revision\n  as the provided chain. The new chain may be then revised independently.\n  \"\"\"\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n\n  new_chain = Chain(name=branch_name, revision=chain.revision)\n  chain_repository.save(new_chain)", "\n\ndef reset(chain_name, revision):\n  \"\"\"Reset a chain to a another revision.\n  This will update the chain to point to the given revision.\n  \"\"\"\n  new_revision = chain_revision_repository.find_one_by({\"id\": ObjectId(revision)})\n  if new_revision is None:\n    raise ValueError(f\"Revision {revision} not found\")\n\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  chain.revision = new_revision.id\n  chain_repository.save(chain)", "\n\ndef list_chains():\n  \"\"\"List all chains.\"\"\"\n  return {chain_name.name: str(chain_name.revision) for chain_name in chain_repository.find_by({})}\n\n\ndef save_results(ctx: LangChainContext, revision_id: str):\n  results = ctx.results(revision_id)\n  for result in results:\n    result_repository.save(result)\n\n  ctx.reset_results()", "\n\ndef run_once(chain_name, input, record):\n  \"\"\"Run a chain revision.\n  The revision is read from the database and fed input from stdin or the given file.\n  The results are ouput as json to stdout.\n  \"\"\"\n  chain = chain_repository.find_one_by({\"name\": chain_name})\n  revision = chain_revision_repository.find_one_by_id(chain.revision)\n\n  filledLLMs = {key: llm.to_llm() for key, llm in revision.llms.items()}\n\n  ctx = LangChainContext(llms=filledLLMs, recording=True)\n  lang_chain = revision.chain.to_lang_chain(ctx)\n  output = lang_chain._call(input)\n\n  if (record):\n    save_results(ctx, revision.id)\n\n  return output", "\n\ndef results(revision_id: str, ancestors: bool, chain_id: Optional[str] = None):\n  \"\"\"Find results for a prompt in for one or more chain revisions.\n  One of chain-name or revision must be specified.\n  If the ancestors flag is set, results for all ancestors of the specified revision are included.\n  \"\"\"\n  revision_ids = [ObjectId(revision_id)]\n  # if ancestors are requested, find all ancestors of the specified revision\n  if ancestors:\n    ancestors_id = find_ancestor_ids(revision_id, chain_revision_repository)\n    revision_ids += [ObjectId(i) for i in ancestors_id]\n\n  return result_repository.find_by({\"revision\": {\"$in\": revision_ids}, \"chain_id\": int(chain_id)})", "\ndef export_chain(chain_name: str) -> str:\n    export_ids = history_by_chain_name(chain_name)\n\n    chain_revisions = [chain_revision_repository.find_one_by_id(id) for id in export_ids]\n    if not all(chain_revisions):\n        missing_id = next((id for id, revision in zip(export_ids, chain_revisions) if not revision), None)\n        raise Exception(\"Could not find revision with id: \" + missing_id)\n\n    return chain_revisions[::-1]", "\n\ndef import_chain(chain_name, chain_details):\n    root_revision = None\n    for revision in chain_details:\n        try:\n          if not revision.parent:\n            root_revision = revision\n          chain_revision_repository.save(revision)\n        except Exception as e:\n          traceback.print_exc()\n\n    leaf_revision = find_leaf_revision(chain_details, root_revision)\n\n    chain = Chain(name=chain_name, revision=leaf_revision)\n    chain_repository.save(chain)", "\n\ndef find_leaf_revision(revisions, current_revision):\n    if (current_revision is None):\n       return current_revision.id\n    id = current_revision.id\n    for revision in revisions:\n        if revision.parent == current_revision.id:\n            return find_leaf_revision(revisions, revision)\n    return current_revision.id", "\n\n"]}
{"filename": "lib/chains/reformat_chain.py", "chunked_list": ["import json\nfrom typing import Dict, List\nfrom langchain.chains.base import Chain\nfrom lib.formatters.extended_formatter import ExtendedFormatter\n\nclass ReformatChain(Chain):\n  input_variables: List[str]\n  formatters: Dict[str, str]\n\n  @property\n  def input_keys(self) -> List[str]:    \n    return self.input_variables\n\n  @property\n  def output_keys(self) -> List[str]:\n    return list(self.formatters.keys())\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    formatter = ExtendedFormatter()\n    return {k: formatter.format(v, **inputs) for k, v in self.formatters.items()}"]}
{"filename": "lib/chains/api_chain.py", "chunked_list": ["import os\nfrom typing import Dict, List, Optional\nfrom langchain.chains.base import Chain\nimport requests\n\nclass APIChain(Chain):\n  url: str\n  method: str\n  headers: Optional[Dict[str, str]]\n  body: Optional[str]\n  output_variable: str\n  input_variables: List[str]\n\n  @property\n  def input_keys(self) -> List[str]:    \n    return self.input_variables\n\n  @property\n  def output_keys(self) -> List[str]:\n    return [self.output_variable]\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    vars = {**os.environ, **inputs}\n\n    f_url = self.url.format(**vars)\n\n    f_headers = {}\n    if self.headers is not None:\n      f_headers = {k: v.format(**vars) for k, v in self.headers.items()}\n\n    if self.method.lower() == 'get':\n      res = requests.get(f_url, headers=f_headers)\n    elif self.method.lower() == 'post':\n      f_body = self.body.format(**vars)\n      res = requests.post(f_url, data=f_body)\n\n    return {self.output_variable: res.text}"]}
{"filename": "lib/chains/case_chain.py", "chunked_list": ["from typing import Dict, List\nfrom langchain.chains.base import Chain\nfrom pydantic import root_validator\n\nclass CaseChain(Chain):\n  categorization_input: str\n  subchains: Dict[str, Chain]\n  default_chain: Chain\n  output_variables: List[str] = [\"text\"]\n  input_keys = []\n  output_keys = []\n\n  @property\n  def input_keys(self) -> List[str]:\n    keys = list(set(self.default_chain.input_keys \\\n      + [key for subchain in self.subchains.values() for key in subchain.input_keys] \\\n      + [self.categorization_input]))\n    keys.sort()   \n    return keys\n\n  @property\n  def output_keys(self) -> List[str]:\n    keys_set = set(self.default_chain.output_keys)\n    for subchain in self.subchains.values():\n      keys_set = keys_set.intersection(subchain.output_keys)\n    keys = list(keys_set)\n    keys.sort()\n    return keys\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    categorization = inputs[self.categorization_input].strip()\n\n    subchain = self.subchains[categorization] if categorization in self.subchains else self.default_chain\n \n    known_values = inputs.copy()\n    outputs = subchain(known_values, return_only_outputs=True)\n    known_values.update(outputs)\n    return {k: known_values[k] for k in self.output_keys}  "]}
{"filename": "lib/chains/__init__.py", "chunked_list": [""]}
{"filename": "lib/chains/vector_search_chain.py", "chunked_list": ["import json\nimport os\nfrom typing import Dict, List, Any\nfrom langchain.chains.base import Chain\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores.base import VectorStore\n\nclass VectorSearchChain(Chain):\n  query: str\n  embedding_engine: str\n  embedding_params: Dict[str, Any]\n  database: str\n  database_params: Dict[str, Any]\n  num_results: int\n  min_score: float = 0.0\n  input_variables: List[str] = []\n  output_key: str = 'results'\n\n  @property\n  def input_keys(self) -> List[str]:\n    return self.input_variables\n\n  @property\n  def output_keys(self) -> List[str]:\n    return [self.output_key]\n  \n  def vector_store(self, query) -> VectorStore:    \n    if self.database == 'pinecone':\n      if 'index' not in self.database_params:\n        raise ValueError('Missing index parameter for Pinecone database')\n\n      # import just-in-time so auth will happen after env vars are loaded\n      import pinecone\n      from langchain.vectorstores.pinecone import Pinecone\n      \n      index = pinecone.Index(self.database_params['index'])\n      return Pinecone(\n        index,\n        query,\n        self.database_params.get('text_key') or 'text',\n        namespace=self.database_params.get('namespace') or ''\n      )\n    else:\n      raise ValueError(f'Unknown database: {self.database}')\n\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    if self.embedding_engine == 'openai':\n      self.embedding_params['openai_api_key'] = os.environ.get(\"OPENAI_API_KEY\")\n      embeddings = OpenAIEmbeddings(**self.embedding_params)\n    elif self.embedding_engine == 'huggingface':\n      model = self.embedding_params.get('model_name') or 'all-mpnet-base-v2'\n      embeddings = HuggingFaceEmbeddings(**self.embedding_params, model_name=model)\n    else:\n      raise ValueError(f'Unknown embedding engine: {self.embedding_engine}')\n\n    vector_store = self.vector_store(embeddings.embed_query)\n    \n    formatted_query = self.query.format(**inputs)\n\n    items = vector_store.similarity_search_with_score(formatted_query, self.num_results, self.database_params.get('filter'), self.database_params.get('namespace'))\n\n    return {self.output_key: json.dumps([{'text': item[0].page_content, 'meta': item[0].metadata, 'score': item[1]} for item in items if item[1] >= self.min_score])}", "class VectorSearchChain(Chain):\n  query: str\n  embedding_engine: str\n  embedding_params: Dict[str, Any]\n  database: str\n  database_params: Dict[str, Any]\n  num_results: int\n  min_score: float = 0.0\n  input_variables: List[str] = []\n  output_key: str = 'results'\n\n  @property\n  def input_keys(self) -> List[str]:\n    return self.input_variables\n\n  @property\n  def output_keys(self) -> List[str]:\n    return [self.output_key]\n  \n  def vector_store(self, query) -> VectorStore:    \n    if self.database == 'pinecone':\n      if 'index' not in self.database_params:\n        raise ValueError('Missing index parameter for Pinecone database')\n\n      # import just-in-time so auth will happen after env vars are loaded\n      import pinecone\n      from langchain.vectorstores.pinecone import Pinecone\n      \n      index = pinecone.Index(self.database_params['index'])\n      return Pinecone(\n        index,\n        query,\n        self.database_params.get('text_key') or 'text',\n        namespace=self.database_params.get('namespace') or ''\n      )\n    else:\n      raise ValueError(f'Unknown database: {self.database}')\n\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    if self.embedding_engine == 'openai':\n      self.embedding_params['openai_api_key'] = os.environ.get(\"OPENAI_API_KEY\")\n      embeddings = OpenAIEmbeddings(**self.embedding_params)\n    elif self.embedding_engine == 'huggingface':\n      model = self.embedding_params.get('model_name') or 'all-mpnet-base-v2'\n      embeddings = HuggingFaceEmbeddings(**self.embedding_params, model_name=model)\n    else:\n      raise ValueError(f'Unknown embedding engine: {self.embedding_engine}')\n\n    vector_store = self.vector_store(embeddings.embed_query)\n    \n    formatted_query = self.query.format(**inputs)\n\n    items = vector_store.similarity_search_with_score(formatted_query, self.num_results, self.database_params.get('filter'), self.database_params.get('namespace'))\n\n    return {self.output_key: json.dumps([{'text': item[0].page_content, 'meta': item[0].metadata, 'score': item[1]} for item in items if item[1] >= self.min_score])}"]}
{"filename": "lib/chains/llm_recording_chain.py", "chunked_list": ["from typing import Dict, List\nfrom pydantic import Field\nfrom langchain.chains import LLMChain\n\nclass LLMRecordingChain(LLMChain):\n  recorded_calls: List[tuple[Dict[str, str], Dict[str,str]]]\n  chain_spec_id: int\n\n  def __init__(self, llm: object, prompt: str, output_key: str, chain_spec_id: int):\n    super().__init__(llm=llm, prompt=prompt, output_key=output_key, chain_spec_id=chain_spec_id, recorded_calls=[])\n\n  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n    output = super()._call(inputs)\n    self.recorded_calls.append((dict(inputs), output[self.output_key]))\n    return output\n\n  @property\n  def calls(self) -> List[tuple[Dict[str, str], Dict[str, str]]]:\n    return self.recorded_calls\n  \n  def reset(self):\n    self.recorded_calls.clear()", ""]}
{"filename": "lib/chains/tests/test_vector_search_chain.py", "chunked_list": ["import json\nimport os\nfrom typing import Any, Iterable, Optional, List, Tuple\nfrom unittest.mock import patch\nfrom langchain.vectorstores.base import VectorStore\nfrom langchain.docstore.document import Document\nfrom chains.vector_search_chain import VectorSearchChain\n\nclass MockVectorStore(VectorStore):\n  def similarity_search_with_score(self, query: str, k: int = 5, filter: Optional[dict] = None, namespace: Optional[str] = None) -> List[Tuple[Document, float]]:\n    assert query == \"How do I open a can of soup?\"\n    return [\n      (Document(page_content=\"Opening cans of soup.\", metadata={}), 0.5),\n      (Document(page_content=\"Opening cans of paint.\", metadata={}), 0.4),\n    ]\n  \n  def add_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n    return super().add_texts(texts, metadatas, **kwargs)\n  \n  def similarity_search(self, query: str, k: int = 4, **kwargs: Any) -> List[Document]:\n    return super().similarity_search(query, k, **kwargs)\n\n  def from_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n    return super().from_texts(texts, metadatas, **kwargs)  ", "class MockVectorStore(VectorStore):\n  def similarity_search_with_score(self, query: str, k: int = 5, filter: Optional[dict] = None, namespace: Optional[str] = None) -> List[Tuple[Document, float]]:\n    assert query == \"How do I open a can of soup?\"\n    return [\n      (Document(page_content=\"Opening cans of soup.\", metadata={}), 0.5),\n      (Document(page_content=\"Opening cans of paint.\", metadata={}), 0.4),\n    ]\n  \n  def add_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n    return super().add_texts(texts, metadatas, **kwargs)\n  \n  def similarity_search(self, query: str, k: int = 4, **kwargs: Any) -> List[Document]:\n    return super().similarity_search(query, k, **kwargs)\n\n  def from_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n    return super().from_texts(texts, metadatas, **kwargs)  ", "\n\ndef test_openai_pinecone_search():\n  os.environ.setdefault(\"OPENAI_API_KEY\", \"test\")\n \n  chain = VectorSearchChain(\n    query=\"How do I open a can of {can_type}?\",\n    embedding_engine=\"openai\",\n    embedding_params={\"openai_api_key\": \"test\"},\n    database=\"pinecone\",\n    database_params={\"index\": \"test\", \"text_key\": \"text\"},\n    input_variables=[],\n    num_results=10,\n  )\n\n  with patch.object(VectorSearchChain, 'vector_store', return_value=MockVectorStore()):\n    response = chain._call({\"can_type\": \"soup\"})\n    results = json.loads(response['results'])\n    assert len(results) == 2\n    assert results[0]['text'] == \"Opening cans of soup.\""]}
{"filename": "lib/chains/tests/test_case_chain.py", "chunked_list": ["from pytest import fixture\nfrom chains.case_chain import CaseChain\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms.fake import FakeListLLM\n\ndef simple_case_chain():\n  return CaseChain(\n    categorization_input=\"categorization\",\n    subchains={\n      \"a\": LLMChain(\n        prompt=PromptTemplate(input_variables=[\"input2\", \"input3\"], template=\"prompt1 {input2} {input3}\"),\n        llm=FakeListLLM(responses=[\"fake_response1\"]),\n        output_key=\"output1\",\n      ),\n      \"b\": LLMChain(\n        prompt=PromptTemplate(input_variables=[\"input3\"], template=\"prompt2 {input3}\"),\n        llm=FakeListLLM(responses=[\"fake_response2\"]),\n        output_key=\"output1\",\n      ),\n    },\n    default_chain=LLMChain(\n      prompt=PromptTemplate(input_variables=[\"input1\", \"input2\"], template=\"prompt3 {input1} {input2}\"),\n      llm=FakeListLLM(responses=[\"fake_response3\"]),\n      output_key=\"output1\",\n    ),\n  )", "\ndef test_input_keys():\n  chain = simple_case_chain()\n  assert chain.input_keys == [\"categorization\", \"input1\", \"input2\", \"input3\"]\n\ndef test_output_keys():\n  chain = simple_case_chain()\n  assert chain.output_keys == [\"output1\"]\n\ndef test_run():\n  chain = simple_case_chain()\n  inputs = {\"input1\": \"input1\", \"input2\": \"input2\", \"input3\": \"input3\"}\n  assert chain.run({\"categorization\": \"a\", **inputs}) == \"fake_response1\"\n  assert chain.run({\"categorization\": \"b\", **inputs}) == \"fake_response2\"\n  assert chain.run({\"categorization\": \"garbage\", **inputs}) == \"fake_response3\"", "\ndef test_run():\n  chain = simple_case_chain()\n  inputs = {\"input1\": \"input1\", \"input2\": \"input2\", \"input3\": \"input3\"}\n  assert chain.run({\"categorization\": \"a\", **inputs}) == \"fake_response1\"\n  assert chain.run({\"categorization\": \"b\", **inputs}) == \"fake_response2\"\n  assert chain.run({\"categorization\": \"garbage\", **inputs}) == \"fake_response3\""]}
{"filename": "lib/chains/tests/__init__.py", "chunked_list": [""]}
{"filename": "lib/chains/tests/test_reformat_chain.py", "chunked_list": ["from chains.reformat_chain import ReformatChain\n\ndef test_reformat_chain_formats_inputs():\n  chain = ReformatChain(\n      formatters={\"output1\": \"{input1}-{input2}\", \"output2\": \"{input2}\"},\n      input_variables=[\"input1\", \"input2\"],\n  )\n\n  inputs = {\"input1\": \"foo\", \"input2\": \"bar\"}\n  output = chain._call(inputs)\n  assert output == {\"output1\": \"foo-bar\", \"output2\": \"bar\"}", "\n\ndef test_reformat_extended_formatting():\n  inputs = {\"result\": \"[34,93,94]\", \"weight\": '.6'}\n\n  formatters = {\n    \"output\": \"{float:weight:w}{parse_json:result:values}{join:values:,}{expr:item*w:x}{x:.2f}\",\n  }\n  chain = ReformatChain(\n      formatters=formatters,\n      input_variables=[\"result\", \"weight\"],\n  )\n\n  output = chain._call(inputs)\n  assert output == {\"output\": \"20.40,55.80,56.40\"}"]}
{"filename": "lib/chains/tests/test_api_chain.py", "chunked_list": ["import requests_mock\nfrom chains.api_chain import APIChain\n\ndef test_api_chain_get():\n  chain = APIChain(\n    url='http://localhost:9200/test?q={query}',\n    method='get',\n    headers={'Authorization': 'Bearer {token}'},\n    output_variable='response',\n    input_variables=['query', 'token']\n  )\n\n  with requests_mock.Mocker() as m:\n    response = '{\"hits\": {\"hits\": [{\"_source\": {\"text\": \"x-ray\"}}]}}'\n    m.get('http://localhost:9200/test?q=x-ray',\n      text=response,      \n      headers={'Authorization': 'Bearer 1234'}\n    )\n    inputs = {\"query\": \"x-ray\", \"token\": \"1234\"}\n    output = chain.run(inputs)\n    assert output == response", "\n\ndef test_api_chain_post():\n  chain = APIChain(\n    url='http://localhost:9200/test',\n    method='post',\n    headers={'Authorization': 'Bearer {token}'},\n    body='{{\"query\": \"{query}\"}}',\n    output_variable='response',\n    input_variables=['query', 'token']\n  )\n\n  with requests_mock.Mocker() as m:\n    response = '{\"hits\": {\"hits\": [{\"_source\": {\"text\": \"x-ray\"}}]}}'\n    m.post('http://localhost:9200/test', \n      text=response,\n      headers={'Authorization': 'Bearer 1234'}\n    )\n    inputs = {\"query\": \"x-ray\", \"token\": \"1234\"}\n    output = chain.run(inputs)\n    assert m.last_request.json() == {\"query\": \"x-ray\"}\n    assert output == response", ""]}
{"filename": "lib/formatters/__init__.py", "chunked_list": [""]}
{"filename": "lib/formatters/extended_formatter.py", "chunked_list": ["import string\nimport re\nimport json\nimport numexpr\n\nclass ExtendedFormatter(string.Formatter):\n  \"\"\"\n  This class extends the standard Python string formatter to add some extra\n  features that are useful for formatting data for the LangChain.\n\n  The following expressions are added:\n\n  - `{join:[varexpr]:[separator]}`: Join an array found at varexpr with the separator. For each\n    item in the array, the rest of the format string is formatted with the item as the `item` variable\n    and the index of the item as the `index` variable. If no separator is provided, the items are\n    joined without a separator. Example:\n    format('{join:data[colors]:, }{index}:{item}', data={'colors': ['r','g','b']})) -> '0:r, 1:g, 2:b'\n  - `{parse_json:[varexpr]:[varname]}`: Parse a JSON string found at varexpr and store the result\n    in a variable with the name varname which will be available in the rest of the expression.\n    If no varname is provided, the variable is named `data`.\n    Example: format('{parse_json:data[json]:person}{person[name]}', data={'json': '{\"name\": \"John\"}'}) -> 'John'\n  - `{let:[varexpr]:[varname]}`: Store the value found at varexpr in a variable with the name varname\n    which will be available in the rest of the expression. If no varname is provided, the variable\n    is named `data`.\n    Example: format('{let:data[name]:person}{person}', data={'name': 'John'}) -> 'John'\n  - `{expr:[expr]:[varname]}`: Evaluate the math expression expr using numexpr and store the result\n    in a variable with the name varname which will be available in the rest of the expression.\n    Note that variables used in [expr] must be ints or floats and cannot be nested format expressions\n    (eg. a[b] or a.b will not work). Use the int or float expressions to prepare data to use in expressions.\n    If no varname is provided, the variable is named `data`.\n    Example: format('{expr:a*b:result}{result}', a=2, b=3) -> '6'\n  - `{int:[varexpr]:[varname]}`: Parse varexpr as an integer in a variable with the name varname which\n    will be available in the rest of the expression. If no varname is provided, the variable is named `data`.\n    Example: format('{int:result[age]:age}{expr:age+4:result}{result}', result={'age': '2.0001'}) -> '6'\n  - `{float:[varexpr]:[varname]}`: Parse varexpr as a float in a variable with the name varname which\n    will be available in the rest of the expression. If no varname is provided, the variable is named `data`.\n    Example: format('{float:result[age]:age}{expr:age+4:result}{result}', result={'age': '2.5'}) -> '6.5'\n  \"\"\"\n  def format(self, format_string, *args, **kwargs):\n    regex = r\"\\{(join|parse_json|let|expr|int|float):([^:\\}]+)(:([^:\\}]+))?\\}(.*)\"\n    match = re.fullmatch(regex, format_string, flags=re.DOTALL)\n\n    if not match:      \n      return super().format(format_string, *args, **kwargs)\n\n    args = match.groups()\n    value = args[1]\n    arg = args[3]\n    rest = args[4]\n\n    if args[0] == \"join\":\n      return self.join(value, arg, rest, args, kwargs)\n    elif args[0] == \"parse_json\":\n      return self.parse_json(value, arg, rest, args, kwargs)\n    elif args[0] == \"let\":\n      return self.let(value, arg, rest, args, kwargs)\n    elif args[0] == \"expr\":\n      return self.expr(value, arg, rest, args, kwargs)\n    elif args[0] == \"int\":\n      return self.int(value, arg, rest, args, kwargs)\n    elif args[0] == \"float\":     \n      return self.float(value, arg, rest, args, kwargs)\n\n    return super().format(format_string, *args, **kwargs)\n\n\n  def join(self, iterator_exp, separator, inner_format, args, kwargs):\n    separator = '' if separator is None else separator\n    iterator = self.get_field(iterator_exp, args, kwargs)[0]\n\n    return separator.join(self.format(inner_format, *args, **{**kwargs, \"item\": item, \"index\": index})\n      for index, item in enumerate(iterator))\n  \n\n  def parse_json(self, json_string_exp, variable_name, rest, args, kwargs):\n    variable_name = 'data' if variable_name is None else variable_name\n    json_string = self.get_field(json_string_exp, args, kwargs)[0]\n    \n    parsed = json.loads(json_string)\n    return self.format(rest, *args, **{**kwargs, variable_name: parsed})\n\n\n  def let(self, value_exp, variable_name, rest, args, kwargs):\n    variable_name = 'data' if variable_name is None else variable_name    \n    value = self.get_field(value_exp, args, kwargs)[0]\n\n    return self.format(rest, *args, **{**kwargs, variable_name: value})\n  \n\n  def int(self, value_exp, variable_name, rest, args, kwargs):\n    variable_name = 'data' if variable_name is None else variable_name    \n    value = self.get_field(value_exp, args, kwargs)[0]\n\n    return self.format(rest, *args, **{**kwargs, variable_name: int(value)})\n  \n\n  def float(self, value_exp, variable_name, rest, args, kwargs):\n    variable_name = 'data' if variable_name is None else variable_name    \n    value = self.get_field(value_exp, args, kwargs)[0]\n\n    return self.format(rest, *args, **{**kwargs, variable_name: float(value)})\n\n  def expr(self, expression, variable_name, rest, args, kwargs):\n    variable_name = 'data' if variable_name is None else variable_name\n    result = numexpr.evaluate(expression, global_dict={}, local_dict=kwargs)\n    return self.format(rest, *args, **{**kwargs, variable_name: result})", ""]}
{"filename": "lib/formatters/tests/test_extended_formatter.py", "chunked_list": ["from formatters.extended_formatter import ExtendedFormatter\n\ndef test_extended_formatter_normal_formatting():  \n  formatter = ExtendedFormatter()\n\n  assert formatter.format(\"Price is ${0:.2f} today\", 42) == \"Price is $42.00 today\"\n  assert formatter.format(\"Price: ${price:.2f}\", price=42) == \"Price: $42.00\"\n  assert formatter.format(\"{name} is {age} years old and likes {animal}\", \n    name='John',\n    age='32',\n    animal='birds') == \"John is 32 years old and likes birds\"\n  \n  nested = formatter.format('Complex number {0} has real {0.real:.2f} and imaginary {0.imag:.2f} parts.', 3-5j)\n  assert nested == 'Complex number (3-5j) has real 3.00 and imaginary -5.00 parts.'\n  assert formatter.format('{:*^30}', 'centered') == '***********centered***********'", "\n\ndef test_extended_formatter_join():\n  data = {'colors': ['red', 'green', 'blue', 'yellow'], 'name': 'John'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{join:data[colors]:, }{item}', data=data) == 'red, green, blue, yellow'\n  assert formatter.format('{join:data[colors]}{item}', data=data) == 'redgreenblueyellow'\n\n  result = \"John likes red, and John likes green, and John likes blue, and John likes yellow\"\n  assert formatter.format('{join:data[colors]:, and }{data[name]} likes {item}', data=data) == result", "\n\ndef test_extended_formatter_parse_json():\n  data = {'json': '{\"a\": [42, 36], \"name\": \"John\"}'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{parse_json:data[json]:person}{person[a][0]}', data=data) == '42'\n  assert formatter.format('{parse_json:data[json]:person}{person[a][1]}', data=data) == '36'\n\n  result = \"John: 42:36\"\n  assert formatter.format('{parse_json:data[json]:person}{person[name]}: {person[a][0]}:{person[a][1]}', data=data) == result", "\n\ndef test_extended_formatter_let():\n  data = {'name': 'John'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{let:data[name]:person}{person}', data=data) == 'John'\n\ndef test_extended_formatter_expr():\n  data = {'a': 42, 'b': 36, 'c': 2}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{expr:c*(a + b):result}{result}', **data) == '156'\n  assert formatter.format('{let:data[a]:a}{let:data[b]:b}{let:data[c]:c}{expr:c*(a + b):result}{result}', data=data) == '156'", "def test_extended_formatter_expr():\n  data = {'a': 42, 'b': 36, 'c': 2}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{expr:c*(a + b):result}{result}', **data) == '156'\n  assert formatter.format('{let:data[a]:a}{let:data[b]:b}{let:data[c]:c}{expr:c*(a + b):result}{result}', data=data) == '156'\n\n\ndef test_extended_formatter_int():\n  data = {'age': '32'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{int:data[age]:age}{expr:age+4:x}{x}', data=data) == '36'", "def test_extended_formatter_int():\n  data = {'age': '32'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{int:data[age]:age}{expr:age+4:x}{x}', data=data) == '36'\n\n\ndef test_extended_formatter_float():\n  data = {'weight': '.493'}\n\n  formatter = ExtendedFormatter()\n  assert formatter.format('{float:data[weight]:weight}{expr:10*weight:x}{x:.2f}', data=data) == '4.93'", "\n\ndef test_extended_formatter_complex_expr():\n  data = {\"result\": '{\"colors\": [\"red\", \"green\", \"blue\", \"yellow\"], \"name\": \"John\"}'}\n\n  parse = '{parse_json:data[result]:person}'\n  let = '{let:person[name]:name}'\n  iterate = '{join:person[colors]:\\n}'\n  expr = '{expr:index + 1:i}'\n  format_str = '{i}. {name} likes {item}'\n\n  formatter = ExtendedFormatter()\n  result = \"1. John likes red\\n2. John likes green\\n3. John likes blue\\n4. John likes yellow\"\n\n  assert formatter.format(f\"{parse}{let}{iterate}{expr}{format_str}\", data=data) == result", ""]}
{"filename": "lib/formatters/tests/__init__.py", "chunked_list": [""]}
{"filename": "lib/model/chain_spec.py", "chunked_list": ["from typing import Annotated, Callable, Dict, List, Literal, Optional, Union, Any\nfrom pydantic import BaseModel, Field\nfrom langchain.chains.base import Chain\nfrom langchain.chains import LLMChain, SequentialChain, TransformChain\nfrom langchain.prompts import PromptTemplate\nfrom lib.model.lang_chain_context import LangChainContext\nfrom lib.chains.case_chain import CaseChain\nfrom lib.chains.api_chain import APIChain\nfrom lib.chains.reformat_chain import ReformatChain\nfrom lib.chains.llm_recording_chain import LLMRecordingChain", "from lib.chains.reformat_chain import ReformatChain\nfrom lib.chains.llm_recording_chain import LLMRecordingChain\nfrom lib.chains.vector_search_chain import VectorSearchChain\n\nChainSpec = Annotated[Union[\n  \"APIChainSpec\",\n  \"SequentialChainSpec\",\n  \"LLMChainSpec\",\n  \"CaseChainSpec\",\n  \"ReformatChainSpec\",", "  \"CaseChainSpec\",\n  \"ReformatChainSpec\",\n  \"TransformChainSpec\",\n  \"VectorSearchChainSpec\"\n  ], Field(discriminator='chain_type')]\n\nclass BaseChainSpec(BaseModel):\n  chain_id: int\n\n  @property\n  def children(self) -> List[ChainSpec]:\n    return []\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    raise NotImplementedError\n  \n  def traverse(self, fn: Callable[[ChainSpec], None]) -> None:\n    fn(self)\n\n    for child in self.children:\n      child.traverse(fn)\n  \n\n  def find_by_chain_id(self, chain_id: int) -> Optional[ChainSpec]:\n    if self.chain_id == chain_id:\n      return self\n\n    for child in self.children:\n      result = child.find_by_chain_id(chain_id)\n\n      if result is not None:\n        return result\n\n    return None\n  \n  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n    return replace(self).copy(deep=True)", "\n\nclass LLMChainSpec(BaseChainSpec):\n  input_keys: List[str]\n  output_key: str\n  chain_type: Literal[\"llm_chain_spec\"] = \"llm_chain_spec\"\n  prompt: str\n  llm_key: str\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    llm = ctx.llms.get(self.llm_key)\n\n    if llm is None:\n      raise ValueError(f\"LLM with key {self.llm_key} not found in context\")\n\n    promptTemplate = PromptTemplate(template=self.prompt, input_variables=self.input_keys)\n\n    if ctx.recording:\n      chain = LLMRecordingChain(llm=llm, prompt=promptTemplate, output_key=self.output_key, chain_spec_id=self.chain_id)\n      ctx.prompts.append(chain)\n      return chain\n\n    return LLMChain(llm=llm, prompt=promptTemplate, output_key=self.output_key)", "\n\nclass SequentialChainSpec(BaseChainSpec):\n  input_keys: List[str]\n  output_keys: List[str]\n  chain_type: Literal[\"sequential_chain_spec\"] = \"sequential_chain_spec\"\n  chains: List[ChainSpec]\n\n  @property\n  def children(self) -> List[ChainSpec]:\n    return list(self.chains)\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    chains = [chain.to_lang_chain(ctx) for chain in self.chains]\n    return SequentialChain(chains=chains, input_variables=self.input_keys, output_variables=self.output_keys)\n  \n  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n    sequential = replace(self).copy(deep=True, exclude={\"chains\"})\n    sequential.chains = [chain.copy_replace(replace) for chain in self.chains]\n    return sequential", "\n\nclass CaseChainSpec(BaseChainSpec):\n  chain_type: Literal[\"case_chain_spec\"] = \"case_chain_spec\"\n  cases: Dict[str, ChainSpec]\n  categorization_key: str\n  default_case: ChainSpec\n\n  @property\n  def children(self) -> List[ChainSpec]:\n    return list(self.cases.values()) + [self.default_case]\n\n  def to_lang_chain(self, ctx: LangChainContext) -> CaseChain:\n    subchains = {key: chain.to_lang_chain(ctx) for key, chain in self.cases.items()}\n    case_chain = CaseChain(\n      subchains=subchains, \n      categorization_input=self.categorization_key,\n      default_chain=self.default_case.to_lang_chain(ctx),\n    )\n    return case_chain\n  \n  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n    case_chain = replace(self).copy(deep=True, exclude={\"cases\"})\n    case_chain.cases = {key: chain.copy_replace(replace) for key, chain in self.cases.items()}\n    return case_chain", "\n\nclass ReformatChainSpec(BaseChainSpec):\n  chain_type: Literal[\"reformat_chain_spec\"] = \"reformat_chain_spec\"\n  formatters: Dict[str, str]\n  input_keys: List[str]\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    return ReformatChain(formatters=self.formatters, input_variables=self.input_keys)\n", "\n\nclass TransformChainSpec(BaseChainSpec):\n  chain_type: Literal[\"transform_chain_spec\"] = \"transform_chain_spec\"\n  transform_func: str\n  input_keys: List[str]\n  output_keys: List[str]\n\n  def create_function(self, body):\n    scope = {}\n    indented = body.replace(\"\\n\", \"\\n  \")\n    code = f\"def f(inputs: dict):\\n  {indented}\"\n    exec(code , scope)\n    return scope[\"f\"]\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    return TransformChain(input_variables=self.input_keys, output_variables=self.output_keys, transform=self.create_function(self.transform_func))", "\n\nclass APIChainSpec(BaseChainSpec):\n  chain_type: Literal[\"api_chain_spec\"] = \"api_chain_spec\"\n  url: str\n  method: str\n  headers: Optional[Dict[str, str]]\n  body: Optional[str]\n  input_keys: List[str]\n  output_key: str\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    return APIChain(\n      url=self.url,\n      method=self.method,\n      headers=self.headers,\n      body=self.body,\n      input_variables=self.input_keys,\n      output_variable=self.output_key,\n    )", "\nclass VectorSearchChainSpec(BaseChainSpec):\n  chain_type: Literal[\"vector_search_chain_spec\"] = \"vector_search_chain_spec\"\n  query: str\n  embedding_engine: str\n  embedding_params: Dict[str, Any]\n  database: str\n  database_params: Dict[str, Any]\n  num_results: int\n  min_score: float = 0.0\n  input_variables: List[str] = []\n  output_key: str = 'results'\n\n  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n    return VectorSearchChain(\n      query=self.query,\n      embedding_engine=self.embedding_engine,\n      embedding_params=self.embedding_params,\n      database=self.database,\n      database_params=self.database_params,\n      input_variables=self.input_variables,\n      num_results=self.num_results,\n      output_key=self.output_key,\n      min_score=self.min_score,\n    )", "\nSequentialChainSpec.update_forward_refs()\nCaseChainSpec.update_forward_refs()\n"]}
{"filename": "lib/model/chain.py", "chunked_list": ["from pydantic import BaseModel\nfrom pydantic_mongo import AbstractRepository, ObjectIdField\n\nclass Chain(BaseModel):\n  id: ObjectIdField = None\n  name: str\n  revision: ObjectIdField\n\n  class Config:\n    json_encoders = {ObjectIdField: str}", "\n\nclass ChainRepository(AbstractRepository[Chain]):\n  class Meta:\n    collection_name = 'chains'\n"]}
{"filename": "lib/model/__init__.py", "chunked_list": [""]}
{"filename": "lib/model/result.py", "chunked_list": ["from typing import Dict\nfrom pydantic import BaseModel\nfrom pydantic_mongo import AbstractRepository, ObjectIdField\nfrom datetime import datetime\n\nclass Result(BaseModel):\n  id: ObjectIdField = None\n  revision: ObjectIdField\n  chain_id: int\n  input: Dict[str, str]\n  output: str\n  recorded: datetime\n\n  class Config:\n    json_encoders = {ObjectIdField: str}", "\n\nclass ResultRepository(AbstractRepository[Result]):\n  class Meta:\n    collection_name = 'results'\n\n"]}
{"filename": "lib/model/chain_revision.py", "chunked_list": ["import json\nfrom typing import Dict, Optional\nfrom pydantic import BaseModel\nfrom pydantic_mongo import AbstractRepository, ObjectIdField\nfrom lib.model.chain_spec import ChainSpec, APIChainSpec, SequentialChainSpec, LLMChainSpec, CaseChainSpec, ReformatChainSpec, TransformChainSpec, VectorSearchChainSpec\nfrom lib.model.llm_spec import LLMSpec, OpenAILLMSpec, HuggingFaceHubLLMSpec, ChatOpenAILLMSpec\n\n\ndef dump_json(obj: object, **kwargs):\n  obj['id'] = str(obj['id']) if obj['id'] is not None else None\n  obj['parent'] = str(obj['parent']) if obj['parent'] is not None else None\n  return json.dumps(obj, **kwargs)", "def dump_json(obj: object, **kwargs):\n  obj['id'] = str(obj['id']) if obj['id'] is not None else None\n  obj['parent'] = str(obj['parent']) if obj['parent'] is not None else None\n  return json.dumps(obj, **kwargs)\n\n\nclass ChainRevision(BaseModel):\n  id: ObjectIdField = None\n  parent: Optional[ObjectIdField]\n  chain: ChainSpec\n  llms: Dict[str, LLMSpec]\n\n  class Config:\n    json_encoders = {\n      ObjectIdField: str,\n    }\n    json_dumps = dump_json", "\n\nChainRevision.update_forward_refs()\n\n\nclass ChainRevisionRepository(AbstractRepository[ChainRevision]):\n  class Meta:\n    collection_name = 'chain_revisions'\n\n\ndef find_ancestor_ids(revision_id: ObjectIdField, repository: ChainRevisionRepository) -> list[ObjectIdField]:\n  revision = repository.find_one_by_id(revision_id)\n  if revision is None:\n    raise ValueError(f\"Revision with id {revision_id} not found\")\n  if revision.parent is None:\n    return []\n  return [revision.parent] + find_ancestor_ids(revision.parent, repository)", "\n\ndef find_ancestor_ids(revision_id: ObjectIdField, repository: ChainRevisionRepository) -> list[ObjectIdField]:\n  revision = repository.find_one_by_id(revision_id)\n  if revision is None:\n    raise ValueError(f\"Revision with id {revision_id} not found\")\n  if revision.parent is None:\n    return []\n  return [revision.parent] + find_ancestor_ids(revision.parent, repository)\n", ""]}
{"filename": "lib/model/llm_spec.py", "chunked_list": ["from typing import Annotated, Callable, Optional, Dict, Literal, Union, TypedDict\nfrom pydantic import BaseModel, Field\nfrom langchain.llms.base import LLM\nfrom langchain.llms.openai import OpenAI\nfrom langchain.llms.huggingface_hub import HuggingFaceHub\nfrom langchain.chat_models.openai import ChatOpenAI\n\nLLMSpec = Annotated[Union[\n  \"OpenAILLMSpec\",\n  \"HuggingFaceHubLLMSpec\",", "  \"OpenAILLMSpec\",\n  \"HuggingFaceHubLLMSpec\",\n  \"ChatOpenAILLMSpec\",\n  ], Field(discriminator='llm_type')]\n\n\nclass BaseLLMSpec(BaseModel):\n  def to_llm(self) -> LLM:\n    raise NotImplementedError\n\n  def copy_replace(self, replace: Callable[[LLMSpec], LLMSpec]):\n    return replace(self).copy(deep=True)", "\n\nclass OpenAILLMSpec(BaseLLMSpec):\n  llm_type: Literal[\"openai\"] = \"openai\"\n  model_name: str\n  temperature: float\n  max_tokens: int\n  top_p: float\n  frequency_penalty: float\n  presence_penalty: float\n  n: int\n  request_timeout: Optional[int]\n  logit_bias: Optional[Dict[int, int]]\n\n  def to_llm(self) -> LLM:\n    return OpenAI(model_name=self.model_name, temperature=self.temperature,\n                  max_tokens=self.max_tokens, top_p=self.top_p, frequency_penalty=self.frequency_penalty,\n                  presence_penalty=self.presence_penalty, n=self.n,\n                  request_timeout=self.request_timeout, logit_bias=self.logit_bias)", "\n\nclass HuggingFaceHubLLMSpec(BaseLLMSpec):\n  class ModelKwargs(TypedDict):\n    temperature: float\n    max_length: int\n\n  llm_type: Literal[\"huggingface_hub\"] = \"huggingface_hub\"\n  repo_id: str\n  task: Optional[str]\n  model_kwargs: Optional[ModelKwargs]\n\n  def to_llm(self) -> LLM:\n    return HuggingFaceHub(model_kwargs=self.model_kwargs, repo_id=self.repo_id, task=self.task)", "\n\nclass ChatOpenAILLMSpec(BaseLLMSpec):\n  llm_type: Literal[\"chat_openai\"] = \"chat_openai\"\n  model_name: str\n  temperature: float\n  max_tokens: int\n  n: int\n  request_timeout: Optional[int]\n\n  def to_llm(self) -> LLM:\n    return ChatOpenAI(model_name=self.model_name, temperature=self.temperature,\n                      max_tokens=self.max_tokens, n=self.n, request_timeout=self.request_timeout)", ""]}
{"filename": "lib/model/lang_chain_context.py", "chunked_list": ["from typing import Dict\nfrom pydantic import BaseModel\nfrom langchain.schema.language_model import BaseLanguageModel\nfrom lib.model.result import Result\nfrom datetime import datetime\n\n\n# provides context for generating lang chains including llms and prompts\nclass LangChainContext(BaseModel):\n  llms: Dict[str, BaseLanguageModel]\n\n  verbose: bool = False\n  recording: bool = False\n  prompts: Dict[str, object] = []\n\n  def results(self, revision_id: str):\n    return [Result(\n        revision=revision_id,\n        chain_id=chain.chain_spec_id,\n        input=call[0],\n        output=call[1],\n        recorded=datetime.now()\n      ) for chain in self.prompts for call in chain.calls]\n\n  def reset_results(self):\n    for chain in self.prompts:\n      chain.reset()", "class LangChainContext(BaseModel):\n  llms: Dict[str, BaseLanguageModel]\n\n  verbose: bool = False\n  recording: bool = False\n  prompts: Dict[str, object] = []\n\n  def results(self, revision_id: str):\n    return [Result(\n        revision=revision_id,\n        chain_id=chain.chain_spec_id,\n        input=call[0],\n        output=call[1],\n        recorded=datetime.now()\n      ) for chain in self.prompts for call in chain.calls]\n\n  def reset_results(self):\n    for chain in self.prompts:\n      chain.reset()", ""]}
{"filename": "lib/model/tests/test_llm_spec.py", "chunked_list": ["import os\nfrom model.llm_spec import OpenAILLMSpec\n\ndef test_llm_spec_serialization():\n  llm = OpenAILLMSpec(model_name=\"davinci\", temperature=0.5, max_tokens=10, top_p=1.0, frequency_penalty=0.0,\n                      presence_penalty=0.0, n=1, request_timeout=10, logit_bias=None)\n\n  serialized = llm.json()\n  deserialized = OpenAILLMSpec.parse_raw(serialized)\n\n  assert deserialized == llm", ""]}
{"filename": "lib/model/tests/test_chain_spec.py", "chunked_list": ["from model.chain_spec import ChainSpec, LLMChainSpec, SequentialChainSpec, CaseChainSpec, APIChainSpec, ReformatChainSpec, TransformChainSpec, VectorSearchChainSpec\nfrom model.chain_revision import ChainRevision\nfrom model.lang_chain_context import LangChainContext\nfrom langchain.llms.fake import FakeListLLM\nfrom model.tests.factory import SpecFactoryContext, llm_factory, sequential_factory, case_factory\n\ndef test_llm_chain_spec_serialization():\n    llm_chain_spec = LLMChainSpec(\n        chain_id=1,\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output1\",\n        prompt=\"prompt\",\n        llm_key=\"llm_key\",\n        chain_type=\"llm_chain_spec\",\n    )\n    serialized = llm_chain_spec.json()\n    assert serialized == '{\"chain_id\": 1, \"input_keys\": [\"input1\", \"input2\"], \"output_key\": \"output1\", \"chain_type\": \"llm_chain_spec\", \"prompt\": \"prompt\", \"llm_key\": \"llm_key\"}'\n\n    revision = ChainRevision(chain=llm_chain_spec, llms={})\n    serialized_revision = revision.json()\n    deserialized = ChainRevision.parse_raw(serialized_revision).chain\n    assert deserialized == llm_chain_spec", "\n\ndef test_llm_chain_spec_to_lang_chain_creates_valid_chain():\n    prompt_template = \"the prompt {input1} {input2}\"\n    llm_chain_spec = LLMChainSpec(\n        chain_id=1,\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output1\",\n        prompt=prompt_template,\n        llm_key=\"test\",\n        chain_type=\"llm_chain_spec\",\n    )\n    llms = llms={\"test\": FakeListLLM(responses=[\"response1\"])}\n    ctx = LangChainContext(llms=llms)\n    llm_chain = llm_chain_spec.to_lang_chain(ctx)\n\n    assert llm_chain.input_keys == [\"input1\", \"input2\"]\n    assert llm_chain.output_key == \"output1\"\n    assert llm_chain.prompt.template == prompt_template\n    assert llm_chain.llm == llms[\"test\"]\n\n    output = llm_chain._call({\"input1\": \"input1\", \"input2\": \"input2\"})\n    assert output == {\"output1\": \"response1\"}", "\n\ndef test_llm_chain_spec_to_lang_chain_creates_recording_chain():\n    prompt_template = \"the prompt {input1} {input2}\"\n    llm_chain_spec = LLMChainSpec(\n        chain_id=1,\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output1\",\n        prompt=prompt_template,\n        llm_key=\"test\",\n        chain_type=\"llm_chain_spec\",\n    )\n    llms = llms={\"test\": FakeListLLM(responses=[\"response1\"])}\n    ctx = LangChainContext(llms=llms, recording=True)\n    llm_chain = llm_chain_spec.to_lang_chain(ctx)\n\n    assert len(ctx.prompts) == 1\n\n    output = llm_chain._call({\"input1\": \"input1\", \"input2\": \"input2\"})\n\n    assert output == {\"output1\": \"response1\"}\n    assert ctx.prompts[0].calls == [\n        ({\"input1\": \"input1\", \"input2\": \"input2\"}, \"response1\")\n    ]", "\n\ndef test_sequential_chain_spec_serialization():\n    sequential_chain_spec = SequentialChainSpec(\n        chain_id=1,\n        input_keys=[\"input1\", \"input2\"],\n        output_keys=[\"output1\", \"output2\"],\n        chain_type=\"sequential_chain_spec\",\n        chains=[\n            LLMChainSpec(\n                chain_id=1,\n                input_keys=[\"input1\", \"input2\"],\n                output_key=\"output1\",\n                prompt=\"prompt\",\n                llm_key=\"llm_key\",\n                chain_type=\"llm_chain_spec\"\n            )\n        ],\n    )\n    serialized = sequential_chain_spec.json()\n    deserialized = SequentialChainSpec.parse_raw(serialized)\n    assert deserialized == sequential_chain_spec ", "\n\ndef test_sequential_chain_spec_to_lang_chain_creates_valid_chain():\n    llm_chain_spec1 = LLMChainSpec(\n                chain_id=1,\n                input_keys=[\"input1\", \"input2\"],\n                output_key=\"llm1_output\",\n                prompt=\"the prompt {input1} {input2}\",\n                llm_key=\"test\",\n                chain_type=\"llm_chain_spec\",\n            )\n    llm_chain_spec2 = LLMChainSpec(\n                chain_id=2,\n                input_keys=[\"llm1_output\", \"input3\"],\n                output_key=\"llm2_output\",\n                prompt=\"the prompt {llm1_output} {input3}\",\n                llm_key=\"test\",\n                chain_type=\"llm_chain_spec\",\n            )\n\n    sequential_chain_spec = SequentialChainSpec(\n        chain_id=3,\n        input_keys=[\"input1\", \"input2\", \"input3\"],\n        output_keys=[\"llm2_output\"],\n        chain_type=\"sequential_chain_spec\",\n        chains=[llm_chain_spec1, llm_chain_spec2],\n    )\n\n    llms = llms={\"test\": FakeListLLM(responses=[\"fake_response1\", \"fake_response2\"])}\n    ctx = LangChainContext(llms=llms)\n    sequential_chain = sequential_chain_spec.to_lang_chain(ctx)\n\n    assert sequential_chain.input_keys == [\"input1\", \"input2\", \"input3\"]\n    assert sequential_chain.output_keys == [\"llm2_output\"]\n    assert sequential_chain.chains[0].input_keys == [\"input1\", \"input2\"]\n    assert sequential_chain.chains[0].output_keys == [\"llm1_output\"]\n    assert sequential_chain.chains[0].prompt.template == llm_chain_spec1.prompt\n    assert sequential_chain.chains[0].llm == llms[\"test\"]\n\n    assert sequential_chain.chains[1].input_keys == [\"llm1_output\", \"input3\"]\n    assert sequential_chain.chains[1].output_keys == [\"llm2_output\"]\n    assert sequential_chain.chains[1].prompt.template == llm_chain_spec2.prompt\n    assert sequential_chain.chains[1].llm == llms[\"test\"]\n\n    output = sequential_chain.run({\"input1\": \"input1\", \"input2\": \"input2\", \"input3\": \"input3\"})\n    assert output == \"fake_response2\"", "\n\ndef test_case_chain_spec_serialization():\n    case_chain_spec = CaseChainSpec(\n        chain_id=3,\n        chain_type=\"case_chain_spec\",\n        categorization_key=\"categorization_input\",\n        cases={\n            \"response1\": LLMChainSpec(\n                chain_id=1,\n                input_keys=[\"input1\", \"input2\"],\n                output_key=\"output\",\n                prompt=\"prompt\",\n                llm_key=\"test\",\n                chain_type=\"llm_chain_spec\"\n            ),\n            \"response2\": LLMChainSpec(\n                chain_id=2,\n                input_keys=[\"input3\", \"input4\"],\n                output_key=\"output\",\n                prompt=\"prompt\",\n                llm_key=\"test\",\n                chain_type=\"llm_chain_spec\"\n            )\n        },\n        default_case=LLMChainSpec(\n            chain_id=4,\n            input_keys=[\"input1\", \"input2\"],\n            output_key=\"output\",\n            prompt=\"prompt\",\n            llm_key=\"test\",\n            chain_type=\"llm_chain_spec\"\n        ),\n    )\n    serialized = case_chain_spec.json()\n    deserialized = CaseChainSpec.parse_raw(serialized)\n    assert deserialized == case_chain_spec", "\n\ndef test_case_chain_spec_to_lang_chain_creates_valid_chain():\n    llms = {\"test1\": FakeListLLM(responses=[\"fake_response1\"]), \"test2\": FakeListLLM(responses=[\"fake_response2\"])}\n    llm_chain_spec1 = LLMChainSpec(\n                chain_id=1,\n                input_keys=[\"input1\", \"input2\"],\n                output_key=\"output\",\n                prompt=\"ll1 prompt {input1} {input2}\",\n                llm_key=\"test1\",\n                chain_type=\"llm_chain_spec\",\n            )\n    llm_chain_spec2 = LLMChainSpec(\n                chain_id=2,\n                input_keys=[\"input3\", \"input4\"],\n                output_key=\"output\",\n                prompt=\"llm2 prompt {input3} {input4}\",\n                llm_key=\"test2\",\n                chain_type=\"llm_chain_spec\",\n            )\n    llm_chain_spec3 = LLMChainSpec(\n                chain_id=4,\n                input_keys=[\"input1\", \"input2\"],\n                output_key=\"output\",\n                prompt=\"default prompt {input1} {input2}\",\n                llm_key=\"test1\",\n                chain_type=\"llm_chain_spec\"\n            )\n\n    case_chain_spec = CaseChainSpec(\n        chain_id=3,\n        chain_type=\"case_chain_spec\",  \n        categorization_key=\"categorization_input\",      \n        cases={\n            \"response1\": llm_chain_spec1,\n            \"response2\": llm_chain_spec2,\n        },\n        default_case=llm_chain_spec3,\n    )\n\n    ctx = LangChainContext(llms=llms)\n    case_chain = case_chain_spec.to_lang_chain(ctx)\n\n    assert case_chain.input_keys == [\"categorization_input\", \"input1\", \"input2\", \"input3\", \"input4\"]\n    assert case_chain.output_keys == [\"output\"]\n    assert case_chain.subchains[\"response1\"].input_keys == [\"input1\", \"input2\"]\n    assert case_chain.subchains[\"response1\"].output_keys == [\"output\"]\n    assert case_chain.subchains[\"response1\"].prompt.template == llm_chain_spec1.prompt\n    assert case_chain.subchains[\"response1\"].llm == llms[\"test1\"]\n\n    assert case_chain.subchains[\"response2\"].input_keys == [\"input3\", \"input4\"]\n    assert case_chain.subchains[\"response2\"].output_keys == [\"output\"]\n    assert case_chain.subchains[\"response2\"].prompt.template == llm_chain_spec2.prompt\n    assert case_chain.subchains[\"response2\"].llm == llms[\"test2\"]\n\n    assert case_chain.default_chain.input_keys == [\"input1\", \"input2\"]\n    assert case_chain.default_chain.output_keys == [\"output\"]\n    assert case_chain.default_chain.prompt.template == llm_chain_spec3.prompt\n    assert case_chain.default_chain.llm == llms[\"test1\"]\n\n    output = case_chain.run({\n        \"categorization_input\": \"response2\",\n        \"input1\": \"input1\", \n        \"input2\": \"input2\",\n        \"input3\": \"input3\",\n        \"input4\": \"input4\"\n    })\n    assert output == \"fake_response2\"", "\n\ndef test_api_chain_spec_serialization():\n    api_chain_spec = APIChainSpec(\n        chain_id=3,\n        chain_type=\"api_chain_spec\",\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output\",\n        url=\"http://test.com\",\n        method=\"POST\",\n        body=\"body {input1} {input2}\",\n        headers={\"header1\": \"header1\"},\n    )\n\n    revision = ChainRevision(chain=api_chain_spec, llms={})\n    serialized_revision = revision.json()\n    deserialized = ChainRevision.parse_raw(serialized_revision).chain\n    assert deserialized == api_chain_spec", "\n\ndef test_api_chain_spec_to_lang_chain_creates_valid_chain():\n    api_chain_spec = APIChainSpec(\n        chain_id=3,\n        chain_type=\"api_chain_spec\",\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output\",\n        url=\"http://test.com\",\n        method=\"POST\",\n        body=\"body {input1} {input2}\",\n        headers={\"header1\": \"header1\"},\n    )\n    ctx = LangChainContext(llms={})\n    api_chain = api_chain_spec.to_lang_chain(ctx)\n\n    assert api_chain.input_keys == [\"input1\", \"input2\"]\n    assert api_chain.output_keys == [\"output\"]\n    assert api_chain.url == \"http://test.com\"\n    assert api_chain.method == \"POST\"\n    assert api_chain.body == \"body {input1} {input2}\"\n    assert api_chain.headers == {\"header1\": \"header1\"}", "\n\ndef test_reformat_chain_spec_serialization():\n    reformat_chain_spec = ReformatChainSpec(\n        chain_id=3,\n        chain_type=\"reformat_chain_spec\",\n        input_keys=[\"input1\", \"input2\"],\n        formatters={\"output1\": \"formatter1\", \"output2\": \"formatter2\"},\n    )\n    serialized = reformat_chain_spec.json()\n    deserialized = ReformatChainSpec.parse_raw(serialized)\n    assert deserialized == reformat_chain_spec", "\n\ndef test_reformat_chain_spec_to_lang_chain_creates_valid_chain():\n    reformat_chain_spec = ReformatChainSpec(\n        chain_id=3,\n        chain_type=\"reformat_chain_spec\",\n        input_keys=[\"input1\", \"input2\"],\n        formatters={\"output1\": \"formatter1\", \"output2\": \"formatter2\"},\n    )\n    ctx = LangChainContext(llms={})\n    reformat_chain = reformat_chain_spec.to_lang_chain(ctx)\n\n    assert reformat_chain.input_keys == [\"input1\", \"input2\"]\n    assert reformat_chain.output_keys == [\"output1\", \"output2\"]\n    assert reformat_chain.formatters == {\"output1\": \"formatter1\", \"output2\": \"formatter2\"}", "\n\ndef test_transform_chain_spec_to_lang_chain_creates_valid_chain():\n    transform_chain_spec = TransformChainSpec(\n        chain_id=4,\n        chain_type=\"transform_chain_spec\",\n        input_keys=[\"x\", \"y\"],\n        output_keys=[\"z\", \"w\"],\n        transform_func=\"\"\"\nz = int(inputs['x']) + int(inputs['y'])\nw = int(inputs['x']) - int(inputs['y'])\nreturn {'z': z, 'w': w}\"\"\"\n    )\n    ctx = LangChainContext(llms={})\n    transform_chain = transform_chain_spec.to_lang_chain(ctx)\n\n    assert transform_chain.input_keys == [\"x\", \"y\"]\n    assert transform_chain.output_keys == [\"z\", \"w\"]\n    assert transform_chain.transform({'x': 4, 'y': 3}) == {'z': 7, 'w': 1}", "\n\ndef test_vector_search_chain_spec_serialization():\n    vector_search_chain_spec = VectorSearchChainSpec(\n        chain_id=3,\n        chain_type=\"vector_search_chain_spec\",\n        input_keys=[\"input1\", \"input2\"],\n        output_key=\"output\",\n        embedding_engine=\"openai\",\n        embedding_params={\"openai_api_key\": \"test\"},\n        database=\"pinecone\",\n        database_params={\"index\": \"test\", \"text_key\": \"text\"},\n        num_results=10,\n        query=\"How does {input}?\"\n    )\n    serialized = vector_search_chain_spec.json()\n    deserialized = VectorSearchChainSpec.parse_raw(serialized)\n    assert deserialized == vector_search_chain_spec", "\n\nclass ChainDict:\n    def __init__(self):\n        self.chains = {}\n    \n    def add_chain(self, chain):\n        self.chains[chain.chain_id] = chain\n\n\ndef test_chain_spec_copy_replace():\n    ctx = SpecFactoryContext()\n    chain = sequential_factory(ctx, chains=[\n        llm_factory(ctx),\n        case_factory(ctx, cases={\n            \"case1\": llm_factory(ctx),\n            \"case2\": sequential_factory(ctx, chains=[llm_factory(ctx), llm_factory(ctx)]),\n        }, default_case=llm_factory(ctx)),\n    ])\n\n    original_specs = ChainDict()\n    chain.traverse(original_specs.add_chain)\n    copied_specs = {}\n    \n    copied_chain = chain.copy_replace(lambda spec: spec)\n    copied_specs = ChainDict()\n    copied_chain.traverse(copied_specs.add_chain)\n\n    assert len(original_specs.chains) == len(copied_specs.chains)\n\n    for chain_id, spec in original_specs.chains.items():\n        copied_spec = copied_specs.chains.get(chain_id)\n        assert copied_spec is not None\n        assert copied_spec == spec\n        assert copied_spec is not spec\n\n    replacement = copied_specs.chains[3]\n    replacement.prompt = \"replaced\"\n    replace_chain = chain.copy_replace(lambda spec: spec if spec.chain_id != 3 else replacement)\n    replace_specs = ChainDict()\n    replace_chain.traverse(replace_specs.add_chain)\n\n    assert len(original_specs.chains) == len(replace_specs.chains)\n    assert replace_specs.chains[3] == replacement\n    assert replace_specs.chains[3].prompt == \"replaced\"", "\n\ndef test_chain_spec_copy_replace():\n    ctx = SpecFactoryContext()\n    chain = sequential_factory(ctx, chains=[\n        llm_factory(ctx),\n        case_factory(ctx, cases={\n            \"case1\": llm_factory(ctx),\n            \"case2\": sequential_factory(ctx, chains=[llm_factory(ctx), llm_factory(ctx)]),\n        }, default_case=llm_factory(ctx)),\n    ])\n\n    original_specs = ChainDict()\n    chain.traverse(original_specs.add_chain)\n    copied_specs = {}\n    \n    copied_chain = chain.copy_replace(lambda spec: spec)\n    copied_specs = ChainDict()\n    copied_chain.traverse(copied_specs.add_chain)\n\n    assert len(original_specs.chains) == len(copied_specs.chains)\n\n    for chain_id, spec in original_specs.chains.items():\n        copied_spec = copied_specs.chains.get(chain_id)\n        assert copied_spec is not None\n        assert copied_spec == spec\n        assert copied_spec is not spec\n\n    replacement = copied_specs.chains[3]\n    replacement.prompt = \"replaced\"\n    replace_chain = chain.copy_replace(lambda spec: spec if spec.chain_id != 3 else replacement)\n    replace_specs = ChainDict()\n    replace_chain.traverse(replace_specs.add_chain)\n\n    assert len(original_specs.chains) == len(replace_specs.chains)\n    assert replace_specs.chains[3] == replacement\n    assert replace_specs.chains[3].prompt == \"replaced\"", "\n\ndef test_base_spec_find_by_chain_id():\n    ctx = SpecFactoryContext()\n    deep_llm = llm_factory(ctx)\n    chain = sequential_factory(ctx, chains=[\n        llm_factory(ctx),\n        case_factory(ctx, cases={\n            \"case1\": llm_factory(ctx),\n            \"case2\": sequential_factory(ctx, chains=[llm_factory(ctx), deep_llm]),\n        }, default_case=llm_factory(ctx)),\n    ])\n\n    assert chain.find_by_chain_id(deep_llm.chain_id) == deep_llm"]}
{"filename": "lib/model/tests/__init__.py", "chunked_list": [""]}
{"filename": "lib/model/tests/test_chain_revision.py", "chunked_list": ["import os\nfrom model.chain_revision import ChainRevision\nfrom model.chain_spec import LLMChainSpec\nfrom model.llm_spec import OpenAILLMSpec\n\n\ndef test_chain_revision_serialization():\n  if os.environ.get(\"OPENAI_API_KEY\") is None:\n    os.environ[\"OPENAI_API_KEY\"] = \"set me!\"\n\n  llm = OpenAILLMSpec(model_name=\"davinci\", temperature=0.5, max_tokens=10, top_p=1.0, frequency_penalty=0.0,\n                      presence_penalty=0.0, n=1, request_timeout=10, logit_bias=None)\n\n  chain_revision = ChainRevision(\n      chain_id=1,\n      chain=LLMChainSpec(\n          chain_id=1,\n          input_keys=[\"input1\", \"input2\"],\n          output_key=\"output1\",\n          prompt=\"prompt\",\n          llm_key=\"llm_key\",\n          chain_type=\"llm_chain_spec\",\n      ),\n      llms={\"llm\": llm},\n  )\n\n  serialized = chain_revision.json()\n  deserialized = ChainRevision.parse_raw(serialized)\n  assert deserialized == chain_revision", ""]}
{"filename": "lib/model/tests/factory.py", "chunked_list": ["from model.chain_spec import LLMChainSpec, SequentialChainSpec, CaseChainSpec, APIChainSpec, ReformatChainSpec\n\nclass SpecFactoryContext:\n  def __init__(self):\n    self._next_id = 0\n\n  def next_id(self):\n    self._next_id += 1\n    return self._next_id\n", "\n\ndef llm_factory(ctx, **kwargs) -> LLMChainSpec:\n  return LLMChainSpec(\n    chain_id=ctx.next_id(),\n    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n    output_key=kwargs.get(\"output_key\", \"output1\"),\n    prompt=kwargs.get(\"prompt\", \"prompt {input1} {input2}\"),\n    llm_key=kwargs.get(\"llm_key\", \"llm_key\"),\n    chain_type=\"llm_chain_spec\",\n  )", "\ndef sequential_factory(ctx, **kwargs) -> SequentialChainSpec:\n  return SequentialChainSpec(\n    chain_id=ctx.next_id(),\n    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n    output_keys=kwargs.get(\"output_keys\", [\"output1\", \"output2\"]),\n    chain_type=\"sequential_chain_spec\",\n    chains=kwargs.get(\"chains\", []),\n  )\n\ndef case_factory(ctx, **kwargs) -> CaseChainSpec:\n  return CaseChainSpec(\n    chain_id=ctx.next_id(),\n    chain_type=\"case_chain_spec\",\n    categorization_key=kwargs.get(\"categorization_key\", \"categorization_input\"),\n    cases=kwargs.get(\"cases\", {}),\n    default_case=kwargs.get(\"default_case\", None),\n  )", "\ndef case_factory(ctx, **kwargs) -> CaseChainSpec:\n  return CaseChainSpec(\n    chain_id=ctx.next_id(),\n    chain_type=\"case_chain_spec\",\n    categorization_key=kwargs.get(\"categorization_key\", \"categorization_input\"),\n    cases=kwargs.get(\"cases\", {}),\n    default_case=kwargs.get(\"default_case\", None),\n  )\n\ndef api_factory(ctx, **kwargs) -> APIChainSpec:\n  return APIChainSpec(\n    chain_id=ctx.next_id(),\n    chain_type=\"api_chain_spec\",\n    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n    output_key=kwargs.get(\"output_key\", \"output1\"),\n    url=kwargs.get(\"url\", \"http://test.com\"),\n    method=kwargs.get(\"method\", \"POST\"),\n    body=kwargs.get(\"body\", \"body {input1} {input2}\"),\n    headers=kwargs.get(\"headers\", {\"header1\": \"header1\"}),\n  )", "\ndef api_factory(ctx, **kwargs) -> APIChainSpec:\n  return APIChainSpec(\n    chain_id=ctx.next_id(),\n    chain_type=\"api_chain_spec\",\n    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n    output_key=kwargs.get(\"output_key\", \"output1\"),\n    url=kwargs.get(\"url\", \"http://test.com\"),\n    method=kwargs.get(\"method\", \"POST\"),\n    body=kwargs.get(\"body\", \"body {input1} {input2}\"),\n    headers=kwargs.get(\"headers\", {\"header1\": \"header1\"}),\n  )", "\ndef reformat_factory(ctx, **kwargs) -> ReformatChainSpec:\n  return ReformatChainSpec(\n    chain_id=ctx.next_id(),\n    chain_type=\"reformat_chain_spec\",\n    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n    formatters=kwargs.get(\"formatters\", {\"output1\": \"formatter1\", \"output2\": \"formatter2\"}),\n  )\n", ""]}
{"filename": "server/__init__.py", "chunked_list": [""]}
{"filename": "server/testbench.py", "chunked_list": ["import logging\n\nimport json\nfrom flask import Flask, Response, request\nfrom bson.json_util import dumps\nfrom flask_cors import CORS\nfrom lib.model.chain_revision import ChainRevision\nfrom werkzeug.exceptions import BadRequest\nfrom pydantic.json import pydantic_encoder\nfrom pydantic import parse_obj_as", "from pydantic.json import pydantic_encoder\nfrom pydantic import parse_obj_as\nfrom typing import List\nimport lib.chain_service as chain_service\n\nlogging.basicConfig(level=logging.INFO)\napp = Flask(__name__)\n\nCORS(app)\n", "CORS(app)\n\n@app.route(\"/\")\ndef check():\n  return Response('\"ok\"', mimetype=\"application/json\")\n\n# Create a REST API for the chain service\n@app.route(\"/chains\", methods=[\"GET\"])\ndef list_chains():\n  chains = chain_service.list_chains()\n  return Response(dumps(chains), mimetype=\"application/json\")", "def list_chains():\n  chains = chain_service.list_chains()\n  return Response(dumps(chains), mimetype=\"application/json\")\n\n@app.route(\"/chain/<chain_name>/revision\", methods=[\"POST\"])\ndef save_revision(chain_name):\n  try:\n    next_revision = ChainRevision.parse_raw(request.data)\n  except Exception as e:\n    print(\"ERROR parsing revision:\", e)\n    return {\"error\": str(e)}, 400\n\n  revision_id = chain_service.save_revision(chain_name, next_revision)\n  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")", "\n@app.route(\"/chain/<chain_name>/revision\", methods=[\"GET\"])\ndef load_by_chain_name(chain_name):\n  revision = chain_service.load_by_chain_name(chain_name)\n  return Response(revision.json(), mimetype=\"application/json\")\n\n@app.route(\"/chain/<chain_name>/history\", methods=[\"GET\"])\ndef history_by_chain_name(chain_name):\n  history = chain_service.history_by_chain_name(chain_name)\n  return Response(dumps(history), mimetype=\"application/json\")", "\n@app.route(\"/chain/<chain_name>/patch\", methods=[\"POST\"])\ndef save_patch(chain_name):\n  revision_id = chain_service.save_patch(chain_name, request.json)\n  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")\n\n@app.route(\"/chain/<revision_id>\", methods=[\"GET\"])\ndef load_by_id(revision_id):\n  revision_id = chain_service.load_by_id(revision_id)\n  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")", "\n@app.route(\"/chain/<chain_name>/results\", methods=[\"GET\"])\ndef load_results_by_chain_name(chain_name):\n  results = chain_service.load_results_by_chain_name(chain_name)\n  return Response(dumps(results), mimetype=\"application/json\")\n\n@app.route(\"/chain/<chain_name>/run\", methods=[\"POST\"])\ndef run_chain(chain_name):\n  input = request.json\n  result = chain_service.run_once(chain_name, input, False)\n  return Response(dumps(result), mimetype=\"application/json\")", "\nif __name__ == \"__main__\":\n  app.run(debug=True)\n\n@app.route(\"/chain/<chain_name>/export\", methods=[\"GET\"])\ndef export_chain(chain_name):\n    exported_chain = chain_service.export_chain(chain_name)\n\n    if not exported_chain:\n      return Response(\n          dumps({\"error\": f\"Chain '{chain_name}' not found.\"}),\n          mimetype=\"application/json\",\n          status=404\n      )\n\n    chain_json = '[' + ','.join(revision.json() for revision in exported_chain) + ']'\n    return Response(\n        chain_json,\n        mimetype=\"application/json\"\n    )", "    \n\n@app.route(\"/chain/<chain_name>/import\", methods=[\"POST\"])\ndef import_chain_route(chain_name):\n    logging.info(f\"Importing chain '{chain_name}'\")\n    try:\n        json_data = request.get_json()\n        if json_data is None:\n            raise BadRequest(\"JSON data not present in request\")\n\n        revisions = parse_obj_as(List[ChainRevision], json_data) \n\n        chain_service.import_chain(chain_name, revisions)\n        return Response(\n            dumps({\"success\": f\"Import of '{chain_name}' successful.\"}),\n            mimetype=\"application/json\",\n            status=200\n        )\n    except Exception as e:\n        logging.error(f\"Import of '{chain_name}' failed. Reason: {str(e)}\")\n        return Response(\n            dumps({\"error\": f\"Import of '{chain_name}' failed. Reason: {str(e)}\"}),\n            mimetype=\"application/json\",\n            status=400\n        )", "\n\ndef allowed_file(filename):\n    ALLOWED_EXTENSIONS = {'json'}\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS"]}
