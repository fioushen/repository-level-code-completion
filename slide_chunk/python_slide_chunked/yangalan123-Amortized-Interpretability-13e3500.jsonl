{"filename": "visualization_intro.py", "chunked_list": ["import os\nimport numpy as np\nfrom collections import Counter\n\nimport thermostat\nfrom tqdm import trange\nfrom scipy.stats import spearmanr\n\nif __name__ == '__main__':\n    data_cache_dir = \"./datasets/yelp_polarity_vis\"\n    data1 = thermostat.load(\"yelp_polarity_amortized_model_output\", data_cache_dir=data_cache_dir)\n    data2 = thermostat.load(\"yelp_polarity_amortized_model_reference\", data_cache_dir=data_cache_dir)\n    tag1 = \"Amortized Model\"\n    tag2 = \"SVS-25\"\n    output_dir = \"visualization/error_analysis_htmls\"\n    os.makedirs(output_dir, exist_ok=True)\n    vocabulary = Counter()\n    bad_vocab = Counter()\n    vocab_to_spearman = dict()\n    for i in trange(min(len(data1), len(data2))):\n        instance1 = data1[i]\n        words = [x[0] for x in instance1.explanation]\n        for word in words:\n            vocabulary[word] += 1\n\n    for i in trange(min(len(data1), len(data2))):\n        instance1 = data1[i]\n        instance2 = data2[i]\n        attr1 = [x[1] for x in instance1.explanation]\n        attr2 = [x[1] for x in instance2.explanation]\n        words = [x[0] for x in instance1.explanation]\n        word_set = set(words)\n        assert len(attr1) == len(attr2)\n        s, p = spearmanr(attr1, attr2)\n        for word in word_set:\n            if word not in vocab_to_spearman:\n                vocab_to_spearman[word] = []\n            vocab_to_spearman[word].append(s)\n        if s < 0.5:\n            for word in word_set:\n                bad_vocab[word] += 1\n    word_freqs = vocabulary.most_common()\n    _counter = 0\n    word_counts = sum([x[1] for x in word_freqs])\n    for word in reversed(word_freqs):\n        if word[1] <= 5:\n            continue\n        if _counter >= 20:\n            break\n        print(word, np.mean(vocab_to_spearman[word[0]]))\n        _counter += 1\n\n    for word in bad_vocab.most_common()[:30]:\n        print(word[0], word[1], word[1]/min(len(data1), len(data2)))", "if __name__ == '__main__':\n    data_cache_dir = \"./datasets/yelp_polarity_vis\"\n    data1 = thermostat.load(\"yelp_polarity_amortized_model_output\", data_cache_dir=data_cache_dir)\n    data2 = thermostat.load(\"yelp_polarity_amortized_model_reference\", data_cache_dir=data_cache_dir)\n    tag1 = \"Amortized Model\"\n    tag2 = \"SVS-25\"\n    output_dir = \"visualization/error_analysis_htmls\"\n    os.makedirs(output_dir, exist_ok=True)\n    vocabulary = Counter()\n    bad_vocab = Counter()\n    vocab_to_spearman = dict()\n    for i in trange(min(len(data1), len(data2))):\n        instance1 = data1[i]\n        words = [x[0] for x in instance1.explanation]\n        for word in words:\n            vocabulary[word] += 1\n\n    for i in trange(min(len(data1), len(data2))):\n        instance1 = data1[i]\n        instance2 = data2[i]\n        attr1 = [x[1] for x in instance1.explanation]\n        attr2 = [x[1] for x in instance2.explanation]\n        words = [x[0] for x in instance1.explanation]\n        word_set = set(words)\n        assert len(attr1) == len(attr2)\n        s, p = spearmanr(attr1, attr2)\n        for word in word_set:\n            if word not in vocab_to_spearman:\n                vocab_to_spearman[word] = []\n            vocab_to_spearman[word].append(s)\n        if s < 0.5:\n            for word in word_set:\n                bad_vocab[word] += 1\n    word_freqs = vocabulary.most_common()\n    _counter = 0\n    word_counts = sum([x[1] for x in word_freqs])\n    for word in reversed(word_freqs):\n        if word[1] <= 5:\n            continue\n        if _counter >= 20:\n            break\n        print(word, np.mean(vocab_to_spearman[word[0]]))\n        _counter += 1\n\n    for word in bad_vocab.most_common()[:30]:\n        print(word[0], word[1], word[1]/min(len(data1), len(data2)))", "\n        # hm1 = instance1.heatmap\n        # html1 = hm1.render()\n        # hm2 = instance2.heatmap\n        # html2 = hm2.render()\n        # f1 = open(os.path.join(output_dir, f\"output_{i}.html\"), \"w\", encoding='utf-8')\n        # # f1.write('<p style=\"font-size: 1.5em; \">Seed = 1</p>\\n')\n        # f1.write(f'<p style=\"font-size: 1.5em; \">spearman: {s:.2f} ({p:.2f})</p>\\n')\n        # f1.write(f'<p style=\"font-size: 1.5em; \">{tag1}</p>\\n')\n        # f1.write(html1 + \"\\n\")", "        # f1.write(f'<p style=\"font-size: 1.5em; \">{tag1}</p>\\n')\n        # f1.write(html1 + \"\\n\")\n        # f1.write(f'<p style=\"font-size: 1.5em; \">{tag2}</p>\\n')\n        # f1.write(html2)\n        # f1.close()\n\n    # print(html1)\n    # html1 = instance1.render()\n    # print(html1)\n    # html2 = instance2.render()", "    # print(html1)\n    # html2 = instance2.render()\n\n"]}
{"filename": "feature_selection.py", "chunked_list": ["import json\nimport torch\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.stats import spearmanr\nimport os\nimport glob\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\nfrom utils import sort_by_file_size", "from datasets import load_dataset, load_from_disk\nfrom utils import sort_by_file_size\n\n\n#task_name = \"yelp_polarity\"\ntask_name = \"mnli\"\nif task_name == \"yelp_polarity\":\n    model_name = \"textattack/bert-base-uncased-yelp-polarity\"\n    dataset = load_from_disk(\"thermostat/experiments/thermostat/datasets/yelp_polarity\")\n    candidates = [\"kernelshap-3600\", \"kernelshap-3600-sample200\", \"kernelshap-500-sample2000\",\n                  \"kernelshap-500-sample8000\", \"svs-3600\", \"lime\", \"lime-200\"]\nelif task_name == \"mnli\":\n    # original textattack model has the problem of label mismatch, fixed by myself,\n    # see issues at: https://github.com/QData/TextAttack/issues/684.\n    # The fixed accuracy_mm is 84.44% and is 7% before the fix applied.\n    model_name = \"chromeNLP/textattack_bert_base_MNLI_fixed\"\n    dataset = load_from_disk(\"thermostat/experiments/thermostat/datasets/multi_nli\")\n    candidates = [\"kernelshap-2000\", \"kernelshap-2000-sample200\", \"kernelshap-2000-sample2000\", \"kernelshap-2000-sample8000\", \"lime-2000\", \"lime-2000-sample200\", \"svs-2000\"]\nelse:\n    raise NotImplementedError", "\n# model_name = \"textattack/bert-base-uncased-imdb\"\n# model_name = \"textattack/bert-base-uncased-MNLI\"\n# model_name = \"textattack/bert-base-uncased-yelp-polarity\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name).cuda()\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\ndef textattack_mnli_label_mapping(label):\n    label_mapping_from_model_to_gt = {\n        0: 2,\n        1: 0,\n        2: 1\n    }\n    return label_mapping_from_model_to_gt[label]", "def textattack_mnli_label_mapping(label):\n    label_mapping_from_model_to_gt = {\n        0: 2,\n        1: 0,\n        2: 1\n    }\n    return label_mapping_from_model_to_gt[label]\n\n\nif model_name == \"textattack/bert-base-uncased-MNLI\":\n    label_mapping = textattack_mnli_label_mapping\nelse:\n    label_mapping = lambda x: x", "\nif model_name == \"textattack/bert-base-uncased-MNLI\":\n    label_mapping = textattack_mnli_label_mapping\nelse:\n    label_mapping = lambda x: x\n# for explainer in [\"kernelshap-2000-sample2000\",]:\ndef get_eraser_performance(attribution, model, input_data, tokenizer, label):\n    top_indexes = list(attribution.argsort()[::-1])\n    num_actual_tokens = input_data[\"attention_mask\"].sum().item()\n    res = []\n    res.append(int(model(**{k: v.cuda() for k, v in input_data.items()})[0].argmax().item() == label))\n    for topP in [0.01, 0.05, 0.10, 0.20, 0.50]:\n        num_token_masked = int(num_actual_tokens * topP)\n        # num_token_masked = topP\n        token_masked = torch.LongTensor(top_indexes[: num_token_masked])\n        _input_ids = input_data['input_ids'].clone()\n        _input_ids[0][token_masked] = tokenizer.pad_token_id\n        _output = model(\n            input_ids=_input_ids.cuda(),\n            attention_mask=input_data[\"attention_mask\"].cuda(),\n            token_type_ids=input_data[\"token_type_ids\"].cuda()\n        )[0]\n        global label_mapping\n        res.append(1 if label_mapping(_output.argmax().item()) == label else 0)\n    return res", "\n\nfor explainer in candidates:\n    if task_name == \"mnli\":\n        path = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/{explainer}/\"\n    elif \"yelp\" in task_name:\n        path = f\"path/to/thermostat/experiments/thermostat/yelp_polarity/bert/{explainer}/\"\n    print(\"NOW evaluating:\", explainer)\n    seed_dirs = glob.glob(path + \"seed_*\")\n    all_correlations = []\n    all_ps = []\n    all_mask_check = 0\n    seed_dir0 = os.path.join(path, seed_dirs[0])\n    seed_file0 = sort_by_file_size(glob.glob(os.path.join(seed_dir0, \"*.jsonl\")))[0]\n    seed_file_path0 = os.path.join(seed_dir0, seed_file0)\n    print(seed_file_path0)\n    all_eraser_res = []\n    for _seed_dir1 in seed_dirs[1: ]:\n        seed_dir1 = os.path.join(path, _seed_dir1)\n        seed_file1 = sort_by_file_size(glob.glob(os.path.join(seed_dir1, \"*.jsonl\")))[0]\n        seed_file_path1 = os.path.join(seed_dir1, seed_file1)\n        print(seed_file_path1)\n        topks = {1: [], 5: [], 10: [], 20: []}\n        all_eraser_res0 = []\n        all_eraser_res1 = []\n        try:\n            with open(seed_file_path0, \"r\", encoding='utf-8') as f_in0, open(seed_file_path1, \"r\", encoding='utf-8') as f_in1:\n                buf0, buf1 = f_in0.readlines(), f_in1.readlines()\n                buf0, buf1 = buf0[:500], buf1[:500]\n                id_counter = 0\n                acc_num = 0\n                for line0, line1 in tqdm(zip(buf0, buf1), total=len(buf0)):\n                    obj0, obj1 = json.loads(line0), json.loads(line1)\n                    attr0, attr1 = obj0[\"attributions\"], obj1[\"attributions\"]\n                    attr0, attr1 = np.array(attr0), np.array(attr1)\n                    assert (obj0[\"dataset\"].get(\"start\", 0) + obj0[\"index_running\"]) == (\n                            obj1[\"dataset\"].get(\"start\", 0) + obj1[\"index_running\"])\n                    data_id = obj0[\"dataset\"].get(\"start\", 0) + obj0[\"index_running\"]\n                    assert ((torch.LongTensor(obj0[\"input_ids\"])) == (torch.LongTensor(obj1[\"input_ids\"]))).all()\n                    assert obj0[\"label\"] == obj1['label']\n                    if ((attr0 == 0) != (attr1 == 0)).any():\n                        all_mask_check += 1\n\n                    in0, in1 = obj0[\"input_ids\"], obj1[\"input_ids\"]\n                    acc_num += 1 if label_mapping(torch.tensor(obj0['predictions']).argmax().item()) == obj0['label'] else 0\n                    assert in0 == in1\n                    postfix = sum(np.array(in0) == 0)\n                    if postfix > 0:\n                        attr0_pruned = attr0[:-postfix]\n                        attr1_pruned = attr1[:-postfix]\n                        in0_pruned = in0[:-postfix]\n                        in1_pruned = in1[:-postfix]\n                    sort0 = attr0.argsort()\n                    sort1 = attr1.argsort()\n\n                    real_instance = dataset[data_id]\n                    if \"nli\" in path:\n                        _input = tokenizer(real_instance[\"premise\"], real_instance[\"hypothesis\"],\n                                           truncation=obj0['model']['tokenization'][\"truncation\"],\n                                           max_length=obj0['model']['tokenization']['max_length'],\n                                           padding=obj0['model'][\"tokenization\"][\"padding\"],\n                                           return_tensors=\"pt\"\n                                           )\n                    else:\n                        _input = tokenizer(real_instance[\"text\"],\n                                           truncation=obj0['model']['tokenization'][\"truncation\"],\n                                           max_length=obj0['model']['tokenization']['max_length'],\n                                           padding=obj0['model'][\"tokenization\"][\"padding\"],\n                                           return_tensors=\"pt\"\n                                           )\n                    # assert (torch.LongTensor(_input[\"input_ids\"].tolist()) == torch.LongTensor(obj0[\"input_ids\"])).all()\n                    assert obj0[\"label\"] == obj1[\"label\"]\n                    # assert (torch.LongTensor(_input[\"input_ids\"].tolist()) == torch.LongTensor(in0_pruned)).all()\n\n                    res0 = get_eraser_performance(attr0, model, _input, tokenizer, obj0['label'])\n                    res1 = get_eraser_performance(attr1, model, _input, tokenizer, obj1['label'])\n                    all_eraser_res0.append(res0)\n                    all_eraser_res1.append(res1)\n\n                    _spearman, _pval = spearmanr(attr0_pruned, attr1_pruned)\n                    all_correlations.append(_spearman)\n                    all_ps.append(_pval)\n                    for key in topks:\n                        topk_intersection = set(sort0[::-1][:key].tolist()) & set(sort1[::-1][:key].tolist())\n                        topk_intersection = [in0[x] for x in sorted(topk_intersection)]\n                        _topk = len(topk_intersection)\n                        topks[key].append(_topk)\n                    id_counter += 1\n            print(\"acc:\", acc_num / len(buf0))\n            print(\n                f\"spearman correlation: {np.mean(all_correlations)} ({np.std(all_correlations)}, {np.min(all_correlations)}, {np.max(all_correlations)})\", )\n            print(f\"spearman ps: {np.mean(all_ps)} ({np.std(all_ps)})\", )\n            print(f\"mask mismatch rate: {all_mask_check / len(all_ps)}\")\n            for key in topks:\n                print(f\"top{key}: {np.mean(topks[key])}\")\n            # print('feat selection res:')\n            if len(all_eraser_res) == 0:\n                all_eraser_res.append(torch.tensor(all_eraser_res0).float().mean(dim=0))\n            all_eraser_res.append(torch.tensor(all_eraser_res1).float().mean(dim=0))\n        except AssertionError as e:\n            print(e)\n            print(\"assertion error, skip ...\")\n    print('feat selection res:')\n    print(\"sample0\")\n    print(all_eraser_res[0])\n    all_eraser_res = torch.stack(all_eraser_res, dim=0)\n    print(\"mean\")\n    print(all_eraser_res.mean(dim=0))\n    print(\"std\")\n    print(all_eraser_res.std(dim=0))", ""]}
{"filename": "samplers.py", "chunked_list": ["import torch\nfrom torch.distributions.categorical import Categorical\n# from fastshap codebase\nimport numpy as np\nfrom scipy.special import binom\n# Shapley Sampler is from FastSHAP Codebase: https://github.com/iancovert/fastshap\n# we thank the authors for releasing their codebase\nclass ShapleySampler:\n    '''\n    For sampling player subsets from the Shapley distribution.\n    Args:\n      num_players: number of players.\n    '''\n\n    def __init__(self, num_players):\n        arange = torch.arange(1, num_players)\n        w = 1 / (arange * (num_players - arange))\n        w = w / torch.sum(w)\n        self.categorical = Categorical(probs=w)\n        self.num_players = num_players\n        self.tril = torch.tril(\n            torch.ones(num_players - 1, num_players, dtype=torch.float32),\n            diagonal=0)\n\n    def sample(self, batch_size, paired_sampling):\n        '''\n        Generate sample.\n        Args:\n          batch_size: number of samples.\n          paired_sampling: whether to use paired sampling.\n        '''\n        num_included = 1 + self.categorical.sample([batch_size])\n        S = self.tril[num_included - 1]\n        # TODO ideally avoid for loops\n        for i in range(batch_size):\n            if paired_sampling and i % 2 == 1:\n                S[i] = 1 - S[i - 1]\n            else:\n                S[i] = S[i, torch.randperm(self.num_players)]\n        return S\n\n    def svs_sample(self, batch_size):\n        buf = []\n        for i in range(batch_size):\n            buf.append(torch.randperm(self.num_players) == torch.arange(self.num_players))\n        return torch.stack(buf, dim=0)\n\n    def get_categorical(self, num_players):\n        arange = torch.arange(1, num_players)\n        w = 1 / (arange * (num_players - arange))\n        w = w / torch.sum(w)\n        return Categorical(probs=w)\n\n    def dummy_sample_with_weight(self, batch_size, paired_sampling, guide_weight):\n        #num_included = 1 + self.categorical.sample([batch_size])\n        # we can only do local normalization when doing importance sampling as global normalizatino is intractable\n        assert guide_weight is not None\n        assert torch.is_tensor(guide_weight)\n        assert batch_size > 2\n        # require: guide_weight [seq_len]\n        assert len(guide_weight.shape) == 1\n        categorical = self.get_categorical(len(guide_weight))\n        num_included = 1 + categorical.sample([batch_size])\n        seq_len = guide_weight.shape[0]\n        #S = torch.zeros(batch_size, seq_len, dtype=torch.float32).cpu()\n        tril = torch.tril(torch.ones(seq_len - 1, seq_len, dtype=torch.float32), diagonal=0)\n        S = tril[num_included - 1]\n        w = torch.ones(batch_size, dtype=torch.float32).cpu()\n        w[0]=100000\n        w[1]=100000\n        S[0]=0\n        S[1]=1\n        #guide_weight_cpu = guide_weight.cpu()\n\n        for i in range(2, batch_size):\n            if paired_sampling and i % 2 == 1:\n                S[i] = 1 - S[i - 1]\n            else:\n                S[i] = S[i, torch.randperm(seq_len)]\n\n        return S, w\n\n    def guided_sample(self, batch_size, paired_sampling, guide_weight):\n        # we can only do local normalization when doing importance sampling as global normalizatino is intractable\n        assert guide_weight is not None\n        assert torch.is_tensor(guide_weight)\n        # require: guide_weight [seq_len]\n        assert len(guide_weight.shape) == 1\n        categorical = self.get_categorical(len(guide_weight))\n        num_included = 1 + categorical.sample([batch_size])\n        seq_len = guide_weight.shape[0]\n        S = torch.zeros(batch_size, seq_len, dtype=torch.float32).cpu()\n        w = torch.zeros(batch_size, dtype=torch.float32).cpu()\n        guide_weight_cpu = guide_weight.cpu()\n\n        for batch_i in range(batch_size):\n            current_guide_weight_cpu = guide_weight_cpu.clone().tolist()\n            for feat_i in range(num_included[batch_i]):\n                current_guide_weight_cpu_tensor = torch.Tensor(current_guide_weight_cpu)\n                cat_dist = torch.softmax(current_guide_weight_cpu_tensor, dim=-1)\n                cat_dist_class = Categorical(cat_dist)\n                sample_feat_id = cat_dist_class.sample()\n                S[batch_i][sample_feat_id] = 1\n                current_guide_weight_cpu = torch.cat([torch.Tensor(current_guide_weight_cpu[:sample_feat_id]), torch.Tensor(current_guide_weight_cpu[sample_feat_id+1:])])\n                w[batch_i] += torch.log(cat_dist[sample_feat_id])\n            w[batch_i] = binom(self.num_players, num_included[batch_i]) * torch.exp(w[batch_i]) + 1e-6\n            w[batch_i] = 1.0 / w[batch_i] \n\n        return S, w", "\n"]}
{"filename": "calibration_using_amortized_model.py", "chunked_list": ["from amortized_model import AmortizedModel\nfrom transformers import AutoTokenizer\nimport torch\nimport os\nimport glob\nfrom tqdm import tqdm\n# required by the bin file loading\n#from InterpCalib.NLI import dataset_utils\n#from NLI import dataset_utils\n", "#from NLI import dataset_utils\n\n# example path, change it to your own\nmodel_fn = \"/path/to/amortized_model_formal/multi_nli/lr_5e-05-epoch_30/seed_3_prop_1.0/model_svs_norm_False_discrete_False.pt\"\nmodel = torch.load(model_fn).cuda().eval()\n\nmodel_cache_dir = \"./models/\"\nmodel_name = \"textattack/bert-base-uncased-MNLI\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n# clone InterpCalib repo and change the path to your own", "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n# clone InterpCalib repo and change the path to your own\n# interpretations_custom is the folder containing the output files, see their README for more details\nsource_dirs = glob.glob(\"/path/to/InterpCalib/NLI/interpretations_custom/shap/mnli_mrpc_*\")\n\nfor source_dir in source_dirs:\n    target_dir = source_dir.replace(\"custom\", \"amortized\")\n    os.makedirs(target_dir, exist_ok=True)\n    bin_fns = glob.glob(os.path.join(source_dir, \"*.bin\"))\n    for bin_fn in tqdm(bin_fns):\n        basename = os.path.basename(bin_fn)\n        data = torch.load(bin_fn)\n        with open(os.path.join(target_dir, basename), \"wb\") as f_out:\n            premise, hypo = data['example'].premise, data['example'].hypothesis\n            batch = tokenizer([premise, ], [hypo, ], truncation=True, return_tensors=\"pt\", return_special_tokens_mask=True)\n            assert (batch['input_ids'][0] == torch.LongTensor(data['example'].input_ids)).all()\n            batch[\"output\"] = torch.stack([torch.tensor([1, ] * len(batch['input_ids'][0])), ] * len(batch['input_ids']))\n            batch[\"prediction_dist\"] = torch.stack([torch.tensor([1, ] * 3), ] * len(batch['input_ids']))\n            output, loss = model(batch)\n            if len(output.shape) == 2:\n                output = output[0]\n            output = output.cpu().detach()\n            assert len(output) == len(data['attribution'])\n            data['attribution'] = output\n            torch.save(data, os.path.join(target_dir, basename))", "\n\n"]}
{"filename": "amortized_model.py", "chunked_list": ["from transformers import AutoModel\nfrom tqdm import tqdm, trange\nimport math\nimport torch\nfrom torch import nn\nimport diffsort\nfrom samplers import ShapleySampler\nfrom sklearn.linear_model import LinearRegression\nclass AmortizedModel(nn.Module):\n    def __init__(self, model_name_or_path, cache_dir, args=None, target_model=None, tokenizer=None):\n        super(AmortizedModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(model_name_or_path, cache_dir)\n        if hasattr(self.args, \"extra_feat_dim\"):\n            self.extra_feat_dim = self.args.extra_feat_dim\n        else:\n            self.extra_feat_dim = 0\n        self.dim = self.model.config.hidden_size + self.extra_feat_dim\n        self.output = nn.Linear(self.dim, 1)\n        self.discrete = False\n        self.multitask = False\n        self.remove_columns = [\"output\", \"output_rank\", \"ft_label\", \"prediction_dist\", \"special_tokens_mask\", \"id\", \"zero_baseline\"]\n        if self.args is not None and self.args.discrete:\n            self.output = nn.Linear(self.dim, 2)\n            self.discrete = True\n            self.loss_func = nn.CrossEntropyLoss(reduction=\"none\")\n        if self.args is not None and hasattr(self.args, \"neuralsort\") and self.args.neuralsort:\n            self.sortnn = diffsort.DiffSortNet(sorting_network_type=self.args.sort_arch, size=512, device='cuda')\n            self.loss_func = torch.nn.BCELoss()\n        if self.args is not None and hasattr(self.args, \"multitask\") and self.args.multitask:\n            self.multitask = True\n            # imdb is binary classification task\n            # [todo]: modify 2 to be some arguments that can specify the number of classification labels\n            self.ft_output = nn.Linear(self.model.config.hidden_size, 2)\n            self.ft_loss_func = nn.CrossEntropyLoss()\n        if self.args is not None and hasattr(self.args, \"fastshap\") and self.args.fastshap:\n            assert self.extra_feat_dim == 0\n            self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n            assert target_model is not None\n            self.target_model = target_model.eval()\n            assert tokenizer is not None\n            self.tokenizer = tokenizer\n            self.target_label = 0\n            self.n_sample = 16\n        if self.args is not None and hasattr(self.args, \"suf_reg\") and self.args.suf_reg:\n            assert target_model is not None\n            self.target_model = target_model.eval()\n            assert tokenizer is not None\n            self.tokenizer = tokenizer\n\n    def create_new_batch(self, batch, device=\"cuda\"):\n        new_batch = dict()\n        for k in batch:\n            if k not in self.remove_columns:\n                # remove irrelevant columns for bert.forward()\n                new_batch[k] = batch[k].to(device)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        if \"prediction_dist\" in batch:\n            batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        return batch, new_batch\n\n    def forward(self, batch, device=\"cuda\"):\n        new_batch = dict()\n        for k in batch:\n            if k not in self.remove_columns:\n                # remove irrelevant columns for bert.forward()\n                new_batch[k] = batch[k].to(device)\n\n        encoding = self.model(**new_batch)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        if \"prediction_dist\" in batch:\n            batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        hidden_states = encoding.last_hidden_state\n        batch_size, seq_len, dim = hidden_states.shape\n        if self.extra_feat_dim > 0:\n            assert \"prediction_dist\" in batch\n            output = self.output(\n                torch.cat(\n                    [hidden_states, batch[\"prediction_dist\"].unsqueeze(1).expand(\n                        batch_size, seq_len, self.extra_feat_dim)],\n                    dim=-1\n                )\n            ).squeeze(dim=-1)\n        else:\n            output = self.output(hidden_states).squeeze(dim=-1)\n        if self.args is not None and hasattr(self.args, \"fastshap\") and self.args.fastshap:\n            # adapted from official fastshap repo code\n            assert len(batch[\"input_ids\"]) == 1, \"batch_size for fastshap must be 1 to allow shapley masking sampling\"\n            attn_mask = new_batch[\"attention_mask\"]\n            sampler = ShapleySampler(attn_mask.sum().item())\n            shap_mask = sampler.sample(batch_size * self.n_sample, paired_sampling=True).to(device)\n            shap_mask = torch.cat([shap_mask, torch.zeros(*shap_mask.shape[:-1], attn_mask.shape[-1] - sampler.num_players).to(attn_mask.device)], dim=-1)\n            # attn_mask_shap = attn_mask * shap_mask\n            zero_mask = torch.zeros_like(attn_mask)\n            expand_batch = dict()\n            expand_output = output.expand(self.n_sample, batch_size, seq_len).reshape(self.n_sample * batch_size, seq_len)\n            for k in batch:\n                if k not in self.remove_columns:\n                    expand_batch[k] = batch[k].to(device).expand(self.n_sample, batch_size, -1).reshape(self.n_sample * batch_size, -1)\n            backup_expand_input_ids = expand_batch[\"input_ids\"].clone()\n            target_model_original_output = self.target_model(**new_batch)[0].detach()\n            original_prediction = target_model_original_output.argmax(-1)\n            # full_original_output = target_model_original_output[torch.arange(batch_size), original_prediction].expand(self.n_sample, batch_size).reshape(self.n_sample * batch_size)\n            expand_batch['input_ids'] = backup_expand_input_ids.masked_fill(~(shap_mask.bool()), self.tokenizer.pad_token_id)\n            target_model_masked_output = self.target_model(**expand_batch)[0].data\n            masked_prediction = target_model_masked_output.argmax(-1)\n            masked_original_output = target_model_masked_output[torch.arange(len(masked_prediction)), original_prediction]\n            expand_batch['input_ids'] = backup_expand_input_ids * 0 + self.tokenizer.pad_token_id\n\n            target_model_zero_output = self.target_model(**expand_batch)[0].data\n            zero_original_output = target_model_zero_output[torch.arange(batch_size), original_prediction]\n            norm_output = expand_output\n            loss_fn = nn.MSELoss()\n            loss = loss_fn(masked_original_output, zero_original_output + (shap_mask * norm_output).sum(dim=-1))\n\n            return self.post_processing(output, loss, encoding, batch, device)\n\n\n\n        # backward compatibility\n        if self.args is not None and hasattr(self.args, \"neuralsort\") and self.args.neuralsort:\n            _, perm_pred = self.sortnn(output)\n            tgt = batch[\"output\"]\n            perm_gt = torch.nn.functional.one_hot(batch[\"output_rank\"]).transpose(-2, -1).float().to(device)\n            loss = self.loss_func(perm_pred, perm_gt)\n            return self.post_processing(output, loss, encoding, batch, device)\n        if not hasattr(self, \"discrete\") or not self.discrete:\n            tgt = batch[\"output\"]\n            if hasattr(self.args, \"normalization\") and self.args.normalization:\n                tgt = 100 * (tgt - tgt.mean(dim=-1, keepdim=True)) / (1e-5 + tgt.std(dim=-1, keepdim=True))\n            if self.args is not None and hasattr(self.args, \"suf_reg\") and self.args.suf_reg:\n                if \"zero_baseline\" not in batch:\n                    new_batch['input_ids'] = new_batch[\"input_ids\"] * 0 + self.tokenizer.pad_token_id\n                    target_model_zero_output = self.target_model(**new_batch)[0].data\n                else:\n                    target_model_zero_output = batch[\"zero_baseline\"].to(device)\n                original_prediction = batch[\"prediction_dist\"].argmax(dim=-1)\n                zero_original_output = target_model_zero_output[torch.arange(batch_size), original_prediction]\n                full_original_output = batch['prediction_dist'][torch.arange(batch_size), original_prediction]\n                output = output + 1/self.model.config.max_position_embeddings * (full_original_output - zero_original_output - output.sum(dim=-1)).unsqueeze(-1)\n            loss = ((new_batch[\"attention_mask\"] * (tgt - output)) ** 2).sum() / new_batch[\"attention_mask\"].sum()\n            return self.post_processing(output, loss, encoding, batch, device)\n        else:\n            gt = batch[\"output\"]\n            val, ind = torch.topk(gt, math.ceil(self.args.top_class_ratio * gt.shape[-1]), dim=-1)\n            tgt = torch.zeros_like(gt).scatter(-1, ind, 1)\n            loss = self.loss_func(\n                output.reshape(-1, output.shape[-1]),\n                tgt.reshape(-1).long(),\n            ).reshape(output.shape[0], output.shape[1])\n            loss = (new_batch[\"attention_mask\"] * loss).sum() / new_batch[\"attention_mask\"].sum()\n            return self.post_processing(torch.argmax(output, dim=-1), loss, encoding, batch, device)\n\n    def post_processing(self, main_output, main_loss, encoding, batch, device):\n        # special handles in case we want to do multi-task fine-tuning\n        if not hasattr(self, \"multitask\"):\n            # backward compatibility\n            return main_output, main_loss\n\n        if not self.multitask:\n            return main_output, main_loss\n        else:\n            pooled_output = encoding.pooler_output\n            labels = batch['ft_label'].to(device)\n            logits = self.ft_output(pooled_output)\n            ft_loss = self.ft_loss_func(logits, labels)\n            return main_output, main_loss, logits, ft_loss\n\n    def svs_compute(self, batch, new_batch, device):\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        batch_size, seq_len = batch['input_ids'].shape\n        num_feature = self.sampler.num_players\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids)\n        original_output = self.target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        target_model_original_output = self.target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        for _sample_i in trange(self.n_sample, desc=\"sampling permutation..\", leave=False):\n            permutation = torch.randperm(num_feature).tolist()\n            current_input = baseline\n            prev_res = initial_logits\n            for _permu_j in trange(num_feature, desc='doing masking...', leave=False):\n                # only update one element at one time, reuse permutation across batch\n                _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n                current_input = current_input * (~_mask) + input_ids * (_mask)\n                new_batch[\"input_ids\"] = current_input\n                # [batch_size]\n                modified_logits = self.target_model(**new_batch)[0].detach()[torch.arange(batch_size), target]\n                # [batch_size, seq_len]  *   ([batch_size] -> [batch_size, 1])\n                output = output + (modified_logits - prev_res).reshape(batch_size, 1) * _mask.float()\n                prev_res = modified_logits\n        return output / self.n_sample\n\n    def _single_run(self, batch, new_batch):\n        encoding = self.model(**new_batch)\n        hidden_states = encoding.last_hidden_state\n        batch_size, seq_len, dim = hidden_states.shape\n        if self.extra_feat_dim > 0:\n            assert \"prediction_dist\" in batch\n            output = self.output(\n                torch.cat(\n                    [hidden_states, batch[\"prediction_dist\"].unsqueeze(1).expand(\n                        batch_size, seq_len, self.extra_feat_dim)],\n                    dim=-1\n                )\n            ).squeeze(dim=-1)\n        else:\n            output = self.output(hidden_states).squeeze(dim=-1)\n        return output\n\n\n    def svs_compute_meta(self, batch, n_samples, device, target_model, use_imp=False, use_init=False, inv_temper=-1):\n        # doing guided importance sampling for ICLR rebuttal\n        batch, new_batch = self.create_new_batch(batch, device)\n        batch_size = new_batch[\"input_ids\"].shape[0]\n        assert batch_size == 1\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        baseline = baseline[0][new_batch[\"attention_mask\"][0] > 0].unsqueeze(0)\n        for key in new_batch:\n            if torch.is_tensor(new_batch[key]):\n                for _batch_i in range(batch_size):\n                    new_batch[key] = new_batch[key][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0].unsqueeze(0)\n        explainer_output = self._single_run(batch, new_batch)\n        for _batch_i in range(batch_size):\n            explainer_output = explainer_output[_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0].unsqueeze(0)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        #hidden_states = encoding.last_hidden_state\n        # batch_size, seq_len, dim = hidden_states.shape\n        batch_size, seq_len = new_batch['input_ids'].shape\n        #batch_size, seq_len = batch['input_ids'].shape\n        #if not hasattr(self, \"sampler\"):\n            #self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n        #num_feature = self.sampler.num_players\n        num_feature = seq_len\n        gumbel_dist = torch.distributions.gumbel.Gumbel(torch.Tensor([0]), torch.Tensor([1]))\n        gumbel_noise = gumbel_dist.sample([n_samples, num_feature]).squeeze(-1)\n        if inv_temper > 0:\n            noised_output = inv_temper * explainer_output + torch.log(gumbel_noise).cuda()\n        else:\n            noised_output = explainer_output + torch.log(gumbel_noise).cuda()\n        noised_output_ranking = torch.argsort(-1.0 * noised_output, dim=-1)\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids).float()\n        if use_init:\n            output += explainer_output\n        original_output = target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        target_model_original_output = target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        for _sample_i in trange(n_samples, desc=\"sampling permutation..\", leave=False):\n            if use_imp:\n                permutation = noised_output_ranking[_sample_i].cpu().tolist()\n            else:\n                permutation = torch.randperm(num_feature).tolist()\n            current_input = baseline\n            prev_res = initial_logits\n            for _permu_j in trange(num_feature, desc='doing masking...', leave=False):\n                # only update one element at one time, reuse permutation across batch\n                _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n                current_input = current_input * (~_mask) + input_ids * (_mask)\n                new_batch[\"input_ids\"] = current_input\n                # [batch_size]\n                modified_logits = target_model(**new_batch)[0].detach()[torch.arange(batch_size), target]\n                # [batch_size, seq_len]  *   ([batch_size] -> [batch_size, 1])\n                output = output + (modified_logits - prev_res).reshape(batch_size, 1) * _mask.float()\n                prev_res = modified_logits\n        return output / n_samples\n\n    def kernelshap_meta(self, batch, n_samples, device, target_model=None):\n        # doing guided importance sampling for ICLR rebuttal\n        batch, new_batch = self.create_new_batch(batch, device)\n        explainer_output = self._single_run(batch, new_batch)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        batch_size, seq_len = batch['input_ids'].shape\n        if not hasattr(self, \"sampler\"):\n            self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n        num_feature = self.sampler.num_players\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids)\n        if target_model is None:\n            original_output = self.target_model(**new_batch)[0].detach()\n        else:\n            original_output = target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        if target_model is None:\n            target_model_original_output = self.target_model(**new_batch)[0].detach()\n        else:\n            target_model_original_output = target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        new_output = []\n\n\n        for _batch_i in trange(batch_size, desc=\"processing instance..\", leave=False):\n            output_batch_i = explainer_output[_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]\n            regressor = LinearRegression()\n            sampler = ShapleySampler(len(output_batch_i))\n            seq_len_i = len(output_batch_i)\n            mask_samples, weights = self.sampler.dummy_sample_with_weight(n_samples, False, output_batch_i)\n            mask_samples = mask_samples.to(device)\n            batch_i_masked = {}\n            # [batch_size, seq_len] * [1, seq_len]\n            batch_i_masked[\"input_ids\"] = (mask_samples * (new_batch[\"input_ids\"][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]).unsqueeze(0)).int()\n            for key in new_batch:\n                if key == \"input_ids\":\n                    continue\n                else:\n                    batch_i_masked[key] = (new_batch[key][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]).unsqueeze(0).expand(n_samples, seq_len_i)\n            if target_model is None:\n                output_i = self.target_model(**batch_i_masked)[0].detach()[:, target[_batch_i]]\n            else:\n                output_i = target_model(**batch_i_masked)[0].detach()[:, target[_batch_i]]\n            try:\n                regressor.fit(mask_samples.cpu().numpy(), output_i.cpu().numpy())\n                new_ks_weight = regressor.coef_\n                new_output.append((new_ks_weight, batch[\"output\"][_batch_i][new_batch['attention_mask'][_batch_i] > 0].cpu().numpy()))\n            except:\n                print(\"cannot fit, debug:\")\n                print(mask_samples.min(), mask_samples.max())\n                print(weights.min(), weights.max())\n                print(output_i.min(), output_i.max())\n        return new_output", "class AmortizedModel(nn.Module):\n    def __init__(self, model_name_or_path, cache_dir, args=None, target_model=None, tokenizer=None):\n        super(AmortizedModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(model_name_or_path, cache_dir)\n        if hasattr(self.args, \"extra_feat_dim\"):\n            self.extra_feat_dim = self.args.extra_feat_dim\n        else:\n            self.extra_feat_dim = 0\n        self.dim = self.model.config.hidden_size + self.extra_feat_dim\n        self.output = nn.Linear(self.dim, 1)\n        self.discrete = False\n        self.multitask = False\n        self.remove_columns = [\"output\", \"output_rank\", \"ft_label\", \"prediction_dist\", \"special_tokens_mask\", \"id\", \"zero_baseline\"]\n        if self.args is not None and self.args.discrete:\n            self.output = nn.Linear(self.dim, 2)\n            self.discrete = True\n            self.loss_func = nn.CrossEntropyLoss(reduction=\"none\")\n        if self.args is not None and hasattr(self.args, \"neuralsort\") and self.args.neuralsort:\n            self.sortnn = diffsort.DiffSortNet(sorting_network_type=self.args.sort_arch, size=512, device='cuda')\n            self.loss_func = torch.nn.BCELoss()\n        if self.args is not None and hasattr(self.args, \"multitask\") and self.args.multitask:\n            self.multitask = True\n            # imdb is binary classification task\n            # [todo]: modify 2 to be some arguments that can specify the number of classification labels\n            self.ft_output = nn.Linear(self.model.config.hidden_size, 2)\n            self.ft_loss_func = nn.CrossEntropyLoss()\n        if self.args is not None and hasattr(self.args, \"fastshap\") and self.args.fastshap:\n            assert self.extra_feat_dim == 0\n            self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n            assert target_model is not None\n            self.target_model = target_model.eval()\n            assert tokenizer is not None\n            self.tokenizer = tokenizer\n            self.target_label = 0\n            self.n_sample = 16\n        if self.args is not None and hasattr(self.args, \"suf_reg\") and self.args.suf_reg:\n            assert target_model is not None\n            self.target_model = target_model.eval()\n            assert tokenizer is not None\n            self.tokenizer = tokenizer\n\n    def create_new_batch(self, batch, device=\"cuda\"):\n        new_batch = dict()\n        for k in batch:\n            if k not in self.remove_columns:\n                # remove irrelevant columns for bert.forward()\n                new_batch[k] = batch[k].to(device)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        if \"prediction_dist\" in batch:\n            batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        return batch, new_batch\n\n    def forward(self, batch, device=\"cuda\"):\n        new_batch = dict()\n        for k in batch:\n            if k not in self.remove_columns:\n                # remove irrelevant columns for bert.forward()\n                new_batch[k] = batch[k].to(device)\n\n        encoding = self.model(**new_batch)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        if \"prediction_dist\" in batch:\n            batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        hidden_states = encoding.last_hidden_state\n        batch_size, seq_len, dim = hidden_states.shape\n        if self.extra_feat_dim > 0:\n            assert \"prediction_dist\" in batch\n            output = self.output(\n                torch.cat(\n                    [hidden_states, batch[\"prediction_dist\"].unsqueeze(1).expand(\n                        batch_size, seq_len, self.extra_feat_dim)],\n                    dim=-1\n                )\n            ).squeeze(dim=-1)\n        else:\n            output = self.output(hidden_states).squeeze(dim=-1)\n        if self.args is not None and hasattr(self.args, \"fastshap\") and self.args.fastshap:\n            # adapted from official fastshap repo code\n            assert len(batch[\"input_ids\"]) == 1, \"batch_size for fastshap must be 1 to allow shapley masking sampling\"\n            attn_mask = new_batch[\"attention_mask\"]\n            sampler = ShapleySampler(attn_mask.sum().item())\n            shap_mask = sampler.sample(batch_size * self.n_sample, paired_sampling=True).to(device)\n            shap_mask = torch.cat([shap_mask, torch.zeros(*shap_mask.shape[:-1], attn_mask.shape[-1] - sampler.num_players).to(attn_mask.device)], dim=-1)\n            # attn_mask_shap = attn_mask * shap_mask\n            zero_mask = torch.zeros_like(attn_mask)\n            expand_batch = dict()\n            expand_output = output.expand(self.n_sample, batch_size, seq_len).reshape(self.n_sample * batch_size, seq_len)\n            for k in batch:\n                if k not in self.remove_columns:\n                    expand_batch[k] = batch[k].to(device).expand(self.n_sample, batch_size, -1).reshape(self.n_sample * batch_size, -1)\n            backup_expand_input_ids = expand_batch[\"input_ids\"].clone()\n            target_model_original_output = self.target_model(**new_batch)[0].detach()\n            original_prediction = target_model_original_output.argmax(-1)\n            # full_original_output = target_model_original_output[torch.arange(batch_size), original_prediction].expand(self.n_sample, batch_size).reshape(self.n_sample * batch_size)\n            expand_batch['input_ids'] = backup_expand_input_ids.masked_fill(~(shap_mask.bool()), self.tokenizer.pad_token_id)\n            target_model_masked_output = self.target_model(**expand_batch)[0].data\n            masked_prediction = target_model_masked_output.argmax(-1)\n            masked_original_output = target_model_masked_output[torch.arange(len(masked_prediction)), original_prediction]\n            expand_batch['input_ids'] = backup_expand_input_ids * 0 + self.tokenizer.pad_token_id\n\n            target_model_zero_output = self.target_model(**expand_batch)[0].data\n            zero_original_output = target_model_zero_output[torch.arange(batch_size), original_prediction]\n            norm_output = expand_output\n            loss_fn = nn.MSELoss()\n            loss = loss_fn(masked_original_output, zero_original_output + (shap_mask * norm_output).sum(dim=-1))\n\n            return self.post_processing(output, loss, encoding, batch, device)\n\n\n\n        # backward compatibility\n        if self.args is not None and hasattr(self.args, \"neuralsort\") and self.args.neuralsort:\n            _, perm_pred = self.sortnn(output)\n            tgt = batch[\"output\"]\n            perm_gt = torch.nn.functional.one_hot(batch[\"output_rank\"]).transpose(-2, -1).float().to(device)\n            loss = self.loss_func(perm_pred, perm_gt)\n            return self.post_processing(output, loss, encoding, batch, device)\n        if not hasattr(self, \"discrete\") or not self.discrete:\n            tgt = batch[\"output\"]\n            if hasattr(self.args, \"normalization\") and self.args.normalization:\n                tgt = 100 * (tgt - tgt.mean(dim=-1, keepdim=True)) / (1e-5 + tgt.std(dim=-1, keepdim=True))\n            if self.args is not None and hasattr(self.args, \"suf_reg\") and self.args.suf_reg:\n                if \"zero_baseline\" not in batch:\n                    new_batch['input_ids'] = new_batch[\"input_ids\"] * 0 + self.tokenizer.pad_token_id\n                    target_model_zero_output = self.target_model(**new_batch)[0].data\n                else:\n                    target_model_zero_output = batch[\"zero_baseline\"].to(device)\n                original_prediction = batch[\"prediction_dist\"].argmax(dim=-1)\n                zero_original_output = target_model_zero_output[torch.arange(batch_size), original_prediction]\n                full_original_output = batch['prediction_dist'][torch.arange(batch_size), original_prediction]\n                output = output + 1/self.model.config.max_position_embeddings * (full_original_output - zero_original_output - output.sum(dim=-1)).unsqueeze(-1)\n            loss = ((new_batch[\"attention_mask\"] * (tgt - output)) ** 2).sum() / new_batch[\"attention_mask\"].sum()\n            return self.post_processing(output, loss, encoding, batch, device)\n        else:\n            gt = batch[\"output\"]\n            val, ind = torch.topk(gt, math.ceil(self.args.top_class_ratio * gt.shape[-1]), dim=-1)\n            tgt = torch.zeros_like(gt).scatter(-1, ind, 1)\n            loss = self.loss_func(\n                output.reshape(-1, output.shape[-1]),\n                tgt.reshape(-1).long(),\n            ).reshape(output.shape[0], output.shape[1])\n            loss = (new_batch[\"attention_mask\"] * loss).sum() / new_batch[\"attention_mask\"].sum()\n            return self.post_processing(torch.argmax(output, dim=-1), loss, encoding, batch, device)\n\n    def post_processing(self, main_output, main_loss, encoding, batch, device):\n        # special handles in case we want to do multi-task fine-tuning\n        if not hasattr(self, \"multitask\"):\n            # backward compatibility\n            return main_output, main_loss\n\n        if not self.multitask:\n            return main_output, main_loss\n        else:\n            pooled_output = encoding.pooler_output\n            labels = batch['ft_label'].to(device)\n            logits = self.ft_output(pooled_output)\n            ft_loss = self.ft_loss_func(logits, labels)\n            return main_output, main_loss, logits, ft_loss\n\n    def svs_compute(self, batch, new_batch, device):\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        batch_size, seq_len = batch['input_ids'].shape\n        num_feature = self.sampler.num_players\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids)\n        original_output = self.target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        target_model_original_output = self.target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        for _sample_i in trange(self.n_sample, desc=\"sampling permutation..\", leave=False):\n            permutation = torch.randperm(num_feature).tolist()\n            current_input = baseline\n            prev_res = initial_logits\n            for _permu_j in trange(num_feature, desc='doing masking...', leave=False):\n                # only update one element at one time, reuse permutation across batch\n                _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n                current_input = current_input * (~_mask) + input_ids * (_mask)\n                new_batch[\"input_ids\"] = current_input\n                # [batch_size]\n                modified_logits = self.target_model(**new_batch)[0].detach()[torch.arange(batch_size), target]\n                # [batch_size, seq_len]  *   ([batch_size] -> [batch_size, 1])\n                output = output + (modified_logits - prev_res).reshape(batch_size, 1) * _mask.float()\n                prev_res = modified_logits\n        return output / self.n_sample\n\n    def _single_run(self, batch, new_batch):\n        encoding = self.model(**new_batch)\n        hidden_states = encoding.last_hidden_state\n        batch_size, seq_len, dim = hidden_states.shape\n        if self.extra_feat_dim > 0:\n            assert \"prediction_dist\" in batch\n            output = self.output(\n                torch.cat(\n                    [hidden_states, batch[\"prediction_dist\"].unsqueeze(1).expand(\n                        batch_size, seq_len, self.extra_feat_dim)],\n                    dim=-1\n                )\n            ).squeeze(dim=-1)\n        else:\n            output = self.output(hidden_states).squeeze(dim=-1)\n        return output\n\n\n    def svs_compute_meta(self, batch, n_samples, device, target_model, use_imp=False, use_init=False, inv_temper=-1):\n        # doing guided importance sampling for ICLR rebuttal\n        batch, new_batch = self.create_new_batch(batch, device)\n        batch_size = new_batch[\"input_ids\"].shape[0]\n        assert batch_size == 1\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        baseline = baseline[0][new_batch[\"attention_mask\"][0] > 0].unsqueeze(0)\n        for key in new_batch:\n            if torch.is_tensor(new_batch[key]):\n                for _batch_i in range(batch_size):\n                    new_batch[key] = new_batch[key][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0].unsqueeze(0)\n        explainer_output = self._single_run(batch, new_batch)\n        for _batch_i in range(batch_size):\n            explainer_output = explainer_output[_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0].unsqueeze(0)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        #hidden_states = encoding.last_hidden_state\n        # batch_size, seq_len, dim = hidden_states.shape\n        batch_size, seq_len = new_batch['input_ids'].shape\n        #batch_size, seq_len = batch['input_ids'].shape\n        #if not hasattr(self, \"sampler\"):\n            #self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n        #num_feature = self.sampler.num_players\n        num_feature = seq_len\n        gumbel_dist = torch.distributions.gumbel.Gumbel(torch.Tensor([0]), torch.Tensor([1]))\n        gumbel_noise = gumbel_dist.sample([n_samples, num_feature]).squeeze(-1)\n        if inv_temper > 0:\n            noised_output = inv_temper * explainer_output + torch.log(gumbel_noise).cuda()\n        else:\n            noised_output = explainer_output + torch.log(gumbel_noise).cuda()\n        noised_output_ranking = torch.argsort(-1.0 * noised_output, dim=-1)\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids).float()\n        if use_init:\n            output += explainer_output\n        original_output = target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        target_model_original_output = target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        for _sample_i in trange(n_samples, desc=\"sampling permutation..\", leave=False):\n            if use_imp:\n                permutation = noised_output_ranking[_sample_i].cpu().tolist()\n            else:\n                permutation = torch.randperm(num_feature).tolist()\n            current_input = baseline\n            prev_res = initial_logits\n            for _permu_j in trange(num_feature, desc='doing masking...', leave=False):\n                # only update one element at one time, reuse permutation across batch\n                _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n                current_input = current_input * (~_mask) + input_ids * (_mask)\n                new_batch[\"input_ids\"] = current_input\n                # [batch_size]\n                modified_logits = target_model(**new_batch)[0].detach()[torch.arange(batch_size), target]\n                # [batch_size, seq_len]  *   ([batch_size] -> [batch_size, 1])\n                output = output + (modified_logits - prev_res).reshape(batch_size, 1) * _mask.float()\n                prev_res = modified_logits\n        return output / n_samples\n\n    def kernelshap_meta(self, batch, n_samples, device, target_model=None):\n        # doing guided importance sampling for ICLR rebuttal\n        batch, new_batch = self.create_new_batch(batch, device)\n        explainer_output = self._single_run(batch, new_batch)\n        batch[\"output\"] = batch[\"output\"].to(device)\n        batch[\"prediction_dist\"] = batch[\"prediction_dist\"].to(device)\n        batch_size, seq_len = batch['input_ids'].shape\n        if not hasattr(self, \"sampler\"):\n            self.sampler = ShapleySampler(self.model.config.max_position_embeddings)\n        num_feature = self.sampler.num_players\n        baseline = new_batch['input_ids'] * (batch[\"special_tokens_mask\"].to(device))\n        mask = torch.arange(num_feature)\n        input_ids = new_batch['input_ids'].clone()\n        # [batch_size, seq_len]\n        output = torch.zeros_like(input_ids)\n        if target_model is None:\n            original_output = self.target_model(**new_batch)[0].detach()\n        else:\n            original_output = target_model(**new_batch)[0].detach()\n        target = original_output.argmax(dim=-1)\n        new_batch['input_ids'] = baseline\n        if target_model is None:\n            target_model_original_output = self.target_model(**new_batch)[0].detach()\n        else:\n            target_model_original_output = target_model(**new_batch)[0].detach()\n        initial_logits = target_model_original_output[torch.arange(batch_size), target]\n        new_output = []\n\n\n        for _batch_i in trange(batch_size, desc=\"processing instance..\", leave=False):\n            output_batch_i = explainer_output[_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]\n            regressor = LinearRegression()\n            sampler = ShapleySampler(len(output_batch_i))\n            seq_len_i = len(output_batch_i)\n            mask_samples, weights = self.sampler.dummy_sample_with_weight(n_samples, False, output_batch_i)\n            mask_samples = mask_samples.to(device)\n            batch_i_masked = {}\n            # [batch_size, seq_len] * [1, seq_len]\n            batch_i_masked[\"input_ids\"] = (mask_samples * (new_batch[\"input_ids\"][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]).unsqueeze(0)).int()\n            for key in new_batch:\n                if key == \"input_ids\":\n                    continue\n                else:\n                    batch_i_masked[key] = (new_batch[key][_batch_i][new_batch[\"attention_mask\"][_batch_i] > 0]).unsqueeze(0).expand(n_samples, seq_len_i)\n            if target_model is None:\n                output_i = self.target_model(**batch_i_masked)[0].detach()[:, target[_batch_i]]\n            else:\n                output_i = target_model(**batch_i_masked)[0].detach()[:, target[_batch_i]]\n            try:\n                regressor.fit(mask_samples.cpu().numpy(), output_i.cpu().numpy())\n                new_ks_weight = regressor.coef_\n                new_output.append((new_ks_weight, batch[\"output\"][_batch_i][new_batch['attention_mask'][_batch_i] > 0].cpu().numpy()))\n            except:\n                print(\"cannot fit, debug:\")\n                print(mask_samples.min(), mask_samples.max())\n                print(weights.min(), weights.max())\n                print(output_i.min(), output_i.max())\n        return new_output", "\n        #\n        # for _sample_i in trange(self.n_sample, desc=\"sampling permutation..\", leave=False):\n        #     permutation = torch.randperm(num_feature).tolist()\n        #     current_input = baseline\n        #     prev_res = initial_logits\n        #     for _permu_j in trange(num_feature, desc='doing masking...', leave=False):\n        #         # only update one element at one time, reuse permutation across batch\n        #         _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n        #         current_input = current_input * (~_mask) + input_ids * (_mask)", "        #         _mask = (mask == permutation[_permu_j]).unsqueeze(0).to(device)\n        #         current_input = current_input * (~_mask) + input_ids * (_mask)\n        #         # print((current_input > 0).sum())\n        #         new_batch[\"input_ids\"] = current_input\n        #         # [batch_size]\n        #         modified_logits = self.target_model(**new_batch)[0].detach()[torch.arange(batch_size), target]\n        #         # [batch_size, seq_len]  *   ([batch_size] -> [batch_size, 1])\n        #         output = output + (modified_logits - prev_res).reshape(batch_size, 1) * _mask.float()\n        #         prev_res = modified_logits\n", "        #         prev_res = modified_logits\n\n\n\n"]}
{"filename": "run.py", "chunked_list": ["import torch\nimport random\nimport argparse\nimport json\nimport numpy as np\nfrom torch import nn, optim\nimport loguru\nfrom tqdm import tqdm\nfrom scipy.stats import spearmanr, kendalltau\nfrom amortized_model import AmortizedModel", "from scipy.stats import spearmanr, kendalltau\nfrom amortized_model import AmortizedModel\nfrom create_dataset import (\n    output_dir as dataset_dir,\n    model_cache_dir\n)\nimport os\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom transformers import DataCollatorForTokenClassification, AutoModelForSequenceClassification, PreTrainedTokenizer, \\", "from datasets import Dataset\nfrom transformers import DataCollatorForTokenClassification, AutoModelForSequenceClassification, PreTrainedTokenizer, \\\n    AutoTokenizer\nfrom config import Args, GetParser\nfrom utils import collate_fn, get_zero_baselines\nfrom metrics import get_eraser_metrics\n\n\ndef running_step(dataloader, model, K, optimizer=None, is_train=False, save=False, args=None):\n    def get_top_k(_output):\n        _rank_output = [(x, i) for i, x in enumerate(_output)]\n        _rank_output.sort(key=lambda x: x[0], reverse=True)\n        _rank_output = [x[1] for x in _rank_output][:K]\n        return _rank_output\n\n    # def dropout(_input):\n    #     _rand = torch.rand_like(_input.float())\n    #     _mask = _rand >= 0.5\n    #     return _mask.long() * _input\n\n    all_loss = 0\n    all_outputs = []\n    all_aux_outputs = []\n    all_refs = []\n    all_attn = []\n    all_ins = []\n    count_elements = 0\n    spearman = []\n    ks_meta_spearman = []\n    ks_meta_spearman_1 = []\n    ks_meta_spearman_2 = []\n    ks_meta_spearman_3 = []\n    ks_meta_spearman_5 = []\n    ks_meta_spearman_use_imp = []\n    ks_meta_spearman_use_imp_temper = []\n    ks_meta_spearman_use_init_1 = []\n    ks_meta_spearman_use_init_2 = []\n    ks_meta_spearman_use_init_3 = []\n    ks_meta_spearman_use_init_5 = []\n    kendals = []\n    intersection = []\n    # dropout = nn.Dropout(inplace=True)\n    desc = \"testing\"\n    do_ks_meta_eval = True\n    if is_train:\n        assert optimizer is not None\n        optimizer.zero_grad()\n        desc = 'training'\n    for batch in tqdm(dataloader, desc=desc):\n        if hasattr(model, \"multitask\") and model.multitask:\n            main_output, main_loss, aux_output, aux_loss = model(batch)\n            output = main_output\n            loss = main_loss\n            all_aux_outputs.extend((aux_output.argmax(dim=-1) == batch[\"ft_label\"].cuda()).detach().cpu().tolist())\n        else:\n            output, loss = model(batch)\n        if is_train:\n            if not hasattr(args, \"discrete\") or not args.discrete:\n                if len(all_aux_outputs) == 0:\n                    loss = loss\n                else:\n                    loss = torch.sqrt(loss) + aux_loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # recording purposes\n        all_loss += loss.item()\n        # # do not count [CLS]\n        # batch[\"attention_mask\"][:, 0] = 0\n        if not is_train and do_ks_meta_eval:\n            global target_model\n            ks_meta_output = model.svs_compute_meta(batch, 10, \"cuda\", target_model).cpu()\n            ks_meta_output_1 = model.svs_compute_meta(batch, 1, \"cuda\", target_model).cpu()\n            ks_meta_output_2 = model.svs_compute_meta(batch, 2, \"cuda\", target_model).cpu()\n            ks_meta_output_3 = model.svs_compute_meta(batch, 3, \"cuda\", target_model).cpu()\n            ks_meta_output_5 = model.svs_compute_meta(batch, 5, \"cuda\", target_model).cpu()\n            ks_meta_output_use_imp = model.svs_compute_meta(batch, 10, \"cuda\", target_model, use_imp=True).cpu()\n            ks_meta_output_use_imp_temper = model.svs_compute_meta(batch, 1, \"cuda\", target_model, use_imp=True, inv_temper=0.1).cpu()\n            ks_meta_output_use_init_1 = model.svs_compute_meta(batch, 1, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_2 = model.svs_compute_meta(batch, 2, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_3 = model.svs_compute_meta(batch, 3, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_5 = model.svs_compute_meta(batch, 5, \"cuda\", target_model, use_init=True).cpu()\n        else:\n            ks_meta_output = None\n            ks_meta_output_1 = None\n            ks_meta_output_2 = None\n            ks_meta_output_3 = None\n            ks_meta_output_5 = None\n            ks_meta_output_use_imp = None\n            ks_meta_output_use_imp_temper = None\n            ks_meta_output_use_init_1 = None\n            ks_meta_output_use_init_2 = None\n            ks_meta_output_use_init_3 = None\n            ks_meta_output_use_init_5 = None\n\n        attn_mask = batch[\"attention_mask\"].cuda()\n        batch[\"output\"] = batch[\"output\"].cuda()\n        for _ind in range(len(output)):\n            _output = output[_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            _ref = batch[\"output\"][_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            all_attn.append(attn_mask.detach().cpu().numpy())\n            all_ins.append(batch['input_ids'].detach().cpu().numpy())\n            _rank_output = get_top_k(_output)\n            _rank_ref = get_top_k(_ref)\n            intersect_num = len(set(_rank_ref) & set(_rank_output))\n            _spearman, p_val = spearmanr(_output, _ref, axis=0)\n            if ks_meta_output is not None and _ind < len(ks_meta_output):\n                if len(attn_mask[_ind]) == len(ks_meta_output[_ind]):\n                   _ks_meta_output = ks_meta_output[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_1 = ks_meta_output_1[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_2 = ks_meta_output_2[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_3 = ks_meta_output_3[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_5 = ks_meta_output_5[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_imp = ks_meta_output_use_imp[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_imp_temper = ks_meta_output_use_imp_temper[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_1 = ks_meta_output_use_init_1[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_2 = ks_meta_output_use_init_2[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_3 = ks_meta_output_use_init_3[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_5 = ks_meta_output_use_init_5[_ind][attn_mask[_ind] > 0]\n                else:\n                   _ks_meta_output = ks_meta_output[_ind]\n                   _ks_meta_output_1 = ks_meta_output_1[_ind]\n                   _ks_meta_output_2 = ks_meta_output_2[_ind]\n                   _ks_meta_output_3 = ks_meta_output_3[_ind]\n                   _ks_meta_output_5 = ks_meta_output_5[_ind]\n                   _ks_meta_output_use_imp = ks_meta_output_use_imp[_ind]\n                   _ks_meta_output_use_imp_temper = ks_meta_output_use_imp_temper[_ind]\n                   _ks_meta_output_use_init_1 = ks_meta_output_use_init_1[_ind]\n                   _ks_meta_output_use_init_2 = ks_meta_output_use_init_2[_ind]\n                   _ks_meta_output_use_init_3 = ks_meta_output_use_init_3[_ind]\n                   _ks_meta_output_use_init_5 = ks_meta_output_use_init_5[_ind]\n                _ks_meta_spearman, _ = spearmanr(_ks_meta_output, _ref, axis=0)\n                _ks_meta_spearman_1, _ = spearmanr(_ks_meta_output_1, _ref, axis=0)\n                _ks_meta_spearman_2, _ = spearmanr(_ks_meta_output_2, _ref, axis=0)\n                _ks_meta_spearman_3, _ = spearmanr(_ks_meta_output_3, _ref, axis=0)\n                _ks_meta_spearman_5, _ = spearmanr(_ks_meta_output_5, _ref, axis=0)\n                _ks_meta_spearman_use_imp, _ = spearmanr(_ks_meta_output_use_imp, _ref, axis=0)\n                _ks_meta_spearman_use_imp_temper, _ = spearmanr(_ks_meta_output_use_imp_temper, _ref, axis=0)\n                _ks_meta_spearman_use_init_1, _ = spearmanr(_ks_meta_output_use_init_1, _ref, axis=0)\n                _ks_meta_spearman_use_init_2, _ = spearmanr(_ks_meta_output_use_init_2, _ref, axis=0)\n                _ks_meta_spearman_use_init_3, _ = spearmanr(_ks_meta_output_use_init_3, _ref, axis=0)\n                _ks_meta_spearman_use_init_5, _ = spearmanr(_ks_meta_output_use_init_5, _ref, axis=0)\n                ks_meta_spearman.append(_ks_meta_spearman)\n                ks_meta_spearman_1.append(_ks_meta_spearman_1)\n                ks_meta_spearman_2.append(_ks_meta_spearman_2)\n                ks_meta_spearman_3.append(_ks_meta_spearman_3)\n                ks_meta_spearman_5.append(_ks_meta_spearman_5)\n                ks_meta_spearman_use_imp.append(_ks_meta_spearman_use_imp)\n                ks_meta_spearman_use_imp_temper.append(_ks_meta_spearman_use_imp_temper)\n                ks_meta_spearman_use_init_1.append(_ks_meta_spearman_use_init_1)\n                ks_meta_spearman_use_init_2.append(_ks_meta_spearman_use_init_2)\n                ks_meta_spearman_use_init_3.append(_ks_meta_spearman_use_init_3)\n                ks_meta_spearman_use_init_5.append(_ks_meta_spearman_use_init_5)\n                global logger\n                if len(ks_meta_spearman) >= 100:\n                    do_ks_meta_eval = False\n                    logger.info(\"ks_meta_spearman: {}\".format(np.mean(ks_meta_spearman)))\n                    logger.info(\"ks_meta_spearman_1: {}\".format(np.mean(ks_meta_spearman_1)))\n                    logger.info(\"ks_meta_spearman_2: {}\".format(np.mean(ks_meta_spearman_2)))\n                    logger.info(\"ks_meta_spearman_3: {}\".format(np.mean(ks_meta_spearman_3)))\n                    logger.info(\"ks_meta_spearman_5: {}\".format(np.mean(ks_meta_spearman_5)))\n                    logger.info(\"ks_meta_spearman_use_imp: {}\".format(np.mean(ks_meta_spearman_use_imp)))\n                    logger.info(\"ks_meta_spearman_use_imp_temper_sample_1: {}\".format(np.mean(ks_meta_spearman_use_imp_temper)))\n                    logger.info(\"ks_meta_spearman_use_init_1: {}\".format(np.mean(ks_meta_spearman_use_init_1)))\n                    logger.info(\"ks_meta_spearman_use_init_2: {}\".format(np.mean(ks_meta_spearman_use_init_2)))\n                    logger.info(\"ks_meta_spearman_use_init_3: {}\".format(np.mean(ks_meta_spearman_use_init_3)))\n                    logger.info(\"ks_meta_spearman_use_init_5: {}\".format(np.mean(ks_meta_spearman_use_init_5)))\n            _kendal, kp_val = kendalltau(_output, _ref)\n            spearman.append(_spearman)\n            kendals.append(_kendal)\n            intersection.append(intersect_num)\n            all_outputs.append(_output)\n            all_refs.append(_ref)\n\n        count_elements += batch[\"attention_mask\"].sum().item()\n    if save and args is not None:\n        torch.save([all_outputs, all_refs, all_attn, all_ins],\n                   os.path.join(os.path.dirname(args.save_path),\n                                os.path.basename(args.save_path).strip(\".pt\"),\n                                \"test_outputs.pkl\")\n                   )\n    return all_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_outputs", "def running_step(dataloader, model, K, optimizer=None, is_train=False, save=False, args=None):\n    def get_top_k(_output):\n        _rank_output = [(x, i) for i, x in enumerate(_output)]\n        _rank_output.sort(key=lambda x: x[0], reverse=True)\n        _rank_output = [x[1] for x in _rank_output][:K]\n        return _rank_output\n\n    # def dropout(_input):\n    #     _rand = torch.rand_like(_input.float())\n    #     _mask = _rand >= 0.5\n    #     return _mask.long() * _input\n\n    all_loss = 0\n    all_outputs = []\n    all_aux_outputs = []\n    all_refs = []\n    all_attn = []\n    all_ins = []\n    count_elements = 0\n    spearman = []\n    ks_meta_spearman = []\n    ks_meta_spearman_1 = []\n    ks_meta_spearman_2 = []\n    ks_meta_spearman_3 = []\n    ks_meta_spearman_5 = []\n    ks_meta_spearman_use_imp = []\n    ks_meta_spearman_use_imp_temper = []\n    ks_meta_spearman_use_init_1 = []\n    ks_meta_spearman_use_init_2 = []\n    ks_meta_spearman_use_init_3 = []\n    ks_meta_spearman_use_init_5 = []\n    kendals = []\n    intersection = []\n    # dropout = nn.Dropout(inplace=True)\n    desc = \"testing\"\n    do_ks_meta_eval = True\n    if is_train:\n        assert optimizer is not None\n        optimizer.zero_grad()\n        desc = 'training'\n    for batch in tqdm(dataloader, desc=desc):\n        if hasattr(model, \"multitask\") and model.multitask:\n            main_output, main_loss, aux_output, aux_loss = model(batch)\n            output = main_output\n            loss = main_loss\n            all_aux_outputs.extend((aux_output.argmax(dim=-1) == batch[\"ft_label\"].cuda()).detach().cpu().tolist())\n        else:\n            output, loss = model(batch)\n        if is_train:\n            if not hasattr(args, \"discrete\") or not args.discrete:\n                if len(all_aux_outputs) == 0:\n                    loss = loss\n                else:\n                    loss = torch.sqrt(loss) + aux_loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # recording purposes\n        all_loss += loss.item()\n        # # do not count [CLS]\n        # batch[\"attention_mask\"][:, 0] = 0\n        if not is_train and do_ks_meta_eval:\n            global target_model\n            ks_meta_output = model.svs_compute_meta(batch, 10, \"cuda\", target_model).cpu()\n            ks_meta_output_1 = model.svs_compute_meta(batch, 1, \"cuda\", target_model).cpu()\n            ks_meta_output_2 = model.svs_compute_meta(batch, 2, \"cuda\", target_model).cpu()\n            ks_meta_output_3 = model.svs_compute_meta(batch, 3, \"cuda\", target_model).cpu()\n            ks_meta_output_5 = model.svs_compute_meta(batch, 5, \"cuda\", target_model).cpu()\n            ks_meta_output_use_imp = model.svs_compute_meta(batch, 10, \"cuda\", target_model, use_imp=True).cpu()\n            ks_meta_output_use_imp_temper = model.svs_compute_meta(batch, 1, \"cuda\", target_model, use_imp=True, inv_temper=0.1).cpu()\n            ks_meta_output_use_init_1 = model.svs_compute_meta(batch, 1, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_2 = model.svs_compute_meta(batch, 2, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_3 = model.svs_compute_meta(batch, 3, \"cuda\", target_model, use_init=True).cpu()\n            ks_meta_output_use_init_5 = model.svs_compute_meta(batch, 5, \"cuda\", target_model, use_init=True).cpu()\n        else:\n            ks_meta_output = None\n            ks_meta_output_1 = None\n            ks_meta_output_2 = None\n            ks_meta_output_3 = None\n            ks_meta_output_5 = None\n            ks_meta_output_use_imp = None\n            ks_meta_output_use_imp_temper = None\n            ks_meta_output_use_init_1 = None\n            ks_meta_output_use_init_2 = None\n            ks_meta_output_use_init_3 = None\n            ks_meta_output_use_init_5 = None\n\n        attn_mask = batch[\"attention_mask\"].cuda()\n        batch[\"output\"] = batch[\"output\"].cuda()\n        for _ind in range(len(output)):\n            _output = output[_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            _ref = batch[\"output\"][_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            all_attn.append(attn_mask.detach().cpu().numpy())\n            all_ins.append(batch['input_ids'].detach().cpu().numpy())\n            _rank_output = get_top_k(_output)\n            _rank_ref = get_top_k(_ref)\n            intersect_num = len(set(_rank_ref) & set(_rank_output))\n            _spearman, p_val = spearmanr(_output, _ref, axis=0)\n            if ks_meta_output is not None and _ind < len(ks_meta_output):\n                if len(attn_mask[_ind]) == len(ks_meta_output[_ind]):\n                   _ks_meta_output = ks_meta_output[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_1 = ks_meta_output_1[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_2 = ks_meta_output_2[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_3 = ks_meta_output_3[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_5 = ks_meta_output_5[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_imp = ks_meta_output_use_imp[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_imp_temper = ks_meta_output_use_imp_temper[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_1 = ks_meta_output_use_init_1[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_2 = ks_meta_output_use_init_2[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_3 = ks_meta_output_use_init_3[_ind][attn_mask[_ind] > 0]\n                   _ks_meta_output_use_init_5 = ks_meta_output_use_init_5[_ind][attn_mask[_ind] > 0]\n                else:\n                   _ks_meta_output = ks_meta_output[_ind]\n                   _ks_meta_output_1 = ks_meta_output_1[_ind]\n                   _ks_meta_output_2 = ks_meta_output_2[_ind]\n                   _ks_meta_output_3 = ks_meta_output_3[_ind]\n                   _ks_meta_output_5 = ks_meta_output_5[_ind]\n                   _ks_meta_output_use_imp = ks_meta_output_use_imp[_ind]\n                   _ks_meta_output_use_imp_temper = ks_meta_output_use_imp_temper[_ind]\n                   _ks_meta_output_use_init_1 = ks_meta_output_use_init_1[_ind]\n                   _ks_meta_output_use_init_2 = ks_meta_output_use_init_2[_ind]\n                   _ks_meta_output_use_init_3 = ks_meta_output_use_init_3[_ind]\n                   _ks_meta_output_use_init_5 = ks_meta_output_use_init_5[_ind]\n                _ks_meta_spearman, _ = spearmanr(_ks_meta_output, _ref, axis=0)\n                _ks_meta_spearman_1, _ = spearmanr(_ks_meta_output_1, _ref, axis=0)\n                _ks_meta_spearman_2, _ = spearmanr(_ks_meta_output_2, _ref, axis=0)\n                _ks_meta_spearman_3, _ = spearmanr(_ks_meta_output_3, _ref, axis=0)\n                _ks_meta_spearman_5, _ = spearmanr(_ks_meta_output_5, _ref, axis=0)\n                _ks_meta_spearman_use_imp, _ = spearmanr(_ks_meta_output_use_imp, _ref, axis=0)\n                _ks_meta_spearman_use_imp_temper, _ = spearmanr(_ks_meta_output_use_imp_temper, _ref, axis=0)\n                _ks_meta_spearman_use_init_1, _ = spearmanr(_ks_meta_output_use_init_1, _ref, axis=0)\n                _ks_meta_spearman_use_init_2, _ = spearmanr(_ks_meta_output_use_init_2, _ref, axis=0)\n                _ks_meta_spearman_use_init_3, _ = spearmanr(_ks_meta_output_use_init_3, _ref, axis=0)\n                _ks_meta_spearman_use_init_5, _ = spearmanr(_ks_meta_output_use_init_5, _ref, axis=0)\n                ks_meta_spearman.append(_ks_meta_spearman)\n                ks_meta_spearman_1.append(_ks_meta_spearman_1)\n                ks_meta_spearman_2.append(_ks_meta_spearman_2)\n                ks_meta_spearman_3.append(_ks_meta_spearman_3)\n                ks_meta_spearman_5.append(_ks_meta_spearman_5)\n                ks_meta_spearman_use_imp.append(_ks_meta_spearman_use_imp)\n                ks_meta_spearman_use_imp_temper.append(_ks_meta_spearman_use_imp_temper)\n                ks_meta_spearman_use_init_1.append(_ks_meta_spearman_use_init_1)\n                ks_meta_spearman_use_init_2.append(_ks_meta_spearman_use_init_2)\n                ks_meta_spearman_use_init_3.append(_ks_meta_spearman_use_init_3)\n                ks_meta_spearman_use_init_5.append(_ks_meta_spearman_use_init_5)\n                global logger\n                if len(ks_meta_spearman) >= 100:\n                    do_ks_meta_eval = False\n                    logger.info(\"ks_meta_spearman: {}\".format(np.mean(ks_meta_spearman)))\n                    logger.info(\"ks_meta_spearman_1: {}\".format(np.mean(ks_meta_spearman_1)))\n                    logger.info(\"ks_meta_spearman_2: {}\".format(np.mean(ks_meta_spearman_2)))\n                    logger.info(\"ks_meta_spearman_3: {}\".format(np.mean(ks_meta_spearman_3)))\n                    logger.info(\"ks_meta_spearman_5: {}\".format(np.mean(ks_meta_spearman_5)))\n                    logger.info(\"ks_meta_spearman_use_imp: {}\".format(np.mean(ks_meta_spearman_use_imp)))\n                    logger.info(\"ks_meta_spearman_use_imp_temper_sample_1: {}\".format(np.mean(ks_meta_spearman_use_imp_temper)))\n                    logger.info(\"ks_meta_spearman_use_init_1: {}\".format(np.mean(ks_meta_spearman_use_init_1)))\n                    logger.info(\"ks_meta_spearman_use_init_2: {}\".format(np.mean(ks_meta_spearman_use_init_2)))\n                    logger.info(\"ks_meta_spearman_use_init_3: {}\".format(np.mean(ks_meta_spearman_use_init_3)))\n                    logger.info(\"ks_meta_spearman_use_init_5: {}\".format(np.mean(ks_meta_spearman_use_init_5)))\n            _kendal, kp_val = kendalltau(_output, _ref)\n            spearman.append(_spearman)\n            kendals.append(_kendal)\n            intersection.append(intersect_num)\n            all_outputs.append(_output)\n            all_refs.append(_ref)\n\n        count_elements += batch[\"attention_mask\"].sum().item()\n    if save and args is not None:\n        torch.save([all_outputs, all_refs, all_attn, all_ins],\n                   os.path.join(os.path.dirname(args.save_path),\n                                os.path.basename(args.save_path).strip(\".pt\"),\n                                \"test_outputs.pkl\")\n                   )\n    return all_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_outputs", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Amortized Model Arguments Parser\")\n    parser = GetParser(parser)\n    global_args = parser.parse_args()\n    logger = loguru.logger\n    # assert global_args.train_bsz == 1 and global_args.test_bsz == 1, \"currently only support batch_size == 1\"\n\n    torch.manual_seed(global_args.seed)\n    random.seed(global_args.seed)\n    target_model = AutoModelForSequenceClassification.from_pretrained(global_args.target_model).cuda()\n    tokenizer = AutoTokenizer.from_pretrained(global_args.target_model)\n    if global_args.target_model == \"textattack/bert-base-uncased-MNLI\":\n        label_mapping_dict = {\n            0: 2,\n            1: 0,\n            2: 1\n        }\n        label_mapping = lambda x: label_mapping_dict[x]\n    else:\n        label_mapping = None\n    K = global_args.topk\n    alL_train_datasets = dict()\n    all_valid_datasets = dict()\n    all_test_datasets = dict()\n    explainers = global_args.explainer\n    if \",\" in explainers:\n        explainers = explainers.split(\",\")\n    else:\n        explainers = [explainers, ]\n    if \"MNLI\" in global_args.target_model:\n        dataset_dir = \"./amortized_dataset/mnli_test\"\n    if \"yelp\" in global_args.target_model:\n        dataset_dir = \"./amortized_dataset/yelp_test\"\n    for explainer in explainers:\n        train_dataset, valid_dataset, test_dataset = torch.load(os.path.join(dataset_dir, f\"data_{explainer}.pkl\"))\n        train_dataset, valid_dataset, test_dataset = Dataset.from_dict(train_dataset), Dataset.from_dict(\n            valid_dataset), Dataset.from_dict(test_dataset)\n        alL_train_datasets[explainer] = train_dataset\n        all_valid_datasets[explainer] = valid_dataset\n        all_test_datasets[explainer] = test_dataset\n    for proportion in [1.0, 0.1, 0.3, 0.5, 0.7, 0.9]:\n        for explainer in explainers:\n            args = Args(seed=global_args.seed, explainer=explainer, proportion=str(proportion),\n                        epochs=global_args.epoch,\n                        batch_size=global_args.train_bsz, normalization=global_args.normalization,\n                        task_name=global_args.task,\n                        discretization=global_args.discrete,\n                        lr=global_args.lr, neuralsort=global_args.neuralsort,\n                        multitask=True if hasattr(global_args, \"multitask\") and global_args.multitask else False,\n                        suf_reg=global_args.suf_reg if hasattr(global_args, \"suf_reg\") and global_args.suf_reg else False,\n                        storage_root=global_args.storage_root\n                        )\n            train_dataset, valid_dataset, test_dataset = alL_train_datasets[explainer], all_valid_datasets[explainer], \\\n                                                         all_test_datasets[explainer]\n            if proportion < 1:\n                id_fn = os.path.join(os.path.dirname(args.save_path),\n                                     os.path.basename(args.save_path).strip(\".pt\"),\n                                     \"training_ids.pkl\")\n                if not os.path.exists(id_fn):\n                    sample_ids = random.sample(range(len(train_dataset)), int(proportion * len(train_dataset)))\n                    os.makedirs(\n                        os.path.join(os.path.dirname(args.save_path),\n                                     os.path.basename(args.save_path).strip(\".pt\"),\n                                     ),\n                        exist_ok=True\n                    )\n                    torch.save(sample_ids,\n                               os.path.join(os.path.dirname(args.save_path),\n                                            os.path.basename(args.save_path).strip(\".pt\"),\n                                            \"training_ids.pkl\")\n                               )\n                else:\n                    sample_ids = torch.load(id_fn)\n                train_dataset = train_dataset.select(sample_ids)\n            train_dataset, valid_dataset, test_dataset = get_zero_baselines([train_dataset, valid_dataset, test_dataset], target_model, tokenizer, args)\n            train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size,\n                                          collate_fn=collate_fn)\n            valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n            if args.fastshap or args.suf_reg:\n                model = AmortizedModel(global_args.amortized_model, cache_dir=model_cache_dir, args=args,\n                                       target_model=target_model, tokenizer=tokenizer).cuda()\n            else:\n                model = AmortizedModel(global_args.amortized_model, cache_dir=model_cache_dir, args=args).cuda()\n\n            optimizer = optim.Adam(model.parameters(), lr=args.lr)\n            log_dir = os.path.join(os.path.dirname(args.save_path), os.path.basename(args.save_path).strip(\".pt\"))\n            handler_id = logger.add(os.path.join(log_dir, \"log_{time}.txt\"))\n            logger.info(json.dumps(vars(args), indent=4))\n            try:\n                model = torch.load(args.save_path)\n            except:\n                os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n                best_valid_spearman = -999999\n                for epoch_i in range(args.epochs):\n                    training_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_output = running_step(\n                        train_dataloader, model, K, optimizer, is_train=True)\n                    logger.info(f\"training loss at epoch {epoch_i}: {training_loss / len(train_dataloader)}\")\n                    logger.info(f\"training spearman (micro-avg): {np.mean(spearman)}\")\n                    logger.info(f\"training top-{K} intersection: {np.mean(intersection)}\")\n\n                    all_outputs = np.concatenate(all_outputs)\n                    all_refs = np.concatenate(all_refs)\n                    logger.info(f\"training spearman: {spearmanr(all_outputs, all_refs)}\")\n                    logger.info(f\"training kendaltau: {kendalltau(all_outputs, all_refs)}\")\n                    if len(all_aux_output) > 0:\n                        logger.info(f\"training aux acc: {np.mean(all_aux_output)}\")\n\n                    if (epoch_i) % args.validation_period == 0:\n                        with torch.no_grad():\n                            valid_loss, valid_all_outputs, valid_all_refs, valid_count_elements, valid_spearman, valid_kendals, valid_intersection, all_valid_aux_output = running_step(\n                                valid_dataloader, model, K, optimizer, is_train=False)\n                            logger.info(f\"Validating at epoch-{epoch_i}\")\n                            valid_all_outputs = np.concatenate(valid_all_outputs)\n                            valid_all_refs = np.concatenate(valid_all_refs)\n                            valid_macro_spearman = spearmanr(valid_all_outputs, valid_all_refs)\n                            valid_macro_kendal = kendalltau(valid_all_outputs, valid_all_refs)\n                            logger.info(f\"validation spearman: {valid_macro_spearman}\")\n                            logger.info(f\"validation kendaltau: {valid_macro_kendal}\")\n                            micro_spearman = np.mean(valid_spearman)\n                            micro_kendal = np.mean(valid_kendals)\n                            logger.info(f\"validation micro spearman: {micro_spearman}\")\n                            logger.info(f\"validation micro kendal: {micro_kendal}\")\n                            if len(all_valid_aux_output) > 0:\n                                logger.info(f\"validation aux acc: {np.mean(all_valid_aux_output)}\")\n                            if valid_macro_spearman.correlation > best_valid_spearman:\n                                best_valid_spearman = valid_macro_spearman.correlation\n                                logger.info(\n                                    f\"best validation spearman at {epoch_i}: {valid_macro_spearman.correlation}, save checkpoint here\")\n                                torch.save(model, args.save_path)\n\n            with torch.no_grad():\n                model = model.eval()\n                for test_explainer in explainers:\n                    handler_id_test = logger.add(\n                        os.path.join(os.path.dirname(args.save_path), os.path.basename(args.save_path).strip(\".pt\"),\n                                     f\"test_log_no_pad_{test_explainer}.txt\"))\n                    test_dataset = all_test_datasets[test_explainer]\n                    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size,\n                                                 collate_fn=collate_fn)\n                    logger.info(f\"doing testing for {test_explainer}\")\n                    test_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_test_aux_output = running_step(\n                        test_dataloader, model, K, optimizer, is_train=False, save=True, args=args)\n\n                    logger.info(f\"testing spearman (micro-avg): {np.mean(spearman)}\")\n                    logger.info(f\"testing kendal (micro-avg): {np.mean(kendals)}\")\n                    logger.info(f\"testing top-{K} intersection: {np.mean(intersection)}\")\n                    logger.info(f\"testing RMSE: {np.sqrt(test_loss / count_elements)}\")\n                    all_outputs = np.concatenate(all_outputs)\n                    all_refs = np.concatenate(all_refs)\n                    logger.info(f\"testing spearman: {spearmanr(all_outputs, all_refs)}\")\n                    logger.info(f\"testing kendaltau: {kendalltau(all_outputs, all_refs)}\")\n                    if len(all_test_aux_output) > 0:\n                        logger.info(f\"testing aux acc: {np.mean(all_test_aux_output)}\")\n\n                    try:\n                        stat_dict = torch.load(os.path.join(log_dir, f\"eraser_stat_dict_{test_explainer}.pt\"))\n                    except:\n                        test_dataloader = DataLoader(test_dataset, batch_size=1,\n                                                     collate_fn=collate_fn)\n                        stat_dict = get_eraser_metrics(test_dataloader, target_model, amortized_model=model,\n                                                       tokenizer=tokenizer, label_mapping=label_mapping)\n                        torch.save(stat_dict, os.path.join(log_dir, f\"eraser_stat_dict_{test_explainer}.pt\"))\n                    logger.info(\"eraser_metrics\")\n                    for k in stat_dict:\n                        for metric in stat_dict[k]:\n                            logger.info(\n                                f\"{k}-{metric}: {np.mean(stat_dict[k][metric]).item()} ({np.std(stat_dict[k][metric]).item()})\")\n                    logger.remove(handler_id_test)\n            #\n            logger.remove(handler_id)", ""]}
{"filename": "config.py", "chunked_list": ["class Args:\n    def __init__(self, seed, task_name, batch_size=16, epochs=20, explainer=\"svs\", proportion=\"1.0\", normalization=True,\n                 discretization=False, validation_period=5, top_class_ratio=0.2, lr=2e-4, multitask=False, neuralsort=True, sort_arch=\"bitonic\",\n                 suf_reg=False, path_name_suffix=\"formal\", storage_root=\"path/to/dir\"):\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.lr = lr\n        # self.explainer = \"svs\"\n        # self.explainer = \"lime\"\n        self.explainer = explainer\n        self.proportion = proportion\n        self.extra_feat_dim = 0\n        self.discrete = discretization\n        self.top_class_ratio = top_class_ratio\n        self.validation_period = validation_period\n        self.seed = seed\n        self.normalization = normalization\n        self.neuralsort = neuralsort\n        self.sort_arch = sort_arch\n        self.multitask = multitask\n        self.suf_reg = suf_reg\n        # path_name_suffix = \"formal\"\n        self.fastshap = False\n        if self.multitask:\n            path_name_suffix = \"multi_task\"\n        if self.fastshap:\n            self.extra_feat_dim = 0\n            path_name_suffix += \"fastshap\"\n        path_name_suffix += f\"/{task_name}\"\n        storage_root = storage_root\n        if self.suf_reg:\n            path_name_suffix += \"_suf_reg\"\n        self.save_path = f\"{storage_root}/amortized_model_{path_name_suffix}\" \\\n                         f\"{'-extradim-' + str(self.extra_feat_dim) if self.extra_feat_dim > 0 else ''}\" \\\n                         f\"/lr_{self.lr}-epoch_{self.epochs}/seed_{self.seed}_prop_{self.proportion}/model_{self.explainer}_norm_{self.normalization}_discrete_{self.discrete}.pt\"\n        if self.neuralsort:\n            self.save_path = f\"{storage_root}/amortized_model_{path_name_suffix}\" \\\n                             f\"{'-extradim-' + str(self.extra_feat_dim) if self.extra_feat_dim > 0 else ''}\" \\\n                             f\"/{self.sort_arch}_trans_sort_lr_{self.lr}-epoch_{self.epochs}/seed_{self.seed}_prop_{self.proportion}/model_{self.explainer}_norm_{self.normalization}_discrete_{self.discrete}.pt\"", "\ndef GetParser(parser):\n    parser.add_argument(\"-s\", \"--seed\", default=1111, type=int, help=\"seed?\")\n    parser.add_argument(\"-e\", \"--epoch\", default=20, type=int, help=\"seed?\")\n    parser.add_argument(\"--lr\", default=2e-4, type=float, help=\"lr?\")\n    parser.add_argument(\"--train_bsz\", default=1, type=int, help=\"batch_size for training?\")\n    parser.add_argument(\"--test_bsz\", default=1, type=int, help=\"batch_size for testing?\")\n    parser.add_argument(\"-K\", \"--topk\", default=50, type=int, help=\"top-k intersection?\")\n    parser.add_argument(\"--explainer\", default=\"lime\", type=str, help=\"which explainer you want to use? svs/lime/lig? if multiple, concat by one single comma\")\n    parser.add_argument(\"--task\", default=\"imdb\", type=str, help=\"which task you want to use? \")\n    parser.add_argument(\"-am\", \"--amortized_model\", default=\"bert-base-uncased\", type=str,\n                        help=\"use which arch for amortized model?\")\n    parser.add_argument(\"-tm\", \"--target_model\", default=\"textattack/bert-base-uncased-MNLI\", type=str,\n    # parser.add_argument(\"-tm\", \"--target_model\", default=\"textattack/bert-base-uncased-imdb\", type=str,\n                        help=\"use which arch for target model?\")\n    parser.add_argument(\"-norm\", \"--normalization\", action=\"store_true\",\n                        help=\"use normalization?\")\n    parser.add_argument(\"-disc\", \"--discrete\", action=\"store_true\",\n                        help=\"use discretization?\")\n    parser.add_argument(\"-sort\", \"--neuralsort\", action=\"store_true\",\n                        help=\"use neuralsorting?\")\n    parser.add_argument(\"-mul\", \"--multitask\", action=\"store_true\",\n                        help=\"use multitasking (add fine-tuning original classification tasks)?\")\n    parser.add_argument(\"--suf_reg\", action=\"store_true\",\n                        help=\"add sufficiency regularization?\")\n    parser.add_argument(\"--storage_root\", type=str, help=\"where to store the output?\")\n    return parser", ""]}
{"filename": "compute_amortized_model_consistency.py", "chunked_list": ["import glob\nimport numpy as np\nfrom scipy.stats import spearmanr\nimport torch\nfrom tqdm import trange\nfrom tqdm import tqdm\n\nfor prop in [0.1, 0.3, 0.5, 0.7, 1.0]:\n    print(f\"eval {prop}\")\n    all_spearman = []\n    all_spearman_1 = []\n    all_spearman_2 = []\n    gt_spearmans = []\n    diffs = []\n\n    # test_outputs path, change this to your own path, for example:\n    seed_path_format = \"/path/to/amortized_model_formal/multi_nli/lr_5e-05-epoch_30/seed_{}_prop_{}/model_svs_norm_False_discrete_False/test_outputs.pkl\"\n    # seed_path_format = \"/path/to/amortized_model_formal/yelp_polarity/lr_5e-05-epoch_30/seed_{}_prop_{}/model_svs_norm_False_discrete_False/test_outputs.pkl\"\n    seeds = [0, 1, 2]\n    seed_gt_spearmans = []\n    seed_all_spearmans = []\n    seed_l2_delta = []\n    for seed_1 in tqdm(range(len(seeds)), position=0, leave=True):\n        for seed_2 in tqdm(range(seed_1 + 1, len(seeds)), position=0, leave=True):\n            seed_path1 = seed_path_format.format(seeds[seed_1], prop)\n            seed_path2 = seed_path_format.format(seeds[seed_2], prop)\n            output_pred1, output_ref1, output_attn1, output_in1 = torch.load(seed_path1)\n            output_pred2, output_ref2, output_attn2, output_in2 = torch.load(seed_path2)\n\n            for i in range(len(output_ref1)):\n                assert (output_attn1[i] == output_attn2[i]).all()\n                assert (output_in1[i] == output_in2[i]).all()\n                sp, p = spearmanr(output_ref1[i], output_ref2[i])\n                gt_spearmans.append(sp)\n                sp, p = spearmanr(output_pred1[i], output_pred2[i])\n                all_spearman.append(sp)\n                all_spearman_1.append(spearmanr(output_pred1[i], output_ref1[i])[0])\n                all_spearman_2.append(spearmanr(output_pred2[i], output_ref2[i])[0])\n                diffs.append(np.linalg.norm(output_pred1[i] - output_pred2[i]))\n            seed_gt_spearmans.append(np.mean(gt_spearmans))\n            seed_all_spearmans.append(np.mean(all_spearman))\n            seed_l2_delta.append(np.mean(diffs))\n    print(\"gt spearman: \", np.mean(seed_gt_spearmans), np.std(seed_gt_spearmans))\n    print(\"all spearman: \", np.mean(seed_all_spearmans), np.std(seed_all_spearmans))\n    print(\"l2_delta: \", np.mean(seed_l2_delta), np.std(seed_l2_delta))", ""]}
{"filename": "draw_feature_selection.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport random\nimport os\nimport csv\nfrom matplotlib import container\ndef get_cmap(n, name='hsv'):\n    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct\n    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n    return plt.cm.get_cmap(name, n)\n", "\nrandom.seed(1)\nplt.rcParams.update({'font.size': 16})\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nfilename = \"feature_selection_yelp.csv\"\n# filename = \"feature_selection_mnli.csv\"\nwith open(filename, 'r', encoding='utf-8') as f_in:\n# with open(\"feature_selection_mnli.csv\", 'r', encoding='utf-8') as f_in:\n    reader = csv.DictReader(f_in)\n    res = dict()\n    for line in reader:\n        for key in line:\n            if \"Ratio\" not in key:\n                if key not in res:\n                    res[key] = []\n                res[key].append(float(line[key]))\n    # xs = [f\"Top {x}% Mask\" for x in [1, 5, 10, 20, 50]]\n    # xs = [f\"{x}%\" for x in [1, 5, 10, 20, 50]]\n    xs = [f\"{x}%\" for x in [1, 5, 10, 20]]\n    target_dir = \"visualization\"\n    os.makedirs(target_dir, exist_ok=True)\n    # cmap = get_cmap(len(res))\n    cmap = [\"red\", \"blue\", \"orange\", \"purple\", \"cyan\", \"green\", \"lime\", \"#bb86fc\"]\n    markers = [\".\", \"v\", \"*\", \"o\", \"s\", \"d\", \"P\", \"p\"]\n    # keys = sorted(res.keys())\n    # cmap = [\"red\", \"#ef9a9a\", \"#e57373\", \"#ef5350\", \"#f44336\", \"#ba68c8\", \"#9c27b0\", \"#7cb342\"]\n    if \"mnli\" in filename:\n        plt.axhline(y=33.33, xmin=0, xmax=4, ls=\"--\", color=\"pink\", label=\"random\")\n        plt.axhline(y=84.65, xmin=0, xmax=4, ls=\"--\", color=\"brown\", label=\"0% mask\")\n    elif \"yelp\" in filename:\n        plt.axhline(y=50.00, xmin=0, xmax=4, ls=\"--\", color=\"pink\", label=\"random\")\n        plt.axhline(y=97.42, xmin=0, xmax=4, ls=\"--\", color=\"brown\", label=\"0% mask\")\n    # keys = [\"svs-25\", \"kernelshap-25\", \"kernelshap-200\", \"kernelshap-2000\", \"kernelshap-8000\", \"lime-25\", \"lime-200\", \"AmortizedModel\"]\n    keys = [\"svs-25\", \"kernelshap-25\", \"kernelshap-200\", \"kernelshap-2000\", \"kernelshap-8000\", \"AmortizedModel\"]\n    # keys = [\"svs-25\", \"kernelshap-25\", \"kernelshap-200\", \"kernelshap-2000\", \"kernelshap-8000\"]\n    for i, key in enumerate(keys):\n        print(key)\n        _label = key if \"kernelshap\" not in key else key.replace(\"kernelshap\", \"ks\")\n        _label = \"Our Model\" if \"Amortized\" in _label else _label\n        # plt.plot(range(len(xs)), res[key][:len(xs)], label=key.lower(), color=cmap[i], marker=markers[i])\n        plt.errorbar(range(len(xs)), res[key + \" (mean)\"][:len(xs)], yerr=res[key + \" (std)\"][: len(xs)], color=cmap[i], capthick=3, ecolor='black', capsize=5, marker=markers[i], label=_label)\n    # plt.plot(range(len(xs)), [33.33, ] * len(xs), color='pink', ls=\"--\", label=\"random\")\n    # plt.plot(range(len(xs)), [84.65, ] * len(xs), color='brown', ls='--', label='0% mask')\n    plt.xticks(range(len(xs)), xs)\n    # get handles\n    handles, labels = plt.gca().get_legend_handles_labels()\n    # remove the errorbars\n    # handles = [h[0] for h in handles]\n    handles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\n    # ax = plt.gca()\n    # box = ax.get_position()\n    # ax.set_position([box.x0, box.y0 + box.height * 0.1,\n    #                  box.width, box.height * 0.9])\n    ax = plt.gca()\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 1, box.height])\n\n    # Put a legend below current axis\n    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n    #           fancybox=True, shadow=True, ncol=5)\n    # plt.legend(handles, labels, loc=\"lower left\")\n    # plt.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, -0.05), ncol=4)\n    plt.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.ylabel(\"Accuracy\", fontsize=22)\n    plt.xlabel(\"Top K% Mask\", fontsize=22)\n    # target_fp = os.path.join(target_dir, \"feature_selection_mnli_wo_amortized.pdf\")\n    # target_fp = os.path.join(target_dir, \"feature_selection_mnli.pdf\")\n    # target_fp = os.path.join(target_dir, \"feature_selection_yelp_wo_amortized_w_errorbar.pdf\")\n    if \"yelp\" in filename:\n        target_fp = os.path.join(target_dir, \"feature_selection_yelp_w_amortized_w_errorbar.pdf\")\n    elif \"mnli\" in filename:\n        target_fp = os.path.join(target_dir, \"feature_selection_mnli_w_amortized_w_errorbar.pdf\")\n    plt.tight_layout()\n\n    plt.savefig(target_fp)\n    plt.show()", "\n\n\n        # print(line)"]}
{"filename": "draw_histogram.py", "chunked_list": ["import math\nimport os\nfrom torch.utils.data import DataLoader\nimport torch\nimport random\nimport argparse\nimport json\nimport numpy as np\nfrom torch import nn, optim\nimport loguru", "from torch import nn, optim\nimport loguru\nfrom tqdm import tqdm\nfrom datasets import Dataset\nfrom create_dataset import (\n    output_dir as dataset_dir,\n    model_cache_dir\n)\nfrom matplotlib import pyplot as plt\nfrom run import collate_fn", "from matplotlib import pyplot as plt\nfrom run import collate_fn\n\nalL_train_datasets = dict()\nall_test_datasets = dict()\noutput_dir = os.path.join(\"visualization\", \"value_histogram\")\nos.makedirs(output_dir, exist_ok=True)\nfor explainer in [\"svs\", \"lig\", \"lime\"]:\n    print(f\"plot values for {explainer}\")\n    train_dataset, test_dataset = torch.load(os.path.join(dataset_dir, f\"data_{explainer}.pkl\"))\n    train_dataset, test_dataset = Dataset.from_dict(train_dataset), Dataset.from_dict(test_dataset)\n    alL_train_datasets[explainer] = train_dataset\n    all_test_datasets[explainer] = test_dataset\n    # train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=collate_fn)\n    # for data in train_dataloader:\n    all_vals = []\n    for data in train_dataset:\n        _vals = data[\"output\"]\n        all_vals.extend(_vals)\n    plt.xlabel(f\"{explainer} values\")\n    plt.hist(all_vals, bins=5)\n    plt.title(f\"histogram for {explainer} values\")\n    plt.savefig(os.path.join(output_dir, f\"train_histogram_{explainer}.pdf\"))\n    plt.clf()\n    all_val_log = [math.log(abs(x) + 1e-7, 10) for x in all_vals]\n    plt.hist(all_val_log, bins=5)\n    plt.title(f\"histogram for log(abs({explainer})) values\")\n    plt.savefig(os.path.join(output_dir, f\"train_log_histogram_{explainer}.pdf\"))\n    plt.clf()", "\n        # print(_vals)\n\n"]}
{"filename": "draw_lr_training.py", "chunked_list": ["from matplotlib import pyplot as plt\nimport glob\nimport os\nimport numpy as np\n\n# intersections = [22, 28, 25, 27, 27]\n# kendall = [0.0707, 0.0281, -0.07, 0.003, 0.0261]\n# spearman = [0.1046, 0.0423, -0.05, 0.005, 0.0389]\n# proportions = [0.1, 0.3, 0.5, 0.7, 1]\n# model_name = \"svs\"", "# proportions = [0.1, 0.3, 0.5, 0.7, 1]\n# model_name = \"svs\"\n\n\n#intersections = [24, 29, 24, 24, 25]\n#kendall = [0.0064, 0.0119, 0.00580, -0.00196, -0.0119]\n#spearman = [0.0096, 0.0177, 0.00866, -0.002954, -0.0180]\nseed_dirs = glob.glob(\"amortized_model_debug/seed_*\")\nmodel_name = \"svs\"\noutput_dir = \"visualization/learning_curve_debug_discrete\"", "model_name = \"svs\"\noutput_dir = \"visualization/learning_curve_debug_discrete\"\nos.makedirs(output_dir, exist_ok=True)\n\nintersections = []\nkendalls = []\nspearmans = []\nlosses = []\nfor seed_dir in seed_dirs:\n    intersection = []\n    spearman = []\n    kendall = []\n    loss = []\n    record_dir = f\"{seed_dir}/model_{model_name}_norm_False_discrete_True\"\n    logs = glob.glob(os.path.join(record_dir, \"log*txt\"))\n    if len(logs) > 1:\n        file_sizes = [(os.path.getsize(_path), _path) for _path in logs]\n        file_sizes.sort(key=lambda x: x[0], reverse=True)\n        logfn = file_sizes[0][1]\n    else:\n        logfn = logs[0]\n    with open(logfn, \"r\", encoding='utf-8') as f_in:\n        for line in f_in:\n            if \"loss at epoch\" in line:\n                _num = float(line.strip().split(\":\")[-1].strip())\n                loss.append(_num)\n            if \"intersection: \" in line:\n                _num = float(line.strip().split(\"intersection: \")[-1])\n                intersection.append(_num)\n            if \"Spearman\" in line:\n                _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                spearman.append(_num)\n            if \"kendal\" in line:\n                _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                kendall.append(_num)\n    losses.append(loss)\n    intersections.append(intersection)\n    spearmans.append(spearman)\n    kendalls.append(kendall)", "for seed_dir in seed_dirs:\n    intersection = []\n    spearman = []\n    kendall = []\n    loss = []\n    record_dir = f\"{seed_dir}/model_{model_name}_norm_False_discrete_True\"\n    logs = glob.glob(os.path.join(record_dir, \"log*txt\"))\n    if len(logs) > 1:\n        file_sizes = [(os.path.getsize(_path), _path) for _path in logs]\n        file_sizes.sort(key=lambda x: x[0], reverse=True)\n        logfn = file_sizes[0][1]\n    else:\n        logfn = logs[0]\n    with open(logfn, \"r\", encoding='utf-8') as f_in:\n        for line in f_in:\n            if \"loss at epoch\" in line:\n                _num = float(line.strip().split(\":\")[-1].strip())\n                loss.append(_num)\n            if \"intersection: \" in line:\n                _num = float(line.strip().split(\"intersection: \")[-1])\n                intersection.append(_num)\n            if \"Spearman\" in line:\n                _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                spearman.append(_num)\n            if \"kendal\" in line:\n                _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                kendall.append(_num)\n    losses.append(loss)\n    intersections.append(intersection)\n    spearmans.append(spearman)\n    kendalls.append(kendall)", "\nlosses = np.array(losses)\nintersections = np.array(intersections)\nspearmans = np.array(spearmans)\nkendalls = np.array(kendalls)\n\nfor ys, yname, color in zip([losses, intersections, kendalls, spearmans], [\"losses\", \"intersections\", \"kendall\", \"spearman\"], [\"black\", \"black\", \"black\", \"black\"]):\n    plt.errorbar(range(len(losses[0])), np.mean(ys, axis=0), yerr=np.std(ys, axis=0), fmt=\"-o\", color=color, capthick=5, ecolor='g', capsize=3)\n    plt.xlabel(\"epoch\")\n    plt.ylabel(yname)\n    plt.title(f\"{model_name}_{yname}\")\n    plt.savefig(os.path.join(output_dir, f\"{model_name}_{yname}.pdf\"))\n    plt.clf()", "\n"]}
{"filename": "metrics.py", "chunked_list": ["import torch\nfrom transformers import PreTrainedTokenizer\nfrom tqdm import tqdm\nimport random\n\ndef get_eraser_metrics(dataloader, target_model, amortized_model, tokenizer: PreTrainedTokenizer, label_mapping=None):\n    stat_dict = {}\n    target_model = target_model.cuda()\n    for batch in tqdm(dataloader, desc=\"eraser_eval\"):\n        output, loss = amortized_model(batch)\n        # again, assuming bsz == 1\n        attn_mask = batch[\"attention_mask\"].clone()\n        #attn_mask[:, 0] = 0\n        attn_mask = attn_mask.squeeze(0).cuda()\n        interpret = batch[\"output\"].squeeze(0).cuda()[attn_mask > 0]\n        output = output.squeeze(0)[attn_mask > 0]\n        sorted_interpret, sorted_interpret_indices = interpret.sort(descending=True)\n        sorted_output, sorted_output_indices = output.sort(descending=True)\n        random_order = sorted_output_indices.cpu().tolist()\n        random.shuffle(random_order)\n        random_order = torch.LongTensor(random_order).to(sorted_output_indices.device)\n        target_model_output = target_model(\n            input_ids=batch[\"input_ids\"].cuda(),\n            attention_mask=batch[\"attention_mask\"].cuda(),\n            token_type_ids=batch[\"token_type_ids\"].cuda() if \"token_type_ids\" in batch else None,\n            position_ids=batch[\"position_ids\"].cuda() if \"position_ids\" in batch else None\n        )\n        target_logits = target_model_output.logits\n        target_model_pred = target_logits.argmax(dim=-1).squeeze(0)\n        target_logits_pred = target_logits[:, target_model_pred]\n        #for K in [10, 50, 100, 200, 500]:\n        for K in [0, 0.01, 0.05, 0.10, 0.20, 0.50]:\n            if K not in stat_dict:\n                stat_dict[K] = {}\n                for model_type in [\"interpret\", \"output\", \"random\"]:\n                    stat_dict[K][f\"sufficiency_{model_type}\"] = []\n                    stat_dict[K][f\"comprehensiveness_{model_type}\"] = []\n            input_ids = batch[\"input_ids\"].clone()\n            for indices, model_type in zip([sorted_interpret_indices, sorted_output_indices, random_order], [\"interpret\", \"output\", \"random\"]):\n                _input_ids = input_ids.clone()\n                # compute sufficiency\n                #_input_ids[:, indices[: K]] = tokenizer.mask_token_id\n                #print(indices)\n                #exit()\n                _input_ids[:, indices[: int(K * len(indices))]] = tokenizer.mask_token_id\n                _target_model_output = target_model(\n                    input_ids=_input_ids.cuda(),\n                    attention_mask=batch[\"attention_mask\"].cuda(),\n                    token_type_ids=batch[\"token_type_ids\"].cuda() if \"token_type_ids\" in batch else None,\n                    position_ids=batch[\"position_ids\"].cuda() if \"position_ids\" in batch else None\n                )\n                _logits = _target_model_output.logits\n                _label = batch[\"ft_label\"].cpu().item()\n                _pred = _logits.argmax(dim=-1).squeeze(0)\n                if label_mapping is not None:\n                    _pred = label_mapping(_pred.item())\n                _pred_logits = _logits[:, _pred]\n                delta = target_logits_pred - _pred_logits\n                #stat_dict[K][f\"comprehensiveness_{model_type}\"].append(delta.cpu().item())\n                stat_dict[K][f\"comprehensiveness_{model_type}\"].append(int(_pred == _label))\n                _input_ids = input_ids.clone()\n                #_input_ids[:, indices[K: ]] = tokenizer.mask_token_id\n                _input_ids[:, indices[int(K * len(indices)): ]] = tokenizer.mask_token_id\n                _target_model_output = target_model(\n                    input_ids=_input_ids.cuda(),\n                    attention_mask=batch[\"attention_mask\"].cuda(),\n                    token_type_ids=batch[\"token_type_ids\"].cuda() if \"token_type_ids\" in batch else None,\n                    position_ids=batch[\"position_ids\"].cuda() if \"position_ids\" in batch else None\n                )\n                _logits = _target_model_output.logits\n                _pred = _logits.argmax(dim=-1).squeeze(0)\n                if label_mapping is not None:\n                    _pred = label_mapping(_pred.item())\n                _pred_logits = _logits[:, _pred]\n                delta = target_logits_pred - _pred_logits\n                #stat_dict[K][f\"sufficiency_{model_type}\"].append(delta.cpu().item())\n                stat_dict[K][f\"sufficiency_{model_type}\"].append(int(_pred == _label))\n\n    return stat_dict", "\n"]}
{"filename": "draw_lr.py", "chunked_list": ["from matplotlib import pyplot as plt\nimport numpy as np\nimport glob\nimport os\n\n# intersections = [22, 28, 25, 27, 27]\n# kendall = [0.0707, 0.0281, -0.07, 0.003, 0.0261]\n# spearman = [0.1046, 0.0423, -0.05, 0.005, 0.0389]\n# proportions = [0.1, 0.3, 0.5, 0.7, 1]\n# model_name = \"svs\"", "# proportions = [0.1, 0.3, 0.5, 0.7, 1]\n# model_name = \"svs\"\n\n\n#intersections = [24, 29, 24, 24, 25]\n#kendall = [0.0064, 0.0119, 0.00580, -0.00196, -0.0119]\n#spearman = [0.0096, 0.0177, 0.00866, -0.002954, -0.0180]\nproportions = [0.1, 0.3, 0.5, 0.7, 1]\n# proportions = [0.1, 0.3, 0.5]\nproportions_str = [\"0.1\", \"0.3\", \"0.5\", \"0.7\", \"1.0\"]", "# proportions = [0.1, 0.3, 0.5]\nproportions_str = [\"0.1\", \"0.3\", \"0.5\", \"0.7\", \"1.0\"]\nplt.rcParams.update({'font.size': 16})\n# plt.rcParams[\"figure.figsize\"] = (10, 6)\n# proportions_str = [\"0.1\", \"0.3\", \"0.5\"]\n# output_dir = \"visualization/learning_curve\"\n# output_dir = \"visualization/learning_curve_mnli\"\n# task_name = \"mnli\"\ntask_name = \"yelp\"\n# output_dir = \"visualization/learning_curve_yelp_with_fastshap_baseline\"", "task_name = \"yelp\"\n# output_dir = \"visualization/learning_curve_yelp_with_fastshap_baseline\"\noutput_dir = f\"visualization/learning_curve_{task_name}_with_fastshap_baseline\"\nos.makedirs(output_dir, exist_ok=True)\n\n\n# for model_name in [\"svs\", \"lig\", \"lime\"]:\nfor model_name in [\"svs\", ]:\n    all_intersections = dict()\n    all_kendal = dict()\n    all_spearman = dict()\n    for target_eval in [\"svs\",]:\n        intersections = []\n        kendall = []\n        spearman = []\n\n        for prop_str in proportions_str:\n            if task_name == \"mnli\":\n                record_dir = f\"path/to/amortized_model_formal/multi_nli/lr_5e-05-epoch_30/seed_*_prop_{prop_str}/model_{model_name}_norm_False_discrete_False\"\n                fastshap_baseline = 0.23\n            else:\n                assert task_name == \"yelp\"\n                record_dir = f\"path/to/amortized_model_formal/yelp_polarity/lr_5e-05-epoch_30/seed_*_prop_{prop_str}/model_{model_name}_norm_False_discrete_False\"\n                fastshap_baseline = 0.18\n            logs = glob.glob(os.path.join(record_dir, f\"test_log_no_pad_{target_eval}.txt\"))\n            _intersections = []\n            _spearmans = []\n            _kendals = []\n            for logfn in logs:\n                with open(logfn, \"r\", encoding='utf-8') as f_in:\n                    for line in f_in:\n                        # if \"loss at epoch\" in line:\n                        #     _num = float(line.strip().split(\":\")[-1].strip())\n                        #     loss.append(_num)\n                        if \"intersection: \" in line:\n                            _num = float(line.strip().split(\"intersection: \")[-1])\n                            _intersections.append(_num)\n                        if \"spearman:\" in line:\n                            _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                            _spearmans.append(_num)\n                        if \"kendaltau:\" in line:\n                            _num = float(line.strip().split(\"correlation=\")[1].split(\",\")[0])\n                            _kendals.append(_num)\n                            break\n            intersections.append(_intersections)\n            spearman.append(_spearmans)\n            kendall.append(_kendals)\n\n        for ys, yname, color in zip([intersections, kendall, spearman], [\"intersections\", \"kendall\", \"spearman\"], [\"r\", \"g\", \"b\"]):\n            print(f\"plotting {yname} for base-{model_name}-eval-{target_eval}\")\n            arr_ys = np.array(ys)\n            plt.errorbar(range(len(proportions)), np.mean(arr_ys, axis=1), yerr=np.std(arr_ys, axis=1), capsize=3, fmt='o-', color=color, label=\"ours\")\n            plt.xticks(range(len(proportions)), [f\"{int(x * 100)}%\" for x in proportions])\n            plt.xlabel(\"proportion of data used\", fontsize=22)\n            plt.ylabel(yname.capitalize() + \" w/ SVS-25\", fontsize=22)\n            if yname == \"spearman\":\n                plt.axhline(y=fastshap_baseline, label=\"fastshap\", linestyle=\"--\")\n            # plt.title(f\"{model_name}_{yname}\")\n                plt.legend()\n            plt.tight_layout()\n            plt.savefig(os.path.join(output_dir, f\"base_{model_name}_target_{target_eval}_{yname}_{task_name}.pdf\"))\n            plt.clf()\n\n        all_intersections[target_eval] = intersections\n        all_kendal[target_eval] = kendall\n        all_spearman[target_eval] = spearman", ""]}
{"filename": "utils.py", "chunked_list": ["import torch\nimport os\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, concatenate_datasets\nfrom typing import *\nfrom tqdm import tqdm\n\ndef collate_fn(features):\n    ret = {}\n    for k in features[0]:\n        if k not in [\"output_rank\", \"ft_label\"]:\n            ret[k] = torch.tensor([feature[k] for feature in features])\n        else:\n            ret[k] = torch.LongTensor([feature[k] for feature in features])\n    return ret", "\ndef sort_by_file_size(paths):\n    tgts = [(x, os.path.getsize(x)) for x in paths]\n    tgts.sort(key=lambda x: x[1], reverse=True)\n    return [x[0] for x in tgts]\n\ndef get_zero_baselines(datasets: List[Dataset], target_model, tokenizer, args, device=\"cuda\"):\n    buf = []\n    for dataset in datasets:\n        dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n        remove_columns = [\"output\", \"output_rank\", \"ft_label\", \"prediction_dist\", \"special_tokens_mask\", \"id\"]\n        zero_baselines = []\n        for batch in tqdm(dataloader, total=len(dataloader), desc=\"adding baseline\"):\n            new_batch = dict()\n            for k in batch:\n                if k not in remove_columns:\n                    # remove irrelevant columns for bert.forward()\n                    new_batch[k] = batch[k].to(device)\n            new_batch['input_ids'] = new_batch[\"input_ids\"] * 0 + tokenizer.pad_token_id\n            target_model_zero_output = target_model(**new_batch)[0].data.cpu()\n            zero_baselines.append(target_model_zero_output)\n        ret = torch.cat(zero_baselines, dim=0)\n        _ds = Dataset.from_dict({\n            \"zero_baseline\": ret\n        })\n        buf.append(concatenate_datasets([dataset, _ds], axis=1))\n    return buf", "\n\n\n"]}
{"filename": "export_model_output_as_thermostat.py", "chunked_list": ["import torch\nimport copy\nimport random\nimport argparse\nimport json\nimport numpy as np\nfrom torch import nn, optim\nimport loguru\nfrom tqdm import tqdm\nfrom scipy.stats import spearmanr, kendalltau", "from tqdm import tqdm\nfrom scipy.stats import spearmanr, kendalltau\nfrom amortized_model import AmortizedModel\nfrom create_dataset import (\n    output_dir as dataset_dir,\n    model_cache_dir\n)\nimport os\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset", "from torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom transformers import DataCollatorForTokenClassification, AutoModelForSequenceClassification, PreTrainedTokenizer, \\\n    AutoTokenizer\nfrom config import Args, GetParser\nfrom utils import collate_fn, get_zero_baselines\nfrom metrics import get_eraser_metrics\n\n\n# example_output = '{\"dataset\": {\"batch_size\": 1, \"columns\": [\"input_ids\", \"attention_mask\", \"special_tokens_mask\", \"token_type_ids\", \"labels\"], \"end\": 3600, \"name\": \"yelp_polarity\", \"root_dir\": \"./experiments/thermostat/datasets\", \"split\": \"test\", \"label_names\": [\"1\", \"2\"], \"version\": \"1.0.0\"}, \"model\": {\"mode_load\": \"hf\", \"name\": \"textattack/bert-base-uncased-yelp-polarity\", \"path_model\": null, \"tokenization\": {\"max_length\": 512, \"padding\": \"max_length\", \"return_tensors\": \"np\", \"special_tokens_mask\": true, \"truncation\": true}, \"tokenizer\": \"PreTrainedTokenizerFast(name_or_path='textattack/bert-base-uncased-yelp-polarity', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\"}, \"explainer\": {\"internal_batch_size\": 1, \"n_samples\": 25, \"name\": \"KernelShap\"}, \"batch\": 0, \"instance\": 0, \"index_running\": 0, \"input_ids\": [101, 10043, 2000, 2060, 4391, 1010, 1045, 2031, 5717, 10821, 2055, 1996, 2326, 2030, 1996, 7597, 1012, 1045, 2031, 2042, 2893, 12824, 2326, 2182, 2005, 1996, 2627, 1019, 2086, 2085, 1010, 1998, 4102, 2000, 2026, 3325, 2007, 3182, 2066, 27233, 3337, 1010, 2122, 4364, 2024, 5281, 1998, 2113, 2054, 2027, 1005, 2128, 2725, 1012, 1032, 6583, 4877, 2080, 1010, 2023, 2003, 2028, 2173, 2008, 1045, 2079, 2025, 2514, 2066, 1045, 2572, 2108, 2579, 5056, 1997, 1010, 2074, 2138, 1997, 2026, 5907, 1012, 2060, 8285, 9760, 2031, 2042, 12536, 2005, 3007, 6026, 2006, 2026, 18173, 1997, 3765, 1010, 1998, 2031, 8631, 2026, 2924, 4070, 4318, 1012, 2021, 2182, 1010, 2026, 2326, 1998, 2346, 6325, 2038, 2035, 2042, 2092, 4541, 1011, 1998, 2292, 2039, 2000, 2033, 2000, 5630, 1012, 1032, 16660, 2094, 2027, 2074, 10601, 1996, 3403, 2282, 1012, 2009, 3504, 1037, 2843, 2488, 2084, 2009, 2106, 1999, 3025, 2086, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"label\": 1, \"attributions\": [-0.15166577696800232, 0.009686493314802647, 0.022048579528927803, -0.010080059990286827, 0.012372241355478764, -0.014180195517838001, 0.022048499435186386, 0.016043271869421005, 0.022048546001315117, -0.00825815461575985, -0.01712176762521267, 0.012372172437608242, -0.007291753310710192, 0.01604328118264675, 0.018203958868980408, -0.025593385100364685, -0.010080037638545036, -0.0021892308723181486, 0.012372259981930256, 0.012372216209769249, 0.023971816524863243, 0.016043292358517647, 0.031862590461969376, -0.028381315991282463, 0.003578625852242112, 0.012372217141091824, -0.019756361842155457, 0.002530973171815276, 0.03623899817466736, -0.010080034844577312, 0.012372217141091824, -0.014180171303451061, 0.009154818020761013, 0.003578625852242112, -0.007291719317436218, 0.012372217141091824, 0.008152471855282784, 0.016043270006775856, 0.0010390281677246094, -0.017149394378066063, 0.0023846065159887075, -0.004741252399981022, 0.012198690325021744, -0.011731443926692009, -0.0012864458840340376, -0.0015238537453114986, 0.017966555431485176, 0.003578625852242112, -0.006065146531909704, 0.016748590394854546, 0.008152471855282784, 0.0023846065159887075, -0.002189238090068102, 0.01796814799308777, 0.003578625852242112, 0.012372217141091824, 0.016043270006775856, 0.014295504428446293, -0.10412800312042236, 0.016043270006775856, -0.0015238537453114986, 0.03623899817466736, 0.013531191274523735, -0.007291719317436218, 0.02204854227602482, 0.016043270006775856, 0.02204854227602482, -0.007291719317436218, -0.004587118048220873, -0.010080034844577312, 0.003578625852242112, 0.035075053572654724, -0.038211312144994736, 0.009154818020761013, 0.003578625852242112, 0.008152471855282784, 0.009686610661447048, 0.012372217141091824, 0.004481421783566475, 0.016043270006775856, 0.016043270006775856, 0.012372217141091824, -0.0012864458840340376, 0.012198690325021744, 0.016748590394854546, 0.016748590394854546, 0.02204854227602482, 0.012372217141091824, -0.002189238090068102, -0.012710582464933395, 0.003578625852242112, 0.01545447576791048, -0.002189238090068102, -0.029719743877649307, 0.012372217141091824, 0.02204854227602482, 0.019564373418688774, -0.002189238090068102, 0.015076815150678158, 0.012904006987810135, 0.016043270006775856, 0.016748590394854546, 0.03186262771487236, 0.003854867070913315, 0.003578625852242112, 0.012372217141091824, 0.018671875819563866, 0.03623899817466736, 0.02204854227602482, 0.03186262771487236, 0.02204854227602482, -0.004278442356735468, -0.004741252399981022, -0.008890857920050621, 0.003578625852242112, 0.003578625852242112, 0.017966555431485176, 0.03186262771487236, -0.010080034844577312, -0.019756361842155457, 0.023971829563379288, 0.012372217141091824, 0.003578625852242112, 0.0040879095904529095, 0.013531191274523735, 0.018203964456915855, 0.009154818020761013, -0.0015238537453114986, 0.003578625852242112, -0.01281975582242012, -0.004741252399981022, 0.016043270006775856, -0.014180171303451061, 0.003578625852242112, 0.03237045556306839, -0.015182516537606716, 0.016043270006775856, 0.02204854227602482, -0.010080034844577312, -0.0012864458840340376, 0.012372217141091824, 0.02204854227602482, 0.013531191274523735, 0.003578625852242112, 0.014935438521206379, 0.018671875819563866, 0.013531191274523735, 0.03623899817466736, 0.016748590394854546, 0.020859969779849052, 0.012372217141091824, 0.03186262771487236, 0.02204854227602482, -0.008412305265665054, 0.03186262771487236, 0.02212294563651085, -0.002189238090068102, 0.018671875819563866, 0.014935438521206379, -0.0015238537453114986, 0.003578625852242112, 0.003578625852242112, 0.003578625852242112, 0.03237045556306839, 0.016043270006775856, -0.004503846634179354, 0.03186262771487236, 0.004481421783566475, 0.016748255118727684, 0.016043270006775856, 0.02204854227602482, 0.012198690325021744, 0.015076815150678158, -0.032937146723270416, -0.0015238537453114986, 0.016043270006775856, 0.02204854227602482, 0.003578625852242112, -0.0015238537453114986, -0.0015238537453114986, -0.017121760174632072, 0.003578625852242112, 0.003578625852242112, 0.002530973171815276, 0.03186262771487236, -0.03287728875875473, 0.0023846065159887075, 0.03623899817466736, 0.012372217141091824, -0.0012864458840340376, 0.008152471855282784, 0.012372217141091824, -0.04084590822458267, 0.02204854227602482, -0.008412305265665054, 0.008152471855282784, -0.045401785522699356, 0.007830922491848469, 0.003578625852242112, 0.0031351549550890923, 0.012372217141091824, 0.009662647731602192, -0.024617265909910202, -0.028381265699863434, 0.009154818020761013, 0.02204854227602482, 0.003578625852242112, 0.016043270006775856, 0.012372217141091824, 0.016043270006775856, -0.04084590822458267, -0.010080034844577312, 0.003854867070913315, 0.016043270006775856, 0.013531191274523735, 0.003176276572048664, 0.02204854227602482, 0.019055737182497978, -0.0053684343583881855, -0.015182516537606716, -0.007291719317436218, 0.02656267210841179, -0.008475658483803272, 0.02204854227602482, -0.00043386375182308257, 0.012372217141091824, 0.016748590394854546, 0.004481421783566475, 0.003854867070913315, 0.008152471855282784, 0.017966555431485176, 0.001068122568540275, 0.003578625852242112, -0.010509118437767029, 1.0285876669513527e-05, 0.03623899817466736, -0.011833010241389275, -4.1121522372122854e-05, 0.016748590394854546, 0.002530973171815276, 0.003578625852242112, 0.012372217141091824, 0.022021381184458733, 0.02204854227602482, -0.010509118437767029, 0.023971829563379288, 0.02915305830538273, 0.009154818020761013, 0.03623899817466736, 0.02204854227602482, -0.06154629588127136, 0.03186262771487236, 0.004481421783566475, 0.002530973171815276, -0.004035932011902332, 0.02204854227602482, 0.016043270006775856, -0.0015238537453114986, 0.016043270006775856, -0.0006864278111606836, -0.009803798981010914, -0.00388179998844862, 0.03186262771487236, 0.003578625852242112, 0.012372217141091824, 0.023345274850726128, 0.012372217141091824, -0.02237599529325962, 0.0084234569221735, -0.018704941496253014, 0.016748590394854546, -0.01588624157011509, 0.008152471855282784, 0.016748590394854546, 0.009662647731602192, 0.023971829563379288, 0.016748590394854546, 0.02204854227602482, -0.01839991845190525, -0.010080034844577312, -0.0015238537453114986, 0.016748590394854546, 0.012198690325021744, 0.016748590394854546, 0.016043270006775856, 0.012372217141091824, 0.012372217141091824, 0.02204854227602482, 0.02204854227602482, 0.008152471855282784, 0.02204854227602482, 0.016043270006775856, 0.02204854227602482, -0.004741252399981022, 0.03623899817466736, -0.0015222595538944006, 7.39634851925075e-05, 0.02204854227602482, 0.02204854227602482, 0.016043270006775856, 0.017966555431485176, 0.03623899817466736, 0.001264021499082446, -0.0015238537453114986, -0.0015238537453114986, 0.02204854227602482, 0.012372217141091824, -0.002189238090068102, 0.02204854227602482, -0.0012864458840340376, -0.00043386375182308257, 0.016748590394854546, 0.02204854227602482, -0.002189238090068102, 0.003578625852242112, 0.016748590394854546, 0.016748590394854546, -0.0012864458840340376, 0.01796814799308777, 0.009154818020761013, 0.02204854227602482, 0.02204854227602482, 0.016043270006775856, 0.014824969694018364, -0.007291719317436218, 0.016748590394854546, 0.008527638390660286, -0.019756361842155457, -0.010509118437767029, 0.002530973171815276, 0.004481421783566475, -0.01945282518863678, -0.0012864458840340376, 0.0023846065159887075, 0.012372217141091824, -0.002189238090068102, 0.016043270006775856, -0.002189238090068102, -0.012561912648379803, 0.016043270006775856, 0.02656267210841179, -0.02269398421049118, -0.02237599529325962, 0.016043270006775856, -0.007291719317436218, 0.012198690325021744, 0.009154818020761013, 0.016043270006775856, -0.0015238537453114986, -0.002189238090068102, 0.016748590394854546, 0.03237045556306839, -0.03827592730522156, 0.0023846065159887075, -0.010080034844577312, 0.012372217141091824, 0.03623899817466736, 0.02204854227602482, 0.003854867070913315, 0.017966555431485176, 0.013531191274523735, 0.012372217141091824, 0.016043270006775856, 0.0023846065159887075, 0.009154818020761013, -0.024617265909910202, 0.03186262771487236, 0.03623899817466736, -0.002189238090068102, -0.010509118437767029, -0.010509118437767029, -0.009803798981010914, 0.02656267210841179, 0.02204854227602482, 0.02204854227602482, -0.012477915734052658, -0.010080034844577312, 0.03186262771487236, 0.02204854227602482, -0.007291719317436218, -0.025430934503674507, 0.003578625852242112, 0.03186262771487236, 0.012372217141091824, 0.0006368431495502591, 0.009154818020761013, 0.016043270006775856, -0.029719743877649307, -0.007291719317436218, 0.016043270006775856, -0.0015238537453114986, -0.028381265699863434, 0.008152471855282784, -0.0012864458840340376, -0.0031351549550890923, 0.016043270006775856, 0.0023846065159887075, 0.016748590394854546, -0.0012864458840340376, 0.012372217141091824, 0.03186262771487236, -0.017121760174632072, 0.016748590394854546, -0.02237599529325962, -0.004503846634179354, 0.02656267210841179, 0.03186262771487236, 0.012198690325021744, 0.008152471855282784, 0.016043270006775856, 0.014295504428446293, 0.016043270006775856, 0.007932491600513458, -0.0015238537453114986, 0.015584642998874187, -0.003452391130849719, 0.03186262771487236, -0.00043386375182308257, 0.003854867070913315, 0.012372217141091824, 0.008152471855282784, 0.004481421783566475, 0.008152471855282784, -0.00043386375182308257, 0.02204854227602482, -0.0012864458840340376, -0.0012864458840340376, -0.002189238090068102, -0.012344200164079666, 0.017966555431485176, 0.01947673410177231, 0.016043270006775856, -0.016788210719823837, 0.016043270006775856, 0.009154818020761013, 0.02204854227602482, 0.02656267210841179, 0.0023846065159887075, -0.010509118437767029, 0.03186262771487236, -0.004278442356735468, 0.016748590394854546, 0.008338750340044498, 0.009154818020761013, 0.02204854227602482, -0.0015222595538944006, 0.02915305830538273, 0.02204854227602482, 0.009296262636780739, -0.010080034844577312, 0.012372217141091824, -0.022973762825131416, -0.002189238090068102, 0.016043270006775856, 0.02204854227602482, 0.001264021499082446, -0.038211312144994736, -0.007291719317436218, 0.005778150167316198, 0.008152471855282784, 0.003578625852242112, 0.012198690325021744, -0.029719743877649307, 0.016043270006775856, -0.007291719317436218, 0.02204854227602482, -0.015182516537606716, 0.016043270006775856, -0.004035932011902332, 0.012372217141091824, 0.013531191274523735, -0.0012864458840340376, 0.016748590394854546, 0.012372217141091824, 0.02204854227602482, 0.017966555431485176, 0.008152471855282784, 0.03186262771487236, 0.03186262771487236, -0.010080034844577312, -0.0002659528108779341, 0.02204854227602482, 0.03186262771487236, 0.016043270006775856, 0.009662647731602192, 0.003578625852242112, 0.008527638390660286, 0.004481421783566475, 0.001304063480347395, 0.016748590394854546, -0.002189238090068102, 0.007729377131909132, -0.020859969779849052, -0.0012864458840340376, 0.001264021499082446, 0.008440319448709488, 0.012372217141091824, 1.4129986573903504e-16, 0.03186262771487236, 0.016043270006775856, 0.002530973171815276, -7.236459887000114e-17, -0.010509118437767029, -0.02237599529325962, 0.02204854227602482, -0.007291719317436218, 0.03186262771487236, 0.003578625852242112, -0.0012864458840340376, -0.0015238537453114986, 0.008440319448709488, 0.023971829563379288], \"predictions\": [-4.52530574798584, 4.283736705780029]}'\ndef get_example_output():\n    # Please change the filepath to your own path\n    filepath = \"/path/to/thermostat/experiments/thermostat/yelp_polarity/bert/kernelshap-3600/seed_1/[date].KernelShap.jsonl\"\n    with open(filepath, \"r\", encoding='utf-8') as f_in:\n        for line in f_in:\n            obj = json.loads(line.strip())\n            return obj", "\n# example_output = '{\"dataset\": {\"batch_size\": 1, \"columns\": [\"input_ids\", \"attention_mask\", \"special_tokens_mask\", \"token_type_ids\", \"labels\"], \"end\": 3600, \"name\": \"yelp_polarity\", \"root_dir\": \"./experiments/thermostat/datasets\", \"split\": \"test\", \"label_names\": [\"1\", \"2\"], \"version\": \"1.0.0\"}, \"model\": {\"mode_load\": \"hf\", \"name\": \"textattack/bert-base-uncased-yelp-polarity\", \"path_model\": null, \"tokenization\": {\"max_length\": 512, \"padding\": \"max_length\", \"return_tensors\": \"np\", \"special_tokens_mask\": true, \"truncation\": true}, \"tokenizer\": \"PreTrainedTokenizerFast(name_or_path='textattack/bert-base-uncased-yelp-polarity', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\"}, \"explainer\": {\"internal_batch_size\": 1, \"n_samples\": 25, \"name\": \"KernelShap\"}, \"batch\": 0, \"instance\": 0, \"index_running\": 0, \"input_ids\": [101, 10043, 2000, 2060, 4391, 1010, 1045, 2031, 5717, 10821, 2055, 1996, 2326, 2030, 1996, 7597, 1012, 1045, 2031, 2042, 2893, 12824, 2326, 2182, 2005, 1996, 2627, 1019, 2086, 2085, 1010, 1998, 4102, 2000, 2026, 3325, 2007, 3182, 2066, 27233, 3337, 1010, 2122, 4364, 2024, 5281, 1998, 2113, 2054, 2027, 1005, 2128, 2725, 1012, 1032, 6583, 4877, 2080, 1010, 2023, 2003, 2028, 2173, 2008, 1045, 2079, 2025, 2514, 2066, 1045, 2572, 2108, 2579, 5056, 1997, 1010, 2074, 2138, 1997, 2026, 5907, 1012, 2060, 8285, 9760, 2031, 2042, 12536, 2005, 3007, 6026, 2006, 2026, 18173, 1997, 3765, 1010, 1998, 2031, 8631, 2026, 2924, 4070, 4318, 1012, 2021, 2182, 1010, 2026, 2326, 1998, 2346, 6325, 2038, 2035, 2042, 2092, 4541, 1011, 1998, 2292, 2039, 2000, 2033, 2000, 5630, 1012, 1032, 16660, 2094, 2027, 2074, 10601, 1996, 3403, 2282, 1012, 2009, 3504, 1037, 2843, 2488, 2084, 2009, 2106, 1999, 3025, 2086, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"label\": 1, \"attributions\": [-0.15166577696800232, 0.009686493314802647, 0.022048579528927803, -0.010080059990286827, 0.012372241355478764, -0.014180195517838001, 0.022048499435186386, 0.016043271869421005, 0.022048546001315117, -0.00825815461575985, -0.01712176762521267, 0.012372172437608242, -0.007291753310710192, 0.01604328118264675, 0.018203958868980408, -0.025593385100364685, -0.010080037638545036, -0.0021892308723181486, 0.012372259981930256, 0.012372216209769249, 0.023971816524863243, 0.016043292358517647, 0.031862590461969376, -0.028381315991282463, 0.003578625852242112, 0.012372217141091824, -0.019756361842155457, 0.002530973171815276, 0.03623899817466736, -0.010080034844577312, 0.012372217141091824, -0.014180171303451061, 0.009154818020761013, 0.003578625852242112, -0.007291719317436218, 0.012372217141091824, 0.008152471855282784, 0.016043270006775856, 0.0010390281677246094, -0.017149394378066063, 0.0023846065159887075, -0.004741252399981022, 0.012198690325021744, -0.011731443926692009, -0.0012864458840340376, -0.0015238537453114986, 0.017966555431485176, 0.003578625852242112, -0.006065146531909704, 0.016748590394854546, 0.008152471855282784, 0.0023846065159887075, -0.002189238090068102, 0.01796814799308777, 0.003578625852242112, 0.012372217141091824, 0.016043270006775856, 0.014295504428446293, -0.10412800312042236, 0.016043270006775856, -0.0015238537453114986, 0.03623899817466736, 0.013531191274523735, -0.007291719317436218, 0.02204854227602482, 0.016043270006775856, 0.02204854227602482, -0.007291719317436218, -0.004587118048220873, -0.010080034844577312, 0.003578625852242112, 0.035075053572654724, -0.038211312144994736, 0.009154818020761013, 0.003578625852242112, 0.008152471855282784, 0.009686610661447048, 0.012372217141091824, 0.004481421783566475, 0.016043270006775856, 0.016043270006775856, 0.012372217141091824, -0.0012864458840340376, 0.012198690325021744, 0.016748590394854546, 0.016748590394854546, 0.02204854227602482, 0.012372217141091824, -0.002189238090068102, -0.012710582464933395, 0.003578625852242112, 0.01545447576791048, -0.002189238090068102, -0.029719743877649307, 0.012372217141091824, 0.02204854227602482, 0.019564373418688774, -0.002189238090068102, 0.015076815150678158, 0.012904006987810135, 0.016043270006775856, 0.016748590394854546, 0.03186262771487236, 0.003854867070913315, 0.003578625852242112, 0.012372217141091824, 0.018671875819563866, 0.03623899817466736, 0.02204854227602482, 0.03186262771487236, 0.02204854227602482, -0.004278442356735468, -0.004741252399981022, -0.008890857920050621, 0.003578625852242112, 0.003578625852242112, 0.017966555431485176, 0.03186262771487236, -0.010080034844577312, -0.019756361842155457, 0.023971829563379288, 0.012372217141091824, 0.003578625852242112, 0.0040879095904529095, 0.013531191274523735, 0.018203964456915855, 0.009154818020761013, -0.0015238537453114986, 0.003578625852242112, -0.01281975582242012, -0.004741252399981022, 0.016043270006775856, -0.014180171303451061, 0.003578625852242112, 0.03237045556306839, -0.015182516537606716, 0.016043270006775856, 0.02204854227602482, -0.010080034844577312, -0.0012864458840340376, 0.012372217141091824, 0.02204854227602482, 0.013531191274523735, 0.003578625852242112, 0.014935438521206379, 0.018671875819563866, 0.013531191274523735, 0.03623899817466736, 0.016748590394854546, 0.020859969779849052, 0.012372217141091824, 0.03186262771487236, 0.02204854227602482, -0.008412305265665054, 0.03186262771487236, 0.02212294563651085, -0.002189238090068102, 0.018671875819563866, 0.014935438521206379, -0.0015238537453114986, 0.003578625852242112, 0.003578625852242112, 0.003578625852242112, 0.03237045556306839, 0.016043270006775856, -0.004503846634179354, 0.03186262771487236, 0.004481421783566475, 0.016748255118727684, 0.016043270006775856, 0.02204854227602482, 0.012198690325021744, 0.015076815150678158, -0.032937146723270416, -0.0015238537453114986, 0.016043270006775856, 0.02204854227602482, 0.003578625852242112, -0.0015238537453114986, -0.0015238537453114986, -0.017121760174632072, 0.003578625852242112, 0.003578625852242112, 0.002530973171815276, 0.03186262771487236, -0.03287728875875473, 0.0023846065159887075, 0.03623899817466736, 0.012372217141091824, -0.0012864458840340376, 0.008152471855282784, 0.012372217141091824, -0.04084590822458267, 0.02204854227602482, -0.008412305265665054, 0.008152471855282784, -0.045401785522699356, 0.007830922491848469, 0.003578625852242112, 0.0031351549550890923, 0.012372217141091824, 0.009662647731602192, -0.024617265909910202, -0.028381265699863434, 0.009154818020761013, 0.02204854227602482, 0.003578625852242112, 0.016043270006775856, 0.012372217141091824, 0.016043270006775856, -0.04084590822458267, -0.010080034844577312, 0.003854867070913315, 0.016043270006775856, 0.013531191274523735, 0.003176276572048664, 0.02204854227602482, 0.019055737182497978, -0.0053684343583881855, -0.015182516537606716, -0.007291719317436218, 0.02656267210841179, -0.008475658483803272, 0.02204854227602482, -0.00043386375182308257, 0.012372217141091824, 0.016748590394854546, 0.004481421783566475, 0.003854867070913315, 0.008152471855282784, 0.017966555431485176, 0.001068122568540275, 0.003578625852242112, -0.010509118437767029, 1.0285876669513527e-05, 0.03623899817466736, -0.011833010241389275, -4.1121522372122854e-05, 0.016748590394854546, 0.002530973171815276, 0.003578625852242112, 0.012372217141091824, 0.022021381184458733, 0.02204854227602482, -0.010509118437767029, 0.023971829563379288, 0.02915305830538273, 0.009154818020761013, 0.03623899817466736, 0.02204854227602482, -0.06154629588127136, 0.03186262771487236, 0.004481421783566475, 0.002530973171815276, -0.004035932011902332, 0.02204854227602482, 0.016043270006775856, -0.0015238537453114986, 0.016043270006775856, -0.0006864278111606836, -0.009803798981010914, -0.00388179998844862, 0.03186262771487236, 0.003578625852242112, 0.012372217141091824, 0.023345274850726128, 0.012372217141091824, -0.02237599529325962, 0.0084234569221735, -0.018704941496253014, 0.016748590394854546, -0.01588624157011509, 0.008152471855282784, 0.016748590394854546, 0.009662647731602192, 0.023971829563379288, 0.016748590394854546, 0.02204854227602482, -0.01839991845190525, -0.010080034844577312, -0.0015238537453114986, 0.016748590394854546, 0.012198690325021744, 0.016748590394854546, 0.016043270006775856, 0.012372217141091824, 0.012372217141091824, 0.02204854227602482, 0.02204854227602482, 0.008152471855282784, 0.02204854227602482, 0.016043270006775856, 0.02204854227602482, -0.004741252399981022, 0.03623899817466736, -0.0015222595538944006, 7.39634851925075e-05, 0.02204854227602482, 0.02204854227602482, 0.016043270006775856, 0.017966555431485176, 0.03623899817466736, 0.001264021499082446, -0.0015238537453114986, -0.0015238537453114986, 0.02204854227602482, 0.012372217141091824, -0.002189238090068102, 0.02204854227602482, -0.0012864458840340376, -0.00043386375182308257, 0.016748590394854546, 0.02204854227602482, -0.002189238090068102, 0.003578625852242112, 0.016748590394854546, 0.016748590394854546, -0.0012864458840340376, 0.01796814799308777, 0.009154818020761013, 0.02204854227602482, 0.02204854227602482, 0.016043270006775856, 0.014824969694018364, -0.007291719317436218, 0.016748590394854546, 0.008527638390660286, -0.019756361842155457, -0.010509118437767029, 0.002530973171815276, 0.004481421783566475, -0.01945282518863678, -0.0012864458840340376, 0.0023846065159887075, 0.012372217141091824, -0.002189238090068102, 0.016043270006775856, -0.002189238090068102, -0.012561912648379803, 0.016043270006775856, 0.02656267210841179, -0.02269398421049118, -0.02237599529325962, 0.016043270006775856, -0.007291719317436218, 0.012198690325021744, 0.009154818020761013, 0.016043270006775856, -0.0015238537453114986, -0.002189238090068102, 0.016748590394854546, 0.03237045556306839, -0.03827592730522156, 0.0023846065159887075, -0.010080034844577312, 0.012372217141091824, 0.03623899817466736, 0.02204854227602482, 0.003854867070913315, 0.017966555431485176, 0.013531191274523735, 0.012372217141091824, 0.016043270006775856, 0.0023846065159887075, 0.009154818020761013, -0.024617265909910202, 0.03186262771487236, 0.03623899817466736, -0.002189238090068102, -0.010509118437767029, -0.010509118437767029, -0.009803798981010914, 0.02656267210841179, 0.02204854227602482, 0.02204854227602482, -0.012477915734052658, -0.010080034844577312, 0.03186262771487236, 0.02204854227602482, -0.007291719317436218, -0.025430934503674507, 0.003578625852242112, 0.03186262771487236, 0.012372217141091824, 0.0006368431495502591, 0.009154818020761013, 0.016043270006775856, -0.029719743877649307, -0.007291719317436218, 0.016043270006775856, -0.0015238537453114986, -0.028381265699863434, 0.008152471855282784, -0.0012864458840340376, -0.0031351549550890923, 0.016043270006775856, 0.0023846065159887075, 0.016748590394854546, -0.0012864458840340376, 0.012372217141091824, 0.03186262771487236, -0.017121760174632072, 0.016748590394854546, -0.02237599529325962, -0.004503846634179354, 0.02656267210841179, 0.03186262771487236, 0.012198690325021744, 0.008152471855282784, 0.016043270006775856, 0.014295504428446293, 0.016043270006775856, 0.007932491600513458, -0.0015238537453114986, 0.015584642998874187, -0.003452391130849719, 0.03186262771487236, -0.00043386375182308257, 0.003854867070913315, 0.012372217141091824, 0.008152471855282784, 0.004481421783566475, 0.008152471855282784, -0.00043386375182308257, 0.02204854227602482, -0.0012864458840340376, -0.0012864458840340376, -0.002189238090068102, -0.012344200164079666, 0.017966555431485176, 0.01947673410177231, 0.016043270006775856, -0.016788210719823837, 0.016043270006775856, 0.009154818020761013, 0.02204854227602482, 0.02656267210841179, 0.0023846065159887075, -0.010509118437767029, 0.03186262771487236, -0.004278442356735468, 0.016748590394854546, 0.008338750340044498, 0.009154818020761013, 0.02204854227602482, -0.0015222595538944006, 0.02915305830538273, 0.02204854227602482, 0.009296262636780739, -0.010080034844577312, 0.012372217141091824, -0.022973762825131416, -0.002189238090068102, 0.016043270006775856, 0.02204854227602482, 0.001264021499082446, -0.038211312144994736, -0.007291719317436218, 0.005778150167316198, 0.008152471855282784, 0.003578625852242112, 0.012198690325021744, -0.029719743877649307, 0.016043270006775856, -0.007291719317436218, 0.02204854227602482, -0.015182516537606716, 0.016043270006775856, -0.004035932011902332, 0.012372217141091824, 0.013531191274523735, -0.0012864458840340376, 0.016748590394854546, 0.012372217141091824, 0.02204854227602482, 0.017966555431485176, 0.008152471855282784, 0.03186262771487236, 0.03186262771487236, -0.010080034844577312, -0.0002659528108779341, 0.02204854227602482, 0.03186262771487236, 0.016043270006775856, 0.009662647731602192, 0.003578625852242112, 0.008527638390660286, 0.004481421783566475, 0.001304063480347395, 0.016748590394854546, -0.002189238090068102, 0.007729377131909132, -0.020859969779849052, -0.0012864458840340376, 0.001264021499082446, 0.008440319448709488, 0.012372217141091824, 1.4129986573903504e-16, 0.03186262771487236, 0.016043270006775856, 0.002530973171815276, -7.236459887000114e-17, -0.010509118437767029, -0.02237599529325962, 0.02204854227602482, -0.007291719317436218, 0.03186262771487236, 0.003578625852242112, -0.0012864458840340376, -0.0015238537453114986, 0.008440319448709488, 0.023971829563379288], \"predictions\": [-4.52530574798584, 4.283736705780029]}'\ndef get_example_output():\n    # Please change the filepath to your own path\n    filepath = \"/path/to/thermostat/experiments/thermostat/yelp_polarity/bert/kernelshap-3600/seed_1/[date].KernelShap.jsonl\"\n    with open(filepath, \"r\", encoding='utf-8') as f_in:\n        for line in f_in:\n            obj = json.loads(line.strip())\n            return obj\n\ndef running_step(dataloader, model, K, optimizer=None, is_train=False, save=False, args=None):\n    def get_top_k(_output):\n        _rank_output = [(x, i) for i, x in enumerate(_output)]\n        _rank_output.sort(key=lambda x: x[0], reverse=True)\n        _rank_output = [x[1] for x in _rank_output][:K]\n        return _rank_output\n\n    # def dropout(_input):\n    #     _rand = torch.rand_like(_input.float())\n    #     _mask = _rand >= 0.5\n    #     return _mask.long() * _input\n\n    all_loss = 0\n    all_outputs = []\n    all_aux_outputs = []\n    all_refs = []\n    all_attn = []\n    all_ins = []\n    count_elements = 0\n    spearman = []\n    kendals = []\n    intersection = []\n    # dropout = nn.Dropout(inplace=True)\n    desc = \"testing\"\n    if is_train:\n        assert optimizer is not None\n        optimizer.zero_grad()\n        desc = 'training'\n    for batch in tqdm(dataloader, desc=desc):\n        # if is_train:\n        # # add masking like FASTSHAP\n        #     dropout(batch[\"attention_mask\"])\n        if hasattr(model, \"multitask\") and model.multitask:\n            main_output, main_loss, aux_output, aux_loss = model(batch)\n            output = main_output\n            loss = main_loss\n            all_aux_outputs.extend((aux_output.argmax(dim=-1) == batch[\"ft_label\"].cuda()).detach().cpu().tolist())\n        else:\n            output, loss = model(batch)\n        if is_train:\n            if not hasattr(args, \"discrete\") or not args.discrete:\n                if len(all_aux_outputs) == 0:\n                    loss = loss\n                else:\n                    loss = torch.sqrt(loss) + aux_loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # recording purposes\n        all_loss += loss.item()\n        # # do not count [CLS]\n        # batch[\"attention_mask\"][:, 0] = 0\n\n        attn_mask = batch[\"attention_mask\"].cuda()\n        batch[\"output\"] = batch[\"output\"].cuda()\n        for _ind in range(len(output)):\n            _output = output[_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            _ref = batch[\"output\"][_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            all_attn.append(attn_mask.detach().cpu().numpy())\n            all_ins.append(batch['input_ids'].detach().cpu().numpy())\n            _rank_output = get_top_k(_output)\n            _rank_ref = get_top_k(_ref)\n            intersect_num = len(set(_rank_ref) & set(_rank_output))\n            _spearman, p_val = spearmanr(_output, _ref, axis=0)\n            _kendal, kp_val = kendalltau(_output, _ref)\n            spearman.append(_spearman)\n            kendals.append(_kendal)\n            intersection.append(intersect_num)\n            all_outputs.append(_output)\n            all_refs.append(_ref)\n            if not is_train:\n                all_aux_outputs.append(output[_ind].detach().cpu().numpy())\n\n        count_elements += batch[\"attention_mask\"].sum().item()\n    if save and args is not None:\n        torch.save([all_outputs, all_refs, all_attn, all_ins],\n                   os.path.join(os.path.dirname(args.save_path),\n                                os.path.basename(args.save_path).strip(\".pt\"),\n                                \"test_outputs_output_verified.pkl\")\n                   )\n    return all_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_outputs", "\ndef running_step(dataloader, model, K, optimizer=None, is_train=False, save=False, args=None):\n    def get_top_k(_output):\n        _rank_output = [(x, i) for i, x in enumerate(_output)]\n        _rank_output.sort(key=lambda x: x[0], reverse=True)\n        _rank_output = [x[1] for x in _rank_output][:K]\n        return _rank_output\n\n    # def dropout(_input):\n    #     _rand = torch.rand_like(_input.float())\n    #     _mask = _rand >= 0.5\n    #     return _mask.long() * _input\n\n    all_loss = 0\n    all_outputs = []\n    all_aux_outputs = []\n    all_refs = []\n    all_attn = []\n    all_ins = []\n    count_elements = 0\n    spearman = []\n    kendals = []\n    intersection = []\n    # dropout = nn.Dropout(inplace=True)\n    desc = \"testing\"\n    if is_train:\n        assert optimizer is not None\n        optimizer.zero_grad()\n        desc = 'training'\n    for batch in tqdm(dataloader, desc=desc):\n        # if is_train:\n        # # add masking like FASTSHAP\n        #     dropout(batch[\"attention_mask\"])\n        if hasattr(model, \"multitask\") and model.multitask:\n            main_output, main_loss, aux_output, aux_loss = model(batch)\n            output = main_output\n            loss = main_loss\n            all_aux_outputs.extend((aux_output.argmax(dim=-1) == batch[\"ft_label\"].cuda()).detach().cpu().tolist())\n        else:\n            output, loss = model(batch)\n        if is_train:\n            if not hasattr(args, \"discrete\") or not args.discrete:\n                if len(all_aux_outputs) == 0:\n                    loss = loss\n                else:\n                    loss = torch.sqrt(loss) + aux_loss\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # recording purposes\n        all_loss += loss.item()\n        # # do not count [CLS]\n        # batch[\"attention_mask\"][:, 0] = 0\n\n        attn_mask = batch[\"attention_mask\"].cuda()\n        batch[\"output\"] = batch[\"output\"].cuda()\n        for _ind in range(len(output)):\n            _output = output[_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            _ref = batch[\"output\"][_ind][attn_mask[_ind] > 0].detach().cpu().numpy()\n            all_attn.append(attn_mask.detach().cpu().numpy())\n            all_ins.append(batch['input_ids'].detach().cpu().numpy())\n            _rank_output = get_top_k(_output)\n            _rank_ref = get_top_k(_ref)\n            intersect_num = len(set(_rank_ref) & set(_rank_output))\n            _spearman, p_val = spearmanr(_output, _ref, axis=0)\n            _kendal, kp_val = kendalltau(_output, _ref)\n            spearman.append(_spearman)\n            kendals.append(_kendal)\n            intersection.append(intersect_num)\n            all_outputs.append(_output)\n            all_refs.append(_ref)\n            if not is_train:\n                all_aux_outputs.append(output[_ind].detach().cpu().numpy())\n\n        count_elements += batch[\"attention_mask\"].sum().item()\n    if save and args is not None:\n        torch.save([all_outputs, all_refs, all_attn, all_ins],\n                   os.path.join(os.path.dirname(args.save_path),\n                                os.path.basename(args.save_path).strip(\".pt\"),\n                                \"test_outputs_output_verified.pkl\")\n                   )\n    return all_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_outputs", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"Amortized Model Arguments Parser\")\n    parser = GetParser(parser)\n    global_args = parser.parse_args()\n    logger = loguru.logger\n    # assert global_args.train_bsz == 1 and global_args.test_bsz == 1, \"currently only support batch_size == 1\"\n\n    torch.manual_seed(global_args.seed)\n    random.seed(global_args.seed)\n    target_model = AutoModelForSequenceClassification.from_pretrained(global_args.target_model).cuda()\n    tokenizer = AutoTokenizer.from_pretrained(global_args.target_model)\n    if global_args.target_model == \"textattack/bert-base-uncased-MNLI\":\n        label_mapping_dict = {\n            0: 2,\n            1: 0,\n            2: 1\n        }\n        label_mapping = lambda x: label_mapping_dict[x]\n    else:\n        label_mapping = None\n    K = global_args.topk\n    alL_train_datasets = dict()\n    all_valid_datasets = dict()\n    all_test_datasets = dict()\n    explainers = global_args.explainer\n    if \",\" in explainers:\n        explainers = explainers.split(\",\")\n    else:\n        explainers = [explainers, ]\n    for explainer in explainers:\n        train_dataset, valid_dataset, test_dataset = torch.load(os.path.join(dataset_dir, f\"data_{explainer}.pkl\"))\n        train_dataset, valid_dataset, test_dataset = Dataset.from_dict(train_dataset), Dataset.from_dict(\n            valid_dataset), Dataset.from_dict(test_dataset)\n        alL_train_datasets[explainer] = train_dataset\n        all_valid_datasets[explainer] = valid_dataset\n        all_test_datasets[explainer] = test_dataset\n    for proportion in [1.0, 0.1, 0.3, 0.5, 0.7, 0.9]:\n        for explainer in explainers:\n            args = Args(seed=global_args.seed, explainer=explainer, proportion=str(proportion),\n                        epochs=global_args.epoch,\n                        batch_size=global_args.train_bsz, normalization=global_args.normalization,\n                        task_name=global_args.task,\n                        discretization=global_args.discrete,\n                        lr=global_args.lr, neuralsort=global_args.neuralsort,\n                        multitask=True if hasattr(global_args, \"multitask\") and global_args.multitask else False,\n                        suf_reg=global_args.suf_reg if hasattr(global_args, \"suf_reg\") and global_args.suf_reg else False,\n                        storage_root=global_args.storage_root,\n                        )\n            train_dataset, valid_dataset, test_dataset = alL_train_datasets[explainer], all_valid_datasets[explainer], \\\n                                                         all_test_datasets[explainer]\n            if proportion < 1:\n                id_fn = os.path.join(os.path.dirname(args.save_path),\n                                     os.path.basename(args.save_path).strip(\".pt\"),\n                                     \"training_ids.pkl\")\n                if not os.path.exists(id_fn):\n                    sample_ids = random.sample(range(len(train_dataset)), int(proportion * len(train_dataset)))\n                    os.makedirs(\n                        os.path.join(os.path.dirname(args.save_path),\n                                     os.path.basename(args.save_path).strip(\".pt\"),\n                                     ),\n                        exist_ok=True\n                    )\n                    torch.save(sample_ids,\n                               os.path.join(os.path.dirname(args.save_path),\n                                            os.path.basename(args.save_path).strip(\".pt\"),\n                                            \"training_ids.pkl\")\n                               )\n                else:\n                    sample_ids = torch.load(id_fn)\n                train_dataset = train_dataset.select(sample_ids)\n            train_dataset, valid_dataset, test_dataset = get_zero_baselines([train_dataset, valid_dataset, test_dataset], target_model, tokenizer, args)\n            train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size,\n                                          collate_fn=collate_fn)\n            valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, collate_fn=collate_fn)\n            if args.fastshap or args.suf_reg:\n                model = AmortizedModel(global_args.amortized_model, cache_dir=model_cache_dir, args=args,\n                                       target_model=target_model, tokenizer=tokenizer).cuda()\n            else:\n                model = AmortizedModel(global_args.amortized_model, cache_dir=model_cache_dir, args=args).cuda()\n\n            optimizer = optim.Adam(model.parameters(), lr=args.lr)\n            # handler_id = logger.add(os.path.join(os.path.dirname(args.save_path), \"log_{time}.txt\"))\n            log_dir = os.path.join(os.path.dirname(args.save_path), os.path.basename(args.save_path).strip(\".pt\"))\n            handler_id = logger.add(os.path.join(log_dir, \"output_verify_no_pad_log_{time}.txt\"))\n            logger.info(json.dumps(vars(args), indent=4))\n            try:\n                model = torch.load(args.save_path)\n            except:\n                os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n                best_valid_spearman = -999999\n                for epoch_i in range(args.epochs):\n                    training_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_aux_output = running_step(\n                        train_dataloader, model, K, optimizer, is_train=True)\n                    logger.info(f\"training loss at epoch {epoch_i}: {training_loss / len(train_dataloader)}\")\n                    logger.info(f\"training spearman (micro-avg): {np.mean(spearman)}\")\n                    logger.info(f\"training top-{K} intersection: {np.mean(intersection)}\")\n\n                    all_outputs = np.concatenate(all_outputs)\n                    all_refs = np.concatenate(all_refs)\n                    logger.info(f\"training spearman: {spearmanr(all_outputs, all_refs)}\")\n                    logger.info(f\"training kendaltau: {kendalltau(all_outputs, all_refs)}\")\n                    if len(all_aux_output) > 0:\n                        logger.info(f\"training aux acc: {np.mean(all_aux_output)}\")\n\n                    if (epoch_i) % args.validation_period == 0:\n                        with torch.no_grad():\n                            valid_loss, valid_all_outputs, valid_all_refs, valid_count_elements, valid_spearman, valid_kendals, valid_intersection, all_valid_aux_output = running_step(\n                                valid_dataloader, model, K, optimizer, is_train=False)\n                            logger.info(f\"Validating at epoch-{epoch_i}\")\n                            valid_all_outputs = np.concatenate(valid_all_outputs)\n                            valid_all_refs = np.concatenate(valid_all_refs)\n                            valid_macro_spearman = spearmanr(valid_all_outputs, valid_all_refs)\n                            valid_macro_kendal = kendalltau(valid_all_outputs, valid_all_refs)\n                            logger.info(f\"validation spearman: {valid_macro_spearman}\")\n                            logger.info(f\"validation kendaltau: {valid_macro_kendal}\")\n                            micro_spearman = np.mean(valid_spearman)\n                            micro_kendal = np.mean(valid_kendals)\n                            logger.info(f\"validation micro spearman: {micro_spearman}\")\n                            logger.info(f\"validation micro kendal: {micro_kendal}\")\n                            if len(all_valid_aux_output) > 0:\n                                logger.info(f\"validation aux acc: {np.mean(all_valid_aux_output)}\")\n                            if valid_macro_spearman.correlation > best_valid_spearman:\n                                best_valid_spearman = valid_macro_spearman.correlation\n                                logger.info(\n                                    f\"best validation spearman at {epoch_i}: {valid_macro_spearman.correlation}, save checkpoint here\")\n                                torch.save(model, args.save_path)\n\n            with torch.no_grad():\n                model = model.eval()\n                for test_explainer in explainers:\n                    handler_id_test = logger.add(\n                        os.path.join(os.path.dirname(args.save_path), os.path.basename(args.save_path).strip(\".pt\"),\n                                     f\"test_log_no_pad_{test_explainer}_output_verify.txt\"))\n                    test_dataset = all_test_datasets[test_explainer]\n                    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size,\n                                                 collate_fn=collate_fn)\n                    logger.info(f\"doing testing for {test_explainer}\")\n                    test_loss, all_outputs, all_refs, count_elements, spearman, kendals, intersection, all_test_aux_output = running_step(\n                        test_dataloader, model, K, optimizer, is_train=False, save=True, args=args)\n\n                    logger.info(f\"testing spearman (micro-avg): {np.mean(spearman)}\")\n                    logger.info(f\"testing kendal (micro-avg): {np.mean(kendals)}\")\n                    logger.info(f\"testing top-{K} intersection: {np.mean(intersection)}\")\n                    logger.info(f\"testing RMSE: {np.sqrt(test_loss / count_elements)}\")\n                    backup_all_outputs = copy.deepcopy(all_outputs)\n                    all_outputs = np.concatenate(all_outputs)\n                    all_refs = np.concatenate(all_refs)\n                    logger.info(f\"testing spearman: {spearmanr(all_outputs, all_refs)}\")\n                    logger.info(f\"testing kendaltau: {kendalltau(all_outputs, all_refs)}\")\n                    if len(all_test_aux_output) > 0:\n                        logger.info(f\"testing aux acc: {np.mean(all_test_aux_output)}\")\n                    example = get_example_output()\n                    example[\"end\"] = len(test_dataloader)\n                    example['explainer']['name'] = \"AmortizedModelBERT\"\n                    counter_id = 0\n                    all_examples_out = list()\n                    all_examples_out_ref = list()\n                    for batch in test_dataloader:\n                        input_ids = batch['input_ids']\n                        attn_mask = batch['attention_mask']\n                        labels = batch['ft_label']\n                        #print(input_ids.shape)\n                        #print(attn_mask.shape)\n                        assert len(input_ids[0]) == len(attn_mask[0])\n                        # assert len(input_ids[0]) == len(all_outputs[counter_id])\n                        for batch_i in range(len(input_ids)):\n                            assert len(input_ids[batch_i][attn_mask[batch_i] > 0]) == len(backup_all_outputs[counter_id])\n                            assert len(all_test_aux_output[counter_id]) == len(input_ids[batch_i])\n                            new_example = copy.deepcopy(example)\n                            new_example['batch'] = counter_id\n                            new_example['index_running'] = counter_id\n                            new_example['input_ids'] = batch['input_ids'][batch_i].tolist()\n                            # new_example['attributions'] = list(all_outputs[counter_id] + [1e-6, ]* len())\n                            new_example['attributions'] = [float(x) for x in list(all_test_aux_output[counter_id])]\n                            new_example['label'] = int(labels[batch_i])\n                            if \"prediction_dist\" in batch:\n                                new_example[\"predictions\"] = [float(x) for x in batch['prediction_dist'][batch_i]]\n                            all_examples_out.append(new_example)\n                            new_example_ref = copy.deepcopy(new_example)\n                            new_example_ref[\"attributions\"] = batch[\"output\"][batch_i].cpu().tolist()\n                            assert len(new_example_ref['attributions']) == len(new_example['attributions'])\n                            all_examples_out_ref.append(new_example_ref)\n\n\n\n                            counter_id += 1\n\n\n                    # change it to the path of your thermostat\n                    example_out_dir = f'/path/to/thermostat/experiments/thermostat/yelp_polarity/bert/AmortizedModel/seed_{args.seed}'\n                    os.makedirs(example_out_dir, exist_ok=True)\n                    with open(os.path.join(example_out_dir, \"output.jsonl\"), \"w\", encoding='utf-8') as f_out:\n                        for line in all_examples_out:\n                            for key in line.keys():\n                                if torch.is_tensor(line[key]):\n                                    line[key] = [float(x) for x in line[key].tolist()]\n                            f_out.write(json.dumps(line) + \"\\n\")\n                    with open(os.path.join(example_out_dir, \"ref.jsonl\"), \"w\", encoding='utf-8') as f_out:\n                        for line in all_examples_out_ref:\n                            for key in line.keys():\n                                if torch.is_tensor(line[key]):\n                                    line[key] = [float(x) for x in line[key].tolist()]\n                            f_out.write(json.dumps(line) + \"\\n\")\n\n\n                    try:\n                        stat_dict = torch.load(os.path.join(log_dir, f\"eraser_stat_dict_{test_explainer}.pt\"))\n                    except:\n                        test_dataloader = DataLoader(test_dataset, batch_size=1,\n                                                     collate_fn=collate_fn)\n                        stat_dict = get_eraser_metrics(test_dataloader, target_model, amortized_model=model,\n                                                       tokenizer=tokenizer, label_mapping=label_mapping)\n                        torch.save(stat_dict, os.path.join(log_dir, f\"eraser_stat_dict_{test_explainer}.pt\"))\n                    logger.info(\"eraser_metrics\")\n                    for k in stat_dict:\n                        for metric in stat_dict[k]:\n                            logger.info(\n                                f\"{k}-{metric}: {np.mean(stat_dict[k][metric]).item()} ({np.std(stat_dict[k][metric]).item()})\")\n                    logger.remove(handler_id_test)\n            #\n            logger.remove(handler_id)", ""]}
{"filename": "dataset_stat.py", "chunked_list": ["from datasets import load_dataset\nimport numpy\nfrom transformers import AutoTokenizer\nfrom collections import Counter\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport os\n# ds_name = \"ag_news\"\n# dataset = load_dataset(ds_name, cache_dir=\"./cache_data\")\ndataset = load_dataset(\"glue\", \"mrpc\", cache_dir=\"./cache_data\")", "# dataset = load_dataset(ds_name, cache_dir=\"./cache_data\")\ndataset = load_dataset(\"glue\", \"mrpc\", cache_dir=\"./cache_data\")\n# dataset = load_dataset(\"yelp_polarity\", cache_dir=\"./cache_data\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=\"./cache\")\nsample_split = dataset[\"train\"]\nlength_count = Counter()\nlengths = []\nfor data in tqdm(sample_split, total=len(sample_split)):\n    # text = data[\"text\"]\n    # tokens = tokenizer(text)[\"input_ids\"]\n    #tokens = tokenizer(data[\"premise\"], data[\"hypothesis\"])[\"input_ids\"]\n    tokens = tokenizer(data[\"sentence1\"], data[\"sentence2\"])[\"input_ids\"]\n    length_count[len(tokens)] += 1\n    lengths.append(2 ** len(tokens) if len(tokens) < 11 else 2**11)\n    if len(lengths) == 100000:\n        break", "print(numpy.mean(lengths))\n# plt.hist(lengths)\n# plt.show()\n\n\n\n\n"]}
{"filename": "cross_gt_correlation.py", "chunked_list": ["import json\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import mean_squared_error\nimport os\nimport glob\ndef sort_by_file_size(paths):\n    tgts = [(x, os.path.getsize(x)) for x in paths]\n    tgts.sort(key=lambda x: x[1], reverse=True)\n    return [x[0] for x in tgts]", "\n# feel free to add other explainer you want to compare\nfor explainer in [\"kernelshap-2000-sample200\",]:\n    # path = f\"path/to/thermostat/experiments/thermostat/yelp_polarity/bert/{explainer}/\"\n    path = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/{explainer}/\"\n    # path2 = f\"path/to/thermostat/experiments/thermostat/yelp_polarity/bert/svs-3600/\"\n    path2 = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/svs-2000/\"\n    print(\"NOW evaluating:\", explainer)\n    seed_dirs = glob.glob(path + \"seed_*\")\n    seed_dirs2 = glob.glob(path2 + \"seed_*\")\n    if len(seed_dirs) < 2:\n        print(\"not enough seed dirs for {}\".format(path))\n        exit()\n    seed_aggr_spearman = []\n    seed_aggr_l2 = []\n    seed_topks = {1: [], 5: [], 10: [], 20: []}\n\n    count = 0\n    mse_count = 0\n    for i in range(len(seed_dirs)):\n        for j in range(len(seed_dirs2)):\n\n            all_correlations = []\n            all_ps = []\n            all_l2 = []\n            all_mask_check = 0\n            seed_dir0 = os.path.join(path, seed_dirs[i])\n            seed_dir1 = os.path.join(path2, seed_dirs2[j])\n            seed_file0 = sort_by_file_size(glob.glob(os.path.join(seed_dir0, \"*.jsonl\")))[0]\n            seed_file1 = sort_by_file_size(glob.glob(os.path.join(seed_dir1, \"*.jsonl\")))[0]\n            seed_file_path0 = os.path.join(seed_dir0, seed_file0)\n            seed_file_path1 = os.path.join(seed_dir1, seed_file1)\n            # print(seed_file_path0)\n            # print(seed_file_path1)\n            topks = {1:[], 5:[], 10:[], 20:[]}\n            with open(seed_file_path0, \"r\", encoding='utf-8') as f_in0, open(seed_file_path1, \"r\", encoding='utf-8') as f_in1:\n                buf0, buf1 = f_in0.readlines(), f_in1.readlines()\n                #assert len(buf0) == len(buf1), f\"{len(buf0)}, {len(buf1)}\"\n                for line0, line1 in tqdm(zip(buf0, buf1), total=len(buf0)):\n                    obj0, obj1 = json.loads(line0), json.loads(line1)\n                    attr0, attr1 = obj0[\"attributions\"], obj1[\"attributions\"]\n                    in0, in1 = obj0[\"input_ids\"], obj1[\"input_ids\"]\n                    attr0, attr1 = np.array(attr0), np.array(attr1)\n                    assert in0 == in1\n                    if ((attr0 == 0) != (attr1 == 0)).any():\n                        all_mask_check += 1\n                    postfix = sum(np.array(in0) == 0)\n                    #assert postfix < len(attr0)\n                    if postfix > 0:\n                        attr0 = attr0[:-postfix]\n                        attr1 = attr1[:-postfix]\n                    assert len(attr0) > 0 and len(attr0) == len(attr1), f\"{len(attr0)}\"\n                    count += 1\n                    #print(attr0)\n                    #print(attr1)\n                    #print(len(attr0), postfix)\n                    _spearman, _pval = spearmanr(attr0, attr1)\n                    all_correlations.append(_spearman)\n                    all_ps.append(_pval)\n                    mse = mean_squared_error(attr0, attr1)\n                    if mse > 1e2:\n                       mse_count += 1\n                    else:\n                        all_l2.append(mean_squared_error(attr0, attr1))\n                    sort0 = attr0.argsort()\n                    sort1 = attr1.argsort()\n                    for key in topks:\n                        _topk = len(set(sort0[::-1][:key].tolist()) & set(sort1[::-1][:key].tolist()))\n                        topks[key].append(_topk)\n            # print(f\"spearman correlation: {np.mean(all_correlations)} ({np.std(all_correlations)}, {np.min(all_correlations)}, {np.max(all_correlations)})\", )\n            # print(f\"spearman ps: {np.mean(all_ps)} ({np.std(all_ps)})\", )\n            # print(f\"mask mismatch rate: {all_mask_check / len(all_ps)}\")\n            # for key in topks:\n            #     print(f\"top{key}: {np.mean(topks[key])}\")\n            seed_aggr_spearman.append(np.mean(all_correlations))\n            seed_aggr_l2.append(np.mean(all_l2))\n            for key in seed_topks:\n                seed_topks[key].append(np.mean(topks[key]))\n    print(f\"spearman correlation: {np.mean(seed_aggr_spearman)} ({np.std(seed_aggr_spearman)}, {np.min(seed_aggr_spearman)}, {np.max(seed_aggr_spearman)})\", )\n    print(f\"MSE correlation: {np.mean(seed_aggr_l2)} ({np.std(seed_aggr_l2)}, {np.min(seed_aggr_l2)}, {np.max(seed_aggr_l2)})\", )\n    print(f\"ignored MSE pairs: {mse_count} / {count}, {mse_count / count}\")\n    for key in seed_topks:\n        print(f\"top{key}: {np.mean(seed_topks[key])} ({np.std(seed_topks[key])})\")", "\n\n\n\n\n\n"]}
{"filename": "internal_correlation.py", "chunked_list": ["import json\nimport pickle\n\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport os\nimport glob", "import os\nimport glob\nfrom matplotlib import container\n\n\ndef sort_by_file_size(paths):\n    tgts = [(x, os.path.getsize(x)) for x in paths]\n    tgts.sort(key=lambda x: x[1], reverse=True)\n    return [x for x in tgts]\n", "\n\nnp.set_printoptions(formatter={'float': '{: 0.2f}'.format})\nplt.rcParams.update({'font.size': 16})\nplt.rcParams[\"figure.figsize\"] = (10, 6)\ncmap = [\"red\", \"blue\", \"orange\", \"purple\", \"cyan\", \"green\", \"lime\", \"#bb86fc\"]\nmarkers = [\".\", \"v\", \"*\", \"o\", \"s\", \"d\", \"P\", \"p\"]\nmethod_length_spearman_decomp = {}\ntask_name = \"mnli\"\ncandidates = [\"svs-2000\", \"kernelshap-2000\", \"kernelshap-2000-sample200\", \"kernelshap-2000-sample2000\", \"kernelshap-2000-sample8000\", \"lime-2000\", \"lime-2000-sample200\"]", "task_name = \"mnli\"\ncandidates = [\"svs-2000\", \"kernelshap-2000\", \"kernelshap-2000-sample200\", \"kernelshap-2000-sample2000\", \"kernelshap-2000-sample8000\", \"lime-2000\", \"lime-2000-sample200\"]\nkeys = [\"svs-25\", \"kernelshap-25\", \"kernelshap-200\", \"kernelshap-2000\", \"kernelshap-8000\", \"lime-25\", \"lime-200\"]\nfor explainer_i, explainer in enumerate(candidates):\n    if task_name == \"yelp\":\n        path = f\"path/to/thermostat/experiments/thermostat/yelp_polarity/bert/{explainer}/\"\n    else:\n        path = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/{explainer}/\"\n    print(\"NOW evaluating:\", explainer)\n    seed_dirs = glob.glob(path + \"seed_*\")\n    seed_aggr_spearman = []\n    seed_aggr_l2 = []\n    seed_topks = {1: [], 5: [], 10: [], 20: []}\n    method_length_spearman_decomp[explainer] = {}\n\n    for i in range(len(seed_dirs)):\n        for j in range(i + 1, len(seed_dirs)):\n\n            all_correlations = []\n            all_ps = []\n            all_l2 = []\n            all_ill_condition = 0\n            all_length_decomp = {}\n            all_mask_check = 0\n            seed_dir0 = os.path.join(path, seed_dirs[i])\n            seed_dir1 = os.path.join(path, seed_dirs[j])\n            seed_file0, seed_file0_size = sort_by_file_size(glob.glob(os.path.join(seed_dir0, \"*.jsonl\")))[0]\n            seed_file1, seed_file1_size = sort_by_file_size(glob.glob(os.path.join(seed_dir1, \"*.jsonl\")))[0]\n            if seed_file0_size > 2 * seed_file1_size or seed_file1_size > 2 * seed_file0_size:\n                print(seed_file0, seed_file0_size)\n                print(seed_file1, seed_file1_size)\n                continue\n            seed_file_path0 = os.path.join(seed_dir0, seed_file0)\n            seed_file_path1 = os.path.join(seed_dir1, seed_file1)\n            topks = {1: [], 5: [], 10: [], 20: []}\n            with open(seed_file_path0, \"r\", encoding='utf-8') as f_in0, open(seed_file_path1, \"r\",\n                                                                             encoding='utf-8') as f_in1:\n                buf0, buf1 = f_in0.readlines(), f_in1.readlines()\n                # assert len(buf0) == len(buf1), f\"{len(buf0)}, {len(buf1)}\"\n                for line0, line1 in tqdm(zip(buf0, buf1), total=min(len(buf0), len(buf1))):\n                    obj0, obj1 = json.loads(line0), json.loads(line1)\n                    attr0, attr1 = obj0[\"attributions\"], obj1[\"attributions\"]\n                    in0, in1 = obj0[\"input_ids\"], obj1[\"input_ids\"]\n                    attr0, attr1 = np.array(attr0), np.array(attr1)\n                    assert in0 == in1\n                    if ((attr0 == 0) != (attr1 == 0)).any():\n                        all_mask_check += 1\n                    postfix = sum(np.array(in0) == 0)\n                    # assert postfix < len(attr0)\n                    if postfix > 0:\n                        attr0 = attr0[:-postfix]\n                        attr1 = attr1[:-postfix]\n                    assert len(attr0) > 0 and len(attr0) == len(attr1), f\"{len(attr0)}\"\n                    _mse = ((attr0 - attr1)**2).sum() / len(attr0)\n                    if _mse > 1e3:\n                        # due to ill-conditioned matrix and float error, kernelshap can give bad values sometimes\n                        all_ill_condition += 1\n                        continue\n                    _spearman, _pval = spearmanr(attr0, attr1)\n                    if len(attr0) not in all_length_decomp:\n                        all_length_decomp[len(attr0)] = []\n                    all_length_decomp[len(attr0)].append(_spearman)\n                    all_correlations.append(_spearman)\n                    all_ps.append(_pval)\n                    all_l2.append(_mse)\n                    sort0 = attr0.argsort()\n                    sort1 = attr1.argsort()\n                    for key in topks:\n                        _topk = len(set(sort0[::-1][:key].tolist()) & set(sort1[::-1][:key].tolist()))\n                        topks[key].append(_topk)\n            if all_ill_condition > 0:\n                print(\"find ill_condition: \", all_ill_condition, 100 * all_ill_condition / min(len(buf0), len(buf1)))\n            seed_aggr_spearman.append(np.mean(all_correlations))\n            seed_aggr_l2.append(np.mean(all_l2))\n            for length in all_length_decomp:\n                if length not in method_length_spearman_decomp[explainer]:\n                    method_length_spearman_decomp[explainer][length] = []\n                method_length_spearman_decomp[explainer][length].append(np.mean(all_length_decomp[length]))\n\n            for key in seed_topks:\n                seed_topks[key].append(np.mean(topks[key]))\n    print(\n        f\"spearman correlation: {np.mean(seed_aggr_spearman)} ({np.std(seed_aggr_spearman)}, {np.min(seed_aggr_spearman)}, {np.max(seed_aggr_spearman)})\", )\n    print(\n        f\"MSE correlation: {np.mean(seed_aggr_l2)} ({np.std(seed_aggr_l2)}, {np.min(seed_aggr_l2)}, {np.max(seed_aggr_l2)})\", )\n    for key in seed_topks:\n        print(f\"top{key}: {np.mean(seed_topks[key])} ({np.std(seed_topks[key])})\")\n    print(f\"${'{:.2f}'.format(np.mean(seed_aggr_spearman))} (\\pm {'{:.2f}'.format(np.std(seed_aggr_spearman))})$ & \"\n          f\"${'{:.2f}'.format(np.mean(seed_topks[5]))} (\\pm {'{:.2f}'.format(np.std(seed_topks[5]))})$ & \"\n          f\"${'{:.2f}'.format(np.mean(seed_topks[10]))} (\\pm {'{:.2f}'.format(np.std(seed_topks[10]))})$ & \"\n          f\"${'{:.2f}'.format(np.mean(seed_aggr_l2))} (\\pm {'{:.2f}'.format(np.std(seed_aggr_l2))})$ & \"\n          )\n    if \"lime\" not in explainer:\n        xs = list(method_length_spearman_decomp[explainer].keys())\n        xs.sort()\n        if task_name == \"mnli\":\n            xs = [x for x in xs if x < 80 and x > 5]\n        ys = [np.mean(method_length_spearman_decomp[explainer][x]) for x in xs]\n        yerr = [np.std(method_length_spearman_decomp[explainer][x]) for x in xs]\n        plt.plot(xs, ys, color=cmap[explainer_i],\n                     marker=markers[explainer_i], label=keys[explainer_i].replace(\"kernelshap\", \"ks\") if \"kernelshap\" in keys[explainer_i] else keys[explainer_i])", "\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles = [h[0] if isinstance(h, container.ErrorbarContainer) else h for h in handles]\nax = plt.gca()\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 1, box.height])\n\nplt.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.ylabel(\"Spearman's Correlation\", fontsize=22)\nplt.xlabel(\"#(Tokens) Per Instance\", fontsize=22)", "plt.ylabel(\"Spearman's Correlation\", fontsize=22)\nplt.xlabel(\"#(Tokens) Per Instance\", fontsize=22)\ntarget_dir = os.path.join(\"visualization\", \"internal_correlation\", task_name)\nos.makedirs(target_dir, exist_ok=True)\ntarget_fp = os.path.join(target_dir, \"internal_correlation_w_length_decomp_wo_errorbar.pdf\")\nplt.tight_layout()\n\nplt.savefig(target_fp)\npickle.dump(method_length_spearman_decomp, open(os.path.join(target_dir, \"dump.pkl\"), \"wb\"))\n", "pickle.dump(method_length_spearman_decomp, open(os.path.join(target_dir, \"dump.pkl\"), \"wb\"))\n"]}
{"filename": "create_dataset.py", "chunked_list": ["import thermostat\nimport random\nimport torch\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport os\n# output_dir = \"./amortized_dataset/imdb_test\"\noutput_dir = \"./amortized_dataset/mnli_test\"\n# output_dir = \"./amortized_dataset/yelp_test\"\nmodel_cache_dir = \"./models/\"\nif __name__ == '__main__':\n    # data_cache_dir = \"./datasets/imdb\"\n    # data_cache_dir = \"./datasets/mnli\"\n    data_cache_dir = \"./datasets/yelp_polarity\"\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(data_cache_dir, exist_ok=True)\n    os.environ[\"HF_DATASETS_CACHE\"] = data_cache_dir\n    # data = thermostat.load(\"imdb-bert-lime\", cache_dir=data_cache_dir)\n    # dataset = load_dataset(\"imdb\")\n    # task = \"multi_nli\"\n    task = \"yelp_polarity\"\n    #dataset = load_from_disk(\"thermostat/experiments/thermostat/datasets/imdb\")\n    # dataset = load_from_disk(f\"thermostat/experiments/thermostat/datasets/{task}\")\n    dataset = load_from_disk(f\"thermostat/experiments/thermostat/datasets/{task}\")\n    # model_name = \"textattack/bert-base-uncased-imdb\"\n    # model_name = \"textattack/bert-base-uncased-MNLI\"\n    model_name = \"textattack/bert-base-uncased-yelp-polarity\"\n    #if model_name == \"textattack/bert-base-uncased-MNLI\":\n        #label_mapping_dict = {\n            #0: 2,\n            #1: 0,\n            #2: 1\n        #}\n        #label_mapping = lambda x: label_mapping_dict[x]\n    #else:\n        #label_mapping = lambda x: x\n    label_mapping = lambda x: x\n    #explainer = \"svs\"\n    for explainer in [\"svs\", ]:\n    # for explainer in [\"svs\", \"lime\", \"lig\"]:\n        data = thermostat.load(f\"{task}-bert-{explainer}\", cache_dir=data_cache_dir)\n        instance = data[0]\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n        # id_pkl_name = \"dumped_split_ids_0803.pkl\"\n        id_pkl_name = \"dumped_split_ids.pkl\"\n        try:\n            # train_ids, valid_ids, test_ids = torch.load(os.path.join(output_dir, \"dumped_split_ids_0523.pkl\"))\n            train_ids, valid_ids, test_ids = torch.load(os.path.join(output_dir, id_pkl_name))\n            print(\"successfully load pre-split data ids\")\n        except:\n            print(\"fail to load pre-split data ids, re-splitting...\")\n            all_ids = list(range(len(data)))\n            assert len(all_ids) > 2000\n            # assert 0.1 * len(all_ids) <= 2000\n            # to make sure our amortized model can compare to traditional interpretation methods\n            # random.shuffle(all_ids)\n            # test_ids = random.sample(list(range(2000)), int(0.1 * len(all_ids)))\n            test_ids = list(range(500)) + random.sample(list(range(500, 3000)),\n                                                        max(int(0.1 * len(all_ids)) - 500, 0))\n            rest_ids = list(set(all_ids) - set(test_ids))\n            random.shuffle(rest_ids)\n            # train_ids = all_ids[: int(0.8 * len(all_ids))]\n            train_ids = rest_ids[: int(0.8 * len(all_ids))]\n            valid_ids = rest_ids[len(train_ids): ]\n            # test_ids = all_ids[len(train_ids) + len(valid_ids): ]\n            torch.save([train_ids, valid_ids, test_ids], os.path.join(output_dir, id_pkl_name))\n        # train_dataset = [dataset[\"test\"][i]['text'] for i in train_ids]\n        # valid_dataset = [dataset[\"test\"][i]['text'] for i in valid_ids]\n        # test_dataset = [dataset[\"test\"][i]['text'] for i in test_ids]\n        # test_ids = [x for x in test_ids if x < 2000]\n        if task == \"multi_nli\":\n            train_dataset = [(dataset[i]['premise'], dataset[i]['hypothesis']) for i in train_ids]\n            valid_dataset = [(dataset[i][\"premise\"], dataset[i]['hypothesis']) for i in valid_ids]\n            test_dataset = [(dataset[i][\"premise\"], dataset[i]['hypothesis']) for i in test_ids]\n        else:\n            train_dataset = [dataset[i]['text'] for i in train_ids]\n            valid_dataset = [dataset[i]['text'] for i in valid_ids]\n            test_dataset = [dataset[i]['text'] for i in test_ids]\n        # train_dataset = tokenizer(train_dataset, return_tensors='pt', padding='max_length', truncation=True)\n        train_dataset = tokenizer(train_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        train_dataset[\"output\"] = [data[i].attributions for i in train_ids]\n        train_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in train_ids]\n        # train_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in train_ids]\n        train_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in train_ids]\n        train_dataset[\"prediction_dist\"] = [data[i].predictions for i in train_ids]\n        train_dataset[\"id\"] = train_ids\n\n        valid_dataset = tokenizer(valid_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        valid_dataset[\"output\"] = [data[i].attributions for i in valid_ids]\n        valid_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in valid_ids]\n        # valid_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in valid_ids]\n        valid_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in valid_ids]\n        valid_dataset[\"prediction_dist\"] = [data[i].predictions for i in valid_ids]\n        valid_dataset[\"id\"] = valid_ids\n\n        test_dataset = tokenizer(test_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        test_dataset[\"output\"] = [data[i].attributions for i in test_ids]\n        test_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in test_ids]\n        # test_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in test_ids]\n        test_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in test_ids]\n        test_dataset[\"prediction_dist\"] = [data[i].predictions for i in test_ids]\n        test_dataset[\"id\"] = test_ids\n        for _dataset, ids, status in zip([train_dataset, valid_dataset, test_dataset], [train_ids, valid_ids, test_ids], ['train', \"valid\", \"test\"]):\n            for id_i, _id in enumerate(ids):\n                assert _dataset[\"input_ids\"][id_i] == data[_id].input_ids\n            print(f\"{status} input ids check complete\")\n\n        torch.save([train_dataset, valid_dataset, test_dataset], os.path.join(output_dir, f\"data_{explainer}.pkl\"))", "# output_dir = \"./amortized_dataset/yelp_test\"\nmodel_cache_dir = \"./models/\"\nif __name__ == '__main__':\n    # data_cache_dir = \"./datasets/imdb\"\n    # data_cache_dir = \"./datasets/mnli\"\n    data_cache_dir = \"./datasets/yelp_polarity\"\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(data_cache_dir, exist_ok=True)\n    os.environ[\"HF_DATASETS_CACHE\"] = data_cache_dir\n    # data = thermostat.load(\"imdb-bert-lime\", cache_dir=data_cache_dir)\n    # dataset = load_dataset(\"imdb\")\n    # task = \"multi_nli\"\n    task = \"yelp_polarity\"\n    #dataset = load_from_disk(\"thermostat/experiments/thermostat/datasets/imdb\")\n    # dataset = load_from_disk(f\"thermostat/experiments/thermostat/datasets/{task}\")\n    dataset = load_from_disk(f\"thermostat/experiments/thermostat/datasets/{task}\")\n    # model_name = \"textattack/bert-base-uncased-imdb\"\n    # model_name = \"textattack/bert-base-uncased-MNLI\"\n    model_name = \"textattack/bert-base-uncased-yelp-polarity\"\n    #if model_name == \"textattack/bert-base-uncased-MNLI\":\n        #label_mapping_dict = {\n            #0: 2,\n            #1: 0,\n            #2: 1\n        #}\n        #label_mapping = lambda x: label_mapping_dict[x]\n    #else:\n        #label_mapping = lambda x: x\n    label_mapping = lambda x: x\n    #explainer = \"svs\"\n    for explainer in [\"svs\", ]:\n    # for explainer in [\"svs\", \"lime\", \"lig\"]:\n        data = thermostat.load(f\"{task}-bert-{explainer}\", cache_dir=data_cache_dir)\n        instance = data[0]\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n        # id_pkl_name = \"dumped_split_ids_0803.pkl\"\n        id_pkl_name = \"dumped_split_ids.pkl\"\n        try:\n            # train_ids, valid_ids, test_ids = torch.load(os.path.join(output_dir, \"dumped_split_ids_0523.pkl\"))\n            train_ids, valid_ids, test_ids = torch.load(os.path.join(output_dir, id_pkl_name))\n            print(\"successfully load pre-split data ids\")\n        except:\n            print(\"fail to load pre-split data ids, re-splitting...\")\n            all_ids = list(range(len(data)))\n            assert len(all_ids) > 2000\n            # assert 0.1 * len(all_ids) <= 2000\n            # to make sure our amortized model can compare to traditional interpretation methods\n            # random.shuffle(all_ids)\n            # test_ids = random.sample(list(range(2000)), int(0.1 * len(all_ids)))\n            test_ids = list(range(500)) + random.sample(list(range(500, 3000)),\n                                                        max(int(0.1 * len(all_ids)) - 500, 0))\n            rest_ids = list(set(all_ids) - set(test_ids))\n            random.shuffle(rest_ids)\n            # train_ids = all_ids[: int(0.8 * len(all_ids))]\n            train_ids = rest_ids[: int(0.8 * len(all_ids))]\n            valid_ids = rest_ids[len(train_ids): ]\n            # test_ids = all_ids[len(train_ids) + len(valid_ids): ]\n            torch.save([train_ids, valid_ids, test_ids], os.path.join(output_dir, id_pkl_name))\n        # train_dataset = [dataset[\"test\"][i]['text'] for i in train_ids]\n        # valid_dataset = [dataset[\"test\"][i]['text'] for i in valid_ids]\n        # test_dataset = [dataset[\"test\"][i]['text'] for i in test_ids]\n        # test_ids = [x for x in test_ids if x < 2000]\n        if task == \"multi_nli\":\n            train_dataset = [(dataset[i]['premise'], dataset[i]['hypothesis']) for i in train_ids]\n            valid_dataset = [(dataset[i][\"premise\"], dataset[i]['hypothesis']) for i in valid_ids]\n            test_dataset = [(dataset[i][\"premise\"], dataset[i]['hypothesis']) for i in test_ids]\n        else:\n            train_dataset = [dataset[i]['text'] for i in train_ids]\n            valid_dataset = [dataset[i]['text'] for i in valid_ids]\n            test_dataset = [dataset[i]['text'] for i in test_ids]\n        # train_dataset = tokenizer(train_dataset, return_tensors='pt', padding='max_length', truncation=True)\n        train_dataset = tokenizer(train_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        train_dataset[\"output\"] = [data[i].attributions for i in train_ids]\n        train_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in train_ids]\n        # train_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in train_ids]\n        train_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in train_ids]\n        train_dataset[\"prediction_dist\"] = [data[i].predictions for i in train_ids]\n        train_dataset[\"id\"] = train_ids\n\n        valid_dataset = tokenizer(valid_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        valid_dataset[\"output\"] = [data[i].attributions for i in valid_ids]\n        valid_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in valid_ids]\n        # valid_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in valid_ids]\n        valid_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in valid_ids]\n        valid_dataset[\"prediction_dist\"] = [data[i].predictions for i in valid_ids]\n        valid_dataset[\"id\"] = valid_ids\n\n        test_dataset = tokenizer(test_dataset,  padding='max_length', truncation=True, return_special_tokens_mask=True)\n        test_dataset[\"output\"] = [data[i].attributions for i in test_ids]\n        test_dataset[\"output_rank\"] = [torch.argsort(torch.tensor(data[i].attributions)).tolist() for i in test_ids]\n        # test_dataset[\"ft_label\"] = [dataset['test'][i][\"label\"] for i in test_ids]\n        test_dataset[\"ft_label\"] = [label_mapping(dataset[i][\"label\"]) for i in test_ids]\n        test_dataset[\"prediction_dist\"] = [data[i].predictions for i in test_ids]\n        test_dataset[\"id\"] = test_ids\n        for _dataset, ids, status in zip([train_dataset, valid_dataset, test_dataset], [train_ids, valid_ids, test_ids], ['train', \"valid\", \"test\"]):\n            for id_i, _id in enumerate(ids):\n                assert _dataset[\"input_ids\"][id_i] == data[_id].input_ids\n            print(f\"{status} input ids check complete\")\n\n        torch.save([train_dataset, valid_dataset, test_dataset], os.path.join(output_dir, f\"data_{explainer}.pkl\"))", "\n        # for data_i, data_entry in enumerate(dataset):\n        #     all_data.append({\n        #         \"output\": [x[1] for x in data[data_i].explanation]\n        #     })\n\n\n        # model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=model_cache_dir)\n\n        # print(len(dataset[\"test\"]))", "\n        # print(len(dataset[\"test\"]))\n        # print(len(data))\n        # print(instance.explanation)\n        # print(len(instance.explanation))\n        # print(instance.attributions[-10:])\n        # print(instance.attributions)\n        # print(len(instance.attributions))\n        # print(tokenizer(dataset['test'][0][\"text\"], padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][:10].sum())\n        # print(tokenizer(dataset['test'][0][\"text\"], padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][-10:].sum())", "        # print(tokenizer(dataset['test'][0][\"text\"], padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][:10].sum())\n        # print(tokenizer(dataset['test'][0][\"text\"], padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][-10:].sum())\n        # print(tokenizer(dataset['test'][0][\"text\"], return_special_tokens_mask=True, padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][:10])\n        # print(tokenizer(dataset['test'][0][\"text\"], return_special_tokens_mask=True, padding=\"max_length\", return_tensors='pt')[\"attention_mask\"][-10:])\n        # dataset_sample = dataset[\"test\"][0]\n        # print(dataset[\"test\"][0])\n        # # for i in range(len(data)):\n        # #     len_lime_sample = len(data[i].explanation)\n        # #     len_data_sample = len(tokenizer(dataset[\"test\"][i][\"text\"], truncation=True)[\"input_ids\"])\n        # #     assert len_lime_sample == len_data_sample, f\"len_lime: {len_lime_sample}, len_data: {len_data_sample}\"", "        # #     len_data_sample = len(tokenizer(dataset[\"test\"][i][\"text\"], truncation=True)[\"input_ids\"])\n        # #     assert len_lime_sample == len_data_sample, f\"len_lime: {len_lime_sample}, len_data: {len_data_sample}\"\n        # print(tokenizer(dataset_sample['text'])[\"input_ids\"])\n        # print(tokenizer.convert_ids_to_tokens(tokenizer(dataset_sample['text'])[\"input_ids\"]))\n        # print(len(tokenizer(dataset_sample['text'])[\"input_ids\"]))\n\n        # print(help(instance))\n"]}
{"filename": "heatmap.py", "chunked_list": ["import os\n\nimport thermostat\nfrom datasets import load_dataset\n\n\ndef render(labels=False):\n    \"\"\" Uses the displaCy visualization tool to render a HTML from the heatmap \"\"\"\n\n    # Call this function once for every text field\n    if len(set([t.text_field for t in self])) > 1:\n        for field in self[0].text_fields:\n            print(f'Heatmap \"{field}\"')\n            Heatmap([t for t in self if t.text_field == field]).render(labels=labels)\n        return\n\n    ents = []\n    colors = {}\n    ii = 0\n    for color_token in self:\n        ff = ii + len(color_token.token)\n\n        # One entity in displaCy contains start and end markers (character index) and optionally a label\n        # The label can be added by setting \"attribution_labels\" to True\n        ent = {\n            'start': ii,\n            'end': ff,\n            'label': str(color_token.score),\n        }\n\n        ents.append(ent)\n        # A \"colors\" dict takes care of the mapping between attribution labels and hex colors\n        colors[str(color_token.score)] = color_token.hex()\n        ii = ff\n\n    to_render = {\n        'text': ''.join([t.token for t in self]),\n        'ents': ents,\n    }\n\n    if labels:\n        template = \"\"\"\n        <mark class=\"entity\" style=\"background: {bg}; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 2;\n        border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n            {text}\n            <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform:\n            uppercase; vertical-align: middle; margin-left: 0.5rem\">{label}</span>\n        </mark>\n        \"\"\"\n    else:\n        template = \"\"\"\n        <mark class=\"entity\" style=\"background: {bg}; padding: 0.15em 0.3em; margin: 0 0.2em; line-height: 2.2;\n        border-radius: 0.25em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n            {text}\n        </mark>\n        \"\"\"\n\n    html = displacy.render(\n        to_render,\n        style='ent',\n        manual=True,\n        jupyter=is_in_jupyter(),\n        options={'template': template,\n                 'colors': colors,\n                 }\n    )\n    return html if not is_in_jupyter() else None", "\n\nif __name__ == '__main__':\n    seed_1_path = \"path/to/thermostat/experiments/thermostat/multi_nli/bert/svs-2000/seed_1/[date1].ShapleyValueSampling.jsonl\"\n    seed_2_path = \"path/to/thermostat/experiments/thermostat/multi_nli/bert/svs-2000/seed_2/[date2].ShapleyValueSampling.jsonl\"\n    ds_1 = load_dataset(\"json\", data_files=[seed_1_path, ])['train']\n    ds_1._info.description = \"Model: textattack/bert-base-uncased-MNLI\\nDataset: MNLI\\nExplainer: svs-2000\"\n    ds_1 = ds_1.add_column(\"idx\", list(range(len(ds_1))))\n    obj_1 = thermostat.Thermopack(ds_1)\n    target_dir = \"visualization/heatmap\"\n    os.makedirs(target_dir, exist_ok=True)\n    for i in range(100):\n        img1 = data_1[i].render()\n        f_1 = open(os.path.join(target_dir, f\"{i}_seed_1.html\"), \"w\", encoding='utf-8')\n        f_1.write(img1)\n        img2 = data_2[i].render()\n        f_2 = open(os.path.join(target_dir, f\"{i}_seed_2.html\"), \"w\", encoding='utf-8')\n        f_2.write(img2)", ""]}
{"filename": "internal_correlation_with_lib.py", "chunked_list": ["import json\nimport thermostat\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.stats import spearmanr\nimport os\nimport glob\ndata_cache_dir = \"./datasets/imdb\"\ndata = thermostat.load(f\"imdb-bert-svs\", cache_dir=data_cache_dir)\nfor explainer in [\"kernelshap-3600-sample200\", \"kernelshap-3600\"]:\n    count = 0\n    path = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/{explainer}/\"\n    seed_dirs = glob.glob(path + \"seed_*\")\n    if len(seed_dirs) > 2:\n        seed_dirs = seed_dirs[:2]\n    all_correlations_1_ref = []\n    all_correlations_2_ref = []\n    all_correlations = []\n    all_ps = []\n    all_mask_check = 0\n    seed_dir0 = os.path.join(path, seed_dirs[0])\n    seed_dir1 = os.path.join(path, seed_dirs[1])\n    seed_file0 = glob.glob(os.path.join(seed_dir0, \"*.jsonl\"))[0]\n    seed_file1 = glob.glob(os.path.join(seed_dir1, \"*.jsonl\"))[0]\n    seed_file_path0 = os.path.join(seed_dir0, seed_file0)\n    seed_file_path1 = os.path.join(seed_dir1, seed_file1)\n    print(seed_file_path0)\n    print(seed_file_path1)\n    with open(seed_file_path0, \"r\", encoding='utf-8') as f_in0, open(seed_file_path1, \"r\", encoding='utf-8') as f_in1:\n        buf0, buf1 = f_in0.readlines(), f_in1.readlines()\n        assert len(buf0) == len(buf1), f\"{len(buf0)}, {len(buf1)}\"\n        for line0, line1 in tqdm(zip(buf0, buf1), total=len(buf0)):\n            obj0, obj1 = json.loads(line0), json.loads(line1)\n            ref = data[count].attributions\n            if count == 0:\n                print(data[count])\n            attr0, attr1 = obj0[\"attributions\"], obj1[\"attributions\"]\n            attr0, attr1 = np.array(attr0), np.array(attr1)\n            if ((attr0 == 0) != (attr1 == 0)).any():\n                all_mask_check += 1\n            _spearman, _pval = spearmanr(attr0, ref)\n            all_correlations_1_ref.append(_spearman)\n            _spearman, _pval = spearmanr(attr1, ref)\n            all_correlations_2_ref.append(_spearman)\n            _spearman, _pval = spearmanr(attr1, attr0)\n            all_correlations.append(_spearman)\n            count += 1\n    print(f\"spearman correlation: {np.mean(all_correlations)} ({np.std(all_correlations)}, {np.min(all_correlations)}, {np.max(all_correlations)})\", )\n    print(f\"spearman correlation: {np.mean(all_correlations_1_ref)} ({np.std(all_correlations_1_ref)}, {np.min(all_correlations_1_ref)}, {np.max(all_correlations_1_ref)})\", )\n    print(f\"spearman correlation: {np.mean(all_correlations_2_ref)} ({np.std(all_correlations_2_ref)}, {np.min(all_correlations_2_ref)}, {np.max(all_correlations_2_ref)})\", )\n    print(f\"mask mismatch rate: {all_mask_check / len(all_correlations)}\")", "data = thermostat.load(f\"imdb-bert-svs\", cache_dir=data_cache_dir)\nfor explainer in [\"kernelshap-3600-sample200\", \"kernelshap-3600\"]:\n    count = 0\n    path = f\"path/to/thermostat/experiments/thermostat/multi_nli/bert/{explainer}/\"\n    seed_dirs = glob.glob(path + \"seed_*\")\n    if len(seed_dirs) > 2:\n        seed_dirs = seed_dirs[:2]\n    all_correlations_1_ref = []\n    all_correlations_2_ref = []\n    all_correlations = []\n    all_ps = []\n    all_mask_check = 0\n    seed_dir0 = os.path.join(path, seed_dirs[0])\n    seed_dir1 = os.path.join(path, seed_dirs[1])\n    seed_file0 = glob.glob(os.path.join(seed_dir0, \"*.jsonl\"))[0]\n    seed_file1 = glob.glob(os.path.join(seed_dir1, \"*.jsonl\"))[0]\n    seed_file_path0 = os.path.join(seed_dir0, seed_file0)\n    seed_file_path1 = os.path.join(seed_dir1, seed_file1)\n    print(seed_file_path0)\n    print(seed_file_path1)\n    with open(seed_file_path0, \"r\", encoding='utf-8') as f_in0, open(seed_file_path1, \"r\", encoding='utf-8') as f_in1:\n        buf0, buf1 = f_in0.readlines(), f_in1.readlines()\n        assert len(buf0) == len(buf1), f\"{len(buf0)}, {len(buf1)}\"\n        for line0, line1 in tqdm(zip(buf0, buf1), total=len(buf0)):\n            obj0, obj1 = json.loads(line0), json.loads(line1)\n            ref = data[count].attributions\n            if count == 0:\n                print(data[count])\n            attr0, attr1 = obj0[\"attributions\"], obj1[\"attributions\"]\n            attr0, attr1 = np.array(attr0), np.array(attr1)\n            if ((attr0 == 0) != (attr1 == 0)).any():\n                all_mask_check += 1\n            _spearman, _pval = spearmanr(attr0, ref)\n            all_correlations_1_ref.append(_spearman)\n            _spearman, _pval = spearmanr(attr1, ref)\n            all_correlations_2_ref.append(_spearman)\n            _spearman, _pval = spearmanr(attr1, attr0)\n            all_correlations.append(_spearman)\n            count += 1\n    print(f\"spearman correlation: {np.mean(all_correlations)} ({np.std(all_correlations)}, {np.min(all_correlations)}, {np.max(all_correlations)})\", )\n    print(f\"spearman correlation: {np.mean(all_correlations_1_ref)} ({np.std(all_correlations_1_ref)}, {np.min(all_correlations_1_ref)}, {np.max(all_correlations_1_ref)})\", )\n    print(f\"spearman correlation: {np.mean(all_correlations_2_ref)} ({np.std(all_correlations_2_ref)}, {np.min(all_correlations_2_ref)}, {np.max(all_correlations_2_ref)})\", )\n    print(f\"mask mismatch rate: {all_mask_check / len(all_correlations)}\")", "\n\n\n\n\n"]}
{"filename": "thermostat/src/thermostat/explainers/svs.py", "chunked_list": ["import torch\nfrom captum.attr import ShapleyValueSampling\nfrom captum.attr import KernelShap\nfrom typing import Dict\n\nfrom thermostat.explain import ExplainerAutoModelInitializer\n\n\nclass ExplainerShapleyValueSampling(ExplainerAutoModelInitializer):\n\n    def __init__(self):\n        super().__init__()\n        self.n_samples = None\n\n    def validate_config(self, config: Dict) -> bool:\n        super().validate_config(config)\n        assert 'n_samples' in config['explainer'], 'Define how many samples to take along the straight line path ' \\\n                                                   'from the baseline.'\n\n    @classmethod\n    def from_config(cls, config):\n        res = super().from_config(config)\n        res.n_samples = config['explainer']['n_samples']\n        res.explainer = ShapleyValueSampling(res.forward_func)\n        return res\n\n    def explain(self, batch):\n        # todo: set model.eval() ? -> in a test self.model.training was False\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        inputs, additional_forward_args = self.get_inputs_and_additional_args(base_model=type(self.model.base_model),\n                                                                              batch=batch)\n        predictions = self.forward_func(inputs, *additional_forward_args)\n        target = torch.argmax(predictions, dim=1)\n        base_line = self.get_baseline(batch=batch)\n        attributions = self.explainer.attribute(inputs=inputs,\n                                                n_samples=self.n_samples,\n                                                additional_forward_args=additional_forward_args,\n                                                target=target,\n                                                baselines=base_line)\n        return attributions, predictions", "class ExplainerShapleyValueSampling(ExplainerAutoModelInitializer):\n\n    def __init__(self):\n        super().__init__()\n        self.n_samples = None\n\n    def validate_config(self, config: Dict) -> bool:\n        super().validate_config(config)\n        assert 'n_samples' in config['explainer'], 'Define how many samples to take along the straight line path ' \\\n                                                   'from the baseline.'\n\n    @classmethod\n    def from_config(cls, config):\n        res = super().from_config(config)\n        res.n_samples = config['explainer']['n_samples']\n        res.explainer = ShapleyValueSampling(res.forward_func)\n        return res\n\n    def explain(self, batch):\n        # todo: set model.eval() ? -> in a test self.model.training was False\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        inputs, additional_forward_args = self.get_inputs_and_additional_args(base_model=type(self.model.base_model),\n                                                                              batch=batch)\n        predictions = self.forward_func(inputs, *additional_forward_args)\n        target = torch.argmax(predictions, dim=1)\n        base_line = self.get_baseline(batch=batch)\n        attributions = self.explainer.attribute(inputs=inputs,\n                                                n_samples=self.n_samples,\n                                                additional_forward_args=additional_forward_args,\n                                                target=target,\n                                                baselines=base_line)\n        return attributions, predictions", "\nclass ExplainerKernelShap(ExplainerShapleyValueSampling):\n    @classmethod\n    def from_config(cls, config):\n        res = super().from_config(config)\n        res.n_samples = config['explainer']['n_samples']\n        res.explainer = KernelShap(res.forward_func)\n        return res\n", ""]}
{"filename": "thermostat/src/thermostat/data/thermostat_configs.py", "chunked_list": ["import datasets\n\n\n_VERSION = datasets.Version('1.0.1', '')\n\n\n# Base arguments for any dataset\n_BASE_KWARGS = dict(\n    features={\n        \"attributions\": \"attributions\",", "    features={\n        \"attributions\": \"attributions\",\n        \"predictions\": \"predictions\",\n        \"input_ids\": \"input_ids\",\n    },\n    citation=\"Coming soon.\",\n    url=\"https://github.com/DFKI-NLP/\",\n)\n\n", "\n\n_YELP_KWARGS = dict(\n    dataset=\"yelp_polarity\",\n    label_classes=[\"1\", \"2\"],\n    label_column=\"label\",\n    text_column=\"text\",\n    **_BASE_KWARGS,\n)\n_YELP_BERT_KWARGS = dict(", ")\n_YELP_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-yelp-polarity\",\n    **_YELP_KWARGS,\n)\n# Base arguments for AG News dataset\n_AGNEWS_KWARGS = dict(\n    dataset=\"ag_news\",\n    label_classes=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"],\n    label_column=\"label\",", "    label_classes=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"],\n    label_column=\"label\",\n    text_column=\"text\",\n    **_BASE_KWARGS,\n)\n_AGNEWS_ALBERT_KWARGS = dict(\n    model=\"textattack/albert-base-v2-ag-news\",\n    **_AGNEWS_KWARGS,\n)\n_AGNEWS_BERT_KWARGS = dict(", ")\n_AGNEWS_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-ag-news\",\n    **_AGNEWS_KWARGS,\n)\n_AGNEWS_ROBERTA_KWARGS = dict(\n    model=\"textattack/roberta-base-ag-news\",\n    **_AGNEWS_KWARGS,\n)\n", ")\n\n# Base arguments for IMDb dataset\n_IMDB_KWARGS = dict(\n    dataset=\"imdb\",\n    label_classes=[\"neg\", \"pos\"],\n    label_column=\"label\",\n    text_column=\"text\",\n    **_BASE_KWARGS,\n)", "    **_BASE_KWARGS,\n)\n_IMDB_ALBERT_KWARGS = dict(\n    model=\"textattack/albert-base-v2-imdb\",\n    **_IMDB_KWARGS,\n)\n_IMDB_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-imdb\",\n    **_IMDB_KWARGS,\n)", "    **_IMDB_KWARGS,\n)\n_IMDB_ELECTRA_KWARGS = dict(\n    model=\"monologg/electra-small-finetuned-imdb\",\n    **_IMDB_KWARGS,\n)\n_IMDB_ROBERTA_KWARGS = dict(\n    model=\"textattack/roberta-base-imdb\",\n    **_IMDB_KWARGS,\n)", "    **_IMDB_KWARGS,\n)\n_IMDB_XLNET_KWARGS = dict(\n    model=\"textattack/xlnet-base-cased-imdb\",\n    **_IMDB_KWARGS,\n)\n\n# Base arguments for MNLI dataset\n_MNLI_KWARGS = dict(\n    dataset=\"multi_nli\",", "_MNLI_KWARGS = dict(\n    dataset=\"multi_nli\",\n    label_column=\"label\",\n    label_classes=[\"entailment\", \"neutral\", \"contradiction\"],\n    text_column=[\"premise\", \"hypothesis\"],\n    **_BASE_KWARGS,\n)\n_MNLI_ALBERT_KWARGS = dict(\n    model=\"prajjwal1/albert-base-v2-mnli\",\n    **_MNLI_KWARGS,", "    model=\"prajjwal1/albert-base-v2-mnli\",\n    **_MNLI_KWARGS,\n)\n_MNLI_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-MNLI\",\n    **_MNLI_KWARGS,\n)\n_MNLI_ELECTRA_KWARGS = dict(\n    model=\"howey/electra-base-mnli\",\n    **_MNLI_KWARGS,", "    model=\"howey/electra-base-mnli\",\n    **_MNLI_KWARGS,\n)\n_MNLI_ROBERTA_KWARGS = dict(\n    model=\"textattack/roberta-base-MNLI\",\n    **_MNLI_KWARGS,\n)\n_MNLI_XLNET_KWARGS = dict(\n    model=\"textattack/xlnet-base-cased-MNLI\",\n    **_MNLI_KWARGS,", "    model=\"textattack/xlnet-base-cased-MNLI\",\n    **_MNLI_KWARGS,\n)\n\n# Base arguments for XNLI dataset\n_XNLI_KWARGS = dict(\n    dataset=\"xnli\",\n    label_column=\"label\",\n    label_classes=[\"entailment\", \"neutral\", \"contradiction\"],\n    text_column=[\"premise\", \"hypothesis\"],", "    label_classes=[\"entailment\", \"neutral\", \"contradiction\"],\n    text_column=[\"premise\", \"hypothesis\"],\n    **_BASE_KWARGS,\n)\n_XNLI_ALBERT_KWARGS = dict(\n    model=\"prajjwal1/albert-base-v2-mnli\",\n    **_XNLI_KWARGS,\n)\n_XNLI_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-MNLI\",", "_XNLI_BERT_KWARGS = dict(\n    model=\"textattack/bert-base-uncased-MNLI\",\n    **_XNLI_KWARGS,\n)\n_XNLI_ELECTRA_KWARGS = dict(\n    model=\"howey/electra-base-mnli\",\n    **_XNLI_KWARGS,\n)\n_XNLI_ROBERTA_KWARGS = dict(\n    model=\"textattack/roberta-base-MNLI\",", "_XNLI_ROBERTA_KWARGS = dict(\n    model=\"textattack/roberta-base-MNLI\",\n    **_XNLI_KWARGS,\n)\n_XNLI_XLNET_KWARGS = dict(\n    model=\"textattack/xlnet-base-cased-MNLI\",\n    **_XNLI_KWARGS,\n)\n\n\nclass ThermostatConfig(datasets.BuilderConfig):\n    \"\"\" BuilderConfig for Thermostat \"\"\"\n\n    def __init__(\n        self,\n        explainer,\n        model,\n        dataset,\n        features,\n        label_column,\n        label_classes,\n        text_column,\n        data_url,\n        citation,\n        url,\n        **kwargs,\n    ):\n        super(ThermostatConfig, self).__init__(version=_VERSION, **kwargs)\n        self.explainer = explainer\n        self.model = model\n        self.dataset = dataset\n        self.features = features\n        self.label_column = label_column\n        self.label_classes = label_classes\n        self.text_column = text_column\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url", "\n\nclass ThermostatConfig(datasets.BuilderConfig):\n    \"\"\" BuilderConfig for Thermostat \"\"\"\n\n    def __init__(\n        self,\n        explainer,\n        model,\n        dataset,\n        features,\n        label_column,\n        label_classes,\n        text_column,\n        data_url,\n        citation,\n        url,\n        **kwargs,\n    ):\n        super(ThermostatConfig, self).__init__(version=_VERSION, **kwargs)\n        self.explainer = explainer\n        self.model = model\n        self.dataset = dataset\n        self.features = features\n        self.label_column = label_column\n        self.label_classes = label_classes\n        self.text_column = text_column\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url", "\n\nbuilder_configs = [\n    ThermostatConfig(\n        name=\"yelp_polarity-bert-svs\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"[...]\",\n        **_YELP_BERT_KWARGS,\n    ),", "        **_YELP_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"yelp_polarity_kshap200_seed1-bert-svs\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"KernelShap\",\n        data_url=\"[...]\",\n        **_YELP_BERT_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"yelp_polarity_kshap200_seed2-bert-svs\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"KernelShap\",\n        data_url=\"[...]\",\n        **_YELP_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"yelp_polarity_amortized_model_output\",", "    ThermostatConfig(\n        name=\"yelp_polarity_amortized_model_output\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"KernelShap\",\n        data_url=\"[...]\",\n        **_YELP_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"yelp_polarity_amortized_model_reference\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",", "        name=\"yelp_polarity_amortized_model_reference\",\n        description=\"Yelp Polarity dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"KernelShap\",\n        data_url=\"[...]\",\n        **_YELP_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-albert-lgxa\",\n        description=\"AG News dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"AG News dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/8XPC557ePpWCBQY/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-albert-lig\",\n        description=\"AG News dataset, ALBERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zS7mcMsdAp5ZENX/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zS7mcMsdAp5ZENX/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-albert-lime\",\n        description=\"AG News dataset, ALBERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/8SLyHdDgRk2pXSL/download\",\n        **_AGNEWS_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/8SLyHdDgRk2pXSL/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-albert-lime-100\",\n        description=\"AG News dataset, ALBERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/W3GT4ZDT2BzR5mj/download\",\n        **_AGNEWS_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/W3GT4ZDT2BzR5mj/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-albert-occlusion\",\n        description=\"AG News dataset, ALBERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Li9HwfKfFqjCQTM/download\",\n        **_AGNEWS_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Li9HwfKfFqjCQTM/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-albert-svs\",\n        description=\"AG News dataset, ALBERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/GLppwQjeBTsLtTC/download\",\n        **_AGNEWS_ALBERT_KWARGS,\n    ),", "        **_AGNEWS_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-bert-lgxa\",\n        description=\"AG News dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zn5mKsryyrX3e58/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"ag_news-bert-lig\",\n        description=\"AG News dataset, BERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Qq3dR7sHsfX9JXZ/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-bert-lime\",", "    ThermostatConfig(\n        name=\"ag_news-bert-lime\",\n        description=\"AG News dataset, BERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/rW8MJyAjBGQxsK9/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-bert-lime-100\",", "    ThermostatConfig(\n        name=\"ag_news-bert-lime-100\",\n        description=\"AG News dataset, BERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/FkSdXZPpN78HSHR/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-bert-occlusion\",", "    ThermostatConfig(\n        name=\"ag_news-bert-occlusion\",\n        description=\"AG News dataset, BERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Grf97s6bJwoZGyx/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-bert-svs\",\n        description=\"AG News dataset, BERT model, Shapley Value Sampling explanations\",", "        name=\"ag_news-bert-svs\",\n        description=\"AG News dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/dCbgsjdW6b9pzo3/download\",\n        **_AGNEWS_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-roberta-lgxa\",\n        description=\"AG News dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"AG News dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Kz9GrYrMZB4gp7E/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-roberta-lig\",\n        description=\"AG News dataset, RoBERTa model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/mpH8sT6tXDoG5qi/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/mpH8sT6tXDoG5qi/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-roberta-lime\",\n        description=\"AG News dataset, RoBERTa model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/qRgBtwfjaXceJoL/download\",\n        **_AGNEWS_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/qRgBtwfjaXceJoL/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-roberta-lime-100\",\n        description=\"AG News dataset, RoBERTa model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/kFyjX2LqBdcW9bp/download\",\n        **_AGNEWS_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/kFyjX2LqBdcW9bp/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"ag_news-roberta-occlusion\",\n        description=\"AG News dataset, RoBERTa model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/78aZttqxKQNdW6J/download\",\n        **_AGNEWS_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/78aZttqxKQNdW6J/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"ag_news-roberta-svs\",\n        description=\"AG News dataset, RoBERTa model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/yabEAY5sLpjxKkW/download\",\n        **_AGNEWS_ROBERTA_KWARGS,\n    ),", "        **_AGNEWS_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-albert-lgxa\",\n        description=\"IMDb dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/srbYBbmKBsGMXWn/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"imdb-albert-lig\",\n        description=\"IMDb dataset, ALBERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zjMddcqewEcwSPG/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-albert-lime\",", "    ThermostatConfig(\n        name=\"imdb-albert-lime\",\n        description=\"IMDb dataset, ALBERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Tgktb4fq4EdXJNx/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-albert-lime-100\",", "    ThermostatConfig(\n        name=\"imdb-albert-lime-100\",\n        description=\"IMDb dataset, ALBERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/FzErcT9TcFcG2Pr/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-albert-occ\",", "    ThermostatConfig(\n        name=\"imdb-albert-occ\",\n        description=\"IMDb dataset, ALBERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/98XqEgbZt9KiSfm/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-albert-svs\",\n        description=\"IMDb dataset, ALBERT model, Shapley Value Sampling explanations\",", "        name=\"imdb-albert-svs\",\n        description=\"IMDb dataset, ALBERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/sQMK2XsknbzK23a/download\",\n        **_IMDB_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-bert-lgxa\",\n        description=\"IMDb dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"IMDb dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/NkPpnyMN8rdWE7L/download\",\n        **_IMDB_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-bert-lig\",\n        description=\"IMDb dataset, BERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/SdrPeJQQSExFQ8e/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/SdrPeJQQSExFQ8e/download\",\n        **_IMDB_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-bert-lime\",\n        description=\"IMDb dataset, BERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/ZQEdEmFtKeGkYWp/download\",\n        **_IMDB_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/ZQEdEmFtKeGkYWp/download\",\n        **_IMDB_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-bert-lime-100\",\n        description=\"IMDb dataset, BERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Qx7z8SFcMTB5bFa/download\",\n        **_IMDB_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/Qx7z8SFcMTB5bFa/download\",\n        **_IMDB_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-bert-occ\",\n        description=\"IMDb dataset, BERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/PjMDBzaoHHqs2WF/download\",\n        **_IMDB_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/PjMDBzaoHHqs2WF/download\",\n        **_IMDB_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-bert-svs\",\n        description=\"IMDb dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/DjmCKdBoWHt8jbX/download\",\n        **_IMDB_BERT_KWARGS,\n    ),", "        **_IMDB_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-electra-lgxa\",\n        description=\"IMDb dataset, ELECTRA model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/WdLYpQerXC5KrHK/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"imdb-electra-lig\",\n        description=\"IMDb dataset, ELECTRA model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/e3Mibf9dqRfobYw/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-electra-lime\",", "    ThermostatConfig(\n        name=\"imdb-electra-lime\",\n        description=\"IMDb dataset, ELECTRA model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/7p2576kFqiQLL9x/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-electra-lime-100\",", "    ThermostatConfig(\n        name=\"imdb-electra-lime-100\",\n        description=\"IMDb dataset, ELECTRA model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/LBqzn6JiQNzwMAC/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-electra-occ\",", "    ThermostatConfig(\n        name=\"imdb-electra-occ\",\n        description=\"IMDb dataset, ELECTRA model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/TZYTgnySrEbm5Xx/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-electra-svs\",\n        description=\"IMDb dataset, ELECTRA model, Shapley Value Sampling explanations\",", "        name=\"imdb-electra-svs\",\n        description=\"IMDb dataset, ELECTRA model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MPHqZwJCP97sA4D/download\",\n        **_IMDB_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-roberta-lgxa\",\n        description=\"IMDb dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"IMDb dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/oWCkBFgsstPakKS/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-roberta-lig\",\n        description=\"IMDb dataset, RoBERTa model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/qJBYkfwppGZ4NF5/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/qJBYkfwppGZ4NF5/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-roberta-lime\",\n        description=\"IMDb dataset, RoBERTa model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/rpsMTw3S6JkQgcF/download\",\n        **_IMDB_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/rpsMTw3S6JkQgcF/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-roberta-lime-100\",\n        description=\"IMDb dataset, RoBERTa model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/YZsAoJmR4EcwnG2/download\",\n        **_IMDB_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/YZsAoJmR4EcwnG2/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-roberta-occ\",\n        description=\"IMDb dataset, RoBERTa model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/wDw9k7PRWwsfQPB/download\",\n        **_IMDB_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/wDw9k7PRWwsfQPB/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-roberta-svs\",\n        description=\"IMDb dataset, RoBERTa model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/339zLEttF6djtBR/download\",\n        **_IMDB_ROBERTA_KWARGS,\n    ),", "        **_IMDB_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-xlnet-lgxa\",\n        description=\"IMDb dataset, XLNet model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/53g8Gw28BX9eiPQ/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"imdb-xlnet-lig\",\n        description=\"IMDb dataset, XLNet model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/sgn79wJgTWjoNjq/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-xlnet-lime\",", "    ThermostatConfig(\n        name=\"imdb-xlnet-lime\",\n        description=\"IMDb dataset, XLNet model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/YCDW67f49wj5NXg/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-xlnet-lime-100\",", "    ThermostatConfig(\n        name=\"imdb-xlnet-lime-100\",\n        description=\"IMDb dataset, XLNet model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/T2KsA8ragxPz6eL/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"imdb-xlnet-occ\",", "    ThermostatConfig(\n        name=\"imdb-xlnet-occ\",\n        description=\"IMDb dataset, XLNet model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/H46msX6FQfFrCpg/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"imdb-xlnet-svs\",\n        description=\"IMDb dataset, XLNet model, Shapley Value Sampling explanations\",", "        name=\"imdb-xlnet-svs\",\n        description=\"IMDb dataset, XLNet model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/y9grFyRQC2rDSaN/download\",\n        **_IMDB_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-albert-lgxa\",\n        description=\"MultiNLI dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"MultiNLI dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/nkRmprdnbb5C4Tx/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-albert-lig\",\n        description=\"MultiNLI dataset, ALBERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/3WAqbXa2DG2RCgz/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/3WAqbXa2DG2RCgz/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-albert-lime\",\n        description=\"MultiNLI dataset, ALBERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/e6JRy9fidSAC5zK/download\",\n        **_MNLI_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/e6JRy9fidSAC5zK/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-albert-lime-100\",\n        description=\"MultiNLI dataset, ALBERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/WB2N3nFkHTGkXY8/download\",\n        **_MNLI_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/WB2N3nFkHTGkXY8/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-albert-occ\",\n        description=\"MultiNLI dataset, ALBERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/F5xWYpyDpwaAPJs/download\",\n        **_MNLI_ALBERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/F5xWYpyDpwaAPJs/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-albert-svs\",\n        description=\"MultiNLI dataset, ALBERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/fffM7w64CnTSzHA/download\",\n        **_MNLI_ALBERT_KWARGS,\n    ),", "        **_MNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-bert-lgxa\",\n        description=\"MultiNLI dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MdjebgkdexA2ZDt/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"multi_nli-bert-lig\",\n        description=\"MultiNLI dataset, BERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/g53nbtnXaFyPLM7/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-bert-lime\",", "    ThermostatConfig(\n        name=\"multi_nli-bert-lime\",\n        description=\"MultiNLI dataset, BERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/ptspBexoHaXtqXD/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-bert-lime-100\",", "    ThermostatConfig(\n        name=\"multi_nli-bert-lime-100\",\n        description=\"MultiNLI dataset, BERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/LjFccwQ2mCAnsmH/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-bert-occ\",", "    ThermostatConfig(\n        name=\"multi_nli-bert-occ\",\n        description=\"MultiNLI dataset, BERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/cHK6YCAo4ESZ3xx/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-bert-svs\",\n        description=\"MultiNLI dataset, BERT model, Shapley Value Sampling explanations\",", "        name=\"multi_nli-bert-svs\",\n        description=\"MultiNLI dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/d5TTHCkAb5TJmbg/download\",\n        **_MNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-electra-lgxa\",\n        description=\"MultiNLI dataset, ELECTRA model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"MultiNLI dataset, ELECTRA model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/2HrCmp9sxJNiKBc/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-electra-lig\",\n        description=\"MultiNLI dataset, ELECTRA model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/2eZnJgCbWd2D4PB/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/2eZnJgCbWd2D4PB/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-electra-lime\",\n        description=\"MultiNLI dataset, ELECTRA model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/WzBwpwC9FoQZCwB/download\",\n        **_MNLI_ELECTRA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/WzBwpwC9FoQZCwB/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-electra-lime-100\",\n        description=\"MultiNLI dataset, ELECTRA model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/TX6jWs9wBdsJA9w/download\",\n        **_MNLI_ELECTRA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/TX6jWs9wBdsJA9w/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-electra-occ\",\n        description=\"MultiNLI dataset, ELECTRA model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MGjQmKK9kynZTQt/download\",\n        **_MNLI_ELECTRA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MGjQmKK9kynZTQt/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-electra-svs\",\n        description=\"MultiNLI dataset, ELECTRA model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zx3rGTpMkRT68tk/download\",\n        **_MNLI_ELECTRA_KWARGS,\n    ),", "        **_MNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-roberta-lgxa\",\n        description=\"MultiNLI dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/SxDwtGCpPzi3DDz/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"multi_nli-roberta-lig\",\n        description=\"MultiNLI dataset, RoBERTa model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/8zaTxTCijG6g7Y5/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-roberta-lime\",", "    ThermostatConfig(\n        name=\"multi_nli-roberta-lime\",\n        description=\"MultiNLI dataset, RoBERTa model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/dY4z4ptcMtiYzZs/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-roberta-lime-100\",", "    ThermostatConfig(\n        name=\"multi_nli-roberta-lime-100\",\n        description=\"MultiNLI dataset, RoBERTa model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/KTQWmCDX2EjHtQE/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-roberta-occ\",", "    ThermostatConfig(\n        name=\"multi_nli-roberta-occ\",\n        description=\"MultiNLI dataset, RoBERTa model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/w5YSxNc6L8QZisG/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-roberta-svs\",\n        description=\"MultiNLI dataset, RoBERTa model, Shapley Value Sampling explanations\",", "        name=\"multi_nli-roberta-svs\",\n        description=\"MultiNLI dataset, RoBERTa model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/3aPeTawM8cbAsEg/download\",\n        **_MNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-lgxa\",\n        description=\"MultiNLI dataset, XLNet model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"MultiNLI dataset, XLNet model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/n79G9kf9jbNx8o7/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-lig\",\n        description=\"MultiNLI dataset, XLNet model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MZr8jTnaCBdMPGe/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MZr8jTnaCBdMPGe/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-lime\",\n        description=\"MultiNLI dataset, XLNet model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/B7tfLSRKBGYxJ3s/download\",\n        **_MNLI_XLNET_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/B7tfLSRKBGYxJ3s/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-lime-100\",\n        description=\"MultiNLI dataset, XLNet model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/SesZACA2AwyefFp/download\",\n        **_MNLI_XLNET_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/SesZACA2AwyefFp/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-occ\",\n        description=\"MultiNLI dataset, XLNet model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/YWjJ6T7n6oeKbJJ/download\",\n        **_MNLI_XLNET_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/YWjJ6T7n6oeKbJJ/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"multi_nli-xlnet-svs\",\n        description=\"MultiNLI dataset, XLNet model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/CXYRFGsR2NFeAZa/download\",\n        **_MNLI_XLNET_KWARGS,\n    ),", "        **_MNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-albert-lgxa\",\n        description=\"XNLI dataset, ALBERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/zCSq69Z853fs3ez/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"xnli-albert-lig\",\n        description=\"XNLI dataset, ALBERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/ZcP34Eg6eb3TrWF/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-albert-lime\",", "    ThermostatConfig(\n        name=\"xnli-albert-lime\",\n        description=\"XNLI dataset, ALBERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/sijLW3ceigxDsKY/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-albert-lime-100\",", "    ThermostatConfig(\n        name=\"xnli-albert-lime-100\",\n        description=\"XNLI dataset, ALBERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/oQW5cRc6GbqHtB6/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-albert-occ\",", "    ThermostatConfig(\n        name=\"xnli-albert-occ\",\n        description=\"XNLI dataset, ALBERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/bEg95CGBtzaFQij/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-albert-svs\",\n        description=\"XNLI dataset, ALBERT model, Shapley Value Sampling explanations\",", "        name=\"xnli-albert-svs\",\n        description=\"XNLI dataset, ALBERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/wekiPq7ijzsCQK4/download\",\n        **_XNLI_ALBERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-bert-lgxa\",\n        description=\"XNLI dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"XNLI dataset, BERT model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/pb3Q6GQodyMqkgJ/download\",\n        **_XNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-bert-lig\",\n        description=\"XNLI dataset, BERT model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MX8YkzjFtpd43PM/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/MX8YkzjFtpd43PM/download\",\n        **_XNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-bert-lime\",\n        description=\"XNLI dataset, BERT model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/KfjqkRTd7FSWSkx/download\",\n        **_XNLI_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/KfjqkRTd7FSWSkx/download\",\n        **_XNLI_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-bert-lime-100\",\n        description=\"XNLI dataset, BERT model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/FXHt989a2En8aZZ/download\",\n        **_XNLI_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/FXHt989a2En8aZZ/download\",\n        **_XNLI_BERT_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-bert-occ\",\n        description=\"XNLI dataset, BERT model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/AHoTKbtSCQ73QxN/download\",\n        **_XNLI_BERT_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/AHoTKbtSCQ73QxN/download\",\n        **_XNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-bert-svs\",\n        description=\"XNLI dataset, BERT model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/D4ctEijzerMoNT8/download\",\n        **_XNLI_BERT_KWARGS,\n    ),", "        **_XNLI_BERT_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-electra-lgxa\",\n        description=\"XNLI dataset, ELECTRA model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/t4ge7pA57gy4dKr/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"xnli-electra-lig\",\n        description=\"XNLI dataset, ELECTRA model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/fHBSRQoAXzo3EKj/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-electra-lime\",", "    ThermostatConfig(\n        name=\"xnli-electra-lime\",\n        description=\"XNLI dataset, ELECTRA model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/XnkiHXgxNsptxTJ/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-electra-lime-100\",", "    ThermostatConfig(\n        name=\"xnli-electra-lime-100\",\n        description=\"XNLI dataset, ELECTRA model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/7zNtxCHxEZk2tzC/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-electra-occ\",", "    ThermostatConfig(\n        name=\"xnli-electra-occ\",\n        description=\"XNLI dataset, ELECTRA model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/2RtRaN8q5fDHWyF/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-electra-svs\",\n        description=\"XNLI dataset, ELECTRA model, Shapley Value Sampling explanations\",", "        name=\"xnli-electra-svs\",\n        description=\"XNLI dataset, ELECTRA model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/T3KKsM5TtsHyCAL/download\",\n        **_XNLI_ELECTRA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-roberta-lgxa\",\n        description=\"XNLI dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",", "        description=\"XNLI dataset, RoBERTa model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/aPoCzcXDCfya3Ww/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-roberta-lig\",\n        description=\"XNLI dataset, RoBERTa model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/iPA6rCfjc49ofpN/download\",", "        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/iPA6rCfjc49ofpN/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-roberta-lime\",\n        description=\"XNLI dataset, RoBERTa model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/pZKo7m4g9WJXfoe/download\",\n        **_XNLI_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/pZKo7m4g9WJXfoe/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-roberta-lime-100\",\n        description=\"XNLI dataset, RoBERTa model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/CHSR7Arw8M56bxN/download\",\n        **_XNLI_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/CHSR7Arw8M56bxN/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-roberta-occ\",\n        description=\"XNLI dataset, RoBERTa model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/XB2tnATQW3tbxPW/download\",\n        **_XNLI_ROBERTA_KWARGS,", "        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/XB2tnATQW3tbxPW/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-roberta-svs\",\n        description=\"XNLI dataset, RoBERTa model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/opYTzjSeWWL7eYg/download\",\n        **_XNLI_ROBERTA_KWARGS,\n    ),", "        **_XNLI_ROBERTA_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-xlnet-lgxa\",\n        description=\"XNLI dataset, XLNet model, Layer Gradient x Activation explanations\",\n        explainer=\"LayerGradientXActivation\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/jBXEkQS7WTz3a7J/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(", "    ),\n    ThermostatConfig(\n        name=\"xnli-xlnet-lig\",\n        description=\"XNLI dataset, XLNet model, Layer Integrated Gradients explanations\",\n        explainer=\"LayerIntegratedGradients\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/kx7cJYFbyCjy58z/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-xlnet-lime\",", "    ThermostatConfig(\n        name=\"xnli-xlnet-lime\",\n        description=\"XNLI dataset, XLNet model, LIME explanations\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/6s4DFPNYpzi8722/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-xlnet-lime-100\",", "    ThermostatConfig(\n        name=\"xnli-xlnet-lime-100\",\n        description=\"XNLI dataset, XLNet model, LIME explanations, 100 samples\",\n        explainer=\"LimeBase\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/ZzN9PSkiRrJNza2/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n    # new change\n    ThermostatConfig(\n        name=\"xnli-xlnet-occ\",", "    ThermostatConfig(\n        name=\"xnli-xlnet-occ\",\n        description=\"XNLI dataset, XLNet model, Occlusion explanations\",\n        explainer=\"Occlusion\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/yEFEyrq4pbGKP4s/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n    ThermostatConfig(\n        name=\"xnli-xlnet-svs\",\n        description=\"XNLI dataset, XLNet model, Shapley Value Sampling explanations\",", "        name=\"xnli-xlnet-svs\",\n        description=\"XNLI dataset, XLNet model, Shapley Value Sampling explanations\",\n        explainer=\"ShapleyValueSampling\",\n        data_url=\"https://cloud.dfki.de/owncloud/index.php/s/fT34Q7CD2GQkdxJ/download\",\n        **_XNLI_XLNET_KWARGS,\n    ),\n]\n"]}
