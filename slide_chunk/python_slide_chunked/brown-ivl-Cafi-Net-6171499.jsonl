{"filename": "nerf/load_brics.py", "chunked_list": ["import os\nimport torch\nimport numpy as np\nimport imageio \nimport json\nimport torch.nn.functional as F\nimport cv2\nimport glob\nimport random\nimport matplotlib.pyplot as plt", "import random\nimport matplotlib.pyplot as plt\nimport pickle\nimport h5py\nfrom scipy.spatial.transform import Rotation as R\n\n\nos.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n\ntrans_t = lambda t : torch.Tensor([", "\ntrans_t = lambda t : torch.Tensor([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,1,t],\n    [0,0,0,1]]).float()\n\nrot_phi = lambda phi : torch.Tensor([\n    [1,0,0,0],\n    [0,np.cos(phi),-np.sin(phi),0],", "    [1,0,0,0],\n    [0,np.cos(phi),-np.sin(phi),0],\n    [0,np.sin(phi), np.cos(phi),0],\n    [0,0,0,1]]).float()\n\nrot_theta = lambda th : torch.Tensor([\n    [np.cos(th),0,-np.sin(th),0],\n    [0,1,0,0],\n    [np.sin(th),0, np.cos(th),0],\n    [0,0,0,1]]).float()", "    [np.sin(th),0, np.cos(th),0],\n    [0,0,0,1]]).float()\n\n\ndef pose_spherical(theta, phi, radius, cam_pose):\n    c2w = trans_t(radius)\n    c2w = rot_phi(phi/180.*np.pi) @ c2w\n    c2w = rot_theta(theta/180.*np.pi) @ c2w\n    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n    device = c2w.device\n    c2w = torch.tensor(c2w).to(device)\n    return c2w", "\n\ndef read_pickle_file(path):\n    objects = []\n    with open(path, \"rb\") as fp:\n        while True:\n            try:\n                obj = pickle.load(fp)\n                objects.append(obj)\n\n            except EOFError:\n                break\n\n    return objects ", "\n\ndef load_dataset(directory, canonical_pose = None, input_pose = None):\n    cam_data_path = os.path.join(directory, \"cam_data.pkl\")\n    cam_data = read_pickle_file(cam_data_path)[0]\n    cams = {\"width\": 1280, \"height\": 720}\n\n    imgs = {}\n    image_dir = os.path.join(directory, \"render/\")\n    images = glob.glob(image_dir + \"**/*.png\", recursive = True)\n    images.sort()\n\n    depth_dir = os.path.join(directory, \"depth/\")\n\n    for i in range(len(images)):\n        image_current = images[i]\n        image_id = os.path.basename(image_current).split(\".\")[0]\n        image_parent_dir = image_current.split(\"/\")[-2]\n\n        cam = cam_data[image_id][\"K\"]\n        [cams[\"fx\"], cams[\"fy\"], cams[\"cx\"], cams[\"cy\"]] = cam\n        c2w = cam_data[image_id][\"extrinsics_opencv\"]\n        c2w = np.vstack([c2w, np.array([0, 0, 0, 1])])\n        c2w = np.linalg.inv(c2w)\n        pose = c2w\n\n        imgs[i] = {\n            \"camera_id\": image_id,\n            \"t\": pose[:3, 3].reshape(3, 1),\n            \"R\": pose[:3, :3],\n            \"path\": images[i],\n            \"pose\": pose\n        }\n\n        imgs[i][\"depth_path\"] = os.path.join(depth_dir, \"%s/%s_depth.npz\" % (image_parent_dir, image_id))\n\n    return imgs, cams", "\ndef main_loader(root_dir, scale, canonical_pose = None, input_pose = None):\n    imgs, cams = load_dataset(root_dir, canonical_pose, input_pose)\n\n    cams[\"fx\"] = fx = cams[\"fx\"] * scale\n    cams[\"fy\"] = fy = cams[\"fy\"] * scale\n    cams[\"cx\"] = cx = cams[\"cx\"] * scale\n    cams[\"cy\"] = cy = cams[\"cy\"] * scale\n\n    rand_key = random.choice(list(imgs))\n    test_img = cv2.imread(imgs[rand_key][\"path\"])\n    h, w = test_img.shape[:2]\n    cams[\"height\"] = round(h * scale)\n    cams[\"width\"] = round(w * scale)\n\n    cams[\"intrinsic_mat\"] = np.array([\n        [fx, 0, cx],\n        [0, -fy, cy],\n        [0, 0, -1]\n        ])\n\n    return imgs, cams ", "\n\ndef load_brics_data(basedir, res = 1, skip = 1, max_ind = 54, canonical_pose = None, input_pose = None, num_poses = 300):\n    imgs, cams = main_loader(basedir, res, canonical_pose, input_pose)\n    all_ids = []\n    all_imgs = []\n    all_poses = []\n    all_depths = []\n\n    for index in range(0, max_ind, skip):\n        all_ids.append(imgs[index][\"camera_id\"])\n\n        n_image = imageio.imread(imgs[index][\"path\"]) / 255.0\n        h, w = n_image.shape[:2]\n        resized_h = round(h * res)\n        resized_w = round(w * res)\n        n_image = cv2.resize(n_image, (resized_w, resized_h), interpolation=cv2.INTER_AREA)\n        all_imgs.append(n_image)\n\n        n_pose = imgs[index][\"pose\"]\n        all_poses.append(n_pose)\n        \n        n_depth = np.load(imgs[index][\"depth_path\"])['arr_0']\n        n_depth = np.where(n_depth == np.inf, 0, n_depth)\n        n_depth = np.where(n_depth > 100, 0, n_depth)\n        n_depth = cv2.resize(n_depth, (resized_w, resized_h), interpolation=cv2.INTER_AREA)\n        all_depths.append(n_depth)\n    \n    all_poses = np.array(all_poses)\n    all_imgs = np.array(all_imgs).astype(np.float32)\n    all_depths = np.array(all_depths).astype(np.float32)\n\n    c2w = all_poses[all_ids.index(\"left_5\"), :, :]\n    input_pose_4 = np.identity(4)\n    canonical_pose_4 = np.identity(4)\n    transform = False\n    canonical_poses = []\n\n    if input_pose is not None:\n        input_pose_4[:3, :3] = input_pose\n        transform = True\n\n    if canonical_pose is not None:\n        canonical_pose_4[:3, :3] = canonical_pose\n        transform = True\n\n    if transform:\n        t = np.array([0.0, -0.5, 4.5]).T  #known from the brics simulator\n        nerf_w_2_transform_w = np.identity(4)\n        nerf_w_2_transform_w[:3, -1] = -t\n        temp = nerf_w_2_transform_w @ c2w \n\n        for i in range(num_poses):\n            angle = np.linspace(0, 360, num_poses)[i]\n            circular_pose = pose_spherical(0.0, angle, 0.0, c2w).cpu().numpy() \n            pose = np.linalg.inv(canonical_pose_4 @ input_pose_4) @ np.linalg.inv(circular_pose) @ temp\n            # pose = np.linalg.inv(canonical_pose_4) @ np.linalg.inv(circular_pose) @ temp\n            pose[:3, -1] += t\n            canonical_poses.append(pose)\n\n        all_poses = np.array(canonical_poses)\n\n    i_val = []\n    sides = [\"back\", \"bottom\", \"front\", \"left\", \"right\", \"top\"]\n    for side_idx in range(len(sides)):\n        panel_idx = np.random.randint(1, 10) \n        val_camera_id = \"%s_%d\" % (sides[side_idx], panel_idx)\n        val_idx = all_ids.index(val_camera_id)\n        i_val.append(val_idx)\n\n    indices = np.arange(len(all_imgs))\n    i_train = np.array(list(set(indices).difference(set(i_val))))\n    i_test = i_val\n    i_split = [i_train, i_val, i_test]\n\n    render_poses = np.array(all_poses)\n\n    return all_imgs, all_poses, render_poses, cams, all_depths, i_split", "\n"]}
{"filename": "nerf/density_vis.py", "chunked_list": ["import numpy as np\nimport open3d as o3d\nimport argparse\nimport os\nfrom sklearn.cluster import KMeans\n\n\ndef cluster_sigmas(sigmas, n_clusters=2, power=1.0, exp=False, scale=1.0):\n    print(\"Number of clusters = \", n_clusters)\n    # dim, _, _ = sigmas.shape\n    # sigmas = sigmas.reshape((-1, 1))\n    # sigmas = sigmas + 1e2\n\n    print(\"Sigmas range = \", np.min(sigmas), np.max(sigmas))\n    relu_sigmas = np.where(sigmas > 0, sigmas, 0)\n    powered_sigmas = relu_sigmas ** power\n    print(\"Sigmas powered range = \", np.min(powered_sigmas), np.max(powered_sigmas))\n    if exp:\n        sigmas = 1. - np.exp(-scale * powered_sigmas)\n    print(\"Sigmas final range = \", np.min(sigmas), np.max(sigmas))\n\n    model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n    model.fit(sigmas)\n    print(\"Cluster centers = \", model.cluster_centers_)\n\n    labels = model.predict(sigmas)\n    (clusters, counts) = np.unique(labels, return_counts=True)\n    bg_label = clusters[np.where(counts == counts.max())[0]]\n    clustered_sigmas = np.where(labels == bg_label, 0, 1)\n    return clustered_sigmas", "# .reshape((dim, dim, dim))\n\n\ndef visualize(sigmas_path, samples_path, sigma_thresh):\n    sigmas = np.load(sigmas_path).reshape((-1, 1))\n    samples = np.load(samples_path).reshape((-1, 3))\n\n    # occ = np.where(sigmas > sigma_thresh)[0]\n    # print(\"Thresholding with %f: Total = %d, Occupied = %d, Occupancy = %f\" % (sigma_thresh, len(sigmas), len(occ), len(occ) / len(sigmas)))\n\n    # thresh_pcd = o3d.geometry.PointCloud()\n    # thresh_points = samples[np.where(sigmas > sigma_thresh)[0]]\n    # thresh_pcd.points = o3d.utility.Vector3dVector(thresh_points)\n    # o3d.visualization.draw_geometries([thresh_pcd])\n\n    clustered_sigmas = cluster_sigmas(sigmas, 2, 2.0, True, 0.3109375)\n    occ = np.where(clustered_sigmas != 0)[0]\n    print(\"Clustering: Total = %d, Occupied = %d, Occupancy = %f\" % (len(sigmas), len(occ), len(occ) / len(sigmas)))\n\n    fg_pcd = o3d.geometry.PointCloud()\n    fg_points = samples[np.where(clustered_sigmas != 0)[0]]\n    fg_pcd.points = o3d.utility.Vector3dVector(fg_points)\n    o3d.visualization.draw_geometries([fg_pcd])", "\n\nif __name__==\"__main__\":\n    parser = argparse.ArgumentParser(description=\"NeRF density field visualization\")\n    parser.add_argument(\"--input\", required=True, type = str)\n    parser.add_argument(\"--res\", default=32, type = int)\n    parser.add_argument(\"--sigma_thresh\", default=10.0, type = float)\n    parser.add_argument(\"--max_files\", default=10, type = int)\n    args = parser.parse_args()\n\n    count = 0\n    for path, dirs, files in os.walk(args.input):\n        for file in files:\n            if \"sigmas_%d.npy\" % (args.res) not in file:\n                continue\n\n            print(\"Processing %s %s\" % (path, file))\n            sigmas_path = os.path.join(path, file)\n            samples_path = os.path.join(path, file.replace(\"sigmas\", \"samples\"))\n            visualize(sigmas_path, samples_path, args.sigma_thresh)\n\n            count += 1\n            if count >= args.max_files:\n                break", ""]}
{"filename": "nerf/run_nerf_helpers.py", "chunked_list": ["import torch\n# torch.autograd.set_detect_anomaly(True)\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\n# Misc\nimg2mse = lambda x, y : torch.mean((x - y) ** 2)\nmse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))", "img2mse = lambda x, y : torch.mean((x - y) ** 2)\nmse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\nto8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n\n\n# Positional encoding (section 5.1)\nclass Embedder:\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.create_embedding_fn()\n        \n    def create_embedding_fn(self):\n        embed_fns = []\n        d = self.kwargs['input_dims']\n        out_dim = 0\n        if self.kwargs['include_input']:\n            embed_fns.append(lambda x : x)\n            out_dim += d\n            \n        max_freq = self.kwargs['max_freq_log2']\n        N_freqs = self.kwargs['num_freqs']\n        \n        if self.kwargs['log_sampling']:\n            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n        else:\n            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n            \n        for freq in freq_bands:\n            for p_fn in self.kwargs['periodic_fns']:\n                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n                out_dim += d\n                    \n        self.embed_fns = embed_fns\n        self.out_dim = out_dim\n        \n    def embed(self, inputs):\n        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)", "\n\ndef get_embedder(multires, i=0):\n    if i == -1:\n        return nn.Identity(), 3\n    \n    embed_kwargs = {\n                'include_input' : True,\n                'input_dims' : 3,\n                'max_freq_log2' : multires-1,\n                'num_freqs' : multires,\n                'log_sampling' : True,\n                'periodic_fns' : [torch.sin, torch.cos],\n    }\n    \n    embedder_obj = Embedder(**embed_kwargs)\n    embed = lambda x, eo=embedder_obj : eo.embed(x)\n    return embed, embedder_obj.out_dim", "\n\n# Model\nclass NeRF(nn.Module):\n    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False, semantic_en=False, num_classes=2):\n        \"\"\" \n        \"\"\"\n        super(NeRF, self).__init__()\n        self.D = D\n        self.W = W\n        self.input_ch = input_ch\n        self.input_ch_views = input_ch_views\n        self.skips = skips\n        self.use_viewdirs = use_viewdirs\n        self.semantic_en = semantic_en \n        self.C = num_classes\n        \n        self.pts_linears = nn.ModuleList(\n            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n        \n        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n\n        ### Implementation according to the paper\n        # self.views_linears = nn.ModuleList(\n        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n        \n        if use_viewdirs:\n            self.feature_linear = nn.Linear(W, W)\n            self.alpha_linear = nn.Linear(W, 1)\n            self.rgb_linear = nn.Linear(W//2, 3)\n\n            if semantic_en:\n                self.semantic = nn.Sequential(\n                        nn.Linear(W, W),\n                        nn.ReLU(True),\n                        nn.Linear(W, W//2),\n                        nn.ReLU(True),\n                        nn.Linear(W//2, self.C),\n                        #nn.Softmax(-1)\n                    )\n\n        else:\n            self.output_linear = nn.Linear(W, output_ch)\n\n    def query_density(self, input_pts):\n        h = input_pts\n        for i, l in enumerate(self.pts_linears):\n            h = self.pts_linears[i](h)\n            h = F.relu(h)\n            if i in self.skips:\n                h = torch.cat([input_pts, h], -1)\n\n        alpha = self.alpha_linear(h)\n        return alpha\n\n    def forward(self, x):\n        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n        h = input_pts\n        for i, l in enumerate(self.pts_linears):\n            h = self.pts_linears[i](h)\n            h = F.relu(h)\n            if i in self.skips:\n                h = torch.cat([input_pts, h], -1)\n\n        pts_embedding = h.clone()\n        if self.use_viewdirs:\n            alpha = self.alpha_linear(h)\n            feature = self.feature_linear(h)\n            h = torch.cat([feature, input_views], -1)\n        \n            for i, l in enumerate(self.views_linears):\n                h = self.views_linears[i](h)\n                h = F.relu(h)\n\n            rgb = self.rgb_linear(h)\n            outputs = torch.cat([rgb, alpha], -1)\n            \n            if self.semantic_en:\n               semantic_map = self.semantic(pts_embedding) \n               outputs = torch.cat([rgb, alpha, semantic_map], -1)\n\n        else:\n            outputs = self.output_linear(h)\n\n        return outputs    \n\n    def load_weights_from_keras(self, weights):\n        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n        \n        # Load pts_linears\n        for i in range(self.D):\n            idx_pts_linears = 2 * i\n            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))    \n            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n        \n        # Load feature_linear\n        idx_feature_linear = 2 * self.D\n        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n\n        # Load views_linears\n        idx_views_linears = 2 * self.D + 2\n        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n\n        # Load rgb_linear\n        idx_rbg_linear = 2 * self.D + 4\n        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n\n        # Load alpha_linear\n        idx_alpha_linear = 2 * self.D + 6\n        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))", "\n\n\n# Ray helpers\ndef get_rays(H, W, K, c2w):\n    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))  # pytorch's meshgrid has indexing='ij'\n    i = i.t()\n    j = j.t()\n    dirs = torch.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -torch.ones_like(i)/K[2][2]], -1)\n    # Rotate ray directions from camera frame to the world frame\n    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n    rays_o = c2w[:3,-1].expand(rays_d.shape)\n    return rays_o, rays_d", "\n\ndef get_rays_np(H, W, K, c2w):\n    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n    dirs = np.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -np.ones_like(i)], -1)\n    # Rotate ray directions from camera frame to the world frame\n    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))\n    return rays_o, rays_d", "\n\n# Hierarchical sampling (section 5.2)\ndef sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n    # Get pdf\n    weights = weights + 1e-5 # prevent nans\n    pdf = weights / torch.sum(weights, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)  # (batch, len(bins))\n\n    # Take uniform samples\n    if det:\n        u = torch.linspace(0., 1., steps=N_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n    else:\n        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n\n    # Pytest, overwrite u with numpy's fixed random numbers\n    if pytest:\n        np.random.seed(0)\n        new_shape = list(cdf.shape[:-1]) + [N_samples]\n        if det:\n            u = np.linspace(0., 1., N_samples)\n            u = np.broadcast_to(u, new_shape)\n        else:\n            u = np.random.rand(*new_shape)\n        u = torch.Tensor(u)\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds-1), inds-1)\n    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\n    # cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n    # bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[...,1]-cdf_g[...,0])\n    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n    t = (u-cdf_g[...,0])/denom\n    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n\n    return samples", ""]}
{"filename": "nerf/run_nerf.py", "chunked_list": ["import os, sys\nimport numpy as np\nimport imageio\nimport json\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm, trange", "import torch.nn.functional as F\nfrom tqdm import tqdm, trange\n\nimport matplotlib.pyplot as plt\n\nfrom run_nerf_helpers import *\n\nfrom load_brics import load_brics_data\n\nimport open3d as o3d", "\nimport open3d as o3d\nimport wandb\nimport gc\nimport copy\n\nimport cv2\nfrom PIL import Image\nimport mcubes\nfrom plyfile import PlyData, PlyElement", "import mcubes\nfrom plyfile import PlyData, PlyElement\nimport math\nfrom sklearn.cluster import KMeans\nimport h5py\nimport pickle\nfrom scipy.spatial.transform import Rotation as R\n\ndevice_idx = 0\ngc.collect()", "device_idx = 0\ngc.collect()\ntorch.cuda.empty_cache()\ndevice = torch.device(\"cuda:%d\" % (device_idx) if torch.cuda.is_available() else \"cpu\")\nDEBUG = False\n\n\ndef read_pickle_file(path):\n    objects = []\n    with open(path, \"rb\") as fp:\n        while True:\n            try:\n                obj = pickle.load(fp)\n                objects.append(obj)\n\n            except EOFError:\n                break\n\n    return objects", "\n\ndef load_models(path):\n    models = []\n    with open(path, \"r\") as f:\n        lines = f.readlines()\n        for line in lines:\n            model = os.path.basename(line[:-1])\n            model = model[:-15]\n            models.append(model) \n\n    return models", "\n\ndef load_h5(path):\n    fx_input = h5py.File(path, 'r')\n    x = fx_input['data'][:]\n    fx_input.close()\n    return x\n\n\ndef batchify(fn, chunk):\n    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n    \"\"\"\n    if chunk is None:\n        return fn\n    def ret(inputs):\n        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n    return ret", "\ndef batchify(fn, chunk):\n    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n    \"\"\"\n    if chunk is None:\n        return fn\n    def ret(inputs):\n        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n    return ret\n", "\n\ndef run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n    \"\"\"Prepares inputs and applies network 'fn'.\n    \"\"\"\n    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n    embedded = embed_fn(inputs_flat)\n\n    if viewdirs is not None:\n        input_dirs = viewdirs[:,None].expand(inputs.shape)\n        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n        embedded_dirs = embeddirs_fn(input_dirs_flat)\n        embedded = torch.cat([embedded, embedded_dirs], -1)\n\n    # embedded.requires_grad = True\n\n    outputs_flat = batchify(fn, netchunk)(embedded)\n    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n\n    # outputs_flat.backward()\n    return outputs", "\n\ndef batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n    \"\"\"Render rays in smaller minibatches to avoid OOM.\n    \"\"\"\n    all_ret = {}\n    for i in range(0, rays_flat.shape[0], chunk):\n        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n        for k in ret:\n            if k not in all_ret:\n                all_ret[k] = []\n            all_ret[k].append(ret[k])\n\n    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n    return all_ret", "\n\ndef render(H, W, K, chunk=1024*32, rays=None, c2w=None,\n                  near=0., far=1.,\n                  use_viewdirs=False, c2w_staticcam=None, gt_image=None, gt_depth=None,\n                  **kwargs):\n    \"\"\"Render rays\n    Args:\n      H: int. Height of image in pixels.\n      W: int. Width of image in pixels.\n      focal: float. Focal length of pinhole camera.\n      chunk: int. Maximum number of rays to process simultaneously. Used to\n        control maximum memory usage. Does not affect final results.\n      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n        each example in batch.\n      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n      near: float or array of shape [batch_size]. Nearest distance for a ray.\n      far: float or array of shape [batch_size]. Farthest distance for a ray.\n      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n       camera while using other c2w argument for viewing directions.\n    Returns:\n      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n      disp_map: [batch_size]. Disparity map. Inverse of depth.\n      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n      extras: dict with everything returned by render_rays().\n    \"\"\"\n    if c2w is not None:\n        # special case to render full image\n        c2w = torch.tensor(c2w).to(device)\n        rays_o, rays_d = get_rays(H, W, K, c2w)\n    else:\n        # use provided ray batch\n        rays_o, rays_d = rays\n\n    if use_viewdirs:\n        # provide ray directions as input\n        viewdirs = rays_d\n        if c2w_staticcam is not None:\n            # special case to visualize effect of viewdirs\n            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n\n    sh = rays_d.shape # [..., 3]\n\n    # Create ray batch\n    rays_o = torch.reshape(rays_o, [-1,3]).float()\n    rays_d = torch.reshape(rays_d, [-1,3]).float()\n\n    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n    rays = torch.cat([rays_o, rays_d, near, far], -1)\n    if use_viewdirs:\n        rays = torch.cat([rays, viewdirs], -1)\n\n    # Render and reshape\n    if gt_depth is not None:\n        gt_depth = torch.tensor(gt_depth).to(device).reshape((-1, 1))\n        points = rays_o + gt_depth * rays_d\n\n        all_ret = {}\n        all_ret['rgb_map'] = torch.tensor(gt_image).to(device)\n        all_ret['disp_map'] = torch.tensor([]).to(device)\n        all_ret['acc_map'] = torch.tensor([]).to(device)\n        all_ret['weights'] = torch.tensor([]).to(device)\n        all_ret['sigma_map'] = torch.tensor([]).to(device)\n        all_ret['sample_points'] = torch.tensor([]).to(device)\n        all_ret['depth_map'] = gt_depth\n        all_ret['points'] = points\n\n    else:\n        all_ret = batchify_rays(rays, chunk, **kwargs)\n        for k in all_ret:\n            k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n            all_ret[k] = torch.reshape(all_ret[k], k_sh)\n\n    all_ret['K'] = K\n    all_ret['c2w'] = c2w\n    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n    ret_list = [all_ret[k] for k in k_extract]\n    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n    return ret_list + [ret_dict]", "\n\ndef render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0, gt_depths=None, model=None, category=\"\"):\n\n    H, W, focal = hwf\n\n    if render_factor!=0:\n        # Render downsampled for speed\n        H = H//render_factor\n        W = W//render_factor\n        focal = focal/render_factor\n\n    rgbs = []\n    disps = []\n    depths = []\n    pcds = []\n    Ks = []\n    c2ws = []\n    weights = []\n    sigmas = []\n    sample_points = []\n\n    t = time.time()\n    for i, c2w in enumerate(tqdm(render_poses)):\n        print(i, time.time() - t)\n        t = time.time()\n        if gt_depths is not None:\n            rgb, disp, acc, extras = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], gt_image=gt_imgs[i], gt_depth=gt_depths[i], **render_kwargs)\n        else:\n            rgb, disp, acc, extras = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n        rgbs.append(rgb.detach().cpu().numpy())\n        disps.append(disp.detach().cpu().numpy())\n\n        if render_kwargs['retdepth']:\n            weights.append(extras['weights'].detach().cpu().numpy())\n            sigmas.append(extras['sigma_map'].detach().cpu().numpy())\n            sample_points.append(extras['sample_points'].detach().cpu().numpy())\n            depths.append(extras['depth_map'].detach().cpu().numpy())\n\n            points = extras['points'].detach().cpu().numpy().reshape(-1, 3)\n            pcd = o3d.geometry.PointCloud()\n            pcd.points = o3d.utility.Vector3dVector(points)\n            pcd.colors = o3d.utility.Vector3dVector(rgbs[-1].reshape(-1, 3))\n            pcds.append(pcd)\n\n            Ks.append(extras['K'])\n            c2ws.append(extras['c2w'].detach().cpu().numpy())\n\n        if i==0:\n            print(rgb.shape, disp.shape)\n\n        \"\"\"\n        if gt_imgs is not None and render_factor==0:\n            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n            print(p)\n        \"\"\"\n\n        if savedir is not None:\n            rgb8 = to8b(rgbs[-1])\n            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n\n            clustered_sigmas = cluster(extras['sigma_map'].detach().cpu().numpy(), 2)\n            samples = extras['sample_points'].detach().cpu().numpy()\n            occ_inds = np.where(clustered_sigmas > 0)\n            occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n            min_corner = np.array([np.min(occ_samples[:, 0]), np.min(occ_samples[:, 1]), np.min(occ_samples[:, 2])])\n            max_corner = np.array([np.max(occ_samples[:, 0]), np.max(occ_samples[:, 1]), np.max(occ_samples[:, 2])])\n\n            can_save_path = \"canonical_renderings/%s/%s_%d.png\" % (category, model, i)\n            if not os.path.exists(os.path.dirname(can_save_path)):\n                os.makedirs(os.path.dirname(can_save_path))\n\n            imageio.imwrite(can_save_path, rgb8)\n\n            if render_kwargs['retdepth']:\n                # weight8 = weights[-1]\n                # weights_filename = os.path.join(savedir, 'weights_{:03d}.npy'.format(i))\n                # np.save(weights_filename, weight8)\n                # sigma8 = sigmas[-1]\n                # sigmas_filename = os.path.join(savedir, 'sigmas_{:03d}.npy'.format(i))\n                # np.save(sigmas_filename, sigma8)\n                # sample8 = sample_points[-1]\n                # samples_filename = os.path.join(savedir, 'samples_{:03d}.npy'.format(i))\n                # np.save(samples_filename, sample8)\n\n                depth8 = depths[-1]\n                depth_filename = os.path.join(savedir, 'depth_{:03d}.npy'.format(i))\n                np.save(depth_filename, depth8)\n                pcd_filename = os.path.join(savedir, '{:03d}.ply'.format(i))\n                o3d.io.write_point_cloud(pcd_filename, pcds[-1])\n\n                c2w8 = c2ws[-1]\n                c2w_filename = os.path.join(savedir, 'c2w_{:03d}.npy'.format(i))\n                np.save(c2w_filename, c2w8)\n                K8 = Ks[-1]\n                K_filename = os.path.join(savedir, 'K_{:03d}.npy'.format(i))\n                np.save(K_filename, K8)\n\n        del rgb, disp, acc, extras\n        torch.cuda.empty_cache()\n\n    rgbs = np.stack(rgbs, 0)\n    disps = np.stack(disps, 0)\n    if render_kwargs['retdepth']:\n        depths = np.stack(depths, 0)\n\n    return rgbs, disps, depths", "\n\ndef create_nerf(args):\n    \"\"\"Instantiate NeRF's MLP model.\n    \"\"\"\n    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n\n    input_ch_views = 0\n    embeddirs_fn = None\n    if args.use_viewdirs:\n        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n    output_ch = 5 if args.N_importance > 0 else 4\n    skips = [4]\n    model = NeRF(D=args.netdepth, W=args.netwidth,\n                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n    grad_vars = list(model.parameters())\n\n    model_fine = None\n    if args.N_importance > 0:\n        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n        grad_vars += list(model_fine.parameters())\n\n    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n                                                                embed_fn=embed_fn,\n                                                                embeddirs_fn=embeddirs_fn,\n                                                                netchunk=args.netchunk\n                                                                )\n\n    # Create optimizer\n    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n\n    start = 0\n    basedir = args.basedir\n    expname = args.expname\n\n    ##########################\n\n    # Load checkpoints\n    if args.ft_path is not None and args.ft_path!='None':\n        ckpts = [args.ft_path]\n    else:\n        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n\n    print('Found ckpts', ckpts)\n    if len(ckpts) > 0 and not args.no_reload:\n        ckpt_path = ckpts[-1]\n        print('Reloading from', ckpt_path)\n        ckpt = torch.load(ckpt_path, map_location=device)\n\n        start = ckpt['global_step']\n        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n\n        # Load model\n        model.load_state_dict(ckpt['network_fn_state_dict'])\n        if model_fine is not None:\n            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n\n    ##########################\n\n    render_kwargs_train = {\n        'embed_fn': embed_fn, \n        'embeddirs_fn': embeddirs_fn,\n        'network_query_fn' : network_query_fn,\n        'perturb' : args.perturb,\n        'N_importance' : args.N_importance,\n        'network_fine' : model_fine,\n        'N_samples' : args.N_samples,\n        'network_fn' : model,\n        'use_viewdirs' : args.use_viewdirs,\n        'white_bkgd' : args.white_bkgd,\n        'raw_noise_std' : args.raw_noise_std,\n        'retdepth': True,\n    }\n\n    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n    render_kwargs_test['perturb'] = False\n    render_kwargs_test['raw_noise_std'] = 0.\n    render_kwargs_test['retdepth'] = True\n    render_kwargs_test['N_importance'] = args.N_importance // 2\n    render_kwargs_test['N_samples'] = args.N_samples // 2\n\n    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer", "\n\ndef raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n    \"\"\"Transforms model's predictions to semantically meaningful values.\n    Args:\n        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n        z_vals: [num_rays, num_samples along ray]. Integration time.\n        rays_d: [num_rays, 3]. Direction of each ray.\n    Returns:\n        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n        acc_map: [num_rays]. Sum of weights along each ray.\n        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n        depth_map: [num_rays]. Estimated distance to object.\n    \"\"\"\n    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n\n    dists = z_vals[...,1:] - z_vals[...,:-1]\n    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)  # [N_rays, N_samples]\n\n    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n\n    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n    noise = 0.\n    if raw_noise_std > 0.:\n        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n\n        # Overwrite randomly sampled data if pytest\n        if pytest:\n            np.random.seed(0)\n            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n            noise = torch.Tensor(noise)\n\n    alpha = raw2alpha(raw[...,3] + noise, dists)  # [N_rays, N_samples]\n    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n\n    depth_map = torch.sum(weights * z_vals, -1)\n    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n    acc_map = torch.sum(weights, -1)\n    sigma_map = raw[..., 3]\n\n    if white_bkgd:\n        rgb_map = rgb_map + (1.-acc_map[...,None])\n\n    return rgb_map, disp_map, acc_map, weights, depth_map, sigma_map", "\n\ndef render_rays(ray_batch,\n                embed_fn,\n                embeddirs_fn,\n                network_fn,\n                network_query_fn,\n                N_samples,\n                retraw=True,\n                retdepth=True,\n                lindisp=False,\n                perturb=0.,\n                N_importance=0,\n                network_fine=None,\n                white_bkgd=False,\n                raw_noise_std=0.,\n                verbose=False,\n                pytest=False):\n    \"\"\"Volumetric rendering.\n    Args:\n      ray_batch: array of shape [batch_size, ...]. All information necessary\n        for sampling along a ray, including: ray origin, ray direction, min\n        dist, max dist, and unit-magnitude viewing direction.\n      network_fn: function. Model for predicting RGB and density at each point\n        in space.\n      network_query_fn: function used for passing queries to network_fn.\n      N_samples: int. Number of different times to sample along each ray.\n      retraw: bool. If True, include model's raw, unprocessed predictions.\n      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n        random points in time.\n      N_importance: int. Number of additional times to sample along each ray.\n        These samples are only passed to network_fine.\n      network_fine: \"fine\" network with same spec as network_fn.\n      white_bkgd: bool. If True, assume a white background.\n      raw_noise_std: ...\n      verbose: bool. If True, print more debugging info.\n    Returns:\n      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n      disp_map: [num_rays]. Disparity map. 1 / depth.\n      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n      rgb0: See rgb_map. Output for coarse model.\n      disp0: See disp_map. Output for coarse model.\n      acc0: See acc_map. Output for coarse model.\n      z_std: [num_rays]. Standard deviation of distances along ray for each\n        sample.\n    \"\"\"\n    N_rays = ray_batch.shape[0]\n    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n\n    t_vals = torch.linspace(0., 1., steps=N_samples)\n    near = near.to(device)\n    far = far.to(device)\n    if not lindisp:\n        z_vals = near * (1.-t_vals) + far * (t_vals)\n    else:\n        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n\n    z_vals = z_vals.expand([N_rays, N_samples])\n\n    if perturb > 0.:\n        # get intervals between samples\n        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n        lower = torch.cat([z_vals[...,:1], mids], -1)\n        # stratified samples in those intervals\n        t_rand = torch.rand(z_vals.shape)\n\n        # Pytest, overwrite u with numpy's fixed random numbers\n        if pytest:\n            np.random.seed(0)\n            t_rand = np.random.rand(*list(z_vals.shape))\n            t_rand = torch.Tensor(t_rand)\n\n        z_vals = lower + (upper - lower) * t_rand\n\n    rays_o = rays_o.to(device)\n    rays_d = rays_d.to(device)\n    viewdirs = viewdirs.to(device)\n    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n\n\n#     raw = run_network(pts)\n    raw = network_query_fn(pts, viewdirs, network_fn)\n    rgb_map, disp_map, acc_map, weights, depth_map, sigma_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n    points = rays_o + depth_map.unsqueeze(1) * rays_d\n\n    if N_importance > 0:\n\n        rgb_map_0, disp_map_0, acc_map_0, weights_0, depth_map_0, sigma_map_0, raw_0, points_0 = rgb_map, disp_map, acc_map, weights, depth_map, sigma_map, raw, points\n\n        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n        z_samples = z_samples.detach()\n\n        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n\n        run_fn = network_fn if network_fine is None else network_fine\n#         raw = run_network(pts, fn=run_fn)\n        raw = network_query_fn(pts, viewdirs, run_fn)\n\n        rgb_map, disp_map, acc_map, weights, depth_map, sigma_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n        points = rays_o + depth_map.unsqueeze(1) * rays_d\n\n    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n    if retraw:\n        ret['raw'] = raw\n    if retdepth:\n        ret['weights'] = weights \n        ret['sigma_map'] = sigma_map\n        ret['sample_points'] = pts\n        ret['depth_map'] = depth_map\n        ret['points'] = points\n\n    if N_importance > 0:\n        ret['rgb0'] = rgb_map_0\n        ret['disp0'] = disp_map_0\n        ret['acc0'] = acc_map_0\n        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n\n        if retraw:\n            ret['raw0'] = raw_0\n        if retdepth:\n            ret['weights0'] = weights_0\n            ret['sigma0'] = sigma_map_0\n            ret['depth0'] = depth_map_0\n            ret['points0'] = points_0\n\n    for k in ret:\n        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n\n    return ret", "\n\ndef config_parser():\n\n    import configargparse\n    parser = configargparse.ArgumentParser()\n    parser.add_argument('--config', is_config_file=True, \n                        help='config file path')\n    parser.add_argument(\"--expname\", type=str, \n                        help='experiment name')\n    parser.add_argument(\"--basedir\", type=str, default='./logs/', \n                        help='where to store ckpts and logs')\n    parser.add_argument(\"--datadir\", type=str, default='./data/brics_renderings/chair', \n                        help='input data directory')\n\n    # training options\n    parser.add_argument(\"--netdepth\", type=int, default=8, \n                        help='layers in network')\n    parser.add_argument(\"--netwidth\", type=int, default=256, \n                        help='channels per layer')\n    parser.add_argument(\"--netdepth_fine\", type=int, default=8, \n                        help='layers in fine network')\n    parser.add_argument(\"--netwidth_fine\", type=int, default=256, \n                        help='channels per layer in fine network')\n    parser.add_argument(\"--N_rand\", type=int, default=32*32*4, \n                        help='batch size (number of random rays per gradient step)')\n    parser.add_argument(\"--lrate\", type=float, default=5e-4, \n                        help='learning rate')\n    parser.add_argument(\"--lrate_decay\", type=int, default=250, \n                        help='exponential learning rate decay (in 1000 steps)')\n    parser.add_argument(\"--chunk\", type=int, default=1024*32, \n                        help='number of rays processed in parallel, decrease if running out of memory')\n    parser.add_argument(\"--netchunk\", type=int, default=1024*64, \n                        help='number of pts sent through network in parallel, decrease if running out of memory')\n    parser.add_argument(\"--no_batching\", action='store_true', \n                        help='only take random rays from 1 image at a time')\n    parser.add_argument(\"--no_reload\", action='store_true', \n                        help='do not reload weights from saved ckpt')\n    parser.add_argument(\"--ft_path\", type=str, default=None, \n                        help='specific weights npy file to reload for coarse network')\n\n    # loss weights\n    parser.add_argument(\"--loss_param\", type=float, default=1,\n                        help='exponential term parameter for depth weighting')\n\n    # rendering options\n    parser.add_argument(\"--N_samples\", type=int, default=64, \n                        help='number of coarse samples per ray')\n    parser.add_argument(\"--N_importance\", type=int, default=0,\n                        help='number of additional fine samples per ray')\n    parser.add_argument(\"--near\", type=float, default=0.,\n                        help='closest point to sample during ray rendering')\n    parser.add_argument(\"--far\", type=float, default=1.,\n                        help='farthest point to sample during ray rendering')\n    parser.add_argument(\"--perturb\", type=float, default=1.,\n                        help='set to 0. for no jitter, 1. for jitter')\n    parser.add_argument(\"--use_viewdirs\", action='store_true', \n                        help='use full 5D input instead of 3D')\n    parser.add_argument(\"--i_embed\", type=int, default=0, \n                        help='set 0 for default positional encoding, -1 for none')\n    parser.add_argument(\"--multires\", type=int, default=10, \n                        help='log2 of max freq for positional encoding (3D location)')\n    parser.add_argument(\"--multires_views\", type=int, default=4, \n                        help='log2 of max freq for positional encoding (2D direction)')\n    parser.add_argument(\"--raw_noise_std\", type=float, default=0., \n                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n\n    parser.add_argument(\"--multi_scene\", action='store_true', \n                        help='render multiple scenes')\n    parser.add_argument(\"--root_dir\", type=str, default='./logs/', \n                        help='path to directory containing all the scenes to be rendered')\n    parser.add_argument(\"--render_only\", action='store_true', \n                        help='do not optimize, reload weights and render out render_poses path')\n    parser.add_argument(\"--render_test\", action='store_true', \n                        help='render the test set instead of render_poses path')\n    parser.add_argument(\"--render_factor\", type=int, default=0, \n                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n    parser.add_argument(\"--canonical_path\", type=str, default=None, \n                        help='canonical data directory')\n    parser.add_argument(\"--num_render_poses\", type=int, default=300, \n                        help='number of poses to render from')\n    parser.add_argument(\"--model_name\", type=str, default=\"\", \n                        help='model name')\n    parser.add_argument(\"--category\", type=str, default=\"\", \n                        help='category name')\n    parser.add_argument(\"--gen_sigmas\", action='store_true', \n                        help='extract the sigmas, i.e., the density field for the model')\n\n    # training options\n    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n                        help='number of steps to train on central crops')\n    parser.add_argument(\"--precrop_frac\", type=float,\n                        default=.5, help='fraction of img taken for central crops') \n    parser.add_argument(\"--iters\", type=int, default=10000,\n                        help='number of steps to train for')\n\n    # dataset options\n    parser.add_argument(\"--dataset_type\", type=str, default='blender', \n                        help='options: brics')\n    parser.add_argument(\"--testskip\", type=int, default=8, \n                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n    parser.add_argument(\"--max_ind\", type=int, default=100,\n                        help='max index used in loader')\n\n    # sigma mesh flags\n    parser.add_argument('--x_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n                        help='x range of the object')\n    parser.add_argument('--y_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n                        help='x range of the object')\n    parser.add_argument('--z_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n                        help='x range of the object')\n    parser.add_argument('--sigma_threshold', type=float, default=20.0,\n                        help='threshold to consider a location is occupied')\n\n    ## blender flags\n    parser.add_argument(\"--white_bkgd\", action='store_true', \n                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n    parser.add_argument(\"--res\", type=float, default=1.0,\n                        help='load blender synthetic data at given resolution instead of 800x800')\n\n    # logging/saving options\n    parser.add_argument(\"--wand_en\", action='store_true',  \n                        help='wandb logging enabled')\n    parser.add_argument(\"--i_print\",   type=int, default=100, \n                        help='frequency of console printout and metric loggin')\n    parser.add_argument(\"--i_img\",     type=int, default=100, \n                        help='frequency of tensorboard image logging')\n    parser.add_argument(\"--i_weights\", type=int, default=10000, \n                        help='frequency of weight ckpt saving')\n    parser.add_argument(\"--i_testset\", type=int, default=5000000, \n                        help='frequency of testset saving')\n    parser.add_argument(\"--i_video\",   type=int, default=5000000, \n                        help='frequency of render_poses video saving')\n\n    return parser", "\n\ndef get_max_cube(minCorner, maxCorner):\n    minPt, maxPt = copy.deepcopy(minCorner), copy.deepcopy(maxCorner)\n    diagLen = math.dist(minPt, maxPt)\n\n    for i in range(len(minPt)):\n        midPt = (minPt[i] + maxPt[i]) / 2\n        minPt[i] = midPt - diagLen / 2\n        maxPt[i] = midPt + diagLen / 2\n\n    return minPt, maxPt", "\n\ndef get_coords(minCoord, maxCoord, sampleCtr=128):\n    xdists = np.linspace(minCoord[0], maxCoord[0], sampleCtr)\n    ydists = np.linspace(minCoord[1], maxCoord[1], sampleCtr)\n    zdists = np.linspace(minCoord[2], maxCoord[2], sampleCtr)\n\n    coords = np.stack(np.meshgrid(xdists, ydists, zdists, indexing='ij'), axis=-1).astype(np.float32)\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(coords.reshape((-1, 3)))\n    return pcd, coords", "\n\ndef cluster(sigmas, n_clusters=2, power=2.0, scale=1.0):\n    print(\"Number of clusters = \", n_clusters)\n    dim1, dim2, dim3 = sigmas.shape\n    sigmas = sigmas.reshape((-1, 1))\n    #sigmas = sigmas + 1e2\n\n    relu_sigmas = np.where(sigmas > 0, sigmas, 0)\n    powered_sigmas = relu_sigmas ** power\n    print(\"Sigmas powered range = \", np.min(powered_sigmas), np.max(powered_sigmas))\n    sigmas = 1. - np.exp(-scale * powered_sigmas)\n    print(\"Sigmas final range = \", np.min(sigmas), np.max(sigmas))\n\n    # model = GaussianMixture(n_components=2,init_params=\"k-means++\",weights_init=[0.9,0.1])\n    model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n    \n    model.fit(sigmas)\n    labels = model.predict(sigmas)\n    (clusters, counts) = np.unique(labels, return_counts=True)\n    fg_label = clusters[np.where(counts == counts.min())[0]]\n    clustered_sigmas = np.where(labels == fg_label, 1, 0)\n    return clustered_sigmas.reshape((dim1, dim2, dim3))", "\n\ndef plot_sigmas(sigmas, save_path, plot_file_name):\n    sigma_hist_vals = sigmas.astype(int).reshape(-1)\n    plt.figure()\n    plt.hist(sigma_hist_vals)\n    plt.show()\n\n    fig_file_path = os.path.join(save_path, plot_file_name)\n    plt.savefig(fig_file_path)", "\n\ndef translate_obj(pts):\n    mean = np.mean(pts, axis=0)\n    pts = pts - mean\n    return pts\n\n\ndef extract_sigmas(N_samples, x_range, y_range, z_range, sigma_threshold, network_query_fn, network_fn, min_b, max_b, save_path, kwargs, use_vertex_normal = True, near_t = 1.0):\n    # define the dense grid for query\n    N = N_samples\n    xmin, xmax = x_range\n    ymin, ymax = y_range\n    zmin, zmax = z_range\n    center = np.array([0, -0.5, 4.5])\n    # assert xmax-xmin == ymax-ymin == zmax-zmin, 'the ranges must have the same length!'\n    x = np.linspace(xmin, xmax, N)\n    y = np.linspace(ymin, ymax, N)\n    z = np.linspace(zmin, zmax, N)\n    samples = np.stack(np.meshgrid(x, y, z), -1)\n\n    xyz_ = torch.FloatTensor(samples.reshape(N ** 2, N, 3)).cuda()\n    # sigma is independent of direction, so any value here will produce the same result \n    dir_ = torch.zeros(N ** 2, 3).cuda()\n    # predict sigma (occupancy) for each grid location\n    print('Predicting occupancy ...')\n    with torch.no_grad():\n        raw = network_query_fn(xyz_, dir_, network_fn)\n\n    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n    clustered_sigma = cluster(sigma, 2)\n    # occ_inds = np.where(sigma > sigma_threshold)\n    occ_inds = np.where(clustered_sigma > 0)\n    occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n\n    min_corner = np.array([np.min(occ_samples[..., 0]), np.min(occ_samples[..., 1]), np.min(occ_samples[..., 2])])\n    max_corner = np.array([np.max(occ_samples[..., 0]), np.max(occ_samples[..., 1]), np.max(occ_samples[..., 2])])\n    min_pt, max_pt = get_max_cube(min_corner, max_corner)\n    box_pcd, coords = get_coords(min_pt, max_pt, N)\n\n    xyz_ = torch.FloatTensor(coords.reshape(N ** 2, N, 3)).cuda()\n    # sigma is independent of direction, so any value here will produce the same result\n    dir_ = torch.zeros(N ** 2, 3).cuda()\n    # predict sigma (occupancy) for each grid location\n    print('Predicting occupancy for resized cube...')\n    with torch.no_grad():\n        raw = network_query_fn(xyz_, dir_, network_fn)\n\n    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n    # plot_sigmas(sigma, save_path, 'resampled_sigmas.png')\n    sigmas_filename = os.path.join(save_path, 'sigmas_%d.npy' % (N))\n    np.save(sigmas_filename, sigma)\n\n    samples = coords.reshape((-1, 3))\n    samples = translate_obj(samples)\n    min_corner = np.array([np.min(samples[:, 0]), np.min(samples[:, 1]), np.min(samples[:, 2])])\n    max_corner = np.array([np.max(samples[:, 0]), np.max(samples[:, 1]), np.max(samples[:, 2])])\n    abs_max = np.max(np.vstack([np.abs(min_corner), np.abs(max_corner)]), axis=0)\n    samples = samples / abs_max\n    samples = samples.reshape((N, N, N, 3))\n    samples_filename = os.path.join(save_path, 'samples_%d.npy' % (N))\n    np.save(samples_filename, samples)\n\n    # print(\"Hi, visualizing now!\")\n    # clustered_sigma = cluster(sigma, 2)\n    # occ_inds = np.where(clustered_sigma > 0)\n    # occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n    # pcd = o3d.geometry.PointCloud()\n    # pcd.points = o3d.utility.Vector3dVector(occ_samples.reshape(-1, 3))\n    # o3d.visualization.draw_geometries([pcd])\n    print('Done!')", "def extract_sigmas(N_samples, x_range, y_range, z_range, sigma_threshold, network_query_fn, network_fn, min_b, max_b, save_path, kwargs, use_vertex_normal = True, near_t = 1.0):\n    # define the dense grid for query\n    N = N_samples\n    xmin, xmax = x_range\n    ymin, ymax = y_range\n    zmin, zmax = z_range\n    center = np.array([0, -0.5, 4.5])\n    # assert xmax-xmin == ymax-ymin == zmax-zmin, 'the ranges must have the same length!'\n    x = np.linspace(xmin, xmax, N)\n    y = np.linspace(ymin, ymax, N)\n    z = np.linspace(zmin, zmax, N)\n    samples = np.stack(np.meshgrid(x, y, z), -1)\n\n    xyz_ = torch.FloatTensor(samples.reshape(N ** 2, N, 3)).cuda()\n    # sigma is independent of direction, so any value here will produce the same result \n    dir_ = torch.zeros(N ** 2, 3).cuda()\n    # predict sigma (occupancy) for each grid location\n    print('Predicting occupancy ...')\n    with torch.no_grad():\n        raw = network_query_fn(xyz_, dir_, network_fn)\n\n    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n    clustered_sigma = cluster(sigma, 2)\n    # occ_inds = np.where(sigma > sigma_threshold)\n    occ_inds = np.where(clustered_sigma > 0)\n    occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n\n    min_corner = np.array([np.min(occ_samples[..., 0]), np.min(occ_samples[..., 1]), np.min(occ_samples[..., 2])])\n    max_corner = np.array([np.max(occ_samples[..., 0]), np.max(occ_samples[..., 1]), np.max(occ_samples[..., 2])])\n    min_pt, max_pt = get_max_cube(min_corner, max_corner)\n    box_pcd, coords = get_coords(min_pt, max_pt, N)\n\n    xyz_ = torch.FloatTensor(coords.reshape(N ** 2, N, 3)).cuda()\n    # sigma is independent of direction, so any value here will produce the same result\n    dir_ = torch.zeros(N ** 2, 3).cuda()\n    # predict sigma (occupancy) for each grid location\n    print('Predicting occupancy for resized cube...')\n    with torch.no_grad():\n        raw = network_query_fn(xyz_, dir_, network_fn)\n\n    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n    # plot_sigmas(sigma, save_path, 'resampled_sigmas.png')\n    sigmas_filename = os.path.join(save_path, 'sigmas_%d.npy' % (N))\n    np.save(sigmas_filename, sigma)\n\n    samples = coords.reshape((-1, 3))\n    samples = translate_obj(samples)\n    min_corner = np.array([np.min(samples[:, 0]), np.min(samples[:, 1]), np.min(samples[:, 2])])\n    max_corner = np.array([np.max(samples[:, 0]), np.max(samples[:, 1]), np.max(samples[:, 2])])\n    abs_max = np.max(np.vstack([np.abs(min_corner), np.abs(max_corner)]), axis=0)\n    samples = samples / abs_max\n    samples = samples.reshape((N, N, N, 3))\n    samples_filename = os.path.join(save_path, 'samples_%d.npy' % (N))\n    np.save(samples_filename, samples)\n\n    # print(\"Hi, visualizing now!\")\n    # clustered_sigma = cluster(sigma, 2)\n    # occ_inds = np.where(clustered_sigma > 0)\n    # occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n    # pcd = o3d.geometry.PointCloud()\n    # pcd.points = o3d.utility.Vector3dVector(occ_samples.reshape(-1, 3))\n    # o3d.visualization.draw_geometries([pcd])\n    print('Done!')", "\n\ndef train(args):\n\n    # Load data\n    K = None\n\n    if args.dataset_type == 'brics':\n        canonical_pose = None\n        input_pose = None\n\n        if args.canonical_path is not None:\n            input_poses_path = os.path.join(args.canonical_path, \"%s_input_rot.h5\" % (args.category)) \n            canonical_poses_path = os.path.join(args.canonical_path, \"%s_canonical.h5\" % (args.category)) \n            canonical_models_path = os.path.join(args.canonical_path, \"%s_files.txt\" % (args.category))\n\n            input_poses = load_h5(input_poses_path)\n            canonical_poses = load_h5(canonical_poses_path)\n            canonical_models = load_models(canonical_models_path)\n\n            if args.model_name not in canonical_models:\n                print(\"%s not found in canonical data\" % (args.model_name))\n                return\n\n            input_pose = input_poses[canonical_models.index(args.model_name)]\n            canonical_pose = canonical_poses[canonical_models.index(args.model_name)]\n\n        images, poses, render_poses, meta, gt_depths, i_split = load_brics_data(args.datadir, args.res, args.testskip, args.max_ind, canonical_pose, input_pose, args.num_render_poses)\n        K = meta['intrinsic_mat']\n        hwf = [meta['height'], meta['width'], meta['fx']]\n        print('Loaded brics', images.shape, poses.shape, render_poses.shape, K, hwf, args.datadir)\n        i_train, i_val, i_test = i_split\n\n        near = args.near\n        far = args.far\n\n        if args.white_bkgd:\n            images = images[..., :3] * images[..., -1:] + (1. - images[..., -1:])\n        else:\n            images = images[..., :3]\n\n    else:\n        print('Unknown dataset type', args.dataset_type, 'exiting')\n        return\n\n    # Cast intrinsics to right types\n    H, W, focal = hwf\n    H, W = int(H), int(W)\n    hwf = [H, W, focal]\n\n    if K is None:\n        K = np.array([\n            [focal, 0, 0.5*W],\n            [0, focal, 0.5*H],\n            [0, 0, 1]\n        ])\n\n    # if args.render_test:\n        # render_poses = np.array(poses[i_test])\n\n    # Create log dir and copy the config file\n    basedir = args.basedir\n    expname = args.expname\n    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n    f = os.path.join(basedir, expname, 'args.txt')\n    with open(f, 'w') as file:\n        for arg in sorted(vars(args)):\n            attr = getattr(args, arg)\n            file.write('{} = {}\\n'.format(arg, attr))\n    if args.config is not None:\n        f = os.path.join(basedir, expname, 'config.txt')\n        with open(f, 'w') as file:\n            file.write(open(args.config, 'r').read())\n\n    # Create nerf model\n    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n    global_step = start\n\n    bds_dict = {\n        'near' : near,\n        'far' : far,\n    }\n    render_kwargs_train.update(bds_dict)\n    render_kwargs_test.update(bds_dict)\n\n    # Move testing data to GPU\n    # render_poses = torch.Tensor(render_poses).to(device)\n\n    # Short circuit if only rendering out from trained model\n    if args.render_only:\n        print('RENDER ONLY')\n        if args.render_test:\n            # render_test switches to test poses\n            images = images\n        else:\n            # Default is smoother render_poses path\n            images = None\n\n        testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start))\n        os.makedirs(testsavedir, exist_ok=True)\n        print('test poses shape', render_poses.shape)\n\n        if args.canonical_path is not None:\n            with torch.no_grad():\n                rgbs, disps, depths = render_path(poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor, model=args.model_name, category=args.category)\n        elif args.render_test:\n            with torch.no_grad():\n                rgbs, disps, depths = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor, model=args.model_name)\n        elif args.gen_sigmas:\n            extract_sigmas(args.N_samples, args.x_range, args.y_range, args.z_range, args.sigma_threshold, render_kwargs_train['network_query_fn'], render_kwargs_train['network_fn'], near, far, testsavedir, render_kwargs_test)\n\n        print('Done rendering', testsavedir)\n        return\n\n    if args.wand_en:\n            wandb.init(project=\"NeRF\",\n                       entity=\"rrc_3d\",\n                       name=expname)\n\n    # Prepare raybatch tensor if batching random rays\n    N_rand = args.N_rand\n    use_batching = not args.no_batching\n    if use_batching:\n        # For random ray batching\n        print('get rays')\n        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n        print('done, concats')\n        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n        rays_rgb = rays_rgb.astype(np.float32)\n\n        if N_rand:\n            print('shuffle rays')\n            np.random.shuffle(rays_rgb)\n\n        print('done')\n        i_batch = 0\n\n    # Move training data to GPU\n    if use_batching:\n        images = torch.Tensor(images).to(device)\n    poses = torch.Tensor(poses).to(device)\n    if use_batching:\n        rays_rgb = torch.Tensor(rays_rgb).to(device)\n\n\n    N_iters = args.iters + 1\n    print('Begin')\n    print('TRAIN views are', i_train)\n    print('TEST views are', i_test)\n    print('VAL views are', i_val)\n\n    # Summary writers\n    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))\n    \n    if not N_rand:\n        N_rand = H * W\n\n    start = start + 1\n    for i in trange(start, N_iters):\n        time0 = time.time()\n\n        # Sample random ray batch\n        if use_batching:\n            # Random over all images\n            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n            batch = torch.transpose(batch, 0, 1)\n            batch_rays, target_s = batch[:2], batch[2]\n\n            i_batch += N_rand\n            if i_batch >= rays_rgb.shape[0]:\n                # print(\"Shuffle data after an epoch!\")\n                # rand_idx = torch.randperm(rays_rgb.shape[0])\n                # rays_rgb = rays_rgb[rand_idx]\n                i_batch = 0\n\n        else:\n            # Random from one image\n            # img_i = np.random.choice(i_train)\n            img_i = i % len(i_train)\n            target = images[img_i]\n            target = torch.Tensor(target).to(device)\n            pose = poses[img_i, :3,:4]\n\n            if not (i % len(i_train)) and (i / len(i_train) > 0):\n                print(\"Completed %d epochs!\" % (i // len(i_train)))\n\n            if N_rand is not None:\n                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n\n                if i < args.precrop_iters:\n                    dH = int(H//2 * args.precrop_frac)\n                    dW = int(W//2 * args.precrop_frac)\n                    coords = torch.stack(\n                        torch.meshgrid(\n                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n                        ), -1)\n                    if i == start:\n                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n                else:\n                    coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n\n                coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n                select_coords = coords[select_inds].long()  # (N_rand, 2)\n                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n                batch_rays = torch.stack([rays_o, rays_d], 0)\n                # target_s = target.reshape((H * W, 3))\n                target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n\n        #####  Core optimization loop  #####\n        rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,\n                                                verbose=i < 10, retraw=True,\n                                                **render_kwargs_train)\n\n        optimizer.zero_grad()\n        depths = extras['depth_map'].detach().cpu()\n        weights = torch.exp(-args.loss_param * depths).to(device)\n        weights = weights.unsqueeze(-1).repeat(1, 3)\n        img_loss = img2mse(rgb, target_s)\n        trans = extras['raw'][...,-1]\n        loss = img_loss\n        psnr = mse2psnr(img_loss)\n\n        if 'rgb0' in extras:\n            depths = extras['depth0'].detach().cpu()\n            weights = torch.exp(-args.loss_param * depths).to(device)\n            weights = weights.unsqueeze(-1).repeat(1, 3)\n            img_loss0 = img2mse(extras['rgb0'], target_s)\n            loss = loss + img_loss0\n            psnr0 = mse2psnr(img_loss0)\n\n        loss.backward()\n        optimizer.step()\n\n        # NOTE: IMPORTANT!\n        ###   update learning rate   ###\n        decay_rate = 0.1\n        decay_steps = args.lrate_decay * 1000\n        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lrate\n        ################################\n\n        dt = time.time()-time0\n        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n        #####           end            #####\n\n        # Rest is logging\n        # print(\"Training rgb info: \", type(rgb), rgb.shape, target_s.shape)\n        # print(np.unique(rgb.detach().cpu().numpy(), return_counts=True))\n        # print(np.unique(target_s.detach().cpu().numpy(), return_counts=True))\n        # tqdm.write(f\"Info: {rgb.shape}, {target_s.shape}, \\nUnique GT Values: {np.unique(target_s.detach().cpu().numpy(), return_counts=True)} \\nUnique Rendered Values: {np.unique(rgb.detach().cpu().numpy(), return_counts=True)}\")\n        if args.wand_en:\n            render_H = int(N_rand ** 0.5)\n            render_W = int(N_rand ** 0.5)\n            if N_rand == H * W:\n                render_H = H\n                render_W = W\n\n            log_dict = {\n                \"Train Loss\": loss.item(),\n                \"Train PSNR\": psnr.item(),\n                \"Rendered vs GT Train Image\": [wandb.Image(rgb.detach().cpu().numpy().reshape((render_H, render_W, 3))), wandb.Image(target_s.detach().cpu().numpy().reshape((render_H, render_W, 3)))]\n                }\n\n            wandb.log(log_dict)\n\n        if i%args.i_weights==0:\n            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n            torch.save({\n                'global_step': global_step,\n                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            }, path)\n            print('Saved checkpoints at', path)\n\n        if i%args.i_video==0 and i > 0:\n            # Turn on testing mode\n            with torch.no_grad():\n                rgbs, disps, depths = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n            print('Done, saving', rgbs.shape, disps.shape)\n            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n\n            # if args.use_viewdirs:\n            #     render_kwargs_test['c2w_staticcam'] = render_poses[0][:3,:4]\n            #     with torch.no_grad():\n            #         rgbs_still, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test)\n            #     render_kwargs_test['c2w_staticcam'] = None\n            #     imageio.mimwrite(moviebase + 'rgb_still.mp4', to8b(rgbs_still), fps=30, quality=8)\n\n        if i%args.i_testset==0 and i > 0:\n            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n            os.makedirs(testsavedir, exist_ok=True)\n            print('test poses shape', poses[i_test].shape)\n            with torch.no_grad():\n                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n            print('Saved test set')\n\n\n    \n        if i%args.i_print==0:\n            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n\n            if i%args.i_img==0:\n                # Log a rendered validation view\n                img_i=np.random.choice(i_val)\n                target = images[img_i]\n                pose = poses[img_i, :3,:4]\n\n                with torch.no_grad():\n                    rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, c2w=pose,\n                                                        **render_kwargs_test)\n\n                img_loss = img2mse(rgb, torch.tensor(target))\n                loss = img_loss\n                psnr = mse2psnr(img_loss)\n\n                if 'rgb0' in extras:\n                    img_loss0 = img2mse(extras['rgb0'], torch.tensor(target))\n                    loss = loss + img_loss0\n                    psnr0 = mse2psnr(img_loss0)\n                    \n                tqdm.write(f\"[TRAIN] Iter: {i} Validation Loss: {loss.item()}  Validation PSNR: {psnr.item()}\")\n                if args.wand_en:\n                    log_dict = {\n                        \"Validation Loss\": loss.item(),\n                        \"Validation PSNR\": psnr.item(),\n                        \"Rendered vs GT Image\": [wandb.Image(rgb.detach().cpu().numpy().reshape((H, W, 3))), wandb.Image(target)],\n                        \"Disparity\": [wandb.Image(disp.detach().cpu().numpy())],\n                        \"Opacity\": [wandb.Image(acc.detach().cpu().numpy())],\n                        \"Depth\": [wandb.Image(extras['depth_map'].detach().cpu().numpy())],\n                        }\n\n                    wandb.log(log_dict)\n\n        \"\"\"\n            print(expname, i, psnr.numpy(), loss.numpy(), global_step.numpy())\n            print('iter time {:.05f}'.format(dt))\n\n            with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_print):\n                tf.contrib.summary.scalar('loss', loss)\n                tf.contrib.summary.scalar('psnr', psnr)\n                tf.contrib.summary.histogram('tran', trans)\n                if args.N_importance > 0:\n                    tf.contrib.summary.scalar('psnr0', psnr0)\n\n\n            if i%args.i_img==0:\n\n                # Log a rendered validation view to Tensorboard\n                img_i=np.random.choice(i_val)\n                target = images[img_i]\n                pose = poses[img_i, :3,:4]\n                with torch.no_grad():\n                    rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose,\n                                                        **render_kwargs_test)\n\n                psnr = mse2psnr(img2mse(rgb, target))\n\n                with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n\n                    tf.contrib.summary.image('rgb', to8b(rgb)[tf.newaxis])\n                    tf.contrib.summary.image('disp', disp[tf.newaxis,...,tf.newaxis])\n                    tf.contrib.summary.image('acc', acc[tf.newaxis,...,tf.newaxis])\n\n                    tf.contrib.summary.scalar('psnr_holdout', psnr)\n                    tf.contrib.summary.image('rgb_holdout', target[tf.newaxis])\n\n\n                if args.N_importance > 0:\n\n                    with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n                        tf.contrib.summary.image('rgb0', to8b(extras['rgb0'])[tf.newaxis])\n                        tf.contrib.summary.image('disp0', extras['disp0'][tf.newaxis,...,tf.newaxis])\n                        tf.contrib.summary.image('z_std', extras['z_std'][tf.newaxis,...,tf.newaxis])\n        \"\"\"\n\n        global_step += 1", "        # del batch, batch_rays, target_s, img_loss, rgb, disp, acc, extras\n\n\nif __name__=='__main__':\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    torch.cuda.set_device(device_idx)\n\n    parser = config_parser()\n    args = parser.parse_args()\n    sel = []\n\n    if args.multi_scene and args.render_only:\n        for dir_name in sorted(os.listdir(args.root_dir)):\n            args.expname = dir_name\n            category_name = dir_name.split(\"_\")[1]\n            args.model_name = dir_name.split(\"_\")[2] + \"_\" + dir_name.split(\"_\")[3]\n            # if args.model_name not in sel:\n                # continue\n            print(\"Processing \", args.model_name)\n            args.ft_path = os.path.join(args.root_dir, dir_name, f\"{args.iters:06d}.tar\")\n            if not os.path.exists(args.ft_path):\n                print(\"Skipping %s as no saved model found!\" % (args.model_name))\n                continue\n\n            train(args)\n\n    else:\n        train(args)", ""]}
{"filename": "nerf/grad_vis.py", "chunked_list": ["import open3d as o3d\nimport numpy as np\nimport os\nimport argparse\n\nimport sys\nimport open3d as o3d\nimport seaborn as sns\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as Rotation", "import numpy as np\nfrom scipy.spatial.transform import Rotation as Rotation\nimport torch\nfrom scipy.spatial import distance\nimport torch.nn\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.ndimage import gaussian_filter\n\ndef get_gradient_density(x):\n    # x - B, H, W, D\n\n    B, H, W, D = x.shape\n    d_x = x[..., 1:, :, :] - x[..., :-1, :, :] # B, H - 1, W, D\n    d_y = x[..., :, 1:, :] - x[..., :, :-1, :] # B, H, W - 1, D\n    d_y = -d_y\n    d_z = x[..., :, :, 1:] - x[..., :, :, :-1] # B, H, W, D - 1\n\n    d_x = torch.nn.functional.interpolate(d_x.unsqueeze(1), (H, W, D)).squeeze(1)\n    d_y = torch.nn.functional.interpolate(d_y.unsqueeze(1), x.shape[1:]).squeeze(1)\n    d_z = torch.nn.functional.interpolate(d_z.unsqueeze(1), x.shape[1:]).squeeze(1)\n\n    gradient = torch.stack([d_x, d_y, d_z], 1)\n\n    return gradient # B, 3, H, W, D", "def get_gradient_density(x):\n    # x - B, H, W, D\n\n    B, H, W, D = x.shape\n    d_x = x[..., 1:, :, :] - x[..., :-1, :, :] # B, H - 1, W, D\n    d_y = x[..., :, 1:, :] - x[..., :, :-1, :] # B, H, W - 1, D\n    d_y = -d_y\n    d_z = x[..., :, :, 1:] - x[..., :, :, :-1] # B, H, W, D - 1\n\n    d_x = torch.nn.functional.interpolate(d_x.unsqueeze(1), (H, W, D)).squeeze(1)\n    d_y = torch.nn.functional.interpolate(d_y.unsqueeze(1), x.shape[1:]).squeeze(1)\n    d_z = torch.nn.functional.interpolate(d_z.unsqueeze(1), x.shape[1:]).squeeze(1)\n\n    gradient = torch.stack([d_x, d_y, d_z], 1)\n\n    return gradient # B, 3, H, W, D", "\n\ndef draw_oriented_pointcloud(x, n, t=1.0):\n    a = x\n    b = x + t * n\n    points = []\n    lines = []\n    for i in range(a.shape[0]):\n        ai = [a[i, 0], a[i, 1], a[i, 2]]\n        bi = [b[i, 0], b[i, 1], b[i, 2]]\n        points.append(ai)\n        points.append(bi)\n        lines.append([2*i, 2*i+1])\n    colors = [[1, 0, 0] for i in range(len(lines))]\n\n    pcd = o3d.geometry.PointCloud()\n    point_colors = np.ones(x.shape)\n    pcd.points = o3d.utility.Vector3dVector(a)\n    pcd.colors = o3d.utility.Vector3dVector(point_colors)\n\n    line_set = o3d.geometry.LineSet(\n        points=o3d.utility.Vector3dVector(points),\n        lines=o3d.utility.Vector2iVector(lines),\n    )\n    line_set.colors = o3d.utility.Vector3dVector(colors)\n    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n    size=0.6, origin=[-1, -1, -1])\n    o3d.visualization.draw_geometries([line_set, pcd,mesh_frame])", "\n\nparser = argparse.ArgumentParser(description=\"NeRF density gradient field visualization\")\nparser.add_argument(\"--input_dir\", required=True, type = str)\nparser.add_argument(\"--res\", default=32, type = int)\nparser.add_argument(\"--max_files\", default=10, type = int)\nargs = parser.parse_args()\n\ncount = 0\nfor file in os.listdir(args.input_dir):\n    if \"_sigmas_%d.npy\" % (args.res) not in file:\n        continue\n\n    sigmas_path = os.path.join(args.input_dir, file)\n    samples_file = file.replace(\"sigmas\", \"samples\")\n    samples_path = os.path.join(args.input_dir, samples_file)\n\n    density = np.load(sigmas_path)\n    coords = np.load(samples_path)\n\n    density = density.transpose(2, 1, 0)\n    density = gaussian_filter(density, sigma=1)\n    density = torch.from_numpy(density)\n    \n    coords = coords.transpose(2, 1, 0, 3)\n    coords = coords.reshape(-1, 3)\n    scale_ = 0.1\n    \n    grad = get_gradient_density(density.unsqueeze(0)).squeeze(0)\n    grad = grad.permute(1, 2, 3, 0).reshape(-1,3)\n    grad = grad.detach().numpy()\n    draw_oriented_pointcloud(coords, grad, scale_)\n\n    count += 1\n    if count >= args.max_files:\n        break", "count = 0\nfor file in os.listdir(args.input_dir):\n    if \"_sigmas_%d.npy\" % (args.res) not in file:\n        continue\n\n    sigmas_path = os.path.join(args.input_dir, file)\n    samples_file = file.replace(\"sigmas\", \"samples\")\n    samples_path = os.path.join(args.input_dir, samples_file)\n\n    density = np.load(sigmas_path)\n    coords = np.load(samples_path)\n\n    density = density.transpose(2, 1, 0)\n    density = gaussian_filter(density, sigma=1)\n    density = torch.from_numpy(density)\n    \n    coords = coords.transpose(2, 1, 0, 3)\n    coords = coords.reshape(-1, 3)\n    scale_ = 0.1\n    \n    grad = get_gradient_density(density.unsqueeze(0)).squeeze(0)\n    grad = grad.permute(1, 2, 3, 0).reshape(-1,3)\n    grad = grad.detach().numpy()\n    draw_oriented_pointcloud(coords, grad, scale_)\n\n    count += 1\n    if count >= args.max_files:\n        break", ""]}
{"filename": "cafi_net/metrics_test.py", "chunked_list": ["import hydra, logging\nimport torch, glob, os\nimport numpy as np\nfrom trainers import *\nfrom models import *\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\nlog = logging.getLogger(__name__)", "\nlog = logging.getLogger(__name__)\n\n@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\ndef run(cfg):\n\n    seed_everything(cfg.utils.seed)\n    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n    log.info(cfg)\n    print(os.getcwd())\n    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n    model.run_metrics(cfg)", "    \n\nif __name__ == '__main__':\n\n    run()\n"]}
{"filename": "cafi_net/main.py", "chunked_list": ["import hydra, logging\nimport torch, glob, os\nimport numpy as np\nfrom trainers import *\nfrom models import *\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\nlog = logging.getLogger(__name__)", "\nlog = logging.getLogger(__name__)\n\n@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\ndef run(cfg):\n\n    seed_everything(cfg.utils.seed)\n    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n    log.info(cfg)\n    print(os.getcwd())\n    checkpoint_callback = ModelCheckpoint(**cfg.callback.model_checkpoint.segmentation.args)\n    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type)(configs = cfg)\n    trainer = Trainer(**cfg.trainer, callbacks = [checkpoint_callback], logger = train_logger)\n    trainer.fit(model)", "    \n\nif __name__ == '__main__':\n\n    run()\n"]}
{"filename": "cafi_net/canonical_render.py", "chunked_list": ["import hydra, logging\nimport torch, glob, os\nimport numpy as np\nfrom trainers import *\nfrom models import *\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\nlog = logging.getLogger(__name__)", "\nlog = logging.getLogger(__name__)\n\n@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\ndef run(cfg):\n\n    seed_everything(cfg.utils.seed)\n    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n    log.info(cfg)\n    print(os.getcwd())\n    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n    model.run_canonica_render(cfg)", "    \n\nif __name__ == '__main__':\n\n    run()\n"]}
{"filename": "cafi_net/tester.py", "chunked_list": ["import hydra, logging\nimport torch, glob, os\nimport numpy as np\nfrom trainers import *\nfrom models import *\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\nlog = logging.getLogger(__name__)", "\nlog = logging.getLogger(__name__)\n\n@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\ndef run(cfg):\n\n    seed_everything(cfg.utils.seed)\n    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n    log.info(cfg)\n    print(os.getcwd())\n    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n    model.run_test(cfg)", "    \n\nif __name__ == '__main__':\n\n    run()\n"]}
{"filename": "cafi_net/vis_utils.py", "chunked_list": ["import seaborn as sns\nimport open3d as o3d\nimport numpy as np\nimport torch\nimport argparse, os, sys\nimport glob, os\n\ndef create_color_samples(N):\n    '''\n    Creates N distinct colors\n    N x 3 output\n    '''\n\n    palette = sns.color_palette(None, N)\n    palette = np.array(palette)\n\n    return palette", "\ndef convert_tensor_2_numpy(x):\n    '''\n    Convert pytorch tensor to numpy\n    '''\n    \n    out = torch.squeeze(x, axis = 0).numpy()\n    \n    return out \n", "\n\ndef save_pointcloud(x, filename = \"./pointcloud.ply\"):\n    '''\n    Save point cloud to the destination given in the filename\n    x can be list of inputs (Nx3) capsules or numpy array of N x 3\n    '''\n\n    label_map = []\n    if isinstance(x, list):\n        \n        pointcloud = []\n        labels = create_color_samples(len(x))\n        for i in range(len(x)):\n            pts = x[i]\n            # print(pts.shape, \"vis\")\n            pts = convert_tensor_2_numpy(pts)\n\n            pointcloud.append(pts)\n            label_map.append(np.repeat(labels[i:(i + 1)], pts.shape[0], axis = 0))\n        \n        # x = np.concatenate(x, axis = 0)\n        pointcloud = np.concatenate(pointcloud, axis = 0)\n        x = pointcloud.copy()\n        label_map = np.concatenate(label_map, axis = 0)\n    else:\n        x = convert_tensor_2_numpy(x)\n        # print(x.shape)\n        label_map = np.ones((len(x), 3)) * 0.5\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(x)\n    pcd.colors = o3d.utility.Vector3dVector(label_map)\n\n    o3d.io.write_point_cloud(filename, pcd)", "\n\n\n\n\ndef visualize_outputs(base_path, pointcloud_name, start = 0, max_num = 10, spacing = 1.0, skip = 1):\n    '''\n    Visualize point clouds in open3D \n    '''\n    \n    filename = glob.glob(os.path.join(base_path, \"\")  + pointcloud_name)\n    filename.sort()\n    print(filename)\n    filename_2 = []\n    for f in filename:\n        #print(f)\n        print(os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + pointcloud_name, f)\n        #if (os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + pointcloud_name) == f:\n        filename_2.append(f)\n    filename = filename_2\n    filename = filename[start::skip]\n    filename = filename[:max_num]\n    # print(filename)\n    num_pcds = len(filename)\n    if skip != 1:\n        num_pcds = len(filename) // skip\n    rows = np.floor(np.sqrt(num_pcds))\n    pcd_list = []\n    arrow_list = []\n    pcd_iter = 0\n    pcd_index = 0\n    for pcd_file in filename:\n        print(pcd_file)\n        if pcd_iter % skip == 0:\n        \n            pcd = o3d.io.read_point_cloud(pcd_file)\n            pcd.colors = pcd.points\n            column_num = pcd_index // rows\n            row_num = pcd_index % rows\n            vector = (row_num * spacing, column_num * spacing, 0)\n            #vector = [0, 0, 0]\n            # print(vector)\n            pcd.translate(vector)\n            pcd_list.append(pcd)\n            pcd_index += 1\n\n            U, S, V = torch.pca_lowrank(torch.tensor(np.array(pcd.points)))\n            arrow = get_arrow(torch.tensor([0, 0, 0]) + torch.tensor(list(vector)), V[:, 0] + torch.tensor(list(vector)))\n            arrow_list.append(arrow)\n\n        pcd_iter +=1\n\n\n    #o3d.visualization.draw_geometries(arrow_list)\n    o3d.visualization.draw_geometries(pcd_list)", "     \ndef draw_geometries(pcds):\n    \"\"\"\n    Draw Geometries\n    Args:\n        - pcds (): [pcd1,pcd2,...]\n    \"\"\"\n    o3d.visualization.draw_geometries(pcds)\n\ndef get_o3d_FOR(origin=[0, 0, 0],size=10):\n    \"\"\" \n    Create a FOR that can be added to the open3d point cloud\n    \"\"\"\n    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n    size=size)\n    mesh_frame.translate(origin)\n    return(mesh_frame)", "\ndef get_o3d_FOR(origin=[0, 0, 0],size=10):\n    \"\"\" \n    Create a FOR that can be added to the open3d point cloud\n    \"\"\"\n    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n    size=size)\n    mesh_frame.translate(origin)\n    return(mesh_frame)\n\ndef vector_magnitude(vec):\n    \"\"\"\n    Calculates a vector's magnitude.\n    Args:\n        - vec (): \n    \"\"\"\n    magnitude = np.sqrt(np.sum(vec**2))\n    return(magnitude)", "\ndef vector_magnitude(vec):\n    \"\"\"\n    Calculates a vector's magnitude.\n    Args:\n        - vec (): \n    \"\"\"\n    magnitude = np.sqrt(np.sum(vec**2))\n    return(magnitude)\n", "\n\ndef calculate_zy_rotation_for_arrow(vec):\n    \"\"\"\n    Calculates the rotations required to go from the vector vec to the \n    z axis vector of the original FOR. The first rotation that is \n    calculated is over the z axis. This will leave the vector vec on the\n    XZ plane. Then, the rotation over the y axis. \n\n    Returns the angles of rotation over axis z and y required to\n    get the vector vec into the same orientation as axis z\n    of the original FOR\n\n    Args:\n        - vec (): \n    \"\"\"\n    # Rotation over z axis of the FOR\n    gamma = np.arctan(vec[1]/vec[0])\n    Rz = np.array([[np.cos(gamma),-np.sin(gamma),0],\n                   [np.sin(gamma),np.cos(gamma),0],\n                   [0,0,1]])\n    # Rotate vec to calculate next rotation\n    vec = Rz.T@vec.reshape(-1,1)\n    vec = vec.reshape(-1)\n    # Rotation over y axis of the FOR\n    beta = np.arctan(vec[0]/vec[2])\n    Ry = np.array([[np.cos(beta),0,np.sin(beta)],\n                   [0,1,0],\n                   [-np.sin(beta),0,np.cos(beta)]])\n    return(Rz, Ry)", "\ndef create_arrow(scale=10):\n    \"\"\"\n    Create an arrow in for Open3D\n    \"\"\"\n    cone_height = scale*0.2\n    cylinder_height = scale*0.8\n    cone_radius = scale/10\n    cylinder_radius = scale/20\n    mesh_frame = o3d.geometry.TriangleMesh.create_arrow(cone_radius=0.2,\n        cone_height=0.1,\n        cylinder_radius=0.1,\n        cylinder_height=0.5)\n    return(mesh_frame)", "\ndef get_arrow(origin=[0, 0, 0], end=None, vec=None):\n    \"\"\"\n    Creates an arrow from an origin point to an end point,\n    or create an arrow from a vector vec starting from origin.\n    Args:\n        - end (): End point. [x,y,z]\n        - vec (): Vector. [i,j,k]\n    \"\"\"\n    scale = 10\n    Ry = Rz = np.eye(3)\n    T = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    T[:3, -1] = origin\n    if end is not None:\n        vec = np.array(end) - np.array(origin)\n    elif vec is not None:\n        vec = np.array(vec)\n    if end is not None or vec is not None:\n        scale = vector_magnitude(vec)\n        Rz, Ry = calculate_zy_rotation_for_arrow(vec)\n    scale = 1\n    mesh = create_arrow(scale)\n    # Create the arrow\n    mesh.rotate(Ry, center=np.array([0, 0, 0]))\n    mesh.rotate(Rz, center=np.array([0, 0, 0]))\n    mesh.translate(origin)\n    return(mesh)", "\n\nif __name__ == \"__main__\":\n    \n\n    # Argument parser\n    parser = argparse.ArgumentParser(description = \"Visualization script\")\n    parser.add_argument(\"--base_path\", help = \"Base path to folder\", required = True)\n    parser.add_argument(\"--pcd\", help = \"PCD string to visualize\", required = True)\n    parser.add_argument(\"--spacing\", help = \"Spacing\", default = 2.0, type = float)\n    parser.add_argument(\"--num_list\", help = \"indices\", nargs=\"+\", default = list(range(9)), type = int)\n    parser.add_argument(\"--start\", help = \"start index\", default = 0, type = int)\n    parser.add_argument(\"--num\", help = \"number of models\", default = 9, type = int)\n    parser.add_argument(\"--skip\", help = \"number of models to skip\", default = 1, type = int)\n\n\n    args = parser.parse_args()\n    #######################################################################\n    \n    \n    visualize_outputs(args.base_path, args.pcd, spacing = args.spacing, start = args.start, max_num = args.num, skip = args.skip)", ""]}
{"filename": "cafi_net/utils/pooling.py", "chunked_list": ["import torch\n\ndef kd_pooling_1d(x, pool_size, pool_mode='avg'):\n    \"\"\"\n    Expects kd tree indexed points\n    x - [B, N_{in}, 3]\n\n    out - [B, N_{out}, 3]\n    \"\"\"\n    #assert (isPowerOfTwo(pool_size))\n    pool_size = pool_size\n    if pool_mode == 'max':\n        pool = torch.nn.MaxPool1d(pool_size)#, stride = 1)\n    else:\n        pool = torch.nn.AvgPool1d(pool_size)#, stride = 1)\n    if isinstance(x, list):\n        y = []\n        for i in range(len(x)):\n            x_pool = pool(x[i].permute(0, 2, 1)).permute(0, 2, 1)\n            x.append(x_pool)\n\n    elif isinstance(x, dict):\n        y = dict()\n        for l in x:\n            if isinstance(l, int):\n                x_pool = pool(x[l].permute(0, 2, 1)).permute(0, 2, 1)\n                y[l] = x_pool\n    else:\n        x_pool = pool(x.permute(0, 2, 1)).permute(0, 2, 1)\n        y = x_pool\n    return y", "\n\ndef kd_pooling_3d(x, pool_size, pool_mode='avg'):\n    \"\"\"\n    Expects kd tree indexed points\n    x - [B, H_{in}, W_{in}, D_{in}, 3]\n\n    out - [B, H_{out}, W_{out}, D_{out}, 3]\n    \"\"\"\n    \n    pool_size = pool_size\n    if pool_mode == 'max':\n        pool = torch.nn.MaxPool3d(pool_size, stride = pool_size)#, stride = 1)\n    else:\n        pool = torch.nn.AvgPool3d(pool_size, stride = pool_size)#, stride = 1)\n    if isinstance(x, list):\n        y = []\n        for i in range(len(x)):\n            x_pool = pool(x[i].permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n            x.append(x_pool)\n\n    elif isinstance(x, dict):\n        y = dict()\n        for l in x:\n            if isinstance(l, int):\n                x_pool = pool(x[l].permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n                y[l] = x_pool\n    else:\n        x_pool = pool(x.permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n        y = x_pool\n    return y", "\nif __name__ == \"__main__\":\n\n    # x = torch.randn((2, 4, 3))\n    x = torch.randn((2, 16, 16, 16, 3))\n\n    out = kd_pooling_3d(x, 2)\n    print(x, out)\n    print(x.shape, out.shape)\n", ""]}
{"filename": "cafi_net/utils/losses.py", "chunked_list": ["import torch\nimport numpy as np\nimport sys\nsys.path.append('../')\nfrom utils.pointcloud_utils import sq_distance_mat,compute_centroids\n\n\n\n\ndef equilibrium_loss(unnormalized_capsules):\n    a = torch.mean(unnormalized_capsules, dim=1, keepdims=False)\n    am = torch.mean(a, dim=-1, keepdims=True)\n    l = torch.subtract(a, am)\n    l = l*l\n    return torch.mean(l)", "\ndef equilibrium_loss(unnormalized_capsules):\n    a = torch.mean(unnormalized_capsules, dim=1, keepdims=False)\n    am = torch.mean(a, dim=-1, keepdims=True)\n    l = torch.subtract(a, am)\n    l = l*l\n    return torch.mean(l)\n    \n \ndef localization_loss_new(points, capsules, centroids):\n\n\n    points_centered = points[:, :, None] - centroids[:, None, :] # B, N, K, 3\n    points_centered_activated = capsules[:, :, :, None] * points_centered\n\n    l = points_centered.permute(0,2,1,3) # B, K, N, 3\n    l_1 = points_centered_activated.permute(0,2,3,1) # B, K, 3, N\n\n    covariance = l_1 @ l\n    loss = torch.mean(torch.diagonal(covariance))\n    return loss", " \ndef localization_loss_new(points, capsules, centroids):\n\n\n    points_centered = points[:, :, None] - centroids[:, None, :] # B, N, K, 3\n    points_centered_activated = capsules[:, :, :, None] * points_centered\n\n    l = points_centered.permute(0,2,1,3) # B, K, N, 3\n    l_1 = points_centered_activated.permute(0,2,3,1) # B, K, 3, N\n\n    covariance = l_1 @ l\n    loss = torch.mean(torch.diagonal(covariance))\n    return loss", "    \n    \ndef l2_distance_batch(x, y):\n    z = x - y\n    z = z * z\n    z = torch.sum(z, dim=-1)\n    z = torch.sqrt(z)\n    return z\n\ndef chamfer_distance_l2_batch(X, Y, sq_dist_mat=None):\n\n    if sq_dist_mat is None:\n\n        # compute distance mat\n\n        D2 = sq_distance_mat(X, Y)\n\n    else:\n\n        D2 = sq_dist_mat\n    \n    dXY = torch.sqrt(torch.max(torch.min(D2, dim=-1, keepdims=False).values, torch.tensor(0.000001)))\n\n    dXY = torch.mean(dXY, dim=1, keepdims=False)\n\n    dYX = torch.sqrt(torch.max(torch.min(D2, dim=1, keepdims=False).values, torch.tensor(0.000001)))\n\n    dYX = torch.mean(dYX, dim=-1, keepdims=False)\n\n    d = dXY + dYX\n\n    return 0.5*d", "\ndef chamfer_distance_l2_batch(X, Y, sq_dist_mat=None):\n\n    if sq_dist_mat is None:\n\n        # compute distance mat\n\n        D2 = sq_distance_mat(X, Y)\n\n    else:\n\n        D2 = sq_dist_mat\n    \n    dXY = torch.sqrt(torch.max(torch.min(D2, dim=-1, keepdims=False).values, torch.tensor(0.000001)))\n\n    dXY = torch.mean(dXY, dim=1, keepdims=False)\n\n    dYX = torch.sqrt(torch.max(torch.min(D2, dim=1, keepdims=False).values, torch.tensor(0.000001)))\n\n    dYX = torch.mean(dYX, dim=-1, keepdims=False)\n\n    d = dXY + dYX\n\n    return 0.5*d"]}
{"filename": "cafi_net/utils/__init__.py", "chunked_list": ["#from . import pointcloud_utils\n#from . import diffusion_utils\n#from . import diffusion_geometry"]}
{"filename": "cafi_net/utils/train_utils.py", "chunked_list": ["from scipy.spatial.transform import Rotation\nimport torch\n\n\ndef random_rotate(x):\n\n    \"\"\"\n    x - B, N, 3\n    out - B, N, 3\n    Randomly rotate point cloud\n    \"\"\"\n    \n    out = perform_rotation(torch.from_numpy(Rotation.random(x.shape[0]).as_matrix()).type_as(x), x)\n\n    return out", "\ndef mean_center(x):\n    \"\"\"\n    x - B, N, 3\n    x_mean - B, N, 3\n    Mean center point cloud\n    \"\"\"\n\n    out = x - x.mean(-2, keepdims = True)\n    return out", "\ndef perform_rotation(R, x):\n    '''\n    Perform rotation on point cloud\n    R - B, 3, 3\n    x - B, N, 3\n\n    out - B, N, 3\n    '''\n    out = torch.einsum(\"bij,bpj->bpi\", R.type_as(x), x)\n\n    return out", "\ndef orthonormalize_basis(basis):\n    \"\"\"\n    Returns orthonormal basis vectors\n    basis - B, 3, 3\n\n    out - B, 3, 3\n    \"\"\"\n    try:\n        u, s, v = torch.svd(basis)\n    except:                     # torch.svd may have convergence issues for GPU and CPU.\n        u, s, v = torch.svd(basis + 1e-3*basis.mean()*torch.rand_like(basis).type_as(basis))\n    # u, s, v = torch.svd(basis)\n    out = u @ v.transpose(-2, -1)    \n\n    return out"]}
{"filename": "cafi_net/utils/pointcloud_utils.py", "chunked_list": ["import torch\nfrom utils.group_points import gather_idx\nimport numpy as np\nimport h5py\nimport open3d as o3d\nimport seaborn as sns\nfrom spherical_harmonics.spherical_cnn import zernike_monoms\nfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans", "from sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\n\n\ndef kron(a, b):\n    \"\"\"\n    Kronecker product of matrices a and b with leading batch dimensions.\n    Batch dimensions are broadcast. The number of them mush\n    :type a: torch.Tensor\n    :type b: torch.Tensor\n    :rtype: torch.Tensor\n    \"\"\"\n    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n    siz0 = res.shape[:-4]\n    return res.reshape(siz0 + siz1)", "    \ndef compute_centroids(points, capsules):\n    return torch.einsum('bij,bic->bcj', points, capsules)\n    \ndef sq_distance_mat(pcd_1, pcd_2):\n\n    r0 = pcd_1 * pcd_1\n    r0 = torch.sum(r0, dim=2, keepdims=True)\n    r1 = (pcd_2 * pcd_2)\n    r1 = torch.sum(r1, dim=2, keepdims=True)\n    r1 = r1.permute(0, 2, 1)\n    sq_distance_mat = r0 - 2. * (pcd_1 @ pcd_2.permute(0, 2, 1)) + r1\n\n    return sq_distance_mat.squeeze(-1)", "    \ndef convert_yzx_to_xyz_basis(basis):\n\n    # basis - N, 3, 3\n\n    rot_y = torch.tensor([[np.cos(np.pi / 2), 0, np.sin(np.pi / 2)]\n              ,[0,              1,                      0], \n              [-np.sin(np.pi / 2), 0, np.cos(np.pi / 2)]]).type_as(basis)\n\n\n    rot_z = torch.tensor([\n                    [np.cos(np.pi / 2), -np.sin(np.pi / 2), 0],\n                    [np.sin(np.pi / 2), np.cos(np.pi / 2), 0],\n                    [0, 0, 1]\n                    ]).type_as(basis)\n\n    transform = (rot_y @ rot_z).unsqueeze(0).type_as(basis)\n    transform = transform.repeat(basis.shape[0], 1, 1)\n    \n    if len(basis.shape) == 4:\n        transform = transform.unsqueeze(1)\n\n    return transform @ basis", "\ndef create_color_samples(N):\n    '''\n    Creates N distinct colors\n    N x 3 output\n    '''\n\n    palette = sns.color_palette(None, N)\n    palette = np.array(palette)\n\n    return palette", "\ndef convert_tensor_2_numpy(x):\n    '''\n    Convert pytorch tensor to numpy\n    '''\n    \n    out = x.squeeze(0).detach().cpu().numpy()\n    \n    return out \n\ndef save_density(x, filename = \"./pointcloud.ply\"):\n    density_int = x.cpu().numpy()\n    shape_ = density_int.shape \n    density_int = density_int.reshape(-1,1)\n    mask_density_int = np.ones_like(density_int) * -1\n        \n    model = KMeans(init=\"k-means++\",n_clusters=2)\n    model.fit(density_int)\n    label = model.predict(density_int)\n    clusters = np.unique(label)\n    \n    if np.mean(density_int[np.where(label == 1)[0]]) > np.mean(density_int[np.where(label == 0)[0]]):\n        fg_idx = np.where(label == 1)[0]\n        bg_idx = np.where(label == 0)[0]\n\n\n    else:\n        fg_idx = np.where(label == 0)[0]\n        bg_idx = np.where(label == 1)[0]\n\n    mask_density_int[fg_idx] = 1.\n    mask_density_int[bg_idx] = 0.\n        \n    mask_density_tensor = torch.from_numpy(mask_density_int)\n    mask_density_int = mask_density_int.reshape(shape_)\n    sampling_grid = get_xyz_grid(x)\n    sampling_grid_np = sampling_grid.detach().cpu().numpy()\n    pts = sampling_grid_np[mask_density_int == 1., :]\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    label_map = np.ones((pts.shape[0], 3)) * 0.5\n    pcd.colors = o3d.utility.Vector3dVector(label_map)\n    o3d.io.write_point_cloud(filename, pcd)", "\ndef save_density(x, filename = \"./pointcloud.ply\"):\n    density_int = x.cpu().numpy()\n    shape_ = density_int.shape \n    density_int = density_int.reshape(-1,1)\n    mask_density_int = np.ones_like(density_int) * -1\n        \n    model = KMeans(init=\"k-means++\",n_clusters=2)\n    model.fit(density_int)\n    label = model.predict(density_int)\n    clusters = np.unique(label)\n    \n    if np.mean(density_int[np.where(label == 1)[0]]) > np.mean(density_int[np.where(label == 0)[0]]):\n        fg_idx = np.where(label == 1)[0]\n        bg_idx = np.where(label == 0)[0]\n\n\n    else:\n        fg_idx = np.where(label == 0)[0]\n        bg_idx = np.where(label == 1)[0]\n\n    mask_density_int[fg_idx] = 1.\n    mask_density_int[bg_idx] = 0.\n        \n    mask_density_tensor = torch.from_numpy(mask_density_int)\n    mask_density_int = mask_density_int.reshape(shape_)\n    sampling_grid = get_xyz_grid(x)\n    sampling_grid_np = sampling_grid.detach().cpu().numpy()\n    pts = sampling_grid_np[mask_density_int == 1., :]\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    label_map = np.ones((pts.shape[0], 3)) * 0.5\n    pcd.colors = o3d.utility.Vector3dVector(label_map)\n    o3d.io.write_point_cloud(filename, pcd)", "   \n\n\ndef save_pointcloud(x, filename = \"./pointcloud.ply\"):\n    '''\n    Save point cloud to the destination given in the filename\n    x can be list of inputs (Nx3) capsules or numpy array of N x 3\n    '''\n\n    label_map = []\n    if isinstance(x, list):\n        \n        pointcloud = []\n        labels = create_color_samples(len(x))\n        for i in range(len(x)):\n            pts = x[i]\n            # print(pts.shape, \"vis\")\n            pts = convert_tensor_2_numpy(pts)\n\n            pointcloud.append(pts)\n            label_map.append(np.repeat(labels[i:(i + 1)], pts.shape[0], axis = 0))\n        \n        # x = np.concatenate(x, axis = 0)\n        pointcloud = np.concatenate(pointcloud, axis = 0)\n        x = pointcloud.copy()\n        label_map = np.concatenate(label_map, axis = 0)\n    else:\n        x = convert_tensor_2_numpy(x)\n        label_map = np.ones((len(x), 3)) * 0.5\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(x)\n    pcd.colors = o3d.utility.Vector3dVector(label_map)\n\n    o3d.io.write_point_cloud(filename, pcd)", "\ndef diameter(x, axis=-2, keepdims=True):\n    return torch.max(x, dim=axis, keepdims=keepdims).values - torch.min(x, dim=axis, keepdims=keepdims).values\n\ndef kdtree_indexing(x, depth=None, return_idx = False):\n    num_points = x.shape[1]\n    #assert isPowerOfTwo(num_points)\n    if depth is None:\n        depth = int(np.log(num_points) / np.log(2.) + 0.1)\n    y = x\n    batch_idx = torch.arange(x.shape[0]).to(torch.int32).to(x.device)\n    batch_idx = torch.reshape(batch_idx, (-1, 1))\n    batch_idx = batch_idx.repeat(1, x.shape[1])\n\n    points_idx = torch.arange(num_points).type_as(x).to(torch.int64)\n    points_idx = torch.reshape(points_idx, (1, -1, 1))\n    points_idx = points_idx.repeat(x.shape[0], 1, 1)\n\n\n\n    for i in range(depth):\n        y_shape = list(y.shape)\n        diam = diameter(y)\n        split_idx = torch.argmax(diam, dim=-1).to(torch.long).to(x.device)\n        split_idx = split_idx.repeat(1, y.shape[1])\n        idx = torch.arange(y.shape[0]).to(torch.long).to(x.device)\n        idx = idx.unsqueeze(-1)\n        idx = idx.repeat(1, y.shape[1])\n        branch_idx = torch.arange(y.shape[1]).to(torch.long).to(x.device)\n        branch_idx = branch_idx.unsqueeze(0)\n        branch_idx = branch_idx.repeat(y.shape[0], 1)\n        split_idx = torch.stack([idx, branch_idx, split_idx], dim=-1)\n        # print(split_idx, split_idx.shape)\n        # Gather implementation required\n        # m = tf.gather_nd(y, split_idx)\n        # print(y.shape)\n        m = gather_idx(y, split_idx)\n        # print(m.shape)\n        sort_idx = torch.argsort(m, dim=-1)\n        sort_idx = torch.stack([idx, sort_idx], dim=-1)\n        # Gather required\n        points_idx = gather_idx(points_idx, sort_idx)\n        points_idx = torch.reshape(points_idx, (-1, int(y.shape[1] // 2), 1))\n        # Gather\n        y = gather_idx(y, sort_idx)\n        y = torch.reshape(y, (-1, int(y.shape[1] // 2), 3))\n\n\n    \n    y = torch.reshape(y, x.shape)\n    if not return_idx:\n        return y\n\n    points_idx = torch.reshape(points_idx, (x.shape[0], x.shape[1]))\n    points_idx_inv = torch.argsort(points_idx, dim=-1)\n    points_idx = torch.stack([batch_idx, points_idx], dim=-1)\n    points_idx_inv = torch.stack([batch_idx, points_idx_inv], dim=-1)\n\n    return y, points_idx, points_idx_inv", "\ndef kdtree_indexing_sdf(x, sdf, depth=None, return_idx = False):\n    num_points = x.shape[1]\n    #assert isPowerOfTwo(num_points)\n    if depth is None:\n        depth = int(np.log(num_points) / np.log(2.) + 0.1)\n    y = x\n    batch_idx = torch.arange(x.shape[0]).to(torch.int32).to(x.device)\n    batch_idx = torch.reshape(batch_idx, (-1, 1))\n    batch_idx = batch_idx.repeat(1, x.shape[1])\n\n    points_idx = torch.arange(num_points).type_as(x).to(torch.int64)\n    points_idx = torch.reshape(points_idx, (1, -1, 1))\n    points_idx = points_idx.repeat(x.shape[0], 1, 1)\n\n\n\n    for i in range(depth):\n        y_shape = list(y.shape)\n        diam = diameter(y)\n        split_idx = torch.argmax(diam, dim=-1).to(torch.long).to(x.device)\n        split_idx = split_idx.repeat(1, y.shape[1])\n        idx = torch.arange(y.shape[0]).to(torch.long).to(x.device)\n        idx = idx.unsqueeze(-1)\n        idx = idx.repeat(1, y.shape[1])\n        branch_idx = torch.arange(y.shape[1]).to(torch.long).to(x.device)\n        branch_idx = branch_idx.unsqueeze(0)\n        branch_idx = branch_idx.repeat(y.shape[0], 1)\n        split_idx = torch.stack([idx, branch_idx, split_idx], dim=-1)\n        # print(split_idx, split_idx.shape)\n        # Gather implementation required\n        # m = tf.gather_nd(y, split_idx)\n        # print(y.shape)\n        m = gather_idx(y, split_idx)\n        # print(m.shape)\n        sort_idx = torch.argsort(m, dim=-1)\n        sort_idx = torch.stack([idx, sort_idx], dim=-1)\n        # Gather required\n        points_idx = gather_idx(points_idx, sort_idx)\n        points_idx = torch.reshape(points_idx, (-1, int(y.shape[1] // 2), 1))\n        # Gather\n        y = gather_idx(y, sort_idx)\n        y = torch.reshape(y, (-1, int(y.shape[1] // 2), 3))\n\n\n    \n    y = torch.reshape(y, x.shape)\n    if not return_idx:\n        return y\n\n    points_idx = torch.reshape(points_idx, (x.shape[0], x.shape[1]))\n    points_idx_inv = torch.argsort(points_idx, dim=-1)\n    points_idx = torch.stack([batch_idx, points_idx], dim=-1)\n    points_idx_inv = torch.stack([batch_idx, points_idx_inv], dim=-1)\n\n    return y, points_idx, points_idx_inv", "\n\n\n\n\ndef get_xyz_grid(grid, rotation = None):\n    \"\"\"\n    Returns an xyz grid in the normalized coordinates to perform density convolution\n    \n    grid - B, C, H, W, D\n    \n    out - B, C, H, W, 3\n    \"\"\"\n\n    if len(grid.shape) == 4:\n        out = grid.unsqueeze(1)\n    else:\n        out = grid\n\n    if rotation is not None:\n        theta = rotation\n    else: \n        theta = torch.eye(3).unsqueeze(0).repeat(out.shape[0], 1, 1).type_as(grid)\n    t = torch.tensor([0, 0, 0]).unsqueeze(0).unsqueeze(2).repeat(theta.shape[0], 1, 1).type_as(grid)\n    theta = torch.cat([theta, t], dim = -1)\n    out_grid = torch.nn.functional.affine_grid(theta, out.shape, align_corners = True)\n\n    return out_grid", "\ndef get_equivariant_density(density_field, normalize = True, scale_grid = True):\n    \"\"\"\n    Get equivariant density grid\n    density_field - B, H, W, D\n    \"\"\"\n\n    out = density_field\n    if len(density_field.shape) == 4:\n        out = density_field.unsqueeze(1)\n\n    B, _1, H, W, D = out.shape\n\n    grid = get_xyz_grid(density_field) # B, H, W, D, 3\n    grid_pts = grid.reshape(B, H*W*D, 3)\n\n    out_density = out.reshape(B, H*W*D, 1)\n    if normalize:\n        density_max = torch.max(out_density, axis = 1, keepdims = True)\n        out_density = out_density / (density_max.values + 1e-8)\n    \n    # eq_grid_pts = zernike_monoms(grid_pts, 3)[1][..., 1:] # B, N, 3, 1\n    # eq_grid_pts = out_density * eq_grid_pts[..., 0] # B, N, 3\n    eq_grid_pts = grid_pts.unsqueeze(-1) #zernike_monoms(grid_pts, 3)[1][..., 1:] # B, N, 3, 1\n    if scale_grid:\n        eq_grid_pts = out_density * eq_grid_pts[..., 0] # B, N, 3\n    else:\n        eq_grid_pts = eq_grid_pts[..., 0]\n        \n    eq_grid_pts = eq_grid_pts.reshape(B, H, W, D, 3)\n\n    return eq_grid_pts", "\n\nclass GroupPoints_grid(torch.nn.Module):\n    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n                 spacing_source=0, spacing_target=0):\n        super(GroupPoints_grid, self).__init__()\n\n        \"\"\"\n        Group points and different scales for pooling\n        \"\"\"\n        self.radius = radius\n        self.radius_target = radius_target\n        self.patch_size_source = patch_size_source\n        self.patch_size_target = patch_size_target\n        self.spacing_source = spacing_source\n        self.spacing_target = spacing_target\n        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n        # self.sphere_sampling = torch.cat([self.sphere_sampling, self.sphere_sampling*2, self.sphere_sampling*3], 0)\n\n    def forward(self, x):\n        \"\"\"\n        source, target - B, H, W, D, 3\n\n        :param x: [source, target]\n        :return: [patches_idx_source, num_incident_points_target]\n        Returns:\n            source patches - B, N, K, 3\n            patches idx source - B, N, K, 2\n            patches size source - B, N\n            patches radius source - B, 1, 1\n            patches dist source - B, N, K\n        \"\"\"\n        assert isinstance(x, dict)\n        source = x[\"source points\"]\n        target = x[\"target points\"]\n\n        B, N_s, _ = source.shape\n        B, N_t, _ = target.shape\n\n        H_s = int(np.cbrt(N_s))\n        H_t = int(np.cbrt(N_t))\n        source = source.reshape(B, H_s, H_s, H_s, 3)\n        target = target.reshape(B, H_t, H_t, H_t, 3)\n        self.sphere_sampling = self.sphere_sampling.type_as(source)\n\n        source_mask = None\n        if \"source mask\" in x:\n            source_mask = x[\"source mask\"]\n\n        target_mask = None\n        if \"target mask\" in x:\n            target_mask = x[\"target mask\"]\n\n        num_points_source = source.shape[1]\n\n        # assert (num_points_source >= self.patch_size_source)\n        if self.patch_size_target is not None:\n            num_points_target = target.shape[1]\n            # assert (num_points_target >= self.patch_size_source)\n\n        # Compute spheres of radius around each point\n        grid_target = get_xyz_grid(target[..., 0].unsqueeze(1))\n        # grid_target = get_xyz_grid(target) # B, H, W, D, 3\n        grid_target = grid_target.unsqueeze(-2).type_as(source) # B, H, W, D, 1, 3\n        B, H, W, D, _, _ = grid_target.shape\n        sphere_samples = self.sphere_sampling.reshape(1, 1, 1, 1, -1, 3).type_as(source)\n        sampling_grid = grid_target + sphere_samples / (grid_target.shape[1] + 1e-8) * self.radius # B, H, W, D, patch_size, 3\n        sampling_grid = sampling_grid.permute((0, -1, 1, 2, 3, 4))\n        sampling_grid = sampling_grid.reshape(B, 3, H, W, -1)\n        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 1))\n        # sampling_grid = sampling_grid.reshape(B, H, W, -1, 3)\n\n        patches = torch.nn.functional.grid_sample(target.transpose(1, -1), sampling_grid, align_corners = True)\n        patches = patches.reshape(B, -1, H, W, D, self.patch_size_source)\n        patches = patches.permute(0, 2, 3, 4, 5, 1)\n        # patches = patches.reshape(B, H, W, D, self.patch_size_source, 3)\n        sampling_grid = sampling_grid.permute((0, 4, 1, 2, 3))\n        sampling_grid = sampling_grid.reshape(B, 3, H, W, D, -1)\n        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 5, 1))\n        y = {}\n        y[\"patches source\"] = patches\n        y[\"patches idx source\"] = sampling_grid\n        patches_dist = torch.sum(torch.square(target.unsqueeze(-2) - patches), axis = -1) # B, H, W, D, patch, 3\n        patches_dist = torch.sqrt(torch.maximum(patches_dist, torch.tensor(0.000000001).type_as(patches_dist)))\n        y[\"patches dist source\"] = patches_dist\n\n        return y", "\n\n\ndef get_gradient_density(x):\n    \n    # x - B, H, W, D\n    \n    B, H, W, D = x.shape\n    \n    data = x.clone()\n    data = data.cpu().detach().numpy()\n    \n    x_grad = torch.from_numpy(np.gradient(data,axis=3,edge_order=2)).to(x.device)\n    y_grad = torch.from_numpy(np.gradient(data,axis=2,edge_order=2)).to(x.device)\n    z_grad = torch.from_numpy(np.gradient(data,axis=1,edge_order=2)).to(x.device)\n    \n   \n    gradient = torch.stack([y_grad, z_grad, x_grad], 1)\n\n    return gradient # B, 3, H, W, D", "\nclass GroupPoints_density(torch.nn.Module):\n    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n                 spacing_source=0, spacing_target=0):\n        super(GroupPoints_density, self).__init__()\n\n        \"\"\"\n        Group points and different scales for pooling\n        \"\"\"\n        #import pdb\n        #pdb.set_trace()\n        self.radius = radius\n        self.radius_target = radius_target\n        self.patch_size_source = patch_size_source \n        self.patch_size_target = patch_size_target #* 3\n        self.spacing_source = spacing_source\n        self.spacing_target = spacing_target\n        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n        \n        # print(self.sphere_sampling.shape)\n\n    def forward(self, x):\n        \"\"\"\n        source, target - B, H, W, D, 3\n\n        :param x: [source, target]\n        :return: [patches_idx_source, num_incident_points_target]\n        Returns:\n            source patches - B, N, K, 3\n            patches idx source - B, N, K, 2\n            patches size source - B, N\n            patches radius source - B, 1, 1\n            patches dist source - B, N, K\n        \"\"\"\n        \n        assert isinstance(x, dict)\n        source = x[\"source points\"]\n        target = x[\"target points\"]\n        source_density = x[\"source density\"]\n        target_density = x[\"target density\"]\n\n        B, N_s, _ = source.shape\n        B, N_t, _ = target.shape\n\n        H_s = int(np.cbrt(N_s))\n        H_t = int(np.cbrt(N_t))\n        source = source.reshape(B, H_s, H_s, H_s, 3)\n        target = target.reshape(B, H_t, H_t, H_t, 3)\n        source_density = source_density.reshape(B, H_s, H_s, H_s, 1)\n        target_density = target_density.reshape(B, H_t, H_t, H_t, 1)\n\n        self.sphere_sampling = self.sphere_sampling.type_as(source)\n        rad = self.radius * torch.ones((B, 1, 1, 1, 1)).type_as(source)\n        \n    \n        # Compute spheres of radius around each point\n        grid_target = get_xyz_grid(target[..., 0].unsqueeze(1))\n        # grid_target = get_xyz_grid(target) # B, H, W, D, 3\n        grid_target = grid_target.unsqueeze(-2).type_as(source) # B, H, W, D, 1, 3\n        B, H, W, D, _, _ = grid_target.shape\n        sphere_samples = self.sphere_sampling.reshape(1, 1, 1, 1, -1, 3).type_as(source)# / (rad.squeeze() + 1e-8)\n        sampling_grid = grid_target + sphere_samples / (grid_target.shape[1] + 1e-8) / (rad.unsqueeze(-2) + 1e-6) # B, H, W, D, patch_size, 3\n        sampling_grid = sampling_grid.permute((0, -1, 1, 2, 3, 4))\n        sampling_grid = sampling_grid.reshape(B, 3, H, W, -1)\n        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 1))\n        # sampling_grid = sampling_grid.reshape(B, H, W, -1, 3)\n\n        source = source / (rad + 1e-6)\n        target = target / (rad + 1e-6)\n\n        patches = torch.nn.functional.grid_sample(source.transpose(1, -1), sampling_grid, align_corners = True)\n        patches_density = torch.nn.functional.grid_sample(source_density.transpose(1, -1), sampling_grid, align_corners = True)\n        # print(patches_density.shape)\n        patches_density = patches_density.reshape(B, 1, H, W, D, self.patch_size_source)\n        patches_density = patches_density.permute(0, 2, 3, 4, 5, 1) # B, H, W, D, K, 1\n        patches = patches.reshape(B, -1, H, W, D, self.patch_size_source) \n        patches = patches.permute(0, 2, 3, 4, 5, 1) # B, H, W, D, K, 3\n        _,_,_,_,d,c = patches.shape\n        patches_flat = patches.reshape(1,-1,d,c)\n        patches = patches - target.unsqueeze(-2) # B, H, W, D, K, 3\n        # patches_density = torch.abs(patches_density - target_density.unsqueeze(-2)) # B, H, W, D, K, 1\n        patches_density = torch.sqrt( 1e-8 + (patches_density * target_density.unsqueeze(-2))) # B, H, W, D, K, 1\n        # patches_density = patches_density # B, H, W, D, K, 1\n        \n        # patches = patches.reshape(B, H, W, D, self.patch_size_source, 3)\n        sampling_grid = sampling_grid.permute((0, 4, 1, 2, 3))\n        sampling_grid = sampling_grid.reshape(B, 3, H, W, D, -1)\n        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 5, 1))\n        patches_dist = torch.sum(torch.square(patches), axis = -1) # B, H, W, D, patch, 3\n        patches_dist = torch.sqrt(torch.maximum(patches_dist, torch.tensor(0.000000001).type_as(patches_dist)))\n\n        mask = torch.lt(rad.type_as(patches_dist) ** 2, (patches_dist * rad)**2)\n        # print(mask.shape)\n\n        # sampling_grid[mask, :] = -10\n        # Reject where mask is 1\n        patches[mask, :] = 0\n        patches_density[mask, :] = 0\n        \n        y = {}\n        y[\"patches source\"] = patches\n        y[\"patches source density\"] = patches_density # B, H, W, D, K, 1\n        y[\"patches idx source\"] = sampling_grid\n        # patches_dist = patches_dist #/ (rad + 1e-6)\n        y[\"patches dist source\"] = patches_dist\n        y[\"patches mask\"] = mask\n        y[\"patches size\"] = torch.sum( 1 - 1*mask, -1)\n\n        return y", "\n\ndef rotate_density(rotation, density_field, affine = True):\n    \"\"\"\n    rotation - B, 3, 3\n    density_field - B, H, W, D or B, C, H, W, D\n    \"\"\"\n    if len(density_field.shape) == 4:\n        out = density_field.unsqueeze(1)\n    else:\n        out = density_field\n\n    rotation = rotation.type_as(density_field)\n    t = torch.tensor([0, 0, 0]).unsqueeze(0).unsqueeze(2).repeat(rotation.shape[0], 1, 1).type_as(density_field)\n    theta = torch.cat([rotation, t], dim = -1)\n    if affine == True:\n        rot_grid = torch.nn.functional.affine_grid(theta, out.shape, align_corners = True)\n    else:\n        x = torch.linspace(-1, 1, density_field.shape[2]).type_as(density_field)\n        grid = torch.stack(torch.meshgrid(x, x, x), axis = -1).unsqueeze(0).repeat(out.shape[0], 1, 1, 1, 1) \n        # print(grid.shape, rotation.shape)\n\n        rot_grid = torch.einsum(\"bij, bhwdj-> bhwdi\", rotation, grid)\n    #print(rot_grid)\n    rotated_grid = torch.nn.functional.grid_sample(out, rot_grid, align_corners = True, mode=\"nearest\")#, padding_mode = \"border\")\n\n    if len(density_field.shape) == 4:\n        rotated_grid = rotated_grid.squeeze(1)\n        \n    return rotated_grid", "\ndef patches_radius(radius, sq_norm):\n    batch_size = sq_norm.shape[0]\n    rad = radius\n    if isinstance(radius, float):\n        rad = radius * torch.ones((batch_size, 1, 1))\n    if isinstance(radius, str):\n        rad = torch.sqrt(torch.maximum(torch.max(sq_norm, dim=2, keepdims=False), torch.tensor(0.0000001).type_as(sq_norm)))\n        if radius == \"avg\":\n            rad = torch.mean(rad, dim=-1, keepdims=False)\n        elif radius == 'min':\n            rad = torch.min(rad, dim=-1, keepdims=False)\n        elif radius.isnumeric():\n            rad = torch.sort(rad, dim=-1)\n            i = int((float(int(radius)) / 100.) * sq_norm.shape[1])\n            i = max(i, 1)\n            rad = torch.mean(rad[:, :i], dim=-1, keepdims=False)\n        rad = torch.reshape(rad, (batch_size, 1, 1))\n    return rad", "\n\ndef gather_idx(x, idx):\n\n\n    \"\"\"\n    x - B, N, 3\n    idx - B, N, K, 2/3\n\n    out - B, N, K, 3\n    \"\"\"\n    num_idx = idx.shape[-1]\n    \n    if idx.shape[-1] == 3:\n        if len(x.shape) == 3:\n            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n            return out\n\n    if len(x.shape) == 2:\n        out = x[idx[..., 0], idx[..., 1]]\n        out[idx[..., 1] < 0] = 0\n    else:\n        out = x[idx[..., 0], idx[..., 1], :]\n        out[idx[..., 1] < 0, :] = 0\n\n    # print(idx[..., 1].shape, out.shape)\n\n    return out", "    \n    \ndef gather_idx_density(x, idx):\n\n\n    \"\"\"\n    x - B, N, 3\n    idx - B, N, K, 2/3\n\n    out - B, N, K, 3\n    \"\"\"\n    num_idx = idx.shape[-1]\n    \n    if idx.shape[-1] == 3:\n        if len(x.shape) == 3:\n            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n            return out\n\n    if len(x.shape) == 2:\n        out = x[idx[..., 0], idx[..., 1]]\n        out[idx[..., 1] < 0] = 0\n    else:\n        out = x[idx[..., 0], idx[..., 1], :]\n        out[idx[..., 1] < 0, :] = 0\n\n    # print(idx[..., 1].shape, out.shape)\n\n    return out", "\ndef compute_patches(source, target, sq_distance_mat, sq_distance_mat_sel, num_samples, spacing, radius, source_mask=None,source_density=None):\n    \n    batch_size = source.shape[0]\n    num_points_source = source.shape[1]\n    num_points_target = target.shape[1]\n    assert (num_samples * (spacing + 1) <= num_points_source)\n    \n    sq_patches_dist, patches_idx = torch.topk(-sq_distance_mat, k=num_samples * (spacing + 1))\n    \n    #B,N,_  = sq_distance_mat.shape\n    \n    #batch_idx         =  torch.arange(0,B).repeat(patches_idx.shape[-1],1).T.unsqueeze(1).repeat(1,N,1).to(torch.int64)\n    #point_idx         = torch.arange(0,N).reshape(N,1).repeat(1,patches_idx.shape[-1]).unsqueeze(0).repeat(B,1,1).to(torch.int64)\n    #sq_patches_dist   = sq_distance_mat[batch_idx,point_idx,patches_idx]\n    \n    sq_patches_dist = -sq_patches_dist\n    if spacing > 0:\n        sq_patches_dist = sq_patches_dist[:, :, 0::(spacing + 1), ...]\n        patches_idx = patches_idx[:, :, 0::(spacing + 1), ...]\n\n    rad = patches_radius(radius, sq_patches_dist).type_as(sq_distance_mat)\n    patches_size = patches_idx.shape[-1]\n\n    # mask = sq_patches_dist < radius ** 2\n    mask = torch.greater_equal(rad.type_as(sq_distance_mat) ** 2, sq_patches_dist)\n    patches_idx = (torch.where(mask, patches_idx, torch.tensor(-1).type_as(patches_idx))).to(torch.int64)\n    if source_mask is not None:\n        source_mask = source_mask < 1\n        source_mask = source_mask.unsqueeze(-1).repeat(1, 1, patches_idx.shape[-1])\n        patches_idx = torch.where(source_mask, patches_idx, torch.tensor(-1).type_as(patches_idx))\n\n    batch_idx = torch.arange(batch_size).type_as(patches_idx)\n    batch_idx = torch.reshape(batch_idx, (batch_size, 1, 1))\n    batch_idx = batch_idx.repeat(1, num_points_target, num_samples)\n    patches_idx = torch.stack([batch_idx, patches_idx], dim = -1).to(torch.long)\n\n    source = (source / (rad + 1e-6))\n    target = (target / (rad + 1e-6))\n    \n    # patches = source[batch_idx.to(torch.long), patches_idx.to(torch.long)]\n    patches = gather_idx(source, patches_idx)\n    \n    b,n,c,_ = patches.shape\n    #density = torch.ones(b,n,c,1)\n    density = gather_idx_density(source_density, patches_idx)\n    # patches = source[patches_idx[..., 0], patches_idx[..., 1], :]\n    # print(patches.shape, \"patch\")\n    patches = patches - target.unsqueeze(-2)\n    \n    \n    \n\n\n    \n\n    if source_mask is not None:\n        mask = source_mask\n    else:\n        mask = torch.ones((batch_size, num_points_source)).type_as(patches)\n    \n    patch_size = gather_idx(mask, patches_idx.to(torch.long))\n    # patch_size = mask[patches_idx[..., 0], patches_idx[..., 1]]\n    patches_size = torch.sum(patch_size, dim=-1, keepdims=False)\n    patches_dist = torch.sqrt(torch.maximum(sq_patches_dist, torch.tensor(0.000000001).type_as(sq_patches_dist)))\n    patches_dist = patches_dist / (rad + 1e-6)\n    \n    mask_cnt     =  torch.count_nonzero(patches_idx[..., 1] <0,dim=-1).unsqueeze(-1)\n    weight_density= torch.sum(density,dim=-2) / mask_cnt\n    \n    return {\"patches\": patches, \"patches idx\": patches_idx, \"patches size\": patches_size, \"patches radius\": rad,\n            \"patches dist\": patches_dist, \"source_density\":density, \"weight_density\":weight_density}", "\n\n\ndef create_mask(density):\n    \n    shape_ = density.shape\n    \n    B,H,W,D = density.shape\n    mask_density_list = []\n    for i in range(shape_[0]):\n        \n        density_int = density[i].cpu().numpy().reshape(-1,1)\n        mask_density_int = np.ones_like(density_int) * -1\n        \n        model = KMeans(init=\"k-means++\",n_clusters=2)\n        model.fit(density_int)\n        label = model.predict(density_int)\n        clusters = np.unique(label)\n        \n        if np.mean(density_int[np.where(label == 1)[0]]) > np.mean(density_int[np.where(label == 0)[0]]):\n            fg_idx = np.where(label == 1)[0]\n            bg_idx = np.where(label == 0)[0]\n\n\n        else:\n            fg_idx = np.where(label == 0)[0]\n            bg_idx = np.where(label == 1)[0]\n\n        mask_density_int[fg_idx] = 1.\n        mask_density_int[bg_idx] = 0.\n            \n        mask_density_tensor = torch.from_numpy(mask_density_int)\n        mask_density_list.append(mask_density_tensor)\n    mask_density = torch.stack(mask_density_list).reshape(shape_)\n    return mask_density ", "\n\nclass GroupPoints_euclidean_density(torch.nn.Module):\n    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n                 spacing_source=0, spacing_target=0):\n        super(GroupPoints_euclidean_density, self).__init__()\n\n        \"\"\"\n        Group points and different scales for pooling\n        \"\"\"\n        \n        self.radius = radius\n        self.radius_target = radius_target\n        self.patch_size_source = patch_size_source \n        self.patch_size_target = patch_size_target #* 3\n        self.spacing_source = spacing_source\n        self.spacing_target = spacing_target\n        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n        #self.sphere_sampling = torch.cat([self.sphere_sampling, self.sphere_sampling*2, self.sphere_sampling*3], 0)\n        \n\n    def forward(self, x):\n        \"\"\"\n        source, target - B, H, W, D, 3\n\n        :param x: [source, target]\n        :return: [patches_idx_source, num_incident_points_target]\n        Returns:\n            source patches - B, N, K, 3\n            patches idx source - B, N, K, 2\n            patches size source - B, N\n            patches radius source - B, 1, 1\n            patches dist source - B, N, K\n        \"\"\"\n        \n        assert isinstance(x, dict)\n        source = x[\"source points\"]\n        target = x[\"target points\"]\n        source_density = x[\"source density\"]\n        target_density = x[\"target density\"]\n        source_mask = None\n        if \"source mask\" in x:\n            source_mask = x[\"source mask\"]\n        #source =  (target.unsqueeze(-2) + self.sphere_sampling).reshape(1,-1,3).to(torch.float64)\n        #source = self.sphere_sampling.unsqueeze(0).to(torch.float64)\n        B, N_s, _ = source.shape\n        B, N_t, _ = target.shape\n        num_points_source = N_s\n        \n        # compute distance mat\n        r0 = target * target\n        r0 = torch.sum(r0, dim=2, keepdims=True)\n        r1 = (source * source)\n        r1 = torch.sum(r1, dim=2, keepdims=True)\n        r1 = r1.permute(0, 2, 1)\n        sq_distance_mat = r0 - 2. * (target @ source.permute(0, 2, 1)) + r1\n        sq_distance_mat_sel = sq_distance_mat/((target_density @ source_density.permute(0,2,1)) + 1e-8)\n        \n        # Returns\n        \n        '''\n        mask_density = torch.ones_like(source_density)\n        mask_density[source_density == 0]= 1e10\n        mask_density = mask_density.squeeze(-1)\n        mask_denisty = mask_density.unsqueeze(-2)\n        sq_distance_mat = sq_distance_mat * mask_density.unsqueeze(-2)'''\n        \n        #index = torch.where(source_density.squeeze(-1) == 0)\n        #sq_distance_mat[index[0],:,index[1]] = 1e10\n        \n        patches = compute_patches(source, target, sq_distance_mat,sq_distance_mat_sel,\n                                   min(self.patch_size_source, num_points_source),\n                                   self.spacing_source, self.radius,\n                                   source_mask=source_mask,source_density=source_density)\n        \n        \n        H_t = int(np.cbrt(N_t))\n        \n        y = {}\n        # concatinate points and density\n        B,N,K,_ = patches[\"patches\"].shape\n        y[\"patches source\"] = patches[\"patches\"].reshape(B,H_t,H_t,H_t,K,-1) # B, N, K, 3\n        #y[\"patches source density\"] = patches[\"source_density\"].reshape(B,H_t,H_t,H_t,K,-1)\n        y[\"patches idx source\"] = patches[\"patches idx\"].reshape(B,H_t,H_t,H_t,K,-1)\n        y[\"patches size source\"] = patches[\"patches size\"]\n        y[\"patches radius source\"] = patches[\"patches radius\"]\n        y[\"patches dist source\"] = patches[\"patches dist\"].reshape(B,H_t,H_t,H_t,K)\n        y[\"weighted density\"]    = patches[\"weight_density\"]\n        return y", "\nif __name__==\"__main__\":\n\n\n    # gi = GroupPoints_density(0.4, 32)\n    # in_dict = {}\n    # x = torch.randn(2, 32 *32 *32, 3).cuda()\n    # y = torch.randn(2, 16 *16 *16, 3).cuda()\n    # x_d = torch.randn(2, 32 *32 *32, 1).cuda()\n    # y_d = torch.randn(2, 16 *16 *16, 1).cuda()\n    # in_dict[\"source points\"] = x\n    # in_dict[\"target points\"] = y\n    # in_dict[\"source density\"] = x_d\n    # in_dict[\"target density\"] = y_d\n\n    # out = gi(in_dict)\n    # for key in out:\n    #     print(key, \" \", out[key].shape)\n\n    #x = torch.randn(2, 32, 32, 32).cuda()\n    # out = get_gradient_density(x)\n    #out = rotate_density(torch.eye(3).unsqueeze(0).repeat(2, 1, 1), x)\n    #out_2 = rotate_density(torch.eye(3).unsqueeze(0).repeat(2, 1, 1), x, affine = False)\n    # print(out - out_2)\n    # print(torch.norm(out, dim = 1).shape)\n    gi = GroupPoints_density(0.4, 32)\n    in_dict = {}\n    x = torch.randn(2, 32 *32 *32, 3)#.cuda()\n    y = torch.randn(2, 16 *16 *16, 3)#.cuda()\n    x_d = torch.randn(2, 32 *32 *32, 1)#.cuda()\n    y_d = torch.randn(2, 16 *16 *16, 1)#.cuda()\n    in_dict[\"source points\"] = x\n    in_dict[\"target points\"] = y\n    in_dict[\"source density\"] = x_d\n    in_dict[\"target density\"] = y_d\n\n    out = gi(in_dict)\n    for key in out:\n        print(key, \" \", out[key].shape)", "    # # x = (torch.ones((2, 1024, 3)) * torch.reshape(torch.arange(1024), (1, -1, 1))).cuda()\n    # # x = torch.randn((2, 1024, 3)).cuda()\n    # filename = \"/home/rahul/research/data/sapien_processed/train_refrigerator.h5\"\n    # f = h5py.File(filename, \"r\")\n    # x = torch.from_numpy(f[\"data\"][:2]).cuda()\n    # # x2 = torch.from_numpy(f[\"data\"][2:4]).cuda()\n    # y, kd, kd_2 = kdtree_indexing(x, return_idx = True)\n    # print(x, x.shape, y, y.shape)\n\n    # print(kd, kd.sha", "\n    # print(kd, kd.sha\n"]}
{"filename": "cafi_net/utils/group_points.py", "chunked_list": ["import torch\nimport numpy as np\n# from utils.pointcloud_utils import get_xyz_grid\n\n\n\ndef patches_radius(radius, sq_norm):\n    batch_size = sq_norm.shape[0]\n    rad = radius\n    if isinstance(radius, float):\n        rad = radius * torch.ones((batch_size, 1, 1))\n    if isinstance(radius, str):\n        rad = torch.sqrt(torch.maximum(torch.max(sq_norm, dim=2, keepdims=False), torch.tensor(0.0000001).type_as(sq_norm)))\n        if radius == \"avg\":\n            rad = torch.mean(rad, dim=-1, keepdims=False)\n        elif radius == 'min':\n            rad = torch.min(rad, dim=-1, keepdims=False)\n        elif radius.isnumeric():\n            rad = torch.sort(rad, dim=-1)\n            i = int((float(int(radius)) / 100.) * sq_norm.shape[1])\n            i = max(i, 1)\n            rad = torch.mean(rad[:, :i], dim=-1, keepdims=False)\n        rad = torch.reshape(rad, (batch_size, 1, 1))\n    return rad", "\n\ndef gather_idx(x, idx):\n\n\n    \"\"\"\n    x - B, N, 3\n    idx - B, N, K, 2/3\n\n    out - B, N, K, 3\n    \"\"\"\n    num_idx = idx.shape[-1]\n    \n    if idx.shape[-1] == 3:\n        if len(x.shape) == 3:\n            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n            return out\n\n    if len(x.shape) == 2:\n        out = x[idx[..., 0], idx[..., 1]]\n        out[idx[..., 1] < 0] = 0\n    else:\n        out = x[idx[..., 0], idx[..., 1], :]\n        out[idx[..., 1] < 0, :] = 0\n\n    # print(idx[..., 1].shape, out.shape)\n\n    return out", "\n\ndef compute_patches_(source, target, sq_distance_mat, num_samples, spacing, radius, source_mask=None):\n    batch_size = source.shape[0]\n    num_points_source = source.shape[1]\n    num_points_target = target.shape[1]\n    assert (num_samples * (spacing + 1) <= num_points_source)\n\n    sq_patches_dist, patches_idx = torch.topk(-sq_distance_mat, k=num_samples * (spacing + 1))\n    sq_patches_dist = -sq_patches_dist\n    if spacing > 0:\n        sq_patches_dist = sq_patches_dist[:, :, 0::(spacing + 1), ...]\n        patches_idx = patches_idx[:, :, 0::(spacing + 1), ...]\n\n    rad = patches_radius(radius, sq_patches_dist).type_as(sq_distance_mat)\n    patches_size = patches_idx.shape[-1]\n\n    # mask = sq_patches_dist < radius ** 2\n    mask = torch.greater_equal(rad.type_as(sq_distance_mat) ** 2, sq_patches_dist)\n    patches_idx = (torch.where(mask, patches_idx, torch.tensor(-1).type_as(patches_idx))).to(torch.int64)\n    if source_mask is not None:\n        source_mask = source_mask < 1\n        source_mask = source_mask.unsqueeze(-1).repeat(1, 1, patches_idx.shape[-1])\n        patches_idx = torch.where(source_mask, patches_idx, torch.tensor(-1).type_as(patches_idx))\n\n    batch_idx = torch.arange(batch_size).type_as(patches_idx)\n    batch_idx = torch.reshape(batch_idx, (batch_size, 1, 1))\n    batch_idx = batch_idx.repeat(1, num_points_target, num_samples)\n    patches_idx = torch.stack([batch_idx, patches_idx], dim = -1).to(torch.long)\n\n    source = (source / (rad + 1e-6))\n    target = (target / (rad + 1e-6))\n    \n    # patches = source[batch_idx.to(torch.long), patches_idx.to(torch.long)]\n    patches = gather_idx(source, patches_idx)\n    # patches = source[patches_idx[..., 0], patches_idx[..., 1], :]\n    # print(patches.shape, \"patch\")\n    patches = patches - target.unsqueeze(-2)\n\n\n    if source_mask is not None:\n        mask = source_mask\n    else:\n        mask = torch.ones((batch_size, num_points_source)).type_as(patches)\n    \n    patch_size = gather_idx(mask, patches_idx.to(torch.long))\n    # patch_size = mask[patches_idx[..., 0], patches_idx[..., 1]]\n    patches_size = torch.sum(patch_size, dim=-1, keepdims=False)\n    patches_dist = torch.sqrt(torch.maximum(sq_patches_dist, torch.tensor(0.000000001).type_as(sq_patches_dist)))\n    patches_dist = patches_dist / (rad + 1e-6)\n\n    return {\"patches\": patches, \"patches idx\": patches_idx, \"patches size\": patches_size, \"patches radius\": rad,\n            \"patches dist\": patches_dist}", "\nclass GroupPoints(torch.nn.Module):\n    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n                 spacing_source=0, spacing_target=0):\n        super(GroupPoints, self).__init__()\n\n        \"\"\"\n        Group points and different scales for pooling\n        \"\"\"\n        self.radius = radius\n        self.radius_target = radius_target\n        self.patch_size_source = patch_size_source\n        self.patch_size_target = patch_size_target\n        self.spacing_source = spacing_source\n        self.spacing_target = spacing_target\n\n    def forward(self, x):\n        \"\"\"\n        :param x: [source, target]\n        :return: [patches_idx_source, num_incident_points_target]\n        \"\"\"\n        assert isinstance(x, dict)\n        source = x[\"source points\"]\n        target = x[\"target points\"]\n\n        source_mask = None\n        if \"source mask\" in x:\n            source_mask = x[\"source mask\"]\n\n        target_mask = None\n        if \"target mask\" in x:\n            target_mask = x[\"target mask\"]\n\n        num_points_source = source.shape[1]\n\n        # assert (num_points_source >= self.patch_size_source)\n        if self.patch_size_target is not None:\n            num_points_target = target.shape[1]\n            # assert (num_points_target >= self.patch_size_source)\n\n        # compute distance mat\n        r0 = target * target\n        r0 = torch.sum(r0, dim=2, keepdims=True)\n        r1 = (source * source)\n        r1 = torch.sum(r1, dim=2, keepdims=True)\n        r1 = r1.permute(0, 2, 1)\n        sq_distance_mat = r0 - 2. * (target @ source.permute(0, 2, 1)) + r1\n\n        # Returns \n        patches = compute_patches_(source, target, sq_distance_mat,\n                                   min(self.patch_size_source, num_points_source),\n                                   self.spacing_source, self.radius,\n                                   source_mask=source_mask)\n        # print(patches[\"patches\"].shape)\n        y = dict()\n        y[\"patches source\"] = patches[\"patches\"] # B, N, K, 3\n        y[\"patches idx source\"] = patches[\"patches idx\"]\n        y[\"patches size source\"] = patches[\"patches size\"]\n        y[\"patches radius source\"] = patches[\"patches radius\"]\n        y[\"patches dist source\"] = patches[\"patches dist\"]\n\n        # y = [patches_source, patches_idx_source, patches_size_source]\n        if self.patch_size_target is not None:\n            sq_distance_mat_t = sq_distance_mat.permute(0, 2, 1)\n            patches = compute_patches_(target, source, sq_distance_mat_t,\n                                       min(self.patch_size_target, num_points_target),\n                                       self.spacing_target, self.radius_target,\n                                       source_mask=target_mask)\n            # y += [patches_target, patches_idx_target, patches_size_target]\n\n            y[\"patches target\"] = patches[\"patches\"]\n            y[\"patches idx target\"] = patches[\"patches idx\"]\n            y[\"patches size target\"] = patches[\"patches size\"]\n            y[\"patches radius target\"] = patches[\"patches radius\"]\n            y[\"patches dist target\"] = patches[\"patches dist\"]\n        # y.append(radius)\n\n        return y", "\n\n\nif __name__ == \"__main__\":\n\n\n    gi = GroupPoints(0.2, 14)\n    x = torch.randn(2, 32, 32, 32)\n    gi(x)\n    # N_pts = 10", "    # N_pts = 10\n    # start = 10\n    # x = torch.ones((2, N_pts, 3)) * torch.arange(N_pts).unsqueeze(-1).unsqueeze(0)\n    # y = torch.ones((2, N_pts, 3)) * torch.arange(start, N_pts + start).unsqueeze(-1).unsqueeze(0)\n    # # print(x, y)\n    # gi = GroupPoints(0.2, 32)\n    # out = gi({\"source points\": x, \"target points\": y})\n\n    # for k in out:\n    #     print(out[k], out[k].shape, \" \", k)", "    # for k in out:\n    #     print(out[k], out[k].shape, \" \", k)"]}
{"filename": "cafi_net/evaluators/metrics.py", "chunked_list": ["from canonicalization_metrics import *\nimport torch\nimport h5py\nimport os, sys, argparse\nimport numpy as np\nsys.path.append(\"../\")\nfrom utils.losses import chamfer_distance_l2_batch, l2_distance_batch\n\n\nif __name__==\"__main__\":", "\nif __name__==\"__main__\":\n\n    # Argument parser\n    parser = argparse.ArgumentParser(\n        description=\"Parser for generating frames\")\n    \n    parser.add_argument(\"--path\", type = str, required = True)\n    parser.add_argument(\"--pc_path\", type = str)\n    parser.add_argument(\"--shape_idx\", type=str, default = None)", "    parser.add_argument(\"--pc_path\", type = str)\n    parser.add_argument(\"--shape_idx\", type=str, default = None)\n    parser.add_argument(\"--rot_idx\", type=str, default = None)\n    parser.add_argument(\"--category\", type=str, default=None)\n    parser.add_argument(\"--n_iter\", default=20, type = int)\n    parser.add_argument(\"--device\", default = \"cpu\")\n    \n    \n    args = parser.parse_args()\n    ########################################################################", "    args = parser.parse_args()\n    ########################################################################\n\n\n    AtlasNetClasses = [\"car.h5\"]\n\n    if args.category is not None:\n        print(\"single category\")\n        AtlasNetClasses = [args.category+\".h5\"]\n    else:", "        AtlasNetClasses = [args.category+\".h5\"]\n    else:\n        print(\"multi category\")\n    ma = 0.\n    mb = 0.\n    mc = 0.\n    k = 0.\n    shpe_idx_array = np.asarray([[ 8, 16, 12,  4, 14,  5, 10, 18, 19, 17,  3,  6,  9,  7,  1, 13,  0,  2,15, 11], [10,  1, 15, 13,  2, 19, 18,  7, 11,  5, 12,  6,  4, 17,  3, 16,  8,  0,9, 14],[19, 12, 14,  9,  6,  0,  5, 13, 17, 18,  2, 10, 15,  8,  7, 16,  3, 11,1,  4],[ 3, 19, 18, 14, 10, 12, 11, 13,  8,  2,  0,  7,  4,  6,  1, 15, 16, 17,9,  5],[ 8,  0, 18, 17,  2,  6,  5, 13, 10, 11,  1,  7, 12, 15, 19, 16,  9,  3,14,  4],[ 7, 18,  0, 15, 12,  5, 19,  8,  9, 17, 13, 11,  1,  4, 14, 10,  6, 16,3,  2],[11, 15,  4,  5, 14, 18,  3, 17,  6, 13, 12,  2, 10,  7, 16,  0,  9, 19,1,  8],[14,  4, 13, 11, 17, 16,  7, 10,  3,  8,  2, 18,  0, 19,  6, 12,  5,  1,15,  9],[ 3,  4,  0, 12, 14,  9, 10,  1,  6,  7,  2,  8, 17, 16, 13, 15, 19, 18,5, 11],[17, 18,  7, 16,  3, 13, 15,  0,  1,  2, 14, 11,  8, 19,  6, 12,  9, 10,4,  5]])\n    args.pc_path = args.path\n", "    args.pc_path = args.path\n\n    for i in range(len(AtlasNetClasses)):\n        print(AtlasNetClasses[i])\n        a = class_consistency_metric(AtlasNetClasses[i], args.path,args.pc_path,shapes_idx_path=args.shape_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n        print(\"Category-Level Consistency: \", a)\n        ma += a\n        b = equivariance_metric(AtlasNetClasses[i], args.path,args.pc_path,idx_path=args.rot_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n        print(\"Instance-Level Consistency: \", b)\n        mb += b", "        print(\"Instance-Level Consistency: \", b)\n        mb += b\n        c = class_consistency_metric_new(AtlasNetClasses[i], args.path,args.pc_path,shpe_idx_array,shapes_idx_path=args.shape_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n        print(\"Ground Truth Equivariance Consistency: \", c)\n\tmc = mc + c\n        k += 1.\n"]}
{"filename": "cafi_net/evaluators/canonicalization_metrics.py", "chunked_list": ["import torch\nimport h5py\nimport os, sys\nimport numpy as np\nsys.path.append(\"../\")\nfrom utils.losses import chamfer_distance_l2_batch, l2_distance_batch\nimport open3d as o3d\nfrom pytorch3d.loss import chamfer_distance\nimport open3d as o3d\ndistance_metric = chamfer_distance_l2_batch", "import open3d as o3d\ndistance_metric = chamfer_distance_l2_batch\n\n# distance_metric = l2_distance_batch\n\ndef orient(r):\n    \"\"\"\n    shape = list(r.shape)\n    shape = shape[:-2]\n    _, u, v = tf.linalg.svd(r)\n\n    R = tf.einsum('bij,bkj->bik', u, v)\n\n\n\n    s = tf.stack([tf.ones(shape), tf.ones(shape), tf.sign(tf.linalg.det(R))], axis=-1)\n    # u = tf.einsum('bj,bij->bij', s, u)\n    u = tf.multiply(tf.expand_dims(s, axis=-1), u)\n    # v = tf.multiply(tf.expand_dims(s, axis=1), v)\n    R = tf.einsum('bij,bkj->bik', u, v)\n    \"\"\"\n\n    return r", "\ndef save_h5_(h5_filename, data,data_dtype='float32'):\n    h5_fout = h5py.File(h5_filename,\"w\")\n\n    h5_fout.create_dataset(\n        'data', data=data,\n        compression='gzip', compression_opts=4,\n        dtype=data_dtype)\n    h5_fout.close()\n        \ndef load_h5(h5_filename):\n    f = h5py.File(h5_filename)\n    print(h5_filename)\n    data = f['data'][:]\n    label = f['label'][:]\n    return (data, label)", "        \ndef load_h5(h5_filename):\n    f = h5py.File(h5_filename)\n    print(h5_filename)\n    data = f['data'][:]\n    label = f['label'][:]\n    return (data, label)\n\ndef save_h5(h5_filename, data, normals=None, subsamplings_idx=None, part_label=None,\n            class_label=None, data_dtype='float32', label_dtype='uint8'):\n    h5_fout = h5py.File(h5_filename)\n\n    h5_fout.create_dataset(\n        'data', data=data,\n        compression='gzip', compression_opts=4,\n        dtype=data_dtype)\n\n    if normals is not None:\n        h5_fout.create_dataset(\n            'normal', data=normals,\n            compression='gzip', compression_opts=4,\n            dtype=data_dtype)\n\n    if subsamplings_idx is not None:\n        for i in range(len(subsamplings_idx)):\n            name = 'sub_idx_' + str(subsamplings_idx[i].shape[1])\n            h5_fout.create_dataset(\n                name, data=subsamplings_idx[i],\n                compression='gzip', compression_opts=1,\n                dtype='int32')\n\n    if part_label is not None:\n        h5_fout.create_dataset(\n            'pid', data=part_label,\n            compression='gzip', compression_opts=1,\n            dtype=label_dtype)\n\n    if class_label is not None:\n        h5_fout.create_dataset(\n            'label', data=class_label,\n            compression='gzip', compression_opts=1,\n            dtype=label_dtype)\n    h5_fout.close()", "def save_h5(h5_filename, data, normals=None, subsamplings_idx=None, part_label=None,\n            class_label=None, data_dtype='float32', label_dtype='uint8'):\n    h5_fout = h5py.File(h5_filename)\n\n    h5_fout.create_dataset(\n        'data', data=data,\n        compression='gzip', compression_opts=4,\n        dtype=data_dtype)\n\n    if normals is not None:\n        h5_fout.create_dataset(\n            'normal', data=normals,\n            compression='gzip', compression_opts=4,\n            dtype=data_dtype)\n\n    if subsamplings_idx is not None:\n        for i in range(len(subsamplings_idx)):\n            name = 'sub_idx_' + str(subsamplings_idx[i].shape[1])\n            h5_fout.create_dataset(\n                name, data=subsamplings_idx[i],\n                compression='gzip', compression_opts=1,\n                dtype='int32')\n\n    if part_label is not None:\n        h5_fout.create_dataset(\n            'pid', data=part_label,\n            compression='gzip', compression_opts=1,\n            dtype=label_dtype)\n\n    if class_label is not None:\n        h5_fout.create_dataset(\n            'label', data=class_label,\n            compression='gzip', compression_opts=1,\n            dtype=label_dtype)\n    h5_fout.close()", "\ndef torch_random_rotation(shape):\n    if isinstance(shape, int):\n        shape = [shape]\n\n    batch_size = shape[0]\n    t = torch.random(shape + [3])\n    c1 = torch.cos(2 * np.pi * t[:, 0])\n    s1 = torch.sin(2 * np.pi * t[:, 0])\n\n    c2 = torch.cos(2 * np.pi * t[:, 1])\n    s2 = torch.sin(2 * np.pi * t[:, 1])\n\n    z = torch.zeros(shape)\n    o = torch.ones(shape)\n\n    R = torch.stack([c1, s1, z, -s1, c1, z, z, z, o], dim=-1)\n    R = torch.reshape(R, shape + [3, 3])\n\n    v1 = torch.sqrt(t[:, -1])\n    v3 = torch.sqrt(1-t[:, -1])\n    v = torch.stack([c2 * v1, s2 * v1, v3], dim=-1)\n    H = torch.tile(torch.unsqueeze(torch.eye(3), 0), (batch_size, 1, 1)) - 2.* torch.einsum('bi,bj->bij', v, v)\n    M = -torch.einsum('bij,bjk->bik', H, R)\n    return M", "\n\ndef batch_of_frames(n_frames, filename, path):\n\n    I = torch.unsqueeze(torch.eye(3),0)\n    R = torch_random_rotation(n_frames - 1)\n\n    R = torch.cat([I, R], dim=0)\n    print(\"R shape\")\n    print(R.shape)\n    print(R)\n\n    h5_fout = h5py.File(os.path.join(path, filename), 'w')\n    h5_fout.create_dataset(\n        'data', data=R,\n        compression='gzip', compression_opts=4,\n        dtype='float32')\n    h5_fout.close()", "\n\n# batch_of_frames(n_frames=128, filename=\"rotations.h5\", path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\")\n\n\n\nAtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\ndef save_rotation(h5_filename, src_path, tar_path, rots_per_shape=512, batch_size=512):\n    filename = os.path.join(src_path, h5_filename)\n    f = h5py.File(filename)\n    print(filename)\n    data = f['data'][:]\n    num_shapes = data.shape[0]\n    num_batches = num_shapes // batch_size\n    residual = num_shapes % batch_size\n    R = []\n\n    \"\"\"\n    if num_batches == 0:\n        batch = tf_random_rotation(rots_per_shape * num_shapes)\n        batch = tf.reshape(batch, (-1, rots_per_shape, 3, 3))\n        R.append(np.asarray(batch, dtype=np.float))\n    \"\"\"\n\n    for i in range(num_batches):\n        a = i*batch_size\n        b = min((i+1)*batch_size, num_shapes)\n        if a < b:\n            batch = torch_random_rotation((b - a)*rots_per_shape)\n            batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n            batch = np.asarray(batch, dtype=np.float)\n            R.append(batch)\n\n\n    if residual > 0:\n        batch = torch_random_rotation(residual * rots_per_shape)\n        batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n        batch = np.asarray(batch, dtype=np.float)\n        R.append(batch)\n\n\n\n    # R = tf.concat(R, axis=0)\n    R = np.concatenate(R, axis=0)\n    print(data.shape)\n    print(R.shape)\n\n    # R = np.asarray(R, dtype=np.float)\n\n\n    h5_fout = h5py.File(os.path.join(tar_path, h5_filename), 'w')\n\n    h5_fout.create_dataset(\n        'data', data=R,\n        compression='gzip', compression_opts=4,\n        dtype='float32')\n    h5_fout.close()", "def save_rotation(h5_filename, src_path, tar_path, rots_per_shape=512, batch_size=512):\n    filename = os.path.join(src_path, h5_filename)\n    f = h5py.File(filename)\n    print(filename)\n    data = f['data'][:]\n    num_shapes = data.shape[0]\n    num_batches = num_shapes // batch_size\n    residual = num_shapes % batch_size\n    R = []\n\n    \"\"\"\n    if num_batches == 0:\n        batch = tf_random_rotation(rots_per_shape * num_shapes)\n        batch = tf.reshape(batch, (-1, rots_per_shape, 3, 3))\n        R.append(np.asarray(batch, dtype=np.float))\n    \"\"\"\n\n    for i in range(num_batches):\n        a = i*batch_size\n        b = min((i+1)*batch_size, num_shapes)\n        if a < b:\n            batch = torch_random_rotation((b - a)*rots_per_shape)\n            batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n            batch = np.asarray(batch, dtype=np.float)\n            R.append(batch)\n\n\n    if residual > 0:\n        batch = torch_random_rotation(residual * rots_per_shape)\n        batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n        batch = np.asarray(batch, dtype=np.float)\n        R.append(batch)\n\n\n\n    # R = tf.concat(R, axis=0)\n    R = np.concatenate(R, axis=0)\n    print(data.shape)\n    print(R.shape)\n\n    # R = np.asarray(R, dtype=np.float)\n\n\n    h5_fout = h5py.File(os.path.join(tar_path, h5_filename), 'w')\n\n    h5_fout.create_dataset(\n        'data', data=R,\n        compression='gzip', compression_opts=4,\n        dtype='float32')\n    h5_fout.close()", "\n\n\"\"\"\nAtlasNetPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\"\n\nfor name in AtlasNetClasses:\n    save_rotation(name, os.path.join(AtlasNetPath, 'valid'), os.path.join(AtlasNetPath, 'rotations_valid'))\n    save_rotation(name, os.path.join(AtlasNetPath, 'train'), os.path.join(AtlasNetPath, 'rotations_train'))\n\n", "\n\nexit(666)\n\"\"\"\n\ndef mean(x, batch_size=512):\n    num_shapes = x.shape[0]\n    num_batches = num_shapes // batch_size\n    remainder = num_shapes // batch_size\n    m = []\n    k = 0.\n\n\n    for i in range(num_batches):\n        a = i * batch_size\n        b = min((i + 1) * batch_size, num_shapes)\n        if a < b:\n            k += float(b - a)\n            batch = x[a:b, ...]\n            m.append(torch.sum(batch, dim=0, keepdims=True))\n\n    if remainder > 0:\n        a = num_batches * batch_size\n        b = num_shapes\n        if a < b:\n            k += float(b - a)\n            batch = x[a:b, ...]\n            m.append(torch.sum(batch, dim=0, keepdims=True))\n\n    m = torch.cat(m, dim=0)\n    m = torch.sum(m, dim=0, keepdims=False)\n    m /= k\n    return m", "\ndef var(x, batch_size=512):\n    num_shapes = x.shape[0]\n    num_batches = num_shapes // batch_size\n    remainder = num_shapes // batch_size\n    v = []\n    k = 0.\n    m = torch.unsqueeze(mean(x, batch_size=512), dim=0)\n\n\n    for i in range(num_batches):\n        a = i * batch_size\n        b = min((i + 1) * batch_size, num_shapes)\n        if a < b:\n            k += float(b - a)\n            xi = x[a:b, ...]\n\n            vi = torch.sub(xi, m)\n            vi = vi * vi\n            vi = torch.sum(vi)\n            v.append(vi)\n\n    if remainder > 0:\n        a = num_batches * batch_size\n        b = num_shapes\n        if a < b:\n            k += float(b - a)\n            xi = x[a:b, ...]\n            vi = torch.sub(xi, m)\n            vi = vi * vi\n            vi = torch.sum(vi)\n            v.append(vi)\n\n    v = torch.stack(v, dim=0)\n    v = torch.sum(v)\n    v /= k\n    return v", "\ndef std(x, batch_size=512):\n    return torch.sqrt(var(x, batch_size))\n\ndef sq_dist_mat(x, y):\n    r0 = torch.mul(x, x)\n    r0 = torch.sum(r0, axis=2, keepdims=True)\n\n    r1 = torch.mul(y, y)\n    r1 = torch.sum(r1, axis=2, keepdims=True)\n    r1 = torch.permute(r1,(0, 2, 1))\n\n    sq_distance_mat = r0 - 2. * torch.matmul(x, y.permute(0, 2, 1)) + r1\n    return sq_distance_mat", "\n\ndef var_(x, axis_mean=0, axis_norm=1):\n    mean = torch.mean(x, dim=axis_mean, keepdims=True)\n    y = torch.sub(x, mean)\n    yn = torch.sum(y * y, dim=axis_norm, keepdims=False)\n    yn = torch.mean(yn, dim=axis_mean, keepdims=False)\n    return yn, mean\n\ndef std_(x, axis_mean=0, axis_norm=1):\n    yn, mean = var_(x, axis_mean=axis_mean, axis_norm=axis_norm)\n    return torch.sqrt(yn), mean", "\ndef std_(x, axis_mean=0, axis_norm=1):\n    yn, mean = var_(x, axis_mean=axis_mean, axis_norm=axis_norm)\n    return torch.sqrt(yn), mean\n\n\ndef pca_align(x):\n    c = torch.mean(x, dim=1, keepdims=True)\n    centred_x = torch.sub(x, c)\n    covar_mat = torch.mean(torch.einsum('bvi,bvj->bvij', centred_x, centred_x), dim=1, keepdims=False)\n    _, v = np.linalg.eigh(covar_mat.detach().numpy())\n    v = torch.from_numpy(v)\n    \n    x = torch.einsum('bij,bvi->bvj', v, centred_x)\n    return x, v.permute(0,2,1)", "\ndef visualize_outputs(data, start = 0, max_num = 10, spacing = 2.0, skip = 1):\n    '''\n    Visualize point clouds in open3D \n    '''\n    \n    \n    num_pcds = 10\n    \n    rows = np.floor(np.sqrt(num_pcds))\n    pcd_list = []\n    arrow_list = []\n    pcd_iter = 0\n    pcd_index = 0\n    for _ in range(num_pcds):\n        pts = data[_].cpu().detach().numpy()\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(pts)\n        pcd.colors = pcd.points\n        column_num = pcd_index // rows\n        row_num = pcd_index % rows\n        vector = (row_num * spacing, column_num * spacing, 0)\n        pcd.translate(vector)\n        pcd_list.append(pcd)\n        pcd_index += 1\n\n        \n    \n    o3d.visualization.draw_geometries(pcd_list)", "    \n\n    \n    \ndef save_pca_frames_(filename, x_path, batch_size=20, num_rots=128):\n   \n    \n    obj_name = filename.split('.')[0]\n    x = loadh5(os.path.join(x_path, filename))\n    x_data = x.copy()\n    r = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    x = torch.from_numpy(x)\n    r = torch.from_numpy(r)\n    num_shapes = x.shape[0]\n    x = extend_(x, batch_size)\n    \n    save_directory = x_path + \"pca\"\n    \n    if not os.path.exists(save_directory):\n        os.mkdir(save_directory)\n    # r = extend_(r, batch_size)\n    \n    h5_filename = save_directory + \"/\"+ obj_name +\".h5\"\n    save_h5_(h5_filename,x_data)\n    \n    h5_filename = save_directory + \"/\"+ obj_name +\"_rotations.h5\"\n    save_h5_(h5_filename, r.cpu().detach().numpy())\n    \n    num_batches = x.shape[0] // batch_size\n    R = []\n\n    for i in range(num_batches):\n        print(100.*i/num_batches)\n        xi = x[i * batch_size:(i + 1) * batch_size, ...]\n\n        Ri = []\n        canonical_frame = []\n        data_ = []\n        input_data= []\n        for j in range(num_rots):\n            xij = torch.einsum(\"ij,bvj->bvi\", r[j, ...], xi)\n            # yij = pca_frame(xij)\n            data,frame = pca_align(xij)\n            Ri.append(data)\n            if j%10 == 0:\n                data_.append(data[0])\n                input_data.append(xij[0])\n            \n                \n            canonical_frame.append(frame)\n            \n        \n        #data_ = torch.stack(data_,axis=0)\n        #input_data = torch.stack(input_data,axis=0)\n        #visualize_outputs(data_)  \n        #visualize_outputs(input_data)          \n        Ri = torch.stack(Ri, axis=1)\n        R.append(np.asarray(Ri, dtype=np.float))\n    R = np.concatenate(R, axis=0)\n    R = R[:num_shapes, ...]\n    \n    \n        \n    canonical_frame = torch.stack(canonical_frame,axis=0).permute(1,0,2,3)\n    h5_filename = save_directory + \"/\"+ obj_name +\"_canonical.h5\"\n    save_h5_(h5_filename, canonical_frame.cpu().detach().numpy())\n    \n    \n    '''\n    filename_ = obj_name + \"_pca.h5\"\n    h5_fout = h5py.File(os.path.join(x_path + \"pca\", filename), 'w')\n    h5_fout.create_dataset(\n        'data', data=R,\n        compression='gzip', compression_opts=4,\n        dtype='float32')\n    h5_fout.close()'''", "\n\ndef normalize(x):\n    s, m = std_(x, axis_mean=1, axis_norm=-1)\n    x = torch.div(torch.sub(x, m), s)\n    return x\n\n\ndef orth_procrustes(x, y):\n    x = normalize(x)\n    y = normalize(y)\n    xty = torch.einsum('bvi,bvj->bij', y, x)\n    s, u, v = torch.linalg.svd(xty)\n    r = torch.einsum('bij,bkj->bik', u, v)\n    return r", "def orth_procrustes(x, y):\n    x = normalize(x)\n    y = normalize(y)\n    xty = torch.einsum('bvi,bvj->bij', y, x)\n    s, u, v = torch.linalg.svd(xty)\n    r = torch.einsum('bij,bkj->bik', u, v)\n    return r\n\n\ndef extend_(x, batch_size):\n    last_batch = x.shape[0] % batch_size\n    if last_batch > 0:\n        X_append = []\n        for i in range(batch_size - last_batch):\n            X_append.append(x[i, ...])\n        X_append = torch.stack(X_append, dim=0)\n        y = torch.cat([x, X_append], dim=0)\n    else:\n        y = x\n    return y", "\ndef extend_(x, batch_size):\n    last_batch = x.shape[0] % batch_size\n    if last_batch > 0:\n        X_append = []\n        for i in range(batch_size - last_batch):\n            X_append.append(x[i, ...])\n        X_append = torch.stack(X_append, dim=0)\n        y = torch.cat([x, X_append], dim=0)\n    else:\n        y = x\n    return y", "\ndef xyz2yzx(x):\n    return torch.stack([x[..., 1], x[..., 2], x[..., 0]], dim=-1)\n\ndef yzx2xyz(x):\n    return torch.stack([x[..., 2], x[..., 0], x[..., 1]], dim=-1)\n\ndef yzx2xyzConj(R):\n    R = yzx2xyz(torch.linalg.matrix_transpose(R))\n    R = torch.linalg.matrix_transpose(R)\n    # return xyz2yzx(R)\n    return R", "\n\n\ndef class_consistency_frames(r, r_can, batch_size=32):\n    \n    num_batches = r.shape[0] // batch_size\n    num_shapes = r.shape[0]\n    num_rots = min(r.shape[1], r_can.shape[1])\n    r = r[:, :num_rots, ...]\n    r_can = r_can[:, :num_rots, ...]\n    r = extend_(r, batch_size)\n    r_can = extend_(r_can, batch_size)\n    \n    \n\n\n    R = []\n    for i in range(num_batches):\n        a = i * batch_size\n        b = (i + 1) * batch_size\n        rj = r[a:b, ...]\n        r_can_j = r_can[a:b, ...]\n        # Ri = tf.matmul(r_can_j, rj, transpose_a=True)\n        #  Ri = tf.matmul(r_can_j, rj, transpose_b=True)\n        # Ri = tf.matmul(r_can_j, r_can_j, transpose_b=True)\n        Ri = r_can_j\n        Ri = orient(Ri)\n        # Ri = tf.matmul(r_can_j, rj)\n        # Ri = np.stack(np.asarray(Ri, dtype=np.float32), axis=1)\n        R.append(np.asarray(Ri, dtype=np.float32))\n    R = np.concatenate(R, axis=0)\n    R = R[:num_shapes, ...]\n    # print(R)\n    return R", "\n\n\ndef visualize_method(filename, x_path,):\n    obj_name = filename.split('.')[0]\n    \n    x = loadh5(os.path.join(x_path, filename))\n    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    x = torch.from_numpy(x)\n    r_can = torch.from_numpy(r_can)\n    r_input = torch.from_numpy(r_input)\n    pcds_ = []\n    pcds_1 = []\n    for i in range(x.shape[0]):\n        for j in range(r_input.shape[0]):\n            rj = r_input[j, ...]\n            xij = torch.einsum(\"ij,bvj->bvi\", rj, x)\n            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can[:,j,:]), xij)\n            if j%10 == 0:\n               pcds_.append(y0i[4]) \n               pcds_1.append(xij[4]) \n               \n    input_data = torch.stack(pcds_,axis=0)\n    input_data_1 = torch.stack(pcds_1,axis=0)\n    visualize_outputs(input_data)  \n    visualize_outputs(input_data_1)  ", "\ndef class_consistency_metric_(x, r_input, r_can, val_points,idx=None, batch_size=32):\n    \n    num_rots = r_input.shape[0]\n    n_shapes = x.shape[0]\n    if idx is None:\n        idx = torch.randperm(n_shapes)\n\n\n    r_can_0 = r_can\n    r_can_1 = r_can[idx]\n    x = extend_(x, batch_size)\n    r_can_0 = extend_(r_can_0, batch_size)\n    r_can_1 = extend_(r_can_1, batch_size)\n    num_batches = x.shape[0] // batch_size\n    D = []\n    for j in range(num_rots):\n        rj = r_input[j, ...]\n        d = []\n        d_ = []\n        for i in range(num_batches):\n            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n            \n            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n            \n            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n            points_val =[1024]*batch_size\n            xij = torch.einsum(\"ij,bvj->bvi\", rj, xi)\n            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), xij)\n            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), xij)\n            for _ in range(y0i.shape[0]):\n                d_.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n        \n       \n        #d = np.concatenate(d, axis=0)\n        d = np.asarray(d_)\n        d = d[:n_shapes, ...]\n        \n        D.append(np.mean(d))\n    D = np.stack(D, axis=0)\n    D = np.mean(D)\n    return float(D)", "\ndef class_consistency_metric_new_(x, r_input, r_can,val_points=None, idx=None, batch_size=32):\n    num_rots = r_input.shape[0]\n    n_shapes = x.shape[0]\n    if idx is None:\n        idx = torch.randperm(n_shapes)\n        \n    rot_idx = torch.randperm(num_rots)\n    r_input_0 = r_input\n    r_input_1  = r_input[rot_idx]\n\n\n    r_can_0 = r_can\n    r_can_1 = r_can[idx]\n    r_can_1 = r_can_1[:,rot_idx,...]\n    \n            \n    x = extend_(x, batch_size)\n    r_can_0 = extend_(r_can_0, batch_size)\n    r_can_1 = extend_(r_can_1, batch_size)\n    num_batches = x.shape[0] // batch_size\n    D = []\n    for j in range(num_rots):\n        rj0 = r_input_0[j, ...]\n        rj1 = r_input_1[j, ...]\n        d = []\n        d_ = []\n        for i in range(num_batches):\n            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n            \n            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n            points_val =[1024]*batch_size\n            xij0 = torch.einsum(\"ij,bvj->bvi\", rj0, xi)\n            xij1 = torch.einsum(\"ij,bvj->bvi\", rj1, xi)\n            \n            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), xij0)\n            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), xij1)\n\n        \n            for _ in range(y0i.shape[0]):\n                d_.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n        #d = np.concatenate(d, axis=0)\n        d = np.asarray(d_)\n        d = d[:n_shapes, ...]\n        \n        D.append(np.mean(d))\n    D = np.stack(D, axis=0)\n    D = np.mean(D)\n    return float(D)", "\n\ndef loadh5(path):\n    fx_input = h5py.File(path, 'r')\n    x = fx_input['data'][:]\n    fx_input.close()\n    return x\n    \ndef load_file_names(path,name):\n    filename = glob.glob(os.path.join(path, \"\")  + name)\n    filename.sort()\n    file_list = []\n    for f in filename:\n        print(os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + name, f)\n        file_list.append(f)\n        \n    return file_list", "def load_file_names(path,name):\n    filename = glob.glob(os.path.join(path, \"\")  + name)\n    filename.sort()\n    file_list = []\n    for f in filename:\n        print(os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + name, f)\n        file_list.append(f)\n        \n    return file_list\n    ", "    \n\n\ndef class_consistency_metric_new(filename, x_path, pc_path,shape_idx_array, shapes_idx_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n    \n    \n    obj_name = filename.split('.')[0]\n    \n    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n    val_points = None\n\n    x = torch.from_numpy(x).to(device)\n    x = (x - torch.mean(x , axis=1,keepdim=True))\n    \n    r_can = torch.from_numpy(r_can).to(device)\n    r_input = torch.from_numpy(r_input).to(device)\n    m = 0.\n    \n    if shapes_idx_path is not None:\n        idx = loadh5(os.path.join(shapes_idx_path, filename))\n        idx = torch.from_numpy(idx).to(torch.int64).to(device)\n        n_iter = min(n_iter, idx.shape[0])\n        for i in range(n_iter):\n            m += class_consistency_metric_new_(x, r_input,\n                                           r_can,val_points,idx[i, ...], batch_size)\n    else:\n        idx = None\n        for i in range(n_iter):\n            m += class_consistency_metric_new_(x, r_input, r_can,val_points, idx, batch_size)\n    return m / n_iter", "    \n    \ndef class_consistency_metric(filename, x_path, pc_path,shapes_idx_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n    \n\n    \n    obj_name = filename.split('.')[0]\n    \n    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n    val_points = None\n\n    x = torch.from_numpy(x).to(device)\n    x = (x - torch.mean(x , axis=1,keepdim=True))\n    r_can = torch.from_numpy(r_can).to(device)\n    r_input = torch.from_numpy(r_input).to(device)\n    m = 0.\n    \n    if shapes_idx_path is not None:\n        idx = loadh5(os.path.join(shapes_idx_path, filename))\n        idx = torch.from_numpy(idx).to(torch.int64).to(device)\n        n_iter = min(n_iter, idx.shape[0])\n        for i in range(n_iter):\n            m += class_consistency_metric_(x, r_input,\n                                           r_can,val_points,idx[i, ...], batch_size)\n    else:\n        idx = None\n        for i in range(n_iter):\n            m += class_consistency_metric_(x, r_input, r_can, val_points,idx, batch_size)\n    return m / n_iter", "\n\ndef equivariance_metric_(x, r_input, r_can, val_points=None, batch_size=20, idx=None):\n\n    num_shapes = x.shape[0]\n    num_rots = r_input.shape[0]\n    if idx is None:\n        idx = torch.randperm(num_rots)\n\n    r_can_0   = r_can\n    r_can_1   = r_can[:,idx]\n    r_input_0 = r_input\n    r_input_1 = r_input[idx]\n    \n    x = extend_(x, batch_size)\n    \n    r_can_0 = extend_(r_can_0, batch_size)\n    r_can_1 = extend_(r_can_1, batch_size)\n    # r_input_0 = extend_(r_input_0, batch_size)\n    # r_input_1 = extend_(r_input_1, batch_size)\n\n    num_batches = x.shape[0] // batch_size\n    D = []\n    for i in range(num_batches):\n        d = []\n        d_ = []\n        for j in range(num_rots):\n            r0j = r_input_0[j, ...]\n            r1j = r_input_1[j, ...]\n            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n            \n            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n            x0ij = torch.einsum(\"ij,bvj->bvi\", r0j, xi)\n            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), x0ij)\n            x1ij = torch.einsum(\"ij,bvj->bvi\", r1j, xi)\n            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), x1ij)\n            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n            points_val = [1024]*batch_size\n            d_int = []\n            for _ in range(y0i.shape[0]):\n                d_int.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n            d_.append(d_int)\n        \n        #d = np.stack(d, axis=1)\n        d = np.asarray(d_)\n        d = np.mean(d, axis=1, keepdims=False)\n        D.append(d)\n    D = np.concatenate(D, axis=0)\n    D = D[:num_shapes, ...]\n    D = np.mean(D)\n    return float(D)", "\n\ndef equivariance_metric(filename, x_path, pc_path,batch_size, idx_path=None, n_iter=10, device = \"cpu\"):\n    \n    obj_name = filename.split('.')[0]\n    \n    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n    val_points = None\n    x = torch.from_numpy(x).to(device)\n    x = (x - torch.mean(x , axis=1,keepdim=True))\n    r_can = torch.from_numpy(r_can).to(device)\n    r_input = torch.from_numpy(r_input).to(device)\n    spacing = 1.0\n    '''\n    for _ in range(r_can.shape[1]):\n        #random_rot_pcd = torch.matmul(r_input,x.permute(1,0)).permute(0,2,1)\n        random_rot_pcd = torch.matmul(r_input[0],x.permute(0,2,1)).permute(0,2,1)\n        can_pcd        = torch.matmul(r_can[:,_],random_rot_pcd.permute(0,2,1)).permute(0,2,1)\n        can_pcd_numpy  = can_pcd.detach().numpy()\n        \n        num_pcds = can_pcd_numpy.shape[0]\n        rows = np.floor(np.sqrt(num_pcds))\n        pcd_list = []\n        arrow_list = []\n        pcd_iter = 0\n        pcd_index = 0\n        for k in range(num_pcds):\n            pcd = o3d.geometry.PointCloud()\n            pcd.points = o3d.utility.Vector3dVector(can_pcd_numpy[k])\n            pcd.colors = pcd.points\n            column_num = pcd_index // rows\n            row_num = pcd_index % rows\n            vector = (row_num * spacing, column_num * spacing, 0)\n            pcd.translate(vector)\n            pcd_list.append(pcd)\n            pcd_index += 1\n\n        #o3d.visualization.draw_geometries(pcd_list)'''\n    \n    m = 0.\n    if idx_path is None:\n        idx = None\n        for i in range(n_iter):\n            m += equivariance_metric_(x, r_input, r_can,val_points,batch_size, idx=idx)\n    else:\n        idx = loadh5(idx_path)\n        idx = torch.from_numpy(idx).to(torch.int64)     \n        n_iter = min(n_iter, idx.shape[0])\n        for i in range(n_iter):\n            m += equivariance_metric_(x, r_input, r_can, batch_size, idx=idx[i, ...])\n    return m / n_iter", "\n\ndef class_consistency_umetric_(x, r_input, r_can, val_points=None,idx_shapes=None, idx_rots=None, batch_size=32):\n\n  \n    num_rots = min(r_input.shape[0], r_can.shape[0])\n    n_shapes = x.shape[0]\n    if idx_shapes is None:\n        idx_shapes = torch.randperm(n_shapes)\n    if idx_rots is None:\n        idx_rots = torch.randperm(num_rots)\n    else:\n        idx_rots = idx_rots[:num_rots, ...]\n\n    r_can_0 = r_can\n    r_can_1 = r_can[:,idx_rots]\n    r_can_1 = r_can_1[idx_shapes]\n    r_input_0 = r_input\n    r_input_1 = r_input[idx_rots]\n    x_0 = x\n    x_1 = x[idx_shapes]\n    #val_points_0 = val_points\n    #val_points_1 = val_points[idx_shapes]\n    \n    val_points_0 = [1024]*batch_size\n    val_points_1 = [1024]*batch_size\n    \n    x_0 = extend_(x_0, batch_size)\n    x_1 = extend_(x_1, batch_size)\n    r_can_0 = extend_(r_can_0, batch_size)\n    r_can_1 = extend_(r_can_1, batch_size)\n\n    num_batches = x.shape[0] // batch_size\n    D = []\n    d_ = []\n    for j in range(num_rots):\n        r0j = r_input_0[j, ...]\n        r1j = r_input_1[j, ...]\n        d = []\n        for i in range(num_batches):\n            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n            x0i = x_0[i * batch_size:(i + 1) * batch_size, ...]\n            x1i = x_1[i * batch_size:(i + 1) * batch_size, ...]\n            x0ij = torch.einsum(\"ij,bvj->bvi\", r0j, x0i)\n            x1ij = torch.einsum(\"ij,bvj->bvi\", r1j, x1i)\n            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), x0ij)\n            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), x1ij)\n            d_int = []\n            for _ in range(y0i.shape[0]):\n                d_int.append(float(chamfer_distance(y0i[_][:int(val_points_0[_])].unsqueeze(0),y1i[_][:int(val_points_1[_])].unsqueeze(0))[0]))\n            d_.append(d_int)\n            \n       \n        \n        d = np.asarray(d_)\n        d = d[:n_shapes, ...]\n        D.append(np.mean(d))\n    D = np.stack(D, axis=0)\n    D = np.mean(D)\n    return float(D)", "\ndef class_consistency_umetric(filename, x_path,pc_path, idx_shapes_path=None, idx_rots_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n    \n    obj_name = filename.split('.')[0]\n    \n    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n    val_points = None\n    x = torch.from_numpy(x).to(device)\n    r_can = torch.from_numpy(r_can).to(device)\n    r_input = torch.from_numpy(r_input).to(device)\n    x = (x - torch.mean(x , axis=1,keepdim=True))\n    if idx_shapes_path is not None:\n        idx_shapes = loadh5(os.path.join(idx_shapes_path, filename))\n        idx_shapes = torch.from_numpy(idx_shapes).to(torch.int64)\n        n_iter = min(n_iter, idx_shapes.shape[0])\n    else:\n        idx_shapes = None\n\n    if idx_rots_path is not None:\n        idx_rots = loadh5(idx_rots_path)\n        idx_rots = torch.from_numpy(idx_rots).to(torch.int64)\n        n_iter = min(n_iter, idx_rots.shape[0])\n    else:\n        idx_rots = None\n\n    m = 0.\n    for i in range(n_iter):\n        ri = None\n        si = None\n        if idx_rots is not None:\n            ri = idx_rots[i, ...]\n        if idx_shapes is not None:\n            si = idx_shapes[i, ...]\n\n        m += class_consistency_umetric_(x, r_input, r_can,val_points,\n                                        idx_shapes=si, idx_rots=ri, batch_size=batch_size)\n\n    return m / n_iter", "\n\ndef icp_class_consistency_metric(x, batch_size=32, n_shuffles=10, n_iter=5):\n    \"\"\"\n    :param x: canonicalized shapes (num_shapes, num_points, 3)\n    :param batch_size:\n    :param n_shuffles: number of times we shuffle x for self comparison\n    :param n_iter: number of icp iterations\n    :return:\n    \"\"\"\n    b = x.shape[0]\n    u_ = b % batch_size\n    n = b // batch_size\n    var_ = 0.\n    m = torch.unsqueeze(torch.reshape(torch.eye(3), (9,)), dim=1)\n    for j in range(n_shuffles):\n        idx = np.random.permutation(x.shape[0])\n        y_ = np.take(x, indices=idx, axis=0)\n        k = 0.\n        varj = 0.\n        for i in range(n):\n            k += 1.\n            r = icp(x[i * batch_size:(i + 1) * batch_size, ...], y_[i * batch_size:(i + 1) * batch_size, ...], n_iter=n_iter)\n            r = torch.reshape(r, (r.shape[0], -1))\n            r_m = torch.sub(r, m)\n            r_m = r_m * r_m\n            rn = torch.sum(r_m, dim=-1)\n            varj += float(torch.mean(rn))\n\n        if u_ > 0:\n            k += u_ / float(batch_size)\n            r = icp(x[n * batch_size:, ...], y_[n * batch_size:, ...], n_iter=n_iter)\n            r = torch.reshape(r, (r.shape[0], -1))\n            r_m = torch.sub(r, m)\n            r_m = r_m * r_m\n            rn = torch.sum(r_m, dim=-1)\n            varj += float(torch.mean(rn))\n        varj /= k\n\n    var_ /= float(n_shuffles)\n    return np.sqrt(var_)", "\n\ndef shapes_permutations(filename, src_path, tar_path):\n    x = loadh5(os.path.join(src_path, filename))\n    n_shapes = x.shape[0]\n    idx = torch.randperm(n_shapes)\n    idx = np.asarray(idx, dtype=np.int)\n\n    h5_fout = h5py.File(os.path.join(tar_path, filename), \"w\")\n    h5_fout.create_dataset(\n        'data', data=idx,\n        compression='gzip', compression_opts=1,\n        dtype='uint8')\n    h5_fout.close()", "\ndef rot_permutations(tar_path, num_rots):\n    filename = \"rotations_permutations.h5\"\n    idx = torch.randperm(num_rots)\n    idx = np.asarray(idx, dtype=np.int)\n\n    h5_fout = h5py.File(os.path.join(tar_path, filename), \"w\")\n    h5_fout.create_dataset(\n        'data', data=idx,\n        compression='gzip', compression_opts=1,\n        dtype='uint8')\n    h5_fout.close()", "\n\nif __name__==\"__main__\":\n\n\n    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\n\n\n    # AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\n    # AtlasNetClasses = [\"plane.h5\"]\n    AtlasNetShapesPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/valid\"\n    AtlasNetRotPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_valid\"\n    r_input_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations.h5\"\n\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full_multicategory\"\n    partial_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_partial\"\n    partial_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_partial_multicategory\"\n\n    \"\"\"\"\n    for f in AtlasNetClasses:\n        shapes_permutations(f, AtlasNetShapesPath, \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\")\n    \"\"\"\n    # rot_permutations(\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\", 128)\n    # exit(666)\n    \"\"\"\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full_multicategory\"\n\n\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_consistency_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_consistency_full_multicategory\"\n    \"\"\"\n    # full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_consistency_full\"\n    # full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_consistency_full_multicategory\"\n\n\n    # ull_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full\"\n    # full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full_multicategory\"\n\n\n    # multicategory full shapes\n\n    print(\"multi category\")\n    ma = 0.\n    mb = 0.\n    k = 0.\n    for i in range(len(AtlasNetClasses)):\n        print(AtlasNetClasses[i])\n        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetRotPath, full_multi_pred_path, batch_size=32)\n        print(\"consistency: \", a)\n        ma += a\n        b = equivariance_metric(AtlasNetClasses[i], AtlasNetRotPath, full_multi_pred_path, batch_size=32)\n        print(\"equivariance: \", b)\n        mb += b\n        k += 1.\n\n    print(\"mean class consistency: \", ma / k)\n    print(\"mean class equivariance: \", mb / k)\n\n\n    print(\"category specific\")\n    ma = 0.\n    mb = 0.\n    k = 0.\n    for i in range(len(AtlasNetClasses)):\n        print(AtlasNetClasses[i])\n        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetRotPath, full_pred_path, batch_size=32)\n        print(\"consistency: \", a)\n        ma += a\n        b = equivariance_metric(AtlasNetClasses[i], AtlasNetRotPath, full_pred_path, batch_size=32)\n        print(\"equivariance: \", b)\n        mb += b\n        k += 1.\n\n    print(\"mean class consistency: \", ma / k)\n    print(\"mean class equivariance: \", mb / k)\n\n\n    '''\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full_multicategory\"\n\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full_multicategory\"\n\n\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/caca_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/caca_full_multicategory\"\n\n    \"\"\"\n    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full\"\n    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full_multicategory\"\n    \"\"\"\n\n    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\n    print(\"multi category\")\n    ma = 0.\n    mb = 0.\n    mc = 0.\n    k = 0.\n    for i in range(len(AtlasNetClasses)):\n        print(AtlasNetClasses[i])\n        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n                                shapes_idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\", batch_size=32)\n        print(\"consistency: \", a)\n        ma += a\n        b = equivariance_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n                                idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\", batch_size=32)\n        print(\"equivariance: \", b)\n        mb += b\n        c = class_consistency_umetric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n                                    idx_shapes_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n                                    idx_rots_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n                                    batch_size=32)\n        mc += c\n        print(\"u_consistency: \", c)\n\n        k += 1.\n\n    print(\"mean multi class consistency: \", ma / k)\n    print(\"mean multi class equivariance: \", mb / k)\n    print(\"mean multi class uconsistency: \", mc / k)\n\n\n    AtlasNetClasses = [\"plane.h5\", \"chair.h5\"]\n\n    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\n\n    print(\"category specific\")\n    ma = 0.\n    mb = 0.\n    mc = 0.\n    k = 0.\n    for i in range(len(AtlasNetClasses)):\n        print(AtlasNetClasses[i])\n        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n                                    shapes_idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n                                    batch_size=32)\n        print(\"consistency: \", a)\n        ma += a\n        b = equivariance_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n                                idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n                                batch_size=32)\n        print(\"equivariance: \", b)\n        mb += b\n        c = class_consistency_umetric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n                                    idx_shapes_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n                                    idx_rots_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n                                    batch_size=32)\n        mc += c\n        print(\"u_consistency: \", c)\n        k += 1.\n\n    print(\"mean class consistency: \", ma / k)\n    print(\"mean class equivariance: \", mb / k)\n    print(\"mean multi class uconsistency: \", mc / k)\n    '''", ""]}
{"filename": "cafi_net/spherical_harmonics/clebsch_gordan_decomposition.py", "chunked_list": ["import numpy as np\nimport torch\nfrom spherical_harmonics.wigner_matrix import complex_wigner_, complex_D_wigner, real_D_wigner, euler_rot_zyz\nfrom spherical_harmonics.wigner_matrix import complex_to_real_sh, real_to_complex_sh\n\nimport scipy\nfrom scipy import linalg, matrix, special\n\n\"\"\"\nClebsch-Gordan coefficients allows to decompose tensor products of irreducible ", "\"\"\"\nClebsch-Gordan coefficients allows to decompose tensor products of irreducible \nrepresentations of SO(3)\n\nhttps://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients#_j2=0\n\n\u27e8 j1 m1 j2 m2 | j3 m3 \u27e9\n\u27e8 j1,j2; m1,m2 | j1,j2; J,M \u27e9\nj1 = j1, j2 = j2, m1 = m1, m2 = m2, j3 = J, m3 = M\n", "j1 = j1, j2 = j2, m1 = m1, m2 = m2, j3 = J, m3 = M\n\nsymmetries:\n\u27e8 j1 m1 j2 m2 | j3 m3 \u27e9 = (-1)**(j3-j1-j2)*\u27e8 j1 -m1 j2 -m2 | j3 -m3 \u27e9\n\u27e8 j1 m1 j2 m2 | j3 m3 \u27e9 = (-1)**(j3-j1-j2)*\u27e8 j2 m1 j1 m2 | j3 m3 \u27e9\n\nwhen j2 = 0 the Clebsch\u2013Gordan coefficients are given by deltaj3j1*deltam3m1\n\"\"\"\n\n\"\"\"", "\n\"\"\"\ncomputes \u27e8 j1 m1 j2 m2 | J M \u27e9 for j1, m1, j2, m2 >= 0\n\"\"\"\ndef clebsch_gordan_(j1, j2, J, m1, m2, M):\n    # d = float((M == m1 + m2))\n    if M != m1 + m2:\n        return 0.0\n\n    A = float((2*J+1)*np.math.factorial(J+j1-j2)*np.math.factorial(J-j1+j2)*np.math.factorial(j1+j2-J))\n    A /= np.math.factorial(J+j1+j2+1)\n\n    B = float(np.math.factorial(J+M)*np.math.factorial(J-M)*np.math.factorial(j1-m1)*\n              np.math.factorial(j1+m1)*np.math.factorial(j2-m2)*np.math.factorial(j2+m2))\n    C = 0.\n\n    b0 = (j1+j2-J)\n    b1 = (j1-m1)\n    b2 = (j2+m2)\n\n    a0 = 0\n    a1 = (J-j2+m1)\n    a2 = (J-j1-m2)\n\n    k2 = np.min([b0, b1, b2])\n    k1 = np.max([-a0, -a1, -a2])\n\n    for k in range(k1, k2+1):\n        a0_ = np.math.factorial(k+a0)\n        a1_ = np.math.factorial(k+a1)\n        a2_ = np.math.factorial(k+a2)\n\n        b0_ = np.math.factorial(b0-k)\n        b1_ = np.math.factorial(b1-k)\n        b2_ = np.math.factorial(b2-k)\n\n        C += ((-1)**k)/(float(a0_*a1_*a2_*b0_*b1_*b2_))\n\n    return np.sqrt(A * B) * C", "\n\"\"\"\ncomputes \u27e8 j1 m1 j2 m2 | J M \u27e9\n\"\"\"\ndef clebsch_gordan_coeff(j1, j2, J, m1, m2, M):\n    if M < 0:\n        if j1 >= j2:\n            return (-1.)**(J-j1-j2)*clebsch_gordan_(j1, j2, J, -m1, -m2, -M)\n        else:\n            return clebsch_gordan_(j2, j1, J, -m2, -m1, -M)\n    else:\n        if j1 >= j2:\n            return clebsch_gordan_(j1, j2, J, m1, m2, M)\n        else:\n            return (-1.) ** (J - j1 - j2) * clebsch_gordan_(j2, j1, J, m2, m1, M)", "\n\n\"\"\"\ncomputes the projection from type (j1, j2) to type J \nQ*kron(Dj1, Dj2)*Q.T = DJ\n\"\"\"\ndef np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=True, dtype=np.float32):\n    #Q = np.zeros(shape=(2 * J + 1, (2 * j1 + 1) * (2 * j2 + 1)), dtype=dtype)\n    Q = np.zeros(shape=(2 * J + 1, 2 * j1 + 1, 2 * j2 + 1), dtype=dtype)\n    for m1 in range(-j1, j1 + 1):\n        for m2 in range(-j2, j2 + 1):\n            m3 = m1 + m2\n            if -J <= m3 <= J:\n                #Q[m3 + J, (2 * j2 + 1) * (m1 + j1) + (m2 + j2)] = clebsch_gordan_coeff(j1, j2, J, m1, m2, m3)\n                Q[m3 + J, m1 + j1, m2 + j2] = clebsch_gordan_coeff(j1, j2, J, m1, m2, m3)\n    Q = np.reshape(Q, newshape=(2*J+1, -1))\n    CRj1 = complex_to_real_sh(j1)\n    RCj1 = np.conjugate(CRj1.T)\n\n    CRj2 = complex_to_real_sh(j2)\n    RCj2 = np.conjugate(CRj2.T)\n\n    CRJ = complex_to_real_sh(J)\n    # RCJ = np.conjugate(CRJ.T)\n    Q = np.matmul(np.matmul(CRJ, Q), np.kron(RCj1, RCj2))\n    Q = np.real(Q)\n    Q = Q.astype(dtype=dtype)\n    if not matrix_shape:\n        Q = np.reshape(Q, newshape=(2*J+1, 2*j1+1, 2*j2+1))\n    return Q", "\ndef sparse_matrix(M, eps=0.00001):\n    m = np.shape(M)[0]\n    n = np.shape(M)[1]\n    idx_ = []\n    coeffs_ = []\n    n_ = 0\n    for i in range(m):\n        idx_.append([])\n        coeffs_.append([])\n        for j in range(n):\n            if np.abs(M[i, j]) > eps:\n                idx_[-1].append(j)\n                coeffs_[-1].append(M[i, j])\n        n_ = max(n_, len(idx_[-1]))\n\n    idx = np.zeros((m, n_), dtype=np.int32)\n    coeffs = np.zeros((m, n_), dtype=np.float32)\n    for i in range(m):\n        for j in range(len(idx_[i])):\n            idx[i, j] = idx_[i][j]\n            coeffs[i, j] = coeffs_[i][j]\n\n    return coeffs, idx", "\n\"\"\"\nComputes the Clebsch Gordan decomposition \n\"\"\"\ndef np_clebsch_gordan_decomposition(j1, j2, matrix_shape=True, l_max=None, dtype=np.float32):\n    Q = []\n    for J in range(abs(j1-j2), min(j1+j2+1, l_max+1)):\n        Q.append(np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=matrix_shape, dtype=dtype))\n    Q = np.concatenate(Q, axis=0)\n    return Q", "\ndef torch_clebsch_gordan_decomposition_(j1, j2, sparse=False, l_max=None, dtype=torch.float32):\n    Q = np_clebsch_gordan_decomposition(j1, j2, matrix_shape=True, l_max=l_max, dtype=np.float32)\n    if sparse:\n        coeffs, idx = sparse_matrix(Q)\n        idx = torch.from_numpy(idx).to(torch.int64)\n        coeffs = torch.from_numpy(coeffs).to(torch.float32)\n        return coeffs, idx\n    else:\n        return torch.from_numpy(Q).to(dtype)", "\ndef representation_type(x):\n    j1 = int((x.shape[-3] - 1) / 2)\n    j2 = int((x.shape[-2] - 1) / 2)\n    return (j1, j2)\n\ndef decompose_(x, Q):\n    \n    j1, j2 = representation_type(x)\n    # Q = Q[(j1, j2)]\n    s = list(x.shape)\n    c = s[-1]\n    s.pop()\n    s[-2] = -1\n    s[-1] = c\n    x = torch.reshape(x, shape=s)\n    x = torch.einsum('ij,...jk->...ik', Q.type_as(x), x)\n    y = []\n    p = 0\n    for J in range(abs(j1-j2), j1+j2+1):\n        y.append(x[..., p:p+2*J+1, :])\n        p += 2*J+1\n    return y", "\ndef sparse_decompose_(x, coeffs, idx):\n    j1, j2 = representation_type(x)\n    # coeffs = Q[(j1, j2)][0]\n    # idx = Q[(j1, j2)][0]\n    s = list(x.shape)\n    c = s[-1]\n    s.pop()\n    s[-2] = -1\n    s[-1] = c\n    x = torch.reshape(x, shape=s)\n\n    x = (x[..., idx, :])\n    x = torch.einsum('ij,...ijk->...ik', coeffs, x)\n    y = []\n    p = 0\n    for J in range(abs(j1-j2), j1+j2+1):\n        y.append(x[..., p:p+2*J+1, :])\n        p += 2*J+1\n    return y", "\n\nclass torch_clebsch_gordan_decomposition:\n    def __init__(self, l_max, l_max_out=None, sparse=False, output_type='dict'):\n        self.dtype = torch.float32\n        self.l_max = l_max\n        self.sparse = sparse\n        self.output_type = output_type\n        if l_max_out is None:\n            self.l_max_out = 2*l_max\n        else:\n            self.l_max_out = l_max_out\n        self.Q = dict()\n\n        for j1 in range(self.l_max+1):\n            for j2 in range(self.l_max+1):\n                if self.l_max_out < abs(j1-j2):\n                    pass\n                elif self.sparse:\n                    coeffs, idx = torch_clebsch_gordan_decomposition_(j1, j2, l_max=self.l_max_out,\n                                                                   sparse=True, dtype=self.dtype)\n                    self.Q[(j1, j2)] = [coeffs, idx]\n                else:\n                    self.Q[(j1, j2)] = torch_clebsch_gordan_decomposition_(j1, j2, l_max=self.l_max_out,\n                                                                        sparse=False, dtype=self.dtype)\n\n    def decompose(self, x):\n        if not isinstance(x, list):\n            x = [x]\n        y = dict()\n        for i in range(len(x)):\n            j1, j2 = representation_type(x[i])\n            if self.sparse:\n                yi = sparse_decompose_(x[i], self.Q[(j1, j2)][0], self.Q[(j1, j2)][1])\n            else:\n                yi = decompose_(x[i], self.Q[(j1, j2)])\n            for J in range(abs(j1-j2), min(j1+j2+1, self.l_max_out+1)):\n                if not str(J) in y:\n                    y[str(J)] = []\n                y[str(J)].append(yi[J-abs(j1-j2)])\n\n        for J in y:\n            y[J] = torch.cat(y[J], dim=-1)\n\n        if self.output_type == 'list':\n            return y.values()\n        else:\n            return y", "\n\"\"\"\ncomputes the Clebsch Gordan decomposition of the Zernike basis (up to degree d) tensored by a dimension 2*k+1\nequivariant feature.\n\"\"\"\n\"\"\"\ndef np_zernike_clebsch_gordan_decomposition(d, k, matrix_shape=True, l_max=None, dtype=np.float32):\n    zerinke_basis_idx = []\n    size_in = 0\n    size_out = 0", "    size_in = 0\n    size_out = 0\n    num_out_features = [0]*(d+1+k+1)\n    for l in range(1, d + 1):\n        for n in range(min(2 * d - l + 1, l + 1)):\n            if (n - l) % 2 == 0:\n                size_in += 2*l + 1\n                zerinke_basis_idx.append((n, l))\n                \n                np_clebsch_gordan_decomposition(l, k, matrix_shape=True, l_max=l_max, dtype=np.float32)", "                \n                np_clebsch_gordan_decomposition(l, k, matrix_shape=True, l_max=l_max, dtype=np.float32)\n                for J in range(abs(l-k), min(l+k+1, l_max+1)):\n                    num_out_features[J] += 1\n                    size_out\n\n\n    Q = np.zeros()\n\n    for i in range(len())", "\n    for i in range(len())\n\n    for J in range(abs(j1-j2), min(j1+j2+1, l_max+1)):\n        Q.append(np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=matrix_shape, dtype=dtype))\n    Q = np.concatenate(Q, axis=0)\n    return Q\n\n\"\"\"\n", "\"\"\"\n\n\ndef real_conj(A, Q):\n    return np.matmul(Q.T, np.matmul(A, Q))\n\ndef complex_conj(A, Q):\n    return np.matmul(np.conjugate(Q.T), np.matmul(A, Q))\n\ndef unit_test4():\n\n    j1 = 2\n    j2 = 2\n    J = 2\n    # cb_dict = clebsch_gordan_dict()\n\n    Q = np.asmatrix(clebsch_gordan_matrix(j1, j2, J, dtype=np.complex64))\n    # Q = np.sqrt(2.) * Q\n\n    # Q_ = np.asmatrix(Q_from_cb_dict(j1, j2, J, cb_dict, dtype=np.complex64))\n    # Q_ = np.sqrt(2.)*Q_\n\n    # Q__ = tensorProductDecompose_(j1, j2, J)\n    # Q__ = np.sqrt(1./2.49634557e-02)*Q__\n\n    angles = np.random.rand(3)\n    # angles = [1., 0., 0.]\n\n\n    Dj1 = complex_wigner_(j1, angles[0], angles[1], angles[2])\n    Dj2 = complex_wigner_(j2, angles[0], angles[1], angles[2])\n    DJ = complex_wigner_(J, angles[0], angles[1], angles[2])\n\n\n\n    print('eee')\n\n    prod = np.kron(Dj1, Dj2)\n\n    y = np.matmul(np.matmul(Q, prod), Q.T) - DJ\n\n    # print(y)\n    # print(np.matmul(Q.T, Q))\n    # print(np.matmul(Q, Q.T))\n\n    # print(np.real(prod))\n    # print(np.real(Q))\n    # print(np.real(Q__))\n    # y = np.matmul(Q, prod) - np.matmul(DJ, Q)\n    # y = np.matmul(y, Q.T)\n    # print(np.linalg.norm(Q - Q_, 'fro'))\n    print(np.linalg.norm(y))\n    print(np.linalg.norm(DJ))", "\ndef unit_test4():\n\n    j1 = 2\n    j2 = 2\n    J = 2\n    # cb_dict = clebsch_gordan_dict()\n\n    Q = np.asmatrix(clebsch_gordan_matrix(j1, j2, J, dtype=np.complex64))\n    # Q = np.sqrt(2.) * Q\n\n    # Q_ = np.asmatrix(Q_from_cb_dict(j1, j2, J, cb_dict, dtype=np.complex64))\n    # Q_ = np.sqrt(2.)*Q_\n\n    # Q__ = tensorProductDecompose_(j1, j2, J)\n    # Q__ = np.sqrt(1./2.49634557e-02)*Q__\n\n    angles = np.random.rand(3)\n    # angles = [1., 0., 0.]\n\n\n    Dj1 = complex_wigner_(j1, angles[0], angles[1], angles[2])\n    Dj2 = complex_wigner_(j2, angles[0], angles[1], angles[2])\n    DJ = complex_wigner_(J, angles[0], angles[1], angles[2])\n\n\n\n    print('eee')\n\n    prod = np.kron(Dj1, Dj2)\n\n    y = np.matmul(np.matmul(Q, prod), Q.T) - DJ\n\n    # print(y)\n    # print(np.matmul(Q.T, Q))\n    # print(np.matmul(Q, Q.T))\n\n    # print(np.real(prod))\n    # print(np.real(Q))\n    # print(np.real(Q__))\n    # y = np.matmul(Q, prod) - np.matmul(DJ, Q)\n    # y = np.matmul(y, Q.T)\n    # print(np.linalg.norm(Q - Q_, 'fro'))\n    print(np.linalg.norm(y))\n    print(np.linalg.norm(DJ))", "\n\n\n\ndef unit_test5():\n    j1 = 1\n    j2 = 1\n\n    angles = np.random.rand(3)\n    # angles = [1.0, 0.0, 0.]\n\n    D0 = np.asmatrix([[1.]], dtype=np.complex64)\n    D1 = complex_wigner_(1, angles[0], angles[1], angles[2])\n    D2 = complex_wigner_(2, angles[0], angles[1], angles[2])\n\n    D = [D0, D1, D2]\n\n    prod = np.kron(D[j1], D[j2])\n    # prod = np.kron(D[j2], D[j1])\n\n    c = 0.0\n    for m1 in range(-j1, j1+1):\n        for k1 in range(-j1, j1+1):\n            for m2 in range(-j2, j2+1):\n                for k2 in range(-j2, j2+1):\n                    a = D[j1][j1 + m1, j1 + k1] * D[j2][j2 + m2, j2 + k2]\n                    b = 0.\n                    # b = prod[(2*j2+1)*(m1+j1) + (m2+j2), (2*j2+1)*(k1+j1) + (k2+j2)]\n\n                    for J in range(abs(j1-j2), j1+j2+1):\n                        if(2*J >= m1+m2+J >= 0 and 2*J >= k1+k2+J >= 0):\n                            b += D[J][m1+m2+J, k1+k2+J] * clebsch_gordan_coeff(j1, j2, J, m1, m2, m1 + m2) * clebsch_gordan_coeff(j1, j2, J, k1, k2, k1 + k2)\n\n                    print('zz')\n                    print(a)\n                    print(b)\n                    print(a-b)\n\n                    c += abs(np.real(a-b))*abs(np.real(a-b))+abs(np.imag(a-b))*abs(np.imag(a-b))\n\n    print('rr')\n    print(np.sqrt(c))", "\n\n\ndef unit_test6():\n    angles = np.random.rand(3)\n    # angles = [0., 1., 0.]\n\n    Q0 = np.asmatrix(clebsch_gordan_matrix(1, 1, 0, dtype=np.complex64))\n    Q1 = np.asmatrix(clebsch_gordan_matrix(1, 1, 1, dtype=np.complex64))\n    Q2 = np.asmatrix(clebsch_gordan_matrix(1, 1, 2, dtype=np.complex64))\n\n    D0 = np.asmatrix([[1.]], dtype=np.complex64)\n    D1 = complex_wigner_(1, angles[0], angles[1], angles[2])\n    D2 = complex_wigner_(2, angles[0], angles[1], angles[2])\n\n    y = np.kron(D1, D1) - real_conj(D2, Q2) - real_conj(D1, Q1) - real_conj(D0, Q0)\n\n    print(np.linalg.norm(y))", "\ndef tensor_decomposition_unit_test___(j, k, J, a, b, c):\n    Dj = complex_D_wigner(j, a, b, c)\n    Dk = complex_D_wigner(k, a, b, c)\n    DJ = complex_D_wigner(k, a, b, c)\n    assert(j+k >= J >= abs(k-j))\n    QJ = clebsch_gordan_matrix(j, k, J)\n\n    y = real_conj(np.kron(Dj, Dk), QJ.T) - DJ\n    print(np.linalg.norm(y))", "\n\ndef tensor_decomposition_unit_test__(j, k, a, b, c):\n    Dj = complex_D_wigner(j, a, b, c)\n    Dk = complex_D_wigner(k, a, b, c)\n\n    D_ = np.zeros(shape=((2*j+1)*(2*k+1), (2*j+1)*(2*k+1)), dtype=np.complex64)\n    D = np.kron(Dj, Dk)\n\n    for J in range(abs(k-j), k+j+1):\n        print('j = ', j, 'k = ', k, 'J = ', J)\n        DJ = complex_D_wigner(J, a, b, c)\n        QJ = clebsch_gordan_matrix(j, k, J)\n        y = real_conj(np.kron(Dj, Dk), QJ.T) - DJ\n        # print(np.linalg.norm(DJ))\n        print(np.linalg.norm(y))\n        D_ += real_conj(DJ, QJ)\n    print('decompose j = ', j, 'k = ', k)\n    print(np.linalg.norm(D - D_))", "\n\ndef tensor_decomposition_unit_test(l):\n    for i in range(10):\n        angles = np.random.rand(3)\n        a = angles[0]\n        b = angles[1]\n        c = angles[2]\n        for j in range(l+1):\n            for k in range(l+1):\n                tensor_decomposition_unit_test__(j, k, a, b, c)", "\ndef invariant_feature(equivariant_features, p, q, Q):\n    # y = tf.einsum('bvqmrc,bvqnrc->bvqmnrc', equivariant_features[p[0]], equivariant_features[p[1]])\n    # the equivariant channels must in the last dimesion\n\n    #y = tf.einsum('bvqrcm,bvqrcn->bvqrcmn', equivariant_features[p[0]], equivariant_features[p[1]])\n    \"\"\"\n    nb = y.get_shape()[0].value\n    nv = y.get_shape()[1].value\n    nq = y.get_shape()[2].value\n    nr = y.get_shape()[3].value\n    nc = y.get_shape()[4].value\n    y = tf.reshape(y, shape=(nb, nv, nq, nr, nc, -1))\n    \"\"\"", "\ndef higher_product_matrix(p, q):\n\n\n    Q = npClebschGordanMatrices(3)\n\n    \"\"\"\n    res = np.eye((2*abs(q[0])+1)*(2*abs(p[1])+1))\n    I = np.eye(1)\n    res = np.real(np.reshape(Q.getMatrix(q[0], p[1], q[1]), newshape=(2*abs(q[1])+1, -1)))\n    for i in range(len(p)-1):\n\n\n        Qi_ = np.real(np.reshape(Q.getMatrix(q[i+1], p[i+2], q[i+2]), newshape=(2*abs(q[i+1])+1, -1)))\n        Qi = np.kron(Qi_, I)\n        res = np.matmul(Qi_, np.kron(res, I))\n        I = np.kron(I, np.eye(2 * abs(p[i + 1]) + 1))\n    \"\"\"\n\n\n    Q1 = np.reshape(Q.getMatrix(q[0], p[1], q[1]), newshape=(2*abs(q[1])+1, -1))\n    Q2 = np.reshape(Q.getMatrix(q[1], p[2], q[2]), newshape=(2 * abs(q[2]) + 1, -1))\n    M = np.real(Q1)\n    M = np.kron(M, np.eye(2*abs(p[2])+1))\n    M = np.matmul(np.real(Q2), M)\n    print(M)\n    print(np.matmul(M, M.transpose()))\n    return", "\n\ndef higher_product(R, X, p, q, Q):\n\n    # print(np.linalg.norm(y))\n    # print(y)\n    X = np.asmatrix(np.random.rand(1, 3))\n    X /= (np.linalg.norm(X))\n    X *= 10.0\n    X_rot = (np.matmul(R.T, X.T)).T\n\n    y = complex_sh_(abs(p[0]), X)\n    y_rot = complex_sh_(abs(p[0]), X_rot)\n    for i in range(len(p)-1):\n        \"\"\"\n        print('uuu')\n        print(X.shape)\n        print(y.shape)\n        print(p)\n        print(complex_sh_(abs(p[i+1]), X).shape)\n        print(Q.getMatrix(q[i], p[i+1], q[i+1]).shape)\n        print('aaa')\n        \"\"\"\n        \"\"\"\n        print('aaaaaa')\n        print(q[i], p[i+1], q[i+1])\n        print(Q.getMatrix(q[i], p[i+1], q[i+1]))\n        print('bbbbbb')\n        \"\"\"\n        X = np.asmatrix(np.random.rand(1, 3))\n        X /= (np.linalg.norm(X))\n        X *= 10.0\n        X_rot = (np.matmul(R.T, X.T)).T\n\n\n        z = np.einsum('jmn,jmn->j', Q.getMatrix(q[i], p[i+1], q[i+1]), Q.getMatrix(q[i], p[i+1], q[i+1]))\n        print('qi, pi+1, qi+1 = ', q[i], p[i + 1], q[i + 1])\n        print('norm z= ', np.linalg.norm(z))\n        y = np.einsum('vm,vn->vmn', y, complex_sh_(abs(p[i+1]), X))\n        y_rot = np.einsum('vm,vn->vmn', y_rot, complex_sh_(abs(p[i+1]), X_rot))\n        y = np.einsum('jmn,vmn->vj', Q.getMatrix(q[i], p[i+1], q[i+1]), y)\n        y_rot = np.einsum('jmn,vmn->vj', Q.getMatrix(q[i], p[i + 1], q[i + 1]), y_rot)\n        # print(y)\n        # print(np.linalg.norm(y))\n\n    return y", "\ndef higher_tensor_decomposition_unit_test():\n    Q = npClebschGordanMatrices(3)\n    p = []\n    q = []\n\n    # degree 1 invariants\n    p.append(np.zeros(shape=(1, 1), dtype=np.int32))\n    q.append(np.zeros(shape=(1, 1), dtype=np.int32))\n\n    p.append(np.array([[1, 1], [2, 2]], dtype=np.int32))\n    q.append(np.array([[1, 0], [2, 0]], dtype=np.int32))\n\n    p.append(np.array([[1, 1, 1],\n                       [1, 1, 2],\n                       [1, 2, 2],\n                       [2, 2, 2]], dtype=np.int32))\n    q.append(np.array([[1, 1, 0],\n                       [1, 2, 0],\n                       [1, 2, 0],\n                       [2, 2, 0]], dtype=np.int32))\n    for i in range(10):\n        angles = np.random.rand(3)\n        X = np.asmatrix(np.random.rand(1, 3))\n        X /= (np.linalg.norm(X))\n        a = angles[0]\n        b = angles[1]\n        c = angles[2]\n        R = euler_rot_zyz(a, b, c)\n        for d in range(len(p)):\n            for j in range(np.size(p[d], 0)):\n                print(p[d][j, :])\n                print(q[d][j, :])\n                z = higher_product(R, X, p[d][j, :], q[d][j, :], Q)\n                print('norm output = ', np.linalg.norm(z))", "                # print(np.linalg.norm(X))\n\ndef real_tensor_decomposition_unit_test__(j, k, a, b, c):\n\n    # CRj = complex_to_real_sh(j)\n    # CRk = complex_to_real_sh(k)\n\n    # K = np.kron(CRj, CRk)\n    # K_T = np.conjugate(K.T)\n\n    Dj = real_D_wigner(j, a, b, c)\n    Dk = real_D_wigner(k, a, b, c)\n\n    D_ = np.zeros(shape=((2*j+1)*(2*k+1), (2*j+1)*(2*k+1)), dtype=np.complex64)\n    D = np.kron(Dj, Dk)\n\n    for J in range(abs(k-j), k+j+1):\n        print('j = ', j, 'k = ', k, 'J = ', J)\n        # CRJ = complex_to_real_sh(J)\n        # RCJ = np.conjugate(CRJ.T)\n        DJ = real_D_wigner(J, a, b, c)\n        QJ = real_Q_from_cb(j, k, J, dtype=np.complex64)\n\n        y = complex_conj(np.kron(Dj, Dk), np.conjugate(QJ.T)) - DJ\n        # print(np.linalg.norm(DJ))\n        print(np.linalg.norm(y))\n        D_ += real_conj(DJ, QJ)\n    print('decompose j = ', j, 'k = ', k)\n    print(np.linalg.norm(D - D_))", "\n\ndef real_tensor_decomposition_unit_test(l):\n    for i in range(3):\n        angles = np.random.rand(3)\n        a = angles[0]\n        b = angles[1]\n        c = angles[2]\n        for j in range(l+1):\n            for k in range(l+1):\n                real_tensor_decomposition_unit_test__(j, k, a, b, c)", "\n\n\n\n\n\n"]}
{"filename": "cafi_net/spherical_harmonics/spherical_cnn.py", "chunked_list": ["from spherical_harmonics.kernels import *\nimport numpy as np\nimport torch\n\ndef torch_fibonnacci_sphere_sampling(num_pts):\n    indices = np.arange(0, num_pts, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/num_pts)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x, y, z = np.cos(theta) * np.sin(phi), np.sin(theta) * np.sin(phi), np.cos(phi)\n    S2 = np.stack([x, y, z], axis=-1)\n    return torch.from_numpy(S2).to(torch.float32)", "\ndef monoms_3D(d):\n    monoms_basis = []\n    for I in range((d + 1) ** 3):\n        i = I % (d + 1)\n        a = int((I - i) / (d + 1))\n        j = a % (d + 1)\n        k = int((a - j) / (d + 1))\n        if (i + j + k == d):\n            monoms_basis.append((i, j, k))\n    monoms_basis = list(set(monoms_basis))\n    monoms_basis = sorted(monoms_basis)\n    return monoms_basis", "\ndef torch_eval_monoms_3D(x, d, axis=-1):\n    m = monoms_3D(d)\n    pows = np.zeros((3, len(m)), dtype=np.int32)\n    for i in range(len(m)):\n        for j in range(3):\n            pows[j, i] = m[i][j]\n    pows = torch.from_numpy(pows).to(torch.float32).to(x.device)\n    n = len(list(x.shape))\n    axis = axis % n\n    shape = [1]*(n+1)\n    shape[axis] = 3\n    shape[-1] = len(m)\n    pows = torch.reshape(pows, shape)\n    x = x.unsqueeze(-1)\n    y = torch.pow(x, pows)\n    y = torch.prod(y, dim = axis, keepdims=False)\n    return y", "\ndef np_monomial_basis_coeffs(polynomials, monoms_basis):\n    n_ = len(monoms_basis)\n    m_ = len(polynomials)\n    M = np.zeros((m_, n_))\n    for i in range(m_):\n        for j in range(n_):\n            M[i, j] = re(polynomials[i].coeff_monomial(monoms_basis[j]))\n    return M\n\ndef torch_monomial_basis_coeffs(polynomials, monoms_basis, dtype=torch.float32):\n    return torch.from_numpy(np_monomial_basis_coeffs(polynomials, monoms_basis)).to(dtype)", "\ndef torch_monomial_basis_coeffs(polynomials, monoms_basis, dtype=torch.float32):\n    return torch.from_numpy(np_monomial_basis_coeffs(polynomials, monoms_basis)).to(dtype)\n\ndef torch_spherical_harmonics_(l, matrix_format=True):\n    monoms = monoms_3D(l)\n    sph_polys = []\n    x, y, z = symbols(\"x y z\")\n    for m in range(2*l+1):\n        sph_polys.append(real_spherical_harmonic(l, m-l, x, y, z, poly = True))\n    coeffs = torch_monomial_basis_coeffs(sph_polys, monoms)\n    if matrix_format:\n        coeffs = torch.reshape(coeffs, (2*l+1, -1))\n    return coeffs", "\nclass zernike_monoms:\n    def __init__(self,max_deg=3):\n        self.max_deg = max_deg\n        self.harmonics = torch_spherical_harmonics(l_max = max_deg)\n    def compute(self,x):\n        max_deg = self.max_deg\n        m = int(max_deg / 2.)\n        n2 = torch.sum(x * x, dim=-1, keepdims=True)\n        n2 = n2.unsqueeze(-1)\n        p = [torch.ones(n2.shape).type_as(x)]\n        for m_ in range(m):\n            p.append(p[-1] * n2)\n\n        y = torch_spherical_harmonics(l_max=max_deg).compute(x)\n        for l in y:\n            y[l] = y[l].unsqueeze(-1)\n\n        z = dict()\n        for d in range(max_deg + 1):\n            z[d] = []\n        for l in y:\n            l_ = int(l)\n            for d in range(m + 1):\n                d_ = 2 * d + l_\n                if d_ <= max_deg:\n                    # print(p[d].shape)\n                    # print(y[l].shape)\n                    zd = (p[d] * y[l])\n                    z[l_].append(zd)\n        for d in z:\n            z[d] = torch.cat(z[d], dim=-1)\n        return z", "\nclass torch_zernike_monoms:\n    def __init__(self, max_deg):\n        self.max_deg = max_deg\n        self.sph_harmonics = torch_spherical_harmonics(l_max=max_deg)\n\n        pass\n\n    def compute(self, x):\n        m = int(self.max_deg / 2.)\n        n2 = torch.sum(x * x, dim=-1, keepdims=True)\n        n2 = n2.unsqueeze(-1)\n        p = [torch.ones(n2.shape).type_as(x)]\n        for m_ in range(m):\n            p.append(p[-1] * n2)\n\n        y = self.sph_harmonics.compute(x)\n        for l in y:\n            y[l] = y[l].unsqueeze(-1)\n\n        z = dict()\n        for d in range(self.max_deg + 1):\n            z[d] = []\n        for l in y:\n            l_ = int(l)\n            for d in range(m + 1):\n                d_ = 2 * d + l_\n                if d_ <= self.max_deg:\n                    # print(p[d].shape)\n                    # print(y[l].shape)\n                    zd = (p[d] * y[l])\n                    z[l_].append(zd)\n        for d in z:\n            z[d] = torch.cat(z[d], dim=-1)\n        return z", "\nclass torch_spherical_harmonics:\n    def __init__(self, l_max=3, l_list=None):\n        if l_list is None:\n            self.l_list = range(l_max+1)\n        else:\n            self.l_list = l_list\n        self.l_max = max(self.l_list)\n        self.Y = dict()\n        for l in self.l_list:\n            self.Y[str(l)] = torch_spherical_harmonics_(l)\n\n    def compute(self, x):\n        Y = dict()\n        for l in self.l_list:\n            ml = torch_eval_monoms_3D(x, l)\n            Y[str(l)] = torch.einsum('mk,...k->...m', self.Y[str(l)].type_as(ml), ml)\n        return Y", "\ndef np_polyhedrons(poly):\n\n\n    C0 = 3 * np.sqrt(2) / 4\n    C1 = 9 * np.sqrt(2) / 8\n\n    tetrakis_hexahedron = np.array([[0.0, 0.0, C1],\n                                    [0.0, 0.0, -C1],\n                                    [C1, 0.0, 0.0],\n                                    [-C1, 0.0, 0.0],\n                                    [0.0, C1, 0.0],\n                                    [0.0, -C1, 0.0],\n                                    [C0, C0, C0],\n                                    [C0, C0, -C0],\n                                    [C0, -C0, C0],\n                                    [C0, -C0, -C0],\n                                    [-C0, C0, C0],\n                                    [-C0, C0, -C0],\n                                    [-C0, -C0, C0],\n                                    [-C0, -C0, -C0]], dtype=np.float32)\n\n    C0 = (1 + np.sqrt(5)) / 4\n    C1 = (3 + np.sqrt(5)) / 4\n\n    regular_dodecahedron = np.array([[0.0, 0.5, C1], [0.0, 0.5, -C1], [0.0, -0.5, C1], [0.0, -0.5, -C1],\n                      [C1, 0.0, 0.5], [C1, 0.0, -0.5], [-C1, 0.0, 0.5], [-C1, 0.0, -0.5],\n                      [0.5, C1, 0.0], [0.5, -C1, 0.0], [-0.5, C1, 0.0], [-0.5, -C1, 0.0],\n                      [C0, C0, C0], [C0, C0, -C0], [C0, -C0, C0], [C0, -C0, -C0],\n                      [-C0, C0, C0], [-C0, C0, -C0], [-C0, -C0, C0], [-C0, -C0, -C0]], dtype=np.float32)\n\n    C0 = 3 * (np.sqrt(5) - 1) / 4\n    C1 = 9 * (9 + np.sqrt(5)) / 76\n    C2 = 9 * (7 + 5 * np.sqrt(5)) / 76\n    C3 = 3 * (1 + np.sqrt(5)) / 4\n\n    pentakis_dodecahedron = np.array([[0.0, C0, C3], [0.0, C0, -C3], [0.0, -C0, C3], [0.0, -C0, -C3],\n                      [C3, 0.0, C0], [C3, 0.0, -C0], [-C3, 0.0, C0], [-C3, 0.0, -C0],\n                      [C0, C3, 0.0], [C0, -C3, 0.0], [-C0, C3, 0.0], [-C0, -C3, 0.0],\n                      [C1, 0.0, C2], [C1, 0.0, -C2], [-C1, 0.0, C2], [-C1, 0.0, -C2],\n                      [C2, C1, 0.0], [C2, -C1, 0.0], [-C2, C1, 0.0], [-C2, -C1, 0.0],\n                      [0.0, C2, C1], [0.0, C2, -C1], [0.0, -C2, C1], [0.0, -C2, -C1],\n                      [1.5, 1.5, 1.5], [1.5, 1.5, -1.5], [1.5, -1.5, 1.5], [1.5, -1.5, -1.5],\n                      [-1.5, 1.5, 1.5], [-1.5, 1.5, -1.5], [-1.5, -1.5, 1.5], [-1.5, -1.5, -1.5]],\n                     dtype=np.float)\n\n\n    C0 = 3 * (15 + np.sqrt(5)) / 44\n    C1 = (5 - np.sqrt(5)) / 2\n    C2 = 3 * (5 + 4 * np.sqrt(5)) / 22\n    C3 = 3 * (5 + np.sqrt(5)) / 10\n    C4 = np.sqrt(5)\n    C5 = (75 + 27 * np.sqrt(5)) / 44\n    C6 = (15 + 9 * np.sqrt(5)) / 10\n    C7 = (5 + np.sqrt(5)) / 2\n    C8 = 3 * (5 + 4 * np.sqrt(5)) / 11\n\n    disdyakis_triacontahedron = np.array([[0.0, 0.0, C8], [0.0, 0.0, -C8], [C8, 0.0, 0.0], [-C8, 0.0, 0.0],\n                                          [0.0, C8, 0.0], [0.0, -C8, 0.0], [0.0, C1, C7], [0.0, C1, -C7],\n                                          [0.0, -C1, C7], [0.0, -C1, -C7], [C7, 0.0, C1], [C7, 0.0, -C1],\n                                          [-C7, 0.0, C1], [-C7, 0.0, -C1], [C1, C7, 0.0], [C1, -C7, 0.0],\n                                          [-C1, C7, 0.0], [-C1, -C7, 0.0], [C3, 0.0, C6], [C3, 0.0, -C6],\n                                          [-C3, 0.0, C6], [-C3, 0.0, -C6], [C6, C3, 0.0], [C6, -C3, 0.0],\n                                          [-C6, C3, 0.0], [-C6, -C3, 0.0], [0.0, C6, C3], [0.0, C6, -C3],\n                                          [0.0, -C6, C3], [0.0, -C6, -C3], [C0, C2, C5], [C0, C2, -C5],\n                                          [C0, -C2, C5], [C0, -C2, -C5], [-C0, C2, C5], [-C0, C2, -C5],\n                                          [-C0, -C2, C5], [-C0, -C2, -C5], [C5, C0, C2], [C5, C0, -C2],\n                                          [C5, -C0, C2], [C5, -C0, -C2], [-C5, C0, C2], [-C5, C0, -C2],\n                                          [-C5, -C0, C2], [-C5, -C0, -C2], [C2, C5, C0], [C2, C5, -C0],\n                                          [C2, -C5, C0], [C2, -C5, -C0], [-C2, C5, C0], [-C2, C5, -C0],\n                                          [-C2, -C5, C0], [-C2, -C5, -C0], [C4, C4, C4], [C4, C4, -C4],\n                                          [C4, -C4, C4], [C4, -C4, -C4], [-C4, C4, C4], [-C4, C4, -C4],\n                                          [-C4, -C4, C4], [-C4, -C4, -C4]], dtype=np.float32)\n\n    P = {'tetrakis_hexahedron':tetrakis_hexahedron,\n         'regular_dodecahedron':regular_dodecahedron,\n         'pentakis_dodecahedron':pentakis_dodecahedron,\n         'disdyakis_triacontahedron':disdyakis_triacontahedron}\n\n    p = P[poly]\n    c = np.mean(p, axis=0, keepdims=True)\n    p = np.subtract(p, c)\n    n = np.linalg.norm(p, axis=-1, keepdims=True)\n    p = np.divide(p, n)\n    return p", "\ndef torch_polyhedrons(poly):\n    return torch.from_numpy(np_polyhedrons(poly)).to(torch.float32)\n\nclass SphericalHarmonicsEval:\n    \"\"\"\n    Inverse Spherical Harmonics Transform layer\n    \"\"\"\n    def __init__(self, base='pentakis', l_max=3, l_list=None, sph_fn=None):\n        self.base = base\n        if sph_fn is not None:\n            if l_list is not None:\n                self.l_list = l_list\n                self.l_max = max(l_list)\n            else:\n                self.l_list = range(l_max+1)\n                self.l_max = l_max\n            self.sph_fn = sph_fn\n        else:\n            self.sph_fn = torch_spherical_harmonics(l_max=l_max, l_list=l_list)\n\n        if isinstance(base, str):\n            S2 = torch_polyhedrons(self.base)\n        else:\n            S2 = base\n\n\n        y = self.sph_fn.compute(S2)\n        self.types = y.keys()\n        Y = []\n        for l in self.types:\n            Y.append(torch.reshape(y[l], (-1, 2*int(l)+1)))\n        self.Y = torch.cat(Y, dim=-1)\n\n    def compute(self, x):\n        X = []\n        for l in self.types:\n            X.append(x[l])\n        X = torch.cat(X, dim=-2)\n        return torch.einsum('vm,...mc->...vc', self.Y.type_as(X), X)", "\n\nclass SphericalHarmonicsCoeffs:\n    \"\"\"\n    Spherical Harmonics Transform layer\n    \"\"\"\n    def __init__(self, base='pentakis', l_max=3, l_list=None, sph_fn=None):\n        self.base = base\n        if l_list is not None:\n            self.l_list = l_list\n            self.l_max = max(l_list)\n        else:\n            self.l_list = list(range(l_max + 1))\n            self.l_max = l_max\n\n        self.split_size = []\n        for i in range(len(self.l_list)):\n            self.split_size.append(2*self.l_list[i] + 1)\n\n        if sph_fn is not None:\n            self.sph_fn = sph_fn\n        else:\n            self.sph_fn = torch_spherical_harmonics(l_max=l_max, l_list=l_list)\n\n        if isinstance(self.base, str):\n            S2 = torch_polyhedrons(self.base)\n        else:\n            S2 = self.base\n\n        y = self.sph_fn.compute(S2)\n\n        self.types = list(y.keys())\n        Y = []\n        for l in self.types:\n            Y.append(torch.reshape(y[l], (-1, 2*int(l)+1)))\n        self.Y = torch.cat(Y, dim=-1)\n        self.S2 = S2\n\n    def compute(self, x):\n        X = []\n        c = torch.einsum('vm,...vc->...mc', self.Y.type_as(x), x) / (self.Y.shape[0] / (4*np.pi))\n        c = torch.split(c, split_size_or_sections=self.split_size, dim=-2)\n\n        C = dict()\n        for i in range(len(self.types)):\n            l = self.types[i]\n            sl = list(x.shape)\n            sl[-2] = 2*int(l)+1\n            C[l] = torch.reshape(c[i], sl)\n        return C\n\n    def get_samples(self):\n        return self.S2", "\nif __name__==\"__main__\":\n\n    device = \"cuda:0\"\n    S2 = torch_fibonnacci_sphere_sampling(64).to(device)\n    # print(S2.shape, \"samples\", S2)\n    y = {}\n\n    for i in range(4):\n        y[str(i)] = (torch.ones((2, 16, 2*i + 1, 128)) * torch.arange(16).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)).to(device)\n\n    x = y.copy()\n    # Inverse Spherical Harmonics Transform\n    y = SphericalHarmonicsEval(l_max=3, base=S2).compute(y)\n    # print(y, y.shape)\n    # Spherical Harmonics Transform\n    y = SphericalHarmonicsCoeffs(l_max=3, base=S2).compute(y)\n    # for key in y:\n    #     print(y[key], y[key].shape)\n    pass"]}
{"filename": "cafi_net/spherical_harmonics/wigner_matrix.py", "chunked_list": ["import numpy as np\nimport scipy\nfrom sympy import *\n# import tensorflow as tf\nfrom scipy import linalg, matrix, special\n# from spherical_harmonics.np_spherical_harmonics import complex_sh_, real_sh_, complex_to_real_sh\n# from sympy.physics.quantum.spin import Rotation\nfrom scipy.spatial.transform.rotation import Rotation\n\n", "\n\n\n\n\"\"\"\nGiven a rotation matrix R and Y_{lk} the spherical harmonics basis \nfor l in NN and k in [|-l, l|] we have \nY_l( R^{-1} x) = D^l(R)Y_l(x) where D^l is the wigner matrix \nSee https://en.wikipedia.org/wiki/Wigner_D-matrix\n", "See https://en.wikipedia.org/wiki/Wigner_D-matrix\n\nIn particular complex and real version of the Wigner D matrix and \nits relation to the Wigner d matrix\n\"\"\"\n\n# change of basis from real to complex spherical harmonics basis\ndef real_to_complex_sh(l):\n    C = np.zeros(shape=(2*l+1, 2*l+1), dtype=np.complex64)\n    c = 1./np.sqrt(2.)\n    for m in range(1, l+1):\n        C[l + m, l + m] = -1j * c\n        C[l + m, l - m] = c\n    for m in range(-l, 0):\n        C[l + m, l + m] = ((-1)**m)*c\n        C[l + m, l - m] = 1j*((-1) ** m)*c\n\n    C[l, l] = 1.\n    C = np.flip(C, 0)\n    C = np.flip(C, 1)\n    # print(C)\n\n    return np.asmatrix(C)", "\n# change of basis from complex to real spherical harmonics basis\ndef complex_to_real_sh(l):\n    return (real_to_complex_sh(l).conjugate()).T\n\n# rotation matrix around z axis\ndef z_rot(a):\n    c = np.cos(a)\n    s = np.sin(a)\n    return np.matrix([[c, -s, 0.], [s, c, 0.], [0., 0., 1.]])", "\n# rotation matrix around y axis\ndef y_rot(a):\n    c = np.cos(a)\n    s = np.sin(a)\n    return np.matrix([[c, 0., s], [0., 1., 0.], [-s, 0., c]])\n\n# rotation matrix for Euler angles in z-y-z convention\ndef euler_rot_zyz(a, b ,c):\n    return np.matmul(np.matmul(z_rot(a), y_rot(b)), z_rot(c))", "def euler_rot_zyz(a, b ,c):\n    return np.matmul(np.matmul(z_rot(a), y_rot(b)), z_rot(c))\n\n\ndef complex_wigner_2_(a, b, c):\n    ea = np.exp(1j*a)\n    eb = np.exp(1j*b)\n    ec = np.exp(1j*c)\n\n    e_a = np.exp(-1j*a)\n    e_b = np.exp(-1j*b)\n    e_c = np.exp(-1j*c)\n\n    e2a = np.exp(1j*2.*a)\n    e2b = np.exp(1j*2.*b)\n    e2c = np.exp(1j*2.*c)\n\n    e_2a = np.exp(-1j*2.*a)\n    e_2b = np.exp(-1j*2.*b)\n    e_2c = np.exp(-1j*2.*c)\n\n    sa = np.imag(ea)\n    ca = np.real(ea)\n\n    # sb = np.imag(eb)\n    # cb = np.real(eb)\n    sb = np.sin(b)\n    cb = np.cos(b)\n\n    sc = np.imag(ec)\n    cc = np.real(ec)\n\n    # c2b = np.real(e2b)\n    # s2b = np.imag(e2b)\n    c2b = np.cos(2.*b)\n    s2b = np.sin(2.*b)\n    \n    d22 = ((1+cb)*(1.+cb))/4.\n    d21 = -sb*(1.+cb)/2.\n    d20 = np.sqrt(3./8.)*sb*sb\n    d2_1 = -sb*(1.-cb)/2.\n    d2_2 = (1.-cb)*(1.-cb)/4.\n    d11 = (2.*cb*cb+cb-1.)/2.\n    d10 = -np.sqrt(3./8.)*s2b\n    d1_1 = (-2.*cb*cb+cb+1.)/2.\n    d00 = (3.*cb*cb-1.)/2.\n\n    d = np.asmatrix([[d22, -d21, d20, -d2_1, d2_2],\n                     [d21, d11, -d10, d1_1, -d2_1],\n                     [d20, d10, d00, -d10, d20],\n                     [d2_1, d1_1, d10, d11, -d21],\n                     [d2_2, d2_1, d20, d21, d22]], dtype=np.complex64)\n\n    # debug d\n\n    d = d.T\n\n    \"\"\"\n    for i in range(-2, 3):\n        for j in range(-2, 3):\n            print( str(i) + ' ' + str(j))\n            print(str(np.real(d[i+2, j+2])) + ' ' + str(((-1.)**(j-i))*np.real(d[j+2, i+2])) + ' ' + str(np.real(d[2-j, 2-i])))\n    \"\"\"\n\n\n    Ea = np.asmatrix([[e_2a, 0., 0., 0., 0.],\n                      [0., e_a, 0., 0., 0.],\n                      [0., 0., 1., 0., 0.],\n                      [0., 0., 0., ea, 0.],\n                      [0., 0., 0., 0., e2a]], dtype=np.complex64)\n\n    Ec = np.asmatrix([[e_2c, 0., 0., 0., 0.],\n                      [0., e_c, 0., 0., 0.],\n                      [0., 0., 1., 0., 0.],\n                      [0., 0., 0., ec, 0.],\n                      [0., 0., 0., 0., e2c]], dtype=np.complex64)\n\n\n    \"\"\"\n    Ea = np.asmatrix([[e2a, 0., 0., 0., 0.],\n                      [0., ea, 0., 0., 0.],\n                      [0., 0., 1., 0., 0.],\n                      [0., 0., 0., e_a, 0.],\n                      [0., 0., 0., 0., e_2a]], dtype=np.complex64)\n\n    Ec = np.asmatrix([[e2c, 0., 0., 0., 0.],\n                      [0., ec, 0., 0., 0.],\n                      [0., 0., 1., 0., 0.],\n                      [0., 0., 0., e_c, 0.],\n                      [0., 0., 0., 0., e_2c]], dtype=np.complex64)\n    \"\"\"\n\n    return np.matmul(np.matmul(Ea, d), Ec)", "\ndef complex_wigner_1_(a, b, c):\n    cb = np.cos(b)\n    sb = np.sin(b)\n\n    ea = np.exp(1j * a)\n    ec = np.exp(1j * c)\n\n    e_a = np.exp(-1j * a)\n    e_c = np.exp(-1j * c)\n\n    d11 = (1.+cb)/2.\n    d10 = -sb/(np.sqrt(2.))\n    d1_1 = (1.-cb)/2.\n    d00 = cb\n\n    d = np.asmatrix([[d11, -d10, d1_1],\n                     [d10, d00, -d10],\n                     [d1_1, d10, d11]], dtype=np.complex64)\n\n    d = d.T\n\n    Ea = np.asmatrix([[e_a, 0., 0.],\n                     [0., 1., 0.],\n                     [0., 0., ea]], dtype=np.complex64)\n\n    Ec = np.asmatrix([[e_c, 0., 0.],\n                     [0., 1., 0.],\n                     [0., 0., ec]], dtype=np.complex64)\n\n    return np.matmul(np.matmul(Ea, d), Ec)", "\n\n\n\n\ndef complex_wigner_(l, a, b, c):\n    assert (l == 0 or l == 1 or l == 2)\n    if l == 0:\n        return np.asmatrix([[1.]], dtype=np.complex64)\n    if l == 1:\n        return complex_wigner_1_(a, b, c)\n    if l == 2:\n        return complex_wigner_2_(a, b, c)", "\n\"\"\"\ncompute the coefficient d^l_{jk} of the wigner d matrix encoding the action of a rotation of angle b \naround the y axis on the real (and complex) spherical harmonic basis of degree l\n\"\"\"\ndef wigner_d_matrix_coeffs(l, j, k, b):\n    p = np.math.factorial(l+j)*np.math.factorial(l-j)*np.math.factorial(l+k)*np.math.factorial(l-k)\n    p = np.sqrt(p)\n\n    # l + k - s >= 0\n    # s >= 0\n    # j - k + s >= 0\n    # l - j - s >= 0\n\n    # l + k >= s\n    # s >= 0\n    # s >= k - j\n    # l - j >= s\n\n    s1 = np.max([0, k-j])\n    s2 = np.min([l+k, l-j])\n    s_ = np.sin(b/2.)\n    c_ = np.cos(b/2.)\n    d = 0.\n    for s in range(s1, s2+1):\n        q = np.math.factorial(l+k-s)*np.math.factorial(s)*np.math.factorial(j-k+s)*np.math.factorial(l-j-s)\n        x = (1.*p)/(1.*q)\n        x *= (-1)**(j-k+s)\n        x *= (c_**(2*l+k-j-2*s))*(s_**(j-k+2*s))\n        d += x\n    return d", "\n\"\"\"\ncompute the wigner d matrix d^l encoding the action of a rotation of angle b \naround the y axis on the real (and complex) spherical harmonic basis of degree l\n\"\"\"\ndef wigner_d_matrix(l, b, dtype=np.float32):\n    d = np.zeros(shape=(2*l+1, 2*l+1), dtype=dtype)\n    \"\"\"\n    for m in range((2*l+1)*(2*l+1)):\n        k = m % (2*l+1)\n        j = np.int((m - k) / (2*l+1))\n        d[j, k] = wigner_d_matrix_coeffs(l, j-l, k-l, b)\n    \"\"\"\n    for j in range(2*l+1):\n        for k in range(2*l+1):\n            d[j, k] = wigner_d_matrix_coeffs(l, j-l, k-l, b)\n    return np.asmatrix(d)", "\n\"\"\"\ncompute the action of rotation of angle a around the z axis on the \ncomplex spherical harmonic basis of degree l\n\"\"\"\ndef diag_exp(l, a):\n    e = np.zeros(shape=(2*l+1, 2*l+1), dtype=np.complex64)\n\n    for m in range(l+1):\n        e[m + l, m + l] = np.exp(m * 1j * a)\n        e[m, m] = np.exp((m - l) * 1j * a)\n\n\n    return np.asmatrix(e)", "\n\n\"\"\"\ndef complex_D_wigner(l, a, b, c):\n    D = diag_exp(l, a)*wigner_d_matrix(l, b, dtype=np.complex64)*diag_exp(l, c)\n    return np.conjugate(D)\n\"\"\"\n\n\ndef complex_D_wigner(l, a, b, c):\n\n    d = wigner_d_matrix(l, b, dtype=np.complex64)\n    # ea = diag_exp(l, a)\n    # ec = diag_exp(l, c)\n    # D = np.matmul(np.matmul(ea, d), ec)\n    D = d\n    # print(d)\n    for p in range(2*l+1):\n        for q in range(2*l+1):\n            # D[q, p] *= np.exp(-(p-l)*1j*a)*np.exp(-(q-l)*1j*c)\n            D[p, q] *= np.exp(-(p - l) * 1j * a) * np.exp(-(q - l) * 1j * c)\n    # np.conjugate(D)\n    # print(D)\n    # D = np.flip(D, axis=0)\n    # D = np.flip(D, axis=1)\n    # D = np.conjugate(D)\n    return D", "\ndef complex_D_wigner(l, a, b, c):\n\n    d = wigner_d_matrix(l, b, dtype=np.complex64)\n    # ea = diag_exp(l, a)\n    # ec = diag_exp(l, c)\n    # D = np.matmul(np.matmul(ea, d), ec)\n    D = d\n    # print(d)\n    for p in range(2*l+1):\n        for q in range(2*l+1):\n            # D[q, p] *= np.exp(-(p-l)*1j*a)*np.exp(-(q-l)*1j*c)\n            D[p, q] *= np.exp(-(p - l) * 1j * a) * np.exp(-(q - l) * 1j * c)\n    # np.conjugate(D)\n    # print(D)\n    # D = np.flip(D, axis=0)\n    # D = np.flip(D, axis=1)\n    # D = np.conjugate(D)\n    return D", "\ndef real_D_wigner(l, a, b, c):\n    C = complex_to_real_sh(l)\n    D = complex_D_wigner(l, a, b, c)\n    D = np.real(C*D*np.conjugate(C.T))\n\n    \"\"\"\n    Da = complex_D_wigner(l, a, 0., 0.)\n    Da = np.real(C * Da * np.conjugate(C.T))\n    Db = complex_D_wigner(l, 0., b, 0.)\n    Db = np.real(C * Db * np.conjugate(C.T))\n    Dc = complex_D_wigner(l, 0., 0., c)\n    Dc = np.real(C * Dc * np.conjugate(C.T))\n    \"\"\"\n\n\n    # return np.conjugate(C.T)*D*C\n    # return np.real(C*D*np.conjugate(C.T))\n    # return np.real(Da*Db*Dc)\n    return D", "\n\n\"\"\"\nreturn a list of real wigner matrices D^l for l in [|0, l_max|]\n\"\"\"\ndef real_D_wigner_from_euler(l_max, a, b, c):\n    D = np.zeros(((l_max+1)**2, (l_max+1)**2))\n    k = 0\n    for l in range(l_max+1):\n        D[k:k+(2*l+1), k:k+(2*l+1)] = real_D_wigner(l, a, b, c)\n        k += 2*l+1\n    return D", "\n\"\"\"\nreturn a list of real wigner matrices D^l for l in [|0, l_max|]\nparametrized by a quaternion q\n\"\"\"\ndef real_D_wigner_from_quaternion(l_max, q):\n    r = Rotation(q)\n    euler = r.as_euler('zyz')\n    return real_D_wigner_from_euler(l_max, euler[0], euler[1], euler[2])\n", "\n# to debug\n\"\"\"\ndef wigner_d_matrix_(l, k1, k2, b):\n    k = np.min([l + k2, l - k2, l + k1, l - k1])\n    a = 0\n    lbd = 0\n    if k == l + k2:\n        a = k1 - k2\n        lbd = k1 - k2", "        a = k1 - k2\n        lbd = k1 - k2\n    if k == l - k2:\n        a = k2 - k1\n        lbd = 0\n    if k == l + k1:\n        a = k2 - k1\n        lbd = 0\n    if k == l - k1:\n        a = k1 - k2", "    if k == l - k1:\n        a = k1 - k2\n        lbd = k1 - k2\n\n    s_ = np.sin(b/2.)\n    c_ = np.sin(b/2.)\n    c = np.cos(b)\n\n    b = 2*(l-k)-a\n    d = scipy.special.jacobi(a, b, k)(c)", "    b = 2*(l-k)-a\n    d = scipy.special.jacobi(a, b, k)(c)\n    d *= (s_**a)*(c_**b)\n    d *= np.sqrt(scipy.special.binom(2*l-k, k+a)/scipy.special.binom(k+b, b))\n    d *= (-1)**lbd\n\n    return d\n\ndef wigner_d_matrix(l, b, dtype=np.float32):\n    d = np.zeros(shape=(2*l+1, 2*l+1), dtype=dtype)", "def wigner_d_matrix(l, b, dtype=np.float32):\n    d = np.zeros(shape=(2*l+1, 2*l+1), dtype=dtype)\n    for k1 in range(-l, l+1):\n        for k2 in range(-l, l+1):\n            d[k1+l, k2+l] = wigner_d_matrix_(l, k1, k2, b)\n    return d\n\ndef wigner_D_matrix(l, a, b, c):\n    D = np.asmatrix(wigner_d_matrix(l, b, dtype=np.complex64))\n    for k1 in range(-l, l+1):", "    D = np.asmatrix(wigner_d_matrix(l, b, dtype=np.complex64))\n    for k1 in range(-l, l+1):\n        D[k1, :] = np.exp(-k1*1j*a)*D[k1, :]\n    for k2 in range(-l, l+1):\n        D[:, k2] = np.exp(-k2 * 1j * c) * D[:, k2]\n    return D\n\"\"\"\n\"\"\"\ndef unit_test3():\n    angles = np.random.rand(3)", "def unit_test3():\n    angles = np.random.rand(3)\n    # angles[0] = 0.0\n    # angles[1] = 0.0\n    # angles[2] = angles[0]\n    # angles = [0., np.pi/2., 0.]\n    # angles = [0., 1., 0.]\n    R = euler_rot_zyz(angles[0], angles[1], angles[2])\n    print(R)\n    X = np.asmatrix(np.random.rand(1, 3))", "    print(R)\n    X = np.asmatrix(np.random.rand(1, 3))\n    X_rot = (np.matmul(R, X.T)).T\n    D = complex_wigner_2_(angles[0], angles[1], angles[2])\n    # D = wigner_D_matrix(2, angles[0], angles[1], angles[2])\n\n    print(angles[0])\n    print(angles[1])\n    print(np.exp(1j*angles[2]))\n    print(D*D.T)", "    print(np.exp(1j*angles[2]))\n    print(D*D.T)\n\n    print('orthogonality')\n    print(np.linalg.norm(D * (D.conjugate()).T-np.eye(5)))\n    # print(np.linalg.norm(D_bis * (D_bis.conjugate()).T - np.eye(5)))\n\n    sh = complex_sh_(2, X)\n    sh_rot = complex_sh_(2, X_rot)\n    D_inv = (D.conjugate()).T", "    sh_rot = complex_sh_(2, X_rot)\n    D_inv = (D.conjugate()).T\n    y = np.matmul(D, sh.T) - sh_rot.T\n    print(np.linalg.norm(y))\n\n    print(np.real(y))\n    print(np.real(sh-sh_rot))\n    print(np.imag(y))\n    print(np.imag(sh - sh_rot))\n", "    print(np.imag(sh - sh_rot))\n\n    # print(complex_sh_2_(np.asmatrix([[0., 1., 0.]])))\n    # print(sh_rot)\n    # print(np.sqrt(np.multiply(np.real(y), np.real(y)) + np.multiply(np.imag(y), np.imag(y))))\n    # print(y)\n    # print(D+D.conjugate())\n\"\"\"\n\ndef complex_wigner_matrix_unit_test_(l, a, b, c, X):\n    # angles = np.random.rand(3)\n    # a = angles[0]\n    # b = angles[1]\n    # c = angles[2]\n\n    D = complex_D_wigner(l, a, b, c)\n\n    # D_ = complex_wigner_(l, a, b, c)\n    # D_ = np.conjugate(D_.T)\n\n\n\n    # print('u')\n    # print(np.linalg.norm(D-D_))\n\n\n\n    R = euler_rot_zyz(a, b, c)\n    X_rot = (np.matmul(R.T, X.T)).T\n\n    Y = complex_sh_(l, X)\n    Y_rot = complex_sh_(l, X_rot)\n\n    y = np.matmul(D, Y) - Y_rot\n    # print(np.linalg.norm(Y))\n    print(np.linalg.norm(y))", "\ndef complex_wigner_matrix_unit_test_(l, a, b, c, X):\n    # angles = np.random.rand(3)\n    # a = angles[0]\n    # b = angles[1]\n    # c = angles[2]\n\n    D = complex_D_wigner(l, a, b, c)\n\n    # D_ = complex_wigner_(l, a, b, c)\n    # D_ = np.conjugate(D_.T)\n\n\n\n    # print('u')\n    # print(np.linalg.norm(D-D_))\n\n\n\n    R = euler_rot_zyz(a, b, c)\n    X_rot = (np.matmul(R.T, X.T)).T\n\n    Y = complex_sh_(l, X)\n    Y_rot = complex_sh_(l, X_rot)\n\n    y = np.matmul(D, Y) - Y_rot\n    # print(np.linalg.norm(Y))\n    print(np.linalg.norm(y))", "\n\ndef complex_wigner_matrix_unit_test():\n    for i in range(10):\n        angles = np.random.rand(3)\n        a = angles[0]\n        b = angles[1]\n        c = angles[2]\n\n        # a = 0.\n        # b = 0.\n        # c = 0.\n\n        X = np.asmatrix(np.random.rand(1, 3))\n        print('wigner D test ', i)\n        for l in range(4):\n            print('l = ', l)\n            complex_wigner_matrix_unit_test_(l, a, b, c, X)", "\ndef real_wigner_matrix_unit_test_(l, a, b, c, X):\n    # angles = np.random.rand(3)\n    # a = angles[0]\n    # b = angles[1]\n    # c = angles[2]\n\n    D = real_D_wigner(l, a, b, c)\n\n    # D_ = complex_wigner_(l, a, b, c)\n    # D_ = np.conjugate(D_.T)\n\n\n\n    # print('u')\n    # print(np.linalg.norm(D-D_))\n\n\n\n    R = euler_rot_zyz(a, b, c)\n    X_rot = (np.matmul(R.T, X.T)).T\n\n    Y = real_sh_(l, X)\n    Y_rot = real_sh_(l, X_rot)\n\n    y = np.matmul(D, Y) - Y_rot\n    # print(np.linalg.norm(Y))\n    print(np.linalg.norm(y))", "\n\ndef real_wigner_matrix_unit_test():\n    for i in range(10):\n        angles = np.random.rand(3)\n        a = angles[0]\n        b = angles[1]\n        c = angles[2]\n\n        # a = 0.\n        # b = 0.\n        # c = 0.\n\n        X = np.asmatrix(np.random.rand(1, 3))\n        print('wigner D test ', i)\n        for l in range(4):\n            print('l = ', l)\n            real_wigner_matrix_unit_test_(l, a, b, c, X)", "\n\n\n\n"]}
{"filename": "cafi_net/spherical_harmonics/kernels_density.py", "chunked_list": ["from spherical_harmonics.kernels import *\nfrom utils.pointcloud_utils import GroupPoints_grid, GroupPoints_density\n\nclass SphericalHarmonicsGaussianKernels_density(torch.nn.Module):\n    def __init__(self, l_max, gaussian_scale, num_shells, transpose=False, bound=True):\n        super(SphericalHarmonicsGaussianKernels_density, self).__init__()\n        self.l_max = l_max\n        self.monoms_idx = torch_monomial_basis_3D_idx(l_max)\n        self.gaussian_scale = gaussian_scale\n        self.num_shells = num_shells\n        self.transpose = True\n        self.Y = torch_spherical_harmonics_basis(l_max, concat=True)\n        self.split_size = []\n        self.sh_idx = []\n        self.bound = bound\n        for l in range(l_max + 1):\n            self.split_size.append(2*l+1)\n            self.sh_idx += [l]*(2*l+1)\n        self.sh_idx = torch.from_numpy(np.array(self.sh_idx)).to(torch.int64)\n\n\n\n    def forward(self, x):\n        #target_density = x[\"target_density\"]\n        #target_density = target_density.squeeze(-1)\n        \n        #target_density[target_density > 0.] = 1\n        #target_density[target_density <= 0.]  = 0\n        \n        \n        if \"patches dist\" in x:\n            patches_dist = x[\"patches dist\"].unsqueeze(-1)\n        else:\n            patches_dist = torch.linalg.norm(x[\"patches\"], dim=-1, keepdims=True)\n        normalized_patches = x[\"patches\"] / torch.maximum(patches_dist, torch.tensor(0.000001).type_as(x[\"patches\"]))\n        if self.transpose:\n            normalized_patches = -normalized_patches\n        # print(normalized_patches.shape)\n        monoms_patches = torch_eval_monom_basis(normalized_patches, self.l_max, idx=self.monoms_idx)\n        # print(self.Y.shape)\n        \n        #x[\"patches_density\"][target_density == 0, ...] = 0.\n        \n        #sh_patches = x[\"patches_density\"] * torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n        \n        sh_patches =  torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n        # print(sh_patches.shape)\n        #return sh_patches\n        shells_rad = torch.arange(self.num_shells).type_as(monoms_patches) / (self.num_shells-1)\n\n        shells_rad = torch.reshape(shells_rad, (1, 1, 1, -1))\n        shells = patches_dist - shells_rad\n        shells = torch.exp(-self.gaussian_scale*(shells * shells))\n        shells_sum = torch.sum(shells, dim=-1, keepdims=True)\n        shells = (shells / torch.maximum(shells_sum, torch.tensor(0.000001).type_as(shells)))\n\n        shells = shells.unsqueeze(-2)\n        if self.bound:\n            shells = torch.where(patches_dist.unsqueeze(-1) <= torch.tensor(1.).type_as(shells), shells, torch.tensor(0.).type_as(shells))\n\n        sh_patches = sh_patches.unsqueeze(-1)\n        sh_patches = shells * sh_patches\n\n\n        # L2 norm\n        l2_norm = torch.sum((sh_patches * sh_patches), dim=2, keepdims=True)\n        l2_norm = torch.split(l2_norm, split_size_or_sections=self.split_size, dim=-2)\n        Y = []\n        for l in range(len(l2_norm)):\n            ml = torch.sum(l2_norm[l], dim=-2, keepdims=True)\n            ml = torch.sqrt(ml + 1e-7)\n            Y.append(ml)\n        l2_norm = torch.cat(Y, dim=-2)\n        l2_norm = torch.mean(l2_norm, dim=1, keepdims=True)\n        l2_norm = torch.maximum(l2_norm, torch.tensor(1e-8).type_as(l2_norm))\n        # print(l2_norm.shape)\n        l2_norm = l2_norm[..., self.sh_idx, :]\n        sh_patches = (sh_patches / (l2_norm + 1e-6))\n\n        return sh_patches", "\nclass ShGaussianKernelConv_grid(torch.nn.Module):\n    def __init__(self, l_max, l_max_out=None, transpose=False, num_source_points=None):\n        super(ShGaussianKernelConv_grid, self).__init__()\n        self.l_max = l_max\n        self.split_size = []\n        for l in range(l_max + 1):\n            self.split_size.append(2 * l + 1)\n        # self.output_type = output_type\n        self.l_max_out = l_max_out\n        # self.transpose = transpose\n        self.num_source_points = num_source_points\n        self.Q = torch_clebsch_gordan_decomposition(l_max=max(l_max_out, l_max),\n                                                 sparse=False,\n                                                 output_type='dict',\n                                                 l_max_out=l_max_out)\n\n    def forward(self, x):\n        assert (isinstance(x, dict))\n        \n        signal = []\n        features_type = []\n        channels_split_size = []\n        for l in x:\n            if l.isnumeric():\n                features_type.append(int(l))\n                channels_split_size.append(x[l].shape[-2] * x[l].shape[-1])\n                signal.append(torch.reshape(x[l], (x[l].shape[0], x[l].shape[1], -1)))\n\n\n        signal = torch.cat(signal, dim=-1)\n        batch_size = signal.shape[0]\n        patch_size = x[\"kernels\"].shape[2]\n        num_shells = x[\"kernels\"].shape[-1]\n\n        # Changed and removed transpose here\n        if \"patches idx\" in x:\n            # print(signal.shape, \"signal\")\n            B, N, _ = signal.shape\n            H = int(np.cbrt(N))\n            # print(x[\"patches idx\"].shape, \"idx\")\n            B2, N_s, K, _ = x[\"patches idx\"].shape\n\n            H_s = int(np.cbrt(N_s))\n            \n            signal = signal.reshape(B, H, H, H, -1).permute(0, 4, 1, 2, 3)\n            \n            id_sample = x[\"patches idx\"]\n            id_sample = id_sample.permute((0, -1, 1, 2))\n            id_sample = id_sample.reshape(B, -1, H_s, H_s, H_s*K)\n            id_sample = id_sample.permute((0, 2, 3, 4, 1))\n            signal = torch.nn.functional.grid_sample(signal, id_sample, align_corners = True).reshape(B, -1, H_s, H_s, H_s, K).permute(0, 2, 3, 4, 5, 1)\n            # print(\"patches mask found\") \n\n            if \"patches mask\" in x:\n                # print(\"patches mask found\") \n                mask_patches = x[\"patches mask\"]\n                signal[mask_patches, :] = 0 \n\n            signal = signal.reshape(B, H_s*H_s*H_s, K, -1)\n            \n            \n        num_points_target = signal.shape[1]\n        kernels = torch.reshape(x[\"kernels\"], (batch_size, num_points_target, patch_size, -1))\n        \n        y = torch.einsum('bvpy,bvpc->bvyc', kernels, signal)\n\n\n\n        # split y\n        # print(channels_split_size, y.shape)\n        y_ = torch.split(y, split_size_or_sections=channels_split_size, dim=-1)\n        y = {str(j): [] for j in range(self.l_max_out + 1)}\n        y_cg = []\n        for i in range(len(channels_split_size)):\n            l = features_type[i]\n            yi = torch.reshape(y_[i], (batch_size, num_points_target, -1, num_shells, 2 * l + 1, x[str(l)].shape[-1]))\n            yi = yi.permute(0, 1, 2, 4, 3, 5)\n            yi = torch.reshape(yi, (batch_size, num_points_target, -1, 2 * l + 1, num_shells*x[str(l)].shape[-1]))\n            yi = torch.split(yi, split_size_or_sections=self.split_size, dim=2)\n            for j in range(len(self.split_size)):\n\n                yij = yi[j]\n                if l == 0:\n                    y[str(j)].append(yij[:, :, :, 0, :])\n                elif j == 0:\n                    y[str(l)].append(yij[:, :, 0, :, :])\n                else:\n                    y_cg.append(yij)\n\n        y_cg = self.Q.decompose(y_cg)\n\n\n        for J in y_cg:\n            if J not in y:\n                y[J] = []\n            y[J].append(y_cg[J])\n        for J in y:\n            y[J] = torch.cat(y[J], dim=-1)\n        return y", "\n\n\nif __name__==\"__main__\":\n\n    gi = GroupPoints_density(0.4, 32)\n    in_dict = {}\n    x = torch.randn(2, 32 *32 *32, 3).cuda()\n    y = torch.randn(2, 16 *16 *16, 3).cuda()\n    x_d = torch.randn(2, 32 *32 *32, 1).cuda()\n    y_d = torch.randn(2, 16 *16 *16, 1).cuda()\n    in_dict[\"source points\"] = x\n    in_dict[\"target points\"] = y\n    in_dict[\"source density\"] = x_d\n    in_dict[\"target density\"] = y_d\n    x2 = y\n    out = gi(in_dict)\n    for key in out:\n        print(key, \" \", out[key].shape)\n    B = x.shape[0]\n    H, W, D= 32, 32, 32\n    K = out[\"patches source\"].shape[-2]\n    # B, H, W, D, K, _ = out[\"patches source\"].shape\n    k = SphericalHarmonicsGaussianKernels_density(l_max = 3, gaussian_scale = 0.1, num_shells = 3, bound = True).cuda()\n    print(out[\"patches source\"].shape)\n    out_2 = k({\"patches\": out[\"patches source\"].reshape(B, -1, K, 3), \"patches dist\": out[\"patches dist source\"].reshape(B, -1, K), \"patches_density\": out[\"patches source density\"].reshape(B, -1, K, 1)}) # B, H*W*D, K, 16, 3\n\n    conv_layer = ShGaussianKernelConv_grid(l_max=3, l_max_out=3).cuda()\n    y = {}\n    x2 = x2.reshape(B, -1, 3)\n    y[\"source points\"] = x.reshape(B, -1, 3)\n    y[\"target points\"] = x2\n    y[\"patches idx\"] = out[\"patches idx source\"].reshape(B, -1, K, 3)\n    y[\"patches dist source\"] = out[\"patches dist source\"].reshape(B, -1, K)\n    y[\"kernels\"] = out_2\n    # w = gauss_normalization(y[\"patches dist source\"], 1./3.)\n\n    \n\n    if '1' in y:\n        y['1'] = torch.cat([y['1'], x2], dim=-1)\n    else:\n        y['1'] = x2.unsqueeze(-1)\n\n    y = conv_layer(y)", "\n    # for key in y:\n    #     print(y[key], \" \", key, \" \", y[key].shape)\n    \n"]}
{"filename": "cafi_net/spherical_harmonics/kernels.py", "chunked_list": ["from sympy import *\nimport numpy as np\nimport torch, h5py\n\nfrom utils.group_points import gather_idx, GroupPoints\nfrom spherical_harmonics.clebsch_gordan_decomposition import torch_clebsch_gordan_decomposition\n\ndef associated_legendre_polynomial(l, m, z, r2):\n    P = 0\n    if l < m:\n        return P\n    for k in range(int((l-m)/2)+1):\n        pk = (-1.)**k * (2.)**(-l) * binomial(l, k) * binomial(2*l-2*k, l)\n        pk *= (factorial(l-2*k) / factorial(l-2*k-m)) * r2**k * z**(l-2*k-m)\n        P += pk\n    P *= np.sqrt(float(factorial(l-m)/factorial(l+m)))\n    return P", "\ndef A(m, x, y):\n    a = 0\n    for p in range(m+1):\n        a += binomial(m, p) * x**p * y**(m-p) * cos((m-p)*(pi/2.))\n    return a\n\ndef B(m, x, y):\n    b = 0\n    for p in range(m+1):\n        b += binomial(m, p) * x**p * y**(m-p) * sin((m-p)*(pi/2.))\n    return b", "\"\"\"\ncomputes the unnormalized real spherical harmonic Y_{lm} as a polynomial\nof the euclidean coordinates x, y, z\n\"\"\"\ndef real_spherical_harmonic(l, m, x, y, z, poly = False):\n    K = np.sqrt((2*l+1)/(2*np.pi))\n    r2 = x**2 + y**2 + z**2\n    if m > 0:\n        Ylm = K * associated_legendre_polynomial(l, m, z, r2) * A(m, x, y)\n    elif m < 0:\n        Ylm = K * associated_legendre_polynomial(l, -m, z, r2) * B(-m, x, y)\n    else:\n        K = np.sqrt((2 * l + 1) / (4 * np.pi))\n        Ylm = K * associated_legendre_polynomial(l, 0, z, r2)\n    if poly:\n        Ylm = Poly(simplify(expand(Ylm)), x, y, z)\n    return Ylm", "\ndef binom(n, k):\n    if k == 0.:\n        return 1.\n    return gamma(n + 1) / (gamma(n-k+1)*gamma(k+1))\n\n\"\"\"\ncomputes radial Zernike polynomials (divided by r^{2l})\n\"\"\"\ndef zernike_polynomial_radial(n, l, D, r2):\n    if (l > n):\n        return 0\n    if ((n-l) % 2 != 0):\n        return 0\n    R = 0\n    for s in range(int((n-l) / 2) + 1):\n        R += (-1)**s * binom((n-l)/2, s)*binom(s-1+(n+l+D)/2., (n-l)/2)*r2**s\n    R *= (-1)**((n-l)/2)*np.sqrt(2*n+D)\n    return R", "\"\"\"\ndef zernike_polynomial_radial(n, l, D, r2):\n    if (l > n):\n        return 0\n    if ((n-l) % 2 != 0):\n        return 0\n    R = 0\n    for s in range(int((n-l) / 2) + 1):\n        R += (-1)**s * binom((n-l)/2, s)*binom(s-1+(n+l+D)/2., (n-l)/2)*r2**s\n    R *= (-1)**((n-l)/2)*np.sqrt(2*n+D)\n    return R", "\n\"\"\"\ncomputes the 3D Zernike polynomials.\n\"\"\"\ndef zernike_kernel_3D(n, l, m, x, y, z):\n    r2 = x**2 + y**2 + z**2\n    return zernike_polynomial_radial(n, l, 3, r2)*real_spherical_harmonic(l, m, x, y, z)\n    # return real_spherical_harmonic(l, m, x, y, z)\n\n\"\"\"", "\n\"\"\"\ncomputes the monomial basis in x, y, z up to degree d\n\"\"\"\ndef monomial_basis_3D(d):\n    monoms_basis = []\n    for I in range((d + 1) ** 3):\n        i = I % (d + 1)\n        a = int((I - i) / (d + 1))\n        j = a % (d + 1)\n        k = int((a - j) / (d + 1))\n        if (i + j + k <= d):\n            monoms_basis.append((i, j, k))\n\n    monoms_basis = list(set(monoms_basis))\n    monoms_basis = sorted(monoms_basis)\n    return monoms_basis", "\ndef torch_monomial_basis_3D_idx(d):\n    m = monomial_basis_3D(d)\n    idx = np.zeros((len(m), 3), dtype=np.int32)\n    for i in range(len(m)):\n        for j in range(3):\n            idx[i, j] = m[i][j]\n    return torch.from_numpy(idx).to(torch.int64)\n\n\"\"\"", "\n\"\"\"\ncomputes the coefficients of a list of polynomials in the monomial basis\n\"\"\"\ndef np_monomial_basis_coeffs(polynomials, monoms_basis):\n    n_ = len(monoms_basis)\n    m_ = len(polynomials)\n    M = np.zeros((m_, n_))\n    for i in range(m_):\n        for j in range(n_):\n            M[i, j] = polynomials[i].coeff_monomial(monoms_basis[j])\n    return M", "\n\nclass ShGaussianKernelConv(torch.nn.Module):\n    def __init__(self, l_max, l_max_out=None, transpose=False, num_source_points=None):\n        super(ShGaussianKernelConv, self).__init__()\n        self.l_max = l_max\n        self.split_size = []\n        for l in range(l_max + 1):\n            self.split_size.append(2 * l + 1)\n        # self.output_type = output_type\n        self.l_max_out = l_max_out\n        # self.transpose = transpose\n        self.num_source_points = num_source_points\n        self.Q = torch_clebsch_gordan_decomposition(l_max=max(l_max_out, l_max),\n                                                 sparse=False,\n                                                 output_type='dict',\n                                                 l_max_out=l_max_out)\n\n    def forward(self, x):\n        assert (isinstance(x, dict))\n        \n        signal = []\n        features_type = []\n        channels_split_size = []\n        for l in x:\n            if l.isnumeric():\n                features_type.append(int(l))\n                channels_split_size.append(x[l].shape[-2] * x[l].shape[-1])\n                signal.append(torch.reshape(x[l], (x[l].shape[0], x[l].shape[1], -1)))\n\n\n        signal = torch.cat(signal, dim=-1)\n        batch_size = signal.shape[0]\n        patch_size = x[\"kernels\"].shape[2]\n        num_shells = x[\"kernels\"].shape[-1]\n\n        # Changed and removed transpose here\n        if \"patches idx\" in x:\n            # print(signal.shape, \"signal shape\", x[\"patches idx\"].shape) B, N, 3\n            signal = gather_idx(signal, x[\"patches idx\"])\n            # print(signal.shape)\n\n        num_points_target = signal.shape[1]\n        kernels = torch.reshape(x[\"kernels\"], (batch_size, num_points_target, patch_size, -1))\n        y = torch.einsum('bvpy,bvpc->bvyc', kernels, signal)\n\n\n\n        # split y\n        # print(channels_split_size, y.shape)\n        y_ = torch.split(y, split_size_or_sections=channels_split_size, dim=-1)\n        y = {str(j): [] for j in range(self.l_max_out + 1)}\n        y_cg = []\n        for i in range(len(channels_split_size)):\n            l = features_type[i]\n            yi = torch.reshape(y_[i], (batch_size, num_points_target, -1, num_shells, 2 * l + 1, x[str(l)].shape[-1]))\n            yi = yi.permute(0, 1, 2, 4, 3, 5)\n            yi = torch.reshape(yi, (batch_size, num_points_target, -1, 2 * l + 1, num_shells*x[str(l)].shape[-1]))\n            yi = torch.split(yi, split_size_or_sections=self.split_size, dim=2)\n            for j in range(len(self.split_size)):\n\n                yij = yi[j]\n                if l == 0:\n                    y[str(j)].append(yij[:, :, :, 0, :])\n                elif j == 0:\n                    y[str(l)].append(yij[:, :, 0, :, :])\n                else:\n                    y_cg.append(yij)\n        \n        y_cg = self.Q.decompose(y_cg)\n\n\n        for J in y_cg:\n            if J not in y:\n                y[J] = []\n            y[J].append(y_cg[J])\n        for J in y:\n            y[J] = torch.cat(y[J], dim=-1)\n        return y", "\n\n\n\"\"\"\ncomputes the coefficients of the spherical harmonics in the monomial basis\n\"\"\"\ndef spherical_harmonics_3D_monomial_basis(l, monoms_basis):\n    x, y, z = symbols(\"x y z\")\n    n_ = len(monoms_basis)\n    M = np.zeros((2*l+1, n_))\n    for m in range(2*l+1):\n        Y = real_spherical_harmonic(l, m-l, x, y, z)\n        Y = expand(Y)\n        Y = poly(Y, x, y, z)\n        for i in range(n_):\n            M[m, i] = N(Y.coeff_monomial(monoms_basis[i]))\n    return M", "\n\"\"\"\ncomputes the coefficients of the Zernike polynomials in the monomial basis\n\"\"\"\ndef zernike_kernel_3D_monomial_basis(n, l, monoms_basis):\n    x, y, z = symbols(\"x y z\")\n    n_ = len(monoms_basis)\n    M = np.zeros((2*l+1, n_))\n    for m in range(2*l+1):\n        Z = zernike_kernel_3D(n, l, m-l, x, y, z)\n        Z = expand(Z)\n        Z = poly(Z, x, y, z)\n        for i in range(n_):\n            M[m, i] = N(Z.coeff_monomial(monoms_basis[i]))\n    return M", "\n\n\"\"\"\ncomputes the matrix of an offset in the monomial basis (up to degree d)\n(m_1(x-a), ..., m_k(x-a)) = A(a).(m_1(x), ..., m_k(x))\n\"\"\"\ndef np_monom_basis_offset(d):\n    monoms_basis = monomial_basis_3D(d)\n    n = len(monoms_basis)\n    idx = np.full(fill_value=-1, shape=(n, n), dtype=np.int32)\n    coeffs = np.zeros(shape=(n, n))\n\n    for i in range(n):\n        pi = monoms_basis[i][0]\n        qi = monoms_basis[i][1]\n        ri = monoms_basis[i][2]\n        for j in range(n):\n            pj = monoms_basis[j][0]\n            qj = monoms_basis[j][1]\n            rj = monoms_basis[j][2]\n            if (pj >= pi) and (qj >= qi) and (rj >= ri):\n                idx[j, i] = monoms_basis.index((pj-pi, qj-qi, rj-ri))\n                coeffs[j, i] = binomial(pj, pi)*binomial(qj, qi)*binomial(rj, ri)*((-1.)**(pj-pi+qj-qi+rj-ri))\n    return coeffs, idx", "\n\n\"\"\"\ncomputes the 3D zernike basis up to degree d\n\"\"\"\ndef np_zernike_kernel_basis(d):\n    monoms_basis = monomial_basis_3D(d)\n    Z = []\n    for l in range(d+1):\n        Zl = []\n        # for n in range(min(2*d - l + 1, l + 1)):\n        #    if (n - l) % 2 == 0:\n        for n in range(l, d+1):\n            if (n - l) % 2 == 0 and d >= n:\n                Zl.append(zernike_kernel_3D_monomial_basis(n, l, monoms_basis))\n        Z.append(np.stack(Zl, axis=0))\n    return Z", "\n\n\n\n\"\"\"\ncomputes the 3D spherical harmonics basis up to degree l_max\n\"\"\"\ndef torch_spherical_harmonics_basis(l_max, concat=False):\n    monoms_basis = monomial_basis_3D(l_max)\n    Y = []\n    for l in range(l_max+1):\n        Yl = spherical_harmonics_3D_monomial_basis(l, monoms_basis)\n        Y.append(torch.from_numpy(Yl).to(torch.float32))\n    if concat:\n        Y = torch.cat(Y, dim=0)\n    return Y", "\ndef np_zernike_kernel(d, n, l):\n    monoms_basis = monomial_basis_3D(d)\n    assert (n >= l and (n - l) % 2 == 0)\n    return zernike_kernel_3D_monomial_basis(n, l, monoms_basis)\n\ndef torch_eval_monom_basis(x, d, idx=None):\n    \"\"\"\n    evaluate monomial basis up to degree d\n    \"\"\"\n\n    batch_size = x.shape[0]\n    num_points = x.shape[1]\n\n    if idx is None:\n        idx = torch_monomial_basis_3D_idx(d)\n    y = []\n    for i in range(3):\n        pows = torch.reshape(torch.arange(d+1), (1, 1, d+1)).to(torch.float32)\n        yi = torch.pow(x[..., i].unsqueeze(-1), pows.type_as(x))\n        y.append(yi[..., idx[..., i]])\n    y = torch.stack(y, dim=-1)\n    y = torch.prod(y, dim=-1, keepdims=False)\n    return y", "\n\n\n\n\nclass SphericalHarmonicsGaussianKernels(torch.nn.Module):\n    def __init__(self, l_max, gaussian_scale, num_shells, transpose=False, bound=True):\n        super(SphericalHarmonicsGaussianKernels, self).__init__()\n        self.l_max = l_max\n        self.monoms_idx = torch_monomial_basis_3D_idx(l_max)\n        self.gaussian_scale = gaussian_scale\n        self.num_shells = num_shells\n        self.transpose = True\n        self.Y = torch_spherical_harmonics_basis(l_max, concat=True)\n        self.split_size = []\n        self.sh_idx = []\n        self.bound = bound\n        for l in range(l_max + 1):\n            self.split_size.append(2*l+1)\n            self.sh_idx += [l]*(2*l+1)\n        self.sh_idx = torch.from_numpy(np.array(self.sh_idx)).to(torch.int64)\n\n\n\n    def forward(self, x):\n        \n        \n        if \"patches dist\" in x:\n            patches_dist = x[\"patches dist\"].unsqueeze(-1)\n        else:\n            patches_dist = torch.linalg.norm(x[\"patches\"], dim=-1, keepdims=True)\n        normalized_patches = x[\"patches\"] / torch.maximum(patches_dist, torch.tensor(0.000001).type_as(x[\"patches\"]))\n        if self.transpose:\n            normalized_patches = -normalized_patches\n        # print(normalized_patches.shape)\n        monoms_patches = torch_eval_monom_basis(normalized_patches, self.l_max, idx=self.monoms_idx)\n        # print(self.Y.shape)\n        sh_patches = torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n        shells_rad = torch.arange(self.num_shells).type_as(monoms_patches) / (self.num_shells-1)\n\n        shells_rad = torch.reshape(shells_rad, (1, 1, 1, -1))\n        shells = patches_dist - shells_rad\n        shells = torch.exp(-self.gaussian_scale*(shells * shells))\n        shells_sum = torch.sum(shells, dim=-1, keepdims=True)\n        shells = (shells / torch.maximum(shells_sum, torch.tensor(0.000001).type_as(shells)))\n\n        shells = shells.unsqueeze(-2)\n        if self.bound:\n            shells = torch.where(patches_dist.unsqueeze(-1) <= torch.tensor(1.).type_as(shells), shells, torch.tensor(0.).type_as(shells))\n\n        sh_patches = sh_patches.unsqueeze(-1)\n        sh_patches = shells * sh_patches\n\n\n        # L2 norm\n        l2_norm = torch.sum((sh_patches * sh_patches), dim=2, keepdims=True)\n        l2_norm = torch.split(l2_norm, split_size_or_sections=self.split_size, dim=-2)\n        Y = []\n        for l in range(len(l2_norm)):\n            ml = torch.sum(l2_norm[l], dim=-2, keepdims=True)\n            ml = torch.sqrt(ml + 1e-7)\n            Y.append(ml)\n        l2_norm = torch.cat(Y, dim=-2)\n        l2_norm = torch.mean(l2_norm, dim=1, keepdims=True)\n        l2_norm = torch.maximum(l2_norm, torch.tensor(1e-8).type_as(l2_norm))\n        # print(l2_norm.shape)\n        l2_norm = l2_norm[..., self.sh_idx, :]\n        sh_patches = (sh_patches / (l2_norm + 1e-6))\n\n        return sh_patches", "\n\n\n\n\n\nif __name__==\"__main__\":\n\n    filename = \"/home/rahul/research/data/sapien_processed/train_refrigerator.h5\"\n    f = h5py.File(filename, \"r\")\n    x = torch.from_numpy(f[\"data\"][:2]).cuda()\n    x2 = torch.from_numpy(f[\"data\"][2:4]).cuda()\n\n    # print(x.shape)\n    # x = (torch.ones((1, 4, 3)) * torch.arange(4).unsqueeze(0).unsqueeze(-1)).cuda()\n    # y = torch_eval_monom_basis(x, 3)\n    # print(x)\n    # print(y, y.shape)\n    gi = GroupPoints(0.2, 32)\n    out = gi({\"source points\": x, \"target points\": x2})\n    \n    sph_kernels = SphericalHarmonicsGaussianKernels(l_max = 3, gaussian_scale = 0.1, num_shells = 3, bound = True).cuda()\n    # x = (torch.ones((1, 100, 32, 3)) * torch.arange(3).unsqueeze(0).unsqueeze(1).unsqueeze(2)).cuda()\n    # print(x.shape)\n    patches = sph_kernels({\"patches\": out[\"patches source\"], \"patches dist\": out[\"patches dist source\"]})\n    # print(patches.shape)\n    conv_layer = ShGaussianKernelConv(l_max=3, l_max_out=3).cuda()\n    y = {}\n    y[\"source points\"] = x\n    y[\"target points\"] = x2\n    y[\"patches idx\"] = out[\"patches idx source\"]\n    y[\"patches dist source\"] = out[\"patches dist source\"]\n    y[\"kernels\"] = patches\n    # w = gauss_normalization(y[\"patches dist source\"], 1./3.)\n\n\n\n    if '1' in y:\n        y['1'] = torch.cat([y['1'], x2], dim=-1)\n    else:\n        y['1'] = x2.unsqueeze(-1)\n\n    y = conv_layer(y)", "    # print(y, y.shape)\n    # for key in y:\n        # print(y[key], \" \", key, \" \", y[key].shape)\n    \n    \n    # print(patches, patches.shape)\n"]}
{"filename": "cafi_net/models/TFN_density.py", "chunked_list": ["import torch\nfrom utils.group_points import GroupPoints\nfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling, SphericalHarmonicsEval, SphericalHarmonicsCoeffs\nfrom spherical_harmonics.kernels import SphericalHarmonicsGaussianKernels, ShGaussianKernelConv\nfrom spherical_harmonics.kernels_density import ShGaussianKernelConv_grid, SphericalHarmonicsGaussianKernels_density\nfrom models.layers import MLP, MLP_layer, set_sphere_weights, apply_layers\nfrom utils.pooling import kd_pooling_3d\nfrom utils.pointcloud_utils import GroupPoints_density, GroupPoints_grid ,GroupPoints_euclidean_density, rotate_density, get_gradient_density,kron\nfrom spherical_harmonics.wigner_matrix import *\nfrom e3nn import o3", "from spherical_harmonics.wigner_matrix import *\nfrom e3nn import o3\nfrom utils.train_utils import mean_center\nfrom spherical_harmonics.clebsch_gordan_decomposition import torch_clebsch_gordan_decomposition\nimport open3d as o3d\n\nclass TFN_grid_density(torch.nn.Module):\n    \"\"\"\n    TFN layer for prediction in Pytorch\n    \"\"\"\n    def __init__(self, sphere_samples = 64, bn_momentum = 0.75, mlp_units = [[32, 32], [64, 64], [128, 256]], l_max = [3, 3, 3], l_max_out = [3, 3, 3], num_shells = [3, 3, 3], radius = [0.4]):\n        super(TFN_grid_density, self).__init__()\n        self.l_max = l_max\n        self.l_max_out = l_max_out\n        self.num_shells = num_shells\n        self.gaussian_scale = []\n        for i in range(len(self.num_shells)):\n            self.gaussian_scale.append(0.69314718056 * ((self.num_shells[i]) ** 2))\n        \n        self.radius = [0.2,0.4,0.8]\n        self.bounded = [True, True, True]\n\n        self.S2 = torch_fibonnacci_sphere_sampling(sphere_samples)\n\n        self.factor = [2, 2, 2]\n        self.patch_size = [1024,512,512]\n\n        self.spacing = [0, 0, 0]\n        self.equivariant_units = [32, 64, 128]\n        self.in_equivariant_channels = [[6, 13, 12, 9], [387, 874, 1065, 966], [771, 1738, 2121, 1926]]\n\n        \n        self.mlp_units =  mlp_units\n        self.in_mlp_units = [32, 64, 128]\n        self.bn_momentum = bn_momentum\n\n        self.grouping_layers = []\n        self.grouping_layers_e = []\n        self.kernel_layers = []\n        self.conv_layers = []\n        self.eval = []\n        self.coeffs = []\n\n        for i in range(len(self.radius)):\n            gi = GroupPoints_density(radius=self.radius[i],\n                             patch_size_source=self.patch_size[i],\n                             spacing_source=self.spacing[i])\n            \n            gi_e = GroupPoints_euclidean_density(radius=self.radius[i],\n                             patch_size_source=self.patch_size[i],\n                             spacing_source=self.spacing[i])\n            self.grouping_layers.append(gi)\n            self.grouping_layers_e.append(gi_e)\n            \n\n            ki = SphericalHarmonicsGaussianKernels_density(l_max=self.l_max[i],\n                                                   gaussian_scale=self.gaussian_scale[i],\n                                                   num_shells=self.num_shells[i],\n                                                   bound=self.bounded[i])\n            ci = ShGaussianKernelConv(l_max=self.l_max[i], l_max_out=self.l_max_out[i])\n\n            self.kernel_layers.append(ki)\n            self.conv_layers.append(ci)\n\n        self.conv_layers = torch.nn.Sequential(*self.conv_layers)\n        self.kernel_layers = torch.nn.Sequential(*self.kernel_layers)\n        self.mlp = []\n        self.equivariant_weights = []\n        self.bn = []\n\n        for i in range(len(self.radius)):\n            self.bn.append(torch.nn.BatchNorm2d(self.equivariant_units[i], momentum=self.bn_momentum))\n            types = [str(l) for l in range(self.l_max_out[i] + 1)]\n            self.equivariant_weights.append(set_sphere_weights(self.in_equivariant_channels[i], self.equivariant_units[i], types=types))\n            self.mlp.append(MLP_layer(self.in_mlp_units[i], self.mlp_units[i], bn_momentum = self.bn_momentum))\n\n        self.mlp = torch.nn.Sequential(*self.mlp)\n        self.bn = torch.nn.Sequential(*self.bn)\n        self.equivariant_weights = torch.nn.Sequential(*self.equivariant_weights)\n\n        self.iSHT = []\n        self.fSHT = []\n        for i in range(len(self.l_max_out)):\n            self.iSHT.append(SphericalHarmonicsEval(l_max=self.l_max_out[i], base=self.S2))\n            self.fSHT.append(SphericalHarmonicsCoeffs(l_max=self.l_max_out[i], base=self.S2))\n            \n        self.Q = torch_clebsch_gordan_decomposition(l_max=l_max_out[-1],\n                                         sparse=False,\n                                         output_type='dict',\n                                         l_max_out=l_max_out[-1])\n        \n    def forward(self, x_grid, x_density):\n        \"\"\"\n        Input:\n            x_grid - [B, H, W, D, 3] - Equivariant density field\n            x_density - [B, H, W, D] - Equivariant density field\n\n        Returns:\n            TFN features - F\n        \"\"\"\n        \n        if len(x_density.shape) == 4:\n            x_density = x_density.unsqueeze(-1) # B, H, W, D, 1\n        points_ = [x_grid]\n        density_ = [x_density]\n        grouped_points = []\n        kernels = []\n\n        \n\n        for i in range(len(self.radius)):\n            pi = kd_pooling_3d(points_[-1], self.factor)\n            di = kd_pooling_3d(density_[-1], self.factor,'max')\n            points_.append(pi)\n            density_.append(di)\n\n        points = []\n        density = []\n        for i in range(len(points_)):\n            pi = points_[i]\n            di = density_[i]            \n            B, H, W, D, _ = pi.shape\n            points.append(pi.reshape(B, -1, 3))\n            density.append(di.reshape(B, -1, 1)) # B, N, 1\n\n        yzx = []\n        for i in range(len(points)):\n            yzx_i = torch.stack([points[i][..., 1], points[i][..., 2], points[i][..., 0]], dim=-1)\n            yzx.append(yzx_i.unsqueeze(-1)) # B, N, 3, 1\n        weighted_density = []\n        for i in range(len(self.radius)):\n            \n            # Finding nearest neighbors of each point to compute graph features\n            \n            \n            gi = self.grouping_layers_e[i]({\"source points\": points[i], \"target points\": points[i + 1], \"source density\": density[i], \"target density\": density[i + 1]})\n            weighted_density.append(gi['weighted density'])\n            B, H, W, D, K, _ = gi[\"patches source\"].shape\n\n            # Computing kernels for patch neighbors\n            ki = self.kernel_layers[i]({\"patches\": gi[\"patches source\"].reshape(B, H*W*D, K, 3), \"patches dist\": gi[\"patches dist source\"].reshape(B, H*W*D, K)})\n            \n\n            # Storing outputs\n            grouped_points.append(gi)\n            kernels.append(ki)\n            \n        ki = kernels[0]\n        features = {}\n        types = [0,1,2,3]\n        ki = ki.squeeze(0)\n        ki = ki.squeeze(-1)\n        dim_start = 0\n        \n        \n        \n        \n        \n        ones_singal =torch.ones((points[0].shape[0], points[0].shape[1], 1, 1)).type_as(x_grid).reshape(-1)\n        ones_singal  = density[0].reshape(-1)\n        y = {'0': ones_singal.reshape(points[0].shape[0], points[0].shape[1], 1, 1).type_as(x_grid)}\n        for i in range(len(self.radius)):\n            y[\"source points\"] = points[i]\n            y[\"target points\"] = points[i + 1]\n            B, H, W, D, K, _ = grouped_points[i][\"patches idx source\"].shape\n            y[\"patches idx\"] = grouped_points[i][\"patches idx source\"].reshape(B, H*W*D, K, -1)\n            y[\"patches dist source\"] = grouped_points[i][\"patches dist source\"].reshape(B, H*W*D, K)\n            y[\"kernels\"] = kernels[i]\n\n            if \"patches mask\" in grouped_points[i]:\n                y[\"patches mask\"] = grouped_points[i][\"patches mask\"]\n            \n            \n            shape_ = density_[i].shape\n            \n            \n            \n            gradient_density_signal_int = get_gradient_density(density[i].reshape(shape_[0],shape_[1],shape_[2],shape_[3]))\n            gradient_density_signal     = gradient_density_signal_int.permute(0,2,3,4,1).reshape(shape_[0],-1,3,1)\n            \n            \n            if '1' in y:\n                y['1'] = torch.cat([y['1'], gradient_density_signal], dim=-1)\n            else:\n                y['1'] =  gradient_density_signal\n                 \n            \n            y = self.conv_layers[i](y)\n            \n            shape_ = density_[i+1].shape\n            \n            gradient_density_signal_int_1 = get_gradient_density(density[i+1].reshape(shape_[0],shape_[1],shape_[2],shape_[3]))\n            gradient_density_signal_1       = gradient_density_signal_int_1.permute(0,2,3,4,1).reshape(shape_[0],-1,3,1)\n            \n            \n            \n            \n            \n            if '1' in y:\n                y['1'] = torch.cat([y['1'],  gradient_density_signal_1], dim=-1)\n            else:\n                y['1'] =  gradient_density_signal_1\n                \n            \n            \n            for key in y.keys():\n                if key.isnumeric():\n                    y[key] = y[key]* density[i+1].unsqueeze(-1)\n            \n            \n            \n            y = apply_layers(y, self.equivariant_weights[i]) # B, d, 2*l + 1, C\n            \n\n            y = self.iSHT[i].compute(y)\n            y = y.permute(0, 3, 1, 2)\n            y = self.bn[i](y)\n            y = torch.nn.ReLU(True)(y)\n            y = y.permute(0, 2, 3, 1)\n            y = self.mlp[i](y)\n            \n            if i < len(self.radius) - 1:\n                y = self.fSHT[i].compute(y)\n        \n        \n        F = torch.max(y, dim=1, keepdims=False).values # B, samples, feature_dim\n\n        return F", "\n\nif __name__ == \"__main__\":\n    sdfPath = \"/home2/aditya.sharm/brics_fields/res_64/plane/train/rotated_fields/02691156_807d735fb9860fd1c863ab010b80d9ed_64_8_sdf.npy\"\n    ptsPath = \"/home2/aditya.sharm/brics_fields/res_64/plane/train/rotated_fields/02691156_807d735fb9860fd1c863ab010b80d9ed_64_8_pts.npy\"\n\n    sdf = np.load(sdfPath,allow_pickle=False)\n    coords = np.load(ptsPath,allow_pickle=False)\n    \n    scale_factor = (coords.max())\n    coords = coords / scale_factor\n    \n    x_in = torch.from_numpy(coords)\n    x_density = torch.from_numpy(sdf)\n    \n    x = x_in\n    \n    \n    \n    x_density = x_density.to(torch.float64)\n    model = TFN_grid_density()\n    \n    type_features = model(x.unsqueeze(0).to(torch.float32), x_density.unsqueeze(0).to(torch.float32))\n    \n    print(type_features.shape) \n    euler_angles_tensor = torch.tensor([2.0*0.785398,0,0])\n    \n    rot_mat = euler_rot_zyz(2.0*0.785398,0,0)\n    types= [1,2,3]\n    wigner_rot_dict = {}\n    print(rot_mat)\n    \n    \n    for type_ in types:\n        \n        wigner_rot = o3.wigner_D(type_, euler_angles_tensor[0], euler_angles_tensor[1], euler_angles_tensor[2])\n        wigner_rot = wigner_rot\n        print(wigner_rot)\n        wigner_rot_dict[str(type_)] = wigner_rot.to(torch.float64)\n        \n    \n    rotated_features_dict = {}\n    \n    for type_ in types:\n        rot_features = torch.matmul(wigner_rot_dict[str(type_)].to(torch.float32),type_features[str(type_)])\n        rotated_features_dict[str(type_)] = rot_features\n    \n    \n    rot_mat = torch.from_numpy(rot_mat)\n    rot_mat = rot_mat\n    rot_mat = rot_mat.type(torch.float32)\n    rot_mat = rot_mat.type_as(x_density)\n    \n    rot_mat = rot_mat.unsqueeze(0)\n    \n    un_rot_density_zyx = x_density.unsqueeze(0).unsqueeze(0)\n    un_rot_density_zyx = un_rot_density_zyx.permute(0,1,4,3,2)\n    \n    x_density_rot_zyx = rotate_density(torch.inverse(rot_mat), un_rot_density_zyx)\n    \n    x_density_rot_zyx = x_density_rot_zyx.squeeze(0)\n    x_density_rot_zyx = x_density_rot_zyx.squeeze(0)\n    x_density_rot = x_density_rot_zyx.permute(2,1,0)\n    \n    x_density_rot[x_density_rot >= 0.5] = 1\n    x_density_rot[x_density_rot < 0.5]  = 0\n    \n\n     \n    rot_type_features = model(x.unsqueeze(0).to(torch.float32), x_density_rot.unsqueeze(0).to(torch.float32))\n    \n    '''\n    for type_ in types:\n        rotated_features_dict[str(type_)] = torch.sum(rotated_features_dict[str(type_)],dim=-1)\n        rot_type_features[str(type_)] = torch.sum(rot_type_features[str(type_)],dim=-1)\n    '''\n    \n    \n    \n    \n    main_idx = 0\n    max_dist_list = []\n    \n    channels = [12,12,9]\n    \n    for type_ in types:\n        \n        for ch_dim in range(channels[type_-1]):\n            rot_feature = rot_type_features[str(type_)][:,:,:,ch_dim].squeeze(-1)\n            dim = (2*type_)+1\n            rot_feature = rot_feature.reshape(16,16,16,dim).unsqueeze(0).permute(0,4,3,2,1)\n            rot_feature_ = rotate_density(rot_mat, rot_feature).squeeze(0)\n            rot_sigma_feature_ = rot_feature_.permute(3,2,1,0).reshape(-1,dim)\n            rot_point_feature = rotated_features_dict[str(type_)][:,:,:,ch_dim].squeeze(-1).squeeze(0)\n            match = 0\n            un_match = 0\n            non_zero_features = 0\n            for index in range(rot_sigma_feature_.shape[0]):\n              if torch.count_nonzero(rot_sigma_feature_[index].type(torch.LongTensor)) > 0:\n                non_zero_features = non_zero_features + 1\n              else:\n                continue\n              if  torch.allclose(rot_sigma_feature_[index],rot_point_feature[index],atol=1e-04):\n                  match = match +1 \n              else:\n                  un_match = un_match + 1\n                \n            print(\"matched features for the type \", type_ ,\" = \",match)\n            #print(\"significant features for the type \", type_ ,\" = \",non_zero_features)\n        \n    '''  \n    for type_ in types:\n        rot_feature = rotated_features_dict[str(type_)]\n        rot_point_feature = rot_type_features[str(type_)]\n        \n        if  torch.allclose(rot_feature,rot_point_feature,atol=1e-03):\n            print(\"Equal for the feature type \",str(type_))\n            print(\"Differece max \",torch.max(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece sum \",torch.sum(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece min \",torch.min(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece mean \",torch.mean(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece median \",torch.median(torch.abs(rot_feature - rot_point_feature)))\n        else:\n            \n            print(\"Not Equal for the feature type \",str(type_))\n            print(\"Differece Norm \",torch.linalg.norm(torch.abs(rot_feature - rot_point_feature), dim=-1).max(),)\n            print(\"Differece max \",torch.max(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece sum \",torch.sum(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece min \",torch.min(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece mean \",torch.mean(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece median \",torch.median(torch.abs(rot_feature - rot_point_feature)))\n            print(\"Differece Mode \",torch.mode(torch.abs(rot_feature - rot_point_feature)))\n    \n    for type_ in types:\n        rot_feature = rotated_features_dict[str(type_)][:,:,:,5:6].squeeze(-1).squeeze(0)\n        rot_point_feature = rot_type_features[str(type_)]\n        match = 0\n        un_match = 0\n        for index in range(rot_feature.shape[0]):\n          \n          if  torch.allclose(rot_feature[index],rot_point_feature[index],atol=1e-03):\n              match = match +1 \n          else:\n              un_match = un_match + 1\n            \n        print(\"matched features for the type \", type_ ,\" = \",match)'''\n    \n    '''\n    main_idx = 0\n    for type_ in types:\n        rot_feature = rotated_features_dict[str(type_)].squeeze(0)\n        rot_point_feature = rot_type_features[str(type_)].squeeze(0)\n        \n        search_idx = []\n        for main_idx in range(rot_feature.shape[0]):\n            main_feature = rot_feature[:,:,0:1].squeeze(-1)[main_idx]\n            found = 0\n            for second_idx in range(rot_point_feature.shape[0]):\n                second_feature = rot_point_feature[second_idx]\n                if second_idx in search_idx:\n                    continue\n                if  torch.allclose(main_feature,second_feature,atol=1e-03):\n                    #print(\"Equal for the feature type  \",str(type_),\" at the idx\", main_idx)\n                    search_idx.append(second_idx)\n                    found = 1\n                    break\n            if not found:\n                print(\"Not Fo\")'''", ""]}
{"filename": "cafi_net/models/layers.py", "chunked_list": ["import torch.nn as nn\nimport torch\nfrom spherical_harmonics.spherical_cnn import SphericalHarmonicsCoeffs\n\n\ndef apply_layers(x, layers):\n    y = dict()\n    # print(x.keys())\n    for l in x:\n        # print(x[l].shape, l)\n        if l.isnumeric():\n            y[l] = layers[int(l)](x[l])\n    return y", "\ndef type_0(x, S2):\n    \"\"\"\n    Spherical Harmonics Transform to extract type 0 features\n    \"\"\"\n\n    y = SphericalHarmonicsCoeffs(l_list=[0], base=S2).compute(x)\n    return y['0']\n\n", "\n\n\ndef type_1(x, S2):\n    \"\"\"\n    Spherical Harmonics Transform to extract type 1 features that are equivariant to SO(3)\n    \"\"\"\n    y = SphericalHarmonicsCoeffs(l_list=[1], base=S2).compute(x)\n    return y['1']\n", "\n\ndef set_sphere_weights(in_channel, out_channel, types):\n    weights = []\n    for l in types:\n        if int(l) == 0:\n            weights.append(nn.Linear(in_channel[int(l)], out_channel))\n        else:\n            weights.append(nn.Linear(in_channel[int(l)], out_channel, bias=False))\n    return torch.nn.Sequential(*weights)", "\nclass MLP(nn.Module):\n\n    def __init__(self, in_channels, out_channels, bn_momentum = 0.75, apply_norm = True, activation = None):\n        super(MLP, self).__init__()\n        \n        self.mlp = nn.Linear(in_channels, out_channels)\n        if apply_norm:\n            self.batchnorm = nn.BatchNorm1d(out_channels, momentum=bn_momentum)\n        self.activation = activation\n        self.apply_norm = apply_norm\n        \n    def forward(self, x):\n        out = self.mlp(x)        \n        if self.activation is not None:\n            out = self.activation(out)\n        if self.apply_norm:\n            out = self.batchnorm(out.transpose(1, -1)).transpose(1, -1)\n\n        return out", "\nclass MLP_layer(nn.Module):\n    def __init__(self, in_channels, units = [32, 64, 128], bn_momentum = 0.75, apply_norm = False, activation = None):\n        super(MLP_layer, self).__init__()\n        \n        self.input_layer = nn.Linear(in_channels, units[0])\n        self.mlp = []\n        self.batchnorm = []\n        for i in range(1, len(units)):\n            self.mlp.append(MLP(units[i-1], units[i], bn_momentum = bn_momentum, apply_norm = apply_norm, activation = activation))\n\n        self.mlp = nn.Sequential(*self.mlp)\n        \n    def forward(self, x):\n        \n        out = self.input_layer(x)\n        out = self.mlp(out)\n        return out"]}
{"filename": "cafi_net/models/__init__.py", "chunked_list": ["from . import TFN_density\nfrom . import Cafi_model\n"]}
{"filename": "cafi_net/models/Cafi_model.py", "chunked_list": ["import pytorch_lightning as pl\nimport torch\nfrom models.TFN_density import TFN_grid_density\nfrom models.layers import MLP, MLP_layer\nfrom utils.group_points import GroupPoints\nfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling, SphericalHarmonicsEval, SphericalHarmonicsCoeffs, zernike_monoms, torch_zernike_monoms\nfrom spherical_harmonics.kernels import SphericalHarmonicsGaussianKernels, ShGaussianKernelConv\nfrom models.layers import MLP, MLP_layer, set_sphere_weights, apply_layers, type_1\nfrom utils.pooling import kd_pooling_3d\n", "from utils.pooling import kd_pooling_3d\n\n\n\nclass Cafi_model(torch.nn.Module):\n\n    def __init__(self, num_capsules = 10, num_frames = 1, sphere_samples = 64, bn_momentum = 0.75, mlp_units = [[32, 32], [64, 64], [128, 256]], radius = [0.4, 0.8, 1.5]):\n        super(Cafi_model, self).__init__()\n\n        self.radius = radius\n        self.bn_momentum = 0.75\n        self.basis_dim = 3\n        self.l_max = [3, 3, 3]\n        self.l_max_out = [3, 3, 3]\n        self.num_shells = [3, 3, 3]\n\n        self.num_capsules = num_capsules\n        self.num_frames = num_frames\n        self.mlp_units = mlp_units\n        self.TFN_arch = TFN_grid_density(sphere_samples = sphere_samples, bn_momentum = bn_momentum, mlp_units = [[32, 32], [64, 64], [128, 256]], l_max = self.l_max, l_max_out = self.l_max_out, num_shells = self.num_shells, radius = self.radius)\n        self.S2 = torch_fibonnacci_sphere_sampling(sphere_samples)\n\n        self.basis_mlp = []\n        self.basis_layer = []\n\n        self.basis_units = [64]\n        for frame_num in range(num_frames):\n            self.basis_mlp.append(MLP_layer(in_channels = self.mlp_units[-1][-1], units = self.basis_units, bn_momentum = self.bn_momentum))\n            self.basis_layer.append(MLP(in_channels = self.basis_units[-1], out_channels=self.basis_dim, apply_norm = False))\n\n        self.basis_mlp = torch.nn.Sequential(*self.basis_mlp)\n        self.basis_layer = torch.nn.Sequential(*self.basis_layer)\n\n\n        self.code_dim = 64\n        self.code_layer_params = [128]\n        self.code_mlp = MLP_layer(in_channels = self.mlp_units[-1][-1], units = self.code_layer_params, bn_momentum = self.bn_momentum)\n        self.code_layer = MLP(in_channels = self.code_layer_params[-1], out_channels=self.code_dim, apply_norm = False)\n\n        self.points_inv_layer = MLP(in_channels = 128, out_channels=3, apply_norm = False)\n        self.num_frames = num_frames\n        self.zernike_harmonics = torch_zernike_monoms(self.l_max_out[-1])\n        self.fSHT = SphericalHarmonicsCoeffs(l_max=self.l_max_out[-1], base=self.S2)\n        self.type_1_basis = SphericalHarmonicsCoeffs(l_list=[1], base=self.S2)\n\n\n    def forward(self, x, x_density):\n        \"\"\"\n        x - B, N, 3 - Batch of point clouds that are kdtree indexed for pooling\n        \"\"\"\n        \n        # Compute TFN features\n        F = self.TFN_arch(x, x_density)\n        x_density_in = x_density        \n        \n        # Equivariant Basis\n        E = []\n\n\n        # Compute equivariant layers\n       \n        for frame_num in range(self.num_frames):\n            basis = self.basis_mlp[frame_num](F)\n            basis = self.basis_layer[frame_num](basis)\n            basis = self.type_1_basis.compute(basis)[\"1\"]\n            basis = torch.nn.functional.normalize(basis, dim=-1, p = 2, eps = 1e-6)\n            E.append(basis)\n\n        B, H, W, D, _ = x.shape\n        x = x.reshape(B, -1, 3)\n        x_density = x_density.reshape(B, -1, 1)\n\n        latent_code = self.code_mlp(F)\n        latent_code = self.code_layer(latent_code)\n        latent_code = self.fSHT.compute(latent_code)\n\n        z = self.zernike_harmonics.compute(x)\n\n        points_code = []\n\n        points_inv = None\n        for l in latent_code:\n            p = torch.einsum('bmi,bvmj->bvij', latent_code[l], z[int(l)])\n            shape = list(p.shape)\n            shape = shape[:-1]\n            shape[-1] = -1\n            p = torch.reshape(p, shape)\n            points_code.append(p)\n            if int(l) == 1:\n                points_inv = p\n\n        points_code = torch.cat(points_code, dim=-1)\n        points_inv = self.points_inv_layer(points_inv)\n\n        if len(x_density_in.shape) == 4:\n            x_density_in = x_density_in.unsqueeze(-1)\n        \n        coords = points_inv.reshape(B, H, W, D, 3)\n        points_inv = torch.nn.functional.grid_sample(x_density_in.permute(0, -1, 1, 2, 3), coords, align_corners=True).squeeze(1)\n\n\n        out = {\"points_inv\": points_inv, \"E\": E, \"coords\": coords}\n\n        return out", "\n"]}
{"filename": "cafi_net/trainers/Canonical_fields_trainer.py", "chunked_list": ["from pkg_resources import declare_namespace\nimport torch\nimport os\nfrom models import *\nfrom utils import *\nfrom utils.train_utils import random_rotate, mean_center, perform_rotation, orthonormalize_basis\nfrom utils.losses import equilibrium_loss , localization_loss_new\nimport datasets\nimport numpy as np\nimport pytorch_lightning as pl", "import numpy as np\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nimport hydra\nfrom utils.pointcloud_utils import kdtree_indexing, save_pointcloud, convert_yzx_to_xyz_basis, get_equivariant_density, rotate_density, sq_distance_mat, save_density, get_gradient_density,create_mask\nfrom scipy.spatial.transform import Rotation\nfrom random import randint\nimport h5py\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans", "from sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\nimport open3d as o3d\nfrom pytorch3d.loss import chamfer_distance\nimport random, sys\n\nclass Canonical_fields_trainer(pl.LightningModule):\n    '''\n    Segmentation trainer to mimic NeSF\n    '''\n\n    def __init__(self, configs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.hparam_config = configs\n        self.Cafinet = getattr(eval(self.hparam_config.model[\"file\"]), \n                                  self.hparam_config.model[\"type\"])(**self.hparam_config.model[\"args\"])\n        \n        self.hparam_config.dataset.args.dataset_path = os.path.join(hydra.utils.get_original_cwd(), self.hparam_config.dataset.args.dataset_path)\n        self.loss_weights = self.hparam_config.loss\n\n\n    \n    def train_dataloader(self):\n        train_data_set = getattr(getattr(datasets, self.hparam_config.dataset.file), self.hparam_config.dataset.type)(**self.hparam_config.dataset.args)\n        train_dataloader = DataLoader(train_data_set, **self.hparam_config.dataset.loader.args, shuffle = True)\n\n        return train_dataloader\n\n    def val_dataloader(self):\n        val_data_set = getattr(getattr(datasets, self.hparam_config.val_dataset.file), self.hparam_config.val_dataset.type)(**self.hparam_config.val_dataset.args)\n        val_dataloader = DataLoader(val_data_set, **self.hparam_config.val_dataset.loader.args, shuffle = False)\n        return val_dataloader\n\n\n    def forward_pass(self, batch, batch_idx, return_outputs=False, rot_mat=None):\n        \n\n        density = batch[\"density\"][0].clone()\n        coords  = batch[\"coords\"][0].clone()\n        \n        \n\n        \n        iter_ =2   \n        if return_outputs == True :\n            iter_ = 1\n        \n        loss_dictionary_list = []   \n        frame_list = []\n        inv_embed_list = []\n        for ii in range(iter_):\n        \n            if rot_mat is None :\n                rotation_1 = torch.from_numpy(Rotation.random(density.shape[0]).as_matrix()).type_as(density)\n            else:\n                rotation_1 = rot_mat.type_as(density)\n                \n            density = batch[\"density\"][ii].clone()\n            coords  = batch[\"coords\"][ii].clone()\n            density = rotate_density(rotation_1, density)\n            \n            B,_,_,_ = density.shape\n            shape_ = density.shape\n             \n            mask_density = create_mask(density).cuda()   \n            \n            out_dict = self.Cafinet(coords, density)\n            \n            batch[\"eq_input_1\"] = coords\n            batch[\"x_input\"] = density\n            batch[\"density_rotated_1\"] = density        \n            batch[\"rotation_1\"] = rotation_1\n            batch[\"mask_density\"] = mask_density\n\n\n            B,_,_,_,_ = out_dict[\"coords\"].shape\n            \n            if return_outputs:\n                out_dict[\"x_input\"] = density\n                loss_dictionary, frame = self.compute_loss(batch, out_dict, return_frame = True)\n\n                out_dict[\"E\"] = frame\n                out_dict[\"coords\"] = out_dict[\"coords\"].reshape(B,-1,3)\n                out_dict[\"input_rotation\"] = rotation_1\n                return loss_dictionary, out_dict\n            else:\n                loss_dictionary , frame  = self.compute_loss(batch, out_dict, return_frame=True)\n                loss_dictionary_list.append(loss_dictionary)\n                frame_list.append(frame)\n                pts_list = []\n                for _indx in range(B):\n                    pts_list.append(out_dict[\"coords\"][_indx][mask_density[_indx] >= 0.5].unsqueeze(0))\n                inv_embed_list.append(pts_list)\n                        \n        \n        for ii in range(1,len(loss_dictionary_list)):\n            for key in loss_dictionary_list[0].keys():\n                loss_dictionary_list[0][key] = loss_dictionary_list[0][key] + loss_dictionary_list[ii][key]\n        \n        for key in loss_dictionary_list[0].keys():\n                loss_dictionary_list[0][key] = loss_dictionary_list[0][key] / (len(loss_dictionary_list))\n        \n        inv_loss = 0.0\n        for _indx in range(B):\n            inv_loss = inv_loss + chamfer_distance(inv_embed_list[0][_indx],inv_embed_list[1][_indx])[0]\n            \n        inv_loss = inv_loss / float(B)\n        \n       \n        loss_dictionary_list[0][\"loss\"]  = loss_dictionary_list[0][\"loss\"] + inv_loss\n        loss_dictionary_list[0][\"inv_loss\"]  = inv_loss\n        \n        return loss_dictionary_list[0]\n\n    def compute_loss(self, batch, outputs, return_frame = False):\n        \"\"\"\n        Computing losses for \n        \"\"\"\n        \n        loss_dictionary = {}\n        loss = 0.0\n        \n        out_density_1 = batch[\"mask_density\"]\n        rotation_1 = batch[\"rotation_1\"]\n\n\n        # First branch\n        density_canonical = outputs[\"points_inv\"]\n        basis_1 = outputs[\"E\"]\n        canonical_coords = outputs[\"coords\"]\n        basis_1 = torch.stack(basis_1, dim = 1) # B, num_frames, 3, 3\n       \n        B, H, W, D, _3 = canonical_coords.shape\n\n        orth_basis_1 = (orthonormalize_basis(basis_1))\n        basis_canonical_to_input = convert_yzx_to_xyz_basis((orth_basis_1))\n        eq_input_1 =batch[\"eq_input_1\"].reshape(B,-1,3)\n        \n\n        \n    \n       \n\n        B, num_frames = orth_basis_1.shape[0], orth_basis_1.shape[1]\n       \n        eq_coords_pred = torch.einsum(\"bfij, bhwdj->bfhwdi\", basis_canonical_to_input, canonical_coords).reshape(B, num_frames, -1, _3)\n\n        \n        mask_object = 1.0*(out_density_1 >= 0.5).reshape(B, 1, -1)\n\n        \n        \n        error_full = torch.sum(torch.sqrt(torch.sum(torch.square(eq_coords_pred - eq_input_1[:, None]), -1) + 1e-8) * mask_object, -1) / (torch.sum(mask_object, -1) + 1e-10)\n                    \n\n\n        values, indices = torch.topk(-error_full, k = 1)\n        \n        \n        orth_basis_frames = basis_canonical_to_input\n        basis_canonical_to_input = basis_canonical_to_input[torch.arange(indices.shape[0]), indices[:, 0]]\n        \n\n        eq_input_pred_best = torch.einsum(\"bij, bhwdj->bhwdi\", basis_canonical_to_input, canonical_coords).reshape(B, -1, _3)\n        l2_loss = torch.mean(torch.sum(torch.sqrt(torch.sum(torch.square(eq_input_pred_best - eq_input_1), -1) + 1e-8) * mask_object.squeeze(1),-1) / (torch.sum(mask_object.squeeze(1),-1) + 1e-10))\n        \n        \n        \n        chamfer_loss = 0.0\n        for idx in range(B):\n            int_mask = mask_object[idx].permute(1,0)\n            int_pts_idx,_ = torch.where(int_mask >=0.5)\n            chamfer_loss = chamfer_loss + chamfer_distance(eq_input_pred_best[idx][int_pts_idx,:].unsqueeze(0),eq_input_1[idx][int_pts_idx,:].unsqueeze(0))[0]\n            \n        chamfer_loss = chamfer_loss / float(B)  \n      \n       \n        orth_loss = torch.mean(torch.abs(basis_1 - orth_basis_1.detach()))\n        I = torch.eye(num_frames).type_as(basis_1).unsqueeze(0)\n        ones = torch.ones(B, num_frames, num_frames).type_as(basis_1)\n        weights = ones - I\n        separation_loss = torch.sum(torch.mean(torch.mean(torch.exp(-(torch.abs(basis_1[:, :, None] - basis_1[:, None]))), -1), -1) * weights) / (torch.sum(weights) + 1e-10)\n        \n        \n       \n\n        if self.loss_weights.l2_loss > 0.0:\n            loss += self.loss_weights.l2_loss * l2_loss\n        \n        if self.loss_weights.chamfer_loss > 0.0:\n            loss += self.loss_weights.chamfer_loss * chamfer_loss\n        \n\n        if self.loss_weights.separation_loss > 0.0:\n            loss += self.loss_weights.separation_loss * separation_loss\n\n\n        if self.loss_weights.orth_loss > 0.0:\n            loss += self.loss_weights.orth_loss * orth_loss\n\n      \n        \n        loss_dictionary[\"loss\"] = loss\n        loss_dictionary[\"l2_loss\"] = l2_loss\n        loss_dictionary[\"orth_loss\"] = orth_loss  \n        loss_dictionary[\"separation_loss\"] = separation_loss\n        loss_dictionary[\"chamfer_loss\"] = chamfer_loss  \n        \n        if return_frame:\n            return loss_dictionary, basis_canonical_to_input\n        return loss_dictionary\n\n    def training_step(self, batch, batch_idx):\n\n        loss_dictionary = self.forward_pass(batch, batch_idx)\n        self.log_loss_dict(loss_dictionary)\n\n        return loss_dictionary[\"loss\"]\n\n    def validation_step(self, batch, batch_idx):\n\n        loss_dictionary = self.forward_pass(batch, batch_idx)\n        self.log_loss_dict(loss_dictionary, val = True)\n\n        return loss_dictionary[\"loss\"]\n\n\n    def configure_optimizers(self):\n\n        optimizer1 = getattr(torch.optim, self.hparam_config.optimizer.type)(list(self.Cafinet.parameters()), **self.hparam_config.optimizer.args)\n        scheduler1 = getattr(torch.optim.lr_scheduler, self.hparam_config.scheduler.type)(optimizer1, **self.hparam_config.scheduler.args)\n\n        return [optimizer1], [scheduler1]\n\n\n\n    def log_loss_dict(self, loss_dictionary, val = False):\n\n        for key in loss_dictionary:\n            if val:\n                self.log(\"val_\" + key, loss_dictionary[key], **self.hparam_config.logging.args)\n            else:\n                self.log(key, loss_dictionary[key], **self.hparam_config.logging.args)\n\n\n    def test_step(self, x):\n        '''\n        Input:\n            x - B, N, 3\n        Output:\n            output_dictionary - dictionary with all outputs and inputs\n        '''\n        output_dictionary = self.forward_pass(x)\n\n        return output_dictionary\n\n\n    def save_outputs(self, save_file_name, out_dict):\n        \"\"\"\n        Save outputs to file\n        \"\"\"\n        save_keys = [\"x_input\", \"canonical_density\"]\n        for key in out_dict:\n            if key in save_keys:\n                pcd_name = save_file_name + \"_\" + key + \".ply\"\n                save_density(out_dict[key], pcd_name)\n        \n    \n    def loadh5(self,path):\n        fx_input = h5py.File(path, 'r')\n        x = fx_input['data'][:]\n        fx_input.close()\n        return x\n\n        \n    def save_h5(self,h5_filename, data,data_dtype='float32'):\n        h5_fout = h5py.File(h5_filename,\"w\")\n\n        h5_fout.create_dataset(\n            'data', data=data,\n            compression='gzip', compression_opts=4,\n            dtype=data_dtype)\n        h5_fout.close()\n    \n    def run_test(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1 , num_rots=1):\n        \n        self.hparam_config = cfg\n\n        self.hparam_config.val_dataset.loader.args.batch_size = 1\n        loader = self.val_dataloader()\n\n        if max_iters is not None:\n            max_iters = min(max_iters, len(loader))\n        else:\n            max_iters = len(loader)\n        \n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n\n        with torch.no_grad():\n            for i, batch in enumerate(loader):\n                if i % skip == 0:\n                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n                    batch[\"coords\"][0]  = batch[\"coords\"][0].cuda()\n                    \n                    x = batch\n                    for _ in range(num_rots):\n                        random_rots = torch.from_numpy(Rotation.random(num_rots).as_matrix())\n                        rot_mat = random_rots[_].reshape(1,3,3)                      \n                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n                        density_1 = output_dict[\"x_input\"]\n                        basis_1 = output_dict[\"E\"]\n                        canonical_density = rotate_density(basis_1, density_1)\n                        output_dict[\"canonical_density\"] = canonical_density                      \n                        save_file_name = os.path.join(save_directory, \"\") + str(_) + \"_ \"+\"%d\" % i \n                        self.save_outputs(save_file_name, output_dict)\n\n              \n    def run_metrics(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1):\n\n        self.hparam_config = cfg\n\n        self.hparam_config.val_dataset.loader.args.batch_size = 1\n        loader = self.val_dataloader()\n\n        if max_iters is not None:\n            max_iters = min(max_iters, len(loader))\n        else:\n            max_iters = len(loader)\n        \n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        \n        random_rots = self.loadh5(self.hparam_config.test_args.rotation_file)\n        random_rots = torch.from_numpy(random_rots)\n        \n        category_name = self.hparam_config.test_args.category_name\n        h5_filename = save_directory + \"/\"+ category_name +\"_rotations.h5\"\n        self.save_h5(h5_filename, random_rots)\n        \n        \n\n        canonical_frame=[]\n        with torch.no_grad():\n            for i, batch in enumerate(loader):\n                print(\"processing for Nerf Model \",i)\n                if i % skip == 0:\n                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n                    batch[\"coords\"][0] = batch[\"coords\"][0].cuda()\n                    x = batch\n                    canonical_frame_obj = []\n                    \n                    for _ in range(random_rots.shape[0]):\n                        rot_mat = random_rots[_].reshape(1,3,3)                                            \n                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n                        basis_1 = output_dict[\"E\"]\n                        canonical_frame_obj.append(torch.inverse(basis_1.squeeze(0)))\n                    \n                    canonical_frame.append(torch.stack(canonical_frame_obj))\n            \n\n            canonical_frame = torch.stack(canonical_frame)\n            h5_filename = save_directory + \"/\"+ category_name +\"_canonical.h5\"\n            self.save_h5(h5_filename, canonical_frame.cpu().detach().numpy())\n    \n    \n    def run_canonica_render(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1):\n        \n        self.hparam_config = cfg\n\n        self.hparam_config.val_dataset.loader.args.batch_size = 1\n        loader = self.val_dataloader()\n\n        if max_iters is not None:\n            max_iters = min(max_iters, len(loader))\n        else:\n            max_iters = len(loader)\n        \n        if not os.path.exists(save_directory):\n            os.makedirs(save_directory)\n        \n        \n        category_name = self.hparam_config.test_args.category_name\n        \n        rotation_list = []\n        canonical_frame=[]\n        file_name_list=[]\n        with torch.no_grad():\n            for i, batch in enumerate(loader):\n                print(\"processing for Nerf Model \",i)\n                if i % skip == 0:\n                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n                    batch[\"coords\"][0] = batch[\"coords\"][0].cuda()\n                    file_name_list.append(batch[\"file_path\"][0])\n                    x = batch\n                    canonical_frame_obj = []\n                    for _ in range(1):\n                        rot_mat = torch.eye(3).reshape(1,3,3).to(torch.float32).cuda() \n                        random_rots = torch.from_numpy(Rotation.random(1).as_matrix())\n                        rot_mat = random_rots[_].reshape(1,3,3) \n                        rotation_list.append(rot_mat)\n                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n                        basis_1 = output_dict[\"E\"]\n                        canonical_frame_obj.append(torch.inverse(basis_1.squeeze(0)))\n                    \n                    canonical_frame.append(torch.stack(canonical_frame_obj))\n            \n            \n            \n            rotation_list = torch.stack(rotation_list,axis=0)\n            h5_filename = save_directory + \"/\"+ category_name +\"_input_rot.h5\"\n            self.save_h5(h5_filename, rotation_list.cpu().detach().numpy())\n            \n            file_name = save_directory + \"/\" + category_name +\"_files.txt\"\n            file_prt = open(file_name,\"w+\")\n            for _ in file_name_list:\n                file_prt.write(_+\"\\n\")\n            file_prt.close()\n\n            \n            canonical_frame = torch.stack(canonical_frame)\n            h5_filename = save_directory + \"/\"+ category_name +\"_canonical.h5\"\n            self.save_h5(h5_filename, canonical_frame.cpu().detach().numpy())", "            \n"]}
{"filename": "cafi_net/trainers/__init__.py", "chunked_list": ["from . import Canonical_fields_trainer\n"]}
{"filename": "cafi_net/datasets/__init__.py", "chunked_list": ["from . import density_dataset\n"]}
{"filename": "cafi_net/datasets/density_dataset.py", "chunked_list": ["\n\nimport sys\nsys.path.append(\"../\")\nimport os\nimport json\nimport glob\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms", "import numpy as np\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader\nimport random, sys\nfrom scipy.ndimage import zoom\nimport utils.pointcloud_utils as pcd_utils\nfrom scipy.spatial.transform import Rotation\nfrom utils.pointcloud_utils import get_xyz_grid\nimport open3d as o3d", "from utils.pointcloud_utils import get_xyz_grid\nimport open3d as o3d\nfrom random import randrange\n\n\n\n\nclass Density_loader_shapenet(Dataset):\n\n    def __init__(self, dataset_path,sigmas_file_pattern):\n        super(Density_loader_shapenet, self).__init__()\n        self.dataset_path = dataset_path\n        self.files = glob.glob(os.path.join(dataset_path, \"\") + sigmas_file_pattern)\n        self.files.sort()\n        self.dimensions = [32, 32, 32]\n        \n        \n    def __len__(self):\n        \n        return len(self.files)\n    \n    def __getitem__(self, key):\n    \n        grid = np.load(self.files[key])\n        grid = np.where(grid <=0,0,grid)\n        grid = grid * grid \n        grid = np.exp(-1.0 * grid)\n\n        grid_new = 1 - grid\n        grid_new = np.clip(grid_new, 0, 1)\n        density_grid = torch.from_numpy(grid_new).to(torch.float32).permute(2,1,0)\n        \n        pair_idx = randrange(int(len(self.files)))\n\n        grid_1 = np.load(self.files[pair_idx])\n        grid_1 = np.where(grid_1 <=0,0,grid_1)\n        grid_1 = grid_1 * grid_1 \n        grid_1 = np.exp(-1.0 * grid_1)\n\n        grid_new_1= 1 - grid_1\n        grid_new_1 = np.clip(grid_new_1, 0, 1)\n        density_grid_1 = torch.from_numpy(grid_new_1).to(torch.float32).permute(2,1,0)\n        \n        density_list = [density_grid,density_grid_1]\n        \n        x = np.linspace(-1,1,self.dimensions[0])\n        y = np.linspace(-1,1,self.dimensions[1])\n        z = np.linspace(-1,1,self.dimensions[2])\n        \n        x_coords, y_coords,z_coords =  np.meshgrid(x,y,z,indexing='ij')\n        \n        coords = torch.from_numpy(np.stack((x_coords,y_coords,z_coords),axis=-1)).to(torch.float32).permute(2,1,0,3)\n        coords_list =  [coords,coords]\n        \n        return {\"density\": density_list , \"coords\":coords_list , \"file_path\": self.files[key]}", ""]}
