{"filename": "finetune.py", "chunked_list": ["import os\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nimport transformers\nimport argparse\nimport warnings", "import argparse\nimport warnings\nfrom huggingface_hub import snapshot_download\n\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom peft import (\n    prepare_model_for_int8_training,", "from peft import (\n    prepare_model_for_int8_training,\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)", "parser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\nparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\nparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--eval_steps\", type=int, default=200)\nparser.add_argument(\"--save_steps\", type=int, default=200)\nparser.add_argument(\"--test_size\", type=int, default=200)\nparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)", "parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\nparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\nargs = parser.parse_args()\n\nif not args.wandb:\n    os.environ[\"WANDB_MODE\"] = \"disable\"\n# optimized for RTX 4090. for larger GPUs, increase some of these?\nMICRO_BATCH_SIZE = 4  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = 128", "MICRO_BATCH_SIZE = 4  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = 128\nMAX_STEPS = None\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 3  # we don't always need 3 tbh\nLEARNING_RATE = 3e-4  # the Karpathy constant\nCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05", "LORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nVAL_SET_SIZE = args.test_size #2000\nUSE_8bit = True\n\nif USE_8bit is True:\n    warnings.warn(\"If your version of bitsandbytes>0.37.2, Please downgrade bitsandbytes's version, for example: pip install bitsandbytes==0.37.2\")\n        \nTARGET_MODULES = [\n    \"q_proj\",", "TARGET_MODULES = [\n    \"q_proj\",\n    \"v_proj\",\n]\nDATA_PATH = args.data_path \nOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\ndevice_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size", "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\nprint(args.model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_8bit=USE_8bit,\n    device_map=device_map,", "    load_in_8bit=USE_8bit,\n    device_map=device_map,\n)\ntokenizer = LlamaTokenizer.from_pretrained(\n    args.model_path, add_eos_token=True\n)\n\nif USE_8bit is True:\n    model = prepare_model_for_int8_training(model)\n", "\nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)", ")\nmodel = get_peft_model(model, config)\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n#tokenizer.padding_side = \"left\"  # Allow batched inference\n\ndata = load_dataset(\"json\", data_files=DATA_PATH)\n\nnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "if args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "\n\nmodel.print_trainable_parameters()\n\ndef generate_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if data_point[\"input\"]:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"", "\n\ndef tokenize(prompt):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )\n    return {\n        \"input_ids\": result[\"input_ids\"][:-1],\n        \"attention_mask\": result[\"attention_mask\"][:-1],\n    }", "\n\ndef generate_and_tokenize_prompt(data_point):\n    # This function masks out the labels for the input,\n    # so that our loss is computed only on the response.\n    user_prompt = (\n        (\n            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n\"\"\"\n        )\n        if data_point[\"input\"]\n        else (\n            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n\"\"\"\n        )\n    )\n    len_user_prompt_tokens = (\n        len(\n            tokenizer(\n                user_prompt,\n                truncation=True,\n                max_length=CUTOFF_LEN + 1,\n            )[\"input_ids\"]\n        )\n        - 1\n    )  # no eos token\n    full_tokens = tokenizer(\n        user_prompt + data_point[\"output\"],\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )[\"input_ids\"][:-1]\n    return {\n        \"input_ids\": full_tokens,\n        \"labels\": [-100] * len_user_prompt_tokens\n        + full_tokens[len_user_prompt_tokens:],\n        \"attention_mask\": [1] * (len(full_tokens)),\n    }", "\n\nif VAL_SET_SIZE > 0:\n    train_val = data[\"train\"].train_test_split(\n        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n    )\n    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\nelse:\n    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    val_data = None", "\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=100,\n        num_train_epochs=EPOCHS,", "        warmup_steps=100,\n        num_train_epochs=EPOCHS,\n        max_steps=MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=20,\n        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,", "        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,\n        save_total_limit=30,\n        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n        ddp_find_unused_parameters=False if ddp else None,\n        report_to=\"wandb\" if args.wandb else [],\n        ignore_data_skip=args.ignore_data_skip,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)", "    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False\n\nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)", ").__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\nprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\ntrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\nmodel.save_pretrained(OUTPUT_DIR)", "\nmodel.save_pretrained(OUTPUT_DIR)\n\n"]}
{"filename": "finetune_fp16.py", "chunked_list": ["import os\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nimport transformers\nimport argparse\nimport warnings", "import argparse\nimport warnings\nfrom huggingface_hub import snapshot_download\n\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom peft import (\n    prepare_model_for_int8_training,", "from peft import (\n    prepare_model_for_int8_training,\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n)\n\ndef get_peft_state_maybe_zero_3(state_dict, bias):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.cpu().clone().detach()\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return", "def get_peft_state_maybe_zero_3(state_dict, bias):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.cpu().clone().detach()\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)", "parser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\nparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\nparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--eval_steps\", type=int, default=200)\nparser.add_argument(\"--save_steps\", type=int, default=200)\nparser.add_argument(\"--test_size\", type=int, default=200)\nparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")", "parser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\nparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\n\nparser.add_argument(\"--local_rank\", type=int, default=-1)\nparser.add_argument(\"--deepspeed\", action=\"store_true\", default=False)\n\nargs = parser.parse_args()\n\nif not args.wandb:\n    os.environ[\"WANDB_MODE\"] = \"disable\"", "\nif not args.wandb:\n    os.environ[\"WANDB_MODE\"] = \"disable\"\n# optimized for RTX 4090. for larger GPUs, increase some of these?\nMICRO_BATCH_SIZE = 2  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = 128\nMAX_STEPS = None\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 3  # we don't always need 3 tbh\nLEARNING_RATE = 3e-4  # the Karpathy constant", "EPOCHS = 3  # we don't always need 3 tbh\nLEARNING_RATE = 3e-4  # the Karpathy constant\nCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nVAL_SET_SIZE = args.test_size #2000\nTARGET_MODULES = [\n    \"q_proj\",\n    \"v_proj\",", "    \"q_proj\",\n    \"v_proj\",\n]\nDATA_PATH = args.data_path\nOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\ndevice_map = {\"\": 0} #\"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size", "ddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\nprint(args.model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map=device_map,", "    torch_dtype=torch.float16,\n    device_map=device_map,\n).half()\ntokenizer = LlamaTokenizer.from_pretrained(\n    args.model_path, add_eos_token=True\n)\n\n#model = prepare_model_for_int8_training(model)\n\nconfig = LoraConfig(", "\nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)", ")\nmodel = get_peft_model(model, config)\n\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n#tokenizer.padding_side = \"left\"  # Allow batched inference\n\ndata = load_dataset(\"json\", data_files=DATA_PATH)\n\nnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "now_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "\n\nmodel.print_trainable_parameters()\n\ndef generate_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if data_point[\"input\"]:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"", "\n\ndef tokenize(prompt):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )\n    return {\n        \"input_ids\": result[\"input_ids\"][:-1],\n        \"attention_mask\": result[\"attention_mask\"][:-1],\n    }", "\n\ndef generate_and_tokenize_prompt(data_point):\n    # This function masks out the labels for the input,\n    # so that our loss is computed only on the response.\n    user_prompt = (\n        (\n            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n\"\"\"\n        )\n        if data_point[\"input\"]\n        else (\n            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n\"\"\"\n        )\n    )\n    len_user_prompt_tokens = (\n        len(\n            tokenizer(\n                user_prompt,\n                truncation=True,\n                max_length=CUTOFF_LEN + 1,\n            )[\"input_ids\"]\n        )\n        - 1\n    )  # no eos token\n    full_tokens = tokenizer(\n        user_prompt + data_point[\"output\"],\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )[\"input_ids\"][:-1]\n    return {\n        \"input_ids\": full_tokens,\n        \"labels\": [-100] * len_user_prompt_tokens\n        + full_tokens[len_user_prompt_tokens:],\n        \"attention_mask\": [1] * (len(full_tokens)),\n    }", "\n\nif VAL_SET_SIZE > 0:\n    train_val = data[\"train\"].train_test_split(\n        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n    )\n    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\nelse:\n    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    val_data = None", "trainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=100,\n        num_train_epochs=EPOCHS,\n        max_steps=MAX_STEPS,", "        num_train_epochs=EPOCHS,\n        max_steps=MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=20,\n        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,", "        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,\n        save_total_limit=30,\n        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n        ddp_find_unused_parameters=False if ddp else None,\n        report_to=\"wandb\" if args.wandb else [],\n        ignore_data_skip=args.ignore_data_skip,\n        deepspeed=\"sample/zero_config.json\" if args.deepspeed else None,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)", "    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False\n\nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)", ").__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\nprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\ntrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\nmodel.save_pretrained(OUTPUT_DIR)", "\nmodel.save_pretrained(OUTPUT_DIR)\n\n"]}
{"filename": "interaction.py", "chunked_list": ["import sys\nimport torch\nfrom peft import PeftModel\nimport transformers\nimport gradio as gr\nimport argparse\nimport warnings\nimport os\n\n", "\n\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--lora_path\", type=str, default=\"./lora-Vicuna/checkpoint-final\")", "parser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--lora_path\", type=str, default=\"./lora-Vicuna/checkpoint-final\")\nparser.add_argument(\"--use_local\", type=int, default=1)\nargs = parser.parse_args()\n\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\nLOAD_8BIT = True\nBASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path", "BASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path\n\n# fix the path for local checkpoint\nlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\nprint(lora_bin_path)\nif not os.path.exists(lora_bin_path) and args.use_local:\n    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n    print(pytorch_bin_path)\n    if os.path.exists(pytorch_bin_path):\n        os.rename(pytorch_bin_path, lora_bin_path)\n        warnings.warn(\"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\")\n    else:\n        assert ('Checkpoint is not Found!')", "if torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:\n    pass", "\nif device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        load_in_8bit=LOAD_8BIT,\n        torch_dtype=torch.float16,\n        device_map=\"auto\", #device_map={\"\": 0},\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        torch_dtype=torch.float16,\n        device_map=\"auto\", #device_map={\"\": 0},\n    )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = PeftModel.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n    )", "\ndef generate_prompt(instruction, input=None):\n    if input:\n        return f\"\"\"The following is a conversation between an AI assistant called Assistant and a human user called User.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\"\"\"\n    else:\n        return f\"\"\"The following is a conversation between an AI assistant called Assistant and a human user called User.\n\n### Instruction:\n{instruction}\n\n### Response:\"\"\"", "\nif not LOAD_8BIT:\n    model.half()  # seems to fix bugs for some users.\n\nmodel.eval()\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\ndef interaction(\n    input,\n    history,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    repetition_penalty=1.0,\n    max_memory=256,\n    **kwargs,\n):\n    now_input = input\n    history = history or []\n    if len(history) != 0:\n        input = \"\\n\".join([\"User:\" + i[0]+\"\\n\"+\"Assistant:\" + i[1] for i in history]) + \"\\n\" + \"User:\" + input\n        if len(input) > max_memory:\n            input = input[-max_memory:]\n    print(input)\n    print(len(input))\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        **kwargs,\n    )\n    with torch.no_grad():\n        generation_output = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n            return_dict_in_generate=True,\n            output_scores=True,\n            max_new_tokens=max_new_tokens,\n            repetition_penalty=float(repetition_penalty),\n        )\n    s = generation_output.sequences[0]\n    output = tokenizer.decode(s)\n    output = output.split(\"### Response:\")[1].strip()\n    output = output.replace(\"Belle\", \"Vicuna\")\n    if 'User:' in output:\n        output = output.split(\"User:\")[0]\n    history.append((now_input, output))\n    print(history)\n    return history, history", "def interaction(\n    input,\n    history,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    repetition_penalty=1.0,\n    max_memory=256,\n    **kwargs,\n):\n    now_input = input\n    history = history or []\n    if len(history) != 0:\n        input = \"\\n\".join([\"User:\" + i[0]+\"\\n\"+\"Assistant:\" + i[1] for i in history]) + \"\\n\" + \"User:\" + input\n        if len(input) > max_memory:\n            input = input[-max_memory:]\n    print(input)\n    print(len(input))\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        **kwargs,\n    )\n    with torch.no_grad():\n        generation_output = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n            return_dict_in_generate=True,\n            output_scores=True,\n            max_new_tokens=max_new_tokens,\n            repetition_penalty=float(repetition_penalty),\n        )\n    s = generation_output.sequences[0]\n    output = tokenizer.decode(s)\n    output = output.split(\"### Response:\")[1].strip()\n    output = output.replace(\"Belle\", \"Vicuna\")\n    if 'User:' in output:\n        output = output.split(\"User:\")[0]\n    history.append((now_input, output))\n    print(history)\n    return history, history", "\nchatbot = gr.Chatbot().style(color_map=(\"green\", \"pink\"))\ndemo = gr.Interface(\n    fn=interaction,\n    inputs=[\n        gr.components.Textbox(\n            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n        ),\n        \"state\",\n        gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\"),", "        \"state\",\n        gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\"),\n        gr.components.Slider(minimum=0, maximum=1, value=0.9, label=\"Top p\"),\n        gr.components.Slider(minimum=0, maximum=100, step=1, value=60, label=\"Top k\"),\n        gr.components.Slider(minimum=1, maximum=5, step=1, value=2, label=\"Beams\"),\n        gr.components.Slider(\n            minimum=1, maximum=2000, step=1, value=128, label=\"Max new tokens\"\n        ),\n        gr.components.Slider(\n            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"", "        gr.components.Slider(\n            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n        ),\n        gr.components.Slider(\n            minimum=0, maximum=2000, step=1, value=256, label=\"max memory\"\n        ),\n    ],\n    outputs=[chatbot, \"state\"],\n    allow_flagging=\"auto\",\n    title=\"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\",", "    allow_flagging=\"auto\",\n    title=\"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\",\n    description=\"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\",\n)\ndemo.queue().launch(share=True, inbrowser=True)"]}
{"filename": "utils.py", "chunked_list": ["import logging\nimport sys\nimport os\nimport torch\nimport json\nfrom typing import Optional, Tuple, Union, List, Callable\nfrom transformers import LlamaForCausalLM\nfrom transformers.generation.logits_process import LogitsProcessor\nfrom transformers.generation.beam_search import BeamSearchScorer\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled", "from transformers.generation.beam_search import BeamSearchScorer\nfrom transformers.deepspeed import is_deepspeed_zero3_enabled\nfrom transformers.generation.utils import (\n    LogitsProcessorList,\n    StoppingCriteriaList,\n    GenerationConfig,\n    GenerationMixin,\n)\nimport warnings\nfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig", "import warnings\nfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\nimport peft\nimport torch.distributed as dist\nfrom torch import nn\nimport copy\nfrom accelerate.hooks import (\n    AlignDevicesHook,\n    add_hook_to_module,\n    remove_hook_from_submodules,", "    add_hook_to_module,\n    remove_hook_from_submodules,\n)\nfrom accelerate.utils import get_balanced_memory\nfrom huggingface_hub import hf_hub_download\nfrom accelerate import dispatch_model, infer_auto_device_map\nfrom peft.utils import PeftType, set_peft_model_state_dict\n\ndef printf(*args,**kargs):\n    if os.environ.get('DEBUG',False):\n        end = '\\n'\n        if 'end' in kargs:\n            end = kargs['end']\n        print(*args, end=end, flush=True)", "def printf(*args,**kargs):\n    if os.environ.get('DEBUG',False):\n        end = '\\n'\n        if 'end' in kargs:\n            end = kargs['end']\n        print(*args, end=end, flush=True)\n\nclass ColorFormatter(logging.Formatter):\n\n    grey = \"\\x1b[38;20m\"\n    blue = \"\\x1b[34;20m\"\n    yellow = \"\\x1b[33;20m\"\n    red = \"\\x1b[31;20m\"\n    bold_red = \"\\x1b[31;1m\"\n    reset = \"\\x1b[0m\"\n\n    def __init__(self, fmt):\n        super().__init__(fmt)\n        self.FORMATS = {\n            logging.DEBUG: self.grey + fmt + self.reset,\n            logging.INFO: self.blue + fmt + self.reset,\n            logging.WARNING: self.yellow + fmt + self.reset,\n            logging.ERROR: self.red + fmt + self.reset,\n            logging.CRITICAL: self.bold_red + fmt + self.reset\n        }\n\n    def format(self, record):\n        log_fmt = self.FORMATS.get(record.levelno)\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)", "\ndef set_console_logger(name):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    consoleHandler = logging.StreamHandler(sys.stdout)\n    consoleHandler.setLevel(logging.INFO)\n    consoleHandler.setFormatter(ColorFormatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n    logger.addHandler(consoleHandler)\n    return logger\n\ndef set_file_logger(name, dir, use_console=False):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    os.makedirs(dir, exist_ok=True)\n\n    if use_console:\n        logger.propagate = False # disable default handler\n        consoleHandler = logging.StreamHandler(sys.stdout)\n        consoleHandler.setLevel(logging.INFO)\n        consoleHandler.setFormatter(ColorFormatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n        logger.addHandler(consoleHandler)\n\n    fileHandler = logging.FileHandler(os.path.join(dir,'session.log'), mode='a') \n    fileHandler.setLevel(logging.INFO)\n    fileHandler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n    logger.addHandler(fileHandler)\n    return logger", "\ndef set_file_logger(name, dir, use_console=False):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    os.makedirs(dir, exist_ok=True)\n\n    if use_console:\n        logger.propagate = False # disable default handler\n        consoleHandler = logging.StreamHandler(sys.stdout)\n        consoleHandler.setLevel(logging.INFO)\n        consoleHandler.setFormatter(ColorFormatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n        logger.addHandler(consoleHandler)\n\n    fileHandler = logging.FileHandler(os.path.join(dir,'session.log'), mode='a') \n    fileHandler.setLevel(logging.INFO)\n    fileHandler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n    logger.addHandler(fileHandler)\n    return logger", "\ndef to_jsonl(data, path):\n    with open(path, 'a') as f:\n        for line in data:\n            f.write(json.dumps(line,ensure_ascii=False)+'\\n')\n\ndef from_json(path):\n    return json.load(open(path))\n\ndef from_jsonl(path):\n    return [json.loads(line) for line in open(path, 'r') ]", "\ndef from_jsonl(path):\n    return [json.loads(line) for line in open(path, 'r') ]\n\ndef to_json(data, path):\n    json.dump(data, open(path, 'w'), ensure_ascii=False)\n\nclass StreamGenerationMixin(GenerationMixin):\n    # support for streamly generation\n    # TODO: group_beam_search\n    @torch.no_grad()\n    def stream_generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        generation_config: Optional[GenerationConfig] = None,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        prefix_allowed_tokens_fn: Optional[\n            Callable[[int, torch.Tensor], List[int]]\n        ] = None,\n        **kwargs,\n    ):\n        if is_deepspeed_zero3_enabled() and dist.world_size() > 1:\n            synced_gpus = True\n        else:\n            synced_gpus = False\n\n        if kwargs.get(\"attention_mask\", None) is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(\n                kwargs[\"input_ids\"].shape[0], self.peft_config.num_virtual_tokens\n            ).to(kwargs[\"input_ids\"].device)\n            kwargs[\"attention_mask\"] = torch.cat(\n                (prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1\n            )\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\n                \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n            )\n            kwargs[\"position_ids\"] = None\n        if kwargs.get(\"token_type_ids\", None) is not None:\n            warnings.warn(\n                \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n            )\n            kwargs[\"token_type_ids\"] = None\n\n        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n\n        if generation_config is None:\n            generation_config = self.generation_config\n        generation_config = copy.deepcopy(generation_config)\n        model_kwargs = generation_config.update(**kwargs)\n\n        bos_token_id, eos_token_id, pad_token_id = (\n            generation_config.bos_token_id,\n            generation_config.eos_token_id,\n            generation_config.pad_token_id,\n        )\n\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n\n        has_default_max_length = (\n            kwargs.get(\"max_length\") is None\n            and generation_config.max_length is not None\n        )\n        if has_default_max_length and generation_config.max_new_tokens is None:\n            warnings.warn(\n                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n                UserWarning,\n            )\n        elif generation_config.max_new_tokens is not None:\n            generation_config.max_length = (\n                generation_config.max_new_tokens + input_ids_seq_length\n            )\n        if generation_config.min_new_tokens is not None:\n            generation_config.min_length = (\n                generation_config.min_new_tokens + input_ids_seq_length\n            )\n\n        if input_ids_seq_length >= generation_config.max_length:\n            input_ids_string = (\n                \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n            )\n\n        # 2. Set generation parameters if not already defined\n        logits_processor = (\n            logits_processor if logits_processor is not None else LogitsProcessorList()\n        )\n        stopping_criteria = (\n            stopping_criteria\n            if stopping_criteria is not None\n            else StoppingCriteriaList()\n        )\n        # 7. determine generation mode\n        is_constraint_gen_mode = (\n            generation_config.constraints is not None or generation_config.force_words_ids is not None\n        )\n\n        is_contrastive_search_gen_mode = (\n            generation_config.top_k is not None\n            and generation_config.top_k > 1\n            and generation_config.do_sample is False\n            and generation_config.penalty_alpha is not None\n            and generation_config.penalty_alpha > 0\n        )\n\n        is_greedy_gen_mode = (\n            (generation_config.num_beams == 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is False\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        # beam=1 and do_sample=True\n        is_sample_gen_mode = (\n            (generation_config.num_beams == 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is True\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_beam_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is False\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_beam_sample_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups == 1)\n            and generation_config.do_sample is True\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        is_group_beam_gen_mode = (\n            (generation_config.num_beams > 1)\n            and (generation_config.num_beam_groups > 1)\n            and not is_constraint_gen_mode\n            and not is_contrastive_search_gen_mode\n        )\n        # 8. prepare distribution pre_processing samplers\n        logits_processor = self._get_logits_processor(\n            generation_config=generation_config,\n            input_ids_seq_length=input_ids_seq_length,\n            encoder_input_ids=input_ids,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            logits_processor=logits_processor,\n        )\n        # 9. prepare stopping criteria\n        stopping_criteria = self._get_stopping_criteria(\n            generation_config=generation_config, stopping_criteria=stopping_criteria\n        )\n        logits_warper = self._get_logits_warper(generation_config)\n\n        if is_greedy_gen_mode:\n            # 11. run greedy search\n            return self.stream_greedy_search(\n                input_ids,\n                logits_processor,\n                stopping_criteria,\n                generation_config,\n                synced_gpus,\n                **model_kwargs,\n            )\n        elif is_sample_gen_mode:\n            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n            input_ids, model_kwargs = self._expand_inputs_for_generation(\n                input_ids=input_ids,\n                expand_size=generation_config.num_return_sequences,\n                is_encoder_decoder=self.config.is_encoder_decoder,\n                **model_kwargs,\n            )\n            return self.stream_sample(\n                generation_config,\n                input_ids,\n                logits_processor,\n                logits_warper,\n                stopping_criteria,\n                synced_gpus,\n                **model_kwargs,\n            )\n        elif is_beam_gen_mode:\n            return self.stream_beam_search(\n                generation_config,\n                input_ids,\n                logits_processor,\n                stopping_criteria,\n                synced_gpus,\n                **model_kwargs,\n            )\n        elif is_beam_sample_gen_mode:\n            # interleave input_ids with `num_beams` additional sequences per batch\n            return self.stream_beam_sample(\n                input_ids,\n                logits_processor,\n                logits_warper,\n                stopping_criteria,\n                generation_config,\n                synced_gpus,\n                **model_kwargs,\n            )\n        else:\n            raise Exception('not implement')\n        \n    def stream_sample(\n        self,\n        generation_config,\n        input_ids,\n        logits_processor,\n        logits_warper,\n        stopping_criteria,\n        synced_gpus,\n        **model_kwargs,\n    ):\n        bos_token_id, eos_token_id, pad_token_id = (\n            generation_config.bos_token_id,\n            generation_config.eos_token_id,\n            generation_config.pad_token_id,\n        )\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n        # keep track of which sequences are already finished\n        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n        this_peer_finished = False  # used by synced_gpus only\n        scores=()\n        # auto-regressive generation\n        while True:\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n            # prepare model inputs\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            # forward pass to get next token\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n            )\n            if synced_gpus and this_peer_finished:\n                continue  # don't waste resources running the code we don't need\n            next_token_logits = outputs.logits[:, -1, :]\n            # pre-process distribution\n            next_token_scores = logits_processor(input_ids, next_token_logits)\n            next_token_scores = logits_warper(input_ids, next_token_scores)\n\n            # sample\n            probs = nn.functional.softmax(next_token_scores, dim=-1)\n            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\n            # finished sentences should have their next token be a padding token\n            if eos_token_id is not None:\n                if pad_token_id is None:\n                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n\n            # update generated ids, model inputs, and length for next step\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n            yield input_ids\n            # if eos_token was found in one sentence, set sentence to finished\n            if eos_token_id_tensor is not None:\n                unfinished_sequences = unfinished_sequences.mul(\n                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n                )\n            \n            # stop when each sentence is finished, or if we exceed the maximum length\n            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n        yield input_ids\n\n    def stream_beam_sample(\n        self,\n        input_ids,\n        logits_processor,\n        logits_warper,\n        stopping_criteria,\n        generation_config,\n        synced_gpus,\n        **model_kwargs,\n    ):\n        bos_token_id, eos_token_id, pad_token_id = (\n            generation_config.bos_token_id,\n            generation_config.eos_token_id,\n            generation_config.pad_token_id,\n        )\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n        num_beams = generation_config.num_beams\n        batch_size, cur_len = input_ids.shape[0], input_ids.shape[-1]\n        beam_scorer = BeamSearchScorer(\n            batch_size=batch_size,\n            num_beams=generation_config.num_beams,\n            device=input_ids.device,\n            length_penalty=generation_config.length_penalty,\n            do_early_stopping=generation_config.early_stopping,\n            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n            max_length=generation_config.max_length,\n        )\n        input_ids, model_kwargs = self._expand_inputs_for_generation(\n            input_ids=input_ids,\n            expand_size=generation_config.num_beams * generation_config.num_return_sequences,\n            is_encoder_decoder=self.config.is_encoder_decoder,\n            **model_kwargs,\n        )\n        scores = ()\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n        beam_scores = beam_scores.view((batch_size * num_beams,))\n\n        this_peer_finished = False  # used by synced_gpus only\n        while True:\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n            )\n\n            if synced_gpus and this_peer_finished:\n                cur_len = cur_len + 1\n                continue  # don't waste resources running the code we don't need\n\n            next_token_logits = outputs.logits[:, -1, :]\n\n            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n            # cannot be generated both before and after the `nn.functional.log_softmax` operation.\n            next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n            next_token_scores = nn.functional.log_softmax(\n                next_token_logits, dim=-1\n            )  # (batch_size * num_beams, vocab_size)\n\n            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(next_token_scores)\n            # Note: logits warpers are intentionally applied after adding running beam scores. On some logits warpers\n            # (like top_p) this is indiferent, but on others (like temperature) it is not. For reference, see\n            # https://github.com/huggingface/transformers/pull/5420#discussion_r449779867\n            next_token_scores = logits_warper(input_ids, next_token_scores)\n\n            # reshape for beam search\n            vocab_size = next_token_scores.shape[-1]\n            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n\n            probs = nn.functional.softmax(next_token_scores, dim=-1)\n\n            next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)\n            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n\n            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n            next_tokens = torch.gather(next_tokens, -1, _indices)\n\n            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n            next_tokens = next_tokens % vocab_size\n\n            # stateless\n            beam_outputs = beam_scorer.process(\n                input_ids,\n                next_token_scores,\n                next_tokens,\n                next_indices,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                beam_indices=None,\n            )\n            beam_scores = beam_outputs[\"next_beam_scores\"]\n            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n            beam_idx = beam_outputs[\"next_beam_indices\"]\n\n            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n            yield input_ids\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n            if model_kwargs[\"past_key_values\"] is not None:\n                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n\n            # increase cur_len\n            cur_len = cur_len + 1\n\n            if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n\n        sequence_outputs = beam_scorer.finalize(\n            input_ids,\n            beam_scores,\n            next_tokens,\n            next_indices,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            max_length=stopping_criteria.max_length,\n            beam_indices=None,\n        )\n        yield sequence_outputs[\"sequences\"]\n\n    def stream_greedy_search(\n        self,\n        input_ids,\n        logits_processor,\n        stopping_criteria,\n        generation_config,\n        synced_gpus,\n        **model_kwargs,\n    ):\n        # init values\n        bos_token_id, eos_token_id, pad_token_id = (\n            generation_config.bos_token_id,\n            generation_config.eos_token_id,\n            generation_config.pad_token_id,\n        )\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n        # init attention / hidden states / scores tuples\n        scores = () \n        # keep track of which sequences are already finished\n        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n        this_peer_finished = False  # used by synced_gpus only\n        while True:\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n\n            # prepare model inputs\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            # forward pass to get next token\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n            )\n\n            if synced_gpus and this_peer_finished:\n                continue  # don't waste resources running the code we don't need\n\n            next_token_logits = outputs.logits[:, -1, :]\n            # pre-process distribution\n            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n            # argmax\n            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n            # finished sentences should have their next token be a padding token\n            if eos_token_id is not None:\n                if pad_token_id is None:\n                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n            # update generated ids, model inputs, and length for next step\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n            yield input_ids\n            # if eos_token was found in one sentence, set sentence to finished\n            if eos_token_id_tensor is not None:\n                unfinished_sequences = unfinished_sequences.mul(\n                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n                )\n\n            # stop when each sentence is finished, or if we exceed the maximum length\n            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n        yield input_ids\n\n    def stream_beam_search(\n        self,\n        generation_config,\n        input_ids,\n        logits_processor,\n        stopping_criteria,\n        synced_gpus,\n        **model_kwargs,\n    ):\n\n        # 10. go into beam search generation modes\n        # 11. prepare beam search scorer\n        bos_token_id, eos_token_id, pad_token_id = (\n            generation_config.bos_token_id,\n            generation_config.eos_token_id,\n            generation_config.pad_token_id,\n        )\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        num_beams = generation_config.num_beams\n        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n        beam_scorer = BeamSearchScorer(\n            batch_size=batch_size,\n            num_beams=generation_config.num_beams,\n            device=input_ids.device,\n            length_penalty=generation_config.length_penalty,\n            do_early_stopping=generation_config.early_stopping,\n            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n            max_length=generation_config.max_length,\n        )\n        # 12. interleave input_ids with `num_beams` additional sequences per batch\n        input_ids, model_kwargs = self._expand_inputs_for_generation(\n            input_ids=input_ids,\n            expand_size=generation_config.num_beams,\n            is_encoder_decoder=self.config.is_encoder_decoder,\n            **model_kwargs,\n        )\n        # beam_search logits\n        batch_beam_size, cur_len = input_ids.shape\n        if num_beams * batch_size != batch_beam_size:\n            raise ValueError(\n                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n            )\n        beam_scores = torch.zeros(\n            (batch_size, num_beams), dtype=torch.float, device=input_ids.device\n        )\n        beam_scores[:, 1:] = -1e9\n        beam_scores = beam_scores.view((batch_size * num_beams,))\n        this_peer_finished = False  # used by synced_gpus only\n        while True:\n\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(\n                    0.0 if this_peer_finished else 1.0\n                ).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n                output_attentions=False,\n                output_hidden_states=False,\n            )\n\n            if synced_gpus and this_peer_finished:\n                cur_len = cur_len + 1\n                continue  # don't waste resources running the code we don't need\n\n            next_token_logits = outputs.logits[:, -1, :]\n            # next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len) hack: adjust tokens for Marian.\n            next_token_scores = nn.functional.log_softmax(\n                next_token_logits, dim=-1\n            )  # (batch_size * num_beams, vocab_size)\n            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n            next_token_scores = next_token_scores_processed + beam_scores[\n                :, None\n            ].expand_as(next_token_scores)\n\n            # reshape for beam search\n            vocab_size = next_token_scores.shape[-1]\n            next_token_scores = next_token_scores.view(\n                batch_size, num_beams * vocab_size\n            )\n\n            # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)\n            next_token_scores, next_tokens = torch.topk(\n                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n            )\n            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n            next_tokens = next_tokens % vocab_size\n            # stateless\n            beam_outputs = beam_scorer.process(\n                input_ids,\n                next_token_scores,\n                next_tokens,\n                next_indices,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                beam_indices=None,\n            )\n            beam_scores = beam_outputs[\"next_beam_scores\"]\n            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n            beam_idx = beam_outputs[\"next_beam_indices\"]\n\n            input_ids = torch.cat(\n                [input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1\n            )\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n            if model_kwargs[\"past_key_values\"] is not None:\n                model_kwargs[\"past_key_values\"] = self._reorder_cache(\n                    model_kwargs[\"past_key_values\"], beam_idx\n                )\n\n            # increase cur_len\n            cur_len = cur_len + 1\n\n            yield input_ids\n\n            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n\n        final_result = beam_scorer.finalize(\n            input_ids,\n            beam_scores,\n            next_tokens,\n            next_indices,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            max_length=stopping_criteria.max_length,\n            beam_indices=None,\n        )\n        yield final_result[\"sequences\"]", "\nclass StreamLlamaForCausalLM(LlamaForCausalLM, StreamGenerationMixin):\n    pass\n\nclass StreamPeftGenerationMixin(PeftModelForCausalLM, StreamGenerationMixin):\n\n    # default it call `model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config)`, not cls!! so inherent PeftModelForCausalLM is non sense\n    @classmethod\n    def from_pretrained(cls, model, model_id, adapter_name=\"default\", is_trainable=False,  **kwargs):\n        # work in peft==0.3.0\n        if peft.__version__ >= '0.3.0' and peft.__version__ != '0.3.0.dev0':\n            # load the config\n            from peft.utils import PromptLearningConfig\n            config = LoraConfig.from_pretrained(model_id)\n\n            if (getattr(model, \"hf_device_map\", None) is not None) and len(\n                set(model.hf_device_map.values()).intersection({\"cpu\", \"disk\"})\n            ) > 0:\n                remove_hook_from_submodules(model)\n\n            if isinstance(config, PromptLearningConfig) and is_trainable:\n                raise ValueError(\"Cannot set a prompt learning adapter to trainable when loading pretrained adapter.\")\n            else:\n                config.inference_mode = not is_trainable\n\n            # here is the hack\n            model = cls(model, config, adapter_name)\n            model.load_adapter(model_id, adapter_name, **kwargs)\n            # NOTICE\n            model.base_model_prepare_inputs_for_generation = model.base_model.prepare_inputs_for_generation\n            model._reorder_cache = model.base_model._reorder_cache\n            return model\n        else:\n            return cls.from_pretrained_old_peft_version(model, model_id, **kwargs)\n\n\n    @classmethod\n    def from_pretrained_old_peft_version(cls, model, model_id, **kwargs):\n        # work well in peft@e536616888d51b453ed354a6f1e243fecb02ea08\n\n        # load the config\n        config = LoraConfig.from_pretrained(model_id)\n\n        if getattr(model, \"hf_device_map\", None) is not None:\n            remove_hook_from_submodules(model)\n\n        # here is the hack\n        model = cls(model, config)\n        model._reorder_cache = model.base_model._reorder_cache\n        # load weights if any\n        if os.path.exists(os.path.join(model_id, \"adapter_model.bin\")):\n            filename = os.path.join(model_id, \"adapter_model.bin\")\n        else:\n            try:\n                filename = hf_hub_download(model_id, \"adapter_model.bin\")\n            except:  # noqa\n                raise ValueError(\n                    f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n                    f\"Please check that the file {'adapter_model.bin'} is present at {model_id}.\"\n                )\n\n        adapters_weights = torch.load(\n            filename,\n            map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n        )\n        # load the weights into the model\n        model = set_peft_model_state_dict(model, adapters_weights)\n        if getattr(model, \"hf_device_map\", None) is not None:\n            device_map = kwargs.get(\"device_map\", \"auto\")\n            max_memory = kwargs.get(\"max_memory\", None)\n            no_split_module_classes = model._no_split_modules\n            if device_map != \"sequential\":\n                max_memory = get_balanced_memory(\n                    model,\n                    max_memory=max_memory,\n                    no_split_module_classes=no_split_module_classes,\n                    low_zero=(device_map == \"balanced_low_0\"),\n                )\n            if isinstance(device_map, str):\n                device_map = infer_auto_device_map(\n                    model,\n                    max_memory=max_memory,\n                    no_split_module_classes=no_split_module_classes,\n                )\n            model = dispatch_model(model, device_map=device_map)\n            hook = AlignDevicesHook(io_same_device=True)\n            if model.peft_config.peft_type == PeftType.LORA:\n                add_hook_to_module(model.base_model.model, hook)\n            else:\n                remove_hook_from_submodules(model.prompt_encoder)\n                add_hook_to_module(model.base_model, hook)\n        return model", ""]}
{"filename": "prompt.py", "chunked_list": ["\nimport transformers\nfrom utils import printf\nimport copy\n\nclass prompt:\n    def __init__(self, tokenizer, max_len, add_eos=True):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.add_eos=add_eos", "\nclass instruct_prompt(prompt):\n    prompt = (\n        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n    )\n    prompt_input = (\n        \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:{instruction}\\n\\n### Input:{input}\\n\\n### Response:\"\n    )\n    prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n    prompt_post = \"User:{input}\\n\\nAssistant:\"\n\n    def preprocess_gen(self, data_point):\n        if 'history' not in data_point:\n        # single instruction format {'instruction':..,'input':..}\n            if 'input' in data_point:\n                user_prompt = self.prompt_input.format_map(data_point)\n            else:\n                user_prompt = self.prompt.format_map(data_point)\n        else:\n        # multi turn format {'history':[..], 'input':[..]}\n            user_prompt = \"\\n\".join([\"User:\" + i['input']+\"\\n\"+\"Assistant:\" + i['output'] for i in data_point['history']]) + \"\\nUser:\" + data_point['input'] + \"\\nAssistant:\"\n            user_prompt = user_prompt[-self.max_len:]\n        user_prompt=self.prompt.format_map({'instruction':user_prompt})\n        input_ids = self.tokenizer(user_prompt)[\"input_ids\"]\n        return input_ids\n\n    def preprocess_train(self, data_point):\n        # single instruction format {'instruction':..,'input':..,'output':..}\n        if 'instruction' in data_point:\n            if 'input' in data_point:\n                user_prompt = self.prompt_input.format_map(data_point)\n            else:\n                user_prompt = self.prompt.format_map(data_point)\n            output = data_point[\"output\"]\n        # multi turn format {'input':[..], 'output':[..]}\n        else:\n            user_prompt = ''\n            lens = len(data_point['input'])\n            for i in range(lens-1):\n                user_prompt += self.prompt_history.format_map({'input':data_point['input'][i],'output':data_point['output'][i]})\n            user_prompt += self.prompt_post.format_map({'input':data_point['input'][-1]})\n            user_prompt = self.prompt.format_map({'instruction': user_prompt})\n            output = data_point['output'][-1]\n\n        len_user_prompt_tokens = (len(self.tokenizer(\n            user_prompt,\n            truncation=True,\n            max_length=self.max_len + 1,\n        )[\"input_ids\"])- 1)  # no eos token\n        full_tokens = self.tokenizer(\n            user_prompt + output,\n            truncation=True,\n            max_length=self.max_len + 1,\n            padding=\"max_length\",\n        )[\"input_ids\"][:-1]\n        return {\n            \"input_ids\": full_tokens,\n            \"labels\": [-100] * len_user_prompt_tokens\n            + full_tokens[len_user_prompt_tokens:],\n            \"attention_mask\": [1] * (len(full_tokens)),\n        }\n\n    def data_collator(self,):\n        return transformers.DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n\n    def postprocess(self, text, render=True):\n        #import pdb;pdb.set_trace()\n        printf(text)\n        output = text.split(\"### Response:\")[1].strip()\n        output = output.replace(\"Belle\", \"Vicuna\")\n        printf(output)\n        if '###' in output:\n            output = output.split(\"###\")[0]\n        if 'User' in output:\n            output = output.split(\"User\")[0]\n        output = output.replace('\ufffd','').replace('</s>', '') \n        if render:\n            # fix gradio chatbot markdown code render bug\n            lines = output.split(\"\\n\")\n            for i, line in enumerate(lines):\n                if \"```\" in line:\n                    if line != \"```\":\n                        lines[i] = f'<pre><code class=\"language-{lines[i][3:]}\">'\n                    else:\n                        lines[i] = '</code></pre>'\n                else:\n                    if i > 0:\n                        lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"__\", '\\_\\_')\n            output =  \"\".join(lines)\n            # output = output.replace('<br/><pre>','\\n<pre>') work for html; but not for gradio\n        return output", "\nclass chat_prompt(prompt):\n    prompt_pre = (\n        \"The following is a conversation between an AI assistant called Assistant and a human user called User. \"\n        \"The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n\"\n    )\n    prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n    prompt_post = \"User:{input}\\n\\nAssistant:\"\n\n    def preprocess_gen(self, data_point):\n        user_prompt = self.prompt_pre\n        len_avail = self.max_len - len(self.tokenizer(user_prompt, add_special_tokens=False)['input_ids'])\n        input_prompt = self.prompt_post.format_map({'input':data_point['input']})\n        len_avail -= len(self.tokenizer(input_prompt, add_special_tokens=False)['input_ids'])\n        lens = len(data_point['history'])\n        tokenized_lens = []\n        for i in range(lens):\n            tmp_prompt = self.prompt_history.format_map(data_point['history'][i])\n            tokenized_lens.append(len(self.tokenizer(tmp_prompt,add_special_tokens=False)[\"input_ids\"]))\n        \n        # \u542f\u53d1\u5f0f\uff1a/2 \u4f18\u5148\u9664\u524d\u9762\u7684\n        i = 0\n        while sum(tokenized_lens) > len_avail and i < lens:\n            history = data_point['history'][i]\n            tmp_len1 = len(history['input'])\n            tmp_len2 = len(history['output'])\n            if tmp_len2 > tmp_len1:\n                history['output'] = history['output'][:tmp_len2//2]\n            else:\n                history['input'] = history['input'][:tmp_len1//2]\n            prompt = self.prompt_history.format_map(history)\n            single_len =(len(self.tokenizer(prompt,add_special_tokens=False)[\"input_ids\"]))\n            tokenized_lens[i] = single_len\n            i += 1\n        total_len = sum(tokenized_lens)\n        # \u8fd8\u4e0d\u591f\u7684\u8bdd \u76f4\u63a5\u622a\u65ad\n        while total_len > len_avail and i < lens - 1 :\n            total_len -= tokenized_lens[i]\n            data_point['history'] = data_point['history'][1:]\n            i += 1\n        # \u6700\u7ec8\u5408\u5e76\n        for i in range(lens):\n            user_prompt += self.prompt_history.format_map(data_point['history'][i])\n        user_prompt += input_prompt\n        printf({'real_input:':user_prompt})\n        inputs = self.tokenizer(user_prompt)[\"input_ids\"]\n        return inputs\n\n    def preprocess_train(self, data_point):\n        user_prompt = self.prompt_pre\n        lens = len(data_point['input'])\n        for i in range(lens-1):\n            user_prompt += self.prompt_history.format_map({'input':data_point['input'][i].strip(),'output':data_point['output'][i].strip()})\n        user_prompt += self.prompt_post.format_map({'input':data_point['input'][-1].strip()})\n\n        len_user_prompt_tokens = len(self.tokenizer(\n            user_prompt,\n            truncation=True,\n            max_length=self.max_len,\n        )[\"input_ids\"]) - 1 # remove extra eos\n        if self.add_eos:\n            full_tokens = self.tokenizer(\n                user_prompt + data_point[\"output\"][-1].strip(),\n                truncation=True,\n                padding=False,\n                max_length=self.max_len,\n            )[\"input_ids\"] # need eos\n        else:\n            full_tokens = self.tokenizer(\n                user_prompt + data_point[\"output\"][-1].strip(),\n                truncation=True,\n                padding=False,\n                max_length=self.max_len+1,\n            )[\"input_ids\"][:-1] # delete eos\n        return {\n            \"input_ids\": full_tokens,\n            \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n            \"attention_mask\": [1] * (len(full_tokens)),\n        }\n\n    def data_collator(self,):\n        return transformers.DataCollatorForSeq2Seq(self.tokenizer)\n\n    def postprocess(self, text, render=False):\n        output = text.split(\"Assistant:\")[-1].strip()\n        if 'User:' in output:\n            output = output.split(\"User:\")[0]\n        output = output.replace('\ufffd','') \n        if render:\n            # fix gradio chatbot markdown code render bug\n            lines = output.split(\"\\n\")\n            for i, line in enumerate(lines):\n                if \"```\" in line:\n                    if line != \"```\":\n                        lines[i] = f'<pre><code class=\"language-{lines[i][3:]}\">'\n                    else:\n                        lines[i] = '</code></pre>'\n                else:\n                    if i > 0:\n                        lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"__\", '\\_\\_')\n            output =  \"\".join(lines)\n            # output = output.replace('<br/><pre>','\\n<pre>') work for html; but not for gradio\n        return output\n\n    def get_data_collator():\n        return transformers.DataCollatorForLanguageModeling", ""]}
{"filename": "finetune_chat.py", "chunked_list": ["from peft import (\n    prepare_model_for_int8_training,\n    LoraConfig,\n    PeftModel,\n    get_peft_model,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n)\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, TrainerCallback, GenerationConfig\nimport os", "from transformers import LlamaForCausalLM, LlamaTokenizer, TrainerCallback, GenerationConfig\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom datasets import load_dataset, Dataset\nimport transformers\nfrom huggingface_hub import snapshot_download\nimport argparse", "from huggingface_hub import snapshot_download\nimport argparse\nimport warnings\nfrom tqdm import tqdm\nfrom functools import partial\nimport utils\nimport prompt\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"", "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\n# 0. prepare args and logger\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\nparser.add_argument(\"--prompt_type\", type=str, default=\"chat\")\nparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\nparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")", "parser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--num_epoch\", type=int, default=3)\nparser.add_argument(\"--micro_batch\", type=int, default=4)\nparser.add_argument(\"--total_batch\", type=int, default=128)\nparser.add_argument(\"--log_steps\", type=int, default=100)\nparser.add_argument(\"--eval_steps\", type=int, default=200)\nparser.add_argument(\"--save_steps\", type=int, default=200)\nparser.add_argument(\"--warmup_ratio\", type=float, default=0.05)\nparser.add_argument(\"--test_size\", type=int, default=200)", "parser.add_argument(\"--warmup_ratio\", type=float, default=0.05)\nparser.add_argument(\"--test_size\", type=int, default=200)\nparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\nparser.add_argument(\"--ignore_data_skip\", type=bool, default=False)\nargs = parser.parse_args()\nif not args.wandb:\n    os.environ[\"WANDB_MODE\"] = \"disable\"\nMICRO_BATCH_SIZE = args.micro_batch  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = args.total_batch", "MICRO_BATCH_SIZE = args.micro_batch  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = args.total_batch\nMAX_STEPS = None\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = args.num_epoch \nLEARNING_RATE = 3e-4  # the Karpathy constant\nCUTOFF_LEN = 2048  \nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05", "LORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nUSE_8bit = True\nVAL_SET_SIZE = args.test_size  # 2000\nTARGET_MODULES = [\n    \"q_proj\",\n    \"v_proj\",\n    \"k_proj\",\n    \"o_proj\",\n    \"down_proj\",", "    \"o_proj\",\n    \"down_proj\",\n    \"gate_proj\",\n    \"up_proj\",\n]\nDATA_PATH = args.data_path  \nOUTPUT_DIR = args.output_path  # \"lora-Vicuna\"\n\ndevice_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))", "device_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n# we must make sure batch_size and gradient_accumulation_steps not changed for resuming training.\nif args.resume_from_checkpoint:\n    old_args_path = os.path.join(args.resume_from_checkpoint, 'training_args.bin')\n    if os.path.exists(old_args_path):\n        old_args = torch.load(old_args_path)\n        if MICRO_BATCH_SIZE != old_args.per_device_train_batch_size:\n            raise Exception(\n                f'current micro batch size {MICRO_BATCH_SIZE} is not equal to the old {old_args.per_device_train_batch_size},'\n                ' This will cause the trainer skips wrong epochs or steps.'\n                f'please change your micro batch size to {old_args.per_device_train_batch_size}'\n                ' or cancel resuming your training'\n                )\n        if GRADIENT_ACCUMULATION_STEPS != old_args.gradient_accumulation_steps:\n            raise Exception(\n                f'current total batch {BATCH_SIZE} is not equal to the old {old_args.gradient_accumulation_steps*old_args.per_device_train_batch_size},'\n                ' This will cause the trainer skips wrong epochs or steps.'\n                f'please change your total batch size to {old_args.gradient_accumulation_steps*old_args.per_device_train_batch_size}'    \n                ' or cancel resuming your training'\n            )\n    else:\n        raise Exception(f'{old_args_path} is not exist!')", "    # checkpoint = os.path.join(args.resume_from_checkpoint, 'pytorch_model.bin')\n\nlogger = utils.set_file_logger(__name__,OUTPUT_DIR)\n# 1. load dataset\nlogger.info(f'>>> processing data from {DATA_PATH}')\nlogger.info(f'>>> using {args}')\n\ntrain_tokenizer = LlamaTokenizer.from_pretrained(args.model_path, add_eos_token=True)\nassert train_tokenizer.eos_token_id == 2, \"Tokenizer eos is wrong!!!\"\n# unk. we want this to be different from the eos token", "assert train_tokenizer.eos_token_id == 2, \"Tokenizer eos is wrong!!!\"\n# unk. we want this to be different from the eos token\ntrain_tokenizer.pad_token_id = 0  \n# cannot use eos in generation!\n# tokenizer.padding_side = \"left\"  # Allow batched inference\ntest_tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\nif args.prompt_type == 'instruct':\n    PROMPT = prompt.instruct_prompt(train_tokenizer, CUTOFF_LEN)\nelif args.prompt_type == 'chat':\n    PROMPT = prompt.chat_prompt(train_tokenizer,CUTOFF_LEN)\nelse:\n    raise Exception('not support')", "# check tokenizer\ndata = load_dataset('json', data_files=DATA_PATH)\nimport random;start = random.randint(1, 100)\nexamples = Dataset.from_dict(data['train'][start:start+5]).map(PROMPT.preprocess_train)\nfor example in examples:\n    logger.info(f'>>> using prompt {args.prompt_type}, prompt example:\\n { train_tokenizer.decode(example[\"input_ids\"]) }')\n    logger.info(f'>>> tokenizer labels: { train_tokenizer.decode([ 0 if l==-100 else l for l in example[\"labels\"]])}')\n    logger.info(f'>>> tokenizer example: { example[\"input_ids\"][:10] }...{ example[\"input_ids\"][-10:]}')\n# 2. load model and checkpoints\nlogger.info(f'>>> load model from {args.model_path}')", "# 2. load model and checkpoints\nlogger.info(f'>>> load model from {args.model_path}')\n\nif USE_8bit is True:\n    assert bnb.__version__ >= '0.37.2', \"Please downgrade bitsandbytes's version, for example: pip install bitsandbytes==0.37.2\"\nmodel = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_8bit=USE_8bit,\n    device_map=device_map,\n    torch_dtype=torch.float16,", "    device_map=device_map,\n    torch_dtype=torch.float16,\n)\nif USE_8bit is True:\n    model = prepare_model_for_int8_training(model)\nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,", "    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)\nif args.resume_from_checkpoint:\n    checkpoint_name = os.path.join(args.resume_from_checkpoint, \"pytorch_model.bin\")\n    # adapter_model.bin\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(args.resume_from_checkpoint, \"adapter_model.bin\")\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            logger.warning(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = None  # So the trainer won't try loading its state\n    # pytorch_model.bin\n    if os.path.exists(checkpoint_name):\n        logger.info(f'>>> load lora from {checkpoint_name}')\n        adapters_weights = torch.load(checkpoint_name)\n        set_peft_model_state_dict(model, adapters_weights)\n    else:\n        raise Exception(f\"Checkpoint {checkpoint_name} not found with resume_from_checkpoint=True!\")", "\ntrainable_params = 0\nall_param = 0\nfor _, param in model.named_parameters():\n    num_params = param.numel()\n    # if using DS Zero 3 and the weights are initialized empty\n    if num_params == 0 and hasattr(param, \"ds_numel\"):\n        num_params = param.ds_numel\n    all_param += num_params\n    if param.requires_grad:\n        trainable_params += num_params", "logger.info(f\">>> trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n# 3. speedup dataset processing by multi-process\nnum_proc = (os.cpu_count())\nif VAL_SET_SIZE > 0:\n    train_val = data[\"train\"].train_test_split(test_size=VAL_SET_SIZE, shuffle=True, seed=42)\n    train_data = train_val[\"train\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\n    val_data = train_val[\"test\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\nelse:\n    train_data = data[\"train\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\n    val_data = None", "now_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    # the trainer will ignore the state max_steps and caculate max_steps based on epochs,\n    # so we mannally set the args.max_step to override it. \n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    train_state_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    if os.path.exists(train_state_path):\n        import json\n        base_train_args = json.load(open(train_state_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            logger.warning(f\"epoch {EPOCHS}:{MAX_STEPS} replace to the base_max_steps {base_max_steps}\")\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\n    assert MAX_STEPS is not None\nelse:\n    MAX_STEPS = now_max_steps", "\n# 4. start training\nclass CustomCallback(TrainerCallback):\n    \n    def __init__(self, trainer) -> None:\n        super().__init__()\n        self.trainer = trainer\n        self.generation_config = GenerationConfig(\n            temperature=1.0,\n            top_p=0.75,\n            top_k=40,\n            num_beams=2,\n            bos_token_id=train_tokenizer.bos_token_id,\n            eos_token_id=train_tokenizer.eos_token_id,\n            pad_token_id=train_tokenizer.pad_token_id,\n            max_new_tokens=1024, # max_length=max_new_tokens+input_sequence\n            min_new_tokens=1, # min_length=min_new_tokens+input_sequence\n            bad_words_ids=test_tokenizer(['\\n\\nUser:','\\n\\nAssistant:'], add_special_tokens=False).input_ids\n        )\n        self.repetition_penalty=1.3\n        self.logger = utils.set_file_logger('transformers.trainer', trainer.args.output_dir)\n\n    def on_log(self, args, state, control, logs, **kwargs):\n        logger.info(logs)", "\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_ratio=args.warmup_ratio,\n        num_train_epochs=EPOCHS,", "        warmup_ratio=args.warmup_ratio,\n        num_train_epochs=EPOCHS,\n        max_steps=MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=args.log_steps,\n        logging_first_step=True, # convenient\n        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,", "        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,\n        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n        ddp_find_unused_parameters=False if ddp else None,\n        report_to=\"wandb\" if args.wandb else [],\n        ignore_data_skip=args.ignore_data_skip,\n    ),\n    data_collator=PROMPT.data_collator()", "    ),\n    data_collator=PROMPT.data_collator()\n)\ntrainer.add_callback(CustomCallback(trainer))\nmodel.config.use_cache = False\n\nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))", "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\ntrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\nmodel.save_pretrained(OUTPUT_DIR)\n", ""]}
{"filename": "test_tokenizer.py", "chunked_list": ["import os\nimport sys\nimport torch\nimport transformers\nimport argparse\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n\n    \nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"yahma/llama-7b-hf\") #yahma/llama-7b-hf #decapoda-research/llama-7b-hf", "parser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"yahma/llama-7b-hf\") #yahma/llama-7b-hf #decapoda-research/llama-7b-hf\nargs = parser.parse_args()\n\ntokenizer = LlamaTokenizer.from_pretrained(\n    args.model_path, add_eos_token=True\n)\n\ntest_text = [\"Hello, nice to meet you!\", \"\u4f60\u597d\u5f88\u9ad8\u5174\u80fd\u89c1\u5230\u4f60\uff01\"]\n\nfor text in test_text:\n    input_ids = tokenizer.encode(text)\n    print(f\"input_ids: {input_ids}\")\n    decode_text = tokenizer.decode(input_ids)\n    print(f\"decode_text: {decode_text}\")", "test_text = [\"Hello, nice to meet you!\", \"\u4f60\u597d\u5f88\u9ad8\u5174\u80fd\u89c1\u5230\u4f60\uff01\"]\n\nfor text in test_text:\n    input_ids = tokenizer.encode(text)\n    print(f\"input_ids: {input_ids}\")\n    decode_text = tokenizer.decode(input_ids)\n    print(f\"decode_text: {decode_text}\")\n\n\"\"\"\nCorrect ==>  yahma/llama-7b-hf + newest Transformers(>=4.28.1):", "\"\"\"\nCorrect ==>  yahma/llama-7b-hf + newest Transformers(>=4.28.1):\n> !!! Beginning with 1 (bos), ending with 2 (eos) !!!\n\ninput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\ndecode_text: <s> Hello, nice to meet you!</s>\ninput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\ndecode_text: <s> \u4f60\u597d\u5f88\u9ad8\u5174\u80fd\u89c1\u5230\u4f60\uff01</s>\n\nCorrect ==> decapoda-research/llama-7b-hf + Old Transformers like our version(transformers @ git+https://github.com/huggingface/transformers.git@0dcb46e7a4a9e587ba84ff35778ab4233a184c11)", "\nCorrect ==> decapoda-research/llama-7b-hf + Old Transformers like our version(transformers @ git+https://github.com/huggingface/transformers.git@0dcb46e7a4a9e587ba84ff35778ab4233a184c11)\ninput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\ndecode_text:  Hello, nice to meet you!\ninput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\ndecode_text:  \u4f60\u597d\u5f88\u9ad8\u5174\u80fd\u89c1\u5230\u4f60\uff01\n\nCorrect ==> decapoda-research/llama-7b-hf + Old Transformers like our version(transformers @ git+https://github.com/huggingface/transformers.git@0dcb46e7a4a9e587ba84ff35778ab4233a184c11)\ninput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\ndecode_text:  Hello, nice to meet you!", "input_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\ndecode_text:  Hello, nice to meet you!\ninput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\ndecode_text:  \u4f60\u597d\u5f88\u9ad8\u5174\u80fd\u89c1\u5230\u4f60\uff01\n\n\n\u8001\u7248\u672ctransformers\u7684\u95ee\u9898\uff1a\u4ee3\u7801\u9ed8\u8ba4\u52a0\u8f7dtokenizer.model\n\u65b0\u7248\u672ctransformers\u7684\u4fee\u6539\uff1a\u65b0\u7248\u672c\u9ed8\u8ba4\u52a0\u8f7dconfig\n\ndecapoda-research\uff1aconfig\u7684bos=0\uff0ceos=1\uff08\u00d7\uff09\uff0ctokenizer.model\u662f\u6b63\u786e\u7684", "\ndecapoda-research\uff1aconfig\u7684bos=0\uff0ceos=1\uff08\u00d7\uff09\uff0ctokenizer.model\u662f\u6b63\u786e\u7684\nyahma\uff1aconfig\u7684bos=1\uff0ceos=2\uff0ctokenizer.model\u662f\u6b63\u786e\u7684\n\"\"\""]}
{"filename": "chat.py", "chunked_list": ["import sys\nimport torch\nfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\nimport transformers\nimport json\nimport gradio as gr\nimport argparse\nimport warnings\nimport os\nfrom datetime import datetime", "import os\nfrom datetime import datetime\nfrom utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM, printf\nimport utils\nimport copy\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\nimport prompt", "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\nimport prompt\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--lora_path\", type=str, default='')\nparser.add_argument(\"--use_typewriter\", type=int, default=1)\nparser.add_argument(\"--prompt_type\", type=str, default='chat')\nparser.add_argument(\"--share_link\", type=int, default=0)\nparser.add_argument(\"--show_beam\", type=int, default=0)", "parser.add_argument(\"--share_link\", type=int, default=0)\nparser.add_argument(\"--show_beam\", type=int, default=0)\nparser.add_argument(\"--int8\", type=int, default=1)\nargs = parser.parse_args()\nargs.fix_token = True\nprintf('>>> args:', args)\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\nLOAD_8BIT = args.int8\nBASE_MODEL = args.model_path", "LOAD_8BIT = args.int8\nBASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path\n\n# fix the path for local checkpoint\nlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\nif args.lora_path != '' and os.path.exists(args.lora_path):\n    if not os.path.exists(lora_bin_path):\n        pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n        printf('>>> load lora from', pytorch_bin_path)\n        if os.path.exists(pytorch_bin_path):\n            os.rename(pytorch_bin_path, lora_bin_path)\n            warnings.warn(\n                \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n            )\n        else:\n            assert ('Checkpoint is not Found!')\n    else:\n        printf('>>> load lora from', lora_bin_path)\nelse:\n    printf('>>> load lora from huggingface url', args.lora_path)", "\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:\n    pass", "\nif device == \"cuda\":\n    print(f'>>> load raw models from {BASE_MODEL}')\n    if args.lora_path == \"\":\n        model = StreamLlamaForCausalLM.from_pretrained(\n            BASE_MODEL,\n            load_in_8bit=LOAD_8BIT,\n            torch_dtype=torch.float16,\n            device_map={\"\": 0},\n        )    \n    else:\n        print(f'>>> load lora models from {LORA_WEIGHTS}')\n        model = LlamaForCausalLM.from_pretrained(\n            BASE_MODEL,\n            load_in_8bit=LOAD_8BIT,\n            torch_dtype=torch.float16,\n            device_map={\"\": 0},\n        )\n        model = StreamPeftGenerationMixin.from_pretrained(\n                model, LORA_WEIGHTS, torch_dtype=torch.float16, load_in_8bit=LOAD_8BIT,  device_map={\"\": 0}\n        )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n    )", "# fix tokenizer bug\nif args.fix_token and tokenizer.eos_token_id != 2:\n    warnings.warn(\n        \"The tokenizer eos token may be wrong. please check you llama-checkpoint\"\n    )\n    model.config.bos_token_id = tokenizer.bos_token_id = 1\n    model.config.eos_token_id = tokenizer.eos_token_id = 2\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # same as unk token id\nif not LOAD_8BIT:\n    model.half()  # seems to fix bugs for some users.", "if not LOAD_8BIT:\n    model.half()  # seems to fix bugs for some users.\n\nmodel.eval()\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\ndef save(\n    inputs,\n    history,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    max_memory=1024,\n    do_sample=False,\n    prompt_type='0',\n    **kwargs, \n):\n    history = [] if history is None else history\n    data_point = {}\n    if prompt_type == 'instruct':\n        PROMPT = prompt.instruct_prompt(tokenizer,max_memory)\n    elif prompt_type == 'chat':\n        PROMPT = prompt.chat_prompt(tokenizer,max_memory)\n    else:\n        raise Exception('not support')\n    data_point['history'] = history\n    # \u5b9e\u9645\u4e0a\u662f\u6bcf\u4e00\u6b65\u90fd\u53ef\u4ee5\u4e0d\u4e00\u6837\uff0c\u8fd9\u91cc\u53ea\u4fdd\u5b58\u6700\u540e\u4e00\u6b65\n    data_point['generation_parameter'] = {\n        \"temperature\":temperature,\n        \"top_p\":top_p,\n        \"top_k\":top_k,\n        \"num_beams\":num_beams,\n        \"bos_token_id\":tokenizer.bos_token_id,\n        \"eos_token_id\":tokenizer.eos_token_id,\n        \"pad_token_id\":tokenizer.pad_token_id,\n        \"max_new_tokens\":max_new_tokens,\n        \"min_new_tokens\":min_new_tokens, \n        \"do_sample\":do_sample,\n        \"repetition_penalty\":repetition_penalty,\n        \"max_memory\":max_memory,\n    }\n    data_point['info'] = args.__dict__\n    print(data_point)\n    if args.int8:\n        file_name = f\"{args.lora_path}/{args.prompt_type.replace(' ','_')}_int8.jsonl\"\n    else:\n        file_name = f\"{args.lora_path}/{args.prompt_type.replace(' ','_')}_fp16.jsonl\"\n    utils.to_jsonl([data_point], file_name)", "\ndef evaluate(\n    inputs,\n    history,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    max_memory=1024,\n    do_sample=False,\n    prompt_type='0',\n    **kwargs,\n):\n    history = [] if history is None else history\n    data_point = {}\n    if prompt_type == 'instruct':\n        PROMPT = prompt.instruct_prompt(tokenizer,max_memory)\n    elif prompt_type == 'chat':\n        PROMPT = prompt.chat_prompt(tokenizer,max_memory)\n    else:\n        raise Exception('not support')\n    \n    data_point['history'] = copy.deepcopy(history)\n    data_point['input'] = inputs\n\n    input_ids = PROMPT.preprocess_gen(data_point)\n    \n    printf('------------------------------')\n    printf(tokenizer.decode(input_ids))\n    input_ids = torch.tensor([input_ids]).to(device) # batch=1\n\n    printf('------------------------------')\n    printf('shape',input_ids.size())\n    printf('------------------------------')\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n        do_sample=do_sample,\n        bad_words_ids=tokenizer(['\\n\\nUser:','\\n\\nAssistant:'], add_special_tokens=False).input_ids,\n\n        **kwargs,\n    )\n    \n    return_text = [(item['input'], item['output']) for item in history]\n    out_memory =False\n    outputs = None\n    with torch.no_grad():\n        # \u6d41\u5f0f\u8f93\u51fa / \u6253\u5b57\u673a\u6548\u679c\n        # streamly output / typewriter style\n        if args.use_typewriter:\n            try:\n                for generation_output in model.stream_generate(\n                    input_ids=input_ids,\n                    generation_config=generation_config,\n                    return_dict_in_generate=True,\n                    output_scores=False,\n                    repetition_penalty=float(repetition_penalty),\n                ):\n                    gen_token = generation_output[0][-1].item()\n                    printf(gen_token, end='(')\n                    printf(tokenizer.decode(gen_token), end=') ')\n                    \n                    outputs = tokenizer.batch_decode(generation_output)\n                    if args.show_beam:\n                        show_text = \"\\n--------------------------------------------\\n\".join(\n                            [ PROMPT.postprocess(output)+\" \u258c\" for output in outputs]\n                        )\n                    else:\n                        show_text = PROMPT.postprocess(outputs[0])+\" \u258c\"\n                    yield return_text +[(inputs, show_text)], history\n            except torch.cuda.OutOfMemoryError:\n                print('CUDA out of memory')\n                import gc\n                gc.collect()\n                torch.cuda.empty_cache()\n                out_memory=True\n            # finally only one\n            printf('[EOS]', end='\\n')\n            show_text = PROMPT.postprocess(outputs[0] if outputs is not None else '### Response:')\n            return_len = len(show_text)\n            if out_memory==True:\n                out_memory=False\n                show_text+= '<p style=\"color:#FF0000\"> [GPU Out Of Memory] </p> '\n            if return_len > 0:\n                output = PROMPT.postprocess(outputs[0], render=False)\n                history.append({\n                    'input': inputs,\n                    'output': output,\n                })\n\n            return_text += [(inputs, show_text)]\n            yield return_text, history\n        # common \n        else:\n            try:\n                generation_output = model.generate(\n                    input_ids=input_ids,\n                    generation_config=generation_config,\n                    return_dict_in_generate=True,\n                    output_scores=True,\n                    max_new_tokens=max_new_tokens,\n                    repetition_penalty=float(repetition_penalty),\n                )\n                s = generation_output.sequences[0]\n                output = tokenizer.decode(s)\n                output = PROMPT.postprocess(output)\n                history.append({\n                    'input': inputs,\n                    'output': output,\n                })\n                return_text += [(inputs, output)]\n                yield return_text, history\n            except torch.cuda.OutOfMemoryError:\n                import gc\n                gc.collect()\n                torch.cuda.empty_cache()\n                show_text = '<p style=\"color:#FF0000\"> [GPU Out Of Memory] </p> '\n                printf(show_text)\n                return_text += [(inputs, show_text)]\n                yield return_text, history", "\ndef clear():\n    import gc\n    gc.collect()\n    torch.cuda.empty_cache()\n    return None, None\n\n\n# gr.Interface\u5bf9chatbot\u7684clear\u6709bug\uff0c\u56e0\u6b64\u6211\u4eec\u91cd\u65b0\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8egr.block\u7684UI\u903b\u8f91\n# gr.Interface has bugs to clear chatbot's history,so we customly implement it based on gr.block\nwith gr.Blocks() as demo:\n    fn = evaluate\n    title = gr.Markdown(\n        \"<h1 style='text-align: center; margin-bottom: 1rem'>\"\n        + \"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\"\n        + \"</h1>\"\n    )\n    description = gr.Markdown(\n        \"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\"\n    )\n    history = gr.components.State()\n    with gr.Row().style(equal_height=False):\n        with gr.Column(variant=\"panel\"):\n            input_component_column = gr.Column()\n            with input_component_column:\n                input = gr.components.Textbox(\n                    lines=2, label=\"Input\", placeholder=\"\u8bf7\u8f93\u5165\u95ee\u9898.\"\n                )\n                temperature = gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\")\n                topp = gr.components.Slider(minimum=0, maximum=1, value=0.9, label=\"Top p\")\n                topk = gr.components.Slider(minimum=0, maximum=100, step=1, value=60, label=\"Top k\")\n                beam_number = gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\")\n                max_new_token = gr.components.Slider(\n                    minimum=1, maximum=2048, step=1, value=256, label=\"Max New Tokens\"\n                )\n                min_new_token = gr.components.Slider(\n                    minimum=1, maximum=1024, step=1, value=5, label=\"Min New Tokens\"\n                )\n                repeat_penal = gr.components.Slider(\n                    minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n                )\n                max_memory = gr.components.Slider(\n                    minimum=0, maximum=2048, step=1, value=2048, label=\"Max Memory\"\n                )\n                do_sample = gr.components.Checkbox(label=\"Use sample\")\n                # must be str, not number !\n                type_of_prompt = gr.components.Dropdown(\n                    ['instruct', 'chat'], value=args.prompt_type, label=\"Prompt Type\", info=\"select the specific prompt; use after clear history\"\n                )\n                input_components = [\n                    input, history, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt\n                ]\n                input_components_except_states = [input, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt]\n            with gr.Row():\n                cancel_btn = gr.Button('Cancel')\n                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n                stop_btn = gr.Button(\"Stop\", variant=\"stop\", visible=False)\n            with gr.Row():\n                reset_btn = gr.Button(\"Reset Parameter\")\n                clear_history = gr.Button(\"Clear History\")\n\n\n        with gr.Column(variant=\"panel\"):\n            chatbot = gr.Chatbot().style(height=1024)\n            output_components = [ chatbot, history ]  \n            with gr.Row():\n                save_btn = gr.Button(\"Save Chat\")\n        def wrapper(*args):\n            # here to support the change between the stop and submit button\n            try:\n                for output in fn(*args):\n                    output = [o for o in output]\n                    # output for output_components, the rest for [button, button]\n                    yield output + [\n                        gr.Button.update(visible=False),\n                        gr.Button.update(visible=True),\n                    ]\n            finally:\n                yield [{'__type__': 'generic_update'}, {'__type__': 'generic_update'}] + [ gr.Button.update(visible=True), gr.Button.update(visible=False)]\n\n        def cancel(history, chatbot):\n            if history == []:\n                return (None, None)\n            return history[:-1], chatbot[:-1]\n\n        extra_output = [submit_btn, stop_btn]\n        save_btn.click(\n            save, \n            input_components, \n            None, \n        )\n        pred = submit_btn.click(\n            wrapper, \n            input_components, \n            output_components + extra_output, \n            api_name=\"predict\",\n            scroll_to_output=True,\n            preprocess=True,\n            postprocess=True,\n            batch=False,\n            max_batch_size=4,\n        )\n        submit_btn.click(\n            lambda: (\n                submit_btn.update(visible=False),\n                stop_btn.update(visible=True),\n            ),\n            inputs=None,\n            outputs=[submit_btn, stop_btn],\n            queue=False,\n        )\n        stop_btn.click(\n            lambda: (\n                submit_btn.update(visible=True),\n                stop_btn.update(visible=False),\n            ),\n            inputs=None,\n            outputs=[submit_btn, stop_btn],\n            cancels=[pred],\n            queue=False,\n        )\n        cancel_btn.click(\n            cancel,\n            inputs=[history, chatbot],\n            outputs=[history, chatbot]\n        )\n        reset_btn.click(\n            None, \n            [],\n            (\n                # input_components ; don't work for history...\n                input_components_except_states\n                + [input_component_column]\n            ),  # type: ignore\n            _js=f\"\"\"() => {json.dumps([\n                getattr(component, \"cleared_value\", None) for component in input_components_except_states ] \n                + ([gr.Column.update(visible=True)])\n                + ([])\n            )}\n            \"\"\",\n        )\n        clear_history.click(clear, None, [history, chatbot], queue=False)", "# gr.Interface\u5bf9chatbot\u7684clear\u6709bug\uff0c\u56e0\u6b64\u6211\u4eec\u91cd\u65b0\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8egr.block\u7684UI\u903b\u8f91\n# gr.Interface has bugs to clear chatbot's history,so we customly implement it based on gr.block\nwith gr.Blocks() as demo:\n    fn = evaluate\n    title = gr.Markdown(\n        \"<h1 style='text-align: center; margin-bottom: 1rem'>\"\n        + \"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\"\n        + \"</h1>\"\n    )\n    description = gr.Markdown(\n        \"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\"\n    )\n    history = gr.components.State()\n    with gr.Row().style(equal_height=False):\n        with gr.Column(variant=\"panel\"):\n            input_component_column = gr.Column()\n            with input_component_column:\n                input = gr.components.Textbox(\n                    lines=2, label=\"Input\", placeholder=\"\u8bf7\u8f93\u5165\u95ee\u9898.\"\n                )\n                temperature = gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\")\n                topp = gr.components.Slider(minimum=0, maximum=1, value=0.9, label=\"Top p\")\n                topk = gr.components.Slider(minimum=0, maximum=100, step=1, value=60, label=\"Top k\")\n                beam_number = gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\")\n                max_new_token = gr.components.Slider(\n                    minimum=1, maximum=2048, step=1, value=256, label=\"Max New Tokens\"\n                )\n                min_new_token = gr.components.Slider(\n                    minimum=1, maximum=1024, step=1, value=5, label=\"Min New Tokens\"\n                )\n                repeat_penal = gr.components.Slider(\n                    minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n                )\n                max_memory = gr.components.Slider(\n                    minimum=0, maximum=2048, step=1, value=2048, label=\"Max Memory\"\n                )\n                do_sample = gr.components.Checkbox(label=\"Use sample\")\n                # must be str, not number !\n                type_of_prompt = gr.components.Dropdown(\n                    ['instruct', 'chat'], value=args.prompt_type, label=\"Prompt Type\", info=\"select the specific prompt; use after clear history\"\n                )\n                input_components = [\n                    input, history, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt\n                ]\n                input_components_except_states = [input, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt]\n            with gr.Row():\n                cancel_btn = gr.Button('Cancel')\n                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n                stop_btn = gr.Button(\"Stop\", variant=\"stop\", visible=False)\n            with gr.Row():\n                reset_btn = gr.Button(\"Reset Parameter\")\n                clear_history = gr.Button(\"Clear History\")\n\n\n        with gr.Column(variant=\"panel\"):\n            chatbot = gr.Chatbot().style(height=1024)\n            output_components = [ chatbot, history ]  \n            with gr.Row():\n                save_btn = gr.Button(\"Save Chat\")\n        def wrapper(*args):\n            # here to support the change between the stop and submit button\n            try:\n                for output in fn(*args):\n                    output = [o for o in output]\n                    # output for output_components, the rest for [button, button]\n                    yield output + [\n                        gr.Button.update(visible=False),\n                        gr.Button.update(visible=True),\n                    ]\n            finally:\n                yield [{'__type__': 'generic_update'}, {'__type__': 'generic_update'}] + [ gr.Button.update(visible=True), gr.Button.update(visible=False)]\n\n        def cancel(history, chatbot):\n            if history == []:\n                return (None, None)\n            return history[:-1], chatbot[:-1]\n\n        extra_output = [submit_btn, stop_btn]\n        save_btn.click(\n            save, \n            input_components, \n            None, \n        )\n        pred = submit_btn.click(\n            wrapper, \n            input_components, \n            output_components + extra_output, \n            api_name=\"predict\",\n            scroll_to_output=True,\n            preprocess=True,\n            postprocess=True,\n            batch=False,\n            max_batch_size=4,\n        )\n        submit_btn.click(\n            lambda: (\n                submit_btn.update(visible=False),\n                stop_btn.update(visible=True),\n            ),\n            inputs=None,\n            outputs=[submit_btn, stop_btn],\n            queue=False,\n        )\n        stop_btn.click(\n            lambda: (\n                submit_btn.update(visible=True),\n                stop_btn.update(visible=False),\n            ),\n            inputs=None,\n            outputs=[submit_btn, stop_btn],\n            cancels=[pred],\n            queue=False,\n        )\n        cancel_btn.click(\n            cancel,\n            inputs=[history, chatbot],\n            outputs=[history, chatbot]\n        )\n        reset_btn.click(\n            None, \n            [],\n            (\n                # input_components ; don't work for history...\n                input_components_except_states\n                + [input_component_column]\n            ),  # type: ignore\n            _js=f\"\"\"() => {json.dumps([\n                getattr(component, \"cleared_value\", None) for component in input_components_except_states ] \n                + ([gr.Column.update(visible=True)])\n                + ([])\n            )}\n            \"\"\",\n        )\n        clear_history.click(clear, None, [history, chatbot], queue=False)", "\ndemo.queue().launch(share=args.share_link)"]}
{"filename": "finetune_4bit.py", "chunked_list": ["import os\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom datasets import load_dataset, Dataset\nimport transformers\nimport argparse\nimport warnings", "import argparse\nimport warnings\nfrom huggingface_hub import snapshot_download\n\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\n\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig", "\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\nfrom peft import (\n    prepare_model_for_kbit_training,\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n)\n\ndef generate_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if data_point[\"input\"]:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"", ")\n\ndef generate_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if data_point[\"input\"]:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"", "\ndef tokenize(prompt):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )\n    return {\n        \"input_ids\": result[\"input_ids\"][:-1],\n        \"attention_mask\": result[\"attention_mask\"][:-1],\n    }", "\n\ndef generate_and_tokenize_prompt(data_point):\n    # This function masks out the labels for the input,\n    # so that our loss is computed only on the response.\n    user_prompt = (\n        (\n            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n\"\"\"\n        )\n        if data_point[\"input\"]\n        else (\n            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n\"\"\"\n        )\n    )\n    len_user_prompt_tokens = (\n        len(\n            tokenizer(\n                user_prompt,\n                truncation=True,\n                max_length=CUTOFF_LEN + 1,\n            )[\"input_ids\"]\n        )\n        - 1\n    )  # no eos token\n    full_tokens = tokenizer(\n        user_prompt + data_point[\"output\"],\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )[\"input_ids\"][:-1]\n    return {\n        \"input_ids\": full_tokens,\n        \"labels\": [-100] * len_user_prompt_tokens\n        + full_tokens[len_user_prompt_tokens:],\n        \"attention_mask\": [1] * (len(full_tokens)),\n    }", "    \nparser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\nparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\nparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\nparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\nparser.add_argument(\"--eval_steps\", type=int, default=200)\nparser.add_argument(\"--save_steps\", type=int, default=200)\nparser.add_argument(\"--test_size\", type=int, default=200)\nparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)", "parser.add_argument(\"--test_size\", type=int, default=200)\nparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\nparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\nparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\nargs = parser.parse_args()\n\nif not args.wandb:\n    os.environ[\"WANDB_MODE\"] = \"disable\"\n# optimized for RTX 4090. for larger GPUs, increase some of these?\nMICRO_BATCH_SIZE = 8  # this could actually be 5 but i like powers of 2", "# optimized for RTX 4090. for larger GPUs, increase some of these?\nMICRO_BATCH_SIZE = 8  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = 128\nMAX_STEPS = None\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 3  # we don't always need 3 tbh\nLEARNING_RATE = 3e-4  # the Karpathy constant\nCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\nLORA_R = 8\nLORA_ALPHA = 16", "LORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nVAL_SET_SIZE = args.test_size #2000\nTARGET_MODULES = [\n    \"q_proj\",\n    \"v_proj\",\n]\nDATA_PATH = args.data_path \nOUTPUT_DIR = args.output_path #\"lora-Vicuna\"", "DATA_PATH = args.data_path \nOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\ndevice_map = {\"\": 0} #\"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\nprint(args.model_path)", "print(args.model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_4bit=True,\n    device_map=device_map,\n)\ntokenizer = LlamaTokenizer.from_pretrained(\n    args.model_path, add_eos_token=True\n)\n", ")\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,", "    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n#tokenizer.padding_side = \"left\"  # Allow batched inference\n\ndata = load_dataset(\"json\", data_files=DATA_PATH)", "\ndata = load_dataset(\"json\", data_files=DATA_PATH)\nimport random;start = random.randint(1, 100)\nexamples = Dataset.from_dict(data['train'][start:start+5]).map(generate_and_tokenize_prompt)\nfor example in examples:\n    print(f'>>> prompt example:\\n { tokenizer.decode(example[\"input_ids\"]) }')\n    print(f'>>> tokenizer labels: { tokenizer.decode([ 0 if l==-100 else l for l in example[\"labels\"]])}')\n    print(f'>>> tokenizer example: { example[\"input_ids\"][:250] }...{ example[\"input_ids\"][-10:]}')\n\nnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "\nnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\nif args.resume_from_checkpoint:\n    if args.lora_remote_checkpoint is not None:\n        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n    # Check the available weights and load them\n    checkpoint_name = os.path.join(\n        args.resume_from_checkpoint, \"pytorch_model.bin\"\n    )  # Full checkpoint\n    if not os.path.exists(checkpoint_name):\n        pytorch_bin_path = checkpoint_name\n        checkpoint_name = os.path.join(\n            args.resume_from_checkpoint, \"adapter_model.bin\"\n        )  # only LoRA model - LoRA config above has to fit\n        if os.path.exists(checkpoint_name):\n            os.rename(checkpoint_name, pytorch_bin_path)\n            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n        else:\n            args.resume_from_checkpoint = (\n                None  # So the trainer won't try loading its state\n            )\n    # The two files above have a different name depending on how they were saved, but are actually the same.\n    if os.path.exists(checkpoint_name):\n        print(f\"Restarting from {checkpoint_name}\")\n        adapters_weights = torch.load(checkpoint_name)\n        model = set_peft_model_state_dict(model, adapters_weights)\n    else:\n        print(f\"Checkpoint {checkpoint_name} not found\")\n    \n    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n    \n    if os.path.exists(train_args_path):\n        import json\n        base_train_args = json.load(open(train_args_path, 'r'))\n        base_max_steps = base_train_args[\"max_steps\"]\n        resume_scale = base_max_steps / now_max_steps\n        if base_max_steps > now_max_steps:\n            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n            EPOCHS = None\n            MAX_STEPS = base_max_steps\n        else:\n            MAX_STEPS = now_max_steps\nelse:\n    MAX_STEPS = now_max_steps", "\n\nmodel.print_trainable_parameters()\n\n\nnum_proc = (os.cpu_count())\nif VAL_SET_SIZE > 0:\n    train_val = data[\"train\"].train_test_split(\n        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n    )\n    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\n    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\nelse:\n    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\n    val_data = None", "\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=100,\n        num_train_epochs=EPOCHS,", "        warmup_steps=100,\n        num_train_epochs=EPOCHS,\n        max_steps=MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=20,\n        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,", "        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,\n        save_total_limit=30,\n        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n        ddp_find_unused_parameters=False if ddp else None,\n        report_to=\"wandb\" if args.wandb else [],\n        ignore_data_skip=args.ignore_data_skip,\n        optim=\"paged_adamw_8bit\",\n    ),", "        optim=\"paged_adamw_8bit\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False\n\nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))", "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\nprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\ntrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n", "trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\nmodel.save_pretrained(OUTPUT_DIR)\n\n"]}
{"filename": "generate_4bit.py", "chunked_list": ["import sys\nimport torch\nfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\nimport transformers\nimport gradio as gr\nimport argparse\nimport warnings\nimport os\nfrom utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\nassert (", "from utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"/model/13B_hf\")\nparser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\nparser.add_argument(\"--use_typewriter\", type=int, default=1)", "parser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\nparser.add_argument(\"--use_typewriter\", type=int, default=1)\nparser.add_argument(\"--use_local\", type=int, default=1)\nargs = parser.parse_args()\nprint(args)\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\nLOAD_8BIT = True\nBASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path", "BASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path\n\n\n# fix the path for local checkpoint\nlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\nprint(lora_bin_path)\nif not os.path.exists(lora_bin_path) and args.use_local:\n    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n    print(pytorch_bin_path)\n    if os.path.exists(pytorch_bin_path):\n        os.rename(pytorch_bin_path, lora_bin_path)\n        warnings.warn(\n            \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n        )\n    else:\n        assert ('Checkpoint is not Found!')", "\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:\n    pass", "\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nif device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        device_map=\"auto\", #{\"\": 0},\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map=\"auto\", #{\"\": 0}\n    )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n    )", "if device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        device_map=\"auto\", #{\"\": 0},\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map=\"auto\", #{\"\": 0}\n    )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n    )", "\nmodel.config.bos_token_id = tokenizer.bos_token_id = 1\nmodel.config.eos_token_id = tokenizer.eos_token_id = 2\n\ndef generate_prompt(instruction, input=None):\n    if input:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\"\"\"", "\n\nif not LOAD_8BIT:\n    model.half()  # seems to fix bugs for some users.\n\nmodel.eval()\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\n\ndef evaluate(\n    input,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    **kwargs,\n):\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        bos_token_id=1,\n        eos_token_id=2,\n        pad_token_id=0,\n        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n        **kwargs,\n    )\n    with torch.no_grad():\n        if args.use_typewriter:\n            for generation_output in model.stream_generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=float(repetition_penalty),\n            ):\n                outputs = tokenizer.batch_decode(generation_output)\n                show_text = \"\\n--------------------------------------------\\n\".join(\n                    [output.split(\"### Response:\")[1].strip().replace('\ufffd','')+\" \u258c\" for output in outputs]\n                )\n                # if show_text== '':\n                #     yield last_show_text\n                # else:\n                yield show_text\n            yield outputs[0].split(\"### Response:\")[1].strip().replace('\ufffd','')\n        else:\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=1.3,\n            )\n            output = generation_output.sequences[0]\n            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n            print(output)\n            yield output", "\n\ndef evaluate(\n    input,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    **kwargs,\n):\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        bos_token_id=1,\n        eos_token_id=2,\n        pad_token_id=0,\n        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n        **kwargs,\n    )\n    with torch.no_grad():\n        if args.use_typewriter:\n            for generation_output in model.stream_generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=float(repetition_penalty),\n            ):\n                outputs = tokenizer.batch_decode(generation_output)\n                show_text = \"\\n--------------------------------------------\\n\".join(\n                    [output.split(\"### Response:\")[1].strip().replace('\ufffd','')+\" \u258c\" for output in outputs]\n                )\n                # if show_text== '':\n                #     yield last_show_text\n                # else:\n                yield show_text\n            yield outputs[0].split(\"### Response:\")[1].strip().replace('\ufffd','')\n        else:\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=1.3,\n            )\n            output = generation_output.sequences[0]\n            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n            print(output)\n            yield output", "\n\ngr.Interface(\n    fn=evaluate,\n    inputs=[\n        gr.components.Textbox(\n            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n        ),\n        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),", "        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n        gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\"),\n        gr.components.Slider(\n            minimum=1, maximum=2000, step=1, value=256, label=\"Max New Tokens\"\n        ),\n        gr.components.Slider(\n            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n        ),", "            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n        ),\n        gr.components.Slider(\n            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n        ),\n    ],\n    outputs=[\n        gr.inputs.Textbox(\n            lines=25,\n            label=\"Output\",", "            lines=25,\n            label=\"Output\",\n        )\n    ],\n    title=\"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\",\n    description=\"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\",\n).queue().launch(share=True)\n"]}
{"filename": "generate.py", "chunked_list": ["import sys\nimport torch\nfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\nimport transformers\nimport gradio as gr\nimport argparse\nimport warnings\nimport os\nfrom utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\nassert (", "from utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, default=\"/model/13B_hf\")\nparser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\nparser.add_argument(\"--use_typewriter\", type=int, default=1)", "parser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\nparser.add_argument(\"--use_typewriter\", type=int, default=1)\nparser.add_argument(\"--use_local\", type=int, default=1)\nargs = parser.parse_args()\nprint(args)\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\nLOAD_8BIT = True\nBASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path", "BASE_MODEL = args.model_path\nLORA_WEIGHTS = args.lora_path\n\n\n# fix the path for local checkpoint\nlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\nprint(lora_bin_path)\nif not os.path.exists(lora_bin_path) and args.use_local:\n    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n    print(pytorch_bin_path)\n    if os.path.exists(pytorch_bin_path):\n        os.rename(pytorch_bin_path, lora_bin_path)\n        warnings.warn(\n            \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n        )\n    else:\n        assert ('Checkpoint is not Found!')", "\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\ntry:\n    if torch.backends.mps.is_available():\n        device = \"mps\"\nexcept:\n    pass", "\nif device == \"cuda\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        load_in_8bit=LOAD_8BIT,\n        torch_dtype=torch.float16,\n        device_map=\"auto\", #device_map={\"\": 0},\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map=\"auto\", #device_map={\"\": 0}\n    )\nelif device == \"mps\":\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n        torch_dtype=torch.float16,\n    )\nelse:\n    model = LlamaForCausalLM.from_pretrained(\n        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n    )\n    model = StreamPeftGenerationMixin.from_pretrained(\n        model,\n        LORA_WEIGHTS,\n        device_map={\"\": device},\n    )", "\n\ndef generate_prompt(instruction, input=None):\n    if input:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\"\"\"", "\n\nif not LOAD_8BIT:\n    model.half()  # seems to fix bugs for some users.\n\nmodel.eval()\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\n\ndef evaluate(\n    input,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    **kwargs,\n):\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        bos_token_id=1,\n        eos_token_id=2,\n        pad_token_id=0,\n        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n        **kwargs,\n    )\n    with torch.no_grad():\n        if args.use_typewriter:\n            for generation_output in model.stream_generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=float(repetition_penalty),\n            ):\n                outputs = tokenizer.batch_decode(generation_output)\n                show_text = \"\\n--------------------------------------------\\n\".join(\n                    [output.split(\"### Response:\")[1].strip().replace('\ufffd','')+\" \u258c\" for output in outputs]\n                )\n                # if show_text== '':\n                #     yield last_show_text\n                # else:\n                yield show_text\n            yield outputs[0].split(\"### Response:\")[1].strip().replace('\ufffd','')\n        else:\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=1.3,\n            )\n            output = generation_output.sequences[0]\n            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n            print(output)\n            yield output", "\n\ndef evaluate(\n    input,\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4,\n    max_new_tokens=128,\n    min_new_tokens=1,\n    repetition_penalty=2.0,\n    **kwargs,\n):\n    prompt = generate_prompt(input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(device)\n    generation_config = GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n        bos_token_id=1,\n        eos_token_id=2,\n        pad_token_id=0,\n        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n        **kwargs,\n    )\n    with torch.no_grad():\n        if args.use_typewriter:\n            for generation_output in model.stream_generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=float(repetition_penalty),\n            ):\n                outputs = tokenizer.batch_decode(generation_output)\n                show_text = \"\\n--------------------------------------------\\n\".join(\n                    [output.split(\"### Response:\")[1].strip().replace('\ufffd','')+\" \u258c\" for output in outputs]\n                )\n                # if show_text== '':\n                #     yield last_show_text\n                # else:\n                yield show_text\n            yield outputs[0].split(\"### Response:\")[1].strip().replace('\ufffd','')\n        else:\n            generation_output = model.generate(\n                input_ids=input_ids,\n                generation_config=generation_config,\n                return_dict_in_generate=True,\n                output_scores=False,\n                repetition_penalty=1.3,\n            )\n            output = generation_output.sequences[0]\n            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n            print(output)\n            yield output", "\n\ngr.Interface(\n    fn=evaluate,\n    inputs=[\n        gr.components.Textbox(\n            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n        ),\n        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),", "        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n        gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\"),\n        gr.components.Slider(\n            minimum=1, maximum=2000, step=1, value=256, label=\"Max New Tokens\"\n        ),\n        gr.components.Slider(\n            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n        ),", "            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n        ),\n        gr.components.Slider(\n            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n        ),\n    ],\n    outputs=[\n        gr.inputs.Textbox(\n            lines=25,\n            label=\"Output\",", "            lines=25,\n            label=\"Output\",\n        )\n    ],\n    title=\"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\",\n    description=\"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\",\n).queue().launch(share=True)\n"]}
{"filename": "tools/quant_llama.py", "chunked_list": ["import argparse\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport quant\n\nfrom gptq import GPTQ\nfrom datautils import get_loaders\n\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n    return res", "from datautils import get_loaders\n\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n    return res\n\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16)\n    model.seqlen = 2048\n    return model", "\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16)\n    model.seqlen = 2048\n    return model", "\n\n@torch.no_grad()\ndef llama_sequential(model, dataloader, dev):\n    print('Starting ...')\n\n    use_cache = model.config.use_cache\n    model.config.use_cache = False\n    layers = model.model.layers\n\n    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n    model.model.norm = model.model.norm.to(dev)\n    layers[0] = layers[0].to(dev)\n\n    dtype = next(iter(model.parameters())).dtype\n    inps = torch.zeros((args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev)\n    cache = {'i': 0, 'attention_mask': None}\n\n    class Catcher(nn.Module):\n\n        def __init__(self, module):\n            super().__init__()\n            self.module = module\n\n        def forward(self, inp, **kwargs):\n            inps[cache['i']] = inp\n            cache['i'] += 1\n            cache['attention_mask'] = kwargs['attention_mask']\n            cache['position_ids'] = kwargs['position_ids']\n            raise ValueError\n\n    layers[0] = Catcher(layers[0])\n    for batch in dataloader:\n        try:\n            model(batch[0].to(dev))\n        except ValueError:\n            pass\n    layers[0] = layers[0].module\n\n    layers[0] = layers[0].cpu()\n    model.model.embed_tokens = model.model.embed_tokens.cpu()\n    model.model.norm = model.model.norm.cpu()\n    torch.cuda.empty_cache()\n\n    outs = torch.zeros_like(inps)\n    attention_mask = cache['attention_mask']\n    position_ids = cache['position_ids']\n\n    print('Ready.')\n\n    quantizers = {}\n    for i in range(len(layers)):\n\n        print(f'Quantizing layer {i+1}/{len(layers)}..')\n        print('+------------------+--------------+------------+-----------+-------+')\n        print('|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |')\n        print('+==================+==============+============+===========+=======+')\n\n        layer = layers[i].to(dev)\n        full = find_layers(layer)\n        if args.true_sequential:\n            sequential = [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]\n        else:\n            sequential = [list(full.keys())]\n\n        for names in sequential:\n            subset = {n: full[n] for n in names}\n            gptq = {}\n            for name in subset:\n                gptq[name] = GPTQ(subset[name])\n                gptq[name].quantizer.configure(args.wbits, perchannel=True, mse=False)\n\n            def add_batch(name):\n\n                def tmp(_, inp, out):\n                    gptq[name].add_batch(inp[0].data, out.data)\n\n                return tmp\n\n            handles = []\n            for name in subset:\n                handles.append(subset[name].register_forward_hook(add_batch(name)))\n            for j in range(args.nsamples):\n                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n            for h in handles:\n                h.remove()\n\n            for name in subset:\n                scale, zero, g_idx, error = gptq[name].fasterquant(percdamp=args.percdamp, groupsize=args.groupsize, actorder=args.act_order, name=name)\n                quantizers['model.layers.%d.%s' % (i, name)] = (gptq[name].quantizer.cpu(), scale.cpu(), zero.cpu(), g_idx.cpu(), args.wbits, args.groupsize)\n                gptq[name].free()\n\n        for j in range(args.nsamples):\n            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n\n        layers[i] = layer.cpu()\n        del layer\n        del gptq\n        torch.cuda.empty_cache()\n\n        inps, outs = outs, inps\n        print('+------------------+--------------+------------+-----------+-------+')\n        print('\\n')\n\n    model.config.use_cache = use_cache\n\n    return quantizers", "\n\n@torch.no_grad()\ndef llama_eval(model, testenc, dev):\n    print('Evaluating ...')\n\n    testenc = testenc.input_ids\n    nsamples = testenc.numel() // model.seqlen\n\n    use_cache = model.config.use_cache\n    model.config.use_cache = False\n    layers = model.model.layers\n\n    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n    layers[0] = layers[0].to(dev)\n\n    dtype = next(iter(model.parameters())).dtype\n    inps = torch.zeros((nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev)\n    cache = {'i': 0, 'attention_mask': None}\n\n    class Catcher(nn.Module):\n\n        def __init__(self, module):\n            super().__init__()\n            self.module = module\n\n        def forward(self, inp, **kwargs):\n            inps[cache['i']] = inp\n            cache['i'] += 1\n            cache['attention_mask'] = kwargs['attention_mask']\n            cache['position_ids'] = kwargs['position_ids']\n            raise ValueError\n\n    layers[0] = Catcher(layers[0])\n    for i in range(nsamples):\n        batch = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)].to(dev)\n        try:\n            model(batch)\n        except ValueError:\n            pass\n    layers[0] = layers[0].module\n\n    layers[0] = layers[0].cpu()\n    model.model.embed_tokens = model.model.embed_tokens.cpu()\n    torch.cuda.empty_cache()\n\n    outs = torch.zeros_like(inps)\n    attention_mask = cache['attention_mask']\n    position_ids = cache['position_ids']\n\n    for i in range(len(layers)):\n        print(i)\n        layer = layers[i].to(dev)\n\n        if args.nearest:\n            subset = find_layers(layer)\n            for name in subset:\n                quantizer = quant.Quantizer()\n                quantizer.configure(args.wbits, perchannel=True, sym=args.sym, mse=False)\n                W = subset[name].weight.data\n                quantizer.find_params(W, weight=True)\n                subset[name].weight.data = quantizer.quantize(W).to(next(iter(layer.parameters())).dtype)\n\n        for j in range(nsamples):\n            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n        layers[i] = layer.cpu()\n        del layer\n        torch.cuda.empty_cache()\n        inps, outs = outs, inps\n\n    if model.model.norm is not None:\n        model.model.norm = model.model.norm.to(dev)\n    model.lm_head = model.lm_head.to(dev)\n\n    testenc = testenc.to(dev)\n    nlls = []\n    for i in range(nsamples):\n        hidden_states = inps[i].unsqueeze(0)\n        if model.model.norm is not None:\n            hidden_states = model.model.norm(hidden_states)\n        lm_logits = model.lm_head(hidden_states)\n        shift_logits = lm_logits[:, :-1, :].contiguous()\n        shift_labels = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * model.seqlen\n        nlls.append(neg_log_likelihood)\n    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n    print(ppl.item())\n\n    model.config.use_cache = use_cache", "\n\n# TODO: perform packing on GPU\ndef llama_pack(model, quantizers, wbits, groupsize):\n    layers = find_layers(model)\n    layers = {n: layers[n] for n in quantizers}\n    quant.make_quant_linear(model, quantizers, wbits, groupsize)\n    qlayers = find_layers(model, [quant.QuantLinear])\n    print('Packing ...')\n    for name in qlayers:\n        print(name)\n        quantizers[name], scale, zero, g_idx, _, _ = quantizers[name]\n        qlayers[name].pack(layers[name], scale, zero, g_idx)\n    print('Done.')\n    return model", "\n\ndef load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM, modeling_utils\n    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n    torch.set_default_dtype(torch.half)\n    modeling_utils._init_weights = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model, layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint))\n    else:\n        model.load_state_dict(torch.load(checkpoint))\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model, transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n    model.seqlen = 2048\n    print('Done.')\n\n    return model", "\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('model', type=str, help='llama model to load')\n    parser.add_argument('dataset', type=str, choices=['wikitext2', 'ptb', 'c4'], help='Where to extract calibration data from.')\n    parser.add_argument('--seed', type=int, default=0, help='Seed for sampling the calibration data.')\n    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration data samples.')\n    parser.add_argument('--percdamp', type=float, default=.01, help='Percent of the average Hessian diagonal to use for dampening.')\n    parser.add_argument('--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16], help='#bits to use for quantization; use 16 for evaluating base model.')\n    parser.add_argument('--groupsize', type=int, default=-1, help='Groupsize to use for quantization; default uses full row.')\n    parser.add_argument('--eval', action='store_true', help='evaluate quantized model.')\n    parser.add_argument('--save', type=str, default='', help='Save quantized checkpoint under this name.')\n    parser.add_argument('--save_safetensors', type=str, default='', help='Save quantized `.safetensors` checkpoint under this name.')\n    parser.add_argument('--quant-directory', type=str, default=None, help='Specify the directory for export quantization parameters to toml format. `None` means no export by default.')\n    parser.add_argument('--act-order', action='store_true', help='Whether to apply the activation order GPTQ heuristic')\n    parser.add_argument('--true-sequential', action='store_true', help='Whether to run in true sequential model.')\n    args = parser.parse_args()\n\n    DEV = torch.device('cuda:0')\n\n    gpu_dist = []\n    model = get_llama(args.model)\n    model.eval()\n\n    dataloader, testloader = get_loaders(args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen)\n\n    if args.wbits < 16:\n        tick = time.time()\n        quantizers = llama_sequential(model, dataloader, DEV)\n        print(time.time() - tick)\n\n\n    if args.eval:\n        datasets = ['wikitext2', 'ptb', 'c4']\n        if args.new_eval:\n            datasets = ['wikitext2', 'ptb-new', 'c4-new']\n        for dataset in datasets:\n            dataloader, testloader = get_loaders(dataset, seed=args.seed, model=args.model, seqlen=model.seqlen)\n            print(dataset)\n            llama_eval(model, testloader, DEV)\n    \n    llama_pack(model, quantizers, args.wbits, args.groupsize)\n    torch.save(model.state_dict(), args.save)", "# bash : CUDA_VISIBLE_DEVICES=0 proxychains python quant_llama.py ../model/llama7b_hf wikitext2 --wbits 4 --groupsize 128 --save llama7b-4bit-128g.pt"]}
{"filename": "tools/reshard.py", "chunked_list": ["# ref: https://gist.github.com/benob/4850a0210b01672175942203aa36d300\nimport os\nimport json\nimport sys\nimport torch\nimport glob\n\n# python test.py 2 xx/checkpoint-1000/ckpt/ outs\n\nif len(sys.argv) != 4:\n    print('usage: %s <new-shards> <input-model-path> <output-model-path>' % sys.argv[0], file=sys.stderr)\n    sys.exit(1)", "\nif len(sys.argv) != 4:\n    print('usage: %s <new-shards> <input-model-path> <output-model-path>' % sys.argv[0], file=sys.stderr)\n    sys.exit(1)\n\nnum_shards = int(sys.argv[1])\ninput_model_dir = sys.argv[2]\noutput_model_dir = sys.argv[3]\n\nwith open(os.path.join(input_model_dir, 'params.json'), 'r') as fp:\n    params = json.loads(fp.read())", "\nwith open(os.path.join(input_model_dir, 'params.json'), 'r') as fp:\n    params = json.loads(fp.read())\n\nassert params['dim'] % num_shards == 0, \"number of shards need to divide parameter dimension %d\" % params['dim']\n\nprint('loading...')\ncheckpoints = [torch.load(path, map_location=torch.device('cpu')) for path in glob.glob(os.path.join(input_model_dir, '*.pth'))]\n\nlayer_kind = {", "\nlayer_kind = {\n    'tok_embeddings': 'ParallelEmbedding',\n    'output': 'ColumnParallelLinear',\n    'attention.wq': 'ColumnParallelLinear',\n    'attention.wk': 'ColumnParallelLinear',\n    'attention.wv': 'ColumnParallelLinear',\n    'attention.wo': 'RowParallelLinear',\n    'feed_forward.w1': 'ColumnParallelLinear',\n    'feed_forward.w2': 'RowParallelLinear',", "    'feed_forward.w1': 'ColumnParallelLinear',\n    'feed_forward.w2': 'RowParallelLinear',\n    'feed_forward.w3': 'ColumnParallelLinear',\n    'attention_norm': None,\n    'ffn_norm': None,\n    'norm': None,\n    'rope.freqs': None,\n}\n\noutput = [dict() for x in range(num_shards)]", "\noutput = [dict() for x in range(num_shards)]\n\nprint('converting...')\nfor key in checkpoints[0].keys():\n    tensors = [m[key] for m in checkpoints]\n    print(key)\n    print('  in shapes=', [p.shape for p in tensors])\n    for pattern, kind in layer_kind.items():\n        if key.replace('.weight', '').endswith(pattern):\n            print('  kind=', kind)\n            if kind == 'ColumnParallelLinear':\n                with torch.no_grad():\n                    merged = torch.cat(tensors, 0)\n                    slice_size = merged.shape[0] // num_shards\n                    for rank in range(num_shards):\n                        output[rank][key] = merged[slice_size * rank: slice_size * (rank + 1),:].clone().detach()\n            elif kind in ('ParallelEmbedding', 'RowParallelLinear'):\n                with torch.no_grad():\n                    merged = torch.cat(tensors, 1)\n                    slice_size = merged.shape[1] // num_shards\n                    for rank in range(num_shards):\n                        output[rank][key] = merged[:,slice_size * rank: slice_size * (rank + 1)].clone().detach()\n            else:\n                for rank in range(num_shards):\n                    output[rank][key] = tensors[0]\n            print('  out shapes=', [output[rank][key].shape for rank in range(num_shards)])\n            print()\n            break\n    else:\n        raise Exception('parameter name not recognized')", "\nprint('saving...')\nos.makedirs(output_model_dir, exist_ok=True)\nwith open(os.path.join(output_model_dir, 'params.json'), 'w') as fp:\n    fp.write(json.dumps(params))\n\nfor rank in range(num_shards):\n    print(' ', rank)\n    torch.save(output[rank], os.path.join(output_model_dir, 'consolidated.%02d.pth' % rank))\n", "\nprint('done.')"]}
{"filename": "tools/merge_lora.py", "chunked_list": ["# This file is adapted from: https://github.com/tloen/alpaca-lora ( for merge ) and https://gist.github.com/benob/4850a0210b01672175942203aa36d300 ( for shard )\n# It can merge the LoRA weights back into the base model for export to PyTorch state_dicts (`consolidated.0x.pth`). The number of shards is according to the user command argument. \n# They should help users who want to run inference in projects like llama.cpp or alpaca.cpp.\n\nimport os\nimport json\nimport torch\nfrom peft import PeftModel, LoraConfig\nimport argparse\nimport transformers", "import argparse\nimport transformers\n\n# args\nparser = argparse.ArgumentParser()\n# The original base model checkpoint dir\nparser.add_argument(\"--model_path\", type=str, default='decapoda-research/llama-7b-hf')\n# The finetuned lora model checkpoint dir\nparser.add_argument(\"--lora_path\",type=str, default='./lora-Vicuna/checkpoint-3000')\n# The output dir", "parser.add_argument(\"--lora_path\",type=str, default='./lora-Vicuna/checkpoint-3000')\n# The output dir\nparser.add_argument(\"--out_path\", type=str, default='./lora-Vicuna/checkpoint-3000-with-lora')\nparser.add_argument(\"--num_shards\", type=int, default=None)\nargs = parser.parse_args()\n\n# \nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"", "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\nparams = {\n    '65B':  {\"dim\": 8192, \"multiple_of\": 256, \"n_heads\": 64, \"n_layers\": 80, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n    '30B': {\"dim\": 6656, \"multiple_of\": 256, \"n_heads\": 52, \"n_layers\": 60, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n    '13B': {\"dim\": 5120, \"multiple_of\": 256, \"n_heads\": 40, \"n_layers\": 40, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n    '7B':  {\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n}", "    '7B':  {\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n}\nNUM_SHARDS = {\n    \"7B\": 1,\n    \"13B\": 2,\n    \"30B\": 4,\n    \"65B\": 8,\n}\nlayer_kind = {\n    'tok_embeddings': 'ParallelEmbedding',", "layer_kind = {\n    'tok_embeddings': 'ParallelEmbedding',\n    'output': 'ColumnParallelLinear',\n    'attention.wq': 'ColumnParallelLinear',\n    'attention.wk': 'ColumnParallelLinear',\n    'attention.wv': 'ColumnParallelLinear',\n    'attention.wo': 'RowParallelLinear',\n    'feed_forward.w1': 'ColumnParallelLinear',\n    'feed_forward.w2': 'RowParallelLinear',\n    'feed_forward.w3': 'ColumnParallelLinear',", "    'feed_forward.w2': 'RowParallelLinear',\n    'feed_forward.w3': 'ColumnParallelLinear',\n    'attention_norm': None,\n    'ffn_norm': None,\n    'norm': None,\n    'rope.freqs': None,\n}\n\nprint(f\">>> load model from {args.model_path} and lora from {args.lora_path}....\")\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)", "print(f\">>> load model from {args.model_path} and lora from {args.lora_path}....\")\ntokenizer = LlamaTokenizer.from_pretrained(args.model_path)\nbase_model = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_8bit=False,\n    torch_dtype=torch.float16,\n    device_map={\"\": \"cpu\"},\n)\nlora_model = PeftModel.from_pretrained(\n    base_model,", "lora_model = PeftModel.from_pretrained(\n    base_model,\n    args.lora_path,\n    device_map={\"\": \"cpu\"},\n    torch_dtype=torch.float16,\n)\n\n# merge weights\nfor layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True", "for layer in lora_model.base_model.model.model.layers:\n    layer.self_attn.q_proj.merge_weights = True\n    layer.self_attn.v_proj.merge_weights = True\n\nlora_model.train(False)\n\nlora_model_sd = lora_model.state_dict()\n\nn_layers = base_model.config.num_hidden_layers\nmodel_size = None\nfor size in params.keys():\n    if n_layers == params[size][\"n_layers\"]:\n        model_size = size\n        print(f\">>> automatically recognize model_size={size}\")", "n_layers = base_model.config.num_hidden_layers\nmodel_size = None\nfor size in params.keys():\n    if n_layers == params[size][\"n_layers\"]:\n        model_size = size\n        print(f\">>> automatically recognize model_size={size}\")\nif model_size is None:\n    raise Exception('cannot recognize model_size! please check if your model is llama-based model')\nn_heads = base_model.config.num_attention_heads\nassert n_heads == params[model_size][\"n_heads\"]", "n_heads = base_model.config.num_attention_heads\nassert n_heads == params[model_size][\"n_heads\"]\ndim = base_model.config.hidden_size\nassert dim == params[model_size][\"dim\"]\ndims_per_head = dim // n_heads\nbase = 10000.0\ninv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\nif args.num_shards is None:\n    num_shards = NUM_SHARDS[model_size]\nelse:\n    num_shards = args.num_shards", "print(f'>>> will split model checkpoint in {num_shards} parts')\n\ndef permute(w):\n    return (\n        w.view(n_heads, dim // n_heads // 2, 2, dim).transpose(1, 2).reshape(dim, dim)\n    )\n\n\ndef unpermute(w):\n    return (\n        w.view(n_heads, 2, dim // n_heads // 2, dim).transpose(1, 2).reshape(dim, dim)\n    )", "def unpermute(w):\n    return (\n        w.view(n_heads, 2, dim // n_heads // 2, dim).transpose(1, 2).reshape(dim, dim)\n    )\n\n\ndef translate_state_dict_key(k):\n    k = k.replace(\"base_model.model.\", \"\")\n    if k == \"model.embed_tokens.weight\":\n        return \"tok_embeddings.weight\"\n    elif k == \"model.norm.weight\":\n        return \"norm.weight\"\n    elif k == \"lm_head.weight\":\n        return \"output.weight\"\n    elif k.startswith(\"model.layers.\"):\n        layer = k.split(\".\")[2]\n        if k.endswith(\".self_attn.q_proj.weight\"):\n            return f\"layers.{layer}.attention.wq.weight\"\n        elif k.endswith(\".self_attn.k_proj.weight\"):\n            return f\"layers.{layer}.attention.wk.weight\"\n        elif k.endswith(\".self_attn.v_proj.weight\"):\n            return f\"layers.{layer}.attention.wv.weight\"\n        elif k.endswith(\".self_attn.o_proj.weight\"):\n            return f\"layers.{layer}.attention.wo.weight\"\n        elif k.endswith(\".mlp.gate_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w1.weight\"\n        elif k.endswith(\".mlp.down_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w2.weight\"\n        elif k.endswith(\".mlp.up_proj.weight\"):\n            return f\"layers.{layer}.feed_forward.w3.weight\"\n        elif k.endswith(\".input_layernorm.weight\"):\n            return f\"layers.{layer}.attention_norm.weight\"\n        elif k.endswith(\".post_attention_layernorm.weight\"):\n            return f\"layers.{layer}.ffn_norm.weight\"\n        elif k.endswith(\"rotary_emb.inv_freq\") or \"lora\" in k:\n            return None\n        else:\n            print(layer, k)\n            raise NotImplementedError\n    else:\n        print(k)\n        raise NotImplementedError", "\n\nnew_state_dict = {}\nfor k, v in lora_model_sd.items():\n    new_k = translate_state_dict_key(k)\n    if new_k is not None:\n        if \"wq\" in new_k or \"wk\" in new_k:\n            new_state_dict[new_k] = unpermute(v)\n        else:\n            new_state_dict[new_k] = v", "\nos.makedirs(args.out_path, exist_ok=True)\nif num_shards == 1:\n    torch.save(new_state_dict, f\"{args.out_path}/consolidated.00.pth\")\n    with open(f\"{args.out_path}/params.json\", \"w\") as f:\n        json.dump(params[model_size], f)\nelse:\n    output = [dict() for x in range(num_shards)]\n    print('>>> start converting to shards...')\n    # sharded the models\n    for key in new_state_dict.keys():\n        tensors = [new_state_dict[key]]\n        print(key)\n        print('  in shapes=', [p.shape for p in tensors])\n        for pattern, kind in layer_kind.items():\n            if key.replace('.weight', '').endswith(pattern):\n                print('  kind=', kind)\n                if kind == 'ColumnParallelLinear':\n                    with torch.no_grad():\n                        merged = torch.cat(tensors, 0)\n                        slice_size = merged.shape[0] // num_shards\n                        for rank in range(num_shards):\n                            output[rank][key] = merged[slice_size * rank: slice_size * (rank + 1),:].clone().detach()\n                elif kind in ('ParallelEmbedding', 'RowParallelLinear'):\n                    with torch.no_grad():\n                        merged = torch.cat(tensors, 1)\n                        slice_size = merged.shape[1] // num_shards\n                        for rank in range(num_shards):\n                            output[rank][key] = merged[:,slice_size * rank: slice_size * (rank + 1)].clone().detach()\n                else:\n                    for rank in range(num_shards):\n                        output[rank][key] = tensors[0]\n                print('  out shapes=', [output[rank][key].shape for rank in range(num_shards)])\n                print()\n                break\n    print('saving...')\n    \n    with open(os.path.join(args.out_path, 'params.json'), 'w') as fp:\n        fp.write(json.dumps(params))\n    \n    for rank in range(num_shards):\n        print(' ', rank)\n        torch.save(output[rank], os.path.join(args.out_path, 'consolidated.%02d.pth' % rank))\n\n    print('done.')", ""]}
{"filename": "tools/datautils.py", "chunked_list": ["import numpy as np\nimport torch\n\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n\n\ndef get_wikitext2(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n    return trainloader, testenc", "\ndef get_wikitext2(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n    return trainloader, testenc", "\n\ndef get_ptb(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n    valdata = load_dataset('ptb_text_only', 'penn_treebank', split='validation')\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n    trainenc = tokenizer(\"\\n\\n\".join(traindata['sentence']), return_tensors='pt')\n    testenc = tokenizer(\"\\n\\n\".join(valdata['sentence']), return_tensors='pt')\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n    return trainloader, testenc", "\n\ndef get_c4(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train', use_auth_token=False)\n    valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation', use_auth_token=False)\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        while True:\n            i = random.randint(0, len(traindata) - 1)\n            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n            if trainenc.input_ids.shape[1] >= seqlen:\n                break\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n\n    import random\n    random.seed(0)\n    valenc = []\n    for _ in range(256):\n        while True:\n            i = random.randint(0, len(valdata) - 1)\n            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')\n            if tmp.input_ids.shape[1] >= seqlen:\n                break\n        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        valenc.append(tmp.input_ids[:, i:j])\n    valenc = torch.hstack(valenc)\n\n    class TokenizerWrapper:\n\n        def __init__(self, input_ids):\n            self.input_ids = input_ids\n\n    valenc = TokenizerWrapper(valenc)\n\n    return trainloader, valenc", "\n\ndef get_ptb_new(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n    return trainloader, testenc", "\n\ndef get_c4_new(nsamples, seed, seqlen, model):\n    from datasets import load_dataset\n    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\n    valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')\n\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n    except:\n        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\n    import random\n    random.seed(seed)\n    trainloader = []\n    for _ in range(nsamples):\n        while True:\n            i = random.randint(0, len(traindata) - 1)\n            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n            if trainenc.input_ids.shape[1] >= seqlen:\n                break\n        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = trainenc.input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        trainloader.append((inp, tar))\n\n    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n    valenc = valenc.input_ids[:, :(256 * seqlen)]\n\n    class TokenizerWrapper:\n\n        def __init__(self, input_ids):\n            self.input_ids = input_ids\n\n    valenc = TokenizerWrapper(valenc)\n\n    return trainloader, valenc", "\n\ndef get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n    if 'wikitext2' in name:\n        return get_wikitext2(nsamples, seed, seqlen, model)\n    if 'ptb' in name:\n        if 'new' in name:\n            return get_ptb_new(nsamples, seed, seqlen, model)\n        return get_ptb(nsamples, seed, seqlen, model)\n    if 'c4' in name:\n        if 'new' in name:\n            return get_c4_new(nsamples, seed, seqlen, model)\n        return get_c4(nsamples, seed, seqlen, model)", ""]}
{"filename": "tools/convert_llama.py", "chunked_list": ["import argparse\nimport os\nfrom transformers.models.llama.convert_llama_weights_to_hf import write_model, write_tokenizer\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--input_dir\",\n        help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n    )\n    parser.add_argument(\n        \"--model_size\",\n        choices=[\"7B\", \"13B\", \"30B\", \"65B\", \"tokenizer_only\"],\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        help=\"Location to write HF model and tokenizer\",\n    )\n    args = parser.parse_args()\n    if args.model_size != \"tokenizer_only\":\n        write_model(\n            model_path=os.path.join(args.output_dir, \"llama-{}\".format(args.model_size).lower()),\n            input_base_path=os.path.join(args.input_dir, args.model_size),\n            model_size=args.model_size,\n        )\n    write_tokenizer(\n        tokenizer_path=os.path.join(args.output_dir, \"llama-{}\".format(args.model_size).lower()),\n        input_tokenizer_path=os.path.join(args.input_dir, \"tokenizer.model\"),\n    )", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "tools/gptq.py", "chunked_list": ["import math\nimport time\n\nimport torch\nimport torch.nn as nn\nimport transformers\nimport quant\nfrom texttable import Texttable\n\ntorch.backends.cuda.matmul.allow_tf32 = False", "\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\ndef torch_snr_error(y_pred: torch.Tensor, y_real: torch.Tensor, reduction: str = 'mean') -> torch.Tensor:\n    \"\"\"\n    Compute SNR between y_pred(tensor) and y_real(tensor)\n    \n    SNR can be calcualted as following equation:\n    \n        SNR(pred, real) = (pred - real) ^ 2 / (real) ^ 2\n    \n    if x and y are matrixs, SNR error over matrix should be the mean value of SNR error over all elements.\n    \n        SNR(pred, real) = mean((pred - real) ^ 2 / (real) ^ 2)\n    Args:\n        y_pred (torch.Tensor): _description_\n        y_real (torch.Tensor): _description_\n        reduction (str, optional): _description_. Defaults to 'mean'.\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n    Returns:\n        torch.Tensor: _description_\n    \"\"\"\n    y_pred = y_pred.type(torch.float32)\n    y_real = y_real.type(torch.float32)\n\n    if y_pred.shape != y_real.shape:\n        raise ValueError(f'Can not compute snr loss for tensors with different shape. '\n                         f'({y_pred.shape} and {y_real.shape})')\n    reduction = str(reduction).lower()\n\n    if y_pred.ndim == 1:\n        y_pred = y_pred.unsqueeze(0)\n        y_real = y_real.unsqueeze(0)\n\n    y_pred = y_pred.flatten(start_dim=1)\n    y_real = y_real.flatten(start_dim=1)\n\n    noise_power = torch.pow(y_pred - y_real, 2).sum(dim=-1)\n    signal_power = torch.pow(y_real, 2).sum(dim=-1)\n    snr = (noise_power) / (signal_power + 1e-7)\n\n    if reduction == 'mean':\n        return torch.mean(snr)\n    elif reduction == 'sum':\n        return torch.sum(snr)\n    elif reduction == 'none':\n        return snr\n    else:\n        raise ValueError(f'Unsupported reduction method.')", "\n\nclass GPTQ:\n\n    def __init__(self, layer, observe=False):\n        self.layer = layer\n        self.dev = self.layer.weight.device\n        W = layer.weight.data.clone()\n        if isinstance(self.layer, nn.Conv2d):\n            W = W.flatten(1)\n        if isinstance(self.layer, transformers.Conv1D):\n            W = W.t()\n        self.rows = W.shape[0]\n        self.columns = W.shape[1]\n        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n        self.nsamples = 0\n        self.quantizer = quant.Quantizer()\n        self.observe = observe\n\n    def add_batch(self, inp, out):\n        # Hessian H = 2 X XT + \u03bb I\n        if self.observe:\n            self.inp1 = inp\n            self.out1 = out\n        else:\n            self.inp1 = None\n            self.out1 = None\n\n        if len(inp.shape) == 2:\n            inp = inp.unsqueeze(0)\n        tmp = inp.shape[0]\n        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n            if len(inp.shape) == 3:\n                inp = inp.reshape((-1, inp.shape[-1]))\n            inp = inp.t()\n        if isinstance(self.layer, nn.Conv2d):\n            unfold = nn.Unfold(self.layer.kernel_size, dilation=self.layer.dilation, padding=self.layer.padding, stride=self.layer.stride)\n            inp = unfold(inp)\n            inp = inp.permute([1, 0, 2])\n            inp = inp.flatten(1)\n        self.H *= self.nsamples / (self.nsamples + tmp)\n        self.nsamples += tmp\n        # inp = inp.float()\n        inp = math.sqrt(2 / self.nsamples) * inp.float()\n        # self.H += 2 / self.nsamples * inp.matmul(inp.t())\n        self.H += inp.matmul(inp.t())\n\n    def print_loss(self, name, q_weight, weight_error, timecost):\n        table = Texttable()\n        name += ' ' * (16 - len(name))\n\n        table.header(['name', 'weight_error', 'fp_inp_SNR', 'q_inp_SNR', 'time'])\n\n        # assign weight\n        self.layer.weight.data = q_weight.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n\n        if self.inp1 is not None:\n            # quantize input to int8\n            quantizer = quant.Quantizer()\n            quantizer.configure(8, perchannel=False, sym=True, mse=False)\n            quantizer.find_params(self.inp1)\n            q_in = quantizer.quantize(self.inp1).type(torch.float16)\n            q_out = self.layer(q_in)\n\n            # get kinds of SNR\n            q_SNR = torch_snr_error(q_out, self.out1).item()\n            fp_SNR = torch_snr_error(self.layer(self.inp1), self.out1).item()\n        else:\n            q_SNR = '-'\n            fp_SNR = '-'\n\n        table.add_row([name, weight_error, fp_SNR, q_SNR, timecost])\n        print(table.draw().split('\\n')[-2])\n\n    def fasterquant(self, blocksize=128, percdamp=.01, groupsize=-1, actorder=False, name=''):\n        self.layer.to(self.dev)\n\n        W = self.layer.weight.data.clone()\n        if isinstance(self.layer, nn.Conv2d):\n            W = W.flatten(1)\n        if isinstance(self.layer, transformers.Conv1D):\n            W = W.t()\n        W = W.float()\n\n        tick = time.time()\n\n        if not self.quantizer.ready():\n            self.quantizer.find_params(W, weight=True)\n\n        H = self.H\n        if not self.observe:\n            del self.H\n        dead = torch.diag(H) == 0\n        H[dead, dead] = 1\n        W[:, dead] = 0\n\n        if actorder:\n            perm = torch.argsort(torch.diag(H), descending=True)\n            W = W[:, perm]\n            H = H[perm][:, perm]\n\n        Losses = torch.zeros_like(W)\n        Q = torch.zeros_like(W)\n\n        damp = percdamp * torch.mean(torch.diag(H))\n        diag = torch.arange(self.columns, device=self.dev)\n        H[diag, diag] += damp\n        H = torch.linalg.cholesky(H)\n        H = torch.cholesky_inverse(H)\n        H = torch.linalg.cholesky(H, upper=True)\n        Hinv = H\n\n        g_idx = []\n        scale = []\n        zero = []\n        now_idx = 1\n\n        for i1 in range(0, self.columns, blocksize):\n            i2 = min(i1 + blocksize, self.columns)\n            count = i2 - i1\n\n            W1 = W[:, i1:i2].clone()\n            Q1 = torch.zeros_like(W1)\n            Err1 = torch.zeros_like(W1)\n            Losses1 = torch.zeros_like(W1)\n            Hinv1 = Hinv[i1:i2, i1:i2]\n\n            for i in range(count):\n                w = W1[:, i]\n                d = Hinv1[i, i]\n\n                if groupsize != -1:\n                    if (i1 + i) % groupsize == 0:\n                        self.quantizer.find_params(W[:, (i1 + i):(i1 + i + groupsize)], weight=True)\n\n                    if ((i1 + i) // groupsize) - now_idx == -1:\n                        scale.append(self.quantizer.scale)\n                        zero.append(self.quantizer.zero)\n                        now_idx += 1\n\n                q = self.quantizer.quantize(w.unsqueeze(1)).flatten()\n                Q1[:, i] = q\n                Losses1[:, i] = (w - q)**2 / d**2\n\n                err1 = (w - q) / d\n                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n                Err1[:, i] = err1\n\n            Q[:, i1:i2] = Q1\n            Losses[:, i1:i2] = Losses1 / 2\n\n            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n\n        torch.cuda.synchronize()\n        error = torch.sum(Losses).item()\n\n        groupsize = groupsize if groupsize != -1 else self.columns\n        g_idx = [i // groupsize for i in range(self.columns)]\n        g_idx = torch.tensor(g_idx, dtype=torch.int32, device=Q.device)\n        if actorder:\n            invperm = torch.argsort(perm)\n            Q = Q[:, invperm]\n            g_idx = g_idx[invperm]\n\n        if isinstance(self.layer, transformers.Conv1D):\n            Q = Q.t()\n\n        self.print_loss(name=name, q_weight=Q, weight_error=error, timecost=(time.time() - tick))\n\n        if scale == []:\n            scale.append(self.quantizer.scale)\n            zero.append(self.quantizer.zero)\n        scale = torch.cat(scale, dim=1)\n        zero = torch.cat(zero, dim=1)\n        return scale, zero, g_idx, error\n\n    def free(self):\n        self.inp1 = None\n        self.out1 = None\n        self.H = None\n        self.Losses = None\n        self.Trace = None\n        torch.cuda.empty_cache()", ""]}
{"filename": "tools/convert_pth_to_ggml.py", "chunked_list": ["# This file is from: https://github.com/ggerganov/llama.cpp \n# And it converts LLaMA model's pytorch_model.bin to ggml compatible file\n\n# Load the model using Torch\n# Iterate over all variables and write them to a binary file.\n# For each variable, write the following:\n#   - Number of dimensions (int)\n#   - Name length (int)\n#   - Dimensions (int[n_dims])\n#   - Name (char[name_length])", "#   - Dimensions (int[n_dims])\n#   - Name (char[name_length])\n#   - Data (float[n_dims])\n#\n# By default, the bigger matrices are converted to 16-bit floats.\n# This can be disabled by adding the \"use-f32\" CLI argument.\n#\n# At the start of the ggml file we write the model parameters\n# and vocabulary.\n#", "# and vocabulary.\n#\nimport os\nimport sys\nimport json\nimport struct\nimport numpy as np\nimport torch\nfrom sentencepiece import SentencePieceProcessor\nimport argparse", "from sentencepiece import SentencePieceProcessor\nimport argparse\n\n# args\nparser = argparse.ArgumentParser()\n# The original base model checkpoint dir\nparser.add_argument(\"--dir_model\", type=str, default='lora-Vicuna/checkpoint-3000-with-lora/ckpt')\n# The finetuned lora model checkpoint dir\nparser.add_argument(\"--dir_out\",type=str, default=None)\n# NOTE: you can find it in llama-7b dir", "parser.add_argument(\"--dir_out\",type=str, default=None)\n# NOTE: you can find it in llama-7b dir\nparser.add_argument(\"--fname_tokenizer\", type=str, default=\"lora-Vicuna/llama-7b/tokenizer.model\")\n# 0=fp32, 1=fp16\nparser.add_argument(\"--ftype\", type=int, default=1)\n# NOTE: this parameter is n_parts split of the `consolidated.0x` checkpoint\nparser.add_argument(\"--shard\", type=int, default=None)\nargs = parser.parse_args()\n\nif args.dir_out is None: dir_out = args.dir_model # output in the same directory as the model", "\nif args.dir_out is None: dir_out = args.dir_model # output in the same directory as the model\n\ndir_model = args.dir_model\nftype=args.ftype\nfname_tokenizer=args.fname_tokenizer\nfname_hparams   = dir_model + \"/params.json\"\n\n# possible data types\n#   ftype == 0 -> float32", "# possible data types\n#   ftype == 0 -> float32\n#   ftype == 1 -> float16\n#\n# map from ftype to string\nftype_str = [\"f32\", \"f16\"]\nif ftype < 0 or ftype > 1:\n    print(\"Invalid ftype: \" + str(ftype))\n    sys.exit(1)\n", "\nfname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\nif os.path.exists(fname_out):\n    print(f\"Skip conversion, it already exists: {fname_out}\")\n    sys.exit(0)\n\nwith open(fname_hparams, \"r\") as f:\n    hparams = json.load(f)\n\ntokenizer = SentencePieceProcessor(fname_tokenizer)", "\ntokenizer = SentencePieceProcessor(fname_tokenizer)\n\nhparams.update({\"vocab_size\": tokenizer.vocab_size()})\n\ndef get_n_parts(dim):\n    if dim == 4096:\n        return 1\n    elif dim == 5120:\n        return 2\n    elif dim == 6656:\n        return 4\n    elif dim == 8192:\n        return 8\n    else:\n        print(\"Invalid dim: \" + str(dim))\n        sys.exit(1)", "\nif args.shard is None: # default\n    n_parts = get_n_parts(hparams[\"dim\"])\nelse:\n    n_parts = args.shard\n\nprint(hparams)\nprint('n_parts = ', n_parts)\n\nfor p in range(n_parts):\n    print('Processing part ', p)\n\n    fname_model = dir_model + \"/consolidated.0\" + str(p) + \".pth\"\n    fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\n    if (p > 0):\n        fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\" + \".\" + str(p)\n\n    model = torch.load(fname_model, map_location=\"cpu\")\n\n    fout = open(fname_out, \"wb\")\n\n    fout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\n    fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n    fout.write(struct.pack(\"i\", hparams[\"dim\"]))\n    fout.write(struct.pack(\"i\", hparams[\"multiple_of\"]))\n    fout.write(struct.pack(\"i\", hparams[\"n_heads\"]))\n    fout.write(struct.pack(\"i\", hparams[\"n_layers\"]))\n    fout.write(struct.pack(\"i\", hparams[\"dim\"] // hparams[\"n_heads\"])) # rot (obsolete)\n    fout.write(struct.pack(\"i\", ftype))\n\n    # Is this correct??\n    for i in range(tokenizer.vocab_size()):\n        if tokenizer.is_unknown(i):\n            # \"<unk>\" token (translated as ??)\n            text = \" \\u2047 \".encode(\"utf-8\")\n            fout.write(struct.pack(\"i\", len(text)))\n            fout.write(text)\n        elif tokenizer.is_control(i):\n            # \"<s>\"/\"</s>\" tokens\n            fout.write(struct.pack(\"i\", 0))\n        elif tokenizer.is_byte(i):\n            # \"<U+XX>\" tokens (which may be invalid UTF-8)\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(\"Invalid token: \" + piece)\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            fout.write(struct.pack(\"i\", 1))\n            fout.write(struct.pack(\"B\", byte_value))\n        else:\n            # normal token. Uses U+2581 (LOWER ONE EIGHTH BLOCK) to represent spaces.\n            text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n            fout.write(struct.pack(\"i\", len(text)))\n            fout.write(text)\n\n    for k, v in model.items():\n        name = k\n        shape = v.shape\n\n        # skip layers.X.attention.inner_attention.rope.freqs\n        if name[-5:] == \"freqs\":\n            continue\n\n        print(\"Processing variable: \" + name + \" with shape: \", shape, \" and type: \", v.dtype)\n\n        #data = tf.train.load_variable(dir_model, name).squeeze()\n        data = v.numpy().squeeze()\n        n_dims = len(data.shape)\n\n        # for efficiency - transpose some matrices\n        # \"model/h.*/attn/c_attn/w\"\n        # \"model/h.*/attn/c_proj/w\"\n        # \"model/h.*/mlp/c_fc/w\"\n        # \"model/h.*/mlp/c_proj/w\"\n        #if name[-14:] == \"/attn/c_attn/w\" or \\\n        #   name[-14:] == \"/attn/c_proj/w\" or \\\n        #   name[-11:] == \"/mlp/c_fc/w\" or \\\n        #   name[-13:] == \"/mlp/c_proj/w\":\n        #    print(\"  Transposing\")\n        #    data = data.transpose()\n\n        dshape = data.shape\n\n        # default type is fp16\n        ftype_cur = 1\n        if ftype == 0 or n_dims == 1:\n            print(\"  Converting to float32\")\n            data = data.astype(np.float32)\n            ftype_cur = 0\n\n        # header\n        sname = name.encode('utf-8')\n        fout.write(struct.pack(\"iii\", n_dims, len(sname), ftype_cur))\n        for i in range(n_dims):\n            fout.write(struct.pack(\"i\", dshape[n_dims - 1 - i]))\n        fout.write(sname)\n\n        # data\n        data.tofile(fout)\n\n    # I hope this deallocates the memory ..\n    model = None\n\n    fout.close()\n\n    print(\"Done. Output file: \" + fname_out + \", (part \", p, \")\")\n    print(\"\")", "\nfor p in range(n_parts):\n    print('Processing part ', p)\n\n    fname_model = dir_model + \"/consolidated.0\" + str(p) + \".pth\"\n    fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\n    if (p > 0):\n        fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\" + \".\" + str(p)\n\n    model = torch.load(fname_model, map_location=\"cpu\")\n\n    fout = open(fname_out, \"wb\")\n\n    fout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\n    fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n    fout.write(struct.pack(\"i\", hparams[\"dim\"]))\n    fout.write(struct.pack(\"i\", hparams[\"multiple_of\"]))\n    fout.write(struct.pack(\"i\", hparams[\"n_heads\"]))\n    fout.write(struct.pack(\"i\", hparams[\"n_layers\"]))\n    fout.write(struct.pack(\"i\", hparams[\"dim\"] // hparams[\"n_heads\"])) # rot (obsolete)\n    fout.write(struct.pack(\"i\", ftype))\n\n    # Is this correct??\n    for i in range(tokenizer.vocab_size()):\n        if tokenizer.is_unknown(i):\n            # \"<unk>\" token (translated as ??)\n            text = \" \\u2047 \".encode(\"utf-8\")\n            fout.write(struct.pack(\"i\", len(text)))\n            fout.write(text)\n        elif tokenizer.is_control(i):\n            # \"<s>\"/\"</s>\" tokens\n            fout.write(struct.pack(\"i\", 0))\n        elif tokenizer.is_byte(i):\n            # \"<U+XX>\" tokens (which may be invalid UTF-8)\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(\"Invalid token: \" + piece)\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            fout.write(struct.pack(\"i\", 1))\n            fout.write(struct.pack(\"B\", byte_value))\n        else:\n            # normal token. Uses U+2581 (LOWER ONE EIGHTH BLOCK) to represent spaces.\n            text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n            fout.write(struct.pack(\"i\", len(text)))\n            fout.write(text)\n\n    for k, v in model.items():\n        name = k\n        shape = v.shape\n\n        # skip layers.X.attention.inner_attention.rope.freqs\n        if name[-5:] == \"freqs\":\n            continue\n\n        print(\"Processing variable: \" + name + \" with shape: \", shape, \" and type: \", v.dtype)\n\n        #data = tf.train.load_variable(dir_model, name).squeeze()\n        data = v.numpy().squeeze()\n        n_dims = len(data.shape)\n\n        # for efficiency - transpose some matrices\n        # \"model/h.*/attn/c_attn/w\"\n        # \"model/h.*/attn/c_proj/w\"\n        # \"model/h.*/mlp/c_fc/w\"\n        # \"model/h.*/mlp/c_proj/w\"\n        #if name[-14:] == \"/attn/c_attn/w\" or \\\n        #   name[-14:] == \"/attn/c_proj/w\" or \\\n        #   name[-11:] == \"/mlp/c_fc/w\" or \\\n        #   name[-13:] == \"/mlp/c_proj/w\":\n        #    print(\"  Transposing\")\n        #    data = data.transpose()\n\n        dshape = data.shape\n\n        # default type is fp16\n        ftype_cur = 1\n        if ftype == 0 or n_dims == 1:\n            print(\"  Converting to float32\")\n            data = data.astype(np.float32)\n            ftype_cur = 0\n\n        # header\n        sname = name.encode('utf-8')\n        fout.write(struct.pack(\"iii\", n_dims, len(sname), ftype_cur))\n        for i in range(n_dims):\n            fout.write(struct.pack(\"i\", dshape[n_dims - 1 - i]))\n        fout.write(sname)\n\n        # data\n        data.tofile(fout)\n\n    # I hope this deallocates the memory ..\n    model = None\n\n    fout.close()\n\n    print(\"Done. Output file: \" + fname_out + \", (part \", p, \")\")\n    print(\"\")", ""]}
{"filename": "tools/quant_generate.py", "chunked_list": ["import sys\nimport torch\nimport torch.nn as nn\nimport transformers\nimport gradio as gr\nimport argparse\nimport warnings\nimport os\nimport quant\nfrom gptq import GPTQ", "import quant\nfrom gptq import GPTQ\nfrom datautils import get_loaders\n\nassert (\n    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n    return res", "\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n    return res\n\ndef load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model, layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n    model.load_state_dict(torch.load(checkpoint), strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model, transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n    model.seqlen = 2048\n    print('Done.')\n\n    return model", "\ndef load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model, layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n    model.load_state_dict(torch.load(checkpoint), strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model, transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n    model.seqlen = 2048\n    print('Done.')\n\n    return model", "\ndef generate_prompt(instruction, input=None):\n    if input:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n        \n        ### Instruction:\n        {instruction}\n\n        ### Input:\n        {input}\n        \n        ### Response:\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n        \n        ### Instruction:\n        {instruction}\n        \n        ### Response:\"\"\"", "\ndef main():\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\",type=str,default=\"decapoda-research/llama-7b-hf\",help=\"llama huggingface model to load\")\n    parser.add_argument(\"--quant_path\",type=str,default=\"llama7b-8bit-128g.pt\",help=\"the quantified model path\")\n    parser.add_argument(\n                        \"--wbits\",\n                        type=int,\n                        default=4,\n                        choices=[2, 3, 4, 8],\n                        help=\"bits to use for quantization; use 8 for evaluating base model.\")\n    \n    parser.add_argument('--text', type=str, default='the mean of life is', help='input text')\n\n    parser.add_argument('--min_length', type=int, default=10, help='The minimum length of the sequence to be generated.')\n\n    parser.add_argument('--max_length', type=int, default=256, help='The maximum length of the sequence to be generated.')\n\n    parser.add_argument('--top_p',\n                        type=float,\n                        default=0.95,\n                        help='If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.')\n\n    parser.add_argument('--temperature', type=float, default=0.1, help='The value used to module the next token probabilities.')\n    parser.add_argument('--repetition_penalty',type=float, default=2.0, help='The parameter for repetition penalty. 1.0 means no penalty(0~10)')\n    parser.add_argument('--groupsize', type=int, default=-1, help='Groupsize to use for quantization; default uses full row.')\n    parser.add_argument('--gradio', action='store_true', help='Whether to use gradio to present results.')\n    args = parser.parse_args()\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n\n    model = load_quant(args.model_path, args.quant_path, args.wbits, args.groupsize)\n    model.to(device)\n    tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n    model.eval()\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n    #[Way1]: drectly generate\n    if not args.gradio:\n        input_ids = tokenizer.encode(args.text, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            generated_ids = model.generate(\n                input_ids,\n                min_new_tokens=args.min_length,\n                max_new_tokens=args.max_length,\n                top_p=args.top_p,\n                temperature=args.temperature,\n                repetition_penalty=args.repetition_penalty,\n            )\n        print(\"*\"*80)\n        print(\"\ud83e\udd99:\", tokenizer.decode([el.item() for el in generated_ids[0]],skip_special_tokens=True))\n    #[Way2]: generate through the gradio interface\n    else:   \n        def evaluate(\n            input,\n            temperature=0.1,\n            top_p=0.75,\n            top_k=40,\n            num_beams=1,\n            max_new_tokens=128,\n            repetition_penalty=1.0,\n            **kwargs,\n        ):\n            prompt = generate_prompt(input)\n            inputs = tokenizer(prompt, return_tensors=\"pt\")\n            input_ids = inputs[\"input_ids\"].to(device)\n            generation_config = GenerationConfig(\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                num_beams=num_beams,\n                **kwargs,\n            )\n            with torch.no_grad():\n                generation_output = model.generate(\n                    input_ids=input_ids,\n                    generation_config=generation_config,\n                    return_dict_in_generate=True,\n                    output_scores=True,\n                    max_new_tokens=max_new_tokens,\n                    repetition_penalty=float(repetition_penalty),\n                )\n            s = generation_output.sequences[0]\n            output = tokenizer.decode(s,skip_special_tokens=True)\n            return output.split(\"### Response:\")[1].strip()\n\n\n        gr.Interface(\n            fn=evaluate,\n            inputs=[\n                gr.components.Textbox(\n                    lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n                ),\n                gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n                gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n                gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n                gr.components.Slider(minimum=1, maximum=5, step=1, value=1, label=\"Beams\"),\n                gr.components.Slider(\n                    minimum=1, maximum=2000, step=1, value=256, label=\"Max tokens\"\n                ),\n                gr.components.Slider(\n                    minimum=0.1, maximum=10.0, step=0.1, value=1.0, label=\"Repetition Penalty\"\n                ),\n            ],\n            outputs=[\n                gr.inputs.Textbox(\n                    lines=5,\n                    label=\"Output\",\n                )\n            ],\n            title=\"Chinese-Vicuna \u4e2d\u6587\u5c0f\u7f8a\u9a7c\",\n            description=\"\u4e2d\u6587\u5c0f\u7f8a\u9a7c\u7531\u5404\u79cd\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90instruction\u6570\u636e\u96c6\uff0c\u7ed3\u5408Alpaca-lora\u7684\u4ee3\u7801\u8bad\u7ec3\u800c\u6765\uff0c\u6a21\u578b\u57fa\u4e8e\u5f00\u6e90\u7684llama7B\uff0c\u4e3b\u8981\u8d21\u732e\u662f\u5bf9\u5e94\u7684lora\u6a21\u578b\u3002\u7531\u4e8e\u4ee3\u7801\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u8f83\u5c0f\uff0c\u5e0c\u671b\u4e3allama\u4e2d\u6587lora\u793e\u533a\u505a\u4e00\u4efd\u8d21\u732e\u3002\",\n        ).launch(share=True)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "tools/application/chatglm_lora_test.py", "chunked_list": ["import os\nimport tqdm\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport peft\nimport loralib as lora", "import peft\nimport loralib as lora\nfrom peft import LoraConfig\n\nimport json\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nfrom accelerate import Accelerator, DeepSpeedPlugin\nfrom transformers import get_linear_schedule_with_warmup", "from accelerate import Accelerator, DeepSpeedPlugin\nfrom transformers import get_linear_schedule_with_warmup\n\n\"\"\"\nextra requirements:\n    pip install icetk\n\"\"\"\n\n# reload the model: no int8, so 14GB is needed\nversion = 'no.pt' # finetune_0.pt", "# reload the model: no int8, so 14GB is needed\nversion = 'no.pt' # finetune_0.pt\nmodel_dir = '/home/liang/lzy_tmp/models/chatglm-6b'\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_dir, trust_remote_code=True)\nconfig = LoraConfig(\n    peft_type=\"LORA\", \n    task_type=\"SEQ_2_SEQ_LM\", \n    r=32, \n    lora_alpha=32, ", "    r=32, \n    lora_alpha=32, \n    target_modules=[\"q\", \"k\", \"v\"],\n    lora_dropout=0.1, \n)\n\nclass QKV_layer(torch.nn.Module):\n    def __init__(self, in_features, out_features):\n        super(QKV_layer, self).__init__()\n        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n\n    def update(self, target_layer):\n        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n\n        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n\n        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n    \n    def forward(self, x):\n        q = self.linear_q(x)\n        k = self.linear_k(x)\n        v = self.linear_v(x)\n        return torch.concat([q,k,v], dim = -1)", "\nif version != 'no.pt':\n    # convert it again\n    for key, module in model.named_modules():\n        if key.endswith('attention'):\n            try:\n                qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n                qkv_layer.update(module.query_key_value)\n                module.query_key_value = qkv_layer\n            except:\n                pass\n            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n\n    # load the LoRA checkpoint\n    model.load_state_dict(torch.load(f'/{model_dir}/{version}'), strict=False)", "\nmodel.half().cuda().eval()\n\n# Let's chat!\nos.makedirs('outs/chatglm-6b/', exist_ok=True)\nwith open(f'outs/chatglm-6b/test_{version}.txt','w') as f:\n    for text in open('sample/test.jsonl'):\n        text = json.loads(text)\n        inputs = text['instruction']\n        print('Q:', inputs)\n        print('Q:', inputs, file=f)\n        response, history = model.chat(tokenizer, inputs, history=[])\n        print('A:', response)\n        print('A:', response, '\\n',file=f)"]}
{"filename": "tools/application/chitchat_finetune.py", "chunked_list": ["import os\nimport sys\nimport wandb\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nimport transformers\nimport argparse\nfrom transformers import LlamaForCausalLM, LlamaTokenizer", "import argparse\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom peft import (\n    prepare_model_for_int8_training,\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n)\n\n# Used for chitchat dataset", "\n# Used for chitchat dataset\n# \u7528\u4e8e\u95f2\u804a\u5bf9\u8bdd\u6570\u636e\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\nparser.add_argument(\"--data_path\", type=str, default=\"datasets/chitchat-1e5.json\") # for example: LCCC \nparser.add_argument(\"--output_path\", type=str, default=\"outs/13B\")\nparser.add_argument(\"--model_path\", type=str, default=\"../model/13B_hf\")\nparser.add_argument(\"--eval_steps\", type=int, default=200)", "parser.add_argument(\"--model_path\", type=str, default=\"../model/13B_hf\")\nparser.add_argument(\"--eval_steps\", type=int, default=200)\nparser.add_argument(\"--save_steps\", type=int, default=200)\nparser.add_argument(\"--test_size\", type=int, default=0)\nargs = parser.parse_args()\n# optimized for RTX 4090. for larger GPUs, increase some of these?\nMICRO_BATCH_SIZE = 24  # this could actually be 5 but i like powers of 2\nBATCH_SIZE = 128\nGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 2  # we don't always need 3 tbh", "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\nEPOCHS = 2  # we don't always need 3 tbh\nLEARNING_RATE = 3e-4  # the Karpathy constant\nCUTOFF_LEN = 341  # max:341\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nVAL_SET_SIZE = args.test_size #2000\nTARGET_MODULES = [\n    \"q_proj\",", "TARGET_MODULES = [\n    \"q_proj\",\n    \"v_proj\",\n]\nDATA_PATH = args.data_path \nOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\ndevice_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size", "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\nif args.wandb:\n    wandb.login(key = '41327ad68395c1a5e5e3827fa5ee97944740250d') # luzhenyi\n    wandb.init(\n        project=\"LoRA\",\n        name=f\"{args.model_path}-{args.data_path}\",\n        config=None,\n    )\nelse:\n    wandb.init(mode='disabled')", "\ntokenizer = LlamaTokenizer.from_pretrained(\n    args.model_path, add_eos_token=True\n)\ntokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\ndata = load_dataset(\"json\", data_files=DATA_PATH)\n\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with an input that provides further context. \"", "    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n    ),\n    \"prompt_no_input\": (\n        \"Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n    ),", "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n    ),\n}\nCHAT_DICT = {\n    'prompt': (\n        \"The following is a conversation between an AI assistant called Bot and a human user called User.\"\n        \"Bot is is intelligent, knowledgeable, wise and polite.\\n\\n\"\n    ),\n    'history': (\n        \"User:\\n{input}\\n\\nBot:{output}\\n\\n\"", "    'history': (\n        \"User:\\n{input}\\n\\nBot:{output}\\n\\n\"\n    ),\n    'input': (\n        \"### User:\\n{input}\\n\\n### Bot:\"\n    )\n}\n\ndef tokenize(prompt):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )\n    return {\n        \"input_ids\": result[\"input_ids\"][:-1],\n        \"attention_mask\": result[\"attention_mask\"][:-1],\n    }", "def tokenize(prompt):\n    # there's probably a way to do this with the tokenizer settings\n    # but again, gotta move fast\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\",\n    )\n    return {\n        \"input_ids\": result[\"input_ids\"][:-1],\n        \"attention_mask\": result[\"attention_mask\"][:-1],\n    }", "def generate_and_tokenize_prompt(data_point):\n    # This function masks out the labels for the input,\n    # so that our loss is computed only on the response.\n    user_prompt = CHAT_DICT['prompt']\n    for history in data_point['history']:\n        user_prompt+= CHAT_DICT['history'].format_map(history) \n    user_prompt += CHAT_DICT['input'].format_map(data_point)\n    len_user_prompt_tokens = (len(tokenizer(\n        user_prompt,\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n    )[\"input_ids\"])- 1)  # no eos token\n    full_tokens = tokenizer(\n        user_prompt + data_point[\"output\"],\n        truncation=True,\n        max_length=CUTOFF_LEN + 1,\n        padding=\"max_length\", # pad\u5230\u6700\u957f\n    )[\"input_ids\"][:-1]\n    return {\n        \"input_ids\": full_tokens,\n        \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n        \"attention_mask\": [1] * (len(full_tokens)),\n    }", "\nif VAL_SET_SIZE > 0:\n    train_val = data[\"train\"].train_test_split(\n        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n    )\n    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\n    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\nelse:\n    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\n    val_data = None", "\nmodel = LlamaForCausalLM.from_pretrained(\n    args.model_path,\n    load_in_8bit=True,\n    device_map=device_map,\n)\n\nmodel = prepare_model_for_int8_training(model)\n\nconfig = LoraConfig(", "\nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=TARGET_MODULES,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, config)", ")\nmodel = get_peft_model(model, config)\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,", "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=100,\n        num_train_epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=20,\n        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,", "        save_strategy=\"steps\",\n        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n        save_steps=args.save_steps,\n        output_dir=OUTPUT_DIR,\n        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n        ddp_find_unused_parameters=False if ddp else None,\n        report_to=\"wandb\" if args.wandb else [],\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)", "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False\n\nold_state_dict = model.state_dict\nmodel.state_dict = (\n    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n).__get__(model, type(model))\n\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)", "\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n    model = torch.compile(model)\n\ntrainer.train()\nmodel.save_pretrained(OUTPUT_DIR)\nprint(\"\\n If there's a warning about missing keys above, please disregard :)\")"]}
{"filename": "tools/application/chatglm_lora_finetune.py", "chunked_list": ["\n### Load Model From huggingface\n\nimport os\nimport tqdm\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nimport torch", "\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport wandb\nimport peft\nimport loralib as lora\nfrom peft import LoraConfig\n\nimport json\nfrom torch.utils.data import DataLoader", "import json\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nfrom accelerate import Accelerator, DeepSpeedPlugin\nfrom transformers import get_linear_schedule_with_warmup\n\n\n\"\"\"\nextra requirements: ", "\"\"\"\nextra requirements: \n    pip install icetk\n\"\"\"\ncheckpoint = \"/model/chatglm-6b\"\ndatafile='datasets/merge.json'\nout_dir= 'outs/chatglm-6b'\nuse_wandb=True\n\nmixed_precision = 'bf16'", "\nmixed_precision = 'bf16'\naccumulate_step = 8\nlog_interval = 100\nPer_GPU_BATCH_SIZE = 2\nMAX_LENGTH = 256 # have huge impact on VRAM: 968:1, 256:4\nconfig = LoraConfig(\n    peft_type=\"LORA\", \n    r=32,\n    lora_alpha=32,", "    r=32,\n    lora_alpha=32,\n    target_modules=[\"q\", \"k\", \"v\"],\n    lora_dropout=0.1, \n)\nLR = 2e-5\nNUM_EPOCHS = 3\nwarm_up_ratio = 0.1\ndevice_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))", "device_map = \"auto\"\nworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\nddp = world_size != 1\nif ddp:\n    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\nif use_wandb:\n    wandb.init(\n        project=\"LoRA\",\n        name=f\"{checkpoint}-{datafile}\",\n        config=None,\n    )\nelse:\n    wandb.init(mode='disabled')", "\nos.makedirs(out_dir, exist_ok=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    checkpoint, \n    trust_remote_code=True,\n    device_map=device_map,\n)\n# BUG: must remove special token '[MASK]'\n# del tokenizer.vocab['MASK'] ", "# BUG: must remove special token '[MASK]'\n# del tokenizer.vocab['MASK'] \n\n\n### Dataset\nEOS_ID = 150005\nPROMPT_DICT = {\n    \"prompt_input\": (\n        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"", "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n    ),\n    \"prompt_no_input\": (\n        \"Below is an instruction that describes a task. \"\n        \"Write a response that appropriately completes the request.\\n\\n\"\n        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n    ),\n}", "    ),\n}\n\nwith open(datafile, 'r') as f:\n    content = json.load(f)\npairs = []\nfor line in content:\n    if line['input'] == '':\n        prompt = PROMPT_DICT['prompt_no_input'].format_map(line)\n    else:\n        prompt = PROMPT_DICT['prompt_input'].format_map(line)\n    completion = line['output']+'</s>'\n    if len(prompt) + len(completion) < MAX_LENGTH:\n        pairs.append({'prompt':prompt, 'completion':completion})", "\nclass AlpacaDataset(Dataset):\n    def __init__(self, pairs, tokenizer) -> None:\n        super().__init__()\n        self.pairs = pairs\n        self.tokenizer = tokenizer\n \n    def __getitem__(self, index):\n        if self.pairs[index]['completion'][-4:] == '</s>':\n            prompt = self.tokenizer.encode(self.pairs[index]['prompt'])\n            completion = self.tokenizer.encode(self.pairs[index]['completion'][:-4], add_special_tokens=False)\n            completion += [EOS_ID]\n        else:\n            prompt = self.tokenizer.encode(self.pairs[index]['prompt'])\n            completion = self.tokenizer.encode(self.pairs[index]['completion'], add_special_tokens=False)\n        if 150001 not in prompt:\n            prompt = self.pairs[index]['prompt'].replace('[MASK]', '//MASK//').replace('[gMASK]', '//gMASK//')\n            completion = self.pairs[index]['completion'].replace('[MASK]', '//MASK//').replace('[gMASK]', '//gMASK//')\n            prompt = self.tokenizer.encode(prompt)\n            completion = self.tokenizer.encode(completion, add_special_tokens=False)\n            if 150001 not in prompt:\n                import pdb; pdb.set_trace()\n        return {'prompt':prompt, 'completion':completion}\n\n    def __len__(self):\n        return len(self.pairs)", "\ndef collate_fn(batch):\n    input_ids = []\n    labels = []\n    position_ids = []\n    device='cuda:0'\n    _max_length = max([len(obj['prompt'])+len(obj['completion']) for obj in batch])\n    attention_mask = torch.ones((len(batch), _max_length, _max_length), device=device)\n    attention_mask.tril_()\n\n    for i, obj in enumerate(batch):\n        context_length = obj['prompt'].index(150004)\n        attention_mask[i, :, :context_length] = 1\n\n        to_pad = _max_length - len(obj['prompt']) - len(obj['completion'])\n\n        input_ids.append(obj['prompt'] + obj['completion'] + [tokenizer.pad_token_id] * to_pad)\n\n        position_ids.append(torch.stack(\n            [torch.arange(0, _max_length, device=device), \n            torch.concat([torch.zeros(context_length - 1, device=device), \n            torch.arange(0, _max_length - context_length + 1, device=device)])]).long()\n        )\n        labels.append(torch.tensor([-100] * len(obj['prompt']) + obj['completion'] + [-100] * to_pad, device=device).long())\n    attention_mask.unsqueeze_(1)\n    attention_mask = (attention_mask < 0.5).bool()\n    return {'input_ids': torch.tensor(input_ids).long(), \n            'attention_mask': attention_mask, \n            'labels': torch.stack(labels),\n            'position_ids':torch.stack(position_ids)}", "\ntrain_dataset = AlpacaDataset(pairs,tokenizer=tokenizer,)\ntrain_dataloader = DataLoader(dataset=train_dataset, collate_fn = collate_fn, shuffle=True, batch_size=Per_GPU_BATCH_SIZE)\n\n# check\nfor step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n    pass\n\nmodel = AutoModel.from_pretrained(\n    checkpoint, ", "model = AutoModel.from_pretrained(\n    checkpoint, \n    trust_remote_code=True,\n)\ndeepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=accumulate_step)\naccelerator = Accelerator(mixed_precision=mixed_precision, gradient_accumulation_steps=accumulate_step, deepspeed_plugin=deepspeed_plugin)\ndevice = accelerator.device\n\n\n### Insert LoRA to model\nclass QKV_layer(torch.nn.Module):\n    def __init__(self, in_features, out_features):\n        super(QKV_layer, self).__init__()\n        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n\n    def update(self, target_layer):\n        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n\n        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n\n        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n    \n    def forward(self, x):\n        q = self.linear_q(x)\n        k = self.linear_k(x)\n        v = self.linear_v(x)\n        return torch.concat([q,k,v], dim = -1)", "\n### Insert LoRA to model\nclass QKV_layer(torch.nn.Module):\n    def __init__(self, in_features, out_features):\n        super(QKV_layer, self).__init__()\n        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n\n    def update(self, target_layer):\n        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n\n        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n\n        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n    \n    def forward(self, x):\n        q = self.linear_q(x)\n        k = self.linear_k(x)\n        v = self.linear_v(x)\n        return torch.concat([q,k,v], dim = -1)", "\nfor key, module in model.named_modules():\n    if key.endswith('attention'):\n        if isinstance(module.query_key_value, peft.tuners.lora.LoraModel):\n            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value.model)\n        else:\n            # Here we split the query_key_value layer into three linear layer for LoRA. But you can also use merged linear.\n            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n            qkv_layer.update(module.query_key_value)\n            module.query_key_value = qkv_layer\n            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)", "\nlora.mark_only_lora_as_trainable(model)\n\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\ntrainable_params = sum([np.prod(p.size()) for p in model_parameters])\nnon_trainable_params = sum([np.prod(p.size()) for p in model_parameters])\nprint('trainable_params:{} ({:.2f}%), non_trainable_params:{}'.format(\n    trainable_params, trainable_params/non_trainable_params*100,non_trainable_params\n))\n", "))\n\n### Training\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=int(len(train_dataloader) / accumulate_step * warm_up_ratio),\n    num_training_steps=(int(len(train_dataloader) / accumulate_step) * NUM_EPOCHS),\n)", "    num_training_steps=(int(len(train_dataloader) / accumulate_step) * NUM_EPOCHS),\n)\nmodel, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\nmodel.to(device).train()\n\nfor epoch in range(NUM_EPOCHS):\n    total_loss = 0\n    for step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss_detach = outputs.loss.detach().cpu().float()\n            # t.set_description(f\"loss: {loss_detach}\")\n            t.set_postfix(loss=loss_detach.item())\n            total_loss += loss_detach\n            loss = outputs.loss\n\n            if accelerator.is_main_process:\n                if step % log_interval == 0:\n                    wandb.log({\n                        'train/loss': loss_detach.item(),\n                    })\n\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n    accelerator.wait_for_everyone()\n    if accelerator.is_main_process:\n        peft_model_id = f\"finetune_{epoch}\"\n        accelerator.save(lora.lora_state_dict(accelerator.unwrap_model(model)), f'{out_dir}/{peft_model_id}.pt')", "    "]}
{"filename": "tools/quant/custom_autotune.py", "chunked_list": ["#https://github.com/fpgaminer/GPTQ-triton\n\"\"\"\nMostly the same as the autotuner in Triton, but with a few changes like using 40 runs instead of 100.\n\"\"\"\n\nimport builtins\nimport math\nimport time\nfrom typing import Dict\n", "from typing import Dict\n\nimport triton\n\n\nclass Autotuner(triton.KernelInterface):\n\n    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, nearest_power_of_two: bool = False):\n        '''\n\t\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t\t'top_k': number of configs to bench\n\t\t\t'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n\t\t\t'nearest_power_of_two'(optional): whether to round key arguments to the nearest power of two when caching tuning results\n\t\t'''\n        if not configs:\n            self.configs = [triton.Config({}, num_warps=4, num_stages=2)]\n        else:\n            self.configs = configs\n        self.key_idx = [arg_names.index(k) for k in key]\n        self.nearest_power_of_two = nearest_power_of_two\n        self.cache = {}\n        # hook to reset all required tensor to zeros before relaunching a kernel\n        self.hook = lambda args: 0\n        if reset_to_zero is not None:\n            self.reset_idx = [arg_names.index(k) for k in reset_to_zero]\n\n            def _hook(args):\n                for i in self.reset_idx:\n                    args[i].zero_()\n\n            self.hook = _hook\n        self.arg_names = arg_names\n        # prune configs\n        if prune_configs_by:\n            perf_model, top_k = prune_configs_by['perf_model'], prune_configs_by['top_k']\n            if 'early_config_prune' in prune_configs_by:\n                early_config_prune = prune_configs_by['early_config_prune']\n        else:\n            perf_model, top_k, early_config_prune = None, None, None\n        self.perf_model, self.configs_top_k = perf_model, top_k\n        self.early_config_prune = early_config_prune\n        self.fn = fn\n\n    def _bench(self, *args, config, **meta):\n        # check for conflicts, i.e. meta-parameters both provided\n        # as kwargs and by the autotuner\n        conflicts = meta.keys() & config.kwargs.keys()\n        if conflicts:\n            raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\n                             \" Make sure that you don't re-define auto-tuned symbols.\")\n        # augment meta-parameters with tunable ones\n        current = dict(meta, **config.kwargs)\n\n        def kernel_call():\n            if config.pre_hook:\n                config.pre_hook(self.nargs)\n            self.hook(args)\n            self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n\n        try:\n            # In testings using only 40 reps seems to be close enough and it appears to be what PyTorch uses\n            # PyTorch also sets fast_flush to True, but I didn't see any speedup so I'll leave the default\n            return triton.testing.do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8), rep=40)\n        except triton.compiler.OutOfResources:\n            return (float('inf'), float('inf'), float('inf'))\n\n    def run(self, *args, **kwargs):\n        self.nargs = dict(zip(self.arg_names, args))\n        if len(self.configs) > 1:\n            key = tuple(args[i] for i in self.key_idx)\n\n            # This reduces the amount of autotuning by rounding the keys to the nearest power of two\n            # In my testing this gives decent results, and greatly reduces the amount of tuning required\n            if self.nearest_power_of_two:\n                key = tuple([2**int(math.log2(x) + 0.5) for x in key])\n\n            if key not in self.cache:\n                # prune configs\n                pruned_configs = self.prune_configs(kwargs)\n                bench_start = time.time()\n                timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n                bench_end = time.time()\n                self.bench_time = bench_end - bench_start\n                self.cache[key] = builtins.min(timings, key=timings.get)\n                self.hook(args)\n                self.configs_timings = timings\n            config = self.cache[key]\n        else:\n            config = self.configs[0]\n        self.best_config = config\n        if config.pre_hook is not None:\n            config.pre_hook(self.nargs)\n        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n\n    def prune_configs(self, kwargs):\n        pruned_configs = self.configs\n        if self.early_config_prune:\n            pruned_configs = self.early_config_prune(self.configs, self.nargs)\n        if self.perf_model:\n            top_k = self.configs_top_k\n            if isinstance(top_k, float) and top_k <= 1.0:\n                top_k = int(len(self.configs) * top_k)\n            if len(pruned_configs) > top_k:\n                est_timing = {config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages, num_warps=config.num_warps) for config in pruned_configs}\n                pruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n        return pruned_configs\n\n    def warmup(self, *args, **kwargs):\n        self.nargs = dict(zip(self.arg_names, args))\n        for config in self.prune_configs(kwargs):\n            self.fn.warmup(\n                *args,\n                num_warps=config.num_warps,\n                num_stages=config.num_stages,\n                **kwargs,\n                **config.kwargs,\n            )\n        self.nargs = None", "\n\ndef autotune(configs, key, prune_configs_by=None, reset_to_zero=None, nearest_power_of_two=False):\n    \"\"\"\n\tDecorator for auto-tuning a :code:`triton.jit`'d function.\n\t.. highlight:: python\n\t.. code-block:: python\n\t\t@triton.autotune(configs=[\n\t\t\ttriton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n\t\t\ttriton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n\t\t\t],\n\t\t\tkey=['x_size'] # the two above configs will be evaluated anytime\n\t\t\t\t\t\t\t# the value of x_size changes\n\t\t)\n\t\t@triton.jit\n\t\tdef kernel(x_ptr, x_size, **META):\n\t\t\tBLOCK_SIZE = META['BLOCK_SIZE']\n\t:note: When all the configurations are evaluated, the kernel will run multiple time.\n\t\t\tThis means that whatever value the kernel updates will be updated multiple times.\n\t\t\tTo avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n\t\t\treset the value of the provided tensor to `zero` before running any configuration.\n\t:param configs: a list of :code:`triton.Config` objects\n\t:type configs: list[triton.Config]\n\t:param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n\t:type key: list[str]\n\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t'top_k': number of configs to bench\n\t\t'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n\t:param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n\t:type reset_to_zero: list[str]\n\t\"\"\"\n\n    def decorator(fn):\n        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, nearest_power_of_two)\n\n    return decorator", "\n\ndef matmul248_kernel_config_pruner(configs, nargs):\n    \"\"\"\n    The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.\n    \"\"\"\n    m = max(2**int(math.ceil(math.log2(nargs['M']))), 16)\n    n = max(2**int(math.ceil(math.log2(nargs['N']))), 16)\n    k = max(2**int(math.ceil(math.log2(nargs['K']))), 16)\n\n    used = set()\n    for config in configs:\n        block_size_m = min(m, config.kwargs['BLOCK_SIZE_M'])\n        block_size_n = min(n, config.kwargs['BLOCK_SIZE_N'])\n        block_size_k = min(k, config.kwargs['BLOCK_SIZE_K'])\n        group_size_m = config.kwargs['GROUP_SIZE_M']\n\n        if (block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps) in used:\n            continue\n\n        used.add((block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps))\n        yield triton.Config({\n            'BLOCK_SIZE_M': block_size_m,\n            'BLOCK_SIZE_N': block_size_n,\n            'BLOCK_SIZE_K': block_size_k,\n            'GROUP_SIZE_M': group_size_m\n        },\n                            num_stages=config.num_stages,\n                            num_warps=config.num_warps)", ""]}
{"filename": "tools/quant/quant_linear.py", "chunked_list": ["import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import custom_bwd, custom_fwd\n\ntry:\n    import triton\n    import triton.language as tl\n    from . import custom_autotune\n    # code based https://github.com/fpgaminer/GPTQ-triton\n    @custom_autotune.autotune(\n        configs=[\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 256,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 128,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 128,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 32,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 64,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 128,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=2, num_warps=8),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 64,\n                'BLOCK_SIZE_K': 64,\n                'GROUP_SIZE_M': 8\n            }, num_stages=3, num_warps=8),\n            triton.Config({\n                'BLOCK_SIZE_M': 32,\n                'BLOCK_SIZE_N': 32,\n                'BLOCK_SIZE_K': 128,\n                'GROUP_SIZE_M': 8\n            }, num_stages=2, num_warps=4),\n        ],\n        key=['M', 'N', 'K'],\n        nearest_power_of_two=True,\n        prune_configs_by={\n            'early_config_prune': custom_autotune.matmul248_kernel_config_pruner,\n            'perf_model': None,\n            'top_k': None,\n        },\n    )\n    @triton.jit\n    def matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales, stride_zeros,\n                          NO_GROUP: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n        \"\"\"\n        Compute the matrix multiplication C = A x B.\n        A is of shape (M, K) float16\n        B is of shape (K//8, N) int32\n        C is of shape (M, N) float16\n        scales is of shape (G, N) float16\n        zeros is of shape (G, N) float16\n        g_ptr is of shape (K) int32 \n        \"\"\"\n        infearure_per_bits = 32 // bits\n\n        pid = tl.program_id(axis=0)\n        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n        group_id = pid // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + (pid % group_size_m)\n        pid_n = (pid % num_pid_in_group) // group_size_m\n\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        a_mask = (offs_am[:, None] < M)\n        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n        b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n        g_ptrs = g_ptr + offs_k\n        # shifter is used to extract the N bits of each element in the 32-bit word from B\n        scales_ptrs = scales_ptr + offs_bn[None, :]\n        zeros_ptrs = zeros_ptr + (offs_bn[None, :] // infearure_per_bits)\n\n        if NO_GROUP:\n            scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n            zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\n            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n            zeros = (zeros + 1)\n        \n        shifter = (offs_k % infearure_per_bits) * bits\n        zeros_shifter = (offs_bn % infearure_per_bits) * bits\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n        for k in range(0, num_pid_k):\n            g_idx = tl.load(g_ptrs)\n\n            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n            if not NO_GROUP: \n                scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n                zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\n                zeros = (zeros >> zeros_shifter[None, :]) & maxq\n                zeros = (zeros + 1)\n\n            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\n            # Now we need to unpack b (which is N-bit values) into 32-bit values\n            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n            b = (b - zeros) * scales  # Scale and shift\n\n            accumulator += tl.dot(a, b)\n            a_ptrs += BLOCK_SIZE_K\n            b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n            g_ptrs += BLOCK_SIZE_K\n\n        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n        c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n        tl.store(c_ptrs, accumulator, mask=c_mask)\n\n    @custom_autotune.autotune(configs=[\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 256,\n            'GROUP_SIZE_M': 8\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_SIZE_M': 128,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 128,\n            'GROUP_SIZE_M': 8\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 128,\n            'GROUP_SIZE_M': 8\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_SIZE_M': 128,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 32,\n            'GROUP_SIZE_M': 8\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 64,\n            'GROUP_SIZE_M': 8\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 32,\n            'BLOCK_SIZE_K': 128,\n            'GROUP_SIZE_M': 8\n        }, num_stages=2, num_warps=8),\n        triton.Config({\n            'BLOCK_SIZE_M': 64,\n            'BLOCK_SIZE_N': 64,\n            'BLOCK_SIZE_K': 64,\n            'GROUP_SIZE_M': 8\n        }, num_stages=3, num_warps=8),\n        triton.Config({\n            'BLOCK_SIZE_M': 32,\n            'BLOCK_SIZE_N': 128,\n            'BLOCK_SIZE_K': 32,\n            'GROUP_SIZE_M': 8\n        }, num_stages=2, num_warps=4),\n    ],\n                              key=['M', 'N', 'K'],\n                              nearest_power_of_two=True)\n    @triton.jit\n    def transpose_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales,\n                                    stride_zeros, NO_GROUP: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n        \"\"\"\n        Compute the matrix multiplication C = A x B.\n        A is of shape (M, N) float16\n        B is of shape (K//8, N) int32\n        C is of shape (M, K) float16\n        scales is of shape (G, N) float16\n        zeros is of shape (G, N) float16\n        g_ptr is of shape (K) int32 \n        \"\"\"\n        infearure_per_bits = 32 // bits\n\n        pid = tl.program_id(axis=0)\n        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        num_pid_in_group = GROUP_SIZE_M * num_pid_k\n        group_id = pid // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + (pid % group_size_m)\n        pid_k = (pid % num_pid_in_group) // group_size_m\n\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        offs_n = tl.arange(0, BLOCK_SIZE_N)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n        a_mask = (offs_am[:, None] < M)\n        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n        b_ptrs = b_ptr + ((offs_bk[:, None] // infearure_per_bits) * stride_bk + offs_n[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n        g_ptrs = g_ptr + offs_bk\n        g_idx = tl.load(g_ptrs)\n\n        # shifter is used to extract the N bits of each element in the 32-bit word from B\n        scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n        zeros_ptrs = zeros_ptr + (offs_n[None, :] // infearure_per_bits) + g_idx[:, None] * stride_zeros\n\n\n        if NO_GROUP:\n            scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n            zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\n            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n            zeros = (zeros + 1)\n        \n        shifter = (offs_bk % infearure_per_bits) * bits\n        zeros_shifter = (offs_n % infearure_per_bits) * bits\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n\n        for k in range(0, num_pid_n):\n            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n            if not NO_GROUP:\n                scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n                zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\n                zeros = (zeros >> zeros_shifter[None, :]) & maxq\n                zeros = (zeros + 1)\n\n            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\n            # Now we need to unpack b (which is N-bit values) into 32-bit values\n            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n            b = (b - zeros) * scales  # Scale and shift\n            b = tl.trans(b)\n\n            accumulator += tl.dot(a, b)\n            a_ptrs += BLOCK_SIZE_N\n            b_ptrs += BLOCK_SIZE_N\n            scales_ptrs += BLOCK_SIZE_N\n            zeros_ptrs += (BLOCK_SIZE_N // infearure_per_bits)\n\n        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :]\n        c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n        tl.store(c_ptrs, accumulator, mask=c_mask)\nexcept:\n    print('trioton not installed.')", "\n\ndef matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n    with torch.cuda.device(input.device):\n        output = torch.empty((input.shape[0], qweight.shape[1]), device='cuda', dtype=torch.float16)\n        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']), )\n        matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], input.shape[1], bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n                                qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0), no_group)\n        return output\n", "\n\ndef transpose_matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n    with torch.cuda.device(input.device):\n        output_dim = (qweight.shape[0] * 32) // bits\n        output = torch.empty((input.shape[0], output_dim), device='cuda', dtype=torch.float16)\n        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']), )\n        transpose_matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], output_dim, bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n                                          qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0), no_group)\n        return output", "\n\nclass QuantLinearFunction(torch.autograd.Function):\n\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float16)\n    def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n        output = matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n        ctx.save_for_backward(qweight, scales, qzeros, g_idx)\n        ctx.bits, ctx.maxq, ctx.no_group = bits, maxq, no_group\n        return output\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output):\n        qweight, scales, qzeros, g_idx = ctx.saved_tensors\n        bits, maxq, no_group = ctx.bits, ctx.maxq, ctx.no_group\n        grad_input = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = transpose_matmul248(grad_output, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n        return grad_input, None, None, None, None, None, None", "\n\nclass QuantLinear(nn.Module):\n\n    def __init__(self, bits, groupsize, infeatures, outfeatures, bias):\n        super().__init__()\n        if bits not in [2, 4, 8]:\n            raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n        self.infeatures = infeatures\n        self.outfeatures = outfeatures\n        self.bits = bits\n        self.maxq = 2**self.bits - 1\n        self.groupsize = groupsize if groupsize != -1 else infeatures\n        self.no_group = math.ceil(infeatures / self.groupsize) == 1\n        \n        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\n        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))\n        self.register_buffer('scales', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))\n        self.register_buffer('g_idx', torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32))\n        if bias:\n            self.register_buffer('bias', torch.zeros((outfeatures), dtype=torch.float16))\n        else:\n            self.bias = None\n\n    def pack(self, linear, scales, zeros, g_idx=None):\n        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx\n\n        scales = scales.t().contiguous()\n        zeros = zeros.t().contiguous()\n        scale_zeros = zeros * scales\n        self.scales = scales.clone().half()\n        if linear.bias is not None:\n            self.bias = linear.bias.clone().half()\n\n        intweight = []\n        for idx in range(self.infeatures):\n            intweight.append(torch.round((linear.weight.data[:, idx] + scale_zeros[self.g_idx[idx]]) / self.scales[self.g_idx[idx]]).to(torch.int)[:, None])\n        intweight = torch.cat(intweight, dim=1)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.numpy().astype(np.uint32)\n        qweight = np.zeros((intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32)\n        i = 0\n        row = 0\n        while row < qweight.shape[0]:\n            if self.bits in [2, 4, 8]:\n                for j in range(i, i + (32 // self.bits)):\n                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n                i += 32 // self.bits\n                row += 1\n            else:\n                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n\n        qweight = qweight.astype(np.int32)\n        self.qweight = torch.from_numpy(qweight)\n\n        zeros -= 1\n        zeros = zeros.numpy().astype(np.uint32)\n        qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32)\n        i = 0\n        col = 0\n        while col < qzeros.shape[1]:\n            if self.bits in [2, 4, 8]:\n                for j in range(i, i + (32 // self.bits)):\n                    qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))\n                i += 32 // self.bits\n                col += 1\n            else:\n                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n\n        qzeros = qzeros.astype(np.int32)\n        self.qzeros = torch.from_numpy(qzeros)\n\n    def forward(self, x):\n        out_shape = x.shape[:-1] + (self.outfeatures, )\n        out = QuantLinearFunction.apply(x.reshape(-1, x.shape[-1]), self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq, self.no_group)\n        out = out + self.bias if self.bias is not None else out\n        return out.reshape(out_shape)", "\n\ndef make_quant_linear(module, names, bits, groupsize, name=''):\n    if isinstance(module, QuantLinear):\n        return\n    for attr in dir(module):\n        tmp = getattr(module, attr)\n        name1 = name + '.' + attr if name != '' else attr\n        if name1 in names:\n            delattr(module, attr)\n            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))\n    for name1, child in module.named_children():\n        make_quant_linear(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1)", "\n\ndef autotune_warmup_linear(model, transpose=False):\n    \"\"\"\n    Pre-tunes the quantized kernel\n    \"\"\"\n    from tqdm import tqdm\n\n    kn_values = {}\n\n    for _, m in model.named_modules():\n        if not isinstance(m, QuantLinear):\n            continue\n\n        k = m.infeatures\n        n = m.outfeatures\n\n        if (k, n) not in kn_values:\n            kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq, m.no_group)\n\n    print(f'Found {len(kn_values)} unique KN Linear values.')\n\n    print('Warming up autotune cache ...')\n    with torch.no_grad():\n        for m in tqdm(range(0, 12)):\n            m = 2**m  # [1, 2048]\n            for (k, n), (qweight, scales, qzeros, g_idx, bits, maxq, no_group) in kn_values.items():\n                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n                matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n                if transpose:\n                    a = torch.randn(m, n, dtype=torch.float16, device='cuda')\n                    transpose_matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n    del kn_values", ""]}
{"filename": "tools/quant/fused_attn.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb\nfrom .quant_linear import *\n\n\nclass QuantLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size,\n        num_heads,\n        qkv_proj,\n        o_proj,\n        rotary_emb,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        if (self.head_dim * num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                             f\" and `num_heads`: {num_heads}).\")\n        self.qkv_proj = qkv_proj\n        self.o_proj = o_proj\n        self.rotary_emb = rotary_emb\n\n    def _shape(self, tensor, seq_len, bsz):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(self, hidden_states, past_key_value=None, attention_mask=None, position_ids=None, output_attentions=False, use_cache=False):\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, q_len, _ = hidden_states.size()\n\n        qkv_states = self.qkv_proj(hidden_states)\n        query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        #transformers==4.29.0:\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        #transformers==4.28.0:\n        # kv_seq_len = key_states.shape[-2]\n        # offset = 0\n        # if past_key_value is not None:\n        #     offset = past_key_value[0].shape[-2]\n        #     kv_seq_len += offset\n        # cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, offset)\n\n        # [bsz, nh, t, hd]\n\n        is_causal = past_key_value is None\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        with torch.backends.cuda.sdp_kernel(enable_math=False):\n            attn_output = F.scaled_dot_product_attention(query_states, key_states, value_states, is_causal=is_causal)\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value", "\nclass QuantLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size,\n        num_heads,\n        qkv_proj,\n        o_proj,\n        rotary_emb,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        if (self.head_dim * num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                             f\" and `num_heads`: {num_heads}).\")\n        self.qkv_proj = qkv_proj\n        self.o_proj = o_proj\n        self.rotary_emb = rotary_emb\n\n    def _shape(self, tensor, seq_len, bsz):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(self, hidden_states, past_key_value=None, attention_mask=None, position_ids=None, output_attentions=False, use_cache=False):\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, q_len, _ = hidden_states.size()\n\n        qkv_states = self.qkv_proj(hidden_states)\n        query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        #transformers==4.29.0:\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        #transformers==4.28.0:\n        # kv_seq_len = key_states.shape[-2]\n        # offset = 0\n        # if past_key_value is not None:\n        #     offset = past_key_value[0].shape[-2]\n        #     kv_seq_len += offset\n        # cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, offset)\n\n        # [bsz, nh, t, hd]\n\n        is_causal = past_key_value is None\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        with torch.backends.cuda.sdp_kernel(enable_math=False):\n            attn_output = F.scaled_dot_product_attention(query_states, key_states, value_states, is_causal=is_causal)\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value", "\n\ndef make_quant_attn(model):\n    \"\"\"\n    Replace all LlamaAttention modules with QuantLlamaAttention modules, fusing the q, k, v projections.\n    \"\"\"\n    for name, m in model.named_modules():\n        if not isinstance(m, LlamaAttention):\n            continue\n\n        q_proj = m.q_proj\n        k_proj = m.k_proj\n        v_proj = m.v_proj\n\n        qweights = torch.cat([q_proj.qweight, k_proj.qweight, v_proj.qweight], dim=1)\n        qzeros = torch.cat([q_proj.qzeros, k_proj.qzeros, v_proj.qzeros], dim=1)\n        scales = torch.cat([q_proj.scales, k_proj.scales, v_proj.scales], dim=1)\n        g_idx = torch.cat([q_proj.g_idx, k_proj.g_idx, v_proj.g_idx], dim=0)\n        bias = torch.cat([q_proj.bias, k_proj.bias, v_proj.bias], dim=0) if q_proj.bias is not None else None\n\n        qkv_layer = QuantLinear(q_proj.bits, q_proj.groupsize, q_proj.infeatures, q_proj.outfeatures + k_proj.outfeatures + v_proj.outfeatures, True if q_proj.bias is not None else False)\n        qkv_layer.qweight = qweights\n        qkv_layer.qzeros = qzeros\n        qkv_layer.scales = scales\n        qkv_layer.g_idx = g_idx\n        qkv_layer.bias = bias\n\n        attn = QuantLlamaAttention(m.hidden_size, m.num_heads, qkv_layer, m.o_proj, m.rotary_emb)\n\n        if '.' in name:\n            parent_name = name.rsplit('.', 1)[0]\n            child_name = name[len(parent_name) + 1:]\n            parent = model.get_submodule(parent_name)\n        else:\n            parent_name = ''\n            parent = model\n            child_name = name\n\n        #print(f\"Replacing {name} with quant_attn; parent: {parent_name}, child's name: {child_name}\")\n\n        setattr(parent, child_name, attn)", ""]}
{"filename": "tools/quant/__init__.py", "chunked_list": ["from .quantizer import Quantizer\nfrom .fused_attn import QuantLlamaAttention, make_quant_attn\nfrom .fused_mlp import QuantLlamaMLP, make_fused_mlp, autotune_warmup_fused\nfrom .quant_linear import QuantLinear, make_quant_linear, autotune_warmup_linear\n"]}
{"filename": "tools/quant/fused_mlp.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom transformers.models.llama.modeling_llama import LlamaMLP\n\ntry:\n    import triton\n    import triton.language as tl\n    from . import custom_autotune\n\n    # code based https://github.com/fpgaminer/GPTQ-triton\n    @custom_autotune.autotune(\n        configs=[\n            triton.Config({\n                'BLOCK_SIZE_M': 256,\n                'BLOCK_SIZE_N': 64,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 256,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 128,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 64,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 128,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 32,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),  # 3090\n            triton.Config({\n                'BLOCK_SIZE_M': 128,\n                'BLOCK_SIZE_N': 16,\n                'BLOCK_SIZE_K': 32,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),  # 3090\n            triton.Config({\n                'BLOCK_SIZE_M': 32,\n                'BLOCK_SIZE_N': 32,\n                'BLOCK_SIZE_K': 128,\n                'GROUP_SIZE_M': 8\n            }, num_stages=2, num_warps=4),  # 3090\n            triton.Config({\n                'BLOCK_SIZE_M': 64,\n                'BLOCK_SIZE_N': 16,\n                'BLOCK_SIZE_K': 64,\n                'GROUP_SIZE_M': 8\n            }, num_stages=4, num_warps=4),  # 3090\n        ],\n        key=['M', 'N', 'K'],\n        nearest_power_of_two=True,\n        prune_configs_by={\n            'early_config_prune': custom_autotune.matmul248_kernel_config_pruner,\n            'perf_model': None,\n            'top_k': None,\n        },\n    )\n    @triton.jit\n    def fusedmatmul_248_kernel(a_ptr, c_ptr, b1_ptr, scales1_ptr, zeros1_ptr, g1_ptr, b2_ptr, scales2_ptr, zeros2_ptr, g2_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n                               stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n        \"\"\"\n        Computes: C = silu(A * B1) * (A * B2)\n        A is of shape (M, K) float16\n        B is of shape (K//8, N) int32\n        C is of shape (M, N) float16\n        scales is of shape (1, N) float16\n        zeros is of shape (1, N//8) int32\n        \"\"\"\n        infearure_per_bits = 32 // bits\n\n        pid = tl.program_id(axis=0)\n        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n        group_id = pid // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + (pid % group_size_m)\n        pid_n = (pid % num_pid_in_group) // group_size_m\n\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        a_mask = (offs_am[:, None] < M)\n        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n        b1_ptrs = b1_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)\n        b2_ptrs = b2_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)\n        g1_ptrs = g1_ptr + offs_k\n        g2_ptrs = g2_ptr + offs_k\n        # shifter is used to extract the N bits of each element in the 32-bit word from B\n        scales1_ptrs = scales1_ptr + offs_bn[None, :]\n        scales2_ptrs = scales2_ptr + offs_bn[None, :]\n        zeros1_ptrs = zeros1_ptr + (offs_bn[None, :] // infearure_per_bits)\n        zeros2_ptrs = zeros2_ptr + (offs_bn[None, :] // infearure_per_bits)\n\n        shifter = (offs_k % infearure_per_bits) * bits\n        zeros_shifter = (offs_bn % infearure_per_bits) * bits\n        accumulator1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        accumulator2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, num_pid_k):\n            g1_idx = tl.load(g1_ptrs)\n            g2_idx = tl.load(g2_ptrs)\n\n            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n            scales1 = tl.load(scales1_ptrs + g1_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n            scales2 = tl.load(scales2_ptrs + g2_idx[:, None] * stride_scales)\n\n            zeros1 = tl.load(zeros1_ptrs + g1_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n            zeros1 = (zeros1 >> zeros_shifter[None, :]) & maxq\n            zeros1 = (zeros1 + 1)\n\n            zeros2 = tl.load(zeros2_ptrs + g2_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n            zeros2 = (zeros2 >> zeros_shifter[None, :]) & maxq\n            zeros2 = (zeros2 + 1)\n\n            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n            b1 = tl.load(b1_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n            b2 = tl.load(b2_ptrs)\n\n            # Now we need to unpack b (which is N-bit values) into 32-bit values\n            b1 = (b1 >> shifter[:, None]) & maxq  # Extract the N-bit values\n            b1 = (b1 - zeros1) * scales1  # Scale and shift\n            accumulator1 += tl.dot(a, b1)\n\n            b2 = (b2 >> shifter[:, None]) & maxq\n            b2 = (b2 - zeros2) * scales2\n            accumulator2 += tl.dot(a, b2)\n\n            a_ptrs += BLOCK_SIZE_K\n            b1_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n            b2_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n            g1_ptrs += BLOCK_SIZE_K\n            g2_ptrs += BLOCK_SIZE_K\n\n        accumulator1 = silu(accumulator1)\n        c = accumulator1 * accumulator2\n        c = c.to(tl.float16)\n        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n        c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n        tl.store(c_ptrs, c, mask=c_mask)\n\n    @triton.jit\n    def silu(x):\n        return x * tl.sigmoid(x)\nexcept:\n    print('triton not installed.')", "\n\nclass QuantLlamaMLP(nn.Module):\n\n    def __init__(\n        self,\n        gate_proj,\n        down_proj,\n        up_proj,\n    ):\n        super().__init__()\n        self.register_buffer('gate_proj_qweight', gate_proj.qweight)\n        self.register_buffer('gate_proj_scales', gate_proj.scales)\n        self.register_buffer('gate_proj_qzeros', gate_proj.qzeros)\n        self.register_buffer('gate_proj_g_idx', gate_proj.g_idx)\n        self.register_buffer('up_proj_qweight', up_proj.qweight)\n        self.register_buffer('up_proj_scales', up_proj.scales)\n        self.register_buffer('up_proj_qzeros', up_proj.qzeros)\n        self.register_buffer('up_proj_g_idx', up_proj.g_idx)\n\n        self.infeatures = gate_proj.infeatures\n        self.intermediate_size = gate_proj.outfeatures\n        self.outfeatures = down_proj.outfeatures\n        self.bits = gate_proj.bits\n        self.maxq = gate_proj.maxq\n\n        self.down_proj = down_proj\n\n    def forward(self, x):\n        return self.down_proj(self.triton_llama_mlp(x))\n\n    def triton_llama_mlp(self, x):\n        with torch.cuda.device(x.device):\n            out_shape = x.shape[:-1] + (self.intermediate_size, )\n            x = x.reshape(-1, x.shape[-1])\n            M, K = x.shape\n            N = self.intermediate_size\n            c = torch.empty((M, N), device='cuda', dtype=torch.float16)\n            grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n            fusedmatmul_248_kernel[grid](x, c, self.gate_proj_qweight, self.gate_proj_scales, self.gate_proj_qzeros, self.gate_proj_g_idx, self.up_proj_qweight, self.up_proj_scales,\n                                         self.up_proj_qzeros, self.up_proj_g_idx, M, N, K, self.bits, self.maxq, x.stride(0), x.stride(1), self.gate_proj_qweight.stride(0),\n                                         self.gate_proj_qweight.stride(1), c.stride(0), c.stride(1), self.gate_proj_scales.stride(0), self.gate_proj_qzeros.stride(0))\n            c = c.reshape(out_shape)\n            return c\n\n    def fused2cuda(self):\n        self.gate_proj_qweight = self.gate_proj_qweight.cuda()\n        self.gate_proj_scales = self.gate_proj_scales.cuda()\n        self.gate_proj_qzeros = self.gate_proj_qzeros.cuda()\n        self.gate_proj_g_idx = self.gate_proj_g_idx.cuda()\n        self.up_proj_qweight = self.up_proj_qweight.cuda()\n        self.up_proj_scales = self.up_proj_scales.cuda()\n        self.up_proj_qzeros = self.up_proj_qzeros.cuda()\n        self.up_proj_g_idx = self.up_proj_g_idx.cuda()\n\n    def fused2cpu(self):\n        self.gate_proj_qweight = self.gate_proj_qweight.cpu()\n        self.gate_proj_scales = self.gate_proj_scales.cpu()\n        self.gate_proj_qzeros = self.gate_proj_qzeros.cpu()\n        self.gate_proj_g_idx = self.gate_proj_g_idx.cpu()\n        self.up_proj_qweight = self.up_proj_qweight.cpu()\n        self.up_proj_scales = self.up_proj_scales.cpu()\n        self.up_proj_qzeros = self.up_proj_qzeros.cpu()\n        self.up_proj_g_idx = self.up_proj_g_idx.cpu()", "\n\ndef make_fused_mlp(m, parent_name=''):\n    \"\"\"\n    Replace all LlamaMLP modules with QuantLlamaMLP modules, which fuses many of the operations.\n    \"\"\"\n    if isinstance(m, LlamaMLP):\n        return QuantLlamaMLP(m.gate_proj, m.down_proj, m.up_proj)\n\n    for name, child in m.named_children():\n        child = make_fused_mlp(child, parent_name=f\"{parent_name}.{name}\")\n\n        if isinstance(child, QuantLlamaMLP):\n            setattr(m, name, child)\n    return m", "\n\ndef autotune_warmup_fused(model):\n    \"\"\"\n    Pre-tunes the quantized kernel\n    \"\"\"\n    from tqdm import tqdm\n\n    kn_values = {}\n\n    for _, m in model.named_modules():\n        if not isinstance(m, QuantLlamaMLP):\n            continue\n\n        k = m.infeatures\n        n = m.intermediate_size\n\n        m.fused2cuda()\n        if (k, n) not in kn_values:\n            kn_values[(k, n)] = m\n\n    print(f'Found {len(kn_values)} unique fused mlp KN values.')\n\n    print('Warming up autotune cache ...')\n    with torch.no_grad():\n        for m in tqdm(range(0, 12)):\n            m = 2**m  # [1, 2048]\n            for (k, n), (modules) in kn_values.items():\n                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n                modules.triton_llama_mlp(a)\n\n        for (k, n), (modules) in kn_values.items():\n            a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n            modules.fused2cpu()\n    del kn_values", ""]}
{"filename": "tools/quant/quantizer.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport math\n\n\nclass Quantizer(nn.Module):\n\n    def __init__(self, shape=1):\n        super(Quantizer, self).__init__()\n        self.register_buffer('maxq', torch.tensor(0))\n        self.register_buffer('scale', torch.zeros(shape))\n        self.register_buffer('zero', torch.zeros(shape))\n\n    def configure(self, bits, perchannel=False, sym=True, mse=False, norm=2.4, grid=100, maxshrink=.8, trits=False):\n\n        self.maxq = torch.tensor(2**bits - 1)\n        self.perchannel = perchannel\n        self.sym = sym\n        self.mse = mse\n        self.norm = norm\n        self.grid = grid\n        self.maxshrink = maxshrink\n        if trits:\n            self.maxq = torch.tensor(-1)\n        self.scale = torch.zeros_like(self.scale)\n\n    def _quantize(self, x, scale, zero, maxq):\n        if maxq < 0:\n            return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero\n        q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n        return scale * (q - zero)\n\n    def find_params(self, x, weight=False):\n        dev = x.device\n        self.maxq = self.maxq.to(dev)\n\n        shape = x.shape\n        if self.perchannel:\n            if weight:\n                x = x.flatten(1)\n            else:\n                if len(shape) == 4:\n                    x = x.permute([1, 0, 2, 3])\n                    x = x.flatten(1)\n                if len(shape) == 3:\n                    x = x.reshape((-1, shape[-1])).t()\n                if len(shape) == 2:\n                    x = x.t()\n        else:\n            x = x.flatten().unsqueeze(0)\n\n        tmp = torch.zeros(x.shape[0], device=dev)\n        xmin = torch.minimum(x.min(1)[0], tmp)\n        xmax = torch.maximum(x.max(1)[0], tmp)\n\n        if self.sym:\n            xmax = torch.maximum(torch.abs(xmin), xmax)\n            tmp = xmin < 0\n            if torch.any(tmp):\n                xmin[tmp] = -xmax[tmp]\n        tmp = (xmin == 0) & (xmax == 0)\n        xmin[tmp] = -1\n        xmax[tmp] = +1\n\n        if self.maxq < 0:\n            self.scale = xmax\n            self.zero = xmin\n        else:\n            self.scale = (xmax - xmin) / self.maxq\n            if self.sym:\n                self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n            else:\n                self.zero = torch.round(-xmin / self.scale)\n\n        if self.mse:\n            best = torch.full([x.shape[0]], float('inf'), device=dev)\n            for i in range(int(self.maxshrink * self.grid)):\n                p = 1 - i / self.grid\n                xmin1 = p * xmin\n                xmax1 = p * xmax\n                scale1 = (xmax1 - xmin1) / self.maxq\n                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n                q = self._quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n                q -= x\n                q.abs_()\n                q.pow_(self.norm)\n                err = torch.sum(q, 1)\n                tmp = err < best\n                if torch.any(tmp):\n                    best[tmp] = err[tmp]\n                    self.scale[tmp] = scale1[tmp]\n                    self.zero[tmp] = zero1[tmp]\n        if not self.perchannel:\n            if weight:\n                tmp = shape[0]\n            else:\n                tmp = shape[1] if len(shape) != 3 else shape[2]\n            self.scale = self.scale.repeat(tmp)\n            self.zero = self.zero.repeat(tmp)\n\n        if weight:\n            shape = [-1] + [1] * (len(shape) - 1)\n            self.scale = self.scale.reshape(shape)\n            self.zero = self.zero.reshape(shape)\n            return\n        if len(shape) == 4:\n            self.scale = self.scale.reshape((1, -1, 1, 1))\n            self.zero = self.zero.reshape((1, -1, 1, 1))\n        if len(shape) == 3:\n            self.scale = self.scale.reshape((1, 1, -1))\n            self.zero = self.zero.reshape((1, 1, -1))\n        if len(shape) == 2:\n            self.scale = self.scale.unsqueeze(0)\n            self.zero = self.zero.unsqueeze(0)\n\n    def quantize(self, x):\n        if self.ready():\n            return self._quantize(x, self.scale, self.zero, self.maxq)\n\n        return x\n\n    def enabled(self):\n        return self.maxq > 0\n\n    def ready(self):\n        return torch.all(self.scale != 0)", ""]}
