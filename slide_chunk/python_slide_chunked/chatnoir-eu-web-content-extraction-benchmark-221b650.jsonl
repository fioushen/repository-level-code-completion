{"filename": "src/extraction_benchmark/wceb.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport click\nfrom extraction_benchmark.cli import *\n\n", "\n\n@click.group()\ndef main():\n    \"\"\"\n    Web Content Extraction Benchmark.\n\n    Reproduction study of various main content extraction / boilerplate removal tools from\n    the scientific literature and the open source community.\n    \"\"\"\n    pass", "\n\nmain.add_command(complexity)\nmain.add_command(eval)\nmain.add_command(extract)\nmain.add_command(convert_datasets)\n\n\nif __name__ == '__main__':\n    main()", "if __name__ == '__main__':\n    main()\n"]}
{"filename": "src/extraction_benchmark/plt.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom matplotlib.pyplot import *\n\nrcParams['figure.dpi'] = 200", "\nrcParams['figure.dpi'] = 200\n\nrcParams['pdf.fonttype'] = 42\nrcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']\nrcParams['font.family'] = 'sans-serif'\n\nMEDIAN_BAR_COLOR = '#e68a38'\nERROR_BAR_COLOR = '#4d4d4d'\n", "ERROR_BAR_COLOR = '#4d4d4d'\n\nrcParams['errorbar.capsize'] = 4\nrcParams['boxplot.meanprops.color'] = 'pink'\nrcParams['boxplot.flierprops.marker'] = '.'\n\n# Lighter version of tab10\nrcParams['axes.prop_cycle'] = cycler(color=['#6caeda', '#ff993e', '#56b356'])\n", ""]}
{"filename": "src/extraction_benchmark/globals.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom extraction_benchmark.paths import *\nfrom extraction_benchmark.extractors import list_extractors\n\n_DATASET_FRIENDLY_NAME_MAP = {", "\n_DATASET_FRIENDLY_NAME_MAP = {\n    'cetd': 'CETD',\n    'cleaneval': 'CleanEval',\n    'cleanportaleval': 'CleanPortalEval',\n    'dragnet': 'Dragnet',\n    'google-trends-2017': 'Google-Trends',\n    'l3s-gn1': 'L3S-GN1',\n    'readability': 'Readability',\n    'scrapinghub': 'Scrapinghub'", "    'readability': 'Readability',\n    'scrapinghub': 'Scrapinghub'\n}\n\nif os.path.isdir(DATASET_RAW_PATH):\n    DATASETS = {k: _DATASET_FRIENDLY_NAME_MAP.get(k, k) for k in os.listdir(DATASET_RAW_PATH)\n                if os.path.isdir(os.path.join(DATASET_RAW_PATH, k))}\nelse:\n    DATASETS = {}\n", "\n_MODEL_FRIENDLY_NAME_MAP = dict(\n    ensemble_best='(Best only)',\n    ensemble_weighted='(Best weighted)',\n    ensemble_majority='(Majority all)',\n\n    bs4='BS4',\n    boilernet='BoilerNet',\n    boilerpipe='Boilerpipe',\n    bte='BTE',", "    boilerpipe='Boilerpipe',\n    bte='BTE',\n    dragnet='Dragnet',\n    extractnet='ExtractNet',\n    go_domdistiller='DOM Distiller',\n    goose3='Goose3',\n    justext='jusText',\n    lxml_cleaner='lxml Cleaner',\n    news_please='news-please',\n    newspaper3k='Newspaper3k',", "    news_please='news-please',\n    newspaper3k='Newspaper3k',\n    readability='Readability',\n    resiliparse='Resiliparse',\n    trafilatura='Trafilatura',\n    web2text='Web2Text',\n    xpath_text='XPath Text',\n)\n\nMODELS = {k: _MODEL_FRIENDLY_NAME_MAP.get(k, k)", "\nMODELS = {k: _MODEL_FRIENDLY_NAME_MAP.get(k, k)\n          for k in list_extractors(names_only=True, include_ensembles=False)}\nMODELS_ALL = {k: _MODEL_FRIENDLY_NAME_MAP.get(k, k)\n              for k in list_extractors(names_only=True, include_ensembles=True)}\nMODELS_ENSEMBLE = [m for m in MODELS_ALL if m.startswith('ensemble_')]\nMODELS_BASELINE = ['bs4', 'html_text', 'inscriptis', 'lxml_cleaner', 'xpath_text']\n\nSCORES = [\n    'rouge',", "SCORES = [\n    'rouge',\n    'levenshtein'\n]\n\nCOMPLEXITIES = [\n    'low',\n    'medium',\n    'high'\n]", "    'high'\n]\n"]}
{"filename": "src/extraction_benchmark/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"]}
{"filename": "src/extraction_benchmark/eval.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom itertools import pairwise, product\nimport math\nfrom multiprocessing import get_context\n", "from multiprocessing import get_context\n\nimport click\nfrom Levenshtein import ratio as levenshtein_ratio\nimport pandas as pd\nfrom rouge_score import rouge_scorer, tokenizers\nfrom tqdm import tqdm\n\nfrom extraction_benchmark.globals import *\nfrom extraction_benchmark import plt", "from extraction_benchmark.globals import *\nfrom extraction_benchmark import plt\nfrom extraction_benchmark.util import jsonl_to_dict, read_jsonl, tokenize_ws\n\n\nclass Tokenizer(tokenizers.Tokenizer):\n    def tokenize(self, text):\n        return tokenize_ws(text)\n\n\ndef rouge_eval(key, model, dataset, target, pred):\n    rouge = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=False, split_summaries=True, tokenizer=Tokenizer())\n\n    scores = []\n    score = rouge.score(target, pred)\n    for s in score:\n        t = dict()\n        t['hash_key'] = key\n        t['model'] = model\n        t['prec'] = score[s].precision\n        t['rec'] = score[s].recall\n        t['f1'] = score[s].fmeasure\n        t['scorer'] = s\n        t['dataset'] = dataset\n\n        if target.strip() == '':\n            t['rec'] = 1.0\n            if pred.strip() == '':\n                t['prec'] = 1.0\n                t['f1'] = 1.0\n\n        scores.append(t)\n\n    return scores", "\n\ndef rouge_eval(key, model, dataset, target, pred):\n    rouge = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=False, split_summaries=True, tokenizer=Tokenizer())\n\n    scores = []\n    score = rouge.score(target, pred)\n    for s in score:\n        t = dict()\n        t['hash_key'] = key\n        t['model'] = model\n        t['prec'] = score[s].precision\n        t['rec'] = score[s].recall\n        t['f1'] = score[s].fmeasure\n        t['scorer'] = s\n        t['dataset'] = dataset\n\n        if target.strip() == '':\n            t['rec'] = 1.0\n            if pred.strip() == '':\n                t['prec'] = 1.0\n                t['f1'] = 1.0\n\n        scores.append(t)\n\n    return scores", "\n\ndef levenshtein_eval(key, model, dataset, target, pred):\n    tokenizer = Tokenizer()\n    target = tokenizer.tokenize(target)\n    pred = tokenizer.tokenize(pred)\n    return [dict(\n        hash_key=key,\n        model=model,\n        dist=levenshtein_ratio(target, pred),\n        scorer='levenshtein',\n        dataset=dataset\n    )]", "\n\ndef _eval_expand_args(args):\n    scorer, model, dataset, answer_path, gt_path = args\n\n    if scorer == 'rouge':\n        scorer_func = rouge_eval\n    elif scorer == 'levenshtein':\n        scorer_func = levenshtein_eval\n    else:\n        raise ValueError('Illegal scorer')\n\n    ground_truth = jsonl_to_dict(gt_path)\n    df = pd.DataFrame()\n    for model_answer in read_jsonl(answer_path):\n        if model_answer['page_id'] not in ground_truth:\n            continue\n        target = ground_truth[model_answer['page_id']].get('plaintext') or ''\n        pred = model_answer.get('plaintext') or ''\n        df = pd.concat([df, pd.DataFrame(scorer_func(model_answer['page_id'], model, dataset, target, pred))])\n\n    store_path = os.path.join(METRICS_PATH, scorer, dataset)\n    os.makedirs(store_path, exist_ok=True)\n    df.to_csv(os.path.join(store_path, f'{scorer}_{model}.csv'), index=False)", "\n\ndef calculcate_scores(metrics, datasets, models, parallelism):\n    \"\"\"\n    Calculate performance scores for pages against the ground truth.\n\n    :param metrics: list of performance scores to calculate (``\"rouge\"`` or ``\"levenshtein\"``)\n    :param datasets: list of dataset names\n    :param models: list of models to evaluate\n    :param parallelism: number of parallel workers to run\n    \"\"\"\n    jobs = []\n    for ds in tqdm(datasets, desc='Loading extractions', leave=False):\n        ground_truth_path = os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl')\n        if not os.path.isfile(ground_truth_path):\n            continue\n\n        for model in models:\n            model_answer_path = os.path.join(MODEL_OUTPUTS_PATH, ds,  f'{model}.jsonl')\n            if os.path.isfile(model_answer_path):\n                jobs.extend([met, model, ds, model_answer_path, ground_truth_path] for met in metrics)\n\n    with get_context('spawn').Pool(processes=parallelism) as pool:\n        for _ in tqdm(pool.imap_unordered(_eval_expand_args, jobs),\n                      total=len(jobs), desc='Evaluating model answers'):\n            pass", "\n\ndef _layout_ax(ax, angle_xticks=True, hlines=True):\n    ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n    if hlines:\n        for y in ax.get_yticks():\n            ax.axhline(y, linewidth=0.25, color='lightgrey', zorder=-1)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    if angle_xticks:\n        ax.set_xticks(ax.get_xticks())\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')", "\n\ndef _map_model_label(label):\n    if label.get_text() in MODELS_ENSEMBLE:\n        label.set_color('#1767b0')\n    elif label.get_text() in MODELS_BASELINE:\n        label.set_color('gray')\n    label.set_text(MODELS_ALL.get(label.get_text(), label.get_text()))\n    return label\n", "\n\ndef _map_axis_tick_labels(axis):\n    ticklabels = [_map_model_label(t) for t in axis.get_ticklabels()]\n    axis.set_ticks(range(len(ticklabels)))\n    axis.set_ticklabels(ticklabels)\n\n\ndef _draw_performance_boxsubplot(ax, model_scores, xlabels, ylabel):\n    ax.boxplot(\n        model_scores,\n        positions=range(len(xlabels)),\n        labels=xlabels,\n        showfliers=False\n    )\n    _map_axis_tick_labels(ax.xaxis)\n    ax.set_ylabel(ylabel)\n    ax.set_ylim((-0.1, 1.1))\n    _layout_ax(ax)", "def _draw_performance_boxsubplot(ax, model_scores, xlabels, ylabel):\n    ax.boxplot(\n        model_scores,\n        positions=range(len(xlabels)),\n        labels=xlabels,\n        showfliers=False\n    )\n    _map_axis_tick_labels(ax.xaxis)\n    ax.set_ylabel(ylabel)\n    ax.set_ylim((-0.1, 1.1))\n    _layout_ax(ax)", "\n\ndef _draw_performance_barsubplot(ax, model_scores, lower_err, upper_err, xlabels, ylabel):\n    ax.bar(\n        xlabels,\n        model_scores,\n        width=0.7,\n        yerr=(lower_err, upper_err),\n        error_kw=dict(lw=0.75, capthick=0.75, ecolor=plt.ERROR_BAR_COLOR),\n    )\n    _map_axis_tick_labels(ax.xaxis)\n    ax.set_ylabel(ylabel)\n    ax.set_ylim((0.0, 1.1))\n    ax.set_xlim((-0.7, len(xlabels) - 0.3))\n    _layout_ax(ax)", "\n\ndef _draw_performance_plot(plot_type, data, layout, suptitle, score_name):\n    fig, axs = plt.subplots(*layout, figsize=(9.5, 3.3 * len(data)))\n\n    if layout == (1, 1):\n        axs = [axs]\n    for ax, d in zip(axs, data):\n        if plot_type == 'box':\n            _draw_performance_boxsubplot(ax, *d)\n        else:\n            _draw_performance_barsubplot(ax, *d)\n\n    plt.suptitle(suptitle)\n    plt.tight_layout()\n    plt.savefig(os.path.join(METRICS_PATH, score_name, f'{score_name}_{plot_type}.pdf'))\n    plt.close()", "\n\ndef _sort_vectors(*vals, reverse=True):\n    \"\"\"Sort multiple vectors / lists by values in the first one.\"\"\"\n    return zip(*sorted(zip(*vals), key=lambda x: x[0], reverse=reverse))\n\n\ndef _write_agg_df_to_files(df, score_name, main_score_col, out_file_base):\n    out_df_max = df.groupby(['dataset']).max()\n\n    def _highlight_max_per_ds(s):\n        def _is_max(idx, val):\n            return val >= out_df_max[s.name][idx[1]]\n\n        return ['font-weight: bold' if _is_max(idx, val) else '' for idx, val in s.items()]\n\n    # Remap series to friendly names\n    def _remap_series_names(s):\n        s.index = pd.Index(data=[MODELS_ALL.get(n, n) for n in s.index.values], name='Model')\n        return s\n\n    os.makedirs(os.path.join(METRICS_PATH, score_name), exist_ok=True)\n    out_styler = df.style.apply(_highlight_max_per_ds).format(precision=3)\n    out_styler.to_excel(out_file_base + '.xlsx')\n\n    # Compile reduced versions of the table with global averages\n    for series in '_micro', '_macro':\n        out_df_reduced = df.loc[:, series, :].sort_values(f'mean_{main_score_col}', ascending=False)\n        out_df_reduced.name = 'Model'\n        if score_name == 'rouge':\n            out_df_reduced.columns = ['Mean Precision', 'Mean Recall', 'Mean F1',\n                                      'Median Precision', 'Median Recall', 'Median F1']\n        else:\n            out_df_reduced.columns = ['Mean Distance', 'Median Distance']\n        out_df_reduced = out_df_reduced.apply(_remap_series_names)\n\n        # XLSX\n        out_styler = out_df_reduced.style.highlight_max(props='font-weight: bold').format(precision=3)\n        out_styler.to_excel(out_file_base + f'{series}.xlsx', float_format='%.3f')\n\n        # LaTeX\n        if score_name == 'rouge':\n            out_df_reduced.columns = ['Mean Precision', 'Mean Recall', 'Mean $F_1$',\n                                      'Median Precision', 'Median Recall', 'Median $F_1$']\n        out_styler = out_df_reduced.style.highlight_max(props=r'bf:').format(precision=3)\n        out_styler.to_latex(os.path.join(out_file_base + f'{series}.tex'))", "\n\ndef _agg_model_at_complexity(complexity, in_df, score_name, score_cols, main_score_col):\n    models = sorted(in_df.index.unique('model'))\n\n    out_df = pd.DataFrame(columns=['model', 'dataset',\n                                   *[f'mean_{c}' for c in score_cols],\n                                   *[f'median_{c}' for c in score_cols]])\n    out_df.set_index(['model', 'dataset'], inplace=True)\n\n    model_main_scores = []\n    model_main_medians = []\n    model_main_means = []\n    model_main_lower_err = []\n    model_main_upper_err = []\n    for m in models:\n        model_df = in_df.loc[m, :, :].drop(columns=['scorer'])\n\n        model_ds_group = model_df.groupby('dataset')\n        mean_ds = model_ds_group.mean()\n        median_ds = model_ds_group.median()\n\n        ds_stats = pd.concat([mean_ds, median_ds], axis=1)\n\n        mean_micro = model_df.mean()\n        median_micro = model_df.median()\n        micro = pd.concat([mean_micro, median_micro]).to_frame('_micro').T\n        micro.index.name = 'dataset'\n        ds_stats = pd.concat([ds_stats, micro])\n\n        mean_macro = mean_ds.mean()\n        median_macro = median_ds.median()\n        macro = pd.concat([mean_macro, median_macro]).to_frame('_macro').T\n        macro.index.name = 'dataset'\n        ds_stats = pd.concat([ds_stats, macro])\n\n        ds_stats.columns = out_df.columns\n\n        ds_stats['model'] = m\n        ds_stats = ds_stats.reset_index().set_index(['model', 'dataset'])\n\n        out_df = pd.concat([out_df, ds_stats.round(3)])\n\n        model_main_scores.append(model_df[main_score_col])\n        model_main_medians.append(median_micro[main_score_col])\n\n        model_main_means.append(mean_micro[main_score_col])\n        model_main_lower_err.append(abs(mean_micro[main_score_col] - model_df[main_score_col].quantile(0.25)))\n        model_main_upper_err.append(abs(model_df[main_score_col].quantile(0.75) - mean_micro[main_score_col]))\n\n    file_suffix = f'_complexity_{complexity}' if complexity != 'all' else ''\n    _write_agg_df_to_files(out_df, score_name, main_score_col,\n                           os.path.join(METRICS_PATH, score_name, f'{score_name}{file_suffix}'))\n\n    _, main_scores, labels = _sort_vectors(model_main_medians, model_main_scores, models)\n    boxplot_data = [main_scores, labels, f'Complexity: {complexity.capitalize()}']\n\n    main_scores, low, high, labels = _sort_vectors(model_main_means, model_main_lower_err, model_main_upper_err, models)\n    barplot_data = [main_scores, low, high, labels, f'Complexity: {complexity.capitalize()}']\n\n    return boxplot_data, barplot_data", "\n\ndef _plot_score_histograms(title, score_df, out_file):\n    models = sorted(score_df.index.unique('model'), key=lambda m: score_df[m, :, :].median(), reverse=True)\n    cols = 4\n    rows = math.ceil(len(models) / cols)\n\n    fig, axs = plt.subplots(rows, cols, sharey=True, figsize=(2 * cols, 2 * rows))\n    for ax, m in zip(axs.flatten(), models):\n        ax.hist(\n            score_df[m, :, :],\n            bins=25\n        )\n        ax.axvline(score_df[m, :, :].median(), color=plt.MEDIAN_BAR_COLOR, linewidth=1)\n        ax.set_ylabel(m)\n        _map_model_label(ax.yaxis.get_label())\n        ax.set_xticks([0, 0.5, 1])\n        ax.set_yticklabels([])\n\n    # Hide empty plots\n    if len(models) % cols:\n        [ax.set_visible(False) for ax in axs[-1][len(models) % cols:].flatten()]\n\n    fig.suptitle(title)\n    plt.tight_layout()\n    plt.savefig(out_file)\n    plt.close()", "\n\ndef aggregate_scores(score_name, models, datasets, complexities):\n    \"\"\"\n    Aggregate evaluation statistics.\n\n    :param score_name: score to aggregated (``\"rouge\"`` or ``\"levenshtein\"``)\n    :param models: list of input model names\n    :param datasets: list of input dataset names\n    :param complexities: list of complexity classes to include\n    \"\"\"\n    score_in_path = os.path.join(METRICS_PATH, score_name)\n    if not os.path.isdir(score_in_path):\n        return\n\n    if score_name == 'rouge':\n        score_cols = ['prec', 'rec', 'f1']\n        main_score_col = 'f1'\n    else:\n        score_cols = ['dist']\n        main_score_col = 'dist'\n\n    comp_quant_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv')\n    q = pd.read_csv(comp_quant_path, index_col=0)\n    compl_range = {'all': None}\n    compl_range.update({k: v for k, v in zip(COMPLEXITIES, pairwise([0, float(q.loc[0.25]), float(q.loc[0.75]), 1]))})\n\n    if score_name == 'rouge':\n        title_box = 'ROUGE-LSum Median $F_1$ Page Scores'\n        title_bar = 'ROUGE-LSum Mean $F_1$ Page Scores (Macro Average)'\n        title_hist = 'ROUGE-LSum $F_1$ Page Scores'\n    else:\n        title_hist = 'Normalized Levenshtein Distances'\n        title_box = 'Normalized Median Levenshtein Distances'\n        title_bar = 'Normalized Mean Levenshtein Distance (Macro Average)'\n\n    with click.progressbar(complexities, label=f'Aggregating \"{score_name}\" scores') as progress:\n        boxplot_data = []\n        barplot_data = []\n        for comp in progress:\n            score_df = pd.DataFrame()\n            for d, m in product(datasets, models):\n                p = os.path.join(score_in_path, d, f'{score_name}_{m}.csv')\n                if not os.path.isfile(p):\n                    continue\n\n                df = pd.read_csv(p, index_col=['model', 'dataset'])\n                if compl_range[comp] is not None:\n                    # Filter input dataframe to include only pages within chosen complexity range\n                    c = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, d, f'{d}_complexity.csv'),\n                                    index_col='hash_key')\n                    c = c[(c['complexity'] >= compl_range[comp][0]) & (c['complexity'] <= compl_range[comp][1])]\n                    df = df[df['hash_key'].isin(c.index)]\n\n                df.set_index(['hash_key'], append=True, inplace=True)\n                score_df = pd.concat([score_df, df])\n\n            hist_file_prefix = f'_complexity_{comp}' if comp != 'all' else ''\n            _plot_score_histograms(f'{title_hist} (Complexity: {comp.capitalize()})', score_df[main_score_col],\n                                   os.path.join(score_in_path, f'{score_name}{hist_file_prefix}_hist.pdf'))\n\n            box, bar = _agg_model_at_complexity(comp, score_df, score_name, score_cols, main_score_col)\n            boxplot_data.append(box)\n            barplot_data.append(bar)\n\n        _draw_performance_plot(\n            'box',\n            boxplot_data,\n            (len(boxplot_data), 1),\n            title_box,\n            score_name)\n\n        _draw_performance_plot(\n            'bar',\n            barplot_data,\n            (len(barplot_data), 1),\n            title_bar,\n            score_name)", ""]}
{"filename": "src/extraction_benchmark/util.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport re\n\n\ndef read_jsonl(file):\n    \"\"\"\n    Read JSONL file and return iterable of dicts.\n\n    :param file: input filename\n    :return: iterable of dicts\n    \"\"\"\n    with open(file, 'r') as f:\n        for line in f:\n            yield json.loads(line)", "\n\ndef read_jsonl(file):\n    \"\"\"\n    Read JSONL file and return iterable of dicts.\n\n    :param file: input filename\n    :return: iterable of dicts\n    \"\"\"\n    with open(file, 'r') as f:\n        for line in f:\n            yield json.loads(line)", "\n\ndef jsonl_to_dict(file):\n    \"\"\"\n    Load a JSONL into a single dict with ``\"page_id\"`` as keys.\n\n    :param file: input file name\n    :return: assembled dict\n    \"\"\"\n    loaded = {}\n    for j in read_jsonl(file):\n        loaded[j['page_id']] = {k: v for k, v in j.items() if k != 'page_id'}\n    return loaded", "\n\n_TOKEN_RE_WS = re.compile(r'\\s+', flags=re.UNICODE | re.MULTILINE)\n\n\ndef tokenize_ws(text):\n    \"\"\"\n    Tokenize text by white space.\n\n    :param text: input text\n    :return: list of tokens\n    \"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    return _TOKEN_RE_WS.split(text)", "\n\n_TOKEN_RE_WORDS = re.compile(r'\\w+', flags=re.UNICODE)\n\n\ndef tokenize_words(text):\n    \"\"\"\n    Tokenize text by extracting Unicode word tokens (skips any non-word tokens).\n\n    :param text: input text\n    :return: list of tokens\n    \"\"\"\n    return _TOKEN_RE_WORDS.findall(text)", ""]}
{"filename": "src/extraction_benchmark/extract.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom contextlib import redirect_stderr, redirect_stdout\nfrom functools import partial\nimport io\nfrom itertools import product", "import io\nfrom itertools import product\nimport json\nimport logging\nfrom multiprocessing import get_context\nimport os\nfrom typing import Any, Dict\nimport warnings\n\nimport click", "\nimport click\n\nfrom extraction_benchmark.dataset_readers import read_datasets, read_raw_dataset\nfrom extraction_benchmark.extractors import extractors\nfrom extraction_benchmark.paths import *\n\n\ndef _dict_to_jsonl(filepath, lines_dict: Dict[str, Any]):\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n    with open(filepath, 'w') as f:\n        for page_id in sorted(lines_dict):\n            json.dump({'page_id': page_id, **lines_dict[page_id]}, f, indent=None, ensure_ascii=False)\n            f.write('\\n')", "def _dict_to_jsonl(filepath, lines_dict: Dict[str, Any]):\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n    with open(filepath, 'w') as f:\n        for page_id in sorted(lines_dict):\n            json.dump({'page_id': page_id, **lines_dict[page_id]}, f, indent=None, ensure_ascii=False)\n            f.write('\\n')\n\n\ndef extract_ground_truth(datasets):\n    \"\"\"\n    Convert ground truth from raw dataset to JSON format.\n\n    :param datasets: list of input dataset\n    :return: set of page IDs that were extracted\n    \"\"\"\n\n    page_ids = set()\n    for ds in datasets:\n        with click.progressbar(read_raw_dataset(ds, True), label=f'Converting ground truth of {ds}') as ds_progress:\n            extracted = {k: v for k, v in ds_progress}\n            page_ids.update(extracted.keys())\n        _dict_to_jsonl(os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl'), extracted)\n    return page_ids", "def extract_ground_truth(datasets):\n    \"\"\"\n    Convert ground truth from raw dataset to JSON format.\n\n    :param datasets: list of input dataset\n    :return: set of page IDs that were extracted\n    \"\"\"\n\n    page_ids = set()\n    for ds in datasets:\n        with click.progressbar(read_raw_dataset(ds, True), label=f'Converting ground truth of {ds}') as ds_progress:\n            extracted = {k: v for k, v in ds_progress}\n            page_ids.update(extracted.keys())\n        _dict_to_jsonl(os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl'), extracted)\n    return page_ids", "\n\ndef extract_raw_html(datasets, page_id_whitelist=None):\n    \"\"\"\n    Convert HTML files from raw dataset to JSON format.\n\n    :param datasets: list of input dataset\n    :param page_id_whitelist: optional list of page IDs to include (if set, IDs not in this list will be skipped)\n    \"\"\"\n    if page_id_whitelist and type(page_id_whitelist) is not set:\n        page_id_whitelist = set(page_id_whitelist)\n\n    for ds in datasets:\n        out_dir = os.path.join(DATASET_COMBINED_HTML_PATH, ds)\n        os.makedirs(out_dir, exist_ok=True)\n        with click.progressbar(read_raw_dataset(ds, False), label=f'Converting HTML of {ds}') as ds_progress:\n            for page_id, val in ds_progress:\n                if page_id_whitelist and page_id not in page_id_whitelist:\n                    continue\n                if not val.get('html'):\n                    continue\n                with open(os.path.join(out_dir, page_id + '.html'), 'w') as f:\n                    f.write(val['html'])", "\n\ndef _extract_with_model_expand_args(args, skip_existing=False, verbose=False):\n    _extract_with_model(*args, skip_existing=skip_existing, verbose=verbose)\n\n\ndef _extract_with_model(model, dataset, skip_existing=False, verbose=False):\n    model, model_name = model\n    out_path = os.path.join(MODEL_OUTPUTS_PATH, dataset, model_name + '.jsonl')\n\n    logger = logging.getLogger('wceb-extract')\n    logger.setLevel(logging.INFO if verbose else logging.ERROR)\n\n    extracted = {}\n    if skip_existing and os.path.isfile(out_path):\n        with open(out_path, 'r') as f:\n            for line in f:\n                j = json.loads(line)\n                extracted[j['page_id']] = {k: v for k, v in j.items() if k != 'page_id'}\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n\n        for file_hash, in_data in read_datasets([dataset], False):\n            if file_hash in extracted:\n                continue\n\n            out_data = dict(plaintext='', model=model_name)\n            try:\n                with redirect_stdout(io.StringIO()) as stdout, redirect_stderr(io.StringIO()) as stderr:\n                    out_data['plaintext'] = model(in_data['html'], page_id=file_hash) or ''\n\n                if stdout.getvalue():\n                    logger.info(stdout.getvalue().strip())\n                if stderr.getvalue():\n                    logger.warning(stderr.getvalue().strip())\n            except Exception as e:\n                logger.warning(f'Error in model {model_name} while extracting {dataset} ({file_hash}):')\n                logger.warning(str(e))\n\n            extracted[file_hash] = out_data\n\n    if not extracted:\n        return\n\n    _dict_to_jsonl(out_path, extracted)", "\n\ndef extract(models, datasets, skip_existing, parallelism, verbose=False):\n    \"\"\"\n    Extract datasets with the selected extraction models.\n\n    :param models: list of extraction model names (if ``ground_truth == False``)\n    :param datasets: list of dataset names under \"datasets/raw\"\n    :param skip_existing: skip models for which an answer file exists already\n    :param parallelism: number of parallel workers\n    :param verbose: log error information\n    \"\"\"\n\n    model = [(getattr(extractors, 'extract_' + m), m) for m in models]\n    jobs = list(product(model, datasets))\n\n    def item_show_func(j):\n        if j:\n            return f'Model: {j[0][1]}, Dataset: {j[1]}'\n\n    if parallelism == 1:\n        with click.progressbar(jobs, label='Running extrators', item_show_func=item_show_func) as progress:\n            for job in progress:\n                _extract_with_model_expand_args(job)\n        return\n\n    with get_context('spawn').Pool(processes=parallelism) as pool:\n        try:\n            with click.progressbar(pool.imap_unordered(partial(_extract_with_model_expand_args,\n                                                               skip_existing=skip_existing, verbose=verbose), jobs),\n                                   length=len(jobs), label='Running extrators') as progress:\n                for _ in progress:\n                    pass\n        except KeyboardInterrupt:\n            pool.terminate()\n\n    click.echo(f'Model outputs written to {MODEL_OUTPUTS_PATH}')", ""]}
{"filename": "src/extraction_benchmark/dataset_readers.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABC, abstractmethod\nimport errno\nimport glob\nimport gzip", "import glob\nimport gzip\nimport hashlib\nfrom itertools import chain\nimport json\nimport os\nimport re\nfrom typing import Any, Dict, Iterable, Optional, Tuple\n\nfrom resiliparse.parse import bytes_to_str, detect_encoding", "\nfrom resiliparse.parse import bytes_to_str, detect_encoding\nfrom resiliparse.parse.html import HTMLTree, NodeType\n\nfrom extraction_benchmark.globals import DATASETS\nfrom extraction_benchmark.paths import *\n\n\nclass DatasetReader(ABC):\n    \"\"\"Abstract dataset reader class.\"\"\"", "class DatasetReader(ABC):\n    \"\"\"Abstract dataset reader class.\"\"\"\n\n    def __init__(self, ground_truth):\n        \"\"\"\n        Initialize dataset reader.\n\n        :param ground_truth: whether the reader should return the raw HTML data or the ground truth.\n        \"\"\"\n        self.is_truth = ground_truth", "        \"\"\"\n        self.is_truth = ground_truth\n        self._iter = iter(self.read())\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._iter)\n", "        return next(self._iter)\n\n    def __len__(self):\n        return self.dataset_size()\n\n    @abstractmethod\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        \"\"\"\n        Return an iterable over the items in the dataset.\n", "        Return an iterable over the items in the dataset.\n\n        Returned items should be tuples with the case / page ID and the case / page data as a dict.\n        The dicts should contain at least an ``\"html\"`` or ``\"plaintext\"`` key, depending on whether\n        the iterated dataset is a raw HTML or a ground truth page.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dataset_size(self) -> Optional[int]:", "    @abstractmethod\n    def dataset_size(self) -> Optional[int]:\n        \"\"\"\n        Return size of dataset or ``None`` if size is unknown.\n\n        :return: size of dataset\n        \"\"\"\n        pass\n\n    @staticmethod", "\n    @staticmethod\n    def _hash(data: bytes):\n        \"\"\"\n        Return SHA-256 hash of input bytes, which can be used as a an ID.\n\n        :param data: input bytes\n        :return: hash of bytes as hex string\n        \"\"\"\n        m = hashlib.sha256()", "        \"\"\"\n        m = hashlib.sha256()\n        m.update(data)\n        return m.hexdigest()\n\n    @classmethod\n    def _file_hash(cls, file: str):\n        \"\"\"\n        Return SHA-256 hash of a file that can be used as a page ID.\n", "        Return SHA-256 hash of a file that can be used as a page ID.\n\n        :param file: input file name\n        :return: hash of the file as hex string\n        \"\"\"\n        with open(file, 'rb') as f:\n            return cls._hash(f.read())\n\n    def _build_dict(self, source_dataset, source_case, content, **kwargs) -> Dict[str, Any]:\n        \"\"\"", "    def _build_dict(self, source_dataset, source_case, content, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Helper method for creating a dict to return in :meth:`read`.\n\n        :param source_dataset: source dataset name\n        :param source_case: source case / file name\n        :param content: HTML or plaintext content\n        :param kwargs: other key / value pairs to include in the dict\n        :return: dict with requested data\n        \"\"\"", "        :return: dict with requested data\n        \"\"\"\n        d = {\n            ('plaintext' if self.is_truth else 'html'): content,\n            **{k: v for k, v in kwargs.items() if v},\n            'source': [source_dataset, source_case] if source_case else [source_dataset]\n        }\n        return d\n\n    @staticmethod", "\n    @staticmethod\n    def _read_file(path, fixed_encoding=None):\n        \"\"\"\n        Helper method for reading a file, detecting its encoding and returning the contents as UTF-8 string.\n        If the input file is GZip-compressed, it will be decompressed automatically.\n\n        :param path: file path\n        :param fixed_encoding: use this fixed encoding instead of trying to detect if from the file\n        :return: UTF-8 string of file contents", "        :param fixed_encoding: use this fixed encoding instead of trying to detect if from the file\n        :return: UTF-8 string of file contents\n        \"\"\"\n        with open(path, 'rb') as f:\n            file_bytes = f.read()\n            if path.endswith('.gz'):\n                file_bytes = gzip.decompress(file_bytes)\n            if fixed_encoding:\n                enc = fixed_encoding\n            else:", "                enc = fixed_encoding\n            else:\n                enc = detect_encoding(file_bytes, max_len=100000, html5_compatible=False) or 'utf-8'\n            return bytes_to_str(file_bytes, encoding=enc, fallback_encodings=['utf-8', 'cp1252'])\n\n\nclass CleanEvalReader(DatasetReader):\n    def __init__(self, ground_truth):\n        super().__init__(ground_truth)\n", "        super().__init__(ground_truth)\n\n        self.dataset_name = 'cleaneval'\n        self.dataset_path = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'orig')\n        self.dataset_path_truth = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'clean')\n\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        read_path = self.dataset_path_truth if self.is_truth else self.dataset_path\n\n        text_tag_re = re.compile(r'(?:^<text [^>]+>\\s*|\\s*</text>$)', flags=re.MULTILINE)", "\n        text_tag_re = re.compile(r'(?:^<text [^>]+>\\s*|\\s*</text>$)', flags=re.MULTILINE)\n\n        for file in os.listdir(read_path):\n            abs_path = os.path.join(read_path, file)\n            content = self._read_file(abs_path)\n            url = None\n            if self.is_truth:\n                url = re.search(r'^\\s*URL: (https?://.+)', content)\n                if url:", "                url = re.search(r'^\\s*URL: (https?://.+)', content)\n                if url:\n                    url = url.group(1)\n                content = HTMLTree.parse(content).body.text\n                content = re.sub(r'\\n +', '\\n', content)\n                content = re.sub(r'^\\s*URL:[^\\n]+\\s*', '', content)    # Strip URL line\n\n            if self.is_truth:\n                abs_path = os.path.join(self.dataset_path, os.path.splitext(file)[0] + '.html')\n            else:", "                abs_path = os.path.join(self.dataset_path, os.path.splitext(file)[0] + '.html')\n            else:\n                content = text_tag_re.sub('', content)\n            source = os.path.splitext(file)[0]\n            yield self._file_hash(abs_path), self._build_dict(self.dataset_name, source, content, url=url)\n\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, self.dataset_name, 'clean', '*.txt')))\n\n", "\n\nclass CleanPortalEvalReader(CleanEvalReader):\n    def __init__(self, ground_truth):\n        super().__init__(ground_truth)\n        self.dataset_name = 'cleanportaleval'\n        self.dataset_path = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'input')\n        self.dataset_path_truth = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'GoldStandard')\n\n    def dataset_size(self) -> Optional[int]:", "\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, self.dataset_name, 'GoldStandard', '*.txt')))\n\n\nclass DragnetReader(DatasetReader):\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'dragnet', 'HTML')\n        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'dragnet', 'corrected', 'Corrected')\n        read_path = dataset_path_truth if self.is_truth else dataset_path", "        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'dragnet', 'corrected', 'Corrected')\n        read_path = dataset_path_truth if self.is_truth else dataset_path\n\n        for file in os.listdir(read_path):\n            abs_path = os.path.join(read_path, file)\n            content = self._read_file(abs_path)\n            if self.is_truth:\n                file = os.path.splitext(os.path.splitext(file)[0])[0]\n                abs_path = os.path.join(dataset_path, file)\n            source = os.path.splitext(file)[0]", "                abs_path = os.path.join(dataset_path, file)\n            source = os.path.splitext(file)[0]\n            yield self._file_hash(abs_path), self._build_dict('dragnet', source, content)\n\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'dragnet', 'corrected', 'Corrected', '*.txt')))\n\n\nclass CETDReader(DatasetReader):\n    def __init__(self, ground_truth):", "class CETDReader(DatasetReader):\n    def __init__(self, ground_truth):\n        super().__init__(ground_truth)\n        self.verticals = ['arstechnica', 'BBC', 'Chaos', 'nytimes', 'wiki', 'YAHOO!']\n\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'cetd')\n        for vertical in self.verticals:\n            sub_path = os.path.join(dataset_path, vertical, 'gold' if self.is_truth else 'original')\n            for file in os.listdir(sub_path):", "            sub_path = os.path.join(dataset_path, vertical, 'gold' if self.is_truth else 'original')\n            for file in os.listdir(sub_path):\n                abs_path = os.path.join(sub_path, file)\n                content = self._read_file(abs_path)\n                if self.is_truth:\n                    abs_path = os.path.join(dataset_path, vertical, 'original', os.path.splitext(file)[0] + '.htm')\n                source = vertical + '_' + os.path.splitext(file)[0]\n                yield self._file_hash(abs_path), self._build_dict('cetd', source, content)\n\n    def dataset_size(self) -> Optional[int]:", "\n    def dataset_size(self) -> Optional[int]:\n        return sum([len(glob.glob(os.path.join(DATASET_RAW_PATH, 'cetd', v, 'gold', '*.txt')))\n                    for v in self.verticals])\n\n\nclass ReadabilityReader(DatasetReader):\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'readability', 'test-pages')\n        for case_dir in os.listdir(dataset_path):", "        dataset_path = os.path.join(DATASET_RAW_PATH, 'readability', 'test-pages')\n        for case_dir in os.listdir(dataset_path):\n            sub_path = 'expected.html' if self.is_truth else 'source.html'\n            abs_path = os.path.join(dataset_path, os.path.join(case_dir, sub_path))\n            content = self._read_file(abs_path)\n            if self.is_truth:\n                content = HTMLTree.parse(content).body.text\n                abs_path = os.path.join(dataset_path, os.path.join(case_dir, 'source.html'))\n            yield self._file_hash(abs_path), self._build_dict('readability', case_dir, content)\n", "            yield self._file_hash(abs_path), self._build_dict('readability', case_dir, content)\n\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'readability', 'test-pages', '*', 'expected.html')))\n\n\nclass ScrapingHubReader(DatasetReader):\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'scrapinghub')\n", "        dataset_path = os.path.join(DATASET_RAW_PATH, 'scrapinghub')\n\n        if self.is_truth:\n            truth_json = json.load(open(os.path.join(dataset_path, 'ground-truth.json'), 'r'))\n            for k, v in truth_json.items():\n                # Instead of using provided hash, re-calculate hash from original HTML file for consistency\n                with gzip.GzipFile(os.path.join(dataset_path, 'html', f'{k}.html.gz'), 'r') as f:\n                    file_hash = self._hash(f.read())\n                yield file_hash, self._build_dict('scrapinghub', k, v['articleBody'], url=v['url'])\n            return", "                yield file_hash, self._build_dict('scrapinghub', k, v['articleBody'], url=v['url'])\n            return\n\n        dataset_path = os.path.join(dataset_path, 'html')\n        for file in os.listdir(dataset_path):\n            abs_path = os.path.join(dataset_path, file)\n            hash_id = os.path.splitext(os.path.splitext(file)[0])[0]\n            with gzip.GzipFile(abs_path, 'r') as f:\n                file_hash = self._hash(f.read())\n            yield file_hash, self._build_dict('scrapinghub', hash_id, self._read_file(abs_path))", "                file_hash = self._hash(f.read())\n            yield file_hash, self._build_dict('scrapinghub', hash_id, self._read_file(abs_path))\n\n    def dataset_size(self) -> Optional[int]:\n        return len(json.load(open(os.path.join(DATASET_RAW_PATH, 'scrapinghub', 'ground-truth.json'), 'r')))\n\n\nclass L3SGN1Reader(DatasetReader):\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'original')", "    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'original')\n        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'annotated')\n        read_path = dataset_path_truth if self.is_truth else dataset_path\n\n        for file in os.listdir(read_path):\n            abs_path = os.path.join(read_path, file)\n            content = self._read_file(abs_path)\n            if self.is_truth:\n                abs_path = os.path.join(dataset_path, file)", "            if self.is_truth:\n                abs_path = os.path.join(dataset_path, file)\n                content = self._extract_with_css_selector(content, '.x-nc-sel1, .x-nc-sel2, .x-nc-sel3')\n            source = os.path.splitext(file)[0]\n            yield self._file_hash(abs_path), self._build_dict('l3s-gn1', source, content)\n\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'annotated', '*.html')))\n\n    @staticmethod", "\n    @staticmethod\n    def _extract_with_css_selector(html, selector):\n        tree = HTMLTree.parse(html)\n        elements = tree.body.query_selector_all(selector)\n        content = ''\n        for e in elements:\n            if len(e.child_nodes) != 1 or e.first_child.type != NodeType.TEXT:\n                # Only count leaf nodes to avoid adding an element multiple times\n                continue", "                # Only count leaf nodes to avoid adding an element multiple times\n                continue\n            if e.parent.tag in ['address', 'article', 'aside', 'blockquote', 'canvas', 'dd', 'div', 'dl', 'dt',\n                                'fieldset',\n                                'figcaption', 'figure', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header',\n                                'hr', 'li', 'main', 'nav', 'noscript', 'ol', 'p', 'pre', 'section', 'table', 'tfoot',\n                                'ul', 'video']:\n                content += '\\n'\n            content += e.text.strip() + ' '\n        return content.strip()", "            content += e.text.strip() + ' '\n        return content.strip()\n\n\nclass GoogleTrends2017Reader(L3SGN1Reader):\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        dataset_path = os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'raw_html')\n        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'prepared_html')\n        read_path = dataset_path_truth if self.is_truth else dataset_path\n", "        read_path = dataset_path_truth if self.is_truth else dataset_path\n\n        for file in os.listdir(read_path):\n            abs_path = os.path.join(read_path, file)\n            content = self._read_file(abs_path)\n            if self.is_truth:\n                abs_path = os.path.join(dataset_path, file)\n                content = self._extract_with_css_selector(content, '[__boilernet_label=\"1\"]')\n            source = os.path.splitext(file)[0]\n            yield self._file_hash(abs_path), self._build_dict('google-trends-2017', source, content)", "            source = os.path.splitext(file)[0]\n            yield self._file_hash(abs_path), self._build_dict('google-trends-2017', source, content)\n\n    def dataset_size(self) -> Optional[int]:\n        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'prepared_html', '*.html')))\n\n\nclass CombinedDatasetReader(DatasetReader):\n    def __init__(self, ground_truth, read_subsets=None):\n        super().__init__(ground_truth)", "    def __init__(self, ground_truth, read_subsets=None):\n        super().__init__(ground_truth)\n        self.subsets = sorted(read_subsets) if read_subsets else sorted(DATASETS)\n\n    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n        if self.is_truth:\n            for ds in self.subsets:\n                with open(os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl')) as f:\n                    for line in f:\n                        j = json.loads(line)", "                    for line in f:\n                        j = json.loads(line)\n                        yield j['page_id'], {k: v for k, v in j.items() if k != 'page_id'}\n            return\n\n        for ds in self.subsets:\n            for filename in glob.glob(os.path.join(DATASET_COMBINED_HTML_PATH, ds, '*.html')):\n                page_id = os.path.splitext(os.path.basename(filename))[0]\n                yield page_id, self._build_dict(ds, page_id, self._read_file(filename, 'utf-8'))\n", "                yield page_id, self._build_dict(ds, page_id, self._read_file(filename, 'utf-8'))\n\n    def dataset_size(self) -> Optional[int]:\n        # Count lines in all truth files\n        return sum(chain(*((1 for _ in open(\n            os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl'), 'r')) for ds in self.subsets)))\n\n\ndef read_raw_dataset(dataset, ground_truth):\n    \"\"\"Read raw (unprocessed datasets).\"\"\"", "def read_raw_dataset(dataset, ground_truth):\n    \"\"\"Read raw (unprocessed datasets).\"\"\"\n    match dataset:\n        case 'cetd':\n            return CETDReader(ground_truth)\n        case 'cleaneval':\n            return CleanEvalReader(ground_truth)\n        case 'cleanportaleval':\n            return CleanPortalEvalReader(ground_truth)\n        case 'dragnet':", "            return CleanPortalEvalReader(ground_truth)\n        case 'dragnet':\n            return DragnetReader(ground_truth)\n        case 'google-trends-2017':\n            return GoogleTrends2017Reader(ground_truth)\n        case 'l3s-gn1':\n            return L3SGN1Reader(ground_truth)\n        case 'readability':\n            return ReadabilityReader(ground_truth)\n        case 'scrapinghub':", "            return ReadabilityReader(ground_truth)\n        case 'scrapinghub':\n            return ScrapingHubReader(ground_truth)\n        case _:\n            raise ValueError(f'Invalid dataset: {dataset}')\n\n\ndef read_datasets(datasets: Iterable[str], ground_truth):\n    \"\"\"Read (subsets of) processed and combined datasets.\"\"\"\n    if not os.path.isdir(DATASET_COMBINED_PATH):", "    \"\"\"Read (subsets of) processed and combined datasets.\"\"\"\n    if not os.path.isdir(DATASET_COMBINED_PATH):\n        raise FileNotFoundError(errno.ENOENT, 'Combined dataset folder not found', DATASET_COMBINED_PATH)\n\n    return CombinedDatasetReader(ground_truth, datasets)\n"]}
{"filename": "src/extraction_benchmark/paths.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nROOT_PATH = os.path.realpath(os.path.join(os.getcwd()))\nDATASET_PATH = os.path.join(ROOT_PATH, 'datasets')", "ROOT_PATH = os.path.realpath(os.path.join(os.getcwd()))\nDATASET_PATH = os.path.join(ROOT_PATH, 'datasets')\n\nDATASET_RAW_PATH = os.path.join(DATASET_PATH, 'raw')\nDATASET_COMBINED_PATH = os.path.join(DATASET_PATH, 'combined')\nDATASET_COMBINED_TRUTH_PATH = os.path.join(DATASET_COMBINED_PATH, 'ground-truth')\nDATASET_COMBINED_HTML_PATH = os.path.join(DATASET_COMBINED_PATH, 'html')\n\nOUTPUTS_PATH = os.path.join(ROOT_PATH, 'outputs')\nHTML_FEATURES_PATH = os.path.join(OUTPUTS_PATH, 'html-features')", "OUTPUTS_PATH = os.path.join(ROOT_PATH, 'outputs')\nHTML_FEATURES_PATH = os.path.join(OUTPUTS_PATH, 'html-features')\nMODEL_OUTPUTS_PATH = os.path.join(OUTPUTS_PATH, 'model-outputs')\nMETRICS_PATH = os.path.join(OUTPUTS_PATH, 'metrics-computed')\nMETRICS_AGG_PATH = os.path.join(METRICS_PATH, '_aggregated')\nMETRICS_COMPLEXITY_PATH = os.path.join(METRICS_PATH, '_complexity')\n\nTHIRD_PARTY_PATH = os.path.join(ROOT_PATH, 'third-party')\n", ""]}
{"filename": "src/extraction_benchmark/complexity.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os.path\nfrom collections import defaultdict\nfrom multiprocessing import Pool\nimport re", "from multiprocessing import Pool\nimport re\n\nimport click\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE", "from sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom resiliparse.parse.html import HTMLTree\n\nfrom extraction_benchmark.dataset_readers import read_datasets\nfrom extraction_benchmark.globals import *\nfrom extraction_benchmark import plt", "from extraction_benchmark.globals import *\nfrom extraction_benchmark import plt\nfrom extraction_benchmark.util import tokenize_words\n\n\ndef calculate(datasets):\n    \"\"\"\n    Calculate page complexities for pages in the given datasets based on the ground truth.\n\n    :param datasets: list of dataset names\n    \"\"\"\n    complexity_total = pd.DataFrame(columns=['complexity'])\n    complexity_total.index.name = 'hash_key'\n    quantile_labels = [0.25, 0.33, 0.5, 0.66, 0.75]\n\n    os.makedirs(METRICS_COMPLEXITY_PATH, exist_ok=True)\n\n    with click.progressbar(datasets, label='Calculating page complexity scores') as ds_progress:\n        for ds in ds_progress:\n            tokens_truth = {}\n            tokens_src = {}\n            for h, truth in read_datasets([ds], True):\n                tokens_truth[h] = len(tokenize_words(truth['plaintext']))\n            for h, src in read_datasets([ds], False):\n                if h not in tokens_truth:\n                    continue\n                # Extract all text tokens except script / style\n                tree = HTMLTree.parse(src['html'])\n                for e in tree.body.query_selector_all('script, style'):\n                    e.decompose()\n                tokens_src[h] = len(tokenize_words(tree.body.text))\n\n            tokens_truth = pd.DataFrame.from_dict(tokens_truth, orient='index')\n            tokens_src = pd.DataFrame.from_dict(tokens_src, orient='index')\n\n            out_path_ds = os.path.join(METRICS_COMPLEXITY_PATH, ds)\n            os.makedirs(out_path_ds, exist_ok=True)\n\n            complexity = 1 - (tokens_truth / tokens_src).clip(lower=0, upper=1)\n            complexity.index.name = 'hash_key'\n            complexity.columns = ['complexity']\n            complexity.to_csv(os.path.join(out_path_ds, f'{ds}_complexity.csv'))\n            complexity['dataset'] = ds\n            quantiles = complexity['complexity'].quantile(quantile_labels)\n            quantiles.to_csv(os.path.join(out_path_ds, f'{ds}_complexity_quantiles.csv'))\n\n            complexity_total = pd.concat([complexity_total, complexity])\n\n    complexity_total.reset_index(inplace=True)\n    complexity_total.set_index(['hash_key', 'dataset'], inplace=True)\n    quantiles = complexity_total.quantile(quantile_labels)\n    quantiles.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity_quantiles.csv'))\n    complexity_total.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity.csv'))\n\n    click.echo(f'Complexity scores written to \"{METRICS_COMPLEXITY_PATH}\".')", "\n\n_WS_RE = re.compile(r'\\s+', flags=re.UNICODE | re.MULTILINE)\n\n\ndef extract_html_features(html):\n    tree = HTMLTree.parse(html)\n    for e in tree.body.query_selector_all('script, style, noscript'):\n        e.decompose()\n    text = _WS_RE.sub(' ', tree.body.text)\n\n    features = defaultdict(float)\n\n    all_tags = tree.body.query_selector_all('*')\n\n    n_tags = len(all_tags)\n    if n_tags != 0:\n        features['h1'] = len(tree.body.query_selector_all('h1')) / n_tags\n        features['h2'] = len(tree.body.query_selector_all('h2')) / n_tags\n        features['h3'] = len(tree.body.query_selector_all('h3')) / n_tags\n        features['h4'] = len(tree.body.query_selector_all('h4')) / n_tags\n        features['h5'] = len(tree.body.query_selector_all('h5')) / n_tags\n        features['h6'] = len(tree.body.query_selector_all('h6')) / n_tags\n        features['p'] = len(tree.body.query_selector_all('p')) / n_tags\n        features['ul'] = len(tree.body.query_selector_all('li')) / n_tags\n        features['table'] = len(tree.body.query_selector_all('table')) / n_tags\n        features['a'] = len(tree.body.query_selector_all('a')) / n_tags\n        features['div'] = len(tree.body.query_selector_all('div')) / n_tags\n        features['br'] = len(tree.body.query_selector_all('br')) / n_tags\n        features['strong'] = len(tree.body.query_selector_all('strong')) / n_tags\n        features['em'] = len(tree.body.query_selector_all('em')) / n_tags\n\n    features['html_to_non_html'] = n_tags / len(tokenize_words(text))\n\n    return features", "\n\ndef calculate_dataset_features(dataset):\n    df = pd.DataFrame()\n    for hash_key, data in read_datasets([dataset], False):\n        features = extract_html_features(data['html'])\n        s = pd.Series(features, name=hash_key)\n        df = pd.concat([df, s.to_frame().T])\n    df.index.name = 'hash_key'\n    out_dir = os.path.join(HTML_FEATURES_PATH, dataset)\n    os.makedirs(out_dir, exist_ok=True)\n    df.to_csv(os.path.join(out_dir, f'{dataset}_html_features.csv'))", "\n\ndef tsne_reduce_dim(X, n_components):\n    return TSNE(n_components=n_components,\n                learning_rate='auto',\n                init='random',\n                perplexity=30,\n                method='barnes_hut',\n                n_jobs=-1,\n                verbose=1).fit_transform(X)", "\n\ndef extract_page_features(dataset, parallelism):\n    with Pool(processes=parallelism) as pool:\n        with click.progressbar(pool.imap_unordered(calculate_dataset_features, dataset),\n                               length=len(dataset), label='Extracting dataset features') as progress:\n            for _ in progress:\n                pass\n\n\ndef _load_html_features(dataset):\n    df_features = pd.DataFrame()\n    df_complexity = pd.DataFrame()\n    with click.progressbar(dataset, label='Loading datasets') as progress:\n        for ds in progress:\n            df_tmp = pd.read_csv(os.path.join(HTML_FEATURES_PATH, ds, f'{ds}_html_features.csv'))\n            df_tmp['dataset'] = ds\n            df_features = pd.concat([df_features, df_tmp], ignore_index=True)\n\n            df_tmp = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, ds, f'{ds}_complexity.csv'))\n            df_tmp['dataset'] = ds\n            df_complexity = pd.concat([df_complexity, df_tmp], ignore_index=True)\n\n    df_features.set_index(['hash_key', 'dataset'], inplace=True)\n    df_complexity.set_index(['hash_key', 'dataset'], inplace=True)\n\n    return df_features.join(df_complexity, how='inner')", "\n\ndef _load_html_features(dataset):\n    df_features = pd.DataFrame()\n    df_complexity = pd.DataFrame()\n    with click.progressbar(dataset, label='Loading datasets') as progress:\n        for ds in progress:\n            df_tmp = pd.read_csv(os.path.join(HTML_FEATURES_PATH, ds, f'{ds}_html_features.csv'))\n            df_tmp['dataset'] = ds\n            df_features = pd.concat([df_features, df_tmp], ignore_index=True)\n\n            df_tmp = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, ds, f'{ds}_complexity.csv'))\n            df_tmp['dataset'] = ds\n            df_complexity = pd.concat([df_complexity, df_tmp], ignore_index=True)\n\n    df_features.set_index(['hash_key', 'dataset'], inplace=True)\n    df_complexity.set_index(['hash_key', 'dataset'], inplace=True)\n\n    return df_features.join(df_complexity, how='inner')", "\n\ndef _reduce_dim_2d(df, label_column):\n    click.echo('Reducing dimensionality to 2D for visualization...')\n\n    scaler = StandardScaler()\n    X = scaler.fit_transform(df)\n    X = tsne_reduce_dim(X, 2)\n\n    df_2d = pd.DataFrame(X, columns=['x', 'y'], index=df.index)\n    df_2d[label_column] = df[label_column]\n    df_2d['complexity'] = df['complexity']\n\n    return df_2d", "\n\ndef _binarize_complexity(values, quantile):\n    p = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv')\n    if not os.path.isfile(p):\n        raise click.FileError(p, 'Please calculate page complexity quantiles first.')\n    quantiles = pd.read_csv(p, index_col=0)\n\n    return [int(x >= quantiles.loc[float(quantile)]['complexity']) for x in values]\n", "\n\ndef logistic_regression_classify(dataset, train_split_size, quantile):\n    df_features = _load_html_features(dataset)\n    df_features['complexity'] = _binarize_complexity(df_features['complexity'], quantile)\n\n    click.echo('Training classifier and predicting pages...')\n    idx_train, idx_test = train_test_split(df_features.index.values, train_size=train_split_size)\n\n    df_train = df_features[df_features.index.isin(idx_train)]\n    df_test = df_features[~df_features.index.isin(idx_train)]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(df_train.drop(columns='complexity'))\n    X_test = scaler.fit_transform(df_test.drop(columns='complexity'))\n    y_pred = LogisticRegression().fit(X_train, df_train['complexity']).predict(X_test)\n\n    df_test = df_test.assign(logreg_label=y_pred)\n    df_test.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_classes.csv'))\n    click.echo(f'Classification written to \"{METRICS_COMPLEXITY_PATH}\"')", "\n\ndef kmeans_cluster(dataset, reduce_dim, n_clusters):\n    df_features = _load_html_features(dataset)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(df_features.drop(columns='complexity'))\n\n    if reduce_dim:\n        X = PCA(n_components=reduce_dim).fit_transform(X)\n\n    click.echo('Clustering datapoints...')\n    labels = KMeans(n_clusters=n_clusters, max_iter=500, n_init=30).fit(X).labels_\n\n    # Ensure cluster labels are aligned with quantiles\n    if sum(labels[labels == 1]) < len(labels[labels == 0]):\n        labels = 1 - labels\n    df_features['kmeans_label'] = labels\n    df_features.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters.csv'))\n    click.echo(f'Clustering written to \"{METRICS_COMPLEXITY_PATH}\"')", "\n\ndef _plot_scatter_axis(df, ax, label_col, title, labels):\n    for i, l in enumerate(labels):\n        filtered = df[df[label_col] == i]\n        ax.scatter(\n            x=filtered['x'],\n            y=filtered['y'],\n            s=4,\n            alpha=0.75,\n            label=l,\n        )\n    ax.legend(loc='lower right', fontsize='small', borderpad=0.4, shadow=False,\n              handlelength=0.5, handletextpad=0.5, edgecolor='none')\n    ax.set_title(title, fontsize='medium')\n    ax.spines['top'].set_visible(False)\n    ax.set_xticks(ticks=np.linspace(*ax.get_xlim(), 5), labels=[])\n    ax.set_yticks(ticks=np.linspace(*ax.get_ylim(), 5), labels=[])\n    ax.spines['right'].set_visible(False)", "\n\ndef visualize_clusters(quantile):\n    \"\"\"\n    Visualize clusters of HTML page features.\n\n    :param quantile: complexity quantile to align with cluster boundaries\n    \"\"\"\n\n    in_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters.csv')\n    if not os.path.isfile(in_path):\n        raise click.FileError(in_path, 'Please calculate page complexities first.')\n\n    df = pd.read_csv(in_path, index_col=['hash_key', 'dataset'])\n    df['complexity'] = _binarize_complexity(df['complexity'], quantile)\n    df_2d = _reduce_dim_2d(df, 'kmeans_label')\n\n    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(5, 2.5))\n    _plot_scatter_axis(df_2d, ax1, 'kmeans_label', '$k$-Means Clustering', ['Cluster 0', 'Cluster 1'])\n    _plot_scatter_axis(df_2d, ax2, 'complexity', 'Complexity Quantiles', ['Low', 'High'])\n\n    plt.tight_layout(pad=0.5)\n    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters_2d.pdf'))\n    df_2d.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters_2d.csv'))\n\n    click.echo(f'Plots written to \"{METRICS_COMPLEXITY_PATH}')", "\n\ndef visualize_classes():\n    \"\"\"\n    Visualize predicted classes of HTML page features.\n    \"\"\"\n\n    in_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_classes.csv')\n    if not os.path.isfile(in_path):\n        raise click.FileError(in_path, 'Please calculate page complexities first.')\n\n    df = pd.read_csv(in_path, index_col=['hash_key', 'dataset'])\n    df_2d = _reduce_dim_2d(df, 'logreg_label')\n\n    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(5, 2.5))\n    _plot_scatter_axis(df_2d, ax1, 'logreg_label', 'Predicted Classes', ['Low', 'High'])\n    _plot_scatter_axis(df_2d, ax2, 'complexity', 'Complexity Quantiles', ['Low', 'High'])\n\n    plt.tight_layout(pad=0.5)\n    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity_classes_2d.pdf'))\n    click.echo(f'Plots written to \"{METRICS_COMPLEXITY_PATH}\\n')\n\n    acc = accuracy_score(df_2d['complexity'], df_2d['logreg_label'])\n    mcc = matthews_corrcoef(df_2d['complexity'], df_2d['logreg_label'])\n    f1 = f1_score(df_2d['complexity'], df_2d['logreg_label'])\n    prec = precision_score(df_2d['complexity'], df_2d['logreg_label'])\n    rec = recall_score(df_2d['complexity'], df_2d['logreg_label'])\n\n    click.echo(f'MCC: {mcc:.3f}')\n    click.echo(f'Accuracy: {acc:.3f}')\n    click.echo(f'F1 Score: {f1:.3f}')\n    click.echo(f'Precision: {prec:.3f}')\n    click.echo(f'Recall: {rec:.3f}')", "\n\ndef visualize_datasets(datasets, low_quantile='0.25', high_quantile='0.75'):\n    \"\"\"\n    Visualize median complexity of the datasets.\n\n    :param datasets: list of dataset names\n    :param low_quantile: low-complexity quantile threshold\n    :param high_quantile: high-complexity quantile threshold\n    \"\"\"\n    complexities = []\n    with click.progressbar(datasets, label='Loading datasets') as progress:\n        for ds in progress:\n            path = os.path.join(METRICS_COMPLEXITY_PATH, ds, f'{ds}_complexity.csv')\n            if not os.path.isfile(path):\n                continue\n            complexities.append(pd.read_csv(path)['complexity'])\n\n    # Sort by median\n    complexities, datasets = zip(*sorted(zip(complexities, datasets), key=lambda x: x[0].median(), reverse=True))\n\n    plt.figure(figsize=(5, 3))\n    plt.boxplot(\n        complexities,\n        positions=range(len(complexities)),\n        labels=[DATASETS[d] for d in datasets],\n    )\n\n    plt.ylabel('Page Complexity')\n    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    plt.ylim((-0.1, 1.1))\n    for y in plt.gca().get_yticks():\n        plt.axhline(y, linewidth=0.25, color='lightgrey', zorder=-1)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity.pdf'))\n\n    complexity_quantiles = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv'), index_col=0)\n    quantile_threshold_low = complexity_quantiles.loc[float(low_quantile)]['complexity']\n    quantile_threshold_high = complexity_quantiles.loc[float(high_quantile)]['complexity']\n\n    # Sort back into alphabetical order\n    complexities, datasets = zip(*sorted(zip(complexities, datasets), key=lambda x: x[1]))\n\n    click.echo('Dataset stats:')\n    click.echo('--------------')\n    for i, compl in enumerate(complexities):\n        low_count = compl[compl < quantile_threshold_low].count()\n        medium_count = compl[(compl >= quantile_threshold_low) & (compl < quantile_threshold_high)].count()\n        high_count = compl[compl >= quantile_threshold_high].count()\n\n        click.echo(f'{datasets[i]:<20} ', nl=False)\n        click.echo(f'pages: {compl.count():<8} ', nl=False)\n        click.echo(f'pages low: {low_count:<8} ', nl=False)\n        click.echo(f'pages medium: {medium_count:<8} ', nl=False)\n        click.echo(f'pages high: {high_count:<8} ', nl=False)\n        click.echo(f'median complexity: {compl.median():.2f}', nl=False)\n        click.echo()\n    click.echo()\n\n    click.echo(f'Complexity plots written to \"{METRICS_COMPLEXITY_PATH}\".')", ""]}
{"filename": "src/extraction_benchmark/extractors/bte.py", "chunked_list": ["#!/usr/bin/env python\n\n\"\"\"\nThis module implements Finn's BTE (Body Text Extraction) algorithm for\nextracting the main body text of a web page and avoiding the surrounding\nirrelevant information. The description of the algorithm can be found\nin A. Finn, N. Kushmerick, and B. Smyth. Fact or Fiction: Content\nclassification for digital libraries. In DELOS Workshop: Personalisation\nand Recommender System in Digital Libraries, 2001.\n", "and Recommender System in Digital Libraries, 2001.\n\nPython implementation by Jan Pomikalek <xpomikal@fi.muni.cz>\n\"\"\"\n\nimport re\n\n\ndef html2text(html_text, preserve_par=False, preserve_head_list_par=False):\n    \"\"\"\n    Converts HTML to plain text with boilerplate removed.\n    If preserve_par is set to True, paragraph mark-up will be preserverd.\n    If preserve_head_list_par is set to True, paragraph mark-up will be\n    preserverd and headers and list items marked with <h> and <l> tags\n    respectively.\n    \"\"\"\n\n    cleaned_text = preclean(html_text)\n    tokens = tokenise(cleaned_text)\n    (start, end) = bte(tokens)\n    main_body = tokens[start:end+1]\n    cleaned_body = find_paragraphs(main_body, preserve_head_list_par)\n\n    # separate paragraphs with newlines\n    blocks = []\n    block = []\n    for token in cleaned_body:\n        if token_value(token) > 0:\n            block.append(token)\n        else:\n            if len(block) > 0:\n                blocks.append(\" \".join(block))\n                block = []\n            if preserve_par or preserve_head_list_par:\n                block.append(token)\n    if len(block) > 0:\n        blocks.append(\" \".join(block))\n\n    return \"\\n\".join(blocks)", "def html2text(html_text, preserve_par=False, preserve_head_list_par=False):\n    \"\"\"\n    Converts HTML to plain text with boilerplate removed.\n    If preserve_par is set to True, paragraph mark-up will be preserverd.\n    If preserve_head_list_par is set to True, paragraph mark-up will be\n    preserverd and headers and list items marked with <h> and <l> tags\n    respectively.\n    \"\"\"\n\n    cleaned_text = preclean(html_text)\n    tokens = tokenise(cleaned_text)\n    (start, end) = bte(tokens)\n    main_body = tokens[start:end+1]\n    cleaned_body = find_paragraphs(main_body, preserve_head_list_par)\n\n    # separate paragraphs with newlines\n    blocks = []\n    block = []\n    for token in cleaned_body:\n        if token_value(token) > 0:\n            block.append(token)\n        else:\n            if len(block) > 0:\n                blocks.append(\" \".join(block))\n                block = []\n            if preserve_par or preserve_head_list_par:\n                block.append(token)\n    if len(block) > 0:\n        blocks.append(\" \".join(block))\n\n    return \"\\n\".join(blocks)", "\n\ndef preclean(html_text):\n    \"\"\"\n    HTML preprocessing -- striping headers, scripts, styles; replacing HTML\n    entities.\n    \"\"\"\n\n    # strip all but body\n    cleaned_text = re.compile('^.*<body(\\s+[^>]*)?>', re.S | re.I\n            ).sub('', html_text)\n    cleaned_text = re.compile('</body>.*$', re.S | re.I\n            ).sub('', cleaned_text)\n\n    # strip scripts\n    cleaned_text = re.compile('<script(\\s+[^>]*)?>(.|\\s)*?</script>',\n            re.S | re.I).sub('<script></script>', cleaned_text)\n\n    # strip styles\n    cleaned_text = re.compile('<style(\\s+[^>]*)?>(.|\\s)*?</style>',\n            re.S | re.I).sub('<style></style>', cleaned_text)\n\n    # html entities\n    cleaned_text = html_entities(cleaned_text)\n\n    return cleaned_text", "\n\ndef html_entities(html_text):\n    \"Substitution of the most commonly used HTML entities.\"\n    html_text = re.sub('&quot;', '\"', html_text)\n    html_text = re.sub('&nbsp;', ' ', html_text)\n    html_text = re.sub('&#39;', \"'\", html_text)\n    return html_text\n\n\ndef tokenise(html_text):\n    \"\"\"\n    Tokenises HTML document to a sequence of HTML tags and strings of\n    non-whitespace characters (words).\n    \"\"\"\n    return [g1 for (g1, g2) in re.findall('(<([^>]|\\s)+>|[^\\s<]+)', html_text)]", "\n\ndef tokenise(html_text):\n    \"\"\"\n    Tokenises HTML document to a sequence of HTML tags and strings of\n    non-whitespace characters (words).\n    \"\"\"\n    return [g1 for (g1, g2) in re.findall('(<([^>]|\\s)+>|[^\\s<]+)', html_text)]\n\n\ndef bte(tokens):\n    \"\"\"\n    BTE algorithm. Expects a sequence of HTML tags and words as input parameter.\n    Outputs a pair of indices which indicate the beginning and end of the main\n    body.\n    \"\"\"\n\n    # find breakpoints\n    breakpoints = []\n    prev_value = None\n    sum_value = 0\n    for i in range(len(tokens)):\n        cur_value = token_value(tokens[i])\n        if prev_value and cur_value != prev_value:\n            breakpoints.append((i-1, sum_value))\n            sum_value = 0\n        sum_value+= cur_value\n        prev_value = cur_value\n    breakpoints.append((len(tokens)-1, sum_value))\n\n    # find breakpoints range which maximises the score\n    max_score = 0\n    max_start = 0\n    max_end   = 0\n    for i in range(len(breakpoints)):\n        score = breakpoints[i][1]\n        if score > max_score:\n            max_score = score\n            if i > 0: max_start = breakpoints[i-1][0]+1\n            else:     max_start = 0\n            max_end   = breakpoints[i][0]\n        for j in range(i+1, len(breakpoints)):\n            score+= breakpoints[j][1]\n            if score > max_score:\n                max_score = score\n                if i > 0: max_start = breakpoints[i-1][0]+1\n                else:     max_start = 0\n                max_end   = breakpoints[j][0]\n\n    return (max_start, max_end)", "\n\ndef bte(tokens):\n    \"\"\"\n    BTE algorithm. Expects a sequence of HTML tags and words as input parameter.\n    Outputs a pair of indices which indicate the beginning and end of the main\n    body.\n    \"\"\"\n\n    # find breakpoints\n    breakpoints = []\n    prev_value = None\n    sum_value = 0\n    for i in range(len(tokens)):\n        cur_value = token_value(tokens[i])\n        if prev_value and cur_value != prev_value:\n            breakpoints.append((i-1, sum_value))\n            sum_value = 0\n        sum_value+= cur_value\n        prev_value = cur_value\n    breakpoints.append((len(tokens)-1, sum_value))\n\n    # find breakpoints range which maximises the score\n    max_score = 0\n    max_start = 0\n    max_end   = 0\n    for i in range(len(breakpoints)):\n        score = breakpoints[i][1]\n        if score > max_score:\n            max_score = score\n            if i > 0: max_start = breakpoints[i-1][0]+1\n            else:     max_start = 0\n            max_end   = breakpoints[i][0]\n        for j in range(i+1, len(breakpoints)):\n            score+= breakpoints[j][1]\n            if score > max_score:\n                max_score = score\n                if i > 0: max_start = breakpoints[i-1][0]+1\n                else:     max_start = 0\n                max_end   = breakpoints[j][0]\n\n    return (max_start, max_end)", "\n\ndef token_value(token):\n    \"Returns -1 if the token is HTML tag, 1 otherwise (if word).\"\n    if token.startswith('<'):\n        return -1\n    else:\n        return 1\n\n\ndef find_paragraphs(tokens, tag_h_l=False):\n    \"\"\"\n    Marks paragraph blocks with <p>. If tag_h_l set to True, headers and\n    list items are also detected and marked with <h> and <l> respectively.\n    \"\"\"\n\n    PAR_FIND_TAGS = ['p', 'div', 'hr', 'blockquote', 'table']\n    PAR_REPLACE_TAG = '<p>'\n    HEADER_FIND_TAGS = ['h1', 'h2', 'h3']\n    HEADER_REPLACE_TAG = '<h>'\n    LIST_FIND_TAGS = ['li']\n    LIST_REPLACE_TAG = '<l>'\n    result = [PAR_REPLACE_TAG]\n\n    in_paragraph = False\n    for token in tokens:\n        if token_value(token) > 0:\n            result.append(token)\n            in_paragraph = True\n        else:\n            if not in_paragraph:\n                continue\n            m = re.search('^<([^\\s>]+)', token)\n            if not m:\n                continue\n            tag = m.group(1).lower()\n            if tag in PAR_FIND_TAGS:\n                result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n            if tag in HEADER_FIND_TAGS:\n                if tag_h_l:\n                    result.append(HEADER_REPLACE_TAG)\n                else:\n                    result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n            if tag in LIST_FIND_TAGS:\n                if tag_h_l:\n                    result.append(LIST_REPLACE_TAG)\n                else:\n                    result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n\n    return result", "\n\ndef find_paragraphs(tokens, tag_h_l=False):\n    \"\"\"\n    Marks paragraph blocks with <p>. If tag_h_l set to True, headers and\n    list items are also detected and marked with <h> and <l> respectively.\n    \"\"\"\n\n    PAR_FIND_TAGS = ['p', 'div', 'hr', 'blockquote', 'table']\n    PAR_REPLACE_TAG = '<p>'\n    HEADER_FIND_TAGS = ['h1', 'h2', 'h3']\n    HEADER_REPLACE_TAG = '<h>'\n    LIST_FIND_TAGS = ['li']\n    LIST_REPLACE_TAG = '<l>'\n    result = [PAR_REPLACE_TAG]\n\n    in_paragraph = False\n    for token in tokens:\n        if token_value(token) > 0:\n            result.append(token)\n            in_paragraph = True\n        else:\n            if not in_paragraph:\n                continue\n            m = re.search('^<([^\\s>]+)', token)\n            if not m:\n                continue\n            tag = m.group(1).lower()\n            if tag in PAR_FIND_TAGS:\n                result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n            if tag in HEADER_FIND_TAGS:\n                if tag_h_l:\n                    result.append(HEADER_REPLACE_TAG)\n                else:\n                    result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n            if tag in LIST_FIND_TAGS:\n                if tag_h_l:\n                    result.append(LIST_REPLACE_TAG)\n                else:\n                    result.append(PAR_REPLACE_TAG)\n                in_paragraph = False\n\n    return result", ""]}
{"filename": "src/extraction_benchmark/extractors/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .extractors import list_extractors\n"]}
{"filename": "src/extraction_benchmark/extractors/ensemble.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\n\nfrom resiliparse.extract.html2text import extract_plain_text\nfrom resiliparse.parse.html import HTMLTree", "from resiliparse.extract.html2text import extract_plain_text\nfrom resiliparse.parse.html import HTMLTree\n\nfrom extraction_benchmark.paths import *\nfrom extraction_benchmark.util import read_jsonl, tokenize_ws\n\n\n_MODEL_ANSWERS = defaultdict(dict)\n_ENSEMBLE_MODELS = {}\n", "_ENSEMBLE_MODELS = {}\n\n\ndef _load_model_answers(input_models):\n    for m in input_models:\n        if m in _MODEL_ANSWERS:\n            continue\n        for ds in os.listdir(MODEL_OUTPUTS_PATH):\n            in_file = os.path.join(MODEL_OUTPUTS_PATH, ds, m + '.jsonl')\n            if not os.path.isfile(in_file):\n                continue\n            for answer in read_jsonl(in_file):\n                _MODEL_ANSWERS[m][answer['page_id']] = ' '.join(tokenize_ws(answer['plaintext']))", "\n\ndef pad_str_zero(s, n):\n    return ('\\0 ' * n) + s + (' \\0' * n)\n\n\ndef pad_str_space(s):\n    return ' ' + s + ' '\n\n\ndef extract_majority_vote(html, page_id, input_models, model_weights, vote_threshold, ngram_size=5):\n    _load_model_answers(input_models)\n\n    tree = HTMLTree.parse(html)\n    text = pad_str_zero(extract_plain_text(\n        tree, main_content=False, preserve_formatting=False, list_bullets=False,\n        links=False, alt_texts=False, noscript=False, form_fields=False), ngram_size - 1)\n    tokens = tokenize_ws(text)\n    token_votes = [0] * len(tokens)\n\n    for ti in range(ngram_size - 1, len(tokens) - ngram_size + 1):\n        ngram_str_l = pad_str_space(' '.join(tokens[ti - ngram_size + 1:ti + 1]))\n        ngram_str_r = pad_str_space(' '.join(tokens[ti:ti + ngram_size]))\n\n        for m, w in zip(input_models, model_weights):\n            answer = pad_str_zero(_MODEL_ANSWERS[m].get(page_id, ''), ngram_size)\n            if ngram_str_l in answer or ngram_str_r in answer:\n                token_votes[ti] += 1 * w\n            if token_votes[ti] >= vote_threshold:\n                break\n\n    # Strip padding (matters only if vote_threshold == 0, but still...)\n    tokens = tokens[ngram_size - 1:len(tokens) - ngram_size + 1]\n    token_votes = token_votes[ngram_size - 1:len(token_votes) - ngram_size + 1]\n\n    return ' '.join(t for t, v in zip(tokens, token_votes) if v >= vote_threshold)", "\n\ndef extract_majority_vote(html, page_id, input_models, model_weights, vote_threshold, ngram_size=5):\n    _load_model_answers(input_models)\n\n    tree = HTMLTree.parse(html)\n    text = pad_str_zero(extract_plain_text(\n        tree, main_content=False, preserve_formatting=False, list_bullets=False,\n        links=False, alt_texts=False, noscript=False, form_fields=False), ngram_size - 1)\n    tokens = tokenize_ws(text)\n    token_votes = [0] * len(tokens)\n\n    for ti in range(ngram_size - 1, len(tokens) - ngram_size + 1):\n        ngram_str_l = pad_str_space(' '.join(tokens[ti - ngram_size + 1:ti + 1]))\n        ngram_str_r = pad_str_space(' '.join(tokens[ti:ti + ngram_size]))\n\n        for m, w in zip(input_models, model_weights):\n            answer = pad_str_zero(_MODEL_ANSWERS[m].get(page_id, ''), ngram_size)\n            if ngram_str_l in answer or ngram_str_r in answer:\n                token_votes[ti] += 1 * w\n            if token_votes[ti] >= vote_threshold:\n                break\n\n    # Strip padding (matters only if vote_threshold == 0, but still...)\n    tokens = tokens[ngram_size - 1:len(tokens) - ngram_size + 1]\n    token_votes = token_votes[ngram_size - 1:len(token_votes) - ngram_size + 1]\n\n    return ' '.join(t for t, v in zip(tokens, token_votes) if v >= vote_threshold)", ""]}
{"filename": "src/extraction_benchmark/extractors/extractors.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\n\ndef extract_bs4(html, **_):\n    from bs4 import BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n    for e in soup(['script', 'style', 'noscript']):\n        e.decompose()\n    return soup.get_text(separator=' ', strip=True)", "\ndef extract_bs4(html, **_):\n    from bs4 import BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n    for e in soup(['script', 'style', 'noscript']):\n        e.decompose()\n    return soup.get_text(separator=' ', strip=True)\n\n\ndef extract_boilerpipe(html, **_):\n    import boilerpipe.extract as boilerpipe\n    text = boilerpipe.Extractor(extractor='ArticleExtractor', html=html)\n    text = text.getText()\n    return str(text)", "\ndef extract_boilerpipe(html, **_):\n    import boilerpipe.extract as boilerpipe\n    text = boilerpipe.Extractor(extractor='ArticleExtractor', html=html)\n    text = text.getText()\n    return str(text)\n\n\ndef extract_xpath_text(html, **_):\n    import lxml.html\n    root = lxml.html.fromstring(html)\n    text = ' '.join(root.xpath('//body[1]//*[not(name()=\"script\") and not(name()=\"style\")]/text()'))\n    text = re.sub(r'(\\s+\\n\\s*)', '\\n', text)\n    return re.sub(r'[ \\t]{2,}', ' ', text)", "def extract_xpath_text(html, **_):\n    import lxml.html\n    root = lxml.html.fromstring(html)\n    text = ' '.join(root.xpath('//body[1]//*[not(name()=\"script\") and not(name()=\"style\")]/text()'))\n    text = re.sub(r'(\\s+\\n\\s*)', '\\n', text)\n    return re.sub(r'[ \\t]{2,}', ' ', text)\n\n\ndef extract_news_please(html, **_):\n    import newsplease\n    return newsplease.NewsPlease.from_html(html, url=None).maintext", "def extract_news_please(html, **_):\n    import newsplease\n    return newsplease.NewsPlease.from_html(html, url=None).maintext\n\n\ndef extract_readability(html, **_):\n    import readability, html_text\n    doc = readability.Document(html)\n    text = html_text.extract_text(doc.summary(html_partial=True))\n    return text", "\n\ndef extract_go_domdistiller(html, **_):\n    from extraction_benchmark.extractors import go_domdistiller\n    return go_domdistiller.extract(html)\n\n\ndef extract_inscriptis(html, **_):\n    import inscriptis\n    text = inscriptis.get_text(html)\n    return text", "\n\ndef extract_html_text(html, **_):\n    import html_text\n    return html_text.extract_text(html)\n\n\ndef extract_resiliparse(html, **_):\n    from resiliparse.extract import html2text\n    from resiliparse.parse.html import HTMLTree\n    return html2text.extract_plain_text(HTMLTree.parse(html),\n                                        preserve_formatting=True,\n                                        main_content=True,\n                                        list_bullets=False,\n                                        comments=False,\n                                        links=False,\n                                        alt_texts=False)", "\n\ndef extract_bte(html, **_):\n    from extraction_benchmark.extractors import bte\n    return bte.html2text(html)\n\n\ndef extract_trafilatura(html, **_):\n    import trafilatura\n    return trafilatura.extract(html, include_comments=False)", "\n\ndef extract_justext(html, **_):\n    import justext\n    article = ' '.join(\n        [p.text for p in justext.justext(html, justext.get_stoplist(\"English\"), 50, 200, 0.1, 0.2, 0.2, 200, True)\n         if not p.is_boilerplate])\n    return article\n\n\ndef extract_goose3(html, **_):\n    from goose3 import Goose, configuration\n    c = configuration.Configuration()\n    c.http_timeout = 5\n\n    with Goose(c) as g:\n        article = g.extract(raw_html=html)\n        return article.cleaned_text", "\n\ndef extract_goose3(html, **_):\n    from goose3 import Goose, configuration\n    c = configuration.Configuration()\n    c.http_timeout = 5\n\n    with Goose(c) as g:\n        article = g.extract(raw_html=html)\n        return article.cleaned_text", "\n\ndef extract_lxml_cleaner(html, **_):\n    from bs4 import BeautifulSoup\n    from lxml.html.clean import Cleaner\n\n    tag_blacklist = [\n        # important\n        'aside', 'embed', 'footer', 'form', 'head', 'iframe', 'menu', 'object', 'script',\n        # other content\n        'applet', 'audio', 'canvas', 'figure', 'map', 'picture', 'svg', 'video',\n        # secondary\n        'area', 'blink', 'button', 'datalist', 'dialog',\n        'frame', 'frameset', 'fieldset', 'link', 'input', 'ins', 'label', 'legend',\n        'marquee', 'math', 'menuitem', 'nav', 'noscript', 'optgroup', 'option',\n        'output', 'param', 'progress', 'rp', 'rt', 'rtc', 'select', 'source',\n        'style', 'track', 'template', 'textarea', 'time', 'use',\n    ]\n\n    cleaner = Cleaner(\n        annoying_tags=False,  # True\n        comments=True,\n        embedded=False,  # True\n        forms=True,  # True\n        frames=True,  # True\n        javascript=True,\n        links=False,\n        meta=False,\n        page_structure=False,\n        processing_instructions=True,\n        remove_unknown_tags=False,\n        safe_attrs_only=False,\n        scripts=True,\n        style=False,\n        kill_tags=tag_blacklist\n    )\n    return BeautifulSoup(cleaner.clean_html(html), 'html.parser').get_text(separator=' ', strip=True)", "\n\ndef extract_boilernet(html, **_):\n    from extraction_benchmark.extractors import boilernet\n    return boilernet.extract(html)\n\n\ndef extract_web2text(html, **_):\n    from extraction_benchmark.extractors import web2text\n    return web2text.extract(html)", "\n\ndef extract_newspaper3k(html, **_):\n    import newspaper\n    article = newspaper.Article('')\n    article.set_html(html)\n    article.parse()\n    return article.text\n\n\ndef extract_dragnet(html, **_):\n    from dragnet import extract_content\n    return extract_content(html, encoding='utf8')", "\n\ndef extract_dragnet(html, **_):\n    from dragnet import extract_content\n    return extract_content(html, encoding='utf8')\n\n\ndef extract_extractnet(html, **_):\n    from extractnet import Extractor\n    return Extractor().extract(html, encoding='utf8').get('content', '')", "\n\ndef _get_ensemble_model_list(best_only=False, weighted=False):\n    def _ls():\n        if best_only or weighted:\n            return [\n                (extract_goose3, 2 if weighted else 1),\n                (extract_readability, 2 if weighted else 1),\n                (extract_trafilatura, 2 if weighted else 1),\n                (extract_go_domdistiller, 1),\n                (extract_resiliparse, 1),\n                (extract_web2text, 1),\n                (extract_bte, 1),\n                (extract_justext, 1),\n                (extract_boilerpipe, 1),\n            ]\n\n        return [(m, 1) for m in list_extractors(names_only=False, include_ensembles=False)]\n\n    return zip(*[(m.__name__.replace('extract_', ''), w) for m, w in _ls()])", "\n\ndef extract_ensemble_majority(html, page_id):\n    from extraction_benchmark.extractors import ensemble\n    models, weights = _get_ensemble_model_list()\n    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))\n\n\ndef extract_ensemble_best(html, page_id):\n    from extraction_benchmark.extractors import ensemble\n    models, weights = _get_ensemble_model_list(best_only=True)\n    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))", "def extract_ensemble_best(html, page_id):\n    from extraction_benchmark.extractors import ensemble\n    models, weights = _get_ensemble_model_list(best_only=True)\n    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))\n\n\ndef extract_ensemble_weighted(html, page_id):\n    from extraction_benchmark.extractors import ensemble\n    models, weights = _get_ensemble_model_list(best_only=True, weighted=True)\n    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))", "\n\ndef list_extractors(names_only=True, include_ensembles=False):\n    \"\"\"\n    Get a list of all supported extraction systems.\n\n    :param names_only: only return a list of strings (otherwise return extractor routines)\n    :param include_ensembles: include ensemble extractors in the list\n    :return: list of extractor names or functions\n    \"\"\"\n    return [(n.replace('extract_', '') if names_only else m) for n, m in globals().items()\n            if n.startswith('extract_') and (not n.startswith('extract_ensemble') or include_ensembles)]", ""]}
{"filename": "src/extraction_benchmark/extractors/boilernet/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nimport json\nimport os\nimport warnings", "import os\nimport warnings\n\nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport nltk\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\n", "import tensorflow as tf\n\nfrom .net.preprocess import get_feature_vector, get_leaves, process\n\nBOILERNET_ROOT_PATH = os.path.dirname(os.path.abspath(__file__))\n\n_model = None\n_word_map = None\n_tag_map = None\n", "_tag_map = None\n\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n\ndef load_model():\n    global _model, _word_map, _tag_map\n    if not _model:\n        _model = tf.keras.models.load_model(os.path.join(BOILERNET_ROOT_PATH, 'model.h5'))\n        nltk.download('punkt', quiet=True)\n        with open(os.path.join(BOILERNET_ROOT_PATH, 'words.json')) as f:\n            _word_map = json.load(f)\n        with open(os.path.join(BOILERNET_ROOT_PATH, 'tags.json')) as f:\n            _tag_map = json.load(f)\n    return _model, _word_map, _tag_map", "def load_model():\n    global _model, _word_map, _tag_map\n    if not _model:\n        _model = tf.keras.models.load_model(os.path.join(BOILERNET_ROOT_PATH, 'model.h5'))\n        nltk.download('punkt', quiet=True)\n        with open(os.path.join(BOILERNET_ROOT_PATH, 'words.json')) as f:\n            _word_map = json.load(f)\n        with open(os.path.join(BOILERNET_ROOT_PATH, 'tags.json')) as f:\n            _tag_map = json.load(f)\n    return _model, _word_map, _tag_map", "\n\ndef extract(html):\n    model, word_map, tag_map = load_model()\n\n    tags = defaultdict(int)\n    words = defaultdict(int)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        doc = BeautifulSoup(html, features='html5lib')\n    processed = process(doc, tags, words)\n    if not processed:\n        return ''\n\n    inputs = [get_feature_vector(w, t, word_map, tag_map) for w, t, _ in processed]\n    inputs = np.expand_dims(np.stack(inputs), 0)\n    predicted = np.around(model.predict(inputs, verbose=0))\n\n    main_content = ''\n    doc = BeautifulSoup(html, features='html5lib')\n    for i, (leaf, _, _) in enumerate(get_leaves(doc.find_all('html')[0])):\n        if predicted[0, i, 0]:\n            main_content += leaf + '\\n'\n    return main_content.strip()", ""]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/train.py", "chunked_list": ["#! /usr/bin/python3\n\n\nimport argparse\nimport os\nimport pickle\nimport csv\nimport math\n\nimport tensorflow as tf", "\nimport tensorflow as tf\nfrom sklearn.utils import class_weight\n\nfrom .leaf_classifier import LeafClassifier\n\n\ndef get_dataset(dataset_file, batch_size, repeat=True):\n    def _read_example(example):\n        desc = {\n            'doc_feature_list': tf.io.VarLenFeature(tf.int64),\n            'doc_label_list': tf.io.VarLenFeature(tf.int64)\n        }\n        _, seq_features = tf.io.parse_single_sequence_example(example, sequence_features=desc)\n        return tf.sparse.to_dense(seq_features['doc_feature_list']), \\\n            tf.sparse.to_dense(seq_features['doc_label_list'])\n\n    buffer_size = 10 * batch_size\n    dataset = tf.data.TFRecordDataset([dataset_file]) \\\n        .map(_read_example, num_parallel_calls=4) \\\n        .prefetch(buffer_size) \\\n        .padded_batch(\n        batch_size=batch_size,\n        padded_shapes=([None, None], [None, 1]),\n        padding_values=(tf.constant(0, dtype=tf.int64), tf.constant(0, dtype=tf.int64))) \\\n        .shuffle(buffer_size=buffer_size)\n    if repeat:\n        return dataset.repeat()\n    return dataset", "\n\ndef get_class_weights(train_set_file):\n    y_train = []\n    for _, y in get_dataset(train_set_file, 1, False):\n        y_train.extend(y.numpy().flatten())\n    return class_weight.compute_class_weight('balanced', [0, 1], y_train)\n\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('DATA_DIR', help='Directory of files produced by the preprocessing script')\n    ap.add_argument('-l', '--num_layers', type=int, default=2, help='The number of RNN layers')\n    ap.add_argument('-u', '--hidden_units', type=int, default=256,\n                    help='The number of hidden LSTM units')\n    ap.add_argument('-d', '--dropout', type=float, default=0.5, help='The dropout percentage')\n    ap.add_argument('-s', '--dense_size', type=int, default=256, help='Size of the dense layer')\n    ap.add_argument('-e', '--epochs', type=int, default=20, help='The number of epochs')\n    ap.add_argument('-b', '--batch_size', type=int, default=16, help='The batch size')\n    ap.add_argument('--interval', type=int, default=5,\n                    help='Calculate metrics and save the model after this many epochs')\n    ap.add_argument('--working_dir', default='train', help='Where to save checkpoints and logs')\n    args = ap.parse_args()\n\n    info_file = os.path.join(args.DATA_DIR, 'info.pkl')\n    with open(info_file, 'rb') as fp:\n        info = pickle.load(fp)\n        train_steps = math.ceil(info['num_train_examples'] / args.batch_size)\n\n    train_set_file = os.path.join(args.DATA_DIR, 'train.tfrecords')\n    train_dataset = get_dataset(train_set_file, args.batch_size)\n\n    dev_set_file = os.path.join(args.DATA_DIR, 'dev.tfrecords')\n    if os.path.isfile(dev_set_file):\n        dev_dataset = get_dataset(dev_set_file, 1, repeat=False)\n    else:\n        dev_dataset = None\n\n    test_set_file = os.path.join(args.DATA_DIR, 'test.tfrecords')\n    if os.path.isfile(test_set_file):\n        test_dataset = get_dataset(test_set_file, 1, repeat=False)\n    else:\n        test_dataset = None\n\n    class_weights = get_class_weights(train_set_file)\n    print('using class weights {}'.format(class_weights))\n\n    kwargs = {'input_size': info['num_words'] + info['num_tags'],\n              'hidden_size': args.hidden_units,\n              'num_layers': args.num_layers,\n              'dropout': args.dropout,\n              'dense_size': args.dense_size}\n    clf = LeafClassifier(**kwargs)\n\n    ckpt_dir = os.path.join(args.working_dir, 'ckpt')\n    log_file = os.path.join(args.working_dir, 'train.csv')\n    os.makedirs(ckpt_dir, exist_ok=True)\n\n    params_file = os.path.join(args.working_dir, 'params.csv')\n    print('writing {}...'.format(params_file))\n    with open(params_file, 'w') as fp:\n        writer = csv.writer(fp)\n        for arg in vars(args):\n            writer.writerow([arg, getattr(args, arg)])\n\n    clf.train(train_dataset, train_steps, args.epochs, log_file, ckpt_dir, class_weights,\n              dev_dataset, info.get('num_dev_examples'),\n              test_dataset, info.get('num_test_examples'),\n              args.interval)", "\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('DATA_DIR', help='Directory of files produced by the preprocessing script')\n    ap.add_argument('-l', '--num_layers', type=int, default=2, help='The number of RNN layers')\n    ap.add_argument('-u', '--hidden_units', type=int, default=256,\n                    help='The number of hidden LSTM units')\n    ap.add_argument('-d', '--dropout', type=float, default=0.5, help='The dropout percentage')\n    ap.add_argument('-s', '--dense_size', type=int, default=256, help='Size of the dense layer')\n    ap.add_argument('-e', '--epochs', type=int, default=20, help='The number of epochs')\n    ap.add_argument('-b', '--batch_size', type=int, default=16, help='The batch size')\n    ap.add_argument('--interval', type=int, default=5,\n                    help='Calculate metrics and save the model after this many epochs')\n    ap.add_argument('--working_dir', default='train', help='Where to save checkpoints and logs')\n    args = ap.parse_args()\n\n    info_file = os.path.join(args.DATA_DIR, 'info.pkl')\n    with open(info_file, 'rb') as fp:\n        info = pickle.load(fp)\n        train_steps = math.ceil(info['num_train_examples'] / args.batch_size)\n\n    train_set_file = os.path.join(args.DATA_DIR, 'train.tfrecords')\n    train_dataset = get_dataset(train_set_file, args.batch_size)\n\n    dev_set_file = os.path.join(args.DATA_DIR, 'dev.tfrecords')\n    if os.path.isfile(dev_set_file):\n        dev_dataset = get_dataset(dev_set_file, 1, repeat=False)\n    else:\n        dev_dataset = None\n\n    test_set_file = os.path.join(args.DATA_DIR, 'test.tfrecords')\n    if os.path.isfile(test_set_file):\n        test_dataset = get_dataset(test_set_file, 1, repeat=False)\n    else:\n        test_dataset = None\n\n    class_weights = get_class_weights(train_set_file)\n    print('using class weights {}'.format(class_weights))\n\n    kwargs = {'input_size': info['num_words'] + info['num_tags'],\n              'hidden_size': args.hidden_units,\n              'num_layers': args.num_layers,\n              'dropout': args.dropout,\n              'dense_size': args.dense_size}\n    clf = LeafClassifier(**kwargs)\n\n    ckpt_dir = os.path.join(args.working_dir, 'ckpt')\n    log_file = os.path.join(args.working_dir, 'train.csv')\n    os.makedirs(ckpt_dir, exist_ok=True)\n\n    params_file = os.path.join(args.working_dir, 'params.csv')\n    print('writing {}...'.format(params_file))\n    with open(params_file, 'w') as fp:\n        writer = csv.writer(fp)\n        for arg in vars(args):\n            writer.writerow([arg, getattr(args, arg)])\n\n    clf.train(train_dataset, train_steps, args.epochs, log_file, ckpt_dir, class_weights,\n              dev_dataset, info.get('num_dev_examples'),\n              test_dataset, info.get('num_test_examples'),\n              args.interval)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/__init__.py", "chunked_list": [""]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/preprocess.py", "chunked_list": ["#! /usr/bin/python3\n\n\nimport os\nimport argparse\nimport pickle\nimport json\nfrom collections import defaultdict\n\nimport nltk", "\nimport nltk\nimport numpy as np\nimport tensorflow as tf\nfrom bs4 import BeautifulSoup, NavigableString\nfrom tqdm import tqdm\n\nfrom .misc import util\n\n\ndef get_leaves(node, tag_list=[], label=0):\n    \"\"\"Return all leaves (NavigableStrings) in a BS4 tree.\"\"\"\n    tag_list_new = tag_list + [node.name]\n    if node.has_attr('__boilernet_label'):\n        label = int(node['__boilernet_label'])\n\n    result = []\n    for c in node.children:\n        if isinstance(c, NavigableString):\n            # might be just whitespace\n            if c.string is not None and c.string.strip():\n                result.append((c, tag_list_new, label))\n        elif c.name not in util.TAGS_TO_IGNORE:\n            result.extend(get_leaves(c, tag_list_new, label))\n    return result", "\n\ndef get_leaves(node, tag_list=[], label=0):\n    \"\"\"Return all leaves (NavigableStrings) in a BS4 tree.\"\"\"\n    tag_list_new = tag_list + [node.name]\n    if node.has_attr('__boilernet_label'):\n        label = int(node['__boilernet_label'])\n\n    result = []\n    for c in node.children:\n        if isinstance(c, NavigableString):\n            # might be just whitespace\n            if c.string is not None and c.string.strip():\n                result.append((c, tag_list_new, label))\n        elif c.name not in util.TAGS_TO_IGNORE:\n            result.extend(get_leaves(c, tag_list_new, label))\n    return result", "\n\ndef get_leaf_representation(node, tag_list, label):\n    \"\"\"Return dicts of words and HTML tags that representat a leaf.\"\"\"\n    tags_dict = defaultdict(int)\n    for tag in tag_list:\n        tags_dict[tag] += 1\n    words_dict = defaultdict(int)\n    for word in nltk.word_tokenize(node.string):\n        words_dict[word.lower()] += 1\n    return dict(words_dict), dict(tags_dict), label", "\n\ndef process(doc, tags, words):\n    \"\"\"\n    Process \"doc\", updating the tag and word counts.\n    Return the document representation, the HTML tags and the words.\n    \"\"\"\n    result = []\n    for leaf, tag_list, is_content in get_leaves(doc.find_all('html')[0]):\n        leaf_representation = get_leaf_representation(leaf, tag_list, is_content)\n        result.append(leaf_representation)\n        words_dict, tags_dict, _ = leaf_representation\n        for word, count in words_dict.items():\n            words[word] += count\n        for tag, count in tags_dict.items():\n            tags[tag] += count\n    return result", "\n\ndef parse(filenames):\n    \"\"\"\n    Read and parse all HTML files.\n    Return the parsed documents and a set of all words and HTML tags.\n    \"\"\"\n    result = {}\n    tags = defaultdict(int)\n    words = defaultdict(int)\n\n    for f in tqdm(filenames):\n        try:\n            with open(f, 'rb') as hfile:\n                doc = BeautifulSoup(hfile, features='html5lib')\n            basename = os.path.basename(f)\n            result[basename] = process(doc, tags, words)\n        except:\n            tqdm.write('error processing {}'.format(f))\n    return result, tags, words", "\n\ndef get_feature_vector(words_dict, tags_dict, word_map, tag_map):\n    \"\"\"Return a feature vector for an item to be classified.\"\"\"\n    vocab_vec = np.zeros(len(word_map), dtype='int32')\n    for word, num in words_dict.items():\n        # if the item is not in the map, use 0 (OOV word)\n        vocab_vec[word_map.get(word, 0)] = num\n\n    tags_vec = np.zeros(len(tag_map), dtype='int32')\n    for tag, num in tags_dict.items():\n        # if the tag is not in the map, use 0 (OOV tag)\n        tags_vec[tag_map.get(tag, 0)] = num\n\n    return np.concatenate([vocab_vec, tags_vec])", "\n\ndef get_vocabulary(d, num=None):\n    \"\"\"Return an integer map of the top-k vocabulary items and add <UNK>.\"\"\"\n    l = sorted(d.keys(), key=d.get, reverse=True)\n    if num is not None:\n        l = l[:num]\n    int_map = util.get_int_map(l, offset=1)\n    int_map['<UNK>'] = 0\n    return int_map", "\n\ndef get_doc_inputs(docs, word_map, tag_map):\n    \"\"\"Transform \"docs\" into the input format accepted by the classifier.\"\"\"\n\n    def _int64_feature(l):\n        \"\"\"Return an int64_list.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=l))\n\n    for doc in docs:\n        doc_features = []\n        doc_labels = []\n        for words_dict, tags_dict, label in doc:\n            feature_vector = get_feature_vector(words_dict, tags_dict, word_map, tag_map)\n            doc_features.append(_int64_feature(feature_vector))\n            doc_labels.append(_int64_feature([label]))\n        doc_feature_list = tf.train.FeatureList(feature=doc_features)\n        doc_label_list = tf.train.FeatureList(feature=doc_labels)\n        yield doc_feature_list, doc_label_list", "\n\ndef write_tfrecords(filename, dataset, word_map, tag_map):\n    \"\"\"Write the dataset to a .tfrecords file.\"\"\"\n    with tf.io.TFRecordWriter(filename) as writer:\n        for doc_feature_list, doc_label_list in get_doc_inputs(dataset, word_map, tag_map):\n            f = {'doc_feature_list': doc_feature_list, 'doc_label_list': doc_label_list}\n            feature_lists = tf.train.FeatureLists(feature_list=f)\n            example = tf.train.SequenceExample(feature_lists=feature_lists)\n            writer.write(example.SerializeToString())", "\n\ndef save(save_path, word_map, tag_map, train_set, dev_set=None, test_set=None):\n    \"\"\"Save the data.\"\"\"\n    os.makedirs(save_path, exist_ok=True)\n\n    with open(os.path.join(save_path, '../words.json'), 'w', encoding='utf-8') as fp:\n        json.dump(word_map, fp)\n\n    with open(os.path.join(save_path, '../tags.json'), 'w', encoding='utf-8') as fp:\n        json.dump(tag_map, fp)\n\n    info = {}\n    info['num_words'] = len(word_map)\n    info['num_tags'] = len(tag_map)\n\n    train_file = os.path.join(save_path, 'train.tfrecords')\n    print('writing {}...'.format(train_file))\n    write_tfrecords(train_file, train_set, word_map, tag_map)\n    info['num_train_examples'] = len(train_set)\n\n    if dev_set is not None:\n        dev_file = os.path.join(save_path, 'dev.tfrecords')\n        print('writing {}...'.format(dev_file))\n        write_tfrecords(dev_file, dev_set, word_map, tag_map)\n        info['num_dev_examples'] = len(dev_set)\n\n    if test_set is not None:\n        test_file = os.path.join(save_path, 'test.tfrecords')\n        print('writing {}...'.format(test_file))\n        write_tfrecords(test_file, test_set, word_map, tag_map)\n        info['num_test_examples'] = len(test_set)\n\n    info_file = os.path.join(save_path, 'info.pkl')\n    with open(info_file, 'wb') as fp:\n        pickle.dump(info, fp)", "\n\ndef read_file(f_name):\n    with open(f_name, encoding='utf-8') as fp:\n        for line in fp:\n            yield line.strip()\n\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('DIRS', nargs='+', help='A list of directories containing the HTML files')\n    ap.add_argument('-s', '--split_dir', help='Directory that contains train-/dev-/testset split')\n    ap.add_argument('-w', '--num_words', type=int, help='Only use the top-k words')\n    ap.add_argument('-t', '--num_tags', type=int, help='Only use the top-l HTML tags')\n    ap.add_argument('--save', default='result', help='Where to save the results')\n    args = ap.parse_args()\n\n    # files required for tokenization\n    nltk.download('punkt')\n\n    filenames = []\n    for d in args.DIRS:\n        filenames.extend(util.get_filenames(d))\n    data, tags, words = parse(filenames)\n    tags = get_vocabulary(tags, args.num_tags)\n    words = get_vocabulary(words, args.num_words)\n\n    if args.split_dir:\n        train_set_file = os.path.join(args.split_dir, 'train_set.txt')\n        dev_set_file = os.path.join(args.split_dir, 'dev_set.txt')\n        test_set_file = os.path.join(args.split_dir, 'test_set.txt')\n        train_set = [data[basename] for basename in read_file(train_set_file)]\n        dev_set = [data[basename] for basename in read_file(dev_set_file)]\n        test_set = [data[basename] for basename in read_file(test_set_file)]\n    else:\n        train_set = data.values()\n        dev_set, test_set = None, None\n\n    save(args.save, words, tags, train_set, dev_set, test_set)", "def main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('DIRS', nargs='+', help='A list of directories containing the HTML files')\n    ap.add_argument('-s', '--split_dir', help='Directory that contains train-/dev-/testset split')\n    ap.add_argument('-w', '--num_words', type=int, help='Only use the top-k words')\n    ap.add_argument('-t', '--num_tags', type=int, help='Only use the top-l HTML tags')\n    ap.add_argument('--save', default='result', help='Where to save the results')\n    args = ap.parse_args()\n\n    # files required for tokenization\n    nltk.download('punkt')\n\n    filenames = []\n    for d in args.DIRS:\n        filenames.extend(util.get_filenames(d))\n    data, tags, words = parse(filenames)\n    tags = get_vocabulary(tags, args.num_tags)\n    words = get_vocabulary(words, args.num_words)\n\n    if args.split_dir:\n        train_set_file = os.path.join(args.split_dir, 'train_set.txt')\n        dev_set_file = os.path.join(args.split_dir, 'dev_set.txt')\n        test_set_file = os.path.join(args.split_dir, 'test_set.txt')\n        train_set = [data[basename] for basename in read_file(train_set_file)]\n        dev_set = [data[basename] for basename in read_file(dev_set_file)]\n        test_set = [data[basename] for basename in read_file(test_set_file)]\n    else:\n        train_set = data.values()\n        dev_set, test_set = None, None\n\n    save(args.save, words, tags, train_set, dev_set, test_set)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/leaf_classifier.py", "chunked_list": ["#! /usr/bin/python3\n\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\n\nclass Metrics(tf.keras.callbacks.Callback):\n    \"\"\"Calculate metrics for a dev-/testset and add them to the logs.\"\"\"\n    def __init__(self, clf, data, steps, interval, prefix=''):\n        self.clf = clf\n        self.data = data\n        self.steps = steps\n        self.interval = interval\n        self.prefix = prefix\n\n    def on_epoch_end(self, epoch, logs):\n        if (epoch + 1) % self.interval == 0:\n            y_true, y_pred = self.clf.eval(self.data, self.steps, desc=self.prefix)\n            p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n        else:\n            p, r, f, s = np.nan, np.nan, np.nan, np.nan\n        logs_new = {'{}_precision'.format(self.prefix): p,\n                    '{}_recall'.format(self.prefix): r,\n                    '{}_f1'.format(self.prefix): f,\n                    '{}_support'.format(self.prefix): s}\n        logs.update(logs_new)", "\n\nclass Saver(tf.keras.callbacks.Callback):\n    \"\"\"Save the model.\"\"\"\n    def __init__(self, path, interval):\n        self.path = path\n        self.interval = interval\n\n    def on_epoch_end(self, epoch, logs):\n        if (epoch + 1) % self.interval == 0:\n            file_name = os.path.join(self.path, 'model.{:03d}.h5'.format(epoch))\n            self.model.save(file_name)", "\n\n# pylint: disable=E1101\nclass LeafClassifier(object):\n    \"\"\"This classifier assigns labels to sequences based on words and HTML tags.\"\"\"\n    def __init__(self, input_size, num_layers, hidden_size, dropout, dense_size):\n        \"\"\"Construct the network.\"\"\"\n        self.input_size = input_size\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.dropout = dropout\n        self.dense_size = dense_size\n        self.model = self._get_model()\n\n    def _get_model(self):\n        \"\"\"Return a keras model.\"\"\"\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.InputLayer(input_shape=(None, self.input_size)))\n        model.add(tf.keras.layers.Dense(self.dense_size, activation='relu'))\n        model.add(tf.keras.layers.Masking(mask_value=0))\n        for _ in range(self.num_layers):\n            lstm = tf.keras.layers.LSTM(self.hidden_size, return_sequences=True)\n            model.add(tf.keras.layers.Bidirectional(lstm))\n        model.add(tf.keras.layers.Dropout(self.dropout))\n        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n        model.compile(loss='binary_crossentropy', optimizer='adam')\n        return model\n\n    def train(self, train_dataset, train_steps, epochs, log_file, ckpt, class_weight=(1, 1),\n              dev_dataset=None, dev_steps=None, test_dataset=None, test_steps=None, interval=1):\n        \"\"\"Train a number of input sequences.\"\"\"\n        callbacks = [Saver(ckpt, interval)]\n        if dev_dataset is not None:\n            callbacks.append(Metrics(self, dev_dataset, dev_steps, interval, 'dev'))\n        if test_dataset is not None:\n            callbacks.append(Metrics(self, test_dataset, test_steps, interval, 'test'))\n        callbacks.append(tf.keras.callbacks.CSVLogger(log_file))\n\n        self.model.fit(train_dataset, steps_per_epoch=train_steps, epochs=epochs,\n                       callbacks=callbacks, class_weight=class_weight)\n\n    def eval(self, dataset, steps, desc=None):\n        \"\"\"Evaluate the model on the test data and return the metrics.\"\"\"\n        y_true, y_pred = [], []\n        for b_x, b_y in tqdm(dataset, total=steps, desc=desc):\n            # somehow this cast is necessary\n            b_x = tf.dtypes.cast(b_x, 'float32')\n\n            y_true.extend(b_y.numpy().flatten())\n            y_pred.extend(np.around(self.model.predict_on_batch(b_x)).flatten())\n        return y_true, y_pred", ""]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/prepare_dataset.py", "chunked_list": ["import argparse\nimport os\n\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup, NavigableString, Comment\n\nimport util\n\n\ndef process_bp(doc):\n    \"\"\"Process HTML files annotated by BoilerPipe. We ignore nodes labeled as \"comment\".\"\"\"\n    for node in doc.find_all(attrs={'__prediction': True}):\n        node['__boilernet_is_content'] = True\n        del node['__prediction']", "\ndef process_bp(doc):\n    \"\"\"Process HTML files annotated by BoilerPipe. We ignore nodes labeled as \"comment\".\"\"\"\n    for node in doc.find_all(attrs={'__prediction': True}):\n        node['__boilernet_is_content'] = True\n        del node['__prediction']\n\n\ndef process_other(doc):\n    \"\"\"Process manually annotated HTML files. We ignore nodes labeled as \"comment\".\"\"\"\n    for node in doc.find_all(attrs={'__label': True}):\n        if node['__label'] != 'comment':\n            node['__boilernet_is_content'] = True\n        del node['__label']", "def process_other(doc):\n    \"\"\"Process manually annotated HTML files. We ignore nodes labeled as \"comment\".\"\"\"\n    for node in doc.find_all(attrs={'__label': True}):\n        if node['__label'] != 'comment':\n            node['__boilernet_is_content'] = True\n        del node['__label']\n\n\ndef process_gn1(doc):\n    \"\"\"Process HTML files from the GN1 dataset.\"\"\"\n    # the classes x-nc-sel0, x-nc-sel4 and x-nc-sel5 do not seem to indicate content\n    nodes = doc.find_all('span', class_='x-nc-sel1') \\\n            + doc.find_all('span', class_='x-nc-sel2') \\\n            + doc.find_all('span', class_='x-nc-sel3')\n    for node in nodes:\n        node['__boilernet_is_content'] = True", "def process_gn1(doc):\n    \"\"\"Process HTML files from the GN1 dataset.\"\"\"\n    # the classes x-nc-sel0, x-nc-sel4 and x-nc-sel5 do not seem to indicate content\n    nodes = doc.find_all('span', class_='x-nc-sel1') \\\n            + doc.find_all('span', class_='x-nc-sel2') \\\n            + doc.find_all('span', class_='x-nc-sel3')\n    for node in nodes:\n        node['__boilernet_is_content'] = True\n\n\ndef remove_comments(doc):\n    \"\"\"Remove all comments from \"doc\".\"\"\"\n    for node in doc.find_all(text=lambda x: isinstance(x, Comment)):\n        node.extract()", "\n\ndef remove_comments(doc):\n    \"\"\"Remove all comments from \"doc\".\"\"\"\n    for node in doc.find_all(text=lambda x: isinstance(x, Comment)):\n        node.extract()\n\n\ndef process(doc, dataset_function):\n    \"\"\"\n    Wrap each NavigableString in a <span> tag.\n    If the string is content, add a __boilernet_label attribute.\n    Remove all HTML comments from the document.\n    \"\"\"\n    remove_comments(doc)\n    dataset_function(doc)\n    for node, is_content in get_leaves(doc.find_all('html')[0]):\n        # if the parent node is already a span, we don't add another one\n        if node.parent.name == 'span':\n            span = node.parent\n        else:\n            span = doc.new_tag('span')\n            node.wrap(span)\n        if is_content:\n            span['__boilernet_label'] = 1\n        else:\n            span['__boilernet_label'] = 0", "def process(doc, dataset_function):\n    \"\"\"\n    Wrap each NavigableString in a <span> tag.\n    If the string is content, add a __boilernet_label attribute.\n    Remove all HTML comments from the document.\n    \"\"\"\n    remove_comments(doc)\n    dataset_function(doc)\n    for node, is_content in get_leaves(doc.find_all('html')[0]):\n        # if the parent node is already a span, we don't add another one\n        if node.parent.name == 'span':\n            span = node.parent\n        else:\n            span = doc.new_tag('span')\n            node.wrap(span)\n        if is_content:\n            span['__boilernet_label'] = 1\n        else:\n            span['__boilernet_label'] = 0", "\n\ndef get_leaves(node, is_content=False):\n    \"\"\"Return all leaves (NavigableStrings) in a BS4 tree.\"\"\"\n    if node.has_attr('__boilernet_is_content'):\n        is_content = True\n        del node['__boilernet_is_content']\n\n    result = []\n    for c in node.children:\n        if isinstance(c, NavigableString) and not isinstance(c, Comment):\n            # might be just whitespace\n            if c.string is not None and c.string.strip():\n                result.append((c, is_content))\n        elif c.name is not None:\n            if c.name.lower() in util.TAGS_TO_IGNORE:\n                # we remove these tags as they are ignored anyway and can make the file very large\n                c.extract()\n            else:\n                result.extend(get_leaves(c, is_content))\n    return result", "\n\ndef main():\n    dataset_functions = {'bp': process_bp, 'gn1': process_gn1, 'other': process_other}\n    ap = argparse.ArgumentParser()\n    ap.add_argument('INPUT', help='Input directory (html files)')\n    ap.add_argument('OUTPUT', help='Output directory')\n    ap.add_argument('DATASET', choices=dataset_functions.keys(), help='Dataset type')\n    ap.add_argument('--prefix', help='Add a prefix to the file names.')\n    args = ap.parse_args()\n    os.makedirs(args.OUTPUT, exist_ok=True)\n\n    for f in tqdm(util.get_filenames(args.INPUT, '.html')):\n        try:\n            with open(f, 'rb') as hfile:\n                doc = BeautifulSoup(hfile, features='html5lib')\n            # for some reason, parsing malformed HTML twice works better\n            doc2 = BeautifulSoup(doc.prettify(), features='html5lib')\n            process(doc2, dataset_functions[args.DATASET])\n            f_name = os.path.basename(f)\n            if args.prefix:\n                f_name = args.prefix + f_name\n            with open(os.path.join(args.OUTPUT, f_name), 'w', encoding='utf-8') as hfile:\n                hfile.write(doc2.prettify())\n        except:\n            tqdm.write('error processing {}'.format(f))", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/__init__.py", "chunked_list": [""]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/util.py", "chunked_list": ["#! /usr/bin/python3\n\n\nimport random\nimport os\n\n\n# everything that is a child of one of these tags is considered boilerplate and hence ignored by the\n# classifier\nTAGS_TO_IGNORE = {'head', 'iframe', 'script', 'meta', 'link', 'style', 'input', 'checkbox',", "# classifier\nTAGS_TO_IGNORE = {'head', 'iframe', 'script', 'meta', 'link', 'style', 'input', 'checkbox',\n                  'button', 'noscript'}\n\n\ndef get_int_map(items, offset=0):\n    \"\"\"Return a dict that maps each unique item in \"items\" to a unique int ID.\"\"\"\n    # we sort the items to guarantee that we get the same mapping every time for a fixed input\n    return {item: i + offset for i, item in enumerate(sorted(set(items)))}\n", "\n\ndef get_filenames(dir_path, filetype='.html'):\n    \"\"\"Return absolute paths to all files of a given type in a directory.\"\"\"\n    # join the dir path and the filename, then filter out directories\n    all_files = filter(os.path.isfile, map(lambda x: os.path.join(dir_path, x), os.listdir(dir_path)))\n    filtered_files = filter(lambda x: x.endswith(filetype), all_files)\n    return list(filtered_files)\n", ""]}
{"filename": "src/extraction_benchmark/extractors/go_domdistiller/__init__.py", "chunked_list": ["import os\nimport subprocess\nfrom tempfile import TemporaryDirectory\n\n\ndef extract(html, **_):\n    cli_path = os.path.join(os.path.dirname(__file__), 'go_domdistiller_cli')\n\n    with TemporaryDirectory() as tmp_dir:\n        p = os.path.join(tmp_dir, 'go_domdistiller.html')\n        with open(p, 'w') as f:\n            f.write(html)\n        result = subprocess.run([cli_path, p], stdout=subprocess.PIPE)\n    return result.stdout.decode().strip()", ""]}
{"filename": "src/extraction_benchmark/extractors/web2text/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport errno\nimport hashlib\nimport os\nimport subprocess", "import os\nimport subprocess\nimport tempfile\n\nfrom extraction_benchmark.paths import THIRD_PARTY_PATH\n\nWEB2TEXT_BASEPATH = os.path.join(THIRD_PARTY_PATH, 'web2text')\nWEB2TEXT_PYTHONPATH = os.path.join(WEB2TEXT_BASEPATH, 'src', 'main', 'python')\nWEB2TEXT_VENV = os.path.join(WEB2TEXT_BASEPATH, 'venv')\n", "WEB2TEXT_VENV = os.path.join(WEB2TEXT_BASEPATH, 'venv')\n\n\nif not os.path.isdir(WEB2TEXT_PYTHONPATH):\n    raise FileNotFoundError(errno.ENOENT, WEB2TEXT_BASEPATH,\n                            'Could not find Web2Text under current working directory. Please ensure you have the '\n                            'submodule checked out and are running this from the repository\\'s root directory.')\n\nif not os.path.isdir(WEB2TEXT_VENV):\n    raise FileNotFoundError(errno.ENOENT, WEB2TEXT_VENV,\n                            'Could not find venv in Web2Text directory. '\n                            'Please follow README instructions to create one')", "if not os.path.isdir(WEB2TEXT_VENV):\n    raise FileNotFoundError(errno.ENOENT, WEB2TEXT_VENV,\n                            'Could not find venv in Web2Text directory. '\n                            'Please follow README instructions to create one')\n\n\ndef extract(html):\n    scala_cmd = ['scala', '-cp', os.path.join(THIRD_PARTY_PATH, 'web2text.jar')]\n    python_cmd = ['python', os.path.join(WEB2TEXT_PYTHONPATH, 'main.py')]\n    hash_id = hashlib.sha256(html.encode()).hexdigest()\n\n    proc_env = os.environ.copy()\n    proc_env['VIRTUAL_ENV'] = WEB2TEXT_VENV\n    proc_env['PATH'] = '{}/bin:{}'.format(proc_env['VIRTUAL_ENV'], proc_env['PATH'])\n    proc_env['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        file_base = os.path.join(tmp_dir, hash_id)\n        html_file = file_base + '.html'\n        features_file = file_base + '.features'\n        labels_file = file_base + '.labels'\n        text_file = file_base + '.txt'\n\n        open(html_file, 'w').write(html)\n        exit_code = subprocess.Popen(\n            scala_cmd + ['ch.ethz.dalab.web2text.ExtractPageFeatures', html_file, features_file],\n            env=proc_env,\n            stderr=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL\n        ).wait()\n        if exit_code != 0:\n            raise RuntimeError('Web2Text ExtractPageFeatures failed.')\n\n        exit_code = subprocess.Popen(\n            python_cmd + ['classify', features_file, labels_file],\n            env=proc_env,\n            stderr=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL\n        ).wait()\n        if exit_code != 0:\n            raise RuntimeError('Web2Text DOM node classification failed.')\n\n        exit_code = subprocess.Popen(\n            scala_cmd + ['ch.ethz.dalab.web2text.ApplyLabelsToPage', html_file, labels_file, text_file],\n            env=proc_env,\n            stderr=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL\n        ).wait()\n        if exit_code != 0:\n            raise RuntimeError('Web2Text ApplyLabelsToPage failed.')\n\n        return open(text_file, 'r').read()", ""]}
{"filename": "src/extraction_benchmark/cli/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .complexity import complexity\nfrom .eval import eval\nfrom .extract import convert_datasets, extract\n", "from .extract import convert_datasets, extract\n"]}
{"filename": "src/extraction_benchmark/cli/eval.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport glob\n\nimport click\nfrom extraction_benchmark.globals import *", "import click\nfrom extraction_benchmark.globals import *\n\n\n@click.group()\ndef eval():\n    \"\"\"\n    Evaluate model answers against the ground truth.\n    \"\"\"\n", "\n\n@eval.command()\n@click.argument('metric', type=click.Choice(['all', *SCORES]))\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'], multiple=True)\n@click.option('--eval-ensembles', help='Evaluate only ensembles', is_flag=True)\n@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\ndef score(metric, dataset, model, eval_ensembles, parallelism):\n    \"\"\"\n    Calculate performance metrics on model answers.\n    \"\"\"\n\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n    if 'all' in model:\n        model = sorted(MODELS_ALL)\n    if eval_ensembles:\n        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_'))\n    metric = sorted(SCORES) if metric == 'all' else [metric]\n\n    if not dataset:\n        click.echo('No datasets selected.', err=True)\n        return\n\n    if not model:\n        click.echo('No models selected.', err=True)\n        return\n\n    import nltk\n    try:\n        # Needed for Rouge\n        nltk.data.find('tokenizers/punkt')\n    except:\n        nltk.download('punkt')\n\n    from extraction_benchmark.eval import calculcate_scores\n    try:\n        calculcate_scores(metric, dataset, model, parallelism)\n    except FileNotFoundError as e:\n        click.FileError(e.filename,\n                        f'Make sure you have converted the raw datasets using \"convert-datasets\".')", "def score(metric, dataset, model, eval_ensembles, parallelism):\n    \"\"\"\n    Calculate performance metrics on model answers.\n    \"\"\"\n\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n    if 'all' in model:\n        model = sorted(MODELS_ALL)\n    if eval_ensembles:\n        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_'))\n    metric = sorted(SCORES) if metric == 'all' else [metric]\n\n    if not dataset:\n        click.echo('No datasets selected.', err=True)\n        return\n\n    if not model:\n        click.echo('No models selected.', err=True)\n        return\n\n    import nltk\n    try:\n        # Needed for Rouge\n        nltk.data.find('tokenizers/punkt')\n    except:\n        nltk.download('punkt')\n\n    from extraction_benchmark.eval import calculcate_scores\n    try:\n        calculcate_scores(metric, dataset, model, parallelism)\n    except FileNotFoundError as e:\n        click.FileError(e.filename,\n                        f'Make sure you have converted the raw datasets using \"convert-datasets\".')", "\n\n@eval.command()\n@click.argument('score', type=click.Choice(['all', *SCORES]))\n@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'], multiple=True)\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[], multiple=True)\n@click.option('-c', '--complexity', type=click.Choice(['all', *COMPLEXITIES]), default=['all', 'low', 'high'],\n              required=True, multiple=True, show_default=True)\ndef aggregate(score, model, dataset, exclude_dataset, complexity):\n    \"\"\"\n    Aggregate calculated performance metrics.\n    \"\"\"\n    score = sorted(SCORES) if score == 'all' else [score]\n\n    if 'all' in model:\n        model = sorted(MODELS_ALL)\n    if 'all' in dataset:\n        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\n    if not dataset:\n        click.echo('No datasets selected.', err=True)\n        return\n\n    if not model:\n        click.echo('No models selected.', err=True)\n        return\n\n    from extraction_benchmark.eval import aggregate_scores\n    try:\n        for s in score:\n            aggregate_scores(s, model, dataset, complexity)\n    except FileNotFoundError as e:\n        raise click.FileError(e.filename, 'Please calculate complexity scores first.')\n\n    click.echo(f'Aggregations written to \"{METRICS_PATH}\"')", "              required=True, multiple=True, show_default=True)\ndef aggregate(score, model, dataset, exclude_dataset, complexity):\n    \"\"\"\n    Aggregate calculated performance metrics.\n    \"\"\"\n    score = sorted(SCORES) if score == 'all' else [score]\n\n    if 'all' in model:\n        model = sorted(MODELS_ALL)\n    if 'all' in dataset:\n        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\n    if not dataset:\n        click.echo('No datasets selected.', err=True)\n        return\n\n    if not model:\n        click.echo('No models selected.', err=True)\n        return\n\n    from extraction_benchmark.eval import aggregate_scores\n    try:\n        for s in score:\n            aggregate_scores(s, model, dataset, complexity)\n    except FileNotFoundError as e:\n        raise click.FileError(e.filename, 'Please calculate complexity scores first.')\n\n    click.echo(f'Aggregations written to \"{METRICS_PATH}\"')", "\n\n@eval.command()\ndef cythonize_rouge():\n    \"\"\"\n    Cythonize Rouge-Score module.\n\n    By cythonizing the Rouge-Score module, the slow scoring performance can be improved slightly.\n    \"\"\"\n\n    click.confirm('This will cythonize the rouge-score module. '\n                  'You will have to reinstall it to revert the changes. Continue?', abort=True)\n\n    import rouge_score\n    path = os.path.dirname(rouge_score.__file__)\n\n    py_files = glob.glob(os.path.join(path, '*.py'))\n    if not py_files:\n        click.echo('No Python files found in module. Has the module already been cythonized?', err=True)\n        return\n\n    for p in py_files:\n        os.rename(p, p + 'x')\n\n    from Cython.Build.Cythonize import main as cython_main\n    cython_main(['cythonize', '-3', '--inplace', *glob.glob(os.path.join(path, '*.pyx'))])", ""]}
{"filename": "src/extraction_benchmark/cli/extract.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport logging\n\nimport click", "\nimport click\nfrom extraction_benchmark.globals import *\n\n\n@click.command()\n@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'],\n              help='Extraction models (\"all\" does not include ensembles)', multiple=True)\n@click.option('--run-ensembles', is_flag=True, help='Run all ensembles (ignores --model)')\n@click.option('-e', '--exclude-model', type=click.Choice(MODELS_ALL), default=['web2text'], show_default=True,", "@click.option('--run-ensembles', is_flag=True, help='Run all ensembles (ignores --model)')\n@click.option('-e', '--exclude-model', type=click.Choice(MODELS_ALL), default=['web2text'], show_default=True,\n              help='Exclude models if \"all\" are selected.', multiple=True)\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[],\n              help='Exclude datasets if \"all\" are selected.', multiple=True)\n@click.option('-s', '--skip-existing', is_flag=True, help='Load existing answer and extract only new')\n@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\n@click.option('-v', '--verbose', help='Verbose output', is_flag=True)\ndef extract(model, run_ensembles, exclude_model, dataset, exclude_dataset, skip_existing, parallelism, verbose):\n    \"\"\"\n    Run main content extractors on the datasets.\n    \"\"\"\n\n    if not os.path.isdir(DATASET_COMBINED_PATH):\n        raise click.UsageError('Combined dataset not found. '\n                               'Please create the converted dataset first using the \"convert-datasets\" command.')\n\n    if run_ensembles:\n        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_') and m not in exclude_model)\n    elif 'all' in model:\n        model = sorted(m for m in MODELS if m not in exclude_model)\n        click.confirm('This will run ALL models. Continue?', abort=True)\n\n    if not os.path.isdir(MODEL_OUTPUTS_PATH):\n        for m in model:\n            if m.startswith('ensemble_'):\n                raise click.UsageError('Model outputs need to be generated before ensemble can be run.')\n\n    if 'all' in dataset:\n        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\n    if not dataset:\n        click.echo('No input datasets found.\\n'\n                   'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n                   'under the current working directory.', err=True)\n        return\n\n    if parallelism > 1 and ('web2text' in model or 'boilernet' in model):\n        click.echo('WARNING: Deep neural models selected. If you run into GPU memory issues, '\n                   'try running with --parallelism=1.', err=True)\n\n    from extraction_benchmark import extract\n    extract.extract(model, dataset, skip_existing, parallelism, verbose)", "@click.option('-v', '--verbose', help='Verbose output', is_flag=True)\ndef extract(model, run_ensembles, exclude_model, dataset, exclude_dataset, skip_existing, parallelism, verbose):\n    \"\"\"\n    Run main content extractors on the datasets.\n    \"\"\"\n\n    if not os.path.isdir(DATASET_COMBINED_PATH):\n        raise click.UsageError('Combined dataset not found. '\n                               'Please create the converted dataset first using the \"convert-datasets\" command.')\n\n    if run_ensembles:\n        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_') and m not in exclude_model)\n    elif 'all' in model:\n        model = sorted(m for m in MODELS if m not in exclude_model)\n        click.confirm('This will run ALL models. Continue?', abort=True)\n\n    if not os.path.isdir(MODEL_OUTPUTS_PATH):\n        for m in model:\n            if m.startswith('ensemble_'):\n                raise click.UsageError('Model outputs need to be generated before ensemble can be run.')\n\n    if 'all' in dataset:\n        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\n    if not dataset:\n        click.echo('No input datasets found.\\n'\n                   'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n                   'under the current working directory.', err=True)\n        return\n\n    if parallelism > 1 and ('web2text' in model or 'boilernet' in model):\n        click.echo('WARNING: Deep neural models selected. If you run into GPU memory issues, '\n                   'try running with --parallelism=1.', err=True)\n\n    from extraction_benchmark import extract\n    extract.extract(model, dataset, skip_existing, parallelism, verbose)", "\n\n@click.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[], multiple=True)\ndef convert_datasets(dataset, exclude_dataset):\n    \"\"\"\n    Combine raw datasets and convert them to a line-delimieted JSON format.\n    \"\"\"\n\n    if not os.path.isdir(DATASET_RAW_PATH):\n        raise click.UsageError('Raw datasets not found. '\n                               'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n                               'under the current working directory.')\n\n    if 'all' in dataset:\n        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\n    from extraction_benchmark import extract\n    page_ids = extract.extract_ground_truth(dataset)\n    extract.extract_raw_html(dataset, page_ids)", ""]}
{"filename": "src/extraction_benchmark/cli/complexity.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport click\nfrom extraction_benchmark.globals import *\n", "from extraction_benchmark.globals import *\n\n\n@click.group()\ndef complexity():\n    \"\"\"\n    Calculate page extraction complexities.\n    \"\"\"\n    pass\n", "\n\n@complexity.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\ndef calculate(dataset):\n    \"\"\"\n    Calculate page complexities.\n\n    Calculate page complexities for given datasets based on the ground truth.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    if not dataset:\n        click.echo('No input datasets found.\\n'\n                   'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n                   'under the current working directory.', err=False)\n        return\n\n    from extraction_benchmark.complexity import calculate\n    calculate(dataset)", "\n\n@complexity.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-l', '--low-quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']),\n              help='Low-complexity quantile threshold', default='0.25')\n@click.option('-h', '--high-quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']),\n              help='High-complexity quantile threshold', default='0.75')\ndef visualize(dataset, low_quantile, high_quantile):\n    \"\"\"\n    Visualize the median complexity of the datasets.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    from extraction_benchmark.complexity import visualize_datasets\n    visualize_datasets(dataset, low_quantile, high_quantile)", "def visualize(dataset, low_quantile, high_quantile):\n    \"\"\"\n    Visualize the median complexity of the datasets.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    from extraction_benchmark.complexity import visualize_datasets\n    visualize_datasets(dataset, low_quantile, high_quantile)\n", "\n\n@complexity.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\ndef extract_features(dataset, parallelism):\n    \"\"\"\n    Extract HTML features.\n\n    Extract HTML features from ground truth pages for complexity clustering.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    from extraction_benchmark.complexity import extract_page_features\n    extract_page_features(dataset, parallelism)", "\n\n@complexity.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-r', '--reduce-dim', type=int,\n              help='Reduce dimensionality before clustering (0 for no reduction)')\n@click.option('-c', '--clusters', type=int, default=2, help='Number of clusters')\ndef cluster(dataset, reduce_dim, clusters):\n    \"\"\"\n    Perform a k-means clustering.\n\n    Perform a k-means clustering of previously extract HTML features to estimate complexity.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    from extraction_benchmark.complexity import kmeans_cluster\n    try:\n        kmeans_cluster(dataset, reduce_dim, clusters)\n    except FileNotFoundError as e:\n        raise click.FileError(e.filename, 'Make sure HTML features have been calculated.')", "\n\n@complexity.command()\n@click.option('-q', '--quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']), default='0.33',\n              help='Quantile boundary')\ndef visualize_clusters(quantile):\n    \"\"\"\n    Visualize k-means clustering.\n\n    Visualize k-means clustering of HTML pages and align clusters with given complexity quantile.\n    \"\"\"\n\n    from extraction_benchmark.complexity import visualize_clusters\n    visualize_clusters(quantile)", "\n\n@complexity.command()\n@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n@click.option('-s', '--split-size', type=click.FloatRange(0.1, 0.9), default=0.25, help='Training split size')\n@click.option('-q', '--quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']), default='0.5',\n              help='Quantile boundary')\ndef classify(dataset, split_size, quantile):\n    \"\"\"\n    Train and evaluate a logistic regression classifier.\n\n    Train a logistic regression classifier on a split of the complexity scores and classify the remaining ones.\n    \"\"\"\n    if 'all' in dataset:\n        dataset = sorted(DATASETS)\n\n    from extraction_benchmark.complexity import logistic_regression_classify\n    try:\n        logistic_regression_classify(dataset, split_size, quantile)\n    except FileNotFoundError as e:\n        raise click.FileError(e.filename, 'Make sure HTML features have been calculated.')", "\n\n@complexity.command()\ndef visualize_classes():\n    \"\"\"\n    Visualize logistic regression classification.\n    \"\"\"\n\n    from extraction_benchmark.complexity import visualize_classes\n    visualize_classes()", ""]}
