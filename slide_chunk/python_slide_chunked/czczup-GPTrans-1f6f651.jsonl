{"filename": "main.py", "chunked_list": ["import os\nimport time\nimport random\nimport argparse\nimport datetime\nimport numpy as np\nimport subprocess\n\nimport torch\nimport torch.backends.cudnn as cudnn", "import torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nfrom timm.utils import ModelEma\nfrom timm.utils import AverageMeter\nfrom config import get_config\nfrom models import build_model\nfrom models import BinaryCrossEntropyLoss, CrossEntropyLoss, DiceLoss\nfrom dataset import build_loader\nfrom lr_scheduler import build_scheduler", "from dataset import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils import accuracy_SBM\nfrom utils import NativeScalerWithGradNormCount as NativeScaler\nfrom utils import (load_checkpoint, load_pretrained, save_checkpoint,\n                   auto_resume_helper, reduce_tensor, load_ema_checkpoint)\nfrom ddp_hooks import fp16_compress_hook\n\ntry:\n    if getattr(torch.cuda.amp, 'autocast') is not None:\n        has_native_amp = True\n    else:\n        has_native_amp = False\nexcept AttributeError:\n    has_native_amp = False", "from ddp_hooks import fp16_compress_hook\n\ntry:\n    if getattr(torch.cuda.amp, 'autocast') is not None:\n        has_native_amp = True\n    else:\n        has_native_amp = False\nexcept AttributeError:\n    has_native_amp = False\n", "\n\ndef parse_option():\n    parser = argparse.ArgumentParser(\n        'GPTrans training and evaluation script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file')\n    parser.add_argument(\"--opts\", help=\"Modify config options by adding 'KEY VALUE' pairs. \", default=None, nargs='+')\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--dataset', type=str, help='dataset name', default=None)\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--pretrained', help='pretrained weight from checkpoint, could be imagenet22k pretrained weight')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, default=1, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true', help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n        help='mixed precision opt level, if O0, no amp is used')\n    parser.add_argument('--output', default='work_dirs', type=str, metavar='PATH',\n        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n    parser.add_argument('--save-ckpt-num', default=1, type=int)\n    parser.add_argument('--use-zero', action='store_true', help=\"whether to use ZeroRedundancyOptimizer (ZeRO) to save memory\")\n\n    # distributed training\n    parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    args, unparsed = parser.parse_known_args()\n    config = get_config(args)\n\n    return args, config", "\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    model.eval()\n\n    for idx, data in enumerate(data_loader):\n        batch_size = data['x'].shape[0]\n        for i in range(100):\n            model(data)\n        torch.cuda.synchronize()\n        logger.info(f\"throughput averaged with 100 times\")\n        tic1 = time.time()\n        for i in range(100):\n            model(data)\n        torch.cuda.synchronize()\n        tic2 = time.time()\n        logger.info(\n            f\"batch_size {batch_size} throughput {100 * batch_size / (tic2 - tic1)}\")\n        return", "\n\ndef build_criterion(config):\n    if config.TRAIN.CRITERION == 'mse':\n        criterion = torch.nn.L1Loss()\n    elif config.TRAIN.CRITERION == 'bce':\n        criterion = BinaryCrossEntropyLoss(config.TRAIN.CLASS_WEIGHTS, config.TRAIN.REDUCE_ZERO_LABEL)\n    elif config.TRAIN.CRITERION == 'ce':\n        criterion = CrossEntropyLoss(config.TRAIN.CLASS_WEIGHTS, config.TRAIN.REDUCE_ZERO_LABEL)\n    elif config.TRAIN.CRITERION == 'dice':\n        criterion = DiceLoss(config.TRAIN.REDUCE_ZERO_LABEL)\n    else:\n        raise ValueError(f'unknown {config.TRAIN.CRITERION}')\n    return criterion", "                \n    \ndef main(config):\n    # prepare data loaders\n    dataset_train, dataset_val, dataset_test, data_loader_train, \\\n        data_loader_val, data_loader_test = build_loader(config)\n\n    # build runner\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    model.cuda()\n    logger.info(str(model))\n\n    # build optimizer\n    optimizer = build_optimizer(config, model)\n    if config.AMP_OPT_LEVEL != \"O0\":\n        config.defrost()\n        if has_native_amp:\n            config.native_amp = True\n            use_amp = 'native'\n        config.freeze()\n\n    # setup automatic mixed-precision (AMP) loss scaling and op casting\n    loss_scaler = None\n    if config.AMP_OPT_LEVEL != \"O0\":\n        if use_amp == 'native':\n            loss_scaler = NativeScaler()\n            if config.LOCAL_RANK == 0:\n                logger.info('Using native Torch AMP. Training in mixed precision.')\n        else:\n            if config.LOCAL_RANK == 0:\n                logger.info('AMP not enabled. Training in float32.')\n\n    # put model on gpus\n    model = torch.nn.parallel.DistributedDataParallel(\n        model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False,\n        gradient_as_bucket_view=True)\n\n    try:\n        model.register_comm_hook(state=None, hook=fp16_compress_hook)\n        logger.info('using fp16_compress_hook!')\n    except:\n        logger.info(\"cannot register fp16_compress_hook!\")\n\n    model_without_ddp = model.module\n\n    n_parameters = sum(p.numel() for p in model.parameters()\n                       if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters / 1000 / 1000}M\")\n    if hasattr(model_without_ddp, 'flops'):\n        flops = model_without_ddp.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    # build learning rate scheduler\n    lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train)) \\\n        if not config.EVAL_MODE else None\n\n    # build criterion\n    criterion = build_criterion(config)\n        \n    if config.DATA.METRIC in ['MAE']:\n        best_performance, best_performance_ema = 99999, 99999\n    else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n        best_performance, best_performance_ema = 0, 0\n        \n    # set auto resume\n    if config.MODEL.RESUME == '' and config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    # set resume and pretrain\n    if config.MODEL.RESUME:\n        best_performance = load_checkpoint(config, model_without_ddp, optimizer,\n                                           lr_scheduler, loss_scaler, logger)\n        performance, loss = validate(config, data_loader_val, model)\n        logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n        performance, loss = validate(config, data_loader_test, model)\n        logger.info(f\"{config.DATA.METRIC} on the {len(dataset_test)} test graphs: {performance:.4f}\")\n    elif config.MODEL.PRETRAINED:\n        load_pretrained(config, model_without_ddp, logger)\n        if data_loader_val is not None:\n            performance, loss = validate(config, data_loader_val, model)\n            logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\n    # evaluate EMA\n    model_ema = None\n    if config.TRAIN.EMA.ENABLE:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(model, decay=config.TRAIN.EMA.DECAY)\n        logger.info(\"Using EMA with decay = %.8f\" % config.TRAIN.EMA.DECAY)\n        if config.MODEL.RESUME:\n            best_performance_ema = load_ema_checkpoint(config, model_ema, logger)\n            performance, loss = validate(config, data_loader_val, model_ema.ema)\n            logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n            performance, loss = validate(config, data_loader_test, model_ema.ema)\n            logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_test)} test graphs: {performance:.4f}\")\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n\n    if config.EVAL_MODE:\n        exit()\n\n    # train\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, criterion, data_loader_train, optimizer,\n                        epoch, lr_scheduler, loss_scaler, model_ema=model_ema)\n        if (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)) and config.TRAIN.OPTIMIZER.USE_ZERO:\n            optimizer.consolidate_state_dict(to=0)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n                            loss_scaler, logger, best_performance=best_performance,\n                            best_performance_ema=best_performance_ema, model_ema=model_ema)\n            \n        if data_loader_val is not None and epoch % config.EVAL_FREQ == 0:\n            performance, loss = validate(config, data_loader_val, model, epoch)\n            logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n            if config.DATA.METRIC in ['MAE']:\n                best_flag = performance < best_performance\n            else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n                best_flag = performance > best_performance\n            if dist.get_rank() == 0 and best_flag:\n                save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n                                loss_scaler, logger, best_performance=best_performance,\n                                best_performance_ema=best_performance_ema,\n                                model_ema=model_ema, best='best')\n            if config.DATA.METRIC in ['MAE']:\n                best_performance = min(best_performance, performance)\n            else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n                best_performance = max(best_performance, performance)\n            logger.info(f'Best {config.DATA.METRIC}: {best_performance:.4f}')\n\n            if config.TRAIN.EMA.ENABLE:\n                performance, loss = validate(config, data_loader_val, model_ema.ema, epoch)\n                logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n                if config.DATA.METRIC in ['MAE']:\n                    best_flag = performance < best_performance_ema\n                else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n                    best_flag = performance > best_performance_ema\n                if dist.get_rank() == 0 and best_flag:\n                    save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n                                    loss_scaler, logger, best_performance=best_performance,\n                                    best_performance_ema=best_performance_ema,\n                                    model_ema=model_ema, best='ema_best')\n                if config.DATA.METRIC in ['MAE']:\n                    best_performance_ema = min(best_performance_ema, performance)\n                else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n                    best_performance_ema = max(best_performance_ema, performance)\n                logger.info(f'Best EMA {config.DATA.METRIC}: {best_performance_ema:.4f}')\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))", "\n\ndef train_one_epoch(config, model, criterion, data_loader, optimizer, epoch,\n                    lr_scheduler, loss_scaler=None, model_ema=None):\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    model_time = AverageMeter()\n    loss_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n\n    for idx, data in enumerate(data_loader):\n        iter_begin_time = time.time()\n        samples = data\n        targets = data['y'].cuda(non_blocking=True)\n        targets = targets.reshape(-1) # for compatibility with node classification\n        \n        with torch.cuda.amp.autocast():\n            outputs = model(samples)\n        loss = criterion(outputs, targets)\n        \n        if config.TRAIN.ACCUMULATION_STEPS > 1: # with gradient accumulation\n            loss = loss / config.TRAIN.ACCUMULATION_STEPS\n            if config.AMP_OPT_LEVEL != \"O0\":\n                is_second_order = hasattr(optimizer, 'is_second_order') and \\\n                    optimizer.is_second_order\n                loss_scaler(loss,\n                            optimizer,\n                            clip_grad=config.TRAIN.CLIP_GRAD,\n                            parameters=model.parameters(),\n                            create_graph=is_second_order,\n                            update_grad=(idx + 1) %\n                            config.TRAIN.ACCUMULATION_STEPS == 0)\n                if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                    optimizer.zero_grad()\n                    if model_ema is not None:\n                        model_ema.update(model)\n            else:\n                loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n                if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    if model_ema is not None:\n                        model_ema.update(model)\n            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                lr_scheduler.step_update(epoch * num_steps + idx)\n        else:  # without gradient accumulation\n            optimizer.zero_grad()\n            if config.AMP_OPT_LEVEL != \"O0\":\n                is_second_order = hasattr(optimizer, 'is_second_order') and \\\n                    optimizer.is_second_order\n                loss_scaler(loss,\n                            optimizer,\n                            clip_grad=config.TRAIN.CLIP_GRAD,\n                            parameters=model.parameters(),\n                            create_graph=is_second_order,\n                            update_grad=(idx + 1) %\n                            config.TRAIN.ACCUMULATION_STEPS == 0)\n                if model_ema is not None:\n                    model_ema.update(model)\n            else:\n                loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n                optimizer.step()\n                if model_ema is not None:\n                    model_ema.update(model)\n            lr_scheduler.step_update(epoch * num_steps + idx)\n\n        if idx % config.PRINT_FREQ == 0:\n            torch.cuda.synchronize()\n    \n            loss_meter.update(loss.item(), targets.size(0))\n            batch_time.update((time.time() - end) / config.PRINT_FREQ)\n            model_time.update(time.time() - iter_begin_time)\n            end = time.time()\n            \n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'model_time {model_time.val:.4f} ({model_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")", "\n\n@torch.no_grad()\ndef calculate_performance(config, output, target):\n    if config.DATA.METRIC == 'MAE':\n        import ogb\n        evaluator = ogb.lsc.PCQM4Mv2Evaluator()\n        input_dict = {'y_pred': output, 'y_true': target}\n        performance = evaluator.eval(input_dict)['mae']\n        if output.shape[0] == 147037:\n            print(\"save the output for the test set!\")\n            input_dict = {'y_pred': output.cpu().numpy()}\n            evaluator.save_test_submission(input_dict=input_dict, dir_path=\"./submit\", mode='test-dev')\n    elif config.DATA.METRIC == 'Accuracy':\n        mask = ~torch.isnan(target)\n        if config.TRAIN.REDUCE_ZERO_LABEL:\n            target = target - 1\n        performance = accuracy_SBM(output[mask], target[mask])\n    elif config.DATA.METRIC == 'ROC_AUC':\n        from ogb.graphproppred import Evaluator\n        evaluator = Evaluator(name=\"ogbg-molhiv\")\n        input_dict = {'y_pred': output[:, None].sigmoid(), 'y_true': target[:, None]}\n        performance = evaluator.eval(input_dict)['rocauc']\n    elif config.DATA.METRIC == 'AP':\n        from ogb.graphproppred import Evaluator\n        evaluator = Evaluator(name=\"ogbg-molpcba\")\n        if config.TRAIN.REDUCE_ZERO_LABEL:\n            target = target - 1\n        target = target.reshape(output.shape)\n        input_dict = {'y_pred': output.sigmoid(), 'y_true': target}\n        performance = evaluator.eval(input_dict)['ap']\n    elif config.DATA.METRIC == 'F1_Score':\n        # TODO: add F1-score\n        performance = None\n    else:\n        raise NotImplementedError\n    performance = torch.tensor(performance, device=output.device)\n    performance = reduce_tensor(performance).item()\n    return performance", "\n\n@torch.no_grad()\ndef validate(config, data_loader, model, epoch=None):\n    criterion = build_criterion(config)\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n\n    end = time.time()\n    \n    outputs, targets = [], []\n    for idx, data in enumerate(data_loader):\n        target = data['y'].cuda(non_blocking=True)\n        target = target.reshape(-1) # for compatibility with node classification\n        output = model(data)\n        outputs.append(output)\n        targets.append(target)\n        \n        # measure accuracy and record loss\n        loss = criterion(output, target)\n        loss = reduce_tensor(loss)\n        loss_meter.update(loss.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(f'Test: [{idx}/{len(data_loader)}]\\t'\n                        f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                        f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                        f'Mem {memory_used:.0f}MB')\n    outputs = torch.cat(outputs, dim=0)\n    targets = torch.cat(targets, dim=0)\n    \n    performance = calculate_performance(config, outputs, targets)\n    if epoch is not None:\n        logger.info(f'[Epoch:{epoch}] * {config.DATA.METRIC} {performance:.4f}')\n    else:\n        logger.info(f' * {config.DATA.METRIC} {performance:.4f}')\n    return performance, loss_meter.avg", "\n\nif __name__ == '__main__':\n    _, config = parse_option()\n\n    if config.AMP_OPT_LEVEL != \"O0\":\n        assert has_native_amp, \"Please update pytorch(1.6+) to support amp!\"\n\n    # init distributed env\n    if 'SLURM_PROCID' in os.environ and int(os.environ['SLURM_NNODES']) != 1:\n        print(\"\\nDist init: SLURM\")\n        rank = int(os.environ['SLURM_PROCID'])\n        gpu = rank % torch.cuda.device_count()\n        config.defrost()\n        config.LOCAL_RANK = gpu\n        config.freeze()\n\n        world_size = int(os.environ[\"SLURM_NTASKS\"])\n        if \"MASTER_PORT\" not in os.environ:\n            os.environ[\"MASTER_PORT\"] = \"29501\"\n        node_list = os.environ[\"SLURM_NODELIST\"]\n        addr = subprocess.getoutput(f\"scontrol show hostname {node_list} | head -n1\")\n        if \"MASTER_ADDR\" not in os.environ:\n            os.environ[\"MASTER_ADDR\"] = addr\n\n        os.environ['RANK'] = str(rank)\n        os.environ['LOCAL_RANK'] = str(gpu)\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())\n        os.environ['WORLD_SIZE'] = str(world_size)\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl',\n                                         init_method='env://',\n                                         world_size=world_size,\n                                         rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    cudnn.benchmark = False\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    print(config.AMP_OPT_LEVEL, _.amp_opt_level)\n\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT,\n                           dist_rank=dist.get_rank(),\n                           name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n    main(config)", ""]}
{"filename": "config.py", "chunked_list": ["import os\nimport yaml\nfrom yacs.config import CfgNode as CN\n\n_C = CN()\n\n# Base config files\n_C.BASE = ['']\n\n# -----------------------------------------------------------------------------", "\n# -----------------------------------------------------------------------------\n# Data settings\n# -----------------------------------------------------------------------------\n_C.DATA = CN()\n# Batch size for a single GPU, could be overwritten by command line argument\n_C.DATA.BATCH_SIZE = 128\n# Path to dataset, could be overwritten by command line argument\n_C.DATA.DATA_PATH = ''\n# Dataset name", "_C.DATA.DATA_PATH = ''\n# Dataset name\n_C.DATA.DATASET = 'pcqm4m'\n# Dataset source\n_C.DATA.SOURCE = 'ogb'\n# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n_C.DATA.PIN_MEMORY = True\n# Number of data loading threads\n_C.DATA.NUM_WORKERS = 16\n# Max nodes", "_C.DATA.NUM_WORKERS = 16\n# Max nodes\n_C.DATA.TRAIN_MAX_NODES = 512\n_C.DATA.INFER_MAX_NODES = 512\n# Multi hop max distance\n_C.DATA.MULTI_HOP_MAX_DIST = 20\n# spatial pos max\n_C.DATA.SPATIAL_POS_MAX = 1024\n# number of atoms\n_C.DATA.NUM_ATOMS = 4608", "# number of atoms\n_C.DATA.NUM_ATOMS = 4608\n# numebr of edges\n_C.DATA.NUM_EDGES = 1536\n# number of in degree\n_C.DATA.NUM_IN_DEGREE = 512\n# number of out degree\n_C.DATA.NUM_OUT_DEGREE = 512\n# number of edge distance\n_C.DATA.NUM_EDGE_DIST = 128", "# number of edge distance\n_C.DATA.NUM_EDGE_DIST = 128\n# number of spatial\n_C.DATA.NUM_SPATIAL = 512\n# edge type\n_C.DATA.EDGE_TYPE = 'multi_hop'\n# metric name\n_C.DATA.METRIC = 'MAE'\n# task type\n_C.DATA.TASK_TYPE = 'graph_regression'", "# task type\n_C.DATA.TASK_TYPE = 'graph_regression'\n# -----------------------------------------------------------------------------\n# Model settings\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Model type\n_C.MODEL.TYPE = 'GPTrans'\n# Model name\n_C.MODEL.NAME = 'GPTrans'", "# Model name\n_C.MODEL.NAME = 'GPTrans'\n# Pretrained weight from checkpoint, could be pcqm4m pretrained weight\n_C.MODEL.PRETRAINED = ''\n# Checkpoint to resume, could be overwritten by command line argument\n_C.MODEL.RESUME = ''\n# Number of classes, overwritten in data preparation\n_C.MODEL.NUM_CLASSES = 1\n# Dropout rate\n_C.MODEL.DROP_RATE = 0.0", "# Dropout rate\n_C.MODEL.DROP_RATE = 0.0\n# Drop path rate\n_C.MODEL.DROP_PATH_RATE = 0.0\n# Drop path type\n_C.MODEL.DROP_PATH_TYPE = 'linear'  # linear, uniform\n# Attention drop rate\n_C.MODEL.ATTN_DROP_RATE = 0.0\n\n# GPTRANS parameters", "\n# GPTRANS parameters\n_C.MODEL.GPTRANS = CN()\n_C.MODEL.GPTRANS.NUM_LAYERS = 24\n_C.MODEL.GPTRANS.NUM_HEADS = 23\n_C.MODEL.GPTRANS.NODE_DIM = 736\n_C.MODEL.GPTRANS.EDGE_DIM = 92\n_C.MODEL.GPTRANS.LAYER_SCALE = None\n_C.MODEL.GPTRANS.MLP_RATIO = 1.0\n_C.MODEL.GPTRANS.POST_NORM = False", "_C.MODEL.GPTRANS.MLP_RATIO = 1.0\n_C.MODEL.GPTRANS.POST_NORM = False\n_C.MODEL.GPTRANS.NUM_CLASSES = 1\n\n# -----------------------------------------------------------------------------\n# Training settings\n# -----------------------------------------------------------------------------\n_C.TRAIN = CN()\n_C.TRAIN.START_EPOCH = 0\n_C.TRAIN.EPOCHS = 300", "_C.TRAIN.START_EPOCH = 0\n_C.TRAIN.EPOCHS = 300\n_C.TRAIN.WARMUP_EPOCHS = 20\n_C.TRAIN.WEIGHT_DECAY = 0.05\n_C.TRAIN.BASE_LR = 5e-4\n_C.TRAIN.WARMUP_LR = 5e-7\n_C.TRAIN.MIN_LR = 5e-6\n# Clip gradient norm\n_C.TRAIN.CLIP_GRAD = None\n# Auto resume from latest checkpoint", "_C.TRAIN.CLIP_GRAD = None\n# Auto resume from latest checkpoint\n_C.TRAIN.AUTO_RESUME = True\n# Gradient accumulation steps\n# could be overwritten by command line argument\n_C.TRAIN.ACCUMULATION_STEPS = 0\n# Whether to use gradient checkpointing to save memory\n# could be overwritten by command line argument\n_C.TRAIN.USE_CHECKPOINT = False\n# criterion type", "_C.TRAIN.USE_CHECKPOINT = False\n# criterion type\n_C.TRAIN.CRITERION = 'mse'\n# class weights\n_C.TRAIN.CLASS_WEIGHTS = None\n# reduce zero label\n_C.TRAIN.REDUCE_ZERO_LABEL = False\n\n# LR scheduler\n_C.TRAIN.LR_SCHEDULER = CN()", "# LR scheduler\n_C.TRAIN.LR_SCHEDULER = CN()\n_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n# Epoch interval to decay LR, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n# LR decay rate, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n\n# Optimizer\n_C.TRAIN.OPTIMIZER = CN()", "# Optimizer\n_C.TRAIN.OPTIMIZER = CN()\n_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n# Optimizer Epsilon\n_C.TRAIN.OPTIMIZER.EPS = 1e-8\n# Optimizer Betas\n_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n# SGD momentum\n_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n# ZeRO", "_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n# ZeRO\n_C.TRAIN.OPTIMIZER.USE_ZERO = False\n\n# EMA\n_C.TRAIN.EMA = CN()\n_C.TRAIN.EMA.ENABLE = False\n_C.TRAIN.EMA.DECAY = 0.9998\n\n# FLAG", "\n# FLAG\n# self.flag_m = cfg.flag_m\n# self.flag_step_size = cfg.flag_step_size\n# self.flag_mag = cfg.flag_mag\n_C.TRAIN.FLAG = CN()\n_C.TRAIN.FLAG.ENABLE = False\n_C.TRAIN.FLAG.FLAG_M = 3\n_C.TRAIN.FLAG.FLAG_STEP_SIZE = 0.01\n_C.TRAIN.FLAG.FLAG_MAG = 0", "_C.TRAIN.FLAG.FLAG_STEP_SIZE = 0.01\n_C.TRAIN.FLAG.FLAG_MAG = 0\n\n# LR_LAYER_DECAY\n_C.TRAIN.LR_LAYER_DECAY = False\n_C.TRAIN.LR_LAYER_DECAY_RATIO = 0.875\n\n# FT head init weights\n_C.TRAIN.RAND_INIT_FT_HEAD = False\n_C.TRAIN.PARAM_EFFICIENT_TUNING = False", "_C.TRAIN.RAND_INIT_FT_HEAD = False\n_C.TRAIN.PARAM_EFFICIENT_TUNING = False\n# -----------------------------------------------------------------------------\n# Augmentation settings\n# -----------------------------------------------------------------------------\n_C.AUG = CN()\n_C.AUG.RANDOM_FEATURE = False\n\n\n# -----------------------------------------------------------------------------", "\n# -----------------------------------------------------------------------------\n# Testing settings\n# -----------------------------------------------------------------------------\n_C.TEST = CN()\n# Whether to use SequentialSampler as validation sampler\n_C.TEST.SEQUENTIAL = False\n\n# -----------------------------------------------------------------------------\n# Misc", "# -----------------------------------------------------------------------------\n# Misc\n# -----------------------------------------------------------------------------\n# Mixed precision opt level, if O0, no amp is used ('O0', 'O1', 'O2')\n# overwritten by command line argument\n_C.AMP_OPT_LEVEL = ''\n# Path to output folder, overwritten by command line argument\n_C.OUTPUT = ''\n# Tag of experiment, overwritten by command line argument\n_C.TAG = 'default'", "# Tag of experiment, overwritten by command line argument\n_C.TAG = 'default'\n# Frequency to save checkpoint\n_C.SAVE_FREQ = 1\n# Frequency to logging info\n_C.PRINT_FREQ = 10\n# eval freq\n_C.EVAL_FREQ = 1\n# Fixed random seed\n_C.SEED = 0", "# Fixed random seed\n_C.SEED = 0\n# Perform evaluation only, overwritten by command line argument\n_C.EVAL_MODE = False\n# Test throughput only, overwritten by command line argument\n_C.THROUGHPUT_MODE = False\n# local rank for DistributedDataParallel, given by command line argument\n_C.LOCAL_RANK = 0\n\n_C.AMP_TYPE = 'float16'", "\n_C.AMP_TYPE = 'float16'\n\n\ndef _update_config_from_file(config, cfg_file):\n    config.defrost()\n    with open(cfg_file, 'r') as f:\n        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\n    for cfg in yaml_cfg.setdefault('BASE', ['']):\n        if cfg:\n            _update_config_from_file(\n                config, os.path.join(os.path.dirname(cfg_file), cfg))\n    print('=> merge config from {}'.format(cfg_file))\n    config.merge_from_file(cfg_file)\n    config.freeze()", "\n\ndef update_config(config, args):\n    _update_config_from_file(config, args.cfg)\n\n    config.defrost()\n    if hasattr(args, 'opts') and args.opts:\n        config.merge_from_list(args.opts)\n\n    # merge from specific arguments\n    if hasattr(args, 'batch_size') and args.batch_size:\n        config.DATA.BATCH_SIZE = args.batch_size\n    if hasattr(args, 'dataset') and args.dataset:\n        config.DATA.DATASET = args.dataset\n    if hasattr(args, 'source') and args.source:\n        config.DATA.SOURCE = args.source\n    if hasattr(args, 'data_path') and args.data_path:\n        config.DATA.DATA_PATH = args.data_path\n    if hasattr(args, 'pretrained') and args.pretrained:\n        config.MODEL.PRETRAINED = args.pretrained\n    if hasattr(args, 'resume') and args.resume:\n        config.MODEL.RESUME = args.resume\n    if hasattr(args, 'accumulation_steps') and args.accumulation_steps:\n        config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n    if hasattr(args, 'use_checkpoint') and args.use_checkpoint:\n        config.TRAIN.USE_CHECKPOINT = True\n    if hasattr(args, 'amp_opt_level') and args.amp_opt_level:\n        config.AMP_OPT_LEVEL = args.amp_opt_level\n    if hasattr(args, 'output') and args.output:\n        config.OUTPUT = args.output\n    if hasattr(args, 'tag') and args.tag:\n        config.TAG = args.tag\n    if hasattr(args, 'eval') and args.eval:\n        config.EVAL_MODE = True\n    if hasattr(args, 'throughput') and args.throughput:\n        config.THROUGHPUT_MODE = True\n    if hasattr(args, 'save_ckpt_num') and args.save_ckpt_num:\n        config.SAVE_CKPT_NUM = args.save_ckpt_num\n    if hasattr(args, 'use_zero') and args.use_zero:\n        config.TRAIN.OPTIMIZER.USE_ZERO = True\n    # set local rank for distributed training\n    if hasattr(args, 'local_rank') and args.local_rank:\n        config.LOCAL_RANK = args.local_rank\n\n    # output folder\n    config.MODEL.NAME = args.cfg.split('/')[-1].replace('.yaml', '')\n    config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME)\n\n    config.freeze()", "\n\ndef get_config(args):\n    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    config = _C.clone()\n    update_config(config, args)\n\n    return config", ""]}
{"filename": "optimizer.py", "chunked_list": ["from torch import optim as optim\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom apex.optimizers import FusedAdam\nimport logging\nlogger = logging.getLogger(__name__)\n\n\ndef build_optimizer(config, model):\n    \"\"\"\n    Build optimizer, set weight decay of normalization to 0 by default.\n    \"\"\"\n    skip = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n\n    parameters = set_weight_decay_and_lr(\n        model,\n        config.TRAIN.WEIGHT_DECAY,\n        config.TRAIN.BASE_LR,\n        skip,\n        skip_keywords,\n        lr_layer_decay=config.TRAIN.LR_LAYER_DECAY,\n        lr_layer_decay_ratio=config.TRAIN.LR_LAYER_DECAY_RATIO,\n    )\n\n    opt_lower = config.TRAIN.OPTIMIZER.NAME.lower()\n    optimizer = None\n    use_zero = config.TRAIN.OPTIMIZER.USE_ZERO\n    if use_zero:\n        logger.info(f\"\\nUse Zero!\")\n        if opt_lower == 'sgd':\n            # an ugly implementation\n            # https://github.com/pytorch/pytorch/issues/71347\n            optimizer = ZeroRedundancyOptimizer(\n                parameters[0]['params'],\n                optimizer_class=optim.SGD,\n                momentum=config.TRAIN.OPTIMIZER.MOMENTUM,\n                nesterov=True,\n                lr=config.TRAIN.BASE_LR,\n                weight_decay=config.TRAIN.WEIGHT_DECAY)\n            if len(parameters[1]['params']) > 0:\n                optimizer.add_param_group({\n                    \"params\": parameters[1]['params'],\n                    'weight_decay': 0.\n                })\n        elif opt_lower == 'adamw':\n            optimizer = ZeroRedundancyOptimizer(\n                parameters[0]['params'],\n                optimizer_class=optim.AdamW,\n                eps=config.TRAIN.OPTIMIZER.EPS,\n                betas=config.TRAIN.OPTIMIZER.BETAS,\n                lr=config.TRAIN.BASE_LR,\n                weight_decay=config.TRAIN.WEIGHT_DECAY)\n            if len(parameters[1]['params']) > 0:\n                optimizer.add_param_group({\n                    \"params\": parameters[1]['params'],\n                    'weight_decay': 0.\n                })\n    else:\n        if opt_lower == 'sgd':\n            optimizer = optim.SGD(parameters,\n                                  momentum=config.TRAIN.OPTIMIZER.MOMENTUM,\n                                  nesterov=True,\n                                  lr=config.TRAIN.BASE_LR,\n                                  weight_decay=config.TRAIN.WEIGHT_DECAY)\n        elif opt_lower == 'adamw':\n            optimizer = optim.AdamW(parameters,\n                                    eps=config.TRAIN.OPTIMIZER.EPS,\n                                    betas=config.TRAIN.OPTIMIZER.BETAS,\n                                    lr=config.TRAIN.BASE_LR,\n                                    weight_decay=config.TRAIN.WEIGHT_DECAY)\n        elif opt_lower == 'fused_adamw':\n            optimizer = FusedAdam(parameters,\n                                  eps=config.TRAIN.OPTIMIZER.EPS,\n                                  betas=config.TRAIN.OPTIMIZER.BETAS,\n                                  lr=config.TRAIN.BASE_LR,\n                                  weight_decay=config.TRAIN.WEIGHT_DECAY)\n    return optimizer", "\n\ndef check_keywords_in_name(name, keywords=()):\n    isin = False\n    for keyword in keywords:\n        if keyword in name:\n            isin = True\n    return isin\n\n\ndef check_keywords_in_dict(name, keywords_dict):\n    for k, v in keywords_dict.items():\n        if k in name:\n            return v\n    return None", "\n\ndef check_keywords_in_dict(name, keywords_dict):\n    for k, v in keywords_dict.items():\n        if k in name:\n            return v\n    return None\n\n\ndef set_weight_decay_and_lr(model, weight_decay, base_lr, skip_list=(), skip_keywords=(),\n                            lr_layer_decay=None, lr_layer_decay_ratio=None, layerwise_lr=True):\n    parameters = {}\n    no_decay_name = []\n    lr_ratio_log = {}\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n\n        # 1. check wd\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (\n                name in skip_list) or check_keywords_in_name(\n                    name, skip_keywords):\n            wd = 0.\n            no_decay_name.append(name)\n        else:\n            wd = weight_decay\n\n        # 2. set weight_decay\n        if lr_layer_decay:\n            logger.info('layer-wise lr decay is used !')\n            assert hasattr(model, 'lr_decay_keywords')\n            lr_ratio_keywords = model.lr_decay_keywords(lr_layer_decay_ratio)\n            # 2. check lr\n            ratio = check_keywords_in_dict(name, lr_ratio_keywords)\n            if ratio is not None:\n                lr = ratio * base_lr\n            else:\n                lr = base_lr\n\n            lr_ratio_log[name] = (base_lr, ratio, wd, param.requires_grad)\n        else:\n            lr = base_lr\n        group_name = f\"weight_decay_{str(wd)}_lr_{str(lr)}\"\n        if group_name not in parameters:\n            parameters[group_name] = {'params': [param], 'weight_decay': wd, 'lr': lr}\n        else:\n            parameters[group_name]['params'].append(param)\n\n    logger.info(f'no decay params: {no_decay_name}')\n    if layerwise_lr:\n        logger.info('lr_ratio_params:')\n        for k, v in lr_ratio_log.items():\n            print(k, v)\n    parameters = list(parameters.values())\n    return parameters", "\ndef set_weight_decay_and_lr(model, weight_decay, base_lr, skip_list=(), skip_keywords=(),\n                            lr_layer_decay=None, lr_layer_decay_ratio=None, layerwise_lr=True):\n    parameters = {}\n    no_decay_name = []\n    lr_ratio_log = {}\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n\n        # 1. check wd\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (\n                name in skip_list) or check_keywords_in_name(\n                    name, skip_keywords):\n            wd = 0.\n            no_decay_name.append(name)\n        else:\n            wd = weight_decay\n\n        # 2. set weight_decay\n        if lr_layer_decay:\n            logger.info('layer-wise lr decay is used !')\n            assert hasattr(model, 'lr_decay_keywords')\n            lr_ratio_keywords = model.lr_decay_keywords(lr_layer_decay_ratio)\n            # 2. check lr\n            ratio = check_keywords_in_dict(name, lr_ratio_keywords)\n            if ratio is not None:\n                lr = ratio * base_lr\n            else:\n                lr = base_lr\n\n            lr_ratio_log[name] = (base_lr, ratio, wd, param.requires_grad)\n        else:\n            lr = base_lr\n        group_name = f\"weight_decay_{str(wd)}_lr_{str(lr)}\"\n        if group_name not in parameters:\n            parameters[group_name] = {'params': [param], 'weight_decay': wd, 'lr': lr}\n        else:\n            parameters[group_name]['params'].append(param)\n\n    logger.info(f'no decay params: {no_decay_name}')\n    if layerwise_lr:\n        logger.info('lr_ratio_params:')\n        for k, v in lr_ratio_log.items():\n            print(k, v)\n    parameters = list(parameters.values())\n    return parameters", ""]}
{"filename": "logger.py", "chunked_list": ["import os\nimport sys\nimport logging\nimport functools\nfrom termcolor import colored\n\n\n@functools.lru_cache()\ndef create_logger(output_dir, dist_rank=0, name=''):\n    # create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n        colored('(%(filename)s %(lineno)d)', 'yellow') + \\\n        ': %(levelname)s %(message)s'\n\n    # create console handlers for master process\n    if dist_rank == 0:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n\n    # create file handlers\n    file_handler = logging.FileHandler(os.path.join(\n        output_dir, f'log_rank{dist_rank}.txt'),\n                                       mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(\n        logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger", "def create_logger(output_dir, dist_rank=0, name=''):\n    # create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n        colored('(%(filename)s %(lineno)d)', 'yellow') + \\\n        ': %(levelname)s %(message)s'\n\n    # create console handlers for master process\n    if dist_rank == 0:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n\n    # create file handlers\n    file_handler = logging.FileHandler(os.path.join(\n        output_dir, f'log_rank{dist_rank}.txt'),\n                                       mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(\n        logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger", ""]}
{"filename": "ddp_hooks.py", "chunked_list": ["from typing import Any, Callable\n\nimport torch\nimport torch.distributed as dist\n\n\ndef _allreduce_fut(process_group: dist.ProcessGroup,\n                   tensor: torch.Tensor) -> torch.futures.Future[torch.Tensor]:\n    \"Averages the input gradient tensor by allreduce and returns a future.\"\n    group_to_use = process_group if process_group is not None else dist.group.WORLD\n\n    # Apply the division first to avoid overflow, especially for FP16.\n    tensor.div_(group_to_use.size())\n\n    return (dist.all_reduce(\n        tensor, group=group_to_use,\n        async_op=True).get_future().then(lambda fut: fut.value()[0]))", "\n\ndef allreduce_hook(\n        process_group: dist.ProcessGroup,\n        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n    \"\"\"\n    This DDP communication hook just calls ``allreduce`` using ``GradBucket``\n    tensors. Once gradient tensors are aggregated across all workers, its ``then``\n    callback takes the mean and returns the result. If user registers this hook,\n    DDP results is expected to be same as the case where no hook was registered.\n    Hence, this won't change behavior of DDP and user can use this as a reference\n    or modify this hook to log useful information or any other purposes while\n    unaffecting DDP behavior.\n\n    Example::\n        >>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n    \"\"\"\n    return _allreduce_fut(process_group, bucket.buffer())", "\n\ndef fp16_compress_hook(\n        process_group: dist.ProcessGroup,\n        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n    \"\"\"\n    This DDP communication hook implements a simple gradient compression\n    approach that casts ``GradBucket`` tensor to half-precision floating-point format (``torch.float16``)\n    and then divides it by the process group size.\n    It allreduces those ``float16`` gradient tensors. Once compressed gradient\n    tensors are allreduced, the chained callback ``decompress`` casts it back to the input data type (such as ``float32``).\n\n    Example::\n        >>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n    \"\"\"\n    group_to_use = process_group if process_group is not None else dist.group.WORLD\n    world_size = group_to_use.size()\n\n    compressed_tensor = bucket.buffer().to(torch.float16).div_(world_size)\n\n    fut = dist.all_reduce(compressed_tensor, group=group_to_use,\n                          async_op=True).get_future()\n\n    def decompress(fut):\n        decompressed_tensor = bucket.buffer()\n        # Decompress in place to reduce the peak memory.\n        # See: https://github.com/pytorch/pytorch/issues/45968\n        decompressed_tensor.copy_(fut.value()[0])\n        return decompressed_tensor\n\n    return fut.then(decompress)", "\n\n# TODO: create an internal helper function and extract the duplicate code in FP16_compress and BF16_compress.\n\n\ndef bf16_compress_hook(\n        process_group: dist.ProcessGroup,\n        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n    \"\"\"\n    Warning: This API is experimental, and it requires NCCL version later than 2.9.6.\n\n    This DDP communication hook implements a simple gradient compression\n    approach that casts ``GradBucket`` tensor to half-precision\n    `Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ (``torch.bfloat16``)\n    and then divides it by the process group size.\n    It allreduces those ``bfloat16`` gradient tensors. Once compressed gradient\n    tensors are allreduced, the chained callback ``decompress`` casts it back to the input data type (such as ``float32``).\n\n    Example::\n        >>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)\n    \"\"\"\n    group_to_use = process_group if process_group is not None else dist.group.WORLD\n    world_size = group_to_use.size()\n\n    compressed_tensor = bucket.buffer().to(torch.bfloat16).div_(world_size)\n\n    fut = dist.all_reduce(compressed_tensor, group=group_to_use,\n                          async_op=True).get_future()\n\n    def decompress(fut):\n        decompressed_tensor = bucket.buffer()\n        # Decompress in place to reduce the peak memory.\n        # See: https://github.com/pytorch/pytorch/issues/45968\n        decompressed_tensor.copy_(fut.value()[0])\n        return decompressed_tensor\n\n    return fut.then(decompress)", "\n\ndef fp16_compress_wrapper(\n    hook: Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]\n) -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:\n    \"\"\"\n    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision\n    floating point format (``torch.float16``), and casts the resulting tensor of the given hook back to\n    the input data type, such as ``float32``.\n\n    Therefore, ``fp16_compress_hook`` is equivalent to ``fp16_compress_wrapper(allreduce_hook)``.\n\n    Example::\n        >>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n        >>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))\n    \"\"\"\n\n    def fp16_compress_wrapper_hook(\n            hook_state,\n            bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n        # Cast bucket tensor to FP16.\n        bucket.set_buffer(bucket.buffer().to(torch.float16))\n\n        fut = hook(hook_state, bucket)\n\n        def decompress(fut):\n            decompressed_tensor = bucket.buffer()\n            # Decompress in place to reduce the peak memory.\n            # See: https://github.com/pytorch/pytorch/issues/45968\n            decompressed_tensor.copy_(fut.value())\n            return decompressed_tensor\n\n        # Decompress after hook has run.\n        return fut.then(decompress)\n\n    return fp16_compress_wrapper_hook", "\n\ndef bf16_compress_wrapper(\n    hook: Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]\n) -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:\n    \"\"\"\n    Warning: This API is experimental, and it requires NCCL version later than 2.9.6.\n\n    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision\n    `Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_  (``torch.bfloat16``),\n    and casts the resulting tensor of the given hook back to the input data type, such as ``float32``.\n\n    Therefore, ``bf16_compress_hook`` is equivalent to ``bf16_compress_wrapper(allreduce_hook)``.\n\n    Example::\n        >>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n        >>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))\n    \"\"\"\n\n    def bf16_compress_wrapper_hook(\n            hook_state,\n            bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n        # Cast bucket tensor to BF16.\n        bucket.set_buffer(bucket.buffer().to(torch.bfloat16))\n\n        fut = hook(hook_state, bucket)\n\n        def decompress(fut):\n            decompressed_tensor = bucket.buffer()\n            # Decompress in place to reduce the peak memory.\n            # See: https://github.com/pytorch/pytorch/issues/45968\n            decompressed_tensor.copy_(fut.value())\n            return decompressed_tensor\n\n        # Decompress after hook has run.\n        return fut.then(decompress)\n\n    return bf16_compress_wrapper_hook", ""]}
{"filename": "utils.py", "chunked_list": ["import os\nimport math\nimport torch\nimport numpy as np\nimport torch.distributed as dist\nfrom collections import OrderedDict\nfrom timm.utils import get_state_dict\nfrom sklearn.metrics import confusion_matrix\ntry:\n    # noinspection PyUnresolvedReferences\n    from apex import amp\nexcept ImportError:\n    amp = None", "try:\n    # noinspection PyUnresolvedReferences\n    from apex import amp\nexcept ImportError:\n    amp = None\n\n\ndef load_ema_checkpoint(config, model_ema, logger):\n    logger.info(\n        f'==============> Resuming form {config.MODEL.RESUME}....................'\n    )\n    if config.MODEL.RESUME.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(config.MODEL.RESUME,\n                                                        map_location='cpu',\n                                                        check_hash=True)\n    else:\n        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\n    assert isinstance(checkpoint, dict)\n    if 'model_ema' in checkpoint:\n        new_state_dict = OrderedDict()\n        for k, v in checkpoint['model_ema'].items():\n            if model_ema.ema_has_module:\n                name = 'module.' + k if not k.startswith('module') else k\n            else:\n                name = k\n            new_state_dict[name] = v\n        msg = model_ema.ema.load_state_dict(new_state_dict, strict=False)\n        logger.info(msg)\n        logger.info('Loaded state_dict_ema')\n    else:\n        logger.warning(\n            'Failed to find state_dict_ema, starting from loaded model weights'\n        )\n\n    best_performance_ema = 0\n    if 'best_performance_ema' in checkpoint:\n        best_performance_ema = checkpoint['best_performance_ema']\n    if 'ema_decay' in checkpoint:\n        model_ema.decay = checkpoint['ema_decay']\n    return best_performance_ema", "\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, scaler, logger):\n    logger.info(\n        f'==============> Resuming form {config.MODEL.RESUME}....................'\n    )\n    if config.MODEL.RESUME.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(config.MODEL.RESUME,\n                                                        map_location='cpu',\n                                                        check_hash=True)\n    else:\n        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\n    print('resuming model')\n    msg = model.load_state_dict(checkpoint['model'], strict=False)\n    logger.info(msg)\n    best_performance = 0.0\n    if not config.EVAL_MODE and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n        if optimizer is not None:\n            print('resuming optimizer')\n            try:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n            except:\n                print('resume optimizer failed')\n        if lr_scheduler is not None:\n            print('resuming lr_scheduler')\n            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n        config.defrost()\n        config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n        config.freeze()\n        if 'amp' in checkpoint and config.AMP_OPT_LEVEL != 'O0' and checkpoint[\n                'config'].AMP_OPT_LEVEL != 'O0':\n            scaler.load_state_dict(checkpoint['amp'])\n        logger.info(\n            f\"=> loaded successfully {config.MODEL.RESUME} (epoch {checkpoint['epoch']})\"\n        )\n        if 'best_performance' in checkpoint:\n            best_performance = checkpoint['best_performance']\n\n    del checkpoint\n    torch.cuda.empty_cache()\n\n    return best_performance", "\n\ndef load_pretrained(config, model, logger):\n    logger.info(\n        f'==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......'\n    )\n    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n    if \"ema\" in config.MODEL.PRETRAINED:\n        state_dict = checkpoint['model_ema']\n        logger.info(f'==============> Loading ema weight......')\n    else:\n        if 'model' in checkpoint:\n            state_dict = checkpoint['model']\n        elif 'module' in checkpoint:\n            state_dict = checkpoint['module']\n\n    if config.TRAIN.RAND_INIT_FT_HEAD:\n        model.head.weight.data = model.head.weight.data * 0.001\n        model.head.bias.data = model.head.bias.data * 0.001\n        del state_dict['head.weight']\n        del state_dict['head.bias']\n        logger.warning(f'Re-init classifier head to 0!')\n        \n    msg = model.load_state_dict(state_dict, strict=False)\n    logger.warning(msg)\n    logger.info(f'=> loaded successfully {config.MODEL.PRETRAINED}')\n\n    del checkpoint\n    torch.cuda.empty_cache()", "\n\ndef save_checkpoint(config, epoch, model, optimizer, lr_scheduler, scaler, logger, best_performance,\n                    model_ema=None, ema_decay=None, best_performance_ema=None, best=None):\n    save_state = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'lr_scheduler': lr_scheduler.state_dict(),\n        'epoch': epoch,\n        'config': config\n    }\n    if model_ema is not None:\n        save_state['model_ema'] = get_state_dict(model_ema)\n    if ema_decay is not None:\n        save_state['ema_decay'] = ema_decay\n    if best_performance is not None:\n        save_state['best_performance'] = best_performance\n    if best_performance_ema is not None:\n        save_state['best_performance_ema'] = best_performance_ema\n    if config.AMP_OPT_LEVEL != 'O0':\n        # save_state['amp'] = amp.state_dict()\n        save_state['amp'] = scaler.state_dict()\n    if best is None:\n        save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth')\n    else:\n        save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{best}.pth')\n    logger.info(f'{save_path} saving......')\n    torch.save(save_state, save_path)\n    logger.info(f'{save_path} saved !!!')\n\n    if dist.get_rank() == 0 and isinstance(epoch, int):\n        to_del = epoch - config.SAVE_CKPT_NUM * config.SAVE_FREQ\n        old_ckpt = os.path.join(config.OUTPUT, f'ckpt_epoch_{to_del}.pth')\n        if os.path.exists(old_ckpt):\n            os.remove(old_ckpt)", "\n\ndef get_grad_norm(parameters, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item()**norm_type\n    total_norm = total_norm**(1. / norm_type)\n    return total_norm", "\n\ndef auto_resume_helper(output_dir):\n    checkpoints = os.listdir(output_dir)\n    checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith('pth')]\n    print(f'All checkpoints founded in {output_dir}: {checkpoints}')\n    if len(checkpoints) > 0:\n        latest_checkpoint = max(\n            [os.path.join(output_dir, d) for d in checkpoints],\n            key=os.path.getmtime)\n        print(f'The latest checkpoint founded: {latest_checkpoint}')\n        resume_file = latest_checkpoint\n    else:\n        resume_file = None\n    return resume_file", "\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= dist.get_world_size()\n    return rt\n\n\ndef all_gather_tensor(tensor_list, tensor):\n    dist.all_gather(tensor_list, tensor)\n    return tensor_list", "\ndef all_gather_tensor(tensor_list, tensor):\n    dist.all_gather(tensor_list, tensor)\n    return tensor_list\n\n\nclass ApexScalerWithGradNormCount:\n    state_dict_key = \"amp\"\n\n    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None,\n                 create_graph=False, update_grad=True):\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None:\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), clip_grad)\n            optimizer.step()\n    \n    def state_dict(self):\n        if 'state_dict' in amp.__dict__:\n            return amp.state_dict()\n\n    def load_state_dict(self, state_dict):\n        if 'load_state_dict' in amp.__dict__:\n            amp.load_state_dict(state_dict)", "\n\n# https://github.com/facebookresearch/ConvNeXt/blob/main/utils.py\nclass NativeScalerWithGradNormCount:\n    state_dict_key = 'amp_scaler'\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None:\n                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n                torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)", "\n\nclass MyAverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\"\"\"\n\n    def __init__(self, max_len=-1):\n        self.val_list = []\n        self.count = []\n        self.max_len = max_len\n        self.val = 0\n        self.avg = 0\n        self.var = 0\n\n    def update(self, val):\n        self.val = val\n        self.avg = 0\n        self.var = 0\n        if not math.isnan(val) and not math.isinf(val):\n            self.val_list.append(val)\n        if self.max_len > 0 and len(self.val_list) > self.max_len:\n            self.val_list = self.val_list[-self.max_len:]\n        if len(self.val_list) > 0:\n            self.avg = np.mean(np.array(self.val_list))\n            self.var = np.std(np.array(self.val_list))", "\n\ndef accuracy_SBM(scores, targets):\n    # build confusion matrix\n    S = targets.cpu().numpy()\n    if scores.size(-1) == 1:\n        C = torch.sigmoid(scores) < 0.5\n        C = (torch.where(C, 0, 1)).cpu().numpy()\n        C = C.squeeze(-1)\n    else:\n        C = scores.argmax(-1).cpu().numpy()\n    CM = confusion_matrix(S, C).astype(np.float32)\n    # calculate accuracy\n    nb_classes = CM.shape[0]\n    targets = targets.cpu().detach().numpy()\n    nb_non_empty_classes = 0\n    pr_classes = np.zeros(nb_classes)\n    for r in range(nb_classes):\n        cluster = np.where(targets==r)[0]\n        if cluster.shape[0] != 0:\n            pr_classes[r] = CM[r,r] / float(cluster.shape[0])\n            if CM[r,r] > 0:\n                nb_non_empty_classes += 1\n        else:\n            pr_classes[r] = 0.0\n    print(\"pre classes acc:\", pr_classes)\n    acc = 100.* np.sum(pr_classes)/ float(nb_classes)\n    return torch.tensor(acc, device=scores.device)", ""]}
{"filename": "lr_scheduler.py", "chunked_list": ["import torch\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.scheduler.step_lr import StepLRScheduler\nfrom timm.scheduler.scheduler import Scheduler\n\n\ndef build_scheduler(config, optimizer, n_iter_per_epoch):\n    num_steps = int(config.TRAIN.EPOCHS * n_iter_per_epoch)\n    warmup_steps = int(config.TRAIN.WARMUP_EPOCHS * n_iter_per_epoch)\n    decay_steps = int(config.TRAIN.LR_SCHEDULER.DECAY_EPOCHS *\n                      n_iter_per_epoch)\n\n    lr_scheduler = None\n    if config.TRAIN.LR_SCHEDULER.NAME == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            # t_mul=1.,\n            lr_min=config.TRAIN.MIN_LR,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            cycle_limit=1,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'linear':\n        lr_scheduler = LinearLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            lr_min_rate=0.01,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'step':\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=decay_steps,\n            decay_rate=config.TRAIN.LR_SCHEDULER.DECAY_RATE,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n\n    return lr_scheduler", "\n\nclass LinearLRScheduler(Scheduler):\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        t_initial: int,\n        lr_min_rate: float,\n        warmup_t=0,\n        warmup_lr_init=0.,\n        t_in_epochs=True,\n        noise_range_t=None,\n        noise_pct=0.67,\n        noise_std=1.0,\n        noise_seed=42,\n        initialize=True,\n    ) -> None:\n        super().__init__(optimizer,\n                         param_group_field=\"lr\",\n                         noise_range_t=noise_range_t,\n                         noise_pct=noise_pct,\n                         noise_std=noise_std,\n                         noise_seed=noise_seed,\n                         initialize=initialize)\n\n        self.t_initial = t_initial\n        self.lr_min_rate = lr_min_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t\n                                 for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            t = t - self.warmup_t\n            total_t = self.t_initial - self.warmup_t\n            lrs = [\n                v - ((v - v * self.lr_min_rate) * (t / total_t))\n                for v in self.base_values\n            ]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None", ""]}
{"filename": "dataset/dataset.py", "chunked_list": ["from functools import lru_cache\nfrom typing import Optional\n\nimport torch\nimport torch.utils.data as data\n\nfrom .collator import collator\nfrom .dgl_datasets import DGLDatasetLookupTable\nfrom .pyg_datasets import PYGDatasetLookupTable\nfrom .ogb_datasets import OGBDatasetLookupTable", "from .pyg_datasets import PYGDatasetLookupTable\nfrom .ogb_datasets import OGBDatasetLookupTable\n\n\nclass BatchedDataDataset(data.Dataset):\n    def __init__(self, dataset, max_nodes=128, multi_hop_max_dist=5, spatial_pos_max=1024):\n        super().__init__()\n        self.dataset = dataset\n        self.max_nodes = max_nodes\n        self.multi_hop_max_dist = multi_hop_max_dist\n        self.spatial_pos_max = spatial_pos_max\n\n    def __getitem__(self, index):\n        item = self.dataset[int(index)]\n        return item\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        return collator(\n            samples,\n            max_node=self.max_nodes,\n            multi_hop_max_dist=self.multi_hop_max_dist,\n            spatial_pos_max=self.spatial_pos_max,\n        )", "\n\ndef pad_1d_unsqueeze_nan(x, padlen):\n    x = x + 1  # pad id = 0\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen], dtype=x.dtype).float()\n        new_x[:] = float('nan')\n        new_x[:xlen] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\nclass TargetDataset(data.Dataset):\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    @lru_cache(maxsize=16)\n    def __getitem__(self, index):\n        return self.dataset[index].y\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        try:\n            return torch.stack(samples, dim=0)\n        except: # only for PATTERN and CLUSTER now\n            max_node_num = max(sample.size(0) for sample in samples)\n            samples = torch.cat([pad_1d_unsqueeze_nan(i, max_node_num) for i in samples])\n            samples = samples - 1 # for PATTERN and CLUSTER here\n            return samples", "\n\nclass GraphDataset:\n    def __init__(self, dataset_spec: Optional[str] = None,\n                 dataset_source: Optional[str] = None, seed: int = 0):\n        super().__init__()\n        if dataset_source == \"dgl\":\n            self.dataset = DGLDatasetLookupTable.GetDGLDataset(dataset_spec, seed=seed)\n        elif dataset_source == \"pyg\":\n            self.dataset = PYGDatasetLookupTable.GetPYGDataset(dataset_spec, seed=seed)\n        elif dataset_source == \"ogb\":\n            self.dataset = OGBDatasetLookupTable.GetOGBDataset(dataset_spec, seed=seed)\n        else:\n            raise ValueError(f\"Unknown dataset source {dataset_source}\")\n        self.setup()\n\n    def setup(self):\n        self.train_idx = self.dataset.train_idx\n        self.valid_idx = self.dataset.valid_idx\n        self.test_idx = self.dataset.test_idx\n\n        self.dataset_train = self.dataset.train_data\n        self.dataset_val = self.dataset.valid_data\n        self.dataset_test = self.dataset.test_data", ""]}
{"filename": "dataset/__init__.py", "chunked_list": ["from .build import build_loader\n\n__all__ = ['build_loader']"]}
{"filename": "dataset/collator.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport torch\n\n\ndef pad_1d_unsqueeze(x, padlen):\n    x = x + 1  # pad id = 0\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen], dtype=x.dtype)\n        new_x[:xlen] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef pad_2d_unsqueeze(x, padlen):\n    x = x + 1  # pad id = 0\n    xlen, xdim = x.size()\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen, xdim], dtype=x.dtype)\n        new_x[:xlen, :] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef pad_attn_bias_unsqueeze(x, padlen):\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype).fill_(float(\"-inf\"))\n        new_x[:xlen, :xlen] = x\n        new_x[xlen:, :xlen] = 0\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef pad_edge_type_unsqueeze(x, padlen):\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen, padlen, x.size(-1)], dtype=x.dtype)\n        new_x[:xlen, :xlen, :] = x\n        x = new_x\n    return x.unsqueeze(0)\n", "\n\ndef pad_spatial_pos_unsqueeze(x, padlen):\n    x = x + 1\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype)\n        new_x[:xlen, :xlen] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef pad_3d_unsqueeze(x, padlen1, padlen2, padlen3):\n    x = x + 1\n    xlen1, xlen2, xlen3, xlen4 = x.size()\n    if xlen1 < padlen1 or xlen2 < padlen2 or xlen3 < padlen3:\n        new_x = x.new_zeros([padlen1, padlen2, padlen3, xlen4], dtype=x.dtype)\n        new_x[:xlen1, :xlen2, :xlen3, :] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef pad_1d_unsqueeze_nan(x, padlen):\n    x = x + 1  # pad id = 0\n    xlen = x.size(0)\n    if xlen < padlen:\n        new_x = x.new_zeros([padlen], dtype=x.dtype).float()\n        new_x[:] = float('nan')\n        new_x[:xlen] = x\n        x = new_x\n    return x.unsqueeze(0)", "\n\ndef collator(items, max_node=512, multi_hop_max_dist=20, spatial_pos_max=20):\n    items = [item for item in items if item is not None and item.x.size(0) <= max_node]\n    items = [\n        (\n            item.idx,\n            item.edge_feature,\n            item.attn_edge_type,\n            item.spatial_pos,\n            item.in_degree,\n            item.out_degree,\n            item.edge_attr,\n            item.edge_index,\n            item.x,\n            item.edge_input[:, :, :multi_hop_max_dist, :],\n            item.y,\n        )\n        for item in items\n    ]\n    (\n        idxs,\n        attn_biases,\n        attn_edge_types,\n        spatial_poses,\n        in_degrees,\n        out_degrees,\n        edge_attrs,\n        edge_indexes,\n        xs,\n        edge_inputs,\n        ys,\n    ) = zip(*items)\n\n    for idx, _ in enumerate(attn_biases):\n        attn_biases[idx][1:, 1:][spatial_poses[idx] >= spatial_pos_max] = float(\"-inf\")\n    max_node_num = max(i.size(0) for i in xs)\n    max_dist = max(i.size(-2) for i in edge_inputs)\n    \n    if ys[0].size(0) == 1:\n        y = torch.cat(ys)\n    else:\n        try:\n            max_edge_num = max([y.size(0) for y in ys])\n            y = torch.cat([pad_1d_unsqueeze_nan(i, max_edge_num) for i in ys])\n        except:\n            y = torch.cat([pad_1d_unsqueeze_nan(i, max_node_num) for i in ys])\n\n    x = torch.cat([pad_2d_unsqueeze(i, max_node_num) for i in xs])\n    edge_input = torch.cat(\n        [pad_3d_unsqueeze(i, max_node_num, max_node_num, max_dist) for i in edge_inputs]\n    )\n    attn_bias = torch.cat(\n        [pad_attn_bias_unsqueeze(i, max_node_num + 1) for i in attn_biases]\n    )\n    attn_edge_type = torch.cat(\n        [pad_edge_type_unsqueeze(i, max_node_num) for i in attn_edge_types]\n    )\n    spatial_pos = torch.cat(\n        [pad_spatial_pos_unsqueeze(i, max_node_num) for i in spatial_poses]\n    )\n    in_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num) for i in in_degrees])\n    # max_edge_num = max([edge_attr.shape[0] for edge_attr in edge_attrs])\n    # consider the god node\n    # edge_index = torch.cat([pad_2d_unsqueeze(i.transpose(-1, -2), max_edge_num) for i in edge_indexes])\n    # edge_index = edge_index.transpose(-1, -2)\n    return dict(\n        idx=torch.LongTensor(idxs),\n        attn_bias=attn_bias,\n        attn_edge_type=attn_edge_type,\n        spatial_pos=spatial_pos,\n        in_degree=in_degree,\n        out_degree=in_degree,  # for undirected graph\n        # edge_index=torch.LongTensor(edge_index),\n        x=x.long(),\n        edge_input=edge_input,\n        y=y,\n    )", ""]}
{"filename": "dataset/wrapper.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport torch\nimport numpy as np\nimport torch_geometric\nfrom ogb.graphproppred import PygGraphPropPredDataset\nfrom ogb.lsc.pcqm4mv2_pyg import PygPCQM4Mv2Dataset\nfrom functools import lru_cache\nimport pyximport", "from functools import lru_cache\nimport pyximport\nimport torch.distributed as dist\n\npyximport.install(setup_args={\"include_dirs\": np.get_include()})\nfrom . import algos\n\n\n@torch.jit.script\ndef convert_to_single_emb(x, offset: int = 512):\n    feature_num = x.size(1) if len(x.size()) > 1 else 1\n    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n    x = x + feature_offset\n    return x", "@torch.jit.script\ndef convert_to_single_emb(x, offset: int = 512):\n    feature_num = x.size(1) if len(x.size()) > 1 else 1\n    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n    x = x + feature_offset\n    return x\n\n\ndef preprocess_item(item):\n    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n    N = x.size(0)\n    x = convert_to_single_emb(x)\n\n    # node adj matrix [N, N] bool\n    adj = torch.zeros([N, N], dtype=torch.bool)\n    adj[edge_index[0, :], edge_index[1, :]] = True\n\n    # edge feature here\n    if edge_attr is None:\n        edge_attr = torch.zeros(edge_index.size(1), 3).long()\n    \n    if len(edge_attr.size()) == 1:\n        edge_attr = edge_attr[:, None]\n    \n    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n    attn_edge_type[edge_index[0, :], edge_index[1, :]] = (\n        convert_to_single_emb(edge_attr) + 1\n    )\n\n    shortest_path_result, path = algos.floyd_warshall(adj.numpy())\n    max_dist = np.amax(shortest_path_result)\n\n    edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n    spatial_pos = torch.from_numpy((shortest_path_result)).long()\n    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n\n    # combine\n    item.x = x\n    item.edge_feature = attn_bias\n    item.attn_edge_type = attn_edge_type\n    item.spatial_pos = spatial_pos\n    item.in_degree = adj.long().sum(dim=1).view(-1)\n    item.out_degree = item.in_degree  # for undirected graph\n    item.edge_input = torch.from_numpy(edge_input).long()\n    return item", "def preprocess_item(item):\n    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n    N = x.size(0)\n    x = convert_to_single_emb(x)\n\n    # node adj matrix [N, N] bool\n    adj = torch.zeros([N, N], dtype=torch.bool)\n    adj[edge_index[0, :], edge_index[1, :]] = True\n\n    # edge feature here\n    if edge_attr is None:\n        edge_attr = torch.zeros(edge_index.size(1), 3).long()\n    \n    if len(edge_attr.size()) == 1:\n        edge_attr = edge_attr[:, None]\n    \n    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n    attn_edge_type[edge_index[0, :], edge_index[1, :]] = (\n        convert_to_single_emb(edge_attr) + 1\n    )\n\n    shortest_path_result, path = algos.floyd_warshall(adj.numpy())\n    max_dist = np.amax(shortest_path_result)\n\n    edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n    spatial_pos = torch.from_numpy((shortest_path_result)).long()\n    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n\n    # combine\n    item.x = x\n    item.edge_feature = attn_bias\n    item.attn_edge_type = attn_edge_type\n    item.spatial_pos = spatial_pos\n    item.in_degree = adj.long().sum(dim=1).view(-1)\n    item.out_degree = item.in_degree  # for undirected graph\n    item.edge_input = torch.from_numpy(edge_input).long()\n    return item", "\n\ndef preprocess_item_tsp(item):\n    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n    N = edge_index.max() + 1\n    x = torch.zeros(N, 1)\n    x = convert_to_single_emb(x)\n    \n    # node adj matrix [N, N] bool\n    adj = torch.zeros([N, N], dtype=torch.bool)\n    adj[edge_index[0, :], edge_index[1, :]] = True\n    \n    # edge feature here\n    if edge_attr is None:\n        edge_attr = torch.zeros(edge_index.size(1), 3).long()\n    \n    if len(edge_attr.size()) == 1:\n        edge_attr = edge_attr[:, None]\n    \n    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n    attn_edge_type[edge_index[0, :], edge_index[1, :]] = (\n        (convert_to_single_emb(edge_attr) + 1).long()\n    )\n    \n    shortest_path_result, path = algos.floyd_warshall(adj.numpy())\n    max_dist = np.amax(shortest_path_result)\n    \n    edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n    spatial_pos = torch.from_numpy((shortest_path_result)).long()\n    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n    \n    # combine\n    item.x = x\n    item.edge_feature = attn_bias\n    item.attn_edge_type = attn_edge_type\n    item.spatial_pos = spatial_pos\n    item.in_degree = adj.long().sum(dim=1).view(-1)\n    item.out_degree = item.in_degree  # for undirected graph\n    item.edge_input = torch.from_numpy(edge_input).long()\n    return item", "\n\nclass MyPygPCQM4MDataset(PygPCQM4Mv2Dataset):\n    def download(self):\n        super(MyPygPCQM4MDataset, self).download()\n\n    def process(self):\n        super(MyPygPCQM4MDataset, self).process()\n\n    @lru_cache(maxsize=16)\n    def __getitem__(self, idx):\n        item = self.get(self.indices()[idx])\n        item.idx = idx\n        return preprocess_item(item)", "\n\nclass MyPygGraphPropPredDataset(PygGraphPropPredDataset):\n    def download(self):\n        if dist.get_rank() == 0:\n            super(MyPygGraphPropPredDataset, self).download()\n        dist.barrier()\n\n    def process(self):\n        if dist.get_rank() == 0:\n            super(MyPygGraphPropPredDataset, self).process()\n        dist.barrier()\n\n    @lru_cache(maxsize=16)\n    def __getitem__(self, idx):\n        item = self.get(self.indices()[idx])\n        item.idx = idx\n        item.y = item.y.reshape(-1)\n        return preprocess_item(item)", ""]}
{"filename": "dataset/build.py", "chunked_list": ["import torch\nimport torch.distributed as dist\nfrom .dataset import GraphDataset\nfrom .dataset import BatchedDataDataset\n\n\ndef build_loader(config):\n    config.defrost()\n    dataset = GraphDataset(dataset_spec=config.DATA.DATASET, dataset_source=config.DATA.SOURCE)\n    config.freeze()\n    \n    dataset_train = BatchedDataDataset(dataset.dataset_train, config.DATA.TRAIN_MAX_NODES,\n                                       config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n          f\"successfully build train dataset, with #samples: {len(dataset_train)}\")\n\n    dataset_val = BatchedDataDataset(dataset.dataset_val, config.DATA.INFER_MAX_NODES,\n                                      config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n          f\"successfully build val dataset, with #samples: {len(dataset_val)}\")\n\n    dataset_test = BatchedDataDataset(dataset.dataset_test, config.DATA.INFER_MAX_NODES,\n                                      config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n          f\"successfully build test dataset, with #samples: {len(dataset_test)}\")\n\n    num_tasks = dist.get_world_size()\n    global_rank = dist.get_rank()\n\n    if dataset_train is not None:\n        sampler_train = torch.utils.data.DistributedSampler(\n            dataset_train,\n            num_replicas=num_tasks,\n            rank=global_rank,\n            shuffle=True)\n\n    if dataset_val is not None:\n        if config.TEST.SEQUENTIAL:\n            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n        else:\n            sampler_val = torch.utils.data.distributed.DistributedSampler(\n                dataset_val, shuffle=False)\n\n    if dataset_test is not None:\n        if config.TEST.SEQUENTIAL:\n            sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n        else:\n            sampler_test = torch.utils.data.distributed.DistributedSampler(\n                dataset_test, shuffle=False)\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train,\n        collate_fn=dataset_train.collater,\n        sampler=sampler_train,\n        batch_size=config.DATA.BATCH_SIZE,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=True,\n        persistent_workers=True) if dataset_train is not None else None\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val,\n        collate_fn=dataset_val.collater,\n        sampler=sampler_val,\n        batch_size=config.DATA.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=False,\n        persistent_workers=True) if dataset_val is not None else None\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test,\n        collate_fn=dataset_test.collater,\n        sampler=sampler_test,\n        batch_size=config.DATA.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=False,\n        persistent_workers=True) if dataset_test is not None else None\n\n    return dataset_train, dataset_val, dataset_test, data_loader_train, \\\n        data_loader_val, data_loader_test", "\n\n\n\n"]}
{"filename": "dataset/dgl_datasets/dgl_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport torch\nimport numpy as np\nfrom copy import copy\nfrom typing import List\nfrom typing import Optional, Tuple\n\nfrom dgl import DGLGraph", "\nfrom dgl import DGLGraph\nfrom dgl.data import DGLDataset\nfrom torch_geometric.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch_geometric.data import Data as PYGGraph\n\nfrom ..wrapper import convert_to_single_emb\nfrom .. import algos\n", "from .. import algos\n\n\nclass DGLDataset(Dataset):\n    def __init__(self,\n        dataset: DGLDataset,\n        seed: int = 0,\n        train_idx=None,\n        valid_idx=None,\n        test_idx=None,\n    ):\n        self.dataset = dataset\n        num_data = len(self.dataset)\n        self.seed = seed\n        if train_idx is None:\n            train_valid_idx, test_idx = train_test_split(\n                np.arange(num_data), test_size=num_data // 10, random_state=seed\n            )\n            train_idx, valid_idx = train_test_split(\n                train_valid_idx, test_size=num_data // 5, random_state=seed\n            )\n        self.train_idx = train_idx\n        self.valid_idx = valid_idx\n        self.test_idx = test_idx\n        self.__indices__ = None\n        self.train_data = self.index_select(train_idx)\n        self.valid_data = self.index_select(valid_idx)\n        self.test_data = self.index_select(test_idx)\n\n    def index_select(self, indices: List[int]):\n        subset = copy(self)\n        subset.__indices__ = indices\n        subset.train_idx = None\n        subset.valid_idx = None\n        subset.test_idx = None\n        subset.train_data = None\n        subset.valid_data = None\n        subset.test_data = None\n        return subset\n\n    def __extract_edge_and_node_features(\n        self, graph_data: DGLGraph\n    ) -> Tuple[\n        Optional[torch.Tensor],\n        Optional[torch.Tensor],\n        Optional[torch.Tensor],\n        Optional[torch.Tensor],\n    ]:\n        def extract_tensor_from_node_or_edge_data(\n            feature_dict: dict, num_nodes_or_edges\n        ):\n            float_feature_list = []\n            int_feature_list = []\n\n            def extract_tensor_from_dict(feature: torch.Tensor):\n                if feature.dtype == torch.int32 or feature.dtype == torch.long:\n                    int_feature_list.append(feature.unsqueeze(1))\n                elif feature.dtype == torch.float32 or feature.dtype == torch.float64:\n                    float_feature_list.append(feature.unsqueeze(1))\n\n            for feature_or_dict in feature_dict:\n                if isinstance(feature_or_dict, torch.Tensor):\n                    extract_tensor_from_dict(feature_or_dict)\n                elif isinstance(feature_or_dict, dict):\n                    for feature in feature_or_dict:\n                        extract_tensor_from_dict(feature)\n            int_feature_tensor = (\n                torch.from_numpy(np.zeros(shape=[num_nodes_or_edges, 1])).long()\n                if len(int_feature_list) == 0\n                else torch.cat(int_feature_list)\n            )\n            float_feature_tensor = (\n                None if len(float_feature_list) == 0 else torch.cat(float_feature_list)\n            )\n            return int_feature_tensor, float_feature_tensor\n\n        node_int_feature, node_float_feature = extract_tensor_from_node_or_edge_data(\n            graph_data.ndata, graph_data.num_nodes()\n        )\n        edge_int_feature, edge_float_feature = extract_tensor_from_node_or_edge_data(\n            graph_data.edata, graph_data.num_edges()\n        )\n        return (\n            node_int_feature,\n            node_float_feature,\n            edge_int_feature,\n            edge_float_feature,\n        )\n\n    def __preprocess_dgl_graph(\n        self, graph_data: DGLGraph, y: torch.Tensor, idx: int\n    ) -> PYGGraph:\n        if not graph_data.is_homogeneous:\n            raise ValueError(\n                \"Heterogeneous DGLGraph is found. Only homogeneous graph is supported.\"\n            )\n        N = graph_data.num_nodes()\n\n        (\n            node_int_feature,\n            node_float_feature,\n            edge_int_feature,\n            edge_float_feature,\n        ) = self.__extract_edge_and_node_features(graph_data)\n        edge_index = graph_data.edges()\n        attn_edge_type = torch.zeros(\n            [N, N, edge_int_feature.shape[1]], dtype=torch.long\n        )\n        attn_edge_type[\n            edge_index[0].long(), edge_index[1].long()\n        ] = convert_to_single_emb(edge_int_feature)\n        dense_adj = graph_data.adj().to_dense().type(torch.int)\n        shortest_path_result, path = algos.floyd_warshall(dense_adj.numpy())\n        max_dist = np.amax(shortest_path_result)\n        edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n        spatial_pos = torch.from_numpy((shortest_path_result)).long()\n        attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n\n        pyg_graph = PYGGraph()\n        pyg_graph.x = convert_to_single_emb(node_int_feature)\n        pyg_graph.adj = dense_adj\n        pyg_graph.attn_bias = attn_bias\n        pyg_graph.attn_edge_type = attn_edge_type\n        pyg_graph.spatial_pos = spatial_pos\n        pyg_graph.in_degree = dense_adj.long().sum(dim=1).view(-1)\n        pyg_graph.out_degree = pyg_graph.in_degree\n        pyg_graph.edge_input = torch.from_numpy(edge_input).long()\n        if y.dim() == 0:\n            y = y.unsqueeze(-1)\n        pyg_graph.y = y\n        pyg_graph.idx = idx\n\n        return pyg_graph\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            if self.__indices__ is not None:\n                idx = self.__indices__[idx]\n            graph, y = self.dataset[idx]\n            return self.__preprocess_dgl_graph(graph, y, idx)\n        else:\n            raise TypeError(\"index to a DGLDataset can only be an integer.\")\n\n    def __len__(self) -> int:\n        return len(self.dataset) if self.__indices__ is None else len(self.__indices__)", ""]}
{"filename": "dataset/dgl_datasets/dgl_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nfrom typing import Optional\nfrom dgl.data import (\n    QM7bDataset,\n    QM9Dataset,\n    QM9EdgeDataset,\n    MiniGCDataset,\n    TUDataset,", "    MiniGCDataset,\n    TUDataset,\n    GINDataset,\n    FakeNewsDataset,\n)\nimport torch.distributed as dist\n\nfrom .dgl_dataset import DGLDataset\n\n\nclass MyQM7bDataset(QM7bDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7bDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyQM7bDataset(QM7bDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7bDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyQM9Dataset(QM9Dataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9Dataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyQM9Dataset(QM9Dataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9Dataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyQM9EdgeDataset(QM9EdgeDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9EdgeDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyQM9EdgeDataset(QM9EdgeDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9EdgeDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyMiniGCDataset(MiniGCDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyMiniGCDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyMiniGCDataset(MiniGCDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyMiniGCDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyTUDataset(TUDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyTUDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyTUDataset(TUDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyTUDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyGINDataset(GINDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyGINDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyGINDataset(GINDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyGINDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass MyFakeNewsDataset(FakeNewsDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyFakeNewsDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyFakeNewsDataset(FakeNewsDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyFakeNewsDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n\nclass DGLDatasetLookupTable:\n    @staticmethod\n    def GetDGLDataset(dataset_name: str, seed: int) -> Optional[DGLDataset]:\n        params = dataset_name.split(\":\")[-1].split(\",\")\n        inner_dataset = None\n\n        if dataset_name == \"qm7b\":\n            inner_dataset = MyQM7bDataset()\n        elif dataset_name.startswith(\"qm9\"):\n            label_keys = None\n            cutoff = 5.0\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"label_keys\":\n                    label_keys = value.split(\"+\")\n                elif name == \"cutoff\":\n                    cutoff = float(value)\n            inner_dataset = MyQM9Dataset(label_keys=label_keys, cutoff=cutoff)\n        elif dataset_name.startswith(\"qm9edge\"):\n            label_keys = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"label_keys\":\n                    label_keys = value.split(\"+\")\n            inner_dataset = MyQM9EdgeDataset(label_keys=label_keys)\n        elif dataset_name.startswith(\"minigc\"):\n            num_graphs = None\n            min_num_v = None\n            max_num_v = None\n            data_seed = seed\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"num_graphs\":\n                    num_graphs = int(value)\n                elif name == \"min_num_v\":\n                    min_num_v = int(value)\n                elif name == \"max_num_v\":\n                    max_num_v = int(value)\n                elif name == \"seed\":\n                    data_seed = int(value)\n            inner_dataset = MyMiniGCDataset(\n                num_graphs, min_num_v, max_num_v, seed=data_seed\n            )\n        elif dataset_name.startswith(\"tu\"):\n            nm = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n            inner_dataset = MyTUDataset(name=nm)\n        elif dataset_name.startswith(\"gin\"):\n            nm = None\n            self_loop = None\n            degree_as_nlabel = False\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n                elif name == \"self_loop\":\n                    if value.lower() == \"false\":\n                        self_loop = False\n                    elif value.lower() == \"true\":\n                        self_loop = True\n                elif name == \"degree_as_nlabel\":\n                    if value.lower() == \"false\":\n                        degree_as_nlabel = False\n                    elif value.lower() == \"true\":\n                        degree_as_nlabel = True\n            inner_dataset = MyGINDataset(\n                name=nm, self_loop=self_loop, degree_as_nlabel=degree_as_nlabel\n            )\n        elif dataset_name.startswith(\"fakenews\"):\n            nm = None\n            feature_name = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n                elif name == \"feature_name\":\n                    feature_name = value\n            inner_dataset = MyFakeNewsDataset(name=nm, feature_name=feature_name)\n        else:\n            raise ValueError(f\"Unknown dataset specificaion {dataset_name}\")\n\n        return (\n            None\n            if inner_dataset is None\n            else DGLDataset(inner_dataset, seed)\n        )", "\n\nclass DGLDatasetLookupTable:\n    @staticmethod\n    def GetDGLDataset(dataset_name: str, seed: int) -> Optional[DGLDataset]:\n        params = dataset_name.split(\":\")[-1].split(\",\")\n        inner_dataset = None\n\n        if dataset_name == \"qm7b\":\n            inner_dataset = MyQM7bDataset()\n        elif dataset_name.startswith(\"qm9\"):\n            label_keys = None\n            cutoff = 5.0\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"label_keys\":\n                    label_keys = value.split(\"+\")\n                elif name == \"cutoff\":\n                    cutoff = float(value)\n            inner_dataset = MyQM9Dataset(label_keys=label_keys, cutoff=cutoff)\n        elif dataset_name.startswith(\"qm9edge\"):\n            label_keys = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"label_keys\":\n                    label_keys = value.split(\"+\")\n            inner_dataset = MyQM9EdgeDataset(label_keys=label_keys)\n        elif dataset_name.startswith(\"minigc\"):\n            num_graphs = None\n            min_num_v = None\n            max_num_v = None\n            data_seed = seed\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"num_graphs\":\n                    num_graphs = int(value)\n                elif name == \"min_num_v\":\n                    min_num_v = int(value)\n                elif name == \"max_num_v\":\n                    max_num_v = int(value)\n                elif name == \"seed\":\n                    data_seed = int(value)\n            inner_dataset = MyMiniGCDataset(\n                num_graphs, min_num_v, max_num_v, seed=data_seed\n            )\n        elif dataset_name.startswith(\"tu\"):\n            nm = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n            inner_dataset = MyTUDataset(name=nm)\n        elif dataset_name.startswith(\"gin\"):\n            nm = None\n            self_loop = None\n            degree_as_nlabel = False\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n                elif name == \"self_loop\":\n                    if value.lower() == \"false\":\n                        self_loop = False\n                    elif value.lower() == \"true\":\n                        self_loop = True\n                elif name == \"degree_as_nlabel\":\n                    if value.lower() == \"false\":\n                        degree_as_nlabel = False\n                    elif value.lower() == \"true\":\n                        degree_as_nlabel = True\n            inner_dataset = MyGINDataset(\n                name=nm, self_loop=self_loop, degree_as_nlabel=degree_as_nlabel\n            )\n        elif dataset_name.startswith(\"fakenews\"):\n            nm = None\n            feature_name = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n                elif name == \"feature_name\":\n                    feature_name = value\n            inner_dataset = MyFakeNewsDataset(name=nm, feature_name=feature_name)\n        else:\n            raise ValueError(f\"Unknown dataset specificaion {dataset_name}\")\n\n        return (\n            None\n            if inner_dataset is None\n            else DGLDataset(inner_dataset, seed)\n        )", ""]}
{"filename": "dataset/dgl_datasets/__init__.py", "chunked_list": ["from .dgl_dataset_lookup_table import DGLDatasetLookupTable\nfrom .dgl_dataset import DGLDataset\n\n__all__ = ['DGLDataset', 'DGLDatasetLookupTable']"]}
{"filename": "dataset/pyg_datasets/pyg_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport copy\nimport torch\nimport numpy as np\nfrom functools import lru_cache\nfrom torch_geometric.data import Dataset\nfrom sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import train_test_split\n\nfrom ..wrapper import preprocess_item\nfrom ..wrapper import preprocess_item_tsp\n\n\nclass PYGDataset(Dataset):\n    def __init__(\n        self,\n        dataset: Dataset,\n        seed: int = 0,\n        train_idx=None,\n        valid_idx=None,\n        test_idx=None,\n        train_set=None,\n        valid_set=None,\n        test_set=None,\n    ):\n        self.dataset = dataset\n        if self.dataset is not None:\n            self.num_data = len(self.dataset)\n        self.seed = seed\n        if train_idx is None and train_set is None:\n            train_valid_idx, test_idx = train_test_split(\n                np.arange(self.num_data),\n                test_size=self.num_data // 10,\n                random_state=seed,\n            )\n            train_idx, valid_idx = train_test_split(\n                train_valid_idx, test_size=self.num_data // 5, random_state=seed\n            )\n            self.train_idx = torch.from_numpy(train_idx)\n            self.valid_idx = torch.from_numpy(valid_idx)\n            self.test_idx = torch.from_numpy(test_idx)\n            self.train_data = self.index_select(self.train_idx)\n            self.valid_data = self.index_select(self.valid_idx)\n            self.test_data = self.index_select(self.test_idx)\n        elif train_set is not None:\n            self.num_data = len(train_set) + len(valid_set) + len(test_set)\n            self.train_data = self.create_subset(train_set)\n            self.valid_data = self.create_subset(valid_set)\n            self.test_data = self.create_subset(test_set)\n            self.train_idx = None\n            self.valid_idx = None\n            self.test_idx = None\n        else:\n            self.num_data = len(train_idx) + len(valid_idx) + len(test_idx)\n            self.train_idx = train_idx\n            self.valid_idx = valid_idx\n            self.test_idx = test_idx\n            self.train_data = self.index_select(self.train_idx)\n            self.valid_data = self.index_select(self.valid_idx)\n            self.test_data = self.index_select(self.test_idx)\n        self.__indices__ = None\n\n    def index_select(self, idx):\n        dataset = copy.copy(self)\n        dataset.dataset = self.dataset.index_select(idx)\n        if isinstance(idx, torch.Tensor):\n            dataset.num_data = idx.size(0)\n        else:\n            dataset.num_data = idx.shape[0]\n        dataset.__indices__ = idx\n        dataset.train_data = None\n        dataset.valid_data = None\n        dataset.test_data = None\n        dataset.train_idx = None\n        dataset.valid_idx = None\n        dataset.test_idx = None\n        return dataset\n\n    def create_subset(self, subset):\n        dataset = copy.copy(self)\n        dataset.dataset = subset\n        dataset.num_data = len(subset)\n        dataset.__indices__ = None\n        dataset.train_data = None\n        dataset.valid_data = None\n        dataset.test_data = None\n        dataset.train_idx = None\n        dataset.valid_idx = None\n        dataset.test_idx = None\n        return dataset\n\n    @lru_cache(maxsize=16)\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            item = self.dataset[idx]\n            item.idx = idx\n            item.y = item.y.reshape(-1)\n            if item.x is not None:\n                return preprocess_item(item)\n            else:\n                return preprocess_item_tsp(item)\n        else:\n            raise TypeError(\"index to a PYGDataset can only be an integer.\")\n\n    def __len__(self):\n        return self.num_data", ""]}
{"filename": "dataset/pyg_datasets/__init__.py", "chunked_list": ["from .pyg_dataset_lookup_table import PYGDatasetLookupTable\nfrom .pyg_dataset import PYGDataset\n\n__all__ = ['PYGDatasetLookupTable', 'PYGDataset']"]}
{"filename": "dataset/pyg_datasets/pyg_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nfrom typing import Optional\nfrom torch_geometric.datasets import *\nfrom torch_geometric.data import Dataset\nimport torch.distributed as dist\nfrom .pyg_dataset import PYGDataset\n\n\nclass MyQM7b(QM7b):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7b, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7b, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyQM7b(QM7b):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7b, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM7b, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyQM9(QM9):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyQM9, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyZINC(ZINC):\n    def __init__(self, subset=True, **kwargs):\n        super(MyZINC, self).__init__(subset=subset, **kwargs)\n        \n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyZINC, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyZINC, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyMoleculeNet(MoleculeNet):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyMoleculeNet, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyMoleculeNet, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyTSP(GNNBenchmarkDataset):\n    def __init__(self, **kwargs):\n        super(MyTSP, self).__init__(name=\"TSP\", **kwargs)\n    \n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyTSP, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n    \n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyTSP, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyPattern(GNNBenchmarkDataset):\n    def __init__(self, **kwargs):\n        super(MyPattern, self).__init__(name=\"PATTERN\", **kwargs)\n        \n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPattern, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPattern, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyCluster(GNNBenchmarkDataset):\n    def __init__(self, **kwargs):\n        super(MyCluster, self).__init__(name=\"CLUSTER\", **kwargs)\n        self.url = 'https://github.com/czczup/storage/releases/download/v0.1.0/'\n    \n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyCluster, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n    \n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyCluster, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "            \n\nclass PYGDatasetLookupTable:\n    @staticmethod\n    def GetPYGDataset(dataset_spec: str, seed: int) -> Optional[Dataset]:\n        split_result = dataset_spec.split(\":\")\n        if len(split_result) == 2:\n            name, params = split_result[0], split_result[1]\n            params = params.split(\",\")\n        elif len(split_result) == 1:\n            name = dataset_spec\n            params = []\n        inner_dataset = None\n        num_class = 1\n\n        train_set = None\n        valid_set = None\n        test_set = None\n\n        root = \"data\"\n        if name == \"qm7b\":\n            inner_dataset = MyQM7b(root=root)\n        elif name == \"qm9\":\n            inner_dataset = MyQM9(root=root)\n        elif name == \"zinc-subset\":\n            inner_dataset = MyZINC(root=root, subset=True)\n            train_set = MyZINC(root=root, split=\"train\", subset=True)\n            valid_set = MyZINC(root=root, split=\"val\", subset=True)\n            test_set = MyZINC(root=root, split=\"test\", subset=True)\n        elif name == \"zinc-full\":\n            inner_dataset = MyZINC(root=root, subset=False)\n            train_set = MyZINC(root=root, split=\"train\", subset=False)\n            valid_set = MyZINC(root=root, split=\"val\", subset=False)\n            test_set = MyZINC(root=root, split=\"test\", subset=False)\n        elif name == \"pattern\":\n            inner_dataset = MyPattern(root=root)\n            train_set = MyPattern(root=root, split=\"train\")\n            valid_set = MyPattern(root=root, split=\"val\")\n            test_set = MyPattern(root=root, split=\"test\")\n        elif name == \"cluster\":\n            inner_dataset = MyCluster(root=root)\n            train_set = MyCluster(root=root, split=\"train\")\n            valid_set = MyCluster(root=root, split=\"val\")\n            test_set = MyCluster(root=root, split=\"test\")\n        elif name == \"tsp\":\n            inner_dataset = MyTSP(root=root)\n            train_set = MyTSP(root=root, split=\"train\")\n            valid_set = MyTSP(root=root, split=\"val\")\n            test_set = MyTSP(root=root, split=\"test\")\n        elif name == \"moleculenet\":\n            nm = None\n            for param in params:\n                name, value = param.split(\"=\")\n                if name == \"name\":\n                    nm = value\n            inner_dataset = MyMoleculeNet(root=root, name=nm)\n        else:\n            raise ValueError(f\"Unknown dataset name {name} for pyg source.\")\n        if train_set is not None:\n            return PYGDataset(\n                    None,\n                    seed,\n                    None,\n                    None,\n                    None,\n                    train_set,\n                    valid_set,\n                    test_set,\n                )\n        else:\n            return (\n                None\n                if inner_dataset is None\n                else PYGDataset(inner_dataset, seed)\n            )", ""]}
{"filename": "dataset/smiles/smiles_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom ogb.utils.mol import smiles2graph\n\nfrom .. import algos\nfrom ..wrapper import preprocess_item", "from .. import algos\nfrom ..wrapper import preprocess_item\nfrom ..pyg_datasets import PYGDataset\n\n\n\nclass SMILESDataset(PYGDataset):\n    def __init__(\n        self,\n        dataset: str,\n        num_class: int,\n        max_node: int,\n        multi_hop_max_dist: int,\n        spatial_pos_max: int,\n    ):\n        self.dataset = np.genfromtxt(dataset, delimiter=\",\", dtype=str)\n        num_data = len(self.dataset)\n        self.num_class = num_class\n        self.__get_graph_metainfo(max_node, multi_hop_max_dist, spatial_pos_max)\n        train_valid_idx, test_idx = train_test_split(num_data // 10)\n        train_idx, valid_idx = train_test_split(train_valid_idx, num_data // 5)\n        self.train_idx = train_idx\n        self.valid_idx = valid_idx\n        self.test_idx = test_idx\n        self.__indices__ = None\n        self.train_data = self.index_select(train_idx)\n        self.valid_data = self.index_select(valid_idx)\n        self.test_data = self.index_select(test_idx)\n\n    def __get_graph_metainfo(\n        self, max_node: int, multi_hop_max_dist: int, spatial_pos_max: int\n    ):\n        self.max_node = min(\n            max_node,\n            torch.max(self.dataset[i][0].num_nodes() for i in range(len(self.dataset))),\n        )\n        max_dist = 0\n        for i in range(len(self.dataset)):\n            pyg_graph = smiles2graph(self.dataset[i])\n            dense_adj = pyg_graph.adj().to_dense().type(torch.int)\n            shortest_path_result, _ = algos.floyd_warshall(dense_adj.numpy())\n            max_dist = max(max_dist, np.amax(shortest_path_result))\n        self.multi_hop_max_dist = min(multi_hop_max_dist, max_dist)\n        self.spatial_pos_max = min(spatial_pos_max, max_dist)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            item = smiles2graph(self.dataset[idx])\n            item.idx = idx\n            return preprocess_item(item)\n        else:\n            raise TypeError(\"index to a PYGDataset can only be an integer.\")", ""]}
{"filename": "dataset/ogb_datasets/__init__.py", "chunked_list": ["from .ogb_dataset_lookup_table import OGBDatasetLookupTable\n\n__all__ = ['OGBDatasetLookupTable']"]}
{"filename": "dataset/ogb_datasets/ogb_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nfrom typing import Optional\nfrom ogb.lsc.pcqm4mv2_pyg import PygPCQM4Mv2Dataset\nfrom ogb.lsc.pcqm4m_pyg import PygPCQM4MDataset\nfrom ogb.graphproppred import PygGraphPropPredDataset\nfrom torch_geometric.data import Dataset\nfrom ..pyg_datasets import PYGDataset\nimport torch.distributed as dist", "from ..pyg_datasets import PYGDataset\nimport torch.distributed as dist\nimport os\n\n\nclass MyPygPCQM4Mv2Dataset(PygPCQM4Mv2Dataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygPCQM4Mv2Dataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygPCQM4Mv2Dataset, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyPygPCQM4MDataset(PygPCQM4MDataset):\n    def download(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygPCQM4MDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygPCQM4MDataset, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass MyPygGraphPropPredDataset(PygGraphPropPredDataset):\n    \n    def download(self):\n        # self.meta_info['url'] = 'https://github.com/czczup/storage/releases/download/v0.1.0/pcba.zip'\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygGraphPropPredDataset, self).download()\n        if dist.is_initialized():\n            dist.barrier()\n\n    def process(self):\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            super(MyPygGraphPropPredDataset, self).process()\n        if dist.is_initialized():\n            dist.barrier()", "\n\nclass OGBDatasetLookupTable:\n    @staticmethod\n    def GetOGBDataset(dataset_name: str, seed: int) -> Optional[Dataset]:\n        inner_dataset = None\n        train_idx = None\n        valid_idx = None\n        test_idx = None\n        if dataset_name == \"ogbg-molhiv\":\n            folder_name = dataset_name.replace(\"-\", \"_\")\n            os.system(f\"mkdir -p data/{folder_name}/\")\n            os.system(f\"touch data/{folder_name}/RELEASE_v1.txt\")\n            inner_dataset = MyPygGraphPropPredDataset(dataset_name, root='data')\n            idx_split = inner_dataset.get_idx_split()\n            train_idx = idx_split[\"train\"]\n            valid_idx = idx_split[\"valid\"]\n            test_idx = idx_split[\"test\"]\n        elif dataset_name == \"ogbg-molpcba\":\n            folder_name = dataset_name.replace(\"-\", \"_\")\n            os.system(f\"mkdir -p data/{folder_name}/\")\n            os.system(f\"touch data/{folder_name}/RELEASE_v1.txt\")\n            inner_dataset = MyPygGraphPropPredDataset(dataset_name, root='data')\n            idx_split = inner_dataset.get_idx_split()\n            train_idx = idx_split[\"train\"]\n            valid_idx = idx_split[\"valid\"]\n            test_idx = idx_split[\"test\"]\n        elif dataset_name == \"pcqm4mv2\":\n            os.system(\"mkdir -p data/pcqm4m-v2/\")\n            os.system(\"touch data/pcqm4m-v2/RELEASE_v1.txt\")\n            inner_dataset = MyPygPCQM4Mv2Dataset(root='data')\n            idx_split = inner_dataset.get_idx_split()\n            train_idx = idx_split[\"train\"]\n            valid_idx = idx_split[\"valid\"]\n            test_idx = idx_split[\"test-dev\"]\n        elif dataset_name == \"pcqm4m\":\n            os.system(\"mkdir -p data/pcqm4m_kddcup2021/\")\n            os.system(\"touch data/pcqm4m_kddcup2021/RELEASE_v1.txt\")\n            inner_dataset = MyPygPCQM4MDataset(root='data')\n            idx_split = inner_dataset.get_idx_split()\n            train_idx = idx_split[\"train\"]\n            valid_idx = idx_split[\"valid\"]\n            test_idx = idx_split[\"test\"]\n        else:\n            raise ValueError(f\"Unknown dataset name {dataset_name} for ogb source.\")\n        return (\n            None\n            if inner_dataset is None\n            else PYGDataset(\n                inner_dataset, seed, train_idx, valid_idx, test_idx\n            )\n        )", ""]}
{"filename": "models/losses.py", "chunked_list": ["import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\n\nclass DiceLoss(nn.Module):\n    \n    def __init__(self, reduce_zero_label=False):\n        super(DiceLoss, self).__init__()\n        print(\"reduce_zero_label:\", reduce_zero_label)\n        self.reduce_zero_label = reduce_zero_label\n\n    def forward(self, input, target, reduce=True):\n        input = torch.sigmoid(input)\n        input = input.reshape(-1)\n        target = target.reshape(-1).float()\n        mask = ~torch.isnan(target)\n        if self.reduce_zero_label:\n            target = target - 1  # start from zero\n        input = input[mask]\n        target = target[mask]\n        \n        a = torch.sum(input * target)\n        b = torch.sum(input * input) + 0.001\n        c = torch.sum(target * target) + 0.001\n        d = (2 * a) / (b + c)\n        loss = 1 - d\n\n        if reduce:\n            loss = torch.mean(loss)\n\n        return loss", "\n\nclass BinaryCrossEntropyLoss(nn.Module):\n    \n    def __init__(self, weight, reduce_zero_label=False):\n        super(BinaryCrossEntropyLoss, self).__init__()\n        print(\"weight:\", weight, \"reduce_zero_label:\", reduce_zero_label)\n        if weight is not None:\n            self.weight = torch.tensor(weight).cuda()\n        else:\n            self.weight = None\n        self.reduce_zero_label = reduce_zero_label\n            \n    def forward(self, outputs, targets):\n        # if outputs.size(-1) == 1 and len(outputs.shape) > 1:\n        #     outputs = outputs.squeeze(-1)\n        outputs = outputs.reshape(-1)\n        targets = targets.reshape(-1)\n        mask = ~torch.isnan(targets)\n        if self.reduce_zero_label:\n            targets = targets - 1  # start from zero\n        if self.weight is not None:\n            loss = F.binary_cross_entropy_with_logits(\n                outputs[mask].float(), targets[mask].float(), self.weight[targets[mask].long()], reduction=\"sum\")\n        else:\n            loss = F.binary_cross_entropy_with_logits(\n                outputs[mask].float(), targets[mask].float(), reduction=\"sum\")\n        sample_size = torch.sum(mask.type(torch.int64))\n        return loss / sample_size", "\n\nclass CrossEntropyLoss(nn.Module):\n    \n    def __init__(self, weight, reduce_zero_label=False):\n        super(CrossEntropyLoss, self).__init__()\n        print(\"weight:\", weight, \"reduce_zero_label:\", reduce_zero_label)\n        if weight is not None:\n            self.weight = torch.tensor(weight).cuda()\n        else:\n            self.weight = None\n        self.reduce_zero_label = reduce_zero_label\n    \n    \n    def forward(self, outputs, targets):\n        mask = ~torch.isnan(targets)\n        if self.reduce_zero_label:\n            targets = targets - 1  # start from zero\n        if self.weight is not None:\n            loss = F.cross_entropy(\n                outputs[mask].float(), targets[mask].long(), self.weight, reduction=\"sum\")\n        else:\n            loss = F.cross_entropy(\n                outputs[mask].float(), targets[mask].long(), reduction=\"sum\")\n        sample_size = torch.sum(mask.type(torch.int64))\n        return loss / sample_size", "        \n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .build import build_model\nfrom .losses import BinaryCrossEntropyLoss, CrossEntropyLoss, DiceLoss\n\n__all__ = ['build_model', 'BinaryCrossEntropyLoss', 'CrossEntropyLoss',\n           'DiceLoss']"]}
{"filename": "models/build.py", "chunked_list": ["from .gptrans import GPTrans\n\n\ndef build_model(config):\n    model_type = config.MODEL.TYPE\n    \n    if model_type == 'GPTrans':\n        model = GPTrans(\n            num_layers=config.MODEL.GPTRANS.NUM_LAYERS,\n            num_heads=config.MODEL.GPTRANS.NUM_HEADS,\n            node_dim=config.MODEL.GPTRANS.NODE_DIM,\n            edge_dim=config.MODEL.GPTRANS.EDGE_DIM,\n            layer_scale=config.MODEL.GPTRANS.LAYER_SCALE,\n            mlp_ratio=config.MODEL.GPTRANS.MLP_RATIO,\n            num_classes=config.MODEL.GPTRANS.NUM_CLASSES,\n            drop_rate=config.MODEL.DROP_RATE,\n            attn_drop_rate=config.MODEL.ATTN_DROP_RATE,\n            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n            num_atoms=config.DATA.NUM_ATOMS,\n            num_edges=config.DATA.NUM_EDGES,\n            num_in_degree=config.DATA.NUM_IN_DEGREE,\n            num_out_degree=config.DATA.NUM_OUT_DEGREE,\n            num_spatial=config.DATA.NUM_SPATIAL,\n            num_edge_dist=config.DATA.NUM_EDGE_DIST,\n            multi_hop_max_dist=config.DATA.MULTI_HOP_MAX_DIST,\n            edge_type=config.DATA.EDGE_TYPE,\n            task_type=config.DATA.TASK_TYPE,\n            with_cp=config.TRAIN.USE_CHECKPOINT,\n            random_feature=config.AUG.RANDOM_FEATURE,\n        )\n    else:\n        raise NotImplementedError(f\"Unkown model: {model_type}\")\n\n    return model", ""]}
{"filename": "models/gptrans.py", "chunked_list": ["import logging\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom timm.models.layers import DropPath\n\nlogger = logging.getLogger(__name__)\n\n\ndef init_params(module, num_layers):\n    if isinstance(module, nn.Linear):\n        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(num_layers))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=0.02)", "\n\ndef init_params(module, num_layers):\n    if isinstance(module, nn.Linear):\n        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(num_layers))\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n    ", "    \n\nclass GraphNodeFeature(nn.Module):\n    \n    def __init__(self, num_heads, num_atoms, num_in_degree, num_out_degree, hidden_dim, num_layers):\n        super(GraphNodeFeature, self).__init__()\n        self.num_heads = num_heads\n        self.num_atoms = num_atoms\n        \n        self.atom_encoder = nn.Embedding(num_atoms + 1, hidden_dim, padding_idx=0)\n        self.in_degree_encoder = nn.Embedding(num_in_degree, hidden_dim, padding_idx=0)\n        self.out_degree_encoder = nn.Embedding(\n            num_out_degree, hidden_dim, padding_idx=0\n        )\n        self.graph_token = nn.Embedding(1, hidden_dim)\n        self.apply(lambda module: init_params(module, num_layers=num_layers))\n    \n    def forward(self, batched_data):\n        x, in_degree, out_degree = (\n            batched_data[\"x\"],\n            batched_data[\"in_degree\"],\n            batched_data[\"out_degree\"],\n        )\n        n_graph, n_node = x.size()[:2]  # [B, T, 9]\n\n        # node feauture + graph token\n        node_feature = self.atom_encoder(x).sum(dim=-2)  # [n_graph, n_node, n_hidden]\n        \n        node_feature = (\n                node_feature\n                + self.in_degree_encoder(in_degree)  # [n_graph, n_node, n_hidden]\n                + self.out_degree_encoder(out_degree)  # [n_graph, n_node, n_hidden]\n        )\n        \n        graph_token_feature = self.graph_token.weight.unsqueeze(0).repeat(n_graph, 1, 1)\n        graph_node_feature = torch.cat([graph_token_feature, node_feature], dim=1)\n        \n        return graph_node_feature", "\n\nclass GraphEdgeFeature(nn.Module):\n    \n    def __init__(self, num_heads, num_edges, num_spatial, num_edge_dist, edge_type,\n                 multi_hop_max_dist, num_layers, edge_dim):\n        super(GraphEdgeFeature, self).__init__()\n        self.num_heads = num_heads\n        self.multi_hop_max_dist = multi_hop_max_dist\n        self.edge_embedding_dim = edge_dim\n        \n        self.edge_encoder = nn.Embedding(num_edges + 1, edge_dim, padding_idx=0)\n        self.edge_type = edge_type\n        if self.edge_type == \"multi_hop\":\n            self.edge_dis_encoder = nn.Embedding(\n                num_edge_dist * edge_dim * edge_dim, 1)\n        self.spatial_pos_encoder = nn.Embedding(num_spatial, edge_dim, padding_idx=0)\n        \n        self.graph_token_virtual_distance = nn.Embedding(1, edge_dim)\n        self.apply(lambda module: init_params(module, num_layers=num_layers))\n    \n    def forward(self, batched_data):\n        attn_bias, spatial_pos, x = (\n            batched_data[\"attn_bias\"],\n            batched_data[\"spatial_pos\"],\n            batched_data[\"x\"],\n        )\n        attn_bias = torch.zeros_like(attn_bias) # avoid nan\n        # in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n        edge_input, attn_edge_type = (\n            batched_data[\"edge_input\"],\n            batched_data[\"attn_edge_type\"],\n        )\n        n_graph, n_node = x.size()[:2]\n        graph_attn_bias = attn_bias.clone()\n        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n            1, self.edge_embedding_dim, 1, 1\n        )  # [n_graph, edge_dim, n_node+1, n_node+1]\n\n        # spatial pos\n        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n        spatial_pos_bias = self.spatial_pos_encoder(spatial_pos).permute(0, 3, 1, 2)\n        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + spatial_pos_bias\n        \n        # reset spatial pos here\n        t = self.graph_token_virtual_distance.weight.view(1, self.edge_embedding_dim, 1)\n        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n\n        # edge feature\n        if self.edge_type == \"multi_hop\": # here\n            spatial_pos_ = spatial_pos.clone()\n            spatial_pos_[spatial_pos_ == 0] = 1  # set pad to 1\n            # set 1 to 1, node_embeds > 1 to node_embeds - 1\n            spatial_pos_ = torch.where(spatial_pos_ > 1, spatial_pos_ - 1, spatial_pos_)\n            if self.multi_hop_max_dist > 0:\n                spatial_pos_ = spatial_pos_.clamp(0, self.multi_hop_max_dist)\n                edge_input = edge_input[:, :, :, : self.multi_hop_max_dist, :]\n            # [n_graph, n_node, n_node, max_dist, n_head]\n            edge_input = self.edge_encoder(edge_input).mean(-2)\n            max_dist = edge_input.size(-2)\n            edge_input_flat = edge_input.permute(3, 0, 1, 2, 4).reshape(\n                max_dist, -1, self.edge_embedding_dim)\n            edge_input_flat = torch.bmm(\n                edge_input_flat,\n                self.edge_dis_encoder.weight.reshape(\n                    -1, self.edge_embedding_dim, self.edge_embedding_dim\n                )[:max_dist, :, :])\n            edge_input = edge_input_flat.reshape(\n                max_dist, n_graph, n_node, n_node, self.edge_embedding_dim\n            ).permute(1, 2, 3, 0, 4)\n            edge_input = (\n                edge_input.sum(-2) / (spatial_pos_.float().unsqueeze(-1))\n            ).permute(0, 3, 1, 2)\n        else:\n            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n            edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n        \n        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input.to(graph_attn_bias.dtype)\n        \n        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)  # [B, n_heads, N, N]\n        return graph_attn_bias", "\n\nclass GraphPropagationAttention(nn.Module):\n    def __init__(self, node_dim, edge_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = node_dim // num_heads\n        self.scale = head_dim ** -0.5\n        \n        self.qkv = nn.Linear(node_dim, node_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(node_dim, node_dim)\n        \n        self.reduce = nn.Conv2d(edge_dim, num_heads, kernel_size=1)\n        self.expand = nn.Conv2d(num_heads, edge_dim, kernel_size=1)\n        if edge_dim != node_dim:\n            self.fc = nn.Linear(edge_dim, node_dim)\n        else:\n            self.fc = nn.Identity()\n        self.proj_drop = nn.Dropout(proj_drop)\n    \n    def forward(self, node_embeds, edge_embeds, padding_mask):\n        # node-to-node propagation\n        B, N, C = node_embeds.shape\n        qkv = self.qkv(node_embeds).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\n        q, k, v = qkv.unbind(0)\n        attn = (q @ k.transpose(-2, -1)) * self.scale # [B, n_head, 1+N, 1+N]\n        attn_bias = self.reduce(edge_embeds) # [B, C, 1+N, 1+N] -> [B, n_head, 1+N, 1+N]\n        attn = attn + attn_bias # [B, n_head, 1+N, 1+N]\n        residual = attn\n\n        attn = attn.masked_fill(padding_mask, float(\"-inf\"))\n        attn = attn.softmax(dim=-1) # [B, C, N, N]\n        attn = self.attn_drop(attn)\n        node_embeds = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\n        # node-to-edge propagation\n        edge_embeds = self.expand(attn + residual)  # [B, n_head, 1+N, 1+N] -> [B, C, 1+N, 1+N]\n\n        # edge-to-node propagation\n        w = edge_embeds.masked_fill(padding_mask, float(\"-inf\"))\n        w = w.softmax(dim=-1)\n        w = (w * edge_embeds).sum(-1).transpose(-1, -2)\n        node_embeds = node_embeds + self.fc(w)\n        node_embeds = self.proj(node_embeds)\n        node_embeds = self.proj_drop(node_embeds)\n\n        return node_embeds, edge_embeds", "\n\nclass FFN(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0., drop_act=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n        self.drop_act = nn.Dropout(drop_act)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop_act(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x", "\n\nclass GPTransBlock(nn.Module):\n    def __init__(self, node_dim, edge_dim, num_heads, mlp_ratio=1., qkv_bias=True, drop=0., drop_act=0.,\n                 with_cp=False, attn_drop=0., drop_path=0., init_values=None):\n        super().__init__()\n        self.with_cp = with_cp\n        self.norm1 = nn.LayerNorm(node_dim)\n        self.gpa = GraphPropagationAttention(node_dim, edge_dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                                             attn_drop=attn_drop, proj_drop=drop)\n        self.norm2 = nn.LayerNorm(node_dim)\n        self.ffn = FFN(in_features=node_dim, hidden_features=int(node_dim * mlp_ratio),\n                       drop=drop, drop_act=drop_act)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        if init_values is not None:\n            self.gamma1 = nn.Parameter(init_values * torch.ones((node_dim)), requires_grad=True)\n            self.gamma2 = nn.Parameter(init_values * torch.ones((node_dim)), requires_grad=True)\n        else:\n            self.gamma1 = None\n            self.gamma2 = None\n\n    def forward(self, node_embeds, edge_embeds, padding_mask):\n        \n        def _inner_forward(x, edge_embeds):\n            if self.gamma1 is not None:\n                attn, edge_embeds_ = self.gpa(self.norm1(x), edge_embeds, padding_mask)\n                edge_embeds = edge_embeds + edge_embeds_\n                x = x + self.drop_path(self.gamma1 * attn)\n                x = x + self.drop_path(self.gamma2 * self.ffn(self.norm2(x)))\n            else:\n                attn, edge_embeds_ = self.gpa(self.norm1(x), edge_embeds, padding_mask)\n                edge_embeds = edge_embeds + edge_embeds_\n                x = x + self.drop_path(attn)\n                x = x + self.drop_path(self.ffn(self.norm2(x)))\n            return x, edge_embeds\n        \n        if self.with_cp and node_embeds.requires_grad:\n            node_embeds, edge_embeds = cp.checkpoint(_inner_forward, node_embeds, edge_embeds)\n        else:\n            node_embeds, edge_embeds = _inner_forward(node_embeds, edge_embeds)\n        \n        return node_embeds, edge_embeds", "\n\nclass GraphEmbedding(nn.Module):\n    def __init__(self, num_atoms, num_in_degree, num_out_degree, num_edges, num_spatial,\n                 num_edge_dist, edge_type, multi_hop_max_dist, num_layers, node_dim,\n                 edge_dim, num_heads, dropout):\n        \n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.embedding_dim = node_dim\n        self.emb_layer_norm = nn.LayerNorm(node_dim)\n        self.graph_node_feature = GraphNodeFeature(\n            num_heads=num_heads,\n            num_atoms=num_atoms,\n            num_in_degree=num_in_degree,\n            num_out_degree=num_out_degree,\n            hidden_dim=node_dim,\n            num_layers=num_layers,\n        )\n        \n        self.graph_edge_feature = GraphEdgeFeature(\n            num_heads=num_heads,\n            num_edges=num_edges,\n            num_spatial=num_spatial,\n            num_edge_dist=num_edge_dist,\n            edge_type=edge_type,\n            multi_hop_max_dist=multi_hop_max_dist,\n            num_layers=num_layers,\n            edge_dim=edge_dim,\n        )\n \n    def forward(self, batched_data, perturb=None):\n        # compute padding mask. This is needed for multi-head attention\n        data_x = batched_data[\"x\"]  # [B, 18 or 19, 9]\n        n_graph, n_node = data_x.size()[:2]\n        padding_mask = (data_x[:, :, 0]).eq(0)  # B node_embeds T\n        padding_mask_cls = torch.zeros(  # not mask\n            n_graph, 1, device=padding_mask.device, dtype=padding_mask.dtype\n        )  # B node_embeds 1\n        padding_mask = torch.cat((padding_mask_cls, padding_mask), dim=1)\n        # B node_embeds (1+T)\n        \n        node_embeds = self.graph_node_feature(batched_data)\n        if perturb is not None:  # perturb is None\n            node_embeds[:, 1:, :] += perturb\n        \n        # node_embeds: B node_embeds T node_embeds C\n        edge_embeds = self.graph_edge_feature(batched_data)\n        node_embeds = self.emb_layer_norm(node_embeds)\n        node_embeds = self.dropout(node_embeds)\n        \n        return node_embeds, edge_embeds, padding_mask", "\n\nclass GPTrans(nn.Module):\n    def __init__(self, num_layers=24, num_heads=23, node_dim=736, edge_dim=92, layer_scale=1.0, mlp_ratio=1.0,\n                 drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, num_atoms=4608, num_edges=1536,\n                 num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dist=128, multi_hop_max_dist=20,\n                 edge_type='multi_hop', qkv_bias=True, num_classes=1, task_type=\"graph_regression\",\n                 random_feature=False, with_cp=False):\n        super(GPTrans, self).__init__()\n        logger.info(f\"drop: {drop_rate}, drop_path_rate: {drop_path_rate}, attn_drop_rate: {attn_drop_rate}\")\n        \n        self.task_type = task_type\n        self.random_feature = random_feature\n        self.graph_embedding = GraphEmbedding(\n            num_atoms=num_atoms,\n            num_in_degree=num_in_degree,\n            num_out_degree=num_out_degree,\n            num_edges=num_edges,\n            num_spatial=num_spatial,\n            num_edge_dist=num_edge_dist,\n            edge_type=edge_type,\n            multi_hop_max_dist=multi_hop_max_dist,\n            num_layers=num_layers,\n            node_dim=node_dim,\n            edge_dim=edge_dim,\n            num_heads=num_heads,\n            dropout=drop_rate,\n        )\n        \n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, num_layers)]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(*[\n            GPTransBlock(node_dim=node_dim, edge_dim=edge_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                         drop_act=drop_rate, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                         qkv_bias=qkv_bias, init_values=layer_scale, with_cp=with_cp) for i in range(num_layers)\n        ])\n    \n        self.fc_layer = nn.Sequential(\n            nn.Linear(node_dim + edge_dim, node_dim),\n            nn.LayerNorm(node_dim),\n            nn.GELU(),\n        )\n        self.head = nn.Linear(node_dim, num_classes, bias=True)\n\n    def forward(self, batched_data, perturb=None):\n        node_embeds, edge_embeds, padding_mask = self.graph_embedding(\n            batched_data,\n            perturb=perturb,\n        )\n        if self.random_feature and self.training:\n            node_embeds += torch.rand_like(node_embeds)\n            edge_embeds += torch.rand_like(edge_embeds)\n        padding_mask = padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool)\n\n        for blk in self.blocks:\n            node_embeds, edge_embeds = blk(node_embeds,  # [B, 1+N, C]\n                                           edge_embeds,  # [B, C, 1+N, 1+N]\n                                           padding_mask) # [B, 1+N, 1]\n        if self.task_type == \"graph_regression\" or self.task_type == \"graph_classification\":\n            x = torch.cat([node_embeds[:, :1, :], edge_embeds[:, :, 0:1, 0].transpose(-1, -2)], dim=2)\n            x = self.fc_layer(x)\n            x = self.head(x)[:, 0, :] # select the virtual node\n            if x.size(-1) == 1: x = x.squeeze(-1)\n        elif self.task_type == \"node_classification\":\n            diag = torch.diagonal(edge_embeds, dim1=-1, dim2=-2).transpose(-1, -2)\n            x = torch.cat([node_embeds, diag], dim=2)[:, 1:, :]\n            x = x.reshape(-1, x.shape[-1])\n            x = self.fc_layer(x)\n            x = self.head(x)\n        return x\n\n    @torch.jit.ignore\n    def lr_decay_keywords(self, decay_ratio=0.87):\n        lr_ratios = {}\n        depth = len(self.blocks) + 1\n        for k, v in self.named_parameters():\n            if \"graph_embedding.\" in k:\n                lr_ratios[k] = decay_ratio ** depth\n            elif \"blocks.\" in k:\n                block_id = int(k.split(\".\")[1])\n                lr_ratios[k] = decay_ratio ** (depth - block_id - 1)\n            elif \"fc_layer.\" in k or \"head.\" in k:\n                lr_ratios[k] = 1\n        return lr_ratios"]}
