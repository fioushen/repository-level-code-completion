{"filename": "main.py", "chunked_list": ["import argparse\nimport os\nimport tqdm\nimport json\nimport pickle\nimport pandas as pd\nimport torch\n\nfrom whisper.tokenizer import LANGUAGES, TO_LANGUAGE_CODE\nfrom whisper.utils import (", "from whisper.tokenizer import LANGUAGES, TO_LANGUAGE_CODE\nfrom whisper.utils import (\n    optional_float,\n    optional_int,\n    str2bool,\n)\nfrom whisper.audio import SAMPLE_RATE\n\nfrom src.utils import set_seeds\nfrom src.url_loader import YoutubeLoader", "from src.utils import set_seeds\nfrom src.url_loader import YoutubeLoader\nfrom src.enhance import SpeechEnhancer\n\nfrom src.diarize import SpeakerDiarizer\nfrom src.asr import SpeechRecognizer\nfrom src.collector import CleanSpeechDetector\n\nfrom src.visualize import viewer\n", "from src.visualize import viewer\n\nfrom src.subtitle_writer import WriteASS\n\ndef get_args():\n    from whisper import available_models\n\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    parser.add_argument('--num_threads', type=int, default=0, required = False, help='number of threads')\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n\n\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n\n    # youtube loader config\n    parser.add_argument('--url', type=str, default='https://www.youtube.com/watch?v=M7h4bbv7XeE', required=False, help='youtube url')\n    parser.add_argument('--yt_dir', type=str, default='data/youtube', required=False, help='mp4 download directory')\n\n    # ASR config\n    parser.add_argument(\"--asr_model\", default=\"small\", choices=available_models(), help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--asr_model_dir\", type=str, default=None, help=\"path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--asr_output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--asr_output_format\", \"-f\", type=str, default=\"all\", choices=[\"all\", \"srt\", \"srt-word\", \"vtt\", \"txt\", \"tsv\", \"ass\", \"ass-char\", \"pickle\", \"vad\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n\n    # speech enhancement config\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n    parser.add_argument('--use_se', type=bool, default=False, required=False, help='True if you use speech enhancement mode')\n\n    # clean speech detector config\n    parser.add_argument(\"--csd_csv_dir\", type=str, default='csd/csv', help=\"path to experiments directory\")\n\n    # speech quality assessment config\n    parser.add_argument(\"--sqa_ssl_model_path\", type=str, default='models/sqa_models/wav2vec_small.pt', help=\"pretrained wav2vec base model path\")\n    parser.add_argument(\"--sqa_model_ckpt_path\", type=str, default='models/sqa_models/model_noresqa_mos.pth', help=\"pretrained NORESQA-MOS model path\")\n    parser.add_argument('--sqa_nmr_wav_dir', type=str, default='/mnt/dataset/daps', required = False, help='path of clean wav file')\n    parser.add_argument('--sqa_nmr_feat_path', type=str, default='sqa/noresqa/feat/daps_nmr_embs.pkl', required = False, help='path of nmr embedding pickle file')\n    parser.add_argument(\"--sqa_nmr_chunk_time\", type=float, default=3.0, help=\"nmr wav chunk time\")\n    parser.add_argument(\"--sqa_nmr_step_size\", type=int, default=75, help=\"embedding step size\")\n\n    # sound classification config\n    parser.add_argument('--sc_ontology_file_path', type=str, default='data/BEATs/ontology.json', required=False, help='path of audioset ontology')\n    parser.add_argument('--sc_labels_indices_csv', type=str, default='data/BEATs/class_labels_indices.csv', required=False, help='csv file of containing audioset label indices')\n    parser.add_argument(\"--beats_model_ckpt_path\", type=str, default='models/sc_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt', help=\"pretrained BEATs model path\")\n    parser.add_argument(\"--sc_chunk_time\", type=float, default=1.0, help=\"sc chunk time\")\n    parser.add_argument(\"--sc_step_ratio\", type=float, default=0.1, help=\"sc step ratio\")\n\n    # alignment params\n    parser.add_argument(\"--align_model\", default=None, help=\"Name of phoneme-level ASR model to do alignment\")\n    parser.add_argument(\"--align_extend\", default=2, type=float, help=\"Seconds before and after to extend the whisper segments for alignment (if not using VAD).\")\n    parser.add_argument(\"--align_from_prev\", default=True, type=bool, help=\"Whether to clip the alignment start time of current segment to the end time of the last aligned word of the previous segment (if not using VAD)\")\n    parser.add_argument(\"--interpolate_method\", default=\"nearest\", choices=[\"nearest\", \"linear\", \"ignore\"], help=\"For word .srt, method to assign timestamps to non-aligned words, or merge them into neighbouring.\")\n    parser.add_argument(\"--no_align\", action='store_true', help=\"Do not perform phoneme alignment\")\n\n    # vad params\n    parser.add_argument(\"--hf_token\", type=str, default='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', help=\"Hugging Face Access Token to access PyAnnote gated models\")\n    parser.add_argument(\"--vad_tmp_dir\", default=\"vad/tmp_wav\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n    parser.add_argument(\"--vad_save_lab_dir\", default=\"vad/lab\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n\n    parser.add_argument(\"--vad_filter\", default=True, help=\"Whether to pre-segment audio with VAD, highly recommended! Produces more accurate alignment + timestamp see WhisperX paper https://arxiv.org/abs/2303.00747\")\n    parser.add_argument(\"--vad_onset\", type=float, default=0.500, help=\"Onset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected\")\n    parser.add_argument(\"--vad_offset\", type=float, default=0.363, help=\"Offset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected.\")\n    parser.add_argument(\"--vad_pad_onset\", type=float, default=0.250, help=\"Padding Onset for VAD (see pyannote.audio)\")\n    parser.add_argument(\"--vad_pad_offset\", type=float, default=0.250, help=\"Padding time for VAD (see pyannote.audio)\")\n\n    # diarization params\n    parser.add_argument(\"--no_diarize\", action=\"store_false\", help=\"Apply diarization to assign speaker labels to each segment/word\")\n    parser.add_argument(\"--min_speakers\", default=None, type=int)\n    parser.add_argument(\"--max_speakers\", default=None, type=int)\n\n    parser.add_argument(\"--diar_exp_dir\", type=str, default='sd', help=\"path to diarization experiments directory\")\n    parser.add_argument('--diar_model_name', type=str, default='pyannote/speaker-diarization@2.1', required=False, help='pretrained speaker diarization model name')\n    parser.add_argument('--diar_embedding', type=str, default='speechbrain/spkrec-ecapa-voxceleb', required=False, help='pretrained speaker diarization model name')\n    # parser.add_argument('--diar_embedding', type=str, default='fbdp1202/mfa-conformer', required=False, help='pretrained speaker diarization model name')\n\n    # whisper params\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=False, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'\u201c\u00bf([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n\n    # parser.add_argument(\"--model_flush\", action=\"store_true\", help=\"Flush memory from each model after use, reduces GPU requirement but slower processing >1 audio file.\")\n\n    # custom\n    parser.add_argument(\"--overwrite\", action='store_true', help=\"Extracting features independently of their existence\")\n    # fmt: on\n\n    # subtitle ass\n    parser.add_argument(\"--ass_dir\", type=str, default='ass', help=\"path to experiments directory\")\n\n\n    args = parser.parse_args().__dict__\n    \n    args['vad_tmp_dir'] = os.path.join(args['exp_dir'], args['vad_tmp_dir'])\n    args['vad_save_lab_dir'] = os.path.join(args['exp_dir'], args['vad_save_lab_dir'])\n    args['sqa_nmr_feat_path'] = os.path.join(args['exp_dir'], args['sqa_nmr_feat_path'])\n    args['csd_csv_dir'] = os.path.join(args['exp_dir'], args['csd_csv_dir'])\n    args['diar_exp_dir'] = os.path.join(args['exp_dir'], args['diar_exp_dir'])\n    args['ass_dir'] = os.path.join(args['exp_dir'], args['ass_dir'])\n\n    return args", "\ndef write_results_json(wav_path, asr_results, df, args, topk=5):\n    results = {}\n    results[\"wav_path\"] = wav_path\n\n    # default values except embedding\n    results[\"sd_cfg\"] = {\n        \"segment\": \"pyannote/segmentation@2022.07\",\n        \"segment_duration\": 5.0,\n        \"segment_step\": 0.1,\n        \"embedding\": args['diar_embedding'],\n        \"embedding_exclude_overlap\": True,\n    }\n    \n    results[\"sc_cfg\"] = {\n        \"model\": \"BEATs\",\n        \"ckpt_path\": \"models/sc_models/BEATs_iter3_plus.pt\",\n        \"chunk_time\": args['sc_chunk_time'],\n        \"step_ratio\": args['sc_step_ratio'],\n    }\n\n    results[\"sqa_cfg\"] = {\n        \"obj_model\": \"TorchAudio-Squim\",\n        \"sbj_model\": \"NORESQA-MOS\",\n        \"max_nmr_wav_time\": args['sqa_nmr_chunk_time'],\n        \"nmr_step_size\": args['sqa_nmr_step_size'],\n        \"nmr_wav_npy\": \"/mnt/dataset/daps/clean_nmr_n100_{}ms.npy\".format(int(args['sqa_nmr_chunk_time']*1000)),\n        \"max_time\": 60,\n    }\n\n    results[\"segments\"] = []\n    assert(len(asr_results[\"segments\"]) == len(df))\n\n    for id in range(len(df)):\n        df_dict = df.iloc[id].to_dict()\n        asr_dict = asr_results[\"segments\"][id]\n        \n        seg_dict = {}\n        seg_dict[\"start\"] = df_dict[\"start\"]\n        seg_dict[\"end\"] = df_dict[\"end\"]\n        seg_dict[\"spk_id\"] = asr_dict[\"speaker\"]\n        seg_dict[\"text\"] = asr_dict[\"text\"]\n        seg_dict[\"audio_tag\"] = []\n\n        key_names = [\"code\", \"name\", \"pred\"]\n        for k in range(topk):\n            audio_tag_dict = {}\n            for key in key_names:\n                audio_tag_dict[key] = df_dict[\"top{}_{}\".format(k+1, key)]\n            seg_dict[\"audio_tag\"].append(audio_tag_dict)\n        \n        seg_dict[\"sqa_tag\"] = {\n            \"pred_mos\": df_dict['NORESQA_MOS']\n        }\n        for key in ['SQUIM_STOI','SQUIM_PESQ','SQUIM_SI-SDR']:\n            if key in df_dict.keys():\n                name = key.lower().replace('squim', 'pred')\n                seg_dict[\"sqa_tag\"][name] = df_dict[key]\n\n        results[\"segments\"].append(seg_dict)\n\n    # results json write \n    result_dir = os.path.join(args['exp_dir'], 'results')\n    os.makedirs(result_dir, exist_ok=True)\n\n    basename = os.path.splitext(os.path.basename(wav_path))[0]\n    with open(os.path.join(result_dir, basename+\".json\"), 'w') as wf:\n        json.dump(results, wf, indent=4)\n    \n    return results", "\ndef main():\n\n    args = get_args()\n    set_seeds(args['seed'])\n\n    overwrite: bool = args.pop(\"overwrite\")\n\n    # The Dark Knight\n    # url = 'https://www.youtube.com/playlist?list=PLrT4uvwaf6uw5ChxpBQnx0dA5fcmXvuB_'\n    # url = 'https://www.youtube.com/watch?v=jane6C4rIwc'\n\n    # \ub0e5\uc774\uc544\ube60\n    # url = 'https://www.youtube.com/playlist?list=PL-28pfEORGTTyRFb-HLE-xlugbi8nDBb3'\n    # url = 'https://www.youtube.com/watch?v=Wb6Oc1_SdJw'\n\n    # Short story audiobooks\n    # url = 'https://www.youtube.com/playlist?list=PLC2RC6xxDj2efWJjsD9ry4TSiH4pU4hHE'\n\n    # \uc608\ub2a5: \ub974\uc138\ub77c\ud54c\n    # url = 'https://www.youtube.com/playlist?list=PLUnnlhhDy3eZqoEIN8q4fMfV9tlOMikob'\n\n    # \ub300\ud654\uccb4 \uc124\uba85\n    # url = 'https://www.youtube.com/watch?v=M7h4bbv7XeE'\n\n    url = args['url']\n\n    downloader = YoutubeLoader(args)\n\n    # download youtube clip\n    dir_list = sorted(downloader(url))\n    del downloader\n\n    # generate wav list\n    wav_list = []\n    for dir_name in dir_list:\n        basename = os.path.basename(dir_name)\n        \n        wav_path = os.path.join(dir_name, 'wav', basename+\".wav\")\n        assert(os.path.exists(wav_path)), \"No Exists Wav File: {}\".format(wav_path)\n\n        wav_list.append(wav_path)\n\n    # run speech enhancement\n    # use_se: bool = args['use_se']\n    use_se: bool = False\n    if use_se:\n        enhancer = SpeechEnhancer(args)\n        se_wav_list = enhancer(wav_list)\n        assert(len(se_wav_list) == len(wav_list)),\\\n            \"Not Match Speech Enhancement Wav File Number ({} != {})\".format(len(se_wav_list), len(wav_list))\n        del enhancer\n\n    # run speaker diarization\n    diarizer = SpeakerDiarizer(args)\n\n    diar_annot_list = []\n    for wav_path in tqdm.tqdm(wav_list):\n        diar_results = diarizer(wav_path)\n        diar_annot_list.append(diar_results)\n    del diarizer\n\n    # run ASR\n    translator = SpeechRecognizer(args)\n    asr_result_list = []\n    for wav_path, diar_annot in tqdm.tqdm(zip(wav_list, diar_annot_list)):\n        asr_result = translator(wav_path, diar_annot)\n        asr_result_list.append(asr_result)\n    del translator\n\n    # run Speech Quality Assessment with Sound Classification\n    detector = CleanSpeechDetector(args)\n\n    df_list = {}\n    for (wav_path, dir_name, asr_result) in tqdm.tqdm(zip(wav_list, dir_list, asr_result_list)):\n        csv_path = os.path.join(args['csd_csv_dir'], os.path.basename(dir_name) + \".csv\")\n        # will be fixed...\n        if os.path.exists(csv_path) and not overwrite and False:\n            df = pd.read_csv(csv_path)\n        else:\n            df = detector(wav_path, results=asr_result, use_se=use_se,\n                        sc_chunk_time=args['sc_chunk_time'], sc_step_ratio=args['sc_step_ratio'])\n        df_list[dir_name] = df\n    del detector\n    print(\"DONE SQA.\")\n    \n    for (wav_path, dir_name, asr_result) in tqdm.tqdm(zip(wav_list, dir_list, asr_result_list)):\n        df = df_list[dir_name]\n        result = write_results_json(wav_path, asr_result, df, args)\n\n        ass_dir = os.path.join(args['ass_dir'], os.path.basename(dir_name))\n        os.makedirs(ass_dir, exist_ok=True)\n\n        writer = WriteASS(ass_dir)\n        writer(result, wav_path)", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "run_diarizate_voxconverse.py", "chunked_list": ["import argparse\nimport os\nimport tqdm\nimport glob\nimport json\nimport pickle\nimport pandas as pd\nimport torch\nfrom whisper.tokenizer import LANGUAGES, TO_LANGUAGE_CODE\nfrom whisper.utils import (", "from whisper.tokenizer import LANGUAGES, TO_LANGUAGE_CODE\nfrom whisper.utils import (\n    optional_float,\n    optional_int,\n    str2bool,\n)\nfrom whisper.audio import SAMPLE_RATE\n\nfrom src.utils import set_seeds\n", "from src.utils import set_seeds\n\nfrom src.enhance import SpeechEnhancer\n\nfrom src.diarize import SpeakerDiarizer\n\nfrom src.visualize import viewer\n\ndef get_args():\n    from whisper import available_models\n\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    parser.add_argument('--num_threads', type=int, default=0, required = False, help='number of threads')\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n\n\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n\n    # youtube loader config\n    parser.add_argument('--yt_dir', type=str, default='data/youtube', required=False, help='mp4 download directory')\n\n    # ASR config\n    parser.add_argument(\"--asr_model\", default=\"small\", choices=available_models(), help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--asr_model_dir\", type=str, default=None, help=\"path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--asr_output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--asr_output_format\", \"-f\", type=str, default=\"all\", choices=[\"all\", \"srt\", \"srt-word\", \"vtt\", \"txt\", \"tsv\", \"ass\", \"ass-char\", \"pickle\", \"vad\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n\n    # speech enhancement config\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n    parser.add_argument('--use_se', type=bool, default=False, required=False, help='True if you use speech enhancement mode')\n\n    # clean speech detector config\n    parser.add_argument(\"--csd_csv_dir\", type=str, default='csd/csv', help=\"path to experiments directory\")\n\n    # speech quality assessment config\n    parser.add_argument(\"--sqa_ssl_model_path\", type=str, default='models/sqa_models/wav2vec_small.pt', help=\"pretrained wav2vec base model path\")\n    parser.add_argument(\"--sqa_model_ckpt_path\", type=str, default='models/sqa_models/model_noresqa_mos.pth', help=\"pretrained NORESQA-MOS model path\")\n    parser.add_argument('--sqa_nmr_wav_dir', type=str, default='/mnt/dataset/daps', required = False, help='path of clean wav file')\n    parser.add_argument('--sqa_nmr_feat_path', type=str, default='sqa/noresqa/feat/daps_nmr_embs.pkl', required = False, help='path of nmr embedding pickle file')\n    parser.add_argument(\"--sqa_nmr_chunk_time\", type=float, default=3.0, help=\"nmr wav chunk time\")\n    parser.add_argument(\"--sqa_nmr_step_size\", type=int, default=75, help=\"embedding step size\")\n\n    # sound classification config\n    parser.add_argument('--sc_ontology_file_path', type=str, default='data/BEATs/ontology.json', required=False, help='path of audioset ontology')\n    parser.add_argument('--sc_labels_indices_csv', type=str, default='data/BEATs/class_labels_indices.csv', required=False, help='csv file of containing audioset label indices')\n    parser.add_argument(\"--beats_model_ckpt_path\", type=str, default='models/sc_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt', help=\"pretrained BEATs model path\")\n    parser.add_argument(\"--sc_chunk_time\", type=float, default=1.0, help=\"sc chunk time\")\n    parser.add_argument(\"--sc_step_ratio\", type=float, default=0.1, help=\"sc step ratio\")\n\n    # alignment params\n    parser.add_argument(\"--align_model\", default=None, help=\"Name of phoneme-level ASR model to do alignment\")\n    parser.add_argument(\"--align_extend\", default=2, type=float, help=\"Seconds before and after to extend the whisper segments for alignment (if not using VAD).\")\n    parser.add_argument(\"--align_from_prev\", default=True, type=bool, help=\"Whether to clip the alignment start time of current segment to the end time of the last aligned word of the previous segment (if not using VAD)\")\n    parser.add_argument(\"--interpolate_method\", default=\"nearest\", choices=[\"nearest\", \"linear\", \"ignore\"], help=\"For word .srt, method to assign timestamps to non-aligned words, or merge them into neighbouring.\")\n    parser.add_argument(\"--no_align\", action='store_true', help=\"Do not perform phoneme alignment\")\n\n    # vad params\n    parser.add_argument(\"--hf_token\", type=str, default='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', help=\"Hugging Face Access Token to access PyAnnote gated models\")\n    parser.add_argument(\"--vad_tmp_dir\", default=\"vad/tmp_wav\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n    parser.add_argument(\"--vad_save_lab_dir\", default=\"vad/lab\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n\n    parser.add_argument(\"--vad_filter\", default=True, help=\"Whether to pre-segment audio with VAD, highly recommended! Produces more accurate alignment + timestamp see WhisperX paper https://arxiv.org/abs/2303.00747\")\n    parser.add_argument(\"--vad_onset\", type=float, default=0.500, help=\"Onset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected\")\n    parser.add_argument(\"--vad_offset\", type=float, default=0.363, help=\"Offset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected.\")\n    parser.add_argument(\"--vad_pad_onset\", type=float, default=0.250, help=\"Padding Onset for VAD (see pyannote.audio)\")\n    parser.add_argument(\"--vad_pad_offset\", type=float, default=0.250, help=\"Padding time for VAD (see pyannote.audio)\")\n\n    # diarization params\n    parser.add_argument(\"--no_diarize\", action=\"store_false\", help=\"Apply diarization to assign speaker labels to each segment/word\")\n    parser.add_argument(\"--min_speakers\", default=None, type=int)\n    parser.add_argument(\"--max_speakers\", default=None, type=int)\n\n    parser.add_argument(\"--diar_exp_dir\", type=str, default='sd/voxconverse/test', help=\"path to diarization experiments directory\")\n    parser.add_argument('--diar_model_name', type=str, default='pyannote/speaker-diarization@2.1', required=False, help='pretrained speaker diarization model name')\n    # parser.add_argument('--diar_embedding', type=str, default='fbdp1202/mfa-conformer', required=False, help='pretrained speaker diarization model name')\n    parser.add_argument('--diar_embedding', type=str, default='speechbrain/spkrec-ecapa-voxceleb', required=False, help='pretrained speaker diarization model name')\n\n    # whisper params\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=False, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'\u201c\u00bf([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n\n    # parser.add_argument(\"--model_flush\", action=\"store_true\", help=\"Flush memory from each model after use, reduces GPU requirement but slower processing >1 audio file.\")\n\n    # custom\n    parser.add_argument(\"--overwrite\", action='store_true', help=\"Extracting features independently of their existence\")\n    # fmt: on\n\n    args = parser.parse_args().__dict__\n    \n    args['vad_tmp_dir'] = os.path.join(args['exp_dir'], args['vad_tmp_dir'])\n    args['vad_save_lab_dir'] = os.path.join(args['exp_dir'], args['vad_save_lab_dir'])\n    args['sqa_nmr_feat_path'] = os.path.join(args['exp_dir'], args['sqa_nmr_feat_path'])\n    args['csd_csv_dir'] = os.path.join(args['exp_dir'], args['csd_csv_dir'])\n\n    args['diar_exp_dir'] = os.path.join(args['exp_dir'], args['diar_exp_dir'], args['diar_embedding'].replace('/', '_'))\n\n    return args", "def get_args():\n    from whisper import available_models\n\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    parser.add_argument('--num_threads', type=int, default=0, required = False, help='number of threads')\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n\n\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n\n    # youtube loader config\n    parser.add_argument('--yt_dir', type=str, default='data/youtube', required=False, help='mp4 download directory')\n\n    # ASR config\n    parser.add_argument(\"--asr_model\", default=\"small\", choices=available_models(), help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--asr_model_dir\", type=str, default=None, help=\"path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--asr_output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--asr_output_format\", \"-f\", type=str, default=\"all\", choices=[\"all\", \"srt\", \"srt-word\", \"vtt\", \"txt\", \"tsv\", \"ass\", \"ass-char\", \"pickle\", \"vad\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n\n    # speech enhancement config\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n    parser.add_argument('--use_se', type=bool, default=False, required=False, help='True if you use speech enhancement mode')\n\n    # clean speech detector config\n    parser.add_argument(\"--csd_csv_dir\", type=str, default='csd/csv', help=\"path to experiments directory\")\n\n    # speech quality assessment config\n    parser.add_argument(\"--sqa_ssl_model_path\", type=str, default='models/sqa_models/wav2vec_small.pt', help=\"pretrained wav2vec base model path\")\n    parser.add_argument(\"--sqa_model_ckpt_path\", type=str, default='models/sqa_models/model_noresqa_mos.pth', help=\"pretrained NORESQA-MOS model path\")\n    parser.add_argument('--sqa_nmr_wav_dir', type=str, default='/mnt/dataset/daps', required = False, help='path of clean wav file')\n    parser.add_argument('--sqa_nmr_feat_path', type=str, default='sqa/noresqa/feat/daps_nmr_embs.pkl', required = False, help='path of nmr embedding pickle file')\n    parser.add_argument(\"--sqa_nmr_chunk_time\", type=float, default=3.0, help=\"nmr wav chunk time\")\n    parser.add_argument(\"--sqa_nmr_step_size\", type=int, default=75, help=\"embedding step size\")\n\n    # sound classification config\n    parser.add_argument('--sc_ontology_file_path', type=str, default='data/BEATs/ontology.json', required=False, help='path of audioset ontology')\n    parser.add_argument('--sc_labels_indices_csv', type=str, default='data/BEATs/class_labels_indices.csv', required=False, help='csv file of containing audioset label indices')\n    parser.add_argument(\"--beats_model_ckpt_path\", type=str, default='models/sc_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt', help=\"pretrained BEATs model path\")\n    parser.add_argument(\"--sc_chunk_time\", type=float, default=1.0, help=\"sc chunk time\")\n    parser.add_argument(\"--sc_step_ratio\", type=float, default=0.1, help=\"sc step ratio\")\n\n    # alignment params\n    parser.add_argument(\"--align_model\", default=None, help=\"Name of phoneme-level ASR model to do alignment\")\n    parser.add_argument(\"--align_extend\", default=2, type=float, help=\"Seconds before and after to extend the whisper segments for alignment (if not using VAD).\")\n    parser.add_argument(\"--align_from_prev\", default=True, type=bool, help=\"Whether to clip the alignment start time of current segment to the end time of the last aligned word of the previous segment (if not using VAD)\")\n    parser.add_argument(\"--interpolate_method\", default=\"nearest\", choices=[\"nearest\", \"linear\", \"ignore\"], help=\"For word .srt, method to assign timestamps to non-aligned words, or merge them into neighbouring.\")\n    parser.add_argument(\"--no_align\", action='store_true', help=\"Do not perform phoneme alignment\")\n\n    # vad params\n    parser.add_argument(\"--hf_token\", type=str, default='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', help=\"Hugging Face Access Token to access PyAnnote gated models\")\n    parser.add_argument(\"--vad_tmp_dir\", default=\"vad/tmp_wav\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n    parser.add_argument(\"--vad_save_lab_dir\", default=\"vad/lab\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n\n    parser.add_argument(\"--vad_filter\", default=True, help=\"Whether to pre-segment audio with VAD, highly recommended! Produces more accurate alignment + timestamp see WhisperX paper https://arxiv.org/abs/2303.00747\")\n    parser.add_argument(\"--vad_onset\", type=float, default=0.500, help=\"Onset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected\")\n    parser.add_argument(\"--vad_offset\", type=float, default=0.363, help=\"Offset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected.\")\n    parser.add_argument(\"--vad_pad_onset\", type=float, default=0.250, help=\"Padding Onset for VAD (see pyannote.audio)\")\n    parser.add_argument(\"--vad_pad_offset\", type=float, default=0.250, help=\"Padding time for VAD (see pyannote.audio)\")\n\n    # diarization params\n    parser.add_argument(\"--no_diarize\", action=\"store_false\", help=\"Apply diarization to assign speaker labels to each segment/word\")\n    parser.add_argument(\"--min_speakers\", default=None, type=int)\n    parser.add_argument(\"--max_speakers\", default=None, type=int)\n\n    parser.add_argument(\"--diar_exp_dir\", type=str, default='sd/voxconverse/test', help=\"path to diarization experiments directory\")\n    parser.add_argument('--diar_model_name', type=str, default='pyannote/speaker-diarization@2.1', required=False, help='pretrained speaker diarization model name')\n    # parser.add_argument('--diar_embedding', type=str, default='fbdp1202/mfa-conformer', required=False, help='pretrained speaker diarization model name')\n    parser.add_argument('--diar_embedding', type=str, default='speechbrain/spkrec-ecapa-voxceleb', required=False, help='pretrained speaker diarization model name')\n\n    # whisper params\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=False, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'\u201c\u00bf([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n\n    # parser.add_argument(\"--model_flush\", action=\"store_true\", help=\"Flush memory from each model after use, reduces GPU requirement but slower processing >1 audio file.\")\n\n    # custom\n    parser.add_argument(\"--overwrite\", action='store_true', help=\"Extracting features independently of their existence\")\n    # fmt: on\n\n    args = parser.parse_args().__dict__\n    \n    args['vad_tmp_dir'] = os.path.join(args['exp_dir'], args['vad_tmp_dir'])\n    args['vad_save_lab_dir'] = os.path.join(args['exp_dir'], args['vad_save_lab_dir'])\n    args['sqa_nmr_feat_path'] = os.path.join(args['exp_dir'], args['sqa_nmr_feat_path'])\n    args['csd_csv_dir'] = os.path.join(args['exp_dir'], args['csd_csv_dir'])\n\n    args['diar_exp_dir'] = os.path.join(args['exp_dir'], args['diar_exp_dir'], args['diar_embedding'].replace('/', '_'))\n\n    return args", "\ndef main():\n\n    args = get_args()\n    set_seeds(args['seed'])\n\n    overwrite: bool = args.pop(\"overwrite\")\n    \n    wav_list = sorted(glob.glob('/mnt/labelmaker/labelmaker/data/sd/voxconverse_0_3/wav/test' + \"/*.wav\"))\n\n    # run speech enhancement\n    # use_se: bool = args['use_se']\n    use_se: bool = False\n    if use_se:\n        enhancer = SpeechEnhancer(args)\n        se_wav_list = enhancer(wav_list)\n        assert(len(se_wav_list) == len(wav_list)),\\\n            \"Not Match Speech Enhancement Wav File Number ({} != {})\".format(len(se_wav_list), len(wav_list))\n        del enhancer\n\n    # run speaker diarization\n    diarizer = SpeakerDiarizer(args)\n\n    diar_annot_list = []\n    for wav_path in tqdm.tqdm(wav_list):\n        diar_results = diarizer(wav_path)\n        diar_annot_list.append(diar_results)\n    del diarizer", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "src/vad.py", "chunked_list": ["import os\nimport time\nimport pickle\nimport numpy as np\nimport soundfile as sf\nimport librosa\nimport ffmpeg\n\nimport torch\n", "import torch\n\nfrom pyannote.audio import Pipeline\nfrom pyannote.audio import Model, Pipeline\n\nfrom whisperx.vad import load_vad_model, Binarize\n\nfrom .utils import get_wav_duration, myfloor, load_annot_from_lab\n\nimport pdb", "\nimport pdb\n\n\nclass SpeechDetector:\n\n    def __init__(self, args):\n\n        # VAD config\n        device: str = args['device']\n        hf_token: str = args['hf_token']\n\n        self.tmp_dir: str = args['vad_tmp_dir']\n        self.save_lab_dir: str = args['vad_save_lab_dir']\n        os.makedirs(self.tmp_dir, exist_ok=True)\n        os.makedirs(self.save_lab_dir, exist_ok=True)\n\n        self.sr: int = args['sr']\n        self.vad_onset: float = args['vad_onset']\n        self.vad_offset: float = args['vad_offset']\n        self.pad_onset: float = args['vad_pad_onset']\n        self.pad_offset: float = args['vad_pad_offset']\n        \n        # VAD setup\n        self.vad_model = load_vad_model(torch.device(device), self.vad_onset, \n                                        self.vad_offset, use_auth_token=hf_token)\n        self.binarize = Binarize(pad_onset=self.pad_onset, pad_offset=self.pad_offset)\n\n    def run_segmentation(self, input_audio_path):\n        print(\"\\n>>Performing VAD...\")\n        segments = self.vad_model(input_audio_path)\n        return segments\n\n    def check_audio_file(self, audio_path):\n        input_audio_path = audio_path\n        if not audio_path.endswith(\".wav\"):\n            print(\">>VAD requires .wav format, converting to wav as a tempfile...\")\n            # tfile = tempfile.NamedTemporaryFile(delete=True, suffix=\".wav\")\n            audio_basename = os.path.splitext(os.path.basename(audio_path))[0]\n            if self.tmp_dir is not None:\n                input_audio_path = os.path.join(self.tmp_dir, audio_basename + \".wav\")\n            else:\n                input_audio_path = os.path.join(os.path.dirname(audio_path), audio_basename + \".wav\")\n            ffmpeg.input(audio_path, threads=0).output(input_audio_path, ac=1, ar=self.sr).run(cmd=[\"ffmpeg\"])\n        \n        return input_audio_path\n\n    def padding_vad(self, segments, wav_duration):\n        segments = self.binarize(segments, wav_duration)\n        return segments\n\n    def apply_vad_segments(self, binary_segments, input_file_path, out_file_path):\n        audio_arr, _  = librosa.load(input_file_path, sr=None)\n\n        audio_seg_list = []\n        for seg in binary_segments.get_timeline():\n            start_f = int(seg.start * self.sr)\n            end_f = int(seg.end * self.sr)\n            audio_seg_list.append(audio_arr[start_f:end_f])\n        \n        vad_audio_arr = np.concatenate(audio_seg_list)\n        sf.write(out_file_path, vad_audio_arr, self.sr)\n\n    @staticmethod\n    def plot_vad_result(annotation, fig_save_path='test_vad.png', verbose=False):\n        from visualize import plot_annotations\n        \n        annotations = [annotation]\n        plot_annotations(annotations, fig_save_path)\n\n        if verbose:\n            print('>>Plot VAD anntations: {}'.format(fig_save_path))\n\n    @staticmethod\n    def save_vad_result(annotation, save_lab_path, verbose=False):\n        with open(save_lab_path, \"w\") as wf:\n            annotation.write_lab(wf)\n        \n        if verbose:\n            print('>>Save VAD results: {}'.format(save_lab_path))\n\n    def __call__(self, audio_file_path, visualize=False, save_vad=True, overwrite=False, verbose=False):\n\n        save_lab_name = os.path.splitext(os.path.basename(audio_file_path))[0]+'.lab'\n        save_lab_path = os.path.join(self.save_lab_dir, save_lab_name)\n        if not overwrite and os.path.exists(save_lab_path):\n            binarized_segments = load_annot_from_lab(save_lab_path)\n            return binarized_segments\n\n        wav_duration = myfloor(get_wav_duration(audio_file_path), 3)\n\n        vad_segments = self.run_segmentation(audio_file_path)\n\n        binarized_segments = self.padding_vad(vad_segments, wav_duration)\n\n        if visualize:\n            # plot vad results\n            self.plot_vad_result(binarized_segments)\n\n        if save_vad:\n            # write vad segment in .lab file\n            print(\">>Save VAD result in {}\".format(save_lab_path))\n            self.save_vad_result(binarized_segments, save_lab_path, verbose=verbose)\n\n        return binarized_segments", ""]}
{"filename": "src/se_analyzer.py", "chunked_list": ["import os\nimport subprocess\nfrom zipfile import ZipFile\nimport glob\n\nimport torch\nfrom torchmetrics import (\n    SignalNoiseRatio,\n    ScaleInvariantSignalNoiseRatio,\n)", "    ScaleInvariantSignalNoiseRatio,\n)\nfrom torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility\nfrom torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality\n\nfrom enhance import SpeechEnhancer\nfrom utils import load_audio\n\nimport pdb\n", "import pdb\n\n\n# valentini dataset homepage: https://datashare.ed.ac.uk/handle/10283/2791\nVALENTINI_DATASET_URLS = {}\nVALENTINI_DATASET_URLS['clean_testset_wav.zip'] = 'https://datashare.ed.ac.uk/bitstream/handle/10283/2791/clean_testset_wav.zip?sequence=1&isAllowed=y'\nVALENTINI_DATASET_URLS['noisy_testset_wav.zip'] = 'https://datashare.ed.ac.uk/bitstream/handle/10283/2791/noisy_testset_wav.zip?sequence=5&isAllowed=y'\nVALENTINI_DATASET_URLS['testset_txt.zip'] = 'https://datashare.ed.ac.uk/bitstream/handle/10283/2791/testset_txt.zip?sequence=8&isAllowed=y'\nVALENTINI_DATASET_URLS['LICENSE'] = 'https://datashare.ed.ac.uk/bitstream/handle/10283/2791/license_text?sequence=11&isAllowed=y'\n", "VALENTINI_DATASET_URLS['LICENSE'] = 'https://datashare.ed.ac.uk/bitstream/handle/10283/2791/license_text?sequence=11&isAllowed=y'\n\n\ndef download_valentini_dataset(out_dir):\n    for (key, url) in VALENTINI_DATASET_URLS.items():\n        out_file_path = os.path.join(out_dir, key)\n        os.makedirs(os.path.dirname(out_file_path), exist_ok=True)\n\n        if os.path.exists(out_file_path):\n            print(\">>Already Exists File: {}\".format(out_file_path))\n            continue\n\n        print(\">>Download File Name: {}\".format(key))\n        \n        cmd = 'wget -O {} {}'.format(out_file_path, url)\n        out = subprocess.call(cmd, shell=True)\n        if out != 0:\n            raise ValueError(\"Download Failed {}.\".format(url))", "\ndef full_zip_extract(out_dir):\n    \n    for (key, url) in VALENTINI_DATASET_URLS.items():\n\n        if not key.endswith(\".zip\"):\n            continue\n\n        out_dir_path = os.path.join(out_dir, os.path.splitext(key)[0])\n        if os.path.exists(out_dir_path):\n            print(\">>Already Exists Directory: {}\".format(out_dir_path))\n            continue\n\n        zip_file_path = os.path.join(out_dir, key)\n\n        print(\">>Extract Zip Name: {}\".format(zip_file_path))\n        with ZipFile(zip_file_path, 'r') as zf:\n            zf.extractall(out_dir)", "\ndef prepare_valentini_dataset(out_dir, overwrite=False):\n    \n    if os.path.exists(out_dir+\"/.done\") and not overwrite:\n        print(\">>Already Exists Valentini Dataset in {}\".format(out_dir))\n        return\n\n    download_valentini_dataset(out_dir)\n\n    full_zip_extract(out_dir)\n\n    f = open(out_dir+\"/.done\", 'w')\n    f.close()", "\n\nclass SE_Analyzer:\n\n    def __init__(self, args):\n    \n        self.sr: str = args['sr']\n        self.data_dir: str = args[\"se_dataset_dir\"]\n        self.se_out_postfix: str = args['se_out_postfix']\n\n        prepare_valentini_dataset(self.data_dir)\n\n        self.clean_dir = os.path.join(self.data_dir, \"clean_testset_wav\")\n        self.noisy_dir = os.path.join(self.data_dir, \"noisy_testset_wav\")\n\n        self.se_dir = os.path.join(self.data_dir, \"se_testset_wav\")\n        os.makedirs(self.se_dir, exist_ok=True)\n\n        # self.prepare_se_dataset(args)\n\n        self.snr = SignalNoiseRatio()\n        self.sisnr = ScaleInvariantSignalNoiseRatio()\n        self.stoi = ShortTimeObjectiveIntelligibility(self.sr, extended=False)\n        self.pesq = PerceptualEvaluationSpeechQuality(\\\n            self.sr, 'nb' if self.sr < 16000 else 'wb')\n\n        self.matrics = {}\n        self.matrics['snr'] = self.get_SNR\n        self.matrics['sisnr'] = self.get_SI_SNR\n        self.matrics['stoi'] = self.get_STOI\n        self.matrics['pesq'] = self.get_PESQ\n\n    def prepare_se_dataset(self, args):\n\n        if os.path.exists(self.data_dir+\"/.se_done\"):\n            print(\">>Already Prepared SE dataset: {}\".format(self.se_dir))\n            return\n\n        noisy_wav_list = sorted(glob.glob(self.noisy_dir+'/*.wav'))\n\n        se_manager = SpeechEnhancer(args)\n        se_wav_list = se_manager(noisy_wav_list, out_wav_dir=self.se_dir)\n\n        assert(len(se_wav_list) == len(noisy_wav_list)),\\\n            print(\"Not Match File Number {} != {}\".format(len(se_wav_list), len(noisy_wav_list)))\n\n        f = open(self.data_dir+\"/.se_done\", 'w')\n        f.close()\n        print(\"Finished Prepare SE dataset in {}\".format(self.se_dir))\n\n    def get_SNR(self, pred, target):\n        return self.snr(pred, target)\n\n    def get_SI_SNR(self, pred, target):\n        return self.sisnr(pred, target)\n\n    def get_STOI(self, pred, target):\n        return self.stoi(pred, target)\n\n    def get_PESQ(self, pred, target):\n        return self.pesq(pred, target)\n\n    def get_all_metrics(self, pred, target):\n\n        result = {}\n        for (key, func) in self.matrics.items():\n            result[key] = func(pred, target)\n        return result\n\n    def __call__(self):\n        \n        wav_list = [os.path.basename(wav_path).replace(self.se_out_postfix,'') \\\n            for wav_path in sorted(glob.glob(self.se_dir+\"/*.wav\"))]\n        \n        for wav_name in wav_list:\n            \n            clean_wav_path = os.path.join(self.clean_dir, wav_name)\n            noisy_wav_path = os.path.join(self.noisy_dir, wav_name)\n            se_wav_path = os.path.join(self.se_dir, wav_name.replace('.wav', self.se_out_postfix+'.wav'))\n\n            assert(os.path.exists(clean_wav_path)), \"No Exists Wav Name: {}\".format(clean_wav_path)\n            assert(os.path.exists(noisy_wav_path)), \"No Exists Wav Name: {}\".format(noisy_wav_path)\n            assert(os.path.exists(se_wav_path)), \"No Exists Wav Name: {}\".format(se_wav_path)\n\n            clean_wav = load_audio(clean_wav_path)\n            noisy_wav = load_audio(noisy_wav_path)\n            se_wav = load_audio(se_wav_path)\n            \n            pos = min(len(clean_wav), len(noisy_wav), len(se_wav))\n            clean_wav = torch.FloatTensor(clean_wav[:pos])\n            noisy_wav = torch.FloatTensor(noisy_wav[:pos])\n            se_wav = torch.FloatTensor(se_wav[:pos])\n\n            assert(clean_wav.shape == noisy_wav.shape), \"({})!=({})\".format(clean_wav.shape, noisy_wav.shape)\n            assert(clean_wav.shape == se_wav.shape), \"({})!=({})\".format(clean_wav.shape, se_wav.shape)\n\n            cn = self.get_all_metrics(clean_wav, noisy_wav)\n            cs = self.get_all_metrics(clean_wav, se_wav)\n            ns = self.get_all_metrics(noisy_wav, se_wav)\n            print(cn)\n            print(cs)\n            print(ns)\n            pdb.set_trace()\n            print('hi')", "\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import torch\n    import argparse\n    from utils import set_seeds\n    from whisper.audio import SAMPLE_RATE\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument(\"--sr\", type=int, default=SAMPLE_RATE, required = False, help=\"sampling rate\")\n\n    parser.add_argument(\"--se_dataset_dir\", type=str, default=\"/mnt/dataset/velentini\", required = False, help=\"path of valentini\")\n\n    # parser.add_argument('--se_out_postfix', type=str, default='', required=False, help='output postfix string')\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n\n    args = parser.parse_args().__dict__\n\n    set_seeds(args[\"seed\"])\n\n    analyzer = SE_Analyzer(args)\n    analyzer()", ""]}
{"filename": "src/diarize.py", "chunked_list": ["import os\n\nfrom pyannote.core import Annotation, Segment\nimport malaya_speech\n\nfrom .visualize import plot_annotations\nfrom .custom_pyannote.pipeline import Pipeline\n\n\nclass SpeakerDiarizer:\n\n    def __init__(self, args):\n        \n        model_name: str = args['diar_model_name']\n        hf_token: str = args['hf_token']\n        self.exp_dir: str = args['diar_exp_dir']\n        os.makedirs(self.exp_dir, exist_ok=True)\n\n        self.se_out_postfix: str = args['se_out_postfix']\n\n        model = Pipeline.from_pretrained(model_name, use_auth_token=hf_token, embedding=args['diar_embedding'])\n\n        self.model = model.to('cuda:0')\n        \n    def __call__(self, wav_path, save_rttm=True, overwrite=False):\n\n        basename = os.path.splitext(os.path.basename(wav_path))[0]\n        exp_dir = os.path.join(self.exp_dir, basename)\n        rttm_dir = os.path.join(exp_dir, 'rttm')\n        fig_dir = os.path.join(exp_dir, 'fig')\n        os.makedirs(rttm_dir, exist_ok=True)\n        os.makedirs(fig_dir, exist_ok=True)\n\n        annotations = []\n\n        rttm_path = os.path.join(rttm_dir, basename + '.rttm')\n        if not overwrite and os.path.exists(rttm_path):\n            print(\"\\n>>SKIP Diarization: {}\".format(wav_path))\n\n            malaya_result = malaya_speech.extra.rttm.load(rttm_path)[basename]\n\n            result = Annotation()\n            for segment, _, label in malaya_result.itertracks():\n                result[Segment(segment.start, segment.end)] = label\n\n        else:\n            print(\"\\n>>Run Diarization: {}\".format(wav_path))\n            result = self.model(wav_path)\n            if save_rttm:\n                with open(rttm_path, 'wt') as wf:\n                    result.write_rttm(wf)\n        annotations.append(result)\n\n        # se_wav_path = os.path.splitext(wav_path)[0] + self.se_out_postfix + '.wav'\n        # if os.path.exists(se_wav_path):\n        #     se_rttm_path = os.path.join(rttm_dir, basename + self.se_out_postfix + '.rttm')\n        #     if os.path.exists(se_rttm_path):\n        #         se_result = malaya_speech.extra.rttm.load(se_rttm_path)\n        #     else:\n        #         se_result = self.model(se_wav_path)\n        #         if save_rttm:\n        #             with open(se_rttm_path, 'wt') as wf:\n        #                 se_result.write_rttm(wf)\n        #     annotations.append(se_result)\n\n        save_fig_path = os.path.join(fig_dir, basename + '.png')\n        if not os.path.exists(save_fig_path) and False:\n            plot_annotations(annotations, save_fig_path, wav_path=wav_path)\n        return result", "\nclass SpeakerDiarizer:\n\n    def __init__(self, args):\n        \n        model_name: str = args['diar_model_name']\n        hf_token: str = args['hf_token']\n        self.exp_dir: str = args['diar_exp_dir']\n        os.makedirs(self.exp_dir, exist_ok=True)\n\n        self.se_out_postfix: str = args['se_out_postfix']\n\n        model = Pipeline.from_pretrained(model_name, use_auth_token=hf_token, embedding=args['diar_embedding'])\n\n        self.model = model.to('cuda:0')\n        \n    def __call__(self, wav_path, save_rttm=True, overwrite=False):\n\n        basename = os.path.splitext(os.path.basename(wav_path))[0]\n        exp_dir = os.path.join(self.exp_dir, basename)\n        rttm_dir = os.path.join(exp_dir, 'rttm')\n        fig_dir = os.path.join(exp_dir, 'fig')\n        os.makedirs(rttm_dir, exist_ok=True)\n        os.makedirs(fig_dir, exist_ok=True)\n\n        annotations = []\n\n        rttm_path = os.path.join(rttm_dir, basename + '.rttm')\n        if not overwrite and os.path.exists(rttm_path):\n            print(\"\\n>>SKIP Diarization: {}\".format(wav_path))\n\n            malaya_result = malaya_speech.extra.rttm.load(rttm_path)[basename]\n\n            result = Annotation()\n            for segment, _, label in malaya_result.itertracks():\n                result[Segment(segment.start, segment.end)] = label\n\n        else:\n            print(\"\\n>>Run Diarization: {}\".format(wav_path))\n            result = self.model(wav_path)\n            if save_rttm:\n                with open(rttm_path, 'wt') as wf:\n                    result.write_rttm(wf)\n        annotations.append(result)\n\n        # se_wav_path = os.path.splitext(wav_path)[0] + self.se_out_postfix + '.wav'\n        # if os.path.exists(se_wav_path):\n        #     se_rttm_path = os.path.join(rttm_dir, basename + self.se_out_postfix + '.rttm')\n        #     if os.path.exists(se_rttm_path):\n        #         se_result = malaya_speech.extra.rttm.load(se_rttm_path)\n        #     else:\n        #         se_result = self.model(se_wav_path)\n        #         if save_rttm:\n        #             with open(se_rttm_path, 'wt') as wf:\n        #                 se_result.write_rttm(wf)\n        #     annotations.append(se_result)\n\n        save_fig_path = os.path.join(fig_dir, basename + '.png')\n        if not os.path.exists(save_fig_path) and False:\n            plot_annotations(annotations, save_fig_path, wav_path=wav_path)\n        return result", "\nif __name__ == '__main__':\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n    from utils import set_seeds\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    # parser.add_argument('--test_wav_path', type=str, default='data/youtube/jane6C4rIwc/wav/jane6C4rIwc.wav', required=False, help='path of test wav file')\n    parser.add_argument('--test_wav_path', type=str, default='data/youtube/XNKF1kOnUL4/wav/XNKF1kOnUL4.wav', required=False, help='path of test wav file')\n\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    parser.add_argument(\"--diar_exp_dir\", type=str, default='sd', help=\"path to diarization experiments directory\")\n    parser.add_argument(\"--hf_token\", type=str, default='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', help=\"Hugging Face Access Token to access PyAnnote gated models\")\n    parser.add_argument('--diar_model_name', type=str, default='pyannote/speaker-diarization@2.1', required=False, help='pretrained speaker diarization model name')\n\n    # speech enhancement config\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n\n    args = parser.parse_args().__dict__\n\n    set_seeds(args['seed'])\n\n    args['diar_exp_dir'] = os.path.join(args['exp_dir'], args['diar_exp_dir'])\n\n    test_wav_path: str = args.pop('test_wav_path')\n    assert(os.path.exists(test_wav_path)), \"No Exists File Name: {}\".format(test_wav_path)\n\n    diarizer = SpeakerDiarizer(args)\n    result = diarizer(test_wav_path)\n\n    print(\"Diarization Done.\")", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/subtitle_writer.py", "chunked_list": ["import json\n\nimport os\nimport zlib\nfrom typing import Callable, TextIO, Iterator, Tuple\nimport pandas as pd\nimport numpy as np\n\nfrom whisper.utils import ResultWriter\n", "from whisper.utils import ResultWriter\n\nimport pdb\n\ndef write_ass(transcript: Iterator[dict],\n            file: TextIO,\n            color: str = None, underline=True,\n            prefmt: str = None, suffmt: str = None,\n            font: str = None, font_size: int = 24,\n            strip=True, **kwargs):\n    \"\"\"\n    Credit: https://github.com/jianfch/stable-ts/blob/ff79549bd01f764427879f07ecd626c46a9a430a/stable_whisper/text_output.py\n        Generate Advanced SubStation Alpha (ass) file from results to\n    display both phrase-level & word-level timestamp simultaneously by:\n     -using segment-level timestamps display phrases as usual\n     -using word-level timestamps change formats (e.g. color/underline) of the word in the displayed segment\n    Note: ass file is used in the same way as srt, vtt, etc.\n    Parameters\n    ----------\n    transcript: dict\n        results from modified model\n    file: TextIO\n        file object to write to\n    color: str\n        color code for a word at its corresponding timestamp\n        <bbggrr> reverse order hexadecimal RGB value (e.g. FF0000 is full intensity blue. Default: 00FF00)\n    underline: bool\n        whether to underline a word at its corresponding timestamp\n    prefmt: str\n        used to specify format for word-level timestamps (must be use with 'suffmt' and overrides 'color'&'underline')\n        appears as such in the .ass file:\n            Hi, {<prefmt>}how{<suffmt>} are you?\n        reference [Appendix A: Style override codes] in http://www.tcax.org/docs/ass-specs.htm\n    suffmt: str\n        used to specify format for word-level timestamps (must be use with 'prefmt' and overrides 'color'&'underline')\n        appears as such in the .ass file:\n            Hi, {<prefmt>}how{<suffmt>} are you?\n        reference [Appendix A: Style override codes] in http://www.tcax.org/docs/ass-specs.htm\n    font: str\n        word font (default: Arial)\n    font_size: int\n        word font size (default: 48)\n    kwargs:\n        used for format styles:\n        'Name', 'Fontname', 'Fontsize', 'PrimaryColour', 'SecondaryColour', 'OutlineColour', 'BackColour', 'Bold',\n        'Italic', 'Underline', 'StrikeOut', 'ScaleX', 'ScaleY', 'Spacing', 'Angle', 'BorderStyle', 'Outline',\n        'Shadow', 'Alignment', 'MarginL', 'MarginR', 'MarginV', 'Encoding'\n\n    \"\"\"\n\n    fmt_style_dict = {'Name': 'Default', 'Fontname': 'Arial', 'Fontsize': '48', 'PrimaryColour': '&Hffffff',\n                    'SecondaryColour': '&Hffffff', 'OutlineColour': '&H0', 'BackColour': '&H0', 'Bold': '0',\n                    'Italic': '0', 'Underline': '0', 'StrikeOut': '0', 'ScaleX': '100', 'ScaleY': '100',\n                    'Spacing': '0', 'Angle': '0', 'BorderStyle': '1', 'Outline': '1', 'Shadow': '0',\n                    'Alignment': '2', 'MarginL': '10', 'MarginR': '10', 'MarginV': '10', 'Encoding': '0'}\n\n    for k, v in filter(lambda x: 'colour' in x[0].lower() and not str(x[1]).startswith('&H'), kwargs.items()):\n        kwargs[k] = f'&H{kwargs[k]}'\n\n    fmt_style_dict.update((k, v) for k, v in kwargs.items() if k in fmt_style_dict)\n\n    if font:\n        fmt_style_dict.update(Fontname=font)\n    if font_size:\n        fmt_style_dict.update(Fontsize=font_size)\n\n    fmts = f'Format: {\", \".join(map(str, fmt_style_dict.keys()))}'\n\n    styles = f'Style: {\",\".join(map(str, fmt_style_dict.values()))}'\n\n    ass_str = f'[Script Info]\\nScriptType: v4.00+\\nPlayResX: 384\\nPlayResY: 288\\nScaledBorderAndShadow: yes\\n\\n' \\\n            f'[V4+ Styles]\\n{fmts}\\n{styles}\\n\\n' \\\n            f'[Events]\\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\\n\\n'\n\n    if prefmt or suffmt:\n        if suffmt:\n            assert prefmt, 'prefmt must be used along with suffmt'\n        else:\n            suffmt = r'\\r'\n    else:\n        if not color:\n            color = 'HFF00'\n        underline_code = r'\\u1' if underline else ''\n\n        prefmt = r'{\\1c&' + f'{color.upper()}&{underline_code}' + '}'\n        suffmt = r'{\\r}'\n    \n    def secs_to_hhmmss(secs: Tuple[float, int]):\n        mm, ss = divmod(secs, 60)\n        hh, mm = divmod(mm, 60)\n        return f'{hh:0>1.0f}:{mm:0>2.0f}:{ss:0>2.2f}'\n\n\n    def dialogue(chars: str, start: float, end: float, idx_0: int, idx_1: int, audio_tag: list = None, sqa_tag: dict = None) -> str:\n        if idx_0 == -1:\n            text = chars\n        else:\n            text = f'{chars[:idx_0]}{prefmt}{chars[idx_0:idx_1]}{suffmt}{chars[idx_1:]}'\n        if audio_tag is not None:\n            text = text + r'{\\fs10}\\N'\n            tag_text_arr = []\n            for tag in audio_tag:\n                tag_text = f'{tag[\"name\"]}: {tag[\"pred\"]:.2f}'\n                tag_text_arr.append(tag_text)\n            text = text + ','.join(tag_text_arr)\n        \n        if sqa_tag is not None:\n            text = text + r'{\\fs10}\\N'\n            sqa_text_arr = []\n            for key, value in sqa_tag.items():\n                sqa_text = f'{key}: {value:.2f}'\n                sqa_text_arr.append(sqa_text)\n            text = text + ','.join(sqa_text_arr)\n\n        return f\"Dialogue: 0,{secs_to_hhmmss(start)},{secs_to_hhmmss(end)},\" \\\n               f\"Default,,0,0,0,,{text.strip() if strip else text}\"\n    \n    ass_arr = []\n\n    for segment in transcript:\n        # if \"12\" in segment['text']:\n            # import pdb; pdb.set_trace()\n\n        if \"spk_id\" in segment:\n            speaker_str = f\"[{segment['spk_id']}]: \"\n        else:\n            speaker_str = \"\"\n            \n        uttr_ts = {\n            \"chars\": speaker_str + segment['text'],\n            \"start\": segment['start'],\n            \"end\": segment['end'],\n            \"idx_0\": -1,\n            \"idx_1\": -1,\n            \"audio_tag\": segment['audio_tag'],\n            \"sqa_tag\": segment['sqa_tag'],\n        }\n\n        ass_arr.append(uttr_ts)\n\n    ass_str += '\\n'.join(map(lambda x: dialogue(**x), ass_arr))\n\n    file.write(ass_str)", "\nclass WriteASS(ResultWriter):\n    extension: str = \"ass\"\n\n    def write_result(self, result: dict, file: TextIO):\n        write_ass(result[\"segments\"], file)\n\nif __name__ == \"__main__\":\n\n    json_path = '/mnt/labelmaker/labelmaker/exps/results/M7h4bbv7XeE.json'\n    ass_dir = \"/mnt/labelmaker/labelmaker/exps/ass/M7h4bbv7XeE\"\n    wav_path = \"/mnt/labelmaker/labelmaker/data/youtube/M7h4bbv7XeE/wav/M7h4bbv7XeE.wav\"\n    with open(json_path, 'r') as f:\n        result = json.load(f)\n    \n    os.makedirs(ass_dir, exist_ok=True)\n\n    writer = WriteASS(ass_dir)\n    writer(result, wav_path)", ""]}
{"filename": "src/asr.py", "chunked_list": ["import ffmpeg\nimport os\nimport warnings\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\nimport tempfile\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom whisper import load_model\nfrom whisper.audio import SAMPLE_RATE", "from whisper import load_model\nfrom whisper.audio import SAMPLE_RATE\n\nfrom whisperx.alignment import load_align_model, align, check_align_model\nfrom whisperx.asr import transcribe, transcribe_with_vad\nfrom whisperx.diarize import DiarizationPipeline, assign_word_speakers\nfrom whisperx.utils import get_writer\nfrom whisperx.vad import load_vad_model\nfrom whisperx.diarize import Segment as SegmentX\n", "from whisperx.diarize import Segment as SegmentX\n\nfrom whisper.audio import (\n    N_SAMPLES,\n    SAMPLE_RATE,\n    CHUNK_LENGTH,\n    log_mel_spectrogram,\n    load_audio\n)\n", ")\n\nfrom whisper.utils import (\n    exact_div,\n    format_timestamp,\n    make_safe,\n)\n\ndef assign_segment_speakers(diarize_df, result_segments, fill_nearest=False):\n    for seg in result_segments:\n        speakers = []\n        # for wdx, wrow in wdf.iterrows():\n        if not np.isnan(seg['start']):\n            diarize_df['intersection'] = np.minimum(diarize_df['end'], seg['end']) - np.maximum(diarize_df['start'], seg['start'])\n            diarize_df['union'] = np.maximum(diarize_df['end'], seg['end']) - np.minimum(diarize_df['start'], seg['start'])\n            # remove no hit\n            if not fill_nearest:\n                dia_tmp = diarize_df[diarize_df['intersection'] > 0]\n            else:\n                dia_tmp = diarize_df\n            if len(dia_tmp) == 0:\n                speaker = None\n            else:\n                speaker = dia_tmp.sort_values(\"intersection\", ascending=False).iloc[0][2]\n        else:\n            speaker = None\n        speakers.append(speaker)\n\n        speaker_count = pd.Series(speakers).value_counts()\n        if len(speaker_count) == 0:\n            seg[\"speaker\"]= \"UNKNOWN\"\n        else:\n            seg[\"speaker\"] = speaker_count.index[0]\n\n    return result_segments", "def assign_segment_speakers(diarize_df, result_segments, fill_nearest=False):\n    for seg in result_segments:\n        speakers = []\n        # for wdx, wrow in wdf.iterrows():\n        if not np.isnan(seg['start']):\n            diarize_df['intersection'] = np.minimum(diarize_df['end'], seg['end']) - np.maximum(diarize_df['start'], seg['start'])\n            diarize_df['union'] = np.maximum(diarize_df['end'], seg['end']) - np.minimum(diarize_df['start'], seg['start'])\n            # remove no hit\n            if not fill_nearest:\n                dia_tmp = diarize_df[diarize_df['intersection'] > 0]\n            else:\n                dia_tmp = diarize_df\n            if len(dia_tmp) == 0:\n                speaker = None\n            else:\n                speaker = dia_tmp.sort_values(\"intersection\", ascending=False).iloc[0][2]\n        else:\n            speaker = None\n        speakers.append(speaker)\n\n        speaker_count = pd.Series(speakers).value_counts()\n        if len(speaker_count) == 0:\n            seg[\"speaker\"]= \"UNKNOWN\"\n        else:\n            seg[\"speaker\"] = speaker_count.index[0]\n\n    return result_segments", "\nclass SpeechRecognizer:\n    \n    def __init__(self, args):\n        asr_model_name: str = args[\"asr_model\"]\n        model_dir: str = args[\"asr_model_dir\"]\n        \n        self.output_dir: str = args[\"asr_output_dir\"]\n        self.output_format: str = args[\"asr_output_format\"]\n        self.device: str = args[\"device\"]\n        # model_flush: bool = args.pop(\"model_flush\")\n        os.makedirs(self.output_dir, exist_ok=True)\n\n        self.tmp_dir: str = args[\"vad_tmp_dir\"]\n        if self.tmp_dir is not None:\n            os.makedirs(self.tmp_dir, exist_ok=True)\n\n        # Align Config\n        align_model: str = args[\"align_model\"]\n        self.align_extend: float = args[\"align_extend\"]\n        self.align_from_prev: bool = args[\"align_from_prev\"]\n        self.interpolate_method: str = args[\"interpolate_method\"]\n        no_align: bool = args[\"no_align\"]\n\n        # load align model and set language\n        if no_align:\n            self.align_model, self.align_metadata = None, None\n        else:\n            align_language = args[\"language\"] if args[\"language\"] is not None else \"en\" # default to loading english if not specified\n            self.align_model, self.align_metadata = load_align_model(align_language, self.device, model_name=align_model)\n\n        # if model_flush:\n        #     print(\">>Model flushing activated... Only loading model after ASR stage\")\n        #     del align_model\n        #     align_model = \"\"\n\n        if asr_model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n            if args[\"language\"] is not None:\n                warnings.warn(\n                    f\"{asr_model_name} is an English-only model but receipted '{args['language']}'; using English instead.\"\n                )\n            args[\"language\"] = \"en\"\n\n        # set temperature\n        temperature = args.pop(\"temperature\")\n        if (increment := args.pop(\"temperature_increment_on_fallback\")) is not None:\n            self.temperature = tuple(np.arange(temperature, 1.0 + 1e-6, increment))\n        else:\n            self.temperature = [temperature]\n\n        # set num threads\n        if (threads := args.pop(\"threads\")) > 0:\n            torch.set_num_threads(threads)\n\n        self.asr_model = load_model(asr_model_name, device=self.device, download_root=model_dir)\n\n        self.args = args\n\n    def asr_merge_chunks(self, segments, chunk_size):\n        curr_end = 0\n        merged_segments = []\n        seg_idxs = []\n        speaker_idxs = []\n\n        assert chunk_size > 0\n\n        segments_list = []\n        for segment, _, label in segments.itertracks(yield_label=True):\n            segments_list.append(SegmentX(segment.start, segment.end, label))\n\n        if len(segments_list) == 0:\n            print(\"No active speech found in audio\")\n            return []\n\n        # assert segments_list, \"segments_list is empty.\"\n        # Make sur the starting point is the start of the segment.\n        curr_start = segments_list[0].start\n\n        for seg in segments_list:\n            if seg.end - curr_start > chunk_size and curr_end-curr_start > 0:\n                merged_segments.append({\n                    \"start\": curr_start,\n                    \"end\": curr_end,\n                    \"segments\": seg_idxs,\n                })\n                curr_start = seg.start\n                seg_idxs = []\n                speaker_idxs = []\n            curr_end = seg.end\n            seg_idxs.append((seg.start, seg.end))\n            speaker_idxs.append(seg.speaker)\n        # add final\n        merged_segments.append({ \n                    \"start\": curr_start,\n                    \"end\": curr_end,\n                    \"segments\": seg_idxs,\n                })    \n        return merged_segments\n    \n    def transcribe_with_vad_info(self, wav_path, diarize_segments, mel = None, verbose = True):\n        audio = load_audio(wav_path)\n        audio = torch.from_numpy(audio)\n\n        prev = 0\n        output = {\"segments\": []}\n\n        # merge segments to approx 30s inputs to make whisper most appropraite\n        diarize_segments = self.asr_merge_chunks(diarize_segments, chunk_size=CHUNK_LENGTH)\n        # diarize_segments = self.asr_chunking(diarize_segments, chunk_size=CHUNK_LENGTH)\n        if len(diarize_segments) == 0:\n            return output\n\n        print(\">>Performing transcription...\")\n        for sdx, seg_t in enumerate(diarize_segments):\n            if verbose:\n                print(f\"~~ Transcribing VAD chunk: ({format_timestamp(seg_t['start'])} --> {format_timestamp(seg_t['end'])}) ~~\")\n            seg_f_start, seg_f_end = int(seg_t[\"start\"] * SAMPLE_RATE), int(seg_t[\"end\"] * SAMPLE_RATE)\n            local_f_start, local_f_end = seg_f_start - prev, seg_f_end - prev\n            audio = audio[local_f_start:] # seek forward\n            seg_audio = audio[:local_f_end-local_f_start] # seek forward\n            prev = seg_f_start\n            local_mel = log_mel_spectrogram(seg_audio, padding=N_SAMPLES)\n            # need to pad\n\n            decode_kwargs = {}\n            for key in [\"task\", \"language\", \"beam_size\", \"patience\", \"length_penalty\", \"suppress_tokens\", \"fp16\", \"prompt\", \"prefix\"]:\n                if key in self.args.keys():\n                    decode_kwargs[key] = self.args.pop(key)\n\n            result = transcribe(self.asr_model, audio, mel=local_mel, temperature=self.temperature, **decode_kwargs)\n            seg_t[\"text\"] = result[\"text\"]\n            output[\"segments\"].append(\n                {\n                    \"start\": seg_t[\"start\"],\n                    \"end\": seg_t[\"end\"],\n                    \"language\": result[\"language\"],\n                    \"text\": result[\"text\"],\n                    \"seg-text\": [x[\"text\"] for x in result[\"segments\"]],\n                    \"seg-start\": [x[\"start\"] for x in result[\"segments\"]],\n                    \"seg-end\": [x[\"end\"] for x in result[\"segments\"]],\n                    }\n                )\n\n        output[\"language\"] = output[\"segments\"][0][\"language\"]\n\n        return output\n\n    def __call__(self, wav_path, diarize_segments, mel = None):\n\n        result = self.transcribe_with_vad_info(wav_path, diarize_segments, mel=mel, verbose=self.args['verbose'])\n\n        # >> Align\n        if self.align_model is not None and len(result[\"segments\"]) > 0:\n            if check_align_model(result[\"language\"]):\n                if result.get(\"language\", \"en\") != self.align_metadata[\"language\"]:\n                    # load new language\n                    print(f\"New language found ({result['language']})! Previous was ({self.align_metadata['language']}), loading new alignment model for new language...\")\n                    self.align_model, self.align_metadata = load_align_model(result[\"language\"], self.device)\n                print(\">>Performing alignment...\")\n                result = align(result[\"segments\"], self.align_model, self.align_metadata, wav_path, self.device,\n                    extend_duration=self.align_extend, start_from_previous=self.align_from_prev, interpolate_method=self.interpolate_method)\n\n        # >> Diarize\n        diarize_df = pd.DataFrame(diarize_segments.itertracks(yield_label=True))\n        diarize_df['start'] = diarize_df[0].apply(lambda x: x.start)\n        diarize_df['end'] = diarize_df[0].apply(lambda x: x.end)\n\n        if not 'word-segments' in result[\"segments\"]:\n            word_segments = None\n            results_segments = assign_segment_speakers(diarize_df, result[\"segments\"])\n        else:\n            results_segments, word_segments = assign_word_speakers(diarize_df, result[\"segments\"])\n\n        result = {\"segments\": results_segments, \"word_segments\": word_segments}\n\n        return result", ""]}
{"filename": "src/utils.py", "chunked_list": ["import os\nimport time\nimport wave\nimport random\nimport librosa\nimport numpy as np\n\nimport torch\n\nfrom pyannote.core import Annotation, Segment", "\nfrom pyannote.core import Annotation, Segment\n\n\ndef logging_time(original_fn):\n    def wrapper_fn(*args, **kwargs):\n        start_time = time.time()\n        result = original_fn(*args, **kwargs)\n        end_time = time.time()\n        print(\"[{}]: {} sec\".format(original_fn.__name__, end_time-start_time))\n        return result\n    return wrapper_fn", "\ndef set_seeds(seed=777, multi_gpu=False):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    if multi_gpu:\n        torch.cuda.manual_seed_all(seed) # if use multi-GPU", "\ndef get_wav_duration(audio_path):\n    with wave.open(audio_path) as handle:\n        t = handle.getnframes() / handle.getframerate()\n    return t\n\ndef load_audio(audio_file_path, sr=16000, chunk_time=0, mono=True):\n    t = get_wav_duration(audio_file_path)\n\n    if chunk_time != 0 and t > chunk_time:\n        # randomly select start time given in Uniform(0, t-chunk_time)\n        n_frames = t*sr\n        n_chunk_frames = int(chunk_time*sr)\n        start = np.random.randint(0, max(0, n_frames-n_chunk_frames))\n        start_t = start/sr\n\n        waveform, _ = librosa.load(audio_file_path, offset=start_t, duration=chunk_time, sr=sr, mono=mono)\n    else:\n        waveform, _ = librosa.load(audio_file_path, sr=sr, mono=mono)\n\n    return waveform", "\ndef load_annot_from_lab(save_lab_path, spk_id='Speech'):\n    seg_arr = np.atleast_2d(np.loadtxt(save_lab_path, usecols=(0, 1)))\n    annot = Annotation()\n    for (start, end) in zip(seg_arr[:,0], seg_arr[:,1]):\n        annot[Segment(start, end)] = spk_id\n    return annot\n\n# def load_annot_from_rttm(save_rttm_path):\n#     seg_arr = np.atleast_2d(np.loadtxt(save_rttm_path, usecols=(3, 4, 7)))", "# def load_annot_from_rttm(save_rttm_path):\n#     seg_arr = np.atleast_2d(np.loadtxt(save_rttm_path, usecols=(3, 4, 7)))\n#     annot = Annotation()\n#     for (start, end, spk_id) in zip(seg_arr[:,0], seg_arr[:,1], seg_arr[:,2]):\n#         annot[Segment(start, end)] = spk_id\n#     return annot\n\ndef myfloor(x, p):\n    v = 10**p\n    return int(x * v)/v", ""]}
{"filename": "src/enhance.py", "chunked_list": ["import os\nimport tqdm\nimport torch\n\nfrom modelscope.pipelines import pipeline\nfrom modelscope.utils.constant import Tasks\n\nclass SpeechEnhancer:\n    \n    def __init__(self, args):\n\n        self.out_postfix: str = args['se_out_postfix']\n        \n        self.model = pipeline(Tasks.acoustic_noise_suppression, model='damo/speech_frcrn_ans_cirm_16k', verbose=False)\n    \n    def run_enhancement(self, in_wav_path, out_wav_dir=None, overwrite=False):\n        if out_wav_dir is None:\n            out_wav_dir = os.path.dirname(in_wav_path)\n\n        out_wav_name = os.path.splitext(os.path.basename(in_wav_path))[0]+self.out_postfix+'.wav'\n        out_wav_path = os.path.join(out_wav_dir, out_wav_name)\n        if not overwrite and os.path.exists(out_wav_path):\n            print(\">>Already Exists SE WAV: {}\".format(out_wav_path))\n            return out_wav_path\n\n        print(\">>Run Speech Enhancement: {}\".format(out_wav_path))\n        _ = self.model(in_wav_path, output_path=out_wav_path)\n        # raw_pcm = self.model(in_wav_path, output_path=out_wav_path)['output_pcm']\n\n        return out_wav_path\n\n    def __call__(self, test_wav_path, out_wav_dir=None, overwrite=False):\n\n        if isinstance(test_wav_path, str):\n\n            out_wav_path = self.run_enhancement(test_wav_path, out_wav_dir=out_wav_dir, overwrite=overwrite)\n            out_wav_list = [out_wav_path]\n\n        elif isinstance(test_wav_path, list):\n\n            out_wav_list = []\n            for in_wav_path in tqdm.tqdm(test_wav_path):\n                out_wav_path = self.run_enhancement(in_wav_path, out_wav_dir=out_wav_dir, overwrite=overwrite)\n                out_wav_list.append(out_wav_path)\n        \n        return out_wav_list", "\nif __name__ == \"__main__\":\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n    from utils import set_seeds\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/speech_with_noise1.wav', required=False, help='path of test wav file')\n    # parser.add_argument('--test_wav_path', type=str, default='data/youtube/_ezibzn6K9Y/wav/_ezibzn6K9Y.wav', required=False, help='path of test wav file')\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN', required=False, help='output postfix string')\n\n    args = parser.parse_args().__dict__\n\n    set_seeds(args['seed'])\n\n    test_wav_path: str = args.pop('test_wav_path')\n    assert(os.path.exists(test_wav_path)), \"No Exists File Name: {}\".format(test_wav_path)\n\n    se_manager = SpeechEnhancer(args)\n    result, out_wav_list = se_manager(test_wav_path)", "    "]}
{"filename": "src/collector.py", "chunked_list": ["import os\nimport tqdm\nimport pandas as pd\n\nimport torch\n\nfrom pyannote.core import Annotation, Segment\n\nfrom .utils import load_audio\n", "from .utils import load_audio\n\nfrom .vad import SpeechDetector\nfrom .sqa import SpeechQualityAssigner\nfrom .classify import SoundClassifier\n\nimport pdb\n\n\nclass CleanSpeechDetector:\n\n    def __init__(self, args):\n\n        self.sr: int = args['sr']\n        self.csd_csv_dir: str = args['csd_csv_dir']\n        os.makedirs(self.csd_csv_dir, exist_ok=True)\n\n        self.se_out_postfix: str = args['se_out_postfix']\n\n        self.vad_manager = SpeechDetector(args)\n        self.sqa_manager = SpeechQualityAssigner(args)\n        self.sc_manager = SoundClassifier(args)\n\n    def set_vad_wav_name(self, audio_file_path, use_se=False):\n\n        vad_audio_path = audio_file_path\n        if use_se:\n            audio_file_name = os.path.splitext(audio_file_path)[0]\n            se_audio_file_path = audio_file_name + self.se_out_postfix + '.wav'\n            if os.path.exists(se_audio_file_path):\n                vad_audio_path = se_audio_file_path\n            else:\n                print(\"No Exists Speech Enhanced wav: {}\".format(se_audio_file_path))\n        \n        return vad_audio_path\n\n    def run_segments(self, input_audio_path, out_vad, topk=5, sc_chunk_time=1.0, sc_step_ratio=0.1, use_round=True):\n\n        waveform = load_audio(input_audio_path, sr=self.sr)\n        waveform = torch.FloatTensor(waveform)\n\n        columns = [\"index\", \"start\", \"end\"]\n        for k in range(topk):\n            for name in ['code', 'name', 'pred']:\n                columns.append(\"top{}_{}\".format(k+1, name))\n        columns.append(\"NORESQA_MOS\")\n\n        results = {}\n        for col in columns:\n            results[col] = []\n\n        for idx, seg in tqdm.tqdm(enumerate(out_vad.get_timeline())):\n            start_t, end_t = seg.start, seg.end\n            start, end = int(start_t*self.sr), int(end_t*self.sr)\n\n            seg_waveform = waveform[start:end]\n\n            sc_results = self.sc_manager.pred_topk_with_label(seg_waveform, chunk_time=sc_chunk_time, step_ratio=sc_step_ratio, topk=topk)\n            mos_score = self.sqa_manager.estimate_score(seg_waveform)\n\n            results[\"index\"].append(idx)\n            results[\"start\"].append(start_t)\n            results[\"end\"].append(end_t)\n            for k, (code, name, prob) in enumerate(sc_results):\n                for key, value in zip(['code', 'name', 'pred'], [code, name, prob]):\n                    results[\"top{}_{}\".format(k+1, key)].append(value)\n            results[\"NORESQA_MOS\"].append(mos_score)\n\n        df = pd.DataFrame.from_dict(results)\n\n        # optional\n        if use_round:\n            df = df.round(3)\n\n        return df\n\n    def __call__(self, audio_file_path, results=None, use_se=False, save_csv=True, overwrite=False,\n                topk=5, sc_chunk_time=1.0, sc_step_ratio=0.1):\n        \n        if results is None:\n            vad_audio_path = self.set_vad_wav_name(audio_file_path, use_se=use_se)\n\n            binarized_segments = self.vad_manager(vad_audio_path)\n        else:\n            binarized_segments = Annotation()\n            segments = results[\"segments\"]\n            for seg_dict in segments:\n                start = seg_dict['start']\n                end = seg_dict['end']\n                spk_id = seg_dict['speaker']\n                binarized_segments[Segment(start, end)] = spk_id\n\n        df = self.run_segments(audio_file_path, binarized_segments, topk=topk, sc_chunk_time=sc_chunk_time, sc_step_ratio=sc_step_ratio)\n\n        if save_csv:\n            save_csv_name = os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv'\n            save_csv_path = os.path.join(self.csd_csv_dir, save_csv_name)\n\n            df.to_csv(save_csv_path)\n\n        return df", "\nclass CleanSpeechDetector:\n\n    def __init__(self, args):\n\n        self.sr: int = args['sr']\n        self.csd_csv_dir: str = args['csd_csv_dir']\n        os.makedirs(self.csd_csv_dir, exist_ok=True)\n\n        self.se_out_postfix: str = args['se_out_postfix']\n\n        self.vad_manager = SpeechDetector(args)\n        self.sqa_manager = SpeechQualityAssigner(args)\n        self.sc_manager = SoundClassifier(args)\n\n    def set_vad_wav_name(self, audio_file_path, use_se=False):\n\n        vad_audio_path = audio_file_path\n        if use_se:\n            audio_file_name = os.path.splitext(audio_file_path)[0]\n            se_audio_file_path = audio_file_name + self.se_out_postfix + '.wav'\n            if os.path.exists(se_audio_file_path):\n                vad_audio_path = se_audio_file_path\n            else:\n                print(\"No Exists Speech Enhanced wav: {}\".format(se_audio_file_path))\n        \n        return vad_audio_path\n\n    def run_segments(self, input_audio_path, out_vad, topk=5, sc_chunk_time=1.0, sc_step_ratio=0.1, use_round=True):\n\n        waveform = load_audio(input_audio_path, sr=self.sr)\n        waveform = torch.FloatTensor(waveform)\n\n        columns = [\"index\", \"start\", \"end\"]\n        for k in range(topk):\n            for name in ['code', 'name', 'pred']:\n                columns.append(\"top{}_{}\".format(k+1, name))\n        columns.append(\"NORESQA_MOS\")\n\n        results = {}\n        for col in columns:\n            results[col] = []\n\n        for idx, seg in tqdm.tqdm(enumerate(out_vad.get_timeline())):\n            start_t, end_t = seg.start, seg.end\n            start, end = int(start_t*self.sr), int(end_t*self.sr)\n\n            seg_waveform = waveform[start:end]\n\n            sc_results = self.sc_manager.pred_topk_with_label(seg_waveform, chunk_time=sc_chunk_time, step_ratio=sc_step_ratio, topk=topk)\n            mos_score = self.sqa_manager.estimate_score(seg_waveform)\n\n            results[\"index\"].append(idx)\n            results[\"start\"].append(start_t)\n            results[\"end\"].append(end_t)\n            for k, (code, name, prob) in enumerate(sc_results):\n                for key, value in zip(['code', 'name', 'pred'], [code, name, prob]):\n                    results[\"top{}_{}\".format(k+1, key)].append(value)\n            results[\"NORESQA_MOS\"].append(mos_score)\n\n        df = pd.DataFrame.from_dict(results)\n\n        # optional\n        if use_round:\n            df = df.round(3)\n\n        return df\n\n    def __call__(self, audio_file_path, results=None, use_se=False, save_csv=True, overwrite=False,\n                topk=5, sc_chunk_time=1.0, sc_step_ratio=0.1):\n        \n        if results is None:\n            vad_audio_path = self.set_vad_wav_name(audio_file_path, use_se=use_se)\n\n            binarized_segments = self.vad_manager(vad_audio_path)\n        else:\n            binarized_segments = Annotation()\n            segments = results[\"segments\"]\n            for seg_dict in segments:\n                start = seg_dict['start']\n                end = seg_dict['end']\n                spk_id = seg_dict['speaker']\n                binarized_segments[Segment(start, end)] = spk_id\n\n        df = self.run_segments(audio_file_path, binarized_segments, topk=topk, sc_chunk_time=sc_chunk_time, sc_step_ratio=sc_step_ratio)\n\n        if save_csv:\n            save_csv_name = os.path.splitext(os.path.basename(audio_file_path))[0]+'.csv'\n            save_csv_path = os.path.join(self.csd_csv_dir, save_csv_name)\n\n            df.to_csv(save_csv_path)\n\n        return df", "\n\nif __name__ == '__main__':\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n    from utils import set_seeds\n    from whisper.audio import SAMPLE_RATE\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # basic config\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    # parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight.wav', required=False, help='path of test wav file')\n    parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight_SE_FRCRN.wav', required=False, help='path of test wav file')\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    parser.add_argument(\"--csd_csv_dir\", type=str, default='csd/csv', help=\"path to experiments directory\")\n\n    # Speech Enhancement config\n    parser.add_argument('--se_out_postfix', type=str, default='_SE_FRCRN.wav', required=False, help='output postfix string')\n\n    # vad config\n    parser.add_argument(\"--vad_tmp_dir\", default=\"vad/tmp_wav\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n    parser.add_argument(\"--vad_save_lab_dir\", default=\"vad/lab\", help=\"Temporary directory to write audio file if input if not .wav format (only for VAD).\")\n    parser.add_argument(\"--hf_token\", type=str, default='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', help=\"Hugging Face Access Token to access PyAnnote gated models\")\n    parser.add_argument(\"--vad_onset\", type=float, default=0.500, help=\"Onset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected\")\n    parser.add_argument(\"--vad_offset\", type=float, default=0.363, help=\"Offset threshold for VAD (see pyannote.audio), reduce this if speech is not being detected.\")\n    parser.add_argument(\"--vad_pad_onset\", type=float, default=0.250, help=\"Padding Onset for VAD (see pyannote.audio)\")\n    parser.add_argument(\"--vad_pad_offset\", type=float, default=0.250, help=\"Padding time for VAD (see pyannote.audio)\")\n\n    # speech quality assessment config\n    parser.add_argument(\"--sqa_ssl_model_path\", type=str, default='models/sqa_models/wav2vec_small.pt', help=\"pretrained wav2vec base model path\")\n    parser.add_argument(\"--sqa_model_ckpt_path\", type=str, default='models/sqa_models/model_noresqa_mos.pth', help=\"pretrained NORESQA-MOS model path\")\n    parser.add_argument('--sqa_nmr_wav_dir', type=str, default='/mnt/dataset/daps', required = False, help='path of clean wav file')\n    parser.add_argument('--sqa_nmr_feat_path', type=str, default='sqa/noresqa/feat/daps_nmr_embs.pkl', required = False, help='path of nmr embedding pickle file')\n    parser.add_argument(\"--sqa_nmr_chunk_time\", type=float, default=3.0, help=\"nmr wav chunk time\")\n    parser.add_argument(\"--sqa_nmr_step_size\", type=int, default=75, help=\"embedding step size\")\n\n    # sound classification config\n    parser.add_argument('--sc_ontology_file_path', type=str, default='data/BEATs/ontology.json', required=False, help='path of audioset ontology')\n    parser.add_argument('--sc_labels_indices_csv', type=str, default='data/BEATs/class_labels_indices.csv', required=False, help='csv file of containing audioset label indices')\n    parser.add_argument(\"--beats_model_ckpt_path\", type=str, default='models/sc_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt', help=\"pretrained BEATs model path\")\n\n    args = parser.parse_args().__dict__\n    args['vad_tmp_dir'] = os.path.join(args['exp_dir'], args['vad_tmp_dir'])\n    args['vad_save_lab_dir'] = os.path.join(args['exp_dir'], args['vad_save_lab_dir'])\n\n    args['sqa_nmr_feat_path'] = os.path.join(args['exp_dir'], args['sqa_nmr_feat_path'])\n\n    args['csd_csv_dir'] = os.path.join(args['exp_dir'], args['csd_csv_dir'])\n\n    set_seeds(args['seed'])\n\n    test_wav_path: str = args.pop('test_wav_path')\n    assert(os.path.exists(test_wav_path)), \"No Exists File Name: {}\".format(test_wav_path)\n   \n    detector = CleanSpeechDetector(args)\n    df = detector(test_wav_path)", ""]}
{"filename": "src/visualize.py", "chunked_list": ["import os\nimport json\nimport pickle\nimport numpy as np\nimport librosa\nimport librosa.display\n\nfrom pyannote.core import Annotation, Segment\nfrom pyannote.core import notebook\n", "from pyannote.core import notebook\n\nimport matplotlib\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot as plt\n\nimport pdb\n\ndef get_labels(json_path):\n    gt_diar = Annotation()\n\n    with open(json_path) as rf:\n        gt = json.load(rf)\n\n    annotations = gt[0]['annotations'][0]['result']\n    for annot in annotations:\n        start   = annot['value']['start']\n        end     = annot['value']['end']\n        spk_id  = annot['value']['labels'][0]\n        gt_diar[Segment(start, end)] = spk_id\n    return gt_diar", "def get_labels(json_path):\n    gt_diar = Annotation()\n\n    with open(json_path) as rf:\n        gt = json.load(rf)\n\n    annotations = gt[0]['annotations'][0]['result']\n    for annot in annotations:\n        start   = annot['value']['start']\n        end     = annot['value']['end']\n        spk_id  = annot['value']['labels'][0]\n        gt_diar[Segment(start, end)] = spk_id\n    return gt_diar", "\n\ndef get_wspx_result(result):\n    wspx_diar = Annotation()\n\n    # segments = result['word_segments']\n    segments = result['segments']\n    for seg in segments:\n        start, end  = seg['start'], seg['end']\n        spk_id      = seg['text'].split(':')[0].replace('[','').replace(']','')\n        wspx_diar[Segment(start, end)] = spk_id\n    return wspx_diar", "\n\ndef plot_waveform(y, ax, xlabel=None, ylabel=None, tight=False):\n    ax.plot(y)\n    if xlabel is not None:\n        ax.set_xlabel(xlabel)\n    if ylabel is not None:\n        ax.set_ylabel(ylabel)\n\n    if tight:\n        ax.autoscale(enable=True, axis='x', tight=True)\n    ax.xaxis.set_visible(False)", "\n\ndef plot_spectrogram(S_dB, sample_rate, ax):\n    librosa.display.specshow(S_dB, sr=sample_rate, x_axis='time', y_axis='hz', ax=ax)\n    ax.autoscale(enable=True, axis='x', tight=True)\n\n\ndef custom_plot(plot_func, data, ax, view_size=None, tight=False, time=True, legend=False):\n    if legend:\n        plot_func(data, ax=ax, time=time, legend=legend)\n    else:\n        plot_func(data, ax=ax, time=time)\n    \n    if view_size is not None:\n        ax.set_xlim([0, view_size])\n    elif tight:\n        ax.autoscale(enable=True, axis='x', tight=True)\n    ax.xaxis.set_visible(False)", "\n\ndef plot_annotations(annotations, save_fig_path, view_size=None, wav_path=None):\n    num_annotation = len(annotations)\n    if num_annotation == 0:\n        print(\"Empty annotation. There are no plot annotation\")\n        return\n\n    if wav_path is not None:\n        num_annotation += 2\n\n    fig, axes = plt.subplots(nrows=num_annotation, ncols=1, figsize=(30, num_annotation*4))\n    if num_annotation == 1:\n        axes = [axes]\n\n    if wav_path is not None:\n        view_size = plot_waveform_and_spectrogram(wav_path, axes[:2], view_size=view_size)\n        axes = axes[2:]\n\n    for _, (ax, annotation) in enumerate(zip(axes, annotations)):\n        custom_plot(notebook.plot_annotation, annotation, ax, view_size=view_size)\n    \n    fig.tight_layout()\n    fig.savefig(save_fig_path)\n    plt.close('all')", "\ndef load_wav_and_spectrogram(audio_file_path):\n    y, sr = librosa.load(audio_file_path)\n    t = ((y.shape[0]*100)//sr)/100.0\n    y = y[:int(sr*t)]\n    S_dB = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=4096, win_length=4096, hop_length=512)), ref=np.max)\n    return y, S_dB, sr, t\n\ndef plot_waveform_and_spectrogram(audio_file_path, axes, view_size=None):\n    y, S_dB, sr, t = load_wav_and_spectrogram(audio_file_path)\n\n    if view_size is None:\n        view_size = t\n\n    if view_size > 0:\n        notebook.crop = Segment(0, view_size)\n\n    plot_waveform(y, axes[0], xlabel='sample rate * time', ylabel='energy', tight=True)\n    plot_spectrogram(S_dB, sr, axes[1])\n\n    return view_size", "def plot_waveform_and_spectrogram(audio_file_path, axes, view_size=None):\n    y, S_dB, sr, t = load_wav_and_spectrogram(audio_file_path)\n\n    if view_size is None:\n        view_size = t\n\n    if view_size > 0:\n        notebook.crop = Segment(0, view_size)\n\n    plot_waveform(y, axes[0], xlabel='sample rate * time', ylabel='energy', tight=True)\n    plot_spectrogram(S_dB, sr, axes[1])\n\n    return view_size", "\ndef viewer(audio_file_path, result):\n    label_path  = '/mnt/whisper/whisper/The_Dark_Knight_gt.json'\n    gt_diar     = get_labels(label_path)\n\n    wspx_diar   = get_wspx_result(result)\n\n    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(30, 18))\n\n    plot_waveform_and_spectrogram(audio_file_path, axes[0:2])\n\n    custom_plot(notebook.plot_annotation,   wspx_diar,  axes[2], view_size=view_size)\n    custom_plot(notebook.plot_annotation,   gt_diar,    axes[3], view_size=view_size)\n    \n    fig.tight_layout()\n    fig.savefig('annotation.png')", "\n\nif __name__ == \"__main__\":\n    audio_path = '/mnt/whisper/whisper/The_Dark_Knight.wav'\n\n    os.makedirs('feat', exist_ok=True)\n    pkl_path = os.path.join('feat',os.path.basename(audio_path).replace('.wav','.pkl'))\n    assert os.path.exists(pkl_path), f'No Exists reusult file: {pkl_path}'\n\n    with open(pkl_path, 'rb') as handle:\n        result = pickle.load(handle)\n\n    viewer(audio_path, result)", ""]}
{"filename": "src/classify.py", "chunked_list": ["import os\nimport json\nimport math\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torchaudio\n", "import torchaudio\n\nfrom .utils import load_audio\nfrom .beats.BEATs import BEATs, BEATsConfig\n\n\nclass SoundClassifier:\n    def __init__(self, args):\n\n        device: str = args['device']\n        self.device = torch.device(device)\n        self.sr = args['sr']\n\n        # load audioset label infomation\n        ontology_file_path: str = args['sc_ontology_file_path']\n        labels_indices_csv_path: str = args['sc_labels_indices_csv']\n        child_dict, code2name = self.load_info_audioset(ontology_file_path, labels_indices_csv_path)\n        self.child_dict = child_dict\n        self.code2name = code2name\n\n        # load BEATs\n        model_ckpt_path: str = args['beats_model_ckpt_path']\n        assert(os.path.exists(model_ckpt_path)), print('No Exists BEATs model file: {}'.format(model_ckpt_path))\n\n        checkpoint = torch.load(model_ckpt_path)\n\n        cfg = BEATsConfig(checkpoint['cfg'])\n        self.model = BEATs(cfg)\n        self.model.load_state_dict(checkpoint['model'])\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.label_dict = checkpoint['label_dict']\n\n    def load_info_audioset(self, ontology_file_path, labels_indices_csv_path):\n        child_dict = self.get_child_dict(ontology_file_path)\n        labels = pd.read_csv(labels_indices_csv_path)\n\n        code2name = {\n            mid:name\n            for mid, name in zip(labels['mid'].to_list(), labels['display_name'].to_list())\n        }\n        return child_dict, code2name\n\n    @staticmethod\n    def get_child_dict(ontology_file_path):\n        \"\"\"\n            File: data/ontology.json\n            Desciption: AudioSet provide Each Class Information, such as child_ids, restrictions etc.,\n            Var:\n                'id': encoded class code (index)\n                'name': class name\n                'restrictions': Class type (None, abstract, blacklist)\n        \"\"\"\n\n        with open(ontology_file_path, 'r', encoding='utf8')as fp:\n            ontology = json.load(fp)\n\n        # make dictionary which contain each class information\n        child_dict = {}\n        for audio_class in ontology:\n            cur_id = audio_class['id']\n            cur_name = audio_class['name']\n            cur_child_ids = audio_class['child_ids']\n            # cur_restriction = audio_class['restrictions']\n            child_dict[cur_id] = (cur_child_ids, cur_name)\n        return child_dict\n\n    def predict(self, waveform, mask=None, chunk_time=1.0, step_ratio=0.1, return_all=False):\n        \"\"\"\n        Parameters\n        ----------\n        waveform: torch.FloatTensor (n_samples,)\n            Input Raw Waveform.\n        mask: torch.BoolTensor (n_samples,)\n            Input Mask\n        chunk_time: float\n            Chunk time\n        step_ratio: float\n            Step ratio\n        Returns\n        ----------\n        preds : torch.FloatTensor (n_classes,)\n            posterior of sound classification.\n        \"\"\"\n        \n        chunk_size = int(chunk_time * self.sr)\n        step_size = chunk_size * step_ratio\n\n        waveform = waveform.to(self.device).unsqueeze(0)\n        n_test_frames = waveform.shape[1]\n\n\n        pred_list = []\n        n_chunk = max(1, int(math.ceil((n_test_frames-chunk_size+step_size)/step_size)))\n        for chunk_id in range(n_chunk):\n\n            start = int(step_size * chunk_id)\n            end = min(start + chunk_size, n_test_frames)\n            duration = int(end-start)\n\n            chunk_waveform = torch.zeros(1,chunk_size).to(self.device)\n            chunk_waveform[:,:duration] = waveform[:,start:end]\n\n            chunk_mask = None\n            if mask is not None:\n                chunk_mask = torch.zeros(1, chunk_size, dtype=torch.bool).to(self.device)\n                chunk_mask[:,:duration] = mask[start:end]\n\n            with torch.no_grad():\n                pred = self.model.extract_features(chunk_waveform, padding_mask=chunk_mask)[0]\n                pred = pred.squeeze(0).detach().cpu()\n            pred_list.append(pred)\n\n        preds = torch.stack(pred_list)\n        # pred = preds.mean(0)\n        pred, _ = preds.max(0)\n\n        if return_all:\n            return pred, preds\n        else:\n            return pred\n\n    def pred_topk_with_label(self, waveform, mask=None, chunk_time=1.0, step_ratio=0.1, topk=5):\n        pred = self.predict(waveform, mask=mask, chunk_time=chunk_time, step_ratio=step_ratio)\n        probs, indices = pred.topk(k=topk)\n        \n        codes = [self.label_dict[idx.item()] for idx in indices]\n        names = [self.code2name[code] for code in codes]\n        results = []\n        for (name, code, prob) in zip(names, codes, probs):\n            results.append((code, name, prob.item()))\n        return results\n\n    def __call__(self, input_audio_path, seg_arr=None):\n\n        waveform = load_audio(input_audio_path, sr=self.sr)\n        waveform = torch.FloatTensor(waveform)\n        if seg_arr is not None:\n            pred_list = []\n            for start, end in zip(seg_arr[:,0], seg_arr[:,1]):\n                seg_waveform = waveform[start:end]\n                pred = self.predict(seg_waveform)\n\n                pred_list.append(pred.numpy())\n            pred = np.stack(pred_list)\n        else:\n            pred = self.predict(waveform)[None,:]\n        return pred", "\n\nif __name__ == \"__main__\":\n    import argparse\n    from utils import set_seeds\n    from whisper.audio import SAMPLE_RATE\n    \n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # basic config\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight.wav', required=False, help='path of test wav file')\n    # parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight_SE_FRCRN.wav', required=False, help='path of test wav file')\n    parser.add_argument('--test_lab_path', type=str, default='/mnt/FRCRN/The_Dark_Knight_SE_FRCRN.lab', required=False, help='path of test wav file')\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n\n    # sound classification config\n    parser.add_argument('--sc_ontology_file_path', type=str, default='data/BEATs/ontology.json', required=False, help='path of audioset ontology')\n    parser.add_argument('--sc_labels_indices_csv', type=str, default='data/BEATs/class_labels_indices.csv', required=False, help='csv file of containing audioset label indices')\n    parser.add_argument(\"--beats_model_ckpt_path\", type=str, default='models/sc_models/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt', help=\"pretrained BEATs model path\")\n\n    args = parser.parse_args().__dict__\n\n    set_seeds(args['seed'])    \n \n    test_wav_path: str = args.pop('test_wav_path')\n    assert(os.path.exists(test_wav_path)), \"No Exists File Name: {}\".format(test_wav_path)\n\n    test_lab_path: str = args.pop('test_lab_path')\n    seg_arr = np.atleast_2d((np.loadtxt(test_lab_path, usecols=(0, 1))*args['sr']).astype(int))\n\n    sc_manager = SoundClassifier(args)\n    results = sc_manager(test_wav_path, seg_arr)", ""]}
{"filename": "src/url_loader.py", "chunked_list": ["import os\nimport tqdm\nimport subprocess\nimport json\nfrom pytube import YouTube, Playlist\nfrom pytube.cli import on_progress\n# from moviepy.editor import AudioFileClip\n\n# from requests_html import HTMLSession\n# from bs4 import BeautifulSoup as bs", "# from requests_html import HTMLSession\n# from bs4 import BeautifulSoup as bs\n# import re\n\n\nclass YoutubeLoader:\n\n    def __init__(self, args):\n        \n        self.num_threads: int = args['num_threads']\n        self.sr: int = args['sr']\n\n        self.data_dir: str = args['yt_dir']\n        self.url_tag: str = \"watch?v=\"\n        self.url_pl_tag: str = \"playlist?list=\"\n\n#########################################################################\n### ref: https://www.javatpoint.com/how-to-extract-youtube-data-in-python\n#########################################################################\n    @staticmethod\n    def extract_video_informations(url):\n\n        # It will download HTML code\n        session = HTMLSession()\n        resp = session.get(url)\n        # execute Javascript  \n        resp.html.render(timeout=60)\n        # create beautiful soup object to parse HTML  \n        soup = bs(resp.html.html, \"html.parser\")\n        # initialize the result dictionary to store data  \n        result = {}\n\n        result[\"title\"] = soup.find(\"meta\", itemprop=\"name\")['content']\n        result[\"views\"] = soup.find(\"meta\",itemprop=\"interactionCount\")['content']\n        result[\"description\"] = soup.find(\"meta\",itemprop=\"description\")['content']\n    \n        result[\"date_published\"] = soup.find(\"meta\", itemprop=\"datePublished\")['content']\n\n        result[\"duration\"] = soup.find(\"span\", {\"class\": \"ytp-time-duration\"}).text\n        result[\"tags\"] = ', '.join([ meta.attrs.get(\"content\") for meta in soup.find_all(\"meta\", {\"property\": \"og:video:tag\"}) ])\n\n        data = re.search(r\"var ytInitialData = ({.*?});\", soup.prettify()).group(1)\n        data_json = json.loads(data)\n        videoPrimaryInfoRenderer = data_json['contents']['twoColumnWatchNextResults']['results']['results']['contents'][0]['videoPrimaryInfoRenderer']  \n        videoSecondaryInfoRenderer = data_json['contents']['twoColumnWatchNextResults']['results']['results']['contents'][1]['videoSecondaryInfoRenderer']  \n        # number of likes  \n        likes_label = videoPrimaryInfoRenderer['videoActions']['menuRenderer']['topLevelButtons'][0]['segmentedLikeDislikeButtonRenderer']['likeButton']['toggleButtonRenderer']['defaultText']['accessibility']['accessibilityData']['label'] # \"No likes\" or \"###,### likes\"  \n        # likes_label = videoPrimaryInfoRenderer['videoActions']['menuRenderer']['topLevelButtons'][0]['toggleButtonRenderer']['defaultText']['accessibility']['accessibilityData']['label'] # \"No likes\" or \"###,### likes\"  \n        likes_str = likes_label.split(' ')[0].replace(',','')  \n        result[\"likes\"] = '0' if likes_str == 'No' else likes_str  \n    \n        channel_tag = soup.find(\"meta\", itemprop=\"channelId\")['content']  \n        # channel name  \n        channel_name = soup.find(\"span\", itemprop=\"author\").next.next['content']  \n        \n        channel_url = f\"https://www.youtube.com/{channel_tag}\"  \n        # number of subscribers as str  \n        channel_subscribers = videoSecondaryInfoRenderer['owner']['videoOwnerRenderer']['subscriberCountText']['accessibility']['accessibilityData']['label']  \n    \n        result['channel'] = {'name': channel_name, 'url': channel_url, 'subscribers': channel_subscribers}\n\n        session.close()\n        return result\n\n    def save_video_info(self, url, yt_info):\n        \n        pos = url.find(self.url_tag)+len(self.url_tag)\n        yt_name = url[pos:]\n\n        yt_dir = os.path.join(self.data_dir, yt_name)\n        os.makedirs(yt_dir, exist_ok=True)\n\n        save_info_path = os.path.join(yt_dir, 'video_info.data')\n        with open(save_info_path, 'w') as wf:\n            for key, value in yt_info.items():\n                if isinstance(value, dict):\n                    wf.write(\"{}\\n\".format(key))\n                    for sub_key, sub_value in value.items():\n                        wf.write(\"\\t{}: {}\\n\".format(sub_key, sub_value))\n                elif isinstance(value, list):\n                    wf.write(\"{}\\n\".format(key))\n                    wf.write(\"\\t[\")\n                    for sub_id, sub_value in enumerate(value):\n                        wf.write(sub_value)\n                        if len(value) != sub_id-1:\n                            wf.write(\",\")\n                    wf.write(\"]\\n\")\n                else:\n                    wf.write(\"{}: {}\\n\".format(key, value))\n\n\n    def dl_high_res_mp4(self, yt, output_path, file_name):\n\n        stream_info = {}\n\n        stream = yt.streams.get_highest_resolution()\n        _ = stream.download(output_path=output_path, filename=file_name)\n\n        stream_info['itag'] = stream.itag\n        stream_info['mime_type'] = stream.mime_type\n        stream_info['resolution'] = stream.resolution\n        stream_info['video_fps'] = stream.fps\n        stream_info['video_codec'] = stream.video_codec\n        stream_info['audio_codec'] = stream.audio_codec\n        return stream_info\n\n    def dl_only_audio_mp4(self, yt, output_path, file_name):\n\n        stream_info = {}\n\n        stream = yt.streams.filter(only_audio=True, file_extension='mp4').order_by('abr').last()\n        _ = stream.download(output_path=output_path, filename=file_name)\n\n        stream_info['itag'] = stream.itag\n        stream_info['mime_type'] = stream.mime_type\n        stream_info['audio_codec'] = stream.audio_codec\n        stream_info['abr'] = stream.abr\n\n        return stream_info\n\n    def dl_captions(self, captions, yt_dir, yt_name):\n\n        caption_info = {}\n        if len(captions) == 0:\n            caption_info[\"code\"]=\"\"\n            return caption_info\n\n        yt_xml_dir = os.path.join(yt_dir, \"xml\")\n        os.makedirs(yt_xml_dir, exist_ok=True)\n\n        codes = []\n        for caption in captions:\n            code = caption.code\n            xml_path = os.path.join(yt_xml_dir,\"{}_{}.xml\".format(yt_name, code))\n            out_xml = caption.xml_captions\n            with open(xml_path, 'w') as wf:\n                wf.write(out_xml)\n            codes.append(code)\n        caption_info[\"code\"] = ','.join(codes)\n        return caption_info\n\n    def convert_mp4_to_wav(self, yt_dir, yt_name):\n\n        hr_video_path = os.path.join(yt_dir, \"mp4\", yt_name+'.mp4')\n        hq_video_path = os.path.join(yt_dir, \"mp4\", yt_name+'_audio.mp4')\n        assert(os.path.exists(hr_video_path) or os.path.exists(hq_video_path)), \"Both Youtube Not Downloaded: {}\".format(url)\n\n        video_file_path = hq_video_path if os.path.exists(hq_video_path) else hr_video_path\n\n        # save wav\n        audio_dir = os.path.join(yt_dir, \"wav\")\n        os.makedirs(audio_dir, exist_ok=True)\n\n        audio_file_path = os.path.join(audio_dir, yt_name+'.wav')\n\n        command = (\"ffmpeg -y -i %s -qscale:a 0 -ac 1 -vn -threads %d -ar %d %s -loglevel panic\" % \\\n            (video_file_path, self.num_threads, self.sr, audio_file_path))\n        out = subprocess.call(command, shell=True, stdout=None)\n        if out != 0:\n            raise ValueError(\"Error: Converting mp4 to wav: {}\".format(command))\n\n    def dl_youtube(self, url, dl_caption=True, save_spec_info=False, overwrite=False):\n\n        print(\"\\n>>Download Youtube URL: {}\".format(url))\n\n        pos = url.find(self.url_tag)+len(self.url_tag)\n        yt_name = url[pos:]\n\n        yt_dir = os.path.join(self.data_dir, yt_name)\n        if not overwrite and os.path.exists(yt_dir+\"/.done\"):\n            print(\">>Already Exists Youtube: {}\".format(url))\n            return yt_dir\n\n        yt_mp4_dir = os.path.join(yt_dir, \"mp4\")\n\n        yt = YouTube(url)\n\n        yt_info = {}\n        if save_spec_info:\n            yt_info = self.extract_video_informations(url)\n        \n        if not 'streamingData' in yt.vid_info.keys():\n            print(\">>Can't Download Youtube: {}\".format(url))\n            return None\n\n        stream = yt.streams.last()\n        yt_info['title'] = stream.title\n        yt_info['duration'] = stream._monostate.duration\n\n\n        yt_info['high_resolution_mp4_info'] = self.dl_high_res_mp4(yt, yt_mp4_dir, yt_name+'.mp4')\n        yt_info['only_audio_mp4_info'] = self.dl_only_audio_mp4(yt, yt_mp4_dir, yt_name+'_audio.mp4')\n        \n        if dl_caption:\n            yt_info['captions'] = self.dl_captions(yt.captions, yt_dir, yt_name)\n\n        self.save_video_info(url, yt_info)\n\n        self.convert_mp4_to_wav(yt_dir, yt_name)\n\n        only_audio_mp4_path = os.path.join(yt_mp4_dir, yt_name+'_audio.mp4')\n        if os.path.exists(only_audio_mp4_path):\n            os.remove(only_audio_mp4_path)\n\n        f = open(yt_dir+\"/.done\", \"w\")\n        f.close()\n        return yt_dir\n\n    def dl_playlist(self, pl_url, overwrite=False, max_n_pl_url=30):\n\n        p = Playlist(pl_url)\n        yt_dir_list = []\n\n        url_list = sorted(p.video_urls)\n        n_pl_url = len(url_list)\n        if n_pl_url == 0:\n            return []\n\n        if max_n_pl_url > 0:\n            n_pl_url = min(n_pl_url, max_n_pl_url)\n            url_list = url_list[:n_pl_url]\n\n        for url in tqdm.tqdm(url_list):\n            yt_dir = self.dl_youtube(url, overwrite=overwrite)\n            if yt_dir is not None:\n                yt_dir_list.append(yt_dir)\n        return yt_dir_list\n\n    def __call__(self, url, overwrite=False):\n        \n        print(\">>Process Youtube URL: {}\".format(url))\n\n        if self.url_tag in url:\n            yt_dir = self.dl_youtube(url, overwrite=overwrite)\n            if yt_dir is None:\n                yt_dir = []\n            else:\n                yt_dir = [yt_dir]\n\n        elif self.url_pl_tag in url:\n            yt_dir = self.dl_playlist(url, overwrite=overwrite)\n\n        else:\n            print(\">>Youtube url format Error, Skip URL: {}\".format(url))\n\n        return yt_dir", "\n\nif __name__ == '__main__':\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n    from whisper.audio import SAMPLE_RATE\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n    parser.add_argument('--yt_url', type=str, default='https://www.youtube.com/watch?v=jane6C4rIwc', required=False, help='path of test wav file')\n    parser.add_argument('--yt_dir', type=str, default='data/youtube', required=False, help='mp4 download directory')\n    parser.add_argument('--num_threads', type=int, default=0, required = False, help='number of threads')\n\n    args = parser.parse_args().__dict__\n    url = 'https://www.youtube.com/watch?v=jane6C4rIwc'\n    # url = 'https://www.youtube.com/playlist?list=PLrT4uvwaf6uw5ChxpBQnx0dA5fcmXvuB_'\n    args['yt_url'] = url\n\n    yt_url: str = args['yt_url']\n\n    yl = YoutubeLoader(args)\n    yt_dir_list = yl(yt_url)", ""]}
{"filename": "src/sqa.py", "chunked_list": ["#Copyright (c) Meta Platforms, Inc. and affiliates.\n#All rights reserved.\n\n#This source code is licensed under the license found in the\n#LICENSE file in the root directory of this source tree.\n\nimport os\nimport sys\nimport math\nimport glob", "import math\nimport glob\nimport tqdm\nimport pickle\nimport tarfile\nimport hashlib\nimport subprocess\nimport numpy as np\n\nfrom scipy import signal", "\nfrom scipy import signal\nimport librosa\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .noresqa.model import NORESQA\nfrom .utils import load_audio", "from .noresqa.model import NORESQA\nfrom .utils import load_audio\n\nDAPS_DATASET_URL = 'https://zenodo.org/record/4660670/files/daps.tar.gz?download=1'\nDAPS_N_CLEAN_WAV_NUM = 100\n\nimport pdb\n\n\ndef md5(fname):\n    hash_md5 = hashlib.md5()\n    with open(fname, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()", "\ndef md5(fname):\n    hash_md5 = hashlib.md5()\n    with open(fname, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\ndef check_daps_dataset(tar_file_path):\n    md5ck = md5(tar_file_path)\n    md5gt = '303c130b7ce2e02b59c7ca5cd595a89c'\n    if md5ck == md5gt:\n        print('Checksum successful {}.'.format(tar_file_path))\n    else:\n        raise Warning('Checksum failed {}.'.format(tar_file_path))", "def check_daps_dataset(tar_file_path):\n    md5ck = md5(tar_file_path)\n    md5gt = '303c130b7ce2e02b59c7ca5cd595a89c'\n    if md5ck == md5gt:\n        print('Checksum successful {}.'.format(tar_file_path))\n    else:\n        raise Warning('Checksum failed {}.'.format(tar_file_path))\n\ndef download_daps_dataset(out_dir):\n\n    out_file_path = os.path.join(out_dir, 'daps.tar.gz')\n    out = subprocess.call('wget {} -O {}'.format(DAPS_DATASET_URL, out_file_path), shell=True)\n    if out != 0:\n        raise ValueError('Download failed {}.'.format(DAPS_DATASET_URL))\n    \n    check_daps_dataset(out_file_path)", "def download_daps_dataset(out_dir):\n\n    out_file_path = os.path.join(out_dir, 'daps.tar.gz')\n    out = subprocess.call('wget {} -O {}'.format(DAPS_DATASET_URL, out_file_path), shell=True)\n    if out != 0:\n        raise ValueError('Download failed {}.'.format(DAPS_DATASET_URL))\n    \n    check_daps_dataset(out_file_path)\n\ndef extract_targz_file(out_dir, tar_file_path):\n    with tarfile.open(tar_file_path, \"r:gz\") as tar:\n        subdir_and_files = [\n            tarinfo for tarinfo in tar.getmembers()\n            if tarinfo.name.startswith(\"daps/clean/\")\n        ]\n        tar.extractall(out_dir, members=subdir_and_files)", "\ndef extract_targz_file(out_dir, tar_file_path):\n    with tarfile.open(tar_file_path, \"r:gz\") as tar:\n        subdir_and_files = [\n            tarinfo for tarinfo in tar.getmembers()\n            if tarinfo.name.startswith(\"daps/clean/\")\n        ]\n        tar.extractall(out_dir, members=subdir_and_files)\n\ndef prepare_daps_dataset(nmr_wav_dir):\n    os.makedirs(nmr_wav_dir, exist_ok=True)\n    if not os.path.exists(os.path.join(nmr_wav_dir, '.done')):\n        tar_file_path = os.path.join(nmr_wav_dir, 'daps.tar.gz')\n        if not os.path.exists(tar_file_path):\n            download_daps_dataset(nmr_wav_dir)\n        \n        extract_targz_file(nmr_wav_dir, tar_file_path)\n\n        f = open(os.path.join(nmr_wav_dir, '.done'), 'wb')\n        f.close()", "\ndef prepare_daps_dataset(nmr_wav_dir):\n    os.makedirs(nmr_wav_dir, exist_ok=True)\n    if not os.path.exists(os.path.join(nmr_wav_dir, '.done')):\n        tar_file_path = os.path.join(nmr_wav_dir, 'daps.tar.gz')\n        if not os.path.exists(tar_file_path):\n            download_daps_dataset(nmr_wav_dir)\n        \n        extract_targz_file(nmr_wav_dir, tar_file_path)\n\n        f = open(os.path.join(nmr_wav_dir, '.done'), 'wb')\n        f.close()", "\n\nclass SpeechQualityAssigner:\n\n    def __init__(self, args):\n\n        output_dim: int = 40\n        ssl_model_path: str = args['sqa_ssl_model_path']\n        device: str = args['device']\n        self.device = torch.device(device)\n        self.sr: int = args['sr']\n        self.model_ckpt_path: str = args['sqa_model_ckpt_path']\n        self.nmr_chunk_time: float = args['sqa_nmr_chunk_time']\n        self.nmr_step_size: int = args['sqa_nmr_step_size']\n\n        nmr_wav_dir: str = args['sqa_nmr_wav_dir']\n        nmr_feat_path: str = args['sqa_nmr_feat_path']\n        os.makedirs(os.path.dirname(nmr_feat_path), exist_ok=True)\n\n        # prepare daps dataset\n        prepare_daps_dataset(nmr_wav_dir)\n\n        self.sqa_model = NORESQA(output=output_dim, output2=output_dim, config_path=ssl_model_path)\n        self.load_parameter(self.model_ckpt_path)\n        self.sqa_model.to(self.device)\n        self.sqa_model.eval()\n\n        nmr_embs = self.load_nmr_emb(nmr_feat_path, nmr_wav_dir)\n        self.nmr_embs = nmr_embs.to(self.device)\n\n    def load_parameter(self, model_ckpt_path):\n\n        model_dict = self.sqa_model.state_dict()\n        params = torch.load(model_ckpt_path, map_location=\"cpu\")['state_dict']\n\n        pretrained_dict = {}\n        for name, param in params.items():\n            name = name.replace('module.', '') if 'module' in name else name\n            pretrained_dict[name] = param\n        model_dict.update(pretrained_dict)\n\n        self.sqa_model.load_state_dict(pretrained_dict)\n\n    def pred_noresqa_mos(self, test_feat, nmr_feat=None):\n\n        with torch.no_grad():\n            score = self.sqa_model(nmr_feat, test_feat).detach().cpu().numpy()[0]\n\n        return score\n\n    def extract_nmr_embbeddings(self, nmr_wav_dir):\n\n        nmr_wav_npy = os.path.join(nmr_wav_dir, 'clean_nmr_n100_{}ms.npy'.format(int(self.nmr_chunk_time*1000)))\n        if not os.path.exists(nmr_wav_npy):\n\n            print(\">>Prepare nmr waveforms\")\n            nmr_wav_list = sorted(glob.glob(nmr_wav_dir+\"/daps/clean/*.wav\"))\n            nmr_wav_list = [wav_path for wav_path in nmr_wav_list\n                if not wav_path.startswith('.')]\n            \n            assert(len(nmr_wav_list) == DAPS_N_CLEAN_WAV_NUM)\n\n            nmr_wav_arr = []\n            for nmr_wav_path in wav_nmr_list:\n                nmr_waveform = load_audio(nmr_wav_path, sr=self.sr, chunk_time=self.nmr_chunk_time)\n                # nmr_waveform shape: (wav_sr*max_nmr_wav_time,)\n                nmr_wav_arr.append(nmr_waveform)\n            nmr_wav_arr = np.stack(nmr_wav_arr)\n            np.save(nmr_wav_npy, nmr_wav_arr)\n        else:\n            print(\">>Load prepared clean nmr waveforms\")\n            nmr_wav_arr = np.load(nmr_wav_npy)\n\n        nmr_feat = torch.FloatTensor(nmr_wav_arr).to(self.device)\n\n        nmr_embs = []\n        for nmr_id in tqdm.tqdm(range(nmr_feat.shape[0])):\n            nmr_feat = nmr_feat[nmr_id:nmr_id+1]\n\n            with torch.no_grad():\n                nmr_emb = self.sqa_model.extract_embeddings(nmr_feat)\n\n            nmr_embs.append(nmr_emb.detach().cpu())\n\n        nmr_embs = torch.vstack(nmr_embs)\n        return nmr_embs\n\n    def load_nmr_emb(self, nmr_feat_path, nmr_wav_dir, overwrite=False):\n\n        if overwrite or not os.path.exists(nmr_feat_path):\n            nmr_embs = self.extract_nmr_embbeddings(nmr_wav_dir)\n            with open(nmr_feat_path, 'wb') as wf:\n                pickle.dump(nmr_embs, wf, protocol=pickle.HIGHEST_PROTOCOL)\n        else:\n            with open(nmr_feat_path, 'rb') as rf:\n                nmr_embs = pickle.load(rf)\n\n        return nmr_embs\n\n    def estimate_score(self, waveform, max_time=60):\n        \"\"\"\n        Parameters\n        ----------\n        waveform: torch.FloatTensor (n_samples,)\n            Input Raw Waveform.\n        Returns\n        ----------\n        mos_score : float\n            Detection score.\n        \"\"\"\n\n        waveform = waveform.to(self.device).unsqueeze(0)\n        with torch.no_grad():\n            nmr_embs = self.nmr_embs\n            \n            # we just use max_time if record has more than max_time\n            max_frames = max_time*self.sr\n            if max_frames < waveform.shape[1]:\n                waveform = waveform[:, :max_frames]\n\n            test_embs = self.sqa_model.extract_embeddings(waveform)\n            test_embs = test_embs.repeat(nmr_embs.shape[0], 1, 1)\n\n            n_test_frames = test_embs.shape[2]\n            n_nmr_frames = self.nmr_embs.shape[2]\n            nmr_step_size = self.nmr_step_size\n\n            mos_scores = []\n            n_chunk = max(1, int(math.ceil(n_test_frames-n_nmr_frames+nmr_step_size)/nmr_step_size))\n            for n in range(n_chunk):\n                start = nmr_step_size*n\n                end = min(start + n_nmr_frames, n_test_frames)\n                input_test_embs = test_embs[:,:,start:end]\n                results = self.sqa_model.estimate_score_bw_embs(nmr_embs[:,:,:end-start], input_test_embs)\n                mos_score = results['mos_score'].mean().detach().cpu().item()\n                mos_scores.append(mos_score)\n            final_mos_score = np.mean(mos_scores)\n        return final_mos_score\n\n    def __call__(self, input_audio_path, seg_arr=None, verbose=False):\n\n        waveform = load_audio(input_audio_path, sr=self.sr)\n        waveform = torch.FloatTensor(waveform)\n        if seg_arr is not None:\n            mos_score_list = []\n            for start, end in zip(seg_arr[:,0], seg_arr[:,1]):\n                seg_waveform = waveform[start:end]\n                mos_score = self.estimate_score(seg_waveform)\n\n                start_t, end_t = start/self.sr, end/self.sr\n                mos_score_list.append([start_t, end_t, mos_score])\n                if verbose:\n                    print(\"{:5.3f} - {:5.3f} ({:3.3f}) : {:.3f}\".format(start_t, end_t, end_t-start_t, mos_score))\n            mos_score = np.array(mos_score_list)\n        else:\n            mos_score = self.estimate_score(waveform)\n        return mos_score", "\n\nif __name__ == '__main__':\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n    from utils import set_seeds\n    from whisper.audio import SAMPLE_RATE\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # basic config\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--seed\", type=int, default=777, help=\"seed number\")\n    parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight.wav', required=False, help='path of test wav file')\n    # parser.add_argument('--test_wav_path', type=str, default='/mnt/FRCRN/The_Dark_Knight_SE_FRCRN.wav', required=False, help='path of test wav file')\n    parser.add_argument('--test_lab_path', type=str, default='/mnt/FRCRN/The_Dark_Knight_SE_FRCRN.lab', required=False, help='path of test wav file')\n    parser.add_argument('--sr', type=int, default=SAMPLE_RATE, required = False, help='sampling rate')\n    parser.add_argument(\"--exp_dir\", type=str, default='exps', help=\"path to experiments directory\")\n\n    # speech quality assessment config\n    parser.add_argument(\"--sqa_ssl_model_path\", type=str, default='models/sqa_models/wav2vec_small.pt', help=\"pretrained wav2vec base model path\")\n    parser.add_argument(\"--sqa_model_ckpt_path\", type=str, default='models/sqa_models/model_noresqa_mos.pth', help=\"pretrained NORESQA-MOS model path\")\n    parser.add_argument('--sqa_nmr_wav_dir', type=str, default='/mnt/dataset/daps', required = False, help='path of clean wav file')\n    parser.add_argument('--sqa_nmr_feat_path', type=str, default='sqa/noresqa/feat/daps_nmr_embs.pkl', required = False, help='path of nmr embedding pickle file')\n    parser.add_argument(\"--sqa_nmr_chunk_time\", type=float, default=3.0, help=\"nmr wav chunk time\")\n    parser.add_argument(\"--sqa_nmr_step_size\", type=int, default=75, help=\"embedding step size\")\n\n    args = parser.parse_args().__dict__\n    args['sqa_nmr_feat_path'] = os.path.join(args['exp_dir'], args['sqa_nmr_feat_path'])\n\n    set_seeds(args['seed'])\n\n    test_wav_path: str = args.pop('test_wav_path')\n    assert(os.path.exists(test_wav_path)), \"No Exists File Name: {}\".format(test_wav_path)\n\n    test_lab_path: str = args.pop('test_lab_path')\n    seg_arr = np.atleast_2d((np.loadtxt(test_lab_path, usecols=(0, 1))*args['sr']).astype(int))\n\n    sqa_manager = SpeechQualityAssigner(args)\n    score = sqa_manager(test_wav_path, seg_arr)", ""]}
{"filename": "src/torchaudio_squim/run_squim.py", "chunked_list": ["import os\nimport wave\nimport glob\nimport tqdm\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport librosa\n", "import librosa\n\nimport torch\nimport torchaudio\nfrom torchaudio.prototype.pipelines import SQUIM_OBJECTIVE, SQUIM_SUBJECTIVE\n\nfrom url_loader import YoutubeLoader\n\nimport pdb\n", "import pdb\n\nDAPS_N_CLEAN_WAV_NUM = 100\n\ndef set_seeds(seed=777, multi_gpu=False):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    if multi_gpu:\n        torch.cuda.manual_seed_all(seed) # if use multi-GPU", "\ndef get_wav_duration(audio_path):\n    with wave.open(audio_path) as handle:\n        t = handle.getnframes() / handle.getframerate()\n    return t\n\ndef load_audio(audio_file_path, sr=16000, chunk_time=0, mono=True):\n    t = get_wav_duration(audio_file_path)\n\n    # if chunk_time > t, we just use all frame (\"padding mode not provided\")\n    if chunk_time != 0 and t > chunk_time:\n        # randomly select start time given in Uniform(0, t-chunk_time)\n        n_frames = t*sr\n        n_chunk_frames = int(chunk_time*sr)\n        start = np.random.randint(0, max(0, n_frames-n_chunk_frames))\n        start_t = start/sr\n\n        waveform, _ = librosa.load(audio_file_path, offset=start_t, duration=chunk_time, sr=sr, mono=mono)\n    else:\n        waveform, _ = librosa.load(audio_file_path, sr=sr, mono=mono)\n\n    return waveform", "\n\n### you should run on recently version of torchaudio\n### which is not released yet, only exists in github repository\n\ndef run_squim_objective(root_wav_dir, result_csv_dir, device, wav_sr=16000, use_round=True):\n\n\n    model = SQUIM_OBJECTIVE.get_model()\n    model.to(device)\n    model.eval()\n\n    csv_list = sorted(glob.glob(result_csv_dir+\"/*.csv\"))\n    for csv_path in tqdm.tqdm(csv_list):\n        yt_name = os.path.splitext(os.path.basename(csv_path))[0]\n        wav_path = os.path.join(root_wav_dir, yt_name, 'wav', yt_name+'.wav')\n        assert(os.path.exists(wav_path)), \"No Exists Wav File: {}\".format(wav_path)\n\n        df = pd.read_csv(csv_path)\n\n        waveform, sr = torchaudio.load(wav_path)\n        waveform = waveform.to(device)\n\n        result_dict = {}\n        score_names = [\"STOI\", \"PESQ\", \"SI-SDR\"]\n        keys = []\n        for name in score_names:\n            key = \"SQUIM_{}\".format(name)\n            result_dict[key] = []\n            keys.append(key)\n\n        for start_t, end_t in zip(df['start'], df['end']):\n\n            seg_start = int(start_t * wav_sr)\n            seg_end = int(end_t * wav_sr)\n\n            scores_dict = {}\n            for key in keys:\n                scores_dict[key] = []\n            \n            seg_waveform = waveform[:,seg_start:seg_end]\n\n            window_time = 3.0\n            window_size = int(wav_sr * window_time)\n            stride_size = int(0.5 * window_size)\n\n            n_frames = int(seg_waveform.shape[1])\n            n_chunks = max(1, math.ceil((n_frames-window_size+stride_size)/stride_size))\n            for chunk_id in range(n_chunks):\n                start = int(stride_size * chunk_id)\n                end = min(start + window_size, n_frames)\n                chunk_waveform = seg_waveform[:, start:end]\n                with torch.no_grad():\n                    scores = model(chunk_waveform)\n                    for (key, score) in zip(keys, scores):\n                        score = score.detach().cpu().item()\n                        scores_dict[key].append(score)\n                \n            scores = []\n            for key in keys:\n                scores.append(np.mean(scores_dict[key]))\n\n            for (key, score) in zip(keys, scores):\n                result_dict[key].append(score)\n                print(\"{}: {:.3f}, \".format(key, score), end='')\n            print(\"\")\n        \n        for key in keys:\n            df[key] = np.array(result_dict[key])\n\n        if use_round:\n            df = df.round(3)\n        \n        df.to_csv(csv_path)\n    \n    print(\">>Done SQUIM OBJECTIVE ESTIMATION.\")", "\ndef run_squim_subjective(root_wav_dir, result_csv_dir, nmr_wav_arr, device, wav_sr=16000, use_round=True):\n\n    device = torch.device(\"cuda\")\n\n    model = SQUIM_SUBJECTIVE.get_model()\n    model.to(device)\n    model.eval()\n\n    csv_list = sorted(glob.glob(result_csv_dir+\"/*.csv\"))\n    for csv_path in tqdm.tqdm(csv_list):\n        yt_name = os.path.splitext(os.path.basename(csv_path))[0]\n        wav_path = os.path.join(root_wav_dir, yt_name, 'wav', yt_name+'.wav')\n        assert(os.path.exists(wav_path)), \"No Exists Wav File: {}\".format(wav_path)\n\n        df = pd.read_csv(csv_path)\n\n        waveform, sr = torchaudio.load(wav_path)\n        nmr_waveform = torch.FloatTensor(nmr_wav_arr).to(device)\n\n        mos_score_list = []\n        for seg_start_t, seg_end_t in zip(df['start'], df['end']):\n\n            seg_start = int(seg_start_t * wav_sr)\n            seg_end = int(seg_end_t * wav_sr)\n\n            seg_waveform = waveform[:,seg_start:seg_end]\n            \n            n_test_frames = seg_waveform.shape[1]\n            chunk_size = nmr_waveform.shape[1]\n            step_size = (chunk_size * 0.5)\n\n            current_id = 0\n            mos_scores = []\n            n_chunk = max(1, int(math.ceil((n_test_frames-chunk_size+step_size)/step_size)))\n            for chunk_id in range(n_chunk):\n                start = int(step_size * chunk_id)\n                end = min(start + chunk_size, n_test_frames)\n                duration = int(end-start)\n\n                chunk_test_waveform = seg_waveform[:, start:end]\n                chunk_test_waveform = chunk_test_waveform.repeat(nmr_wav_arr.shape[0], 1)\n\n                chunk_test_waveform = chunk_test_waveform.to(device)\n\n                chunk_nmr_waveform = nmr_waveform[:,:duration]\n                batch_size = 32\n                n_samples = chunk_test_waveform.shape[0]\n                n_chunk = int((n_samples-1)/batch_size) + 1\n                scores = []\n                with torch.no_grad():\n                    for chunk_id in range(n_chunk):\n                        b_start = chunk_id * batch_size\n                        b_end = min((chunk_id+1) * batch_size, n_samples)\n                        score = model(chunk_test_waveform[b_start:b_end], chunk_nmr_waveform[b_start:b_end])\n                        scores.append(score)\n                scores = torch.concat(scores)\n                score = scores.mean().detach().cpu().item()\n                mos_scores.append(score)\n            \n            final_score = np.mean(mos_scores)\n            print(\"SQUIM_MOS: {}\".format(final_score))\n\n            mos_score_list.append(final_score)\n        \n        df['SQUIM_MOS'] = np.array(mos_score_list)\n\n        if use_round:\n            df = df.round(3)\n        \n        df.to_csv(csv_path)", "\n\nif __name__ == '__main__':\n    \"\"\"\n    Get an argument parser.\n    \"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--sr', type=int, default=16000, required = False, help='sampling rate')\n    parser.add_argument('--yt_url', type=str, default='https://www.youtube.com/watch?v=jane6C4rIwc', required=False, help='path of test wav file')\n    parser.add_argument('--yt_dir', type=str, default='data/youtube', required=False, help='mp4 download directory')\n    parser.add_argument('--num_threads', type=int, default=0, required = False, help='number of threads')\n\n    args = parser.parse_args().__dict__\n    # url = 'https://www.youtube.com/watch?v=jane6C4rIwc'\n    # url = 'https://www.youtube.com/playlist?list=PLrT4uvwaf6uw5ChxpBQnx0dA5fcmXvuB_'\n    # \ub0e5\uc774\uc544\ube60\n    url = 'https://www.youtube.com/playlist?list=PL-28pfEORGTTyRFb-HLE-xlugbi8nDBb3'\n\n    args['yt_url'] = url\n\n    yt_url: str = args['yt_url']\n\n    yl = YoutubeLoader(args)\n    yt_dir_list = yl(yt_url)\n\n    wav_dir = '/mnt/labelmaker/labelmaker/data/youtube'\n    csv_dir = '/mnt/labelmaker/labelmaker/exps/csd/csv'\n\n    wav_sr = 16000\n    use_round = True\n    max_nmr_wav_time = 3.0\n    device = torch.device(\"cuda\")\n    \n    set_seeds()\n\n    nmr_dir = '/mnt/dataset/daps'\n    nmr_wav_npy = os.path.join(nmr_dir, 'clean_nmr_n100_{}ms.npy'.format(max_nmr_wav_time*1000))\n    if not os.path.exists(nmr_wav_npy):\n\n        print(\">>Prepare nmr waveforms\")\n        nmr_wav_list = sorted(glob.glob(nmr_dir+\"/daps/clean/*.wav\"))\n        nmr_wav_list = [wav_path for wav_path in nmr_wav_list\n            if not wav_path.startswith('.')]\n        \n        assert(len(nmr_wav_list) == DAPS_N_CLEAN_WAV_NUM), \"Error not match NMR wav file number: {} : 100\".foramt(len(nmr_wav_list))\n        \n        nmr_wav_arr = []\n        for nmr_wav_path in nmr_wav_list:\n\n            nmr_waveform = load_audio(nmr_wav_path, sr=wav_sr, chunk_time=max_nmr_wav_time)\n            # nmr_waveform shape: (wav_sr*max_nmr_wav_time,)\n            nmr_wav_arr.append(nmr_waveform)\n\n        nmr_wav_arr = np.stack(nmr_wav_arr)\n\n        np.save(nmr_wav_npy, nmr_wav_arr)\n    else:\n        print(\">>Load prepared clean nmr waveforms\")\n        nmr_wav_arr = np.load(nmr_wav_npy)\n\n    # run_squim_objective(wav_dir, csv_dir, device, wav_sr=wav_sr, use_round=use_round)\n\n    run_squim_subjective(wav_dir, csv_dir, nmr_wav_arr, device, wav_sr=wav_sr, use_round=use_round)", ""]}
{"filename": "src/FRCRN/ans_pipeline.py", "chunked_list": ["# Copyright (c) Alibaba, Inc. and its affiliates.\n\nimport io\nfrom typing import Any, Dict\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\n", "import torch\n\nfrom modelscope.fileio import File\nfrom modelscope.metainfo import Pipelines\nfrom modelscope.outputs import OutputKeys\nfrom modelscope.pipelines.base import Input, Pipeline\nfrom modelscope.pipelines.builder import PIPELINES\nfrom modelscope.utils.audio.audio_utils import audio_norm\nfrom modelscope.utils.constant import Tasks\n", "from modelscope.utils.constant import Tasks\n\n\n@PIPELINES.register_module(\n    Tasks.acoustic_noise_suppression,\n    module_name=Pipelines.speech_frcrn_ans_cirm_16k)\nclass ANSPipeline(Pipeline):\n    r\"\"\"ANS (Acoustic Noise Suppression) Inference Pipeline .\n\n    When invoke the class with pipeline.__call__(), it accept only one parameter:\n        inputs(str): the path of wav file\n    \"\"\"\n    SAMPLE_RATE = 16000\n\n    def __init__(self, model, **kwargs):\n        \"\"\"\n        use `model` and `preprocessor` to create a kws pipeline for prediction\n        Args:\n            model: model id on modelscope hub.\n        \"\"\"\n        super().__init__(model=model, **kwargs)\n        self.model.eval()\n\n    def preprocess(self, inputs: Input, **preprocess_params) -> Dict[str, Any]:\n        if isinstance(inputs, bytes):\n            data1, fs = sf.read(io.BytesIO(inputs))\n        elif isinstance(inputs, str):\n            file_bytes = File.read(inputs)\n            data1, fs = sf.read(io.BytesIO(file_bytes))\n        else:\n            raise TypeError(f'Unsupported type {type(inputs)}.')\n        if len(data1.shape) > 1:\n            data1 = data1[:, 0]\n        if fs != self.SAMPLE_RATE:\n            data1 = librosa.resample(\n                data1, orig_sr=fs, target_sr=self.SAMPLE_RATE)\n        data1 = audio_norm(data1)\n        data = data1.astype(np.float32)\n        inputs = np.reshape(data, [1, data.shape[0]])\n        return {'ndarray': inputs, 'nsamples': data.shape[0]}\n\n    def forward(self, inputs: Dict[str, Any],\n                **forward_params) -> Dict[str, Any]:\n        ndarray = inputs['ndarray']\n        if isinstance(ndarray, torch.Tensor):\n            ndarray = ndarray.cpu().numpy()\n        nsamples = inputs['nsamples']\n        decode_do_segement = False\n        # window = 16000\n        window = 16000\n        stride = int(window * 0.75)\n        print('inputs:{}'.format(ndarray.shape))\n        b, t = ndarray.shape  # size()\n        if t > window * 5:\n            decode_do_segement = True\n\n        if t < window:\n            ndarray = np.concatenate(\n                [ndarray, np.zeros((ndarray.shape[0], window - t))], 1)\n        elif t < window + stride:\n            padding = window + stride - t\n            print('padding: {}'.format(padding))\n            ndarray = np.concatenate(\n                [ndarray, np.zeros((ndarray.shape[0], padding))], 1)\n        else:\n            if (t - window) % stride != 0:\n                padding = t - (t - window) // stride * stride\n                print('padding: {}'.format(padding))\n                ndarray = np.concatenate(\n                    [ndarray, np.zeros((ndarray.shape[0], padding))], 1)\n        print('inputs after padding:{}'.format(ndarray.shape))\n        with torch.no_grad():\n            ndarray = torch.from_numpy(np.float32(ndarray)).to(self.device)\n\n            if b == 1:\n                mimic_batch = 50\n                tmp_ndarray = torch.zeros(mimic_batch, window).to(self.device)\n            else:\n                raise NotImplemented(\"SORRY!\")\n\n            b, t = ndarray.shape\n            if decode_do_segement:\n                outputs = np.zeros(t)\n                give_up_length = (window - stride) // 2\n\n                current_idx = 0\n                tmp_current_idx = 0\n\n                while current_idx + window <= t:\n                    print('current_idx: {}'.format(current_idx))\n                    mimic_count = 0\n                    while tmp_current_idx + window <= t and mimic_count < mimic_batch:\n                        tmp_ndarray[mimic_count] = ndarray[0, tmp_current_idx:tmp_current_idx+window]\n\n                        mimic_count += 1\n                        tmp_current_idx += stride\n                    \n                    tmp_input = dict(noisy=tmp_ndarray)\n                    tmp_output = self.model(tmp_input, )['wav_l2'].cpu().numpy()\n                    \n                    for mimic_pos in range(mimic_count):\n                        end_index = current_idx + window - give_up_length\n                        if current_idx == 0:\n                            outputs[current_idx:\n                                    end_index] = tmp_output[mimic_pos,:-give_up_length]\n                        else:\n                            outputs[current_idx\n                                    + give_up_length:end_index] = tmp_output[mimic_pos,\n                                        give_up_length:-give_up_length]\n                        current_idx += stride\n            else:\n                outputs = self.model(\n                    dict(noisy=ndarray))['wav_l2'][0].cpu().numpy()\n        outputs = (outputs[:nsamples] * 32768).astype(np.int16).tobytes()\n        return {OutputKeys.OUTPUT_PCM: outputs}\n\n    def postprocess(self, inputs: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n        if 'output_path' in kwargs.keys():\n            sf.write(\n                kwargs['output_path'],\n                np.frombuffer(inputs[OutputKeys.OUTPUT_PCM], dtype=np.int16),\n                self.SAMPLE_RATE)\n        return inputs", ""]}
{"filename": "src/noresqa/model.py", "chunked_list": ["#Copyright (c) Meta Platforms, Inc. and affiliates.\n#All rights reserved.\n\n#This source code is licensed under the license found in the\n#LICENSE file in the root directory of this source tree.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F", "import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import weight_norm\nimport numpy as np\nfrom librosa.filters import mel as librosa_mel_fn\nfrom torch.nn import Parameter\nfrom functools import wraps\nimport fairseq\nfrom fairseq import tasks\nimport pickle", "from fairseq import tasks\nimport pickle\n\nclass model_dimred(nn.Module):\n\n    def __init__(self, in_channel=64, conv1x1=16, reduce3x3=24, conv3x3=32, reduce5x5=16, conv5x5=8, pool_proj=8, pool=2):\n        super(model_dimred, self).__init__()\n\n        self.modules1 = nn.ModuleList()\n        self.modules1.append(nn.Conv2d(in_channel, conv1x1, 1, (1,1), 0))\n        self.modules1.append(nn.Conv2d(in_channel, reduce3x3, 1, 1, 0))\n        self.modules1.append(nn.Conv2d(reduce3x3, conv3x3, 3, (1,1), 1))\n        self.modules1.append(nn.Conv2d(in_channel, reduce5x5, 1, 1, 0))\n        self.modules1.append(nn.Conv2d(reduce5x5, conv5x5, 5, (1,1), 2))\n        self.modules1.append(nn.MaxPool2d((3,3),stride=(1,1),padding=(1,1)))\n        self.modules1.append(nn.Conv2d(in_channel, pool_proj, 1, 1, 0))\n        self.modules1.append(nn.MaxPool2d((1,pool)))\n\n    def forward(self, x):\n\n        a = F.relu(self.modules1[0](x))\n        b = F.relu(self.modules1[2]((F.relu(self.modules1[1](x)))))\n        c = F.relu(self.modules1[4]((F.relu(self.modules1[3](x)))))\n        d = F.relu(self.modules1[5](x))\n        d = F.relu(self.modules1[6](d))\n        x1 = torch.cat((a, b, c, d), axis=1)\n        x2 = F.relu(self.modules1[7](x1))\n        return x2", "\n\nclass BaseEncoder(nn.Module):\n    def __init__(self,dev=torch.device('cpu')):\n        super(BaseEncoder, self).__init__()\n        self.dev = dev\n\n        self.modelA = model_dimred(in_channel=2, pool=4)\n        self.modelB = model_dimred(in_channel=64, pool=4)\n        self.modelC = model_dimred(in_channel=64, pool=4)\n        self.modelD = model_dimred(in_channel=64, pool=2)\n\n\n    def forward(self,x):\n        x = (self.modelD(self.modelC(self.modelB(self.modelA(x)))))\n        return x", "\n\nclass which_clean(nn.Module):\n    def __init__(self):\n        super(which_clean, self).__init__()\n        n_layers = 2\n\n        self.encoder = nn.ModuleList()\n        self.ebatch = nn.ModuleList()\n        self.dp = nn.ModuleList()\n        filter_size = 5\n        dp_num = 0.50\n        self.encoder.append(nn.Conv1d(128,32,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(32))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(32,8,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(8))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(8,2,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(2))\n        self.dp.append(nn.Dropout(p=dp_num))\n\n\n    def forward(self,x):\n\n        for i in range(3):\n            x = self.encoder[i](x)\n            x = self.ebatch[i](x)\n            if i!=2:\n                x = F.leaky_relu(x,0.1)\n            x = self.dp[i](x)\n        return x", "\nclass how_snr(nn.Module):\n    def __init__(self, dim_emb=32, output=50):\n        super(how_snr, self).__init__()\n        n_layers = 2\n\n        self.encoder = nn.ModuleList()\n        self.ebatch = nn.ModuleList()\n        self.dp = nn.ModuleList()\n        filter_size = 5\n        dp_num = 0.50\n        self.encoder.append(nn.Conv1d(128,64,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(64))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(64,32,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(32))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(32,output,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(output))\n        self.dp.append(nn.Dropout(p=dp_num))\n\n    def forward(self,x):\n\n        for i in range(3):\n            x = self.encoder[i](x)\n            x = self.ebatch[i](x)\n            if i!=2:\n                x = F.leaky_relu(x,0.1)\n            x = self.dp[i](x)\n        return x", "\nclass how_snr_snr(nn.Module):\n    def __init__(self, dim_emb=32, output=50):\n        super(how_snr_snr, self).__init__()\n        n_layers = 2\n\n        self.encoder = nn.ModuleList()\n        self.ebatch = nn.ModuleList()\n        self.dp = nn.ModuleList()\n        filter_size = 5\n        dp_num = 0.50\n        self.encoder.append(nn.Conv1d(128,64,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(64))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(64,32,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(32))\n        self.dp.append(nn.Dropout(p=dp_num))\n        self.encoder.append(nn.Conv1d(32,output,filter_size,padding=filter_size//2))\n        self.ebatch.append(nn.BatchNorm1d(output))\n        self.dp.append(nn.Dropout(p=dp_num))\n\n    def forward(self,x):\n\n        for i in range(3):\n            x = self.encoder[i](x)\n            x = self.ebatch[i](x)\n            if i!=2:\n                x = F.leaky_relu(x,0.1)\n            x = self.dp[i](x)\n        return x", "\nclass NORESQA(nn.Module):\n\n    def __init__(self, dev=torch.device('cpu'), minit=1, output=20, output2=16, config_path='models/wav2vec_small.pt'):\n        super(NORESQA, self).__init__()\n\n        SSL_OUT_DIM=768\n        ssl_model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([config_path])\n\n        ssl_model = ssl_model[0]\n\n        ssl_model.remove_pretraining_modules()\n        self.main_model = MosPredictor(ssl_model, SSL_OUT_DIM)\n        self.linear_layer = nn.Linear(SSL_OUT_DIM, 32)\n\n        self.quantification = PoolAtt(d_input=64, output_size=5)\n        self.preference = PoolAtt(d_input=64, output_size=2)\n\n    def extract_embeddings(self, x):\n        emb = self.linear_layer(self.main_model(x)).permute(0,2,1)\n        return emb\n\n    def get_MOS_score(self, merged_emb, n_wins_tensor):\n        quantf = self.quantification(merged_emb, n_wins_tensor)\n        \n        att = F.softmax(quantf, dim=1)\n        B = torch.linspace(0, 4, steps=5).to(merged_emb.device)\n        mos_score = (att*B).sum(axis=1)\n        \n        return 5.0 - mos_score\n\n    def estimate_score_bw_embs(self, emb1, emb2, mask=None):\n        merged_emb = torch.cat((emb1, emb2), 1)\n\n        # n_wins = merged_emb.shape[2]\n        # B = [n_wins for n in range(merged_emb.shape[0])]\n        # n_wins_tensor = torch.from_numpy(np.asarray(B)).to(merged_emb.device)\n\n        merged_emb = merged_emb.permute(0,2,1)\n\n        results = {}\n        results['preference'] = self.preference(merged_emb, mask)\n        results['mos_score'] = self.get_MOS_score(merged_emb, mask)\n        return results\n\n    def forward(self, x1, x2):\n        emb1 = self.extract_embeddings(x1)\n        emb2 = self.extract_embeddings(x2)\n\n        results = self.estimate_score_bw_embs(emb1, emb2)\n        return results", "\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1 or classname.find('BatchNorm') != -1 or classname.find('Linear') != -1:\n        torch.nn.init.normal_(m.weight)\n        try:\n            torch.nn.init.constant_(m.bias, 0.01)\n        except:\n            pass", "\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=dilation, dilation=dilation))\n\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=dilation, dilation=dilation))\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n                                 self.conv2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)", "\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x1):\n\n        x1 = x1.reshape(x1.shape[0],-1,x1.shape[2])\n        x = self.network(x1)\n        return x", "\n\nclass MosPredictor(nn.Module):\n    def __init__(self, ssl_model, ssl_out_dim):\n        super(MosPredictor, self).__init__()\n        self.ssl_model = ssl_model\n        self.ssl_features = ssl_out_dim\n\n    def forward(self, wav):\n        wav = wav.squeeze(1)  ## [batches, audio_len]\n        res = self.ssl_model(wav, mask=False, features_only=True)\n        x = res['x']\n\n        return x", "\n\nclass PoolAtt(torch.nn.Module):\n    '''\n    PoolAtt: Attention-Pooling module.\n    '''\n    def __init__(self, d_input, output_size):\n        super().__init__()\n\n        self.linear1 = nn.Linear(d_input, 1)\n        self.linear2 = nn.Linear(d_input, output_size)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n            x: (B, T, C)\n            mask: (B, 1, T)\n                description: False if we don't use frames (e.g. short duration speech)\n        \"\"\"\n        att = self.linear1(x) # B X T X C\n\n        att = att.transpose(2,1) # B X 1 X T\n        if mask is not None:\n            att[~mask] = float(\"-Inf\")\n        # mask = torch.arange(att.shape[2])[None, :] < n_wins[:, None].to('cpu').to(torch.long)\n        # att[~mask.unsqueeze(1)] = float(\"-Inf\")\n        \n        att = F.softmax(att, dim=2)\n        x = torch.bmm(att, x)\n        x = x.squeeze(1)\n        x = self.linear2(x)\n\n        return x", ""]}
{"filename": "src/noresqa/__init__.py", "chunked_list": [""]}
{"filename": "src/beats/backbone.py", "chunked_list": ["# --------------------------------------------------------\n# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n# Github source: https://github.com/microsoft/unilm/tree/master/beats\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\nimport math", "\nimport math\nimport numpy as np\nfrom typing import Dict, Optional, Tuple\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.nn import LayerNorm, Parameter\nfrom .modules import (\n    GradMultiply,", "from .modules import (\n    GradMultiply,\n    SamePad,\n    get_activation_fn,\n    GLU_Linear,\n    quant_noise,\n)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        self.dropout = args.dropout\n        self.embedding_dim = args.encoder_embed_dim\n\n        self.pos_conv = nn.Conv1d(\n            self.embedding_dim,\n            self.embedding_dim,\n            kernel_size=args.conv_pos,\n            padding=args.conv_pos // 2,\n            groups=args.conv_pos_groups,\n        )\n        dropout = 0\n        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n        nn.init.constant_(self.pos_conv.bias, 0)\n\n        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n\n        if hasattr(args, \"relative_position_embedding\"):\n            self.relative_position_embedding = args.relative_position_embedding\n            self.num_buckets = args.num_buckets\n            self.max_distance = args.max_distance\n        else:\n            self.relative_position_embedding = False\n            self.num_buckets = 0\n            self.max_distance = 0\n\n        self.layers = nn.ModuleList(\n            [\n                TransformerSentenceEncoderLayer(\n                    embedding_dim=self.embedding_dim,\n                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n                    num_attention_heads=args.encoder_attention_heads,\n                    dropout=self.dropout,\n                    attention_dropout=args.attention_dropout,\n                    activation_dropout=args.activation_dropout,\n                    activation_fn=args.activation_fn,\n                    layer_norm_first=args.layer_norm_first,\n                    deep_norm=args.deep_norm,\n                    has_relative_attention_bias=self.relative_position_embedding,\n                    num_buckets=self.num_buckets,\n                    max_distance=self.max_distance,\n                    gru_rel_pos=args.gru_rel_pos,\n                    encoder_layers=args.encoder_layers,\n                )\n                for i in range(args.encoder_layers)\n            ]\n        )\n        if self.relative_position_embedding:\n            for i in range(1, args.encoder_layers):\n                del self.layers[i].self_attn.relative_attention_bias\n                self.layers[i].self_attn.relative_attention_bias = self.layers[0].self_attn.relative_attention_bias\n\n        self.layer_norm_first = args.layer_norm_first\n        self.layer_norm = LayerNorm(self.embedding_dim)\n        self.layerdrop = args.encoder_layerdrop\n\n        self.apply(init_bert_params)\n\n        if args.deep_norm:\n            deep_norm_beta = math.pow(8 * args.encoder_layers, -1 / 4)\n            for i in range(args.encoder_layers):\n                nn.init.xavier_normal_(self.layers[i].self_attn.k_proj.weight, gain=1)\n                nn.init.xavier_normal_(self.layers[i].self_attn.v_proj.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].self_attn.q_proj.weight, gain=1)\n                nn.init.xavier_normal_(self.layers[i].self_attn.out_proj.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].fc1.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].fc2.weight, gain=deep_norm_beta)\n\n        self.layer_wise_gradient_decay_ratio = getattr(args, \"layer_wise_gradient_decay_ratio\", 1)\n\n    def forward(self, x, padding_mask=None, layer=None):\n        x, layer_results = self.extract_features(x, padding_mask, layer)\n\n        if self.layer_norm_first and layer is None:\n            x = self.layer_norm(x)\n\n        return x, layer_results\n\n    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n\n        if padding_mask is not None:\n            x[padding_mask] = 0\n\n        x_conv = self.pos_conv(x.transpose(1, 2))\n        x_conv = x_conv.transpose(1, 2)\n        x += x_conv\n\n        if not self.layer_norm_first:\n            x = self.layer_norm(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        layer_results = []\n        z = None\n        if tgt_layer is not None:\n            layer_results.append((x, z))\n        r = None\n        pos_bias = None\n        for i, layer in enumerate(self.layers):\n            if self.layer_wise_gradient_decay_ratio != 1.0:\n                x = GradMultiply.apply(x, self.layer_wise_gradient_decay_ratio)\n            dropout_probability = np.random.random()\n            if not self.training or (dropout_probability > self.layerdrop):\n                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False, pos_bias=pos_bias)\n            if tgt_layer is not None:\n                layer_results.append((x, z))\n            if i == tgt_layer:\n                r = x\n                break\n\n        if r is not None:\n            x = r\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        return x, layer_results", "\nclass TransformerEncoder(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        self.dropout = args.dropout\n        self.embedding_dim = args.encoder_embed_dim\n\n        self.pos_conv = nn.Conv1d(\n            self.embedding_dim,\n            self.embedding_dim,\n            kernel_size=args.conv_pos,\n            padding=args.conv_pos // 2,\n            groups=args.conv_pos_groups,\n        )\n        dropout = 0\n        std = math.sqrt((4 * (1.0 - dropout)) / (args.conv_pos * self.embedding_dim))\n        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n        nn.init.constant_(self.pos_conv.bias, 0)\n\n        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(args.conv_pos), nn.GELU())\n\n        if hasattr(args, \"relative_position_embedding\"):\n            self.relative_position_embedding = args.relative_position_embedding\n            self.num_buckets = args.num_buckets\n            self.max_distance = args.max_distance\n        else:\n            self.relative_position_embedding = False\n            self.num_buckets = 0\n            self.max_distance = 0\n\n        self.layers = nn.ModuleList(\n            [\n                TransformerSentenceEncoderLayer(\n                    embedding_dim=self.embedding_dim,\n                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n                    num_attention_heads=args.encoder_attention_heads,\n                    dropout=self.dropout,\n                    attention_dropout=args.attention_dropout,\n                    activation_dropout=args.activation_dropout,\n                    activation_fn=args.activation_fn,\n                    layer_norm_first=args.layer_norm_first,\n                    deep_norm=args.deep_norm,\n                    has_relative_attention_bias=self.relative_position_embedding,\n                    num_buckets=self.num_buckets,\n                    max_distance=self.max_distance,\n                    gru_rel_pos=args.gru_rel_pos,\n                    encoder_layers=args.encoder_layers,\n                )\n                for i in range(args.encoder_layers)\n            ]\n        )\n        if self.relative_position_embedding:\n            for i in range(1, args.encoder_layers):\n                del self.layers[i].self_attn.relative_attention_bias\n                self.layers[i].self_attn.relative_attention_bias = self.layers[0].self_attn.relative_attention_bias\n\n        self.layer_norm_first = args.layer_norm_first\n        self.layer_norm = LayerNorm(self.embedding_dim)\n        self.layerdrop = args.encoder_layerdrop\n\n        self.apply(init_bert_params)\n\n        if args.deep_norm:\n            deep_norm_beta = math.pow(8 * args.encoder_layers, -1 / 4)\n            for i in range(args.encoder_layers):\n                nn.init.xavier_normal_(self.layers[i].self_attn.k_proj.weight, gain=1)\n                nn.init.xavier_normal_(self.layers[i].self_attn.v_proj.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].self_attn.q_proj.weight, gain=1)\n                nn.init.xavier_normal_(self.layers[i].self_attn.out_proj.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].fc1.weight, gain=deep_norm_beta)\n                nn.init.xavier_normal_(self.layers[i].fc2.weight, gain=deep_norm_beta)\n\n        self.layer_wise_gradient_decay_ratio = getattr(args, \"layer_wise_gradient_decay_ratio\", 1)\n\n    def forward(self, x, padding_mask=None, layer=None):\n        x, layer_results = self.extract_features(x, padding_mask, layer)\n\n        if self.layer_norm_first and layer is None:\n            x = self.layer_norm(x)\n\n        return x, layer_results\n\n    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n\n        if padding_mask is not None:\n            x[padding_mask] = 0\n\n        x_conv = self.pos_conv(x.transpose(1, 2))\n        x_conv = x_conv.transpose(1, 2)\n        x += x_conv\n\n        if not self.layer_norm_first:\n            x = self.layer_norm(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        layer_results = []\n        z = None\n        if tgt_layer is not None:\n            layer_results.append((x, z))\n        r = None\n        pos_bias = None\n        for i, layer in enumerate(self.layers):\n            if self.layer_wise_gradient_decay_ratio != 1.0:\n                x = GradMultiply.apply(x, self.layer_wise_gradient_decay_ratio)\n            dropout_probability = np.random.random()\n            if not self.training or (dropout_probability > self.layerdrop):\n                x, z, pos_bias = layer(x, self_attn_padding_mask=padding_mask, need_weights=False, pos_bias=pos_bias)\n            if tgt_layer is not None:\n                layer_results.append((x, z))\n            if i == tgt_layer:\n                r = x\n                break\n\n        if r is not None:\n            x = r\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        return x, layer_results", "\n\nclass TransformerSentenceEncoderLayer(nn.Module):\n    def __init__(\n            self,\n            embedding_dim: float = 768,\n            ffn_embedding_dim: float = 3072,\n            num_attention_heads: float = 8,\n            dropout: float = 0.1,\n            attention_dropout: float = 0.1,\n            activation_dropout: float = 0.1,\n            activation_fn: str = \"relu\",\n            layer_norm_first: bool = False,\n            deep_norm: bool = False,\n            has_relative_attention_bias: bool = False,\n            num_buckets: int = 0,\n            max_distance: int = 0,\n            rescale_init: bool = False,\n            gru_rel_pos: bool = False,\n            encoder_layers: int = 0,\n    ) -> None:\n\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.dropout = dropout\n        self.activation_dropout = activation_dropout\n\n        self.activation_name = activation_fn\n        self.activation_fn = get_activation_fn(activation_fn)\n        self.self_attn = MultiheadAttention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            self_attention=True,\n            has_relative_attention_bias=has_relative_attention_bias,\n            num_buckets=num_buckets,\n            max_distance=max_distance,\n            rescale_init=rescale_init,\n            gru_rel_pos=gru_rel_pos,\n        )\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(self.activation_dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.layer_norm_first = layer_norm_first\n\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n\n        if self.activation_name == \"glu\":\n            self.fc1 = GLU_Linear(self.embedding_dim, ffn_embedding_dim, \"swish\")\n        else:\n            self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n\n        self.final_layer_norm = LayerNorm(self.embedding_dim)\n\n        self.deep_norm = deep_norm\n        if self.deep_norm:\n            self.deep_norm_alpha = math.pow(2 * encoder_layers, 1 / 4)\n        else:\n            self.deep_norm_alpha = 1\n\n    def forward(\n            self,\n            x: torch.Tensor,\n            self_attn_mask: torch.Tensor = None,\n            self_attn_padding_mask: torch.Tensor = None,\n            need_weights: bool = False,\n            pos_bias=None\n    ):\n        residual = x\n\n        if self.layer_norm_first:\n            x = self.self_attn_layer_norm(x)\n            x, attn, pos_bias = self.self_attn(\n                query=x,\n                key=x,\n                value=x,\n                key_padding_mask=self_attn_padding_mask,\n                need_weights=False,\n                attn_mask=self_attn_mask,\n                position_bias=pos_bias\n            )\n            x = self.dropout1(x)\n            x = residual + x\n\n            residual = x\n            x = self.final_layer_norm(x)\n            if self.activation_name == \"glu\":\n                x = self.fc1(x)\n            else:\n                x = self.activation_fn(self.fc1(x))\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            x = self.dropout3(x)\n            x = residual + x\n        else:\n            x, attn, pos_bias = self.self_attn(\n                query=x,\n                key=x,\n                value=x,\n                key_padding_mask=self_attn_padding_mask,\n                need_weights=need_weights,\n                attn_mask=self_attn_mask,\n                position_bias=pos_bias\n            )\n\n            x = self.dropout1(x)\n            x = residual * self.deep_norm_alpha + x\n\n            x = self.self_attn_layer_norm(x)\n\n            residual = x\n            if self.activation_name == \"glu\":\n                x = self.fc1(x)\n            else:\n                x = self.activation_fn(self.fc1(x))\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            x = self.dropout3(x)\n            x = residual * self.deep_norm_alpha + x\n            x = self.final_layer_norm(x)\n\n        return x, attn, pos_bias", "\n\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n            self,\n            embed_dim,\n            num_heads,\n            kdim=None,\n            vdim=None,\n            dropout=0.0,\n            bias=True,\n            add_bias_kv=False,\n            add_zero_attn=False,\n            self_attention=False,\n            encoder_decoder_attention=False,\n            q_noise=0.0,\n            qn_block_size=8,\n            has_relative_attention_bias=False,\n            num_buckets=32,\n            max_distance=128,\n            gru_rel_pos=False,\n            rescale_init=False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout_module = nn.Dropout(dropout)\n\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n\n        self.head_dim = embed_dim // num_heads\n        self.q_head_dim = self.head_dim\n        self.k_head_dim = self.head_dim\n        assert (\n                self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        k_bias = True\n        if rescale_init:\n            k_bias = False\n\n        k_embed_dim = embed_dim\n        q_embed_dim = embed_dim\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, k_embed_dim, bias=k_bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, q_embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.gru_rel_pos = gru_rel_pos\n        if self.gru_rel_pos:\n            self.grep_linear = nn.Linear(self.q_head_dim, 8)\n            self.grep_a = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n        if self.has_relative_attention_bias:\n            nn.init.xavier_normal_(self.relative_attention_bias.weight)\n\n    def _relative_positions_bucket(self, relative_positions, bidirectional=True):\n        num_buckets = self.num_buckets\n        max_distance = self.max_distance\n        relative_buckets = 0\n\n        if bidirectional:\n            num_buckets = num_buckets // 2\n            relative_buckets += (relative_positions > 0).to(torch.long) * num_buckets\n            relative_positions = torch.abs(relative_positions)\n        else:\n            relative_positions = -torch.min(relative_positions, torch.zeros_like(relative_positions))\n\n        max_exact = num_buckets // 2\n        is_small = relative_positions < max_exact\n\n        relative_postion_if_large = max_exact + (\n                torch.log(relative_positions.float() / max_exact)\n                / math.log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n        ).to(torch.long)\n        relative_postion_if_large = torch.min(\n            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)\n        )\n\n        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length):\n        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_positions_bucket(\n            relative_position,\n            bidirectional=True\n        )\n        relative_position_bucket = relative_position_bucket.to(self.relative_attention_bias.weight.device)\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.permute([2, 0, 1])\n        return values\n\n    def forward(\n            self,\n            query,\n            key: Optional[Tensor],\n            value: Optional[Tensor],\n            key_padding_mask: Optional[Tensor] = None,\n            incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n            need_weights: bool = True,\n            static_kv: bool = False,\n            attn_mask: Optional[Tensor] = None,\n            before_softmax: bool = False,\n            need_head_weights: bool = False,\n            position_bias: Optional[Tensor] = None\n    ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        is_tpu = query.device.type == \"xla\"\n\n        tgt_len, bsz, embed_dim = query.size()\n        src_len = tgt_len\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        if key is not None:\n            src_len, key_bsz, _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert src_len, bsz == value.shape[:2]\n\n        if self.has_relative_attention_bias and position_bias is None:\n            position_bias = self.compute_bias(tgt_len, src_len)\n            position_bias = position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, src_len)\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n        alpha = 32\n        q *= 1 / alpha\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n                .view(tgt_len, bsz * self.num_heads, self.q_head_dim)\n                .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                    .view(-1, bsz * self.num_heads, self.k_head_dim)\n                    .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                    .view(-1, bsz * self.num_heads, self.head_dim)\n                    .transpose(0, 1)\n            )\n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n                src_len = k.size(1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        assert k.size(1) == src_len\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = (attn_weights - attn_weights.max(dim=-1, keepdim=True)[0]) * alpha\n        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            if not is_tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if before_softmax:\n            return attn_weights, v, position_bias\n\n        if position_bias is not None:\n            attn_mask_rel_pos = position_bias\n            if self.gru_rel_pos == 1:\n                query_layer = q.view(bsz, self.num_heads, tgt_len, self.q_head_dim) * alpha / self.scaling\n                _B, _H, _L, __ = query_layer.size()\n                gate_a, gate_b = torch.sigmoid(self.grep_linear(query_layer).view(\n                    _B, _H, _L, 2, 4).sum(-1, keepdim=False)).chunk(2, dim=-1)\n                gate_a_1 = gate_a * (gate_b * self.grep_a - 1.0) + 2.0\n                attn_mask_rel_pos = gate_a_1.view(bsz * self.num_heads, tgt_len, 1) * position_bias\n\n            attn_mask_rel_pos = attn_mask_rel_pos.view(attn_weights.size())\n\n            attn_weights = attn_weights + attn_mask_rel_pos\n\n        attn_weights_float = F.softmax(\n            attn_weights, dim=-1\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(\n                bsz, self.num_heads, tgt_len, src_len\n            ).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights, position_bias\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n            key_padding_mask: Optional[Tensor],\n            prev_key_padding_mask: Optional[Tensor],\n            batch_size: int,\n            src_len: int,\n            static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            if src_len > prev_key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - prev_key_padding_mask.size(1)),\n                    device=prev_key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask.float(), filler.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = prev_key_padding_mask.float()\n        elif key_padding_mask is not None:\n            if src_len > key_padding_mask.size(1):\n                filler = torch.zeros(\n                    (batch_size, src_len - key_padding_mask.size(1)),\n                    device=key_padding_mask.device,\n                )\n                new_key_padding_mask = torch.cat(\n                    [filler.float(), key_padding_mask.float()], dim=1\n                )\n            else:\n                new_key_padding_mask = key_padding_mask.float()\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    def _get_input_buffer(\n            self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n            self,\n            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n            buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights", "\n\ndef init_bert_params(module):\n    \"\"\"\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of\n           in_project_weight for MultiHeadAttention initialized using\n           the normal distribution (to be validated).\n    \"\"\"\n\n    def normal_(data):\n        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n        # so that the RNG is consistent with and without FSDP\n        data.copy_(\n            data.cpu().normal_(mean=0.0, std=0.02).to(data.device)\n        )\n\n    if isinstance(module, nn.Linear):\n        normal_(module.weight.data)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        normal_(module.weight.data)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    if isinstance(module, MultiheadAttention):\n        normal_(module.q_proj.weight.data)\n        normal_(module.k_proj.weight.data)\n        normal_(module.v_proj.weight.data)", ""]}
{"filename": "src/beats/__init__.py", "chunked_list": [""]}
{"filename": "src/beats/Tokenizers.py", "chunked_list": ["# --------------------------------------------------------\n# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n# Github source: https://github.com/microsoft/unilm/tree/master/beats\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\n", "\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torchaudio.compliance.kaldi as ta_kaldi\n\nfrom backbone import (\n    TransformerEncoder,\n)", "    TransformerEncoder,\n)\nfrom quantizer import (\n    NormEMAVectorQuantizer,\n)\n\nimport logging\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass TokenizersConfig:\n    def __init__(self, cfg=None):\n        self.input_patch_size: int = -1  # path size of patch embedding\n        self.embed_dim: int = 512  # patch embedding dimension\n        self.conv_bias: bool = False  # include bias in conv encoder\n\n        self.encoder_layers: int = 12  # num encoder layers in the transformer\n        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n        self.encoder_attention_heads: int = 12  # num encoder attention heads\n        self.activation_fn: str = \"gelu\"  # activation function to use\n\n        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n\n        # dropouts\n        self.dropout: float = 0.1  # dropout probability for the transformer\n        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n\n        # positional embeddings\n        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n\n        # relative position embedding\n        self.relative_position_embedding: bool = False  # apply relative position embedding\n        self.num_buckets: int = 320  # number of buckets for relative position embedding\n        self.max_distance: int = 1280  # maximum distance for relative position embedding\n        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n\n        # quantizer\n        self.quant_n: int = 1024 # codebook number in quantizer\n        self.quant_dim: int = 256    # codebook dimension in quantizer\n\n        if cfg is not None:\n            self.update(cfg)\n\n    def update(self, cfg: dict):\n        self.__dict__.update(cfg)", "\n\nclass Tokenizers(nn.Module):\n    def __init__(\n            self,\n            cfg: TokenizersConfig,\n    ) -> None:\n        super().__init__()\n        logger.info(f\"Tokenizers Config: {cfg.__dict__}\")\n\n        self.cfg = cfg\n\n        self.embed = cfg.embed_dim\n        self.post_extract_proj = (\n            nn.Linear(self.embed, cfg.encoder_embed_dim)\n            if self.embed != cfg.encoder_embed_dim\n            else None\n        )\n\n        self.input_patch_size = cfg.input_patch_size\n        self.patch_embedding = nn.Conv2d(1, self.embed, kernel_size=self.input_patch_size, stride=self.input_patch_size,\n                                         bias=cfg.conv_bias)\n\n        self.dropout_input = nn.Dropout(cfg.dropout_input)\n\n        assert not cfg.deep_norm or not cfg.layer_norm_first\n        self.encoder = TransformerEncoder(cfg)\n        self.layer_norm = LayerNorm(self.embed)\n\n        self.quantize = NormEMAVectorQuantizer(\n            n_embed=cfg.quant_n, embedding_dim=cfg.quant_dim, beta=1.0, kmeans_init=True, decay=0.99,\n        )\n        self.quant_n = cfg.quant_n\n        self.quantize_layer = nn.Sequential(\n            nn.Linear(cfg.encoder_embed_dim, cfg.encoder_embed_dim),\n            nn.Tanh(),\n            nn.Linear(cfg.encoder_embed_dim, cfg.quant_dim)  # for quantize\n        )\n\n    def forward_padding_mask(\n            self,\n            features: torch.Tensor,\n            padding_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        extra = padding_mask.size(1) % features.size(1)\n        if extra > 0:\n            padding_mask = padding_mask[:, :-extra]\n        padding_mask = padding_mask.view(\n            padding_mask.size(0), features.size(1), -1\n        )\n        padding_mask = padding_mask.all(-1)\n        return padding_mask\n\n    def preprocess(\n            self,\n            source: torch.Tensor,\n            fbank_mean: float = 15.41663,\n            fbank_std: float = 6.55582,\n    ) -> torch.Tensor:\n        fbanks = []\n        for waveform in source:\n            waveform = waveform.unsqueeze(0) * 2 ** 15\n            fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n            fbanks.append(fbank)\n        fbank = torch.stack(fbanks, dim=0)\n        fbank = (fbank - fbank_mean) / (2 * fbank_std)\n        return fbank\n\n    def extract_labels(\n            self,\n            source: torch.Tensor,\n            padding_mask: Optional[torch.Tensor] = None,\n            fbank_mean: float = 15.41663,\n            fbank_std: float = 6.55582,\n    ):\n        fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std)\n\n        if padding_mask is not None:\n            padding_mask = self.forward_padding_mask(fbank, padding_mask)\n\n        fbank = fbank.unsqueeze(1)\n        features = self.patch_embedding(fbank)\n        features = features.reshape(features.shape[0], features.shape[1], -1)\n        features = features.transpose(1, 2)\n        features = self.layer_norm(features)\n\n        if padding_mask is not None:\n            padding_mask = self.forward_padding_mask(features, padding_mask)\n\n        if self.post_extract_proj is not None:\n            features = self.post_extract_proj(features)\n\n        x = self.dropout_input(features)\n\n        x, layer_results = self.encoder(\n            x,\n            padding_mask=padding_mask,\n        )\n\n        quantize_input = self.quantize_layer(x)\n        quantize_feature, embed_loss, embed_ind = self.quantize(quantize_input)\n\n        return embed_ind", "\n"]}
{"filename": "src/beats/modules.py", "chunked_list": ["# --------------------------------------------------------\n# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n# Github source: https://github.com/microsoft/unilm/tree/master/beats\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\nimport math", "\nimport math\nimport warnings\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\n\n\nclass GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None", "class GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None", "\n\nclass SamePad(nn.Module):\n    def __init__(self, kernel_size, causal=False):\n        super().__init__()\n        if causal:\n            self.remove = kernel_size - 1\n        else:\n            self.remove = 1 if kernel_size % 2 == 0 else 0\n\n    def forward(self, x):\n        if self.remove > 0:\n            x = x[:, :, : -self.remove]\n        return x", "\n\nclass Swish(nn.Module):\n    def __init__(self):\n        super(Swish, self).__init__()\n        self.act = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        return x * self.act(x)\n", "\n\nclass GLU_Linear(nn.Module):\n    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n        super(GLU_Linear, self).__init__()\n\n        self.glu_type = glu_type\n        self.output_dim = output_dim\n\n        if glu_type == \"sigmoid\":\n            self.glu_act = torch.nn.Sigmoid()\n        elif glu_type == \"swish\":\n            self.glu_act = Swish()\n        elif glu_type == \"relu\":\n            self.glu_act = torch.nn.ReLU()\n        elif glu_type == \"gelu\":\n            self.glu_act = torch.nn.GELU()\n\n        if bias_in_glu:\n            self.linear = nn.Linear(input_dim, output_dim * 2, True)\n        else:\n            self.linear = nn.Linear(input_dim, output_dim * 2, False)\n\n    def forward(self, x):\n        # to be consistent with GLU_Linear, we assume the input always has the #channel (#dim) in the last dimension of the tensor, so need to switch the dimension first for 1D-Conv case\n        x = self.linear(x)\n\n        if self.glu_type == \"bilinear\":\n            x = (x[:, :, 0:self.output_dim] * x[:, :, self.output_dim:self.output_dim * 2])\n        else:\n            x = (x[:, :, 0:self.output_dim] * self.glu_act(x[:, :, self.output_dim:self.output_dim * 2]))\n\n        return x", "\n\ndef gelu_accurate(x):\n    if not hasattr(gelu_accurate, \"_a\"):\n        gelu_accurate._a = math.sqrt(2 / math.pi)\n    return (\n        0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\n    )\n\n\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)", "\n\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)\n\n\ndef get_activation_fn(activation: str):\n    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return gelu\n    elif activation == \"gelu_fast\":\n        warnings.warn(\n            \"--activation-fn=gelu_fast has been renamed to gelu_accurate\"\n        )\n        return gelu_accurate\n    elif activation == \"gelu_accurate\":\n        return gelu_accurate\n    elif activation == \"tanh\":\n        return torch.tanh\n    elif activation == \"linear\":\n        return lambda x: x\n    elif activation == \"glu\":\n        return lambda x: x\n    else:\n        raise RuntimeError(\"--activation-fn {} not supported\".format(activation))", "\n\ndef quant_noise(module, p, block_size):\n    \"\"\"\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in \"Training with Quantization Noise for Extreme Model Compression\"\n\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n\n    Remarks:\n        - Module weights must have the right sizes wrt the block size\n        - Only Linear, Embedding and Conv2d modules are supported for the moment\n        - For more detail on how to quantize by blocks with convolutional weights,\n          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n        - We implement the simplest form of noise here as stated in the paper\n          which consists in randomly dropping blocks\n    \"\"\"\n\n    # if no quantization noise, don't register hook\n    if p <= 0:\n        return module\n\n    # supported modules\n    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n\n    # test whether module.weight has the right sizes wrt block_size\n    is_conv = module.weight.ndim == 4\n\n    # 2D matrix\n    if not is_conv:\n        assert (\n            module.weight.size(1) % block_size == 0\n        ), \"Input features must be a multiple of block sizes\"\n\n    # 4D matrix\n    else:\n        # 1x1 convolutions\n        if module.kernel_size == (1, 1):\n            assert (\n                module.in_channels % block_size == 0\n            ), \"Input channels must be a multiple of block sizes\"\n        # regular convolutions\n        else:\n            k = module.kernel_size[0] * module.kernel_size[1]\n            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n\n    def _forward_pre_hook(mod, input):\n        # no noise for evaluation\n        if mod.training:\n            if not is_conv:\n                # gather weight and sizes\n                weight = mod.weight\n                in_features = weight.size(1)\n                out_features = weight.size(0)\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                mask = torch.zeros(\n                    in_features // block_size * out_features, device=weight.device\n                )\n                mask.bernoulli_(p)\n                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n\n            else:\n                # gather weight and sizes\n                weight = mod.weight\n                in_channels = mod.in_channels\n                out_channels = mod.out_channels\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                if mod.kernel_size == (1, 1):\n                    mask = torch.zeros(\n                        int(in_channels // block_size * out_channels),\n                        device=weight.device,\n                    )\n                    mask.bernoulli_(p)\n                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n                else:\n                    mask = torch.zeros(\n                        weight.size(0), weight.size(1), device=weight.device\n                    )\n                    mask.bernoulli_(p)\n                    mask = (\n                        mask.unsqueeze(2)\n                        .unsqueeze(3)\n                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n                    )\n\n            # scale weights and apply mask\n            mask = mask.to(\n                torch.bool\n            )  # x.bool() is not currently supported in TorchScript\n            s = 1 / (1 - p)\n            mod.weight.data = s * weight.masked_fill(mask, 0)\n\n    module.register_forward_pre_hook(_forward_pre_hook)\n    return module", "\n"]}
{"filename": "src/beats/quantizer.py", "chunked_list": ["# --------------------------------------------------------\n# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n# Github source: https://github.com/microsoft/unilm/tree/master/beats\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on VQGAN code bases\n# https://github.com/CompVis/taming-transformers\n# --------------------------------------------------------'\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as distributed\n\ntry:\n    from einops import rearrange, repeat\nexcept ImportError:\n    pass", "\n\ndef l2norm(t):\n    return F.normalize(t, p=2, dim=-1)\n\n\ndef ema_inplace(moving_avg, new, decay):\n    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n\n\ndef sample_vectors(samples, num):\n    num_samples, device = samples.shape[0], samples.device\n\n    if num_samples >= num:\n        indices = torch.randperm(num_samples, device=device)[:num]\n    else:\n        indices = torch.randint(0, num_samples, (num,), device=device)\n\n    return samples[indices]", "\n\ndef sample_vectors(samples, num):\n    num_samples, device = samples.shape[0], samples.device\n\n    if num_samples >= num:\n        indices = torch.randperm(num_samples, device=device)[:num]\n    else:\n        indices = torch.randint(0, num_samples, (num,), device=device)\n\n    return samples[indices]", "\n\ndef kmeans(samples, num_clusters, num_iters=10, use_cosine_sim=False):\n    dim, dtype, device = samples.shape[-1], samples.dtype, samples.device\n\n    means = sample_vectors(samples, num_clusters)\n\n    for _ in range(num_iters):\n        if use_cosine_sim:\n            dists = samples @ means.t()\n        else:\n            diffs = rearrange(samples, 'n d -> n () d') \\\n                    - rearrange(means, 'c d -> () c d')\n            dists = -(diffs ** 2).sum(dim=-1)\n\n        buckets = dists.max(dim=-1).indices\n        bins = torch.bincount(buckets, minlength=num_clusters)\n        zero_mask = bins == 0\n        bins_min_clamped = bins.masked_fill(zero_mask, 1)\n\n        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)\n        new_means.scatter_add_(0, repeat(buckets, 'n -> n d', d=dim), samples)\n        new_means = new_means / bins_min_clamped[..., None]\n\n        if use_cosine_sim:\n            new_means = l2norm(new_means)\n\n        means = torch.where(zero_mask[..., None], means, new_means)\n\n    return means, bins", "\n\nclass EmbeddingEMA(nn.Module):\n    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5, kmeans_init=True, codebook_init_path=''):\n        super().__init__()\n        self.num_tokens = num_tokens\n        self.codebook_dim = codebook_dim\n        self.decay = decay\n        self.eps = eps\n        if codebook_init_path == '':\n            if not kmeans_init:\n                weight = torch.randn(num_tokens, codebook_dim)\n                weight = l2norm(weight)\n            else:\n                weight = torch.zeros(num_tokens, codebook_dim)\n            self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n        else:\n            print(f\"load init codebook weight from {codebook_init_path}\")\n            codebook_ckpt_weight = torch.load(codebook_init_path, map_location='cpu')\n            weight = codebook_ckpt_weight.clone()\n            self.register_buffer('initted', torch.Tensor([True]))\n\n        self.weight = nn.Parameter(weight, requires_grad=False)\n        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad=False)\n        self.embed_avg = nn.Parameter(weight.clone(), requires_grad=False)\n        # self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n        self.update = True\n\n    @torch.jit.ignore\n    def init_embed_(self, data):\n        if self.initted:\n            return\n        print(\"Performing Kemans init for codebook\")\n        embed, cluster_size = kmeans(data, self.num_tokens, 10, use_cosine_sim=True)\n        self.weight.data.copy_(embed)\n        self.cluster_size.data.copy_(cluster_size)\n        self.initted.data.copy_(torch.Tensor([True]))\n\n    def forward(self, embed_id):\n        return F.embedding(embed_id, self.weight)\n\n    def cluster_size_ema_update(self, new_cluster_size):\n        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n\n    def embed_avg_ema_update(self, new_embed_avg):\n        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n\n    def weight_update(self, num_tokens):\n        n = self.cluster_size.sum()\n        smoothed_cluster_size = (\n                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n        )\n        # normalize embedding average with smoothed cluster size\n        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n        # embed_normalized = l2norm(self.embed_avg / smoothed_cluster_size.unsqueeze(1))\n        self.weight.data.copy_(embed_normalized)", "\n\ndef norm_ema_inplace(moving_avg, new, decay):\n    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n    moving_avg.data.copy_(l2norm(moving_avg.data))\n\n\nclass NormEMAVectorQuantizer(nn.Module):\n    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n                 statistic_code_usage=True, kmeans_init=False, codebook_init_path=''):\n        super().__init__()\n        self.codebook_dim = embedding_dim\n        self.num_tokens = n_embed\n        self.beta = beta\n        self.decay = decay\n\n        # learnable = True if orthogonal_reg_weight > 0 else False\n        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps, kmeans_init, codebook_init_path)\n\n        self.statistic_code_usage = statistic_code_usage\n        if statistic_code_usage:\n            self.register_buffer('cluster_size', torch.zeros(n_embed))\n        if distributed.is_available() and distributed.is_initialized():\n            print(\"ddp is enable, so use ddp_reduce to sync the statistic_code_usage for each gpu!\")\n            self.all_reduce_fn = distributed.all_reduce\n        else:\n            self.all_reduce_fn = nn.Identity()\n\n    def reset_cluster_size(self, device):\n        if self.statistic_code_usage:\n            self.register_buffer('cluster_size', torch.zeros(self.num_tokens))\n            self.cluster_size = self.cluster_size.to(device)\n\n    def forward(self, z):\n        # reshape z -> (batch, height, width, channel) and flatten\n        # z, 'b c h w -> b h w c'\n        # z = rearrange(z, 'b c h w -> b h w c')\n        # z = z.transpose(1, 2)\n        z = l2norm(z)\n        z_flattened = z.reshape(-1, self.codebook_dim)\n\n        self.embedding.init_embed_(z_flattened)\n\n        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight)  # 'n d -> d n'\n\n        encoding_indices = torch.argmin(d, dim=1)\n\n        z_q = self.embedding(encoding_indices).view(z.shape)\n\n        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n\n        if not self.training:\n            with torch.no_grad():\n                cluster_size = encodings.sum(0)\n                self.all_reduce_fn(cluster_size)\n                ema_inplace(self.cluster_size, cluster_size, self.decay)\n\n        if self.training and self.embedding.update:\n            # EMA cluster size\n\n            bins = encodings.sum(0)\n            self.all_reduce_fn(bins)\n\n            # self.embedding.cluster_size_ema_update(bins)\n            ema_inplace(self.cluster_size, bins, self.decay)\n\n            zero_mask = (bins == 0)\n            bins = bins.masked_fill(zero_mask, 1.)\n\n            embed_sum = z_flattened.t() @ encodings\n            self.all_reduce_fn(embed_sum)\n\n            embed_normalized = (embed_sum / bins.unsqueeze(0)).t()\n            embed_normalized = l2norm(embed_normalized)\n\n            embed_normalized = torch.where(zero_mask[..., None], self.embedding.weight,\n                                           embed_normalized)\n            norm_ema_inplace(self.embedding.weight, embed_normalized, self.decay)\n\n        # compute loss for embedding\n        loss = self.beta * F.mse_loss(z_q.detach(), z)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        # z_q, 'b h w c -> b c h w'\n        # z_q = rearrange(z_q, 'b h w c -> b c h w')\n        # z_q = z_q.transpose(1, 2)\n        return z_q, loss, encoding_indices", ""]}
{"filename": "src/beats/BEATs.py", "chunked_list": ["# --------------------------------------------------------\n# BEATs: Audio Pre-Training with Acoustic Tokenizers (https://arxiv.org/abs/2212.09058)\n# Github source: https://github.com/microsoft/unilm/tree/master/beats\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on fairseq code bases\n# https://github.com/pytorch/fairseq\n# --------------------------------------------------------\n\n", "\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torchaudio.compliance.kaldi as ta_kaldi\n\nfrom .backbone import (\n    TransformerEncoder,\n)", "    TransformerEncoder,\n)\n\nimport logging\nfrom typing import Optional\n\n# logger = logging.getLogger(__name__)\n\n\nclass BEATsConfig:\n    def __init__(self, cfg=None):\n        self.input_patch_size: int = -1  # path size of patch embedding\n        self.embed_dim: int = 512  # patch embedding dimension\n        self.conv_bias: bool = False  # include bias in conv encoder\n\n        self.encoder_layers: int = 12  # num encoder layers in the transformer\n        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n        self.encoder_attention_heads: int = 12  # num encoder attention heads\n        self.activation_fn: str = \"gelu\"  # activation function to use\n\n        self.layer_wise_gradient_decay_ratio: float = 1.0  # ratio for layer-wise gradient decay\n        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n\n        # dropouts\n        self.dropout: float = 0.1  # dropout probability for the transformer\n        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n\n        # positional embeddings\n        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n\n        # relative position embedding\n        self.relative_position_embedding: bool = False  # apply relative position embedding\n        self.num_buckets: int = 320  # number of buckets for relative position embedding\n        self.max_distance: int = 1280  # maximum distance for relative position embedding\n        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n\n        # label predictor\n        self.finetuned_model: bool = False  # whether the model is a fine-tuned model.\n        self.predictor_dropout: float = 0.1  # dropout probability for the predictor\n        self.predictor_class: int = 527  # target class number for the predictor\n\n        if cfg is not None:\n            self.update(cfg)\n\n    def update(self, cfg: dict):\n        self.__dict__.update(cfg)", "\nclass BEATsConfig:\n    def __init__(self, cfg=None):\n        self.input_patch_size: int = -1  # path size of patch embedding\n        self.embed_dim: int = 512  # patch embedding dimension\n        self.conv_bias: bool = False  # include bias in conv encoder\n\n        self.encoder_layers: int = 12  # num encoder layers in the transformer\n        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n        self.encoder_attention_heads: int = 12  # num encoder attention heads\n        self.activation_fn: str = \"gelu\"  # activation function to use\n\n        self.layer_wise_gradient_decay_ratio: float = 1.0  # ratio for layer-wise gradient decay\n        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n\n        # dropouts\n        self.dropout: float = 0.1  # dropout probability for the transformer\n        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n\n        # positional embeddings\n        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n\n        # relative position embedding\n        self.relative_position_embedding: bool = False  # apply relative position embedding\n        self.num_buckets: int = 320  # number of buckets for relative position embedding\n        self.max_distance: int = 1280  # maximum distance for relative position embedding\n        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n\n        # label predictor\n        self.finetuned_model: bool = False  # whether the model is a fine-tuned model.\n        self.predictor_dropout: float = 0.1  # dropout probability for the predictor\n        self.predictor_class: int = 527  # target class number for the predictor\n\n        if cfg is not None:\n            self.update(cfg)\n\n    def update(self, cfg: dict):\n        self.__dict__.update(cfg)", "\n\nclass BEATs(nn.Module):\n    def __init__(\n            self,\n            cfg: BEATsConfig,\n    ) -> None:\n        super().__init__()\n        # logger.info(f\"BEATs Config: {cfg.__dict__}\")\n\n        self.cfg = cfg\n\n        self.embed = cfg.embed_dim\n        self.post_extract_proj = (\n            nn.Linear(self.embed, cfg.encoder_embed_dim)\n            if self.embed != cfg.encoder_embed_dim\n            else None\n        )\n\n        self.input_patch_size = cfg.input_patch_size\n        self.patch_embedding = nn.Conv2d(1, self.embed, kernel_size=self.input_patch_size, stride=self.input_patch_size,\n                                         bias=cfg.conv_bias)\n\n        self.dropout_input = nn.Dropout(cfg.dropout_input)\n\n        assert not cfg.deep_norm or not cfg.layer_norm_first\n        self.encoder = TransformerEncoder(cfg)\n        self.layer_norm = LayerNorm(self.embed)\n\n        if cfg.finetuned_model:\n            self.predictor_dropout = nn.Dropout(cfg.predictor_dropout)\n            self.predictor = nn.Linear(cfg.encoder_embed_dim, cfg.predictor_class)\n        else:\n            self.predictor = None\n\n    def forward_padding_mask(\n            self,\n            features: torch.Tensor,\n            padding_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        extra = padding_mask.size(1) % features.size(1)\n        if extra > 0:\n            padding_mask = padding_mask[:, :-extra]\n        padding_mask = padding_mask.view(\n            padding_mask.size(0), features.size(1), -1\n        )\n        padding_mask = padding_mask.all(-1)\n        return padding_mask\n\n    def preprocess(\n            self,\n            source: torch.Tensor,\n            fbank_mean: float = 15.41663,\n            fbank_std: float = 6.55582,\n    ) -> torch.Tensor:\n        fbanks = []\n        for waveform in source:\n            waveform = waveform.unsqueeze(0) * 2 ** 15\n            fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n            fbanks.append(fbank)\n        fbank = torch.stack(fbanks, dim=0)\n        fbank = (fbank - fbank_mean) / (2 * fbank_std)\n        return fbank\n\n    def extract_features(\n            self,\n            source: torch.Tensor,\n            padding_mask: Optional[torch.Tensor] = None,\n            fbank_mean: float = 15.41663,\n            fbank_std: float = 6.55582,\n    ):\n        fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std)\n\n        if padding_mask is not None:\n            padding_mask = self.forward_padding_mask(fbank, padding_mask)\n\n        fbank = fbank.unsqueeze(1)\n        features = self.patch_embedding(fbank)\n        features = features.reshape(features.shape[0], features.shape[1], -1)\n        features = features.transpose(1, 2)\n        features = self.layer_norm(features)\n\n        if padding_mask is not None:\n            padding_mask = self.forward_padding_mask(features, padding_mask)\n\n        if self.post_extract_proj is not None:\n            features = self.post_extract_proj(features)\n\n        x = self.dropout_input(features)\n\n        x, layer_results = self.encoder(\n            x,\n            padding_mask=padding_mask,\n        )\n\n        if self.predictor is not None:\n            x = self.predictor_dropout(x)\n            logits = self.predictor(x)\n            \n            if padding_mask is not None and padding_mask.any():\n                logits[padding_mask] = 0\n                logits = logits.sum(dim=1)\n                logits = logits / (~padding_mask).sum(dim=1).unsqueeze(-1).expand_as(logits)\n            else:\n                logits = logits.mean(dim=1)\n\n            lprobs = torch.sigmoid(logits)\n\n            return lprobs, padding_mask\n        else:\n            return x, padding_mask", ""]}
{"filename": "src/custom_pyannote/speaker_verification.py", "chunked_list": ["# MIT License\n#\n# Copyright (c) 2021 CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport warnings\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    from backports.cached_property import cached_property", "try:\n    from functools import cached_property\nexcept ImportError:\n    from backports.cached_property import cached_property\n\nfrom typing import Text, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nimport torchaudio\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom pyannote.audio import Inference, Model, Pipeline\nfrom pyannote.audio.core.inference import BaseInference\nfrom pyannote.audio.core.io import AudioFile\nfrom pyannote.audio.core.model import CACHE_DIR\nfrom pyannote.audio.pipelines.utils import PipelineModel, get_model", "from pyannote.audio.core.model import CACHE_DIR\nfrom pyannote.audio.pipelines.utils import PipelineModel, get_model\n\nbackend = torchaudio.get_audio_backend()\ntry:\n    from speechbrain.pretrained import (\n        EncoderClassifier as SpeechBrain_EncoderClassifier,\n    )\n\n    SPEECHBRAIN_IS_AVAILABLE = True\nexcept ImportError:\n    SPEECHBRAIN_IS_AVAILABLE = False\nfinally:\n    torchaudio.set_audio_backend(backend)", "\ntry:\n    from nemo.collections.asr.models import (\n        EncDecSpeakerLabelModel as NeMo_EncDecSpeakerLabelModel,\n    )\n\n    NEMO_IS_AVAILABLE = True\nexcept ImportError:\n    NEMO_IS_AVAILABLE = False\n", "\n\ntry:\n    from .models.embeddings.MFA_Conformer import (\n        MFA_Conformer,\n    )\n\n    FBDP1202_IS_AVAILABLE = True\nexcept ImportError:\n    FBDP1202_IS_AVAILABLE = False", "\n\nclass NeMoPretrainedSpeakerEmbedding(BaseInference):\n    def __init__(\n        self,\n        embedding: Text = \"nvidia/speakerverification_en_titanet_large\",\n        device: torch.device = None,\n    ):\n\n        if not NEMO_IS_AVAILABLE:\n            raise ImportError(\n                f\"'NeMo' must be installed to use '{embedding}' embeddings. \"\n                \"Visit https://nvidia.github.io/NeMo/ for installation instructions.\"\n            )\n\n        super().__init__()\n        self.embedding = embedding\n        self.device = device or torch.device(\"cpu\")\n\n        self.model_ = NeMo_EncDecSpeakerLabelModel.from_pretrained(self.embedding)\n        self.model_.freeze()\n        self.model_.to(self.device)\n\n    def to(self, device: torch.device):\n        self.model_.to(device)\n        self.device = device\n        return self\n\n    @cached_property\n    def sample_rate(self) -> int:\n        return self.model_._cfg.train_ds.get(\"sample_rate\", 16000)\n\n    @cached_property\n    def dimension(self) -> int:\n\n        input_signal = torch.rand(1, self.sample_rate).to(self.device)\n        input_signal_length = torch.tensor([self.sample_rate]).to(self.device)\n        _, embeddings = self.model_(\n            input_signal=input_signal, input_signal_length=input_signal_length\n        )\n        _, dimension = embeddings.shape\n        return dimension\n\n    @cached_property\n    def metric(self) -> str:\n        return \"cosine\"\n\n    @cached_property\n    def min_num_samples(self) -> int:\n\n        lower, upper = 2, round(0.5 * self.sample_rate)\n        middle = (lower + upper) // 2\n        while lower + 1 < upper:\n            try:\n                input_signal = torch.rand(1, middle).to(self.device)\n                input_signal_length = torch.tensor([middle]).to(self.device)\n\n                _ = self.model_(\n                    input_signal=input_signal, input_signal_length=input_signal_length\n                )\n\n                upper = middle\n            except RuntimeError:\n                lower = middle\n\n            middle = (lower + upper) // 2\n\n        return upper\n\n    def __call__(\n        self, waveforms: torch.Tensor, masks: torch.Tensor = None\n    ) -> np.ndarray:\n        \"\"\"\n\n        Parameters\n        ----------\n        waveforms : (batch_size, num_channels, num_samples)\n            Only num_channels == 1 is supported.\n        masks : (batch_size, num_samples), optional\n\n        Returns\n        -------\n        embeddings : (batch_size, dimension)\n\n        \"\"\"\n\n        batch_size, num_channels, num_samples = waveforms.shape\n        assert num_channels == 1\n\n        waveforms = waveforms.squeeze(dim=1)\n\n        if masks is None:\n            signals = waveforms.squeeze(dim=1)\n            wav_lens = signals.shape[1] * torch.ones(batch_size)\n\n        else:\n\n            batch_size_masks, _ = masks.shape\n            assert batch_size == batch_size_masks\n\n            # TODO: speed up the creation of \"signals\"\n            # preliminary profiling experiments show\n            # that it accounts for 15% of __call__\n            # (the remaining 85% being the actual forward pass)\n\n            imasks = F.interpolate(\n                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n            ).squeeze(dim=1)\n\n            imasks = imasks > 0.5\n\n            signals = pad_sequence(\n                [waveform[imask] for waveform, imask in zip(waveforms, imasks)],\n                batch_first=True,\n            )\n\n            wav_lens = imasks.sum(dim=1)\n\n        max_len = wav_lens.max()\n\n        # corner case: every signal is too short\n        if max_len < self.min_num_samples:\n            return np.NAN * np.zeros((batch_size, self.dimension))\n\n        too_short = wav_lens < self.min_num_samples\n        wav_lens[too_short] = max_len\n\n        _, embeddings = self.model_(\n            input_signal=waveforms.to(self.device),\n            input_signal_length=wav_lens.to(self.device),\n        )\n\n        embeddings = embeddings.cpu().numpy()\n        embeddings[too_short.cpu().numpy()] = np.NAN\n\n        return embeddings", "\n\nclass SpeechBrainPretrainedSpeakerEmbedding(BaseInference):\n    \"\"\"Pretrained SpeechBrain speaker embedding\n\n    Parameters\n    ----------\n    embedding : str\n        Name of SpeechBrain model\n    device : torch.device, optional\n        Device\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> get_embedding = SpeechBrainPretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n    >>> assert waveforms.ndim == 3\n    >>> batch_size, num_channels, num_samples = waveforms.shape\n    >>> assert num_channels == 1\n    >>> embeddings = get_embedding(waveforms)\n    >>> assert embeddings.ndim == 2\n    >>> assert embeddings.shape[0] == batch_size\n\n    >>> assert binary_masks.ndim == 1\n    >>> assert binary_masks.shape[0] == batch_size\n    >>> embeddings = get_embedding(waveforms, masks=binary_masks)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: Text = \"speechbrain/spkrec-ecapa-voxceleb\",\n        device: torch.device = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n\n        if not SPEECHBRAIN_IS_AVAILABLE:\n            raise ImportError(\n                f\"'speechbrain' must be installed to use '{embedding}' embeddings. \"\n                \"Visit https://speechbrain.github.io for installation instructions.\"\n            )\n\n        super().__init__()\n        self.embedding = embedding\n        self.device = device or torch.device(\"cpu\")\n        self.use_auth_token = use_auth_token\n\n        self.classifier_ = SpeechBrain_EncoderClassifier.from_hparams(\n            source=self.embedding,\n            savedir=f\"{CACHE_DIR}/speechbrain\",\n            run_opts={\"device\": self.device},\n            use_auth_token=self.use_auth_token,\n        )\n\n    def to(self, device: torch.device):\n        self.classifier_ = SpeechBrain_EncoderClassifier.from_hparams(\n            source=self.embedding,\n            savedir=f\"{CACHE_DIR}/speechbrain\",\n            run_opts={\"device\": device},\n            use_auth_token=self.use_auth_token,\n        )\n        self.device = device\n        return self\n\n    @cached_property\n    def sample_rate(self) -> int:\n        return self.classifier_.audio_normalizer.sample_rate\n\n    @cached_property\n    def dimension(self) -> int:\n        dummy_waveforms = torch.rand(1, 16000).to(self.device)\n        *_, dimension = self.classifier_.encode_batch(dummy_waveforms).shape\n        return dimension\n\n    @cached_property\n    def metric(self) -> str:\n        return \"cosine\"\n\n    @cached_property\n    def min_num_samples(self) -> int:\n\n        lower, upper = 2, round(0.5 * self.sample_rate)\n        middle = (lower + upper) // 2\n        while lower + 1 < upper:\n            try:\n                _ = self.classifier_.encode_batch(\n                    torch.randn(1, middle).to(self.device)\n                )\n                upper = middle\n            except RuntimeError:\n                lower = middle\n\n            middle = (lower + upper) // 2\n\n        return upper\n\n    def __call__(\n        self, waveforms: torch.Tensor, masks: torch.Tensor = None\n    ) -> np.ndarray:\n        \"\"\"\n\n        Parameters\n        ----------\n        waveforms : (batch_size, num_channels, num_samples)\n            Only num_channels == 1 is supported.\n        masks : (batch_size, num_samples), optional\n\n        Returns\n        -------\n        embeddings : (batch_size, dimension)\n\n        \"\"\"\n\n        batch_size, num_channels, num_samples = waveforms.shape\n        assert num_channels == 1\n\n        waveforms = waveforms.squeeze(dim=1)\n\n        if masks is None:\n            signals = waveforms.squeeze(dim=1)\n            wav_lens = signals.shape[1] * torch.ones(batch_size)\n\n        else:\n\n            batch_size_masks, _ = masks.shape\n            assert batch_size == batch_size_masks\n\n            # TODO: speed up the creation of \"signals\"\n            # preliminary profiling experiments show\n            # that it accounts for 15% of __call__\n            # (the remaining 85% being the actual forward pass)\n\n            imasks = F.interpolate(\n                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n            ).squeeze(dim=1)\n\n            imasks = imasks > 0.5\n\n            signals = pad_sequence(\n                [\n                    waveform[imask].contiguous()\n                    for waveform, imask in zip(waveforms, imasks)\n                ],\n                batch_first=True,\n            )\n\n            wav_lens = imasks.sum(dim=1)\n\n        max_len = wav_lens.max()\n\n        # corner case: every signal is too short\n        if max_len < self.min_num_samples:\n            return np.NAN * np.zeros((batch_size, self.dimension))\n\n        too_short = wav_lens < self.min_num_samples\n        wav_lens = wav_lens / max_len\n        wav_lens[too_short] = 1.0\n\n        with torch.no_grad():\n            embeddings = (\n                self.classifier_.encode_batch(signals, wav_lens=wav_lens)\n                .squeeze(dim=1)\n                .cpu()\n                .numpy()\n            )\n\n        embeddings[too_short.cpu().numpy()] = np.NAN\n\n        return embeddings", "\n\nclass MFAConformerPretrainedSpeakerEmbedding(BaseInference):\n    \"\"\"Pretrained SpeechBrain speaker embedding\n\n    Parameters\n    ----------\n    embedding : str\n        Name of SpeechBrain model\n    device : torch.device, optional\n        Device\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> get_embedding = MFAConformerPretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n    >>> assert waveforms.ndim == 3\n    >>> batch_size, num_channels, num_samples = waveforms.shape\n    >>> assert num_channels == 1\n    >>> embeddings = get_embedding(waveforms)\n    >>> assert embeddings.ndim == 2\n    >>> assert embeddings.shape[0] == batch_size\n\n    >>> assert binary_masks.ndim == 1\n    >>> assert binary_masks.shape[0] == batch_size\n    >>> embeddings = get_embedding(waveforms, masks=binary_masks)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: Text = \"fbdp1202/mfa-conformer\",\n        device: torch.device = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n\n        if not FBDP1202_IS_AVAILABLE:\n            raise ImportError(\n                f\"fbdp1202/MFA-Conformer\"\n            )\n\n        super().__init__()\n        self.embedding = embedding\n        self.device = device or torch.device(\"cpu\")\n        self.use_auth_token = use_auth_token\n\n        self.classifier_ = MFA_Conformer()\n        model_checkpoint_path = '/mnt/labelmaker/labelmaker/models/embedding/nnet/model.pth'\n\n        self.loadParameters(model_checkpoint_path, device)\n        self.classifier_.eval()\n\n    def loadParameters(self, path, device):\n        self_state = self.classifier_.state_dict()\n        loaded_state = torch.load(path, map_location=device)\n        for name, param in loaded_state.items():\n            if '__L__.' in name:\n                continue\n\n            origname = name\n            if name not in self_state:\n                name = name.replace(\"module.\", \"\")\n                name = name.replace(\"__S__.\", \"\")\n\n                if name not in self_state:\n                    print(\"{} is not in the model.\".format(origname))\n                    continue\n\n            if self_state[name].size() != loaded_state[origname].size():\n                print(\"Wrong parameter length: {}, model: {}, loaded: {}\".format(origname, self_state[name].size(), loaded_state[origname].size()))\n                continue\n\n            self_state[name].copy_(param)\n\n    def to(self, device: torch.device):\n        self.classifier_ = MFA_Conformer()\n        model_checkpoint_path = '/mnt/labelmaker/labelmaker/models/embedding/nnet/model.pth'\n        self.loadParameters(model_checkpoint_path, device)\n        self.classifier_.to(device)\n        self.classifier_.eval()\n\n        self.device = device\n        return self\n\n    @cached_property\n    def sample_rate(self) -> int:\n        return 16000\n\n    @cached_property\n    def dimension(self) -> int:\n        dummy_waveforms = torch.rand(1, 16000).to(self.device)\n        *_, dimension = self.classifier_(dummy_waveforms).shape\n        return dimension\n\n    @cached_property\n    def metric(self) -> str:\n        return \"cosine\"\n\n    @cached_property\n    def min_num_samples(self) -> int:\n\n        lower, upper = 2, round(0.5 * self.sample_rate)\n        middle = (lower + upper) // 2\n        while lower + 1 < upper:\n            try:\n                _ = self.classifier_(\n                    torch.randn(1, middle).to(self.device)\n                )\n                upper = middle\n            except RuntimeError:\n                lower = middle\n\n            middle = (lower + upper) // 2\n\n        return upper\n\n    def __call__(\n        self, waveforms: torch.Tensor, masks: torch.Tensor = None\n    ) -> np.ndarray:\n        \"\"\"\n\n        Parameters\n        ----------\n        waveforms : (batch_size, num_channels, num_samples)\n            Only num_channels == 1 is supported.\n        masks : (batch_size, num_samples), optional\n\n        Returns\n        -------\n        embeddings : (batch_size, dimension)\n\n        \"\"\"\n\n        batch_size, num_channels, num_samples = waveforms.shape\n        assert num_channels == 1\n\n        waveforms = waveforms.squeeze(dim=1)\n\n        if masks is None:\n            signals = waveforms.squeeze(dim=1)\n            wav_lens = signals.shape[1] * torch.ones(batch_size)\n\n        else:\n\n            batch_size_masks, _ = masks.shape\n            assert batch_size == batch_size_masks\n\n            # TODO: speed up the creation of \"signals\"\n            # preliminary profiling experiments show\n            # that it accounts for 15% of __call__\n            # (the remaining 85% being the actual forward pass)\n\n            imasks = F.interpolate(\n                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n            ).squeeze(dim=1)\n\n            imasks = imasks > 0.5\n\n            signals = pad_sequence(\n                [\n                    waveform[imask].contiguous()\n                    for waveform, imask in zip(waveforms, imasks)\n                ],\n                batch_first=True,\n            )\n\n            wav_lens = imasks.sum(dim=1)\n\n        max_len = wav_lens.max()\n\n        # corner case: every signal is too short\n        if max_len < self.min_num_samples:\n            return np.NAN * np.zeros((batch_size, self.dimension))\n\n        too_short = wav_lens < self.min_num_samples\n        wav_lens = wav_lens / max_len\n        wav_lens[too_short] = 1.0\n\n        with torch.no_grad():\n            signals = signals.to(self.device)\n            wav_lens = wav_lens.to(self.device)\n            # import pdb\n            # pdb.set_trace()\n            embeddings = (\n                self.classifier_(signals, wav_lens=wav_lens)\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n        embeddings[too_short.cpu().numpy()] = np.NAN\n\n        return embeddings", "\nclass PyannoteAudioPretrainedSpeakerEmbedding(BaseInference):\n    \"\"\"Pretrained pyannote.audio speaker embedding\n\n    Parameters\n    ----------\n    embedding : PipelineModel\n        pyannote.audio model\n    device : torch.device, optional\n        Device\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> get_embedding = PyannoteAudioPretrainedSpeakerEmbedding(\"pyannote/embedding\")\n    >>> assert waveforms.ndim == 3\n    >>> batch_size, num_channels, num_samples = waveforms.shape\n    >>> assert num_channels == 1\n    >>> embeddings = get_embedding(waveforms)\n    >>> assert embeddings.ndim == 2\n    >>> assert embeddings.shape[0] == batch_size\n\n    >>> assert masks.ndim == 1\n    >>> assert masks.shape[0] == batch_size\n    >>> embeddings = get_embedding(waveforms, masks=masks)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: PipelineModel = \"pyannote/embedding\",\n        device: torch.device = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n        super().__init__()\n        self.embedding = embedding\n        self.device = device or torch.device(\"cpu\")\n\n        self.model_: Model = get_model(self.embedding, use_auth_token=use_auth_token)\n        self.model_.eval()\n        self.model_.to(self.device)\n\n    def to(self, device: torch.device):\n        self.model_.to(device)\n        self.device = device\n        return self\n\n    @cached_property\n    def sample_rate(self) -> int:\n        return self.model_.audio.sample_rate\n\n    @cached_property\n    def dimension(self) -> int:\n        return self.model_.introspection.dimension\n\n    @cached_property\n    def metric(self) -> str:\n        return \"cosine\"\n\n    @cached_property\n    def min_num_samples(self) -> int:\n        return self.model_.introspection.min_num_samples\n\n    def __call__(\n        self, waveforms: torch.Tensor, masks: torch.Tensor = None\n    ) -> np.ndarray:\n        with torch.no_grad():\n            if masks is None:\n                embeddings = self.model_(waveforms.to(self.device))\n            else:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    embeddings = self.model_(\n                        waveforms.to(self.device), weights=masks.to(self.device)\n                    )\n        return embeddings.cpu().numpy()", "\n\ndef PretrainedSpeakerEmbedding(\n    embedding: PipelineModel,\n    device: torch.device = None,\n    use_auth_token: Union[Text, None] = None,\n):\n    \"\"\"Pretrained speaker embedding\n\n    Parameters\n    ----------\n    embedding : Text\n        Can be a SpeechBrain (e.g. \"speechbrain/spkrec-ecapa-voxceleb\")\n        or a pyannote.audio model.\n    device : torch.device, optional\n        Device\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> get_embedding = PretrainedSpeakerEmbedding(\"pyannote/embedding\")\n    >>> get_embedding = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n    >>> get_embedding = PretrainedSpeakerEmbedding(\"nvidia/speakerverification_en_titanet_large\")\n    >>> assert waveforms.ndim == 3\n    >>> batch_size, num_channels, num_samples = waveforms.shape\n    >>> assert num_channels == 1\n    >>> embeddings = get_embedding(waveforms)\n    >>> assert embeddings.ndim == 2\n    >>> assert embeddings.shape[0] == batch_size\n\n    >>> assert masks.ndim == 1\n    >>> assert masks.shape[0] == batch_size\n    >>> embeddings = get_embedding(waveforms, masks=masks)\n    \"\"\"\n\n    if isinstance(embedding, str) and \"speechbrain\" in embedding:\n        return SpeechBrainPretrainedSpeakerEmbedding(\n            embedding, device=device, use_auth_token=use_auth_token\n        )\n\n    elif isinstance(embedding, str) and \"nvidia\" in embedding:\n        return NeMoPretrainedSpeakerEmbedding(embedding, device=device)\n\n    elif isinstance(embedding, str) and \"fbdp1202\" in embedding:\n        return MFAConformerPretrainedSpeakerEmbedding(\n            embedding, device=device, use_auth_token=use_auth_token\n        )\n\n    else:\n        return PyannoteAudioPretrainedSpeakerEmbedding(\n            embedding, device=device, use_auth_token=use_auth_token\n        )", "\n\nclass SpeakerEmbedding(Pipeline):\n    \"\"\"Speaker embedding pipeline\n\n    This pipeline assumes that each file contains exactly one speaker\n    and extracts one single embedding from the whole file.\n\n    Parameters\n    ----------\n    embedding : Model, str, or dict, optional\n        Pretrained embedding model. Defaults to \"pyannote/embedding\".\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n    segmentation : Model, str, or dict, optional\n        Pretrained segmentation (or voice activity detection) model.\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n        Defaults to no voice activity detection.\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> from pyannote.audio.pipelines import SpeakerEmbedding\n    >>> pipeline = SpeakerEmbedding()\n    >>> emb1 = pipeline(\"speaker1.wav\")\n    >>> emb2 = pipeline(\"speaker2.wav\")\n    >>> from scipy.spatial.distance import cdist\n    >>> distance = cdist(emb1, emb2, metric=\"cosine\")[0,0]\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: PipelineModel = \"pyannote/embedding\",\n        segmentation: PipelineModel = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n        super().__init__()\n\n        self.embedding = embedding\n        self.segmentation = segmentation\n\n        self.embedding_model_: Model = get_model(\n            embedding, use_auth_token=use_auth_token\n        )\n\n        if self.segmentation is not None:\n            segmentation_model: Model = get_model(\n                self.segmentation, use_auth_token=use_auth_token\n            )\n            self._segmentation = Inference(\n                segmentation_model,\n                pre_aggregation_hook=lambda scores: np.max(\n                    scores, axis=-1, keepdims=True\n                ),\n            )\n\n    def apply(self, file: AudioFile) -> np.ndarray:\n\n        device = self.embedding_model_.device\n\n        # read audio file and send it to GPU\n        waveform = self.embedding_model_.audio(file)[0][None].to(device)\n\n        if self.segmentation is None:\n            weights = None\n        else:\n            # obtain voice activity scores\n            weights = self._segmentation(file).data\n            # HACK -- this should be fixed upstream\n            weights[np.isnan(weights)] = 0.0\n            weights = torch.from_numpy(weights**3)[None, :, 0].to(device)\n\n        # extract speaker embedding on parts of\n        with torch.no_grad():\n            return self.embedding_model_(waveform, weights=weights).cpu().numpy()", "\n\ndef main(\n    protocol: str = \"VoxCeleb.SpeakerVerification.VoxCeleb1\",\n    subset: str = \"test\",\n    embedding: str = \"pyannote/embedding\",\n    segmentation: str = None,\n):\n\n    import typer\n    from pyannote.database import FileFinder, get_protocol\n    from pyannote.metrics.binary_classification import det_curve\n    from scipy.spatial.distance import cdist\n    from tqdm import tqdm\n\n    pipeline = SpeakerEmbedding(embedding=embedding, segmentation=segmentation)\n\n    protocol = get_protocol(protocol, preprocessors={\"audio\": FileFinder()})\n\n    y_true, y_pred = [], []\n\n    emb = dict()\n\n    trials = getattr(protocol, f\"{subset}_trial\")()\n\n    for t, trial in enumerate(tqdm(trials)):\n\n        audio1 = trial[\"file1\"][\"audio\"]\n        if audio1 not in emb:\n            emb[audio1] = pipeline(audio1)\n\n        audio2 = trial[\"file2\"][\"audio\"]\n        if audio2 not in emb:\n            emb[audio2] = pipeline(audio2)\n\n        y_pred.append(cdist(emb[audio1], emb[audio2], metric=\"cosine\")[0][0])\n        y_true.append(trial[\"reference\"])\n\n    _, _, _, eer = det_curve(y_true, np.array(y_pred), distances=True)\n    typer.echo(\n        f\"{protocol.name} | {subset} | {embedding} | {segmentation} | EER = {100 * eer:.3f}%\"\n    )", "\n\nif __name__ == \"__main__\":\n    import typer\n\n    typer.run(main)\n"]}
{"filename": "src/custom_pyannote/speaker_diarization.py", "chunked_list": ["# The MIT License (MIT)\n#\n# Copyright (c) 2021- CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Speaker diarization pipelines\"\"\"\n\nimport functools\nimport itertools", "import functools\nimport itertools\nimport math\nimport time\nfrom typing import Callable, Optional, Text, Union\n\nimport numpy as np\nimport torch\nfrom einops import rearrange\nfrom pyannote.core import Annotation, SlidingWindow, SlidingWindowFeature", "from einops import rearrange\nfrom pyannote.core import Annotation, SlidingWindow, SlidingWindowFeature\nfrom pyannote.metrics.diarization import GreedyDiarizationErrorRate\nfrom pyannote.pipeline.parameter import ParamDict, Uniform\n\nfrom pyannote.audio import Audio, Inference, Model, Pipeline\nfrom pyannote.audio.core.io import AudioFile\n# from pyannote.audio.pipelines.clustering import Clustering\nfrom .clustering import Clustering\n# from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding", "from .clustering import Clustering\n# from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\nfrom .speaker_verification import PretrainedSpeakerEmbedding\nfrom pyannote.audio.pipelines.utils import (\n    PipelineModel,\n    SpeakerDiarizationMixin,\n    get_model,\n)\nfrom pyannote.audio.utils.signal import binarize\n", "from pyannote.audio.utils.signal import binarize\n\n# import sys\n# sys.path.append(\"..\")\n# from utils import logging_time\nfrom ..utils import logging_time\n\n\nimport pdb\n", "import pdb\n\n\ndef batchify(iterable, batch_size: int = 32, fillvalue=None):\n    \"\"\"Batchify iterable\"\"\"\n    # batchify('ABCDEFG', 3) --> ['A', 'B', 'C']  ['D', 'E', 'F']  [G, ]\n    args = [iter(iterable)] * batch_size\n    return itertools.zip_longest(*args, fillvalue=fillvalue)\n\n\nclass SpeakerDiarization(SpeakerDiarizationMixin, Pipeline):\n    \"\"\"Speaker diarization pipeline\n\n    Parameters\n    ----------\n    segmentation : Model, str, or dict, optional\n        Pretrained segmentation model. Defaults to \"pyannote/segmentation@2022.07\".\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n    segmentation_duration: float, optional\n        The segmentation model is applied on a window sliding over the whole audio file.\n        `segmentation_duration` controls the duration of this window. Defaults to the\n        duration used when training the model (model.specifications.duration).\n    segmentation_step: float, optional\n        The segmentation model is applied on a window sliding over the whole audio file.\n        `segmentation_step` controls the step of this window, provided as a ratio of its\n        duration. Defaults to 0.1 (i.e. 90% overlap between two consecuive windows).\n    embedding : Model, str, or dict, optional\n        Pretrained embedding model. Defaults to \"pyannote/embedding@2022.07\".\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n    embedding_exclude_overlap : bool, optional\n        Exclude overlapping speech regions when extracting embeddings.\n        Defaults (False) to use the whole speech.\n    clustering : str, optional\n        Clustering algorithm. See pyannote.audio.pipelines.clustering.Clustering\n        for available options. Defaults to \"HiddenMarkovModelClustering\".\n    segmentation_batch_size : int, optional\n        Batch size used for speaker segmentation. Defaults to 32.\n    embedding_batch_size : int, optional\n        Batch size used for speaker embedding. Defaults to 32.\n    der_variant : dict, optional\n        Optimize for a variant of diarization error rate.\n        Defaults to {\"collar\": 0.0, \"skip_overlap\": False}. This is used in `get_metric`\n        when instantiating the metric: GreedyDiarizationErrorRate(**der_variant).\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> pipeline = SpeakerDiarization()\n    >>> diarization = pipeline(\"/path/to/audio.wav\")\n    >>> diarization = pipeline(\"/path/to/audio.wav\", num_speakers=4)\n    >>> diarization = pipeline(\"/path/to/audio.wav\", min_speakers=2, max_speakers=10)\n\n    Hyper-parameters\n    ----------------\n    segmentation.threshold\n    segmentation.min_duration_off\n    clustering.???\n    \"\"\"\n\n    def __init__(\n        self,\n        segmentation: PipelineModel = \"pyannote/segmentation@2022.07\",\n        segmentation_duration: float = None,\n        segmentation_step: float = 0.1,\n        embedding: PipelineModel = \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\",\n        embedding_exclude_overlap: bool = False,\n        clustering: str = \"HiddenMarkovModelClustering\",\n        embedding_batch_size: int = 32,\n        segmentation_batch_size: int = 32,\n        der_variant: dict = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n\n        super().__init__()\n\n        self.segmentation_model = segmentation\n        model: Model = get_model(segmentation, use_auth_token=use_auth_token)\n\n        self.segmentation_batch_size = segmentation_batch_size\n        self.segmentation_duration = (\n            segmentation_duration or model.specifications.duration\n        )\n        self.segmentation_step = segmentation_step\n\n        self.embedding = embedding\n        self.embedding_batch_size = embedding_batch_size\n        self.embedding_exclude_overlap = embedding_exclude_overlap\n\n        self.klustering = clustering\n\n        self.der_variant = der_variant or {\"collar\": 0.0, \"skip_overlap\": False}\n\n        self._segmentation = Inference(\n            model,\n            duration=self.segmentation_duration,\n            step=self.segmentation_step * self.segmentation_duration,\n            skip_aggregation=True,\n            batch_size=self.segmentation_batch_size,\n        )\n        self._frames: SlidingWindow = self._segmentation.model.introspection.frames\n\n        if self._segmentation.model.specifications.powerset:\n            self.segmentation = ParamDict(\n                min_duration_off=Uniform(0.0, 1.0),\n            )\n\n        else:\n            self.segmentation = ParamDict(\n                threshold=Uniform(0.1, 0.9),\n                min_duration_off=Uniform(0.0, 1.0),\n            )\n\n        if self.klustering == \"OracleClustering\":\n            metric = \"not_applicable\"\n\n        else:\n            # custom\n            self._embedding = PretrainedSpeakerEmbedding(\n                self.embedding, use_auth_token=use_auth_token\n            )\n            self._audio = Audio(sample_rate=self._embedding.sample_rate, mono=True)\n            metric = self._embedding.metric\n\n        try:\n            Klustering = Clustering[clustering]\n        except KeyError:\n            raise ValueError(\n                f'clustering must be one of [{\", \".join(list(Clustering.__members__))}]'\n            )\n        self.clustering = Klustering.value(metric=metric)\n\n    def default_parameters(self):\n\n        if (\n            self.segmentation_model == \"pyannote/segmentation@2022.07\"\n            and self.segmentation_duration == 5.0\n            and self.segmentation_step == 0.1\n            and self.embedding\n            == \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\"\n            and self.embedding_exclude_overlap == True\n            and self.clustering == \"HiddenMarkovModelClustering\"\n        ):\n            return {\n                \"segmentation\": {\n                    \"threshold\": 0.58,\n                    \"min_duration_off\": 0.0,\n                },\n                \"clustering\": {\n                    \"single_cluster_detection\": {\n                        \"quantile\": 0.05,\n                        \"threshold\": 1.15,\n                    },\n                    \"covariance_type\": \"diag\",\n                    \"threshold\": 0.35,\n                },\n            }\n\n        raise NotImplementedError()\n\n    def classes(self):\n        speaker = 0\n        while True:\n            yield f\"SPEAKER_{speaker:02d}\"\n            speaker += 1\n\n    @property\n    def CACHED_SEGMENTATION(self):\n        return \"training_cache/segmentation\"\n\n    @logging_time\n    def get_segmentations(self, file, hook=None) -> SlidingWindowFeature:\n        \"\"\"Apply segmentation model\n\n        Parameter\n        ---------\n        file : AudioFile\n        hook : Optional[Callable]\n\n        Returns\n        -------\n        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n        \"\"\"\n\n        if hook is not None:\n            hook = functools.partial(hook, \"segmentation\", None)\n\n        if self.training:\n            if self.CACHED_SEGMENTATION in file:\n                segmentations = file[self.CACHED_SEGMENTATION]\n            else:\n                segmentations = self._segmentation(file, hook=hook)\n                file[self.CACHED_SEGMENTATION] = segmentations\n        else:\n            segmentations: SlidingWindowFeature = self._segmentation(file, hook=hook)\n\n        return segmentations\n\n    @logging_time\n    def get_embeddings(\n        self,\n        file,\n        binary_segmentations: SlidingWindowFeature,\n        exclude_overlap: bool = False,\n        hook: Optional[Callable] = None,\n    ):\n        \"\"\"Extract embeddings for each (chunk, speaker) pair\n\n        Parameters\n        ----------\n        file : AudioFile\n        binary_segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n            Binarized segmentation.\n        exclude_overlap : bool, optional\n            Exclude overlapping speech regions when extracting embeddings.\n            In case non-overlapping speech is too short, use the whole speech.\n        hook: Optional[Callable]\n            Called during embeddings after every batch to report the progress\n\n        Returns\n        -------\n        embeddings : (num_chunks, num_speakers, dimension) array\n        \"\"\"\n\n        # when optimizing the hyper-parameters of this pipeline with frozen\n        # \"segmentation.threshold\", one can reuse the embeddings from the first trial,\n        # bringing a massive speed up to the optimization process (and hence allowing to use\n        # a larger search space).\n        if self.training:\n\n            # we only re-use embeddings if they were extracted based on the same value of the\n            # \"segmentation.threshold\" hyperparameter or if the segmentation model relies on\n            # `powerset` mode\n            cache = file.get(\"training_cache/embeddings\", dict())\n            if (\"embeddings\" in cache) and (\n                self._segmentation.model.specifications.powerset\n                or (cache[\"segmentation.threshold\"] == self.segmentation.threshold)\n            ):\n                return cache[\"embeddings\"]\n\n        duration = binary_segmentations.sliding_window.duration\n        num_chunks, num_frames, num_speakers = binary_segmentations.data.shape\n\n        if exclude_overlap:\n            # minimum number of samples needed to extract an embedding\n            # (a lower number of samples would result in an error), 640(40ms)\n            min_num_samples = self._embedding.min_num_samples\n\n            # corresponding minimum number of frames (80000)=(5.0 * 16000)\n            num_samples = duration * self._embedding.sample_rate\n            min_num_frames = math.ceil(num_frames * min_num_samples / num_samples)\n\n            # zero-out frames with overlapping speech\n            clean_frames = 1.0 * (\n                np.sum(binary_segmentations.data, axis=2, keepdims=True) < 2\n            )\n            clean_segmentations = SlidingWindowFeature(\n                binary_segmentations.data * clean_frames,\n                binary_segmentations.sliding_window,\n            )\n\n        else:\n            min_num_frames = -1\n            clean_segmentations = SlidingWindowFeature(\n                binary_segmentations.data, binary_segmentations.sliding_window\n            )\n\n        def iter_waveform_and_mask():\n            for (chunk, masks), (_, clean_masks) in zip(\n                binary_segmentations, clean_segmentations\n            ):\n                # chunk: Segment(t, t + duration)\n                # masks: (num_frames, local_num_speakers) np.ndarray\n\n                waveform, _ = self._audio.crop(\n                    file,\n                    chunk,\n                    duration=duration,\n                    mode=\"pad\",\n                )\n                # waveform: (1, num_samples) torch.Tensor\n\n                # mask may contain NaN (in case of partial stitching)\n                masks = np.nan_to_num(masks, nan=0.0).astype(np.float32)\n                clean_masks = np.nan_to_num(clean_masks, nan=0.0).astype(np.float32)\n\n                for mask, clean_mask in zip(masks.T, clean_masks.T):\n                    # mask: (num_frames, ) np.ndarray\n\n                    if np.sum(clean_mask) > min_num_frames:\n                        used_mask = clean_mask\n                    else:\n                        used_mask = mask\n\n                    yield waveform[None], torch.from_numpy(used_mask)[None]\n                    # w: (1, 1, num_samples) torch.Tensor\n                    # m: (1, num_frames) torch.Tensor\n\n        batches = batchify(\n            iter_waveform_and_mask(),\n            batch_size=self.embedding_batch_size,\n            fillvalue=(None, None),\n        )\n\n        batch_count = math.ceil(num_chunks * num_speakers / self.embedding_batch_size)\n\n        embedding_batches = []\n\n        for i, batch in enumerate(batches, 1):\n            waveforms, masks = zip(*filter(lambda b: b[0] is not None, batch))\n\n            waveform_batch = torch.vstack(waveforms)\n            # (batch_size, 1, num_samples) torch.Tensor\n\n            mask_batch = torch.vstack(masks)\n            # (batch_size, num_frames) torch.Tensor\n            embedding_batch: np.ndarray = self._embedding(\n                waveform_batch, masks=mask_batch\n            )\n            # (batch_size, dimension) np.ndarray\n\n            embedding_batches.append(embedding_batch)\n\n            if hook is not None:\n                hook(\"embeddings\", embedding_batch, total=batch_count, completed=i)\n\n        embedding_batches = np.vstack(embedding_batches)\n\n        embeddings = rearrange(embedding_batches, \"(c s) d -> c s d\", c=num_chunks)\n\n        # caching embeddings for subsequent trials\n        # (see comments at the top of this method for more details)\n        if self.training:\n\n            if self._segmentation.model.specifications.powerset:\n                file[\"training_cache/embeddings\"] = {\n                    \"embeddings\": embeddings,\n                }\n            else:\n                file[\"training_cache/embeddings\"] = {\n                    \"segmentation.threshold\": self.segmentation.threshold,\n                    \"embeddings\": embeddings,\n                }\n\n        return embeddings\n    \n    @logging_time\n    def reconstruct(\n        self,\n        segmentations: SlidingWindowFeature,\n        hard_clusters: np.ndarray,\n        count: SlidingWindowFeature,\n    ) -> SlidingWindowFeature:\n        \"\"\"Build final discrete diarization out of clustered segmentation\n\n        Parameters\n        ----------\n        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n            Raw speaker segmentation.\n        hard_clusters : (num_chunks, num_speakers) array\n            Output of clustering step.\n        count : (total_num_frames, 1) SlidingWindowFeature\n            Instantaneous number of active speakers.\n\n        Returns\n        -------\n        discrete_diarization : SlidingWindowFeature\n            Discrete (0s and 1s) diarization.\n        \"\"\"\n\n        num_chunks, num_frames, local_num_speakers = segmentations.data.shape\n\n        num_clusters = np.max(hard_clusters) + 1\n        clustered_segmentations = np.NAN * np.zeros(\n            (num_chunks, num_frames, num_clusters)\n        )\n\n        for c, (cluster, (chunk, segmentation)) in enumerate(\n            zip(hard_clusters, segmentations)\n        ):\n\n            # cluster is (local_num_speakers, )-shaped\n            # segmentation is (num_frames, local_num_speakers)-shaped\n            for k in np.unique(cluster):\n                if k == -2:\n                    continue\n\n                # TODO: can we do better than this max here?\n                clustered_segmentations[c, :, k] = np.max(\n                    segmentation[:, cluster == k], axis=1\n                )\n\n        clustered_segmentations = SlidingWindowFeature(\n            clustered_segmentations, segmentations.sliding_window\n        )\n\n        return self.to_diarization(clustered_segmentations, count)\n\n    @logging_time\n    def apply(\n        self,\n        file: AudioFile,\n        num_speakers: int = None,\n        min_speakers: int = None,\n        max_speakers: int = None,\n        hook: Optional[Callable] = None,\n    ) -> Annotation:\n        \"\"\"Apply speaker diarization\n\n        Parameters\n        ----------\n        file : AudioFile\n            Processed file.\n        num_speakers : int, optional\n            Number of speakers, when known.\n        min_speakers : int, optional\n            Minimum number of speakers. Has no effect when `num_speakers` is provided.\n        max_speakers : int, optional\n            Maximum number of speakers. Has no effect when `num_speakers` is provided.\n        hook : callable, optional\n            Callback called after each major steps of the pipeline as follows:\n                hook(step_name,      # human-readable name of current step\n                     step_artefact,  # artifact generated by current step\n                     file=file)      # file being processed\n            Time-consuming steps call `hook` multiple times with the same `step_name`\n            and additional `completed` and `total` keyword arguments usable to track\n            progress of current step.\n\n        Returns\n        -------\n        diarization : Annotation\n            Speaker diarization\n        \"\"\"\n\n        # setup hook (e.g. for debugging purposes)\n        hook = self.setup_hook(file, hook=hook)\n\n        num_speakers, min_speakers, max_speakers = self.set_num_speakers(\n            num_speakers=num_speakers,\n            min_speakers=min_speakers,\n            max_speakers=max_speakers,\n        )\n\n        segmentations = self.get_segmentations(file, hook=hook)\n        hook(\"segmentation\", segmentations)\n        #   shape: (num_chunks, num_frames, local_num_speakers)\n\n        # estimate frame-level number of instantaneous speakers\n        count = self.speaker_count(\n            segmentations,\n            onset=0.5\n            if self._segmentation.model.specifications.powerset\n            else self.segmentation.threshold,\n            frames=self._frames,\n        )\n        hook(\"speaker_counting\", count)\n        #   shape: (num_frames, 1)\n        #   dtype: int\n\n        # exit early when no speaker is ever active\n        if np.nanmax(count.data) == 0.0:\n            return Annotation(uri=file[\"uri\"])\n\n        # binarize segmentation\n        if self._segmentation.model.specifications.powerset:\n            binarized_segmentations = segmentations\n        else:\n            binarized_segmentations: SlidingWindowFeature = binarize(\n                segmentations,\n                onset=self.segmentation.threshold,\n                initial_state=False,\n            )\n        \n\n        if self.klustering == \"OracleClustering\":\n            embeddings = None\n        else:\n            embeddings = self.get_embeddings(\n                file,\n                binarized_segmentations,\n                exclude_overlap=self.embedding_exclude_overlap,\n                hook=hook,\n            )\n            hook(\"embeddings\", embeddings)\n            #   shape: (num_chunks, local_num_speakers, dimension)\n\n        start = time.time()\n        hard_clusters, _ = self.clustering(\n            embeddings=embeddings,\n            segmentations=binarized_segmentations,\n            num_clusters=num_speakers,\n            min_clusters=min_speakers,\n            max_clusters=max_speakers,\n            file=file,  # <== for oracle clustering\n            frames=self._frames,  # <== for oracle clustering\n        )\n        print(\"[Clustering] {} sec\".format(time.time()-start))\n        #   hard_clusters: (num_chunks, num_speakers)\n\n        # reconstruct discrete diarization from raw hard clusters\n\n        # keep track of inactive speakers\n        inactive_speakers = np.sum(binarized_segmentations.data, axis=1) == 0\n        #   shape: (num_chunks, num_speakers)\n\n        hard_clusters[inactive_speakers] = -2\n        discrete_diarization = self.reconstruct(\n            segmentations,\n            hard_clusters,\n            count,\n        )\n        hook(\"discrete_diarization\", discrete_diarization)\n\n        # convert to continuous diarization\n        start = time.time()\n        diarization = self.to_annotation(\n            discrete_diarization,\n            min_duration_on=0.0,\n            min_duration_off=self.segmentation.min_duration_off,\n        )\n        print(\"[to_annotation] {} sec\".format(time.time()-start))\n        diarization.uri = file[\"uri\"]\n\n        # when reference is available, use it to map hypothesized speakers\n        # to reference speakers (this makes later error analysis easier\n        # but does not modify the actual output of the diarization pipeline)\n        if \"annotation\" in file and file[\"annotation\"]:\n            return self.optimal_mapping(file[\"annotation\"], diarization)\n\n        # when reference is not available, rename hypothesized speakers\n        # to human-readable SPEAKER_00, SPEAKER_01, ...\n        return diarization.rename_labels(\n            {\n                label: expected_label\n                for label, expected_label in zip(diarization.labels(), self.classes())\n            }\n        )\n\n    def get_metric(self) -> GreedyDiarizationErrorRate:\n        return GreedyDiarizationErrorRate(**self.der_variant)", "\n\nclass SpeakerDiarization(SpeakerDiarizationMixin, Pipeline):\n    \"\"\"Speaker diarization pipeline\n\n    Parameters\n    ----------\n    segmentation : Model, str, or dict, optional\n        Pretrained segmentation model. Defaults to \"pyannote/segmentation@2022.07\".\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n    segmentation_duration: float, optional\n        The segmentation model is applied on a window sliding over the whole audio file.\n        `segmentation_duration` controls the duration of this window. Defaults to the\n        duration used when training the model (model.specifications.duration).\n    segmentation_step: float, optional\n        The segmentation model is applied on a window sliding over the whole audio file.\n        `segmentation_step` controls the step of this window, provided as a ratio of its\n        duration. Defaults to 0.1 (i.e. 90% overlap between two consecuive windows).\n    embedding : Model, str, or dict, optional\n        Pretrained embedding model. Defaults to \"pyannote/embedding@2022.07\".\n        See pyannote.audio.pipelines.utils.get_model for supported format.\n    embedding_exclude_overlap : bool, optional\n        Exclude overlapping speech regions when extracting embeddings.\n        Defaults (False) to use the whole speech.\n    clustering : str, optional\n        Clustering algorithm. See pyannote.audio.pipelines.clustering.Clustering\n        for available options. Defaults to \"HiddenMarkovModelClustering\".\n    segmentation_batch_size : int, optional\n        Batch size used for speaker segmentation. Defaults to 32.\n    embedding_batch_size : int, optional\n        Batch size used for speaker embedding. Defaults to 32.\n    der_variant : dict, optional\n        Optimize for a variant of diarization error rate.\n        Defaults to {\"collar\": 0.0, \"skip_overlap\": False}. This is used in `get_metric`\n        when instantiating the metric: GreedyDiarizationErrorRate(**der_variant).\n    use_auth_token : str, optional\n        When loading private huggingface.co models, set `use_auth_token`\n        to True or to a string containing your hugginface.co authentication\n        token that can be obtained by running `huggingface-cli login`\n\n    Usage\n    -----\n    >>> pipeline = SpeakerDiarization()\n    >>> diarization = pipeline(\"/path/to/audio.wav\")\n    >>> diarization = pipeline(\"/path/to/audio.wav\", num_speakers=4)\n    >>> diarization = pipeline(\"/path/to/audio.wav\", min_speakers=2, max_speakers=10)\n\n    Hyper-parameters\n    ----------------\n    segmentation.threshold\n    segmentation.min_duration_off\n    clustering.???\n    \"\"\"\n\n    def __init__(\n        self,\n        segmentation: PipelineModel = \"pyannote/segmentation@2022.07\",\n        segmentation_duration: float = None,\n        segmentation_step: float = 0.1,\n        embedding: PipelineModel = \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\",\n        embedding_exclude_overlap: bool = False,\n        clustering: str = \"HiddenMarkovModelClustering\",\n        embedding_batch_size: int = 32,\n        segmentation_batch_size: int = 32,\n        der_variant: dict = None,\n        use_auth_token: Union[Text, None] = None,\n    ):\n\n        super().__init__()\n\n        self.segmentation_model = segmentation\n        model: Model = get_model(segmentation, use_auth_token=use_auth_token)\n\n        self.segmentation_batch_size = segmentation_batch_size\n        self.segmentation_duration = (\n            segmentation_duration or model.specifications.duration\n        )\n        self.segmentation_step = segmentation_step\n\n        self.embedding = embedding\n        self.embedding_batch_size = embedding_batch_size\n        self.embedding_exclude_overlap = embedding_exclude_overlap\n\n        self.klustering = clustering\n\n        self.der_variant = der_variant or {\"collar\": 0.0, \"skip_overlap\": False}\n\n        self._segmentation = Inference(\n            model,\n            duration=self.segmentation_duration,\n            step=self.segmentation_step * self.segmentation_duration,\n            skip_aggregation=True,\n            batch_size=self.segmentation_batch_size,\n        )\n        self._frames: SlidingWindow = self._segmentation.model.introspection.frames\n\n        if self._segmentation.model.specifications.powerset:\n            self.segmentation = ParamDict(\n                min_duration_off=Uniform(0.0, 1.0),\n            )\n\n        else:\n            self.segmentation = ParamDict(\n                threshold=Uniform(0.1, 0.9),\n                min_duration_off=Uniform(0.0, 1.0),\n            )\n\n        if self.klustering == \"OracleClustering\":\n            metric = \"not_applicable\"\n\n        else:\n            # custom\n            self._embedding = PretrainedSpeakerEmbedding(\n                self.embedding, use_auth_token=use_auth_token\n            )\n            self._audio = Audio(sample_rate=self._embedding.sample_rate, mono=True)\n            metric = self._embedding.metric\n\n        try:\n            Klustering = Clustering[clustering]\n        except KeyError:\n            raise ValueError(\n                f'clustering must be one of [{\", \".join(list(Clustering.__members__))}]'\n            )\n        self.clustering = Klustering.value(metric=metric)\n\n    def default_parameters(self):\n\n        if (\n            self.segmentation_model == \"pyannote/segmentation@2022.07\"\n            and self.segmentation_duration == 5.0\n            and self.segmentation_step == 0.1\n            and self.embedding\n            == \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\"\n            and self.embedding_exclude_overlap == True\n            and self.clustering == \"HiddenMarkovModelClustering\"\n        ):\n            return {\n                \"segmentation\": {\n                    \"threshold\": 0.58,\n                    \"min_duration_off\": 0.0,\n                },\n                \"clustering\": {\n                    \"single_cluster_detection\": {\n                        \"quantile\": 0.05,\n                        \"threshold\": 1.15,\n                    },\n                    \"covariance_type\": \"diag\",\n                    \"threshold\": 0.35,\n                },\n            }\n\n        raise NotImplementedError()\n\n    def classes(self):\n        speaker = 0\n        while True:\n            yield f\"SPEAKER_{speaker:02d}\"\n            speaker += 1\n\n    @property\n    def CACHED_SEGMENTATION(self):\n        return \"training_cache/segmentation\"\n\n    @logging_time\n    def get_segmentations(self, file, hook=None) -> SlidingWindowFeature:\n        \"\"\"Apply segmentation model\n\n        Parameter\n        ---------\n        file : AudioFile\n        hook : Optional[Callable]\n\n        Returns\n        -------\n        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n        \"\"\"\n\n        if hook is not None:\n            hook = functools.partial(hook, \"segmentation\", None)\n\n        if self.training:\n            if self.CACHED_SEGMENTATION in file:\n                segmentations = file[self.CACHED_SEGMENTATION]\n            else:\n                segmentations = self._segmentation(file, hook=hook)\n                file[self.CACHED_SEGMENTATION] = segmentations\n        else:\n            segmentations: SlidingWindowFeature = self._segmentation(file, hook=hook)\n\n        return segmentations\n\n    @logging_time\n    def get_embeddings(\n        self,\n        file,\n        binary_segmentations: SlidingWindowFeature,\n        exclude_overlap: bool = False,\n        hook: Optional[Callable] = None,\n    ):\n        \"\"\"Extract embeddings for each (chunk, speaker) pair\n\n        Parameters\n        ----------\n        file : AudioFile\n        binary_segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n            Binarized segmentation.\n        exclude_overlap : bool, optional\n            Exclude overlapping speech regions when extracting embeddings.\n            In case non-overlapping speech is too short, use the whole speech.\n        hook: Optional[Callable]\n            Called during embeddings after every batch to report the progress\n\n        Returns\n        -------\n        embeddings : (num_chunks, num_speakers, dimension) array\n        \"\"\"\n\n        # when optimizing the hyper-parameters of this pipeline with frozen\n        # \"segmentation.threshold\", one can reuse the embeddings from the first trial,\n        # bringing a massive speed up to the optimization process (and hence allowing to use\n        # a larger search space).\n        if self.training:\n\n            # we only re-use embeddings if they were extracted based on the same value of the\n            # \"segmentation.threshold\" hyperparameter or if the segmentation model relies on\n            # `powerset` mode\n            cache = file.get(\"training_cache/embeddings\", dict())\n            if (\"embeddings\" in cache) and (\n                self._segmentation.model.specifications.powerset\n                or (cache[\"segmentation.threshold\"] == self.segmentation.threshold)\n            ):\n                return cache[\"embeddings\"]\n\n        duration = binary_segmentations.sliding_window.duration\n        num_chunks, num_frames, num_speakers = binary_segmentations.data.shape\n\n        if exclude_overlap:\n            # minimum number of samples needed to extract an embedding\n            # (a lower number of samples would result in an error), 640(40ms)\n            min_num_samples = self._embedding.min_num_samples\n\n            # corresponding minimum number of frames (80000)=(5.0 * 16000)\n            num_samples = duration * self._embedding.sample_rate\n            min_num_frames = math.ceil(num_frames * min_num_samples / num_samples)\n\n            # zero-out frames with overlapping speech\n            clean_frames = 1.0 * (\n                np.sum(binary_segmentations.data, axis=2, keepdims=True) < 2\n            )\n            clean_segmentations = SlidingWindowFeature(\n                binary_segmentations.data * clean_frames,\n                binary_segmentations.sliding_window,\n            )\n\n        else:\n            min_num_frames = -1\n            clean_segmentations = SlidingWindowFeature(\n                binary_segmentations.data, binary_segmentations.sliding_window\n            )\n\n        def iter_waveform_and_mask():\n            for (chunk, masks), (_, clean_masks) in zip(\n                binary_segmentations, clean_segmentations\n            ):\n                # chunk: Segment(t, t + duration)\n                # masks: (num_frames, local_num_speakers) np.ndarray\n\n                waveform, _ = self._audio.crop(\n                    file,\n                    chunk,\n                    duration=duration,\n                    mode=\"pad\",\n                )\n                # waveform: (1, num_samples) torch.Tensor\n\n                # mask may contain NaN (in case of partial stitching)\n                masks = np.nan_to_num(masks, nan=0.0).astype(np.float32)\n                clean_masks = np.nan_to_num(clean_masks, nan=0.0).astype(np.float32)\n\n                for mask, clean_mask in zip(masks.T, clean_masks.T):\n                    # mask: (num_frames, ) np.ndarray\n\n                    if np.sum(clean_mask) > min_num_frames:\n                        used_mask = clean_mask\n                    else:\n                        used_mask = mask\n\n                    yield waveform[None], torch.from_numpy(used_mask)[None]\n                    # w: (1, 1, num_samples) torch.Tensor\n                    # m: (1, num_frames) torch.Tensor\n\n        batches = batchify(\n            iter_waveform_and_mask(),\n            batch_size=self.embedding_batch_size,\n            fillvalue=(None, None),\n        )\n\n        batch_count = math.ceil(num_chunks * num_speakers / self.embedding_batch_size)\n\n        embedding_batches = []\n\n        for i, batch in enumerate(batches, 1):\n            waveforms, masks = zip(*filter(lambda b: b[0] is not None, batch))\n\n            waveform_batch = torch.vstack(waveforms)\n            # (batch_size, 1, num_samples) torch.Tensor\n\n            mask_batch = torch.vstack(masks)\n            # (batch_size, num_frames) torch.Tensor\n            embedding_batch: np.ndarray = self._embedding(\n                waveform_batch, masks=mask_batch\n            )\n            # (batch_size, dimension) np.ndarray\n\n            embedding_batches.append(embedding_batch)\n\n            if hook is not None:\n                hook(\"embeddings\", embedding_batch, total=batch_count, completed=i)\n\n        embedding_batches = np.vstack(embedding_batches)\n\n        embeddings = rearrange(embedding_batches, \"(c s) d -> c s d\", c=num_chunks)\n\n        # caching embeddings for subsequent trials\n        # (see comments at the top of this method for more details)\n        if self.training:\n\n            if self._segmentation.model.specifications.powerset:\n                file[\"training_cache/embeddings\"] = {\n                    \"embeddings\": embeddings,\n                }\n            else:\n                file[\"training_cache/embeddings\"] = {\n                    \"segmentation.threshold\": self.segmentation.threshold,\n                    \"embeddings\": embeddings,\n                }\n\n        return embeddings\n    \n    @logging_time\n    def reconstruct(\n        self,\n        segmentations: SlidingWindowFeature,\n        hard_clusters: np.ndarray,\n        count: SlidingWindowFeature,\n    ) -> SlidingWindowFeature:\n        \"\"\"Build final discrete diarization out of clustered segmentation\n\n        Parameters\n        ----------\n        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n            Raw speaker segmentation.\n        hard_clusters : (num_chunks, num_speakers) array\n            Output of clustering step.\n        count : (total_num_frames, 1) SlidingWindowFeature\n            Instantaneous number of active speakers.\n\n        Returns\n        -------\n        discrete_diarization : SlidingWindowFeature\n            Discrete (0s and 1s) diarization.\n        \"\"\"\n\n        num_chunks, num_frames, local_num_speakers = segmentations.data.shape\n\n        num_clusters = np.max(hard_clusters) + 1\n        clustered_segmentations = np.NAN * np.zeros(\n            (num_chunks, num_frames, num_clusters)\n        )\n\n        for c, (cluster, (chunk, segmentation)) in enumerate(\n            zip(hard_clusters, segmentations)\n        ):\n\n            # cluster is (local_num_speakers, )-shaped\n            # segmentation is (num_frames, local_num_speakers)-shaped\n            for k in np.unique(cluster):\n                if k == -2:\n                    continue\n\n                # TODO: can we do better than this max here?\n                clustered_segmentations[c, :, k] = np.max(\n                    segmentation[:, cluster == k], axis=1\n                )\n\n        clustered_segmentations = SlidingWindowFeature(\n            clustered_segmentations, segmentations.sliding_window\n        )\n\n        return self.to_diarization(clustered_segmentations, count)\n\n    @logging_time\n    def apply(\n        self,\n        file: AudioFile,\n        num_speakers: int = None,\n        min_speakers: int = None,\n        max_speakers: int = None,\n        hook: Optional[Callable] = None,\n    ) -> Annotation:\n        \"\"\"Apply speaker diarization\n\n        Parameters\n        ----------\n        file : AudioFile\n            Processed file.\n        num_speakers : int, optional\n            Number of speakers, when known.\n        min_speakers : int, optional\n            Minimum number of speakers. Has no effect when `num_speakers` is provided.\n        max_speakers : int, optional\n            Maximum number of speakers. Has no effect when `num_speakers` is provided.\n        hook : callable, optional\n            Callback called after each major steps of the pipeline as follows:\n                hook(step_name,      # human-readable name of current step\n                     step_artefact,  # artifact generated by current step\n                     file=file)      # file being processed\n            Time-consuming steps call `hook` multiple times with the same `step_name`\n            and additional `completed` and `total` keyword arguments usable to track\n            progress of current step.\n\n        Returns\n        -------\n        diarization : Annotation\n            Speaker diarization\n        \"\"\"\n\n        # setup hook (e.g. for debugging purposes)\n        hook = self.setup_hook(file, hook=hook)\n\n        num_speakers, min_speakers, max_speakers = self.set_num_speakers(\n            num_speakers=num_speakers,\n            min_speakers=min_speakers,\n            max_speakers=max_speakers,\n        )\n\n        segmentations = self.get_segmentations(file, hook=hook)\n        hook(\"segmentation\", segmentations)\n        #   shape: (num_chunks, num_frames, local_num_speakers)\n\n        # estimate frame-level number of instantaneous speakers\n        count = self.speaker_count(\n            segmentations,\n            onset=0.5\n            if self._segmentation.model.specifications.powerset\n            else self.segmentation.threshold,\n            frames=self._frames,\n        )\n        hook(\"speaker_counting\", count)\n        #   shape: (num_frames, 1)\n        #   dtype: int\n\n        # exit early when no speaker is ever active\n        if np.nanmax(count.data) == 0.0:\n            return Annotation(uri=file[\"uri\"])\n\n        # binarize segmentation\n        if self._segmentation.model.specifications.powerset:\n            binarized_segmentations = segmentations\n        else:\n            binarized_segmentations: SlidingWindowFeature = binarize(\n                segmentations,\n                onset=self.segmentation.threshold,\n                initial_state=False,\n            )\n        \n\n        if self.klustering == \"OracleClustering\":\n            embeddings = None\n        else:\n            embeddings = self.get_embeddings(\n                file,\n                binarized_segmentations,\n                exclude_overlap=self.embedding_exclude_overlap,\n                hook=hook,\n            )\n            hook(\"embeddings\", embeddings)\n            #   shape: (num_chunks, local_num_speakers, dimension)\n\n        start = time.time()\n        hard_clusters, _ = self.clustering(\n            embeddings=embeddings,\n            segmentations=binarized_segmentations,\n            num_clusters=num_speakers,\n            min_clusters=min_speakers,\n            max_clusters=max_speakers,\n            file=file,  # <== for oracle clustering\n            frames=self._frames,  # <== for oracle clustering\n        )\n        print(\"[Clustering] {} sec\".format(time.time()-start))\n        #   hard_clusters: (num_chunks, num_speakers)\n\n        # reconstruct discrete diarization from raw hard clusters\n\n        # keep track of inactive speakers\n        inactive_speakers = np.sum(binarized_segmentations.data, axis=1) == 0\n        #   shape: (num_chunks, num_speakers)\n\n        hard_clusters[inactive_speakers] = -2\n        discrete_diarization = self.reconstruct(\n            segmentations,\n            hard_clusters,\n            count,\n        )\n        hook(\"discrete_diarization\", discrete_diarization)\n\n        # convert to continuous diarization\n        start = time.time()\n        diarization = self.to_annotation(\n            discrete_diarization,\n            min_duration_on=0.0,\n            min_duration_off=self.segmentation.min_duration_off,\n        )\n        print(\"[to_annotation] {} sec\".format(time.time()-start))\n        diarization.uri = file[\"uri\"]\n\n        # when reference is available, use it to map hypothesized speakers\n        # to reference speakers (this makes later error analysis easier\n        # but does not modify the actual output of the diarization pipeline)\n        if \"annotation\" in file and file[\"annotation\"]:\n            return self.optimal_mapping(file[\"annotation\"], diarization)\n\n        # when reference is not available, rename hypothesized speakers\n        # to human-readable SPEAKER_00, SPEAKER_01, ...\n        return diarization.rename_labels(\n            {\n                label: expected_label\n                for label, expected_label in zip(diarization.labels(), self.classes())\n            }\n        )\n\n    def get_metric(self) -> GreedyDiarizationErrorRate:\n        return GreedyDiarizationErrorRate(**self.der_variant)", ""]}
{"filename": "src/custom_pyannote/pipeline.py", "chunked_list": ["# MIT License\n#\n# Copyright (c) 2021 CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport warnings\nfrom collections import OrderedDict\nfrom collections.abc import Iterator", "from collections import OrderedDict\nfrom collections.abc import Iterator\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Text, Union\n\nimport torch\nimport yaml\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import RepositoryNotFoundError", "from huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import RepositoryNotFoundError\nfrom pyannote.core.utils.helper import get_class_by_name\nfrom pyannote.database import FileFinder, ProtocolFile\nfrom pyannote.pipeline import Pipeline as _Pipeline\n\nfrom pyannote.audio import Audio, __version__\nfrom pyannote.audio.core.inference import BaseInference\nfrom pyannote.audio.core.io import AudioFile\nfrom pyannote.audio.core.model import CACHE_DIR, Model", "from pyannote.audio.core.io import AudioFile\nfrom pyannote.audio.core.model import CACHE_DIR, Model\n\nPIPELINE_PARAMS_NAME = \"config.yaml\"\n\n\nclass Pipeline(_Pipeline):\n    @classmethod\n    def from_pretrained(\n        cls,\n        checkpoint_path: Union[Text, Path],\n        hparams_file: Union[Text, Path] = None,\n        use_auth_token: Union[Text, None] = None,\n        cache_dir: Union[Path, Text] = CACHE_DIR,\n        **kwargs,\n    ) -> \"Pipeline\":\n        \"\"\"Load pretrained pipeline\n\n        Parameters\n        ----------\n        checkpoint_path : Path or str\n            Path to pipeline checkpoint, or a remote URL,\n            or a pipeline identifier from the huggingface.co model hub.\n        hparams_file: Path or str, optional\n        use_auth_token : str, optional\n            When loading a private huggingface.co pipeline, set `use_auth_token`\n            to True or to a string containing your hugginface.co authentication\n            token that can be obtained by running `huggingface-cli login`\n        cache_dir: Path or str, optional\n            Path to model cache directory. Defauorch/pyannote\" when unset.\n        \"\"\"\n\n        checkpoint_path = str(checkpoint_path)\n\n        if os.path.isfile(checkpoint_path):\n            config_yml = checkpoint_path\n\n        else:\n            if \"@\" in checkpoint_path:\n                model_id = checkpoint_path.split(\"@\")[0]\n                revision = checkpoint_path.split(\"@\")[1]\n            else:\n                model_id = checkpoint_path\n                revision = None\n\n            try:\n                config_yml = hf_hub_download(\n                    model_id,\n                    PIPELINE_PARAMS_NAME,\n                    repo_type=\"model\",\n                    revision=revision,\n                    library_name=\"pyannote\",\n                    library_version=__version__,\n                    cache_dir=cache_dir,\n                    # force_download=False,\n                    # proxies=None,\n                    # etag_timeout=10,\n                    # resume_download=False,\n                    use_auth_token=use_auth_token,\n                    # local_files_only=False,\n                    # legacy_cache_layout=False,\n                )\n\n            except RepositoryNotFoundError:\n                print(\n                    f\"\"\"\nCould not download '{model_id}' pipeline.\nIt might be because the pipeline is private or gated so make\nsure to authenticate. Visit https://hf.co/settings/tokens to\ncreate your access token and retry with:\n\n   >>> Pipeline.from_pretrained('{model_id}',\n   ...                          use_auth_token=YOUR_AUTH_TOKEN)\n\nIf this still does not work, it might be because the pipeline is gated:\nvisit https://hf.co/{model_id} to accept the user conditions.\"\"\"\n                )\n                return None\n\n        with open(config_yml, \"r\") as fp:\n            config = yaml.load(fp, Loader=yaml.SafeLoader)\n\n        # initialize pipeline\n        pipeline_name = config[\"pipeline\"][\"name\"]\n        if pipeline_name == 'pyannote.audio.pipelines.SpeakerDiarization':\n            pipeline_name = 'src.custom_pyannote.SpeakerDiarization'\n\n        Klass = get_class_by_name(\n            pipeline_name, default_module_name=\"pyannote.pipeline.blocks\"\n        )\n        params = config[\"pipeline\"].get(\"params\", {})\n        params.setdefault(\"use_auth_token\", use_auth_token)\n        if \"embedding\" in kwargs.keys():\n            if kwargs[\"embedding\"] is not None and not params[\"embedding\"] == kwargs[\"embedding\"]:\n                params[\"embedding\"] = kwargs[\"embedding\"]\n                print(\">>Embedding Type: {}\".format(kwargs[\"embedding\"]))\n\n        pipeline = Klass(**params)\n\n        # freeze  parameters\n        if \"freeze\" in config:\n            params = config[\"freeze\"]\n            pipeline.freeze(params)\n\n        if \"params\" in config:\n            pipeline.instantiate(config[\"params\"])\n\n        if hparams_file is not None:\n            pipeline.load_params(hparams_file)\n\n        if \"preprocessors\" in config:\n            preprocessors = {}\n            for key, preprocessor in config.get(\"preprocessors\", {}).items():\n\n                # preprocessors:\n                #    key:\n                #       name: package.module.ClassName\n                #       params:\n                #          param1: value1\n                #          param2: value2\n                if isinstance(preprocessor, dict):\n                    Klass = get_class_by_name(\n                        preprocessor[\"name\"], default_module_name=\"pyannote.audio\"\n                    )\n                    params = preprocessor.get(\"params\", {})\n                    preprocessors[key] = Klass(**params)\n                    continue\n\n                try:\n                    # preprocessors:\n                    #    key: /path/to/database.yml\n                    preprocessors[key] = FileFinder(database_yml=preprocessor)\n\n                except FileNotFoundError:\n                    # preprocessors:\n                    #    key: /path/to/{uri}.wav\n                    template = preprocessor\n                    preprocessors[key] = template\n\n            pipeline.preprocessors = preprocessors\n\n        # send pipeline to specified device\n        if \"device\" in config:\n            device = torch.device(config[\"device\"])\n            pipeline.to(device)\n\n        return pipeline\n\n    def __init__(self):\n        super().__init__()\n        self._models: Dict[str, Model] = OrderedDict()\n        self._inferences: Dict[str, BaseInference] = OrderedDict()\n\n    def __getattr__(self, name):\n        \"\"\"(Advanced) attribute getter\n\n        Adds support for Model and Inference attributes,\n        which are iterated over by Pipeline.to() method.\n\n        See pyannote.pipeline.Pipeline.__getattr__.\n        \"\"\"\n\n        if \"_models\" in self.__dict__:\n            _models = self.__dict__[\"_models\"]\n            if name in _models:\n                return _models[name]\n\n        if \"_inferences\" in self.__dict__:\n            _inferences = self.__dict__[\"_inferences\"]\n            if name in _inferences:\n                return _inferences[name]\n\n        return super().__getattr__(name)\n\n    def __setattr__(self, name, value):\n        \"\"\"(Advanced) attribute setter\n\n        Adds support for Model and Inference attributes,\n        which are iterated over by Pipeline.to() method.\n\n        See pyannote.pipeline.Pipeline.__setattr__.\n        \"\"\"\n\n        def remove_from(*dicts):\n            for d in dicts:\n                if name in d:\n                    del d[name]\n\n        _parameters = self.__dict__.get(\"_parameters\")\n        _instantiated = self.__dict__.get(\"_instantiated\")\n        _pipelines = self.__dict__.get(\"_pipelines\")\n        _models = self.__dict__.get(\"_models\")\n        _inferences = self.__dict__.get(\"_inferences\")\n\n        if isinstance(value, Model):\n            if _models is None:\n                msg = \"cannot assign models before Pipeline.__init__() call\"\n                raise AttributeError(msg)\n            remove_from(\n                self.__dict__, _inferences, _parameters, _instantiated, _pipelines\n            )\n            _models[name] = value\n            return\n\n        if isinstance(value, BaseInference):\n            if _inferences is None:\n                msg = \"cannot assign inferences before Pipeline.__init__() call\"\n                raise AttributeError(msg)\n            remove_from(self.__dict__, _models, _parameters, _instantiated, _pipelines)\n            _inferences[name] = value\n            return\n\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n\n        if name in self._models:\n            del self._models[name]\n\n        elif name in self._inferences:\n            del self._inferences[name]\n\n        else:\n            super().__delattr__(name)\n\n    @staticmethod\n    def setup_hook(file: AudioFile, hook: Optional[Callable] = None) -> Callable:\n        def noop(*args, **kwargs):\n            return\n\n        return partial(hook or noop, file=file)\n\n    def default_parameters(self):\n        raise NotImplementedError()\n\n    def classes(self) -> Union[List, Iterator]:\n        \"\"\"Classes returned by the pipeline\n\n        Returns\n        -------\n        classes : list of string or string iterator\n            Finite list of strings when classes are known in advance\n            (e.g. [\"MALE\", \"FEMALE\"] for gender classification), or\n            infinite string iterator when they depend on the file\n            (e.g. \"SPEAKER_00\", \"SPEAKER_01\", ... for speaker diarization)\n\n        Usage\n        -----\n        >>> from collections.abc import Iterator\n        >>> classes = pipeline.classes()\n        >>> if isinstance(classes, Iterator):  # classes depend on the input file\n        >>> if isinstance(classes, list):      # classes are known in advance\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def __call__(self, file: AudioFile, **kwargs):\n        if not self.instantiated:\n            # instantiate with default parameters when available\n            try:\n                default_parameters = self.default_parameters()\n            except NotImplementedError:\n                raise RuntimeError(\n                    \"A pipeline must be instantiated with `pipeline.instantiate(parameters)` before it can be applied.\"\n                )\n\n            try:\n                self.instantiate(default_parameters)\n            except ValueError:\n                raise RuntimeError(\n                    \"A pipeline must be instantiated with `pipeline.instantiate(paramaters)` before it can be applied. \"\n                    \"Tried to use parameters provided by `pipeline.default_parameters()` but those are not compatible. \"\n                )\n\n            warnings.warn(\n                f\"The pipeline has been automatically instantiated with {default_parameters}.\"\n            )\n\n        file = Audio.validate_file(file)\n\n        if hasattr(self, \"preprocessors\"):\n            file = ProtocolFile(file, lazy=self.preprocessors)\n\n        return self.apply(file, **kwargs)\n\n    def to(self, device):\n        \"\"\"Send pipeline to `device`\"\"\"\n\n        for _, pipeline in self._pipelines.items():\n            if hasattr(pipeline, \"to\"):\n                _ = pipeline.to(device)\n\n        for _, model in self._models.items():\n            _ = model.to(device)\n\n        for _, inference in self._inferences.items():\n            _ = inference.to(device)\n\n        return self", ""]}
{"filename": "src/custom_pyannote/__init__.py", "chunked_list": ["from .speaker_diarization import SpeakerDiarization\n\n__all__ = [\n    \"SpeakerDiarization\",\n]\n"]}
{"filename": "src/custom_pyannote/clustering.py", "chunked_list": ["# The MIT License (MIT)\n#\n# Copyright (c) 2021- CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Clustering pipelines\"\"\"\n\n\nimport random", "\nimport random\nfrom enum import Enum\nfrom typing import Tuple\n\nimport numpy as np\nfrom einops import rearrange\nfrom hmmlearn.hmm import GaussianHMM\nfrom pyannote.core import SlidingWindow, SlidingWindowFeature\nfrom pyannote.pipeline import Pipeline", "from pyannote.core import SlidingWindow, SlidingWindowFeature\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import (\n    Categorical,\n    Integer,\n    LogUniform,\n    ParamDict,\n    Uniform,\n)\nfrom scipy.cluster.hierarchy import fcluster, linkage", ")\nfrom scipy.cluster.hierarchy import fcluster, linkage\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.spatial.distance import cdist, pdist\n\nfrom pyannote.audio import Inference\nfrom pyannote.audio.core.io import AudioFile\nfrom pyannote.audio.pipelines.utils import oracle_segmentation\nfrom pyannote.audio.utils.permutation import permutate\n\ntry:\n    from finch import FINCH\n\n    FINCH_IS_AVAILABLE = True\n\nexcept ImportError:\n    FINCH_IS_AVAILABLE = False", "from pyannote.audio.utils.permutation import permutate\n\ntry:\n    from finch import FINCH\n\n    FINCH_IS_AVAILABLE = True\n\nexcept ImportError:\n    FINCH_IS_AVAILABLE = False\n", "\n\nclass BaseClustering(Pipeline):\n    def __init__(\n        self,\n        metric: str = \"cosine\",\n        max_num_embeddings: int = 1000,\n        constrained_assignment: bool = False,\n    ):\n\n        super().__init__()\n        self.metric = metric\n        self.max_num_embeddings = max_num_embeddings\n        self.constrained_assignment = constrained_assignment\n\n    def set_num_clusters(\n        self,\n        num_embeddings: int,\n        num_clusters: int = None,\n        min_clusters: int = None,\n        max_clusters: int = None,\n    ):\n\n        min_clusters = num_clusters or min_clusters or 1\n        min_clusters = max(1, min(num_embeddings, min_clusters))\n        max_clusters = num_clusters or max_clusters or num_embeddings\n        max_clusters = max(1, min(num_embeddings, max_clusters))\n\n        if min_clusters > max_clusters:\n            raise ValueError(\n                f\"min_clusters must be smaller than (or equal to) max_clusters \"\n                f\"(here: min_clusters={min_clusters:g} and max_clusters={max_clusters:g}).\"\n            )\n\n        if min_clusters == max_clusters:\n            num_clusters = min_clusters\n\n        return num_clusters, min_clusters, max_clusters\n\n    def filter_embeddings(\n        self,\n        embeddings: np.ndarray,\n        segmentations: SlidingWindowFeature = None,\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Filter NaN embeddings and downsample embeddings\n\n        Parameters\n        ----------\n        embeddings : (num_chunks, num_speakers, dimension) array\n            Sequence of embeddings.\n        segmentations : (num_chunks, num_frames, num_speakers) array\n            Binary segmentations.\n\n        Returns\n        -------\n        filtered_embeddings : (num_embeddings, dimension) array\n        chunk_idx : (num_embeddings, ) array\n        speaker_idx : (num_embeddings, ) array\n        \"\"\"\n        chunk_idx, speaker_idx = np.where(~np.any(np.isnan(embeddings), axis=2))\n\n        # sample max_num_embeddings embeddings\n        num_embeddings = len(chunk_idx)\n        if num_embeddings > self.max_num_embeddings:\n            indices = list(range(num_embeddings))\n            random.shuffle(indices)\n            indices = sorted(indices[: self.max_num_embeddings])\n            chunk_idx = chunk_idx[indices]\n            speaker_idx = speaker_idx[indices]\n\n        return embeddings[chunk_idx, speaker_idx], chunk_idx, speaker_idx\n\n    def constrained_argmax(self, soft_clusters: np.ndarray) -> np.ndarray:\n\n        soft_clusters = np.nan_to_num(soft_clusters, nan=np.nanmin(soft_clusters))\n        num_chunks, num_speakers, num_clusters = soft_clusters.shape\n        # num_chunks, num_speakers, num_clusters\n\n        hard_clusters = -2 * np.ones((num_chunks, num_speakers), dtype=np.int8)\n\n        for c, cost in enumerate(soft_clusters):\n            speakers, clusters = linear_sum_assignment(cost, maximize=True)\n            for s, k in zip(speakers, clusters):\n                hard_clusters[c, s] = k\n\n        return hard_clusters\n\n    def assign_embeddings(\n        self,\n        embeddings: np.ndarray,\n        train_chunk_idx: np.ndarray,\n        train_speaker_idx: np.ndarray,\n        train_clusters: np.ndarray,\n        constrained: bool = False,\n    ):\n        \"\"\"Assign embeddings to the closest centroid\n\n        Cluster centroids are computed as the average of the train embeddings\n        previously assigned to them.\n\n        Parameters\n        ----------\n        embeddings : (num_chunks, num_speakers, dimension)-shaped array\n            Complete set of embeddings.\n        train_chunk_idx : (num_embeddings,)-shaped array\n        train_speaker_idx : (num_embeddings,)-shaped array\n            Indices of subset of embeddings used for \"training\".\n        train_clusters : (num_embedding,)-shaped array\n            Clusters of the above subset\n        constrained : bool, optional\n            Use constrained_argmax, instead of (default) argmax.\n\n        Returns\n        -------\n        soft_clusters : (num_chunks, num_speakers, num_clusters)-shaped array\n        hard_clusters : (num_chunks, num_speakers)-shaped array\n        \"\"\"\n\n        # TODO: option to add a new (dummy) cluster in case num_clusters < max(frame_speaker_count)\n\n        num_clusters = np.max(train_clusters) + 1\n        num_chunks, num_speakers, dimension = embeddings.shape\n\n        train_embeddings = embeddings[train_chunk_idx, train_speaker_idx]\n\n        centroids = np.vstack(\n            [\n                np.mean(train_embeddings[train_clusters == k], axis=0)\n                for k in range(num_clusters)\n            ]\n        )\n\n        # compute distance between embeddings and clusters\n        e2k_distance = rearrange(\n            cdist(\n                rearrange(embeddings, \"c s d -> (c s) d\"),\n                centroids,\n                metric=self.metric,\n            ),\n            \"(c s) k -> c s k\",\n            c=num_chunks,\n            s=num_speakers,\n        )\n        soft_clusters = 2 - e2k_distance\n\n        # assign each embedding to the cluster with the most similar centroid\n        if constrained:\n            hard_clusters = self.constrained_argmax(soft_clusters)\n        else:\n            hard_clusters = np.argmax(soft_clusters, axis=2)\n\n        # TODO: add a flag to revert argmax for trainign subset\n        # hard_clusters[train_chunk_idx, train_speaker_idx] = train_clusters\n\n        return hard_clusters, soft_clusters\n\n    def __call__(\n        self,\n        embeddings: np.ndarray,\n        segmentations: SlidingWindowFeature = None,\n        num_clusters: int = None,\n        min_clusters: int = None,\n        max_clusters: int = None,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"Apply clustering\n\n        Parameters\n        ----------\n        embeddings : (num_chunks, num_speakers, dimension) array\n            Sequence of embeddings.\n        segmentations : (num_chunks, num_frames, num_speakers) array\n            Binary segmentations.\n        num_clusters : int, optional\n            Number of clusters, when known. Default behavior is to use\n            internal threshold hyper-parameter to decide on the number\n            of clusters.\n        min_clusters : int, optional\n            Minimum number of clusters. Has no effect when `num_clusters` is provided.\n        max_clusters : int, optional\n            Maximum number of clusters. Has no effect when `num_clusters` is provided.\n\n        Returns\n        -------\n        hard_clusters : (num_chunks, num_speakers) array\n            Hard cluster assignment (hard_clusters[c, s] = k means that sth speaker\n            of cth chunk is assigned to kth cluster)\n        soft_clusters : (num_chunks, num_speakers, num_clusters) array\n            Soft cluster assignment (the higher soft_clusters[c, s, k], the most likely\n            the sth speaker of cth chunk belongs to kth cluster)\n        \"\"\"\n\n        train_embeddings, train_chunk_idx, train_speaker_idx = self.filter_embeddings(\n            embeddings,\n            segmentations=segmentations,\n        )\n\n        num_embeddings, _ = train_embeddings.shape\n        num_clusters, min_clusters, max_clusters = self.set_num_clusters(\n            num_embeddings,\n            num_clusters=num_clusters,\n            min_clusters=min_clusters,\n            max_clusters=max_clusters,\n        )\n\n        if max_clusters < 2:\n            # do NOT apply clustering when min_clusters = max_clusters = 1\n            num_chunks, num_speakers, _ = embeddings.shape\n            hard_clusters = np.zeros((num_chunks, num_speakers), dtype=np.int8)\n            soft_clusters = np.ones((num_chunks, num_speakers, 1))\n            return hard_clusters, soft_clusters\n\n        train_clusters = self.cluster(\n            train_embeddings,\n            min_clusters,\n            max_clusters,\n            num_clusters=num_clusters,\n        )\n\n        hard_clusters, soft_clusters = self.assign_embeddings(\n            embeddings,\n            train_chunk_idx,\n            train_speaker_idx,\n            train_clusters,\n            constrained=self.constrained_assignment,\n        )\n\n        return hard_clusters, soft_clusters", "\n\nclass FINCHClustering(BaseClustering):\n    \"\"\"FINCH clustering\n\n    Parameters\n    ----------\n    metric : {\"cosine\", \"euclidean\", ...}, optional\n        Distance metric to use. Defaults to \"cosine\".\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"cosine\",\n        max_num_embeddings: int = np.inf,\n        constrained_assignment: bool = False,\n    ):\n\n        if not FINCH_IS_AVAILABLE:\n            raise ImportError(\n                \"'finch-clust' must be installed to use FINCH clustering. \"\n                \"Visit https://pypi.org/project/finch-clust/ for installation instructions.\"\n            )\n\n        super().__init__(\n            metric=metric,\n            max_num_embeddings=max_num_embeddings,\n            constrained_assignment=constrained_assignment,\n        )\n\n        self.threshold = Uniform(0.0, 2.0)  # assume unit-normalized embeddings\n        self.method = Categorical([\"average\", \"complete\", \"single\"])\n\n    def cluster(\n        self,\n        embeddings: np.ndarray,\n        min_clusters: int,\n        max_clusters: int,\n        num_clusters: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        embeddings : (num_embeddings, dimension) array\n            Embeddings\n        min_clusters : int\n            Minimum number of clusters\n        max_clusters : int\n            Maximum number of clusters\n        num_clusters : int, optional\n            Actual number of clusters. Default behavior is to estimate it based\n            on values provided for `min_clusters`,  `max_clusters`, and `threshold`.\n\n        Returns\n        -------\n        clusters : (num_embeddings, ) array\n            0-indexed cluster indices.\n        \"\"\"\n\n        num_embeddings, _ = embeddings.shape\n        if num_embeddings == 1:\n            return np.zeros((1,), dtype=np.uint8)\n\n        # apply FINCH clustering and keep (supposedly pure) penultimate partition\n        clusters, _, _ = FINCH(\n            embeddings,\n            initial_rank=None,\n            req_clust=None,\n            distance=self.metric,\n            ensure_early_exit=True,\n            verbose=False,\n        )\n\n        _, num_partitions = clusters.shape\n        if num_partitions < 2:\n            clusters = clusters[:, 0]\n        else:\n            clusters = clusters[:, -2]\n        num_clusters = np.max(clusters) + 1\n\n        # compute centroids\n        centroids = np.vstack(\n            [np.mean(embeddings[clusters == k], axis=0) for k in range(num_clusters)]\n        )\n\n        # perform agglomerative clustering on centroids\n        dendrogram = linkage(centroids, metric=self.metric, method=self.method)\n        klusters = fcluster(dendrogram, self.threshold, criterion=\"distance\") - 1\n\n        # update clusters\n        clusters = -clusters\n        for i, k in enumerate(klusters):\n            clusters[clusters == -i] = k\n\n        # TODO: handle min/max/num_clusters\n        # TODO: handle min_cluster_size\n\n        return clusters", "\n\nclass AgglomerativeClustering(BaseClustering):\n    \"\"\"Agglomerative clustering\n\n    Parameters\n    ----------\n    metric : {\"cosine\", \"euclidean\", ...}, optional\n        Distance metric to use. Defaults to \"cosine\".\n\n    Hyper-parameters\n    ----------------\n    method : {\"average\", \"centroid\", \"complete\", \"median\", \"single\", \"ward\"}\n        Linkage method.\n    threshold : float in range [0.0, 2.0]\n        Clustering threshold.\n    min_cluster_size : int in range [1, 20]\n        Minimum cluster size\n\n    Usage\n    -----\n    >>> clustering = AgglomerativeClustering(metric=\"cosine\")\n    >>> clustering.instantiate({\"method\": \"average\",\n    ...                         \"threshold\": 1.0,\n    ...                         \"min_cluster_size\": 1})\n    >>> clusters, _  = clustering(embeddings,           # shape\n    ...                           num_clusters=None,\n    ...                           min_clusters=None,\n    ...                           max_clusters=None)\n    where `embeddings` is a np.ndarray with shape (num_embeddings, embedding_dimension)\n    and `clusters` is a np.ndarray with shape (num_embeddings, )\n    \"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"cosine\",\n        max_num_embeddings: int = np.inf,\n        constrained_assignment: bool = False,\n    ):\n\n        super().__init__(\n            metric=metric,\n            max_num_embeddings=max_num_embeddings,\n            constrained_assignment=constrained_assignment,\n        )\n\n        self.threshold = Uniform(0.0, 2.0)  # assume unit-normalized embeddings\n        self.method = Categorical(\n            [\"average\", \"centroid\", \"complete\", \"median\", \"single\", \"ward\", \"weighted\"]\n        )\n\n        # minimum cluster size\n        self.min_cluster_size = Integer(1, 20)\n\n    def cluster(\n        self,\n        embeddings: np.ndarray,\n        min_clusters: int,\n        max_clusters: int,\n        num_clusters: int = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        embeddings : (num_embeddings, dimension) array\n            Embeddings\n        min_clusters : int\n            Minimum number of clusters\n        max_clusters : int\n            Maximum number of clusters\n        num_clusters : int, optional\n            Actual number of clusters. Default behavior is to estimate it based\n            on values provided for `min_clusters`,  `max_clusters`, and `threshold`.\n\n        Returns\n        -------\n        clusters : (num_embeddings, ) array\n            0-indexed cluster indices.\n        \"\"\"\n        num_embeddings, _ = embeddings.shape\n\n        # heuristic to reduce self.min_cluster_size when num_embeddings is very small\n        # (0.1 value is kind of arbitrary, though)\n        min_cluster_size = min(\n            self.min_cluster_size, max(1, round(0.1 * num_embeddings))\n        )\n\n        # linkage function will complain when there is just one embedding to cluster\n        if num_embeddings == 1:\n            return np.zeros((1,), dtype=np.uint8)\n\n        # centroid, median, and Ward method only support \"euclidean\" metric\n        # therefore we unit-normalize embeddings to somehow make them \"euclidean\"\n        if self.metric == \"cosine\" and self.method in [\"centroid\", \"median\", \"ward\"]:\n            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                embeddings /= np.linalg.norm(embeddings, axis=-1, keepdims=True)\n            dendrogram: np.ndarray = linkage(\n                embeddings, method=self.method, metric=\"euclidean\"\n            )\n\n        # other methods work just fine with any metric\n        else:\n            dendrogram: np.ndarray = linkage(\n                embeddings, method=self.method, metric=self.metric\n            )\n\n        # import matplotlib\n        # import matplotlib.pyplot as plt\n        # matplotlib.use('Agg')\n        # from sklearn.metrics.pairwise import cosine_similarity\n        # print(embeddings.shape)\n        # scr_mx = cosine_similarity(embeddings)\n        # plt.imshow(scr_mx, cmap='jet', interpolation='none')\n        # plt.savefig('mfa-conformer.png')\n        # plt.close('all')\n        \n        # apply the predefined threshold\n        clusters = fcluster(dendrogram, self.threshold, criterion=\"distance\") - 1\n\n        # split clusters into two categories based on their number of items:\n        # large clusters vs. small clusters\n        cluster_unique, cluster_counts = np.unique(\n            clusters,\n            return_counts=True,\n        )\n        large_clusters = cluster_unique[cluster_counts >= min_cluster_size]\n        num_large_clusters = len(large_clusters)\n\n        # force num_clusters to min_clusters in case the actual number is too small\n        if num_large_clusters < min_clusters:\n            num_clusters = min_clusters\n\n        # force num_clusters to max_clusters in case the actual number is too large\n        elif num_large_clusters > max_clusters:\n            num_clusters = max_clusters\n\n        if num_clusters is not None:\n\n            # switch stopping criterion from \"inter-cluster distance\" stopping to \"iteration index\"\n            _dendrogram = np.copy(dendrogram)\n            _dendrogram[:, 2] = np.arange(num_embeddings - 1)\n\n            best_iteration = num_embeddings - 1\n            best_num_large_clusters = 1\n\n            # traverse the dendrogram by going further and further away\n            # from the \"optimal\" threshold\n\n            for iteration in np.argsort(np.abs(dendrogram[:, 2] - self.threshold)):\n\n                # only consider iterations that might have resulted\n                # in changing the number of (large) clusters\n                new_cluster_size = _dendrogram[iteration, 3]\n                if new_cluster_size < min_cluster_size:\n                    continue\n\n                # estimate number of large clusters at considered iteration\n                clusters = fcluster(_dendrogram, iteration, criterion=\"distance\") - 1\n                cluster_unique, cluster_counts = np.unique(clusters, return_counts=True)\n                large_clusters = cluster_unique[cluster_counts >= min_cluster_size]\n                num_large_clusters = len(large_clusters)\n\n                # keep track of iteration that leads to the number of large clusters\n                # as close as possible to the target number of clusters.\n                if abs(num_large_clusters - num_clusters) < abs(\n                    best_num_large_clusters - num_clusters\n                ):\n                    best_iteration = iteration\n                    best_num_large_clusters = num_large_clusters\n\n                # stop traversing the dendrogram as soon as we found a good candidate\n                if num_large_clusters == num_clusters:\n                    break\n\n            # re-apply best iteration in case we did not find a perfect candidate\n            if best_num_large_clusters != num_clusters:\n                clusters = (\n                    fcluster(_dendrogram, best_iteration, criterion=\"distance\") - 1\n                )\n                cluster_unique, cluster_counts = np.unique(clusters, return_counts=True)\n                large_clusters = cluster_unique[cluster_counts >= min_cluster_size]\n                num_large_clusters = len(large_clusters)\n                print(\n                    f\"Found only {num_large_clusters} clusters. Using a smaller value than {min_cluster_size} for `min_cluster_size` might help.\"\n                )\n\n        if num_large_clusters == 0:\n            clusters[:] = 0\n            return clusters\n\n        small_clusters = cluster_unique[cluster_counts < min_cluster_size]\n        if len(small_clusters) == 0:\n            return clusters\n\n        # re-assign each small cluster to the most similar large cluster based on their respective centroids\n        large_centroids = np.vstack(\n            [\n                np.mean(embeddings[clusters == large_k], axis=0)\n                for large_k in large_clusters\n            ]\n        )\n        small_centroids = np.vstack(\n            [\n                np.mean(embeddings[clusters == small_k], axis=0)\n                for small_k in small_clusters\n            ]\n        )\n        centroids_cdist = cdist(large_centroids, small_centroids, metric=self.metric)\n        for small_k, large_k in enumerate(np.argmin(centroids_cdist, axis=0)):\n            clusters[clusters == small_clusters[small_k]] = large_clusters[large_k]\n\n        # re-number clusters from 0 to num_large_clusters\n        _, clusters = np.unique(clusters, return_inverse=True)\n        return clusters", "\n\nclass OracleClustering(BaseClustering):\n    \"\"\"Oracle clustering\"\"\"\n\n    def __call__(\n        self,\n        segmentations: SlidingWindowFeature = None,\n        file: AudioFile = None,\n        frames: SlidingWindow = None,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"Apply oracle clustering\n\n        Parameters\n        ----------\n        segmentations : (num_chunks, num_frames, num_speakers) array\n            Binary segmentations.\n        file : AudioFile\n        frames : SlidingWindow\n\n        Returns\n        -------\n        hard_clusters : (num_chunks, num_speakers) array\n            Hard cluster assignment (hard_clusters[c, s] = k means that sth speaker\n            of cth chunk is assigned to kth cluster)\n        soft_clusters : (num_chunks, num_speakers, num_clusters) array\n            Soft cluster assignment (the higher soft_clusters[c, s, k], the most likely\n            the sth speaker of cth chunk belongs to kth cluster)\n        \"\"\"\n\n        num_chunks, num_frames, num_speakers = segmentations.data.shape\n        window = segmentations.sliding_window\n\n        oracle_segmentations = oracle_segmentation(file, window, frames=frames)\n        #   shape: (num_chunks, num_frames, true_num_speakers)\n\n        file[\"oracle_segmentations\"] = oracle_segmentations\n\n        _, oracle_num_frames, num_clusters = oracle_segmentations.data.shape\n\n        segmentations = segmentations.data[:, : min(num_frames, oracle_num_frames)]\n        oracle_segmentations = oracle_segmentations.data[\n            :, : min(num_frames, oracle_num_frames)\n        ]\n\n        hard_clusters = -2 * np.ones((num_chunks, num_speakers), dtype=np.int8)\n        soft_clusters = np.zeros((num_chunks, num_speakers, num_clusters))\n        for c, (segmentation, oracle) in enumerate(\n            zip(segmentations, oracle_segmentations)\n        ):\n            _, (permutation, *_) = permutate(oracle[np.newaxis], segmentation)\n            for j, i in enumerate(permutation):\n                if i is None:\n                    continue\n                hard_clusters[c, i] = j\n                soft_clusters[c, i, j] = 1.0\n\n        return hard_clusters, soft_clusters", "\n\nclass HiddenMarkovModelClustering(BaseClustering):\n    \"\"\"Hidden Markov Model with Gaussian states\"\"\"\n\n    def __init__(\n        self,\n        metric: str = \"cosine\",\n        constrained_assignment: bool = False,\n    ):\n\n        if metric not in [\"euclidean\", \"cosine\"]:\n            raise ValueError(\"`metric` must be one of {'cosine', 'euclidean'}\")\n\n        super().__init__(\n            metric=metric,\n            constrained_assignment=constrained_assignment,\n        )\n\n        self.single_cluster_detection = ParamDict(\n            quantile=LogUniform(1e-3, 1e-1),\n            threshold=Uniform(0.0, 2.0),\n        )\n\n        self.covariance_type = Categorical([\"spherical\", \"diag\", \"full\", \"tied\"])\n        self.threshold = Uniform(0.0, 2.0)\n\n    def filter_embeddings(\n        self, embeddings: np.ndarray, segmentations: SlidingWindowFeature\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Parameters\n        ----------\n        embeddings : (num_chunks, num_speakers, dimension) array\n            Sequence of embeddings.\n        segmentations : (num_chunks, num_frames, num_speakers) array\n            Binary segmentations.\n\n        Returns\n        -------\n        train_embeddings : (num_steps, dimension) array\n        chunk_idx : (num_steps, ) array\n        speaker_idx : (num_steps, ) array\n\n        \"\"\"\n        num_chunks, _, _ = embeddings.shape\n\n        # focus on center of each chunk\n        duration = segmentations.sliding_window.duration\n        step = segmentations.sliding_window.step\n\n        ratio = 0.5 * (duration - step) / duration\n        center_segmentations = Inference.trim(segmentations, warm_up=(ratio, ratio))\n        #   shape: num_chunks, num_center_frames, num_speakers\n\n        # number of frames during which speakers are active\n        # in the center of the chunk\n        num_active_frames: np.ndarray = np.sum(center_segmentations.data, axis=1)\n        #   shape: (num_chunks, num_speakers)\n\n        priors = num_active_frames / (\n            np.sum(num_active_frames, axis=1, keepdims=True) + 1e-8\n        )\n        #   shape: (num_chunks, local_num_speakers)\n\n        speaker_idx = np.argmax(priors, axis=1)\n        # (num_chunks, )\n\n        # TODO: generate alternative sequences that only differs from train_embeddings\n        # in regions where there is overlap.\n\n        train_embeddings = embeddings[range(num_chunks), speaker_idx]\n        # (num_chunks, dimension)\n\n        # remove chunks with one of the following property:\n        # * there is no active speaker in the center of the chunk\n        # * embedding extraction has failed for the most active speaker in the center of the chunk\n        center_is_non_speech = np.max(num_active_frames, axis=1) == 0.0\n        embedding_is_invalid = np.any(np.isnan(train_embeddings), axis=1)\n        chunk_idx = np.where(~(embedding_is_invalid | center_is_non_speech))[0]\n        # (num_chunks, )\n\n        return (train_embeddings[chunk_idx], chunk_idx, speaker_idx[chunk_idx])\n\n    def fit_hmm(self, n_components, train_embeddings):\n\n        hmm = GaussianHMM(\n            n_components=n_components,\n            covariance_type=self.covariance_type,\n            n_iter=100,\n            random_state=42,\n            implementation=\"log\",\n            verbose=False,\n        )\n        hmm.fit(train_embeddings)\n\n        return hmm\n\n    def cluster(\n        self,\n        embeddings: np.ndarray,\n        min_clusters: int,\n        max_clusters: int,\n        num_clusters: int = None,\n    ):\n\n        num_embeddings = len(embeddings)\n\n        # FIXME\n        if max_clusters == num_embeddings:\n            max_clusters = min(max_clusters, 20)\n\n        if self.metric == \"cosine\":\n            # unit-normalize embeddings to somehow make them \"euclidean\"\n            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n                euclidean_embeddings = embeddings / np.linalg.norm(\n                    embeddings, axis=-1, keepdims=True\n                )\n        elif self.metric == \"euclidean\":\n            euclidean_embeddings = embeddings\n\n        # when the number of clusters is provided, fit a HMM with\n        # that many states and return the decoded sequence of states\n        if num_clusters is not None:\n            hmm = self.fit_hmm(num_clusters, euclidean_embeddings)\n\n            try:\n                train_clusters = hmm.predict(euclidean_embeddings)\n            except ValueError:\n                # ValueError: startprob_ must sum to 1 (got nan)\n                # TODO: display a warning that something went wrong\n                train_clusters = np.zeros((num_embeddings,), dtype=np.int8)\n\n            return train_clusters\n\n        # heuristic for detecting cases where there is just one large cluster\n        # (and a few meaningless outliers)\n        if min_clusters == 1:\n\n            # Example with quantile = 1% and threshold = 0.4:\n            # if 99% (100% - 1%) of pairwise distance are smaller than 0.4,\n            # then we assume that the others are outliers and return one cluster\n            if (\n                np.quantile(\n                    pdist(euclidean_embeddings, metric=\"euclidean\"),\n                    1.0 - self.single_cluster_detection[\"quantile\"],\n                )\n                < self.single_cluster_detection[\"threshold\"]\n            ):\n\n                return np.zeros((num_embeddings,), dtype=np.int8)\n\n            # otherwise, we make sure to return at least 2 clusters\n            min_clusters = max(2, min_clusters)\n            max_clusters = max(2, max_clusters)\n\n        # fit a HMM with increasing number of states and stop adding\n        # when the distance between the two closest states\n        #  - either no longer increases\n        #  - or no longer goes above a threshold\n        # the selected number of states is the last one for which the\n        # criterion goes above {threshold}.\n\n        # THIS IS A TERRIBLE CRITERION THAT NEEDS TO BE FIXED\n\n        history = [-np.inf]\n        patience = min(3, max_clusters - min_clusters)\n        num_clusters = min_clusters\n\n        for n_components in range(min_clusters, max_clusters + 1):\n\n            hmm = self.fit_hmm(n_components, euclidean_embeddings)\n            try:\n                train_clusters = hmm.predict(euclidean_embeddings)\n            except ValueError:  # ValueError: startprob_ must sum to 1 (got nan)\n                # stop adding states as there too many and not enough\n                # training data to train it in a reliable manner.\n                break\n\n            # stop early if too few states were found\n            if len(np.unique(train_clusters)) < n_components:\n                break\n\n            # compute distance between the two closest centroids\n            centroids = np.vstack(\n                [\n                    np.mean(embeddings[train_clusters == k], axis=0)\n                    for k in range(n_components)\n                ]\n            )\n            centroids_pdist = pdist(centroids, metric=self.metric)\n            current_criterion = np.min(centroids_pdist)\n\n            increasing = current_criterion > max(history)\n            big_enough = current_criterion > self.threshold\n\n            if increasing or big_enough:\n                num_clusters = n_components\n\n            elif n_components == num_clusters + patience:\n                break\n\n            history.append(current_criterion)\n\n        hmm = self.fit_hmm(num_clusters, euclidean_embeddings)\n        try:\n            train_clusters = hmm.predict(euclidean_embeddings)\n        except ValueError:\n            # ValueError: startprob_ must sum to 1 (got nan)\n            train_clusters = np.zeros((num_embeddings,), dtype=np.int8)\n\n        return train_clusters", "\n\nclass Clustering(Enum):\n    AgglomerativeClustering = AgglomerativeClustering\n    FINCHClustering = FINCHClustering\n    HiddenMarkovModelClustering = HiddenMarkovModelClustering\n    OracleClustering = OracleClustering\n"]}
