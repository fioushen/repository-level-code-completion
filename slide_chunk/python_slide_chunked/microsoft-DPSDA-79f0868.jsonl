{"filename": "main.py", "chunked_list": ["import argparse\nimport logging\nimport os\nimport numpy as np\nimport imageio\nfrom torchvision.utils import make_grid\nimport torch\nfrom dpsda.logging import setup_logging\nfrom dpsda.data_loader import load_data\nfrom dpsda.feature_extractor import extract_features", "from dpsda.data_loader import load_data\nfrom dpsda.feature_extractor import extract_features\nfrom dpsda.metrics import make_fid_stats\nfrom dpsda.metrics import compute_fid\nfrom dpsda.dp_counter import dp_nn_histogram\nfrom dpsda.arg_utils import str2bool\nfrom apis import get_api_class_from_name\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--api',\n        type=str,\n        required=True,\n        choices=['DALLE', 'stable_diffusion', 'improved_diffusion'],\n        help='Which foundation model API to use')\n    parser.add_argument(\n        '--plot_images',\n        type=str2bool,\n        default=True,\n        help='Whether to save generated images in PNG files')\n    parser.add_argument(\n        '--data_checkpoint_path',\n        type=str,\n        default=\"\",\n        help='Path to the data checkpoint')\n    parser.add_argument(\n        '--data_checkpoint_step',\n        type=int,\n        default=-1,\n        help='Iteration of the data checkpoint')\n    parser.add_argument(\n        '--num_samples_schedule',\n        type=str,\n        default='50000,'*9 + '50000',\n        help='Number of samples to generate at each iteration')\n    parser.add_argument(\n        '--variation_degree_schedule',\n        type=str,\n        default='0,'*9 + '0',\n        help='Variation degree at each iteration')\n    parser.add_argument(\n        '--num_fid_samples',\n        type=int,\n        default=50000,\n        help='Number of generated samples to compute FID')\n    parser.add_argument(\n        '--num_private_samples',\n        type=int,\n        default=50000,\n        help='Number of private samples to load')\n    parser.add_argument(\n        '--noise_multiplier',\n        type=float,\n        default=0.0,\n        help='Noise multiplier for DP NN histogram')\n    parser.add_argument(\n        '--lookahead_degree',\n        type=int,\n        default=0,\n        help=('Lookahead degree for computing distances between private and '\n              'generated images'))\n    parser.add_argument(\n        '--feature_extractor',\n        type=str,\n        default='clip_vit_b_32',\n        choices=['inception_v3', 'clip_vit_b_32', 'original'],\n        help='Which image feature extractor to use')\n    parser.add_argument(\n        '--num_nearest_neighbor',\n        type=int,\n        default=1,\n        help='Number of nearest neighbors to find in DP NN histogram')\n    parser.add_argument(\n        '--nn_mode',\n        type=str,\n        default='L2',\n        choices=['L2', 'IP'],\n        help='Which distance metric to use in DP NN histogram')\n    parser.add_argument(\n        '--private_image_size',\n        type=int,\n        default=1024,\n        help='Size of private images')\n    parser.add_argument(\n        '--tmp_folder',\n        type=str,\n        default='result/tmp',\n        help='Temporary folder for storing intermediate results')\n    parser.add_argument(\n        '--result_folder',\n        type=str,\n        default='result',\n        help='Folder for storing results')\n    parser.add_argument(\n        '--data_folder',\n        type=str,\n        required=True,\n        help='Folder that contains the private images')\n    parser.add_argument(\n        '--count_threshold',\n        type=float,\n        default=0.0,\n        help='Threshold for DP NN histogram')\n    parser.add_argument(\n        '--compute_fid',\n        type=str2bool,\n        default=True,\n        help='Whether to compute FID')\n    parser.add_argument(\n        '--fid_dataset_name',\n        type=str,\n        default='customized_dataset',\n        help=('Name of the dataset for computing FID against. If '\n              'fid_dataset_name and fid_dataset_split in combination are one '\n              'of the precomputed datasets in '\n              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n              ' then the precomputed statistics will be used. Otherwise, the '\n              'statistics will be computed using the private samples and saved'\n              ' with fid_dataset_name and fid_dataset_split for future use.'))\n    parser.add_argument(\n        '--fid_dataset_split',\n        type=str,\n        default='train',\n        help=('Split of the dataset for computing FID against. If '\n              'fid_dataset_name and fid_dataset_split in combination are one '\n              'of the precomputed datasets in '\n              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n              ' then the precomputed statistics will be used. Otherwise, the '\n              'statistics will be computed using the private samples and saved'\n              ' with fid_dataset_name and fid_dataset_split for future use.'))\n    parser.add_argument(\n        '--fid_model_name',\n        type=str,\n        default='inception_v3',\n        choices=['inception_v3', 'clip_vit_b_32'],\n        help='Which embedding network to use for computing FID')\n    parser.add_argument(\n        '--make_fid_stats',\n        type=str2bool,\n        default=True,\n        help='Whether to compute FID stats for the private samples')\n    parser.add_argument(\n        '--data_loading_batch_size',\n        type=int,\n        default=100,\n        help='Batch size for loading private samples')\n    parser.add_argument(\n        '--feature_extractor_batch_size',\n        type=int,\n        default=500,\n        help='Batch size for feature extraction')\n    parser.add_argument(\n        '--fid_batch_size',\n        type=int,\n        default=500,\n        help='Batch size for computing FID')\n    parser.add_argument(\n        '--gen_class_cond',\n        type=str2bool,\n        default=False,\n        help='Whether to generate class labels')\n    parser.add_argument(\n        '--initial_prompt',\n        action='append',\n        type=str,\n        help='Initial prompt for image generation. It can be specified '\n             'multiple times to provide a list of prompts. If the API accepts '\n             'prompts, the initial samples will be generated with these '\n             'prompts')\n    parser.add_argument(\n        '--image_size',\n        type=str,\n        default='1024x1024',\n        help='Size of generated images in the format of HxW')\n    args, api_args = parser.parse_known_args()\n\n    args.num_samples_schedule = list(map(\n        int, args.num_samples_schedule.split(',')))\n    variation_degree_type = (float if '.' in args.variation_degree_schedule\n                             else int)\n    args.variation_degree_schedule = list(map(\n        variation_degree_type, args.variation_degree_schedule.split(',')))\n\n    if len(args.num_samples_schedule) != len(args.variation_degree_schedule):\n        raise ValueError('The length of num_samples_schedule and '\n                         'variation_degree_schedule should be the same')\n\n    api_class = get_api_class_from_name(args.api)\n    api = api_class.from_command_line_args(api_args)\n\n    return args, api", "\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--api',\n        type=str,\n        required=True,\n        choices=['DALLE', 'stable_diffusion', 'improved_diffusion'],\n        help='Which foundation model API to use')\n    parser.add_argument(\n        '--plot_images',\n        type=str2bool,\n        default=True,\n        help='Whether to save generated images in PNG files')\n    parser.add_argument(\n        '--data_checkpoint_path',\n        type=str,\n        default=\"\",\n        help='Path to the data checkpoint')\n    parser.add_argument(\n        '--data_checkpoint_step',\n        type=int,\n        default=-1,\n        help='Iteration of the data checkpoint')\n    parser.add_argument(\n        '--num_samples_schedule',\n        type=str,\n        default='50000,'*9 + '50000',\n        help='Number of samples to generate at each iteration')\n    parser.add_argument(\n        '--variation_degree_schedule',\n        type=str,\n        default='0,'*9 + '0',\n        help='Variation degree at each iteration')\n    parser.add_argument(\n        '--num_fid_samples',\n        type=int,\n        default=50000,\n        help='Number of generated samples to compute FID')\n    parser.add_argument(\n        '--num_private_samples',\n        type=int,\n        default=50000,\n        help='Number of private samples to load')\n    parser.add_argument(\n        '--noise_multiplier',\n        type=float,\n        default=0.0,\n        help='Noise multiplier for DP NN histogram')\n    parser.add_argument(\n        '--lookahead_degree',\n        type=int,\n        default=0,\n        help=('Lookahead degree for computing distances between private and '\n              'generated images'))\n    parser.add_argument(\n        '--feature_extractor',\n        type=str,\n        default='clip_vit_b_32',\n        choices=['inception_v3', 'clip_vit_b_32', 'original'],\n        help='Which image feature extractor to use')\n    parser.add_argument(\n        '--num_nearest_neighbor',\n        type=int,\n        default=1,\n        help='Number of nearest neighbors to find in DP NN histogram')\n    parser.add_argument(\n        '--nn_mode',\n        type=str,\n        default='L2',\n        choices=['L2', 'IP'],\n        help='Which distance metric to use in DP NN histogram')\n    parser.add_argument(\n        '--private_image_size',\n        type=int,\n        default=1024,\n        help='Size of private images')\n    parser.add_argument(\n        '--tmp_folder',\n        type=str,\n        default='result/tmp',\n        help='Temporary folder for storing intermediate results')\n    parser.add_argument(\n        '--result_folder',\n        type=str,\n        default='result',\n        help='Folder for storing results')\n    parser.add_argument(\n        '--data_folder',\n        type=str,\n        required=True,\n        help='Folder that contains the private images')\n    parser.add_argument(\n        '--count_threshold',\n        type=float,\n        default=0.0,\n        help='Threshold for DP NN histogram')\n    parser.add_argument(\n        '--compute_fid',\n        type=str2bool,\n        default=True,\n        help='Whether to compute FID')\n    parser.add_argument(\n        '--fid_dataset_name',\n        type=str,\n        default='customized_dataset',\n        help=('Name of the dataset for computing FID against. If '\n              'fid_dataset_name and fid_dataset_split in combination are one '\n              'of the precomputed datasets in '\n              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n              ' then the precomputed statistics will be used. Otherwise, the '\n              'statistics will be computed using the private samples and saved'\n              ' with fid_dataset_name and fid_dataset_split for future use.'))\n    parser.add_argument(\n        '--fid_dataset_split',\n        type=str,\n        default='train',\n        help=('Split of the dataset for computing FID against. If '\n              'fid_dataset_name and fid_dataset_split in combination are one '\n              'of the precomputed datasets in '\n              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n              ' then the precomputed statistics will be used. Otherwise, the '\n              'statistics will be computed using the private samples and saved'\n              ' with fid_dataset_name and fid_dataset_split for future use.'))\n    parser.add_argument(\n        '--fid_model_name',\n        type=str,\n        default='inception_v3',\n        choices=['inception_v3', 'clip_vit_b_32'],\n        help='Which embedding network to use for computing FID')\n    parser.add_argument(\n        '--make_fid_stats',\n        type=str2bool,\n        default=True,\n        help='Whether to compute FID stats for the private samples')\n    parser.add_argument(\n        '--data_loading_batch_size',\n        type=int,\n        default=100,\n        help='Batch size for loading private samples')\n    parser.add_argument(\n        '--feature_extractor_batch_size',\n        type=int,\n        default=500,\n        help='Batch size for feature extraction')\n    parser.add_argument(\n        '--fid_batch_size',\n        type=int,\n        default=500,\n        help='Batch size for computing FID')\n    parser.add_argument(\n        '--gen_class_cond',\n        type=str2bool,\n        default=False,\n        help='Whether to generate class labels')\n    parser.add_argument(\n        '--initial_prompt',\n        action='append',\n        type=str,\n        help='Initial prompt for image generation. It can be specified '\n             'multiple times to provide a list of prompts. If the API accepts '\n             'prompts, the initial samples will be generated with these '\n             'prompts')\n    parser.add_argument(\n        '--image_size',\n        type=str,\n        default='1024x1024',\n        help='Size of generated images in the format of HxW')\n    args, api_args = parser.parse_known_args()\n\n    args.num_samples_schedule = list(map(\n        int, args.num_samples_schedule.split(',')))\n    variation_degree_type = (float if '.' in args.variation_degree_schedule\n                             else int)\n    args.variation_degree_schedule = list(map(\n        variation_degree_type, args.variation_degree_schedule.split(',')))\n\n    if len(args.num_samples_schedule) != len(args.variation_degree_schedule):\n        raise ValueError('The length of num_samples_schedule and '\n                         'variation_degree_schedule should be the same')\n\n    api_class = get_api_class_from_name(args.api)\n    api = api_class.from_command_line_args(api_args)\n\n    return args, api", "\n\ndef log_samples(samples, additional_info, folder, plot_images):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    np.savez(\n        os.path.join(folder, 'samples.npz'),\n        samples=samples,\n        additional_info=additional_info)\n    if plot_images:\n        for i in range(samples.shape[0]):\n            imageio.imwrite(os.path.join(folder, f'{i}.png'), samples[i])", "\n\ndef load_samples(path):\n    data = np.load(path)\n    samples = data['samples']\n    additional_info = data['additional_info']\n    return samples, additional_info\n\n\ndef log_count(count, clean_count, path):\n    dirname = os.path.dirname(path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n    np.savez(path, count=count, clean_count=clean_count)", "\ndef log_count(count, clean_count, path):\n    dirname = os.path.dirname(path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n    np.savez(path, count=count, clean_count=clean_count)\n\n\ndef round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)", "def round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\n\ndef visualize(samples, packed_samples, count, folder, suffix=''):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    samples = samples.transpose((0, 3, 1, 2))\n    packed_samples = packed_samples.transpose((0, 1, 4, 2, 3))\n\n    ids = np.argsort(count)[::-1][:5]\n    print(count[ids])\n    vis_samples = []\n    for i in range(len(ids)):\n        vis_samples.append(samples[ids[i]])\n        for j in range(packed_samples.shape[1]):\n            vis_samples.append(packed_samples[ids[i]][j])\n    vis_samples = np.stack(vis_samples)\n    vis_samples = make_grid(\n        torch.Tensor(vis_samples),\n        nrow=packed_samples.shape[1] + 1).numpy().transpose((1, 2, 0))\n    vis_samples = round_to_uint8(vis_samples)\n    imageio.imsave(\n        os.path.join(folder, f'visualize_top_{suffix}.png'), vis_samples)\n\n    ids = np.argsort(count)[:5]\n    print(count[ids])\n    vis_samples = []\n    for i in range(len(ids)):\n        vis_samples.append(samples[ids[i]])\n        for j in range(packed_samples.shape[1]):\n            vis_samples.append(packed_samples[ids[i]][j])\n    vis_samples = np.stack(vis_samples)\n    vis_samples = make_grid(\n        torch.Tensor(vis_samples),\n        nrow=packed_samples.shape[1] + 1).numpy().transpose((1, 2, 0))\n    vis_samples = round_to_uint8(vis_samples)\n    imageio.imsave(\n        os.path.join(folder, f'visualize_bottom_{suffix}.png'), vis_samples)", "\n\ndef log_fid(folder, fid, t):\n    with open(os.path.join(folder, 'fid.csv'), 'a') as f:\n        f.write(f'{t} {fid}\\n')\n\n\ndef main():\n    args, api = parse_args()\n    if os.path.exists(args.result_folder):\n        raise RuntimeError(f'{args.result_folder} exists')\n    os.makedirs(args.result_folder)\n    setup_logging(os.path.join(args.result_folder, 'log.log'))\n    logging.info(f'config: {args}')\n    logging.info(f'API config: {api.args}')\n\n    all_private_samples, all_private_labels = load_data(\n        data_dir=args.data_folder,\n        batch_size=args.data_loading_batch_size,\n        image_size=args.private_image_size,\n        class_cond=args.gen_class_cond,\n        num_private_samples=args.num_private_samples)\n\n    private_classes = list(sorted(set(list(all_private_labels))))\n    private_num_classes = len(private_classes)\n    logging.info(f'Private_num_classes: {private_num_classes}')\n\n    logging.info('Extracting features')\n    all_private_features = extract_features(\n        data=all_private_samples,\n        tmp_folder=args.tmp_folder,\n        model_name=args.feature_extractor,\n        res=args.private_image_size,\n        batch_size=args.feature_extractor_batch_size)\n    logging.info(f'all_private_features.shape: {all_private_features.shape}')\n\n    if args.make_fid_stats:\n        logging.info('Computing FID stats')\n        make_fid_stats(\n            samples=all_private_samples,\n            dataset=args.fid_dataset_name,\n            dataset_res=args.private_image_size,\n            dataset_split=args.fid_dataset_split,\n            tmp_folder=args.tmp_folder,\n            model_name=args.fid_model_name,\n            batch_size=args.fid_batch_size)\n\n    # Generating initial samples.\n    if args.data_checkpoint_path != '':\n        logging.info(\n            f'Loading data checkpoint from {args.data_checkpoint_path}')\n        samples, additional_info = load_samples(args.data_checkpoint_path)\n        if args.data_checkpoint_step < 0:\n            raise ValueError('data_checkpoint_step should be >= 0')\n        start_t = args.data_checkpoint_step + 1\n    else:\n        logging.info('Generating initial samples')\n        samples, additional_info = api.image_random_sampling(\n            prompts=args.initial_prompt,\n            num_samples=args.num_samples_schedule[0],\n            size=args.image_size)\n        log_samples(\n            samples=samples,\n            additional_info=additional_info,\n            folder=f'{args.result_folder}/{0}',\n            plot_images=args.plot_images)\n        if args.data_checkpoint_step >= 0:\n            logging.info('Ignoring data_checkpoint_step')\n        start_t = 1\n\n    if args.compute_fid:\n        logging.info('Computing FID')\n        fid = compute_fid(\n            samples=samples,\n            tmp_folder=args.tmp_folder,\n            num_fid_samples=args.num_fid_samples,\n            dataset_res=args.private_image_size,\n            dataset=args.fid_dataset_name,\n            dataset_split=args.fid_dataset_split,\n            model_name=args.fid_model_name,\n            batch_size=args.fid_batch_size)\n        logging.info(f'fid={fid}')\n        log_fid(args.result_folder, fid, 0)\n\n    for t in range(start_t, len(args.num_samples_schedule)):\n        logging.info(f't={t}')\n        assert samples.shape[0] % private_num_classes == 0\n        num_samples_per_class = samples.shape[0] // private_num_classes\n\n        if args.lookahead_degree == 0:\n            packed_samples = np.expand_dims(samples, axis=1)\n        else:\n            logging.info('Running image variation')\n            packed_samples = api.image_variation(\n                images=samples,\n                additional_info=additional_info,\n                num_variations_per_image=args.lookahead_degree,\n                size=args.image_size,\n                variation_degree=args.variation_degree_schedule[t])\n\n        packed_features = []\n        logging.info('Running feature extraction')\n        for i in range(packed_samples.shape[1]):\n            sub_packed_features = extract_features(\n                data=packed_samples[:, i],\n                tmp_folder=args.tmp_folder,\n                model_name=args.feature_extractor,\n                res=args.private_image_size,\n                batch_size=args.feature_extractor_batch_size)\n            logging.info(\n                f'sub_packed_features.shape: {sub_packed_features.shape}')\n            packed_features.append(sub_packed_features)\n        packed_features = np.mean(packed_features, axis=0)\n\n        logging.info('Computing histogram')\n        count = []\n        for class_i, class_ in enumerate(private_classes):\n            sub_count, sub_clean_count = dp_nn_histogram(\n                public_features=packed_features[\n                    num_samples_per_class * class_i:\n                    num_samples_per_class * (class_i + 1)],\n                private_features=all_private_features[\n                    all_private_labels == class_],\n                noise_multiplier=args.noise_multiplier,\n                num_nearest_neighbor=args.num_nearest_neighbor,\n                mode=args.nn_mode,\n                threshold=args.count_threshold)\n            log_count(\n                sub_count,\n                sub_clean_count,\n                f'{args.result_folder}/{t}/count_class{class_}.npz')\n            count.append(sub_count)\n        count = np.concatenate(count)\n        for class_i, class_ in enumerate(private_classes):\n            visualize(\n                samples=samples[\n                    num_samples_per_class * class_i:\n                    num_samples_per_class * (class_i + 1)],\n                packed_samples=packed_samples[\n                    num_samples_per_class * class_i:\n                    num_samples_per_class * (class_i + 1)],\n                count=count[\n                    num_samples_per_class * class_i:\n                    num_samples_per_class * (class_i + 1)],\n                folder=f'{args.result_folder}/{t}',\n                suffix=f'class{class_}')\n\n        logging.info('Generating new indices')\n        assert args.num_samples_schedule[t] % private_num_classes == 0\n        new_num_samples_per_class = (\n            args.num_samples_schedule[t] // private_num_classes)\n        new_indices = []\n        for class_i in private_classes:\n            sub_count = count[\n                num_samples_per_class * class_i:\n                num_samples_per_class * (class_i + 1)]\n            sub_new_indices = np.random.choice(\n                np.arange(num_samples_per_class * class_i,\n                          num_samples_per_class * (class_i + 1)),\n                size=new_num_samples_per_class,\n                p=sub_count / np.sum(sub_count))\n            new_indices.append(sub_new_indices)\n        new_indices = np.concatenate(new_indices)\n        new_samples = samples[new_indices]\n        new_additional_info = additional_info[new_indices]\n\n        logging.info('Generating new samples')\n        new_new_samples = api.image_variation(\n            images=new_samples,\n            additional_info=new_additional_info,\n            num_variations_per_image=1,\n            size=args.image_size,\n            variation_degree=args.variation_degree_schedule[t])\n        new_new_samples = np.squeeze(new_new_samples, axis=1)\n        new_new_additional_info = new_additional_info\n\n        if args.compute_fid:\n            logging.info('Computing FID')\n            new_new_fid = compute_fid(\n                new_new_samples,\n                tmp_folder=args.tmp_folder,\n                num_fid_samples=args.num_fid_samples,\n                dataset_res=args.private_image_size,\n                dataset=args.fid_dataset_name,\n                dataset_split=args.fid_dataset_split,\n                model_name=args.fid_model_name,\n                batch_size=args.fid_batch_size)\n            logging.info(f'fid={new_new_fid}')\n            log_fid(args.result_folder, new_new_fid, t)\n\n        samples = new_new_samples\n        additional_info = new_new_additional_info\n        log_samples(\n            samples=samples,\n            additional_info=additional_info,\n            folder=f'{args.result_folder}/{t}',\n            plot_images=args.plot_images)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "apis/stable_diffusion_api.py", "chunked_list": ["import torch\nimport torchvision.transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\nfrom .api import API\nfrom dpsda.pytorch_utils import dev", "from .api import API\nfrom dpsda.pytorch_utils import dev\n\n\ndef _round_to_uint8(image):\n    return np.around(np.clip(image * 255, a_min=0, a_max=255)).astype(np.uint8)\n\n\nclass StableDiffusionAPI(API):\n    def __init__(self, random_sampling_checkpoint,\n                 random_sampling_guidance_scale,\n                 random_sampling_num_inference_steps,\n                 random_sampling_batch_size,\n                 variation_checkpoint,\n                 variation_guidance_scale,\n                 variation_num_inference_steps,\n                 variation_batch_size,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._random_sampling_checkpoint = random_sampling_checkpoint\n        self._random_sampling_guidance_scale = random_sampling_guidance_scale\n        self._random_sampling_num_inference_steps = \\\n            random_sampling_num_inference_steps\n        self._random_sampling_batch_size = random_sampling_batch_size\n\n        self._random_sampling_pipe = StableDiffusionPipeline.from_pretrained(\n            self._random_sampling_checkpoint, torch_dtype=torch.float16)\n        self._random_sampling_pipe.safety_checker = None\n        self._random_sampling_pipe = self._random_sampling_pipe.to(dev())\n\n        self._variation_checkpoint = variation_checkpoint\n        self._variation_guidance_scale = variation_guidance_scale\n        self._variation_num_inference_steps = variation_num_inference_steps\n        self._variation_batch_size = variation_batch_size\n\n        self._variation_pipe = \\\n            StableDiffusionImg2ImgPipeline.from_pretrained(\n                self._variation_checkpoint,\n                torch_dtype=torch.float16)\n        self._variation_pipe.safety_checker = None\n        self._variation_pipe = self._variation_pipe.to(dev())\n\n    @staticmethod\n    def command_line_parser():\n        parser = super(\n            StableDiffusionAPI, StableDiffusionAPI).command_line_parser()\n        parser.add_argument(\n            '--random_sampling_checkpoint',\n            type=str,\n            required=True,\n            help='The path to the checkpoint for random sampling API')\n        parser.add_argument(\n            '--random_sampling_guidance_scale',\n            type=float,\n            default=7.5,\n            help='The guidance scale for random sampling API')\n        parser.add_argument(\n            '--random_sampling_num_inference_steps',\n            type=int,\n            default=50,\n            help='The number of diffusion steps for random sampling API')\n        parser.add_argument(\n            '--random_sampling_batch_size',\n            type=int,\n            default=10,\n            help='The batch size for random sampling API')\n\n        parser.add_argument(\n            '--variation_checkpoint',\n            type=str,\n            required=True,\n            help='The path to the checkpoint for variation API')\n        parser.add_argument(\n            '--variation_guidance_scale',\n            type=float,\n            default=7.5,\n            help='The guidance scale for variation API')\n        parser.add_argument(\n            '--variation_num_inference_steps',\n            type=int,\n            default=50,\n            help='The number of diffusion steps for variation API')\n        parser.add_argument(\n            '--variation_batch_size',\n            type=int,\n            default=10,\n            help='The batch size for variation API')\n        return parser\n\n    def image_random_sampling(self, num_samples, size, prompts):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size using OpenAI's Image API.\n\n        Args:\n            num_samples (int):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n                \"1024x1024\".\n            prompts (List[str]):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with length num_samples containing prompts for\n                each image.\n        \"\"\"\n        max_batch_size = self._random_sampling_batch_size\n        images = []\n        return_prompts = []\n        width, height = list(map(int, size.split('x')))\n        for prompt_i, prompt in enumerate(prompts):\n            num_samples_for_prompt = (num_samples + prompt_i) // len(prompts)\n            num_iterations = int(np.ceil(\n                float(num_samples_for_prompt) / max_batch_size))\n            for iteration in tqdm(range(num_iterations)):\n                batch_size = min(\n                    max_batch_size,\n                    num_samples_for_prompt - iteration * max_batch_size)\n                images.append(_round_to_uint8(self._random_sampling_pipe(\n                    prompt=prompt,\n                    width=width,\n                    height=height,\n                    num_inference_steps=(\n                        self._random_sampling_num_inference_steps),\n                    guidance_scale=self._random_sampling_guidance_scale,\n                    num_images_per_prompt=batch_size,\n                    output_type='np').images))\n            return_prompts.extend([prompt] * num_samples_for_prompt)\n        return np.concatenate(images, axis=0), np.array(return_prompts)\n\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array using OpenAI's Image Variation API.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing prompts provided by\n                image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n                and \"1024x1024\".\n            variation_degree (float):\n                The image variation degree, between 0~1. A larger value means\n                more variation.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        if not (0 <= variation_degree <= 1):\n            raise ValueError('variation_degree should be between 0 and 1')\n        variations = []\n        for _ in tqdm(range(num_variations_per_image)):\n            sub_variations = self._image_variation(\n                images=images,\n                prompts=list(additional_info),\n                size=size,\n                variation_degree=variation_degree)\n            variations.append(sub_variations)\n        return np.stack(variations, axis=1)\n\n    def _image_variation(self, images, prompts, size, variation_degree):\n        width, height = list(map(int, size.split('x')))\n        variation_transform = T.Compose([\n            T.Resize(\n                (width, height),\n                interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(\n                [0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5])])\n        images = [variation_transform(Image.fromarray(im))\n                  for im in images]\n        images = torch.stack(images).to(dev())\n        max_batch_size = self._variation_batch_size\n        variations = []\n        num_iterations = int(np.ceil(\n            float(images.shape[0]) / max_batch_size))\n        for iteration in tqdm(range(num_iterations), leave=False):\n            variations.append(self._variation_pipe(\n                prompt=prompts[iteration * max_batch_size:\n                               (iteration + 1) * max_batch_size],\n                image=images[iteration * max_batch_size:\n                             (iteration + 1) * max_batch_size],\n                num_inference_steps=self._variation_num_inference_steps,\n                strength=variation_degree,\n                guidance_scale=self._variation_guidance_scale,\n                num_images_per_prompt=1,\n                output_type='np').images)\n        variations = _round_to_uint8(np.concatenate(variations, axis=0))\n        return variations", "class StableDiffusionAPI(API):\n    def __init__(self, random_sampling_checkpoint,\n                 random_sampling_guidance_scale,\n                 random_sampling_num_inference_steps,\n                 random_sampling_batch_size,\n                 variation_checkpoint,\n                 variation_guidance_scale,\n                 variation_num_inference_steps,\n                 variation_batch_size,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._random_sampling_checkpoint = random_sampling_checkpoint\n        self._random_sampling_guidance_scale = random_sampling_guidance_scale\n        self._random_sampling_num_inference_steps = \\\n            random_sampling_num_inference_steps\n        self._random_sampling_batch_size = random_sampling_batch_size\n\n        self._random_sampling_pipe = StableDiffusionPipeline.from_pretrained(\n            self._random_sampling_checkpoint, torch_dtype=torch.float16)\n        self._random_sampling_pipe.safety_checker = None\n        self._random_sampling_pipe = self._random_sampling_pipe.to(dev())\n\n        self._variation_checkpoint = variation_checkpoint\n        self._variation_guidance_scale = variation_guidance_scale\n        self._variation_num_inference_steps = variation_num_inference_steps\n        self._variation_batch_size = variation_batch_size\n\n        self._variation_pipe = \\\n            StableDiffusionImg2ImgPipeline.from_pretrained(\n                self._variation_checkpoint,\n                torch_dtype=torch.float16)\n        self._variation_pipe.safety_checker = None\n        self._variation_pipe = self._variation_pipe.to(dev())\n\n    @staticmethod\n    def command_line_parser():\n        parser = super(\n            StableDiffusionAPI, StableDiffusionAPI).command_line_parser()\n        parser.add_argument(\n            '--random_sampling_checkpoint',\n            type=str,\n            required=True,\n            help='The path to the checkpoint for random sampling API')\n        parser.add_argument(\n            '--random_sampling_guidance_scale',\n            type=float,\n            default=7.5,\n            help='The guidance scale for random sampling API')\n        parser.add_argument(\n            '--random_sampling_num_inference_steps',\n            type=int,\n            default=50,\n            help='The number of diffusion steps for random sampling API')\n        parser.add_argument(\n            '--random_sampling_batch_size',\n            type=int,\n            default=10,\n            help='The batch size for random sampling API')\n\n        parser.add_argument(\n            '--variation_checkpoint',\n            type=str,\n            required=True,\n            help='The path to the checkpoint for variation API')\n        parser.add_argument(\n            '--variation_guidance_scale',\n            type=float,\n            default=7.5,\n            help='The guidance scale for variation API')\n        parser.add_argument(\n            '--variation_num_inference_steps',\n            type=int,\n            default=50,\n            help='The number of diffusion steps for variation API')\n        parser.add_argument(\n            '--variation_batch_size',\n            type=int,\n            default=10,\n            help='The batch size for variation API')\n        return parser\n\n    def image_random_sampling(self, num_samples, size, prompts):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size using OpenAI's Image API.\n\n        Args:\n            num_samples (int):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n                \"1024x1024\".\n            prompts (List[str]):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with length num_samples containing prompts for\n                each image.\n        \"\"\"\n        max_batch_size = self._random_sampling_batch_size\n        images = []\n        return_prompts = []\n        width, height = list(map(int, size.split('x')))\n        for prompt_i, prompt in enumerate(prompts):\n            num_samples_for_prompt = (num_samples + prompt_i) // len(prompts)\n            num_iterations = int(np.ceil(\n                float(num_samples_for_prompt) / max_batch_size))\n            for iteration in tqdm(range(num_iterations)):\n                batch_size = min(\n                    max_batch_size,\n                    num_samples_for_prompt - iteration * max_batch_size)\n                images.append(_round_to_uint8(self._random_sampling_pipe(\n                    prompt=prompt,\n                    width=width,\n                    height=height,\n                    num_inference_steps=(\n                        self._random_sampling_num_inference_steps),\n                    guidance_scale=self._random_sampling_guidance_scale,\n                    num_images_per_prompt=batch_size,\n                    output_type='np').images))\n            return_prompts.extend([prompt] * num_samples_for_prompt)\n        return np.concatenate(images, axis=0), np.array(return_prompts)\n\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array using OpenAI's Image Variation API.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing prompts provided by\n                image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n                and \"1024x1024\".\n            variation_degree (float):\n                The image variation degree, between 0~1. A larger value means\n                more variation.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        if not (0 <= variation_degree <= 1):\n            raise ValueError('variation_degree should be between 0 and 1')\n        variations = []\n        for _ in tqdm(range(num_variations_per_image)):\n            sub_variations = self._image_variation(\n                images=images,\n                prompts=list(additional_info),\n                size=size,\n                variation_degree=variation_degree)\n            variations.append(sub_variations)\n        return np.stack(variations, axis=1)\n\n    def _image_variation(self, images, prompts, size, variation_degree):\n        width, height = list(map(int, size.split('x')))\n        variation_transform = T.Compose([\n            T.Resize(\n                (width, height),\n                interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(\n                [0.5, 0.5, 0.5],\n                [0.5, 0.5, 0.5])])\n        images = [variation_transform(Image.fromarray(im))\n                  for im in images]\n        images = torch.stack(images).to(dev())\n        max_batch_size = self._variation_batch_size\n        variations = []\n        num_iterations = int(np.ceil(\n            float(images.shape[0]) / max_batch_size))\n        for iteration in tqdm(range(num_iterations), leave=False):\n            variations.append(self._variation_pipe(\n                prompt=prompts[iteration * max_batch_size:\n                               (iteration + 1) * max_batch_size],\n                image=images[iteration * max_batch_size:\n                             (iteration + 1) * max_batch_size],\n                num_inference_steps=self._variation_num_inference_steps,\n                strength=variation_degree,\n                guidance_scale=self._variation_guidance_scale,\n                num_images_per_prompt=1,\n                output_type='np').images)\n        variations = _round_to_uint8(np.concatenate(variations, axis=0))\n        return variations", ""]}
{"filename": "apis/api.py", "chunked_list": ["from abc import ABC, abstractmethod\nimport argparse\n\n\nclass API(ABC):\n    def __init__(self, args=None):\n        self.args = args\n\n    @staticmethod\n    def command_line_parser():\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            '--api_help',\n            action='help')\n        return parser\n\n    @classmethod\n    def from_command_line_args(cls, args):\n        \"\"\"\n        Creating the API from command line arguments.\n\n        Args:\n            args: (List[str]):\n            The command line arguments\n        Returns:\n            API:\n                The API object.\n        \"\"\"\n        args = cls.command_line_parser().parse_args(args)\n        return cls(**vars(args), args=args)\n\n    @abstractmethod\n    def image_random_sampling(self, num_samples, size, prompts=None):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size.\n\n        Args:\n            num_samples (int, optional):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\", e.g., \"1024x1024\".\n            prompts (List[str], optional):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with the first dimension equaling to\n                num_samples containing additional information such as labels.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree=None):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing additional information such as labels or\n                prompts provided by image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\", e.g., \"1024x1024\".\n            variation_degree (int or float, optional):\n                The degree of image variation.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        pass", ""]}
{"filename": "apis/__init__.py", "chunked_list": ["from .api import API\n\n\ndef get_api_class_from_name(name):\n    # Lazy import to improve loading speed and reduce libary dependency.\n    if name == 'DALLE':\n        from .dalle_api import DALLEAPI\n        return DALLEAPI\n    elif name == 'stable_diffusion':\n        from .stable_diffusion_api import StableDiffusionAPI\n        return StableDiffusionAPI\n    elif name == 'improved_diffusion':\n        from .improved_diffusion_api import ImprovedDiffusionAPI\n        return ImprovedDiffusionAPI\n    else:\n        raise ValueError(f'Unknown API name {name}')", "\n\n__all__ = ['get_api_class_from_name', 'API']\n"]}
{"filename": "apis/dalle_api.py", "chunked_list": ["import openai\nimport numpy as np\nfrom imageio.v2 import imread\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nimport logging\n", "import logging\n\nfrom .api import API\n\nfrom tenacity import (\n    retry,\n    retry_if_not_exception_type,\n    stop_after_attempt,\n    wait_random_exponential,\n)", "    wait_random_exponential,\n)\n\n\nclass DALLEAPI(API):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._openai_api_key = os.environ['OPENAI_API_KEY']\n        openai.api_key = self._openai_api_key\n\n    @staticmethod\n    def command_line_parser():\n        return super(DALLEAPI, DALLEAPI).command_line_parser()\n\n    def image_random_sampling(self, num_samples, size, prompts):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size using OpenAI's Image API.\n\n        Args:\n            num_samples (int):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n                \"1024x1024\".\n            prompts (List[str]):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with length num_samples containing prompts for\n                each image.\n        \"\"\"\n        max_batch_size = 10\n        images = []\n        return_prompts = []\n        for prompt_i, prompt in enumerate(prompts):\n            num_samples_for_prompt = (num_samples + prompt_i) // len(prompts)\n            num_iterations = int(np.ceil(\n                float(num_samples_for_prompt) / max_batch_size))\n            for iteration in tqdm(range(num_iterations)):\n                batch_size = min(\n                    max_batch_size,\n                    num_samples_for_prompt - iteration * max_batch_size)\n                images.append(_dalle2_random_sampling(\n                    prompt=prompt, num_samples=batch_size, size=size))\n            return_prompts.extend([prompt] * num_samples_for_prompt)\n        return np.concatenate(images, axis=0), np.array(return_prompts)\n\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree=None):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array using OpenAI's Image Variation API.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing prompts provided by\n                image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n                and \"1024x1024\".\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        if variation_degree is not None:\n            logging.info(f'Ignoring variation degree {variation_degree}')\n        if additional_info is not None:\n            logging.info('Ignoring additional info')\n        max_batch_size = 10\n        variations = []\n        for iteration in tqdm(range(int(np.ceil(\n                float(num_variations_per_image) / max_batch_size)))):\n            batch_size = min(\n                max_batch_size,\n                num_variations_per_image - iteration * max_batch_size)\n            sub_variations = []\n            for image in tqdm(images, leave=False):\n                sub_variations.append(_dalle2_image_variation(\n                    image=image,\n                    num_variations_per_image=batch_size,\n                    size=size))\n            sub_variations = np.array(sub_variations, dtype=np.uint8)\n            variations.append(sub_variations)\n        return np.concatenate(variations, axis=1)", "\n\n# Decorator is retry logic to get around rate limits. COMMENT OUT WHEN\n# DEBUGGING! Otherwise it will constantly retry on errors.\n@retry(\n    retry=retry_if_not_exception_type(openai.error.InvalidRequestError),\n    wait=wait_random_exponential(min=1, max=60),\n    stop=stop_after_attempt(15),\n)\ndef _dalle2_random_sampling(prompt, num_samples=10, size=\"1024x1024\"):\n    \"\"\"\n    Generates a specified number of random image samples based on a given\n    prompt and size using OpenAI's Image API.\n\n    Args:\n        prompt (str): The text prompt to generate images from.\n        num_samples (int, optional): The number of image samples to generate.\n            Default is 10. Max of 10.\n        size (str, optional): The size of the generated images in the format\n            \"widthxheight\". Default is \"1024x1024\". Options include \"256x256\",\n            \"512x512\", and \"1024x1024\".\n\n    Returns:\n        numpy.ndarray: A numpy array of shape [num_samples x image size x \n            image size x channels] containing the generated image samples as\n            numpy arrays.\n    \"\"\"\n    response = openai.Image.create(prompt=prompt, n=num_samples, size=size)\n    image_urls = [image[\"url\"] for image in response[\"data\"]]\n    images = [\n        imread(BytesIO(requests.get(url).content)) for url in image_urls\n    ]  # Store as np array\n    return np.array(images)", ")\ndef _dalle2_random_sampling(prompt, num_samples=10, size=\"1024x1024\"):\n    \"\"\"\n    Generates a specified number of random image samples based on a given\n    prompt and size using OpenAI's Image API.\n\n    Args:\n        prompt (str): The text prompt to generate images from.\n        num_samples (int, optional): The number of image samples to generate.\n            Default is 10. Max of 10.\n        size (str, optional): The size of the generated images in the format\n            \"widthxheight\". Default is \"1024x1024\". Options include \"256x256\",\n            \"512x512\", and \"1024x1024\".\n\n    Returns:\n        numpy.ndarray: A numpy array of shape [num_samples x image size x \n            image size x channels] containing the generated image samples as\n            numpy arrays.\n    \"\"\"\n    response = openai.Image.create(prompt=prompt, n=num_samples, size=size)\n    image_urls = [image[\"url\"] for image in response[\"data\"]]\n    images = [\n        imread(BytesIO(requests.get(url).content)) for url in image_urls\n    ]  # Store as np array\n    return np.array(images)", "\n\n@retry(\n    retry=retry_if_not_exception_type(openai.error.InvalidRequestError),\n    wait=wait_random_exponential(min=1, max=60),\n    stop=stop_after_attempt(15),\n)\ndef _dalle2_image_variation(image, num_variations_per_image,\n                            size=\"1024x1024\"):\n    \"\"\"\n    Generates a specified number of variations for one image in the input\n    array using OpenAI's Image Variation API.\n\n    Args:\n        images (numpy.ndarray): A numpy array of shape [channels\n            x image size x image size] containing the input images as numpy\n            arrays of type uint8.\n        num_variations_per_image (int): The number of variations to generate\n            for each input image.\n        size (str, optional): The size of the generated image variations in the\n            format \"widthxheight\". Default is \"1024x1024\". Options include\n            \"256x256\", \"512x512\", and \"1024x1024\".\n\n    Returns:\n        numpy.ndarray: A numpy array of shape [\n            num_variations_per_image x image size x image size x channels]\n            containing the generated image variations as numpy arrays of type\n            uint8.\n    \"\"\"\n    im = Image.fromarray(image)\n    with BytesIO() as buffer:\n        im.save(buffer, format=\"PNG\")\n        buffer.seek(0)\n        response = openai.Image.create_variation(\n            image=buffer, n=num_variations_per_image, size=size\n        )\n    image_urls = [image[\"url\"] for image in response[\"data\"]]\n    image_variations = [\n        imread(BytesIO(requests.get(url).content)) for url in image_urls\n    ]\n\n    return np.array(image_variations, dtype=np.uint8)", ""]}
{"filename": "apis/improved_diffusion_api.py", "chunked_list": ["import torch\nimport numpy as np\nfrom tqdm import tqdm\nimport logging\n\nfrom .api import API\nfrom dpsda.arg_utils import str2bool\n\nfrom .improved_diffusion.unet import create_model\nfrom improved_diffusion import dist_util", "from .improved_diffusion.unet import create_model\nfrom improved_diffusion import dist_util\nfrom improved_diffusion.script_util import NUM_CLASSES\nfrom .improved_diffusion.gaussian_diffusion import create_gaussian_diffusion\n\n\ndef _round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\n\nclass ImprovedDiffusionAPI(API):\n    def __init__(self, model_image_size, num_channels, num_res_blocks,\n                 learn_sigma, class_cond, use_checkpoint,\n                 attention_resolutions, num_heads, num_heads_upsample,\n                 use_scale_shift_norm, dropout, diffusion_steps, sigma_small,\n                 noise_schedule, use_kl, predict_xstart, rescale_timesteps,\n                 rescale_learned_sigmas, timestep_respacing, model_path,\n                 batch_size, use_ddim, clip_denoised, use_data_parallel,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._model = create_model(\n            image_size=model_image_size,\n            num_channels=num_channels,\n            num_res_blocks=num_res_blocks,\n            learn_sigma=learn_sigma,\n            class_cond=class_cond,\n            use_checkpoint=use_checkpoint,\n            attention_resolutions=attention_resolutions,\n            num_heads=num_heads,\n            num_heads_upsample=num_heads_upsample,\n            use_scale_shift_norm=use_scale_shift_norm,\n            dropout=dropout)\n        self._diffusion = create_gaussian_diffusion(\n            steps=diffusion_steps,\n            learn_sigma=learn_sigma,\n            sigma_small=sigma_small,\n            noise_schedule=noise_schedule,\n            use_kl=use_kl,\n            predict_xstart=predict_xstart,\n            rescale_timesteps=rescale_timesteps,\n            rescale_learned_sigmas=rescale_learned_sigmas,\n            timestep_respacing=timestep_respacing)\n        self._model.load_state_dict(\n            dist_util.load_state_dict(model_path, map_location=\"cpu\"))\n        self._model.to(dist_util.dev())\n        self._model.eval()\n        self._sampler = Sampler(model=self._model, diffusion=self._diffusion)\n        if use_data_parallel:\n            self._sampler = torch.nn.DataParallel(self._sampler)\n        self._batch_size = batch_size\n        self._use_ddim = use_ddim\n        self._image_size = model_image_size\n        self._clip_denoised = clip_denoised\n        self._class_cond = class_cond\n\n    @staticmethod\n    def command_line_parser():\n        parser = super(\n            ImprovedDiffusionAPI, ImprovedDiffusionAPI).command_line_parser()\n        parser.description = (\n            'See https://github.com/openai/improved-diffusion for the details'\n            ' of the arguments.')\n        parser.add_argument(\n            '--model_image_size',\n            type=int,\n            default=64)\n        parser.add_argument(\n            '--num_channels',\n            type=int,\n            default=128)\n        parser.add_argument(\n            '--num_res_blocks',\n            type=int,\n            default=2)\n        parser.add_argument(\n            '--learn_sigma',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--class_cond',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--use_checkpoint',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--attention_resolutions',\n            type=str,\n            default='16,8')\n        parser.add_argument(\n            '--num_heads',\n            type=int,\n            default=4)\n        parser.add_argument(\n            '--num_heads_upsample',\n            type=int,\n            default=-1)\n        parser.add_argument(\n            '--use_scale_shift_norm',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--dropout',\n            type=float,\n            default=0.0)\n        parser.add_argument(\n            '--diffusion_steps',\n            type=int,\n            default=1000)\n        parser.add_argument(\n            '--sigma_small',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--noise_schedule',\n            type=str,\n            default='linear')\n        parser.add_argument(\n            '--use_kl',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--predict_xstart',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--rescale_timesteps',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--rescale_learned_sigmas',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--timestep_respacing',\n            type=str,\n            default='')\n        parser.add_argument(\n            '--model_path',\n            type=str,\n            required=True)\n        parser.add_argument(\n            '--batch_size',\n            type=int,\n            default=100)\n        parser.add_argument(\n            '--use_ddim',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--clip_denoised',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--use_data_parallel',\n            type=str2bool,\n            default=True,\n            help='Whether to use DataParallel to speed up sampling')\n        return parser\n\n    def image_random_sampling(self, num_samples, size, prompts):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size using OpenAI's Image API.\n\n        Args:\n            num_samples (int):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n                \"1024x1024\".\n            prompts (List[str]):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with length num_samples containing labels for\n                each image.\n        \"\"\"\n        width, height = list(map(int, size.split('x')))\n        if width != self._image_size or height != self._image_size:\n            raise ValueError(\n                f'width and height must be equal to {self._image_size}')\n        samples, labels = sample(\n            sampler=self._sampler,\n            start_t=0,\n            num_samples=num_samples,\n            batch_size=self._batch_size,\n            use_ddim=self._use_ddim,\n            image_size=self._image_size,\n            clip_denoised=self._clip_denoised,\n            class_cond=self._class_cond)\n        samples = _round_to_uint8((samples + 1.0) * 127.5)\n        samples = samples.transpose(0, 2, 3, 1)\n        torch.cuda.empty_cache()\n        return samples, labels\n\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array using OpenAI's Image Variation API.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing labels provided by\n                image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n                and \"1024x1024\".\n            variation_degree (int):\n                The diffusion step to add noise to the images to before running\n                the denoising steps. The value should between 0 and\n                timestep_respacing-1. 0 means the step that is closest to\n                noise. timestep_respacing-1 means the step that is closest to\n                clean image. A smaller value will result in more variation.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        width, height = list(map(int, size.split('x')))\n        if width != self._image_size or height != self._image_size:\n            raise ValueError(\n                f'width and height must be equal to {self._image_size}')\n        images = images.astype(np.float32) / 127.5 - 1.0\n        images = images.transpose(0, 3, 1, 2)\n        variations = []\n        for _ in tqdm(range(num_variations_per_image)):\n            sub_variations = self._image_variation(\n                images=images,\n                labels=additional_info,\n                variation_degree=variation_degree)\n            variations.append(sub_variations)\n        variations = np.stack(variations, axis=1)\n\n        variations = _round_to_uint8((variations + 1.0) * 127.5)\n        variations = variations.transpose(0, 1, 3, 4, 2)\n        torch.cuda.empty_cache()\n        return variations\n\n    def _image_variation(self, images, labels, variation_degree):\n        samples, _ = sample(\n            sampler=self._sampler,\n            start_t=variation_degree,\n            start_image=torch.Tensor(images).to(dist_util.dev()),\n            labels=(None if not self._class_cond\n                    else torch.LongTensor(labels).to(dist_util.dev())),\n            num_samples=images.shape[0],\n            batch_size=self._batch_size,\n            use_ddim=self._use_ddim,\n            image_size=self._image_size,\n            clip_denoised=self._clip_denoised,\n            class_cond=self._class_cond)\n        return samples", "\n\nclass ImprovedDiffusionAPI(API):\n    def __init__(self, model_image_size, num_channels, num_res_blocks,\n                 learn_sigma, class_cond, use_checkpoint,\n                 attention_resolutions, num_heads, num_heads_upsample,\n                 use_scale_shift_norm, dropout, diffusion_steps, sigma_small,\n                 noise_schedule, use_kl, predict_xstart, rescale_timesteps,\n                 rescale_learned_sigmas, timestep_respacing, model_path,\n                 batch_size, use_ddim, clip_denoised, use_data_parallel,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._model = create_model(\n            image_size=model_image_size,\n            num_channels=num_channels,\n            num_res_blocks=num_res_blocks,\n            learn_sigma=learn_sigma,\n            class_cond=class_cond,\n            use_checkpoint=use_checkpoint,\n            attention_resolutions=attention_resolutions,\n            num_heads=num_heads,\n            num_heads_upsample=num_heads_upsample,\n            use_scale_shift_norm=use_scale_shift_norm,\n            dropout=dropout)\n        self._diffusion = create_gaussian_diffusion(\n            steps=diffusion_steps,\n            learn_sigma=learn_sigma,\n            sigma_small=sigma_small,\n            noise_schedule=noise_schedule,\n            use_kl=use_kl,\n            predict_xstart=predict_xstart,\n            rescale_timesteps=rescale_timesteps,\n            rescale_learned_sigmas=rescale_learned_sigmas,\n            timestep_respacing=timestep_respacing)\n        self._model.load_state_dict(\n            dist_util.load_state_dict(model_path, map_location=\"cpu\"))\n        self._model.to(dist_util.dev())\n        self._model.eval()\n        self._sampler = Sampler(model=self._model, diffusion=self._diffusion)\n        if use_data_parallel:\n            self._sampler = torch.nn.DataParallel(self._sampler)\n        self._batch_size = batch_size\n        self._use_ddim = use_ddim\n        self._image_size = model_image_size\n        self._clip_denoised = clip_denoised\n        self._class_cond = class_cond\n\n    @staticmethod\n    def command_line_parser():\n        parser = super(\n            ImprovedDiffusionAPI, ImprovedDiffusionAPI).command_line_parser()\n        parser.description = (\n            'See https://github.com/openai/improved-diffusion for the details'\n            ' of the arguments.')\n        parser.add_argument(\n            '--model_image_size',\n            type=int,\n            default=64)\n        parser.add_argument(\n            '--num_channels',\n            type=int,\n            default=128)\n        parser.add_argument(\n            '--num_res_blocks',\n            type=int,\n            default=2)\n        parser.add_argument(\n            '--learn_sigma',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--class_cond',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--use_checkpoint',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--attention_resolutions',\n            type=str,\n            default='16,8')\n        parser.add_argument(\n            '--num_heads',\n            type=int,\n            default=4)\n        parser.add_argument(\n            '--num_heads_upsample',\n            type=int,\n            default=-1)\n        parser.add_argument(\n            '--use_scale_shift_norm',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--dropout',\n            type=float,\n            default=0.0)\n        parser.add_argument(\n            '--diffusion_steps',\n            type=int,\n            default=1000)\n        parser.add_argument(\n            '--sigma_small',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--noise_schedule',\n            type=str,\n            default='linear')\n        parser.add_argument(\n            '--use_kl',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--predict_xstart',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--rescale_timesteps',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--rescale_learned_sigmas',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--timestep_respacing',\n            type=str,\n            default='')\n        parser.add_argument(\n            '--model_path',\n            type=str,\n            required=True)\n        parser.add_argument(\n            '--batch_size',\n            type=int,\n            default=100)\n        parser.add_argument(\n            '--use_ddim',\n            type=str2bool,\n            default=False)\n        parser.add_argument(\n            '--clip_denoised',\n            type=str2bool,\n            default=True)\n        parser.add_argument(\n            '--use_data_parallel',\n            type=str2bool,\n            default=True,\n            help='Whether to use DataParallel to speed up sampling')\n        return parser\n\n    def image_random_sampling(self, num_samples, size, prompts):\n        \"\"\"\n        Generates a specified number of random image samples based on a given\n        prompt and size using OpenAI's Image API.\n\n        Args:\n            num_samples (int):\n                The number of image samples to generate.\n            size (str, optional):\n                The size of the generated images in the format\n                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n                \"1024x1024\".\n            prompts (List[str]):\n                The text prompts to generate images from. Each promot will be\n                used to generate num_samples/len(prompts) number of samples.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x width x height x\n                channels] with type np.uint8 containing the generated image\n                samples as numpy arrays.\n            numpy.ndarray:\n                A numpy array with length num_samples containing labels for\n                each image.\n        \"\"\"\n        width, height = list(map(int, size.split('x')))\n        if width != self._image_size or height != self._image_size:\n            raise ValueError(\n                f'width and height must be equal to {self._image_size}')\n        samples, labels = sample(\n            sampler=self._sampler,\n            start_t=0,\n            num_samples=num_samples,\n            batch_size=self._batch_size,\n            use_ddim=self._use_ddim,\n            image_size=self._image_size,\n            clip_denoised=self._clip_denoised,\n            class_cond=self._class_cond)\n        samples = _round_to_uint8((samples + 1.0) * 127.5)\n        samples = samples.transpose(0, 2, 3, 1)\n        torch.cuda.empty_cache()\n        return samples, labels\n\n    def image_variation(self, images, additional_info,\n                        num_variations_per_image, size, variation_degree):\n        \"\"\"\n        Generates a specified number of variations for each image in the input\n        array using OpenAI's Image Variation API.\n\n        Args:\n            images (numpy.ndarray):\n                A numpy array of shape [num_samples x width x height\n                x channels] containing the input images as numpy arrays of type\n                uint8.\n            additional_info (numpy.ndarray):\n                A numpy array with the first dimension equaling to\n                num_samples containing labels provided by\n                image_random_sampling.\n            num_variations_per_image (int):\n                The number of variations to generate for each input image.\n            size (str):\n                The size of the generated image variations in the\n                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n                and \"1024x1024\".\n            variation_degree (int):\n                The diffusion step to add noise to the images to before running\n                the denoising steps. The value should between 0 and\n                timestep_respacing-1. 0 means the step that is closest to\n                noise. timestep_respacing-1 means the step that is closest to\n                clean image. A smaller value will result in more variation.\n\n        Returns:\n            numpy.ndarray:\n                A numpy array of shape [num_samples x num_variations_per_image\n                x width x height x channels] containing the generated image\n                variations as numpy arrays of type uint8.\n        \"\"\"\n        width, height = list(map(int, size.split('x')))\n        if width != self._image_size or height != self._image_size:\n            raise ValueError(\n                f'width and height must be equal to {self._image_size}')\n        images = images.astype(np.float32) / 127.5 - 1.0\n        images = images.transpose(0, 3, 1, 2)\n        variations = []\n        for _ in tqdm(range(num_variations_per_image)):\n            sub_variations = self._image_variation(\n                images=images,\n                labels=additional_info,\n                variation_degree=variation_degree)\n            variations.append(sub_variations)\n        variations = np.stack(variations, axis=1)\n\n        variations = _round_to_uint8((variations + 1.0) * 127.5)\n        variations = variations.transpose(0, 1, 3, 4, 2)\n        torch.cuda.empty_cache()\n        return variations\n\n    def _image_variation(self, images, labels, variation_degree):\n        samples, _ = sample(\n            sampler=self._sampler,\n            start_t=variation_degree,\n            start_image=torch.Tensor(images).to(dist_util.dev()),\n            labels=(None if not self._class_cond\n                    else torch.LongTensor(labels).to(dist_util.dev())),\n            num_samples=images.shape[0],\n            batch_size=self._batch_size,\n            use_ddim=self._use_ddim,\n            image_size=self._image_size,\n            clip_denoised=self._clip_denoised,\n            class_cond=self._class_cond)\n        return samples", "\n\ndef sample(sampler, num_samples, start_t, batch_size, use_ddim,\n           image_size, clip_denoised, class_cond,\n           start_image=None, labels=None):\n    all_images = []\n    all_labels = []\n    batch_cnt = 0\n    cnt = 0\n    while cnt < num_samples:\n        current_batch_size = \\\n            (batch_size if start_image is None\n             else min(batch_size,\n                      start_image.shape[0] - batch_cnt * batch_size))\n        shape = (current_batch_size, 3, image_size, image_size)\n        model_kwargs = {}\n        if class_cond:\n            if labels is None:\n                classes = torch.randint(\n                    low=0, high=NUM_CLASSES, size=(current_batch_size,),\n                    device=dist_util.dev()\n                )\n            else:\n                classes = labels[batch_cnt * batch_size:\n                                 (batch_cnt + 1) * batch_size]\n            model_kwargs[\"y\"] = classes\n        sample = sampler(\n            clip_denoised=clip_denoised,\n            model_kwargs=model_kwargs,\n            start_t=max(start_t, 0),\n            start_image=(None if start_image is None\n                         else start_image[batch_cnt * batch_size:\n                                          (batch_cnt + 1) * batch_size]),\n            use_ddim=use_ddim,\n            noise=torch.randn(*shape, device=dist_util.dev()),\n            image_size=image_size)\n        batch_cnt += 1\n\n        all_images.append(sample.detach().cpu().numpy())\n\n        if class_cond:\n            all_labels.append(classes.detach().cpu().numpy())\n\n        cnt += sample.shape[0]\n        logging.info(f\"Created {cnt} samples\")\n\n    all_images = np.concatenate(all_images, axis=0)\n    all_images = all_images[: num_samples]\n\n    if class_cond:\n        all_labels = np.concatenate(all_labels, axis=0)\n        all_labels = all_labels[: num_samples]\n    else:\n        all_labels = np.zeros(shape=(num_samples,))\n    return all_images, all_labels", "\n\nclass Sampler(torch.nn.Module):\n    \"\"\"\n    A wrapper around the model and diffusion modules that handles the entire\n    sampling process, so as to reduce the communiation rounds between GPUs when\n    using DataParallel.\n    \"\"\"\n    def __init__(self, model, diffusion):\n        super().__init__()\n        self._model = model\n        self._diffusion = diffusion\n\n    def forward(self, clip_denoised, model_kwargs, start_t, start_image,\n                use_ddim, noise, image_size):\n        sample_fn = (\n            self._diffusion.p_sample_loop if not use_ddim\n            else self._diffusion.ddim_sample_loop)\n        sample = sample_fn(\n            self._model,\n            (noise.shape[0], 3, image_size, image_size),\n            clip_denoised=clip_denoised,\n            model_kwargs=model_kwargs,\n            start_t=max(start_t, 0),\n            start_image=start_image,\n            noise=noise,\n            device=noise.device)\n        return sample", ""]}
{"filename": "apis/improved_diffusion/gaussian_diffusion.py", "chunked_list": ["\"\"\"\nThis code contains minor edits from the original code at\nhttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\nand\nhttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/script_util.py\nto support sampling from the middle of the diffusion process with start_t and\nstart_image arguments.\n\"\"\"\n\nimport torch as th", "\nimport torch as th\nfrom improved_diffusion.respace import SpacedDiffusion\nfrom improved_diffusion.respace import space_timesteps\nfrom improved_diffusion.gaussian_diffusion import _extract_into_tensor\nfrom improved_diffusion import gaussian_diffusion as gd\n\n\nclass SkippedSpacedDiffusion(SpacedDiffusion):\n    def p_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model.\n\n        :param model: the model module.\n        :param shape: the shape of the samples, (N, C, H, W).\n        :param noise: if specified, the noise from the encoder to sample.\n                      Should be of the same shape as `shape`.\n        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param device: if specified, the device to create the samples on.\n                       If not specified, use a model parameter's device.\n        :param progress: if True, show a tqdm progress bar.\n        :return: a non-differentiable batch of samples.\n        \"\"\"\n        final = None\n        for sample in self.p_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            start_t=start_t,\n            start_image=start_image,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def p_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model and yield intermediate samples from\n        each timestep of diffusion.\n\n        Arguments are the same as p_sample_loop().\n        Returns a generator over dicts, where each dict is the return value of\n        p_sample().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n        indices = indices[start_t:]\n        if start_image is not None:\n            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n            img = self.q_sample(start_image, t=t_batch, noise=img)\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def ddim_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model using DDIM.\n\n        Same usage as p_sample().\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = _extract_into_tensor(\n            self.alphas_cumprod_prev, t, x.shape)\n        sigma = (\n            eta\n            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n        )\n        # Equation 12.\n        noise = th.randn_like(x)\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n        )\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        sample = mean_pred + nonzero_mask * sigma * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_reverse_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t+1} from the model using DDIM reverse ODE.\n        \"\"\"\n        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = (\n            _extract_into_tensor(\n                self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n            - out[\"pred_xstart\"]\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n        alpha_bar_next = _extract_into_tensor(\n            self.alphas_cumprod_next, t, x.shape)\n\n        # Equation 12. reversed\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n            + th.sqrt(1 - alpha_bar_next) * eps\n        )\n\n        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model using DDIM.\n\n        Same usage as p_sample_loop().\n        \"\"\"\n        final = None\n        for sample in self.ddim_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            eta=eta,\n            start_t=start_t,\n            start_image=start_image,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def ddim_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Use DDIM to sample from the model and yield intermediate samples from\n        each timestep of DDIM.\n\n        Same usage as p_sample_loop_progressive().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n        indices = indices[start_t:]\n        if start_image is not None:\n            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n            img = self.q_sample(start_image, t=t_batch, noise=img)\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.ddim_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    model_kwargs=model_kwargs,\n                    eta=eta,\n                )\n                yield out\n                img = out[\"sample\"]", "class SkippedSpacedDiffusion(SpacedDiffusion):\n    def p_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model.\n\n        :param model: the model module.\n        :param shape: the shape of the samples, (N, C, H, W).\n        :param noise: if specified, the noise from the encoder to sample.\n                      Should be of the same shape as `shape`.\n        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n        :param denoised_fn: if not None, a function which applies to the\n            x_start prediction before it is used to sample.\n        :param model_kwargs: if not None, a dict of extra keyword arguments to\n            pass to the model. This can be used for conditioning.\n        :param device: if specified, the device to create the samples on.\n                       If not specified, use a model parameter's device.\n        :param progress: if True, show a tqdm progress bar.\n        :return: a non-differentiable batch of samples.\n        \"\"\"\n        final = None\n        for sample in self.p_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            start_t=start_t,\n            start_image=start_image,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def p_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model and yield intermediate samples from\n        each timestep of diffusion.\n\n        Arguments are the same as p_sample_loop().\n        Returns a generator over dicts, where each dict is the return value of\n        p_sample().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n        indices = indices[start_t:]\n        if start_image is not None:\n            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n            img = self.q_sample(start_image, t=t_batch, noise=img)\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.p_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    model_kwargs=model_kwargs,\n                )\n                yield out\n                img = out[\"sample\"]\n\n    def ddim_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t-1} from the model using DDIM.\n\n        Same usage as p_sample().\n        \"\"\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n        alpha_bar_prev = _extract_into_tensor(\n            self.alphas_cumprod_prev, t, x.shape)\n        sigma = (\n            eta\n            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n        )\n        # Equation 12.\n        noise = th.randn_like(x)\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n        )\n        nonzero_mask = (\n            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n        )  # no noise when t == 0\n        sample = mean_pred + nonzero_mask * sigma * noise\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_reverse_sample(\n        self,\n        model,\n        x,\n        t,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        eta=0.0,\n    ):\n        \"\"\"\n        Sample x_{t+1} from the model using DDIM reverse ODE.\n        \"\"\"\n        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n        out = self.p_mean_variance(\n            model,\n            x,\n            t,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n        )\n        # Usually our model outputs epsilon, but we re-derive it\n        # in case we used x_start or x_prev prediction.\n        eps = (\n            _extract_into_tensor(\n                self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n            - out[\"pred_xstart\"]\n        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n        alpha_bar_next = _extract_into_tensor(\n            self.alphas_cumprod_next, t, x.shape)\n\n        # Equation 12. reversed\n        mean_pred = (\n            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n            + th.sqrt(1 - alpha_bar_next) * eps\n        )\n\n        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def ddim_sample_loop(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Generate samples from the model using DDIM.\n\n        Same usage as p_sample_loop().\n        \"\"\"\n        final = None\n        for sample in self.ddim_sample_loop_progressive(\n            model,\n            shape,\n            noise=noise,\n            clip_denoised=clip_denoised,\n            denoised_fn=denoised_fn,\n            model_kwargs=model_kwargs,\n            device=device,\n            progress=progress,\n            eta=eta,\n            start_t=start_t,\n            start_image=start_image,\n        ):\n            final = sample\n        return final[\"sample\"]\n\n    def ddim_sample_loop_progressive(\n        self,\n        model,\n        shape,\n        noise=None,\n        clip_denoised=True,\n        denoised_fn=None,\n        model_kwargs=None,\n        device=None,\n        progress=False,\n        eta=0.0,\n        start_t=0,\n        start_image=None,\n    ):\n        \"\"\"\n        Use DDIM to sample from the model and yield intermediate samples from\n        each timestep of DDIM.\n\n        Same usage as p_sample_loop_progressive().\n        \"\"\"\n        if device is None:\n            device = next(model.parameters()).device\n        assert isinstance(shape, (tuple, list))\n        if noise is not None:\n            img = noise\n        else:\n            img = th.randn(*shape, device=device)\n        indices = list(range(self.num_timesteps))[::-1]\n        indices = indices[start_t:]\n        if start_image is not None:\n            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n            img = self.q_sample(start_image, t=t_batch, noise=img)\n        if progress:\n            # Lazy import so that we don't depend on tqdm.\n            from tqdm.auto import tqdm\n\n            indices = tqdm(indices)\n\n        for i in indices:\n            t = th.tensor([i] * shape[0], device=device)\n            with th.no_grad():\n                out = self.ddim_sample(\n                    model,\n                    img,\n                    t,\n                    clip_denoised=clip_denoised,\n                    denoised_fn=denoised_fn,\n                    model_kwargs=model_kwargs,\n                    eta=eta,\n                )\n                yield out\n                img = out[\"sample\"]", "\n\ndef create_gaussian_diffusion(\n    *,\n    steps=1000,\n    learn_sigma=False,\n    sigma_small=False,\n    noise_schedule=\"linear\",\n    use_kl=False,\n    predict_xstart=False,\n    rescale_timesteps=False,\n    rescale_learned_sigmas=False,\n    timestep_respacing=\"\",\n):\n    betas = gd.get_named_beta_schedule(noise_schedule, steps)\n    if use_kl:\n        loss_type = gd.LossType.RESCALED_KL\n    elif rescale_learned_sigmas:\n        loss_type = gd.LossType.RESCALED_MSE\n    else:\n        loss_type = gd.LossType.MSE\n    if not timestep_respacing:\n        timestep_respacing = [steps]\n    return SkippedSpacedDiffusion(\n        use_timesteps=space_timesteps(steps, timestep_respacing),\n        betas=betas,\n        model_mean_type=(\n            gd.ModelMeanType.EPSILON if not predict_xstart\n            else gd.ModelMeanType.START_X\n        ),\n        model_var_type=(\n            (\n                gd.ModelVarType.FIXED_LARGE\n                if not sigma_small\n                else gd.ModelVarType.FIXED_SMALL\n            )\n            if not learn_sigma\n            else gd.ModelVarType.LEARNED_RANGE\n        ),\n        loss_type=loss_type,\n        rescale_timesteps=rescale_timesteps,\n    )", ""]}
{"filename": "apis/improved_diffusion/unet.py", "chunked_list": ["\"\"\"\nThis code contains minor edits from the original code at\nhttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/unet.py\nand\nhttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/script_util.py\nto avoid calling self.input_blocks.parameters() in the original code, which is\nnot supported by DataParallel.\n\"\"\"\n\n", "\n\nimport torch\nfrom improved_diffusion.unet import UNetModel\nfrom improved_diffusion.script_util import NUM_CLASSES\n\n\nclass FP32UNetModel(UNetModel):\n    @property\n    def inner_dtype(self):\n        return torch.float32", "\n\ndef create_model(\n    image_size,\n    num_channels,\n    num_res_blocks,\n    learn_sigma,\n    class_cond,\n    use_checkpoint,\n    attention_resolutions,\n    num_heads,\n    num_heads_upsample,\n    use_scale_shift_norm,\n    dropout,\n):\n    if image_size == 256:\n        channel_mult = (1, 1, 2, 2, 4, 4)\n    elif image_size == 64:\n        channel_mult = (1, 2, 3, 4)\n    elif image_size == 32:\n        channel_mult = (1, 2, 2, 2)\n    else:\n        raise ValueError(f\"unsupported image size: {image_size}\")\n\n    attention_ds = []\n    for res in attention_resolutions.split(\",\"):\n        attention_ds.append(image_size // int(res))\n\n    return FP32UNetModel(\n        in_channels=3,\n        model_channels=num_channels,\n        out_channels=(3 if not learn_sigma else 6),\n        num_res_blocks=num_res_blocks,\n        attention_resolutions=tuple(attention_ds),\n        dropout=dropout,\n        channel_mult=channel_mult,\n        num_classes=(NUM_CLASSES if class_cond else None),\n        use_checkpoint=use_checkpoint,\n        num_heads=num_heads,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n    )", ""]}
{"filename": "apis/improved_diffusion/__init__.py", "chunked_list": [""]}
{"filename": "data/get_camelyon17.py", "chunked_list": ["from wilds import get_dataset\nfrom tqdm import tqdm\nimport os\n\n\ndef save(dataset, path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    for i in tqdm(range(len(dataset))):\n        image, label, _ = dataset[i]\n        image.save(f'{path}/{label.item()}_{i}.png')", "\n\nif __name__ == '__main__':\n    dataset = get_dataset(dataset=\"camelyon17\", download=True)\n    train_data = dataset.get_subset(\"train\")\n    val_data = dataset.get_subset(\"val\")\n    test_data = dataset.get_subset(\"test\")\n\n    save(train_data, 'camelyon17_train')\n    save(val_data, 'camelyon17_test')\n    save(test_data, 'camelyon17_val')", ""]}
{"filename": "data/get_cifar10.py", "chunked_list": ["import os\nimport tempfile\n\nimport torchvision\nfrom tqdm.auto import tqdm\n\nCLASSES = (\n    \"plane\",\n    \"car\",\n    \"bird\",", "    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)", "    \"truck\",\n)\n\n\ndef main():\n    for split in [\"train\", \"test\"]:\n        out_dir = f\"cifar10_{split}\"\n        if os.path.exists(out_dir):\n            print(f\"skipping split {split} since {out_dir} already exists.\")\n            continue\n\n        print(\"downloading...\")\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            dataset = torchvision.datasets.CIFAR10(\n                root=tmp_dir, train=split == \"train\", download=True\n            )\n\n        print(\"dumping images...\")\n        os.mkdir(out_dir)\n        for i in tqdm(range(len(dataset))):\n            image, label = dataset[i]\n            filename = os.path.join(out_dir, f\"{CLASSES[label]}_{i:05d}.png\")\n            image.save(filename)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "dpsda/feature_extractor.py", "chunked_list": ["import imageio\nimport cleanfid\nimport os\nimport shutil\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom cleanfid.resize import make_resizer\n\n\ndef round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)", "\n\ndef round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\n\ndef extract_features(\n        data, mode='clean', device=torch.device('cuda'), use_dataparallel=True,\n        num_workers=12, batch_size=5000, custom_fn_resize=None, description='',\n        verbose=True, custom_image_tranform=None, tmp_folder='tmp_feature',\n        model_name=\"inception_v3\", res=32):\n    if model_name == 'original':\n        return data.reshape((data.shape[0], -1)).astype(np.float32)\n    elif model_name == \"inception_v3\":\n        feat_model = cleanfid.features.build_feature_extractor(\n            mode=mode,\n            device=device,\n            use_dataparallel=use_dataparallel\n        )\n    elif model_name == \"clip_vit_b_32\":\n        from cleanfid.clip_features import CLIP_fx, img_preprocess_clip\n        clip_fx = CLIP_fx(\"ViT-B/32\", device=device)\n        feat_model = clip_fx\n        custom_fn_resize = img_preprocess_clip\n    else:\n        raise Exception(f'Unknown model_name {model_name}')\n    if not os.path.exists(tmp_folder):\n        os.makedirs(tmp_folder)\n    else:\n        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n        os.makedirs(tmp_folder)\n    assert data.dtype == np.uint8\n    resizer = make_resizer(\n        library='PIL', quantize_after=False, filter='bicubic',\n        output_size=(res, res))\n    # TODO: in memory processing for computing features.\n    for i in tqdm(range(data.shape[0])):\n        image = round_to_uint8(resizer(data[i]))\n        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n    files = [os.path.join(tmp_folder, f'{i}.png')\n             for i in range(data.shape[0])]\n\n    np_feats = cleanfid.fid.get_files_features(\n        l_files=files,\n        model=feat_model,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        device=device,\n        mode=mode,\n        custom_fn_resize=custom_fn_resize,\n        custom_image_tranform=custom_image_tranform,\n        description=description,\n        fdir=tmp_folder,\n        verbose=verbose)\n    return np_feats.astype(np.float32)", ""]}
{"filename": "dpsda/metrics.py", "chunked_list": ["import os\nimport shutil\nimport numpy as np\nimport imageio\nfrom cleanfid import fid\nfrom cleanfid.resize import make_resizer\nimport torch\nimport cleanfid\nfrom cleanfid.features import build_feature_extractor\nfrom cleanfid.fid import get_folder_features", "from cleanfid.features import build_feature_extractor\nfrom cleanfid.fid import get_folder_features\nfrom tqdm import tqdm\n\n\ndef round_to_uint8(image):\n    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\n\ndef cleanfid_make_custom_stats(\n        name, fdir, split, res, num=None, mode=\"clean\",\n        model_name=\"inception_v3\", num_workers=0, batch_size=64,\n        device=torch.device(\"cuda\"), verbose=True):\n    stats_folder = os.path.join(os.path.dirname(cleanfid.__file__), \"stats\")\n    os.makedirs(stats_folder, exist_ok=True)\n    if model_name == \"inception_v3\":\n        model_modifier = \"\"\n    else:\n        model_modifier = \"_\" + model_name\n    outf = os.path.join(\n        stats_folder,\n        f\"{name}_{mode}{model_modifier}_{split}_{res}.npz\".lower())\n    # if the custom stat file already exists\n    if os.path.exists(outf):\n        msg = f\"The statistics file {name} already exists. \"\n        msg += \"Use remove_custom_stats function to delete it first.\"\n        print(msg)\n        return\n    if model_name == \"inception_v3\":\n        feat_model = build_feature_extractor(mode, device)\n        custom_fn_resize = None\n        custom_image_tranform = None\n    elif model_name == \"clip_vit_b_32\":\n        from cleanfid.clip_features import CLIP_fx, img_preprocess_clip\n        clip_fx = CLIP_fx(\"ViT-B/32\")\n        feat_model = clip_fx\n        custom_fn_resize = img_preprocess_clip\n        custom_image_tranform = None\n    else:\n        raise ValueError(\n            f\"The entered model name - {model_name} was not recognized.\")\n\n    # get all inception features for folder images\n    np_feats = get_folder_features(\n        fdir, feat_model, num_workers=num_workers, num=num,\n        batch_size=batch_size, device=device, verbose=verbose,\n        mode=mode, description=f\"custom stats: {os.path.basename(fdir)} : \",\n        custom_image_tranform=custom_image_tranform,\n        custom_fn_resize=custom_fn_resize)\n\n    mu = np.mean(np_feats, axis=0)\n    sigma = np.cov(np_feats, rowvar=False)\n    print(f\"saving custom FID stats to {outf}\")\n    np.savez_compressed(outf, mu=mu, sigma=sigma)", "\ndef cleanfid_make_custom_stats(\n        name, fdir, split, res, num=None, mode=\"clean\",\n        model_name=\"inception_v3\", num_workers=0, batch_size=64,\n        device=torch.device(\"cuda\"), verbose=True):\n    stats_folder = os.path.join(os.path.dirname(cleanfid.__file__), \"stats\")\n    os.makedirs(stats_folder, exist_ok=True)\n    if model_name == \"inception_v3\":\n        model_modifier = \"\"\n    else:\n        model_modifier = \"_\" + model_name\n    outf = os.path.join(\n        stats_folder,\n        f\"{name}_{mode}{model_modifier}_{split}_{res}.npz\".lower())\n    # if the custom stat file already exists\n    if os.path.exists(outf):\n        msg = f\"The statistics file {name} already exists. \"\n        msg += \"Use remove_custom_stats function to delete it first.\"\n        print(msg)\n        return\n    if model_name == \"inception_v3\":\n        feat_model = build_feature_extractor(mode, device)\n        custom_fn_resize = None\n        custom_image_tranform = None\n    elif model_name == \"clip_vit_b_32\":\n        from cleanfid.clip_features import CLIP_fx, img_preprocess_clip\n        clip_fx = CLIP_fx(\"ViT-B/32\")\n        feat_model = clip_fx\n        custom_fn_resize = img_preprocess_clip\n        custom_image_tranform = None\n    else:\n        raise ValueError(\n            f\"The entered model name - {model_name} was not recognized.\")\n\n    # get all inception features for folder images\n    np_feats = get_folder_features(\n        fdir, feat_model, num_workers=num_workers, num=num,\n        batch_size=batch_size, device=device, verbose=verbose,\n        mode=mode, description=f\"custom stats: {os.path.basename(fdir)} : \",\n        custom_image_tranform=custom_image_tranform,\n        custom_fn_resize=custom_fn_resize)\n\n    mu = np.mean(np_feats, axis=0)\n    sigma = np.cov(np_feats, rowvar=False)\n    print(f\"saving custom FID stats to {outf}\")\n    np.savez_compressed(outf, mu=mu, sigma=sigma)", "\n\ndef make_fid_stats(samples, dataset, dataset_res, dataset_split,\n                   tmp_folder='tmp_fid', batch_size=5000,\n                   model_name='inception_v3'):\n    if not os.path.exists(tmp_folder):\n        os.makedirs(tmp_folder)\n    else:\n        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n        os.makedirs(tmp_folder)\n    assert samples.dtype == np.uint8\n    resizer = make_resizer(\n        library='PIL', quantize_after=False, filter='bicubic',\n        output_size=(dataset_res, dataset_res))\n    # TODO: in memory processing for computing stats.\n    for i in tqdm(range(samples.shape[0])):\n        image = round_to_uint8(resizer(samples[i]))\n        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n    cleanfid_make_custom_stats(\n        name=dataset, fdir=tmp_folder, split=dataset_split, res=dataset_res,\n        batch_size=batch_size, model_name=model_name)", "\n\ndef compute_fid(samples, tmp_folder='tmp_fid', dataset='cifar10',\n                dataset_res=32, dataset_split='train', batch_size=5000,\n                num_fid_samples=10000, model_name='inception_v3'):\n    if not os.path.exists(tmp_folder):\n        os.makedirs(tmp_folder)\n    else:\n        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n        os.makedirs(tmp_folder)\n    if num_fid_samples == samples.shape[0]:\n        ids = np.arange(samples.shape[0])\n    elif num_fid_samples < samples.shape[0]:\n        ids = np.random.choice(\n            samples.shape[0], size=num_fid_samples, replace=False)\n    else:\n        ids = np.random.choice(\n            samples.shape[0], size=num_fid_samples, replace=True)\n    samples = samples[ids]\n    assert samples.dtype == np.uint8\n    resizer = make_resizer(\n        library='PIL', quantize_after=False, filter='bicubic',\n        output_size=(dataset_res, dataset_res))\n    # TODO: in memory processing for computing FID.\n    for i in range(samples.shape[0]):\n        image = round_to_uint8(resizer(samples[i]))\n        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n    score = fid.compute_fid(\n        tmp_folder, dataset_name=dataset, dataset_split=dataset_split,\n        dataset_res=dataset_res, batch_size=batch_size, model_name=model_name)\n    return score", ""]}
{"filename": "dpsda/arg_utils.py", "chunked_list": ["import argparse\n\n\ndef str2bool(v):\n    # From:\n    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')"]}
{"filename": "dpsda/dataset.py", "chunked_list": ["from PIL import Image\nimport blobfile as bf\nfrom torch.utils.data import Dataset\n\n\ndef _list_image_files_recursively(data_dir):\n    results = []\n    for entry in sorted(bf.listdir(data_dir)):\n        full_path = bf.join(data_dir, entry)\n        ext = entry.split('.')[-1]\n        if \".\" in entry and ext.lower() in ['jpg', 'jpeg', 'png', 'gif']:\n            results.append(full_path)\n        elif bf.isdir(full_path):\n            results.extend(_list_image_files_recursively(full_path))\n    return results", "\n\nclass ImageDataset(Dataset):\n    def __init__(self, folder, transform):\n        super().__init__()\n        self.folder = folder\n        self.transform = transform\n        \n        self.local_images = _list_image_files_recursively(folder)\n        class_names = [bf.basename(path).split('_')[0]\n                       for path in self.local_images]\n        sorted_classes = {x: i for i, x in enumerate(sorted(set(class_names)))}\n        self.local_classes = [sorted_classes[x] for x in class_names]\n\n    def __len__(self):\n        return len(self.local_images)\n\n    def __getitem__(self, idx):\n        path = self.local_images[idx]\n        with bf.BlobFile(path, 'rb') as f:\n            pil_image = Image.open(f)\n            pil_image.load()\n\n        arr = self.transform(pil_image)\n\n        label = self.local_classes[idx]\n        return arr, label", ""]}
{"filename": "dpsda/__init__.py", "chunked_list": [""]}
{"filename": "dpsda/data_loader.py", "chunked_list": ["import torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport logging\n\nfrom .dataset import ImageDataset\n\n\ndef load_data(data_dir, batch_size, image_size, class_cond,\n              num_private_samples):\n    transform = T.Compose([\n        T.Resize(image_size),\n        T.CenterCrop(image_size),\n        T.ToTensor()\n    ])\n    dataset = ImageDataset(folder=data_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=10,\n                        pin_memory=torch.cuda.is_available(), drop_last=False)\n    all_samples = []\n    all_labels = []\n    cnt = 0\n    for batch, cond in loader:\n        all_samples.append(batch.cpu().numpy())\n\n        if class_cond:\n            all_labels.append(cond.cpu().numpy())\n\n        cnt += batch.shape[0]\n\n        logging.info(f'loaded {cnt} samples')\n        if batch.shape[0] < batch_size:\n            logging.info('WARNING: containing incomplete batch. Please check'\n                         'num_private_samples')\n\n        if cnt >= num_private_samples:\n            break\n\n    all_samples = np.concatenate(all_samples, axis=0)\n    all_samples = all_samples[:num_private_samples]\n    all_samples = np.around(np.clip(\n        all_samples * 255, a_min=0, a_max=255)).astype(np.uint8)\n    all_samples = np.transpose(all_samples, (0, 2, 3, 1))\n    if class_cond:\n        all_labels = np.concatenate(all_labels, axis=0)\n        all_labels = all_labels[:num_private_samples]\n    else:\n        all_labels = np.zeros(shape=all_samples.shape[0], dtype=np.int64)\n    return all_samples, all_labels", "\ndef load_data(data_dir, batch_size, image_size, class_cond,\n              num_private_samples):\n    transform = T.Compose([\n        T.Resize(image_size),\n        T.CenterCrop(image_size),\n        T.ToTensor()\n    ])\n    dataset = ImageDataset(folder=data_dir, transform=transform)\n    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=10,\n                        pin_memory=torch.cuda.is_available(), drop_last=False)\n    all_samples = []\n    all_labels = []\n    cnt = 0\n    for batch, cond in loader:\n        all_samples.append(batch.cpu().numpy())\n\n        if class_cond:\n            all_labels.append(cond.cpu().numpy())\n\n        cnt += batch.shape[0]\n\n        logging.info(f'loaded {cnt} samples')\n        if batch.shape[0] < batch_size:\n            logging.info('WARNING: containing incomplete batch. Please check'\n                         'num_private_samples')\n\n        if cnt >= num_private_samples:\n            break\n\n    all_samples = np.concatenate(all_samples, axis=0)\n    all_samples = all_samples[:num_private_samples]\n    all_samples = np.around(np.clip(\n        all_samples * 255, a_min=0, a_max=255)).astype(np.uint8)\n    all_samples = np.transpose(all_samples, (0, 2, 3, 1))\n    if class_cond:\n        all_labels = np.concatenate(all_labels, axis=0)\n        all_labels = all_labels[:num_private_samples]\n    else:\n        all_labels = np.zeros(shape=all_samples.shape[0], dtype=np.int64)\n    return all_samples, all_labels", ""]}
{"filename": "dpsda/pytorch_utils.py", "chunked_list": ["import torch\n\n\ndef dev():\n    \"\"\"\n    Get the device to use for torch.distributed.\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    return torch.device('cpu')", ""]}
{"filename": "dpsda/logging.py", "chunked_list": ["import logging\n\n\ndef setup_logging(log_file):\n    log_formatter = logging.Formatter(\n        fmt=('%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  '\n             '%(message)s'),\n        datefmt='%m/%d/%Y %H:%M:%S %p')\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_formatter)\n    root_logger.addHandler(console_handler)\n\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(log_formatter)\n    root_logger.addHandler(file_handler)\n\n    pil_logger = logging.getLogger('PIL')\n    pil_logger.setLevel(logging.INFO)", ""]}
{"filename": "dpsda/dp_counter.py", "chunked_list": ["import faiss\nimport logging\nimport numpy as np\nfrom collections import Counter\nimport torch\n\n\ndef dp_nn_histogram(public_features, private_features, noise_multiplier,\n                    num_packing=1, num_nearest_neighbor=1, mode='L2',\n                    threshold=0.0):\n    assert public_features.shape[0] % num_packing == 0\n    num_true_public_features = public_features.shape[0] // num_packing\n    faiss_res = faiss.StandardGpuResources()\n    if mode == 'L2':\n        index = faiss.IndexFlatL2(public_features.shape[1])\n    elif mode == 'IP':\n        index = faiss.IndexFlatIP(public_features.shape[1])\n    else:\n        raise Exception(f'Unknown mode {mode}')\n    if torch.cuda.is_available():\n        index = faiss.index_cpu_to_gpu(faiss_res, 0, index)\n\n    index.add(public_features)\n    logging.info(f'Number of samples in index: {index.ntotal}')\n\n    _, ids = index.search(private_features, k=num_nearest_neighbor)\n    logging.info('Finished search')\n    counter = Counter(list(ids.flatten()))\n    count = np.zeros(shape=num_true_public_features)\n    for k in counter:\n        count[k % num_true_public_features] += counter[k]\n    logging.info(f'Clean count sum: {np.sum(count)}')\n    logging.info(f'Clean count num>0: {np.sum(count > 0)}')\n    logging.info(f'Largest clean counters: {sorted(count)[::-1][:50]}')\n    count = np.asarray(count)\n    clean_count = count.copy()\n    count += (np.random.normal(size=len(count)) * np.sqrt(num_nearest_neighbor)\n              * noise_multiplier)\n    logging.info(f'Noisy count sum: {np.sum(count)}')\n    logging.info(f'Noisy count num>0: {np.sum(count > 0)}')\n    logging.info(f'Largest noisy counters: {sorted(count)[::-1][:50]}')\n    count = np.clip(count, a_min=threshold, a_max=None)\n    count = count - threshold\n    logging.info(f'Clipped noisy count sum: {np.sum(count)}')\n    logging.info(f'Clipped noisy count num>0: {np.sum(count > 0)}')\n    logging.info(f'Clipped largest noisy counters: {sorted(count)[::-1][:50]}')\n    return count, clean_count", "\n\n    \n\n"]}
