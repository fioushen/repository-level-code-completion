{"filename": "eval_atrr.py", "chunked_list": ["\"\"\"\nMain script for evaluating the baseline method ATRR:\nPang, Tianyu, et al. \"Adversarial Training with Rectified Rejection.\" arXiv preprint arXiv:2105.14785 (2021).\nhttps://arxiv.org/pdf/2105.14785.pdf\n\nBased on the official code from: https://github.com/P2333/Rectified-Rejection\n\"\"\"\n\nimport argparse\nimport logging", "import argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy", "import utils.torch\nimport utils.numpy\nimport models\nimport json\nimport math\nfrom attacks.objectives import *\nfrom attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset\nfrom utils.constants import *\nfrom imgaug import augmenters as iaa", "from utils.constants import *\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom autoattack import AutoAttack\n\n\ndef eval_outer_attack(attack_method,\n                      outer_attack_config, \n                      model, \n                      test_dataloader, \n                      num_classes,\n                      threshold, \n                      tempC, \n                      epsilon, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    if attack_method == 'pgd':\n        objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_accept_misclassify_loss)\n        attacker = MBATRRLinfPGDAttack(model,\n                                     objective,\n                                     tempC=tempC,\n                                     epsilon=epsilon,\n                                     **outer_attack_config)\n    elif attack_method == 'auto_attack':\n        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n    elif attack_method == 'ATRROA':\n        objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_targeted_loss)\n        attacker = MBATRRLinfPGDAttackMultitargeted(\n                                    model,\n                                    objective,\n                                    num_classes,\n                                    tempC=tempC,\n                                    epsilon=epsilon,\n                                    **outer_attack_config)\n    else:\n        raise KeyError(f'Not supported attack method {attack_method}')\n\n    adv_labels = None\n    adv_probs = None\n    adv_aux_probs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\n        # large perturbations\n        if attack_method == 'auto_attack':\n            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        else:\n            adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            adv_logits, adv_aux_outputs = model(adv_inputs, return_aux=True)\n\n        con_pre, _ = torch.softmax(adv_logits * tempC, dim=1).max(1) # predicted label and confidence\n        adv_aux_outputs = adv_aux_outputs.sigmoid().squeeze()\n        evi_outputs = con_pre * adv_aux_outputs\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        adv_aux_probs = utils.numpy.concatenate(adv_aux_probs, evi_outputs.detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if cnt >= n_samples:\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = np.logical_and(adv_preds != adv_labels, adv_aux_probs >= threshold)\n    return adv_error", "\n\ndef eval_inner_attack(attack_method,\n                      inner_attack_config, \n                      model, \n                      test_dataloader, \n                      threshold, \n                      tempC, \n                      epsilon, \n                      eps0_range, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    inner_attack_results = {}\n    for epsilon_0 in eps0_range:\n        if attack_method == 'ATRRIA':\n            radius_objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_reject_loss)\n            radius_attacker = MBATRRLinfPGDAttack(model,\n                                                radius_objective,\n                                                tempC=tempC,\n                                                epsilon=epsilon_0,\n                                                **inner_attack_config)\n        else:\n            raise KeyError(f'Not supported attack method {attack_method}')\n        cnt = 0\n        adv_probs_2 = None\n        adv_aux_probs_2 = None\n        for b, (inputs, targets) in enumerate(test_dataloader):\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n            # small perturbations\n            if epsilon_0 > 0.:\n                adv_inputs = radius_attacker.perturb(inputs, targets)\n            else:\n                adv_inputs = inputs\n\n            with torch.no_grad():\n                adv_logits, adv_aux_outputs = model(adv_inputs, return_aux=True)\n\n            con_pre, _ = torch.softmax(adv_logits * tempC, dim=1).max(1) # predicted label and confidence\n            adv_aux_outputs = adv_aux_outputs.sigmoid().squeeze()\n            evi_outputs = con_pre * adv_aux_outputs\n            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n            adv_aux_probs_2 = utils.numpy.concatenate(adv_aux_probs_2, evi_outputs.detach().cpu().numpy())\n            cnt += inputs.shape[0]\n            if cnt >= n_samples:\n                break\n\n        adv_error_2 = adv_aux_probs_2 < threshold # regard rejection as error\n        inner_attack_results[epsilon_0] = adv_error_2\n    return inner_attack_results", "        \n\ndef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n    \n    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\n    curve_x = []\n    curve_y = []\n    for epsilon_0 in inner_attack_results:\n        curve_x.append(epsilon_0/epsilon)\n        adv_error_2 = inner_attack_results[epsilon_0]\n        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n        curve_y.append(1 - final_adv_error)\n        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\n    return np.array(curve_x), np.array(curve_y)", "\n\ndef eval(model, test_dataloader, tpr, tempC, logger):\n    model.eval()\n\n    clean_probs = None\n    clean_labels = None\n    clean_aux_probs = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs, aux_outputs = model(inputs, return_aux=True)\n\n        con_pre, _ = torch.softmax(outputs * tempC, dim=1).max(1) # predicted label and confidence\n        aux_outputs = aux_outputs.sigmoid().squeeze()\n        evi_outputs = con_pre * aux_outputs\n        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n        clean_aux_probs = utils.numpy.concatenate(clean_aux_probs, evi_outputs.detach().cpu().numpy())\n        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\n    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n    val_probs = clean_probs[test_N:]\n    val_labels = clean_labels[test_N:]\n    val_aux_probs = clean_aux_probs[test_N:]\n    test_probs = clean_probs[:test_N]\n    test_labels = clean_labels[:test_N]\n    test_aux_probs = clean_aux_probs[:test_N]\n\n    # Find the confidence threshold using a validation set\n    val_preds = np.argmax(val_probs, axis=1)\n    val_errors = (val_preds != val_labels)\n    # sorted_val_aux_probs = np.sort(val_aux_probs)\n    sorted_val_aux_probs = np.sort(val_aux_probs[np.logical_not(val_errors)])  # sorted confidence of correctly classified\n    cutoff = math.floor(sorted_val_aux_probs.shape[0] * round((1. - tpr), 2))\n    thresh_detec = sorted_val_aux_probs[cutoff]\n\n    test_preds = np.argmax(test_probs, axis=1)\n    test_errors = (test_preds != test_labels)\n    test_acc = 1. - np.mean(test_errors)\n    # Clean accuracy within the accepted inputs\n    mask_accept = (test_aux_probs >= thresh_detec)\n    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n    rejection_rate = 1. - np.mean(mask_accept)\n    logger.info(f\"threshold: {thresh_detec:.4f}\")\n    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")\n\n    return thresh_detec", "\ndef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n    if combine_adv_error is None:\n        return curr_adv_error\n    else:\n        return curr_adv_error|combine_adv_error\n\n\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n    return combine_inner_results", "def combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n    return combine_inner_results\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n    parser.add_argument('--tempC', type=float, default=1.0, help=\"temperature parameter\")\n    # args parse\n    return parser.parse_args()", "\n\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    epsilon = config['epsilon']\n    batch_size = config['batch_size']\n    tempC = args.tempC\n    checkpoint_dir = args.checkpoint_dir\n\n    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    N_class = N_CLASSES[dataset]\n    tpr = TPR_THRESHOLD[dataset]\n    if dataset == 'cifar10':\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        \n    elif dataset == 'gtsrb':\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n\n    elif dataset == 'mnist':\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\n    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNetTwoBranchDenseV1(N_class, resolution, out_dim=1, use_BN=True, along=True)\n    elif model_arch == \"resnet20\":\n        model = models.ResNetTwoBranchDenseV1(N_class, resolution, blocks=[3, 3, 3], out_dim=1, use_BN=True, along=True)\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNetTwoBranchDenseV1(N_class, resolution, depth=28, width=10, out_dim=1, use_BN=True, along=True)\n    else:\n        raise ValueError\n\n    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n    model.load_state_dict(checkpoint['model'])\n    model.cuda()\n\n    # evaluate clean and get threshold\n    threshold = eval(model, test_dataloader, tpr, tempC, logger)\n\n    final_outer_adv_error = None\n    final_inner_attack_results = None\n    \n    # evaluate robustness with rejection under AutoAttack\n    outer_attack_config = None\n    outer_attack_method = 'auto_attack'\n    outer_file_name = f\"outer_{outer_attack_method}\"\n    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n    outer_file_name += \"_result.npy\"\n    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n    else:\n        outer_adv_error = eval_outer_attack(outer_attack_method,\n                                            outer_attack_config, \n                                              model, \n                                              test_dataloader, \n                                              N_class,\n                                              threshold, \n                                              tempC, \n                                              epsilon,  \n                                              n_samples=N_SAMPLES)\n    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n    \n    # evaluate robustness with rejection under multitargeted attack\n    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n    outer_attack_method = 'ATRROA'\n    inner_attack_method = 'ATRRIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        outer_attack_config.update({'base_lr': base_lr})\n        outer_file_name = f\"outer_{outer_attack_method}\"\n        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n        outer_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n        else:\n            outer_adv_error = eval_outer_attack(outer_attack_method,\n                                                outer_attack_config, \n                                                  model, \n                                                  test_dataloader, \n                                                  N_class,\n                                                  threshold, \n                                                  tempC, \n                                                  epsilon, \n                                                  n_samples=N_SAMPLES)\n        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                     inner_attack_config, \n                                                      model, \n                                                      test_dataloader, \n                                                      threshold, \n                                                      tempC, \n                                                      epsilon, \n                                                      eps0_range, \n                                                      n_samples=N_SAMPLES)\n        \n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        \n        logger.info(f'{outer_file_name} saved!')\n        logger.info(f'{inner_file_name} saved!')\n        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n      \n    # Evaluate robustness curve\n    logger.info('Final results under an ensemble of attacks:')\n    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "train_at.py", "chunked_list": ["import argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy", "import utils.torch\nimport utils.numpy\nimport models\nimport json\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom attacks.objectives import UntargetedObjective\nfrom attacks.mb_pgd_attack import MBLinfPGDAttack\nfrom utils.dataset import CustomDataset\n", "from utils.dataset import CustomDataset\n\n\ndef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n    model.eval()\n\n    clean_losses = None\n    clean_accs = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n\n    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%\".format(np.mean(clean_losses), np.mean(clean_accs)*100))\n\n    adv_losses = None\n    adv_accs = None\n    adv_successes = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        # small perturbations\n        adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            logits = model(adv_inputs)\n        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n\n    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(np.mean(adv_losses), np.mean(adv_accs)*100))", "\ndef lr_schedule(t, lr_max):\n    if t < 100:\n        return lr_max\n    elif t < 105:\n        return lr_max / 10.\n    else:\n        return lr_max / 100.\n\ndef train_robust(model, \n                train_dataloader,\n                attacker, \n                optimizer, \n                scheduler, \n                fraction,\n                N_class,\n                epoch,\n                lr_max,\n                print_freq,\n                logger):\n    \n    num_training_iter = len(train_dataloader)\n    for b, (inputs, targets) in enumerate(train_dataloader):\n        \n        if scheduler is None:\n            epoch_now = epoch + (b + 1) / len(train_dataloader)\n            lr = lr_schedule(epoch_now, lr_max)\n            optimizer.param_groups[0].update(lr=lr)\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        split = int((1-fraction)*inputs.size(0))\n        # update fraction for correct loss computation\n        fraction = 1 - split / float(inputs.size(0))\n\n        clean_inputs = inputs[:split]\n        adv_inputs = inputs[split:]\n        clean_targets = targets[:split]\n        adv_targets = targets[split:]\n\n        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\n        if adv_inputs.shape[0] < inputs.shape[0]: # fraction is not 1\n            inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n        else:\n            inputs = adv_examples\n\n        model.train()\n        optimizer.zero_grad()\n        logits = model(inputs)\n        clean_logits = logits[:split]\n        adv_logits = logits[split:]\n\n        adv_loss = utils.torch.classification_loss(adv_logits, adv_targets)\n        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets)\n\n        if adv_inputs.shape[0] < inputs.shape[0]:\n            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets)\n            # clean_error = utils.torch.classification_error(clean_logits, clean_targets)\n            loss = (1 - fraction) * clean_loss + fraction * adv_loss\n        else:\n            clean_loss = torch.zeros(1)\n            # clean_error = torch.zeros(1)\n            loss = adv_loss\n\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            logger.info(\"Progress: {:d}/{:d}, adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(b+1, \n                                                                                        num_training_iter, \n                                                                                        adv_loss.item(),\n                                                                                        adv_acc.item()*100,\n                                                                                        ))", "\ndef train_robust(model, \n                train_dataloader,\n                attacker, \n                optimizer, \n                scheduler, \n                fraction,\n                N_class,\n                epoch,\n                lr_max,\n                print_freq,\n                logger):\n    \n    num_training_iter = len(train_dataloader)\n    for b, (inputs, targets) in enumerate(train_dataloader):\n        \n        if scheduler is None:\n            epoch_now = epoch + (b + 1) / len(train_dataloader)\n            lr = lr_schedule(epoch_now, lr_max)\n            optimizer.param_groups[0].update(lr=lr)\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        split = int((1-fraction)*inputs.size(0))\n        # update fraction for correct loss computation\n        fraction = 1 - split / float(inputs.size(0))\n\n        clean_inputs = inputs[:split]\n        adv_inputs = inputs[split:]\n        clean_targets = targets[:split]\n        adv_targets = targets[split:]\n\n        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\n        if adv_inputs.shape[0] < inputs.shape[0]: # fraction is not 1\n            inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n        else:\n            inputs = adv_examples\n\n        model.train()\n        optimizer.zero_grad()\n        logits = model(inputs)\n        clean_logits = logits[:split]\n        adv_logits = logits[split:]\n\n        adv_loss = utils.torch.classification_loss(adv_logits, adv_targets)\n        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets)\n\n        if adv_inputs.shape[0] < inputs.shape[0]:\n            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets)\n            # clean_error = utils.torch.classification_error(clean_logits, clean_targets)\n            loss = (1 - fraction) * clean_loss + fraction * adv_loss\n        else:\n            clean_loss = torch.zeros(1)\n            # clean_error = torch.zeros(1)\n            loss = adv_loss\n\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            logger.info(\"Progress: {:d}/{:d}, adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(b+1, \n                                                                                        num_training_iter, \n                                                                                        adv_loss.item(),\n                                                                                        adv_acc.item()*100,\n                                                                                        ))", "        \n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n    # args parse\n    return parser.parse_args()\n\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n        \n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n        \n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n    \n  \n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD-pp':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n        scheduler = None\n    elif optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n    attacker = MBLinfPGDAttack(model, \n                                objective, \n                                epsilon=epsilon, \n                                max_iterations=nb_iter, \n                                base_lr=eps_iter, \n                                momentum=0.0, \n                                lr_factor=1.5, \n                                backtrack=False, \n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_robust(model, \n                    train_dataloader,\n                    attacker,\n                    optimizer, \n                    scheduler, \n                    fraction,\n                    N_class,\n                    epoch,\n                    lr,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n        \n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n        \n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n    \n  \n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD-pp':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n        scheduler = None\n    elif optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n    attacker = MBLinfPGDAttack(model, \n                                objective, \n                                epsilon=epsilon, \n                                max_iterations=nb_iter, \n                                base_lr=eps_iter, \n                                momentum=0.0, \n                                lr_factor=1.5, \n                                backtrack=False, \n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_robust(model, \n                    train_dataloader,\n                    attacker,\n                    optimizer, \n                    scheduler, \n                    fraction,\n                    N_class,\n                    epoch,\n                    lr,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "train_ccat.py", "chunked_list": ["\"\"\"\nMain script for training the baseline method CCAT:\nStutz, David, Matthias Hein, and Bernt Schiele. \"Confidence-calibrated adversarial training: Generalizing to unseen attacks.\"\nInternational Conference on Machine Learning. PMLR, 2020.\n\nBased on the official code from: https://github.com/davidstutz/confidence-calibrated-adversarial-training\n\"\"\"\n\nimport argparse\nimport logging", "import argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy", "import utils.torch\nimport utils.numpy\nimport models\nimport json\nimport functools\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom attacks.objectives import UntargetedObjective\nfrom attacks.mb_pgd_attack import MBLinfPGDAttack\nfrom utils.dataset import CustomDataset", "from attacks.mb_pgd_attack import MBLinfPGDAttack\nfrom utils.dataset import CustomDataset\n\n\ndef test(model, attacker, test_dataloader, max_batches, logger):\n    model.eval()\n\n    clean_losses = None\n    clean_accs = None\n    clean_confs = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n        clean_confs = utils.numpy.concatenate(clean_confs, torch.max(torch.nn.functional.softmax(outputs, dim=1), dim=1)[0].detach().cpu().numpy())\n\n    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%, clean_conf: {:.4f}\".format(np.mean(clean_losses), np.mean(clean_accs)*100, np.mean(clean_confs)))\n\n    adv_losses = None\n    adv_accs = None\n    adv_confs = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        # small perturbations\n        adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            logits = model(adv_inputs)\n        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n        adv_confs = utils.numpy.concatenate(adv_confs, torch.max(torch.nn.functional.softmax(logits, dim=1), dim=1)[0].detach().cpu().numpy())\n\n    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%, adv_conf: {:.4f}\".format(np.mean(adv_losses), np.mean(adv_accs)*100, np.mean(adv_confs)))", "\n\ndef power_transition(perturbations, epsilon=0.3, gamma=1):\n    \"\"\"\n    Power transition rule.\n\n    :param perturbations: perturbations\n    :type perturbations: torch.autograd.Variable\n    :param norm: norm\n    :type norm: attacks.norms.Norm\n    :param epsilon: epsilon\n    :type epsilon: float\n    :param gamma: gamma\n    :type gamma: float\n    :return: gamma, norms\n    :rtype: torch.autograd.Variable, torch.autograd.Variable\n    \"\"\"\n\n    # returned value determines importance of uniform distribution:\n    # (1 - ret)*one_hot + ret*uniform\n\n    norms = torch.max(torch.abs(perturbations.view(perturbations.size()[0], -1)), dim=1)[0]\n    return 1 - torch.pow(1 - torch.min(torch.ones_like(norms), norms / epsilon), gamma), norms", "\ndef partial(f, *args, **kwargs):\n    \"\"\"\n    Create partial while preserving __name__ and __doc__.\n\n    :param f: function\n    :type f: callable\n    :param args: arguments\n    :type args: dict\n    :param kwargs: keyword arguments\n    :type kwargs: dict\n    :return: partial\n    :rtype: callable\n    \"\"\"\n    p = functools.partial(f, *args, **kwargs)\n    functools.update_wrapper(p, f)\n    return p", "\ndef train_robust(model, \n                train_dataloader,\n                attacker, \n                transition,\n                optimizer, \n                scheduler,\n                fraction,\n                N_class,\n                print_freq,\n                logger):\n    \n    num_training_iter = len(train_dataloader)\n    for b, (inputs, targets) in enumerate(train_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        distributions = utils.torch.one_hot(targets, N_class)\n\n        split = int((1-fraction)*inputs.size(0))\n        # update fraction for correct loss computation\n        fraction = 1 - split / float(inputs.size(0))\n\n        clean_inputs = inputs[:split]\n        adv_inputs = inputs[split:]\n        clean_targets = targets[:split]\n        adv_targets = targets[split:]\n        clean_distributions = distributions[:split]\n        adv_distributions = distributions[split:]\n\n        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\n        if adv_inputs.shape[0] < inputs.shape[0]: # fraction is not 1\n            inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n        else:\n            inputs = adv_examples\n        \n        gamma, adv_norms = transition(adv_examples - adv_inputs)\n        gamma = utils.torch.expand_as(gamma, adv_distributions)\n        adv_distributions = adv_distributions*(1 - gamma)\n        adv_distributions += gamma*torch.ones_like(adv_distributions)/N_class\n        \n        model.train()\n        optimizer.zero_grad()\n        logits = model(inputs)\n        clean_logits = logits[:split]\n        adv_logits = logits[split:]\n\n        adv_loss = utils.torch.cross_entropy_divergence(adv_logits, adv_distributions)\n        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets)\n        adv_conf = torch.max(torch.nn.functional.softmax(adv_logits, dim=1), dim=1)[0]\n\n        if adv_inputs.shape[0] < inputs.shape[0]:\n            clean_loss = utils.torch.cross_entropy_divergence(clean_logits, clean_distributions)\n            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets)\n            clean_conf = torch.max(torch.nn.functional.softmax(clean_logits, dim=1), dim=1)[0]\n            loss = (1 - fraction) * clean_loss + fraction * adv_loss\n        else:\n            clean_loss = torch.zeros(1)\n            clean_acc = torch.ones(1)\n            loss = adv_loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            logger.info(\"Progress: {:d}/{:d}, clean_loss: {:.2f}, adv_loss: {:.2f}, clean_acc: {:.2f}%, clean_conf: {:.4f}, adv_acc: {:.2f}%, adv_conf: {:.4f}\".format(b+1, \n                                                                                                                                num_training_iter, \n                                                                                                                                clean_loss.item(),\n                                                                                                                                adv_loss.item(),\n                                                                                                                                clean_acc.item()*100,\n                                                                                                                                clean_conf.mean().item(),\n                                                                                                                                adv_acc.item()*100,\n                                                                                                                                adv_conf.mean().item()\n                                                                                                                                ))", "        \n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n    # args parse\n    return parser.parse_args()\n\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    gamma = config['gamma']\n    attack_base_lr = config['attack_base_lr']\n    max_iterations = config['max_iterations']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n    \n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n  \n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n    objective = UntargetedObjective(loss=utils.torch.f7p_loss)\n    attacker = MBLinfPGDAttack(model, \n                                objective, \n                                epsilon=epsilon, \n                                max_iterations=max_iterations, \n                                base_lr=attack_base_lr, \n                                momentum=0.9, \n                                lr_factor=1.5, \n                                backtrack=True, \n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    transition = partial(power_transition, epsilon=epsilon, gamma=gamma)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_robust(model, \n                    train_dataloader,\n                    attacker,\n                    transition,\n                    optimizer, \n                    scheduler,\n                    fraction,\n                    N_class,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    gamma = config['gamma']\n    attack_base_lr = config['attack_base_lr']\n    max_iterations = config['max_iterations']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n    \n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n  \n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n    objective = UntargetedObjective(loss=utils.torch.f7p_loss)\n    attacker = MBLinfPGDAttack(model, \n                                objective, \n                                epsilon=epsilon, \n                                max_iterations=max_iterations, \n                                base_lr=attack_base_lr, \n                                momentum=0.9, \n                                lr_factor=1.5, \n                                backtrack=True, \n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    transition = partial(power_transition, epsilon=epsilon, gamma=gamma)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_robust(model, \n                    train_dataloader,\n                    attacker,\n                    transition,\n                    optimizer, \n                    scheduler,\n                    fraction,\n                    N_class,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "train_rcd.py", "chunked_list": ["\"\"\"\nMain script for training the baseline method RCD:\nProvably robust classification of adversarial examples with detection, Fatemeh Sheikholeslami, Ali Lotfi Rezaabad,\nZico Kolter (https://openreview.net/pdf?id=sRA5rLNpmQc), ICLR 2021\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json", "import models\nimport json\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom attacks.objectives import UntargetedObjective\nfrom attacks.mb_pgd_attack import MBRCDStratifiedLinfPGDAttack\nfrom utils.dataset import CustomDataset\n\n\ndef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n    model.eval()\n\n    clean_accs = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        clean_error = utils.torch.classification_error(outputs, targets, reduction='none')\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - clean_error.detach().cpu().numpy())\n\n    logger.info(\"clean acc: {:.2%}\".format(np.mean(clean_accs)))\n\n    adv_accs_1 = None\n    adv_accs_2 = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n\n        # Generate adversarial examples\n        adv_inputs = attacker.perturb(inputs, targets)\n\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_split = int(attacker.fraction * adv_logits.size(0))\n        adv_logits_1 = adv_logits[:adv_split]\n        adv_targets_1 = targets[:adv_split]\n\n        adv_logits_2 = adv_logits[adv_split:]\n        adv_targets_2 = targets[adv_split:]\n\n        if attacker.fraction > 0.0:\n            adv_acc_1 = 1 - utils.torch.classification_error(adv_logits_1, adv_targets_1, reduction=\"none\")\n            adv_accs_1 = utils.numpy.concatenate(adv_accs_1, adv_acc_1.detach().cpu().numpy())\n\n        if attacker.fraction < 1.0:\n            adv_d_error_2 = utils.torch.classification_error(adv_logits_2, torch.ones_like(adv_targets_2) * N_class, reduction=\"none\")\n            adv_error_2 = utils.torch.classification_error(adv_logits_2, adv_targets_2, reduction=\"none\")\n            adv_acc_2 = 1 - torch.min(torch.stack([adv_error_2, adv_d_error_2], dim=1), dim=1)[0].float()\n            adv_accs_2 = utils.numpy.concatenate(adv_accs_2, adv_acc_2.detach().cpu().numpy())\n\n    if adv_accs_1 is None:\n        adv_accs_1 = np.ones(1)\n    if adv_accs_2 is None:\n        adv_accs_2 = np.ones(1)\n\n    logger.info(\"adv_acc_1: {:.2%}, adv_acc_2: {:.2%}\".format(np.mean(adv_accs_1), np.mean(adv_accs_2)))", "\ndef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n    model.eval()\n\n    clean_accs = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        clean_error = utils.torch.classification_error(outputs, targets, reduction='none')\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - clean_error.detach().cpu().numpy())\n\n    logger.info(\"clean acc: {:.2%}\".format(np.mean(clean_accs)))\n\n    adv_accs_1 = None\n    adv_accs_2 = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n\n        # Generate adversarial examples\n        adv_inputs = attacker.perturb(inputs, targets)\n\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_split = int(attacker.fraction * adv_logits.size(0))\n        adv_logits_1 = adv_logits[:adv_split]\n        adv_targets_1 = targets[:adv_split]\n\n        adv_logits_2 = adv_logits[adv_split:]\n        adv_targets_2 = targets[adv_split:]\n\n        if attacker.fraction > 0.0:\n            adv_acc_1 = 1 - utils.torch.classification_error(adv_logits_1, adv_targets_1, reduction=\"none\")\n            adv_accs_1 = utils.numpy.concatenate(adv_accs_1, adv_acc_1.detach().cpu().numpy())\n\n        if attacker.fraction < 1.0:\n            adv_d_error_2 = utils.torch.classification_error(adv_logits_2, torch.ones_like(adv_targets_2) * N_class, reduction=\"none\")\n            adv_error_2 = utils.torch.classification_error(adv_logits_2, adv_targets_2, reduction=\"none\")\n            adv_acc_2 = 1 - torch.min(torch.stack([adv_error_2, adv_d_error_2], dim=1), dim=1)[0].float()\n            adv_accs_2 = utils.numpy.concatenate(adv_accs_2, adv_acc_2.detach().cpu().numpy())\n\n    if adv_accs_1 is None:\n        adv_accs_1 = np.ones(1)\n    if adv_accs_2 is None:\n        adv_accs_2 = np.ones(1)\n\n    logger.info(\"adv_acc_1: {:.2%}, adv_acc_2: {:.2%}\".format(np.mean(adv_accs_1), np.mean(adv_accs_2)))", "\n\ndef lr_schedule(t, lr_max):\n    if t < 100:\n        return lr_max\n    elif t < 105:\n        return lr_max / 10.\n    else:\n        return lr_max / 100.\n", "\n\ndef train_robust_detection(model,\n                           train_dataloader,\n                           attacker,\n                           optimizer,\n                           scheduler,\n                           fraction,\n                           lamb_1,\n                           lamb_2,\n                           epoch,\n                           lr_max,\n                           N_class,\n                           print_freq,\n                           logger):\n\n    num_training_iter = len(train_dataloader)\n    for b, (inputs, targets) in enumerate(train_dataloader):\n\n        if scheduler is None:\n            epoch_now = epoch + (b + 1) / num_training_iter\n            lr = lr_schedule(epoch_now, lr_max)\n            optimizer.param_groups[0].update(lr=lr)\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        b_size = inputs.size(0)\n        split = int((1-fraction) * b_size)\n        # update fraction for correct loss computation\n        fraction = 1 - split / float(b_size)\n\n        clean_inputs = inputs[:split]\n        adv_inputs = inputs[split:]\n        clean_targets = targets[:split]\n        adv_targets = targets[split:]\n\n        double_adv_inputs = torch.cat((adv_inputs, clean_inputs), 0)\n        double_adv_targets = torch.cat((adv_targets, clean_targets), 0)\n        # Generate adversarial examples\n        adv_examples = attacker.perturb(double_adv_inputs, double_adv_targets)\n\n        if adv_inputs.shape[0] < b_size: # fraction is not 1\n            combined_inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n        else:\n            combined_inputs = adv_examples\n\n        model.train()\n        optimizer.zero_grad()\n        logits = model(combined_inputs)\n        clean_logits = logits[:split]\n        adv_logits = logits[split:]\n\n        adv_split = int(attacker.fraction * adv_logits.size(0))\n        if attacker.fraction > 0.0:\n            # robust loss\n            adv_logits_1 = adv_logits[:adv_split]\n            adv_targets_1 = double_adv_targets[:adv_split]\n            adv_loss_1 = utils.torch.classification_loss(adv_logits_1, adv_targets_1, reduction=\"mean\")\n            adv_acc_1 = 1 - utils.torch.classification_error(adv_logits_1, adv_targets_1, reduction=\"mean\")\n        else:\n            adv_loss_1 = torch.zeros(1)\n            adv_acc_1 = torch.ones(1)\n\n        if attacker.fraction < 1.0:\n            # robust abstain loss\n            adv_logits_2 = adv_logits[adv_split:]\n            adv_targets_2 = double_adv_targets[adv_split:]\n            adv_loss_2 = utils.torch.robust_abstain_loss(adv_logits_2, adv_targets_2, reduction=\"mean\")\n            adv_error_2 = utils.torch.classification_error(adv_logits_2, adv_targets_2, reduction=\"none\")\n            adv_d_error_2 = utils.torch.classification_error(adv_logits_2, torch.ones_like(adv_targets_2) * N_class, reduction=\"none\")\n            adv_acc_2 = 1 - torch.mean(torch.min(torch.stack([adv_error_2, adv_d_error_2], dim=1), dim=1)[0].float())\n        else:\n            adv_loss_2 = torch.zeros(1)\n            adv_acc_2 = torch.ones(1)\n\n        if adv_inputs.shape[0] < b_size:\n            # Loss on clean inputs\n            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets, reduction=\"mean\")\n            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets, reduction=\"mean\")\n            # Combined loss\n            loss = fraction * (adv_loss_1 + lamb_1 * adv_loss_2) + (1 - fraction) * lamb_2 * clean_loss\n        else:\n            clean_acc = torch.ones(1)\n            loss = adv_loss_1 + lamb_1 * adv_loss_2\n\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, clean_acc: {:.2%}, adv_acc_1: {:.2%}, adv_acc_2: {:.2%}\".format(b+1,\n                                                                                                        num_training_iter,\n                                                                                                        loss.item(),\n                                                                                                        clean_acc.item(),\n                                                                                                        adv_acc_1.item(),\n                                                                                                        adv_acc_2.item()))", "\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n    # args parse\n    return parser.parse_args()\n", "\n\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    # fraction of clean inputs\n    fraction = config['fraction']\n    # adv_fraction: fraction of robust abstain examples\n    adv_fraction = config['adv_fraction']\n    lamb_1 = config['lamb_1']\n    lamb_2 = config['lamb_2']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n\n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class+1, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class+1, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class+1, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD-pp':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n        scheduler = None\n    elif optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n    if fraction >= 1.0:\n        raise ValueError(\"Fraction of clean inputs should be less than 1\")\n\n    objective_1 = UntargetedObjective(loss=utils.torch.classification_loss)\n    objective_2 = UntargetedObjective(loss=utils.torch.robust_abstain_loss)\n    attacker = MBRCDStratifiedLinfPGDAttack(model,\n                                objective_1,\n                                objective_2,\n                                fraction=adv_fraction,\n                                epsilon=epsilon,\n                                max_iterations=nb_iter,\n                                base_lr=eps_iter,\n                                momentum=0.0,\n                                lr_factor=1.5,\n                                backtrack=False,\n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n\n        train_robust_detection(model,\n                            train_dataloader,\n                            attacker,\n                            optimizer,\n                            scheduler,\n                            fraction,\n                            lamb_1,\n                            lamb_2,\n                            epoch,\n                            lr,\n                            N_class,\n                            print_freq,\n                            logger)\n        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_cpr.py", "chunked_list": ["\"\"\"\nMain script for evaluating the proposed method CPR.\n\"\"\"\n\nfrom functools import partial\nimport argparse\nimport logging\nimport os\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json\nimport math", "import json\nimport math\nfrom attacks.objectives import *\nfrom attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset\nfrom utils.constants import *\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom autoattack import AutoAttack\n", "from autoattack import AutoAttack\n\n\nclass ConsistentRejDefense(torch.nn.Module):\n\n    def __init__(self, model, defense_attack_params):\n        super(ConsistentRejDefense, self).__init__()\n        self.model = model\n        objective = UntargetedObjective(loss=utils.torch.classification_loss)\n        self.defense_attacker = MBLinfPGDAttack(model,\n                                    objective,\n                                    **defense_attack_params)\n\n    def transform(self, inputs):\n        with torch.no_grad():\n            logits = self.model(inputs)\n        pred_labels = torch.max(logits, axis=1)[1]\n        adv_inputs = self.defense_attacker.perturb(inputs, pred_labels)\n        return adv_inputs\n\n    def forward(self, inputs):\n        with torch.no_grad():\n            logits = self.model(inputs)\n        pred_labels = torch.max(logits, axis=1)[1]\n        adv_inputs = self.defense_attacker.perturb(inputs, pred_labels)\n        with torch.no_grad():\n            adv_logits = self.model(adv_inputs)\n        adv_pred_labels = torch.max(adv_logits, axis=1)[1]\n        adv_confs = (pred_labels==adv_pred_labels).float()\n        return logits, adv_confs", "\n\ndef eval_outer_attack(attack_method,\n                      outer_attack_config,\n                      model,\n                      test_dataloader,\n                      num_classes,\n                      threshold,\n                      epsilon,\n                      defense_attack_params,\n                      validation, \n                      total_samples,\n                      n_samples=N_SAMPLES):\n    model.eval()\n    defense = ConsistentRejDefense(model, defense_attack_params)\n    if attack_method == 'auto_attack':\n        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n    elif attack_method == 'HCMOA':\n        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n        attacker = MBConfLinfPGDAttackMultitargeted(model,\n                                                    objective,\n                                                    num_classes,\n                                                    epsilon=epsilon,\n                                                    **outer_attack_config)\n    elif attack_method == 'CHCMOA':\n        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n        attacker = MBCONSRLinfPGDAttackMultitargeted(model,\n                                                defense,\n                                                objective,\n                                                num_classes,\n                                                epsilon=epsilon,\n                                                **outer_attack_config)\n    else:\n        raise KeyError(f'Not supported attack method {attack_method}')\n\n    adv_labels = None\n    adv_probs = None\n    adv_confs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        if validation and cnt < total_samples - n_samples:\n            # If validation: the last `n_samples` are used for evaluating the robustness with rejection\n            cnt += inputs.shape[0]\n            continue\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\n        # large perturbations\n        if attack_method == 'auto_attack':\n            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        else:\n            adv_inputs = attacker.perturb(inputs, targets)\n        adv_logits, batch_adv_conf = defense(adv_inputs)\n\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        adv_confs = utils.numpy.concatenate(adv_confs, batch_adv_conf.detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if (not validation) and cnt >= n_samples:\n            # If not validation: the first `n_samples` are used for evaluating the robustness with rejection\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = np.logical_and(adv_preds != adv_labels, adv_confs >= threshold)\n\n    return adv_error", "\n\ndef eval_inner_attack(attack_method,\n                      inner_attack_config,\n                      model,\n                      test_dataloader,\n                      num_classes,\n                      threshold,\n                      epsilon,\n                      eps0_range,\n                      defense_attack_params,\n                      validation, \n                      total_samples,\n                      n_samples=N_SAMPLES):\n    model.eval()\n    defense = ConsistentRejDefense(model, defense_attack_params)\n    inner_attack_results = {}\n    for epsilon_0 in eps0_range:\n        if attack_method == 'LCIA':\n            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n        elif attack_method == 'CLCIA':\n            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n        elif attack_method == 'PDIA':\n            radius_objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n        else:\n            raise KeyError(f'Not supported attack method {attack_method}')\n\n        if attack_method == 'CLCIA':\n            radius_attacker = MBCONSRLinfPGDAttack(model,\n                                                defense,\n                                                radius_objective,\n                                                epsilon=epsilon_0,\n                                                **inner_attack_config)\n        elif attack_method == 'PDIA':\n            radius_attacker = MBCONSRLinfPGDInnerAttackMultitargeted(model,\n                                                defense,\n                                                radius_objective,\n                                                num_classes=num_classes,\n                                                epsilon=epsilon_0,\n                                                **inner_attack_config)\n        else:\n            radius_attacker = MBConfLinfPGDAttack(model,\n                                                radius_objective,\n                                                epsilon=epsilon_0,\n                                                **inner_attack_config)\n        cnt = 0\n        adv_probs = None\n        adv_confs = None\n        for b, (inputs, targets) in enumerate(test_dataloader):\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n            if validation and cnt < total_samples - n_samples:\n                cnt += inputs.shape[0]\n                continue\n            # small perturbations\n            if epsilon_0 > 0.:\n                adv_inputs = radius_attacker.perturb(inputs, targets)\n            else:\n                adv_inputs = inputs\n\n            adv_logits, batch_adv_conf = defense(adv_inputs)\n\n            adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n            adv_confs = utils.numpy.concatenate(adv_confs, batch_adv_conf.detach().cpu().numpy())\n            cnt += inputs.shape[0]\n            if (not validation) and cnt >= n_samples:\n                break\n\n        inner_attack_results[epsilon_0] = adv_confs < threshold\n\n    return inner_attack_results", "\n\ndef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n\n    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\n    curve_x = []\n    curve_y = []\n    for epsilon_0 in inner_attack_results:\n        curve_x.append(epsilon_0 / epsilon)\n        adv_error_2 = inner_attack_results[epsilon_0]\n        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n        curve_y.append(1 - final_adv_error)\n        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\n    return np.array(curve_x), np.array(curve_y)", "\n\ndef eval(model, test_dataloader, tpr, defense_attack_params, logger):\n    model.eval()\n    defense = ConsistentRejDefense(model, defense_attack_params)\n\n    clean_probs = None\n    clean_confs = None\n    clean_labels = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        outputs, batch_conf = defense(inputs)\n\n        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n        clean_confs = utils.numpy.concatenate(clean_confs, batch_conf.detach().cpu().numpy())\n\n    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n    val_probs = clean_probs[test_N:]\n    val_labels = clean_labels[test_N:]\n    val_confs = clean_confs[test_N:]\n    test_probs = clean_probs[:test_N]\n    test_labels = clean_labels[:test_N]\n    test_confs = clean_confs[:test_N]\n\n    # Find the confidence threshold using a validation set\n    val_preds = np.argmax(val_probs, axis=1)\n    val_errors = (val_preds != val_labels)\n    val_acc = 1. - np.mean(val_errors)\n    threshold = 0.5\n    val_mask_accept = (val_confs >= threshold)\n    val_acc_with_detection = 1. - np.sum(val_errors & val_mask_accept) / np.sum(val_mask_accept)\n    val_rejection_rate = 1. - np.mean(val_mask_accept)\n    logger.info(f\"threshold: {threshold:.4f}\")\n    logger.info(f\"clean val accuracy: {val_acc:.2%}\")\n    logger.info(f\"clean val accuracy with detection: {val_acc_with_detection:.2%}, clean val rejection rate: {val_rejection_rate:.2%}\")\n\n    test_preds = np.argmax(test_probs, axis=1)\n    test_errors = (test_preds != test_labels)\n    test_acc = 1. - np.mean(test_errors)\n    # Clean accuracy within the accepted inputs\n    test_mask_accept = (test_confs >= threshold)\n    test_acc_with_detection = 1. - np.sum(test_errors & test_mask_accept) / np.sum(test_mask_accept)\n    test_rejection_rate = 1. - np.mean(test_mask_accept)\n    logger.info(f\"clean test accuracy: {test_acc:.2%}\")\n    logger.info(f\"clean test accuracy with detection: {test_acc_with_detection:.2%}, clean test rejection rate: {test_rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-test_rejection_rate)/(test_acc_with_detection+1-test_rejection_rate):.2%}\")\n\n    return threshold", "\n\ndef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n    if combine_adv_error is None:\n        return curr_adv_error\n    else:\n        return curr_adv_error | combine_adv_error\n\n\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\n    return combine_inner_results", "\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\n    return combine_inner_results\n", "\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Evaluate the robustness with rejection of CCAT')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n    parser.add_argument('--validation', action='store_true', help='whether to use validation set to select hyper-parameters')\n    # args parse\n    return parser.parse_args()", "\n\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    epsilon = config['epsilon']\n    batch_size = config['batch_size']\n    checkpoint_dir = args.checkpoint_dir\n    validation = args.validation\n\n    N_class = N_CLASSES[dataset]\n    tpr = TPR_THRESHOLD[dataset]\n    if dataset == 'cifar10':\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n\n        defense_attack_params = {'epsilon': 0.0055,\n                             'max_iterations': 10,\n                             'base_lr': 0.001,\n                             'momentum': 0.0,\n                             'lr_factor': 1.5,\n                             'backtrack': False,\n                             'rand_init_name': \"zero\",\n                             'num_rand_init': 1,\n                             'clip_min': 0.0,\n                             'clip_max': 1.0}\n\n    elif dataset == 'gtsrb':\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n\n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n\n        defense_attack_params = {'epsilon': 0.0055,\n                             'max_iterations': 10,\n                             'base_lr': 0.001,\n                             'momentum': 0.0,\n                             'lr_factor': 1.5,\n                             'backtrack': False,\n                             'rand_init_name': \"zero\",\n                             'num_rand_init': 1,\n                             'clip_min': 0.0,\n                             'clip_max': 1.0}\n\n    elif dataset == 'svhn':\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n\n        defense_attack_params = {'epsilon': 0.0055,\n                             'max_iterations': 10,\n                             'base_lr': 0.001,\n                             'momentum': 0.0,\n                             'lr_factor': 1.5,\n                             'backtrack': False,\n                             'rand_init_name': \"zero\",\n                             'num_rand_init': 1,\n                             'clip_min': 0.0,\n                             'clip_max': 1.0}\n\n    elif dataset == 'mnist':\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        defense_attack_params = {'epsilon': 0.1,\n                             'max_iterations': 20,\n                             'base_lr': 0.01,\n                             'momentum': 0.0,\n                             'lr_factor': 1.5,\n                             'backtrack': False,\n                             'rand_init_name': \"zero\",\n                             'num_rand_init': 1,\n                             'clip_min': 0.0,\n                             'clip_max': 1.0}\n\n    else:\n        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\n    output_dir = checkpoint_dir.replace('checkpoints/', 'results/consistent_rejection_v3/')\n    defense_epsilon = defense_attack_params['epsilon']\n    defense_iter = defense_attack_params['max_iterations']\n    if defense_iter==10:\n        output_dir = os.path.join(output_dir, f'defense_eps_{defense_epsilon}')\n    else:\n        output_dir = os.path.join(output_dir, f'defense_eps_{defense_epsilon}_iter_{defense_iter}')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n    model.load_state_dict(checkpoint['model'])\n\n    model.cuda()\n    # evaluate clean and get threshold\n    threshold = eval(model, test_dataloader, tpr,\n                    defense_attack_params=defense_attack_params,\n                    logger=logger)\n    n_test = len(test_dataset)\n\n    final_outer_adv_error = None\n    final_inner_attack_results = None\n\n    # evaluate robustness with rejection under AutoAttack\n    outer_attack_config = None\n    outer_attack_method = 'auto_attack'\n    outer_file_name = f\"outer_{outer_attack_method}\"\n    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n    outer_file_name += '_val_' if validation else ''\n    outer_file_name += \"_result.npy\"\n    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n    else:\n        outer_adv_error = eval_outer_attack(outer_attack_method,\n                                            outer_attack_config,\n                                            model,\n                                            test_dataloader,\n                                            N_class,\n                                            threshold,\n                                            epsilon,\n                                            defense_attack_params=defense_attack_params,\n                                            validation=validation, \n                                            total_samples=n_test,\n                                            n_samples=N_SAMPLES)\n\n    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\n    # evaluate robustness with rejection under BPDA multitargeted attack\n    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n    outer_attack_method = 'CHCMOA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        outer_attack_config.update({'base_lr': base_lr})\n        outer_file_name = f\"outer_{outer_attack_method}\"\n        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n        outer_file_name += '_val_' if validation else ''\n        outer_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n        else:\n            outer_adv_error = eval_outer_attack(outer_attack_method,\n                                                outer_attack_config,\n                                                model,\n                                                test_dataloader,\n                                                N_class,\n                                                threshold,\n                                                epsilon,\n                                                defense_attack_params=defense_attack_params,\n                                                validation=validation, \n                                                total_samples=n_test,\n                                                n_samples=N_SAMPLES)\n\n        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n        logger.info(f'{outer_file_name} saved!')\n\n    # evaluate robustness with rejection under BPDA inner attack\n    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n    inner_attack_method = 'CLCIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += '_val_' if validation else ''\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                    inner_attack_config,\n                                                    model,\n                                                    test_dataloader,\n                                                    N_class,\n                                                    threshold,\n                                                    epsilon,\n                                                    eps0_range,\n                                                    defense_attack_params=defense_attack_params,\n                                                    validation=validation, \n                                                    total_samples=n_test,\n                                                    n_samples=N_SAMPLES)\n\n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        logger.info(f'{inner_file_name} saved!')\n\n    # evaluate robustness with rejection under BPDA inner multitargeted attack\n    inner_attack_config = CONFIG_MULTITARGET_ATTACK_INNER\n    inner_attack_method = 'PDIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += '_val_' if validation else ''\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                    inner_attack_config,\n                                                    model,\n                                                    test_dataloader,\n                                                    N_class,\n                                                    threshold,\n                                                    epsilon,\n                                                    eps0_range,\n                                                    defense_attack_params=defense_attack_params,\n                                                    validation=validation, \n                                                    total_samples=n_test,\n                                                    n_samples=N_SAMPLES)\n\n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        logger.info(f'{inner_file_name} saved!')\n\n    # evaluate robustness with rejection under multitargeted attack\n    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n    outer_attack_method = 'HCMOA'\n    inner_attack_method = 'LCIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        outer_attack_config.update({'base_lr': base_lr})\n        outer_file_name = f\"outer_{outer_attack_method}\"\n        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n        outer_file_name += '_val_' if validation else ''\n        outer_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n        else:\n            outer_adv_error = eval_outer_attack(outer_attack_method,\n                                                outer_attack_config,\n                                                model,\n                                                test_dataloader,\n                                                N_class,\n                                                threshold,\n                                                epsilon,\n                                                defense_attack_params=defense_attack_params,\n                                                validation=validation, \n                                                total_samples=n_test,\n                                                n_samples=N_SAMPLES)\n\n        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += '_val_' if validation else ''\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                    inner_attack_config,\n                                                    model,\n                                                    test_dataloader,\n                                                    N_class,\n                                                    threshold,\n                                                    epsilon,\n                                                    eps0_range,\n                                                    defense_attack_params=defense_attack_params,\n                                                    validation=validation, \n                                                    total_samples=n_test,\n                                                    n_samples=N_SAMPLES)\n\n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        logger.info(f'{outer_file_name} saved!')\n        logger.info(f'{inner_file_name} saved!')\n        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n\n    # Evaluate robustness curve\n    logger.info('Final results under an ensemble of attacks:')\n    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_rcd.py", "chunked_list": ["\"\"\"\nMain script for evaluating the baseline method RCD:\nProvably robust classification of adversarial examples with detection, Fatemeh Sheikholeslami, Ali Lotfi Rezaabad,\nZico Kolter (https://openreview.net/pdf?id=sRA5rLNpmQc), ICLR 2021\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json", "import models\nimport json\nfrom attacks.objectives import *\nfrom attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset\nfrom utils.constants import *\nfrom autoattack import AutoAttack\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom functools import partial", "import utils.imgaug_lib\nfrom functools import partial\n\ndef get_classifier_output(inputs, model, num_classes):\n    return model(inputs)[:, :num_classes]\n\n\ndef eval_outer_attack(attack_method,\n                      outer_attack_config, \n                      model, \n                      test_dataloader, \n                      num_classes,\n                      epsilon, \n                      validation, \n                      total_samples, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    if attack_method == 'pgd':\n        objective = UntargetedObjective(loss=utils.torch.rcd_accept_misclassify_loss)\n        attacker = MBConfLinfPGDAttack(model,\n                                     objective,\n                                     epsilon=epsilon,\n                                     **outer_attack_config)\n    elif attack_method == 'auto_attack':\n        attacker = AutoAttack(partial(get_classifier_output, model=model, num_classes=num_classes), \n                              norm='Linf', \n                              eps=epsilon, \n                              version='standard', \n                              verbose=False)\n    elif attack_method == 'RCDOA':\n        objective = UntargetedObjective(loss=utils.torch.rcd_targeted_loss)\n        attacker = MBRCDLinfPGDAttackMultitargeted(\n                                    model,\n                                    objective,\n                                    num_classes,\n                                    epsilon=epsilon,\n                                    **outer_attack_config)\n    else:\n        raise KeyError(f'Not supported attack method {attack_method}')\n\n    adv_labels = None\n    adv_probs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        if validation and cnt < total_samples - n_samples:\n            # If validation: the last `n_samples` are used for evaluating the robustness with rejection\n            cnt += inputs.shape[0]\n            continue\n\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n        # large perturbations\n        if attack_method == 'auto_attack':\n            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        else:\n            adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if (not validation) and cnt >= n_samples:\n            # If not validation: the first `n_samples` are used for evaluating the robustness with rejection\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = np.logical_and(adv_preds != adv_labels, adv_preds != (np.ones_like(adv_labels) * num_classes))\n    return adv_error", "\n\ndef eval_inner_attack(attack_method,\n                      inner_attack_config, \n                      model, \n                      test_dataloader, \n                      num_classes,\n                      epsilon, \n                      eps0_range, \n                      validation, \n                      total_samples, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    inner_attack_results = {}\n    for epsilon_0 in eps0_range:\n        if attack_method == 'RCDIA':\n            radius_objective = UntargetedObjective(loss=utils.torch.rcd_reject_loss)\n            radius_attacker = MBConfLinfPGDAttack(model,\n                                                radius_objective,\n                                                epsilon=epsilon_0,\n                                                **inner_attack_config)\n        else:\n            raise KeyError(f'Not supported attack method {attack_method}')\n        cnt = 0\n        adv_probs_2 = None\n        for b, (inputs, targets) in enumerate(test_dataloader):\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n            if validation and cnt < total_samples - n_samples:\n                cnt += inputs.shape[0]\n                continue\n\n            # small perturbations\n            if epsilon_0 > 0.:\n                adv_inputs = radius_attacker.perturb(inputs, targets)\n            else:\n                adv_inputs = inputs\n                \n            with torch.no_grad():\n                adv_logits = model(adv_inputs)\n\n            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n            cnt += inputs.shape[0]\n            if (not validation) and cnt >= n_samples:\n                break\n        \n        adv_preds_2 = np.argmax(adv_probs_2, axis=1)   # prediction confidence\n        adv_error_2 = (adv_preds_2 == num_classes) # regard rejection as error\n        inner_attack_results[epsilon_0] = adv_error_2\n    return inner_attack_results", "        \n\ndef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n    \n    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\n    curve_x = []\n    curve_y = []\n    for epsilon_0 in inner_attack_results:\n        curve_x.append(epsilon_0/epsilon)\n        adv_error_2 = inner_attack_results[epsilon_0]\n        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n        curve_y.append(1 - final_adv_error)\n        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\n    return np.array(curve_x), np.array(curve_y)", "\n\ndef eval(model, test_dataloader, N_class, logger):\n    model.eval()\n\n    clean_probs = None\n    clean_labels = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\n    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n    val_probs = clean_probs[test_N:]\n    val_labels = clean_labels[test_N:]\n    test_probs = clean_probs[:test_N]\n    test_labels = clean_labels[:test_N]\n\n    test_preds = np.argmax(test_probs, axis=1)\n    test_errors = (test_preds != test_labels)\n    test_acc = 1. - np.mean(test_errors)\n    # Clean accuracy within the accepted inputs\n    mask_accept = (test_preds != (np.ones_like(test_labels) * N_class))\n    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n    rejection_rate = 1. - np.mean(mask_accept)\n    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")", "\ndef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n    if combine_adv_error is None:\n        return curr_adv_error\n    else:\n        return curr_adv_error|combine_adv_error\n\n\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n    return combine_inner_results", "def combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n    return combine_inner_results\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n    parser.add_argument('--validation', action='store_true', help='whether to use validation set to select hyper-parameters')\n    # args parse\n    return parser.parse_args()", "\n\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    epsilon = config['epsilon']\n    batch_size = config['batch_size']\n    checkpoint_dir = args.checkpoint_dir\n    validation = args.validation\n    \n    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    N_class = N_CLASSES[dataset]\n    if dataset == 'cifar10':\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n        \n    elif dataset == 'gtsrb':\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        \n    elif dataset == 'mnist':\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\n    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class+1, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class+1, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class+1, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n    model.load_state_dict(checkpoint['model'])\n    model.cuda()\n\n    # evaluate clean and get threshold\n    eval(model, test_dataloader, N_class, logger)\n\n    # evaluate robustness\n    n_test = len(test_dataset)\n    \n    final_outer_adv_error = None\n    final_inner_attack_results = None\n    \n    # evaluate robustness with rejection under AutoAttack\n    outer_attack_config = None\n    outer_attack_method = 'auto_attack'\n    outer_file_name = f\"outer_{outer_attack_method}\"\n    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n    outer_file_name += '_val_' if validation else ''\n    outer_file_name += \"_result.npy\"\n    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n    else:\n        outer_adv_error = eval_outer_attack(outer_attack_method,\n                                            outer_attack_config, \n                                          model, \n                                          test_dataloader, \n                                          N_class,\n                                          epsilon, \n                                          validation, \n                                          total_samples=n_test, \n                                          n_samples=N_SAMPLES)\n    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n    \n    # evaluate robustness with rejection under multitargeted attack\n    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n    outer_attack_method = 'RCDOA'\n    inner_attack_method = 'RCDIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        outer_attack_config.update({'base_lr': base_lr})\n        outer_file_name = f\"outer_{outer_attack_method}\"\n        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n        outer_file_name += '_val_' if validation else ''\n        outer_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n        else:\n            outer_adv_error = eval_outer_attack(outer_attack_method,\n                                                outer_attack_config, \n                                              model, \n                                              test_dataloader, \n                                              N_class,\n                                              epsilon, \n                                              validation, \n                                              total_samples=n_test, \n                                              n_samples=N_SAMPLES)\n        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += '_val_' if validation else ''\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                     inner_attack_config, \n                                                      model, \n                                                      test_dataloader, \n                                                      N_class,\n                                                      epsilon, \n                                                      eps0_range, \n                                                      validation, \n                                                      total_samples=n_test, \n                                                      n_samples=N_SAMPLES)\n        \n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        \n        logger.info(f'{outer_file_name} saved!')\n        logger.info(f'{inner_file_name} saved!')\n        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n      \n    # Evaluate robustness curve\n    logger.info('Final results under an ensemble of attacks:')\n    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n    if validation:\n        np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_val_result.npy\"), result_dict)\n    else:\n        np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "train_trades.py", "chunked_list": ["import argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch", "from torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json\nfrom attacks.objectives import TargetedObjective, UntargetedObjective\nfrom attacks.mb_pgd_attack import MBLinfPGDAttack\nimport utils.dataset\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib", "from imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom utils.dataset import CustomDataset\nimport utils.trades\n\n\ndef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n    model.eval()\n\n    clean_losses = None\n    clean_accs = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n\n    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%\".format(np.mean(clean_losses), np.mean(clean_accs)*100))\n\n    adv_losses = None\n    adv_accs = None\n    adv_successes = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        # small perturbations\n        adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            logits = model(adv_inputs)\n        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n\n    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(np.mean(adv_losses), np.mean(adv_accs)*100))", "\ndef adjust_learning_rate(dataset, optimizer, epoch, lr_max):\n    \"\"\"decrease the learning rate\"\"\"\n    lr = lr_max\n    if dataset=='cifar10':\n        if epoch >= 75:\n            lr = lr_max * 0.1\n        if epoch >= 90:\n            lr = lr_max * 0.01\n        if epoch >= 100:\n            lr = lr_max * 0.001\n    if dataset=='svhn':\n        if epoch >= 150:\n            lr = lr_max * 0.1\n        if epoch >= 180:\n            lr = lr_max * 0.01\n        if epoch >= 200:\n            lr = lr_max * 0.001\n    elif dataset=='gtsrb':\n        if epoch >= 40:\n            lr = lr_max * 0.1\n        if epoch >= 45:\n            lr = lr_max * 0.01\n    elif dataset=='mnist':\n        if epoch >= 55:\n            lr = lr_max * 0.1\n        if epoch >= 75:\n            lr = lr_max * 0.01\n        if epoch >= 90:\n            lr = lr_max * 0.001\n    else:\n        raise ValueError\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr", "\ndef train_trades(dataset,\n                model,\n                train_dataloader,\n                adv_configs,\n                optimizer,\n                scheduler,\n                N_class,\n                epoch,\n                lr_max,\n                print_freq,\n                logger):\n\n    num_training_iter = len(train_dataloader)\n    model.train()\n    for b, (inputs, targets) in enumerate(train_dataloader):\n\n        if scheduler is None:\n            adjust_learning_rate(dataset, optimizer, epoch, lr_max)\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n\n        optimizer.zero_grad()\n\n        # calculate robust loss\n        loss = utils.trades.trades_loss(model=model,\n                           x_natural=inputs,\n                           y=targets,\n                           optimizer=optimizer,\n                           step_size=adv_configs['step_size'],\n                           epsilon=adv_configs['epsilon'],\n                           perturb_steps=adv_configs['num_steps'],\n                           beta=adv_configs['beta'])\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            logger.info(\"Progress: {:d}/{:d}, adv_loss: {:.2f}\".format(b+1,\n                                                                       num_training_iter,\n                                                                       loss.item()))", "\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n    # args parse\n    return parser.parse_args()\n\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    beta = config['beta']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'cifar100':\n        N_class = 100\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR100('./datasets/cifar100', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR100('./datasets/cifar100', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n\n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD':\n        if dataset in ['cifar10', 'gtsrb', 'svhn']:\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n        elif dataset == 'mnist':\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        else:\n            raise ValueError\n        scheduler = None\n    else:\n        raise ValueError\n\n    adv_configs = {'step_size': eps_iter,\n                  'epsilon': epsilon,\n                  'num_steps': nb_iter,\n                  'beta': beta}\n\n    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n    attacker = MBLinfPGDAttack(model,\n                                objective,\n                                epsilon=epsilon,\n                                max_iterations=nb_iter,\n                                base_lr=eps_iter,\n                                momentum=0.0,\n                                lr_factor=1.5,\n                                backtrack=False,\n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_trades(dataset,\n                    model,\n                    train_dataloader,\n                    adv_configs,\n                    optimizer,\n                    scheduler,\n                    N_class,\n                    epoch,\n                    lr,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    beta = config['beta']\n    fraction = config['fraction']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'cifar100':\n        N_class = 100\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR100('./datasets/cifar100', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR100('./datasets/cifar100', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n\n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    model.cuda()\n    if optimizer_name == 'SGD':\n        if dataset in ['cifar10', 'gtsrb', 'svhn']:\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n        elif dataset == 'mnist':\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        else:\n            raise ValueError\n        scheduler = None\n    else:\n        raise ValueError\n\n    adv_configs = {'step_size': eps_iter,\n                  'epsilon': epsilon,\n                  'num_steps': nb_iter,\n                  'beta': beta}\n\n    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n    attacker = MBLinfPGDAttack(model,\n                                objective,\n                                epsilon=epsilon,\n                                max_iterations=nb_iter,\n                                base_lr=eps_iter,\n                                momentum=0.0,\n                                lr_factor=1.5,\n                                backtrack=False,\n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n        train_trades(dataset,\n                    model,\n                    train_dataloader,\n                    adv_configs,\n                    optimizer,\n                    scheduler,\n                    N_class,\n                    epoch,\n                    lr,\n                    print_freq,\n                    logger)\n        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_ccat.py", "chunked_list": ["\"\"\"\nMain script for evaluating the baseline method CCAT:\nStutz, David, Matthias Hein, and Bernt Schiele. \"Confidence-calibrated adversarial training: Generalizing to unseen attacks.\"\nInternational Conference on Machine Learning. PMLR, 2020.\n\nBased on the official code from: https://github.com/davidstutz/confidence-calibrated-adversarial-training\nAlso used for evaluating the baseline method Adversarial Training + Rejection.\n\"\"\"\n\nfrom functools import partial", "\nfrom functools import partial\nimport argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets", "from torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json\nimport math\nfrom attacks.objectives import *\nfrom attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset", "from attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset\nfrom utils.constants import *\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom autoattack import AutoAttack\n\n\ndef eval_outer_attack(attack_method,\n                      outer_attack_config, \n                      model, \n                      test_dataloader, \n                      num_classes,\n                      threshold, \n                      epsilon, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    if attack_method == 'auto_attack':\n        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n    elif attack_method == 'HCMOA':\n        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n        attacker = MBConfLinfPGDAttackMultitargeted(model,\n                                                    objective,\n                                                    num_classes,\n                                                    epsilon=epsilon,\n                                                    **outer_attack_config)\n    else:\n        raise KeyError(f'Not supported attack method {attack_method}')\n\n    adv_labels = None\n    adv_probs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\n        # large perturbations\n        if attack_method == 'auto_attack':\n            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        else:\n            adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if cnt >= n_samples:\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_confs = np.max(adv_probs, axis=1)   # prediction confidence\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = np.logical_and(adv_preds != adv_labels, adv_confs >= threshold)\n\n    return adv_error", "def eval_outer_attack(attack_method,\n                      outer_attack_config, \n                      model, \n                      test_dataloader, \n                      num_classes,\n                      threshold, \n                      epsilon, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    if attack_method == 'auto_attack':\n        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n    elif attack_method == 'HCMOA':\n        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n        attacker = MBConfLinfPGDAttackMultitargeted(model,\n                                                    objective,\n                                                    num_classes,\n                                                    epsilon=epsilon,\n                                                    **outer_attack_config)\n    else:\n        raise KeyError(f'Not supported attack method {attack_method}')\n\n    adv_labels = None\n    adv_probs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\n        # large perturbations\n        if attack_method == 'auto_attack':\n            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        else:\n            adv_inputs = attacker.perturb(inputs, targets)\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if cnt >= n_samples:\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_confs = np.max(adv_probs, axis=1)   # prediction confidence\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = np.logical_and(adv_preds != adv_labels, adv_confs >= threshold)\n\n    return adv_error", "\n\ndef eval_inner_attack(attack_method,\n                      inner_attack_config, \n                      model, \n                      test_dataloader, \n                      threshold, \n                      epsilon, \n                      eps0_range, \n                      n_samples=N_SAMPLES):\n    model.eval()\n    inner_attack_results = {}\n    for epsilon_0 in eps0_range:\n        if attack_method == 'LCIA':\n            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n        else:\n            raise KeyError(f'Not supported attack method {attack_method}')\n\n        radius_attacker = MBConfLinfPGDAttack(model,\n                                              radius_objective,\n                                              epsilon=epsilon_0,\n                                              **inner_attack_config)\n        cnt = 0\n        adv_probs_2 = None\n        for b, (inputs, targets) in enumerate(test_dataloader):\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n            # small perturbations\n            if epsilon_0 > 0.:\n                adv_inputs = radius_attacker.perturb(inputs, targets)\n            else:\n                adv_inputs = inputs\n                \n            with torch.no_grad():\n                adv_logits = model(adv_inputs)\n\n            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n            cnt += inputs.shape[0]\n            if cnt >= n_samples:\n                break\n        \n        adv_confs_2 = np.max(adv_probs_2, axis=1)   # prediction confidence\n        adv_error_2 = adv_confs_2 < threshold   # regard rejection as error\n        inner_attack_results[epsilon_0] = adv_error_2\n\n    return inner_attack_results", "        \n\ndef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n    \n    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\n    curve_x = []\n    curve_y = []\n    for epsilon_0 in inner_attack_results:\n        curve_x.append(epsilon_0 / epsilon)\n        adv_error_2 = inner_attack_results[epsilon_0]\n        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n        curve_y.append(1 - final_adv_error)\n        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\n    return np.array(curve_x), np.array(curve_y)", "\n\ndef eval(model, test_dataloader, tpr, logger):\n    model.eval()\n\n    clean_probs = None\n    clean_labels = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\n    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n    val_probs = clean_probs[test_N:]\n    val_labels = clean_labels[test_N:]\n    test_probs = clean_probs[:test_N]\n    test_labels = clean_labels[:test_N]\n\n    # Find the confidence threshold using a validation set\n    val_confs = np.max(val_probs, axis=1)\n    val_preds = np.argmax(val_probs, axis=1)\n    val_errors = (val_preds != val_labels)\n    # sorted_val_confs = np.sort(val_confs)\n    sorted_val_confs = np.sort(val_confs[np.logical_not(val_errors)])   # sorted confidence of correctly classified\n    cutoff = math.floor(sorted_val_confs.shape[0] * round((1. - tpr), 2))\n    threshold = sorted_val_confs[cutoff]\n\n    test_confs = np.max(test_probs, axis=1)\n    test_preds = np.argmax(test_probs, axis=1)\n    test_errors = (test_preds != test_labels)\n    test_acc = 1. - np.mean(test_errors)\n    # Clean accuracy within the accepted inputs\n    mask_accept = (test_confs >= threshold)\n    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n    rejection_rate = 1. - np.mean(mask_accept)\n    logger.info(f\"threshold: {threshold:.4f}\")\n    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")\n\n    return threshold", "\n\ndef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n    if combine_adv_error is None:\n        return curr_adv_error\n    else:\n        return curr_adv_error | combine_adv_error\n\n\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\n    return combine_inner_results", "\ndef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n    if combine_inner_results is None:\n        return curr_inner_results\n\n    for eps0 in curr_inner_results:\n        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\n    return combine_inner_results\n", "\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='Evaluate the robustness with rejection of CCAT')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n    # args parse\n    return parser.parse_args()\n", "\n\ndef main():\n\n    args = get_args()\n    \n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    epsilon = config['epsilon']\n    batch_size = config['batch_size']\n    checkpoint_dir = args.checkpoint_dir\n    \n    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    N_class = N_CLASSES[dataset]\n    tpr = TPR_THRESHOLD[dataset]\n    if dataset == 'cifar10':\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n        \n    elif dataset == 'gtsrb':\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        \n    elif dataset == 'mnist':\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\n    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n    model.load_state_dict(checkpoint['model'])\n\n    model.cuda()\n    # evaluate clean and get threshold\n    threshold = eval(model, test_dataloader, tpr, logger)\n\n    final_outer_adv_error = None\n    final_inner_attack_results = None\n    \n    # evaluate robustness with rejection under AutoAttack\n    outer_attack_config = None\n    outer_attack_method = 'auto_attack'\n    outer_file_name = f\"outer_{outer_attack_method}\"\n    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n    outer_file_name += \"_result.npy\"\n    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n    else:\n        outer_adv_error = eval_outer_attack(outer_attack_method,\n                                            outer_attack_config, \n                                            model,\n                                            test_dataloader,\n                                            N_class,\n                                            threshold,\n                                            epsilon,\n                                            n_samples=N_SAMPLES)\n\n    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n    \n    # evaluate robustness with rejection under multitargeted attack\n    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n    outer_attack_method = 'HCMOA'\n    inner_attack_method = 'LCIA'\n    for base_lr in BASE_LR_RANGE[dataset]:\n        outer_attack_config.update({'base_lr': base_lr})\n        outer_file_name = f\"outer_{outer_attack_method}\"\n        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n        outer_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n        else:\n            outer_adv_error = eval_outer_attack(outer_attack_method,\n                                                outer_attack_config, \n                                                model,\n                                                test_dataloader,\n                                                N_class,\n                                                threshold,\n                                                epsilon,\n                                                n_samples=N_SAMPLES)\n\n        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\n        inner_attack_config.update({'base_lr': base_lr})\n        inner_file_name = f\"inner_{inner_attack_method}\"\n        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n        inner_file_name += \"_result.npy\"\n        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n        else:\n            inner_attack_results = eval_inner_attack(inner_attack_method,\n                                                    inner_attack_config,\n                                                    model,\n                                                    test_dataloader,\n                                                    threshold,\n                                                    epsilon,\n                                                    eps0_range,\n                                                    n_samples=N_SAMPLES)\n\n        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n        logger.info(f'{outer_file_name} saved!')\n        logger.info(f'{inner_file_name} saved!')\n        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n\n    # Evaluate robustness curve\n    logger.info('Final results under an ensemble of attacks:')\n    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "train_atrr.py", "chunked_list": ["\"\"\"\nMain script for training the baseline method ATRR:\nPang, Tianyu, et al. \"Adversarial Training with Rectified Rejection.\" arXiv preprint arXiv:2105.14785 (2021).\nhttps://arxiv.org/pdf/2105.14785.pdf\n\nBased on the official code from: https://github.com/P2333/Rectified-Rejection\n\"\"\"\n\nimport argparse\nimport logging", "import argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy", "import utils.torch\nimport utils.numpy\nimport models\nimport json\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom attacks.objectives import UntargetedObjective\nfrom attacks.mb_pgd_attack import *\nfrom utils.dataset import CustomDataset\n", "from utils.dataset import CustomDataset\n\n\ndef test(model, attacker, test_dataloader, max_batches, logger):\n    model.eval()\n    clean_accs = None\n\n    for b, (inputs, targets) in enumerate(test_dataloader):\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        clean_error = utils.torch.classification_error(outputs, targets, reduction='none')\n        clean_accs = utils.numpy.concatenate(clean_accs, 1 - clean_error.detach().cpu().numpy())\n\n    logger.info(\"clean acc: {:.2%}\".format(np.mean(clean_accs)))\n\n    adv_accs = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        if b >= max_batches:\n            break\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n\n        # Generate adversarial examples\n        adv_inputs = attacker.perturb(inputs, targets)\n        # Indicator for large perturbations\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_error = utils.torch.classification_error(adv_logits, targets, reduction=\"none\")\n        adv_accs = utils.numpy.concatenate(adv_accs, 1 - adv_error.detach().cpu().numpy())\n\n    logger.info(\"adv_acc: {:.2%}\".format(np.mean(adv_accs)))", "\n\ndef lr_schedule(t, lr_max):\n    if t < 100:\n        return lr_max\n    elif t < 105:\n        return lr_max / 10.\n    else:\n        return lr_max / 100.\n", "\n\ndef train_robust_detection(model,\n                           train_dataloader,\n                           attacker,\n                           optimizer,\n                           scheduler,\n                           fraction,\n                           lamb,\n                           tempC, \n                           tempC_trueonly,\n                           epoch,\n                           lr_max,\n                           print_freq,\n                           logger):\n\n    num_training_iter = len(train_dataloader)\n    BCEcriterion = nn.BCELoss(reduction='mean')\n    for b, (inputs, targets) in enumerate(train_dataloader):\n        \n        if scheduler is None:\n            epoch_now = epoch + (b + 1) / num_training_iter\n            lr = lr_schedule(epoch_now, lr_max)\n            optimizer.param_groups[0].update(lr=lr)\n\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        b_size = inputs.size(0)\n        split = int((1-fraction) * b_size)\n        # update fraction for correct loss computation\n        fraction = 1 - split / float(b_size)\n\n        clean_inputs = inputs[:split]\n        adv_inputs = inputs[split:]\n        clean_targets = targets[:split]\n        adv_targets = targets[split:]\n        \n        # Generate adversarial examples\n        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\n        if adv_inputs.shape[0] < b_size: # fraction is not 1\n            combined_inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n        else:\n            combined_inputs = adv_examples\n\n        model.train()\n        optimizer.zero_grad()\n        # `d_outputs` should correspond to the probability of rejection\n        logits, aux_outputs = model(combined_inputs, return_aux=True)\n        clean_logits = logits[:split]\n        clean_aux_outputs = aux_outputs[:split]\n        adv_logits = logits[split:]\n        adv_aux_outputs = aux_outputs[split:]\n        \n        # robust loss\n        adv_loss = utils.torch.classification_loss(adv_logits, adv_targets, reduction=\"mean\")\n        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets, reduction=\"mean\")\n\n        # aux loss for detection\n        robust_output_s = torch.softmax(adv_logits * tempC, dim=1)\n        robust_con_pre, robust_con_label = robust_output_s.max(1) # predicted label and confidence\n        robust_output_s_ = torch.softmax(adv_logits * tempC_trueonly, dim=1)\n        robust_con_y = robust_output_s_[torch.tensor(range(adv_inputs.size(0))), adv_targets].detach() # predicted prob on the true label y\n        \n        correct_index = torch.where(adv_logits.max(1)[1] == adv_targets)[0]\n        robust_con_pre[correct_index] = robust_con_pre[correct_index].detach()\n\n        robust_output_aux = adv_aux_outputs.sigmoid().squeeze() # bs, Calibration function A \\in [0,1]\n        robust_detector = robust_con_pre * robust_output_aux\n        aux_loss = BCEcriterion(robust_detector, robust_con_y)\n\n        if adv_inputs.shape[0] < b_size:\n            # Loss on clean inputs\n            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets, reduction=\"mean\")\n            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets, reduction=\"mean\")\n            # Combined loss\n            loss = (1 - fraction) * clean_loss + fraction * (adv_loss + lamb * aux_loss)\n        else:\n            clean_acc = torch.ones(1)\n            loss = adv_loss + lamb * aux_loss\n\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        if (b+1) % print_freq == 0:\n            if adv_inputs.shape[0] < b_size:\n                logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, clean_acc: {:.2%}, adv_acc: {:.2%}\".format(b+1,\n                                                                                                        num_training_iter,\n                                                                                                        loss.item(),\n                                                                                                        clean_acc.item(),\n                                                                                                        adv_acc.item()))\n            else:\n                logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, adv_acc: {:.2%}\".format(b+1,\n                                                                                        num_training_iter,\n                                                                                        loss.item(),\n                                                                                        adv_acc.item()))", "\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', '-s', type=int, default=0)\n    parser.add_argument('--config-file', '-c', type=str, required=True, help='config file')\n    parser.add_argument('--output-dir', '-o', type=str, required=True, help='output dir')\n    parser.add_argument('--pretrain-model-dir', help='dir to load pretrained models', type=str, default='')\n    # args parse\n    return parser.parse_args()", "\n\ndef main():\n\n    args = get_args()\n\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    lr = config['lr']\n    optimizer_name = config['optimizer_name']\n    epsilon = config['epsilon']\n    eps_iter = config['eps_iter']\n    nb_iter = config['nb_iter']\n    print_freq = config['print_freq']\n    checkpoint_freq = config['checkpoint_freq']\n    max_batches = config['max_batches']\n    # fraction of clean inputs\n    fraction = config['fraction']\n    lamb = config['lamb']\n    tempC = config['tempC']\n    tempC_trueonly = config['tempC_trueonly']\n    nepoch = config['nepoch']\n    batch_size = config['batch_size']\n    output_dir = args.output_dir\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    if dataset == 'cifar10':\n        N_class = 10\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n        \n    elif dataset == 'gtsrb':\n        N_class = 43\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        \n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        eps_iter /= 255.\n\n    elif dataset == 'mnist':\n        N_class = 10\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNetTwoBranchDenseV1(N_class, resolution, out_dim=1, use_BN=True, along=True)\n    elif model_arch == \"resnet20\":\n        model = models.ResNetTwoBranchDenseV1(N_class, resolution, blocks=[3, 3, 3], out_dim=1, use_BN=True, along=True)\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNetTwoBranchDenseV1(N_class, resolution, depth=28, width=10, out_dim=1, use_BN=True, along=True)\n    else:\n        raise ValueError\n\n    model.cuda()\n\n    if optimizer_name == 'SGD-pp':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n        scheduler = None\n    elif optimizer_name == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n    else:\n        raise ValueError\n\n\n    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n    attacker = MBLinfPGDAttack(model, \n                                objective, \n                                epsilon=epsilon, \n                                max_iterations=nb_iter, \n                                base_lr=eps_iter, \n                                momentum=0.0, \n                                lr_factor=1.5, \n                                backtrack=False, \n                                rand_init_name=\"random+zero\",\n                                num_rand_init=1,\n                                clip_min=0.0,\n                                clip_max=1.0)\n\n    for epoch in range(nepoch):\n        logger.info(\"Epoch: {:d}\".format(epoch))\n\n        train_robust_detection(model,\n                            train_dataloader,\n                            attacker,\n                            optimizer,\n                            scheduler,\n                            fraction,\n                            lamb,\n                            tempC, \n                            tempC_trueonly,\n                            epoch,\n                            lr,\n                            print_freq,\n                            logger)\n        test(model, attacker, test_dataloader, max_batches, logger)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            torch.save({\n                'epoch': epoch + 1,\n                'model': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\n    torch.save({\n            'epoch': epoch + 1,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, os.path.join(output_dir, 'classifier.pth.tar'))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_at.py", "chunked_list": ["\"\"\"Evaluation of standard adversarial training without rejection.\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets", "from torchvision import transforms\nfrom torchvision import datasets\nimport utils.torch\nimport utils.numpy\nimport models\nimport json\nfrom attacks.objectives import UntargetedObjective\nfrom attacks.mb_pgd_attack import MBConfLinfPGDAttack\nfrom utils.dataset import CustomDataset\nfrom utils.constants import *", "from utils.dataset import CustomDataset\nfrom utils.constants import *\nfrom imgaug import augmenters as iaa\nimport utils.imgaug_lib\nfrom autoattack import AutoAttack\n\n\ndef eval_robustness_curve(model, test_dataloader, epsilon, eps0_range, logger=None, n_samples=N_SAMPLES):\n    model.eval()\n\n    # Use AutoAttack\n    attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n    \n    adv_labels = None\n    adv_probs = None\n    cnt = 0\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\n        # large perturbations\n        adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n        with torch.no_grad():\n            adv_logits = model(adv_inputs)\n\n        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n        cnt += inputs.shape[0]\n        if cnt >= n_samples:\n            break\n\n    # Error on adversarial inputs: accept and misclassify\n    adv_confs = np.max(adv_probs, axis=1)   # prediction confidence\n    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n    adv_error = adv_preds != adv_labels\n    logger.info(f\"robustness with reject: {1-np.mean(adv_error):.2%}\")\n\n    curve_x = []\n    curve_y = []\n    for epsilon_0 in eps0_range:\n        curve_x.append(epsilon_0/epsilon)\n        \n        final_adv_error = np.mean(adv_error)\n        curve_y.append(1 - final_adv_error)\n\n    return np.array(curve_x), np.array(curve_y)", "\n\ndef eval(model, test_dataloader, logger):\n    # Accuracy on clean inputs\n    model.eval()\n\n    clean_probs = None\n    clean_labels = None\n    for b, (inputs, targets) in enumerate(test_dataloader):\n        \n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\n    N = clean_probs.shape[0]\n    test_N = int(N * (1 - VAL_RATIO))\n    test_probs = clean_probs[:test_N]\n    test_labels = clean_labels[:test_N]\n\n    test_preds = np.argmax(test_probs, axis=1)\n    test_errors = (test_preds != test_labels)\n    test_acc = 1 - np.mean(test_errors)\n    logger.info(f\"clean accuracy: {test_acc:.2%}, F1 score: {test_acc*2/(test_acc+1)}\")", "        \n\ndef get_args():\n    parser = argparse.ArgumentParser(description='train robust model with detection')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--config-file', type=str, required=True, help='config file')\n    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n    # args parse\n    return parser.parse_args()\n", "\n\ndef main():\n\n    args = get_args()\n    # Set random seed\n    utils.torch.set_seed(args.seed)\n\n    with open(args.config_file) as config_file:\n        config = json.load(config_file)\n\n    dataset = config['dataset']\n    model_arch = config['model_arch']\n    epsilon = config['epsilon']\n    batch_size = config['batch_size']\n    checkpoint_dir = args.checkpoint_dir\n    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n    \n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format='[%(asctime)s] - %(message)s',\n        datefmt='%Y/%m/%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=[\n            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n            logging.StreamHandler()\n        ])\n\n    logger.info(args)\n    logger.info(config)\n\n    N_class = N_CLASSES[dataset]\n    if dataset == 'cifar10':\n        resolution = (3, 32, 32)\n        transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n        \n    elif dataset == 'gtsrb':\n        resolution = (3, 32, 32)\n        train_loaded = np.load('datasets/gtsrb/train.npz')\n        X_train = train_loaded['images']\n        y_train = train_loaded['labels']\n        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n        X_test = test_loaded['images']\n        y_test = test_loaded['labels']\n        \n        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n        epsilon /= 255.\n\n    elif dataset == 'svhn':\n        N_class = 10\n        resolution = (3, 32, 32)\n        augmenters = [iaa.CropAndPad(\n            percent=(0, 0.2),\n            pad_mode='edge',\n        ),\n        iaa.ContrastNormalization((0.7, 1.3))]\n        transform_train = transforms.Compose([\n            np.asarray,\n            iaa.Sequential([\n                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n                utils.imgaug_lib.Clip(),\n            ]).augment_image,\n            np.copy,\n            transforms.ToTensor(),\n        ])\n        transform_test = transforms.ToTensor()\n\n        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n        epsilon /= 255.\n        \n    elif dataset == 'mnist':\n        resolution = (1, 28, 28)\n        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    else:\n        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\n    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n    # Model Setup\n    if model_arch == \"lenet\":\n        model = models.FixedLeNet(N_class, resolution)\n    elif model_arch == \"resnet20\":\n        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n    elif model_arch == \"wideresnet\":\n        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n    else:\n        raise ValueError\n\n    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n    model.load_state_dict(checkpoint['model'])\n    model.cuda()\n\n    eval(model, test_dataloader, logger)\n    \n    curve_x, curve_y = eval_robustness_curve(model, test_dataloader, epsilon,  eps0_range,\n                                                            logger=logger, n_samples=N_SAMPLES)\n    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_at_result.npy\"), result_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "attacks/objectives.py", "chunked_list": ["from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport utils.torch\n\n\nclass TargetedObjective(nn.Module):\n    \"\"\"\n    Targeted objective based on a loss, e.g., the cross-entropy loss.\n    \"\"\"\n\n    def __init__(self, loss=utils.torch.classification_loss):\n        \"\"\"\n        Constructor.\n\n        :param loss: loss function to use\n        :type loss: callable\n        \"\"\"\n\n        super(TargetedObjective, self).__init__()\n\n        self.loss = loss\n        \"\"\" (callable) Loss. \"\"\"\n\n    def forward(self, logits, target_classes, perturbations=None):\n        \"\"\"\n        Objective function.\n\n        :param logits: logit output of the network\n        :type logits: torch.autograd.Variable\n        :param perturbations: perturbations\n        :type perturbations: torch.autograd.Variable or None\n        :return: error\n        :rtype: torch.autograd.Variable\n        \"\"\"\n        if perturbations is not None:\n            return -self.loss(logits, target_classes, perturbations=perturbations, reduction='none')\n        else:\n            return -self.loss(logits, target_classes, reduction='none')", "\n\nclass UntargetedObjective(nn.Module):\n    \"\"\"\n    Untargeted loss based objective, e.g., cross-entropy loss.\n    \"\"\"\n\n    def __init__(self, loss=utils.torch.classification_loss):\n        \"\"\"\n        Constructor.\n\n        :param loss: loss function to use\n        :type loss: callable\n        \"\"\"\n\n        super(UntargetedObjective, self).__init__()\n\n        self.loss = loss\n        \"\"\" (callable) Loss. \"\"\"\n\n    def forward(self, logits, true_classes, perturbations=None):\n        \"\"\"\n        Objective function.\n\n        :param logits: logit output of the network\n        :type logits: torch.autograd.Variable\n        :param perturbations: perturbations\n        :type perturbations: torch.autograd.Variable or None\n        :return: error\n        :rtype: torch.autograd.Variable\n        \"\"\"\n        # assert self.loss is not None\n        if perturbations is not None:\n            return self.loss(logits, true_classes, perturbations=perturbations, reduction='none')\n        else:\n            return self.loss(logits, true_classes, reduction='none')", "\n\nclass SelectiveUntargetedObjective(UntargetedObjective):\n    \"\"\"\n    Untargeted loss based objective for selective classifier, e.g., cross-entropy loss.\n    \"\"\"\n\n    def __init__(self, loss):\n        \"\"\"\n        Constructor.\n\n        :param loss: loss function to use\n        :type loss: callable\n        \"\"\"\n\n        super(SelectiveUntargetedObjective, self).__init__(loss)\n\n    def forward(self, logits, d_logits, true_classes, perturbations=None):\n        \"\"\"\n        Objective function.\n\n        :param logits: logit output of the network\n        :type logits: torch.autograd.Variable\n        :param perturbations: perturbations\n        :type perturbations: torch.autograd.Variable or None\n        :return: error\n        :rtype: torch.autograd.Variable\n        \"\"\"\n        # assert self.loss is not None\n        if perturbations is not None:\n            return self.loss(logits, d_logits, true_classes, perturbations=perturbations, reduction='none')\n        else:\n            return self.loss(logits, d_logits, true_classes, reduction='none')", ""]}
{"filename": "attacks/mb_pgd_attack.py", "chunked_list": ["from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport utils.torch\nimport numpy as np\n\n\nclass MBBPDAAttack:\n\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.defense = defense\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.base_lr = base_lr\n        self.momentum = momentum\n        self.lr_factor = lr_factor\n        self.backtrack = backtrack\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, adv_x, y):\n        outputs = self.model(adv_x)\n        loss = self.objective(outputs, y)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        loss = self.get_loss(t_adv_x, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n\n        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        for ii in range(self.max_iterations):\n            adv_x = x + delta\n            t_adv_x = self.defense.transform(adv_x)\n            t_adv_x = nn.Parameter(t_adv_x)\n            t_adv_x.requires_grad_()\n            loss = self.get_loss(t_adv_x, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = t_adv_x.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\n            if self.backtrack:\n                next_perturbs = delta + torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n                next_perturbs.data = torch.clamp(next_perturbs.data, min=-self.epsilon, max=self.epsilon)\n                next_perturbs.data = torch.clamp(x.data + next_perturbs.data, min=self.clip_min, max=self.clip_max) - x.data\n                with torch.no_grad():\n                    next_adv_x = x + next_perturbs\n                    t_next_adv_x = self.defense.transform(next_adv_x)\n                    next_error = self.get_loss(t_next_adv_x, y)\n\n                # Update learning rate if requested.\n                for b in range(batch_size):\n                    if next_error[b].item() >= loss.data[b]:\n                        delta[b].data += lrs[b]*global_gradients[b].data\n                    else:\n                        lrs[b] = max(lrs[b] / self.lr_factor, 1e-20)\n            else:\n                delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\n            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        loss = self.get_loss(t_adv_x, y)\n\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\nclass MBBPDAAttack:\n\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.defense = defense\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.base_lr = base_lr\n        self.momentum = momentum\n        self.lr_factor = lr_factor\n        self.backtrack = backtrack\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, adv_x, y):\n        outputs = self.model(adv_x)\n        loss = self.objective(outputs, y)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        loss = self.get_loss(t_adv_x, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n\n        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        for ii in range(self.max_iterations):\n            adv_x = x + delta\n            t_adv_x = self.defense.transform(adv_x)\n            t_adv_x = nn.Parameter(t_adv_x)\n            t_adv_x.requires_grad_()\n            loss = self.get_loss(t_adv_x, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = t_adv_x.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\n            if self.backtrack:\n                next_perturbs = delta + torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n                next_perturbs.data = torch.clamp(next_perturbs.data, min=-self.epsilon, max=self.epsilon)\n                next_perturbs.data = torch.clamp(x.data + next_perturbs.data, min=self.clip_min, max=self.clip_max) - x.data\n                with torch.no_grad():\n                    next_adv_x = x + next_perturbs\n                    t_next_adv_x = self.defense.transform(next_adv_x)\n                    next_error = self.get_loss(t_next_adv_x, y)\n\n                # Update learning rate if requested.\n                for b in range(batch_size):\n                    if next_error[b].item() >= loss.data[b]:\n                        delta[b].data += lrs[b]*global_gradients[b].data\n                    else:\n                        lrs[b] = max(lrs[b] / self.lr_factor, 1e-20)\n            else:\n                delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\n            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        loss = self.get_loss(t_adv_x, y)\n\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBConfBPDAAttack(MBBPDAAttack):\n\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model, \n                defense,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        self.rand_init_name = \"zero\"\n        worst_loss, worst_perb = self.perturb_once(x, y)\n\n        self.rand_init_name = \"random\"\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            cond = curr_worst_loss > worst_loss\n            worst_loss[cond] = curr_worst_loss[cond]\n            worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBConfBPDAAttackMultitargeted(MBConfBPDAAttack):\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                defense,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass BPDAAttack:\n    def __init__(self, model=None, defense=None, device=None, epsilon=None, learning_rate=0.5,\n                 max_iterations=100, clip_min=0, clip_max=1):\n        self.model = model\n        self.epsilon = epsilon\n        self.loss_fn = nn.CrossEntropyLoss(reduction='sum')\n        self.defense = defense\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n        self.LEARNING_RATE = learning_rate\n        self.MAX_ITERATIONS = max_iterations\n        self.device = device\n\n    def generate(self, x, y):\n        \"\"\"\n        Given examples (X_nat, y), returns their adversarial\n        counterparts with an attack length of epsilon.\n        \"\"\"\n\n        adv = x.detach().clone()\n\n        lower = np.clip(x.detach().cpu().numpy() - self.epsilon, self.clip_min, self.clip_max)\n        upper = np.clip(x.detach().cpu().numpy() + self.epsilon, self.clip_min, self.clip_max)\n\n        for i in range(self.MAX_ITERATIONS):\n            adv_purified = self.defense(adv)\n            adv_purified.requires_grad_()\n            adv_purified.retain_grad()\n\n            scores = self.model(adv_purified)\n            loss = self.loss_fn(scores, y)\n            loss.backward()\n\n            grad_sign = adv_purified.grad.data.sign()\n\n            # early stop, only for batch_size = 1\n            # p = torch.argmax(F.softmax(scores), 1)\n            # if y != p:\n            #     break\n\n            adv += self.LEARNING_RATE * grad_sign\n\n            adv_img = np.clip(adv.detach().cpu().numpy(), lower, upper)\n            adv = torch.Tensor(adv_img).to(self.device)\n        return adv", "\n\nclass RandomAttack:\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.objective = objective\n        self.epsilon = epsilon\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs = self.model(adv_x)\n        loss = self.objective(outputs, y)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n        self.random_init(delta, x, self.rand_init_name)\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBLinfPGDAttack:\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.base_lr = base_lr\n        self.momentum = momentum\n        self.lr_factor = lr_factor\n        self.backtrack = backtrack\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs = self.model(adv_x)\n        loss = self.objective(outputs, y)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n\n        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        delta = nn.Parameter(delta)\n        delta.requires_grad_()\n\n        for ii in range(self.max_iterations):\n            loss = self.get_loss(x, delta, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = delta.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\n            if self.backtrack:\n                next_perturbs = delta + torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n                next_perturbs.data = torch.clamp(next_perturbs.data, min=-self.epsilon, max=self.epsilon)\n                next_perturbs.data = torch.clamp(x.data + next_perturbs.data, min=self.clip_min, max=self.clip_max) - x.data\n                with torch.no_grad():\n                    next_error = self.get_loss(x, next_perturbs, y)\n\n                # Update learning rate if requested.\n                for b in range(batch_size):\n                    if next_error[b].item() >= loss.data[b]:\n                        delta[b].data += lrs[b]*global_gradients[b].data\n                    else:\n                        lrs[b] = max(lrs[b] / self.lr_factor, 1e-20)\n            else:\n                delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\n            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n            delta.grad.data.zero_()\n\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBConfLinfPGDAttack(MBLinfPGDAttack):\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model, \n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        self.rand_init_name = \"zero\"\n        worst_loss, worst_perb = self.perturb_once(x, y)\n\n        self.rand_init_name = \"random\"\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            cond = curr_worst_loss > worst_loss\n            worst_loss[cond] = curr_worst_loss[cond]\n            worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBSATRLinfPGDAttack(MBConfLinfPGDAttack):\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, d_outputs = self.model(adv_x, return_d=True)\n        loss = self.objective(outputs, d_outputs, y)\n        return loss", "\n\nclass MBATRRLinfPGDAttack(MBConfLinfPGDAttack):\n\n    def __init__(self, \n                model, \n                objective, \n                tempC=1.0,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n        self.tempC = tempC\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, aux_outputs = self.model(adv_x, return_aux=True)\n        # con_pre, _ = torch.softmax(outputs * self.tempC, dim=1).max(dim=1, keepdim=True) # predicted label and confidence\n        aux_outputs = aux_outputs.sigmoid()\n        # evi_outputs = con_pre * aux_outputs\n        loss = self.objective(outputs, aux_outputs, y)\n        return loss", "\n\nclass MBSATRStratifiedLinfPGDAttack(MBLinfPGDAttack):\n    \"\"\"\n    Stratified PGD Attack for the proposed method SATR\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    \"\"\"\n\n    def __init__(\n                self, \n                model, \n                objective, \n                rr_objective,\n                fraction,\n                epsilon=0.3, \n                epsilon_0 = 0.1,\n                max_iterations=100, \n                outer_base_lr=0.1, \n                inner_base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                multitargeted=False,\n                num_classes=10,\n                clip_min=0.0,\n                clip_max=1.0):\n        \n        super().__init__(model, \n                objective, \n                epsilon, \n                max_iterations, \n                outer_base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n        self.epsilon_0 = epsilon_0\n        self.fraction = fraction\n        self.rr_objective = rr_objective\n        self.multitargeted = multitargeted\n        self.num_classes = num_classes\n        self.outer_base_lr = outer_base_lr\n        self.inner_base_lr = inner_base_lr\n\n    def get_split(self, size_inp):\n        return int(self.fraction * size_inp)\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            size_inp = delta.size(0)\n            delta.data.normal_()\n            u = torch.zeros(size_inp).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(size_inp, -1), dim=1)[0]\n            linf_norm = linf_norm.view(size_inp, 1, 1, 1)\n            split = self.get_split(size_inp)\n            if self.fraction > 0.0:\n                delta.data[:split] = self.epsilon * delta.data[:split] * linf_norm.data[:split]\n            if self.fraction < 1.0:\n                delta.data[split:] = self.epsilon_0 * delta.data[split:] * linf_norm.data[split:]\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, d_outputs = self.model(adv_x, return_d=True)\n        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n        split = self.get_split(x.size(0))\n\n        if self.fraction == 1.0:\n            targets = y[:split]\n            adv_outputs = outputs[:split]\n            adv_d_outputs = d_outputs[:split]\n            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets)\n            return loss_1\n        elif self.fraction == 0.0:\n            targets_2 = y[split:]\n            adv_outputs_2 = outputs[split:]\n            adv_d_outputs_2 = d_outputs[split:]\n            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2)\n            return loss_2\n        else:\n            targets = y[:split]\n            adv_outputs = outputs[:split]\n            adv_d_outputs = d_outputs[:split]\n            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets)\n            targets_2 = y[split:]\n            adv_outputs_2 = outputs[split:]\n            adv_d_outputs_2 = d_outputs[split:]\n            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2)\n            return torch.cat((loss_1, loss_2), dim=0)\n\n    def perturb_once(self, x, y):\n\n        split = self.get_split(x.size(0))\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n        outer_lrs = (torch.ones_like(success_errors[:split]).float() * self.outer_base_lr).cuda()\n        inner_lrs = (torch.ones_like(success_errors[split:]).float() * self.inner_base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        delta = nn.Parameter(delta)\n        delta.requires_grad_()\n\n        for ii in range(self.max_iterations):\n            loss = self.get_loss(x, delta, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = delta.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n            \n            delta.data[:split] += torch.mul(utils.torch.expand_as(outer_lrs, global_gradients[:split]), global_gradients[:split])\n            delta.data[split:] += torch.mul(utils.torch.expand_as(inner_lrs, global_gradients[split:]), global_gradients[split:])\n\n            if self.fraction > 0.0:\n                delta.data[:split] = torch.clamp(delta.data[:split], min=-self.epsilon, max=self.epsilon)\n            if self.fraction < 1.0:\n                delta.data[split:] = torch.clamp(delta.data[split:], min=-self.epsilon_0, max=self.epsilon_0)\n\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n            delta.grad.data.zero_()\n\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n    \n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n        \n        if self.multitargeted:\n            split = self.get_split(x.size(0))\n            y_delta = np.random.randint(1, self.num_classes, size=split)\n            y[:split] = (y[:split] + y_delta) % self.num_classes\n            \n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBRCDStratifiedLinfPGDAttack(MBLinfPGDAttack):\n    \"\"\"\n    Stratified PGD Attack for the RCD baseline\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    \"\"\"\n\n    def __init__(\n                self, \n                model, \n                objective_1, \n                objective_2,\n                fraction,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n        \n        super().__init__(model, \n                None, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n        self.fraction = fraction\n        self.objective_1 = objective_1\n        self.objective_2 = objective_2\n\n    def get_split(self, size_inp):\n        return int(self.fraction * size_inp)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs = self.model(adv_x)\n        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n        split = self.get_split(x.size(0))\n\n        if self.fraction == 1.0 or self.fraction == 0.0:\n            raise ValueError\n        else:\n            targets_1 = y[:split]\n            adv_outputs_1 = outputs[:split]\n            loss_1 = self.objective_1(adv_outputs_1, targets_1)\n\n            targets_2 = y[split:]\n            adv_outputs_2 = outputs[split:]\n            loss_2 = self.objective_2(adv_outputs_2, targets_2)\n            return torch.cat((loss_1, loss_2), dim=0)", "\n\nclass MBSATRLinfPGDAttackMultitargeted(MBSATRLinfPGDAttack):\n    def __init__(self, \n                model, \n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n    \nclass MBRCDLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n    \n    def __init__(self, \n                model, \n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBATRRLinfPGDAttackMultitargeted(MBATRRLinfPGDAttack):\n    \n    def __init__(self, \n                model, \n                objective, \n                num_classes,\n                tempC=1.0,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                tempC, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n\n        self.tempC = tempC\n        self.num_classes = num_classes\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBConfLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n    def __init__(self, \n                model, \n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBCONSRLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        self.defense = defense\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        perb = t_adv_x.data-adv_x.detach().data\n        t_adv_x = x + delta + perb\n        outputs = self.model(adv_x)\n        t_outputs = self.model(t_adv_x)\n        loss = self.objective(outputs, y) + self.objective(t_outputs, y)\n        return loss\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for y_delta in range(1, self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\n\nclass MBCONSRLinfPGDAttack(MBConfLinfPGDAttack):\n\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model, \n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.defense = defense\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        perb = t_adv_x.data-adv_x.detach().data\n        t_adv_x = x + delta + perb\n        outputs = self.model(adv_x)\n        t_outputs = self.model(t_adv_x)\n        loss = self.objective(outputs, y) + self.objective(t_outputs, y)\n        return loss", "\n\nclass MBCONSRLinfPGDInnerAttackMultitargeted(MBConfLinfPGDAttack):\n    def __init__(self, \n                model, \n                defense,\n                objective, \n                num_classes,\n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                lr_factor=1.5, \n                backtrack=True, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        super().__init__(model,\n                objective, \n                epsilon, \n                max_iterations, \n                base_lr, \n                momentum, \n                lr_factor, \n                backtrack, \n                rand_init_name,\n                num_rand_init,\n                clip_min,\n                clip_max)\n        self.num_classes = num_classes\n        self.defense = defense\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        t_adv_x = self.defense.transform(adv_x)\n        perb = t_adv_x.data-adv_x.detach().data\n        t_adv_x = x + delta + perb\n        outputs = self.model(adv_x)\n        t_outputs = self.model(t_adv_x)\n        loss = self.objective(outputs, y) - self.objective(t_outputs, y)\n        return loss\n        \n    def perturb(self, x, y):\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = torch.empty(x.size()[0], device=x.device)\n        worst_loss[:] = -np.inf\n        worst_perb = torch.zeros_like(x)\n        for target_y in range(self.num_classes):\n            for k in range(self.num_rand_init):\n                curr_worst_loss, curr_worst_perb = self.perturb_once(x, target_y)\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", ""]}
{"filename": "attacks/pgd_attack.py", "chunked_list": ["from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nimport utils.torch\nimport numpy as np\n\n\nclass LinfPGDAttack:\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.base_lr = base_lr\n        self.momentum = momentum\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, d_outputs = self.model(adv_x, return_d=True)\n        loss = self.objective(outputs, d_outputs, y, perturbations=delta)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n\n        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        delta = nn.Parameter(delta)\n        delta.requires_grad_()\n\n        for ii in range(self.max_iterations):\n            loss = self.get_loss(x, delta, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = delta.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\n            delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n            delta.grad.data.zero_()\n\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "\nclass LinfPGDAttack:\n\n    def __init__(self, \n                model, \n                objective, \n                epsilon=0.3, \n                max_iterations=100, \n                base_lr=0.1, \n                momentum=0.9, \n                rand_init_name=\"random\",\n                num_rand_init=1,\n                clip_min=0.0,\n                clip_max=1.0):\n\n        self.model = model\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.base_lr = base_lr\n        self.momentum = momentum\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            delta.data.normal_()\n            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, d_outputs = self.model(adv_x, return_d=True)\n        loss = self.objective(outputs, d_outputs, y, perturbations=delta)\n        return loss\n\n    def perturb_once(self, x, y):\n\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n\n        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        delta = nn.Parameter(delta)\n        delta.requires_grad_()\n\n        for ii in range(self.max_iterations):\n            loss = self.get_loss(x, delta, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = delta.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\n            delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n            delta.grad.data.zero_()\n\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n\n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n\n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", "    \nclass SATRStratifiedLinfPGDAttack:\n    \"\"\"\n    Stratified PGD Attack for the proposed method SATR\n\n    :param predict: forward pass function.\n    :param loss_fn: loss function.\n    :param eps: maximum distortion.\n    :param nb_iter: number of iterations.\n    :param eps_iter: attack step size.\n    :param rand_init: (optional bool) random initialization.\n    :param clip_min: mininum value per input dimension.\n    :param clip_max: maximum value per input dimension.\n    :param targeted: if the attack is targeted.\n    \"\"\"\n\n    def __init__(\n                self, \n                model, \n                objective, \n                rr_objective,\n                fraction,\n                epsilon=0.3, \n                epsilon_0 = 0.1,\n                max_iterations=100, \n                outer_base_lr=0.1, \n                inner_base_lr=0.1, \n                momentum=0.9,\n                rand_init_name=\"random\",\n                num_rand_init=1,\n                num_classes=10,\n                clip_min=0.0,\n                clip_max=1.0):\n        \n        self.model = model\n        self.objective = objective\n        self.epsilon = epsilon\n        self.max_iterations = max_iterations\n        self.momentum = momentum\n        self.rand_init_name = rand_init_name\n        self.num_rand_init = num_rand_init\n        self.clip_min = clip_min\n        self.clip_max = clip_max\n        self.epsilon_0 = epsilon_0\n        self.fraction = fraction\n        self.rr_objective = rr_objective\n        self.num_classes = num_classes\n        self.outer_base_lr = outer_base_lr\n        self.inner_base_lr = inner_base_lr\n\n    def get_split(self, size_inp):\n        return int(self.fraction * size_inp)\n\n    def random_init(self, delta, x, random_name=\"random\"):\n        \n        if random_name == 'random':\n            size_inp = delta.size(0)\n            delta.data.normal_()\n            u = torch.zeros(size_inp).uniform_(0, 1).cuda()\n            linf_norm = u / torch.max(delta.abs().view(size_inp, -1), dim=1)[0]\n            linf_norm = linf_norm.view(size_inp, 1, 1, 1)\n            split = self.get_split(size_inp)\n            if self.fraction > 0.0:\n                delta.data[:split] = self.epsilon * delta.data[:split] * linf_norm.data[:split]\n            if self.fraction < 1.0:\n                delta.data[split:] = self.epsilon_0 * delta.data[split:] * linf_norm.data[split:]\n        elif random_name == 'zero':\n            delta.data.zero_()\n        else:\n            raise ValueError\n\n        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\n    def get_loss(self, x, delta, y):\n        adv_x = x + delta\n        outputs, d_outputs = self.model(adv_x, return_d=True)\n        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n        split = self.get_split(x.size(0))\n\n        if self.fraction == 1.0:\n            targets = y[:split]\n            adv_outputs = outputs[:split]\n            adv_d_outputs = d_outputs[:split]\n            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets, perturbations=delta)\n            return loss_1\n        elif self.fraction == 0.0:\n            targets_2 = y[split:]\n            adv_outputs_2 = outputs[split:]\n            adv_d_outputs_2 = d_outputs[split:]\n            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2, perturbations=delta)\n            return loss_2\n        else:\n            targets = y[:split]\n            adv_outputs = outputs[:split]\n            adv_d_outputs = d_outputs[:split]\n            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets, perturbations=delta)\n            targets_2 = y[split:]\n            adv_outputs_2 = outputs[split:]\n            adv_d_outputs_2 = d_outputs[split:]\n            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2, perturbations=delta)\n            return torch.cat((loss_1, loss_2), dim=0)\n\n    def perturb_once(self, x, y):\n\n        split = self.get_split(x.size(0))\n        delta = torch.zeros_like(x)\n        batch_size = x.shape[0]\n        global_gradients = torch.zeros_like(delta)\n        loss = self.get_loss(x, delta, y)\n        success_errors = loss.data.clone()\n        success_perturbs = delta.data.clone()\n        outer_lrs = (torch.ones_like(success_errors[:split]).float() * self.outer_base_lr).cuda()\n        inner_lrs = (torch.ones_like(success_errors[split:]).float() * self.inner_base_lr).cuda()\n        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\n        if self.rand_init_name == 'random+zero':\n            random_name = random.choice([\"random\", \"zero\"])\n            self.random_init(delta, x, random_name)\n        else:\n            self.random_init(delta, x, self.rand_init_name)\n\n        delta = nn.Parameter(delta)\n        delta.requires_grad_()\n\n        for ii in range(self.max_iterations):\n            loss = self.get_loss(x, delta, y)\n\n            cond = loss.data > success_errors\n            success_errors[cond] = loss.data[cond]\n            success_perturbs[cond] = delta.data[cond]\n\n            loss.mean().backward()\n            grad = delta.grad.data\n\n            # normalize and add momentum.\n            grad.data = torch.sign(grad.data)\n            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n            \n            delta.data[:split] += torch.mul(utils.torch.expand_as(outer_lrs, global_gradients[:split]), global_gradients[:split])\n            delta.data[split:] += torch.mul(utils.torch.expand_as(inner_lrs, global_gradients[split:]), global_gradients[split:])\n\n            if self.fraction > 0.0:\n                delta.data[:split] = torch.clamp(delta.data[:split], min=-self.epsilon, max=self.epsilon)\n            if self.fraction < 1.0:\n                delta.data[split:] = torch.clamp(delta.data[split:], min=-self.epsilon_0, max=self.epsilon_0)\n            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\n            delta.grad.data.zero_()\n\n        loss = self.get_loss(x, delta, y)\n        cond = loss.data > success_errors\n        success_errors[cond] = loss.data[cond]\n        success_perturbs[cond] = delta.data[cond]\n\n        return success_errors, success_perturbs\n    \n    def perturb(self, x, y):\n        \"\"\"\n        Given examples (x, y), returns their adversarial counterparts with\n        an attack length of eps.\n\n        :param x: input tensor.\n        :param y: label tensor.\n                  - if None and self.targeted=False, compute y as predicted\n                    labels.\n                  - if self.targeted=True, then y must be the targeted labels.\n        :return: tensor containing perturbed inputs.\n        \"\"\"\n\n        self.model.eval()\n\n        x = x.detach().clone().cuda()\n        y = y.detach().clone().cuda()\n            \n        worst_loss = None\n        worst_perb = None\n        for k in range(self.num_rand_init):\n            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n            if worst_loss is None:\n                worst_loss = curr_worst_loss\n                worst_perb = curr_worst_perb\n            else:\n                cond = curr_worst_loss > worst_loss\n                worst_loss[cond] = curr_worst_loss[cond]\n                worst_perb[cond] = curr_worst_perb[cond]\n\n        return x + worst_perb", ""]}
{"filename": "utils/imgaug_lib.py", "chunked_list": ["from imgaug import augmenters as iaa\nfrom imgaug import dtypes as iadt\nimport numpy\n\nclass Clip(iaa.Augmenter):\n    \"\"\"\n    Clip augmenter.\n    \"\"\"\n\n    def __init__(self, min=0, max=1, name=None, deterministic=False, random_state=None):\n        super(Clip, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n\n        self.min = min\n        \"\"\" (float) Minimum.\"\"\"\n\n        self.max = max\n        \"\"\" (float) Maximum. \"\"\"\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(images, allowed=[\"float32\"], disallowed=[\n            \"bool\", \"uint8\", \"uint16\",\n            \"int8\", \"int16\", \"float16\",\n            \"uint32\", \"uint64\", \"uint128\",\n            \"uint256\", \"int32\", \"int64\",\n            \"int128\", \"int256\", \"float64\",\n            \"float96\", \"float128\", \"float256\"\n        ], augmenter=self)\n\n        converted_images = numpy.clip(images, self.min, self.max)\n        return converted_images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return []", "\n\nclass Image(iaa.Augmenter):\n    \"\"\"\n    Image augmenter.\n    \"\"\"\n\n    def __init__(self, name=None, deterministic=False, random_state=None):\n        super(Image, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n\n    def _augment_images(self, images, random_state, parents, hooks):\n        iadt.gate_dtypes(images, allowed=[\"float32\"], disallowed=[\n            \"bool\", \"uint8\", \"uint16\",\n            \"int8\", \"int16\", \"float16\",\n            \"uint32\", \"uint64\", \"uint128\",\n            \"uint256\", \"int32\", \"int64\",\n            \"int128\", \"int256\", \"float64\",\n            \"float96\", \"float128\", \"float256\"\n        ], augmenter=self)\n\n        converted_images = (images * 255).astype(numpy.uint8)\n        return converted_images\n\n    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n        return heatmaps\n\n    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n        return keypoints_on_images\n\n    def get_parameters(self):\n        return []"]}
{"filename": "utils/lib.py", "chunked_list": ["\"\"\"\nSome I/O utilities.\n\"\"\"\n\nimport os\nimport json\nimport numpy as np\nimport importlib\nimport pickle\nimport gc", "import pickle\nimport gc\nimport socket\nimport functools\n\n# See https://github.com/h5py/h5py/issues/961\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport h5py\nwarnings.resetwarnings()", "import h5py\nwarnings.resetwarnings()\n\n# https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n\n\ndef write_hdf5(filepath, tensors, keys='tensor'):\n    \"\"\"\n    Write a simple tensor, i.e. numpy array ,to HDF5.\n\n    :param filepath: path to file to write\n    :type filepath: str\n    :param tensors: tensor to write\n    :type tensors: numpy.ndarray or [numpy.ndarray]\n    :param keys: key to use for tensor\n    :type keys: str or [str]\n    \"\"\"\n\n    #opened_hdf5() # To be sure as there were some weird opening errors.\n    assert type(tensors) == np.ndarray or isinstance(tensors, list)\n    if isinstance(tensors, list) or isinstance(keys, list):\n        assert isinstance(tensors, list) and isinstance(keys, list)\n        assert len(tensors) == len(keys)\n\n    if not isinstance(tensors, list):\n        tensors = [tensors]\n    if not isinstance(keys, list):\n        keys = [keys]\n\n    makedir(os.path.dirname(filepath))\n\n    # Problem that during experiments, too many h5df files are open!\n    # https://stackoverflow.com/questions/29863342/close-an-open-h5py-data-file\n    with h5py.File(filepath, 'w') as h5f:\n\n        for i in range(len(tensors)):\n            tensor = tensors[i]\n            key = keys[i]\n\n            chunks = list(tensor.shape)\n            if len(chunks) > 2:\n                chunks[2] = 1\n                if len(chunks) > 3:\n                    chunks[3] = 1\n                    if len(chunks) > 4:\n                        chunks[4] = 1\n\n            h5f.create_dataset(key, data=tensor, chunks=tuple(chunks), compression='gzip')\n        h5f.close()\n        return", "def write_hdf5(filepath, tensors, keys='tensor'):\n    \"\"\"\n    Write a simple tensor, i.e. numpy array ,to HDF5.\n\n    :param filepath: path to file to write\n    :type filepath: str\n    :param tensors: tensor to write\n    :type tensors: numpy.ndarray or [numpy.ndarray]\n    :param keys: key to use for tensor\n    :type keys: str or [str]\n    \"\"\"\n\n    #opened_hdf5() # To be sure as there were some weird opening errors.\n    assert type(tensors) == np.ndarray or isinstance(tensors, list)\n    if isinstance(tensors, list) or isinstance(keys, list):\n        assert isinstance(tensors, list) and isinstance(keys, list)\n        assert len(tensors) == len(keys)\n\n    if not isinstance(tensors, list):\n        tensors = [tensors]\n    if not isinstance(keys, list):\n        keys = [keys]\n\n    makedir(os.path.dirname(filepath))\n\n    # Problem that during experiments, too many h5df files are open!\n    # https://stackoverflow.com/questions/29863342/close-an-open-h5py-data-file\n    with h5py.File(filepath, 'w') as h5f:\n\n        for i in range(len(tensors)):\n            tensor = tensors[i]\n            key = keys[i]\n\n            chunks = list(tensor.shape)\n            if len(chunks) > 2:\n                chunks[2] = 1\n                if len(chunks) > 3:\n                    chunks[3] = 1\n                    if len(chunks) > 4:\n                        chunks[4] = 1\n\n            h5f.create_dataset(key, data=tensor, chunks=tuple(chunks), compression='gzip')\n        h5f.close()\n        return", "\n\ndef read_hdf5(filepath, key='tensor', efficient=False):\n    \"\"\"\n    Read a tensor, i.e. numpy array, from HDF5.\n\n    :param filepath: path to file to read\n    :type filepath: str\n    :param key: key to read\n    :type key: str\n    :param efficient: effienct reaidng\n    :type efficient: bool\n    :return: tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    #opened_hdf5() # To be sure as there were some weird opening errors.\n    assert os.path.exists(filepath), 'file %s not found' % filepath\n\n    if efficient:\n        h5f = h5py.File(filepath, 'r')\n        assert key in [key for key in h5f.keys()], 'key %s does not exist in %s with keys %s' % (key, filepath, ', '.join(h5f.keys()))\n        return h5f[key]\n    else:\n        with h5py.File(filepath, 'r') as h5f:\n            assert key in [key for key in h5f.keys()], 'key %s does not exist in %s with keys %s' % (key, filepath, ', '.join(h5f.keys()))\n            return h5f[key][()]", "\n\ndef check_hdf5(filepath, key='tensor'):\n    \"\"\"\n    Check a file without loading data.\n\n    :param filepath: path to file to read\n    :type filepath: str\n    :param key: key to read\n    :type key: str\n    :return: can be loaded or not\n    :rtype: bool\n    \"\"\"\n\n    opened_hdf5()  # To be sure as there were some weird opening errors.\n    assert os.path.exists(filepath), 'file %s not found' % filepath\n\n    try:\n        with h5py.File(filepath, 'r') as h5f:\n            assert key in [key for key in h5f.keys()], 'key %s does not exist in %s' % (key, filepath)\n            tensor = h5f.get('tensor')\n            # That's it ...\n            return True\n    except:\n        return False", "\n\ndef opened_hdf5():\n    \"\"\"\n    Close all open HDF5 files and report number of closed files.\n\n    :return: number of closed files\n    :rtype: int\n    \"\"\"\n\n    opened = 0\n    for obj in gc.get_objects():  # Browse through ALL objects\n        try:\n            # is instance check may also fail!\n            if isinstance(obj, h5py.File):  # Just HDF5 files\n                obj.close()\n                opened += 1\n        except:\n            pass  # Was already closed\n    return opened", "\n\ndef write_pickle(file, mixed):\n    \"\"\"\n    Write a variable to pickle.\n\n    :param file: path to file to write\n    :type file: str\n    :return: mixed\n    :rtype: mixed\n    \"\"\"\n\n    makedir(os.path.dirname(file))\n    handle = open(file, 'wb')\n    pickle.dump(mixed, handle)\n    handle.close()", "\n\ndef read_pickle(file):\n    \"\"\"\n    Read pickle file.\n\n    :param file: path to file to read\n    :type file: str\n    :return: mixed\n    :rtype: mixed\n    \"\"\"\n\n    assert os.path.exists(file), 'file %s not found' % file\n\n    handle = open(file, 'rb')\n    results = pickle.load(handle)\n    handle.close()\n    return results", "\n\ndef read_json(file):\n    \"\"\"\n    Read a JSON file.\n\n    :param file: path to file to read\n    :type file: str\n    :return: parsed JSON as dict\n    :rtype: dict\n    \"\"\"\n\n    assert os.path.exists(file), 'file %s not found' % file\n\n    with open(file, 'r') as fp:\n        return json.load(fp)", "\n\ndef write_json(file, data):\n    \"\"\"\n    Read a JSON file.\n\n    :param file: path to file to read\n    :type file: str\n    :param data: data to write\n    :type data: mixed\n    :return: parsed JSON as dict\n    :rtype: dict\n    \"\"\"\n\n    makedir(os.path.dirname(file))\n    with open(file, 'w') as fp:\n        json.dump(data, fp)", "\n\ndef makedir(dir):\n    \"\"\"\n    Creates directory if it does not exist.\n\n    :param dir: directory path\n    :type dir: str\n    \"\"\"\n\n    if dir and not os.path.exists(dir):\n        os.makedirs(dir)", "\n\ndef remove(filepath):\n    \"\"\"\n    Remove a file.\n\n    :param filepath: path to file\n    :type filepath: str\n    \"\"\"\n\n    if os.path.isfile(filepath) and os.path.exists(filepath):\n        os.unlink(filepath)", "\n\ndef get_class(module_name, class_name):\n    \"\"\"\n    See https://stackoverflow.com/questions/1176136/convert-string-to-python-class-object?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa.\n\n    :param module_name: module holding class\n    :type module_name: str\n    :param class_name: class name\n    :type class_name: str\n    :return: class or False\n    \"\"\"\n    # load the module, will raise ImportError if module cannot be loaded\n    try:\n        m = importlib.import_module(module_name)\n    except ImportError as e:\n        log('%s' % e, LogLevel.ERROR)\n        return False\n    # get the class, will raise AttributeError if class cannot be found\n    try:\n        c = getattr(m, class_name)\n    except AttributeError as e:\n        log('%s' % e, LogLevel.ERROR)\n        return False\n    return c", "\n\ndef hostname():\n    \"\"\"\n    Get hostname.\n\n    :return: hostname\n    :rtype: str\n    \"\"\"\n\n    return socket.gethostname()", "\n\ndef pid():\n    \"\"\"\n    PID.\n\n    :return: PID\n    :rtype: int\n    \"\"\"\n\n    return os.getpid()", "\n\ndef partial(f, *args, **kwargs):\n    \"\"\"\n    Create partial while preserving __name__ and __doc__.\n\n    :param f: function\n    :type f: callable\n    :param args: arguments\n    :type args: dict\n    :param kwargs: keyword arguments\n    :type kwargs: dict\n    :return: partial\n    :rtype: callable\n    \"\"\"\n    p = functools.partial(f, *args, **kwargs)\n    functools.update_wrapper(p, f)\n    return p"]}
{"filename": "utils/torch.py", "chunked_list": ["import torch\nimport numpy\nimport scipy.ndimage\nimport math\nfrom . import numpy as cnumpy\nimport random\n\n\nSMALL_VALUE = 1e-8\n", "SMALL_VALUE = 1e-8\n\n\ndef set_seed(seed):\n    \"\"\"Sets seed\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    torch.manual_seed(seed)\n    numpy.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True", "\n\ndef memory():\n    \"\"\"\n    Get memory usage.\n\n    :return: memory usage\n    :rtype: str\n    \"\"\"\n\n    index = torch.cuda.current_device()\n\n    # results are in bytes\n    # Decimal\n    # Value \tMetric\n    # 1000 \tkB \tkilobyte\n    # 1000^2 \tMB \tmegabyte\n    # 1000^3 \tGB \tgigabyte\n    # 1000^4 \tTB \tterabyte\n    # 1000^5 \tPB \tpetabyte\n    # 1000^6 \tEB \texabyte\n    # 1000^7 \tZB \tzettabyte\n    # 1000^8 \tYB \tyottabyte\n    # Binary\n    # Value \tIEC \tJEDEC\n    # 1024 \tKiB \tkibibyte \tKB \tkilobyte\n    # 1024^2 \tMiB \tmebibyte \tMB \tmegabyte\n    # 1024^3 \tGiB \tgibibyte \tGB \tgigabyte\n    # 1024^4 \tTiB \ttebibyte \t-\n    # 1024^5 \tPiB \tpebibyte \t-\n    # 1024^6 \tEiB \texbibyte \t-\n    # 1024^7 \tZiB \tzebibyte \t-\n    # 1024^8 \tYiB \tyobibyte \t-\n\n    return '%g/%gMiB' % (\n        BMiB(torch.cuda.memory_allocated(index) + torch.cuda.memory_cached(index)),\n        BMiB(torch.cuda.max_memory_allocated(index) + torch.cuda.max_memory_cached(index)),\n    )", "\n\ndef is_cuda(mixed):\n    \"\"\"\n    Check if model/tensor is on CUDA.\n\n    :param mixed: model or tensor\n    :type mixed: torch.nn.Module or torch.autograd.Variable or torch.Tensor\n    :return: on cuda\n    :rtype: bool\n    \"\"\"\n\n    assert isinstance(mixed, torch.nn.Module) or isinstance(mixed, torch.autograd.Variable) \\\n        or isinstance(mixed, torch.Tensor), 'mixed has to be torch.nn.Module, torch.autograd.Variable or torch.Tensor'\n\n    is_cuda = False\n    if isinstance(mixed, torch.nn.Module):\n        is_cuda = True\n        for parameters in list(mixed.parameters()):\n            is_cuda = is_cuda and parameters.is_cuda\n    if isinstance(mixed, torch.autograd.Variable):\n        is_cuda = mixed.is_cuda\n    if isinstance(mixed, torch.Tensor):\n        is_cuda = mixed.is_cuda\n\n    return is_cuda", "\n\ndef estimate_size(mixed):\n    \"\"\"\n    Estimate tensor size.\n\n    :param tensor: tensor or model\n    :type tensor: numpy.ndarray, torch.tensor, torch.autograd.Variable or torch.nn.Module\n    :return: size in bits\n    :rtype: int\n    \"\"\"\n\n    # PyTorch types:\n    # Data type \tdtype \tCPU tensor \tGPU tensor\n    # 32-bit floating point \ttorch.float32 or torch.float \ttorch.FloatTensor \ttorch.cuda.FloatTensor\n    # 64-bit floating point \ttorch.float64 or torch.double \ttorch.DoubleTensor \ttorch.cuda.DoubleTensor\n    # 16-bit floating point \ttorch.float16 or torch.half \ttorch.HalfTensor \ttorch.cuda.HalfTensor\n    # 8-bit integer (unsigned) \ttorch.uint8 \ttorch.ByteTensor \ttorch.cuda.ByteTensor\n    # 8-bit integer (signed) \ttorch.int8 \ttorch.CharTensor \ttorch.cuda.CharTensor\n    # 16-bit integer (signed) \ttorch.int16 or torch.short \ttorch.ShortTensor \ttorch.cuda.ShortTensor\n    # 32-bit integer (signed) \ttorch.int32 or torch.int \ttorch.IntTensor \ttorch.cuda.IntTensor\n    # 64-bit integer (signed) \ttorch.int64 or torch.long \ttorch.LongTensor \ttorch.cuda.LongTensor\n\n    # Numpy types:\n    # Data type \tDescription\n    # bool_ \tBoolean (True or False) stored as a byte\n    # int_ \tDefault integer type (same as C long; normally either int64 or int32)\n    # intc \tIdentical to C int (normally int32 or int64)\n    # intp \tInteger used for indexing (same as C ssize_t; normally either int32 or int64)\n    # int8 \tByte (-128 to 127)\n    # int16 \tInteger (-32768 to 32767)\n    # int32 \tInteger (-2147483648 to 2147483647)\n    # int64 \tInteger (-9223372036854775808 to 9223372036854775807)\n    # uint8 \tUnsigned integer (0 to 255)\n    # uint16 \tUnsigned integer (0 to 65535)\n    # uint32 \tUnsigned integer (0 to 4294967295)\n    # uint64 \tUnsigned integer (0 to 18446744073709551615)\n    # float_ \tShorthand for float64.\n    # float16 \tHalf precision float: sign bit, 5 bits exponent, 10 bits mantissa\n    # float32 \tSingle precision float: sign bit, 8 bits exponent, 23 bits mantissa\n    # float64 \tDouble precision float: sign bit, 11 bits exponent, 52 bits mantissa\n    # complex_ \tShorthand for complex128.\n    # complex64 \tComplex number, represented by two 32-bit floats (real and imaginary components)\n    # complex128 \tComplex number, represented by two 64-bit floats (real and imaginary components)\n\n    types8 = [\n        torch.uint8, torch.int8,\n        numpy.int8, numpy.uint8, numpy.bool_,\n    ]\n\n    types16 = [\n        torch.float16, torch.half,\n        torch.int16, torch.short,\n        numpy.int16, numpy.uint16, numpy.float16,\n    ]\n\n    types32 = [\n        torch.float32, torch.float,\n        torch.int32, torch.int,\n        numpy.int32, numpy.uint32, numpy.float32,\n    ]\n\n    types64 = [\n        torch.float64, torch.double,\n        torch.int64, torch.long,\n        numpy.int64, numpy.uint64, numpy.float64, numpy.complex64,\n        numpy.int_, numpy.float_\n    ]\n\n    types128 = [\n        numpy.complex_, numpy.complex128\n    ]\n\n    if isinstance(mixed, torch.nn.Module):\n\n        size = 0\n        modules = mixed.modules()\n        for module in modules:\n            for parameters in list(module.parameters()):\n                size += estimate_size(parameters)\n        return size\n\n    if isinstance(mixed, (torch.Tensor, numpy.ndarray)):\n\n        if mixed.dtype in types128:\n            bits = 128\n        elif mixed.dtype in types64:\n            bits = 64\n        elif mixed.dtype in types32:\n            bits = 32\n        elif mixed.dtype in types16:\n            bits = 16\n        elif mixed.dtype in types8:\n            bits = 8\n        else:\n            assert False, 'could not identify torch.Tensor or numpy.ndarray type %s' % mixed.type()\n\n        size = numpy.prod(mixed.shape)\n        return size*bits\n\n    elif isinstance(mixed, torch.autograd.Variable):\n        return estimate_size(mixed.data)\n    else:\n        assert False, 'unsupported tensor size for estimating size, either numpy.ndarray, torch.tensor or torch.autograd.Variable'", "\n\ndef bits2MiB(bits):\n    \"\"\"\n    Convert bits to MiB.\n\n    :param bits: number of bits\n    :type bits: int\n    :return: MiB\n    :rtype: float\n    \"\"\"\n\n    return bits/(8*1024*1024)", "\n\ndef bits2MB(bits):\n    \"\"\"\n    Convert bits to MB.\n\n    :param bits: number of bits\n    :type bits: int\n    :return: MiB\n    :rtype: float\n    \"\"\"\n\n    return bits/(8*1000*1000)", "\n\ndef bytes2MiB(bytes):\n    \"\"\"\n    Convert bytes to MiB.\n\n    :param bytes: number of bytes\n    :type bytes: int\n    :return: MiB\n    :rtype: float\n    \"\"\"\n\n    return bytes/(1024*1024)", "\n\ndef bytes2MB(bytes):\n    \"\"\"\n    Convert bytes to MB.\n\n    :param bytes: number of bytes\n    :type bytes: int\n    :return: MiB\n    :rtype: float\n    \"\"\"\n\n    return bytes/(1000*1000)", "\n\nbMiB = bits2MiB\nBMiB = bytes2MiB\nbMB = bits2MB\nBMB = bytes2MB\n\n\ndef binary_labels(classes):\n    \"\"\"\n    Convert 0,1 labels to -1,1 labels.\n\n    :param classes: classes as B x 1\n    :type classes: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    classes[classes == 0] = -1\n    return classes", "def binary_labels(classes):\n    \"\"\"\n    Convert 0,1 labels to -1,1 labels.\n\n    :param classes: classes as B x 1\n    :type classes: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    classes[classes == 0] = -1\n    return classes", "\n\ndef one_hot(classes, C):\n    \"\"\"\n    Convert class labels to one-hot vectors.\n\n    :param classes: classes as B x 1\n    :type classes: torch.autograd.Variable or torch.Tensor\n    :param C: number of classes\n    :type C: int\n    :return: one hot vector as B x C\n    :rtype: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    assert isinstance(classes, torch.autograd.Variable) or isinstance(classes, torch.Tensor), 'classes needs to be torch.autograd.Variable or torch.Tensor'\n    assert len(classes.size()) == 2 or len(classes.size()) == 1, 'classes needs to have rank 2 or 1'\n    assert C > 0\n\n    if len(classes.size()) < 2:\n        classes = classes.view(-1, 1)\n\n    one_hot = torch.Tensor(classes.size(0), C)\n    if is_cuda(classes):\n         one_hot = one_hot.cuda()\n\n    if isinstance(classes, torch.autograd.Variable):\n        one_hot = torch.autograd.Variable(one_hot)\n\n    one_hot.zero_()\n    one_hot.scatter_(1, classes, 1)\n\n    return one_hot", "\n\ndef project_ball(tensor, epsilon=1, ord=2):\n    \"\"\"\n    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n\n    **Assumes the first dimension to be batch dimension, which is preserved.**\n\n    :param tensor: variable or tensor\n    :type tensor: torch.autograd.Variable or torch.Tensor\n    :param epsilon: radius of ball.\n    :type epsilon: float\n    :param ord: order of norm\n    :type ord: int\n    :return: projected vector\n    :rtype: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    assert isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.autograd.Variable), 'given tensor should be torch.Tensor or torch.autograd.Variable'\n\n    if ord == 0:\n        assert epsilon >= 0\n        sorted, _ = torch.sort(tensor.view(tensor.size()[0], -1), dim=1)\n        k = int(math.ceil(epsilon))\n        assert k > 0\n        thresholds = sorted[:, -k]\n        mask = (tensor >= expand_as(thresholds, tensor)).type(tensor.type())\n\n        tensor *= mask\n    elif ord == 1:\n        # ! Does not allow differentiation obviously!\n        cuda = is_cuda(tensor)\n        array = tensor.detach().cpu().numpy()\n        array = cnumpy.project_ball(array, epsilon=epsilon, ord=ord)\n        tensor = torch.from_numpy(array)\n        if cuda:\n            tensor = tensor.cuda()\n    elif ord == 2:\n        size = tensor.size()\n        flattened_size = numpy.prod(numpy.array(size[1:]))\n\n        tensor = tensor.view(-1, flattened_size)\n        clamped = torch.clamp(epsilon/torch.norm(tensor, 2, dim=1), max=1)\n        clamped = clamped.view(-1, 1)\n\n        tensor = tensor * clamped\n        if len(size) == 4:\n            tensor = tensor.view(-1, size[1], size[2], size[3])\n        elif len(size) == 2:\n            tensor = tensor.view(-1, size[1])\n    elif ord == float('inf'):\n        tensor = torch.clamp(tensor, min=-epsilon, max=epsilon)\n    else:\n        raise NotImplementedError()\n\n    return tensor", "\n\ndef project_sphere(tensor, epsilon=1, ord=2):\n    \"\"\"\n    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n\n    **Assumes the first dimension to be batch dimension, which is preserved.**\n\n    :param tensor: variable or tensor\n    :type tensor: torch.autograd.Variable or torch.Tensor\n    :param epsilon: radius of ball.\n    :type epsilon: float\n    :param ord: order of norm\n    :type ord: int\n    :return: projected vector\n    :rtype: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    assert isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.autograd.Variable), 'given tensor should be torch.Tensor or torch.autograd.Variable'\n\n    size = tensor.size()\n    flattened_size = numpy.prod(numpy.array(size[1:]))\n\n    tensor = tensor.view(-1, flattened_size)\n    tensor = tensor/torch.norm(tensor, dim=1, ord=ord).view(-1, 1)\n    tensor *= epsilon\n\n    if len(size) == 4:\n        tensor = tensor.view(-1, size[1], size[2], size[3])\n    elif len(size) == 2:\n        tensor = tensor.view(-1, size[1])\n\n    return tensor", "\n\ndef tensor_or_value(mixed):\n    \"\"\"\n    Get tensor or single value.\n\n    :param mixed: variable, tensor or value\n    :type mixed: mixed\n    :return: tensor or value\n    :rtype: torch.Tensor or value\n    \"\"\"\n\n    if isinstance(mixed, torch.Tensor):\n        if mixed.numel() > 1:\n            return mixed\n        else:\n            return mixed.item()\n    elif isinstance(mixed, torch.autograd.Variable):\n        return tensor_or_value(mixed.cpu().data)\n    else:\n        return mixed", "\n\ndef as_variable(mixed, cuda=False, requires_grad=False):\n    \"\"\"\n    Get a tensor or numpy array as variable.\n\n    :param mixed: input tensor\n    :type mixed: torch.Tensor or numpy.ndarray\n    :param device: gpu or not\n    :type device: bool\n    :param requires_grad: gradients\n    :type requires_grad: bool\n    :return: variable\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    assert isinstance(mixed, numpy.ndarray) or isinstance(mixed, torch.Tensor), 'input needs to be numpy.ndarray or torch.Tensor'\n\n    if isinstance(mixed, numpy.ndarray):\n        mixed = torch.from_numpy(mixed)\n\n    if cuda:\n        mixed = mixed.cuda()\n    return torch.autograd.Variable(mixed, requires_grad)", "\n\ndef tile(a, dim, n_tile):\n    \"\"\"\n    Numpy-like tiling in torch.\n    https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/2\n\n    :param a: tensor\n    :type a: torch.Tensor or torch.autograd.Variable\n    :param dim: dimension to tile\n    :type dim: int\n    :param n_tile: number of tiles\n    :type n_tile: int\n    :return: tiled tensor\n    :rtype: torch.Tensor or torch.autograd.Variable\n    \"\"\"\n\n    init_dim = a.size(dim)\n    repeat_idx = [1] * a.dim()\n    repeat_idx[dim] = n_tile\n    a = a.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(numpy.concatenate([init_dim * numpy.arange(n_tile) + i for i in range(init_dim)]))\n    if is_cuda(a):\n        order_index = order_index.cuda()\n    return torch.index_select(a, dim, order_index)", "\n\ndef expand_as(tensor, tensor_as):\n    \"\"\"\n    Expands the tensor using view to allow broadcasting.\n\n    :param tensor: input tensor\n    :type tensor: torch.Tensor or torch.autograd.Variable\n    :param tensor_as: reference tensor\n    :type tensor_as: torch.Tensor or torch.autograd.Variable\n    :return: tensor expanded with singelton dimensions as tensor_as\n    :rtype: torch.Tensor or torch.autograd.Variable\n    \"\"\"\n\n    view = list(tensor.size())\n    for i in range(len(tensor.size()), len(tensor_as.size())):\n        view.append(1)\n\n    return tensor.view(view)", "\n\ndef get_exponential_scheduler(optimizer, batches_per_epoch, gamma=0.97):\n    \"\"\"\n    Get exponential scheduler.\n\n    Note that the resulting optimizer's step function is called after each batch!\n\n    :param optimizer: optimizer\n    :type optimizer: torch.optim.Optimizer\n    :param batches_per_epoch: number of batches per epoch\n    :type batches_per_epoch: int\n    :param gamma: gamma\n    :type gamma: float\n    :return: scheduler\n    :rtype: torch.optim.lr_scheduler.LRScheduler\n    \"\"\"\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: gamma ** math.floor(epoch/batches_per_epoch)])", "\n\ndef classification_error(logits, targets, reduction='mean'):\n    \"\"\"\n    Accuracy.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduce: reduce to number or keep per element\n    :type reduce: bool\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n    if logits.size()[1] > 1:\n        # softmax transformation is not needed since only the argmax index is used for calculating accuracy\n        probs = logits\n        # probs = torch.nn.functional.softmax(logits, dim=1)\n    else:\n        probs = torch.sigmoid(logits)\n\n    return prob_classification_error(probs, targets, reduction=reduction)", "\n\ndef prob_classification_error(probs, targets, reduction='mean'):\n    \"\"\"\n    Accuracy.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduce: reduce to number or keep per element\n    :type reduce: bool\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n    # assert probs.size()[0] == targets.size()[0]\n    # assert len(list(targets.size())) == 1\n    # assert len(list(probs.size())) == 2\n\n    if probs.size()[1] > 1:\n        values, indices = torch.max(probs, dim=1)\n    else:\n        # Threshold is assumed to be at 0.5. Prediction = 1 if probability >= 0.5; else 0\n        indices = torch.round(probs).view(-1)\n\n    errors = torch.clamp(torch.abs(indices.long() - targets.long()), max=1)\n    if reduction == 'mean':\n        return torch.mean(errors.float())\n    elif reduction == 'sum':\n        return torch.sum(errors.float())\n    else:\n        return errors", "\n\ndef negat_log_proba(probs, reduction=\"none\"):\n    # Stable calculation of `-log(p)`\n    return torch.nn.functional.binary_cross_entropy(torch.clamp(probs, min=0.0, max=1.0),\n                                                    torch.ones_like(probs).cuda(), reduction=reduction)\n\n\ndef negat_log_one_minus_proba(probs, reduction=\"none\"):\n    # Stable calculation of `-log(1 - p)`\n    return torch.nn.functional.binary_cross_entropy(torch.clamp(probs, min=0.0, max=1.0),\n                                                    torch.zeros_like(probs).cuda(), reduction=reduction)", "def negat_log_one_minus_proba(probs, reduction=\"none\"):\n    # Stable calculation of `-log(1 - p)`\n    return torch.nn.functional.binary_cross_entropy(torch.clamp(probs, min=0.0, max=1.0),\n                                                    torch.zeros_like(probs).cuda(), reduction=reduction)\n\n\ndef reduce_loss(loss, reduction):\n    if reduction == \"none\":\n        return loss\n    elif reduction == \"mean\":\n        return torch.mean(loss)\n    elif reduction == \"sum\":\n        return torch.sum(loss)\n    else:\n        raise ValueError(\"Invalid value '{}' for reduction\".format(reduction))", "\n\ndef rcd_accept_misclassify_loss_bak(logits, targets, reduction='mean'):\n    # Attack loss function: -log(h_y(x) + h_{k+1}(x))\n    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n    total_class = logits.size(1)\n    masked_prob_outputs = prob_outputs * (one_hot(targets, total_class) +\n                                          one_hot(torch.ones_like(targets) * (total_class - 1), total_class))\n    loss = negat_log_proba(torch.sum(masked_prob_outputs, dim=1))\n    return reduce_loss(loss, reduction)", "\n\ndef rcd_accept_misclassify_loss(logits, targets, reduction='mean'):\n    # Attack loss function: -log[1 - max_{j \\notin {y, k+1}} h_j(x)]\n    N, K = logits.size()\n    prob = torch.nn.functional.softmax(logits, dim=1)\n\n    # `(K-1, K-2)` array where row `i` of the array will have the value `i` omitted\n    indices_temp = numpy.array([[j for j in range(K-1) if j != i] for i in range(K-1)])\n    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n    indices = torch.tensor(indices_temp[targets_np, :])\n\n    masked_prob = prob[torch.unsqueeze(torch.arange(N), 1), indices]\n    max_prob = torch.max(masked_prob, dim=1)[0]\n    loss = -negat_log_proba(max_prob)\n    return reduce_loss(loss, reduction)", "\n\ndef rcd_reject_loss(logits, targets, reduction='mean'):\n\n    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n    loss = -negat_log_proba(prob_outputs[:, -1])\n    # total_class = logits.size(1)  # k + 1\n    # masked_prob_outputs = prob_outputs * one_hot(torch.ones_like(targets) * (total_class - 1), total_class)\n    # loss = -torch.log(1 - torch.sum(masked_prob_outputs, dim=1) + SMALL_VALUE)\n\n    return reduce_loss(loss, reduction)", "\n\ndef robust_detection_loss(logits, targets, reduction='mean'):\n\n    N, K = logits.size()\n    prob = torch.nn.functional.softmax(logits, dim=1)\n\n    # `(K-1, K-2)` array where row `i` of the array will have the value `i` omitted\n    indices_temp = numpy.array([[j for j in range(K-1) if j != i] for i in range(K-1)])\n    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n    indices = torch.tensor(indices_temp[targets_np, :])\n\n    masked_prob = prob[torch.unsqueeze(torch.arange(N), 1), indices]\n    max_prob = torch.max(masked_prob, dim=1)[0]\n    return reduce_loss(max_prob, reduction)", "\n\ndef uniform_confidence_loss(logits, targets, reduction='mean', scaling_factor=100.0):\n    # Log-sum-exp based attack objective for confidence based rejection methods\n    loss = torch.logsumexp(logits, dim=1) - (1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\n    return reduce_loss(loss, reduction)\n\n\ndef uniform_confidence_loss_v2(logits, targets, reduction='mean', scaling_factor=100.0):\n    # Log-sum-exp based attack objective for confidence based rejection methods\n    # Use the loss below to directly minimize the maximum logit\n    loss = (-1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\n    return reduce_loss(loss, reduction)", "\ndef uniform_confidence_loss_v2(logits, targets, reduction='mean', scaling_factor=100.0):\n    # Log-sum-exp based attack objective for confidence based rejection methods\n    # Use the loss below to directly minimize the maximum logit\n    loss = (-1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\n    return reduce_loss(loss, reduction)\n\n\ndef hinge_confidence_loss(logits, targets, reduction='mean', scaling_factor=100.0, thresh_reject=1.0):\n    # Hinge loss based attack objective for confidence based rejection methods\n    preds = torch.softmax(logits, axis=1)\n    loss = torch.nn.functional.relu(thresh_reject -\n                                    (1. / scaling_factor) * torch.logsumexp(scaling_factor * preds, dim=1))\n\n    return reduce_loss(loss, reduction)", "\ndef hinge_confidence_loss(logits, targets, reduction='mean', scaling_factor=100.0, thresh_reject=1.0):\n    # Hinge loss based attack objective for confidence based rejection methods\n    preds = torch.softmax(logits, axis=1)\n    loss = torch.nn.functional.relu(thresh_reject -\n                                    (1. / scaling_factor) * torch.logsumexp(scaling_factor * preds, dim=1))\n\n    return reduce_loss(loss, reduction)\n\n\ndef softmax_square_loss(logits, targets, reduction='mean'):\n    # Used as attack objective for confidence based rejection methods CCAT and adversarial training. Maximizing the\n    # squared-softmax loss pushes the predictions close to uniform over all classes\n    softmax_output = torch.softmax(logits, axis=1)\n    loss = negat_log_proba(torch.sum(softmax_output * softmax_output, axis=1))\n\n    return reduce_loss(loss, reduction)", "\n\ndef softmax_square_loss(logits, targets, reduction='mean'):\n    # Used as attack objective for confidence based rejection methods CCAT and adversarial training. Maximizing the\n    # squared-softmax loss pushes the predictions close to uniform over all classes\n    softmax_output = torch.softmax(logits, axis=1)\n    loss = negat_log_proba(torch.sum(softmax_output * softmax_output, axis=1))\n\n    return reduce_loss(loss, reduction)\n", "\n\ndef entropy_confidence_loss(logits, targets, reduction='mean'):\n    # Used as attack objective for confidence based rejection methods CCAT and adversarial training. Maximizing the\n    # entropy loss pushes the predictions close to uniform over all classes\n    proba = torch.softmax(logits, axis=1)\n    loss = torch.sum(proba * negat_log_proba(proba), axis=1)\n\n    return reduce_loss(loss, reduction)\n", "\n\ndef high_conf_misclassify_loss(logits, true_classes, reduction='mean'):\n    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n    masked_prob_outputs = prob_outputs * (1 - one_hot(true_classes, prob_outputs.size(1)))\n    # maximum prob. of a class different from the true class\n    loss = -negat_log_proba(torch.max(masked_prob_outputs, dim=1)[0])\n\n    return reduce_loss(loss, reduction)\n", "\n\ndef f7p_loss(logits, true_classes, reduction='mean'):\n    # Used as attack objective for confidence based rejection methods CCAT and adversarial training\n    if logits.size(1) > 1:\n        current_probabilities = torch.nn.functional.softmax(logits, dim=1)\n        current_probabilities = current_probabilities * (1 - one_hot(true_classes, current_probabilities.size(1)))\n        loss = torch.max(current_probabilities, dim=1)[0]   # maximum prob. of a class different from the true class\n    else:\n        # binary\n        prob = torch.nn.functional.sigmoid(logits.view(-1))\n        loss = true_classes.float() * (1. - prob) + (1. - true_classes.float()) * prob\n\n    return reduce_loss(loss, reduction)", "\n\ndef cw_loss(logits, true_classes, target_classes, reduction='mean'):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    assert logits.size()[0] == true_classes.size()[0]\n    assert len(list(true_classes.size())) == 1  # or (len(list(targets.size())) == 2 and targets.size(1) == 1)\n    assert len(list(target_classes.size())) == 1\n    assert len(list(logits.size())) == 2\n\n    if logits.size()[1] > 1:\n        u = torch.arange(logits.shape[0])\n        loss = -(logits[u, true_classes] - torch.max((1 - one_hot(true_classes, logits.size(1))) * logits, dim=1)[0])\n    else:\n        raise ValueError\n    \n    return reduce_loss(loss, reduction)", "\n\ndef conf_cls_loss(logits, d_probs, targets, reduction='mean'):\n    # Cross entropy loss for correct classification: -log f_y(x)\n    return classification_loss(logits, targets, reduction=reduction)\n\n\n'''\nNotation:\n`h_{\\bot}(x)` is the predicted probability of rejection and `h_y(x)` is the predicted probability of class `y`.", "Notation:\n`h_{\\bot}(x)` is the predicted probability of rejection and `h_y(x)` is the predicted probability of class `y`.\n'''\ndef conf_accept_loss(logits, d_probs, targets, reduction='mean'):\n    # Cross entropy loss for accept: log[h_{\\bot}(x)]\n    return -negat_log_proba(d_probs.view(-1), reduction)\n\n\ndef conf_accept_cls_loss(logits, d_probs, targets, reduction='mean'):\n    # Cross entropy loss for accept and correctly classify: -log[h_y(x) * (1 - h_{\\bot}(x))]\n    return classification_loss(logits, targets, reduction=reduction) + negat_log_one_minus_proba(d_probs.view(-1), reduction)", "def conf_accept_cls_loss(logits, d_probs, targets, reduction='mean'):\n    # Cross entropy loss for accept and correctly classify: -log[h_y(x) * (1 - h_{\\bot}(x))]\n    return classification_loss(logits, targets, reduction=reduction) + negat_log_one_minus_proba(d_probs.view(-1), reduction)\n\n\ndef conf_prob_loss(logits, d_probs, targets, reduction='mean'):\n    # loss function: -log[(1 - h_{\\bot}(x)) h_y(x) + h_{\\bot}(x)]\n    probs = torch.softmax(logits, dim=1)\n    combine_probs = probs * (1 - d_probs) + d_probs\n\n    return torch.nn.functional.nll_loss(-1. * negat_log_proba(combine_probs), targets, reduction=reduction)", "\n\n'''\nDifferent versions of the attack loss function in the larger epsilon-ball for the proposed method SATR are named \n`conf_prob_loss_*` and defined below.\n'''\ndef rcd_targeted_loss(logits, targets, reduction='mean'):\n    batch_size, num_classes = logits.size()\n    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n\n    return reduce_loss(log_probs[torch.arange(batch_size), targets], reduction)", "\n\ndef atrr_targeted_loss(logits, aux_probs, targets, reduction='mean'):\n    batch_size, num_classes = logits.size()\n    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n    combined_probs = log_probs[torch.arange(batch_size), targets] - negat_log_proba(aux_probs.view(-1))\n    return reduce_loss(combined_probs, reduction)\n\n\ndef satr_targeted_loss(logits, d_probs, targets, reduction='mean'):\n    # A more numerically stable implementation:\n    # Calculates the log-softmax function directly instead of softmax followed by log.\n    # Takes the sum of the log-probabilities, rather than product followed by log.\n    batch_size, num_classes = logits.size()\n    combined_probs = torch.nn.functional.log_softmax(logits, dim=1) - negat_log_proba(1. - d_probs)\n\n    return reduce_loss(combined_probs[torch.arange(batch_size), targets], reduction)", "\ndef satr_targeted_loss(logits, d_probs, targets, reduction='mean'):\n    # A more numerically stable implementation:\n    # Calculates the log-softmax function directly instead of softmax followed by log.\n    # Takes the sum of the log-probabilities, rather than product followed by log.\n    batch_size, num_classes = logits.size()\n    combined_probs = torch.nn.functional.log_softmax(logits, dim=1) - negat_log_proba(1. - d_probs)\n\n    return reduce_loss(combined_probs[torch.arange(batch_size), targets], reduction)\n", "\n\ndef ccat_targeted_loss(logits, targets, reduction='mean'):\n    batch_size, num_classes = logits.size()\n    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n\n    return reduce_loss(log_probs[torch.arange(batch_size), targets], reduction)\n\n\ndef conf_prob_loss_A1(logits, d_probs, targets, reduction='mean'):\n    # Attack loss function: -log[(1 - h_{\\bot}(x))(1 - max_{j \\noteq y} h_j(x)) + h_{\\bot}(x)]\n    d_probs = d_probs.view(-1)\n    probs = torch.softmax(logits, dim=1)\n    masked_probs = probs * (1 - one_hot(targets, probs.size(1)))\n    combine_probs = torch.max(masked_probs, dim=1)[0] * (1-d_probs)\n\n    return -negat_log_proba(combine_probs, reduction)", "\ndef conf_prob_loss_A1(logits, d_probs, targets, reduction='mean'):\n    # Attack loss function: -log[(1 - h_{\\bot}(x))(1 - max_{j \\noteq y} h_j(x)) + h_{\\bot}(x)]\n    d_probs = d_probs.view(-1)\n    probs = torch.softmax(logits, dim=1)\n    masked_probs = probs * (1 - one_hot(targets, probs.size(1)))\n    combine_probs = torch.max(masked_probs, dim=1)[0] * (1-d_probs)\n\n    return -negat_log_proba(combine_probs, reduction)\n", "\n\ndef conf_prob_loss_A2(logits, d_probs, targets, reduction='mean'):\n    # Attack loss function: -log[(1 - h_{\\bot}(x)) h_y(x) + h_{\\bot}(x)]\n    return conf_prob_loss(logits, d_probs, targets, reduction)\n\n\ndef conf_prob_loss_A3(logits, d_probs, targets, reduction='mean'):\n    # Minimum of the cross-entropy loss of classification and the binary cross-entropy loss of rejection.\n    # min{-log(h_y(x)), -log(h_{\\bot}(x))}\n    loss_1 = classification_loss(logits, targets, reduction='none') \n    loss_2 = negat_log_proba(d_probs.view(-1))\n    loss = torch.min(torch.stack([loss_1, loss_2], dim=1), dim=1)[0]\n\n    return reduce_loss(loss, reduction)", "\n\ndef conf_prob_loss_A4(logits, d_probs, targets, reduction='mean'):\n    # Sum of the cross-entropy loss of classification and the binary cross-entropy loss of rejection.\n    # -log(h_y(x)) - log(h_{\\bot}(x))\n    loss_1 = classification_loss(logits, targets, reduction='none') \n    loss_2 = negat_log_proba(d_probs.view(-1))\n    loss = loss_1 + loss_2\n\n    return reduce_loss(loss, reduction)", "\n\ndef atrr_reject_loss(logits, aux_probs, targets, reduction='mean', scaling_factor=100.0):\n    # softmax_output = torch.softmax(logits, axis=1)\n    # K = logits.size(1)\n    # l2_norm_prob = (torch.sum(softmax_output * softmax_output, axis=1) - 1 / K) * (K / (K - 1))\n    max_conf_loss = torch.logsumexp(logits, dim=1) - (1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n    loss = negat_log_proba(aux_probs.view(-1)) + max_conf_loss\n\n    return reduce_loss(loss, reduction)", "\n\ndef atrr_accept_misclassify_loss(logits, aux_probs, targets, reduction='mean'):\n    current_prob = torch.nn.functional.softmax(logits, dim=1)\n    current_prob = current_prob * (1 - one_hot(targets, current_prob.size(1)))\n    mis_conf = torch.max(current_prob, dim=1)[0]\n    loss = -negat_log_proba(mis_conf * aux_probs.view(-1))\n\n    return reduce_loss(loss, reduction)\n", "\n\ndef robust_abstain_loss(logits, targets, reduction='mean'):\n    N, K = logits.size()\n    loss_1 = torch.nn.functional.cross_entropy(logits[:, :K-1], targets, reduction='none')\n\n    # `(K, K-1)` array where row `i` of the array will have the value `i` omitted\n    indices_temp = numpy.array([[j for j in range(K) if j != i] for i in range(K)])\n    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n    indices = torch.tensor(indices_temp[targets_np, :])\n\n    '''\n    indices = torch.tensor(\n        [[j for j in range(K) if j != targets[i].item()] for i in range(N)]\n    )\n    '''\n    # Class `K-2` corresponds to rejection for array of size `K-1`\n    loss_2 = torch.nn.functional.cross_entropy(logits[torch.unsqueeze(torch.arange(N), 1), indices],\n                                               torch.ones_like(targets) * (K - 2), reduction='none')\n    loss = torch.min(torch.stack([loss_1, loss_2], dim=1), dim=1)[0]\n    return reduce_loss(loss, reduction)", "\n\ndef classification_loss(logits, targets, reduction='mean'):\n    \"\"\"\n    Calculates either the multi-class or binary cross-entropy loss.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n    # assert logits.size()[0] == targets.size()[0]\n    # assert len(list(targets.size())) == 1  # or (len(list(targets.size())) == 2 and targets.size(1) == 1)\n    # assert len(list(logits.size())) == 2\n\n    if logits.size()[1] > 1:\n        return torch.nn.functional.cross_entropy(logits, targets, reduction=reduction)\n    else:\n        # probability 1 is class 1\n        # probability 0 is class 0\n        return torch.nn.functional.binary_cross_entropy(torch.sigmoid(logits).view(-1), targets.float(), reduction=reduction)", "\n\ndef step_function_perturbation(perturb, epsilon_0, alpha=1e-4, norm_type='inf', smooth_approx=False, temperature=0.01):\n    \"\"\"\n    Step function applied to the perturbation norm. By default, it computes the exact step function which is not\n    differentiable. If a smooth approximation based on the sigmoid function is needed, set `smooth_approx=True` and set\n    the `temperature` to a suitably small value.\n\n    :param perturb: Torch Tensor with the perturbation. Can be a tensor of shape `(b, d1, ...)`, where `b` is the batch\n                    size and the rest are dimensions. Can also be a single vector of shape `[d]`.\n    :param epsilon_0: Radius of the smaller perturbation ball - a small positive value.\n    :param alpha: Small negative offset for the step function. The step function's value is `-alpha` when the\n                  perturbation norm is less than `epsilon_0`.\n    :param norm_type: Type of norm: 'inf' for infinity norm, '1', '2' etc for the other types of norm.\n    :param smooth_approx: Set to True to get a sigmoid-based approximation of the step function.\n    :param temperature: small non-negative value that controls  the steepness of the sigmoid approximation.\n\n    :returns: tensor of function values computed for each element in the batch. Has shape `[b]`.\n    \"\"\"\n    assert isinstance(perturb, (torch.Tensor, torch.autograd.Variable)), (\"Input 'perturb' should be of type \"\n                                                                          \"torch.Tensor or torch.autograd.Variable\")\n    s = perturb.shape\n    dim = 1\n    if len(s) > 2:\n        perturb = perturb.view(s[0], -1)    # flatten into a vector\n    elif len(s) == 1:\n        # single vector\n        dim = None\n\n    if norm_type == 'inf':\n        norm_type = float('inf')\n    elif not isinstance(norm_type, (int, float)):\n        # example: norm_type = '2'\n        norm_type = int(norm_type)\n\n    norm_val = torch.linalg.vector_norm(perturb, ord=norm_type, dim=dim)\n    if not smooth_approx:\n        return torch.where(norm_val <= epsilon_0, -1. * alpha, 1.)\n    else:\n        return torch.sigmoid((1. / temperature) * (norm_val - epsilon_0)) - alpha", "\n\ndef ramp_function_perturbation(perturb, epsilon_0, epsilon, alpha=1e-4, norm_type='inf'):\n    \"\"\"\n    Ramp function applied to the perturbation norm as defined in the paper.\n\n    :param perturb: Torch Tensor with the perturbation. Can be a tensor of shape `(b, d1, ...)`, where `b` is the batch\n                    size and the rest are dimensions. Can also be a single vector of shape `(d)`.\n    :param epsilon_0: Radius of the smaller perturbation ball - a small positive value.\n    :param epsilon: Radius of the larger perturbation ball. Should be >= `epsilon_0`.\n    :param alpha: Small negative offset for the step function. The step function's value is `-alpha` when the\n                  perturbation norm is less than `epsilon_0`.\n    :param norm_type: Type of norm: 'inf' for infinity norm, '1', '2' etc for the other types of norm.\n\n    :returns: tensor of function values computed for each element in the batch. Has shape `[b]`.\n    \"\"\"\n    assert isinstance(perturb, (torch.Tensor, torch.autograd.Variable)), (\"Input 'perturb' should be of type \"\n                                                                          \"torch.Tensor or torch.autograd.Variable\")\n    assert epsilon >= epsilon_0, \"Value of 'epsilon' cannot be smaller than 'epsilon_0'\"\n    s = perturb.shape\n    dim = 1\n    if len(s) > 2:\n        perturb = perturb.view(s[0], -1)    # flatten into a vector\n    elif len(s) == 1:\n        # single vector\n        dim = None\n\n    if norm_type == 'inf':\n        norm_type = float('inf')\n    elif not isinstance(norm_type, (int, float)):\n        # example: norm_type = '2'\n        norm_type = int(norm_type)\n\n    norm_val = torch.linalg.vector_norm(perturb, ord=norm_type, dim=dim)\n    temp = torch.maximum(norm_val - epsilon_0, torch.zeros_like(norm_val))\n\n    return ((1. + alpha) / (epsilon - epsilon_0)) * temp - alpha", "\n\ndef max_p_loss(logits, targets=None, reduction='mean'):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    max_log = torch.max(torch.nn.functional.softmax(logits, dim=1), dim=1)[0]\n    if reduction == 'mean':\n        return torch.mean(max_log)\n    elif reduction == 'sum':\n        return torch.sum(max_log)\n    else:\n        return max_log", "\n\ndef max_log_loss(logits, targets=None, reduction='mean'):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted classes\n    :type logits: torch.autograd.Variable\n    :param targets: target classes\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    max_log = torch.max(torch.nn.functional.log_softmax(logits, dim=1), dim=1)[0]\n    if reduction == 'mean':\n        return torch.mean(max_log)\n    elif reduction == 'sum':\n        return torch.sum(max_log)\n    else:\n        return max_log", "\n\ndef cross_entropy_divergence(logits, targets, reduction='mean'):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted logits\n    :type logits: torch.autograd.Variable\n    :param targets: target distributions\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    assert len(list(logits.size())) == len(list(targets.size()))\n    assert logits.size()[0] == targets.size()[0]\n    assert logits.size()[1] == targets.size()[1]\n    assert logits.size()[1] > 1\n\n    divergences = torch.sum(- targets * torch.nn.functional.log_softmax(logits, dim=1), dim=1)\n    if reduction == 'mean':\n        return torch.mean(divergences)\n    elif reduction == 'sum':\n        return torch.sum(divergences)\n    else:\n        return divergences", "\n\ndef bhattacharyya_coefficient(logits, targets):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted logits\n    :type logits: torch.autograd.Variable\n    :param targets: target distributions\n    :type targets: torch.autograd.Variable\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    assert len(list(logits.size())) == len(list(targets.size()))\n    assert logits.size()[0] == targets.size()[0]\n    assert logits.size()[1] == targets.size()[1]\n    assert logits.size()[1] > 1\n\n    # http://www.cse.yorku.ca/~kosta/CompVis_Notes/bhattacharyya.pdf\n    # torch.sqrt not differentiable at zero\n    return torch.clamp(torch.sum(torch.sqrt(torch.nn.functional.softmax(logits, dim=1) * targets + SMALL_VALUE), dim=1), min=0, max=1)", "\n\ndef bhattacharyya_divergence(logits, targets, reduction='mean'):\n    \"\"\"\n    Loss.\n\n    :param logits: predicted logits\n    :type logits: torch.autograd.Variable\n    :param targets: target distributions\n    :type targets: torch.autograd.Variable\n    :param reduction: reduction type\n    :type reduction: str\n    :return: error\n    :rtype: torch.autograd.Variable\n    \"\"\"\n\n    divergences = - 2*torch.log(bhattacharyya_coefficient(logits, targets))\n    if reduction == 'mean':\n        return torch.mean(divergences)\n    elif reduction == 'sum':\n        return torch.sum(divergences)\n    else:\n        return divergences", "\n\ndef linear_transition(perturbations, norm, epsilon=0.3, gamma=1):\n    \"\"\"\n    Linear transition rule.\n\n    :param perturbations: perturbations\n    :type perturbations: torch.autograd.Variable\n    :param norm: norm\n    :type norm: attacks.norms.Norm\n    :param epsilon: epsilon\n    :type epsilon: float\n    :param gamma: gamma\n    :type gamma: float\n    :return: gamma, norms\n    :rtype: torch.autograd.Variable, torch.autograd.Variable\n    \"\"\"\n\n    norms = norm(perturbations)\n    return torch.min(torch.ones_like(norms), gamma * norms / epsilon), norms", "\n\ndef power_transition(perturbations, norm, epsilon=0.3, gamma=1):\n    \"\"\"\n    Power transition rule.\n\n    :param perturbations: perturbations\n    :type perturbations: torch.autograd.Variable\n    :param norm: norm\n    :type norm: attacks.norms.Norm\n    :param epsilon: epsilon\n    :type epsilon: float\n    :param gamma: gamma\n    :type gamma: float\n    :return: gamma, norms\n    :rtype: torch.autograd.Variable, torch.autograd.Variable\n    \"\"\"\n\n    # returned value determines importance of uniform distribution:\n    # (1 - ret)*one_hot + ret*uniform\n\n    norms = norm(perturbations)\n    return 1 - torch.pow(1 - torch.min(torch.ones_like(norms), norms / epsilon), gamma), norms", "\n\ndef exponential_transition(perturbations, norm, epsilon=0.3, gamma=1):\n    \"\"\"\n    Exponential transition rule.\n\n    :param perturbations: perturbations\n    :type perturbations: torch.autograd.Variable\n    :param norm: norm\n    :type norm: attacks.norms.Norm\n    :param epsilon: epsilon\n    :type epsilon: float\n    :param gamma: gamma\n    :type gamma: float\n    :return: gamma, norms\n    :rtype: torch.autograd.Variable, torch.autograd.Variable\n    \"\"\"\n\n    norms = norm(perturbations)\n    return 1 - torch.exp(-gamma * norms), norms", "\n\nclass View(torch.nn.Module):\n    \"\"\"\n    Simple view layer.\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Constructor.\n\n        :param args: shape\n        :type args: [int]\n        \"\"\"\n\n        super(View, self).__init__()\n\n        self.shape = args\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return input.view(self.shape)", "\n\nclass Flatten(torch.nn.Module):\n    \"\"\"\n    Flatten module.\n    \"\"\"\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return input.view(input.shape[0], -1)", "\n\nclass Clamp(torch.nn.Module):\n    \"\"\"\n    Wrapper for clamp.\n    \"\"\"\n\n    def __init__(self, min=0, max=1):\n        \"\"\"\n        Constructor.\n        \"\"\"\n\n        super(Clamp, self).__init__()\n\n        self.min = min\n        \"\"\" (float) Min value. \"\"\"\n\n        self.max = max\n        \"\"\" (float) Max value. \"\"\"\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return torch.clamp(torch.clamp(input, min=self.min), max=self.max)", "\n\nclass Scale(torch.nn.Module):\n    \"\"\"\n    Simply scaling layer, mainly to allow simple saving and loading.\n    \"\"\"\n\n    def __init__(self, shape):\n        \"\"\"\n        Constructor.\n\n        :param shape: shape\n        :type shape: [int]\n        \"\"\"\n\n        super(Scale, self).__init__()\n\n        self.weight = torch.nn.Parameter(torch.zeros(shape)) # min\n        self.bias = torch.nn.Parameter(torch.ones(shape)) # max\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return expand_as(self.weight, input) + torch.mul(expand_as(self.bias, input) - expand_as(self.weight, input), input)", "\n\nclass Entropy(torch.nn.Module):\n    \"\"\"\n    Entropy computation based on logits.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Constructor.\n        \"\"\"\n\n        super(Entropy, self).__init__()\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return -1.*torch.sum(torch.nn.functional.softmax(input, dim=1) * torch.nn.functional.log_softmax(input, dim=1))", "\n\nclass Normalize(torch.nn.Module):\n    \"\"\"\n    Normalization layer to be learned.\n    \"\"\"\n\n    def __init__(self, n_channels):\n        \"\"\"\n        Constructor.\n\n        :param n_channels: number of channels\n        :type n_channels: int\n        \"\"\"\n\n        super(Normalize, self).__init__()\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = torch.nn.Parameter(torch.ones(n_channels))\n        self.bias = torch.nn.Parameter(torch.zeros(n_channels))\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return (input - self.bias.view(1, -1, 1, 1))/self.weight.view(1, -1, 1, 1)", "\n\nclass GaussianLayer(torch.nn.Module):\n    \"\"\"\n    Gaussian convolution.\n\n    See https://pytorch.org/docs/stable/nn.html.\n    \"\"\"\n\n    def __init__(self, sigma=3, channels=3):\n        \"\"\"\n\n        \"\"\"\n        super(GaussianLayer, self).__init__()\n\n        self.sigma = sigma\n        \"\"\" (float) Sigma. \"\"\"\n\n        padding = math.ceil(self.sigma)\n        kernel = 2*padding + 1\n\n        self.seq = torch.nn.Sequential(\n            torch.nn.ReflectionPad2d((padding, padding, padding, padding)),\n            torch.nn.Conv2d(channels, channels, kernel, stride=1, padding=0, bias=None, groups=channels)\n        )\n\n        n = numpy.zeros((kernel, kernel))\n        n[padding, padding] = 1\n\n        k = scipy.ndimage.gaussian_filter(n, sigma=self.sigma)\n        for name, f in self.named_parameters():\n            f.data.copy_(torch.from_numpy(k))\n\n    def forward(self, input):\n        \"\"\"\n        Forward pass.\n\n        :param input: input\n        :type input: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        return self.seq(input)", ""]}
{"filename": "utils/trades.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\n\ndef squared_l2_norm(x):\n    flattened = x.view(x.unsqueeze(0).shape[0], -1)\n    return (flattened ** 2).sum(1)", "\n\ndef l2_norm(x):\n    return squared_l2_norm(x).sqrt()\n\n\ndef trades_loss(model,\n                x_natural,\n                y,\n                optimizer,\n                step_size=0.003,\n                epsilon=0.031,\n                perturb_steps=10,\n                beta=1.0,\n                distance='l_inf'):\n    # define KL-loss\n    criterion_kl = nn.KLDivLoss(size_average=False)\n    model.eval()\n    batch_size = len(x_natural)\n    # generate adversarial example\n    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n    if distance == 'l_inf':\n        for _ in range(perturb_steps):\n            x_adv.requires_grad_()\n            with torch.enable_grad():\n                loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n                                       F.softmax(model(x_natural), dim=1))\n            grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n            x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n            x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n    elif distance == 'l_2':\n        delta = 0.001 * torch.randn(x_natural.shape).cuda().detach()\n        delta = Variable(delta.data, requires_grad=True)\n\n        # Setup optimizers\n        optimizer_delta = optim.SGD([delta], lr=epsilon / perturb_steps * 2)\n\n        for _ in range(perturb_steps):\n            adv = x_natural + delta\n\n            # optimize\n            optimizer_delta.zero_grad()\n            with torch.enable_grad():\n                loss = (-1) * criterion_kl(F.log_softmax(model(adv), dim=1),\n                                           F.softmax(model(x_natural), dim=1))\n            loss.backward()\n            # renorming gradient\n            grad_norms = delta.grad.view(batch_size, -1).norm(p=2, dim=1)\n            delta.grad.div_(grad_norms.view(-1, 1, 1, 1))\n            # avoid nan or inf if gradient is 0\n            if (grad_norms == 0).any():\n                delta.grad[grad_norms == 0] = torch.randn_like(delta.grad[grad_norms == 0])\n            optimizer_delta.step()\n\n            # projection\n            delta.data.add_(x_natural)\n            delta.data.clamp_(0, 1).sub_(x_natural)\n            delta.data.renorm_(p=2, dim=0, maxnorm=epsilon)\n        x_adv = Variable(x_natural + delta, requires_grad=False)\n    else:\n        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n    model.train()\n\n    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n    # zero gradient\n    optimizer.zero_grad()\n    # calculate robust loss\n    logits = model(x_natural)\n    loss_natural = F.cross_entropy(logits, y)\n    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n                                                    F.softmax(model(x_natural), dim=1))\n    loss = loss_natural + beta * loss_robust\n    return loss"]}
{"filename": "utils/dataset.py", "chunked_list": ["import os\nimport torch\nimport torch.utils.data # needs to be imported separately\nimport utils.lib as utils\nimport numpy\nimport skimage.transform\nfrom PIL import Image\nimport torch\n\n", "\n\n# base directory for data\nBASE_DATA = './datasets'\n\n# Common extension types used.\nTXT_EXT = '.txt'\nHDF5_EXT = '.h5'\nSTATE_EXT = '.pth.tar'\nLOG_EXT = '.log'", "STATE_EXT = '.pth.tar'\nLOG_EXT = '.log'\nPNG_EXT = '.png'\nPICKLE_EXT = '.pkl'\nTEX_EXT = '.tex'\nMAT_EXT = '.mat'\nGZIP_EXT = '.gz'\n\n# Naming conventions.\ndef data_file(name, ext=HDF5_EXT):\n    \"\"\"\n    Generate path to data file.\n\n    :param name: name of file\n    :type name: str\n    :param ext: extension (including period)\n    :type ext: str\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return os.path.join(BASE_DATA, name) + ext", "# Naming conventions.\ndef data_file(name, ext=HDF5_EXT):\n    \"\"\"\n    Generate path to data file.\n\n    :param name: name of file\n    :type name: str\n    :param ext: extension (including period)\n    :type ext: str\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return os.path.join(BASE_DATA, name) + ext", "\n\ndef raw_svhn_train_file():\n    \"\"\"\n    Raw SVHN training directory.\n\n    :return: fielpath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/train_32x32', '.mat')", "\n\ndef raw_svhn_test_file():\n    \"\"\"\n    Raw SVHN training directory.\n\n    :return: fielpath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/test_32x32', '.mat')", "\n\ndef svhn_train_images_file():\n    \"\"\"\n    SVHN train images.\n\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/train_images', HDF5_EXT)", "\n\ndef svhn_test_images_file():\n    \"\"\"\n    SVHN test images.\n\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/test_images', HDF5_EXT)", "\n\ndef svhn_train_labels_file():\n    \"\"\"\n    SVHN train labels.\n\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/train_labels', HDF5_EXT)", "\n\ndef svhn_test_labels_file():\n    \"\"\"\n    SVHN test labels.\n\n    :return: filepath\n    :rtype: str\n    \"\"\"\n\n    return data_file('svhn/test_labels', HDF5_EXT)", "\n\nclass CleanDataset(torch.utils.data.Dataset):\n    \"\"\"\n    General, clean dataset used for training, testing and attacking.\n    \"\"\"\n\n    def __init__(self, images, labels, indices=None, transform=None):\n        \"\"\"\n        Constructor.\n\n        :param images: images/inputs\n        :type images: str or numpy.ndarray\n        :param labels: labels\n        :type labels: str or numpy.ndarray\n        :param indices: indices\n        :type indices: numpy.ndarray\n        :param resize: resize in [channels, height, width\n        :type resize: resize\n        \"\"\"\n\n        self.images_file = None\n        \"\"\" (str) File images were loaded from. \"\"\"\n\n        self.labels_file = None\n        \"\"\" (str) File labels were loaded from. \"\"\"\n\n        if isinstance(images, str):\n            self.images_file = images\n            images = utils.read_hdf5(self.images_file)\n        if not images.dtype == numpy.float32:\n            images = images.astype(numpy.float32)\n\n        if isinstance(labels, str):\n            self.labels_file = labels\n            labels = utils.read_hdf5(self.labels_file)\n        labels = numpy.squeeze(labels)\n        if not labels.dtype == int:\n            labels = labels.astype(int)\n\n        assert isinstance(images, numpy.ndarray)\n        assert isinstance(labels, numpy.ndarray)\n        assert images.shape[0] == labels.shape[0]\n\n        if indices is None:\n            indices = range(images.shape[0])\n        assert numpy.min(indices) >= 0\n        assert numpy.max(indices) < images.shape[0]\n\n        self.images = images[indices]\n        # self.images = []\n        # for i in indices:\n        #     self.images.append(Image.fromarray((images[i] * 255).astype(numpy.uint8)))\n        \"\"\" (numpy.ndarray) Inputs. \"\"\"\n\n        self.labels = labels[indices]\n        \"\"\" (numpy.ndarray) Labels. \"\"\"\n\n        self.transform = transform\n        \"\"\" (numpy.ndarray) Possible attack targets. \"\"\"\n\n    def __getitem__(self, index):\n        assert index < len(self)\n        \n        if self.transform is None:\n            return self.images[index], self.labels[index]\n        else:\n            return self.transform(self.images[index]), self.labels[index]\n\n    def __len__(self):\n        assert len(self.images) == self.labels.shape[0]\n        return len(self.images)\n\n    def __add__(self, other):\n        return torch.utils.data.ConcatDataset([self, other])", "\n\nclass SVHNTrainSet(CleanDataset):\n    def __init__(self, transform=None):\n        super(SVHNTrainSet, self).__init__(svhn_train_images_file(), svhn_train_labels_file(), None, transform)\n\n\nclass SVHNTestSet(CleanDataset):\n    def __init__(self, transform=None):\n        super(SVHNTestSet, self).__init__(svhn_test_images_file(), svhn_test_labels_file(), range(10000), transform)", "\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data, targets, transform=None):\n        self.data = data\n        self.targets = torch.LongTensor(targets)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.targets[index]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y\n\n    def __len__(self):\n        return len(self.data)"]}
{"filename": "utils/numpy.py", "chunked_list": ["import numpy\nimport scipy.stats\nimport math\n\n\ndef one_hot(array, N):\n    \"\"\"\n    Convert an array of numbers to an array of one-hot vectors.\n\n    :param array: classes to convert\n    :type array: numpy.ndarray\n    :param N: number of classes\n    :type N: int\n    :return: one-hot vectors\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    array = array.astype(int)\n    assert numpy.max(array) < N\n    assert numpy.min(array) >= 0\n\n    one_hot = numpy.zeros((array.shape[0], N))\n    one_hot[numpy.arange(array.shape[0]), array] = 1\n    return one_hot", "\n\ndef expand_as(array, array_as):\n    \"\"\"\n    Expands the tensor using view to allow broadcasting.\n\n    :param array: input tensor\n    :type array: numpy.ndarray\n    :param array_as: reference tensor\n    :type array_as: torch.Tensor or torch.autograd.Variable\n    :return: tensor expanded with singelton dimensions as tensor_as\n    :rtype: torch.Tensor or torch.autograd.Variable\n    \"\"\"\n\n    shape = list(array.shape)\n    for i in range(len(array.shape), len(array_as.shape)):\n        shape.append(1)\n\n    return array.reshape(shape)", "\n\ndef concatenate(array1, array2, axis=0):\n    \"\"\"\n    Basically a wrapper for numpy.concatenate, with the exception\n    that the array itself is returned if its None or evaluates to False.\n\n    :param array1: input array or None\n    :type array1: mixed\n    :param array2: input array\n    :type array2: numpy.ndarray\n    :param axis: axis to concatenate\n    :type axis: int\n    :return: concatenated array\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    assert isinstance(array2, numpy.ndarray)\n    if array1 is not None:\n        assert isinstance(array1, numpy.ndarray)\n        return numpy.concatenate((array1, array2), axis=axis)\n    else:\n        return array2", "\n\ndef exponential_norm(batch_size, dim, epsilon=1, ord=2):\n    \"\"\"\n    Sample vectors uniformly by norm and direction separately.\n\n    :param batch_size: how many vectors to sample\n    :type batch_size: int\n    :param dim: dimensionality of vectors\n    :type dim: int\n    :param epsilon: epsilon-ball\n    :type epsilon: float\n    :param ord: norm to use\n    :type ord: int\n    :return: batch_size x dim tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    random = numpy.random.randn(batch_size, dim)\n    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n    random *= epsilon\n\n    truncated_normal = scipy.stats.truncexpon.rvs(1, loc=0, scale=0.9, size=(batch_size, 1))\n    random *= numpy.repeat(truncated_normal, axis=1, repeats=dim)\n\n    return random", "\n\ndef uniform_norm(batch_size, dim, epsilon=1, ord=2):\n    \"\"\"\n    Sample vectors uniformly by norm and direction separately.\n\n    :param batch_size: how many vectors to sample\n    :type batch_size: int\n    :param dim: dimensionality of vectors\n    :type dim: int\n    :param epsilon: epsilon-ball\n    :type epsilon: float\n    :param ord: norm to use\n    :type ord: int\n    :return: batch_size x dim tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    random = numpy.random.randn(batch_size, dim)\n    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n    random *= epsilon\n    uniform = numpy.random.uniform(0, 1, (batch_size, 1))  # exponent is only difference!\n    random *= numpy.repeat(uniform, axis=1, repeats=dim)\n\n    return random", "\n\ndef uniform_ball(batch_size, dim, epsilon=1, ord=2):\n    \"\"\"\n    Sample vectors uniformly in the n-ball.\n\n    See Harman et al., On decompositional algorithms for uniform sampling from n-spheres and n-balls.\n\n    :param batch_size: how many vectors to sample\n    :type batch_size: int\n    :param dim: dimensionality of vectors\n    :type dim: int\n    :param epsilon: epsilon-ball\n    :type epsilon: float\n    :param ord: norm to use\n    :type ord: int\n    :return: batch_size x dim tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    random = numpy.random.randn(batch_size, dim)\n    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n    random *= epsilon\n    uniform = numpy.random.uniform(0, 1, (batch_size, 1)) ** (1. / dim)\n    random *= numpy.repeat(uniform, axis=1, repeats=dim)\n\n    return random", "\n\ndef uniform_sphere(batch_size, dim, epsilon=1, ord=2):\n    \"\"\"\n    Sample vectors uniformly on the n-sphere.\n\n    See Harman et al., On decompositional algorithms for uniform sampling from n-spheres and n-balls.\n\n    :param batch_size: how many vectors to sample\n    :type batch_size: int\n    :param dim: dimensionality of vectors\n    :type dim: int\n    :param epsilon: epsilon-ball\n    :type epsilon: float\n    :param ord: norm to use\n    :type ord: int\n    :return: batch_size x dim tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    random = numpy.random.randn(batch_size, dim)\n    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n    random *= epsilon\n\n    return random", "\n\ndef truncated_normal(size, lower=-2, upper=2):\n    \"\"\"\n    Sample from truncated normal.\n\n    See https://stackoverflow.com/questions/18441779/how-to-specify-upper-and-lower-limits-when-using-numpy-random-normal.\n\n    :param size: size of vector\n    :type size: [int]\n    :param lower: lower bound\n    :type lower: float\n    :param upper: upper bound\n    :type upper: float\n    :return: batch_size x dim tensor\n    :rtype: numpy.ndarray\n    \"\"\"\n\n    return scipy.stats.truncnorm.rvs(lower, upper, size=size)", "\n\ndef project_simplex(v, s=1):\n    \"\"\"\n    Taken from https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246.\n\n    Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    \"\"\"\n\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and numpy.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = numpy.sort(v)[::-1]\n    cssv = numpy.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = numpy.nonzero(u * numpy.arange(1, n+1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / rho\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\ndef projection_simplex_sort(v, z=1):\n    n_features = v.shape[0]\n    u = numpy.sort(v)[::-1]\n    cssv = numpy.cumsum(u) - z\n    ind = numpy.arange(n_features) + 1\n    cond = u - cssv / ind > 0\n    rho = ind[cond][-1]\n    theta = cssv[cond][-1] / float(rho)\n    w = numpy.maximum(v - theta, 0)\n    return w", "\n\ndef projection_simplex_pivot(v, z=1, random_state=None):\n    rs = numpy.random.RandomState(random_state)\n    n_features = len(v)\n    U = numpy.arange(n_features)\n    s = 0\n    rho = 0\n    while len(U) > 0:\n        G = []\n        L = []\n        k = U[rs.randint(0, len(U))]\n        ds = v[k]\n        for j in U:\n            if v[j] >= v[k]:\n                if j != k:\n                    ds += v[j]\n                    G.append(j)\n            elif v[j] < v[k]:\n                L.append(j)\n        drho = len(G) + 1\n        if s + ds - (rho + drho) * v[k] < z:\n            s += ds\n            rho += drho\n            U = L\n        else:\n            U = G\n    theta = (s - z) / float(rho)\n    return numpy.maximum(v - theta, 0)", "\n\ndef projection_simplex_bisection(v, z=1, tau=0.0001, max_iter=1000):\n    lower = 0\n    upper = numpy.max(v)\n    current = numpy.inf\n\n    for it in xrange(max_iter):\n        if numpy.abs(current) / z < tau and current < 0:\n            break\n\n        theta = (upper + lower) / 2.0\n        w = numpy.maximum(v - theta, 0)\n        current = numpy.sum(w) - z\n        if current <= 0:\n            upper = theta\n        else:\n            lower = theta\n    return w", "\n\ndef project_ball(array, epsilon=1, ord=2):\n    \"\"\"\n    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n\n    **Assumes the first dimension to be batch dimension, which is preserved.**\n\n    :param array: array\n    :type array: numpy.ndarray\n    :param epsilon: radius of ball.\n    :type epsilon: float\n    :param ord: order of norm\n    :type ord: int\n    :return: projected vector\n    :rtype: torch.autograd.Variable or torch.Tensor\n    \"\"\"\n\n    assert isinstance(array, numpy.ndarray), 'given tensor should be numpy.ndarray'\n\n    if ord == 0:\n        assert epsilon >= 1\n        size = array.shape\n        flattened_size = numpy.prod(numpy.array(size[1:]))\n\n        array = array.reshape(-1, flattened_size)\n        sorted = numpy.sort(array, axis=1)\n\n        k = int(math.ceil(epsilon))\n        thresholds = sorted[:, -k]\n\n        mask = (array >= expand_as(thresholds, array)).astype(float)\n        array *= mask\n    elif ord == 1:\n        size = array.shape\n        flattened_size = numpy.prod(numpy.array(size[1:]))\n\n        array = array.reshape(-1, flattened_size)\n\n        if False: #  Version 1\n            # https://github.com/ftramer/MultiRobustness/blob/master/pgd_attack.py\n            l1 = numpy.sum(numpy.abs(array), axis=1)\n            epsilons = numpy.ones((array.shape[0]))*epsilon\n            to_project = l1 > epsilons\n            if numpy.any(to_project):\n                n = numpy.sum(to_project)\n                d = array[to_project]#.reshape(n, -1)  # n * N (N=h*w*ch)\n                abs_d = numpy.abs(d)  # n * N\n                mu = -numpy.sort(-abs_d, axis=-1)  # n * N\n                cumsums = mu.cumsum(axis=-1)  # n * N\n                eps_d = epsilons[to_project]\n                js = 1.0 / numpy.arange(1, array.shape[1] + 1)\n                temp = mu - js * (cumsums - numpy.expand_dims(eps_d, -1))\n                rho = numpy.argmin(temp > 0, axis=-1)\n                theta = 1.0 / (1 + rho) * (cumsums[range(n), rho] - eps_d)\n                sgn = numpy.sign(d)\n                d = sgn * numpy.maximum(abs_d - numpy.expand_dims(theta, -1), 0)\n                array[to_project] = d.reshape(-1, array.shape[1])\n\n        if True: # Version 2\n            for i in range(array.shape[0]):\n                # compute the vector of absolute values\n                u = numpy.abs(array[i])\n                # check if v is already a solution\n                if u.sum() <= epsilon:\n                    # L1-norm is <= s\n                    continue\n                # v is not already a solution: optimum lies on the boundary (norm == s)\n                # project *u* on the simplex\n                #w = project_simplex(u, s=epsilon)\n                w = projection_simplex_sort(u, z=epsilon)\n                # compute the solution to the original problem on v\n                w *= numpy.sign(array[i])\n                array[i] = w\n\n        if len(size) == 4:\n            array = array.reshape(-1, size[1], size[2], size[3])\n        elif len(size) == 2:\n            array = array.reshape(-1, size[1])\n    elif ord == 2:\n        size = array.shape\n        flattened_size = numpy.prod(numpy.array(size[1:]))\n\n        array = array.reshape(-1, flattened_size)\n        clamped = numpy.clip(epsilon/numpy.linalg.norm(array, 2, axis=1), a_min=None, a_max=1)\n        clamped = clamped.reshape(-1, 1)\n\n        array = array * clamped\n        if len(size) == 4:\n            array = array.reshape(-1, size[1], size[2], size[3])\n        elif len(size) == 2:\n            array = array.reshape(-1, size[1])\n    elif ord == float('inf'):\n        array = numpy.clip(array, a_min=-epsilon, a_max=epsilon)\n    else:\n        raise NotImplementedError()\n\n    return array", "\n\ndef max_detector(probabilities):\n    return numpy.max(probabilities, axis=1)\n"]}
{"filename": "utils/constants.py", "chunked_list": ["\nBASE_LR_RANGE = {'mnist': [0.1, 0.01, 0.005],\n                 'cifar10': [0.01, 0.005, 0.001],\n                 'gtsrb': [0.01, 0.005, 0.001],\n                 'svhn': [0.01, 0.005, 0.001]}\n\n# Range of alpha values used in the definition of robust error with rejection\nALPHA_LIST = [0.0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 1.0]\n\nGAP_FACTOR = {", "\nGAP_FACTOR = {\n    'mnist': 0.02,\n    'cifar10': 0.5/255,\n    'gtsrb': 0.5/255,\n    'svhn': 0.5/255\n}\n\n# Number of classes\nN_CLASSES = {", "# Number of classes\nN_CLASSES = {\n    'mnist': 10,\n    'cifar10': 10,\n    'gtsrb': 43,\n    'svhn': 10\n}\n\n# Target true positive rate for selecting the rejection threshold\nTPR_THRESHOLD = {", "# Target true positive rate for selecting the rejection threshold\nTPR_THRESHOLD = {\n    'mnist': 0.99,\n    'cifar10': 0.95,\n    'gtsrb': 0.95,\n    'svhn': 0.95\n}\n\n# Fraction of samples used for validation\nVAL_RATIO = 0.1", "# Fraction of samples used for validation\nVAL_RATIO = 0.1\n\n# Sample size used for calculating the robustness with rejection\nN_SAMPLES = 1000\n\n# Setting for the adaptive attacks\nCONFIG_PGDBT_ATTACK_OUTER = {\n    'max_iterations': 1000,\n    'base_lr': 0.001,", "    'max_iterations': 1000,\n    'base_lr': 0.001,\n    'momentum': 0.9,\n    'lr_factor': 1.1,\n    'backtrack': True,\n    'rand_init_name': 'random',\n    'num_rand_init': 10,\n    'clip_min': 0.0,\n    'clip_max': 1.0\n}", "    'clip_max': 1.0\n}\n\nCONFIG_PGD_ATTACK_INNER = {\n    'max_iterations': 200,\n    'base_lr': 0.1,\n    'momentum': 0.9,\n    'lr_factor': 1.25,\n    'backtrack': False,\n    'rand_init_name': 'random',", "    'backtrack': False,\n    'rand_init_name': 'random',\n    'num_rand_init': 5,\n    'clip_min': 0.0,\n    'clip_max': 1.0\n}\n\nCONFIG_MULTITARGET_ATTACK_OUTER = {\n    'max_iterations': 200,\n    'base_lr': 0.1,", "    'max_iterations': 200,\n    'base_lr': 0.1,\n    'momentum': 0.9,\n    'lr_factor': 1.1,\n    'backtrack': False,\n    'rand_init_name': 'random',\n    'num_rand_init': 5,\n    'clip_min': 0.0,\n    'clip_max': 1.0\n}", "    'clip_max': 1.0\n}\n\nCONFIG_MULTITARGET_ATTACK_INNER = {\n     'max_iterations': 200,\n     'base_lr': 0.1,\n     'momentum': 0.9,\n     'lr_factor': 1.25,\n     'backtrack': False,\n     'rand_init_name': 'random',", "     'backtrack': False,\n     'rand_init_name': 'random',\n     'num_rand_init': 1,\n     'clip_min': 0.0,\n     'clip_max': 1.0\n}\n\n\n# Plot colors and markers\n# https://matplotlib.org/2.0.2/examples/color/named_colors.html", "# Plot colors and markers\n# https://matplotlib.org/2.0.2/examples/color/named_colors.html\nCOLORS = ['r', 'b', 'c', 'orange', 'g', 'm', 'lawngreen', 'grey', 'hotpink', 'y', 'steelblue', 'tan',\n          'lightsalmon', 'navy', 'gold']\nMARKERS = ['o', '^', 'v', 's', '*', 'x', 'd', '>', '<', '1', 'h', 'P', '_', '2', '|', '3', '4']\n\n\ndef lr_schedule(t, lr_max):\n    # Learning rate schedule\n    if t < 100:\n        return lr_max\n    elif t < 105:\n        return lr_max / 10.\n    else:\n        return lr_max / 100.", ""]}
{"filename": "models/preact_resnet_block.py", "chunked_list": ["\"\"\"\nPre-activation ResNet block in PyTorch.\nTaken from https://github.com/locuslab/robust_union/.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBlock(nn.Module):\n    \"\"\"\n    Pre-Activation ResNet block.\n    \"\"\"\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        \"\"\"\n        Constructor.\n\n        :param in_planes: number of input planes\n        :type in_planes: int\n        :param planes: output planes\n        :type planes: int\n        :param stride: stride for block\n        :type stride: int\n        \"\"\"\n\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out", "\nclass PreActBlock(nn.Module):\n    \"\"\"\n    Pre-Activation ResNet block.\n    \"\"\"\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        \"\"\"\n        Constructor.\n\n        :param in_planes: number of input planes\n        :type in_planes: int\n        :param planes: output planes\n        :type planes: int\n        :param stride: stride for block\n        :type stride: int\n        \"\"\"\n\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out"]}
{"filename": "models/mlp.py", "chunked_list": ["import torch\nimport utils.torch\nfrom .classifier import Classifier\nfrom operator import mul\nfrom functools import reduce\n\n\nclass MLP(Classifier):\n    \"\"\"\n    MLP classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), units=[64, 64, 64], activation=torch.nn.ReLU, normalization=torch.nn.BatchNorm1d, bias=True, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param activation: activation function\n        :type activation: None or torch.nn.Module\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param bias: whether to use bias\n        :type bias: bool\n        \"\"\"\n\n        super(MLP, self).__init__(N_class, resolution, **kwargs)\n\n        self.units = units\n        \"\"\" ([int]) Units. \"\"\"\n\n        self.activation = activation\n        \"\"\" (callable) activation\"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.bias = bias\n        \"\"\" (bool) Bias. \"\"\"\n\n        gain = 1\n        if self.activation is not None:\n            gain = torch.nn.init.calculate_gain(self.activation().__class__.__name__.lower())\n\n        # not overwriting self.units!\n        units = [reduce(mul, self.resolution, 1)] + self.units\n        view = utils.torch.View(-1, units[0])\n        self.append_layer('view0', view)\n\n        for layer in range(1, len(units)):\n            in_features = units[layer - 1]\n            out_features = units[layer]\n\n            lin = torch.nn.Linear(in_features=in_features, out_features=out_features, bias=self.bias)\n            torch.nn.init.kaiming_normal_(lin.weight, gain)\n            if self.bias:\n                torch.nn.init.constant_(lin.bias, 0)\n            self.append_layer('lin%d' % layer, lin)\n\n            if self.activation:\n                act = self.activation()\n                self.append_layer('act%d' % layer, act)\n\n            if self.normalization is not None:\n                bn = self.normalization(out_features)\n                torch.nn.init.constant_(bn.weight, 1)\n                torch.nn.init.constant_(bn.bias, 0)\n                self.append_layer('bn%d' % layer, bn)\n\n        logits = torch.nn.Linear(units[-1], self._N_output)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.append_layer('logits', logits)", "\n"]}
{"filename": "models/preact_resnet_bottleneck.py", "chunked_list": ["\"\"\"\nPre-activation ResNet bottleneck block in PyTorch.\nTaken from https://github.com/locuslab/robust_union/.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBottleneck(nn.Module):\n    \"\"\"\n    Pre-Activation ResNet bottleneck block.\n    \"\"\"\n\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        \"\"\"\n        Constructor.\n\n        :param in_planes: number of input planes\n        :type in_planes: int\n        :param planes: output planes\n        :type planes: int\n        :param stride: stride for block\n        :type stride: int\n        \"\"\"\n\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out", "\nclass PreActBottleneck(nn.Module):\n    \"\"\"\n    Pre-Activation ResNet bottleneck block.\n    \"\"\"\n\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        \"\"\"\n        Constructor.\n\n        :param in_planes: number of input planes\n        :type in_planes: int\n        :param planes: output planes\n        :type planes: int\n        :param stride: stride for block\n        :type stride: int\n        \"\"\"\n\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out"]}
{"filename": "models/wide_resnet_block.py", "chunked_list": ["\"\"\"\nWide ResNet block.\nTaken from https://github.com/meliketoy/wide-resnet.pytorch.\n\"\"\"\nimport torch\nimport numpy\n\n\nclass WideResNetBlock(torch.nn.Module):\n    \"\"\"\n    Wide ResNet block.\n    \"\"\"\n\n    def __init__(self, inplanes, planes, stride=1, dropout=0, normalization=True):\n        \"\"\"\n        Constructor.\n\n        :param inplanes: input channels\n        :type inplanes: int\n        :param planes: output channels\n        :type planes: int\n        :param stride: stride\n        :type stride: int\n        :param dropout: dropout rate\n        :type dropout: float\n        :param normalization: whether to use normalization\n        :type normalization: bool\n        \"\"\"\n\n        assert inplanes > 0\n        assert planes > 0\n        assert planes >= inplanes\n        assert stride >= 1\n        assert dropout >= 0 and dropout < 1\n\n        super(WideResNetBlock, self).__init__()\n\n        self.normalization = normalization\n        \"\"\" (bool) Normalization or not. \"\"\"\n\n        self.dropout = dropout\n        \"\"\" (float) Dropout factor. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        if self.normalization:\n            self.bn1 = torch.nn.BatchNorm2d(inplanes)\n            torch.nn.init.constant_(self.bn1.weight, 1)\n            torch.nn.init.constant_(self.bn1.bias, 0)\n\n        self.relu1 = torch.nn.ReLU(inplace=self.inplace)\n        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(self.conv1.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(self.conv1.bias, 0)\n\n        if self.dropout > 1e-3:\n            self.drop1 = torch.nn.Dropout(p=dropout)\n        if self.normalization:\n            self.bn2 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.bn2.weight, 1)\n            torch.nn.init.constant_(self.bn2.bias, 0)\n\n        self.relu2 = torch.nn.ReLU(inplace=self.inplace)\n        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(self.conv2.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(self.conv2.bias, 0)\n\n        self.shortcut = torch.nn.Sequential()\n        if stride != 1 or inplanes != planes:\n            self.shortcut = torch.nn.Sequential(\n                torch.nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        if self.normalization:\n            out = self.bn1(x)\n            out = self.relu1(out)\n        else:\n            out = self.relu1(x)\n        out = self.conv1(out)\n        if self.dropout > 1e-3:\n            out = self.drop1(out)\n        if self.normalization:\n            out = self.bn2(out)\n        out = self.relu2(out)\n        out = self.conv2(out)\n\n        out += self.shortcut(x)\n\n        return out", "class WideResNetBlock(torch.nn.Module):\n    \"\"\"\n    Wide ResNet block.\n    \"\"\"\n\n    def __init__(self, inplanes, planes, stride=1, dropout=0, normalization=True):\n        \"\"\"\n        Constructor.\n\n        :param inplanes: input channels\n        :type inplanes: int\n        :param planes: output channels\n        :type planes: int\n        :param stride: stride\n        :type stride: int\n        :param dropout: dropout rate\n        :type dropout: float\n        :param normalization: whether to use normalization\n        :type normalization: bool\n        \"\"\"\n\n        assert inplanes > 0\n        assert planes > 0\n        assert planes >= inplanes\n        assert stride >= 1\n        assert dropout >= 0 and dropout < 1\n\n        super(WideResNetBlock, self).__init__()\n\n        self.normalization = normalization\n        \"\"\" (bool) Normalization or not. \"\"\"\n\n        self.dropout = dropout\n        \"\"\" (float) Dropout factor. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        if self.normalization:\n            self.bn1 = torch.nn.BatchNorm2d(inplanes)\n            torch.nn.init.constant_(self.bn1.weight, 1)\n            torch.nn.init.constant_(self.bn1.bias, 0)\n\n        self.relu1 = torch.nn.ReLU(inplace=self.inplace)\n        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(self.conv1.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(self.conv1.bias, 0)\n\n        if self.dropout > 1e-3:\n            self.drop1 = torch.nn.Dropout(p=dropout)\n        if self.normalization:\n            self.bn2 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.bn2.weight, 1)\n            torch.nn.init.constant_(self.bn2.bias, 0)\n\n        self.relu2 = torch.nn.ReLU(inplace=self.inplace)\n        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(self.conv2.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(self.conv2.bias, 0)\n\n        self.shortcut = torch.nn.Sequential()\n        if stride != 1 or inplanes != planes:\n            self.shortcut = torch.nn.Sequential(\n                torch.nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        if self.normalization:\n            out = self.bn1(x)\n            out = self.relu1(out)\n        else:\n            out = self.relu1(x)\n        out = self.conv1(out)\n        if self.dropout > 1e-3:\n            out = self.drop1(out)\n        if self.normalization:\n            out = self.bn2(out)\n        out = self.relu2(out)\n        out = self.conv2(out)\n\n        out += self.shortcut(x)\n\n        return out"]}
{"filename": "models/preact_resnet.py", "chunked_list": ["\"\"\"\nPre-activation ResNet in PyTorch.\nTaken from https://github.com/locuslab/robust_union/.\n\"\"\"\n\nimport utils.torch\nfrom .classifier import Classifier\nfrom .preact_resnet_block import *\nfrom .preact_resnet_bottleneck import *\n", "from .preact_resnet_bottleneck import *\n\n\nclass PreActResNet(Classifier):\n    \"\"\"\n    More or less fixed pre-activation ResNet.\n    \"\"\"\n\n    def __init__(self, N_class, resolution, blocks, bottleneck=False, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes\n        :type N_class: int\n        :param resolution: resolution\n        :type resolution: [int]\n        :param blocks: number of layers per blocks, for exactly 4 blocks\n        :type blocks: [int] of length 4\n        :param bottleneck: whether to use bottleneck blocks\n        :type bottleneck: False\n        \"\"\"\n\n        super(PreActResNet, self).__init__(N_class, resolution, **kwargs)\n\n        assert len(blocks) == 4\n        self.in_planes = 64\n\n        self.blocks = blocks\n        \"\"\" ([int]) Blocks. \"\"\"\n\n        self.bottleneck = bottleneck\n        \"\"\" (bool) Inplace. \"\"\"\n\n        if self.bottleneck:\n            block = PreActBottleneck\n        else:\n            block = PreActBlock\n\n        self.append_layer('conv1', nn.Conv2d(resolution[0], 64, kernel_size=3, stride=1, padding=1, bias=False))\n        self.append_layer('layer1', self._make_layer(block, 64, blocks[0], stride=1))\n        self.append_layer('layer2', self._make_layer(block, 128, blocks[1], stride=2))\n        self.append_layer('layer3', self._make_layer(block, 256, blocks[2], stride=2))\n        self.append_layer('layer4', self._make_layer(block, 512, blocks[3], stride=2))\n\n        downsampled = 2*2*2\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.append_layer('avgpool', pool)\n\n        view = utils.torch.View(-1, self.in_planes)\n        self.append_layer('view', view)\n\n        #out = F.avg_pool2d(out, 4)\n        #out = out.view(out.size(0), -1)\n\n        self.append_layer('linear', nn.Linear(512*block.expansion, N_class))\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)"]}
{"filename": "models/wide_resnet.py", "chunked_list": ["\"\"\"\nWide ResNet.\nTaken from https://github.com/meliketoy/wide-resnet.pytorch.\n\"\"\"\nimport numpy\nimport torch\nimport utils.torch\nfrom .classifier import Classifier\nfrom .wide_resnet_block import WideResNetBlock\nimport torch.nn as nn", "from .wide_resnet_block import WideResNetBlock\nimport torch.nn as nn\n\n\nclass WideResNet(Classifier):\n    \"\"\"\n    Wide Res-Net.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param depth: depth from which to calculate the blocks\n        :type depth: int\n        :param depth: width factor\n        :type depth: int\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param dropout: dropout rate\n        :type dropout: float\n        \"\"\"\n\n        super(WideResNet, self).__init__(N_class, resolution, **kwargs)\n\n        self.depth = depth\n        \"\"\" (int) Depth. \"\"\"\n\n        self.width = width\n        \"\"\" (int) Width. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.dropout = dropout\n        \"\"\" (int) Dropout. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.in_planes = channels\n        \"\"\" (int) Helper for channels. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n        n = int((depth-4)/6)\n        k = width\n\n        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n\n        downsampled = 1\n        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(conv.bias, 0)\n        self.append_layer('conv0', conv)\n\n        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n        self.append_layer('block1', block1)\n        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n        downsampled *= 2\n        self.append_layer('block2', block2)\n        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n        downsampled *= 2\n        self.append_layer('block3', block3)\n\n        if self.normalization:\n            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n            torch.nn.init.constant_(bn.weight, 1)\n            torch.nn.init.constant_(bn.bias, 0)\n            self.append_layer('bn3', bn)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.append_layer('relu3', relu)\n\n        representation = planes[3]\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.append_layer('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.append_layer('view', view)\n\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(planes[3], self._N_output)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.append_layer('logits', logits)\n\n    def _wide_layer(self, block, planes, blocks, stride):\n        strides = [stride] + [1]*(blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n            self.in_planes = planes\n\n        return torch.nn.Sequential(*layers)", "\n\nclass WideResNetTwoBranch(torch.nn.Module):\n    \"\"\"\n    Wide Res-Net.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param depth: depth from which to calculate the blocks\n        :type depth: int\n        :param depth: width factor\n        :type depth: int\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param dropout: dropout rate\n        :type dropout: float\n        \"\"\"\n\n        super(WideResNetTwoBranch, self).__init__(**kwargs)\n\n        self.N_class = N_class\n        self.resolution = resolution\n        self.depth = depth\n        \"\"\" (int) Depth. \"\"\"\n\n        self.width = width\n        \"\"\" (int) Width. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.dropout = dropout\n        \"\"\" (int) Dropout. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.in_planes = channels\n        \"\"\" (int) Helper for channels. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n        self.feature_layers = nn.Sequential()\n\n        n = int((depth-4)/6)\n        k = width\n\n        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n\n        downsampled = 1\n        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(conv.bias, 0)\n        self.feature_layers.add_module('conv0', conv)\n\n        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n        self.feature_layers.add_module('block1', block1)\n        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n        downsampled *= 2\n        self.feature_layers.add_module('block2', block2)\n        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n        downsampled *= 2\n        self.feature_layers.add_module('block3', block3)\n\n        if self.normalization:\n            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n            torch.nn.init.constant_(bn.weight, 1)\n            torch.nn.init.constant_(bn.bias, 0)\n            self.feature_layers.add_module('bn3', bn)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.feature_layers.add_module('relu3', relu)\n\n        representation = planes[3]\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.feature_layers.add_module('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.feature_layers.add_module('view', view)\n\n        self.classifier_layers = nn.Sequential()\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(planes[3], self.N_class)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.classifier_layers.add_module('logits', logits)\n\n        self.dense_layers = nn.Sequential()\n        # # Shallow detector\n        # self.dense_layers.add_module(\"d0\", nn.Linear(representation, 256))\n        # self.dense_layers.add_module(\"d1\", nn.BatchNorm1d(256))\n        # self.dense_layers.add_module(\"d2\", nn.ReLU())\n        # self.dense_layers.add_module(\"d3\", nn.Linear(256, 1))\n\n        # Detector with more layers\n        self.dense_layers.add_module(\"d0\", nn.Linear(representation, 1024))\n        self.dense_layers.add_module(\"bn0\", nn.BatchNorm1d(1024))\n        self.dense_layers.add_module(\"rl0\", nn.ReLU())\n        for j in range(5):\n            self.dense_layers.add_module(f\"d{j + 1:d}\", nn.Linear(1024, 1024))\n            self.dense_layers.add_module(f\"rl{j + 1:d}\", nn.ReLU())\n\n        self.dense_layers.add_module(\"de\", nn.Linear(1024, 1))\n\n    def _wide_layer(self, block, planes, blocks, stride):\n        strides = [stride] + [1]*(blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n            self.in_planes = planes\n\n        return torch.nn.Sequential(*layers)\n\n    def forward(self, x, return_d=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n        d_output = self.dense_layers(feature)\n        d_output = torch.sigmoid(d_output)\n        if return_d:\n            return cls_output, d_output\n        else:\n            return cls_output", "\n\nclass WideResNetTwoBranchDenseV1(torch.nn.Module):\n    \"\"\"\n    Wide Res-Net.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, out_dim=10, use_BN=False, along=False, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param depth: depth from which to calculate the blocks\n        :type depth: int\n        :param depth: width factor\n        :type depth: int\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param dropout: dropout rate\n        :type dropout: float\n        \"\"\"\n\n        super(WideResNetTwoBranchDenseV1, self).__init__(**kwargs)\n\n        self.N_class = N_class\n        self.along = along\n        self.resolution = resolution\n        self.depth = depth\n        \"\"\" (int) Depth. \"\"\"\n\n        self.width = width\n        \"\"\" (int) Width. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.dropout = dropout\n        \"\"\" (int) Dropout. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.in_planes = channels\n        \"\"\" (int) Helper for channels. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n        self.feature_layers = nn.Sequential()\n\n        n = int((depth-4)/6)\n        k = width\n\n        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n\n        downsampled = 1\n        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        torch.nn.init.constant_(conv.bias, 0)\n        self.feature_layers.add_module('conv0', conv)\n\n        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n        self.feature_layers.add_module('block1', block1)\n        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n        downsampled *= 2\n        self.feature_layers.add_module('block2', block2)\n        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n        downsampled *= 2\n        self.feature_layers.add_module('block3', block3)\n\n        if self.normalization:\n            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n            torch.nn.init.constant_(bn.weight, 1)\n            torch.nn.init.constant_(bn.bias, 0)\n            self.feature_layers.add_module('bn3', bn)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.feature_layers.add_module('relu3', relu)\n\n        representation = planes[3]\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.feature_layers.add_module('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.feature_layers.add_module('view', view)\n\n        self.classifier_layers = nn.Sequential()\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(planes[3], self.N_class)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.classifier_layers.add_module('logits', logits)\n\n        if use_BN:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(representation, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n        else:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(representation, 256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n\n    def _wide_layer(self, block, planes, blocks, stride):\n        strides = [stride] + [1]*(blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n            self.in_planes = planes\n\n        return torch.nn.Sequential(*layers)\n\n    def forward(self, x, return_aux=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n\n        if self.along:\n            evidence_return = self.dense_layers(feature)\n        else:\n            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n        \n        if return_aux:\n            return cls_output, evidence_return\n        else:\n            return cls_output", "\n\nclass WideResNetConf(torch.nn.Module):\n    \"\"\"\n    Wide Res-Net.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0,\n                 conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param depth: depth from which to calculate the blocks\n        :type depth: int\n        :param depth: width factor\n        :type depth: int\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param dropout: dropout rate\n        :type dropout: float\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        super(WideResNetConf, self).__init__(**kwargs)\n\n        self.N_class = N_class\n        self.conf_approx = conf_approx\n        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n        self.temperature = temperature\n        self.model = WideResNet(N_class, resolution, depth, width, normalization, channels, dropout)\n        if self.conf_approx == \"network\":\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 1024))\n            self.conf_net.add_module(\"bn0\", nn.BatchNorm1d(1024))\n            self.conf_net.add_module(\"rl0\", nn.ReLU())\n            for k in range(5):\n                self.conf_net.add_module(f\"d{k+1:d}\", nn.Linear(1024, 1024))\n                self.conf_net.add_module(f\"rl{k+1:d}\", nn.ReLU())\n\n            self.conf_net.add_module(\"de\", nn.Linear(1024, 1))\n        else:\n            # Simple scale and shift of the confidence approximation score\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n        \n    def forward(self, x, return_d=False):\n        # classifier prediction logits\n        output = self.model(x)\n        # detector prediction\n        if self.conf_approx == 'logsumexp':\n            # `output` has the prediction logits\n            # `d_output` should correspond to the probability of rejection\n            '''\n            softmax_output = torch.nn.functional.softmax(output, dim=1)\n            K = softmax_output.size(1)\n            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n                                                                             keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n            '''\n            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n\n        elif self.conf_approx == 'network':\n            d_output = torch.sigmoid(self.conf_net(output))\n\n        if return_d:\n            return output, d_output\n        else:\n            return output", "\n\nclass WideResNetEnsemble(torch.nn.Module):\n    \"\"\"\n    Wide Res-Net.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0,\n                 conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param depth: depth from which to calculate the blocks\n        :type depth: int\n        :param depth: width factor\n        :type depth: int\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param dropout: dropout rate\n        :type dropout: float\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        super(WideResNetEnsemble, self).__init__(**kwargs)\n        self.N_class = N_class\n        # Classifier network C_1\n        self.classifier = WideResNet(N_class, resolution, depth, width, normalization, channels, dropout)\n        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n        # but is not used for the final prediction\n        self.classifier_with_reject = WideResNetConf(N_class,\n                                                     resolution,\n                                                     depth=depth,\n                                                     width=width,\n                                                     conf_approx=conf_approx,\n                                                     temperature=temperature)\n        \n    def forward(self, x, return_d=False):\n        cls_output = self.classifier(x)\n        if return_d:\n            _, d_output = self.classifier_with_reject(x, return_d=True)\n            return cls_output, d_output\n        else:\n            return cls_output", ""]}
{"filename": "models/lenet.py", "chunked_list": ["import torch\nimport utils.torch\nfrom .classifier import Classifier\n\n\nclass LeNet(Classifier):\n    \"\"\"\n    General LeNet classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), channels=64, activation=torch.nn.ReLU, normalization=True, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param channels: channels to start with\n        :type channels: int\n        :param units: units per layer\n        :type units: [int]\n        :param activation: activation function\n        :type activation: None or torch.nn.Module\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param bias: whether to use bias\n        :type bias: bool\n        \"\"\"\n\n        super(LeNet, self).__init__(N_class, resolution, **kwargs)\n\n        # the constructor parameters must be available as attributes for state to work\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.activation = activation\n        \"\"\" (callable) activation\"\"\"\n\n        self.normalization = normalization\n        \"\"\" (bool) Normalization. \"\"\"\n\n        layer = 0\n        layers = []\n        resolutions = []\n\n        gain = 1\n        if self.activation is not None:\n            gain = torch.nn.init.calculate_gain(self.activation().__class__.__name__.lower())\n\n        while True:\n            input_channels = self.resolution[0] if layer == 0 else layers[layer - 1]\n            output_channels = self.channels if layer == 0 else layers[layer - 1] * 2\n\n            conv = torch.nn.Conv2d(input_channels, output_channels, kernel_size=5, stride=1, padding=2)\n            #torch.nn.init.normal_(conv.weight, mean=0, std=0.1)\n            torch.nn.init.kaiming_normal_(conv.weight, gain)\n            torch.nn.init.constant_(conv.bias, 0.1)\n            self.append_layer('conv%d' % layer, conv)\n\n            if self.normalization:\n                bn = torch.nn.BatchNorm2d(output_channels)\n                torch.nn.init.constant_(bn.weight, 1)\n                torch.nn.init.constant_(bn.bias, 0)\n                self.append_layer('bn%d' % layer, bn)\n\n            if self.activation:\n                relu = self.activation()\n                self.append_layer('act%d' % layer, relu)\n\n            pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n            self.append_layer('pool%d' % layer, pool)\n\n            layers.append(output_channels)\n            resolutions.append([\n                self.resolution[1] // 2 if layer == 0 else resolutions[layer - 1][0] // 2,\n                self.resolution[2] // 2 if layer == 0 else resolutions[layer - 1][1] // 2,\n            ])\n            if resolutions[-1][0] // 2 < 3 or resolutions[-1][0] % 2 == 1 or resolutions[-1][1] // 2 < 3 or resolutions[-1][1] % 2 == 1:\n                break\n\n            layer += 1\n\n        representation = int(resolutions[-1][0] * resolutions[-1][1] * layers[-1])\n        view = utils.torch.View(-1, representation)\n        self.append_layer('view', view)\n\n        fc = torch.nn.Linear(representation, 1024)\n        self.append_layer('fc%d' % layer, fc)\n\n        if self.activation:\n            relu = self.activation()\n            self.append_layer('act%d' % layer, relu)\n\n        logits = torch.nn.Linear(1024, self._N_output)\n        #torch.nn.init.normal_(conv.weight, mean=0, std=0.1)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0.1)\n        self.append_layer('logits', logits)", "\n"]}
{"filename": "models/__init__.py", "chunked_list": ["\"\"\"\nVarious models. All models extend Classifier allowing to be easily saved an loaded using common.state.\n\"\"\"\n\nfrom .classifier import Classifier\nfrom .lenet import LeNet\nfrom .mlp import MLP\nfrom .resnet import ResNet, ResNetTwoBranch, ResNetConf, ResNetTwoBranchDenseV1, ResNetEnsemble\nfrom .wide_resnet import WideResNet, WideResNetTwoBranch, WideResNetConf, WideResNetTwoBranchDenseV1, WideResNetEnsemble\nfrom .fixed_lenet import FixedLeNet, FixedLeNetTwoBranch, FixedLeNetConf, FixedLeNetTwoBranchDenseV1, FixedLeNetEnsemble", "from .wide_resnet import WideResNet, WideResNetTwoBranch, WideResNetConf, WideResNetTwoBranchDenseV1, WideResNetEnsemble\nfrom .fixed_lenet import FixedLeNet, FixedLeNetTwoBranch, FixedLeNetConf, FixedLeNetTwoBranchDenseV1, FixedLeNetEnsemble\nfrom .preact_resnet import PreActResNet\n"]}
{"filename": "models/resnet_block.py", "chunked_list": ["\"\"\"\nResNet block.\nTake from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py.\n\"\"\"\nimport torch\nimport utils.torch\n\n\nclass ResNetBlock(torch.nn.Module):\n    \"\"\"\n    ResNet block.\n    \"\"\"\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, normalization=True):\n        \"\"\"\n        Constructor.\n\n        :param inplanes: input channels\n        :type inplanes: int\n        :param planes: output channels\n        :type planes: int\n        :param stride: stride\n        :type stride: int\n        :param downsample: whether to downsample\n        :type downsample: bool\n        :param normalization: whether to use normalization\n        :type normalization: bool\n        \"\"\"\n\n        super(ResNetBlock, self).__init__()\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\n        self.normalization = normalization\n        if self.normalization:\n            self.norm1 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.norm1.weight, 1)\n            torch.nn.init.constant_(self.norm1.bias, 0)\n\n        self.relu = torch.nn.ReLU(inplace=self.inplace)\n        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n\n        if self.normalization:\n            self.norm2 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.norm2.weight, 1)\n            torch.nn.init.constant_(self.norm2.bias, 0)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n\n        :param x: input\n        :type x: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        out = self.conv1(x)\n        if self.normalization:\n            out = self.norm1(out)\n\n        out = self.relu(out)\n        out = self.conv2(out)\n        if self.normalization:\n            out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        else:\n            identity = x\n\n        out += identity\n        out = self.relu(out)\n\n        return out", "class ResNetBlock(torch.nn.Module):\n    \"\"\"\n    ResNet block.\n    \"\"\"\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, normalization=True):\n        \"\"\"\n        Constructor.\n\n        :param inplanes: input channels\n        :type inplanes: int\n        :param planes: output channels\n        :type planes: int\n        :param stride: stride\n        :type stride: int\n        :param downsample: whether to downsample\n        :type downsample: bool\n        :param normalization: whether to use normalization\n        :type normalization: bool\n        \"\"\"\n\n        super(ResNetBlock, self).__init__()\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\n        self.normalization = normalization\n        if self.normalization:\n            self.norm1 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.norm1.weight, 1)\n            torch.nn.init.constant_(self.norm1.bias, 0)\n\n        self.relu = torch.nn.ReLU(inplace=self.inplace)\n        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n\n        if self.normalization:\n            self.norm2 = torch.nn.BatchNorm2d(planes)\n            torch.nn.init.constant_(self.norm2.weight, 1)\n            torch.nn.init.constant_(self.norm2.bias, 0)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass.\n\n        :param x: input\n        :type x: torch.autograd.Variable\n        :return: output\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        out = self.conv1(x)\n        if self.normalization:\n            out = self.norm1(out)\n\n        out = self.relu(out)\n        out = self.conv2(out)\n        if self.normalization:\n            out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        else:\n            identity = x\n\n        out += identity\n        out = self.relu(out)\n\n        return out"]}
{"filename": "models/fixed_lenet.py", "chunked_list": ["import torch\nimport utils.torch\nfrom .classifier import Classifier\nimport torch.nn as nn\n\n\nclass FixedLeNet(Classifier):\n    \"\"\"\n    Fixed LeNet architecture, working on MNIST architectures only.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 28, 28), **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        \"\"\"\n\n        assert resolution[0] == 1\n        assert resolution[1] == 28\n        assert resolution[2] == 28\n\n        super(FixedLeNet, self).__init__(N_class, resolution, **kwargs)\n\n        self.append_layer('0', nn.Conv2d(resolution[0], 32, 5, padding=2))\n        self.append_layer('1', nn.ReLU())\n        self.append_layer('2', nn.MaxPool2d(2, 2))\n        self.append_layer('3', nn.Conv2d(32, 64, 5, padding=2))\n        self.append_layer('4', nn.ReLU())\n        self.append_layer('5', nn.MaxPool2d(2, 2))\n        self.append_layer('6', utils.torch.Flatten())\n        self.append_layer('7', nn.Linear(7 * 7 * 64, 1024))\n        self.append_layer('8', nn.ReLU())\n        self.append_layer('9', nn.Linear(1024, self.N_class))", "\n\nclass FixedLeNetTwoBranch(torch.nn.Module):\n    \"\"\"\n    Fixed LeNet architecture, working on MNIST architectures only.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 28, 28), **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        \"\"\"\n\n        assert resolution[0] == 1\n        assert resolution[1] == 28\n        assert resolution[2] == 28\n\n        super(FixedLeNetTwoBranch, self).__init__(**kwargs)\n        self.N_class = N_class\n\n        self.feature_layers = nn.Sequential()\n        self.feature_layers.add_module(\"f0\", nn.Conv2d(resolution[0], 32, 5, padding=2))\n        self.feature_layers.add_module(\"f1\", nn.ReLU())\n        self.feature_layers.add_module(\"f2\", nn.MaxPool2d(2, 2))\n        self.feature_layers.add_module(\"f3\", nn.Conv2d(32, 64, 5, padding=2))\n        self.feature_layers.add_module(\"f4\", nn.ReLU())\n        self.feature_layers.add_module(\"f5\", nn.MaxPool2d(2, 2))\n        self.feature_layers.add_module(\"f6\", utils.torch.Flatten())\n        self.feature_layers.add_module(\"f7\", nn.Linear(7 * 7 * 64, 1024))\n        self.feature_layers.add_module(\"f8\", nn.ReLU())\n\n        self.classifier_layers = nn.Sequential()\n        self.classifier_layers.add_module(\"c0\", nn.Linear(1024, self.N_class))\n        \n        # Shallow detector\n        # self.dense_layers = nn.Sequential()\n        # self.dense_layers.add_module(\"d0\", nn.Linear(1024, 256))\n        # self.dense_layers.add_module(\"d1\", nn.ReLU())\n        # self.dense_layers.add_module(\"d2\", nn.Linear(256, 1))\n        \n        # Deeper detector\n        self.dense_layers = nn.Sequential()\n        self.dense_layers.add_module(\"d0\", nn.Linear(1024, 256))\n        self.dense_layers.add_module(\"d1\", nn.ReLU())\n        self.dense_layers.add_module(\"d2\", nn.Linear(256, 256))\n        self.dense_layers.add_module(\"d3\", nn.ReLU())\n        self.dense_layers.add_module(\"d4\", nn.Linear(256, 1))\n    \n    def forward(self, x, return_d=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n        d_output = self.dense_layers(feature)\n        d_output = torch.sigmoid(d_output)\n        \n        if return_d:\n            return cls_output, d_output\n        else:\n            return cls_output", "\n\nclass FixedLeNetTwoBranchDenseV1(torch.nn.Module):\n    \"\"\"\n    Fixed LeNet architecture, working on MNIST architectures only.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 28, 28), out_dim=10, use_BN=False, along=False, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        \"\"\"\n\n        assert resolution[0] == 1\n        assert resolution[1] == 28\n        assert resolution[2] == 28\n\n        super(FixedLeNetTwoBranchDenseV1, self).__init__(**kwargs)\n        self.N_class = N_class\n        self.along = along\n\n        self.feature_layers = nn.Sequential()\n        self.feature_layers.add_module(\"f0\", nn.Conv2d(resolution[0], 32, 5, padding=2))\n        self.feature_layers.add_module(\"f1\", nn.ReLU())\n        self.feature_layers.add_module(\"f2\", nn.MaxPool2d(2, 2))\n        self.feature_layers.add_module(\"f3\", nn.Conv2d(32, 64, 5, padding=2))\n        self.feature_layers.add_module(\"f4\", nn.ReLU())\n        self.feature_layers.add_module(\"f5\", nn.MaxPool2d(2, 2))\n        self.feature_layers.add_module(\"f6\", utils.torch.Flatten())\n        self.feature_layers.add_module(\"f7\", nn.Linear(7 * 7 * 64, 1024))\n        self.feature_layers.add_module(\"f8\", nn.ReLU())\n\n        self.classifier_layers = nn.Sequential()\n        self.classifier_layers.add_module(\"c0\", nn.Linear(1024, self.N_class))\n\n        if use_BN:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(1024, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n        else:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(1024, 256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n    \n    def forward(self, x, return_aux=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n\n        if self.along:\n            evidence_return = self.dense_layers(feature)\n        else:\n            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n        \n        if return_aux:\n            return cls_output, evidence_return\n        else:\n            return cls_output", "\n\nclass FixedLeNetConf(torch.nn.Module):\n    \"\"\"\n    Fixed LeNet architecture, working on MNIST architectures only.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 28, 28), conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        assert resolution[0] == 1\n        assert resolution[1] == 28\n        assert resolution[2] == 28\n\n        super(FixedLeNetConf, self).__init__(**kwargs)\n        self.N_class = N_class\n        self.conf_approx = conf_approx\n        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n        self.temperature = temperature\n        self.model = FixedLeNet(N_class)\n        if self.conf_approx == \"network\":\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 256))\n            self.conf_net.add_module(\"d1\", nn.ReLU())\n            self.conf_net.add_module(\"d2\", nn.Linear(256, 256))\n            self.conf_net.add_module(\"d3\", nn.ReLU())\n            self.conf_net.add_module(\"d4\", nn.Linear(256, 1))\n        else:\n            # Simple scale and shift of the confidence approximation score\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n    \n    def forward(self, x, return_d=False):\n        # classifier prediction logits\n        output = self.model(x)\n        # detector prediction\n        if self.conf_approx == 'logsumexp':\n            # `output` has the prediction logits\n            # `d_output` should correspond to the probability of rejection\n            '''\n            softmax_output = torch.nn.functional.softmax(output, dim=1)\n            K = softmax_output.size(1)\n            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n                                                                            keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n            '''\n            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n\n        elif self.conf_approx == 'network':\n            d_output = torch.sigmoid(self.conf_net(output))\n\n        if return_d:\n            return output, d_output\n        else:\n            return output", "\n\nclass FixedLeNetEnsemble(torch.nn.Module):\n    \"\"\"\n    Fixed LeNet architecture, working on MNIST architectures only.\n    \"\"\"\n\n    def __init__(self, N_class, resolution, conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        assert resolution[0] == 1\n        assert resolution[1] == 28\n        assert resolution[2] == 28\n\n        super(FixedLeNetEnsemble, self).__init__(**kwargs)\n        self.N_class = N_class\n        # Classifier network C_1\n        self.classifier = FixedLeNet(N_class, resolution)\n        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n        # but is not used for the final prediction\n        self.classifier_with_reject = FixedLeNetConf(N_class,\n                                                     resolution,\n                                                     conf_approx=conf_approx,\n                                                     temperature=temperature)\n    \n    def forward(self, x, return_d=False):\n        cls_output = self.classifier(x)\n        if return_d:\n            _, d_output = self.classifier_with_reject(x, return_d=True)\n            return cls_output, d_output\n        else:\n            return cls_output", ""]}
{"filename": "models/classifier.py", "chunked_list": ["import torch\nfrom .resnet_block import ResNetBlock\nfrom .wide_resnet_block import WideResNetBlock\nimport utils.torch\n\n\nclass Classifier(torch.nn.Module):\n    \"\"\"\n    Base classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        The keyword arguments, resolution, number of classes and other architecture parameters\n        from subclasses are saved as attributes. This allows to easily save and load the model\n        using common.state without knowing the exact architecture in advance.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution\n        :type resolution: [int]\n        \"\"\"\n\n        super(Classifier, self).__init__()\n\n        assert N_class > 0, 'positive N_class expected'\n        assert len(resolution) <= 3\n\n        self.N_class = int(N_class)  # Having strange bug where torch complaints about numpy.in64 being passed to nn.Linear.\n        \"\"\" (int) Number of classes. \"\"\"\n\n        self.resolution = list(resolution)\n        \"\"\" ([int]) Resolution as (channels, height, width) \"\"\"\n\n        self.kwargs = kwargs\n        \"\"\" (dict) Kwargs. \"\"\"\n\n        self.include_clamp = self.kwargs_get('clamp', True)\n        \"\"\" (bool) Whether to apply input clamping. \"\"\"\n\n        self.include_whiten = self.kwargs_get('whiten', False)\n        \"\"\" (bool) Whether to apply input whitening/normalization. \"\"\"\n\n        self.include_scale = self.kwargs_get('scale', False)\n        \"\"\" (bool) Whether to apply input scaling. \"\"\"\n\n        # __ attributes are private, which is important for the State to work properly.\n        self.__layers = []\n        \"\"\" ([str]) Will hold layer names. \"\"\"\n\n        self._N_output = self.N_class if self.N_class > 2 else 1\n        \"\"\" (int) Number of outputs. \"\"\"\n\n        if self.include_clamp:\n            self.append_layer('clamp', utils.torch.Clamp())\n\n        assert not (self.include_whiten and self.include_scale)\n\n        if self.include_whiten:\n            # Note that the weight and bias needs to set manually corresponding to mean and std!\n            whiten = utils.torch.Normalize(resolution[0])\n            self.append_layer('whiten', whiten)\n\n        if self.include_scale:\n            # Note that the weight and bias needs to set manually!\n            scale = utils.torch.Scale(1)\n            scale.weight.data[0] = -1\n            scale.bias.data[0] = 1\n            self.append_layer('scale', scale)\n\n    def kwargs_get(self, key, default):\n        \"\"\"\n        Get argument if not None.\n\n        :param key: key\n        :type key: str\n        :param default: default value\n        :type default: mixed\n        :return: value\n        :rtype: mixed\n        \"\"\"\n\n        value = self.kwargs.get(key, default)\n        if value is None:\n            value = default\n        return value\n\n    def append_layer(self, name, layer):\n        \"\"\"\n        Add a layer.\n\n        :param name: layer name\n        :type name: str\n        :param layer: layer\n        :type layer: torch.nn.Module\n        \"\"\"\n\n        setattr(self, name, layer)\n        self.__layers.append(name)\n\n    def prepend_layer(self, name, layer):\n        \"\"\"\n        Add a layer.\n\n        :param name: layer name\n        :type name: str\n        :param layer: layer\n        :type layer: torch.nn.Module\n        \"\"\"\n\n        self.insert_layer(0, name, layer)\n\n    def insert_layer(self, index, name, layer):\n        \"\"\"\n        Add a layer.\n\n        :param index: index\n        :type index: int\n        :param name: layer name\n        :type name: str\n        :param layer: layer\n        :type layer: torch.nn.Module\n        \"\"\"\n\n        setattr(self, name, layer)\n        self.__layers.insert(index, name)\n\n    def forward(self, image, return_features=False):\n        \"\"\"\n        Forward pass, takes an image and outputs the predictions.\n\n        :param image: input image\n        :type image: torch.autograd.Variable\n        :param return_features: whether to also return representation layer\n        :type return_features: bool\n        :return: logits\n        :rtype: torch.autograd.Variable\n        \"\"\"\n\n        features = []\n        output = image\n\n        # separate loops for memory constraints\n        if return_features:\n            for name in self.__layers:\n                output = getattr(self, name)(output)\n                features.append(output)\n            return output, features\n        else:\n            for name in self.__layers:\n                output = getattr(self, name)(output)\n            return output\n\n    def layers(self):\n        \"\"\"\n        Get layer names.\n\n        :return: layer names\n        :rtype: [str]\n        \"\"\"\n\n        return self.__layers\n\n    def __str__(self):\n        \"\"\"\n        Print network.\n        \"\"\"\n\n        string = ''\n        for name in self.__layers:\n            string += '(' + name + ', ' + getattr(self, name).__class__.__name__ + ')\\n'\n            if isinstance(getattr(self, name), torch.nn.Sequential) or isinstance(getattr(self, name), ResNetBlock) or isinstance(getattr(self, name), WideResNetBlock):\n                for module in getattr(self, name).modules():\n                    string += '\\t(' + module.__class__.__name__ + ')\\n'\n        return string", "\n"]}
{"filename": "models/resnet.py", "chunked_list": ["\"\"\"\nResNet.\nTake from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py.\n\"\"\"\nimport torch\nimport utils.torch\nfrom .classifier import Classifier\nfrom .resnet_block import ResNetBlock\nimport torch.nn as nn\n", "import torch.nn as nn\n\n\nclass ResNet(Classifier):\n    \"\"\"\n    Simple classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param blocks: layers per block\n        :type blocks: [int]\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        \"\"\"\n\n        super(ResNet, self).__init__(N_class, resolution, **kwargs)\n\n        self.blocks = blocks\n        \"\"\" ([int]) Blocks. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n        self.append_layer('conv1', conv1)\n\n        if self.normalization:\n            norm1 = torch.nn.BatchNorm2d(self.channels)\n            torch.nn.init.constant_(norm1.weight, 1)\n            torch.nn.init.constant_(norm1.bias, 0)\n            self.append_layer('norm1', norm1)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.append_layer('relu1', relu)\n\n        downsampled = 1\n        for i in range(len(self.blocks)):\n            in_planes = (2 ** max(0, i - 1)) * self.channels\n            out_planes = (2 ** i) * self.channels\n            layers = self.blocks[i]\n            stride = 2 if i > 0 else 1\n\n            downsample = None\n            if stride != 1 or in_planes != out_planes:\n                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\n                if self.normalization:\n                    bn = torch.nn.BatchNorm2d(out_planes)\n                    torch.nn.init.constant_(bn.weight, 1)\n                    torch.nn.init.constant_(bn.bias, 0)\n                    downsample = torch.nn.Sequential(*[conv, bn])\n                else:\n                    downsample = torch.nn.Sequential(*[conv])\n\n            sequence = []\n            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n            for _ in range(1, layers):\n                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n\n            self.append_layer('block%d' % i, torch.nn.Sequential(*sequence))\n            downsampled *= stride\n\n        representation = out_planes\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.append_layer('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.append_layer('view', view)\n\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(representation, self._N_output)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.append_layer('logits', logits)", "\n\nclass ResNetTwoBranch(torch.nn.Module):\n    \"\"\"\n    Simple classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param blocks: layers per block\n        :type blocks: [int]\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        \"\"\"\n\n        super(ResNetTwoBranch, self).__init__(**kwargs)\n        \n        self.N_class = N_class\n        self.resolution = resolution\n        self.blocks = blocks\n        \"\"\" ([int]) Blocks. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        self.feature_layers = nn.Sequential()\n        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n        self.feature_layers.add_module('conv1', conv1)\n\n        if self.normalization:\n            norm1 = torch.nn.BatchNorm2d(self.channels)\n            torch.nn.init.constant_(norm1.weight, 1)\n            torch.nn.init.constant_(norm1.bias, 0)\n            self.feature_layers.add_module('norm1', norm1)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.feature_layers.add_module('relu1', relu)\n\n        downsampled = 1\n        for i in range(len(self.blocks)):\n            in_planes = (2 ** max(0, i - 1)) * self.channels\n            out_planes = (2 ** i) * self.channels\n            layers = self.blocks[i]\n            stride = 2 if i > 0 else 1\n\n            downsample = None\n            if stride != 1 or in_planes != out_planes:\n                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\n                if self.normalization:\n                    bn = torch.nn.BatchNorm2d(out_planes)\n                    torch.nn.init.constant_(bn.weight, 1)\n                    torch.nn.init.constant_(bn.bias, 0)\n                    downsample = torch.nn.Sequential(*[conv, bn])\n                else:\n                    downsample = torch.nn.Sequential(*[conv])\n\n            sequence = []\n            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n            for _ in range(1, layers):\n                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n\n            self.feature_layers.add_module('block%d' % i, torch.nn.Sequential(*sequence))\n            downsampled *= stride\n\n        representation = out_planes\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.feature_layers.add_module('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.feature_layers.add_module('view', view)\n\n        self.classifier_layers = nn.Sequential()\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(representation, self.N_class)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.classifier_layers.add_module('logits', logits)\n\n        self.dense_layers = nn.Sequential()\n        # # Shallow detector\n        # self.dense_layers.add_module(\"d0\", nn.Linear(representation, 256))\n        # self.dense_layers.add_module(\"d1\", nn.BatchNorm1d(256))\n        # self.dense_layers.add_module(\"d2\", nn.ReLU())\n        # self.dense_layers.add_module(\"d3\", nn.Linear(256, 1))\n\n        # Detector with more layers\n        self.dense_layers.add_module(\"d0\", nn.Linear(representation, 1024))\n        self.dense_layers.add_module(\"bn0\", nn.BatchNorm1d(1024))\n        self.dense_layers.add_module(\"rl0\", nn.ReLU())\n        for j in range(5):\n            self.dense_layers.add_module(f\"d{j + 1:d}\", nn.Linear(1024, 1024))\n            self.dense_layers.add_module(f\"rl{j + 1:d}\", nn.ReLU())\n\n        self.dense_layers.add_module(\"de\", nn.Linear(1024, 1))\n\n    def forward(self, x, return_d=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n        d_output = self.dense_layers(feature)\n        d_output = torch.sigmoid(d_output)\n        if return_d:\n            return cls_output, d_output\n        else:\n            return cls_output", "\n\nclass ResNetTwoBranchDenseV1(torch.nn.Module):\n\n    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, out_dim=10, use_BN=False, along=False, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param blocks: layers per block\n        :type blocks: [int]\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        \"\"\"\n\n        super(ResNetTwoBranchDenseV1, self).__init__(**kwargs)\n        \n        self.N_class = N_class\n        self.along = along\n        self.resolution = resolution\n        self.blocks = blocks\n        \"\"\" ([int]) Blocks. \"\"\"\n\n        self.channels = channels\n        \"\"\" (int) Channels. \"\"\"\n\n        self.normalization = normalization\n        \"\"\" (callable) Normalization. \"\"\"\n\n        self.inplace = False\n        \"\"\" (bool) Inplace. \"\"\"\n\n        self.feature_layers = nn.Sequential()\n        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n        self.feature_layers.add_module('conv1', conv1)\n\n        if self.normalization:\n            norm1 = torch.nn.BatchNorm2d(self.channels)\n            torch.nn.init.constant_(norm1.weight, 1)\n            torch.nn.init.constant_(norm1.bias, 0)\n            self.feature_layers.add_module('norm1', norm1)\n\n        relu = torch.nn.ReLU(inplace=self.inplace)\n        self.feature_layers.add_module('relu1', relu)\n\n        downsampled = 1\n        for i in range(len(self.blocks)):\n            in_planes = (2 ** max(0, i - 1)) * self.channels\n            out_planes = (2 ** i) * self.channels\n            layers = self.blocks[i]\n            stride = 2 if i > 0 else 1\n\n            downsample = None\n            if stride != 1 or in_planes != out_planes:\n                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\n                if self.normalization:\n                    bn = torch.nn.BatchNorm2d(out_planes)\n                    torch.nn.init.constant_(bn.weight, 1)\n                    torch.nn.init.constant_(bn.bias, 0)\n                    downsample = torch.nn.Sequential(*[conv, bn])\n                else:\n                    downsample = torch.nn.Sequential(*[conv])\n\n            sequence = []\n            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n            for _ in range(1, layers):\n                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n\n            self.feature_layers.add_module('block%d' % i, torch.nn.Sequential(*sequence))\n            downsampled *= stride\n\n        representation = out_planes\n        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n        self.feature_layers.add_module('avgpool', pool)\n\n        view = utils.torch.View(-1, representation)\n        self.feature_layers.add_module('view', view)\n\n        self.classifier_layers = nn.Sequential()\n        gain = torch.nn.init.calculate_gain('relu')\n        logits = torch.nn.Linear(representation, self.N_class)\n        torch.nn.init.kaiming_normal_(logits.weight, gain)\n        torch.nn.init.constant_(logits.bias, 0)\n        self.classifier_layers.add_module('logits', logits)\n\n        if use_BN:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(representation, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n        else:\n            self.dense_layers = nn.Sequential(\n                nn.Linear(representation, 256),\n                nn.ReLU(),\n                nn.Linear(256, out_dim)\n                )\n\n    def forward(self, x, return_aux=False):\n        feature = self.feature_layers(x)\n        cls_output = self.classifier_layers(feature)\n\n        if self.along:\n            evidence_return = self.dense_layers(feature)\n        else:\n            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n        \n        if return_aux:\n            return cls_output, evidence_return\n        else:\n            return cls_output", "\n\nclass ResNetConf(torch.nn.Module):\n    \"\"\"\n    Simple classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64,\n                 conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param blocks: layers per block\n        :type blocks: [int]\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        super(ResNetConf, self).__init__(**kwargs)\n        \n        self.N_class = N_class\n        self.conf_approx = conf_approx\n        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n        self.temperature = temperature\n        self.model = ResNet(N_class, resolution, blocks, normalization, channels)\n        if self.conf_approx == \"network\":\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 1024))\n            self.conf_net.add_module(\"bn0\", nn.BatchNorm1d(1024))\n            self.conf_net.add_module(\"rl0\", nn.ReLU())\n            for k in range(5):\n                self.conf_net.add_module(f\"d{k+1:d}\", nn.Linear(1024, 1024))\n                self.conf_net.add_module(f\"rl{k+1:d}\", nn.ReLU())\n\n            self.conf_net.add_module(\"de\", nn.Linear(1024, 1))\n        else:\n            # Simple scale and shift of the confidence approximation score\n            self.conf_net = nn.Sequential()\n            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n\n    def forward(self, x, return_d=False):\n        # classifier prediction logits\n        output = self.model(x)\n        # detector prediction\n        if self.conf_approx == 'logsumexp':\n            # `output` has the prediction logits\n            # `d_output` should correspond to the probability of rejection\n            '''\n            softmax_output = torch.nn.functional.softmax(output, dim=1)\n            K = softmax_output.size(1)\n            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n                                                                             keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n            '''\n            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n\n        elif self.conf_approx == 'network':\n            d_output = torch.sigmoid(self.conf_net(output))\n\n        if return_d:\n            return output, d_output\n        else:\n            return output", "\n\nclass ResNetEnsemble(torch.nn.Module):\n    \"\"\"\n    Simple classifier.\n    \"\"\"\n\n    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64,\n                 conf_approx='network', temperature=0.01, **kwargs):\n        \"\"\"\n        Initialize classifier.\n\n        :param N_class: number of classes to classify\n        :type N_class: int\n        :param resolution: resolution (assumed to be square)\n        :type resolution: int\n        :param blocks: layers per block\n        :type blocks: [int]\n        :param normalization: normalization to use\n        :type normalization: None or torch.nn.Module\n        :param channels: channels to start with\n        :type channels: int\n        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n                           ['logsumexp', 'network'].\n        :type conf_approx: string\n        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n        :type temperature: float\n        \"\"\"\n        super(ResNetEnsemble, self).__init__(**kwargs)\n        self.N_class = N_class\n        # Classifier network C_1\n        self.classifier = ResNet(N_class, resolution, blocks, normalization, channels)\n        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n        # but is not used for the final prediction\n        self.classifier_with_reject = ResNetConf(N_class,\n                                                 resolution,\n                                                 blocks=blocks,\n                                                 conf_approx=conf_approx,\n                                                 temperature=temperature)\n\n    def forward(self, x, return_d=False):\n        cls_output = self.classifier(x)\n        if return_d:\n            _, d_output = self.classifier_with_reject(x, return_d=True)\n            return cls_output, d_output\n        else:\n            return cls_output", ""]}
