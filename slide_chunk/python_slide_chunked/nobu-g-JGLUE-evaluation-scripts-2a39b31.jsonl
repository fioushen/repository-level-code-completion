{"filename": "scripts/gen_table.py", "chunked_list": ["from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\n\nimport wandb\nfrom tabulate import tabulate\nfrom wandb.apis.public import Run, Sweep\n\nTASKS = {\n    \"marc_ja/accuracy\": \"MARC-ja/acc\",", "TASKS = {\n    \"marc_ja/accuracy\": \"MARC-ja/acc\",\n    \"jsts/pearson\": \"JSTS/pearson\",\n    \"jsts/spearman\": \"JSTS/spearman\",\n    \"jnli/accuracy\": \"JNLI/acc\",\n    \"jsquad/exact_match\": \"JSQuAD/EM\",\n    \"jsquad/f1\": \"JSQuAD/F1\",\n    \"jcqa/accuracy\": \"JComQA/acc\",\n}\nMODELS = {", "}\nMODELS = {\n    \"roberta_base\": \"nlp-waseda/roberta-base-japanese\",\n    \"roberta_large\": \"nlp-waseda/roberta-large-japanese-seq512\",\n    \"deberta_base\": \"ku-nlp/deberta-v2-base-japanese\",\n    \"deberta_large\": \"ku-nlp/deberta-v2-large-japanese\",\n}\n\n\n@dataclass(frozen=True)\nclass RunSummary:\n    metric: float\n    lr: float\n    max_epochs: int\n    batch_size: int", "\n@dataclass(frozen=True)\nclass RunSummary:\n    metric: float\n    lr: float\n    max_epochs: int\n    batch_size: int\n\n\ndef main():\n    api = wandb.Api()\n    name_to_sweep_path: dict[str, str] = {\n        line.split()[0]: line.split()[1] for line in Path(\"sweep_status.txt\").read_text().splitlines()\n    }\n    table: list[list[Optional[RunSummary]]] = []\n    for model in MODELS.keys():\n        items: list[Optional[RunSummary]] = []\n        for task in TASKS.keys():\n            task, metric_name = task.split(\"/\")\n            sweep: Sweep = api.sweep(name_to_sweep_path[f\"{task}-{model}\"])\n            if sweep.state == \"FINISHED\":\n                run: Optional[Run] = sweep.best_run()\n                assert run is not None\n                metric_name = \"valid/\" + metric_name\n                items.append(\n                    RunSummary(\n                        metric=run.summary[metric_name],\n                        lr=run.config[\"lr\"],\n                        max_epochs=run.config[\"max_epochs\"],\n                        batch_size=run.config[\"effective_batch_size\"],\n                    )\n                )\n            else:\n                items.append(None)\n        table.append(items)\n    print(\"Scores of best runs:\")\n    print(\n        tabulate(\n            [\n                [model] + [item.metric if item else \"-\" for item in items]\n                for model, items in zip(MODELS.values(), table)\n            ],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            floatfmt=\".3f\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )\n    print()\n    print(\"Learning rates of best runs:\")\n    print(\n        tabulate(\n            [[model] + [item.lr if item else \"-\" for item in items] for model, items in zip(MODELS.values(), table)],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )\n    print()\n    print(\"Training epochs of best runs:\")\n    print(\n        tabulate(\n            [\n                [model] + [item.max_epochs if item else \"-\" for item in items]\n                for model, items in zip(MODELS.values(), table)\n            ],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )", "\ndef main():\n    api = wandb.Api()\n    name_to_sweep_path: dict[str, str] = {\n        line.split()[0]: line.split()[1] for line in Path(\"sweep_status.txt\").read_text().splitlines()\n    }\n    table: list[list[Optional[RunSummary]]] = []\n    for model in MODELS.keys():\n        items: list[Optional[RunSummary]] = []\n        for task in TASKS.keys():\n            task, metric_name = task.split(\"/\")\n            sweep: Sweep = api.sweep(name_to_sweep_path[f\"{task}-{model}\"])\n            if sweep.state == \"FINISHED\":\n                run: Optional[Run] = sweep.best_run()\n                assert run is not None\n                metric_name = \"valid/\" + metric_name\n                items.append(\n                    RunSummary(\n                        metric=run.summary[metric_name],\n                        lr=run.config[\"lr\"],\n                        max_epochs=run.config[\"max_epochs\"],\n                        batch_size=run.config[\"effective_batch_size\"],\n                    )\n                )\n            else:\n                items.append(None)\n        table.append(items)\n    print(\"Scores of best runs:\")\n    print(\n        tabulate(\n            [\n                [model] + [item.metric if item else \"-\" for item in items]\n                for model, items in zip(MODELS.values(), table)\n            ],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            floatfmt=\".3f\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )\n    print()\n    print(\"Learning rates of best runs:\")\n    print(\n        tabulate(\n            [[model] + [item.lr if item else \"-\" for item in items] for model, items in zip(MODELS.values(), table)],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )\n    print()\n    print(\"Training epochs of best runs:\")\n    print(\n        tabulate(\n            [\n                [model] + [item.max_epochs if item else \"-\" for item in items]\n                for model, items in zip(MODELS.values(), table)\n            ],\n            headers=[\"Model\"] + list(TASKS.values()),\n            tablefmt=\"github\",\n            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n        )\n    )", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\nimport sys\n\nimport pytest\nfrom transformers import AutoTokenizer, PreTrainedTokenizerBase\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../src\"))\n\n\n@pytest.fixture()\ndef tokenizer() -> PreTrainedTokenizerBase:\n    return AutoTokenizer.from_pretrained(\"ku-nlp/deberta-v2-tiny-japanese\")", "\n@pytest.fixture()\ndef tokenizer() -> PreTrainedTokenizerBase:\n    return AutoTokenizer.from_pretrained(\"ku-nlp/deberta-v2-tiny-japanese\")\n"]}
{"filename": "tests/metrics/test_squad.py", "chunked_list": ["import pytest\nfrom torchmetrics import SQuAD\n\nCASES = [\n    {\n        \"preds\": [\n            {\"prediction_text\": \"1976\", \"id\": \"000\"},\n        ],\n        \"target\": [{\"answers\": {\"answer_start\": [97], \"text\": [\"1976\"]}, \"id\": \"000\"}],\n        \"exact_match\": 1.0,", "        \"target\": [{\"answers\": {\"answer_start\": [97], \"text\": [\"1976\"]}, \"id\": \"000\"}],\n        \"exact_match\": 1.0,\n        \"f1\": 1.0,  # precision: 1 / 1, recall: 1 / 1\n    },\n    {\n        \"preds\": [\n            {\"prediction_text\": \"2 \u6642\u9593 21 \u5206\", \"id\": \"001\"},\n        ],\n        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 \u6642\u9593\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,", "        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 \u6642\u9593\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,\n        \"f1\": 2 / 3,  # precision: 2 / 4, recall: 2 / 2\n    },\n    {\n        \"preds\": [\n            {\"prediction_text\": \"2 \u6642\u9593 21 \u5206\", \"id\": \"001\"},\n        ],\n        \"target\": [{\"answers\": {\"answer_start\": [10, 10], \"text\": [\"2 \u6642\u9593\", \"2 \u6642\u9593 21 \u5206\"]}, \"id\": \"001\"}],\n        \"exact_match\": 1.0,", "        \"target\": [{\"answers\": {\"answer_start\": [10, 10], \"text\": [\"2 \u6642\u9593\", \"2 \u6642\u9593 21 \u5206\"]}, \"id\": \"001\"}],\n        \"exact_match\": 1.0,\n        \"f1\": 1.0,  # precision: 4 / 4, recall: 4 / 4\n    },\n    {\n        \"preds\": [\n            {\"prediction_text\": \"2 \u6642\u9593 21 \u5206\", \"id\": \"001\"},\n        ],\n        \"target\": [{\"answers\": {\"answer_start\": [10, 12], \"text\": [\"2 \u6642\u9593\", \"\u6642\u9593 21 \u5206\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,", "        \"target\": [{\"answers\": {\"answer_start\": [10, 12], \"text\": [\"2 \u6642\u9593\", \"\u6642\u9593 21 \u5206\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,\n        \"f1\": 6 / 7,  # precision: 3 / 4, recall: 3 / 3\n    },\n    {\n        \"preds\": [\n            {\"prediction_text\": \"2 \u6642 \u9593 2 1 \u5206\", \"id\": \"001\"},\n        ],\n        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 \u6642 \u9593\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,", "        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 \u6642 \u9593\"]}, \"id\": \"001\"}],\n        \"exact_match\": 0.0,\n        \"f1\": 2 / 3,  # precision: 3 / 6, recall: 3 / 3\n    },\n]\n\n\n@pytest.mark.parametrize(\"case\", CASES)\ndef test_jsquad(case: dict):\n    metric = SQuAD()\n    metrics = metric(case[\"preds\"], case[\"target\"])\n    assert metrics[\"exact_match\"].item() / 100.0 == pytest.approx(case[\"exact_match\"])\n    assert metrics[\"f1\"].item() / 100.0 == pytest.approx(case[\"f1\"])", "def test_jsquad(case: dict):\n    metric = SQuAD()\n    metrics = metric(case[\"preds\"], case[\"target\"])\n    assert metrics[\"exact_match\"].item() / 100.0 == pytest.approx(case[\"exact_match\"])\n    assert metrics[\"f1\"].item() / 100.0 == pytest.approx(case[\"f1\"])\n"]}
{"filename": "tests/datasets/test_jsquad.py", "chunked_list": ["from typing import Any\n\nfrom datasets import Dataset as HFDataset  # type: ignore[attr-defined]\nfrom datasets import load_dataset  # type: ignore[attr-defined]\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\n\nfrom datamodule.datasets.jsquad import JSQuADDataset\n\n\ndef test_init(tokenizer: PreTrainedTokenizerBase):\n    _ = JSQuADDataset(\"train\", tokenizer, max_seq_length=128, segmenter_kwargs=DictConfig({}), limit_examples=3)", "\n\ndef test_init(tokenizer: PreTrainedTokenizerBase):\n    _ = JSQuADDataset(\"train\", tokenizer, max_seq_length=128, segmenter_kwargs=DictConfig({}), limit_examples=3)\n\n\ndef test_raw_examples():\n    dataset: HFDataset = load_dataset(\"shunk031/JGLUE\", name=\"JSQuAD\", split=\"validation\")\n    for example in dataset:\n        assert isinstance(example[\"id\"], str)\n        assert isinstance(example[\"title\"], str)\n        assert isinstance(example[\"context\"], str)\n        assert isinstance(example[\"question\"], str)\n        assert isinstance(example[\"answers\"], dict)\n        texts = example[\"answers\"][\"text\"]\n        answer_starts = example[\"answers\"][\"answer_start\"]\n        for text, answer_start in zip(texts, answer_starts):\n            assert example[\"context\"][answer_start:].startswith(text)\n        assert example[\"is_impossible\"] is False", "\n\ndef test_examples(tokenizer: PreTrainedTokenizerBase):\n    max_seq_length = 128\n    dataset = JSQuADDataset(\"validation\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=10)\n    for example in dataset.hf_dataset:\n        for answer in example[\"answers\"]:\n            if answer[\"answer_start\"] == -1:\n                continue\n            assert example[\"context\"][answer[\"answer_start\"] :].startswith(answer[\"text\"])", "\n\ndef test_getitem(tokenizer: PreTrainedTokenizerBase):\n    max_seq_length = 128\n    dataset = JSQuADDataset(\"train\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=3)\n    for i in range(len(dataset)):\n        feature = dataset[i]\n        assert len(feature.input_ids) == max_seq_length\n        assert len(feature.attention_mask) == max_seq_length\n        assert len(feature.token_type_ids) == max_seq_length\n        assert isinstance(feature.start_positions, int)\n        assert isinstance(feature.end_positions, int)", "\n\ndef test_features_0(tokenizer: PreTrainedTokenizerBase):\n    max_seq_length = 128\n    dataset = JSQuADDataset(\"validation\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=1)\n    example: dict[str, Any] = dict(\n        id=\"a10336p0q0\",\n        title=\"\u6885\u96e8\",\n        context=\"\u6885\u96e8 [SEP] \u6885\u96e8 \uff08 \u3064\u3086 \u3001 \u3070\u3044\u3046 \uff09 \u306f \u3001 \u5317\u6d77\u9053 \u3068 \u5c0f\u7b20\u539f \u8af8\u5cf6 \u3092 \u9664\u304f \u65e5\u672c \u3001 \u671d\u9bae \u534a\u5cf6 \u5357\u90e8 \u3001 \u4e2d\u56fd \u306e \u5357\u90e8 \u304b\u3089 \u9577\u6c5f \u6d41\u57df \u306b \u304b\u3051\u3066 \u306e \u6cbf\u6d77 \u90e8 \u3001 \u304a\u3088\u3073 \u53f0\u6e7e \u306a\u3069 \u3001 \u6771 \u30a2\u30b8\u30a2 \u306e \u5e83\u7bc4\u56f2\u306b \u304a\u3044\u3066 \u307f \u3089\u308c\u308b \u7279\u6709\u306e \u6c17\u8c61 \u73fe\u8c61 \u3067 \u3001 5 \u6708 \u304b\u3089 7 \u6708 \u306b \u304b\u3051\u3066 \u6765\u308b \u66c7\u308a \u3084 \u96e8 \u306e \u591a\u3044 \u671f\u9593 \u306e \u3053\u3068 \u3002 \u96e8\u5b63 \u306e \u4e00\u7a2e \u3067\u3042\u308b \u3002\",\n        question=\"\u65e5\u672c \u3067 \u6885\u96e8 \u304c \u306a\u3044 \u306e \u306f \u5317\u6d77\u9053 \u3068 \u3069\u3053 \u304b \u3002\",\n        answers=[\n            dict(text=\"\u5c0f\u7b20\u539f \u8af8\u5cf6\", answer_start=35),\n            dict(text=\"\u5c0f\u7b20\u539f \u8af8\u5cf6 \u3092 \u9664\u304f \u65e5\u672c\", answer_start=35),\n            dict(text=\"\u5c0f\u7b20\u539f \u8af8\u5cf6\", answer_start=35),\n        ],\n        is_impossible=False,\n    )\n    features = dataset[0]\n    question_tokens: list[str] = tokenizer.tokenize(example[\"question\"])\n    context_tokens: list[str] = tokenizer.tokenize(example[\"context\"])\n    input_tokens = (\n        [tokenizer.cls_token] + question_tokens + [tokenizer.sep_token] + context_tokens + [tokenizer.sep_token]\n    )\n    padded_input_tokens = input_tokens + [tokenizer.pad_token] * (max_seq_length - len(input_tokens))\n    assert features.input_ids == tokenizer.convert_tokens_to_ids(padded_input_tokens)\n    assert features.attention_mask == [1] * len(input_tokens) + [0] * (max_seq_length - len(input_tokens))\n    assert features.token_type_ids == [0] * (len(question_tokens) + 2) + [1] * (len(context_tokens) + 1) + [0] * (\n        max_seq_length - len(input_tokens)\n    )\n\n    assert 0 <= features.start_positions <= features.end_positions < max_seq_length\n    answer_span = slice(features.start_positions, features.end_positions + 1)\n    tokenized_answer_text: str = tokenizer.decode(features.input_ids[answer_span])\n    answers: list[dict[str, Any]] = example[\"answers\"]\n    assert tokenized_answer_text == answers[0][\"text\"]", ""]}
{"filename": "src/train.py", "chunked_list": ["import logging\nimport math\nimport warnings\nfrom typing import Union\n\nimport hydra\nimport torch\nimport transformers.utils.logging as hf_logging\nimport wandb\nfrom lightning import Callback, LightningModule, Trainer, seed_everything", "import wandb\nfrom lightning import Callback, LightningModule, Trainer, seed_everything\nfrom lightning.pytorch.loggers import Logger\nfrom lightning.pytorch.utilities.warnings import PossibleUserWarning\nfrom omegaconf import DictConfig, ListConfig\n\nfrom datamodule.datamodule import DataModule\n\nhf_logging.set_verbosity(hf_logging.ERROR)\nwarnings.filterwarnings(", "hf_logging.set_verbosity(hf_logging.ERROR)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=r\"It is recommended to use .+ when logging on epoch level in distributed setting to accumulate the metric\"\n    r\" across devices\",\n    category=PossibleUserWarning,\n)\nlogging.getLogger(\"torch\").setLevel(logging.WARNING)\n\n", "\n\n@hydra.main(version_base=None, config_path=\"../configs\")\ndef main(cfg: DictConfig):\n    if isinstance(cfg.devices, str):\n        cfg.devices = list(map(int, cfg.devices.split(\",\"))) if \",\" in cfg.devices else int(cfg.devices)\n    if isinstance(cfg.max_batches_per_device, str):\n        cfg.max_batches_per_device = int(cfg.max_batches_per_device)\n    if isinstance(cfg.num_workers, str):\n        cfg.num_workers = int(cfg.num_workers)\n    cfg.seed = seed_everything(seed=cfg.seed, workers=True)\n\n    logger: Union[Logger, bool] = cfg.get(\"logger\", False) and hydra.utils.instantiate(cfg.get(\"logger\"))\n    callbacks: list[Callback] = list(map(hydra.utils.instantiate, cfg.get(\"callbacks\", {}).values()))\n\n    # Calculate gradient_accumulation_steps assuming DDP\n    num_devices: int = 1\n    if isinstance(cfg.devices, (list, ListConfig)):\n        num_devices = len(cfg.devices)\n    elif isinstance(cfg.devices, int):\n        num_devices = cfg.devices\n    cfg.trainer.accumulate_grad_batches = math.ceil(\n        cfg.effective_batch_size / (cfg.max_batches_per_device * num_devices)\n    )\n    batches_per_device = cfg.effective_batch_size // (num_devices * cfg.trainer.accumulate_grad_batches)\n    # if effective_batch_size % (accumulate_grad_batches * num_devices) != 0, then\n    # cause an error of at most accumulate_grad_batches * num_devices compared in effective_batch_size\n    # otherwise, no error\n    cfg.effective_batch_size = batches_per_device * num_devices * cfg.trainer.accumulate_grad_batches\n    cfg.datamodule.batch_size = batches_per_device\n\n    trainer: Trainer = hydra.utils.instantiate(\n        cfg.trainer,\n        logger=logger,\n        callbacks=callbacks,\n        devices=cfg.devices,\n    )\n\n    datamodule = DataModule(cfg=cfg.datamodule)\n\n    model: LightningModule = hydra.utils.instantiate(cfg.module.cls, hparams=cfg, _recursive_=False)\n    if cfg.compile is True:\n        model = torch.compile(model)\n\n    trainer.fit(model=model, datamodule=datamodule)\n    trainer.test(model=model, datamodule=datamodule, ckpt_path=\"best\" if not trainer.fast_dev_run else None)\n\n    wandb.finish()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/test.py", "chunked_list": ["import logging\nimport warnings\nfrom typing import Union\n\nimport hydra\nimport torch\nimport transformers.utils.logging as hf_logging\nfrom lightning import Callback, LightningModule, Trainer\nfrom lightning.pytorch.loggers import Logger\nfrom lightning.pytorch.trainer.states import TrainerFn", "from lightning.pytorch.loggers import Logger\nfrom lightning.pytorch.trainer.states import TrainerFn\nfrom lightning.pytorch.utilities.warnings import PossibleUserWarning\nfrom omegaconf import DictConfig, ListConfig, OmegaConf\n\nfrom datamodule.datamodule import DataModule\n\nhf_logging.set_verbosity(hf_logging.ERROR)\nwarnings.filterwarnings(\n    \"ignore\",", "warnings.filterwarnings(\n    \"ignore\",\n    message=r\"It is recommended to use .+ when logging on epoch level in distributed setting to accumulate the metric\"\n    r\" across devices\",\n    category=PossibleUserWarning,\n)\nlogging.getLogger(\"torch\").setLevel(logging.WARNING)\n\n\n@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"eval\")\ndef main(eval_cfg: DictConfig):\n    if isinstance(eval_cfg.devices, str):\n        eval_cfg.devices = (\n            list(map(int, eval_cfg.devices.split(\",\"))) if \",\" in eval_cfg.devices else int(eval_cfg.devices)\n        )\n    if isinstance(eval_cfg.max_batches_per_device, str):\n        eval_cfg.max_batches_per_device = int(eval_cfg.max_batches_per_device)\n    if isinstance(eval_cfg.num_workers, str):\n        eval_cfg.num_workers = int(eval_cfg.num_workers)\n\n    # Load saved model and configs\n    model: LightningModule = hydra.utils.call(eval_cfg.module.load_from_checkpoint, _recursive_=False)\n    if eval_cfg.compile is True:\n        model = torch.compile(model)\n\n    train_cfg: DictConfig = model.hparams\n    OmegaConf.set_struct(train_cfg, False)  # enable to add new key-value pairs\n    cfg = OmegaConf.merge(train_cfg, eval_cfg)\n    assert isinstance(cfg, DictConfig)\n\n    logger: Union[Logger, bool] = cfg.get(\"logger\", False) and hydra.utils.instantiate(cfg.get(\"logger\"))\n    callbacks: list[Callback] = list(map(hydra.utils.instantiate, cfg.get(\"callbacks\", {}).values()))\n\n    num_devices: int = 1\n    if isinstance(cfg.devices, (list, ListConfig)):\n        num_devices = len(cfg.devices)\n    elif isinstance(cfg.devices, int):\n        num_devices = cfg.devices\n    cfg.effective_batch_size = cfg.max_batches_per_device * num_devices\n    cfg.datamodule.batch_size = cfg.max_batches_per_device\n\n    trainer: Trainer = hydra.utils.instantiate(\n        cfg.trainer,\n        logger=logger,\n        callbacks=callbacks,\n        devices=cfg.devices,\n    )\n\n    datamodule = DataModule(cfg=cfg.datamodule)\n    datamodule.setup(stage=TrainerFn.TESTING)\n    if cfg.eval_set == \"test\":\n        dataloader = datamodule.test_dataloader()\n    elif cfg.eval_set == \"valid\":\n        dataloader = datamodule.val_dataloader()\n    else:\n        raise ValueError(f\"invalid eval_set: {cfg.eval_set}\")\n    trainer.test(model=model, dataloaders=dataloader)", "\n@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"eval\")\ndef main(eval_cfg: DictConfig):\n    if isinstance(eval_cfg.devices, str):\n        eval_cfg.devices = (\n            list(map(int, eval_cfg.devices.split(\",\"))) if \",\" in eval_cfg.devices else int(eval_cfg.devices)\n        )\n    if isinstance(eval_cfg.max_batches_per_device, str):\n        eval_cfg.max_batches_per_device = int(eval_cfg.max_batches_per_device)\n    if isinstance(eval_cfg.num_workers, str):\n        eval_cfg.num_workers = int(eval_cfg.num_workers)\n\n    # Load saved model and configs\n    model: LightningModule = hydra.utils.call(eval_cfg.module.load_from_checkpoint, _recursive_=False)\n    if eval_cfg.compile is True:\n        model = torch.compile(model)\n\n    train_cfg: DictConfig = model.hparams\n    OmegaConf.set_struct(train_cfg, False)  # enable to add new key-value pairs\n    cfg = OmegaConf.merge(train_cfg, eval_cfg)\n    assert isinstance(cfg, DictConfig)\n\n    logger: Union[Logger, bool] = cfg.get(\"logger\", False) and hydra.utils.instantiate(cfg.get(\"logger\"))\n    callbacks: list[Callback] = list(map(hydra.utils.instantiate, cfg.get(\"callbacks\", {}).values()))\n\n    num_devices: int = 1\n    if isinstance(cfg.devices, (list, ListConfig)):\n        num_devices = len(cfg.devices)\n    elif isinstance(cfg.devices, int):\n        num_devices = cfg.devices\n    cfg.effective_batch_size = cfg.max_batches_per_device * num_devices\n    cfg.datamodule.batch_size = cfg.max_batches_per_device\n\n    trainer: Trainer = hydra.utils.instantiate(\n        cfg.trainer,\n        logger=logger,\n        callbacks=callbacks,\n        devices=cfg.devices,\n    )\n\n    datamodule = DataModule(cfg=cfg.datamodule)\n    datamodule.setup(stage=TrainerFn.TESTING)\n    if cfg.eval_set == \"test\":\n        dataloader = datamodule.test_dataloader()\n    elif cfg.eval_set == \"valid\":\n        dataloader = datamodule.val_dataloader()\n    else:\n        raise ValueError(f\"invalid eval_set: {cfg.eval_set}\")\n    trainer.test(model=model, dataloaders=dataloader)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metrics/jsquad.py", "chunked_list": ["# This file contains code adapted from transformers\n# (https://github.com/huggingface/transformers/blob/main/examples/flax/question-answering/utils_qa.py)\n# Copyright 2020 The HuggingFace Team All rights reserved.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n\nfrom typing import Any\n\nimport numpy as np\nimport torch\nfrom torchmetrics import Metric, SQuAD", "import torch\nfrom torchmetrics import Metric, SQuAD\n\nfrom datamodule.datasets import JSQuADDataset\n\n\nclass JSQuADMetric(Metric):\n    is_differentiable: bool = False\n    higher_is_better: bool = True\n    full_state_update: bool = False\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.squad = SQuAD()\n\n    def update(\n        self,\n        example_ids: torch.Tensor,  # (b)\n        batch_start_logits: torch.Tensor,  # (b, seq)\n        batch_end_logits: torch.Tensor,  # (b, seq)\n        dataset: JSQuADDataset,\n    ) -> None:\n        preds = []\n        target = []\n        for example_id, start_logits, end_logits in zip(\n            example_ids.tolist(), batch_start_logits.tolist(), batch_end_logits.tolist()\n        ):\n            example = dataset.hf_dataset[example_id]\n            prediction_text: str = _postprocess_predictions(start_logits, end_logits, example)\n            preds.append(\n                {\n                    \"prediction_text\": self._postprocess_text(prediction_text),\n                    \"id\": example_id,\n                }\n            )\n            target.append(\n                {\n                    \"answers\": {\n                        \"text\": [self._postprocess_text(answer[\"text\"]) for answer in example[\"answers\"]],\n                        \"answer_start\": [answer[\"answer_start\"] for answer in example[\"answers\"]],\n                    },\n                    \"id\": example_id,\n                }\n            )\n        self.squad.update(preds, target)\n\n    def compute(self) -> dict[str, torch.Tensor]:\n        return {k: v / 100.0 for k, v in self.squad.compute().items()}\n\n    @staticmethod\n    def _postprocess_text(text: str) -> str:\n        \"\"\"\u53e5\u70b9\u3092\u9664\u53bb\u3057\uff0c\u6587\u5b57\u5358\u4f4d\u306b\u5206\u5272\"\"\"\n        return \" \".join(text.replace(\" \", \"\").rstrip(\"\u3002\"))", "\n\ndef _postprocess_predictions(\n    start_logits: list[float],\n    end_logits: list[float],\n    example: dict[str, Any],\n    n_best_size: int = 20,\n    max_answer_length: int = 30,\n) -> str:\n    \"\"\"\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n\n    Args:\n        start_logits (:obj:`List[float]`):\n            The logits corresponding to the start of the span for each token.\n        end_logits (:obj:`List[float]`):\n            The logits corresponding to the end of the span for each token.\n        example: The processed dataset.\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\n            The total number of n-best predictions to generate when looking for an answer.\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n            are not conditioned on one another.\n    \"\"\"\n    prelim_predictions = []\n\n    # This is what will allow us to map some the positions in our logits to span of texts in the original\n    # context.\n    offset_mapping = example[\"offset_mapping\"]\n    # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n    # available in the current feature.\n    token_is_max_context = example.get(\"token_is_max_context\")\n\n    # Go through all possibilities for the `n_best_size` greater start and end logits.\n    start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n    end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n    for start_index in start_indexes:\n        for end_index in end_indexes:\n            # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n            # to part of the input_ids that are not in the context.\n            if (\n                start_index >= len(offset_mapping)\n                or end_index >= len(offset_mapping)\n                or offset_mapping[start_index] is None\n                or len(offset_mapping[start_index]) < 2\n                or offset_mapping[end_index] is None\n                or len(offset_mapping[end_index]) < 2\n            ):\n                continue\n            # Don't consider answers with a length that is either < 0 or > max_answer_length.\n            if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                continue\n            # Don't consider answer that don't have the maximum context available (if such information is\n            # provided).\n            if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                continue\n\n            prelim_predictions.append(\n                {\n                    \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                    \"score\": start_logits[start_index] + end_logits[end_index],\n                    \"start_logit\": start_logits[start_index],\n                    \"end_logit\": end_logits[end_index],\n                }\n            )\n\n    # Only keep the best `n_best_size` predictions.\n    predictions: list[dict[str, Any]] = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n    # Use the offsets to gather the answer text in the original context.\n    context = example[\"context\"]\n    for pred in predictions:\n        offsets = pred.pop(\"offsets\")\n        pred[\"text\"] = context[offsets[0] : offsets[1]]\n    # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n    # failure.\n    if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n        predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n    # Compute the softmax of all scores (we do it with numpy to stay independent of torch/tf in this file, using\n    # the LogSumExp trick).\n    scores = np.array([pred.pop(\"score\") for pred in predictions])\n    exp_scores = np.exp(scores - np.max(scores))\n    probs = exp_scores / exp_scores.sum()\n    # Include the probabilities in our predictions.\n    for prob, pred in zip(probs, predictions):\n        pred[\"probability\"] = prob\n\n    return predictions[0][\"text\"]  # return top 1 prediction", ""]}
{"filename": "src/metrics/__init__.py", "chunked_list": ["from .jsquad import JSQuADMetric\n\n__all__ = [\"JSQuADMetric\"]\n"]}
{"filename": "src/datamodule/__init__.py", "chunked_list": [""]}
{"filename": "src/datamodule/datamodule.py", "chunked_list": ["from dataclasses import fields, is_dataclass\nfrom typing import Any, Optional, Union\n\nimport hydra\nimport lightning\nimport torch\nfrom lightning.pytorch.trainer.states import TrainerFn\nfrom omegaconf import DictConfig\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader, Dataset", "from torch import Tensor\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass DataModule(lightning.LightningDataModule):\n    def __init__(self, cfg: DictConfig) -> None:\n        super().__init__()\n        self.cfg: DictConfig = cfg\n        self.batch_size: int = cfg.batch_size\n        self.num_workers: int = cfg.num_workers\n\n        self.train_dataset: Optional[Dataset] = None\n        self.valid_dataset: Optional[Dataset] = None\n        self.test_dataset: Optional[Dataset] = None\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        if stage == TrainerFn.FITTING:\n            self.train_dataset = hydra.utils.instantiate(self.cfg.train)\n        if stage in (TrainerFn.FITTING, TrainerFn.VALIDATING, TrainerFn.TESTING):\n            self.valid_dataset = hydra.utils.instantiate(self.cfg.valid)\n        if stage == TrainerFn.TESTING:\n            self.test_dataset = hydra.utils.instantiate(self.cfg.test)\n\n    def train_dataloader(self) -> DataLoader:\n        assert self.train_dataset is not None\n        return self._get_dataloader(dataset=self.train_dataset, shuffle=True)\n\n    def val_dataloader(self) -> DataLoader:\n        assert self.valid_dataset is not None\n        return self._get_dataloader(self.valid_dataset, shuffle=False)\n\n    def test_dataloader(self) -> DataLoader:\n        assert self.test_dataset is not None\n        return self._get_dataloader(self.test_dataset, shuffle=False)\n\n    def _get_dataloader(self, dataset: Dataset, shuffle: bool) -> DataLoader:\n        return DataLoader(\n            dataset=dataset,\n            batch_size=self.batch_size,\n            shuffle=shuffle,\n            num_workers=self.num_workers,\n            collate_fn=dataclass_data_collator,\n            pin_memory=True,\n        )", "\n\ndef dataclass_data_collator(features: list[Any]) -> dict[str, Union[Tensor, list[str]]]:\n    first: Any = features[0]\n    assert is_dataclass(first), \"Data must be a dataclass\"\n    batch: dict[str, Union[Tensor, list[str]]] = {}\n    for field in fields(first):\n        feats = [getattr(f, field.name) for f in features]\n        if \"text\" in field.name:\n            batch[field.name] = feats\n        else:\n            batch[field.name] = torch.as_tensor(feats)\n    return batch", ""]}
{"filename": "src/datamodule/datasets/jsquad.py", "chunked_list": ["import os\nfrom typing import Any, Optional\n\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\nfrom datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import QuestionAnsweringFeatures, batch_segment\n", "from datamodule.datasets.util import QuestionAnsweringFeatures, batch_segment\n\n\nclass JSQuADDataset(BaseDataset[QuestionAnsweringFeatures]):\n    def __init__(\n        self,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        segmenter_kwargs: DictConfig,\n        limit_examples: int = -1,\n    ) -> None:\n        super().__init__(\"JSQuAD\", split, tokenizer, max_seq_length, limit_examples)\n\n        self.hf_dataset = self.hf_dataset.map(\n            preprocess,\n            batched=True,\n            batch_size=100,\n            fn_kwargs=dict(segmenter_kwargs=segmenter_kwargs),\n            num_proc=os.cpu_count(),\n        ).map(\n            lambda x: self.tokenizer(\n                x[\"question\"],\n                x[\"context\"],\n                padding=PaddingStrategy.MAX_LENGTH,\n                truncation=\"only_second\",\n                max_length=self.max_seq_length,\n                return_offsets_mapping=True,\n            ),\n            batched=True,\n        )\n\n        # skip invalid examples for training\n        if self.split == \"train\":\n            self.hf_dataset = self.hf_dataset.filter(\n                lambda example: any(answer[\"answer_start\"] >= 0 for answer in example[\"answers\"])\n            )\n\n    def __getitem__(self, index: int) -> QuestionAnsweringFeatures:\n        example: dict[str, Any] = self.hf_dataset[index]\n        start_positions = end_positions = 0\n        for answer in example[\"answers\"]:\n            start_positions, end_positions = self._get_token_span(example, answer[\"text\"], answer[\"answer_start\"])\n            if start_positions > 0 or end_positions > 0:\n                break\n\n        return QuestionAnsweringFeatures(\n            example_ids=index,\n            input_ids=example[\"input_ids\"],\n            attention_mask=example[\"attention_mask\"],\n            token_type_ids=example[\"token_type_ids\"],\n            start_positions=start_positions,\n            end_positions=end_positions,\n        )\n\n    @staticmethod\n    def _get_token_span(example: dict[str, Any], answer_text: str, answer_start: int) -> tuple[int, int]:\n        \"\"\"\u30b9\u30d1\u30f3\u306e\u4f4d\u7f6e\u306b\u3064\u3044\u3066\u3001\u6587\u5b57\u5358\u4f4d\u304b\u3089\u30c8\u30fc\u30af\u30f3\u5358\u4f4d\u306b\u5909\u63db\"\"\"\n        # token_type_ids:\n        #   0: 1\u756a\u76ee\u306e\u5165\u529b(=`question`)\u306e\u30c8\u30fc\u30af\u30f3 or \u30d1\u30c7\u30a3\u30f3\u30b0\n        #   1: 2\u756a\u76ee\u306e\u5165\u529b(=`context`)\u306e\u30c8\u30fc\u30af\u30f3\n        token_type_ids: list[int] = example[\"token_type_ids\"]\n        # \u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u6587\u5b57\u306e\u30b9\u30d1\u30f3\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u3092\u4fdd\u6301\u3057\u305f\u5909\u6570\n        # \"\u4eac\u90fd \u5927\u5b66\" -> [\"[CLS]\", \"\u2581\u4eac\u90fd\", \"\u2581\u5927\u5b66\", \"[SEP]\"] \u306e\u3088\u3046\u306b\u5206\u5272\u3055\u308c\u305f\u5834\u5408\u3001\n        #   [(0, 0), (0, 2), (2, 5), (0, 0)]\n        offset_mapping: list[tuple[int, int]] = example[\"offset_mapping\"]\n        context: str = example[\"context\"]\n        assert len(offset_mapping) == len(token_type_ids)\n        token_to_char_start_index = [x[0] for x in offset_mapping]\n        token_to_char_end_index = [x[1] for x in offset_mapping]\n        answer_end = answer_start + len(answer_text)\n        token_start_index = token_end_index = 0\n        for token_index, (token_type_id, char_start_index, char_end_index) in enumerate(\n            zip(token_type_ids, token_to_char_start_index, token_to_char_end_index)\n        ):\n            if token_type_id != 1 or char_start_index == char_end_index == 0:\n                continue\n            # \u534a\u89d2\u30b9\u30da\u30fc\u30b9\u304c\u7121\u8996\u3055\u308c\u3066\u3044\u306a\u3044\u6642\u304c\u3042\u308b\u305f\u3081\u3001\u305d\u306e\u5834\u5408\u306f\u30de\u30c3\u30d4\u30f3\u30b0\u30921\u3064\u305a\u3089\u3059\n            if context[char_start_index] == \" \":\n                char_start_index += 1\n            if answer_start == char_start_index:\n                token_start_index = token_index\n            if answer_end == char_end_index:\n                token_end_index = token_index\n        return token_start_index, token_end_index", "\n\ndef preprocess(examples, segmenter_kwargs: dict[str, Any]) -> dict[str, Any]:\n    titles: list[str]\n    bodies: list[str]\n    titles, bodies = zip(*[context.split(\" [SEP] \") for context in examples[\"context\"]])\n    segmented_titles = batch_segment(titles, **segmenter_kwargs)\n    segmented_bodies = batch_segment(bodies, **segmenter_kwargs)\n    segmented_contexts = [f\"{title} [SEP] {body}\" for title, body in zip(segmented_titles, segmented_bodies)]\n    segmented_questions = batch_segment(examples[\"question\"], **segmenter_kwargs)\n    batch_answers: list[list[dict]] = []\n    for answers, segmented_context, title in zip(examples[\"answers\"], segmented_contexts, titles):\n        processed_answers: list[dict] = []\n        for answer_text, answer_start in zip(answers[\"text\"], answers[\"answer_start\"]):\n            segmented_answer_text, answer_start = find_segmented_answer(\n                segmented_context, answer_text, answer_start, len(title)\n            )\n            if answer_start is None:\n                processed_answers.append(dict(text=answer_text, answer_start=-1))\n                continue\n            assert segmented_answer_text is not None\n            processed_answers.append(dict(text=segmented_answer_text, answer_start=answer_start))\n        batch_answers.append(processed_answers)\n    return {\"context\": segmented_contexts, \"question\": segmented_questions, \"answers\": batch_answers}", "\n\ndef find_segmented_answer(\n    segmented_context: str, answer_text: str, answer_start: int, sep_index: int\n) -> tuple[Optional[str], Optional[int]]:\n    \"\"\"\u5358\u8a9e\u533a\u5207\u308a\u3055\u308c\u305f context \u304b\u3089\u5358\u8a9e\u533a\u5207\u308a\u3055\u308c\u305f answer \u306e\u30b9\u30d1\u30f3\u3092\u63a2\u7d22\n\n    Args:\n        segmented_context: \u5358\u8a9e\u533a\u5207\u308a\u3055\u308c\u305f context\n        answer_text: answer \u306e\u6587\u5b57\u5217\n        answer_start: answer \u306e\u6587\u5b57\u5358\u4f4d\u958b\u59cb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        sep_index: [SEP] \u306e\u6587\u5b57\u5358\u4f4d\u958b\u59cb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n\n    Returns:\n        Optional[str]: \u5358\u8a9e\u533a\u5207\u308a\u3055\u308c\u305f answer\uff08\u898b\u3064\u304b\u3089\u306a\u3051\u308c\u3070 None\uff09\n        Optional[int]: \u5358\u8a9e\u533a\u5207\u308a\u3055\u308c\u305f context \u306b\u304a\u3051\u308b answer \u306e\u6587\u5b57\u5358\u4f4d\u958b\u59cb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\uff08\u898b\u3064\u304b\u3089\u306a\u3051\u308c\u3070 None\uff09\n    \"\"\"\n    words = segmented_context.split(\" \")\n    char_to_word_index = {}\n    char_index = 0\n    for word_index, word in enumerate(words):\n        char_to_word_index[char_index] = word_index\n        # [SEP]\u3060\u3051\u524d\u5f8c\u306e\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3092\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081+2\u3059\u308b\n        char_length = len(word) + 2 if word == \"[SEP]\" else len(word)\n        char_index += char_length\n\n    # \u7b54\u3048\u306e\u30b9\u30d1\u30f3\u306e\u958b\u59cb\u4f4d\u7f6e\u304c\u5358\u8a9e\u533a\u5207\u308a\u306b\u6cbf\u3046\u304b\u30c1\u30a7\u30c3\u30af\n    if answer_start in char_to_word_index:\n        word_index = char_to_word_index[answer_start]\n        buf = []\n        for word in words[word_index:]:\n            buf.append(word)\n            # \u5206\u304b\u3061\u66f8\u304d\u3057\u3066\u3082\u7b54\u3048\u306e\u30b9\u30d1\u30f3\u304c\u898b\u3064\u304b\u308b\u5834\u5408\n            if \"\".join(buf) == answer_text:\n                offset = 2 if answer_start >= sep_index else 0\n                return \" \".join(buf), answer_start + word_index - offset\n    return None, None", ""]}
{"filename": "src/datamodule/datasets/base.py", "chunked_list": ["from abc import ABC\nfrom typing import Generic, TypeVar\n\nfrom datasets import Dataset as HFDataset  # type: ignore[attr-defined]\nfrom datasets import load_dataset  # type: ignore[attr-defined]\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizerBase\n\nFeatureType = TypeVar(\"FeatureType\")\n", "FeatureType = TypeVar(\"FeatureType\")\n\n\nclass BaseDataset(Dataset[FeatureType], Generic[FeatureType], ABC):\n    def __init__(\n        self,\n        dataset_name: str,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        limit_examples: int = -1,\n    ) -> None:\n        self.split: str = split\n        self.tokenizer: PreTrainedTokenizerBase = tokenizer\n        self.max_seq_length: int = max_seq_length\n\n        # NOTE: JGLUE does not provide test set.\n        if self.split == \"test\":\n            self.split = \"validation\"\n        # columns: id, title, context, question, answers, is_impossible\n        self.hf_dataset: HFDataset = load_dataset(\"shunk031/JGLUE\", name=dataset_name, split=self.split)\n        if limit_examples > 0:\n            self.hf_dataset = self.hf_dataset.select(range(limit_examples))\n\n    def __getitem__(self, index: int) -> FeatureType:\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        return len(self.hf_dataset)", ""]}
{"filename": "src/datamodule/datasets/jnli.py", "chunked_list": ["import os\nfrom typing import Any\n\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\nfrom datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n", "from datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\n\nclass JNLIDataset(BaseDataset[SequenceClassificationFeatures]):\n    def __init__(\n        self,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        segmenter_kwargs: DictConfig,\n        limit_examples: int = -1,\n    ) -> None:\n        super().__init__(\"JNLI\", split, tokenizer, max_seq_length, limit_examples)\n\n        self.hf_dataset = self.hf_dataset.map(\n            lambda x: {\n                \"segmented1\": batch_segment(x[\"sentence1\"], **segmenter_kwargs),  # type: ignore\n                \"segmented2\": batch_segment(x[\"sentence2\"], **segmenter_kwargs),  # type: ignore\n            },\n            batched=True,\n            batch_size=100,\n            num_proc=os.cpu_count(),\n        ).map(\n            lambda x: self.tokenizer(\n                x[\"segmented1\"],\n                x[\"segmented2\"],\n                padding=PaddingStrategy.MAX_LENGTH,\n                truncation=True,\n                max_length=self.max_seq_length,\n            ),\n            batched=True,\n        )\n\n    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n        example: dict[str, Any] = self.hf_dataset[index]\n        return SequenceClassificationFeatures(\n            input_ids=example[\"input_ids\"],\n            attention_mask=example[\"attention_mask\"],\n            token_type_ids=example[\"token_type_ids\"],\n            labels=example[\"label\"],  # 0: entailment, 1: contradiction, 2: neutral\n        )", ""]}
{"filename": "src/datamodule/datasets/__init__.py", "chunked_list": ["from .jcqa import JCommonsenseQADataset\nfrom .jnli import JNLIDataset\nfrom .jsquad import JSQuADDataset\nfrom .jsts import JSTSDataset\nfrom .marc_ja import MARCJaDataset\n\n__all__ = [\"MARCJaDataset\", \"JSTSDataset\", \"JNLIDataset\", \"JSQuADDataset\", \"JCommonsenseQADataset\"]\n"]}
{"filename": "src/datamodule/datasets/util.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Optional\n\nimport jaconv\nimport MeCab\nfrom rhoknp import Jumanpp\n\n\n@dataclass(frozen=True)\nclass SequenceClassificationFeatures:\n    input_ids: list[int]\n    attention_mask: list[int]\n    token_type_ids: list[int]\n    labels: int", "@dataclass(frozen=True)\nclass SequenceClassificationFeatures:\n    input_ids: list[int]\n    attention_mask: list[int]\n    token_type_ids: list[int]\n    labels: int\n\n\n@dataclass(frozen=True)\nclass MultipleChoiceFeatures:\n    input_ids: list[list[int]]\n    attention_mask: list[list[int]]\n    token_type_ids: list[list[int]]\n    labels: int", "@dataclass(frozen=True)\nclass MultipleChoiceFeatures:\n    input_ids: list[list[int]]\n    attention_mask: list[list[int]]\n    token_type_ids: list[list[int]]\n    labels: int\n\n\n@dataclass(frozen=True)\nclass QuestionAnsweringFeatures:\n    example_ids: int\n    input_ids: list[int]\n    attention_mask: list[int]\n    token_type_ids: list[int]\n    start_positions: int\n    end_positions: int", "@dataclass(frozen=True)\nclass QuestionAnsweringFeatures:\n    example_ids: int\n    input_ids: list[int]\n    attention_mask: list[int]\n    token_type_ids: list[int]\n    start_positions: int\n    end_positions: int\n\n\ndef batch_segment(\n    texts: list[str], analyzer: str = \"jumanpp\", h2z: bool = True, mecab_dic_dir: Optional[str] = None\n) -> list[str]:\n    segmenter = WordSegmenter(analyzer, h2z, mecab_dic_dir)\n    return [segmenter.get_segmented_string(text) for text in texts]", "\n\ndef batch_segment(\n    texts: list[str], analyzer: str = \"jumanpp\", h2z: bool = True, mecab_dic_dir: Optional[str] = None\n) -> list[str]:\n    segmenter = WordSegmenter(analyzer, h2z, mecab_dic_dir)\n    return [segmenter.get_segmented_string(text) for text in texts]\n\n\nclass WordSegmenter:\n    def __init__(self, analyzer: str, h2z: bool, mecab_dic_dir: Optional[str] = None):\n        self._analyzer: str = analyzer\n        self._h2z: bool = h2z\n\n        if self._analyzer == \"jumanpp\":\n            self._jumanpp = Jumanpp()\n        elif self._analyzer == \"mecab\":\n            tagger_options = []\n            if mecab_dic_dir is not None:\n                tagger_options += f\"-d {mecab_dic_dir}\".split()\n            self._mecab = MeCab.Tagger(\" \".join(tagger_options))\n\n    def get_words(self, string: str) -> list[str]:\n        words: list[str] = []\n\n        if self._analyzer == \"jumanpp\":\n            sentence = self._jumanpp.apply_to_sentence(string)\n\n            for morpheme in sentence.morphemes:\n                words.append(morpheme.text)\n        elif self._analyzer == \"mecab\":\n            self._mecab.parse(\"\")\n            node = self._mecab.parseToNode(string)\n            while node:\n                word = node.surface\n                if node.feature.split(\",\")[0] != \"BOS/EOS\":\n                    words.append(word)\n                node = node.next\n        elif self._analyzer == \"char\":\n            for char in string:\n                words.append(char)\n        else:\n            NotImplementedError(f\"unknown analyzer: {self._analyzer}\")\n\n        return words\n\n    def get_segmented_string(self, string: str) -> str:\n        if self._h2z is True:\n            string = jaconv.h2z(string)\n        words = self.get_words(string)\n        return \" \".join(word for word in words)", "\nclass WordSegmenter:\n    def __init__(self, analyzer: str, h2z: bool, mecab_dic_dir: Optional[str] = None):\n        self._analyzer: str = analyzer\n        self._h2z: bool = h2z\n\n        if self._analyzer == \"jumanpp\":\n            self._jumanpp = Jumanpp()\n        elif self._analyzer == \"mecab\":\n            tagger_options = []\n            if mecab_dic_dir is not None:\n                tagger_options += f\"-d {mecab_dic_dir}\".split()\n            self._mecab = MeCab.Tagger(\" \".join(tagger_options))\n\n    def get_words(self, string: str) -> list[str]:\n        words: list[str] = []\n\n        if self._analyzer == \"jumanpp\":\n            sentence = self._jumanpp.apply_to_sentence(string)\n\n            for morpheme in sentence.morphemes:\n                words.append(morpheme.text)\n        elif self._analyzer == \"mecab\":\n            self._mecab.parse(\"\")\n            node = self._mecab.parseToNode(string)\n            while node:\n                word = node.surface\n                if node.feature.split(\",\")[0] != \"BOS/EOS\":\n                    words.append(word)\n                node = node.next\n        elif self._analyzer == \"char\":\n            for char in string:\n                words.append(char)\n        else:\n            NotImplementedError(f\"unknown analyzer: {self._analyzer}\")\n\n        return words\n\n    def get_segmented_string(self, string: str) -> str:\n        if self._h2z is True:\n            string = jaconv.h2z(string)\n        words = self.get_words(string)\n        return \" \".join(word for word in words)", ""]}
{"filename": "src/datamodule/datasets/jcqa.py", "chunked_list": ["import os\nfrom itertools import chain\nfrom typing import Any\n\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\nfrom datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import MultipleChoiceFeatures, batch_segment", "from datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import MultipleChoiceFeatures, batch_segment\n\nCHOICE_NAMES = [\"choice0\", \"choice1\", \"choice2\", \"choice3\", \"choice4\"]\nNUM_CHOICES = len(CHOICE_NAMES)\n\n\nclass JCommonsenseQADataset(BaseDataset[MultipleChoiceFeatures]):\n    def __init__(\n        self,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        segmenter_kwargs: DictConfig,\n        limit_examples: int = -1,\n    ) -> None:\n        super().__init__(\"JCommonsenseQA\", split, tokenizer, max_seq_length, limit_examples)\n\n        def preprocess_function(examples) -> dict[str, list[list[Any]]]:\n            # (example, 5)\n            first_sentences: list[list[str]] = [[question] * NUM_CHOICES for question in examples[\"question\"]]\n            second_sentences: list[list[str]] = [\n                [examples[name][i] for name in CHOICE_NAMES] for i in range(len(examples[\"question\"]))\n            ]\n            # Tokenize\n            tokenized_examples = self.tokenizer(\n                list(chain(*first_sentences)),\n                list(chain(*second_sentences)),\n                truncation=True,\n                max_length=self.max_seq_length,\n                padding=PaddingStrategy.MAX_LENGTH,\n            )\n            # Un-flatten\n            return {\n                k: [v[i : i + NUM_CHOICES] for i in range(0, len(v), NUM_CHOICES)]\n                for k, v in tokenized_examples.items()\n            }\n\n        self.hf_dataset = self.hf_dataset.map(\n            lambda x: {\n                key: batch_segment(x[key], **segmenter_kwargs) for key in [\"question\"] + CHOICE_NAMES  # type: ignore\n            },\n            batched=True,\n            batch_size=100,\n            num_proc=os.cpu_count(),\n        ).map(\n            preprocess_function,\n            batched=True,\n        )\n\n    def __getitem__(self, index: int) -> MultipleChoiceFeatures:\n        example: dict[str, Any] = self.hf_dataset[index]\n        return MultipleChoiceFeatures(\n            input_ids=example[\"input_ids\"],\n            attention_mask=example[\"attention_mask\"],\n            token_type_ids=example[\"token_type_ids\"],\n            labels=example[\"label\"],\n        )", ""]}
{"filename": "src/datamodule/datasets/jsts.py", "chunked_list": ["import os\nfrom typing import Any\n\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\nfrom datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n", "from datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\n\nclass JSTSDataset(BaseDataset[SequenceClassificationFeatures]):\n    def __init__(\n        self,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        segmenter_kwargs: DictConfig,\n        limit_examples: int = -1,\n    ) -> None:\n        super().__init__(\"JSTS\", split, tokenizer, max_seq_length, limit_examples)\n\n        self.hf_dataset = self.hf_dataset.map(\n            lambda x: {\n                \"segmented1\": batch_segment(x[\"sentence1\"], **segmenter_kwargs),  # type: ignore\n                \"segmented2\": batch_segment(x[\"sentence2\"], **segmenter_kwargs),  # type: ignore\n            },\n            batched=True,\n            batch_size=100,\n            num_proc=os.cpu_count(),\n        ).map(\n            lambda x: self.tokenizer(\n                x[\"segmented1\"],\n                x[\"segmented2\"],\n                padding=PaddingStrategy.MAX_LENGTH,\n                truncation=True,\n                max_length=self.max_seq_length,\n            ),\n            batched=True,\n        )\n\n    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n        example: dict[str, Any] = self.hf_dataset[index]\n        return SequenceClassificationFeatures(\n            input_ids=example[\"input_ids\"],\n            attention_mask=example[\"attention_mask\"],\n            token_type_ids=example[\"token_type_ids\"],\n            labels=example[\"label\"],\n        )", ""]}
{"filename": "src/datamodule/datasets/marc_ja.py", "chunked_list": ["import os\nfrom typing import Any\n\nfrom omegaconf import DictConfig\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\nfrom datamodule.datasets.base import BaseDataset\nfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n", "from datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\n\nclass MARCJaDataset(BaseDataset[SequenceClassificationFeatures]):\n    def __init__(\n        self,\n        split: str,\n        tokenizer: PreTrainedTokenizerBase,\n        max_seq_length: int,\n        segmenter_kwargs: DictConfig,\n        limit_examples: int = -1,\n    ) -> None:\n        super().__init__(\"MARC-ja\", split, tokenizer, max_seq_length, limit_examples)\n\n        self.hf_dataset = self.hf_dataset.map(\n            lambda x: {\"segmented\": batch_segment(x[\"sentence\"], **segmenter_kwargs)},  # type: ignore\n            batched=True,\n            batch_size=100,\n            num_proc=os.cpu_count(),\n        ).map(\n            lambda x: self.tokenizer(\n                x[\"segmented\"],\n                padding=PaddingStrategy.MAX_LENGTH,\n                truncation=True,\n                max_length=self.max_seq_length,\n            ),\n            batched=True,\n        )\n\n    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n        example: dict[str, Any] = self.hf_dataset[index]\n        return SequenceClassificationFeatures(\n            input_ids=example[\"input_ids\"],\n            attention_mask=example[\"attention_mask\"],\n            token_type_ids=example[\"token_type_ids\"],\n            labels=example[\"label\"],\n        )", ""]}
{"filename": "src/modules/jsquad.py", "chunked_list": ["import torch\nfrom omegaconf import DictConfig\nfrom transformers import AutoConfig, AutoModelForQuestionAnswering, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import QuestionAnsweringModelOutput\n\nfrom datamodule.datasets.jsquad import JSQuADDataset\nfrom metrics import JSQuADMetric\nfrom modules.base import BaseModule\n\n\nclass JSQuADModule(BaseModule):\n    MODEL_ARGS = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\"]\n\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config: PretrainedConfig = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            finetuning_task=\"JSQuAD\",\n        )\n        self.model: PreTrainedModel = AutoModelForQuestionAnswering.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = JSQuADMetric()\n\n    def forward(self, batch: dict[str, torch.Tensor]) -> QuestionAnsweringModelOutput:\n        return self.model(**{k: v for k, v in batch.items() if k in self.MODEL_ARGS})\n\n    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        out: QuestionAnsweringModelOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n        out: QuestionAnsweringModelOutput = self(batch)\n        dataset: JSQuADDataset = self.trainer.val_dataloaders.dataset\n        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n\n    def on_validation_epoch_end(self) -> None:\n        self.log_dict({f\"valid/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()\n\n    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n        out: QuestionAnsweringModelOutput = self(batch)\n        dataset: JSQuADDataset = self.trainer.test_dataloaders.dataset\n        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n\n    def on_test_epoch_end(self) -> None:\n        self.log_dict({f\"test/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()", "\n\nclass JSQuADModule(BaseModule):\n    MODEL_ARGS = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\"]\n\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config: PretrainedConfig = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            finetuning_task=\"JSQuAD\",\n        )\n        self.model: PreTrainedModel = AutoModelForQuestionAnswering.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = JSQuADMetric()\n\n    def forward(self, batch: dict[str, torch.Tensor]) -> QuestionAnsweringModelOutput:\n        return self.model(**{k: v for k, v in batch.items() if k in self.MODEL_ARGS})\n\n    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        out: QuestionAnsweringModelOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n        out: QuestionAnsweringModelOutput = self(batch)\n        dataset: JSQuADDataset = self.trainer.val_dataloaders.dataset\n        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n\n    def on_validation_epoch_end(self) -> None:\n        self.log_dict({f\"valid/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()\n\n    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n        out: QuestionAnsweringModelOutput = self(batch)\n        dataset: JSQuADDataset = self.trainer.test_dataloaders.dataset\n        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n\n    def on_test_epoch_end(self) -> None:\n        self.log_dict({f\"test/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()", ""]}
{"filename": "src/modules/base.py", "chunked_list": ["import copy\nfrom typing import Any\n\nimport hydra\nfrom lightning import LightningModule\nfrom omegaconf import DictConfig, OmegaConf\n\n\nclass BaseModule(LightningModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__()\n        self.save_hyperparameters(hparams)\n\n    def configure_optimizers(self):\n        # Split weights in two groups, one with weight decay and the other not.\n        no_decay = (\"bias\", \"LayerNorm.weight\")\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n                ],\n                \"weight_decay\": self.hparams.optimizer.weight_decay,\n                \"name\": \"decay\",\n            },\n            {\n                \"params\": [\n                    p for n, p in self.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n                ],\n                \"weight_decay\": 0.0,\n                \"name\": \"no_decay\",\n            },\n        ]\n        optimizer = hydra.utils.instantiate(\n            self.hparams.optimizer, params=optimizer_grouped_parameters, _convert_=\"partial\"\n        )\n        total_steps = self.trainer.estimated_stepping_batches\n        warmup_steps = self.hparams.warmup_steps or total_steps * self.hparams.warmup_ratio\n        if hasattr(self.hparams.scheduler, \"num_warmup_steps\"):\n            self.hparams.scheduler.num_warmup_steps = warmup_steps\n        if hasattr(self.hparams.scheduler, \"num_training_steps\"):\n            self.hparams.scheduler.num_training_steps = total_steps\n        lr_scheduler = hydra.utils.instantiate(self.hparams.scheduler, optimizer=optimizer)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}}\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\n        hparams: DictConfig = copy.deepcopy(checkpoint[\"hyper_parameters\"])\n        OmegaConf.set_struct(hparams, False)\n        checkpoint[\"hyper_parameters\"] = hparams", "class BaseModule(LightningModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__()\n        self.save_hyperparameters(hparams)\n\n    def configure_optimizers(self):\n        # Split weights in two groups, one with weight decay and the other not.\n        no_decay = (\"bias\", \"LayerNorm.weight\")\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n                ],\n                \"weight_decay\": self.hparams.optimizer.weight_decay,\n                \"name\": \"decay\",\n            },\n            {\n                \"params\": [\n                    p for n, p in self.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n                ],\n                \"weight_decay\": 0.0,\n                \"name\": \"no_decay\",\n            },\n        ]\n        optimizer = hydra.utils.instantiate(\n            self.hparams.optimizer, params=optimizer_grouped_parameters, _convert_=\"partial\"\n        )\n        total_steps = self.trainer.estimated_stepping_batches\n        warmup_steps = self.hparams.warmup_steps or total_steps * self.hparams.warmup_ratio\n        if hasattr(self.hparams.scheduler, \"num_warmup_steps\"):\n            self.hparams.scheduler.num_warmup_steps = warmup_steps\n        if hasattr(self.hparams.scheduler, \"num_training_steps\"):\n            self.hparams.scheduler.num_training_steps = total_steps\n        lr_scheduler = hydra.utils.instantiate(self.hparams.scheduler, optimizer=optimizer)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}}\n\n    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\n        hparams: DictConfig = copy.deepcopy(checkpoint[\"hyper_parameters\"])\n        OmegaConf.set_struct(hparams, False)\n        checkpoint[\"hyper_parameters\"] = hparams", ""]}
{"filename": "src/modules/jnli.py", "chunked_list": ["from typing import Any\n\nimport torch\nfrom omegaconf import DictConfig\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, PretrainedConfig, PreTrainedModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nfrom modules.base import BaseModule\n", "from modules.base import BaseModule\n\n\nclass JNLIModule(BaseModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config: PretrainedConfig = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            num_labels=3,\n            finetuning_task=\"JNLI\",\n        )\n        self.model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = MulticlassAccuracy(num_classes=3, average=\"micro\")\n\n    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n        return self.model(**batch)\n\n    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n        out: SequenceClassifierOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_validation_epoch_end(self) -> None:\n        self.log(\"valid/accuracy\", self.metric.compute())\n        self.metric.reset()\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_test_epoch_end(self) -> None:\n        self.log(\"test/accuracy\", self.metric.compute())\n        self.metric.reset()", ""]}
{"filename": "src/modules/__init__.py", "chunked_list": ["from .jcqa import JCommonsenseQAModule\nfrom .jnli import JNLIModule\nfrom .jsquad import JSQuADModule\nfrom .jsts import JSTSModule\nfrom .marc_ja import MARCJaModule\n\n__all__ = [\"MARCJaModule\", \"JSTSModule\", \"JNLIModule\", \"JSQuADModule\", \"JCommonsenseQAModule\"]\n"]}
{"filename": "src/modules/jcqa.py", "chunked_list": ["from typing import Any\n\nimport torch\nfrom omegaconf import DictConfig\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom transformers import AutoConfig, AutoModelForMultipleChoice\nfrom transformers.modeling_outputs import MultipleChoiceModelOutput\n\nfrom datamodule.datasets.jcqa import NUM_CHOICES\nfrom modules.base import BaseModule", "from datamodule.datasets.jcqa import NUM_CHOICES\nfrom modules.base import BaseModule\n\n\nclass JCommonsenseQAModule(BaseModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            num_labels=NUM_CHOICES,\n            finetuning_task=\"JCommonsenseQA\",\n        )\n        self.model = AutoModelForMultipleChoice.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = MulticlassAccuracy(num_classes=NUM_CHOICES, average=\"micro\")\n\n    def forward(self, batch: dict[str, Any]) -> MultipleChoiceModelOutput:\n        return self.model(**batch)\n\n    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n        out: MultipleChoiceModelOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        out: MultipleChoiceModelOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_validation_epoch_end(self) -> None:\n        self.log(\"valid/accuracy\", self.metric.compute())\n        self.metric.reset()\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        out: MultipleChoiceModelOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_test_epoch_end(self) -> None:\n        self.log(\"test/accuracy\", self.metric.compute())\n        self.metric.reset()", ""]}
{"filename": "src/modules/jsts.py", "chunked_list": ["from typing import Any\n\nimport torch\nfrom omegaconf import DictConfig\nfrom torchmetrics import MetricCollection, PearsonCorrCoef, SpearmanCorrCoef\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nfrom modules.base import BaseModule\n", "from modules.base import BaseModule\n\n\nclass JSTSModule(BaseModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            num_labels=1,\n            finetuning_task=\"JSTS\",\n        )\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = MetricCollection({\"spearman\": SpearmanCorrCoef(), \"pearson\": PearsonCorrCoef()})\n\n    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n        return self.model(**batch)\n\n    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n        out: SequenceClassifierOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.squeeze(out.logits, dim=-1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_validation_epoch_end(self) -> None:\n        self.log_dict({f\"valid/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.squeeze(out.logits, dim=-1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_test_epoch_end(self) -> None:\n        self.log_dict({f\"test/{key}\": value for key, value in self.metric.compute().items()})\n        self.metric.reset()", ""]}
{"filename": "src/modules/marc_ja.py", "chunked_list": ["from typing import Any\n\nimport torch\nfrom omegaconf import DictConfig\nfrom torchmetrics.classification import MulticlassAccuracy\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nfrom modules.base import BaseModule\n", "from modules.base import BaseModule\n\n\nclass MARCJaModule(BaseModule):\n    def __init__(self, hparams: DictConfig) -> None:\n        super().__init__(hparams)\n        config = AutoConfig.from_pretrained(\n            hparams.model_name_or_path,\n            num_labels=2,\n            finetuning_task=\"MARC-ja\",\n        )\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            hparams.model_name_or_path,\n            config=config,\n        )\n        self.metric = MulticlassAccuracy(num_classes=2, average=\"micro\")\n\n    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n        return self.model(**batch)\n\n    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n        out: SequenceClassifierOutput = self(batch)\n        self.log(\"train/loss\", out.loss)\n        return out.loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_validation_epoch_end(self) -> None:\n        self.log(\"valid/accuracy\", self.metric.compute())\n        self.metric.reset()\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        out: SequenceClassifierOutput = self(batch)\n        preds = torch.argmax(out.logits, dim=1)  # (b)\n        self.metric.update(preds, batch[\"labels\"])\n\n    def on_test_epoch_end(self) -> None:\n        self.log(\"test/accuracy\", self.metric.compute())\n        self.metric.reset()", ""]}
