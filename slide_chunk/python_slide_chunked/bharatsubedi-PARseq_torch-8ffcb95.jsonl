{"filename": "onnx_helper.py", "chunked_list": ["#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#", "# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np", "\nimport numpy as np\n# import tensorflow as tf\nimport tensorrt as trt\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\n# For ONNX:\n\nclass ONNXClassifierWrapper():\n    def __init__(self, file, num_classes, target_dtype = np.float32):\n        \n        self.target_dtype = target_dtype\n        self.num_classes = num_classes\n        self.load(file)\n        \n        self.stream = None\n      \n    def load(self, file):\n        f = open(file, \"rb\")\n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n\n        engine = runtime.deserialize_cuda_engine(f.read())\n        self.context = engine.create_execution_context()\n        \n    def allocate_memory(self, batch):\n        self.output = np.empty(self.num_classes, dtype = self.target_dtype) # Need to set both input and output precisions to FP16 to fully enable FP16\n\n        # Allocate device memory\n        self.d_input = cuda.mem_alloc(1 * batch.nbytes)\n        self.d_output = cuda.mem_alloc(1 * self.output.nbytes)\n\n        self.bindings = [int(self.d_input), int(self.d_output)]\n\n        self.stream = cuda.Stream()\n        \n    def predict(self, batch): # result gets copied into output\n        if self.stream is None:\n            self.allocate_memory(batch)\n            \n        # Transfer input data to device\n        cuda.memcpy_htod_async(self.d_input, batch, self.stream)\n        # Execute model\n        self.context.execute_async_v2(self.bindings, self.stream.handle, None)\n        # Transfer predictions back\n        cuda.memcpy_dtoh_async(self.output, self.d_output, self.stream)\n        # Syncronize threads\n        self.stream.synchronize()\n        \n        return self.output", "# For ONNX:\n\nclass ONNXClassifierWrapper():\n    def __init__(self, file, num_classes, target_dtype = np.float32):\n        \n        self.target_dtype = target_dtype\n        self.num_classes = num_classes\n        self.load(file)\n        \n        self.stream = None\n      \n    def load(self, file):\n        f = open(file, \"rb\")\n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n\n        engine = runtime.deserialize_cuda_engine(f.read())\n        self.context = engine.create_execution_context()\n        \n    def allocate_memory(self, batch):\n        self.output = np.empty(self.num_classes, dtype = self.target_dtype) # Need to set both input and output precisions to FP16 to fully enable FP16\n\n        # Allocate device memory\n        self.d_input = cuda.mem_alloc(1 * batch.nbytes)\n        self.d_output = cuda.mem_alloc(1 * self.output.nbytes)\n\n        self.bindings = [int(self.d_input), int(self.d_output)]\n\n        self.stream = cuda.Stream()\n        \n    def predict(self, batch): # result gets copied into output\n        if self.stream is None:\n            self.allocate_memory(batch)\n            \n        # Transfer input data to device\n        cuda.memcpy_htod_async(self.d_input, batch, self.stream)\n        # Execute model\n        self.context.execute_async_v2(self.bindings, self.stream.handle, None)\n        # Transfer predictions back\n        cuda.memcpy_dtoh_async(self.output, self.d_output, self.stream)\n        # Syncronize threads\n        self.stream.synchronize()\n        \n        return self.output", "\ndef convert_onnx_to_engine(onnx_filename, engine_filename = None, max_batch_size = 32, max_workspace_size = 1 << 30, fp16_mode = True):\n    logger = trt.Logger(trt.Logger.WARNING)\n    with trt.Builder(logger) as builder, builder.create_network() as network, trt.OnnxParser(network, logger) as parser:\n        builder.max_workspace_size = max_workspace_size\n        builder.fp16_mode = fp16_mode\n        builder.max_batch_size = max_batch_size\n\n        print(\"Parsing ONNX file.\")\n        with open(onnx_filename, 'rb') as model:\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n\n        print(\"Building TensorRT engine. This may take a few minutes.\")\n        engine = builder.build_cuda_engine(network)\n\n        if engine_filename:\n            with open(engine_filename, 'wb') as f:\n                f.write(engine.serialize())\n\n        return engine, logger"]}
{"filename": "train.py", "chunked_list": ["\nimport math\nimport argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport time\nimport os\nimport sys\nimport shutil\nfrom omegaconf import OmegaConf", "import shutil\nfrom omegaconf import OmegaConf\nimport numpy as np\nimport string\nfrom typing import List\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport torch.distributed as dist", "from torch.optim.lr_scheduler import OneCycleLR\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom torch.utils.tensorboard import SummaryWriter \n\nfrom data.module import SceneTextDataModule\nfrom model.parseq import PARSeq\nfrom torch.utils.data import DataLoader", "from model.parseq import PARSeq\nfrom torch.utils.data import DataLoader\nfrom glob import glob\n\nif 'LOCAL_RANK' in os.environ:\n    # Environment variables set by torch.distributed.launch or torchrun\n    LOCAL_RANK = int(os.environ['LOCAL_RANK'])\n    WORLD_SIZE = int(os.environ['WORLD_SIZE'])\n    WORLD_RANK = int(os.environ['RANK'])\n    print('LOCAL_RANK, WORLD_SIZE, WORLD_RANK: ', LOCAL_RANK, WORLD_SIZE, WORLD_RANK)\nelse:\n    sys.exit(\"Can't find the evironment variables for local rank\")", "\ndef set_random_seeds(random_seed=0):\n    torch.manual_seed(random_seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(random_seed)\n    random.seed(random_seed)\n\ndef sync_across_gpus(t):\n    gather_t_tensor = [torch.ones_like(t) for _ in range(WORLD_SIZE)]\n    torch.distributed.all_gather(gather_t_tensor, t)\n    return torch.stack(gather_t_tensor)", "def sync_across_gpus(t):\n    gather_t_tensor = [torch.ones_like(t) for _ in range(WORLD_SIZE)]\n    torch.distributed.all_gather(gather_t_tensor, t)\n    return torch.stack(gather_t_tensor)\n\ndef prepare(dataset, rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, \\\n                                 shuffle=True, drop_last=False)\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, \\\n                            num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n    \n    return dataloader", "\nclass Balanced_Batch(SceneTextDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataloader_list = []\n        self.dataloader_iter_list = []\n        self.batch_size_list = []\n        self.root_dir = self.train_dir\n        \n    def prepare(self, rank, world_size, pin_memory=False, num_workers=0):\n        sub_folders = glob(os.path.join(self.train_dir, '*'))\n\n        for folder, folder_weight in self.data_weights:\n            self.train_dir = os.path.join(self.root_dir, folder)\n            self._train_dataset = None\n            print(\"FOLDER/WEIGHT/DIRECTORY: \", folder, folder_weight, self.train_dir)\n            assert os.path.exists(self.train_dir), f'directory {self.train_dir} does not exist. check the folders and weights argument' \n            folder_batch_size = max(round(self.batch_size * folder_weight), 1)\n            folder_dataset = self.train_dataset\n            sampler = DistributedSampler(folder_dataset, num_replicas=world_size, rank=rank, \\\n                                         shuffle=True, drop_last=False)\n            folder_dataloader = DataLoader(folder_dataset, batch_size=folder_batch_size, pin_memory=pin_memory, \n                                    num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n#             print('folder data len: ', len(folder_dataset), len(folder_dataloader))\n            self.batch_size_list.append(folder_batch_size)\n            self.dataloader_list.append(folder_dataloader)\n            self.dataloader_iter_list.append(iter(folder_dataloader))\n            \n        self.batch_size = sum(self.batch_size_list)\n\n    def get_batch(self):\n        balanced_batch_images = []\n        balanced_batch_texts = []\n\n        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n            try:\n                image, text = data_loader_iter.next()\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except StopIteration:\n                self.dataloader_iter_list[i] = iter(self.dataloader_list[i])\n                image, text = self.dataloader_iter_list[i].next()\n                balanced_batch_images.append(image)\n                balanced_batch_texts += text\n            except ValueError:\n                pass\n\n        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n\n        return balanced_batch_images, balanced_batch_texts", "\ndef main(args):\n    \n    config = OmegaConf.load(args.config)\n    \n    # updating korean charset\n    chars = \"\"\n    with open(\n        os.path.join(config.data_loader.character.dict_dir, \"charset.txt\"), \"r\"\n    ) as f:\n        curr_char = f.read()\n    curr_char = curr_char.strip()\n    chars += curr_char\n\n    chars = \"\".join(sorted(set(chars)))\n    config.model.charset_train = chars\n    config.model.charset_test = config.model.charset_train\n\n    # Special handling for PARseq\n    if config.model.get('perm_mirrored', False):\n        assert config.model.perm_num % 2 == 0, 'perm_num should be even if perm_mirrored = True'\n    if ~os.path.exists(config.log_dir):\n        os.makedirs(config.log_dir, exist_ok=True)\n    \n    # config save\n    OmegaConf.save(config, os.path.join(config.log_dir, \"config.yaml\"))\n    \n    # Tensorboard logs\n    exp_log = SummaryWriter(os.path.join(config.log_dir, 'tf_runs'))\n    \n    dist.init_process_group(\"nccl\", rank=WORLD_RANK, world_size=WORLD_SIZE)\n\n    # Balanced data setting\n    datamodule = Balanced_Batch(**config.data)\n    datamodule.prepare(WORLD_RANK, WORLD_SIZE, num_workers=config.data.num_workers)\n    val_dataset = datamodule.val_dataset\n    config.data.batch_size = datamodule.batch_size\n    config.model.batch_size = config.data.batch_size\n    print('Updated Batch_Size: ', config.data.batch_size)\n    \n    val_loader = prepare(val_dataset, WORLD_RANK, WORLD_SIZE, batch_size=config.data.batch_size, \\\n                           num_workers=config.data.num_workers)\n\n    print('val-loader length: ', len(val_loader))\n    \n    if LOCAL_RANK == 0:\n        print('Baseline Model Training ... \\n')\n    model = PARSeq(**config.model)\n    save_name = os.path.join(config.log_dir, 'Baseline')\n\n    ckpt_savepath = f'{save_name}_parseq_ckpt.pth'\n    best_ckpt_savepath = f'{save_name}_best_parseq_ckpt.pth'\n    \n    if not config.get('resume', None) and config.get('pretrained', None):\n        pretrained_ckpt = torch.load(config.pretrained_ckpt, map_location='cuda:{}'.format(LOCAL_RANK))\n        model.load_state_dict(pretrained_ckpt['model'])\n    \n    if config.get('resume', None):  \n        ckpt = torch.load(config.resume, map_location='cuda:{}'.format(LOCAL_RANK))\n        model.load_state_dict(ckpt['model'])\n        \n    model = model.to(LOCAL_RANK)\n    \n    device = torch.device('cuda:{}'.format(LOCAL_RANK))\n    model._device = device\n    model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK, find_unused_parameters=True)\n\n    estimated_stepping_batches = config.trainer.num_iters\n    \n    # setting optimizers and schedular\n    filtered_parameters = []\n    params_num = []\n    for p in filter(lambda p: p.requires_grad, model.parameters()):\n        filtered_parameters.append(p)\n        params_num.append(np.prod(p.size()))\n    print('Trainable params num : ', sum(params_num))\n    \n    config.model.lr = config.model.lr * math.sqrt(WORLD_SIZE) * config.model.batch_size / 256\n    optimizer = torch.optim.AdamW(filtered_parameters, lr=config.model.lr, \\\n                                weight_decay=config.model.weight_decay)\n    \n    sched = OneCycleLR(optimizer, config.model.lr, estimated_stepping_batches, \\\n                       pct_start=config.model.warmup_pct,\n                           cycle_momentum=False)\n    iter_start = 0\n    if config.get('resume', None):\n        optimizer.load_state_dict(ckpt['optimizer'])\n        iter_start = ckpt['iter']\n    \n    validate_iter = config.trainer.validate_iter\n    num_iters = config.trainer.num_iters\n    best_acc = 0\n    \n    with tqdm(range(iter_start, num_iters)) as t_iter:\n        for iterr in t_iter:\n            if iterr % validate_iter == 0:\n                start_timer = time.time()\n            img, label = datamodule.get_batch()\n            img = img.to(device)\n\n            loss = model.module.training_step(img, label)                \n            log_loss = (torch.sum(sync_across_gpus(loss))/WORLD_SIZE)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            sched.step()\n            \n            t_iter.set_postfix(Loss=loss.item())\n            exp_log.add_scalars('train/loss', {f'process_{LOCAL_RANK}':loss.item()}, iterr)\n\n            if LOCAL_RANK == 0:\n                exp_log.add_scalars('train/loss', {f'average':log_loss.item()}, iterr)\n                exp_log.add_scalars('train/lr', {'lr_change':optimizer.param_groups[0]['lr']}, iterr)\n        \n            if iterr > 0 and iterr % validate_iter == 0:\n                torch.cuda.synchronize()\n                outputs = []\n                for batch_idx, val_data in tqdm(enumerate(val_loader)):\n                    with torch.no_grad():\n                        img, label = val_data\n                        img = img.to(device)\n                        outputs.append(model.module.validation_step((img, label), batch_idx))\n\n                acc, ned, loss = model.module._aggregate_results(outputs)\n                exp_log.add_scalars('val/loss', {f'process_{LOCAL_RANK}':loss}, iterr)\n                exp_log.add_scalars('val/acc', {f'process_{LOCAL_RANK}':acc}, iterr)\n                exp_log.add_scalars('val/ned', {f'process_{LOCAL_RANK}':ned}, iterr)\n\n                acc_avg = (torch.sum(sync_across_gpus(torch.tensor(acc).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n                ned_avg = (torch.sum(sync_across_gpus(torch.tensor(ned).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n                loss_avg = (torch.sum(sync_across_gpus(loss.to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n\n                end_timer = time.time()\n                elapsed = end_timer - start_timer\n                time_avg = (torch.sum(sync_across_gpus(torch.tensor(elapsed).to(f'cuda:{LOCAL_RANK}')))/WORLD_SIZE)\n\n                exp_log.add_scalars('train/time', {f'time_per_{validate_iter}iter_{LOCAL_RANK}':elapsed}, iterr)\n\n                if LOCAL_RANK == 0:\n                    print(\"Validation Accuracy: \", acc_avg.item()*100)\n                    print(\"Validation NED: \", ned_avg.item()*100)\n                    print(\"Validation Loss: \", loss_avg.item())\n\n                    exp_log.add_scalars('val/loss', {f'avg_loss':loss_avg.item()}, iterr)\n                    exp_log.add_scalars('val/acc', {f'avg_acc':acc_avg.item()}, iterr)\n                    exp_log.add_scalars('val/ned', {f'avg_ned':ned_avg.item()}, iterr)\n                    exp_log.add_scalars('train/time', {f'avg_time_per_{validate_iter}iter':time_avg.item()}, iterr)\n\n                    torch.save({'model':model.module.state_dict(), 'optimizer':optimizer.state_dict(), 'cfg':config}, ckpt_savepath)\n                    if best_acc < acc_avg:\n                        best_acc = acc_avg\n                        shutil.copyfile(ckpt_savepath, best_ckpt_savepath)\n\n    exp_log.close()", "    \nif __name__ == '__main__':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('config', help='path of baseline or CR-based self-supervised config')\n    #parser.add_argument('--korean_chars', type=str, default='./configs/korean_charset.txt', help='korean characters list text file')\n    args = parser.parse_args()\n    \n    main(args)\n", ""]}
{"filename": "pytorch2onnx.py", "chunked_list": ["import torch._C\nimport torch\nimport torch.serialization\nimport onnx\nimport onnxruntime as rt\nimport numpy as np\nfrom model.parseq_test import PARSeq\nfrom omegaconf import DictConfig, open_dict, OmegaConf\n\n", "\n\n# args\nOUTPUT_FILE = 'parseq_ar_r1.onnx'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nckpt_path = '/outputs/baseline_all/best_parseq_ckpt.pth'\nconfig_path = '/outputs/baseline_all/config.yaml'\n\n# config load\nconfig = OmegaConf.load(config_path)", "# config load\nconfig = OmegaConf.load(config_path)\n# config.model.decode_ar = False\n# config.model.decode_ar = False\n# model load \nmodel = PARSeq(**config.model)\n# h, w = config.model.img_size\n# model = torch.jit.script(model, example_inputs={model: torch.randn(1, 3, h, w, requires_grad=True)})\n\nmodel.load_state_dict(torch.load(ckpt_path)['model'])", "\nmodel.load_state_dict(torch.load(ckpt_path)['model'])\nmodel._device = torch.device(device)\nmodel = model.eval().to(device)\n# print('Model parameters: ', config.model, sep='\\n')\n\n# input define\nh, w = config.model.img_size\nx = torch.randn(1, 3, h, w)\nx = x.to(device)", "x = torch.randn(1, 3, h, w)\nx = x.to(device)\n\n# input test results\nout = model(x)\n\n#Onnx export details\ndynamic_axes = None\ndynamic_export = False\nopset_version = 14", "dynamic_export = False\nopset_version = 14\nshow = True\n#dynamic_axes = {'input' : {0 : 'batch_size'},\n#                     'output' : {0 : 'batch_size'}}\nif dynamic_export:\n    dynamic_axes = {\n        'input': {\n            0: 'batch',\n#             1: 'channel',\n#             2: 'height',\n#             3: 'widht'\n        },\n        'output': {\n            0: 'batch',\n            1: 'max_len',\n#             2: 'charset_len'\n        }\n    }", "\nwith torch.no_grad():\n    torch.onnx.export(\n        model, x,\n        OUTPUT_FILE,\n        training=None,\n        input_names=['input'],\n        output_names=['output'],\n        export_params=True,\n        verbose=show,\n        opset_version=opset_version,\n        dynamic_axes=dynamic_axes)\n    print(f'Successfully exported ONNX model: {OUTPUT_FILE}')", "\n# helper function\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n## Onnx and torch model output similarity check\n\n# Onnx model loading\nonnx_model = onnx.load(OUTPUT_FILE)\nonnx.checker.check_model(onnx_model)", "onnx_model = onnx.load(OUTPUT_FILE)\nonnx.checker.check_model(onnx_model)\n\nort_session = rt.InferenceSession(OUTPUT_FILE)\n\n# compute ONNX Runtime output prediction\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nort_outs = ort_session.run(None, ort_inputs)\n\n# compare ONNX Runtime and PyTorch results", "\n# compare ONNX Runtime and PyTorch results\nnp.testing.assert_allclose(to_numpy(out), ort_outs[0], rtol=1e-05, atol=1e-01)\n\nprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"]}
{"filename": "test_onnx.py", "chunked_list": ["#!/usr/bin/env python3\n# Scene Text Recognition Model Hub\n# Copyright 2022 Darwin Bautista\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#", "#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom glob import glob\nimport argparse", "from glob import glob\nimport argparse\nimport string\nimport sys\nimport os\nfrom dataclasses import dataclass\nfrom typing import Sequence, Any, Optional, Tuple, List\nfrom omegaconf import DictConfig, open_dict, OmegaConf\nfrom tqdm import tqdm\nimport torch", "from tqdm import tqdm\nimport torch\nimport onnx\nimport onnxruntime as rt\nfrom data.module import SceneTextDataModule\nfrom model.parseq_test import PARSeq\nfrom model.tokenizer_utils import Tokenizer\nimport numpy as np\nfrom nltk import edit_distance\nfrom torch import Tensor", "from nltk import edit_distance\nfrom torch import Tensor\n\n@dataclass\nclass BatchResult:\n    num_samples: int\n    correct: int\n    ned: float\n    confidence: float\n    label_length: int", "#     loss: Tensor\n#     loss_numel: int\n        \n@dataclass\nclass Result:\n    dataset: str\n    num_samples: int\n    accuracy: float\n    ned: float\n    confidence: float\n    label_length: float", "\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\ndef print_results_table(results: List[Result], file=None):\n    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n    w = max(w, len('Dataset'), len('Combined'))\n    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n    c = Result('Combined', 0, 0, 0, 0, 0)\n    for res in results:\n        c.num_samples += res.num_samples\n        c.accuracy += res.num_samples * res.accuracy\n        c.ned += res.num_samples * res.ned\n        c.confidence += res.num_samples * res.confidence\n        c.label_length += res.num_samples * res.label_length\n        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n    c.accuracy /= c.num_samples\n    c.ned /= c.num_samples\n    c.confidence /= c.num_samples\n    c.label_length /= c.num_samples\n    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)", "\n\n@torch.inference_mode()\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', help='path of baseline or CR-based self-supervised config')\n    parser.add_argument('--data_root', default='data')\n    parser.add_argument('--batch_size', type=int, default=126)\n    parser.add_argument('--num_workers', type=int, default=4)\n    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n    parser.add_argument('--device', default='cuda:2')\n    \n    args = parser.parse_args()\n    \n    args.config = '/outputs/baseline_all/config.yaml'\n    OUTPUT_FILE = 'parseq_ar_r1.onnx'\n    config = OmegaConf.load(args.config)\n    print(config.data.charset_train)\n    \n    tokenizer = Tokenizer(config.data.charset_train)\n    # Onnx model loading\n    onnx_model = onnx.load(OUTPUT_FILE)\n    onnx.checker.check_model(onnx_model)\n    ort_session = rt.InferenceSession(OUTPUT_FILE)\n    \n    args.data_root = 'data/'\n    \n    train_dir = os.path.join(args.data_root, 'train')\n    val_dir = os.path.join(args.data_root, 'valid')\n    test_dir = os.path.join(args.data_root, 'valid')\n    \n    datamodule = SceneTextDataModule(root_dir = args.data_root, \n                                      train_dir = train_dir, \n                                      val_dir = val_dir,\n                                      test_dir = test_dir,\n                                      img_size = config.data.img_size,\n                                      max_label_length = config.data.max_label_length,\n                                      charset_train = config.data.charset_test, # hp.charset_train,\n                                      charset_test = config.data.charset_test, # hp.charset_test,\n                                      batch_size = args.batch_size,\n                                      num_workers = args.num_workers,\n                                      remove_whitespace = False, \n                                      normalize_unicode = False,\n                                      augment = False,\n                                      rotation = args.rotation\n                                      )\n\n\n    test_folders = glob(os.path.join(test_dir, '*'))\n    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))[3:5]\n    \n    results = {}\n    max_width = max(map(len, test_set))\n    for name, dataloader in datamodule.test_dataloaders(test_set).items():\n        total = 0\n        correct = 0\n        ned = 0\n        confidence = 0\n        label_length = 0\n        for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n            res = _eval_step(ort_session, (imgs.to(args.device), labels), tokenizer)['output']\n            total += res.num_samples\n            correct += res.correct\n            ned += res.ned\n            confidence += res.confidence\n            label_length += res.label_length\n        accuracy = 100 * correct / total\n        mean_ned = 100 * (1 - ned / total)\n        mean_conf = 100 * confidence / total\n        mean_label_length = label_length / total\n        results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n#         break\n    \n    result_groups = {\n        t : [t] for t in test_set\n    }\n\n    if args.new:\n        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n    save_folder = os.path.join('/'.join(args.config.split('/')[:-1]), 'test_logs')\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    with open(os.path.join(save_folder, os.path.basename(args.config)[:-4] + '.log.txt'), 'w') as f:\n        for out in [f, sys.stdout]:\n            for group, subset in result_groups.items():\n                print(f'{group} set:', file=out)\n                print_results_table([results[s] for s in subset], out)  \n                print('\\n', file=out)", "\ndef _eval_step(ort_session, batch, tokenizer): #-> Optional[STEP_OUTPUT]:\n    images, labels = batch\n\n    correct = 0\n    total = 0\n    ned = 0\n    confidence = 0\n    label_length = 0\n    \n    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(images)}\n    logits = ort_session.run(None, ort_inputs)[0]\n    \n    probs = torch.tensor(logits).softmax(-1)\n    preds, probs = tokenizer.decode(probs)\n    for pred, prob, gt in zip(preds, probs, labels):\n        confidence += prob.prod().item()\n#             pred = charset_adapter(pred)\n        # Follow ICDAR 2019 definition of N.E.D.\n        ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n        if pred == gt:\n            correct += 1\n        total += 1\n        label_length += len(pred)\n    return dict(output=BatchResult(total, correct, ned, confidence, label_length))", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "test_trt.py", "chunked_list": ["from glob import glob\nimport argparse\nimport string\nimport sys\nimport os\nfrom dataclasses import dataclass\nfrom typing import Sequence, Any, Optional, Tuple, List\nfrom omegaconf import DictConfig, open_dict, OmegaConf\nfrom tqdm import tqdm\nimport torch", "from tqdm import tqdm\nimport torch\nimport onnx\nimport onnxruntime as rt\nfrom data.module import SceneTextDataModule\nfrom model.parseq import PARSeq\nfrom model.tokenizer_utils import Tokenizer\nimport numpy as np\nfrom nltk import edit_distance\nfrom torch import Tensor", "from nltk import edit_distance\nfrom torch import Tensor\n\nimport tensorrt as trt\nimport pycuda.driver as cuda\n\ntry:\n    import pycuda.autoprimaryctx\nexcept ModuleNotFoundError:\n    import pycuda.autoinit", "    \nimport common\n\nTRT_LOGGER = trt.Logger()\n\n@dataclass\nclass BatchResult:\n    num_samples: int\n    correct: int\n    ned: float\n    confidence: float\n    label_length: int", "#     loss: Tensor\n#     loss_numel: int\n        \n@dataclass\nclass Result:\n    dataset: str\n    num_samples: int\n    accuracy: float\n    ned: float\n    confidence: float\n    label_length: float", "\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\ndef print_results_table(results: List[Result], file=None):\n    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n    w = max(w, len('Dataset'), len('Combined'))\n    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n    c = Result('Combined', 0, 0, 0, 0, 0)\n    for res in results:\n        c.num_samples += res.num_samples\n        c.accuracy += res.num_samples * res.accuracy\n        c.ned += res.num_samples * res.ned\n        c.confidence += res.num_samples * res.confidence\n        c.label_length += res.num_samples * res.label_length\n        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n    c.accuracy /= c.num_samples\n    c.ned /= c.num_samples\n    c.confidence /= c.num_samples\n    c.label_length /= c.num_samples\n    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)", "\n    \ndef get_engine(engine_file_path):\n    if os.path.exists(engine_file_path):\n        # If a serialized engine exists, use it instead of building an engine.\n        print(\"Reading engine from file {}\".format(engine_file_path))\n        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n            return runtime.deserialize_cuda_engine(f.read())\n    else:\n        raise Exception(f\"File not found: {engine_file_path}\")", "\n@torch.inference_mode()\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', help='path of baseline or CR-based self-supervised config')\n    parser.add_argument('--data_root', default='data')\n    parser.add_argument('--batch_size', type=int, default=1)\n    parser.add_argument('--num_workers', type=int, default=4)\n    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n    parser.add_argument('--device', default='cuda:2')\n    \n    args = parser.parse_args()\n    \n    args.config = 'outputs/exp_logs_baseline_all/config.yaml'\n    engine_file_path = 'parseq_ar_r1.trt'\n    config = OmegaConf.load(args.config)\n    \n    tokenizer = Tokenizer(config.data.charset_train)\n    \n    args.data_root = 'dataset'\n    \n    train_dir = os.path.join(args.data_root, 'train')\n    val_dir = os.path.join(args.data_root, 'valid')\n    test_dir = os.path.join(args.data_root, 'valid')\n    \n    datamodule = SceneTextDataModule(root_dir = args.data_root, \n                                      train_dir = train_dir, \n                                      val_dir = val_dir,\n                                      test_dir = test_dir,\n                                      img_size = config.data.img_size,\n                                      max_label_length = config.data.max_label_length,\n                                      charset_train = config.data.charset_test, # hp.charset_train,\n                                      charset_test = config.data.charset_test, # hp.charset_test,\n                                      batch_size = args.batch_size,\n                                      num_workers = args.num_workers,\n                                      remove_whitespace = False, \n                                      normalize_unicode = False,\n                                      augment = False,\n                                      rotation = args.rotation\n                                      )\n\n\n    test_folders = glob(os.path.join(test_dir, '*'))\n    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))[:]\n    \n    results = {}\n    max_width = max(map(len, test_set))\n    \n    # trt engine \n    with get_engine(engine_file_path) as engine, engine.create_execution_context() as context:\n        inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n        # Do inference\n        for name, dataloader in datamodule.test_dataloaders(test_set).items():\n            print(\"Running inference on {}...\".format(name))\n            total = 0\n            correct = 0\n            ned = 0\n            confidence = 0\n            label_length = 0\n            for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n                inputs[0].host = np.array(imgs, dtype=np.float32)\n                trt_outputs = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n                trt_outputs = [out.reshape(1, 26, 1794) for out in trt_outputs]\n                res = _eval_step(trt_outputs, labels, tokenizer)['output']\n                total += res.num_samples\n                correct += res.correct\n                ned += res.ned\n                confidence += res.confidence\n                label_length += res.label_length\n            accuracy = 100 * correct / total\n            mean_ned = 100 * (1 - ned / total)\n            mean_conf = 100 * confidence / total\n            mean_label_length = label_length / total\n            results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n#         break\n    \n    result_groups = {\n        t : [t] for t in test_set\n    }\n\n    if args.new:\n        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n    save_folder = os.path.join('/'.join(args.config.split('/')[:-1]), 'trt_test_logs')\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    with open(os.path.join(save_folder, os.path.basename(args.config)[:-4] + '.log.txt'), 'w') as f:\n        for out in [f, sys.stdout]:\n            for group, subset in result_groups.items():\n                print(f'{group} set:', file=out)\n                print_results_table([results[s] for s in subset], out)  \n                print('\\n', file=out)", "\ndef _eval_step(trt_outputs, labels, tokenizer): #-> Optional[STEP_OUTPUT]:\n\n    correct = 0\n    total = 0\n    ned = 0\n    confidence = 0\n    label_length = 0\n    \n    logits = trt_outputs[0]\n    probs = torch.tensor(logits).softmax(-1)\n    preds, probs = tokenizer.decode(probs)\n    for pred, prob, gt in zip(preds, probs, labels):\n        confidence += prob.prod().item()\n        # Follow ICDAR 2019 definition of N.E.D.\n        ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n        if pred == gt:\n            correct += 1\n        total += 1\n        label_length += len(pred)\n    return dict(output=BatchResult(total, correct, ned, confidence, label_length))", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "common.py", "chunked_list": ["#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#", "# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse", "\nimport argparse\nimport os\n\nimport numpy as np\n\n# Use autoprimaryctx if available (pycuda >= 2021.1) to\n# prevent issues with other modules that rely on the primary\n# device context.\ntry:\n    import pycuda.autoprimaryctx\nexcept ModuleNotFoundError:\n    import pycuda.autoinit", "# device context.\ntry:\n    import pycuda.autoprimaryctx\nexcept ModuleNotFoundError:\n    import pycuda.autoinit\n\nimport pycuda.driver as cuda\nimport tensorrt as trt\n\ntry:\n    # Sometimes python does not understand FileNotFoundError\n    FileNotFoundError\nexcept NameError:\n    FileNotFoundError = IOError", "\ntry:\n    # Sometimes python does not understand FileNotFoundError\n    FileNotFoundError\nexcept NameError:\n    FileNotFoundError = IOError\n\nEXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n\n\ndef GiB(val):\n    return val * 1 << 30", "\n\ndef GiB(val):\n    return val * 1 << 30\n\n\ndef add_help(description):\n    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    args, _ = parser.parse_known_args()\n", "\n\ndef find_sample_data(description=\"Runs a TensorRT Python sample\", subfolder=\"\", find_files=[], err_msg=\"\"):\n    \"\"\"\n    Parses sample arguments.\n\n    Args:\n        description (str): Description of the sample.\n        subfolder (str): The subfolder containing data relevant to this sample\n        find_files (str): A list of filenames to find. Each filename will be replaced with an absolute path.\n\n    Returns:\n        str: Path of data directory.\n    \"\"\"\n\n    # Standard command-line arguments for all samples.\n    kDEFAULT_DATA_ROOT = os.path.join(os.sep, \"usr\", \"src\", \"tensorrt\", \"data\")\n    parser = argparse.ArgumentParser(description=description, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        \"-d\",\n        \"--datadir\",\n        help=\"Location of the TensorRT sample data directory, and any additional data directories.\",\n        action=\"append\",\n        default=[kDEFAULT_DATA_ROOT],\n    )\n    args, _ = parser.parse_known_args()\n\n    def get_data_path(data_dir):\n        # If the subfolder exists, append it to the path, otherwise use the provided path as-is.\n        data_path = os.path.join(data_dir, subfolder)\n        if not os.path.exists(data_path):\n            if data_dir != kDEFAULT_DATA_ROOT:\n                print(\"WARNING: \" + data_path + \" does not exist. Trying \" + data_dir + \" instead.\")\n            data_path = data_dir\n        # Make sure data directory exists.\n        if not (os.path.exists(data_path)) and data_dir != kDEFAULT_DATA_ROOT:\n            print(\n                \"WARNING: {:} does not exist. Please provide the correct data path with the -d option.\".format(\n                    data_path\n                )\n            )\n        return data_path\n\n    data_paths = [get_data_path(data_dir) for data_dir in args.datadir]\n    return data_paths, locate_files(data_paths, find_files, err_msg)", "\n\ndef locate_files(data_paths, filenames, err_msg=\"\"):\n    \"\"\"\n    Locates the specified files in the specified data directories.\n    If a file exists in multiple data directories, the first directory is used.\n\n    Args:\n        data_paths (List[str]): The data directories.\n        filename (List[str]): The names of the files to find.\n\n    Returns:\n        List[str]: The absolute paths of the files.\n\n    Raises:\n        FileNotFoundError if a file could not be located.\n    \"\"\"\n    found_files = [None] * len(filenames)\n    for data_path in data_paths:\n        # Find all requested files.\n        for index, (found, filename) in enumerate(zip(found_files, filenames)):\n            if not found:\n                file_path = os.path.abspath(os.path.join(data_path, filename))\n                if os.path.exists(file_path):\n                    found_files[index] = file_path\n\n    # Check that all files were found\n    for f, filename in zip(found_files, filenames):\n        if not f or not os.path.exists(f):\n            raise FileNotFoundError(\n                \"Could not find {:}. Searched in data paths: {:}\\n{:}\".format(filename, data_paths, err_msg)\n            )\n    return found_files", "\n\n# Simple helper data class that's a little nicer to use than a 2-tuple.\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n\n    def __str__(self):\n        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()", "\n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(engine):\n    inputs = []\n    outputs = []\n    bindings = []\n    stream = cuda.Stream()\n    for binding in engine:\n        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        # Allocate host and device buffers\n        host_mem = cuda.pagelocked_empty(size, dtype)\n        device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings, stream", "\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]", "\n\n# This function is generalized for multiple inputs/outputs for full dimension networks.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference_v2(context, bindings, inputs, outputs, stream):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]"]}
{"filename": "test.py", "chunked_list": ["from glob import glob\nimport argparse\nimport string\nimport sys\nimport os\nfrom dataclasses import dataclass\nfrom typing import List\nfrom omegaconf import DictConfig, open_dict, OmegaConf\nfrom tqdm import tqdm\nimport torch", "from tqdm import tqdm\nimport torch\nimport string\nfrom data.module import SceneTextDataModule\nfrom model.parseq import PARSeq\n\n@dataclass\nclass Result:\n    dataset: str\n    num_samples: int\n    accuracy: float\n    ned: float\n    confidence: float\n    label_length: float", "\n\ndef print_results_table(results: List[Result], file=None):\n    w = max(map(len, map(getattr, results, ['dataset'] * len(results))))\n    w = max(w, len('Dataset'), len('Combined'))\n    print('| {:<{w}} | # samples | Accuracy | 1 - NED | Confidence | Label Length |'.format('Dataset', w=w), file=file)\n    print('|:{:-<{w}}:|----------:|---------:|--------:|-----------:|-------------:|'.format('----', w=w), file=file)\n    c = Result('Combined', 0, 0, 0, 0, 0)\n    for res in results:\n        c.num_samples += res.num_samples\n        c.accuracy += res.num_samples * res.accuracy\n        c.ned += res.num_samples * res.ned\n        c.confidence += res.num_samples * res.confidence\n        c.label_length += res.num_samples * res.label_length\n        print(f'| {res.dataset:<{w}} | {res.num_samples:>9} | {res.accuracy:>8.2f} | {res.ned:>7.2f} '\n              f'| {res.confidence:>10.2f} | {res.label_length:>12.2f} |', file=file)\n    c.accuracy /= c.num_samples\n    c.ned /= c.num_samples\n    c.confidence /= c.num_samples\n    c.label_length /= c.num_samples\n    print('|-{:-<{w}}-|-----------|----------|---------|------------|--------------|'.format('----', w=w), file=file)\n    print(f'| {c.dataset:<{w}} | {c.num_samples:>9} | {c.accuracy:>8.2f} | {c.ned:>7.2f} '\n          f'| {c.confidence:>10.2f} | {c.label_length:>12.2f} |', file=file)", "\n\n@torch.inference_mode()\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('config', help='path of baseline or CR-based self-supervised config')\n    parser.add_argument('--checkpoint', default= '', help=\"Model checkpoint (or 'pretrained=<model_id>')\")\n    #parser.add_argument('--korean_chars', type=str, default='./configs/korean_charset.txt', help='korean characters list text file')\n    parser.add_argument('--data_root', default='data')\n    parser.add_argument('--batch_size', type=int, default=512)\n    parser.add_argument('--num_workers', type=int, default=4)\n    parser.add_argument('--cased', action='store_true', default=False, help='Cased comparison')\n    parser.add_argument('--punctuation', action='store_true', default=False, help='Check punctuation')\n    parser.add_argument('--new', action='store_true', default=False, help='Evaluate on new benchmark datasets')\n    parser.add_argument('--rotation', type=int, default=0, help='Angle of rotation (counter clockwise) in degrees.')\n    parser.add_argument('--device', default='cuda:0')\n    args = parser.parse_args()\n\n    config = OmegaConf.load(args.config)\n    if not args.checkpoint:\n        print('\\nDEFAULT CHECKPOINT PATH SET TO CONFIG LOG_DIR.\\n')\n        if config.data.consistency_regularization:\n            args.checkpoint = os.path.join(config.log_dir, 'CR_best_parseq_ckpt')\n        else:\n            args.checkpoint = os.path.join(config.log_dir, 'Baseline_best_parseq_ckpt')\n    print('CHECKPOINT PATH: ', args.checkpoint)\n\n    # updating korean charset\n    chars = \"\"\n    with open(\n        os.path.join(config.data_loader.character.dict_dir, \"charset.txt\"), \"r\"\n    ) as f:\n        curr_char = f.read()\n    curr_char = curr_char.strip()\n    chars += curr_char\n    chars = \"\".join(sorted(set(chars)))\n    \n    config.model.charset_train = chars\n    config.model.charset_test = config.model.charset_train\n    \n    # Special handling for PARseq\n    if config.model.get('perm_mirrored', False):\n        assert config.model.perm_num % 2 == 0, 'perm_num should be even if perm_mirrored = True'\n    \n    args.data_root = 'dataset'\n    \n    model = PARSeq(**config.model)\n    model.load_state_dict(torch.load(args.checkpoint)['model'])\n    model._device = args.device\n    model = model.eval().to(args.device)\n\n    print('Model parameters: ', config.model, sep='\\n')\n    \n    train_dir = os.path.join(args.data_root, 'train')\n    val_dir = os.path.join(args.data_root, 'valid')\n    test_dir = os.path.join(args.data_root, 'valid')\n    \n    datamodule = SceneTextDataModule(root_dir = args.data_root, \n                                      train_dir = train_dir, \n                                      val_dir = val_dir,\n                                      test_dir = test_dir,\n                                      img_size = config.data.img_size,\n                                      max_label_length = config.data.max_label_length,\n                                      charset_train = config.data.charset_test, # hp.charset_train,\n                                      charset_test = config.data.charset_test, # hp.charset_test,\n                                      batch_size = args.batch_size,\n                                      num_workers = args.num_workers,\n                                      remove_whitespace = False, \n                                      normalize_unicode = False,\n                                      augment = False,\n                                      rotation = args.rotation\n                                      )\n\n\n    test_folders = glob(os.path.join(test_dir, '*'))\n    test_set= sorted(set([t.split('/')[-1] for t in test_folders]))\n    \n    results = {}\n    max_width = max(map(len, test_set))\n    for name, dataloader in datamodule.test_dataloaders(test_set).items():\n        total = 0\n        correct = 0\n        ned = 0\n        confidence = 0\n        label_length = 0\n        for imgs, labels in tqdm(iter(dataloader), desc=f'{name:>{max_width}}'): # f'{name:>{max_width}}'\n            res = model.test_step((imgs.to(args.device), labels), -1)['output']\n            total += res.num_samples\n            correct += res.correct\n            ned += res.ned\n            confidence += res.confidence\n            label_length += res.label_length\n        accuracy = 100 * correct / total\n        mean_ned = 100 * (1 - ned / total)\n        mean_conf = 100 * confidence / total\n        mean_label_length = label_length / total\n        results[name] = Result(name, total, accuracy, mean_ned, mean_conf, mean_label_length)\n    \n    result_groups = {\n        t : [t] for t in test_set\n    }\n\n    if args.new:\n        result_groups.update({'New': SceneTextDataModule.TEST_NEW})\n    save_folder = os.path.join('/'.join(args.checkpoint.split('/')[:-1]), 'test_logs')\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    with open(os.path.join(save_folder, os.path.basename(args.checkpoint)[:-4] + '.log.txt'), 'w') as f:\n        for out in [f, sys.stdout]:\n            for group, subset in result_groups.items():\n                print(f'{group} set:', file=out)\n                print_results_table([results[s] for s in subset], out)  \n                print('\\n', file=out)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "onnx2trt.py", "chunked_list": ["\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorrt as trt\nimport pycuda.driver as cuda\n\n# Use autoprimaryctx if available (pycuda >= 2021.1) to\n# prevent issues with other modules that rely on the primary\n# device context.\ntry:\n    import pycuda.autoprimaryctx\nexcept ModuleNotFoundError:\n    import pycuda.autoinit", "# prevent issues with other modules that rely on the primary\n# device context.\ntry:\n    import pycuda.autoprimaryctx\nexcept ModuleNotFoundError:\n    import pycuda.autoinit\n\nimport sys, os\n\nsys.path.insert(1, os.path.join(sys.path[0], \"..\"))", "\nsys.path.insert(1, os.path.join(sys.path[0], \"..\"))\nimport common\n# from downloader import getFilePath\n\nTRT_LOGGER = trt.Logger()\n\nimport torch\nfrom torchvision.transforms import Normalize\n", "from torchvision.transforms import Normalize\n\nfrom skimage import io\nfrom skimage.transform import resize\nfrom skimage.color import gray2rgb\nimport matplotlib.pyplot as plt\n\nfrom model.tokenizer_utils import Tokenizer\n\ndef preprocess_image(img):\n    norm = Normalize(0.5, 0.5)\n    result = norm(torch.from_numpy(img).transpose(0,2).transpose(1,2))\n    return np.array(result, dtype=np.float16)", "\ndef preprocess_image(img):\n    norm = Normalize(0.5, 0.5)\n    result = norm(torch.from_numpy(img).transpose(0,2).transpose(1,2))\n    return np.array(result, dtype=np.float16)\n\ndef get_engine(onnx_file_path, engine_file_path=\"\"):\n    \"\"\"Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it.\"\"\"\n\n    def build_engine():\n        \"\"\"Takes an ONNX file and creates a TensorRT engine to run inference with\"\"\"\n        with trt.Builder(TRT_LOGGER) as builder, builder.create_network(\n            common.EXPLICIT_BATCH\n        ) as network, builder.create_builder_config() as config, trt.OnnxParser(\n            network, TRT_LOGGER\n        ) as parser, trt.Runtime(\n            TRT_LOGGER\n        ) as runtime:\n            config.max_workspace_size = 1 << 28  # 256MiB\n            builder.max_batch_size = 1\n            # Parse model file\n            if not os.path.exists(onnx_file_path):\n                print(\n                    \"ONNX file {} not found, please run yolov3_to_onnx.py first to generate it.\".format(onnx_file_path)\n                )\n                exit(0)\n            print(\"Loading ONNX file from path {}...\".format(onnx_file_path))\n            with open(onnx_file_path, \"rb\") as model:\n                print(\"Beginning ONNX file parsing\")\n                if not parser.parse(model.read()):\n                    print(\"ERROR: Failed to parse the ONNX file.\")\n                    for error in range(parser.num_errors):\n                        print(parser.get_error(error))\n                    return None\n            # The actual yolov3.onnx is generated with batch size 64. Reshape input to batch size 1\n            network.get_input(0).shape = [1, 3, 32, 128]\n            print(\"Completed parsing of ONNX file\")\n            print(\"Building an engine from file {}; this may take a while...\".format(onnx_file_path))\n            plan = builder.build_serialized_network(network, config)\n            engine = runtime.deserialize_cuda_engine(plan)\n            print(\"Completed creating Engine\")\n            with open(engine_file_path, \"wb\") as f:\n                f.write(plan)\n            return engine\n\n    if os.path.exists(engine_file_path):\n        # If a serialized engine exists, use it instead of building an engine.\n        print(\"Reading engine from file {}\".format(engine_file_path))\n        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n            return runtime.deserialize_cuda_engine(f.read())\n    else:\n        return build_engine()", "\n\ndef main():\n    \"\"\"Create a TensorRT engine for ONNX-based YOLOv3-608 and run inference.\"\"\"\n\n    # Try to load a previously generated YOLOv3-608 network graph in ONNX format:\n    onnx_file_path = \"parseq_ar_r1.onnx\"\n    engine_file_path = \"parseq_ar_r1.trt\"\n#     test_folder_path = ''\n    \n    # Download a dog image and save it to the following file path:\n#     input_image_path = getFilePath(\"samples/python/yolov3_onnx/dog.jpg\")\n\n    img_path = 'sample.png'\n    img = resize(gray2rgb(io.imread(img_path)), (32, 128))\n    input_batch = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), 1, axis=0), dtype=np.float32)\n    print('input shape: ', input_batch.shape)\n    preprocessed_images = np.array([preprocess_image(image) for image in input_batch])\n\n    # Do inference with TensorRT\n    trt_outputs = []\n    with get_engine(onnx_file_path, engine_file_path) as engine, engine.create_execution_context() as context:\n        inputs, outputs, bindings, stream = common.allocate_buffers(engine)\n        # Do inference\n        print(\"Running inference on image {}...\".format(img_path))\n        # Set host input to the image. The common.do_inference function will copy the input to the GPU before executing.\n        inputs[0].host = preprocessed_images\n        trt_outputs = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n    trt_outputs = [out.reshape(1, 26, 1794) for out in trt_outputs]\n    print('Type of Generated outputs: ', type(trt_outputs))\n    print('Len of Generated outputs: ', (trt_outputs[0].shape))\n    print('Generated outputs: ', trt_outputs)\n    \n    pred_labels = post_process(trt_outputs)\n    print('Final results: ', pred_labels)", "\n    \ndef post_process(trt_outputs):\n    with open('./char_dicts/charset.txt', 'r') as f:\n        charset = f.read()\n    tokenizer = Tokenizer(charset)\n    trt_logits = [torch.nn.functional.softmax(torch.from_numpy(out), dim=-1) for out in trt_outputs]\n    final_outputs = []\n    for logit in trt_logits:\n        preds, probs = tokenizer.decode(logit)\n        final_outputs.append([preds[0], probs])\n        \n    return final_outputs", "if __name__ == \"__main__\":\n    main()"]}
{"filename": "data/transforms.py", "chunked_list": ["import math\nimport numbers\nimport random\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.transforms import Compose\n", "from torchvision.transforms import Compose\n\n\nclass ImageToArray(object):\n\n    def __call__(self, img):\n        return np.array(img)\n\n\nclass ImageToPIL(object):\n\n    def __call__(self, img):\n        return Image.fromarray(img)", "\nclass ImageToPIL(object):\n\n    def __call__(self, img):\n        return Image.fromarray(img)\n\n\ndef sample_asym(magnitude, size=None):\n    return np.random.beta(1, 4, size) * magnitude\n", "\n\ndef sample_sym(magnitude, size=None):\n    return (np.random.beta(4, 4, size=size) - 0.5) * 2 * magnitude\n\n\ndef sample_uniform(low, high, size=None):\n    return np.random.uniform(low, high, size=size)\n\n\ndef get_interpolation(type='random'):\n    if type == 'random':\n        choice = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA]\n        interpolation = choice[random.randint(0, len(choice) - 1)]\n    elif type == 'nearest':\n        interpolation = cv2.INTER_NEAREST\n    elif type == 'linear':\n        interpolation = cv2.INTER_LINEAR\n    elif type == 'cubic':\n        interpolation = cv2.INTER_CUBIC\n    elif type == 'area':\n        interpolation = cv2.INTER_AREA\n    else:\n        raise TypeError('Interpolation types only nearest, linear, cubic, area are supported!')\n    return interpolation", "\n\ndef get_interpolation(type='random'):\n    if type == 'random':\n        choice = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA]\n        interpolation = choice[random.randint(0, len(choice) - 1)]\n    elif type == 'nearest':\n        interpolation = cv2.INTER_NEAREST\n    elif type == 'linear':\n        interpolation = cv2.INTER_LINEAR\n    elif type == 'cubic':\n        interpolation = cv2.INTER_CUBIC\n    elif type == 'area':\n        interpolation = cv2.INTER_AREA\n    else:\n        raise TypeError('Interpolation types only nearest, linear, cubic, area are supported!')\n    return interpolation", "\n\nclass CVRandomRotation(object):\n    def __init__(self, degrees=15):\n        assert isinstance(degrees, numbers.Number), \"degree should be a single number.\"\n        assert degrees >= 0, \"degree must be positive.\"\n        self.degrees = degrees\n\n    @staticmethod\n    def get_params(degrees):\n        return sample_sym(degrees)\n\n    def __call__(self, img):\n        angle = self.get_params(self.degrees)\n        src_h, src_w = img.shape[:2]\n        M = cv2.getRotationMatrix2D(center=(src_w / 2, src_h / 2), angle=angle, scale=1.0)\n        abs_cos, abs_sin = abs(M[0, 0]), abs(M[0, 1])\n        dst_w = int(src_h * abs_sin + src_w * abs_cos)\n        dst_h = int(src_h * abs_cos + src_w * abs_sin)\n        M[0, 2] += (dst_w - src_w) / 2\n        M[1, 2] += (dst_h - src_h) / 2\n\n        flags = get_interpolation()\n        return cv2.warpAffine(img, M, (dst_w, dst_h), flags=flags, borderMode=cv2.BORDER_REPLICATE)", "\n\nclass CVRandomAffine(object):\n    def __init__(self, degrees, translate=None, scale=None, shear=None):\n        assert isinstance(degrees, numbers.Number), \"degree should be a single number.\"\n        assert degrees >= 0, \"degree must be positive.\"\n        self.degrees = degrees\n\n        if translate is not None:\n            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n                \"translate should be a list or tuple and it must be of length 2.\"\n            for t in translate:\n                if not (0.0 <= t <= 1.0):\n                    raise ValueError(\"translation values should be between 0 and 1\")\n        self.translate = translate\n\n        if scale is not None:\n            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n                \"scale should be a list or tuple and it must be of length 2.\"\n            for s in scale:\n                if s <= 0:\n                    raise ValueError(\"scale values should be positive\")\n        self.scale = scale\n\n        if shear is not None:\n            if isinstance(shear, numbers.Number):\n                if shear < 0:\n                    raise ValueError(\"If shear is a single number, it must be positive.\")\n                self.shear = [shear]\n            else:\n                assert isinstance(shear, (tuple, list)) and (len(shear) == 2), \\\n                    \"shear should be a list or tuple and it must be of length 2.\"\n                self.shear = shear\n        else:\n            self.shear = shear\n\n    def _get_inverse_affine_matrix(self, center, angle, translate, scale, shear):\n        # https://github.com/pytorch/vision/blob/v0.4.0/torchvision/transforms/functional.py#L717\n        from numpy import sin, cos, tan\n\n        if isinstance(shear, numbers.Number):\n            shear = [shear, 0]\n\n        if not isinstance(shear, (tuple, list)) and len(shear) == 2:\n            raise ValueError(\n                \"Shear should be a single value or a tuple/list containing \" +\n                \"two values. Got {}\".format(shear))\n\n        rot = math.radians(angle)\n        sx, sy = [math.radians(s) for s in shear]\n\n        cx, cy = center\n        tx, ty = translate\n\n        # RSS without scaling\n        a = cos(rot - sy) / cos(sy)\n        b = -cos(rot - sy) * tan(sx) / cos(sy) - sin(rot)\n        c = sin(rot - sy) / cos(sy)\n        d = -sin(rot - sy) * tan(sx) / cos(sy) + cos(rot)\n\n        # Inverted rotation matrix with scale and shear\n        # det([[a, b], [c, d]]) == 1, since det(rotation) = 1 and det(shear) = 1\n        M = [d, -b, 0,\n             -c, a, 0]\n        M = [x / scale for x in M]\n\n        # Apply inverse of translation and of center translation: RSS^-1 * C^-1 * T^-1\n        M[2] += M[0] * (-cx - tx) + M[1] * (-cy - ty)\n        M[5] += M[3] * (-cx - tx) + M[4] * (-cy - ty)\n\n        # Apply center translation: C * RSS^-1 * C^-1 * T^-1\n        M[2] += cx\n        M[5] += cy\n        return M\n\n    @staticmethod\n    def get_params(degrees, translate, scale_ranges, shears, height):\n        angle = sample_sym(degrees)\n        if translate is not None:\n            max_dx = translate[0] * height\n            max_dy = translate[1] * height\n            translations = (np.round(sample_sym(max_dx)), np.round(sample_sym(max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = sample_uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            if len(shears) == 1:\n                shear = [sample_sym(shears[0]), 0.]\n            elif len(shears) == 2:\n                shear = [sample_sym(shears[0]), sample_sym(shears[1])]\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear\n\n    def __call__(self, img):\n        src_h, src_w = img.shape[:2]\n        angle, translate, scale, shear = self.get_params(\n            self.degrees, self.translate, self.scale, self.shear, src_h)\n\n        M = self._get_inverse_affine_matrix((src_w / 2, src_h / 2), angle, (0, 0), scale, shear)\n        M = np.array(M).reshape(2, 3)\n\n        startpoints = [(0, 0), (src_w - 1, 0), (src_w - 1, src_h - 1), (0, src_h - 1)]\n        project = lambda x, y, a, b, c: int(a * x + b * y + c)\n        endpoints = [(project(x, y, *M[0]), project(x, y, *M[1])) for x, y in startpoints]\n\n        rect = cv2.minAreaRect(np.array(endpoints))\n        bbox = cv2.boxPoints(rect).astype(dtype=np.int)\n        max_x, max_y = bbox[:, 0].max(), bbox[:, 1].max()\n        min_x, min_y = bbox[:, 0].min(), bbox[:, 1].min()\n\n        dst_w = int(max_x - min_x)\n        dst_h = int(max_y - min_y)\n        M[0, 2] += (dst_w - src_w) / 2\n        M[1, 2] += (dst_h - src_h) / 2\n\n        # add translate\n        dst_w += int(abs(translate[0]))\n        dst_h += int(abs(translate[1]))\n        if translate[0] < 0: M[0, 2] += abs(translate[0])\n        if translate[1] < 0: M[1, 2] += abs(translate[1])\n\n        flags = get_interpolation()\n        return cv2.warpAffine(img, M, (dst_w, dst_h), flags=flags, borderMode=cv2.BORDER_REPLICATE)", "\n\nclass CVRandomPerspective(object):\n    def __init__(self, distortion=0.5):\n        self.distortion = distortion\n\n    def get_params(self, width, height, distortion):\n        offset_h = sample_asym(distortion * height / 2, size=4).astype(dtype=np.int)\n        offset_w = sample_asym(distortion * width / 2, size=4).astype(dtype=np.int)\n        topleft = (offset_w[0], offset_h[0])\n        topright = (width - 1 - offset_w[1], offset_h[1])\n        botright = (width - 1 - offset_w[2], height - 1 - offset_h[2])\n        botleft = (offset_w[3], height - 1 - offset_h[3])\n\n        startpoints = [(0, 0), (width - 1, 0), (width - 1, height - 1), (0, height - 1)]\n        endpoints = [topleft, topright, botright, botleft]\n        return np.array(startpoints, dtype=np.float32), np.array(endpoints, dtype=np.float32)\n\n    def __call__(self, img):\n        height, width = img.shape[:2]\n        startpoints, endpoints = self.get_params(width, height, self.distortion)\n        M = cv2.getPerspectiveTransform(startpoints, endpoints)\n\n        # TODO: more robust way to crop image\n        rect = cv2.minAreaRect(endpoints)\n        bbox = cv2.boxPoints(rect).astype(dtype=np.int)\n        max_x, max_y = bbox[:, 0].max(), bbox[:, 1].max()\n        min_x, min_y = bbox[:, 0].min(), bbox[:, 1].min()\n        min_x, min_y = max(min_x, 0), max(min_y, 0)\n\n        flags = get_interpolation()\n        img = cv2.warpPerspective(img, M, (max_x, max_y), flags=flags, borderMode=cv2.BORDER_REPLICATE)\n        img = img[min_y:, min_x:]\n        return img", "\n\nclass CVRescale(object):\n\n    def __init__(self, factor=4, base_size=(128, 512)):\n        \"\"\" Define image scales using gaussian pyramid and rescale image to target scale.\n        \n        Args:\n            factor: the decayed factor from base size, factor=4 keeps target scale by default.\n            base_size: base size the build the bottom layer of pyramid\n        \"\"\"\n        if isinstance(factor, numbers.Number):\n            self.factor = round(sample_uniform(0, factor))\n        elif isinstance(factor, (tuple, list)) and len(factor) == 2:\n            self.factor = round(sample_uniform(factor[0], factor[1]))\n        else:\n            raise Exception('factor must be number or list with length 2')\n        # assert factor is valid\n        self.base_h, self.base_w = base_size[:2]\n\n    def __call__(self, img):\n        if self.factor == 0: return img\n        src_h, src_w = img.shape[:2]\n        cur_w, cur_h = self.base_w, self.base_h\n        scale_img = cv2.resize(img, (cur_w, cur_h), interpolation=get_interpolation())\n        for _ in range(self.factor):\n            scale_img = cv2.pyrDown(scale_img)\n        scale_img = cv2.resize(scale_img, (src_w, src_h), interpolation=get_interpolation())\n        return scale_img", "\n\nclass CVGaussianNoise(object):\n    def __init__(self, mean=0, var=20):\n        self.mean = mean\n        if isinstance(var, numbers.Number):\n            self.var = max(int(sample_asym(var)), 1)\n        elif isinstance(var, (tuple, list)) and len(var) == 2:\n            self.var = int(sample_uniform(var[0], var[1]))\n        else:\n            raise Exception('degree must be number or list with length 2')\n\n    def __call__(self, img):\n        noise = np.random.normal(self.mean, self.var ** 0.5, img.shape)\n        img = np.clip(img + noise, 0, 255).astype(np.uint8)\n        return img", "\n\nclass CVMotionBlur(object):\n    def __init__(self, degrees=12, angle=90):\n        if isinstance(degrees, numbers.Number):\n            self.degree = max(int(sample_asym(degrees)), 1)\n        elif isinstance(degrees, (tuple, list)) and len(degrees) == 2:\n            self.degree = int(sample_uniform(degrees[0], degrees[1]))\n        else:\n            raise Exception('degree must be number or list with length 2')\n        self.angle = sample_uniform(-angle, angle)\n\n    def __call__(self, img):\n        M = cv2.getRotationMatrix2D((self.degree // 2, self.degree // 2), self.angle, 1)\n        motion_blur_kernel = np.zeros((self.degree, self.degree))\n        motion_blur_kernel[self.degree // 2, :] = 1\n        motion_blur_kernel = cv2.warpAffine(motion_blur_kernel, M, (self.degree, self.degree))\n        motion_blur_kernel = motion_blur_kernel / self.degree\n        img = cv2.filter2D(img, -1, motion_blur_kernel)\n        img = np.clip(img, 0, 255).astype(np.uint8)\n        return img", "\n\nclass CVGeometry(object):\n    def __init__(self, degrees=15, translate=(0.3, 0.3), scale=(0.5, 2.),\n                 shear=(45, 15), distortion=0.5, p=0.5):\n        self.p = p\n        type_p = random.random()\n        if type_p < 0.33:\n            self.transforms = CVRandomRotation(degrees=degrees)\n        elif type_p < 0.66:\n            self.transforms = CVRandomAffine(degrees=degrees, translate=translate, scale=scale, shear=shear)\n        else:\n            self.transforms = CVRandomPerspective(distortion=distortion)\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            img = np.array(img)\n            return Image.fromarray(self.transforms(img))\n        else:\n            return img", "\n\nclass CVDeterioration(object):\n    def __init__(self, var, degrees, factor, p=0.5):\n        self.p = p\n        transforms = []\n        if var is not None:\n            transforms.append(CVGaussianNoise(var=var))\n        if degrees is not None:\n            transforms.append(CVMotionBlur(degrees=degrees))\n        if factor is not None:\n            transforms.append(CVRescale(factor=factor))\n\n        random.shuffle(transforms)\n        transforms = Compose(transforms)\n        self.transforms = transforms\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            img = np.array(img)\n            return Image.fromarray(self.transforms(img))\n        else:\n            return img", "\n\nclass CVColorJitter(object):\n    def __init__(self, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1, p=0.5):\n        self.p = p\n        self.transforms = transforms.ColorJitter(brightness=brightness, contrast=contrast,\n                                                 saturation=saturation, hue=hue)\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return self.transforms(img)\n        else:\n            return img", "\n\nclass ImageToArray(object):\n\n    def __call__(self, img):\n        return np.array(img)\n\n\nclass ImageToPIL(object):\n\n    def __call__(self, img):\n        return Image.fromarray(img)", "class ImageToPIL(object):\n\n    def __call__(self, img):\n        return Image.fromarray(img)\n"]}
{"filename": "data/dataset.py", "chunked_list": ["import glob\nimport io\nimport logging\nimport unicodedata\nfrom pathlib import Path, PurePath\nfrom typing import Callable, Optional, Union, Tuple, List\n\nimport random\nimport numpy as np\nimport re", "import numpy as np\nimport re\nimport torch\nimport lmdb\nfrom PIL import Image\nimport cv2\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision import transforms\n\nfrom data.utils import CharsetAdapter", "\nfrom data.utils import CharsetAdapter\nfrom data.transforms import ImageToArray, ImageToPIL, CVColorJitter, CVDeterioration, CVGeometry\n\nlog = logging.getLogger(__name__)\n\ndef build_tree_dataset(root: Union[PurePath, str], *args, **kwargs):\n    try:\n        kwargs.pop('root')  # prevent 'root' from being passed via kwargs\n    except KeyError:\n        pass\n    root = Path(root).absolute()\n    log.info(f'dataset root:\\t{root}')\n\n    consistency_regularization = kwargs.get('consistency_regularization', None)\n    exclude_folder = kwargs.get('exclude_folder', [])\n    try :\n        kwargs.pop('exclude_folder')\n    except KeyError : # prevent 'exclude_folder' passed to Dataset\n        pass \n\n\n    datasets = []\n    for mdb in glob.glob(str(root / '**/data.mdb'), recursive=True):\n        mdb = Path(mdb)\n        ds_name = str(mdb.parent.relative_to(root))\n        ds_root = str(mdb.parent.absolute())\n\n        if str(mdb.parts[-2]) in exclude_folder : # child folder name in exclude_folder e.g) Vertical\n            continue\n\n        if consistency_regularization :\n            dataset = ConsistencyRegulariationLmdbDataset(ds_root, *args, **kwargs)\n        else : \n            dataset = LmdbDataset(ds_root, *args, **kwargs)\n        log.info(f'\\tlmdb:\\t{ds_name}\\tnum samples: {len(dataset)}')\n        print(f'\\tlmdb:\\t{ds_name}\\tnum samples: {len(dataset)}')\n        datasets.append(dataset)\n    return ConcatDataset(datasets)", "\nclass LmdbDataset(Dataset):\n    \"\"\"Dataset interface to an LMDB database.\n\n    It supports both labelled and unlabelled datasets. For unlabelled datasets, the image index itself is returned\n    as the label. Unicode characters are normalized by default. Case-sensitivity is inferred from the charset.\n    Labels are transformed according to the charset.\n    \"\"\"\n\n    def __init__(self, root: str, charset: str, max_label_len: int, min_image_dim: int = 0,\n                 remove_whitespace: bool = True, normalize_unicode: bool = True,\n                 unlabelled: bool = False, transform: Optional[Callable] = None,\n                 limit_size: bool = False, size_of_limit: Optional[int] = None, img_size: Optional[Tuple] = (32,128), \n                 consistency_regularization = False, is_training: bool = False, twinreader_folders: Optional[List] = []):\n        self._env = None\n        self.root = root\n        self.unlabelled = unlabelled\n        self.transform = transform\n        self.labels = []\n        self.filtered_index_list = []\n        self.index_list = []\n        self.img_size = img_size\n        self.limit_size = limit_size\n        self.size_of_limit = size_of_limit\n        self.min_image_dim = min_image_dim\n        self.normalize_unicode = normalize_unicode\n        self.remove_whitespace = remove_whitespace\n        self.max_label_len = max_label_len\n        self.is_training = is_training\n        self.charset_adapter = CharsetAdapter(charset)\n        \n        if self.limit_size and self.root.split('/')[-1] in twinreader_folders:\n            self.limit_size = False\n        \n        self.consistency_regularization = consistency_regularization\n        \n        if not self.is_training:\n            self.num_samples = self._preprocess_labels(charset, remove_whitespace, normalize_unicode,\n                                                   max_label_len, min_image_dim)\n        else:\n            self.num_samples = self.train_num_samples()\n            \n    def train_num_samples(self):\n        with self._create_env() as env, env.begin() as txn:\n            num_samples = int(txn.get('num-samples'.encode()))\n            if self.unlabelled:\n                return num_samples\n            self.index_list = range(num_samples)\n            if self.limit_size :\n                self.index_list = np.random.permutation(self.index_list)[:self.size_of_limit]\n            return len(self.index_list)\n\n    def __del__(self):\n        if self._env is not None:\n            self._env.close()\n            self._env = None\n\n    def _create_env(self):\n        return lmdb.open(self.root, max_readers=1, readonly=True, create=False,\n                         readahead=False, meminit=False, lock=False)\n\n    @property\n    def env(self):\n        if self._env is None:\n            self._env = self._create_env()\n        return self._env\n\n    def _preprocess_labels(self, charset, remove_whitespace, normalize_unicode, max_label_len, min_image_dim):\n        charset_adapter = CharsetAdapter(charset)\n        with self._create_env() as env, env.begin() as txn:\n            num_samples = int(txn.get('num-samples'.encode()))\n            if self.unlabelled:\n                return num_samples\n            index_list = range(num_samples)\n            if self.limit_size :\n                index_list = np.random.permutation(index_list)[:self.limit_size]\n                \n            for index in index_list : #range(num_samples):\n                index += 1  # lmdb starts with 1\n                label_key = f'label-{index:09d}'.encode()\n                label = txn.get(label_key).decode()\n                #\n                label = edit_label(label, charset_adapter.charset) # edit self.charset in CharsetAdapter\n                if len(label) > max_label_len:\n                    continue\n                label = charset_adapter(label)\n                # We filter out samples which don't contain any supported characters\n                if not label:\n                    continue\n                # Filter images that are too small.\n                if self.min_image_dim > 0:\n                    img_key = f'image-{index:09d}'.encode()\n                    buf = io.BytesIO(txn.get(img_key))\n                    w, h = Image.open(buf).size\n                    if w < self.min_image_dim or h < self.min_image_dim:\n                        continue\n                self.labels.append(label)\n                self.filtered_index_list.append(index)\n        return len(self.labels)\n\n    def __len__(self):\n        return self.num_samples\n    \n    def next_sample(self):\n        next_index = random.randint(0, len(self) - 1)\n        if self.limit_size:\n            next_index = self.index_list[next_index]\n        return self.get(next_index)\n    \n    def get(self, index):\n        index += 1  # lmdb starts with 1\n        with self._create_env() as env, env.begin() as txn:\n            label_key = f'label-{index:09d}'.encode()\n            label = txn.get(label_key).decode()\n            #\n            label = edit_label(label, self.charset_adapter.charset) # edit self.charset in CharsetAdapter\n            if len(label) > self.max_label_len:\n                return self.next_sample()\n            label = self.charset_adapter(label)\n            # We filter out samples which don't contain any supported characters\n            \n            if not label:\n                return self.next_sample()\n\n            # Filter images that are too small.\n            img_key = f'image-{index:09d}'.encode()\n            buf = io.BytesIO(txn.get(img_key))\n            img = Image.open(buf).convert('RGB')\n            if self.min_image_dim > 0:\n                w, h = img.size\n    #             w, h = Image.open(buf).size\n                if w < self.min_image_dim or h < self.min_image_dim:\n                    return self.next_sample()\n            img=np.array(img)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img = np.dstack([img,img,img])\n            img=Image.fromarray(img)        \n            w,h=img.size\n            # vertical data handling\n            if h/w>2.5:\n                img=img.rotate(90, expand=True)\n            if self.transform is not None:\n                img = self.transform(img)\n            \n        return img, label\n\n    def __getitem__(self, index):\n        if self.is_training:\n            if self.limit_size:\n                index = self.index_list[index]\n            return self.get(index)\n        \n        if self.unlabelled:\n            label = index\n        else:\n            label = self.labels[index]\n            index = self.filtered_index_list[index]\n\n        img_key = f'image-{index:09d}'.encode()\n        with self.env.begin() as txn:\n            imgbuf = txn.get(img_key)\n        buf = io.BytesIO(imgbuf)\n        img = Image.open(buf).convert('RGB')\n        img=np.array(img)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = np.dstack([img,img,img])\n        img=Image.fromarray(img)        \n        w,h=img.size\n        # vertical data handling\n        if h/w>2.5:\n            img=img.rotate(90, expand=True)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, label", "\ndef edit_label(label, char_list) :\n################################## New editions\n#     match=re.findall(r'\\[UNK[0-9]*\\]',label)\n#     if match:\n#         for mat in match: label=label.replace(mat,'[UNK]')\n    out_of_char = r'[^{}]'.format(re.escape(''.join(char_list)))\n    label = label.replace('###','[UNK]')\n    label = re.sub(out_of_char, \"[UNK]\", label)\n\n    return label", "\n\n"]}
{"filename": "data/__init__.py", "chunked_list": [""]}
{"filename": "data/aa_overrides.py", "chunked_list": ["\"\"\"Extends default ops to accept optional parameters.\"\"\"\nfrom functools import partial\n\nfrom timm.data.auto_augment import _LEVEL_DENOM, _randomly_negate, LEVEL_TO_ARG, NAME_TO_OP, rotate\n\n\ndef rotate_expand(img, degrees, **kwargs):\n    \"\"\"Rotate operation with expand=True to avoid cutting off the characters\"\"\"\n    kwargs['expand'] = True\n    return rotate(img, degrees, **kwargs)", "\n\ndef _level_to_arg(level, hparams, key, default):\n    magnitude = hparams.get(key, default)\n    level = (level / _LEVEL_DENOM) * magnitude\n    level = _randomly_negate(level)\n    return level,\n\n\ndef apply():\n    # Overrides\n    NAME_TO_OP.update({\n        'Rotate': rotate_expand\n    })\n    LEVEL_TO_ARG.update({\n        'Rotate': partial(_level_to_arg, key='rotate_deg', default=30.),\n        'ShearX': partial(_level_to_arg, key='shear_x_pct', default=0.3),\n        'ShearY': partial(_level_to_arg, key='shear_y_pct', default=0.3),\n        'TranslateXRel': partial(_level_to_arg, key='translate_x_pct', default=0.45),\n        'TranslateYRel': partial(_level_to_arg, key='translate_y_pct', default=0.45),\n    })", "\ndef apply():\n    # Overrides\n    NAME_TO_OP.update({\n        'Rotate': rotate_expand\n    })\n    LEVEL_TO_ARG.update({\n        'Rotate': partial(_level_to_arg, key='rotate_deg', default=30.),\n        'ShearX': partial(_level_to_arg, key='shear_x_pct', default=0.3),\n        'ShearY': partial(_level_to_arg, key='shear_y_pct', default=0.3),\n        'TranslateXRel': partial(_level_to_arg, key='translate_x_pct', default=0.45),\n        'TranslateYRel': partial(_level_to_arg, key='translate_y_pct', default=0.45),\n    })", ""]}
{"filename": "data/utils.py", "chunked_list": ["\nimport re\nfrom abc import ABC, abstractmethod\nfrom itertools import groupby\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.utils.rnn import pad_sequence\n", "from torch.nn.utils.rnn import pad_sequence\n\n\nclass CharsetAdapter:\n    \"\"\"Transforms labels according to the target charset.\"\"\"\n\n    def __init__(self, target_charset) -> None:\n        super().__init__()\n        self.charset = target_charset ###\n        self.lowercase_only = target_charset == target_charset.lower()\n        self.uppercase_only = target_charset == target_charset.upper()\n#         self.unsupported = f'[^{re.escape(target_charset)}]'\n\n    def __call__(self, label):\n        if self.lowercase_only:\n            label = label.lower()\n        elif self.uppercase_only:\n            label = label.upper()\n        # Remove unsupported characters\n#         label = re.sub(self.unsupported, '', label)\n        return label", "\n\nclass BaseTokenizer(ABC):\n\n    def __init__(self, charset: str, specials_first: tuple = (), specials_last: tuple = ()) -> None:\n        self._itos = specials_first + tuple(charset+'\u0932') + specials_last\n        self._stoi = {s: i for i, s in enumerate(self._itos)}\n\n    def __len__(self):\n        return len(self._itos)\n\n    def _tok2ids(self, tokens: str) -> List[int]:\n        return [self._stoi[s] for s in tokens]\n\n    def _ids2tok(self, token_ids: List[int], join: bool = True) -> str:\n        tokens = [self._itos[i] for i in token_ids]\n        return ''.join(tokens) if join else tokens\n\n    @abstractmethod\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        \"\"\"Encode a batch of labels to a representation suitable for the model.\n\n        Args:\n            labels: List of labels. Each can be of arbitrary length.\n            device: Create tensor on this device.\n\n        Returns:\n            Batched tensor representation padded to the max label length. Shape: N, L\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        \"\"\"Internal method which performs the necessary filtering prior to decoding.\"\"\"\n        raise NotImplementedError\n\n    def decode(self, token_dists: Tensor, raw: bool = False) -> Tuple[List[str], List[Tensor]]:\n        \"\"\"Decode a batch of token distributions.\n\n        Args:\n            token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n            raw: return unprocessed labels (will return list of list of strings)\n\n        Returns:\n            list of string labels (arbitrary length) and\n            their corresponding sequence probabilities as a list of Tensors\n        \"\"\"\n        batch_tokens = []\n        batch_probs = []\n        for dist in token_dists:\n            probs, ids = dist.max(-1)  # greedy selection\n            if not raw:\n                probs, ids = self._filter(probs, ids)\n            tokens = self._ids2tok(ids, not raw)\n            batch_tokens.append(tokens)\n            batch_probs.append(probs)\n        return batch_tokens, batch_probs", "\n\nclass Tokenizer(BaseTokenizer):\n    BOS = '[B]'\n    EOS = '[E]'\n    PAD = '[P]'\n\n    def __init__(self, charset: str) -> None:\n        specials_first = (self.EOS,)\n        specials_last = (self.BOS, self.PAD)\n        super().__init__(charset, specials_first, specials_last)\n        self.eos_id, self.bos_id, self.pad_id = [self._stoi[s] for s in specials_first + specials_last]\n\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        batch = [torch.as_tensor([self.bos_id] + self._tok2ids(y) + [self.eos_id], dtype=torch.long, device=device)\n                 for y in labels]\n        return pad_sequence(batch, batch_first=True, padding_value=self.pad_id)\n\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        ids = ids.tolist()\n        try:\n            eos_idx = ids.index(self.eos_id)\n        except ValueError:\n            eos_idx = len(ids)  # Nothing to truncate.\n        # Truncate after EOS\n        ids = ids[:eos_idx]\n        probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n        return probs, ids", "\n\nclass CTCTokenizer(BaseTokenizer):\n    BLANK = '[B]'\n\n    def __init__(self, charset: str) -> None:\n        # BLANK uses index == 0 by default\n        super().__init__(charset, specials_first=(self.BLANK,))\n        self.blank_id = self._stoi[self.BLANK]\n\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        # We use a padded representation since we don't want to use CUDNN's CTC implementation\n        batch = [torch.as_tensor(self._tok2ids(y), dtype=torch.long, device=device) for y in labels]\n        return pad_sequence(batch, batch_first=True, padding_value=self.blank_id)\n\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        # Best path decoding:\n        ids = list(zip(*groupby(ids.tolist())))[0]  # Remove duplicate tokens\n        ids = [x for x in ids if x != self.blank_id]  # Remove BLANKs\n        # `probs` is just pass-through since all positions are considered part of the path\n        return probs, ids", ""]}
{"filename": "data/module.py", "chunked_list": ["from pathlib import PurePath\nfrom typing import Optional, Callable, Sequence, Tuple, List\n\n# import pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as T\n\nfrom .dataset import build_tree_dataset, LmdbDataset\n    \nclass SceneTextDataModule():\n    def __init__(self, root_dir: str, train_dir: str, val_dir : str, test_dir : str, \n                img_size: Sequence[int],\n                max_label_length: int, \n                charset_train: str, \n                charset_test: str, \n                batch_size: int, \n                num_workers: int, \n                augment: bool, \n                remove_whitespace: bool = True, \n                normalize_unicode: bool = True,\n                min_image_dim: int = 0, \n                rotation: int = 0, \n                collate_fn: Optional[Callable] = None, \n                limit_size : bool = False , \n                size_of_limit : int = None,\n                consistency_regularization: Optional[bool] = False, \n                exclude_folder: Optional[List] = [],\n                data_weights: Optional[List] = []):\n        super().__init__()\n        self.root_dir = root_dir\n        self.train_dir = train_dir\n        self.val_dir = val_dir\n        self.test_dir = test_dir\n        self.img_size = tuple(img_size)\n        self.max_label_length = max_label_length\n        self.charset_train = charset_train\n        self.charset_test = charset_test\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.augment = augment\n        self.remove_whitespace = remove_whitespace\n        self.normalize_unicode = normalize_unicode\n        self.min_image_dim = min_image_dim\n        self.rotation = rotation\n        self.collate_fn = collate_fn\n        \n        self.limit_size = limit_size\n        self.size_of_limit = size_of_limit\n        self.consistency_regularization = consistency_regularization\n        self.exclude_folder = exclude_folder\n        self.data_weights = data_weights\n\n        self._train_dataset = None\n        self._val_dataset = None\n\n    @staticmethod\n    def get_transform(img_size: Tuple[int], augment: bool = False, rotation: int = 0, consistency_regularization: Optional[bool] = False):\n\n        if consistency_regularization :\n            from .augmentation_pipelines import get_augmentation_pipeline\n            augmentation_severity = 2 # 2 suits to document image\n            pipeline = get_augmentation_pipeline(augmentation_severity)\n            # pipeline.append(iaa.Resize(img_size))\n            return pipeline.augment_image\n\n        else :\n            transforms = []\n            if augment:\n                from .augment import rand_augment_transform\n                transforms.append(rand_augment_transform())\n            if rotation:\n                transforms.append(lambda img: img.rotate(rotation, expand=True))\n            transforms.extend([\n                T.Resize(img_size, T.InterpolationMode.BICUBIC),\n                T.ToTensor(),\n                T.Normalize(0.5, 0.5)\n            ])\n            return T.Compose(transforms)        \n        \n    @property\n    def train_dataset(self):\n        if self._train_dataset is None:\n            transform = self.get_transform(self.img_size, self.augment, consistency_regularization = self.consistency_regularization)\n            root = PurePath(self.train_dir)\n            self._train_dataset = build_tree_dataset(root, self.charset_train, self.max_label_length,\n                                                     self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                                     transform=transform, limit_size = self.limit_size, size_of_limit = self.size_of_limit,\n                                                     consistency_regularization = self.consistency_regularization,\n                                                     img_size = self.img_size, twinreader_folders = self.exclude_folder, is_training = True\n                                                     )\n        return self._train_dataset\n\n    @property\n    def val_dataset(self):\n        if self._val_dataset is None:\n            transform = self.get_transform(self.img_size)\n            root = PurePath(self.val_dir)\n            self._val_dataset = build_tree_dataset(root, self.charset_test, self.max_label_length, \n                                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                                   img_size = self.img_size,\n                                                   transform=transform, limit_size = False, is_training = False)\n        return self._val_dataset\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n                          pin_memory=True, collate_fn=self.collate_fn)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n                          pin_memory=True, collate_fn=self.collate_fn)\n\n    def test_dataloaders(self, subset):\n        transform = self.get_transform(self.img_size, rotation=self.rotation)\n        root = PurePath(self.test_dir)\n        datasets = {s: LmdbDataset(str(root / s), self.charset_test, self.max_label_length,\n                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                   transform=transform, is_training=False) for s in subset}\n        return {k: DataLoader(v, batch_size=self.batch_size, num_workers=self.num_workers,\n                              pin_memory=True, collate_fn=self.collate_fn)\n                for k, v in datasets.items()}", "    \nclass SceneTextDataModule():\n    def __init__(self, root_dir: str, train_dir: str, val_dir : str, test_dir : str, \n                img_size: Sequence[int],\n                max_label_length: int, \n                charset_train: str, \n                charset_test: str, \n                batch_size: int, \n                num_workers: int, \n                augment: bool, \n                remove_whitespace: bool = True, \n                normalize_unicode: bool = True,\n                min_image_dim: int = 0, \n                rotation: int = 0, \n                collate_fn: Optional[Callable] = None, \n                limit_size : bool = False , \n                size_of_limit : int = None,\n                consistency_regularization: Optional[bool] = False, \n                exclude_folder: Optional[List] = [],\n                data_weights: Optional[List] = []):\n        super().__init__()\n        self.root_dir = root_dir\n        self.train_dir = train_dir\n        self.val_dir = val_dir\n        self.test_dir = test_dir\n        self.img_size = tuple(img_size)\n        self.max_label_length = max_label_length\n        self.charset_train = charset_train\n        self.charset_test = charset_test\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.augment = augment\n        self.remove_whitespace = remove_whitespace\n        self.normalize_unicode = normalize_unicode\n        self.min_image_dim = min_image_dim\n        self.rotation = rotation\n        self.collate_fn = collate_fn\n        \n        self.limit_size = limit_size\n        self.size_of_limit = size_of_limit\n        self.consistency_regularization = consistency_regularization\n        self.exclude_folder = exclude_folder\n        self.data_weights = data_weights\n\n        self._train_dataset = None\n        self._val_dataset = None\n\n    @staticmethod\n    def get_transform(img_size: Tuple[int], augment: bool = False, rotation: int = 0, consistency_regularization: Optional[bool] = False):\n\n        if consistency_regularization :\n            from .augmentation_pipelines import get_augmentation_pipeline\n            augmentation_severity = 2 # 2 suits to document image\n            pipeline = get_augmentation_pipeline(augmentation_severity)\n            # pipeline.append(iaa.Resize(img_size))\n            return pipeline.augment_image\n\n        else :\n            transforms = []\n            if augment:\n                from .augment import rand_augment_transform\n                transforms.append(rand_augment_transform())\n            if rotation:\n                transforms.append(lambda img: img.rotate(rotation, expand=True))\n            transforms.extend([\n                T.Resize(img_size, T.InterpolationMode.BICUBIC),\n                T.ToTensor(),\n                T.Normalize(0.5, 0.5)\n            ])\n            return T.Compose(transforms)        \n        \n    @property\n    def train_dataset(self):\n        if self._train_dataset is None:\n            transform = self.get_transform(self.img_size, self.augment, consistency_regularization = self.consistency_regularization)\n            root = PurePath(self.train_dir)\n            self._train_dataset = build_tree_dataset(root, self.charset_train, self.max_label_length,\n                                                     self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                                     transform=transform, limit_size = self.limit_size, size_of_limit = self.size_of_limit,\n                                                     consistency_regularization = self.consistency_regularization,\n                                                     img_size = self.img_size, twinreader_folders = self.exclude_folder, is_training = True\n                                                     )\n        return self._train_dataset\n\n    @property\n    def val_dataset(self):\n        if self._val_dataset is None:\n            transform = self.get_transform(self.img_size)\n            root = PurePath(self.val_dir)\n            self._val_dataset = build_tree_dataset(root, self.charset_test, self.max_label_length, \n                                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                                   img_size = self.img_size,\n                                                   transform=transform, limit_size = False, is_training = False)\n        return self._val_dataset\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n                          pin_memory=True, collate_fn=self.collate_fn)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, persistent_workers=self.num_workers > 0,\n                          pin_memory=True, collate_fn=self.collate_fn)\n\n    def test_dataloaders(self, subset):\n        transform = self.get_transform(self.img_size, rotation=self.rotation)\n        root = PurePath(self.test_dir)\n        datasets = {s: LmdbDataset(str(root / s), self.charset_test, self.max_label_length,\n                                   self.min_image_dim, self.remove_whitespace, self.normalize_unicode,\n                                   transform=transform, is_training=False) for s in subset}\n        return {k: DataLoader(v, batch_size=self.batch_size, num_workers=self.num_workers,\n                              pin_memory=True, collate_fn=self.collate_fn)\n                for k, v in datasets.items()}", ""]}
{"filename": "data/augment.py", "chunked_list": ["from functools import partial\n\nimport imgaug.augmenters as iaa\nimport numpy as np\nfrom PIL import ImageFilter, Image\nfrom timm.data import auto_augment\n\nfrom . import aa_overrides\n\naa_overrides.apply()", "\naa_overrides.apply()\n\n_OP_CACHE = {}\n\n\ndef _get_op(key, factory):\n    try:\n        op = _OP_CACHE[key]\n    except KeyError:\n        op = factory()\n        _OP_CACHE[key] = op\n    return op", "\n\ndef _get_param(level, img, max_dim_factor, min_level=1):\n    max_level = max(min_level, max_dim_factor * max(img.size))\n    return round(min(level, max_level))\n\n\ndef gaussian_blur(img, radius, **__):\n    radius = _get_param(radius, img, 0.02)\n    key = 'gaussian_blur_' + str(radius)\n    op = _get_op(key, lambda: ImageFilter.GaussianBlur(radius))\n    return img.filter(op)", "\n\ndef motion_blur(img, k, **__):\n    k = _get_param(k, img, 0.08, 3) | 1  # bin to odd values\n    key = 'motion_blur_' + str(k)\n    op = _get_op(key, lambda: iaa.MotionBlur(k))\n    return Image.fromarray(op(image=np.asarray(img)))\n\n\ndef gaussian_noise(img, scale, **_):\n    scale = _get_param(scale, img, 0.25) | 1  # bin to odd values\n    key = 'gaussian_noise_' + str(scale)\n    op = _get_op(key, lambda: iaa.AdditiveGaussianNoise(scale=scale))\n    return Image.fromarray(op(image=np.asarray(img)))", "\ndef gaussian_noise(img, scale, **_):\n    scale = _get_param(scale, img, 0.25) | 1  # bin to odd values\n    key = 'gaussian_noise_' + str(scale)\n    op = _get_op(key, lambda: iaa.AdditiveGaussianNoise(scale=scale))\n    return Image.fromarray(op(image=np.asarray(img)))\n\n\ndef poisson_noise(img, lam, **_):\n    lam = _get_param(lam, img, 0.2) | 1  # bin to odd values\n    key = 'poisson_noise_' + str(lam)\n    op = _get_op(key, lambda: iaa.AdditivePoissonNoise(lam))\n    return Image.fromarray(op(image=np.asarray(img)))", "def poisson_noise(img, lam, **_):\n    lam = _get_param(lam, img, 0.2) | 1  # bin to odd values\n    key = 'poisson_noise_' + str(lam)\n    op = _get_op(key, lambda: iaa.AdditivePoissonNoise(lam))\n    return Image.fromarray(op(image=np.asarray(img)))\n\n\ndef _level_to_arg(level, _hparams, max):\n    level = max * level / auto_augment._LEVEL_DENOM\n    return level,", "\n\n_RAND_TRANSFORMS = auto_augment._RAND_INCREASING_TRANSFORMS.copy()\n_RAND_TRANSFORMS.remove('SharpnessIncreasing')  # remove, interferes with *blur ops\n_RAND_TRANSFORMS.extend([\n    'GaussianBlur',\n    # 'MotionBlur',\n    # 'GaussianNoise',\n    'PoissonNoise'\n])", "    'PoissonNoise'\n])\nauto_augment.LEVEL_TO_ARG.update({\n    'GaussianBlur': partial(_level_to_arg, max=4),\n    'MotionBlur': partial(_level_to_arg, max=20),\n    'GaussianNoise': partial(_level_to_arg, max=0.1 * 255),\n    'PoissonNoise': partial(_level_to_arg, max=40)\n})\nauto_augment.NAME_TO_OP.update({\n    'GaussianBlur': gaussian_blur,", "auto_augment.NAME_TO_OP.update({\n    'GaussianBlur': gaussian_blur,\n    'MotionBlur': motion_blur,\n    'GaussianNoise': gaussian_noise,\n    'PoissonNoise': poisson_noise\n})\n\n\ndef rand_augment_transform(magnitude=5, num_layers=3):\n    # These are tuned for magnitude=5, which means that effective magnitudes are half of these values.\n    hparams = {\n        'rotate_deg': 30,\n        'shear_x_pct': 0.9,\n        'shear_y_pct': 0.2,\n        'translate_x_pct': 0.10,\n        'translate_y_pct': 0.30\n    }\n    ra_ops = auto_augment.rand_augment_ops(magnitude, hparams, transforms=_RAND_TRANSFORMS)\n    # Supply weights to disable replacement in random selection (i.e. avoid applying the same op twice)\n    choice_weights = [1. / len(ra_ops) for _ in range(len(ra_ops))]\n    return auto_augment.RandAugment(ra_ops, num_layers, choice_weights)", "def rand_augment_transform(magnitude=5, num_layers=3):\n    # These are tuned for magnitude=5, which means that effective magnitudes are half of these values.\n    hparams = {\n        'rotate_deg': 30,\n        'shear_x_pct': 0.9,\n        'shear_y_pct': 0.2,\n        'translate_x_pct': 0.10,\n        'translate_y_pct': 0.30\n    }\n    ra_ops = auto_augment.rand_augment_ops(magnitude, hparams, transforms=_RAND_TRANSFORMS)\n    # Supply weights to disable replacement in random selection (i.e. avoid applying the same op twice)\n    choice_weights = [1. / len(ra_ops) for _ in range(len(ra_ops))]\n    return auto_augment.RandAugment(ra_ops, num_layers, choice_weights)", ""]}
{"filename": "model/parseq_test.py", "chunked_list": ["# Scene Text Recognition Model Hub\n# Copyright 2022 Darwin Bautista\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom functools import partial\nfrom itertools import permutations", "from functools import partial\nfrom itertools import permutations\nfrom typing import Sequence, Any, Optional, Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n", "from torch import Tensor\n\nfrom timm.models.helpers import named_apply\nfrom dataclasses import dataclass\nfrom nltk import edit_distance\n\nfrom .utils import init_weights\nfrom .modules import DecoderLayer, Decoder, Encoder, TokenEmbedding\nfrom .tokenizer_utils import Tokenizer, BaseTokenizer\n", "from .tokenizer_utils import Tokenizer, BaseTokenizer\n\n@dataclass\nclass BatchResult:\n    num_samples: int\n    correct: int\n    ned: float\n    confidence: float\n    label_length: int\n    loss: Tensor\n    loss_numel: int", "\nclass PARSeq(nn.Module):\n\n    def __init__(self, charset_train: str, charset_test: str, max_label_length: int,\n                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float,\n                 img_size: Sequence[int], patch_size: Sequence[int], embed_dim: int,\n                 enc_num_heads: int, enc_mlp_ratio: int, enc_depth: int,\n                 dec_num_heads: int, dec_mlp_ratio: int, dec_depth: int,\n                 perm_num: int, perm_forward: bool, perm_mirrored: bool,\n                 decode_ar: bool, refine_iters: int, dropout: float, **kwargs: Any) -> None:\n        tokenizer = Tokenizer(charset_train)\n        super().__init__()\n        \n        self.tokenizer = tokenizer\n        self.bos_id = self.tokenizer.bos_id\n        self.eos_id = self.tokenizer.eos_id\n        self.pad_id = self.tokenizer.pad_id\n        \n        self.batch_size = batch_size\n        self.lr = lr\n        self.warmup_pct = warmup_pct\n        self.weight_decay = weight_decay\n        \n        self._device = None\n        self.max_label_length = max_label_length\n        self.decode_ar = decode_ar\n        self.refine_iters = refine_iters\n\n        self.encoder = Encoder(img_size, patch_size, embed_dim=embed_dim, depth=enc_depth, num_heads=enc_num_heads,\n                               mlp_ratio=enc_mlp_ratio)\n        decoder_layer = DecoderLayer(embed_dim, dec_num_heads, embed_dim * dec_mlp_ratio, dropout)\n        self.decoder = Decoder(decoder_layer, num_layers=dec_depth, norm=nn.LayerNorm(embed_dim))\n\n        # Perm/attn mask stuff\n        self.rng = np.random.default_rng()\n        self.max_gen_perms = perm_num // 2 if perm_mirrored else perm_num\n        self.perm_forward = perm_forward\n        self.perm_mirrored = perm_mirrored\n\n        # We don't predict <bos> nor <pad>\n        self.head = nn.Linear(embed_dim, len(self.tokenizer) - 2)\n        self.text_embed = TokenEmbedding(len(self.tokenizer), embed_dim)\n\n        # +1 for <eos>\n        self.pos_queries = nn.Parameter(torch.Tensor(1, max_label_length + 1, embed_dim))\n        self.dropout = nn.Dropout(p=dropout)\n        # Encoder has its own init.\n        named_apply(partial(init_weights, exclude=['encoder']), self)\n        nn.init.trunc_normal_(self.pos_queries, std=.02)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        param_names = {'text_embed.embedding.weight', 'pos_queries'}\n        enc_param_names = {'encoder.' + n for n in self.encoder.no_weight_decay()}\n        return param_names.union(enc_param_names)\n\n    def encode(self, img: torch.Tensor):\n        return self.encoder(img)\n\n    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[Tensor] = None,\n               tgt_padding_mask: Optional[Tensor] = None, tgt_query: Optional[Tensor] = None,\n               tgt_query_mask: Optional[Tensor] = None):\n        N, L = tgt.shape\n        # <bos> stands for the null context. We only supply position information for characters after <bos>.\n        null_ctx = self.text_embed(tgt[:, :1])\n        tgt_emb = self.pos_queries[:, :L - 1] + self.text_embed(tgt[:, 1:])\n        tgt_emb = self.dropout(torch.cat([null_ctx, tgt_emb], dim=1))\n        if tgt_query is None:\n            tgt_query = self.pos_queries[:, :L].expand(N, -1, -1)\n        tgt_query = self.dropout(tgt_query)\n        return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask)\n    \n    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n        targets = self.tokenizer.encode(labels, self._device)\n        targets = targets[:, 1:]  # Discard <bos>\n        max_len = targets.shape[1] - 1  # exclude <eos> from count\n        logits = self.forward(images, max_len)\n        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n        loss_numel = (targets != self.pad_id).sum()\n        return logits, loss, loss_numel\n    \n    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n        testing = max_length is None\n        max_length = self.max_label_length if max_length is None else min(max_length, self.max_label_length)\n        bs = images.shape[0]\n        # +1 for <eos> at end of sequence.\n        num_steps = max_length + 1\n        memory = self.encode(images)\n\n        # Query positions up to `num_steps`\n        pos_queries = self.pos_queries[:, :num_steps].expand(bs, -1, -1)\n\n        # Special case for the forward permutation. Faster than using `generate_attn_masks()`\n        tgt_mask = query_mask = torch.triu(torch.full((num_steps, num_steps), float('-inf'), device=self._device), 1)\n\n        if self.decode_ar:\n            tgt_in = torch.full((bs, num_steps), self.pad_id, dtype=torch.long, device=self._device)\n            tgt_in[:, 0] = self.bos_id\n\n            logits = []\n            for i in range(num_steps):\n                j = i + 1  # next token index\n                # Efficient decoding:\n                # Input the context up to the ith token. We use only one query (at position = i) at a time.\n                # This works because of the lookahead masking effect of the canonical (forward) AR context.\n                # Past tokens have no access to future tokens, hence are fixed once computed.\n                tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n                                      tgt_query_mask=query_mask[i:j, :j])\n                # the next token probability is in the output's ith token position\n                p_i = self.head(tgt_out)\n                logits.append(p_i)\n                if j < num_steps:\n                    # greedy decode. add the next token index to the target input\n                    tgt_in[:, j] = p_i.squeeze().argmax(-1)\n#                     tgt_in[:, j] = p_i.argmax(-1).squeeze()\n                    # Efficient batch decoding: If all output words have at least one EOS token, end decoding.\n#                     if torch.onnx.is_in_onnx_export() and torch.where(torch.where(tgt_in == torch.tensor(self.eos_id), 1, 0).sum(dim=-1).sum() == bs, 1, 0):\n#                         break\n#                     el\n#                     if testing and (tgt_in == self.eos_id).any(dim=-1).all():\n#                         break\n\n            logits = torch.cat(logits, dim=1)\n        else:\n            # No prior context, so input is just <bos>. We query all positions.\n            tgt_in = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n            tgt_out = self.decode(tgt_in, memory, tgt_query=pos_queries)\n            logits = self.head(tgt_out)\n\n        if self.refine_iters:\n            # For iterative refinement, we always use a 'cloze' mask.\n            # We can derive it from the AR forward mask by unmasking the token context to the right.\n            query_mask[torch.triu(torch.ones(num_steps, num_steps, dtype=torch.bool, device=self._device), 2)] = 0\n            bos = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n            for i in range(self.refine_iters):\n                # Prior context is the previous output.\n                tgt_in = torch.cat([bos, logits[:, :-1].argmax(-1)], dim=1)\n                tgt_padding_mask = ((tgt_in == self.eos_id).int().cumsum(-1) > 0)  # mask tokens beyond the first EOS token.\n                tgt_out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask,\n                                      tgt_query=pos_queries, tgt_query_mask=query_mask[:, :tgt_in.shape[1]])\n                logits = self.head(tgt_out)\n\n        return logits\n\n    def gen_tgt_perms(self, tgt):\n        \"\"\"Generate shared permutations for the whole batch.\n           This works because the same attention mask can be used for the shorter sequences\n           because of the padding mask.\n        \"\"\"\n        # We don't permute the position of BOS, we permute EOS separately\n        max_num_chars = tgt.shape[1] - 2\n        # Special handling for 1-character sequences\n        if max_num_chars == 1:\n            return torch.arange(3, device=self._device).unsqueeze(0)\n        perms = [torch.arange(max_num_chars, device=self._device)] if self.perm_forward else []\n        # Additional permutations if needed\n        max_perms = math.factorial(max_num_chars)\n        if self.perm_mirrored:\n            max_perms //= 2\n        num_gen_perms = min(self.max_gen_perms, max_perms)\n        # For 4-char sequences and shorter, we generate all permutations and sample from the pool to avoid collisions\n        # Note that this code path might NEVER get executed since the labels in a mini-batch typically exceed 4 chars.\n        if max_num_chars < 5:\n            # Pool of permutations to sample from. We only need the first half (if complementary option is selected)\n            # Special handling for max_num_chars == 4 which correctly divides the pool into the flipped halves\n            if max_num_chars == 4 and self.perm_mirrored:\n                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]\n            else:\n                selector = list(range(max_perms))\n            perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=self._device)[selector]\n            # If the forward permutation is always selected, no need to add it to the pool for sampling\n            if self.perm_forward:\n                perm_pool = perm_pool[1:]\n            perms = torch.stack(perms)\n            if len(perm_pool):\n                i = self.rng.choice(len(perm_pool), size=num_gen_perms - len(perms), replace=False)\n                perms = torch.cat([perms, perm_pool[i]])\n        else:\n            perms.extend([torch.randperm(max_num_chars, device=self._device) for _ in range(num_gen_perms - len(perms))])\n            perms = torch.stack(perms)\n        if self.perm_mirrored:\n            # Add complementary pairs\n            comp = perms.flip(-1)\n            # Stack in such a way that the pairs are next to each other.\n            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n        # NOTE:\n        # The only meaningful way of permuting the EOS position is by moving it one character position at a time.\n        # However, since the number of permutations = T! and number of EOS positions = T + 1, the number of possible EOS\n        # positions will always be much less than the number of permutations (unless a low perm_num is set).\n        # Thus, it would be simpler to just train EOS using the full and null contexts rather than trying to evenly\n        # distribute it across the chosen number of permutations.\n        # Add position indices of BOS and EOS\n        bos_idx = perms.new_zeros((len(perms), 1))\n        eos_idx = perms.new_full((len(perms), 1), max_num_chars + 1)\n        perms = torch.cat([bos_idx, perms + 1, eos_idx], dim=1)\n        # Special handling for the reverse direction. This does two things:\n        # 1. Reverse context for the characters\n        # 2. Null context for [EOS] (required for learning to predict [EOS] in NAR mode)\n        if len(perms) > 1:\n            perms[1, 1:] = max_num_chars + 1 - torch.arange(max_num_chars + 1, device=self._device)\n        return perms\n\n    def generate_attn_masks(self, perm):\n        \"\"\"Generate attention masks given a sequence permutation (includes pos. for bos and eos tokens)\n        :param perm: the permutation sequence. i = 0 is always the BOS\n        :return: lookahead attention masks\n        \"\"\"\n        sz = perm.shape[0]\n        mask = torch.zeros((sz, sz), device=self._device)\n        for i in range(sz):\n            query_idx = perm[i]\n            masked_keys = perm[i + 1:]\n            mask[query_idx, masked_keys] = float('-inf')\n        content_mask = mask[:-1, :-1].clone()\n        mask[torch.eye(sz, dtype=torch.bool, device=self._device)] = float('-inf')  # mask \"self\"\n        query_mask = mask[1:, :-1]\n        return content_mask, query_mask\n\n    def training_step(self, images, labels): #-> STEP_OUTPUT:\n        tgt = self.tokenizer.encode(labels, self._device)\n\n        # Encode the source sequence (i.e. the image codes)\n        memory = self.encode(images)\n\n        # Prepare the target sequences (input and output)\n        tgt_perms = self.gen_tgt_perms(tgt)\n        tgt_in = tgt[:, :-1]\n        tgt_out = tgt[:, 1:]\n        # The [EOS] token is not depended upon by any other token in any permutation ordering\n        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\n        loss = 0\n        loss_numel = 0\n        n = (tgt_out != self.pad_id).sum().item()\n        for i, perm in enumerate(tgt_perms):\n            tgt_mask, query_mask = self.generate_attn_masks(perm)\n            out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n            logits = self.head(out).flatten(end_dim=1)\n            loss += n * F.cross_entropy(logits, tgt_out.flatten(), ignore_index=self.pad_id)\n            loss_numel += n\n            # After the second iteration (i.e. done with canonical and reverse orderings),\n            # remove the [EOS] tokens for the succeeding perms\n            if i == 1:\n                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n                n = (tgt_out != self.pad_id).sum().item()\n        loss /= loss_numel\n\n        return loss\n\n    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n        images, labels = batch\n\n        correct = 0\n        total = 0\n        ned = 0\n        confidence = 0\n        label_length = 0\n        if validation:\n            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n        else:\n            # At test-time, we shouldn't specify a max_label_length because the test-time charset used\n            # might be different from the train-time charset. max_label_length in eval_logits_loss() is computed\n            # based on the transformed label, which could be wrong if the actual gt label contains characters existing\n            # in the train-time charset but not in the test-time charset. For example, \"aishahaleyes.blogspot.com\"\n            # is exactly 25 characters, but if processed by CharsetAdapter for the 36-char set, it becomes 23 characters\n            # long only, which sets max_label_length = 23. This will cause the model prediction to be truncated.\n            logits = self.forward(images)\n            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\n        probs = logits.softmax(-1)\n        preds, probs = self.tokenizer.decode(probs)\n        for pred, prob, gt in zip(preds, probs, labels):\n            confidence += prob.prod().item()\n#             pred = self.charset_adapter(pred)\n            # Follow ICDAR 2019 definition of N.E.D.\n            pred=pred.replace('\u0932','[UNK]')\n            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n            if pred == gt:\n                correct += 1\n            total += 1\n            label_length += len(pred)\n        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\n    @staticmethod\n    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n        if not outputs:\n            return 0., 0., 0.\n        total_loss = 0\n        total_loss_numel = 0\n        total_n_correct = 0\n        total_norm_ED = 0\n        total_size = 0\n        for result in outputs:\n            result = result['output']\n            total_loss += result.loss_numel * result.loss\n            total_loss_numel += result.loss_numel\n            total_n_correct += result.correct\n            total_norm_ED += result.ned\n            total_size += result.num_samples\n        acc = total_n_correct / total_size\n        ned = (1 - total_norm_ED / total_size)\n        loss = total_loss / total_loss_numel\n        return acc, ned, loss\n\n    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, True)\n\n    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, False)", "    \n\nclass PARSeqWithConsistencyRegularization(PARSeq):\n    \n    def __init__(self, *args, cr_loss_weight, supervised_flag, teacher_one_hot, use_threshold, kl_div, ema, ema_decay, **kwargs: Any) -> None :\n        super().__init__(*args, **kwargs)\n        \n        # consistency regularization\n        self.cr_loss_weight = cr_loss_weight\n        self.supervised_flag = supervised_flag\n        self.teacher_one_hot_labels = teacher_one_hot\n        self.use_threshold = use_threshold\n        self.kl_div = kl_div\n        self.ema = ema\n        self.ema_decay = ema_decay\n\n\n    def get_supervised_loss(self, images, labels):\n\n        tgt = self.tokenizer.encode(labels, self._device)\n        \n        teacher_img, student_img = images[:,0], images[:,1]\n\n        # Encode the source sequence (i.e. the image codes)\n        teacher_memory = self.encode(teacher_img)\n        student_memory = self.encode(student_img)\n\n        # Prepare the target sequences (input and output)\n        tgt_perms = self.gen_tgt_perms(tgt)\n        tgt_in = tgt[:, :-1]\n        tgt_out = tgt[:, 1:]\n        # The [EOS] token is not depended upon by any other token in any permutation ordering\n        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\n        teacher_loss = 0\n        student_loss = 0\n        \n        loss_numel = 0\n        n = (tgt_out != self.pad_id).sum().item()\n        for i, perm in enumerate(tgt_perms):\n            tgt_mask, query_mask = self.generate_attn_masks(perm)\n\n            teacher_out = self.decode(tgt_in, teacher_memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n            teacher_logits = self.head(teacher_out)\n            teacher_loss += n * F.cross_entropy(teacher_logits.flatten(end_dim=1), tgt_out.flatten(), ignore_index=self.pad_id)\n            \n            student_out = self.decode(tgt_in, student_memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n            student_logits = self.head(student_out)\n            student_loss += n * F.cross_entropy(student_logits.flatten(end_dim=1), tgt_out.flatten(), ignore_index=self.pad_id)\n            \n            loss_numel += n\n            # After the second iteration (i.e. done with canonical and reverse orderings),\n            # remove the [EOS] tokens for the succeeding perms\n            if i == 1:\n                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n                n = (tgt_out != self.pad_id).sum().item()\n        teacher_loss /= loss_numel\n        student_loss /= loss_numel\n        loss = teacher_loss + student_loss\n        \n        return loss, teacher_logits, student_logits, tgt_out\n\n    \n\n    def training_step(self, images, labels): #-> STEP_OUTPUT:\n        ## semimtr.modules.model_fusion_consistency_regularization.py\n        # Decoder returns query        \n        # Encoder visiontransformer(transformer encoder with image embedding) returns forward_features \n        loss = 0\n        if self.supervised_flag : \n            ce_loss_ts, teacher_logits, student_logits, labels_encoded = self.get_supervised_loss(images, labels)\n            loss += ce_loss_ts\n        \n        mask = torch.sum(labels_encoded != self.pad_id, dim=1)\n\n        teacher_logits, threshold_mask = self.create_teacher_labels(teacher_logits)\n\n        masked_student_logits = self._flatten(student_logits, mask)\n        masked_teacher_logits = self._flatten(teacher_logits, mask)\n        ce_loss_student_teacher = F.cross_entropy(masked_student_logits.view(-1, masked_student_logits.shape[-1]), \\\n                                                  masked_teacher_logits.view(-1, masked_teacher_logits.shape[-1]), reduction='none')\n\n        if threshold_mask:\n            ce_loss_student_teacher = ce_loss_student_teacher * threshold_mask.view(-1, threshold_mask.shape[-1])\n        ce_loss_student_teacher = ce_loss_student_teacher.mean()\n        \n        loss += ce_loss_student_teacher * self.cr_loss_weight\n\n        return loss\n    \n    def create_teacher_labels(self, teacher_predictions):\n        self.threshold_value = 0.9\n        self.teacher_stop_gradients=True\n        pt_logits_teacher = teacher_predictions\n        if self.teacher_stop_gradients:\n            pt_logits_teacher = pt_logits_teacher.detach()\n        pt_labels_teacher = F.softmax(pt_logits_teacher, dim=-1)\n        max_values, max_indices = torch.max(pt_labels_teacher, dim=-1, keepdim=True)\n        if self.teacher_one_hot_labels:\n            pt_labels_teacher = torch.zeros_like(pt_logits_teacher).scatter_(-1, max_indices, 1)\n        threshold_mask = (max_values.squeeze() > self.threshold_value).float() if self.use_threshold else None\n        return pt_labels_teacher, threshold_mask   \n\n    def _flatten(self, sources, lengths):\n        return torch.cat([t[:l] for t, l in zip(sources, lengths)])"]}
{"filename": "model/base.py", "chunked_list": ["import math\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, List\nimport torch\nimport torch.nn.functional as F\nfrom nltk import edit_distance\nfrom torch import Tensor\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import OneCycleLR", "from torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nfrom model.tokenizer_utils import CTCTokenizer, Tokenizer, BaseTokenizer\n\n\n@dataclass\nclass BatchResult:\n    num_samples: int\n    correct: int\n    ned: float\n    confidence: float\n    label_length: int\n    loss: Tensor\n    loss_numel: int", "\n\nclass BaseSystem(torch.nn.Module):\n\n    def __init__(self, tokenizer: BaseTokenizer, charset_test: str,\n                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.lr = lr\n        self.warmup_pct = warmup_pct\n        self.weight_decay = weight_decay\n\n    @abstractmethod\n    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n        \"\"\"Inference\n\n        Args:\n            images: Batch of images. Shape: N, Ch, H, W\n            max_length: Max sequence length of the output. If None, will use default.\n\n        Returns:\n            logits: N, L, C (L = sequence length, C = number of classes, typically len(charset_train) + num specials)\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n        \"\"\"Like forward(), but also computes the loss (calls forward() internally).\n\n        Args:\n            images: Batch of images. Shape: N, Ch, H, W\n            labels: Text labels of the images\n\n        Returns:\n            logits: N, L, C (L = sequence length, C = number of classes, typically len(charset_train) + num specials)\n            loss: mean loss for the batch\n            loss_numel: number of elements the loss was calculated from\n        \"\"\"\n        raise NotImplementedError\n    \n    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n        images, labels = batch\n\n        correct = 0\n        total = 0\n        ned = 0\n        confidence = 0\n        label_length = 0\n        if validation:\n            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n        else:\n            logits = self.forward(images)\n            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\n        probs = logits.softmax(-1)\n        preds, probs = self.tokenizer.decode(probs)\n        for pred, prob, gt in zip(preds, probs, labels):\n            confidence += prob.prod().item()\n            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n            if pred == gt:\n                correct += 1\n            total += 1\n            label_length += len(pred)\n        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\n    @staticmethod\n    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n        if not outputs:\n            return 0., 0., 0.\n        total_loss = 0\n        total_loss_numel = 0\n        total_n_correct = 0\n        total_norm_ED = 0\n        total_size = 0\n        for result in outputs:\n            result = result['output']\n            total_loss += result.loss_numel * result.loss\n            total_loss_numel += result.loss_numel\n            total_n_correct += result.correct\n            total_norm_ED += result.ned\n            total_size += result.num_samples\n        acc = total_n_correct / total_size\n        ned = (1 - total_norm_ED / total_size)\n        loss = total_loss / total_loss_numel\n        return acc, ned, loss\n\n    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, True)\n\n    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, False)", "\n\nclass CrossEntropySystem(BaseSystem):\n\n    def __init__(self, charset_train: str, charset_test: str,\n                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n        tokenizer = Tokenizer(charset_train)\n        super().__init__(tokenizer, charset_test, batch_size, lr, warmup_pct, weight_decay)\n        self.bos_id = tokenizer.bos_id\n        self.eos_id = tokenizer.eos_id\n        self.pad_id = tokenizer.pad_id\n\n    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n        targets = self.tokenizer.encode(labels, self._device)\n        targets = targets[:, 1:]  # Discard <bos>\n        max_len = targets.shape[1] - 1  # exclude <eos> from count\n        logits = self.forward(images, max_len)\n        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n        loss_numel = (targets != self.pad_id).sum()\n        return logits, loss, loss_numel", "\n\nclass CTCSystem(BaseSystem):\n\n    def __init__(self, charset_train: str, charset_test: str,\n                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float) -> None:\n        tokenizer = CTCTokenizer(charset_train)\n        super().__init__(tokenizer, charset_test, batch_size, lr, warmup_pct, weight_decay)\n        self.blank_id = tokenizer.blank_id\n\n    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n        targets = self.tokenizer.encode(labels, self._device)\n        logits = self.forward(images)\n        log_probs = logits.log_softmax(-1).transpose(0, 1)  # swap batch and seq. dims\n        T, N, _ = log_probs.shape\n        input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long, device=self._device)\n        target_lengths = torch.as_tensor(list(map(len, labels)), dtype=torch.long, device=self._device)\n        loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=self.blank_id, zero_infinity=True)\n        return logits, loss, N", ""]}
{"filename": "model/tokenizer_utils.py", "chunked_list": ["import re\nfrom abc import ABC, abstractmethod\nfrom itertools import groupby\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass CharsetAdapter:\n    \"\"\"Transforms labels according to the target charset.\"\"\"\n\n    def __init__(self, target_charset) -> None:\n        super().__init__()\n        self.charset = target_charset ###\n        self.lowercase_only = target_charset == target_charset.lower()\n        self.uppercase_only = target_charset == target_charset.upper()\n#         self.unsupported = f'[^{re.escape(target_charset)}]'\n\n    def __call__(self, label):\n        if self.lowercase_only:\n            label = label.lower()\n        elif self.uppercase_only:\n            label = label.upper()\n        return label", "\n\nclass CharsetAdapter:\n    \"\"\"Transforms labels according to the target charset.\"\"\"\n\n    def __init__(self, target_charset) -> None:\n        super().__init__()\n        self.charset = target_charset ###\n        self.lowercase_only = target_charset == target_charset.lower()\n        self.uppercase_only = target_charset == target_charset.upper()\n#         self.unsupported = f'[^{re.escape(target_charset)}]'\n\n    def __call__(self, label):\n        if self.lowercase_only:\n            label = label.lower()\n        elif self.uppercase_only:\n            label = label.upper()\n        return label", "\n\nclass BaseTokenizer(ABC):\n\n    def __init__(self, charset: str, specials_first: tuple = (), specials_last: tuple = ()) -> None:\n        self._itos = specials_first + tuple(charset+'[UNK]') + specials_last\n        self._stoi = {s: i for i, s in enumerate(self._itos)}\n\n    def __len__(self):\n        return len(self._itos)\n\n    def _tok2ids(self, tokens: str) -> List[int]:\n        return [self._stoi[s] for s in tokens]\n\n    def _ids2tok(self, token_ids: List[int], join: bool = True) -> str:\n        tokens = [self._itos[i] for i in token_ids]\n        return ''.join(tokens) if join else tokens\n\n    @abstractmethod\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        \"\"\"Encode a batch of labels to a representation suitable for the model.\n\n        Args:\n            labels: List of labels. Each can be of arbitrary length.\n            device: Create tensor on this device.\n\n        Returns:\n            Batched tensor representation padded to the max label length. Shape: N, L\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        \"\"\"Internal method which performs the necessary filtering prior to decoding.\"\"\"\n        raise NotImplementedError\n\n    def decode(self, token_dists: Tensor, raw: bool = False) -> Tuple[List[str], List[Tensor]]:\n        \"\"\"Decode a batch of token distributions.\n\n        Args:\n            token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n            raw: return unprocessed labels (will return list of list of strings)\n\n        Returns:\n            list of string labels (arbitrary length) and\n            their corresponding sequence probabilities as a list of Tensors\n        \"\"\"\n        batch_tokens = []\n        batch_probs = []\n        for dist in token_dists:\n            probs, ids = dist.max(-1)  # greedy selection\n            if not raw:\n                probs, ids = self._filter(probs, ids)\n            tokens = self._ids2tok(ids, not raw)\n            batch_tokens.append(tokens)\n            batch_probs.append(probs)\n        return batch_tokens, batch_probs", "\n\nclass Tokenizer(BaseTokenizer):\n    BOS = '[B]'\n    EOS = '[E]'\n    PAD = '[P]'\n\n    def __init__(self, charset: str) -> None:\n        specials_first = (self.EOS,)\n        specials_last = (self.BOS, self.PAD)\n        super().__init__(charset, specials_first, specials_last)\n        self.eos_id, self.bos_id, self.pad_id = [self._stoi[s] for s in specials_first + specials_last]\n\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        batch = [torch.as_tensor([self.bos_id] + self._tok2ids(y) + [self.eos_id], dtype=torch.long, device=device)\n                 for y in labels]\n        return pad_sequence(batch, batch_first=True, padding_value=self.pad_id)\n\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        ids = ids.tolist()\n        try:\n            eos_idx = ids.index(self.eos_id)\n        except ValueError:\n            eos_idx = len(ids)  # Nothing to truncate.\n        # Truncate after EOS\n        ids = ids[:eos_idx]\n        probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n        return probs, ids", "\n\nclass CTCTokenizer(BaseTokenizer):\n    BLANK = '[B]'\n\n    def __init__(self, charset: str) -> None:\n        # BLANK uses index == 0 by default\n        super().__init__(charset, specials_first=(self.BLANK,))\n        self.blank_id = self._stoi[self.BLANK]\n\n    def encode(self, labels: List[str], device: Optional[torch.device] = None) -> Tensor:\n        # We use a padded representation since we don't want to use CUDNN's CTC implementation\n        batch = [torch.as_tensor(self._tok2ids(y), dtype=torch.long, device=device) for y in labels]\n        return pad_sequence(batch, batch_first=True, padding_value=self.blank_id)\n\n    def _filter(self, probs: Tensor, ids: Tensor) -> Tuple[Tensor, List[int]]:\n        # Best path decoding:\n        ids = list(zip(*groupby(ids.tolist())))[0]  # Remove duplicate tokens\n        ids = [x for x in ids if x != self.blank_id]  # Remove BLANKs\n        # `probs` is just pass-through since all positions are considered part of the path\n        return probs, ids", ""]}
{"filename": "model/utils.py", "chunked_list": ["from pathlib import PurePath\nfrom typing import Sequence\n\nimport torch\nfrom torch import nn\n\nimport yaml\n\n\nclass InvalidModelError(RuntimeError):\n    \"\"\"Exception raised for any model-related error (creation, loading)\"\"\"", "\nclass InvalidModelError(RuntimeError):\n    \"\"\"Exception raised for any model-related error (creation, loading)\"\"\"\n\n\n_WEIGHTS_URL = {\n    'parseq-tiny': 'https://github.com/baudm/parseq/releases/download/v1.0.0/parseq_tiny-e7a21b54.pt',\n    'parseq': 'https://github.com/baudm/parseq/releases/download/v1.0.0/parseq-bb5792a6.pt',\n    'abinet': 'https://github.com/baudm/parseq/releases/download/v1.0.0/abinet-1d1e373e.pt',\n    'trba': 'https://github.com/baudm/parseq/releases/download/v1.0.0/trba-cfaed284.pt',", "    'abinet': 'https://github.com/baudm/parseq/releases/download/v1.0.0/abinet-1d1e373e.pt',\n    'trba': 'https://github.com/baudm/parseq/releases/download/v1.0.0/trba-cfaed284.pt',\n    'vitstr': 'https://github.com/baudm/parseq/releases/download/v1.0.0/vitstr-26d0fcf4.pt',\n    'crnn': 'https://github.com/baudm/parseq/releases/download/v1.0.0/crnn-679d0e31.pt',\n}\n\n\ndef _get_config(experiment: str, **kwargs):\n    \"\"\"Emulates hydra config resolution\"\"\"\n    root = PurePath(__file__).parents[2]\n    with open(root / 'configs/main.yaml', 'r') as f:\n        config = yaml.load(f, yaml.Loader)['model']\n    with open(root / f'configs/charset/94_full.yaml', 'r') as f:\n        config.update(yaml.load(f, yaml.Loader)['model'])\n    with open(root / f'configs/experiment/{experiment}.yaml', 'r') as f:\n        exp = yaml.load(f, yaml.Loader)\n    # Apply base model config\n    model = exp['defaults'][0]['override /model']\n    with open(root / f'configs/model/{model}.yaml', 'r') as f:\n        config.update(yaml.load(f, yaml.Loader))\n    # Apply experiment config\n    if 'model' in exp:\n        config.update(exp['model'])\n    config.update(kwargs)\n    # Workaround for now: manually cast the lr to the correct type.\n    config['lr'] = float(config['lr'])\n    return config", "\n\ndef _get_model_class(key):\n    if 'abinet' in key:\n        from .abinet.system import ABINet as ModelClass\n    elif 'crnn' in key:\n        from .crnn.system import CRNN as ModelClass\n    elif 'parseq' in key:\n        from .parseq.system import PARSeq as ModelClass\n    elif 'trba' in key:\n        from .trba.system import TRBA as ModelClass\n    elif 'trbc' in key:\n        from .trba.system import TRBC as ModelClass\n    elif 'vitstr' in key:\n        from .vitstr.system import ViTSTR as ModelClass\n    else:\n        raise InvalidModelError(\"Unable to find model class for '{}'\".format(key))\n    return ModelClass", "\n\ndef get_pretrained_weights(experiment):\n    try:\n        url = _WEIGHTS_URL[experiment]\n    except KeyError:\n        raise InvalidModelError(\"No pretrained weights found for '{}'\".format(experiment)) from None\n    return torch.hub.load_state_dict_from_url(url=url, map_location='cpu', check_hash=True)\n\n\ndef create_model(experiment: str, pretrained: bool = False, **kwargs):\n    try:\n        config = _get_config(experiment, **kwargs)\n    except FileNotFoundError:\n        raise InvalidModelError(\"No configuration found for '{}'\".format(experiment)) from None\n    ModelClass = _get_model_class(experiment)\n    model = ModelClass(**config)\n    if pretrained:\n        model.load_state_dict(get_pretrained_weights(experiment))\n    return model", "\n\ndef create_model(experiment: str, pretrained: bool = False, **kwargs):\n    try:\n        config = _get_config(experiment, **kwargs)\n    except FileNotFoundError:\n        raise InvalidModelError(\"No configuration found for '{}'\".format(experiment)) from None\n    ModelClass = _get_model_class(experiment)\n    model = ModelClass(**config)\n    if pretrained:\n        model.load_state_dict(get_pretrained_weights(experiment))\n    return model", "\n\ndef load_from_checkpoint(checkpoint_path: str, **kwargs):\n    if checkpoint_path.startswith('pretrained='):\n        model_id = checkpoint_path.split('=', maxsplit=1)[1]\n        model = create_model(model_id, True, **kwargs)\n    else:\n        ModelClass = _get_model_class(checkpoint_path)\n        model = ModelClass.load_from_checkpoint(checkpoint_path, **kwargs)\n    return model", "\n\ndef parse_model_args(args):\n    kwargs = {}\n    arg_types = {t.__name__: t for t in [int, float, str]}\n    arg_types['bool'] = lambda v: v.lower() == 'true'  # special handling for bool\n    for arg in args:\n        name, value = arg.split('=', maxsplit=1)\n        name, arg_type = name.split(':', maxsplit=1)\n        kwargs[name] = arg_types[arg_type](value)\n    return kwargs", "\n\ndef init_weights(module: nn.Module, name: str = '', exclude: Sequence[str] = ()):\n    \"\"\"Initialize the weights using the typical initialization schemes used in SOTA models.\"\"\"\n    if any(map(name.startswith, exclude)):\n        return\n    if isinstance(module, nn.Linear):\n        nn.init.trunc_normal_(module.weight, std=.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        nn.init.trunc_normal_(module.weight, std=.02)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    elif isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        nn.init.zeros_(module.bias)", ""]}
{"filename": "model/modules.py", "chunked_list": ["import math\nfrom typing import Optional\nimport torch\nfrom torch import nn as nn, Tensor\nfrom torch.nn import functional as F\nfrom torch.nn.modules import transformer\n\nfrom timm.models.vision_transformer import VisionTransformer, PatchEmbed\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"A Transformer decoder layer supporting two-stream attention (XLNet)\n       This implements a pre-LN decoder, as opposed to the post-LN default in PyTorch.\"\"\"\n\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='gelu',\n                 layer_norm_eps=1e-5):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm_q = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm_c = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = transformer._get_activation_fn(activation)\n\n    def __setstate__(self, state):\n        if 'activation' not in state:\n            state['activation'] = F.gelu\n        super().__setstate__(state)\n\n    def forward_stream(self, tgt: Tensor, tgt_norm: Tensor, tgt_kv: Tensor, memory: Tensor, tgt_mask: Optional[Tensor],\n                       tgt_key_padding_mask: Optional[Tensor]):\n        tgt2, sa_weights = self.self_attn(tgt_norm, tgt_kv, tgt_kv, attn_mask=tgt_mask,\n                                          key_padding_mask=tgt_key_padding_mask)\n        tgt = tgt + self.dropout1(tgt2)\n\n        tgt2, ca_weights = self.cross_attn(self.norm1(tgt), memory, memory)\n        tgt = tgt + self.dropout2(tgt2)\n\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm2(tgt)))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt, sa_weights, ca_weights\n\n    def forward(self, query, content, memory, query_mask: Optional[Tensor] = None, content_mask: Optional[Tensor] = None,\n                content_key_padding_mask: Optional[Tensor] = None, update_content: bool = True):\n        query_norm = self.norm_q(query)\n        content_norm = self.norm_c(content)\n        query = self.forward_stream(query, query_norm, content_norm, memory, query_mask, content_key_padding_mask)[0]\n        if update_content:\n            content = self.forward_stream(content, content_norm, content_norm, memory, content_mask,\n                                          content_key_padding_mask)[0]\n        return query, content", "\n\nclass DecoderLayer(nn.Module):\n    \"\"\"A Transformer decoder layer supporting two-stream attention (XLNet)\n       This implements a pre-LN decoder, as opposed to the post-LN default in PyTorch.\"\"\"\n\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='gelu',\n                 layer_norm_eps=1e-5):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm_q = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.norm_c = nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = transformer._get_activation_fn(activation)\n\n    def __setstate__(self, state):\n        if 'activation' not in state:\n            state['activation'] = F.gelu\n        super().__setstate__(state)\n\n    def forward_stream(self, tgt: Tensor, tgt_norm: Tensor, tgt_kv: Tensor, memory: Tensor, tgt_mask: Optional[Tensor],\n                       tgt_key_padding_mask: Optional[Tensor]):\n        tgt2, sa_weights = self.self_attn(tgt_norm, tgt_kv, tgt_kv, attn_mask=tgt_mask,\n                                          key_padding_mask=tgt_key_padding_mask)\n        tgt = tgt + self.dropout1(tgt2)\n\n        tgt2, ca_weights = self.cross_attn(self.norm1(tgt), memory, memory)\n        tgt = tgt + self.dropout2(tgt2)\n\n        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm2(tgt)))))\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt, sa_weights, ca_weights\n\n    def forward(self, query, content, memory, query_mask: Optional[Tensor] = None, content_mask: Optional[Tensor] = None,\n                content_key_padding_mask: Optional[Tensor] = None, update_content: bool = True):\n        query_norm = self.norm_q(query)\n        content_norm = self.norm_c(content)\n        query = self.forward_stream(query, query_norm, content_norm, memory, query_mask, content_key_padding_mask)[0]\n        if update_content:\n            content = self.forward_stream(content, content_norm, content_norm, memory, content_mask,\n                                          content_key_padding_mask)[0]\n        return query, content", "\n\nclass Decoder(nn.Module):\n    __constants__ = ['norm']\n\n    def __init__(self, decoder_layer, num_layers, norm):\n        super().__init__()\n        self.layers = transformer._get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(self, query, content, memory, query_mask: Optional[Tensor] = None, content_mask: Optional[Tensor] = None,\n                content_key_padding_mask: Optional[Tensor] = None):\n        for i, mod in enumerate(self.layers):\n            last = i == len(self.layers) - 1\n            query, content = mod(query, content, memory, query_mask, content_mask, content_key_padding_mask,\n                                 update_content=not last)\n        query = self.norm(query)\n        return query", "\n\nclass Encoder(VisionTransformer):\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n                 qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed):\n        super().__init__(img_size, patch_size, in_chans, embed_dim=embed_dim, depth=depth, num_heads=num_heads,\n                         mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n                         drop_path_rate=drop_path_rate, embed_layer=embed_layer,\n                         num_classes=0, global_pool='', class_token=False)  # these disable the classifier head\n\n    def forward(self, x):\n        # Return all tokens\n        return self.forward_features(x)", "\n\nclass TokenEmbedding(nn.Module):\n\n    def __init__(self, charset_size: int, embed_dim: int):\n        super().__init__()\n        self.embedding = nn.Embedding(charset_size, embed_dim)\n        self.embed_dim = embed_dim\n\n    def forward(self, tokens: torch.Tensor):\n        return math.sqrt(self.embed_dim) * self.embedding(tokens)", ""]}
{"filename": "model/parseq.py", "chunked_list": ["import math\nfrom functools import partial\nfrom itertools import permutations\nfrom typing import Sequence, Any, Optional, Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor", "import torch.nn.functional as F\nfrom torch import Tensor\n\nfrom timm.models.helpers import named_apply\nfrom dataclasses import dataclass\nfrom nltk import edit_distance\n\nfrom .utils import init_weights\nfrom .modules import DecoderLayer, Decoder, Encoder, TokenEmbedding\nfrom .tokenizer_utils import Tokenizer, BaseTokenizer", "from .modules import DecoderLayer, Decoder, Encoder, TokenEmbedding\nfrom .tokenizer_utils import Tokenizer, BaseTokenizer\n\n@dataclass\nclass BatchResult:\n    num_samples: int\n    correct: int\n    ned: float\n    confidence: float\n    label_length: int\n    loss: Tensor\n    loss_numel: int", "\nclass PARSeq(nn.Module):\n\n    def __init__(self, charset_train: str, charset_test: str, max_label_length: int,\n                 batch_size: int, lr: float, warmup_pct: float, weight_decay: float,\n                 img_size: Sequence[int], patch_size: Sequence[int], embed_dim: int,\n                 enc_num_heads: int, enc_mlp_ratio: int, enc_depth: int,\n                 dec_num_heads: int, dec_mlp_ratio: int, dec_depth: int,\n                 perm_num: int, perm_forward: bool, perm_mirrored: bool,\n                 decode_ar: bool, refine_iters: int, dropout: float, **kwargs: Any) -> None:\n        tokenizer = Tokenizer(charset_train)\n        super().__init__()\n        \n        self.tokenizer = tokenizer\n        self.bos_id = self.tokenizer.bos_id\n        self.eos_id = self.tokenizer.eos_id\n        self.pad_id = self.tokenizer.pad_id\n        \n        self.batch_size = batch_size\n        self.lr = lr\n        self.warmup_pct = warmup_pct\n        self.weight_decay = weight_decay\n        \n        self._device = None\n        self.max_label_length = max_label_length\n        self.decode_ar = decode_ar\n        self.refine_iters = refine_iters\n\n        self.encoder = Encoder(img_size, patch_size, embed_dim=embed_dim, depth=enc_depth, num_heads=enc_num_heads,\n                               mlp_ratio=enc_mlp_ratio)\n        decoder_layer = DecoderLayer(embed_dim, dec_num_heads, embed_dim * dec_mlp_ratio, dropout)\n        self.decoder = Decoder(decoder_layer, num_layers=dec_depth, norm=nn.LayerNorm(embed_dim))\n\n        # Perm/attn mask stuff\n        self.rng = np.random.default_rng()\n        self.max_gen_perms = perm_num // 2 if perm_mirrored else perm_num\n        self.perm_forward = perm_forward\n        self.perm_mirrored = perm_mirrored\n\n        # We don't predict <bos> nor <pad>\n        self.head = nn.Linear(embed_dim, len(self.tokenizer) - 2)\n        self.text_embed = TokenEmbedding(len(self.tokenizer), embed_dim)\n\n        # +1 for <eos>\n        self.pos_queries = nn.Parameter(torch.Tensor(1, max_label_length + 1, embed_dim))\n        self.dropout = nn.Dropout(p=dropout)\n        # Encoder has its own init.\n        named_apply(partial(init_weights, exclude=['encoder']), self)\n        nn.init.trunc_normal_(self.pos_queries, std=.02)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        param_names = {'text_embed.embedding.weight', 'pos_queries'}\n        enc_param_names = {'encoder.' + n for n in self.encoder.no_weight_decay()}\n        return param_names.union(enc_param_names)\n\n    def encode(self, img: torch.Tensor):\n        return self.encoder(img)\n\n    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[Tensor] = None,\n               tgt_padding_mask: Optional[Tensor] = None, tgt_query: Optional[Tensor] = None,\n               tgt_query_mask: Optional[Tensor] = None):\n        N, L = tgt.shape\n        # <bos> stands for the null context. We only supply position information for characters after <bos>.\n        null_ctx = self.text_embed(tgt[:, :1])\n        tgt_emb = self.pos_queries[:, :L - 1] + self.text_embed(tgt[:, 1:])\n        tgt_emb = self.dropout(torch.cat([null_ctx, tgt_emb], dim=1))\n        if tgt_query is None:\n            tgt_query = self.pos_queries[:, :L].expand(N, -1, -1)\n        tgt_query = self.dropout(tgt_query)\n        return self.decoder(tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask)\n    \n    def forward_logits_loss(self, images: Tensor, labels: List[str]) -> Tuple[Tensor, Tensor, int]:\n        targets = self.tokenizer.encode(labels, self._device)\n        targets = targets[:, 1:]  # Discard <bos>\n        max_len = targets.shape[1] - 1  # exclude <eos> from count\n        logits = self.forward(images, max_len)\n        loss = F.cross_entropy(logits.flatten(end_dim=1), targets.flatten(), ignore_index=self.pad_id)\n        loss_numel = (targets != self.pad_id).sum()\n        return logits, loss, loss_numel\n    \n    def forward(self, images: Tensor, max_length: Optional[int] = None) -> Tensor:\n        testing = max_length is None\n        max_length = self.max_label_length if max_length is None else min(max_length, self.max_label_length)\n        bs = images.shape[0]\n        # +1 for <eos> at end of sequence.\n        num_steps = max_length + 1\n        memory = self.encode(images)\n\n        # Query positions up to `num_steps`\n        pos_queries = self.pos_queries[:, :num_steps].expand(bs, -1, -1)\n\n        # Special case for the forward permutation. Faster than using `generate_attn_masks()`\n        tgt_mask = query_mask = torch.triu(torch.full((num_steps, num_steps), float('-inf'), device=self._device), 1)\n\n        if self.decode_ar:\n            tgt_in = torch.full((bs, num_steps), self.pad_id, dtype=torch.long, device=self._device)\n            tgt_in[:, 0] = self.bos_id\n\n            logits = []\n            for i in range(num_steps):\n                j = i + 1  # next token index\n                tgt_out = self.decode(tgt_in[:, :j], memory, tgt_mask[:j, :j], tgt_query=pos_queries[:, i:j],\n                                      tgt_query_mask=query_mask[i:j, :j])\n                # the next token probability is in the output's ith token position\n                p_i = self.head(tgt_out)\n                logits.append(p_i)\n                if j < num_steps:\n                    # greedy decode. add the next token index to the target input\n                    tgt_in[:, j] = p_i.squeeze().argmax(-1)\n                    if testing and (tgt_in == self.eos_id).any(dim=-1).all():\n                        break\n\n            logits = torch.cat(logits, dim=1)\n        else:\n            # No prior context, so input is just <bos>. We query all positions.\n            tgt_in = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n            tgt_out = self.decode(tgt_in, memory, tgt_query=pos_queries)\n            logits = self.head(tgt_out)\n\n        if self.refine_iters:\n            # For iterative refinement, we always use a 'cloze' mask.\n            # We can derive it from the AR forward mask by unmasking the token context to the right.\n            query_mask[torch.triu(torch.ones(num_steps, num_steps, dtype=torch.bool, device=self._device), 2)] = 0\n            bos = torch.full((bs, 1), self.bos_id, dtype=torch.long, device=self._device)\n            for i in range(self.refine_iters):\n                # Prior context is the previous output.\n                tgt_in = torch.cat([bos, logits[:, :-1].argmax(-1)], dim=1)\n                tgt_padding_mask = ((tgt_in == self.eos_id).int().cumsum(-1) > 0)  # mask tokens beyond the first EOS token.\n                tgt_out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask,\n                                      tgt_query=pos_queries, tgt_query_mask=query_mask[:, :tgt_in.shape[1]])\n                logits = self.head(tgt_out)\n\n        return logits\n\n    def gen_tgt_perms(self, tgt):\n        \"\"\"Generate shared permutations for the whole batch.\n           This works because the same attention mask can be used for the shorter sequences\n           because of the padding mask.\n        \"\"\"\n        # We don't permute the position of BOS, we permute EOS separately\n        max_num_chars = tgt.shape[1] - 2\n        # Special handling for 1-character sequences\n        if max_num_chars == 1:\n            return torch.arange(3, device=self._device).unsqueeze(0)\n        perms = [torch.arange(max_num_chars, device=self._device)] if self.perm_forward else []\n        # Additional permutations if needed\n        max_perms = math.factorial(max_num_chars)\n        if self.perm_mirrored:\n            max_perms //= 2\n        num_gen_perms = min(self.max_gen_perms, max_perms)\n        if max_num_chars < 5:\n            if max_num_chars == 4 and self.perm_mirrored:\n                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]\n            else:\n                selector = list(range(max_perms))\n            perm_pool = torch.as_tensor(list(permutations(range(max_num_chars), max_num_chars)), device=self._device)[selector]\n            # If the forward permutation is always selected, no need to add it to the pool for sampling\n            if self.perm_forward:\n                perm_pool = perm_pool[1:]\n            perms = torch.stack(perms)\n            if len(perm_pool):\n                i = self.rng.choice(len(perm_pool), size=num_gen_perms - len(perms), replace=False)\n                perms = torch.cat([perms, perm_pool[i]])\n        else:\n            perms.extend([torch.randperm(max_num_chars, device=self._device) for _ in range(num_gen_perms - len(perms))])\n            perms = torch.stack(perms)\n        if self.perm_mirrored:\n            # Add complementary pairs\n            comp = perms.flip(-1)\n            # Stack in such a way that the pairs are next to each other.\n            perms = torch.stack([perms, comp]).transpose(0, 1).reshape(-1, max_num_chars)\n        bos_idx = perms.new_zeros((len(perms), 1))\n        eos_idx = perms.new_full((len(perms), 1), max_num_chars + 1)\n        perms = torch.cat([bos_idx, perms + 1, eos_idx], dim=1)\n        if len(perms) > 1:\n            perms[1, 1:] = max_num_chars + 1 - torch.arange(max_num_chars + 1, device=self._device)\n        return perms\n\n    def generate_attn_masks(self, perm):\n        \"\"\"Generate attention masks given a sequence permutation (includes pos. for bos and eos tokens)\n        :param perm: the permutation sequence. i = 0 is always the BOS\n        :return: lookahead attention masks\n        \"\"\"\n        sz = perm.shape[0]\n        mask = torch.zeros((sz, sz), device=self._device)\n        for i in range(sz):\n            query_idx = perm[i]\n            masked_keys = perm[i + 1:]\n            mask[query_idx, masked_keys] = float('-inf')\n        content_mask = mask[:-1, :-1].clone()\n        mask[torch.eye(sz, dtype=torch.bool, device=self._device)] = float('-inf')  # mask \"self\"\n        query_mask = mask[1:, :-1]\n        return content_mask, query_mask\n\n    def training_step(self, images, labels): #-> STEP_OUTPUT:\n        tgt = self.tokenizer.encode(labels, self._device)\n\n        # Encode the source sequence (i.e. the image codes)\n        memory = self.encode(images)\n\n        # Prepare the target sequences (input and output)\n        tgt_perms = self.gen_tgt_perms(tgt)\n        tgt_in = tgt[:, :-1]\n        tgt_out = tgt[:, 1:]\n        # The [EOS] token is not depended upon by any other token in any permutation ordering\n        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n\n        loss = 0\n        loss_numel = 0\n        n = (tgt_out != self.pad_id).sum().item()\n        for i, perm in enumerate(tgt_perms):\n            tgt_mask, query_mask = self.generate_attn_masks(perm)\n            out = self.decode(tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask)\n            logits = self.head(out).flatten(end_dim=1)\n            loss += n * F.cross_entropy(logits, tgt_out.flatten(), ignore_index=self.pad_id)\n            loss_numel += n\n            if i == 1:\n                tgt_out = torch.where(tgt_out == self.eos_id, self.pad_id, tgt_out)\n                n = (tgt_out != self.pad_id).sum().item()\n        loss /= loss_numel\n\n        return loss\n\n    def _eval_step(self, batch, validation: bool): #-> Optional[STEP_OUTPUT]:\n        images, labels = batch\n\n        correct = 0\n        total = 0\n        ned = 0\n        confidence = 0\n        label_length = 0\n        if validation:\n            logits, loss, loss_numel = self.forward_logits_loss(images, labels)\n        else:\n            logits = self.forward(images)\n            loss = loss_numel = None  # Only used for validation; not needed at test-time.\n\n        probs = logits.softmax(-1)\n        preds, probs = self.tokenizer.decode(probs)\n        for pred, prob, gt in zip(preds, probs, labels):\n            confidence += prob.prod().item()\n            ned += edit_distance(pred, gt) / max(len(pred), len(gt))\n            pred = pred.replace('\u0932','[UNK]').replace('###','[UNK]').replace(' ','').replace('[U]','[UNK]')\n            gt = gt.replace('###','[UNK]').replace(' ','')\n            if pred == gt:\n                correct += 1\n            total += 1\n            label_length += len(pred)\n        return dict(output=BatchResult(total, correct, ned, confidence, label_length, loss, loss_numel))\n\n    @staticmethod\n    def _aggregate_results(outputs) -> Tuple[float, float, float]:\n        if not outputs:\n            return 0., 0., 0.\n        total_loss = 0\n        total_loss_numel = 0\n        total_n_correct = 0\n        total_norm_ED = 0\n        total_size = 0\n        for result in outputs:\n            result = result['output']\n            total_loss += result.loss_numel * result.loss\n            total_loss_numel += result.loss_numel\n            total_n_correct += result.correct\n            total_norm_ED += result.ned\n            total_size += result.num_samples\n        acc = total_n_correct / total_size\n        ned = (1 - total_norm_ED / total_size)\n        loss = total_loss / total_loss_numel\n        return acc, ned, loss\n\n    def validation_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, True)\n\n    def test_step(self, batch, batch_idx): #-> Optional[STEP_OUTPUT]:\n        return self._eval_step(batch, False)", "    \n\n"]}
