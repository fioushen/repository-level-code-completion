{"filename": "qax/tracer.py", "chunked_list": ["import warnings\n\nimport jax\nfrom jax.core import get_aval, Tracer, full_lower\n\nfrom .implicit_array import ImplicitArray\n\n\n\nclass ImplicitArrayTracer(Tracer):\n    def __init__(self, trace, value):\n        super().__init__(trace)\n        self.value = value\n\n    @property\n    def aval(self):\n        if isinstance(self.value, ImplicitArray):\n            return jax.ShapedArray(self.value.shape, self.value.dtype)\n        return get_aval(self.value)\n\n    def full_lower(self):\n        if isinstance(self.value, ImplicitArray):\n            return self\n\n        return full_lower(self.value)", "\nclass ImplicitArrayTracer(Tracer):\n    def __init__(self, trace, value):\n        super().__init__(trace)\n        self.value = value\n\n    @property\n    def aval(self):\n        if isinstance(self.value, ImplicitArray):\n            return jax.ShapedArray(self.value.shape, self.value.dtype)\n        return get_aval(self.value)\n\n    def full_lower(self):\n        if isinstance(self.value, ImplicitArray):\n            return self\n\n        return full_lower(self.value)", "\nclass ImplicitArrayTrace(Trace):\n    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n\n    def process_primitive(self, primitive, tracers, params):\n        vals = [t.value for t in tracers]\n        n_implicit = sum(isinstance(v, ImplicitArray) for v in vals)\n        assert 1 <= n_implicit <= 2\n        if n_implicit == 2:\n            warnings.warn(f'Encountered op {primitive.name} with two implicit inputs so second will be materialized.')\n            vals[1] = vals[1].materialize()", ""]}
{"filename": "qax/primitives.py", "chunked_list": ["from abc import ABC\nfrom itertools import count\n\nimport jax\n\nfrom plum import Dispatcher, Function\n\nclass ArrayValue(ABC):\n    pass\n", "\nArrayValue.register(jax.Array)\n\n_dispatch = Dispatcher()\n\n_primitive_ids = count()\n\ndef get_lax_primitive_by_name(name):\n    return getattr(jax.lax, f'{name}_p')\n\ndef get_primitive_handler(primitive):\n    if isinstance(primitive, str):\n        primitive = get_lax_primitive_by_name(primitive)\n    handler = _dispatch.functions.get(primitive)\n    if handler is None:\n        def _not_impl_handler(primitive : jax.core.Primitive, *args, **kwargs):\n            return NotImplemented\n        _not_impl_handler.__doc__ = 'Default handler for {primitive.name}'\n        handler = Function(_not_impl_handler)\n        handler.register(_not_impl_handler, precedence=-1e9)\n        handler.__name__ = f'{primitive.name}_{next(_primitive_ids)}'\n        _dispatch.functions[primitive] = handler\n    return handler", "\ndef get_primitive_handler(primitive):\n    if isinstance(primitive, str):\n        primitive = get_lax_primitive_by_name(primitive)\n    handler = _dispatch.functions.get(primitive)\n    if handler is None:\n        def _not_impl_handler(primitive : jax.core.Primitive, *args, **kwargs):\n            return NotImplemented\n        _not_impl_handler.__doc__ = 'Default handler for {primitive.name}'\n        handler = Function(_not_impl_handler)\n        handler.register(_not_impl_handler, precedence=-1e9)\n        handler.__name__ = f'{primitive.name}_{next(_primitive_ids)}'\n        _dispatch.functions[primitive] = handler\n    return handler", "\ndef primitive_handler(primitives, precedence=0):\n    if isinstance(primitives, (str, jax.core.Primitive)):\n        primitives = [primitives]\n    def decorator(fn):\n        for primitive in primitives:\n            handler = get_primitive_handler(primitive)\n            handler.register(fn, precedence=precedence)\n    return decorator\n\ndef default_handler(primitive, *args, **params):\n    subfuns, bind_params = primitive.get_bind_params(params)\n    return primitive.bind(*subfuns, *args, **bind_params)", "\ndef default_handler(primitive, *args, **params):\n    subfuns, bind_params = primitive.get_bind_params(params)\n    return primitive.bind(*subfuns, *args, **bind_params)\n"]}
{"filename": "qax/__init__.py", "chunked_list": ["from .implicit import implicit_array\nfrom .implicit.implicit_array import ImplicitArray, use_implicit_args, aux_field, UninitializedAval\nfrom .primitives import default_handler, primitive_handler, ArrayValue\n\nfrom .utils import EmptyNode, materialize_nested, freeze_keys\nfrom .common import type_utils\n\nfrom . import symbols\n", ""]}
{"filename": "qax/utils.py", "chunked_list": ["from .implicit.implicit_utils import *\nfrom .common.utils import *\n"]}
{"filename": "qax/constants.py", "chunked_list": ["# WARNING: This file is obviously super incomplete, and is\n# currently just for convenience in testing.\n\nCOMMUTATIVE_OPS = frozenset([\n    'add',\n    'bitwise_and',\n    'bitwise_or',\n    'bitwise_xor',\n    'eq',\n    'max',", "    'eq',\n    'max',\n    'min',\n    'mul',\n    'ne',\n])\n\nELEMENTWISE_UNOPS = frozenset([\n    'abs',\n    'acos',", "    'abs',\n    'acos',\n    'acosh',\n    'asin',\n    'asinh',\n    'atan',\n    'atanh',\n    'bessel_i0e',\n    'bessel_i1e',\n    'cbrt',", "    'bessel_i1e',\n    'cbrt',\n    'ceil',\n    'clz',\n    'conj',\n    'convert_element_type',\n    'copy',\n    'cos',\n    'cosh',\n    'digamma',", "    'cosh',\n    'digamma',\n    'erf_inv',\n    'erf',\n    'erfc',\n    'exp',\n    'expm1',\n    'floor',\n    'imag',\n    'integer_pow',", "    'imag',\n    'integer_pow',\n    'is_finite',\n    'lgamma',\n    'log1p',\n    'log',\n    'logistic',\n    'neg',\n    'not',\n    'population_count',", "    'not',\n    'population_count',\n    'real',\n    'reduce_precision',\n    'round',\n    'rsqrt',\n    'sign',\n    'sin',\n    'sinh',\n    'sqrt',", "    'sinh',\n    'sqrt',\n    'tan',\n    'tanh',\n])\n\nELEMENTWISE_BINOPS = frozenset([\n    'add',\n    'and',\n    'atan2',", "    'and',\n    'atan2',\n    'complex',\n    'div',\n    'eq',\n    'ge',\n    'gt',\n    'igamma_grad_a',\n    'igamma',\n    'igammac',", "    'igamma',\n    'igammac',\n    'le',\n    'lt',\n    'max',\n    'min',\n    'mul',\n    'ne',\n    'nextafter',\n    'or',", "    'nextafter',\n    'or',\n    'pow',\n    'random_gamma_grad',\n    'rem',\n    'shift_left',\n    'shift_right_arithmetic',\n    'shift_right_logical',\n    'sub',\n    'xor',", "    'sub',\n    'xor',\n])\n\nREDUCTION_OPS = frozenset([\n    'argmax',\n    'argmin',\n    'reduce_and',\n    'reduce_max',\n    'reduce_min',", "    'reduce_max',\n    'reduce_min',\n    'reduce_or',\n    'reduce_prod',\n    'reduce_sum',\n    'reduce_xor',\n])\n\nCUMULATIVE_REDUCTION_OPS = frozenset([\n    'cumlogsumexp',", "CUMULATIVE_REDUCTION_OPS = frozenset([\n    'cumlogsumexp',\n    'cummax',\n    'cummin',\n    'cumprod',\n    'cumsum',\n])\n"]}
{"filename": "qax/symbols.py", "chunked_list": ["\"\"\"\nImplementation of SymbolicConstant, an ImplicitArray subclass representing jnp.full(shape, value, dtype).\nThe data is stored in pytree auxilary data, and all supported operations are run at compile time.\nThis is useful for forcing constant folding in cases where the JIT would not know that an argument\nis constant, or for lowering the cost of constant folding on large constant arrays.\nThese do not respect NaNs in certain ways, e.g. 0 * NaN = 0, max(inf, NaN) = inf\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom functools import partial", "from dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\n\n", "\n\nfrom .implicit.implicit_array import ArrayValue, ImplicitArray, UninitializedAval, aux_field, use_implicit_args\nfrom .implicit import implicit_utils as iu\nfrom .primitives import get_primitive_handler, primitive_handler, default_handler\nfrom .common.type_utils import Complement\nfrom .constants import ELEMENTWISE_BINOPS, ELEMENTWISE_UNOPS\n\n_GENERAL = -2\n_SPECIALIZED = -1", "_GENERAL = -2\n_SPECIALIZED = -1\n\ndef _get_shape_dtype(x, shape, dtype):\n    if shape is None:\n        shape = np.shape(x)\n    else:\n        shape = jax.core.canonicalize_shape(shape)\n\n    if dtype is None:\n        jax.lax.dtype(x)\n    return shape, dtype", "\ndef _out_shape_dtype(primitive, *args, **kwargs):\n    out_aval = jax.eval_shape(\n        partial(default_handler, primitive, **kwargs),\n        *(jax.core.get_aval(x) for x in args)\n    )\n    return jax.tree_map(\n        lambda x: (x.shape, x.dtype),\n        out_aval\n    )", "\ndef symbolic_zero_like(x, shape=None, dtype=None):\n    dtype = jax.lax.dtype(x) if dtype is None else dtype\n    return symbolic_full_like(x, 0, shape=shape, dtype=dtype)\n\ndef symbolic_full_like(x, fill_value, shape=None, dtype=None):\n    shape, _ = _get_shape_dtype(x, shape, None)\n    if dtype is None:\n        dtype = jax.lax.dtype(fill_value)\n\n    return SymbolicConstant(fill_value, shape=shape, dtype=dtype)", "\n@dataclass\nclass SymbolicConstant(ImplicitArray):\n    value : Any = aux_field()\n    weak_type : bool = aux_field(default=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n        with jax.ensure_compile_time_eval():\n            self.value = jnp.asarray(self.value, dtype=self.dtype)\n\n    def compute_dtype(self):\n        return jax.lax.dtype(self.value)\n\n    def materialize(self):\n        return jnp.full(self.shape, self.value, dtype=self.dtype)\n\n    def copy(self):\n        return jax.tree_map(lambda x: x, self)", "\n@use_implicit_args\ndef broadcast_to(val, shape):\n    return jnp.broadcast_to(val, shape)\n\n@use_implicit_args\ndef astype(val, dtype):\n    return val.astype(dtype)\n\n@primitive_handler([", "\n@primitive_handler([\n    'reshape',\n    'broadcast_in_dim',\n    'reduce_min',\n    'reduce_max',\n    'reduce_or',\n    'reduce_and'\n])\ndef unchanged_value_op(primitive, sym : SymbolicConstant, **kwargs):\n    out_shape, out_dtype = _out_shape_dtype(primitive, sym, **kwargs)\n    return SymbolicConstant(sym.value, shape=out_shape, dtype=out_dtype)", "])\ndef unchanged_value_op(primitive, sym : SymbolicConstant, **kwargs):\n    out_shape, out_dtype = _out_shape_dtype(primitive, sym, **kwargs)\n    return SymbolicConstant(sym.value, shape=out_shape, dtype=out_dtype)\n\ndef _op_and_reshape(primitive, lhs, rhs, flip=False):\n    \"\"\"\n    Close over one arg so we can do math at tracing time, but let the other one get traced\n    \"\"\"\n    if flip:\n        lhs, rhs = (rhs, lhs)\n    @use_implicit_args\n    def inner(arg):\n        other = rhs\n        if flip:\n            arg, other = (other, arg)\n\n        result = default_handler(primitive, arg, other)\n        return result\n    return inner(rhs)", "\ndef special_case_binop(name, identity=None, annihilator=None, flip=False):\n    lhs_type = SymbolicConstant\n    rhs_type = Complement[ArrayValue, SymbolicConstant]\n    if flip:\n        lhs_type, rhs_type = rhs_type, lhs_type\n\n    @primitive_handler(name, precedence=_SPECIALIZED)\n    def handler(primitive, lhs : lhs_type, rhs : rhs_type, **kwargs):\n        out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)\n        with jax.ensure_compile_time_eval():\n            if lhs.value == identity:\n                return broadcast_to(astype(rhs, out_dtype), out_shape)\n\n            if lhs.value == annihilator:\n                return SymbolicConstant(lhs.value, shape=out_shape, dtype=out_dtype)\n\n            return _op_and_reshape(primitive, lhs.value, rhs)", "\nspecial_case_binop('add', identity=0)\nspecial_case_binop('mul', identity=1, annihilator=0)\nspecial_case_binop('and', annihilator=0)\nspecial_case_binop('or', identity=0)\nspecial_case_binop('xor', identity=0)\n\nspecial_case_binop('sub', identity=0, flip=True)\nspecial_case_binop('div', identity=1, flip=True)\nspecial_case_binop('exp', identity=1, flip=True)", "special_case_binop('div', identity=1, flip=True)\nspecial_case_binop('exp', identity=1, flip=True)\n\nspecial_case_binop('min', identity=float('inf'), annihilator=float('-inf'))\nspecial_case_binop('max', identity=float('-inf'), annihilator=float('inf'))\n\ndef eval_default_handler(primitive, *args, **kwargs):\n    with jax.ensure_compile_time_eval():\n        result = primitive.bind(*args, **kwargs)\n    return result", "\n@primitive_handler(ELEMENTWISE_UNOPS, precedence=_GENERAL)\ndef handle_unop(primitive, sym : SymbolicConstant, **kwargs):\n    print(f'Handling {primitive} with {sym}')\n    new_val = eval_default_handler(primitive, sym.value, **kwargs)\n    return symbolic_full_like(sym, new_val)\n\n@primitive_handler(ELEMENTWISE_BINOPS, precedence=_GENERAL)\ndef handle_binop(primitive, lhs : SymbolicConstant, rhs : SymbolicConstant, **kwargs):\n    out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)\n    new_val = eval_default_handler(primitive, lhs.value, rhs.value, **kwargs)\n    return symbolic_full_like(lhs, new_val, shape=out_shape, dtype=out_dtype)", "def handle_binop(primitive, lhs : SymbolicConstant, rhs : SymbolicConstant, **kwargs):\n    out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)\n    new_val = eval_default_handler(primitive, lhs.value, rhs.value, **kwargs)\n    return symbolic_full_like(lhs, new_val, shape=out_shape, dtype=out_dtype)\n\n@primitive_handler(['reduce_sum', 'reduce_prod'])\ndef reduce_sum(primitive, sym : SymbolicConstant, *, axes):\n    out_shape, out_dtype = _out_shape_dtype(primitive, sym, axes=axes)\n    with jax.ensure_compile_time_eval():\n        if sym.value == 0:\n            return SymbolicConstant(0, shape=out_shape, dtype=out_dtype)\n\n        orig_size = np.prod(sym.shape)\n        new_size = np.prod(out_shape)\n\n        n_combined = orig_size // new_size\n\n        new_val = sym.value\n        if primitive.name == 'reduce_sum':\n            new_val = new_val * n_combined\n        else:\n            new_val = new_val ** n_combined\n\n    return SymbolicConstant(new_val, shape=out_shape, dtype=out_dtype)", "\n@primitive_handler('select_n')\ndef handle_select_n(primitive, cond_val, *arg_vals : SymbolicConstant):\n    if len(set(val.value.item() for val in arg_vals)) != 1:\n        return NotImplemented\n\n    out_shape, out_dtype = _out_shape_dtype(primitive, cond_val, *arg_vals)\n    return SymbolicConstant(arg_vals[0].value, shape=out_shape, dtype=out_dtype)\n\n", "\n"]}
{"filename": "qax/common/__init__.py", "chunked_list": ["\n"]}
{"filename": "qax/common/utils.py", "chunked_list": ["from functools import wraps\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import tree_util\nfrom jax.dtypes import float0\nimport optax\n\nfrom ..implicit.implicit_array import use_implicit_args\nfrom ..symbols import SymbolicConstant", "from ..implicit.implicit_array import use_implicit_args\nfrom ..symbols import SymbolicConstant\n\ndef vmap_all_but_one(f, axis, out_ndim=0):\n    \"\"\"\n    Repeatedly calls vmap to map over all axes except for `axis.`\n    All args will be mapped on the same dimensions.\n    \"\"\"\n    @wraps(f)\n    def inner(*args):\n        n_dim = args[0].ndim\n        if axis >= n_dim:\n            raise ValueError(f'Axis {axis} is out of bounds for array of dimension {n_dim}')\n        fn = f\n        vmap_dim = 1\n        out_dim = out_ndim\n        for i in reversed(range(n_dim)):\n            if i == axis:\n                vmap_dim = 0\n                out_dim = 0\n            else:\n                fn = jax.vmap(fn, vmap_dim, out_dim)\n        return fn(*args)\n    return inner", "\ndef freeze_subtrees(optimizer : optax.GradientTransformation, label_fn, use_scalar_zeros=False):\n    \"\"\"\n    Utility which wraps an optimizer such that subtrees specified by\n    label_fn will receive zeros as updates.\n    Subtrees to be frozen should be labeled with \"freeze\"\n    and all other subtrees should be labeled with \"train\"\n    \"\"\"\n    multi_transformed_optimizer = optax.multi_transform(\n        {\n            'freeze': set_to_zero_scalar() if use_scalar_zeros else optax.set_to_zero(),\n            'train': optimizer\n        },\n        label_fn\n    )\n\n    def new_update(grads, opt_state, params):\n        def map_float0(param, grad):\n            if grad.dtype == float0:\n                return jnp.zeros((), param.dtype) if use_scalar_zeros else jnp.zeros_like(param)\n            return grad\n\n        fixed_grads = jax.tree_map(map_float0, params, grads)\n        return multi_transformed_optimizer.update(fixed_grads, opt_state, params)\n\n    return optax.GradientTransformation(\n        multi_transformed_optimizer.init,\n        new_update\n    )", "\ndef freeze_keys(optimizer : optax.GradientTransformation, arr_type, keys, use_scalar_zeros=False) -> optax.GradientTransformation:\n    keys = set(keys)\n    def label_leaf(leaf):\n        if not isinstance(leaf, arr_type):\n            return 'train'\n\n        children, aux_data = leaf.tree_flatten_with_keys()\n        labels = ['freeze' if key in keys else 'train' for key, _ in children]\n        struct = leaf.tree_unflatten(aux_data, labels)\n        return struct\n\n    def label_fn(root):\n        return jax.tree_map(label_leaf, root, is_leaf=lambda x: isinstance(x, arr_type))\n\n    return freeze_subtrees(optimizer, label_fn, use_scalar_zeros=use_scalar_zeros)", "\ndef apply_updates(params : optax.Params, updates : optax.Updates) -> optax.Params:\n    \"\"\"\n    Like optax.apply_updates, but updates can be SymbolicConstant instances\n    \"\"\"\n    updates_flat, update_struct = tree_util.tree_flatten(updates, is_leaf=lambda x: isinstance(x, SymbolicConstant))\n    semi_flat_params = update_struct.flatten_up_to(params)\n\n    updated_flat = use_implicit_args(optax.apply_updates)(semi_flat_params, updates_flat)\n    updated = update_struct.unflatten(updated_flat)\n    return updated", "\ndef set_to_zero_scalar() -> optax.GradientTransformation:\n    \"\"\"\n    Returns a gradient transformation that sets all gradients to 0 in order to\n    make downstream constant folding cheaper.\n    \"\"\"\n    def init_fn(params):\n        del params\n        return optax.EmptyState()\n\n    def update_fn(updates, state, params=None):\n        return jax.tree_map(lambda x: jnp.zeros((), x.dtype), updates), state\n\n    return optax.GradientTransformation(init_fn, update_fn)", ""]}
{"filename": "qax/common/type_utils.py", "chunked_list": ["from typing import Any, Optional, Tuple\n\nfrom beartype.vale import IsInstance\nfrom plum import dispatch, parametric\n\nclass _ComplementMeta(type):\n    def __instancecheck__(self, x):\n        a, b = self.type_parameter\n        return (\n            a is None or (\n                isinstance(x, a) and not isinstance(x, b)\n            )\n        )", "\n@parametric\nclass Complement(metaclass=_ComplementMeta):\n    \"\"\"\n    Relative complement\n    I.e. Complement[A, B] = A - B\n    \"\"\"\n    @classmethod\n    @dispatch\n    def __init_type_parameter__(\n        cls,\n        a: Optional[Any],\n        b: Optional[Any],\n    ):\n        return a, b\n\n    @classmethod\n    @dispatch\n    def __le_type_parameter__(\n        cls,\n        left: Tuple[Optional[Any], Optional[Any]],\n        right: Tuple[Optional[Any], Optional[Any]],\n    ):\n        a_left, b_left = left\n        a_right, b_right = right\n\n        return issubclass(a_left, a_right) and issubclass(b_right, b_left)", "\n"]}
{"filename": "qax/implicit/implicit_array.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom dataclasses import dataclass, field, fields, is_dataclass\nfrom functools import partial, wraps\nfrom itertools import chain\nfrom typing import ClassVar, Optional\nimport warnings\n\nimport jax", "\nimport jax\nfrom jax.api_util import flatten_fun, flatten_fun_nokwargs\nfrom jax import core\nimport jax.linear_util as lu\nimport jax.interpreters.partial_eval as pe\nfrom jax.tree_util import register_pytree_with_keys_class\nimport jax.numpy as jnp\n\nfrom jax._src.typing import DTypeLike, Shape", "\nfrom jax._src.typing import DTypeLike, Shape\n\nfrom .. import constants\nfrom ..primitives import ArrayValue, get_primitive_handler\n\nfrom . import implicit_utils as iu\n\ndef _with_implicit_flat(fun: lu.WrappedFun) -> lu.WrappedFun:\n  # Splitting to avoid leaks based on https://github.com/google/jax/blob/0dffdf4645db7bf7a9fadd4bcfe9ec0368a8ecb9/jax/_src/interpreters/batching.py#L539\n    f = _implicit_inner(fun)\n    return _implicit_outer(f)", "def _with_implicit_flat(fun: lu.WrappedFun) -> lu.WrappedFun:\n  # Splitting to avoid leaks based on https://github.com/google/jax/blob/0dffdf4645db7bf7a9fadd4bcfe9ec0368a8ecb9/jax/_src/interpreters/batching.py#L539\n    f = _implicit_inner(fun)\n    return _implicit_outer(f)\n\n@lu.transformation\ndef _implicit_outer(*in_vals):\n    with core.new_main(ImplicitArrayTrace) as main:\n        outs = yield (main, *in_vals), {}\n        del main\n    yield outs", "\n@lu.transformation\ndef _implicit_inner(main, *in_vals):\n    trace = main.with_cur_sublevel()\n    in_tracers = [\n        ImplicitArrayTracer(trace, val) if isinstance(val, ImplicitArray) else val\n        for val in in_vals\n    ]\n    outs = yield in_tracers, {}\n    out_vals = [trace.full_raise(t).value for t in outs]\n    yield out_vals", "\ndef use_implicit_args(f):\n    \"\"\"\n    Decorator which allows a function to accept arguments which subclass ImplicitArray, possibly\n    including further ImplicitArray instances as children.\n    Any number of arguments (including 0) may be ImplicitArrays.\n    \"\"\"\n    @wraps(f)\n    def implicit_f(*args, **kwargs):\n        flat_args, in_tree = iu.tree_flatten_with_implicit((args, kwargs))\n        f_flat, out_tree = flatten_fun(lu.wrap_init(f), in_tree)\n        f_wrapped = _with_implicit_flat(f_flat)\n        outs_flat = f_wrapped.call_wrapped(*flat_args)\n        return out_tree().unflatten(outs_flat)\n\n    return implicit_f", "\ndef aux_field(metadata=None, **kwargs):\n    metadata = dict(metadata) if metadata else {}\n    metadata['implicit_array_aux'] = True\n    return field(metadata=metadata, **kwargs)\n\nclass UninitializedAval(Exception):\n    def __init__(self, kind):\n        super().__init__(_AVAL_ERROR_MESSAGE.format(kind))\n", "\n# This descriptor and the below context manager support discovering the aval\n# of an ImplicitArray. We don't want to throw an error just because a shape\n# wasn't passed, since it may be possible to infer it via materialization\nclass _AvalDescriptor:\n    def __set_name__(self, owner, name):\n        self._name = f'_{name}'\n\n    def __get__(self, obj, owner=None):\n        if obj is None:\n            return None\n        result = getattr(obj, self._name, None)\n        if result is None:\n            raise UninitializedAval(kind=self._name[1:])\n        return result\n\n    def __set__(self, obj, value):\n        setattr(obj, self._name, value)", "\n# Context manager used for disabling UninitializedAval errors\n# during tree flattening only\n_aval_discovery = ContextVar('aval_discovery', default=False)\n@contextmanager\ndef _aval_discovery_context():\n    token = _aval_discovery.set(True)\n    try:\n        yield\n    finally:\n        _aval_discovery.reset(token)", "\n@dataclass\nclass _ImplicitArrayBase(ArrayValue,ABC):\n    commute_ops : ClassVar[bool] = True\n    default_shape : ClassVar[Optional[Shape]] = None\n    default_dtype : ClassVar[Optional[DTypeLike]] = None\n\n    shape : Optional[Shape] = aux_field(kw_only=True, default=None)\n    dtype : DTypeLike = aux_field(kw_only=True, default=None)\n", "\n@dataclass\nclass ImplicitArray(_ImplicitArrayBase):\n    \"\"\"\n    Abstract class for representing an abstract array of a given shape/dtype without actually instantiating it.\n    Subclasses must implement the materialize method, which defines the relationship between the implicit array\n    and the value it represents. Subclasses are valid arguments to functions decorated with qax.use_implicit_args.\n\n    All subclasses are automatically registered as pytrees using jax.tree_util.register_pytree_with_keys_class.\n    Any dataclass attributes added will be included as children, unless they are decorated with qax.aux_field\n    in which case they are passed as auxiliary data during flattening.\n\n    The represented shape and dtype may be defined in any of the following ways:\n        - Explicitly passing shape/dtype keyword arguments at initialization\n        - Overriding the default_shape/default_dtype class variables\n        - Overriding the compute_shape/compute_dtype methods, which are called during __post_init__\n        - Overriding __post_init__ and manually setting shape/dtype before calling super().__post_init__\n        - None of the above, in which case an shape/dtype will be inferred by by running jax.eval_shape()\n          on the subclass's materialize method.\n    \"\"\"\n\n    shape = _AvalDescriptor()\n    dtype = _AvalDescriptor()\n\n    def __post_init__(self):\n        try:\n            aval = _get_materialization_aval(self)\n        except UninitializedAval:\n            # Materialization depends on currently uninitialized shape/dtype\n            aval = None\n\n        shape = None\n        try:\n            shape = self.shape\n        except UninitializedAval as e:\n            shape = self.shape = self.compute_shape()\n\n        if aval is not None:\n            if shape is None:\n                self.shape = aval.shape\n            elif shape != aval.shape:\n                warnings.warn(f'ImplicitArray shape {shape} does not match materialization shape {aval.shape}')\n        elif shape is None:\n            raise UninitializedAval('shape')\n\n        dtype = None\n        try:\n            dtype = self.dtype\n        except UninitializedAval as e:\n            dtype = self.dtype = self.compute_dtype()\n\n        if dtype is None and aval is None:\n            # We have a shape but not a dtype, try once again to infer the dtype\n            aval = _get_materialization_aval(self)\n\n        if aval is not None:\n            if dtype is None:\n                self.dtype = aval.dtype\n            elif dtype != aval.dtype:\n                warnings.warn(f'ImplicitArray dtype {dtype} does not match materialization dtype {aval.dtype}')\n        elif dtype is None:\n            raise UninitializedAval('dtype')\n\n\n\n    def compute_shape(self):\n        \"\"\"\n        Override this method if the subclass instance's shape should be computed based on its other properties.\n        Returns: shape\n        \"\"\"\n        return self.default_shape\n\n    def compute_dtype(self):\n        \"\"\"\n        Override this method if the subclass instance's dtype should be computed based on its other properties.\n        Returns: dtype\n        \"\"\"\n        return self.default_dtype\n\n    @property\n    def aval(self):\n        return core.ShapedArray(self.shape, self.dtype)\n\n    @classmethod\n    def default_handler(cls, primitive, *args, params=None):\n        if params is None:\n            params = {}\n        return materialize_handler(primitive, *args, params=params)\n\n    @abstractmethod\n    def materialize(self):\n        pass\n\n    def tree_flatten_with_keys(self):\n        children = []\n        aux_data = []\n        for name, is_aux in _get_names_and_aux(self):\n            try:\n                value = getattr(self, name)\n            except UninitializedAval:\n                if not _aval_discovery.get():\n                    raise\n                value = None\n            if is_aux:\n                aux_data.append(value)\n            else:\n                children.append((name, value))\n\n        return children, aux_data\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        child_it = iter(children)\n        aux_it = iter(aux_data)\n        obj = cls.__new__(cls)\n        for name, is_aux in _get_names_and_aux(cls):\n            value = next(aux_it if is_aux else child_it)\n            setattr(obj, name, value)\n\n        return obj\n\n    def handle_primitive(self, primitive, *args, params):\n        handler = lu.wrap_init(partial(get_primitive_handler(primitive), primitive))\n        use_params = params\n\n        if len(args) == 2 and self.commute_ops:\n            args, use_params = _maybe_swap_args(primitive.name, args, use_params)\n\n        #maybe_kwargs = {'params': params} if params else {}\n        flat_args, in_tree = iu.flatten_one_implicit_layer((args, params))\n        flat_handler, out_tree = flatten_fun(handler, in_tree)\n\n        result = use_implicit_args(flat_handler.call_wrapped)(*flat_args)\n        return jax.tree_util.tree_unflatten(out_tree(), result)\n\n    def __init_subclass__(cls, commute_ops=True, **kwargs):\n        super().__init_subclass__(**kwargs)\n\n        if not is_dataclass(cls):\n            raise TypeError(f'{cls.__name__} must be a dataclass')\n        core.pytype_aval_mappings[cls] = lambda x: x.aval\n        register_pytree_with_keys_class(cls)\n        return cls", "\ndef _get_names_and_aux(obj):\n    for val in fields(obj):\n        yield val.name, bool(val.metadata.get('implicit_array_aux'))\n\ndef _materialize_all(it):\n    return [iu.materialize_nested(val) if isinstance(val, ImplicitArray) else val for val in it]\n\ndef _maybe_swap_args(op_name, args, params):\n    if isinstance(args[0], ImplicitArray):\n        return args, params\n    if op_name in constants.COMMUTATIVE_OPS:\n        return args[::-1], params\n\n    return args, params", "def _maybe_swap_args(op_name, args, params):\n    if isinstance(args[0], ImplicitArray):\n        return args, params\n    if op_name in constants.COMMUTATIVE_OPS:\n        return args[::-1], params\n\n    return args, params\n\nclass ImplicitArrayTracer(core.Tracer):\n    def __init__(self, trace, value):\n        super().__init__(trace)\n        self.value = value\n\n    @property\n    def aval(self):\n        if isinstance(self.value, ImplicitArray):\n            return self.value.aval\n        return core.get_aval(self.value)\n\n    def full_lower(self):\n        if isinstance(self.value, ImplicitArray):\n            return self\n\n        return core.full_lower(self.value)", "class ImplicitArrayTracer(core.Tracer):\n    def __init__(self, trace, value):\n        super().__init__(trace)\n        self.value = value\n\n    @property\n    def aval(self):\n        if isinstance(self.value, ImplicitArray):\n            return self.value.aval\n        return core.get_aval(self.value)\n\n    def full_lower(self):\n        if isinstance(self.value, ImplicitArray):\n            return self\n\n        return core.full_lower(self.value)", "\nclass ImplicitArrayTrace(core.Trace):\n    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n\n    def process_primitive(self, primitive, tracers, params):\n        outs = NotImplemented\n        vals = [t.value for t in tracers]\n        implicit_idx = next(i for i, v in enumerate(vals) if isinstance(v, ImplicitArray))\n\n        # First try to handle the primitive using custom handlers\n        outs = vals[implicit_idx].handle_primitive(primitive, *vals, params=params)\n\n        if outs is NotImplemented:\n            # For higher order primitives most users won't implement custom\n            # logic, so there shouldn't be a warning\n            if primitive.name in _default_handlers:\n                outs = _default_handlers[primitive.name](primitive, *vals, params=params)\n            else:\n                warnings.warn(f'Primitive {primitive.name} was not handled by class {vals[implicit_idx].__class__.__name__}, so implicit args will be materialized.')\n\n        if outs is NotImplemented:\n            outs = vals[implicit_idx].default_handler(primitive, *vals, params=params)\n\n        if primitive.multiple_results:\n            return [ImplicitArrayTracer(self, out) for out in outs]\n        return ImplicitArrayTracer(self, outs)", "\ndef wrap_jaxpr(jaxpr, vals_with_implicits, return_closed=True):\n    if isinstance(jaxpr, jax.core.ClosedJaxpr):\n        literals = jaxpr.literals\n        jaxpr = jaxpr.jaxpr\n    else:\n        literals = []\n\n    wrapped_fn = lu.wrap_init(use_implicit_args(partial(core.eval_jaxpr, jaxpr)))\n    flat_args, in_tree = jax.tree_util.tree_flatten((literals, *vals_with_implicits))\n    flat_fn, out_tree = flatten_fun_nokwargs(wrapped_fn, in_tree)\n\n    new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fn, [core.get_aval(v) for v in flat_args])\n\n    ret = (jax.core.ClosedJaxpr(new_jaxpr, consts),) if return_closed else (new_jaxpr, consts)\n    return *ret, flat_args, out_tree()", "\ndef _transform_jaxpr_output(jaxpr, jaxpr_args, orig_out_struct, out_transform):\n    def eval_fn(literals, *args):\n        output = use_implicit_args(partial(core.eval_jaxpr, jaxpr.jaxpr))(literals, *args)\n        unflattened_output = orig_out_struct.unflatten(output)\n        return out_transform(unflattened_output)\n\n    wrapped = lu.wrap_init(eval_fn)\n\n    flat_args, in_tree = jax.tree_util.tree_flatten((jaxpr.literals, *jaxpr_args))\n    flat_fn, out_tree = flatten_fun_nokwargs(wrapped, in_tree)\n    new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fn, [core.get_aval(v) for v in flat_args])\n\n    return jax.core.ClosedJaxpr(new_jaxpr, consts), out_tree()", "\ndef _match_branches(branches, arg_vals):\n    out_avals = []\n    new_jaxprs = []\n    flat_inputs = None\n    branch_out_struct = None\n    for branch in branches:\n        new_jaxpr, flat_inputs, branch_out_struct = wrap_jaxpr(branch, arg_vals)\n        new_jaxprs.append((new_jaxpr, branch_out_struct))\n        out_avals.append(\n            branch_out_struct.unflatten(\n                jax.eval_shape(\n                    partial(core.eval_jaxpr, new_jaxpr.jaxpr), new_jaxpr.literals, *flat_inputs\n                )\n            )\n        )\n\n    out_transforms = iu.get_common_prefix_transforms(out_avals)\n    new_branches = []\n    out_struct = None\n    for (new_jaxpr, orig_out_struct), transform in zip(new_jaxprs, out_transforms):\n        new_jaxpr, out_struct = _transform_jaxpr_output(new_jaxpr, flat_inputs, orig_out_struct, transform)\n        new_branches.append(new_jaxpr)\n\n    return tuple(new_branches), out_struct, flat_inputs", "\ndef _handle_cond(primitive, *vals, params):\n    cond_val, *arg_vals = vals\n    subfuns, bind_params = primitive.get_bind_params(params)\n\n    new_branches, out_struct, flat_inputs = _match_branches(params['branches'], arg_vals)\n    bind_params['branches'] = new_branches\n    bind_params['linear'] = _broadcast_tuple(params['linear'], arg_vals)\n\n    outs = primitive.bind(*subfuns, cond_val, *flat_inputs, **bind_params)\n    return jax.tree_util.tree_unflatten(out_struct, outs)", "\ndef _handle_remat2(primitive, *vals, params):\n    subfuns, bind_params = primitive.get_bind_params(params)\n    new_jaxpr, consts, flat_inputs, out_tree = wrap_jaxpr(bind_params['jaxpr'], vals, return_closed=False)\n    new_jaxpr = pe.convert_constvars_jaxpr(new_jaxpr)\n    bind_params['jaxpr'] = new_jaxpr\n    outs = primitive.bind(*subfuns, *consts, *flat_inputs, **bind_params)\n    return jax.tree_util.tree_unflatten(out_tree, outs)\n\n\ndef _handle_pjit(primitive, *vals, params):\n    new_jaxpr, flat_inputs, out_tree = wrap_jaxpr(params['jaxpr'], vals)\n    donated_invars = _broadcast_tuple(params['donated_invars'], vals)\n    in_shardings = _broadcast_tuple(params['in_shardings'], vals)\n    out_shardings = _broadcast_tuple(params['out_shardings'], out_tree)\n\n    subfuns, bind_params = primitive.get_bind_params(params)\n    bind_params['jaxpr'] = new_jaxpr\n    bind_params['donated_invars'] = donated_invars\n    bind_params['in_shardings'] = in_shardings\n    bind_params['out_shardings'] = out_shardings\n    outs = primitive.bind(*subfuns, *flat_inputs, **bind_params)\n    return jax.tree_util.tree_unflatten(out_tree, outs)", "\n\ndef _handle_pjit(primitive, *vals, params):\n    new_jaxpr, flat_inputs, out_tree = wrap_jaxpr(params['jaxpr'], vals)\n    donated_invars = _broadcast_tuple(params['donated_invars'], vals)\n    in_shardings = _broadcast_tuple(params['in_shardings'], vals)\n    out_shardings = _broadcast_tuple(params['out_shardings'], out_tree)\n\n    subfuns, bind_params = primitive.get_bind_params(params)\n    bind_params['jaxpr'] = new_jaxpr\n    bind_params['donated_invars'] = donated_invars\n    bind_params['in_shardings'] = in_shardings\n    bind_params['out_shardings'] = out_shardings\n    outs = primitive.bind(*subfuns, *flat_inputs, **bind_params)\n    return jax.tree_util.tree_unflatten(out_tree, outs)", "\n_default_handlers = {\n    'cond': _handle_cond,\n    'remat2': _handle_remat2,\n    'pjit': _handle_pjit,\n}\n\ndef materialize_handler(primitive, *vals, params):\n    vals = _materialize_all(vals)\n    subfuns, bind_params = primitive.get_bind_params(params)\n    result = use_implicit_args(primitive.bind)(*subfuns, *vals, **bind_params)\n    return result", "\ndef _broadcast_tuple(t, trees):\n    if isinstance(trees, jax.tree_util.PyTreeDef):\n        trees = jax.tree_util.tree_unflatten(trees, range(trees.num_leaves))\n    assert len(t) == len(trees)\n    return tuple(chain.from_iterable(\n        (tuple_val for _ in jax.tree_util.tree_leaves(tree))\n        for tuple_val, tree in zip(t, trees)\n    ))\n\ndef _get_materialization_aval(imp_arr):\n    with _aval_discovery_context(), _filter_materialization_warnings():\n        result = jax.eval_shape(\n            partial(iu.materialize_nested, full=True),\n            imp_arr\n        )\n    return result", "\ndef _get_materialization_aval(imp_arr):\n    with _aval_discovery_context(), _filter_materialization_warnings():\n        result = jax.eval_shape(\n            partial(iu.materialize_nested, full=True),\n            imp_arr\n        )\n    return result\n\n@contextmanager\ndef _filter_materialization_warnings():\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', message='Primitive.*was not handled')\n        yield", "\n@contextmanager\ndef _filter_materialization_warnings():\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', message='Primitive.*was not handled')\n        yield\n\n_AVAL_ERROR_MESSAGE = (\n    '{} was not set during initialization. Shape and dtype may be set by:'\n    '\\n\\t1. Directly passing them as keyword arguments to ImplicitArray instances'", "    '{} was not set during initialization. Shape and dtype may be set by:'\n    '\\n\\t1. Directly passing them as keyword arguments to ImplicitArray instances'\n    '\\n\\t2. Overriding the default_shape/default_dtype class attributes'\n    '\\n\\t3. Overriding the compute_shape/compute_dtype methods'\n    '\\n\\t4. Overriding __post_init__ and setting their values there'\n    '\\n\\t5. None of the above, in which case `materialize()` will be called in an attempt to infer them.'\n    ' If their values are required in order to compute the materialization this will be unsuccessful.'\n)\n", ""]}
{"filename": "qax/implicit/implicit_utils.py", "chunked_list": ["from functools import partial, wraps\nfrom itertools import chain\n\nimport jax\nfrom jax.api_util import flatten_fun_nokwargs\nfrom jax import core\nimport jax.linear_util as lu\nfrom jax import tree_util\n\nfrom . import implicit_array as ia", "\nfrom . import implicit_array as ia\n\nclass _EmptyNodeCls:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n", "\nEmptyNode = _EmptyNodeCls()\n\ntree_util.register_pytree_node(\n    _EmptyNodeCls,\n    lambda node: ((), None),\n    lambda _, __: EmptyNode\n)\n\ndef combine_leaf_predicate(base_fn, is_leaf):\n    @wraps(base_fn)\n    def new_fn(*args, new_is_leaf=None):\n        if new_is_leaf is None:\n            combined_is_leaf = is_leaf\n        else:\n            def combined_is_leaf(arg):\n                return is_leaf(arg) or new_is_leaf(arg)\n        return base_fn(*args, is_leaf=combined_is_leaf)\n    return new_fn", "\ndef combine_leaf_predicate(base_fn, is_leaf):\n    @wraps(base_fn)\n    def new_fn(*args, new_is_leaf=None):\n        if new_is_leaf is None:\n            combined_is_leaf = is_leaf\n        else:\n            def combined_is_leaf(arg):\n                return is_leaf(arg) or new_is_leaf(arg)\n        return base_fn(*args, is_leaf=combined_is_leaf)\n    return new_fn", "\ndef leaf_predicate(x):\n    return isinstance(x, (ia.ImplicitArray, _EmptyNodeCls))\n\ntree_map_with_implicit = combine_leaf_predicate(jax.tree_map, leaf_predicate)\ntree_map_with_path_with_implicit = combine_leaf_predicate(tree_util.tree_map_with_path, leaf_predicate)\ntree_flatten_with_implicit = combine_leaf_predicate(tree_util.tree_flatten, leaf_predicate)\ntree_flatten_with_path_with_implicit = combine_leaf_predicate(tree_util.tree_flatten_with_path, leaf_predicate)\ntree_leaves_with_implicit = combine_leaf_predicate(tree_util.tree_leaves, leaf_predicate)\ntree_structure_with_implicit = combine_leaf_predicate(tree_util.tree_structure, leaf_predicate)", "tree_leaves_with_implicit = combine_leaf_predicate(tree_util.tree_leaves, leaf_predicate)\ntree_structure_with_implicit = combine_leaf_predicate(tree_util.tree_structure, leaf_predicate)\n\ndef flatten_one_implicit_layer(tree):\n    def is_leaf_below_node(node, x):\n        return isinstance(x, ia.ImplicitArray) and x is not node\n\n    def replace_subtree_implicits(node):\n        return tree_util.tree_map(lambda _: 1, node, is_leaf=partial(is_leaf_below_node, node))\n\n    prototype = tree_map_with_implicit(replace_subtree_implicits, tree)\n    struct = tree_util.tree_structure(prototype)\n\n    leaves = tree_leaves_with_implicit(tree)\n    leaves = list(chain.from_iterable(\n        tree_util.tree_leaves(leaf, is_leaf=partial(is_leaf_below_node, leaf))\n        if isinstance(leaf, ia.ImplicitArray) else\n        [leaf] for leaf in leaves\n    ))\n    return leaves, struct", "\ndef implicit_depth(tree):\n    leaves = tree_leaves_with_implicit(tree)\n    depth = 0\n    while True:\n        next_leaves = []\n        any_implicit = False\n        for leaf in leaves:\n            if not isinstance(leaf, ia.ImplicitArray):\n                continue\n            any_implicit = True\n            next_leaves.extend(flatten_one_implicit_layer(leaf)[0])\n\n        if not any_implicit:\n            return depth\n\n        depth += 1\n        leaves = next_leaves", "\ndef _map_leaves_with_implicit_path(f, leaves, is_leaf, path_prefix=()):\n    mapped_leaves = []\n    for idx, leaf in enumerate(leaves):\n        path = path_prefix + (idx,)\n        if not isinstance(leaf, ia.ImplicitArray) or is_leaf(path, leaf):\n            mapped_leaves.append(f(leaf))\n            continue\n\n        subtree, substruct = flatten_one_implicit_layer(leaf)\n        mapped_subtree = _map_leaves_with_implicit_path(\n            f,\n            subtree,\n            is_leaf=is_leaf,\n            path_prefix=path\n        )\n        mapped_leaves.append(tree_util.tree_unflatten(substruct, mapped_subtree))\n    return mapped_leaves", "\ndef _get_pruning_transform(tree, materialization_paths):\n    if not materialization_paths:\n        return lambda x: x\n    def is_leaf(path, leaf):\n        return path in materialization_paths\n\n    def materialize_subtrees(tree):\n        leaves, struct = tree_flatten_with_implicit(tree)\n\n        mapped_leaves =  _map_leaves_with_implicit_path(partial(materialize_nested, full=True), leaves, is_leaf)\n        return tree_util.tree_unflatten(struct, mapped_leaves)\n\n    return materialize_subtrees", "\ndef get_common_prefix_transforms(trees):\n    \"\"\"\n    Given an iterable of pytrees which have the same structure after all\n    ImplicitArray instances are materialized, return a list of callables\n    which will transform each tree into the largest common structure\n    obtainable via materialization of ImplicitArrays.\n    \"\"\"\n    if len(trees) <= 1:\n        return [lambda x: x for _ in trees]\n\n    all_leaves, structures = zip(*(tree_flatten_with_implicit(tree) for tree in trees))\n    post_materialization_avals = [core.get_aval(leaf) for leaf in all_leaves[0]]\n    for i, (leaves, structure) in enumerate(zip(all_leaves[1:], structures[1:]), 1):\n        if structure != structures[0]:\n            raise ValueError('Trees do not have the same structure after materialization')\n\n        for leaf, expected_aval in zip(leaves, post_materialization_avals):\n            aval = core.get_aval(leaf)\n            if not (aval.shape == expected_aval.shape and aval.dtype == expected_aval.dtype):\n                raise ValueError(\n                    f'Trees do not have the same avals after materialization. Tree 0: {expected_aval}, Tree {i}: {aval}'\n                )\n\n    # Stack will contain tuples of (path, nodes)\n    # path = a sequence of integers specifying which child\n    # was taken at each _flatten_one_implicit_layer call\n    # or the first flatten_with_implicit call\n    # nodes = one node from each tree\n    stack = []\n\n    all_leaves = []\n    for tree in trees:\n        all_leaves.append(tree_leaves_with_implicit(tree))\n\n    for i, nodes in enumerate(zip(*all_leaves)):\n        stack.append(((i,), nodes))\n\n    materialization_paths = set()\n    while stack:\n        path_prefix, nodes = stack.pop()\n        if not any(isinstance(node, ia.ImplicitArray) for node in nodes):\n               continue\n\n        all_leaves, all_structures = zip(*(\n            flatten_one_implicit_layer(node) for node in nodes\n        ))\n        node_structures = set(all_structures)\n        if len(node_structures) > 1:\n            materialization_paths.add(path_prefix)\n            continue\n\n        aval_diff = False\n        for leaves in zip(*all_leaves):\n            first_aval = core.get_aval(leaves[0])\n            shape = first_aval.shape\n            dtype = first_aval.dtype\n            for leaf in leaves[1:]:\n                aval = core.get_aval(leaf)\n                if not (aval.shape == shape and aval.dtype == dtype):\n                    materialization_paths.add(path_prefix)\n                    aval_diff = True\n            if aval_diff:\n                break\n\n        if aval_diff:\n            continue\n\n        for i, leaf_group in enumerate(zip(*all_leaves)):\n            stack.append((path_prefix + (i,), leaf_group))\n\n    return [_get_pruning_transform(tree, materialization_paths) for tree in trees]", "\ndef materialize_nested(implicit_arr, full=False):\n    \"\"\"\n    Materialize an ImplicitArray instance, handling the case where implicit_arr.materialize()\n    involves further ImplicitArray instances.\n    Arguments:\n        implicit_arr: An ImplicitArray instance\n        full: If True, repeatedly materialize until the result is a concrete array\n    Returns:\n        The materialized array\n    \"\"\"\n    while isinstance(implicit_arr, ia.ImplicitArray):\n        wrapped = lu.wrap_init(type(implicit_arr).materialize)\n        flat, in_tree = flatten_one_implicit_layer((implicit_arr,))\n        flat_fn, out_tree = flatten_fun_nokwargs(wrapped, in_tree)\n        out_flat = ia.use_implicit_args(flat_fn.call_wrapped)(*flat)\n        implicit_arr = jax.tree_util.tree_unflatten(out_tree(), out_flat)\n\n        if not full:\n            break\n\n    return implicit_arr", "\n"]}
{"filename": "qax/implicit/__init__.py", "chunked_list": [""]}
{"filename": "tests/grad.py", "chunked_list": ["from dataclasses import dataclass\nimport jax\nimport jax.numpy as jnp\n\nfrom qax import ArrayValue, use_implicit_args, ImplicitArray, primitive_handler\n\n\n@dataclass\nclass TwoMatricesInATrenchcoat(ImplicitArray):\n    a : ArrayValue\n    b : ArrayValue\n\n    def materialize(self):\n        return self.a + self.b", "class TwoMatricesInATrenchcoat(ImplicitArray):\n    a : ArrayValue\n    b : ArrayValue\n\n    def materialize(self):\n        return self.a + self.b\n\n@primitive_handler('mul')\ndef handler(primitive, x : TwoMatricesInATrenchcoat, y : jax.Array):\n    return TwoMatricesInATrenchcoat(x.a * y, x.b * y)", "def handler(primitive, x : TwoMatricesInATrenchcoat, y : jax.Array):\n    return TwoMatricesInATrenchcoat(x.a * y, x.b * y)\n\ndef test_grad():\n    shape = (5, 7)\n    k1, k2, k3 = jax.random.split(jax.random.PRNGKey(0), 3)\n    x = TwoMatricesInATrenchcoat(\n        jax.random.normal(k1, shape),\n        jax.random.normal(k2, shape),\n    )\n    y = jax.random.normal(k3, shape)\n\n    @use_implicit_args\n    def f(x, y):\n        return jnp.sum(x * y)\n\n    def explicit_f(a, b, y):\n        return jnp.sum((a * y) + (b * y))\n\n    x_grad = jax.grad(f)(x, y)\n    y_grad = jax.grad(f, 1)(x, y)\n\n    a_grad_expected = jax.grad(explicit_f)(x.a, x.b, y)\n    b_grad_expected = jax.grad(explicit_f, 1)(x.b, x.a, y)\n    y_grad_expected = jax.grad(explicit_f, 2)(x.a, x.b, y)\n\n    assert jnp.allclose(x_grad.a, a_grad_expected)\n    assert jnp.allclose(x_grad.b, b_grad_expected)\n    assert jnp.allclose(y_grad, y_grad_expected)", "\n"]}
{"filename": "tests/primitives.py", "chunked_list": ["from dataclasses import dataclass\n\nimport jax\nimport jax.numpy as jnp\n\nimport pytest\n\nfrom qax import ArrayValue, use_implicit_args, ImplicitArray, primitive_handler, default_handler\nfrom qax.constants import CUMULATIVE_REDUCTION_OPS, ELEMENTWISE_UNOPS, ELEMENTWISE_BINOPS, REDUCTION_OPS\n", "from qax.constants import CUMULATIVE_REDUCTION_OPS, ELEMENTWISE_UNOPS, ELEMENTWISE_BINOPS, REDUCTION_OPS\n\nprimitive_example_params = {\n    'convert_element_type': {'new_dtype': jnp.float32, 'weak_type': False},\n    'integer_pow': {'y': 3},\n    'reduce_precision': {'exponent_bits': 4, 'mantissa_bits': 12},\n    'round': {'rounding_method': jax.lax.RoundingMethod.AWAY_FROM_ZERO}\n}\n\nfor op in REDUCTION_OPS:\n    primitive_example_params[op] = {'axes': (1,)}", "\nfor op in REDUCTION_OPS:\n    primitive_example_params[op] = {'axes': (1,)}\n\nfor op in CUMULATIVE_REDUCTION_OPS:\n    primitive_example_params[op] = {'axis': 1, 'reverse': False}\n\nprimitive_example_params['argmax'] = primitive_example_params['argmin'] = {'axes': (1,), 'index_dtype': jnp.int32}\n\ninput_dtypes = {", "\ninput_dtypes = {\n    'clz': jnp.int32,\n    'not': jnp.uint8,\n    'population_count': jnp.int32,\n    'imag': jnp.complex64,\n    'real': jnp.complex64,\n    'shift_right_logical': (jnp.int32, jnp.int32),\n    'shift_right_arithmetic': (jnp.int32, jnp.int32),\n    'shift_left': (jnp.int32, jnp.int32),", "    'shift_right_arithmetic': (jnp.int32, jnp.int32),\n    'shift_left': (jnp.int32, jnp.int32),\n    'or': (jnp.int32, jnp.int32),\n    'and': (jnp.int32, jnp.int32),\n    'xor': (jnp.int32, jnp.int32),\n}\n\ndef make_class_for_primitive(primitive):\n    @dataclass\n    class StackedArray(ImplicitArray):\n        a : ArrayValue\n        b : ArrayValue\n\n        def materialize(self):\n            return jnp.concatenate((self.a, self.b), axis=0)\n\n        def __repr__(self):\n            return f'StackedArray({self.a}, {self.b})'\n\n    return StackedArray", "\n@pytest.mark.parametrize('primitive', ELEMENTWISE_UNOPS)\ndef test_unop(primitive):\n    StackedArray = make_class_for_primitive(primitive)\n\n    @primitive_handler(primitive)\n    def handler(primitive, arg : StackedArray, **kwargs):\n        new_a = default_handler(primitive, arg.a, **kwargs)\n        new_b = default_handler(primitive, arg.b, **kwargs)\n        return StackedArray(new_a, new_b,)\n\n    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n\n    def f(x):\n        params = primitive_example_params.get(primitive, {})\n        return lax_primitive.bind(x, **params)\n\n    to_type = input_dtypes.get(primitive, jnp.float32)\n    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n    stacked = StackedArray(x, y)\n    expected = f(stacked.materialize())\n\n    with_implicit = use_implicit_args(f)(stacked).materialize()\n\n    close = jnp.isclose(with_implicit, expected)\n    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n    assert jnp.all(close | nan_agree)", "\n@pytest.mark.parametrize('primitive', ELEMENTWISE_BINOPS)\ndef test_binop(primitive):\n    StackedArray = make_class_for_primitive(primitive)\n\n    @primitive_handler(primitive)\n    def handler(primitive, arg1 : StackedArray, arg2 : StackedArray, **kwargs):\n        new_a = default_handler(primitive, arg1.a, arg2.a, **kwargs)\n        new_b = default_handler(primitive, arg1.b, arg2.b, **kwargs)\n        return StackedArray(new_a, new_b)\n\n    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n    def f(x, y):\n        params = primitive_example_params.get(primitive, {})\n        return lax_primitive.bind(x, y, **params)\n\n    lhs_type, rhs_type = input_dtypes.get(primitive, (jnp.float32, jnp.float32))\n    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(lhs_type)\n    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(lhs_type)\n    stacked1 = StackedArray(x, y)\n\n    z = jax.random.normal(jax.random.PRNGKey(2), (3, 7)).astype(rhs_type)\n    w = jax.random.normal(jax.random.PRNGKey(3), (9, 7)).astype(rhs_type)\n    stacked2 = StackedArray(z, w)\n\n    expected = f(stacked1.materialize(), stacked2.materialize())\n\n    with_implicit = use_implicit_args(f)(stacked1, stacked2).materialize()\n\n    close = jnp.isclose(with_implicit, expected)\n    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n    assert jnp.all(close | nan_agree)", "\n@pytest.mark.parametrize('primitive', REDUCTION_OPS)\ndef test_reduction(primitive):\n    StackedArray = make_class_for_primitive(primitive)\n\n    @primitive_handler(primitive)\n    def handler(primitive, arg : StackedArray, *, axes, **params):\n        if 0 in axes:\n            raise ValueError('Tests should use axis 1')\n        a_reduced = default_handler(primitive, arg.a, axes=axes, **params)\n        b_reduced = default_handler(primitive, arg.b, axes=axes, **params)\n        return StackedArray(a_reduced, b_reduced)\n\n    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n    def f(x):\n        params = primitive_example_params.get(primitive, {})\n        return lax_primitive.bind(x, **params)\n\n    to_type = input_dtypes.get(primitive, jnp.int32)\n    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n    stacked = StackedArray(x, y)\n    expected = f(stacked.materialize())\n\n    with_implicit = use_implicit_args(f)(stacked).materialize()\n\n    close = jnp.isclose(with_implicit, expected)\n    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n    assert jnp.all(close | nan_agree)", "\n@pytest.mark.parametrize('primitive', CUMULATIVE_REDUCTION_OPS)\ndef test_cumulative_reduction(primitive):\n    StackedArray = make_class_for_primitive(primitive)\n    @primitive_handler(primitive)\n    def handler(primitive, arg : StackedArray, *, axis, **params):\n        if axis != 1:\n            raise ValueError('Tests should use axis 1')\n\n        a_reduced = default_handler(primitive, arg.a, axis=axis, **params)\n        b_reduced = default_handler(primitive, arg.b, axis=axis, **params)\n\n        return StackedArray(a_reduced, b_reduced)\n\n    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n    def f(x):\n        params = primitive_example_params.get(primitive, {})\n        return lax_primitive.bind(x, **params)\n\n    to_type = input_dtypes.get(primitive, jnp.float32)\n    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n    stacked = StackedArray(x, y)\n    expected = f(stacked.materialize())\n\n    with_implicit = use_implicit_args(f)(stacked).materialize()\n\n    close = jnp.isclose(with_implicit, expected)\n    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n    assert jnp.all(close | nan_agree)", ""]}
{"filename": "tests/transform.py", "chunked_list": ["from dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Union\nimport warnings\n\nimport jax\nfrom jax.core import Primitive\nfrom jax.experimental import pjit\nimport jax.numpy as jnp\nimport numpy as np", "import jax.numpy as jnp\nimport numpy as np\nimport pytest\n\nfrom qax import ImplicitArray, use_implicit_args, primitive_handler\nfrom qax import utils\n\nWARN_PATTERN = '.*implicit args will be materialized'\n\n@dataclass\nclass ImplicitConst(ImplicitArray):\n    default_dtype = jnp.float32\n    value : Any\n    dummy_val : Any\n\n    def materialize(self):\n        return jnp.full(self.shape, self.value, dtype=self.dtype)", "\n@dataclass\nclass ImplicitConst(ImplicitArray):\n    default_dtype = jnp.float32\n    value : Any\n    dummy_val : Any\n\n    def materialize(self):\n        return jnp.full(self.shape, self.value, dtype=self.dtype)\n", "\n@primitive_handler([jax.lax.mul_p, jax.lax.sub_p])\ndef mul_handler(primitive : Primitive, a : ImplicitConst, b : Union[ImplicitConst, jax.Array], **params):\n    def op(lhs, rhs):\n        return lhs * rhs if primitive.name == 'mul' else lhs - rhs\n    assert not params\n    if isinstance(b, ImplicitConst):\n        return ImplicitConst(op(a.value, b.value), a.dummy_val, shape=a.shape, dtype=a.dtype)\n    if b.shape == ():\n        new_value = op(a.value, b)\n        return ImplicitConst(new_value, a.dummy_val, shape=a.shape, dtype=a.dtype)\n    return op(a.value, b)", "\n@pytest.fixture\ndef const():\n    shape = (2, 3)\n    return ImplicitConst(2, -173, shape=shape)\n\ndef test_transform(const):\n    @use_implicit_args\n    def f(x, y):\n        return x * y\n\n    print(f'Const: {const}')\n    assert f(const, jnp.ones(const.shape))[0, 0] == const.value", "\ndef test_pjit(const):\n    @use_implicit_args\n    @pjit.pjit\n    def f(x, y):\n        return x * y\n\n    assert f(const, jnp.ones(const.shape))[0, 0] == const.value\n\ndef test_remat(const):\n    @use_implicit_args\n    @jax.checkpoint\n    def f(x, y):\n        return x * y\n\n    result = f(const, jnp.ones(const.shape))\n    assert result.shape == const.shape\n    assert result[0, 0] == const.value", "\ndef test_remat(const):\n    @use_implicit_args\n    @jax.checkpoint\n    def f(x, y):\n        return x * y\n\n    result = f(const, jnp.ones(const.shape))\n    assert result.shape == const.shape\n    assert result[0, 0] == const.value", "\ndef test_materialize(const):\n    def f(x):\n        return 3 + x\n    with pytest.warns(UserWarning, match=WARN_PATTERN):\n        use_implicit_args(f)(const)\n\n\ndef test_cond(const):\n    @use_implicit_args\n    def f(x, y):\n        def true_fn(x):\n            return x * jnp.ones(x.shape)\n        def false_fn(x):\n            return x * jnp.zeros(x.shape) + 5\n        return jnp.sum(jax.lax.cond(y, true_fn, false_fn, x))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error', message=WARN_PATTERN)\n        assert f(const, True) == const.value * np.prod(const.shape)\n        assert f(const, False) == 5 * np.prod(const.shape)", "def test_cond(const):\n    @use_implicit_args\n    def f(x, y):\n        def true_fn(x):\n            return x * jnp.ones(x.shape)\n        def false_fn(x):\n            return x * jnp.zeros(x.shape) + 5\n        return jnp.sum(jax.lax.cond(y, true_fn, false_fn, x))\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error', message=WARN_PATTERN)\n        assert f(const, True) == const.value * np.prod(const.shape)\n        assert f(const, False) == 5 * np.prod(const.shape)", "\ndef test_cond_materialize_branch(const):\n    @use_implicit_args\n    def f(x, y):\n        def true_fn(x):\n            return x\n        def false_fn(x):\n            return jnp.ones(x.shape)\n        return jax.lax.cond(y, true_fn, false_fn, x)\n\n    result = f(const, True)\n    assert isinstance(result, jax.Array)\n    assert result.shape == const.shape\n    assert jnp.all(result == const.value)", "\ndef test_cond_partial_materialize_branch():\n    @use_implicit_args\n    def f(x, y, z):\n        def true_fn(x, y):\n            return y * y\n\n        def false_fn(x, y):\n            return x * x\n\n        return jax.lax.cond(z, true_fn, false_fn, x, y)\n\n    shape = (2, 3)\n    x = ImplicitConst(2., -173, shape=shape)\n    y = ImplicitConst(\n            value=ImplicitConst(1., -173, shape=()),\n            dummy_val=-173,\n            shape=shape\n    )\n    #y._materialize()\n\n    result = f(x, y, True)\n    assert isinstance(result, ImplicitConst)\n    assert isinstance(result.value, jax.Array)\n    assert result.shape == (2, 3)\n    assert jnp.all(result.value == 1)", "\ndef test_switch(const):\n    @use_implicit_args\n    def f(x, i):\n        branch_fn = lambda a, x: jnp.sum(a * x)\n        branches = [partial(branch_fn, jnp.asarray(i)) for i in range(3)]\n\n        return jax.lax.switch(i, branches, x)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error', message='.*switch was not handled')\n        assert f(const, 0) == 0", "\ndef test_no_implicit_args():\n    def f(x):\n        return jnp.sum(x ** 2)\n    assert use_implicit_args(f)(jnp.ones((3, 3))) == 9\n\ndef test_vmap():\n    def f(x, y):\n        return jnp.sum(x * y)\n\n    xs = ImplicitConst(\n        jnp.arange(3),\n        jnp.arange(-100, -97),\n        shape=(7, 11)\n    )\n    ys = jax.random.normal(jax.random.PRNGKey(0), (7, 11))\n\n    x_value = jnp.tile(jnp.arange(3)[:, None, None], (1, 7, 11))\n\n    vmapped_f = jax.vmap(f, in_axes=(0, None))\n    implicit_f = jax.vmap(use_implicit_args(f), in_axes=(0, None))\n\n    result = implicit_f(xs, ys)\n    expected_result = vmapped_f(x_value, ys)\n\n    assert jnp.allclose(result, expected_result)", ""]}
{"filename": "tests/utils.py", "chunked_list": ["from dataclasses import dataclass\nimport jax\nimport jax.numpy as jnp\nfrom jax.tree_util import tree_structure\nimport pytest\n\nimport optax\n\nfrom qax import ArrayValue, utils, ImplicitArray\n", "from qax import ArrayValue, utils, ImplicitArray\n\n@dataclass\nclass Container(ImplicitArray):\n    a : ArrayValue\n    b : ArrayValue\n\n    def materialize(self):\n        return self.a\n", "\n@pytest.fixture(scope='module', params=[0, 1, 2, 3])\ndef container_with_depth(request):\n    a = jnp.arange(10)\n    for d in range(request.param):\n        a = Container(a, jnp.zeros(d))\n\n    return a, request.param\n\ndef test_count_depth(container_with_depth):\n    container, depth = container_with_depth\n    assert utils.implicit_depth(container) == depth", "\ndef test_count_depth(container_with_depth):\n    container, depth = container_with_depth\n    assert utils.implicit_depth(container) == depth\n\ndef test_flatten_one_layer(container_with_depth):\n    container, depth = container_with_depth\n    pytree = [{'x': container}, {'y': container}]\n    flat, struct = utils.flatten_one_implicit_layer(pytree)\n\n    unflattened = jax.tree_util.tree_unflatten(struct, flat)\n    assert jax.tree_util.tree_structure(unflattened) == jax.tree_util.tree_structure(pytree)\n    assert utils.implicit_depth(flat) == max(depth - 1, 0)", "\ndef _get_prefix(*containers):\n    return [transform(c) for c, transform in zip(containers, utils.get_common_prefix_transforms(containers))]\n\ndef test_prefix():\n    c1 = Container(\n        a=Container(jnp.zeros(10), jnp.zeros(10)),\n        b=Container(jnp.zeros(3), jnp.zeros(13))\n    )\n    c2 = Container(\n        a=Container(jnp.zeros(10), jnp.zeros(10)),\n        b=jnp.zeros(3)\n    )\n\n    full_materialized_c1, _ = _get_prefix(c1, jnp.ones(10))\n    assert isinstance(full_materialized_c1, jax.Array)\n    assert jnp.all(full_materialized_c1 == jnp.zeros(10))\n\n    c3 = Container(\n        a=Container(jnp.zeros(10), jnp.zeros(3)),\n        b=Container(jnp.zeros(3), jnp.zeros(13))\n    )\n\n    prefix_c1, prefix_c3 = _get_prefix(c1, c3)\n    expected = Container(a=jnp.zeros(10), b=Container(jnp.zeros(3), jnp.zeros(13)))\n    assert tree_structure(prefix_c1) == tree_structure(prefix_c3) == tree_structure(expected)\n\n    c4 = Container(\n        a=Container(\n            a=Container(jnp.ones(10), jnp.zeros(3)),\n            b=jnp.zeros(3)\n        ),\n        b=jnp.zeros(10)\n    )\n\n    c5 = Container(\n        a=jnp.zeros(10),\n        b=Container(\n            Container(jnp.zeros(10), jnp.zeros(3)),\n            Container(jnp.zeros(3), jnp.zeros(13))\n        )\n    )\n\n    prefix_c4, prefix_c5 = _get_prefix(c4, c5)\n    expected = Container(a=jnp.zeros(10), b=jnp.zeros(10))\n    assert tree_structure(prefix_c4) == tree_structure(prefix_c5) == tree_structure(expected)", ""]}
{"filename": "tests/nested.py", "chunked_list": ["from dataclasses import dataclass\nimport warnings\n\nimport jax\nimport jax.numpy as jnp\n\nfrom qax import ArrayValue, ImplicitArray, use_implicit_args, primitive_handler\n\n@dataclass\nclass Outer(ImplicitArray):\n    x : ArrayValue\n\n    def materialize(self):\n        return 2 * (self.x ** 1)", "@dataclass\nclass Outer(ImplicitArray):\n    x : ArrayValue\n\n    def materialize(self):\n        return 2 * (self.x ** 1)\n\n@primitive_handler(jax.lax.mul_p)\ndef mul(primitive, arg : Outer, other : jax.Array):\n    return Outer(arg.x * other)", "def mul(primitive, arg : Outer, other : jax.Array):\n    return Outer(arg.x * other)\n\n@dataclass\nclass Inner(ImplicitArray):\n    value : ArrayValue\n\n    def materialize(self):\n        return jnp.full(self.shape, self.value, dtype=self.dtype)\n", "\n@primitive_handler(jax.lax.integer_pow_p)\ndef pow(primitive, arg : Inner, *, y):\n    new_value = arg.value ** y\n    return Inner(new_value, shape=arg.shape, dtype=arg.dtype)\n\ndef test_nested():\n    @use_implicit_args\n    def f(x):\n        return jnp.sum(x)\n\n    inner = Inner(3, shape=(2, 3), dtype=jnp.float32)\n    nested = Outer(inner)\n    result = f(nested)\n    assert result == 36", "\ndef test_nested_with_operation():\n    @use_implicit_args\n    def f(x):\n        return jnp.sum(x * jnp.ones(x.shape))\n\n    inner = Inner(3, shape=(2, 3), dtype=jnp.float32)\n    nested = Outer(inner)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('error', message='Primitive mul was not handled by class Outer')\n        result = f(nested)\n    assert result == 36", ""]}
{"filename": "tests/symbols.py", "chunked_list": ["import itertools\nimport operator as fn\n\nimport pytest\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nimport qax", "\nimport qax\nfrom qax.symbols import SymbolicConstant\n\nadd = qax.use_implicit_args(fn.add)\nmul = qax.use_implicit_args(fn.mul)\n\ndefault_shape = (3, 5)\n\n@pytest.fixture\ndef arr():\n    return jax.random.normal(jax.random.PRNGKey(0), default_shape)", "\n@pytest.fixture\ndef arr():\n    return jax.random.normal(jax.random.PRNGKey(0), default_shape)\n\n@pytest.fixture\ndef zeros():\n    return SymbolicConstant(0, shape=default_shape, dtype=jnp.float32)\n\n@pytest.fixture\ndef ones():\n    return SymbolicConstant(1, shape=default_shape, dtype=jnp.float32)", "\n@pytest.fixture\ndef ones():\n    return SymbolicConstant(1, shape=default_shape, dtype=jnp.float32)\n\n@pytest.fixture\ndef pis():\n    return SymbolicConstant(jnp.pi, shape=default_shape, dtype=jnp.float32)\n\ndef test_add(zeros, arr, pis):\n    z_plus_z = add(zeros, zeros)\n\n    assert isinstance(z_plus_z, SymbolicConstant)\n    assert z_plus_z.value == 0\n\n    assert jnp.allclose(add(zeros, arr), arr)\n\n    pi_plus_pi = add(pis, pis)\n    assert isinstance(pi_plus_pi, SymbolicConstant)\n    assert jnp.isclose(pi_plus_pi.value, 2 * jnp.pi)", "\ndef test_add(zeros, arr, pis):\n    z_plus_z = add(zeros, zeros)\n\n    assert isinstance(z_plus_z, SymbolicConstant)\n    assert z_plus_z.value == 0\n\n    assert jnp.allclose(add(zeros, arr), arr)\n\n    pi_plus_pi = add(pis, pis)\n    assert isinstance(pi_plus_pi, SymbolicConstant)\n    assert jnp.isclose(pi_plus_pi.value, 2 * jnp.pi)", "\ndef test_mul(zeros, ones, arr, pis):\n    zero_times_one = mul(zeros, ones)\n\n    assert isinstance(zero_times_one, SymbolicConstant)\n    assert zero_times_one.value == 0\n\n    assert jnp.all(mul(ones, arr) == arr)\n\n    pi_times_pi = mul(pis, pis)\n    assert isinstance(pi_times_pi, SymbolicConstant)\n    assert jnp.isclose(pi_times_pi.value, jnp.pi ** 2)\n\n    assert mul(pis, ones).value == jnp.pi", "\n_names = ['zeros', 'ones', 'pis']\n@pytest.mark.parametrize('fn,lhs,rhs', itertools.product(\n    [jax.lax.add, jax.lax.mul, jax.lax.sub, jax.lax.atan2, jax.lax.max],\n    _names,\n    _names\n))\ndef test_binop(fn, lhs, rhs, request):\n    lhs = request.getfixturevalue(lhs)\n    rhs = request.getfixturevalue(rhs)\n    expected = fn(lhs.materialize(), rhs.materialize())\n    result = qax.use_implicit_args(fn)(lhs, rhs)\n    assert isinstance(result, SymbolicConstant)\n    assert jnp.allclose(result.value, expected)\n    assert result.shape == expected.shape\n    assert result.dtype == expected.dtype", "\n@pytest.mark.parametrize('fn,arg', itertools.product(\n    [jnp.sum, jnp.prod, jnp.all, jnp.any, jnp.sin, jnp.isfinite],\n    _names\n))\ndef test_unop(fn, arg, request):\n    value = request.getfixturevalue(arg)\n    expected = fn(value.materialize())\n    result = qax.use_implicit_args(fn)(value)\n    assert isinstance(result, SymbolicConstant)\n    assert jnp.allclose(result.value, expected)\n    assert result.shape == expected.shape\n    assert result.dtype == expected.dtype", "\ndef test_select_n(zeros, ones):\n    @qax.use_implicit_args\n    def f(c, x, y):\n        return jax.lax.select_n(c, x, y)\n\n    assert isinstance(f(True, zeros, ones), jnp.ndarray)\n    assert isinstance(f(False, zeros, zeros), SymbolicConstant)\n", ""]}
{"filename": "examples/zero.py", "chunked_list": ["from typing import Union\nimport jax\nimport jax.numpy as jnp\n\nfrom qax import ImplicitArray, primitive_handler, use_implicit_args\n\nclass ImplicitZeros(ImplicitArray):\n    default_dtype = jnp.float32\n\n    def materialize(self):\n        return jnp.zeros(self.shape, dtype=self.dtype)", "\ndef binop_shape_dtype(x, y):\n    return {\n        'shape': jnp.broadcast_shapes(x.shape, y.shape),\n        'dtype': jnp.result_type(x.dtype, y.dtype),\n    }\n\n@primitive_handler(jax.lax.mul_p)\ndef do_mul(primitive, x : ImplicitZeros, y : jax.Array):\n    print('Invoked do_mul')\n    return ImplicitZeros(**binop_shape_dtype(x, y))", "def do_mul(primitive, x : ImplicitZeros, y : jax.Array):\n    print('Invoked do_mul')\n    return ImplicitZeros(**binop_shape_dtype(x, y))\n\n@primitive_handler([jax.lax.add_p, jax.lax.mul_p])\ndef handle_both_implicit(primitive, x : ImplicitZeros, y : ImplicitZeros):\n    print('Invoked handle_both_implicit')\n    return ImplicitZeros(**binop_shape_dtype(x, y))\n\n@primitive_handler(jax.lax.add_p)\ndef handle_add_general(primitive, x : ImplicitZeros, y : jax.Array):\n    print('Invoked handle_add')\n    shape_dtype = binop_shape_dtype(x, y)\n    return jnp.broadcast_to(y, shape_dtype['shape']).astype(shape_dtype['dtype'])", "\n@primitive_handler(jax.lax.add_p)\ndef handle_add_general(primitive, x : ImplicitZeros, y : jax.Array):\n    print('Invoked handle_add')\n    shape_dtype = binop_shape_dtype(x, y)\n    return jnp.broadcast_to(y, shape_dtype['shape']).astype(shape_dtype['dtype'])\n\ndef main():\n    @jax.jit\n    @use_implicit_args\n    def f(x, y):\n                                 # If x and y are both of type ImplicitZeros, the result will be:\n\n        z = x + y                # z: ImplicitZeros  output: Invoked handle_both_implicit\n        w = z * jnp.ones_like(z) # w: ImplicitZeros  output: Invoked do_mul\n        a = jnp.sum(w)           # a: jax.Array      output: UserWarning: Primitive reduce_sum was not\n                                 #                           handled by class ImplicitZeros, so implicit\n                                 #                           args will be materialized\n        b =  w + a               # b: jax.Array      output: Invoked handle_add\n        return b\n\n    zeros = ImplicitZeros(shape=(2, 3))\n\n    result = jax.jit(f)(zeros, zeros)\n\n    assert isinstance(result, jax.Array)\n    assert result.shape == zeros.shape\n    assert jnp.all(result == 0)\n\n    # The decorated f will also work with mixed arguments or non-implicit arguments\n    jnp_ones = jnp.ones(zeros.shape)\n    f(zeros, jnp_ones)\n    f(jnp_ones, zeros)\n    f(jnp_ones, jnp_ones)", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "examples/identity.py", "chunked_list": ["from dataclasses import dataclass, InitVar\nfrom functools import partial\nfrom typing import Union\n\nimport jax\nimport jax.numpy as jnp\nimport qax\n\nfrom examples.minimal_lora import LoraMatrix\n", "from examples.minimal_lora import LoraMatrix\n\n@dataclass\nclass Eye(qax.ImplicitArray):\n    dim : InitVar[int]\n\n    def __init__(self, dim, dtype=jnp.float32):\n        super().__init__(shape=(dim, dim), dtype=dtype)\n\n    def materialize(self):\n        return jnp.eye(self.shape[0], dtype=self.dtype)", "\n@qax.primitive_handler(jax.lax.dot_general_p)\ndef dot_handler(primitive, lhs : Union[Eye,jax.Array], rhs : Union[Eye,jax.Array], *, dimension_numbers, **kwargs):\n    lhs_aval = jax.core.get_aval(lhs)\n    rhs_aval = jax.core.get_aval(rhs)\n\n    out_aval = jax.eval_shape(\n        partial(qax.default_handler, primitive, dimension_numbers=dimension_numbers, **kwargs),\n        lhs_aval, rhs_aval\n    )\n\n    lhs_is_eye = isinstance(lhs, Eye)\n    rhs_is_eye = isinstance(rhs, Eye)\n\n    (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n    # It's 1 AM and I can only conceptualize dot_generals during the PM hours\n    # so I will only be implementing 1-2D x 2D matmuls\n    if not (\n        1 <= lhs.aval.ndim <= 2\n        and rhs.aval.ndim <= 2\n        and len(lhs_contract) == len(rhs_contract) == 1\n        and lhs_batch == rhs_batch == ()\n    ):\n        return NotImplemented\n\n    if lhs_is_eye and rhs_is_eye:\n        return Eye(out_aval.shape[0], dtype=out_aval.dtype)\n\n    result = rhs if lhs_is_eye else lhs\n    return result.astype(out_aval.dtype)", "\ndef main():\n    @qax.use_implicit_args\n    def f(a, b):\n        return a @ b\n\n    w = Eye(3)\n    x = jnp.arange(39, dtype=jnp.float32).reshape(3, 13)\n\n\n    print(f(w, x))\n\n    dim = 128\n    rank = 16\n    eye_plus_low_rank = LoraMatrix(\n        W=Eye(dim),\n        A=jax.random.normal(jax.random.PRNGKey(0), (dim, rank)),\n        B=jnp.zeros((dim, rank))\n    )\n\n    x = jax.random.normal(jax.random.PRNGKey(1), (73, dim))\n    print(jnp.sum(f(x, eye_plus_low_rank)))", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "examples/nullable_array.py", "chunked_list": ["\"\"\"\nExample of an ImplicitArray which represents an array + a boolean mask representing the validity of each entry.\nThis is a proof of concept and is not optimized for performance.\n\"\"\"\nfrom dataclasses import dataclass\n\nimport jax\nimport jax.numpy as jnp\n\nfrom qax import ArrayValue, ImplicitArray, default_handler, primitive_handler, use_implicit_args", "\nfrom qax import ArrayValue, ImplicitArray, default_handler, primitive_handler, use_implicit_args\n\nfrom qax.constants import ELEMENTWISE_BINOPS, ELEMENTWISE_UNOPS, REDUCTION_OPS\n\n@dataclass\nclass NullableArray(ImplicitArray):\n    val : ArrayValue\n    mask : ArrayValue\n\n    def materialize(self):\n        return self.val", "\n@primitive_handler(ELEMENTWISE_UNOPS)\ndef handle_unop(primitive, nullable_val : NullableArray, **params):\n    val = default_handler(primitive, nullable_val.val, **params)\n    return NullableArray(val, nullable_val.mask)\n\n@primitive_handler(ELEMENTWISE_BINOPS)\ndef handle_binop(primitive, lhs : ArrayValue, rhs : ArrayValue, **params):\n    lhs_is_nullable = isinstance(lhs, NullableArray)\n    rhs_is_nullable = isinstance(rhs, NullableArray)\n    mask = lhs.mask if lhs_is_nullable else None\n\n    if lhs_is_nullable:\n        lhs = lhs.val\n\n    if rhs_is_nullable:\n        mask = rhs.mask if mask is None else mask & rhs.mask\n        rhs = rhs.val\n\n    out_val = default_handler(primitive, lhs, rhs, **params)\n    return NullableArray(out_val, mask)", "\n@primitive_handler(REDUCTION_OPS)\ndef handle_reduction(primitive, null_arr : NullableArray, **params):\n    new_val = default_handler(primitive, null_arr.val, **params)\n    new_mask = default_handler(jax.lax.reduce_and_p, null_arr.mask, **params)\n    return NullableArray(new_val, new_mask)\n\n@jax.jit\n@use_implicit_args\ndef f(x, y):\n    return jnp.sum(-x * y, axis=0)", "@use_implicit_args\ndef f(x, y):\n    return jnp.sum(-x * y, axis=0)\n\nif __name__ == '__main__':\n    x = NullableArray(\n        val=jnp.ones((2, 3)),\n        mask=jnp.asarray(\n            [[True, False, True],\n             [False, True, True]]\n        )\n    )\n\n    y = NullableArray(\n        val=jnp.full((2, 3), 3),\n        mask=jnp.asarray(\n            [[False, True, True],\n             [True, True, True]]\n        )\n    )\n\n    output = f(x, y)\n    print(f'Result: {output.val}')\n    print(f'Mask: {output.mask}')", ""]}
{"filename": "examples/combining.py", "chunked_list": ["from typing import Union\n\nimport jax\nimport jax.numpy as jnp\n\nfrom qax import use_implicit_args, primitive_handler\nfrom examples.zero import ImplicitZeros\nfrom examples.const import ImplicitConst\n\n@use_implicit_args\ndef f(x, y):\n    return x * y", "\n@use_implicit_args\ndef f(x, y):\n    return x * y\n\ndef main():\n    shape = (2, 3)\n    zeros = ImplicitZeros(shape=shape, dtype=jnp.float32)\n    const = ImplicitConst(1., shape=shape)\n\n    assert isinstance(f(const, zeros), jax.Array)\n    @primitive_handler('mul')\n    def heterogenous_handler(primitive, x: Union[ImplicitZeros, ImplicitConst], y: Union[ImplicitZeros, ImplicitConst]):\n        out_shape = jnp.broadcast_shapes(x.shape, y.shape)\n        out_dtype = jnp.result_type(x.dtype, y.dtype)\n        return ImplicitZeros(shape=out_shape, dtype=out_dtype)\n\n    assert isinstance(f(const, zeros), ImplicitZeros)", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "examples/nesting.py", "chunked_list": ["import jax\nimport jax.numpy as jnp\n\nfrom qax import use_implicit_args\nfrom examples.nullable_array import NullableArray\nfrom examples.zero import ImplicitZeros\n\n@jax.jit\n@use_implicit_args\ndef f(x, y):\n    return (x * y) + 3", "@use_implicit_args\ndef f(x, y):\n    return (x * y) + 3\n\nshape = (2, 3)\nx = NullableArray(\n    val=ImplicitZeros(shape=shape),\n    mask=jnp.asarray(\n        [[True, False, True],\n         [False, True, True]],", "        [[True, False, True],\n         [False, True, True]],\n    )\n)\n\ny = NullableArray(\n    val=jnp.ones(shape),\n    mask=jnp.asarray(\n        [[True, True, True],\n         [False, False, True]]", "        [[True, True, True],\n         [False, False, True]]\n    )\n)\n\nresult = f(x, y)\nprint(f'Result:\\n{result.val}')\nprint(f'Mask:\\n{result.mask}')\n", ""]}
{"filename": "examples/const.py", "chunked_list": ["from dataclasses import dataclass\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom qax import aux_field, ArrayValue, ImplicitArray, primitive_handler, use_implicit_args, default_handler\n\n# To define the behavior we want, we subclass qax.ImplicitArray\n# To define additional fields we also need to mark this class as\n# a dataclass", "# To define additional fields we also need to mark this class as\n# a dataclass\n\n@dataclass\nclass ImplicitConst(ImplicitArray):\n    # Dataclass attributes may be used to define the arrays which\n    # determine the concrete array being represented\n    # In this case it's a single JAX scalar\n    value : ArrayValue\n\n    # ImplicitArrays are pytrees, so all attributes are automatically\n    # marked as pytree children. To instead mark one as auxiliary data\n    # use the qax.aux_field decorator\n    my_aux_value : str = aux_field(default='some_metadata')\n\n    # There are several ways to control the shape and dtype of an ImplicitArray\n    # They are:\n    # 1. Pass shape/dtype kwargs to the constructor\n    # 2. Override the compute_shape/commute_dtype methods\n    # 3. Override the default_shape/default_dtype class attributes\n    # 4. Manually override __post_init__ and set the self.shape/self.dtype values yourself\n    # 5. Do none of the above, in which case materialize() will be abstractly evaluated\n    # in an attempt to derive the values. That won't work in this case since we need\n    # to know them in order to call jnp.full\n\n    default_dtype = jnp.float32\n\n    def compute_dtype(self):\n        # We're doing this instead of just self.value.dtype since we might get\n        # a python scalar\n        return jax.core.get_aval(self.value).dtype\n\n    # The way we can guarantee that our ImplicitArrays will work\n    # with pre-existing code is that whenever we hit an op\n    # that we haven't written custom behavior for, the\n    # ImplicitArray instance will be materialized into a\n    # dense array and the default behavior will be used\n    def materialize(self):\n        return jnp.full(self.shape, self.value, dtype=self.dtype)", "\n# The way we define custom behavior is by writing a function\n# and decorating it with the primitive_handler decorator\n# The type annotations are used for multiple dispatch with\n# plum so make sure to get them right!\n#\n# For commutative ops, the ImplicitArray instance will always be made the\n# lhs, but this isn't true for non-commutative ops as we'll see below\n@primitive_handler('mul')\ndef mul(primitive, a: ImplicitConst, b: jax.Array):\n    \"\"\"\n    Arguments:\n        - primitive: A JAX primitive\n        - const: An argument guaranteed to be an ImplicitConst instance\n        - other: An argument which will either be an ImplicitConst or a JAX typj\n    \"\"\"\n\n    # Get the output shape in case there's any broadcasting going on\n    out_shape = jnp.broadcast_shapes(a.shape, b.shape)\n    if b.size == 1:\n        # If we get multiplied by a scalar, we can \n        # output another ImplicitConst instance\n        # rather than materializing the dense array\n        return ImplicitConst(a.value * b.reshape(1)[0], shape=out_shape)\n\n    # In the general case we just multiply our constant value by the other array\n    result = b * a.value\n    return jnp.broadcast_to(result, out_shape)", "@primitive_handler('mul')\ndef mul(primitive, a: ImplicitConst, b: jax.Array):\n    \"\"\"\n    Arguments:\n        - primitive: A JAX primitive\n        - const: An argument guaranteed to be an ImplicitConst instance\n        - other: An argument which will either be an ImplicitConst or a JAX typj\n    \"\"\"\n\n    # Get the output shape in case there's any broadcasting going on\n    out_shape = jnp.broadcast_shapes(a.shape, b.shape)\n    if b.size == 1:\n        # If we get multiplied by a scalar, we can \n        # output another ImplicitConst instance\n        # rather than materializing the dense array\n        return ImplicitConst(a.value * b.reshape(1)[0], shape=out_shape)\n\n    # In the general case we just multiply our constant value by the other array\n    result = b * a.value\n    return jnp.broadcast_to(result, out_shape)", "\n# We can also define the case where both arguments are ImplicitConsts\n@primitive_handler('mul')\ndef mul(primitive, a: ImplicitConst, b: ImplicitConst):\n    out_shape = jnp.broadcast_shapes(a.shape, b.shape)\n    return ImplicitConst(a.value * b.value, shape=out_shape)\n\n# You can use one handler for multiple primitives by passing an iterable to the decorator\n@primitive_handler(['sin', 'cos', 'exp'])\ndef elementwise_unop(primitive, arg : ImplicitConst):\n    # In a lot of cases the logic doesn't have anything\n    # to do with the exact primitive being used so \n    # we can just use qax.default_handler to execute\n    result = default_handler(primitive, arg.value)\n    return ImplicitConst(result, shape=arg.shape)", "@primitive_handler(['sin', 'cos', 'exp'])\ndef elementwise_unop(primitive, arg : ImplicitConst):\n    # In a lot of cases the logic doesn't have anything\n    # to do with the exact primitive being used so \n    # we can just use qax.default_handler to execute\n    result = default_handler(primitive, arg.value)\n    return ImplicitConst(result, shape=arg.shape)\n\n# If the primitive has any params (such as reduction axes) the handler will receive\n# them as a param kwarg", "# If the primitive has any params (such as reduction axes) the handler will receive\n# them as a param kwarg\n#\n# The above handlers were registered using the primitive name, which is\n# is using the actual lax primitive under the hood. You can also use\n# the actual primitive, which is done here\n@primitive_handler(jax.lax.reduce_sum_p)\ndef reduce_sum(primitive, a: ImplicitConst, *, axes):\n    sum_result = np.prod([a.shape[i] for i in axes]) * a.value\n    new_shape = tuple(d for d in a.shape if d not in axes)\n    return ImplicitConst(sum_result, shape=new_shape)", "\n\n# This decorator makes it so that `f` can handle inputs which are instances\n# of ImplicitArray subclasses (or pytrees containing such instances)\n# You can also still call it with ordinary JAX inputs\n@use_implicit_args\ndef f(a, b):\n    c = a * b\n    d = jnp.sin(c)\n    return jnp.sum(d)", "\ndef main():\n    shape = (5, 7)\n\n    a_full = jnp.full(shape, 3.)\n    a_implicit = ImplicitConst(3., shape=shape)\n\n    b_full = jnp.full(shape, 2.)\n    b_implicit = ImplicitConst(2., shape=shape)\n\n    result = f(a_full, b_full)\n\n    full_implicit_result = f(a_implicit, b_implicit)\n    mixed_result = f(a_implicit, b_full)\n\n\n    # We get the same result each time (other than some floating point error)\n    # In the second case, we were able to avoid materializing the ImplicitConst\n    # so we get an ImplicitConst as an output\n    print(result)               # -9.779543\n    print(full_implicit_result) # ImplicitConst(-9.779541969299316, (5, 7))\n    print(mixed_result)         # -9.779543\n\n    # We can also nest ImplicitArray instances (even if they're different subclasses)\n    nested_b = ImplicitConst(\n        value=ImplicitConst(2., shape=()),\n        shape=shape\n    )\n\n    nested_result = f(a_implicit, nested_b)\n    print(nested_result) # ImplicitConst(ImplicitConst(-9.779541969299316, ()), (5, 7))", "\nif __name__ == '__main__':\n    main()\n"]}
