{"filename": "config.py", "chunked_list": ["import argparse\nfrom tokenize import group\n\n\ndef get_config():\n    \"\"\"\n    The configuration parser for common hyperparameters of all environment.\n    Please reach each `scripts/train/<env>_runner.py` file to find private hyperparameters\n    only used in <env>.\n    \"\"\"\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser = _get_prepare_config(parser)\n    parser = _get_replaybuffer_config(parser)\n    parser = _get_network_config(parser)\n    parser = _get_recurrent_config(parser)\n    parser = _get_optimizer_config(parser)\n    parser = _get_ppo_config(parser)\n    parser = _get_selfplay_config(parser)\n    parser = _get_pbt_config(parser)\n    parser = _get_eval_config(parser)\n    return parser", "\n\ndef _get_prepare_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Prepare parameters:\n        --env-name <str>\n            specify the name of environment\n        --algorithm-name <str>\n            specifiy the algorithm, including `[\"ppo\", \"mappo\"]`\n        --experiment-name <str>\n            an identifier to distinguish different experiment.\n        --seed <int>\n            set seed for numpy and torch\n        --cuda\n            by default False, will use CPU to train; or else will use GPU;\n        --num-env-steps <float>\n            number of env steps to train (default: 1e7)\n        --model-dir <str>\n            by default None. set the path to pretrained model.\n        --use-wandb\n            [for wandb usage], by default False, if set, will log date to wandb server.\n        --user-name <str>\n            [for wandb usage], to specify user's name for simply collecting training data.\n        --wandb-name <str>\n            [for wandb usage], to specify user's name for simply collecting training data.\n    \"\"\"\n    group = parser.add_argument_group(\"Prepare parameters\")\n    group.add_argument(\"--env-name\", type=str, default='SumoAnts-v0',\n                       help=\"specify the name of environment\")\n    group.add_argument(\"--algorithm-name\", type=str, default='ppo', choices=[\"ppo\", \"mappo\"],\n                       help=\"Specifiy the algorithm (default ppo)\")\n    group.add_argument(\"--experiment-name\", type=str, default=\"check\",\n                       help=\"An identifier to distinguish different experiment.\")\n    group.add_argument(\"--seed\", type=int, default=1,\n                       help=\"Random seed for numpy/torch\")\n    group.add_argument(\"--cuda\", action='store_true', default=False,\n                       help=\"By default False, will use CPU to train; or else will use GPU;\")\n    group.add_argument(\"--num-env-steps\", type=float, default=1e7,\n                       help='Number of environment steps to train (default: 1e7)')\n    group.add_argument(\"--model-dir\", type=str, default=None,\n                       help=\"By default None. set the path to pretrained model.\")\n    group.add_argument(\"--use-wandb\", action='store_true', default=False,\n                       help=\"[for wandb usage], by default False, if set, will log date to wandb server.\")\n    group.add_argument(\"--user-name\", type=str, default='jyh',\n                       help=\"[for wandb usage], to specify user's name for simply collecting training data.\")\n    group.add_argument(\"--wandb-name\", type=str, default='jyh',\n                       help=\"[for wandb usage], to specify user's name for simply collecting training data.\")\n    group.add_argument(\"--capture-video\", action='store_true', default=False,\n                       help=\"use wandb to capture video\")\n    return parser", "\n\ndef _get_replaybuffer_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Replay Buffer parameters:\n        --gamma <float>\n            discount factor for rewards (default: 0.99)\n        --buffer-size <int>\n            the maximum storage in the buffer.\n        --use-proper-time-limits\n            by default, the return value does consider limits of time. If set, compute returns with considering time limits factor.\n        --use-gae\n            by default, use generalized advantage estimation. If set, do not use gae.\n        --gae-lambda <float>\n            gae lambda parameter (default: 0.95)\n    \"\"\"\n    group = parser.add_argument_group(\"Replay Buffer parameters\")\n    group.add_argument(\"--gamma\", type=float, default=0.99,\n                       help='discount factor for rewards (default: 0.99)')\n    group.add_argument(\"--buffer-size\", type=int, default=200,\n                       help=\"maximum storage in the buffer.\")\n    group.add_argument(\"--use-gae\", action='store_false', default=True,\n                       help='Whether to use generalized advantage estimation')\n    group.add_argument(\"--gae-lambda\", type=float, default=0.95,\n                       help='gae lambda parameter (default: 0.95)')\n    group.add_argument(\"--use-risk-sensitive\", action='store_true', default=False)\n    group.add_argument(\"--use-reward-hyper\", action='store_true', default=False)\n    group.add_argument(\"--tau-list\", type=str, default=\"0.1 0.4 0.5 0.6 0.9\")\n    group.add_argument(\"--reward-list\", type=str, default=\"0.05 0.2 1.0 5.0 20.0\")\n    return parser", "\n\ndef _get_network_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Network parameters:\n        --hidden-size <str>\n            dimension of hidden layers for mlp pre-process networks\n        --act-hidden-size <int>\n            dimension of hidden layers for actlayer\n        --activation-id\n            choose 0 to use Tanh, 1 to use ReLU, 2 to use LeakyReLU, 3 to use ELU\n        --use-feature-normalization\n            by default False, otherwise apply LayerNorm to normalize feature extraction inputs.\n        --gain\n            by default 0.01, use the gain # of last action layer\n    \"\"\"\n    group = parser.add_argument_group(\"Network parameters\")\n    group.add_argument(\"--hidden-size\", type=str, default='128 128',\n                       help=\"Dimension of hidden layers for mlp pre-process networks (default '128 128')\")\n    group.add_argument(\"--act-hidden-size\", type=str, default='128 128',\n                       help=\"Dimension of hidden layers for actlayer (default '128 128')\")\n    group.add_argument(\"--activation-id\", type=int, default=1,\n                       help=\"Choose 0 to use Tanh, 1 to use ReLU, 2 to use LeakyReLU, 3 to use ELU (default 1)\")\n    group.add_argument(\"--use-feature-normalization\", action='store_true', default=False,\n                       help=\"Whether to apply LayerNorm to the feature extraction inputs\")\n    group.add_argument(\"--gain\", type=float, default=0.01,\n                       help=\"The gain # of last action layer\")\n    group.add_argument(\"--use-cnn\", action='store_true', default=False,\n                       help=\"Whether to use cnn\")\n    return parser", "\n\ndef _get_recurrent_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Recurrent parameters:\n        --use-recurrent-policy\n            by default, use Recurrent Policy. If set, do not use.\n        --recurrent-hidden-size <int>\n            Dimension of hidden layers for recurrent layers (default 128).\n        --recurrent-hidden-layers <int>\n            The number of recurrent layers (default 1).\n        --data-chunk-length <int>\n            Time length of chunks used to train a recurrent_policy, default 10.\n    \"\"\"\n    group = parser.add_argument_group(\"Recurrent parameters\")\n    group.add_argument(\"--use-recurrent-policy\", action='store_false', default=True,\n                       help='Whether to use a recurrent policy')\n    group.add_argument(\"--recurrent-hidden-size\", type=int, default=128,\n                       help=\"Dimension of hidden layers for recurrent layers (default 128)\")\n    group.add_argument(\"--recurrent-hidden-layers\", type=int, default=1,\n                       help=\"The number of recurrent layers (default 1)\")\n    group.add_argument(\"--data-chunk-length\", type=int, default=10,\n                       help=\"Time length of chunks used to train a recurrent_policy (default 10)\")\n    return parser", "\n\ndef _get_optimizer_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Optimizer parameters:\n        --lr <float>\n            learning rate parameter (default: 3e-4, fixed).\n    \"\"\"\n    group = parser.add_argument_group(\"Optimizer parameters\")\n    group.add_argument(\"--lr\", type=float, default=3e-4,\n                       help='learning rate (default: 3e-4)')\n    return parser", "\n\ndef _get_ppo_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    PPO parameters:\n        --ppo-epoch <int>\n            number of ppo epochs (default: 10)\n        --clip-param <float>\n            ppo clip parameter (default: 0.2)\n        --use-clipped-value-loss\n            by default false. If set, clip value loss.\n        --num-mini-batch <int>\n            number of batches for ppo (default: 1)\n        --value-loss-coef <float>\n            ppo value loss coefficient (default: 1)\n        --entropy-coef <float>\n            ppo entropy term coefficient (default: 0.01)\n        --use-max-grad-norm\n            by default, use max norm of gradients. If set, do not use.\n        --max-grad-norm <float>\n            max norm of gradients (default: 0.5)\n    \"\"\"\n    group = parser.add_argument_group(\"PPO parameters\")\n    group.add_argument(\"--ppo-epoch\", type=int, default=4,\n                       help='number of ppo epochs (default: 4)')\n    group.add_argument(\"--clip-param\", type=float, default=0.2,\n                       help='ppo clip parameter (default: 0.2)')\n    group.add_argument(\"--use-clipped-value-loss\", action='store_true', default=False,\n                       help=\"By default false. If set, clip value loss.\")\n    group.add_argument(\"--num-mini-batch\", type=int, default=5,\n                       help='number of batches for ppo (default: 5)')\n    group.add_argument(\"--value-loss-coef\", type=float, default=1,\n                       help='ppo value loss coefficient (default: 1)')\n    group.add_argument(\"--entropy-coef\", type=float, default=0.01,\n                       help='entropy term coefficient (default: 0.01)')\n    group.add_argument(\"--use-max-grad-norm\", action='store_false', default=True,\n                       help=\"By default, use max norm of gradients. If set, do not use.\")\n    group.add_argument(\"--max-grad-norm\", type=float, default=2,\n                       help='max norm of gradients (default: 2)')\n    return parser", "\n\ndef _get_selfplay_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Selfplay parameters:\n        --use-selfplay\n            by default false. If set, use selfplay algorithms.\n        --selfplay-algorithm <str>\n            specifiy the selfplay algorithm, including `[\"sp\", \"fsp\"]`\n        --n-choose-opponents <int>\n            number of different opponents chosen for rollout. (default 1)\n        --init-elo <float>\n            initial ELO for policy performance. (default 1000.0)\n    \"\"\"\n    group = parser.add_argument_group(\"Selfplay parameters\")\n    group.add_argument(\"--use-selfplay\", action='store_true', default=False,\n                       help=\"By default false. If set, use selfplay algorithms.\")\n    group.add_argument(\"--selfplay-algorithm\", type=str, default=\"fsp\", help=\"selfplay algorithm\")\n    group.add_argument(\"--random-side\", action='store_true', default=False, help=\"random agent side when env reset\")\n    group.add_argument('--init-elo', type=float, default=1000.0,\n                       help=\"initial ELO for policy performance. (default 1000.0)\")\n    return parser", "\ndef _get_pbt_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    PBT parameters:\n        --population-size\n            by default 1\n        --num-parallel-each-agent \n            by default 4\n    \"\"\"\n    group = parser.add_argument_group(\"PBT parameters\")\n    group.add_argument(\"--population-size\", type=int, default=1,\n                       help=\"number of agents in the population\")\n    group.add_argument(\"--num-parallel-each-agent\", type=int, default=1,\n                       help=\"number of subprocesses for each agent\")\n    group.add_argument(\"--exploit-elo-threshold\", type=int, default=500,\n                       help=\"\")\n    return parser", "\ndef _get_eval_config(parser: argparse.ArgumentParser):\n    \"\"\"\n    Eval parameters:\n        --eval-episodes <int>\n            number of episodes of a single evaluation.\n    \"\"\"\n    group = parser.add_argument_group(\"Eval parameters\")\n    group.add_argument(\"--eval-episodes\", type=int, default=1,\n                       help=\"number of episodes of a single evaluation. (default 1)\")\n    return parser", "\n\nif __name__ == \"__main__\":\n    parser = get_config()\n    all_args = parser.parse_args()\n"]}
{"filename": "main_selfplay_test.py", "chunked_list": ["import gym\nimport torch\nimport numpy as np\nimport os\nimport sys\nimport logging\nimport time\nfrom pathlib import Path\nfrom torch.utils.tensorboard import SummaryWriter\n", "from torch.utils.tensorboard import SummaryWriter\n\nfrom ppo.ppo_trainer import PPOTrainer\nfrom ppo.ppo_policy import PPOPolicy\nfrom ppo.ppo_data_collectors import BaseDataCollector, SelfPlayDataCollector, make_env\nfrom config import get_config\nimport envs\nimport wandb\n\ndef main(args):\n    parser = get_config()\n    all_args = parser.parse_known_args(args)[0]\n    all_args.buffer_size = 1000\n    all_args.env_name = 'SumoAnts-v0'\n    all_args.eval_episodes = 1\n    all_args.num_env_steps = 1e6\n    all_args.num_mini_batch = 1\n    all_args.ppo_epoch = 4\n    all_args.cuda = True\n    all_args.lr = 1e-4\n    # all_args.use_risk_sensitive = True\n    all_args.use_gae = True\n    all_args.tau = 0.5\n    all_args.seed = 0\n    all_args.use_wandb = False\n    all_args.capture_video = False\n    all_args.env_id = 0\n\n    str_time = time.strftime(\"%b%d-%H%M%S\", time.localtime())\n    run_dir = Path(os.path.dirname(os.path.abspath(__file__))) / \"runs\" / all_args.env_name / all_args.experiment_name\n    if all_args.use_wandb:\n        wandb.init(\n            project=all_args.env_name, \n            entity=all_args.wandb_name, \n            name=all_args.experiment_name, \n            monitor_gym=True, \n            config=all_args,\n            dir=str(run_dir))\n        s = str(wandb.run.dir).split('/')[:-1]\n        s.append('models')\n        save_dir= '/'.join(s)\n    else:\n        run_dir = run_dir / str_time\n        save_dir = str(run_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    all_args.save_dir = save_dir\n    env = make_env(all_args.env_name)\n    collector = SelfPlayDataCollector(all_args)\n    trainer = PPOTrainer(all_args, env.observation_space, env.action_space)\n    # writer = SummaryWriter(run_dir)\n    # writer.add_text(\n    #     \"hyperparameters\",\n    #     \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(all_args).items()])),\n    # )\n    torch.save(trainer.policy.params(), f\"{save_dir}/agent_0.pt\")\n    num_epochs = int(all_args.num_env_steps // all_args.buffer_size)\n    for epoch in range(num_epochs):\n        # train\n        params = torch.load(f\"{str(save_dir)}/agent_{epoch}.pt\")\n        buffer = collector.collect_data(ego_params=params, enm_params=params, hyper_params={'tau':0.5})\n\n        params, train_info = trainer.train(params=params, buffer=buffer)\n\n        # eval and record info\n        elo_gain, eval_info = collector.evaluate_data(ego_params=params, enm_params=params)\n\n        cur_steps = (epoch + 1) * all_args.buffer_size\n        info = {**train_info, **eval_info}\n        if all_args.use_wandb:\n            for k, v in info.items():\n                wandb.log({k: v}, step=cur_steps)\n        train_reward, eval_reward = train_info['episode_reward'], eval_info['episode_reward']\n        print(f\"Epoch {epoch} / {num_epochs} , train episode reward {train_reward}, evaluation episode reward {eval_reward}\")\n\n        # save\n        torch.save(params, f\"{str(save_dir)}/agent_{epoch+1}.pt\")", "\ndef main(args):\n    parser = get_config()\n    all_args = parser.parse_known_args(args)[0]\n    all_args.buffer_size = 1000\n    all_args.env_name = 'SumoAnts-v0'\n    all_args.eval_episodes = 1\n    all_args.num_env_steps = 1e6\n    all_args.num_mini_batch = 1\n    all_args.ppo_epoch = 4\n    all_args.cuda = True\n    all_args.lr = 1e-4\n    # all_args.use_risk_sensitive = True\n    all_args.use_gae = True\n    all_args.tau = 0.5\n    all_args.seed = 0\n    all_args.use_wandb = False\n    all_args.capture_video = False\n    all_args.env_id = 0\n\n    str_time = time.strftime(\"%b%d-%H%M%S\", time.localtime())\n    run_dir = Path(os.path.dirname(os.path.abspath(__file__))) / \"runs\" / all_args.env_name / all_args.experiment_name\n    if all_args.use_wandb:\n        wandb.init(\n            project=all_args.env_name, \n            entity=all_args.wandb_name, \n            name=all_args.experiment_name, \n            monitor_gym=True, \n            config=all_args,\n            dir=str(run_dir))\n        s = str(wandb.run.dir).split('/')[:-1]\n        s.append('models')\n        save_dir= '/'.join(s)\n    else:\n        run_dir = run_dir / str_time\n        save_dir = str(run_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    all_args.save_dir = save_dir\n    env = make_env(all_args.env_name)\n    collector = SelfPlayDataCollector(all_args)\n    trainer = PPOTrainer(all_args, env.observation_space, env.action_space)\n    # writer = SummaryWriter(run_dir)\n    # writer.add_text(\n    #     \"hyperparameters\",\n    #     \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(all_args).items()])),\n    # )\n    torch.save(trainer.policy.params(), f\"{save_dir}/agent_0.pt\")\n    num_epochs = int(all_args.num_env_steps // all_args.buffer_size)\n    for epoch in range(num_epochs):\n        # train\n        params = torch.load(f\"{str(save_dir)}/agent_{epoch}.pt\")\n        buffer = collector.collect_data(ego_params=params, enm_params=params, hyper_params={'tau':0.5})\n\n        params, train_info = trainer.train(params=params, buffer=buffer)\n\n        # eval and record info\n        elo_gain, eval_info = collector.evaluate_data(ego_params=params, enm_params=params)\n\n        cur_steps = (epoch + 1) * all_args.buffer_size\n        info = {**train_info, **eval_info}\n        if all_args.use_wandb:\n            for k, v in info.items():\n                wandb.log({k: v}, step=cur_steps)\n        train_reward, eval_reward = train_info['episode_reward'], eval_info['episode_reward']\n        print(f\"Epoch {epoch} / {num_epochs} , train episode reward {train_reward}, evaluation episode reward {eval_reward}\")\n\n        # save\n        torch.save(params, f\"{str(save_dir)}/agent_{epoch+1}.pt\")", "\n        # save\n    # writer.close()\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n\n\n\n", "\n\n"]}
{"filename": "main_pbt_selfplay.py", "chunked_list": ["import numpy as np\nimport torch\nimport ray\nimport time\nimport os\nimport sys\nimport gym\nimport random\nimport logging\nimport wandb", "import logging\nimport wandb\nimport setproctitle\n\nfrom config import get_config\nimport envs\nfrom pathlib import Path\nfrom ppo.ppo_data_collectors import DataCollectorMix, make_env\nfrom ppo.ppo_trainer import PBTPPOTrainer, PPOTrainer\nfrom ppo.ppo_policy import PPOPolicy", "from ppo.ppo_trainer import PBTPPOTrainer, PPOTrainer\nfrom ppo.ppo_policy import PPOPolicy\nfrom util.util_population import population_based_exploit_explore\nfrom util.util_selfplay import get_algorithm\n\n\n\n'''\nPopulation: N agents\n", "Population: N agents\n\n    for each agent, we have M data-collector, 1 trainer:\n\n    for each trainning step t:\n        for each agent i:\n            1. select K opponent to collect S samples\n            2. run_results = data-collector.collect.remote(ego_model, enm_model, hypyer_param)\n                2.1 load model\n                2.2 collect data via inner environment", "                2.1 load model\n                2.2 collect data via inner environment\n                2.3 save data into inner PPOBuffer\n                2.4 return PPOBuffer\n        3. buffer_list = [[ray.get(run_results) for _ in range(M)] for _ in range(N)]\n        for each agent i:\n            4. info = trainer.update.remote(ego_model, buffer_list, hyper_param)\n                4.1 load model\n                4.2 ppo update with buffer data\n                4.3 save model to file", "                4.2 ppo update with buffer data\n                4.3 save model to file\n                4.4 return trainning info\n        5. info = [[ray.get(info) for _ in range(M)] for _ in range(N)]\n        6. eval + elo_update\n        7. population exploit\n'''\n\n\ndef load_enm_params(run_dir: str, enm_idx: tuple):\n    agent_id, t = enm_idx\n    return torch.load(f\"{run_dir}/agent{agent_id}_history{t}.pt\", map_location=torch.device('cpu'))", "\ndef load_enm_params(run_dir: str, enm_idx: tuple):\n    agent_id, t = enm_idx\n    return torch.load(f\"{run_dir}/agent{agent_id}_history{t}.pt\", map_location=torch.device('cpu'))\n\ndef main(args):\n    # init config\n    parser = get_config()\n    all_args = parser.parse_known_args(args)[0]\n    random.seed(all_args.seed)\n    np.random.seed(all_args.seed)\n    torch.manual_seed(all_args.seed)\n    torch.cuda.manual_seed(all_args.seed)\n\n    tau_hyper = [float(tau) for tau in all_args.tau_list.split(' ')]\n    reward_hyper = [float(r) for r in all_args.reward_list.split(' ')]\n    setproctitle.setproctitle(str(all_args.env_name)+'@'+ str(all_args.user_name))\n    env = make_env(all_args.env_name)\n\n    str_time = time.strftime(\"%b%d-%H%M%S\", time.localtime())\n    run_dir = Path(os.path.dirname(os.path.abspath(__file__))) / \"runs\" / all_args.env_name / all_args.experiment_name\n    if not os.path.exists(run_dir):\n        os.makedirs(run_dir)\n    if all_args.use_wandb:\n        wandb.init(\n            project=all_args.env_name, \n            entity=all_args.wandb_name, \n            name=all_args.experiment_name, \n            group=all_args.experiment_name,\n            job_type='charts',\n            config=all_args,\n            dir=str(run_dir))\n        s = str(wandb.run.dir).split('/')[:-1]\n        s.append('models')\n        save_dir= '/'.join(s)\n    else:\n        run_dir = run_dir / str_time\n        save_dir = str(run_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    all_args.save_dir = save_dir\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n        handlers=[\n            logging.FileHandler(save_dir + \"_ouput.log\"),\n            logging.StreamHandler()\n        ]\n    )\n\n    # init population\n    ray.init()\n    selfplay_algo = get_algorithm(all_args.selfplay_algorithm)\n    population = {}\n    population_elos = {}\n    population_hypers = {}\n    for agent_id in range(all_args.population_size):\n        population[agent_id] = PPOPolicy(all_args, env.observation_space, env.action_space)\n        if all_args.model_dir is not None:\n            population[agent_id].restore_from_path(all_args.model_dir, epoch='latest')\n        params = population[agent_id].params()\n        torch.save(params, f'{save_dir}/agent{agent_id}_history0.pt')\n        torch.save(params, f'{save_dir}/agent{agent_id}_latest.pt')\n        population_elos[agent_id] = {0: all_args.init_elo}\n        population_hypers[agent_id] = {}\n        if all_args.use_risk_sensitive:\n            population_hypers[agent_id]['tau'] = tau_hyper[agent_id]\n        if all_args.use_reward_hyper:\n            population_hypers[agent_id]['reward'] = reward_hyper[agent_id]\n    \n    M = all_args.num_parallel_each_agent\n    N = all_args.population_size\n    data_collector_pools = []\n    for agent_id in range(N):\n        data_collectors = []\n        for i in range(M):\n            all_args.env_id = agent_id * M + i\n            data_collectors.append(DataCollectorMix.remote(all_args))\n        data_collector_pools.append(data_collectors)\n    ppo_trainers = [PBTPPOTrainer.remote(all_args, env.observation_space, env.action_space) for _ in range(N)]\n\n    logging.info(\"Init over.\")\n    num_epochs = int(all_args.num_env_steps // M // all_args.buffer_size)\n\n    for epoch in range(num_epochs):\n        cur_steps = epoch * all_args.buffer_size * M\n        # data collect\n        data_results = []\n        for agent_id in range(N):\n            enm_idxs, enm_elos = selfplay_algo.choose_opponents(agent_id, population_elos, M)\n            ego_model = population[agent_id].params(device='cpu')\n            results = []\n            for i in range(M):  \n                enm_model = load_enm_params(save_dir, enm_idxs[i])\n                res = data_collector_pools[agent_id][i].collect_data.remote(\n                    ego_params=ego_model, \n                    enm_params=enm_model, \n                    hyper_params=population_hypers[agent_id]\n                )\n                results.append(res)\n            data_results.append(results)\n        \n        buffers = [[ray.get(data_results[agent_id][i]) for i in range(M)] for agent_id in range(N)]\n        # ppo train\n        train_results = []\n        for agent_id in range(N):\n            ego_model = population[agent_id].params(device='cuda')\n            res = ppo_trainers[agent_id].train.remote(\n                buffer=buffers[agent_id], \n                params=ego_model, \n                hyper_params=population_hypers[agent_id]\n            )\n            train_results.append(res)\n        train_infos = [ray.get(train_results[i]) for i in range(N)]\n        for agent_id, (param, info) in enumerate(train_infos):\n            population[agent_id].restore_from_params(param)\n            torch.save(param, f'{save_dir}/agent{agent_id}_history{epoch+1}.pt')\n            torch.save(param, f'{save_dir}/agent{agent_id}_latest.pt')\n            train_reward = info[\"episode_reward\"]\n            logging.info(f\"Epoch {epoch} / {num_epochs}, Agent{agent_id}, train episode reward {train_reward}\")\n            if all_args.use_wandb and agent_id == 0:\n                for k, v in info.items():\n                    wandb.log({k: v}, step=cur_steps)\n\n        # evaluate and update elo\n        eval_results = []\n        enm_infos = {}\n        for agent_id in range(N):\n            enm_idxs, enm_elos = selfplay_algo.choose_opponents(agent_id, population_elos, M)\n            enm_infos[agent_id] = enm_idxs\n            ego_model = population[agent_id].params(device='cpu')\n            results = []\n            for i in range(M):  \n                enm_model = load_enm_params(save_dir, enm_idxs[i])\n                res = data_collector_pools[agent_id][i].evaluate_data.remote(\n                    ego_params=ego_model, \n                    enm_params=enm_model, \n                    hyper_params=population_hypers[agent_id],\n                    ego_elo=population_elos[agent_id][epoch],\n                    enm_elo=enm_elos[i],\n                )\n                results.append(res)\n            eval_results.append(results)\n        \n        eval_datas = [[ray.get(eval_results[agent_id][i]) for i in range(M)] for agent_id in range(N)]\n        for agent_id in range(N):\n            elo_gains, eval_infos = list(zip(*eval_datas[agent_id]))\n            population_elos[agent_id][epoch+1] = population_elos[agent_id][epoch]\n            for i, (enm_id, enm_t) in enumerate(enm_infos[agent_id]):\n                population_elos[agent_id][epoch+1] += elo_gains[i]\n                population_elos[enm_id][enm_t] -= elo_gains[i]\n            eval_reward = np.mean([info['episode_reward'] for info in eval_infos])\n            logging.info(f\"Epoch {epoch} / {num_epochs}, Agent{agent_id}, eval episode reward {eval_reward}\")\n            if all_args.use_wandb and agent_id == 0:\n                wandb.log({\"eval_episode_reward\": eval_reward}, step=cur_steps)\n\n        # exploit and explore\n        population_elos, population_hypers = population_based_exploit_explore(epoch, population_elos, population_hypers, save_dir, all_args.exploit_elo_threshold)\n        # save checkoutpoint\n        checkpoint = {\n            \"epoch\": epoch,\n            \"population_elos\": population_elos,\n            \"population_hypers\": population_hypers\n        }\n        if all_args.use_wandb:\n            for agent_id in range(N):\n                wandb.log({f\"agent{agent_id}_tau\": population_hypers[agent_id]['tau']}, step=cur_steps)\n                wandb.log({f\"agent{agent_id}_elo\": population_elos[agent_id][epoch]}, step=cur_steps)\n        torch.save(checkpoint, f\"{save_dir}/checkpoint_latest.pt\")", "        \nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n\n    \n\n    \n"]}
{"filename": "render/render_sumoant.py", "chunked_list": ["import numpy as np\nimport torch\nimport gym\nimport os\nimport sys\nfrom pathlib import Path\nbase_dir = str(Path(__file__).resolve().parent.parent)\nsys.path.append(base_dir)\nimport envs\nfrom PIL import Image", "import envs\nfrom PIL import Image\nfrom config import get_config\nfrom ppo.ppo_buffer import PPOBuffer\nfrom ppo.ppo_policy import PPOPolicy\nfrom gym.wrappers import RecordVideo\nfrom ppo.ppo_data_collectors import make_env\n\n\ndef _t2n(x):\n    return x.detach().cpu().numpy()", "\ndef _t2n(x):\n    return x.detach().cpu().numpy()\n\nepisode_rewards = 0\nrender_video = True\nrender_image = False\nnum_agents = 2\nargs = sys.argv[1:]\nparser = get_config()", "args = sys.argv[1:]\nparser = get_config()\nall_args = parser.parse_known_args(args)[0]\nall_args.env_name = 'SumoAnts-v0'\nenv = make_env(all_args.env_name)\nego_policy = PPOPolicy(all_args, env.observation_space, env.action_space)\nenm_policy = PPOPolicy(all_args, env.observation_space, env.action_space)\nego_dir = \"\"\nenm_dir = \"\"\nego_path = \"\"", "enm_dir = \"\"\nego_path = \"\"\nenm_path = \"\"\nego_policy.restore_from_params(torch.load(ego_dir+ego_path))\nenm_policy.restore_from_params(torch.load(enm_dir+enm_path))\nif render_video == True:\n    env = RecordVideo(env, video_folder=\"render/render_videos\", name_prefix=f\"0.1vs0.9\")\nenv.seed(0)\n\nprint(\"Start render\")", "\nprint(\"Start render\")\nobs = env.reset()\nstep = 0\nif render_image and step % 1 == 0:\n    arr = env.render(mode=\"rgb_array\")\n    img = Image.fromarray(arr)\n    img.save(f\"render/images/step{step}.png\")\nego_rnn_states = np.zeros((1, 1, 128), dtype=np.float32)\nenm_obs =  obs[num_agents // 2:, ...]", "ego_rnn_states = np.zeros((1, 1, 128), dtype=np.float32)\nenm_obs =  obs[num_agents // 2:, ...]\nego_obs =  obs[:num_agents // 2, ...]\nenm_rnn_states = np.zeros_like(ego_rnn_states, dtype=np.float32)\nmasks = np.ones((num_agents//2, 1))\nwhile True:\n    step += 1\n    # env.render()\n    ego_actions, ego_rnn_states = ego_policy.act(ego_obs, ego_rnn_states, masks, deterministic=False)\n    ego_actions = _t2n(ego_actions)", "    ego_actions, ego_rnn_states = ego_policy.act(ego_obs, ego_rnn_states, masks, deterministic=False)\n    ego_actions = _t2n(ego_actions)\n    ego_rnn_states = _t2n(ego_rnn_states)\n    # print(ego_actions)\n    enm_actions, enm_rnn_states = enm_policy.act(enm_obs, enm_rnn_states, masks, deterministic=False)\n    enm_actions = _t2n(enm_actions)\n    enm_rnn_states = _t2n(enm_rnn_states)\n    actions = np.concatenate((ego_actions, enm_actions), axis=0)\n    obs, rewards, dones, infos = env.step(actions)\n    rewards = rewards[:num_agents // 2, ...]", "    obs, rewards, dones, infos = env.step(actions)\n    rewards = rewards[:num_agents // 2, ...]\n    episode_rewards += rewards\n    if render_image and step % 1 == 0:\n        arr = env.render(mode=\"rgb_array\")\n        img = Image.fromarray(arr)\n        img.save(f\"render/images/step{step}.png\")\n    if dones.all():\n        print(infos)\n        break", "    enm_obs =  obs[num_agents // 2:, ...]\n    ego_obs =  obs[:num_agents // 2, ...]\n\n"]}
{"filename": "ppo/ppo_trainer.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport numpy as np\nimport ray\n\nfrom typing import Union, List\nfrom util.util_util import check, get_gard_norm\nfrom ppo.ppo_policy import PPOPolicy\nfrom ppo.ppo_buffer import PPOBuffer\n\nclass PPOTrainer():\n    def __init__(self, args, obs_space, act_space):\n        self.device = torch.device(\"cuda:0\") if args.cuda and torch.cuda.is_available() \\\n                                            else torch.device(\"cpu\")\n        self.tpdv = dict(dtype=torch.float32, device=self.device)\n        # ppo config\n        self.ppo_epoch = args.ppo_epoch\n        self.clip_param = args.clip_param\n        self.use_clipped_value_loss = args.use_clipped_value_loss\n        self.num_mini_batch = args.num_mini_batch\n        self.value_loss_coef = args.value_loss_coef\n        self.entropy_coef = args.entropy_coef\n        self.use_max_grad_norm = args.use_max_grad_norm\n        self.max_grad_norm = args.max_grad_norm\n        # rnn configs\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.data_chunk_length = args.data_chunk_length\n        self.buffer_size = args.buffer_size\n        self.policy = PPOPolicy(args, obs_space, act_space)\n\n    def ppo_update(self, sample):\n\n        obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n            returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch = sample\n\n        old_action_log_probs_batch = check(old_action_log_probs_batch).to(**self.tpdv)\n        advantages_batch = check(advantages_batch).to(**self.tpdv)\n        returns_batch = check(returns_batch).to(**self.tpdv)\n        value_preds_batch = check(value_preds_batch).to(**self.tpdv)\n\n        # Reshape to do in a single forward pass for all steps\n        values, action_log_probs, dist_entropy = self.policy.evaluate_actions(obs_batch,\n                                                                         rnn_states_actor_batch,\n                                                                         rnn_states_critic_batch,\n                                                                         actions_batch,\n                                                                         masks_batch)\n\n        # Obtain the loss function\n        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n        surr1 = ratio * advantages_batch\n        surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages_batch\n        policy_loss = torch.sum(torch.min(surr1, surr2), dim=-1, keepdim=True)\n        policy_loss = -policy_loss.mean()\n\n        if self.use_clipped_value_loss:\n            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n            value_losses = (values - returns_batch).pow(2)\n            value_losses_clipped = (value_pred_clipped - returns_batch).pow(2)\n            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped)\n        else:\n            value_loss = 0.5 * (returns_batch - values).pow(2)\n        value_loss = value_loss.mean()\n\n        policy_entropy_loss = -dist_entropy.mean()\n\n        loss = policy_loss + value_loss * self.value_loss_coef + policy_entropy_loss * self.entropy_coef\n\n        # Optimize the loss function\n        self.policy.optimizer.zero_grad()\n        loss.backward()\n        if self.use_max_grad_norm:\n            actor_grad_norm = nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm).item()\n            critic_grad_norm = nn.utils.clip_grad_norm_(self.policy.critic.parameters(), self.max_grad_norm).item()\n        else:\n            actor_grad_norm = get_gard_norm(self.policy.actor.parameters())\n            critic_grad_norm = get_gard_norm(self.policy.critic.parameters())\n        self.policy.optimizer.step()\n\n        return policy_loss, value_loss, policy_entropy_loss, ratio, actor_grad_norm, critic_grad_norm\n\n    def train(self, buffer: Union[List[PPOBuffer], PPOBuffer], params, hyper_params={}):\n        if 'entropy' in hyper_params:\n            self.entropy_coef = hyper_params['entropy']\n        self.policy.restore_from_params(params)\n        buffer = [buffer] if isinstance(buffer, PPOBuffer) else buffer\n        train_info = {}\n        train_info['value_loss'] = 0\n        train_info['policy_loss'] = 0\n        train_info['policy_entropy_loss'] = 0\n        train_info['actor_grad_norm'] = 0\n        train_info['critic_grad_norm'] = 0\n        train_info['ratio'] = 0\n        train_info['episode_reward'] = 0\n        train_info['episode_length'] = 0\n\n        for _ in range(self.ppo_epoch):\n            if self.use_recurrent_policy:\n                data_generator = PPOBuffer.recurrent_generator(buffer, self.num_mini_batch, self.data_chunk_length)\n            else:\n                raise NotImplementedError\n\n            for sample in data_generator:\n\n                policy_loss, value_loss, policy_entropy_loss, ratio, \\\n                    actor_grad_norm, critic_grad_norm = self.ppo_update(sample)\n\n                train_info['value_loss'] += value_loss.item()\n                train_info['policy_loss'] += policy_loss.item()\n                train_info['policy_entropy_loss'] += policy_entropy_loss.item()\n                train_info['actor_grad_norm'] += actor_grad_norm\n                train_info['critic_grad_norm'] += critic_grad_norm\n                train_info['ratio'] += ratio.mean().item()\n\n        num_updates = self.ppo_epoch * self.num_mini_batch\n\n        for k in train_info.keys():\n            train_info[k] /= num_updates\n        \n        episode_dones = max(np.sum(np.concatenate([buf.masks==0 for buf in buffer])),1)\n        episode_reward = np.sum(np.concatenate([buf.rewards for buf in buffer])) / episode_dones\n        episode_length = self.buffer_size * len(buffer) / episode_dones\n        train_info['episode_reward'] = episode_reward\n        train_info['episode_length'] = episode_length\n        return self.policy.params(), train_info", "from ppo.ppo_buffer import PPOBuffer\n\nclass PPOTrainer():\n    def __init__(self, args, obs_space, act_space):\n        self.device = torch.device(\"cuda:0\") if args.cuda and torch.cuda.is_available() \\\n                                            else torch.device(\"cpu\")\n        self.tpdv = dict(dtype=torch.float32, device=self.device)\n        # ppo config\n        self.ppo_epoch = args.ppo_epoch\n        self.clip_param = args.clip_param\n        self.use_clipped_value_loss = args.use_clipped_value_loss\n        self.num_mini_batch = args.num_mini_batch\n        self.value_loss_coef = args.value_loss_coef\n        self.entropy_coef = args.entropy_coef\n        self.use_max_grad_norm = args.use_max_grad_norm\n        self.max_grad_norm = args.max_grad_norm\n        # rnn configs\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.data_chunk_length = args.data_chunk_length\n        self.buffer_size = args.buffer_size\n        self.policy = PPOPolicy(args, obs_space, act_space)\n\n    def ppo_update(self, sample):\n\n        obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n            returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch = sample\n\n        old_action_log_probs_batch = check(old_action_log_probs_batch).to(**self.tpdv)\n        advantages_batch = check(advantages_batch).to(**self.tpdv)\n        returns_batch = check(returns_batch).to(**self.tpdv)\n        value_preds_batch = check(value_preds_batch).to(**self.tpdv)\n\n        # Reshape to do in a single forward pass for all steps\n        values, action_log_probs, dist_entropy = self.policy.evaluate_actions(obs_batch,\n                                                                         rnn_states_actor_batch,\n                                                                         rnn_states_critic_batch,\n                                                                         actions_batch,\n                                                                         masks_batch)\n\n        # Obtain the loss function\n        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n        surr1 = ratio * advantages_batch\n        surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages_batch\n        policy_loss = torch.sum(torch.min(surr1, surr2), dim=-1, keepdim=True)\n        policy_loss = -policy_loss.mean()\n\n        if self.use_clipped_value_loss:\n            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n            value_losses = (values - returns_batch).pow(2)\n            value_losses_clipped = (value_pred_clipped - returns_batch).pow(2)\n            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped)\n        else:\n            value_loss = 0.5 * (returns_batch - values).pow(2)\n        value_loss = value_loss.mean()\n\n        policy_entropy_loss = -dist_entropy.mean()\n\n        loss = policy_loss + value_loss * self.value_loss_coef + policy_entropy_loss * self.entropy_coef\n\n        # Optimize the loss function\n        self.policy.optimizer.zero_grad()\n        loss.backward()\n        if self.use_max_grad_norm:\n            actor_grad_norm = nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm).item()\n            critic_grad_norm = nn.utils.clip_grad_norm_(self.policy.critic.parameters(), self.max_grad_norm).item()\n        else:\n            actor_grad_norm = get_gard_norm(self.policy.actor.parameters())\n            critic_grad_norm = get_gard_norm(self.policy.critic.parameters())\n        self.policy.optimizer.step()\n\n        return policy_loss, value_loss, policy_entropy_loss, ratio, actor_grad_norm, critic_grad_norm\n\n    def train(self, buffer: Union[List[PPOBuffer], PPOBuffer], params, hyper_params={}):\n        if 'entropy' in hyper_params:\n            self.entropy_coef = hyper_params['entropy']\n        self.policy.restore_from_params(params)\n        buffer = [buffer] if isinstance(buffer, PPOBuffer) else buffer\n        train_info = {}\n        train_info['value_loss'] = 0\n        train_info['policy_loss'] = 0\n        train_info['policy_entropy_loss'] = 0\n        train_info['actor_grad_norm'] = 0\n        train_info['critic_grad_norm'] = 0\n        train_info['ratio'] = 0\n        train_info['episode_reward'] = 0\n        train_info['episode_length'] = 0\n\n        for _ in range(self.ppo_epoch):\n            if self.use_recurrent_policy:\n                data_generator = PPOBuffer.recurrent_generator(buffer, self.num_mini_batch, self.data_chunk_length)\n            else:\n                raise NotImplementedError\n\n            for sample in data_generator:\n\n                policy_loss, value_loss, policy_entropy_loss, ratio, \\\n                    actor_grad_norm, critic_grad_norm = self.ppo_update(sample)\n\n                train_info['value_loss'] += value_loss.item()\n                train_info['policy_loss'] += policy_loss.item()\n                train_info['policy_entropy_loss'] += policy_entropy_loss.item()\n                train_info['actor_grad_norm'] += actor_grad_norm\n                train_info['critic_grad_norm'] += critic_grad_norm\n                train_info['ratio'] += ratio.mean().item()\n\n        num_updates = self.ppo_epoch * self.num_mini_batch\n\n        for k in train_info.keys():\n            train_info[k] /= num_updates\n        \n        episode_dones = max(np.sum(np.concatenate([buf.masks==0 for buf in buffer])),1)\n        episode_reward = np.sum(np.concatenate([buf.rewards for buf in buffer])) / episode_dones\n        episode_length = self.buffer_size * len(buffer) / episode_dones\n        train_info['episode_reward'] = episode_reward\n        train_info['episode_length'] = episode_length\n        return self.policy.params(), train_info", "\n@ray.remote(num_gpus=0.2)\nclass PBTPPOTrainer(PPOTrainer):\n\n    def __init__(self, args, obs_space, act_space):\n        PPOTrainer.__init__(self, args, obs_space, act_space)\n"]}
{"filename": "ppo/ppo_data_collectors.py", "chunked_list": ["import imp\nimport numpy as np\nimport torch\nimport time\nimport gym\nimport ray\nimport envs\nfrom ppo.ppo_buffer import PPOBuffer\nfrom ppo.ppo_policy import PPOPolicy\nfrom gym.core import Wrapper", "from ppo.ppo_policy import PPOPolicy\nfrom gym.core import Wrapper\nimport wandb\ndef _t2n(x):\n    return x.detach().cpu().numpy()\n\nclass GymEnv(Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        self.action_shape = self.env.action_space.shape\n        self.num_agents = 1\n\n    def reset(self):\n        observation = self.env.reset()\n        return np.array(observation).reshape((1, -1))\n\n    def step(self, action):\n        action = np.array(action).reshape(self.action_shape)\n        observation, reward, done, info = self.env.step(action)\n        observation = np.array(observation).reshape((1, -1))\n        done = np.array(done).reshape((1,-1))\n        reward = np.array(reward).reshape((1, -1))\n        info['score'] = 0.5\n        return observation, reward, done, info", "\ndef make_env(env_name):\n    if env_name in ['Ant-v2', 'Hopper-v2']: # single-agent\n        env = GymEnv(gym.make(env_name))\n    else: # selfplay\n        env = gym.make(env_name)\n    return env\n\nclass BaseDataCollector(object):\n    def __init__(self, args) -> None:\n        self.args = args\n        self.env = make_env(args.env_name)\n        if args.capture_video and args.use_wandb and args.env_id == 0:\n            wandb.init(\n                project=args.env_name, \n                entity=args.wandb_name, \n                name=args.experiment_name, \n                group=args.experiment_name,\n                job_type=\"video\",\n                monitor_gym=True)\n            self.env = gym.wrappers.RecordVideo(self.env, args.save_dir+\"_videos\")\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.buffer_size = args.buffer_size\n        self.num_agents = getattr(self.env, 'num_agents', 1)\n        self.random_side = args.random_side\n        self.buffer = PPOBuffer(args, self.num_agents, self.env.observation_space, self.env.action_space)\n        self.ego = PPOPolicy(args, self.env.observation_space, self.env.action_space)\n    \n    def collect_data(self, ego_params, hyper_params={}):\n        self.buffer.clear()\n        if 'tau' in hyper_params:\n            self.buffer.tau = hyper_params['tau'] \n        if 'reward' in hyper_params:\n            self.buffer.reward_hyper = hyper_params['reward']\n        self.ego.restore_from_params(ego_params)\n        obs = self.reset()\n        self.buffer.obs[0] = obs.copy()\n        for step in range(self.buffer_size):\n            # 1. get actions\n            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = \\\n                self.ego.get_actions(self.buffer.obs[step],\n                                     self.buffer.rnn_states_actor[step],\n                                     self.buffer.rnn_states_critic[step],\n                                     self.buffer.masks[step])\n            values = _t2n(values)\n            actions = _t2n(actions)\n            action_log_probs = _t2n(action_log_probs)\n            rnn_states_actor = _t2n(rnn_states_actor)\n            rnn_states_critic = _t2n(rnn_states_critic)\n            # 2. env step\n            obs, rewards, dones, info = self.step(actions)\n            if np.all(dones):\n                obs = self.reset()\n                rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n                rnn_states_critic = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n                masks = np.zeros((self.num_agents, 1), dtype=np.float32)\n            else:\n                masks = np.ones((self.num_agents, 1), dtype=np.float32)\n            # 3. insert experience in buffer\n            self.buffer.insert(obs, actions, rewards, masks, action_log_probs, values, rnn_states_actor, rnn_states_critic)\n        status_code = 0 if step > 0 else 1\n        last_value = self.ego.get_values(self.buffer.obs[-1], self.buffer.rnn_states_critic[-1], self.buffer.masks[-1])\n        self.buffer.compute_returns(_t2n(last_value))\n        return self.buffer\n    \n    @torch.no_grad()\n    def evaluate_data(self, ego_params, hyper_params={}, ego_elo=0, enm_elo=0):\n        self.ego.restore_from_params(ego_params)\n        total_episode_rewards = []\n        eval_scores = []\n        max_episode_length = self.args.buffer_size\n        episode_reward = 0\n        eval_episodes = self.args.eval_episodes\n        for _ in range(eval_episodes):\n            obs = self.reset()\n            rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n            masks = np.ones((self.num_agents, 1))\n            step = 0\n            while True:\n                action, rnn_states_actor = self.ego.act(obs, rnn_states_actor, masks)\n                action = _t2n(action)\n                rnn_states_actor = _t2n(rnn_states_actor)\n                obs, reward, done, info = self.step(action)\n                step += 1\n                episode_reward += reward\n                if np.all(done):\n                    total_episode_rewards.append(episode_reward)\n                    episode_reward = 0\n                    if isinstance(info, tuple) or isinstance(info, list):\n                        score = info[0]['score']\n                    elif isinstance(info, dict):\n                        score = info['score']\n                    else:\n                        raise NotImplementedError\n                    eval_scores.append(score)\n                    break\n                if step >= max_episode_length:\n                    break\n        expected_score = 1 / (1 + 10 ** ((enm_elo - ego_elo) / 400))\n        elo_gain = 32 * (np.mean(eval_scores) - expected_score)\n        eval_info = {}\n        eval_info[\"episode_reward\"] = np.mean(total_episode_rewards)\n        return  elo_gain, eval_info\n\n    def reset(self):\n        return self.env.reset()\n    \n    def step(self, action):\n        return self.env.step(action)", "class BaseDataCollector(object):\n    def __init__(self, args) -> None:\n        self.args = args\n        self.env = make_env(args.env_name)\n        if args.capture_video and args.use_wandb and args.env_id == 0:\n            wandb.init(\n                project=args.env_name, \n                entity=args.wandb_name, \n                name=args.experiment_name, \n                group=args.experiment_name,\n                job_type=\"video\",\n                monitor_gym=True)\n            self.env = gym.wrappers.RecordVideo(self.env, args.save_dir+\"_videos\")\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.buffer_size = args.buffer_size\n        self.num_agents = getattr(self.env, 'num_agents', 1)\n        self.random_side = args.random_side\n        self.buffer = PPOBuffer(args, self.num_agents, self.env.observation_space, self.env.action_space)\n        self.ego = PPOPolicy(args, self.env.observation_space, self.env.action_space)\n    \n    def collect_data(self, ego_params, hyper_params={}):\n        self.buffer.clear()\n        if 'tau' in hyper_params:\n            self.buffer.tau = hyper_params['tau'] \n        if 'reward' in hyper_params:\n            self.buffer.reward_hyper = hyper_params['reward']\n        self.ego.restore_from_params(ego_params)\n        obs = self.reset()\n        self.buffer.obs[0] = obs.copy()\n        for step in range(self.buffer_size):\n            # 1. get actions\n            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = \\\n                self.ego.get_actions(self.buffer.obs[step],\n                                     self.buffer.rnn_states_actor[step],\n                                     self.buffer.rnn_states_critic[step],\n                                     self.buffer.masks[step])\n            values = _t2n(values)\n            actions = _t2n(actions)\n            action_log_probs = _t2n(action_log_probs)\n            rnn_states_actor = _t2n(rnn_states_actor)\n            rnn_states_critic = _t2n(rnn_states_critic)\n            # 2. env step\n            obs, rewards, dones, info = self.step(actions)\n            if np.all(dones):\n                obs = self.reset()\n                rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n                rnn_states_critic = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n                masks = np.zeros((self.num_agents, 1), dtype=np.float32)\n            else:\n                masks = np.ones((self.num_agents, 1), dtype=np.float32)\n            # 3. insert experience in buffer\n            self.buffer.insert(obs, actions, rewards, masks, action_log_probs, values, rnn_states_actor, rnn_states_critic)\n        status_code = 0 if step > 0 else 1\n        last_value = self.ego.get_values(self.buffer.obs[-1], self.buffer.rnn_states_critic[-1], self.buffer.masks[-1])\n        self.buffer.compute_returns(_t2n(last_value))\n        return self.buffer\n    \n    @torch.no_grad()\n    def evaluate_data(self, ego_params, hyper_params={}, ego_elo=0, enm_elo=0):\n        self.ego.restore_from_params(ego_params)\n        total_episode_rewards = []\n        eval_scores = []\n        max_episode_length = self.args.buffer_size\n        episode_reward = 0\n        eval_episodes = self.args.eval_episodes\n        for _ in range(eval_episodes):\n            obs = self.reset()\n            rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n            masks = np.ones((self.num_agents, 1))\n            step = 0\n            while True:\n                action, rnn_states_actor = self.ego.act(obs, rnn_states_actor, masks)\n                action = _t2n(action)\n                rnn_states_actor = _t2n(rnn_states_actor)\n                obs, reward, done, info = self.step(action)\n                step += 1\n                episode_reward += reward\n                if np.all(done):\n                    total_episode_rewards.append(episode_reward)\n                    episode_reward = 0\n                    if isinstance(info, tuple) or isinstance(info, list):\n                        score = info[0]['score']\n                    elif isinstance(info, dict):\n                        score = info['score']\n                    else:\n                        raise NotImplementedError\n                    eval_scores.append(score)\n                    break\n                if step >= max_episode_length:\n                    break\n        expected_score = 1 / (1 + 10 ** ((enm_elo - ego_elo) / 400))\n        elo_gain = 32 * (np.mean(eval_scores) - expected_score)\n        eval_info = {}\n        eval_info[\"episode_reward\"] = np.mean(total_episode_rewards)\n        return  elo_gain, eval_info\n\n    def reset(self):\n        return self.env.reset()\n    \n    def step(self, action):\n        return self.env.step(action)", "\nclass SelfPlayDataCollector(BaseDataCollector):\n\n    def __init__(self, args):\n        super().__init__(args)\n        self.enm = PPOPolicy(args, self.env.observation_space, self.env.action_space)\n        self.num_agents = self.num_agents // 2\n        self.buffer = PPOBuffer(args, self.num_agents, self.env.observation_space, self.env.action_space)\n    \n    def collect_data(self, ego_params, enm_params, hyper_params={}):\n        self.enm.restore_from_params(enm_params)\n        return super().collect_data(ego_params, hyper_params)\n    \n    @torch.no_grad()\n    def evaluate_data(self, ego_params, enm_params, hyper_params={}, ego_elo=0, enm_elo=0):\n        self.enm.restore_from_params(enm_params)\n        return super().evaluate_data(ego_params, hyper_params, ego_elo, enm_elo)\n    \n    def reset(self):\n        if self.random_side:\n            self.ego_side = np.random.randint(2) # shuffle rl control side\n        else:\n            self.ego_side = 0\n        self.enm_rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n        self.enm_masks = np.ones((self.num_agents, 1))\n        obs = super().reset()\n        return self._parse_obs(obs)\n    \n    def step(self, action):\n        enm_action, enm_rnn_states_actor = self.enm.act(self.enm_obs, self.enm_rnn_states_actor, self.enm_masks)\n        enm_action = _t2n(enm_action)\n        self.enm_rnn_states_actor = _t2n(enm_rnn_states_actor)\n        actions = np.concatenate((action, enm_action), axis=0)\n        n_obs, rewards, done, info = self.env.step(actions)\n        if self.ego_side == 0:\n            ego_rewards = rewards[:self.num_agents, ...]\n            ego_done = done[:self.num_agents, ...]\n        else:\n            ego_rewards = rewards[self.num_agents:, ...]\n            ego_done = done[self.num_agents:, ...]\n        return self._parse_obs(n_obs), ego_rewards, ego_done, info\n    \n    def _parse_obs(self, obs):\n        if self.ego_side == 0:\n            self.enm_obs = obs[self.num_agents:, ...]\n            ego_obs = obs[:self.num_agents, ...]\n        else:\n            self.enm_obs = obs[:self.num_agents, ...]\n            ego_obs = obs[self.num_agents:, ...]\n        return ego_obs", "\n@ray.remote(num_cpus=0.5)\nclass DataCollectorMix(object):\n    def __init__(self, args) -> None:\n        self.collector = None\n        self.mode = None\n        self.args = args\n    \n    def set_collector(self, mode):\n        if self.mode == mode:\n            return\n        self.mode = mode\n        if self.collector is not None:\n            self.collector.close()\n        if self.mode == 'base':\n            self.collector = BaseDataCollector(self.args)\n        elif self.mode == 'selfplay':\n            self.collector = SelfPlayDataCollector(self.args)\n        else:\n            raise NotImplementedError\n\n    def collect_data(self, ego_params, enm_params=None, hyper_params={}):\n        if enm_params == None:\n            mode = 'base'\n        elif isinstance(enm_params, dict):\n            mode = 'selfplay'\n        else:\n            raise NotImplementedError\n        self.set_collector(mode)\n        if self.mode == 'base':\n            return self.collector.collect_data(ego_params, hyper_params)\n        elif self.mode == 'selfplay':\n            return self.collector.collect_data(ego_params, enm_params, hyper_params)\n        else: \n            raise NotImplementedError\n    \n    def evaluate_data(self, ego_params, enm_params=None, hyper_params={}, ego_elo=0, enm_elo=0):\n        if enm_params == None:\n            mode = 'base'\n        elif isinstance(enm_params, dict):\n            mode = 'selfplay'\n        else:\n            raise NotImplementedError\n        self.set_collector(mode)\n        if self.mode == 'base':\n            return self.collector.evaluate_data(ego_params, hyper_params, ego_elo, enm_elo)\n        elif self.mode == 'selfplay':\n            return self.collector.evaluate_data(ego_params, enm_params, hyper_params, ego_elo, enm_elo)\n        else: \n            raise NotImplementedError"]}
{"filename": "ppo/ppo_policy.py", "chunked_list": ["import torch\nfrom ppo.ppo_actor import PPOActor\nfrom ppo.ppo_critic import PPOCritic\n\n\nclass PPOPolicy:\n    def __init__(self, args, obs_space, act_space):\n\n        self.args = args\n        self.device = torch.device(\"cuda:0\") if args.cuda and torch.cuda.is_available() \\\n                                            else torch.device(\"cpu\")\n        # optimizer config\n        self.lr = args.lr\n\n        self.obs_space = obs_space\n        self.act_space = act_space\n\n        self.actor = PPOActor(args, self.obs_space, self.act_space, self.device)\n        self.critic = PPOCritic(args, self.obs_space, self.device)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.actor.parameters()},\n            {'params': self.critic.parameters()}\n        ], lr=self.lr)\n\n    def get_actions(self, obs, rnn_states_actor, rnn_states_critic, masks):\n        \"\"\"\n        Returns:\n            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n        \"\"\"\n        actions, action_log_probs, rnn_states_actor = self.actor(obs, rnn_states_actor, masks)\n        values, rnn_states_critic = self.critic(obs, rnn_states_critic, masks)\n        return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n\n    def get_values(self, obs, rnn_states_critic, masks):\n        \"\"\"\n        Returns:\n            values\n        \"\"\"\n        values, _ = self.critic(obs, rnn_states_critic, masks)\n        return values\n\n    def evaluate_actions(self, obs, rnn_states_actor, rnn_states_critic, action, masks, active_masks=None):\n        \"\"\"\n        Returns:\n            values, action_log_probs, dist_entropy\n        \"\"\"\n        action_log_probs, dist_entropy = self.actor.evaluate_actions(obs, rnn_states_actor, action, masks, active_masks)\n        values, _ = self.critic(obs, rnn_states_critic, masks)\n        return values, action_log_probs, dist_entropy\n\n    def act(self, obs, rnn_states_actor, masks, deterministic=False):\n        \"\"\"\n        Returns:\n            actions, rnn_states_actor\n        \"\"\"\n        actions, _, rnn_states_actor = self.actor(obs, rnn_states_actor, masks, deterministic)\n        return actions, rnn_states_actor\n\n    def prep_training(self):\n        self.actor.train()\n        self.critic.train()\n\n    def prep_rollout(self):\n        self.actor.eval()\n        self.critic.eval()\n\n    def copy(self):\n        return PPOPolicy(self.args, self.obs_space, self.act_space, self.device)\n\n    def params(self, device='cuda'):\n        checkpoint = {}\n        if device == 'cuda':\n            checkpoint['actor_state_dict'] = self.actor.state_dict()\n            checkpoint['critic_state_dict'] = self.critic.state_dict()\n        elif device == 'cpu':\n            checkpoint['actor_state_dict'] = {k: v.cpu() for k, v in self.actor.state_dict().items()}\n            checkpoint['critic_state_dict'] = {k: v.cpu() for k, v in self.critic.state_dict().items()}\n\n        return checkpoint\n        \n    def save(self, rootpath, epoch=0):\n        checkpoint = {}\n        checkpoint['actor_state_dict'] = self.actor.state_dict()\n        checkpoint['critic_state_dict'] = self.critic.state_dict()\n        torch.save(checkpoint, str(rootpath) + f'/agent_{epoch}.pt')\n        torch.save(checkpoint, str(rootpath) + f'/agent_latest.pt')\n    \n    def restore_from_path(self, rootpath, epoch=''):\n        checkpoint = torch.load(str(rootpath) + f'/agent0_{epoch}.pt')\n        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n        if 'critic_state_dict' in checkpoint.keys():\n            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n    \n    def restore_from_params(self, params):\n        self.actor.load_state_dict(params['actor_state_dict'])\n        if 'critic_state_dict' in params.keys():\n            self.critic.load_state_dict(params['critic_state_dict'])"]}
{"filename": "ppo/ppo_buffer.py", "chunked_list": ["import numpy as np\nimport torch\nfrom typing import Union, List\n\nfrom util.util_util import get_shape_from_space\n\nclass PPOBuffer():\n\n    @staticmethod\n    def _flatten(T: int, N: int, x: np.ndarray):\n        return x.reshape(T * N, *x.shape[2:])\n\n    @staticmethod\n    def _cast(x: np.ndarray):\n        return x.transpose(1, 0, *range(2, x.ndim)).reshape(-1, *x.shape[2:])\n\n    def __init__(self, args, num_agents, obs_space, act_space):\n        # buffer config\n        self.buffer_size = args.buffer_size\n        self.num_agents = num_agents\n        self.gamma = args.gamma\n        self.use_gae = args.use_gae\n        self.gae_lambda = args.gae_lambda\n        self.use_risk_sensitive = args.use_risk_sensitive\n        self.reward_hyper = 1\n        # rnn config\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n\n        obs_shape = get_shape_from_space(obs_space)\n        act_shape = get_shape_from_space(act_space)\n\n        # (o_0, a_0, r_0, d_1, o_1, ... , d_T, o_T)\n        self.obs = np.zeros((self.buffer_size + 1, self.num_agents, *obs_shape), dtype=np.float32)\n        self.actions = np.zeros((self.buffer_size, self.num_agents, *act_shape), dtype=np.float32)\n        self.rewards = np.zeros((self.buffer_size, self.num_agents, 1), dtype=np.float32)\n        # NOTE: masks[t] = 1 - dones[t-1], which represents whether obs[t] is a terminal state\n        self.masks = np.ones((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n        # NOTE: bad_masks[t] = 'bad_transition' in info[t-1], which indicates whether obs[t] a true terminal state or time limit end state\n        self.bad_masks = np.ones((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n\n        # pi(a)\n        self.action_log_probs = np.zeros((self.buffer_size, self.num_agents, 1), dtype=np.float32)\n        # V(o), R(o) while advantage = returns - value_preds\n        self.value_preds = np.zeros((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n        self.returns = np.zeros((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n        # rnn\n        self.rnn_states_actor = np.zeros((self.buffer_size + 1, self.num_agents,\n                                          self.recurrent_hidden_layers, self.recurrent_hidden_size), dtype=np.float32)\n        self.rnn_states_critic = np.zeros_like(self.rnn_states_actor)\n        self.step = 0\n\n    @property\n    def advantages(self) -> np.ndarray:\n        advantages = self.returns[:-1] - self.value_preds[:-1]  # type: np.ndarray\n        return (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n\n    def insert(self,\n               obs: np.ndarray,\n               actions: np.ndarray,\n               rewards: np.ndarray,\n               masks: np.ndarray,\n               action_log_probs: np.ndarray,\n               value_preds: np.ndarray,\n               rnn_states_actor: np.ndarray,\n               rnn_states_critic: np.ndarray,\n               bad_masks: Union[np.ndarray, None] = None,\n               **kwargs):\n        \"\"\"Insert numpy data.\n        Args:\n            obs:                o_{t+1}\n            actions:            a_{t}\n            rewards:            r_{t}\n            masks:              mask[t+1] = 1 - done_{t}\n            action_log_probs:   log_prob(a_{t})\n            value_preds:        value(o_{t})\n            rnn_states_actor:   ha_{t+1}\n            rnn_states_critic:  hc_{t+1}\n        \"\"\"\n        self.obs[self.step + 1] = obs.copy()\n        self.actions[self.step] = actions.copy()\n        self.rewards[self.step] = rewards.copy()\n        self.masks[self.step + 1] = masks.copy()\n        self.action_log_probs[self.step] = action_log_probs.copy()\n        self.value_preds[self.step] = value_preds.copy()\n        self.rnn_states_actor[self.step + 1] = rnn_states_actor.copy()\n        self.rnn_states_critic[self.step + 1] = rnn_states_critic.copy()\n        if bad_masks is not None:\n            self.bad_masks[self.step + 1] = bad_masks.copy()\n\n        self.step = (self.step + 1) % self.buffer_size\n\n    def after_update(self):\n        \"\"\"Copy last timestep data to first index. Called after update to model.\"\"\"\n        self.obs[0] = self.obs[-1].copy()\n        self.masks[0] = self.masks[-1].copy()\n        self.bad_masks[0] = self.bad_masks[-1].copy()\n        self.rnn_states_actor[0] = self.rnn_states_actor[-1].copy()\n        self.rnn_states_critic[0] = self.rnn_states_critic[-1].copy()\n\n    def clear(self):\n        self.step = 0\n        self.obs = np.zeros_like(self.obs, dtype=np.float32)\n        self.actions = np.zeros_like(self.actions, dtype=np.float32)\n        self.rewards = np.zeros_like(self.rewards, dtype=np.float32)\n        self.masks = np.ones_like(self.masks, dtype=np.float32)\n        self.bad_masks = np.ones_like(self.bad_masks, dtype=np.float32)\n        self.action_log_probs = np.zeros_like(self.action_log_probs, dtype=np.float32)\n        self.value_preds = np.zeros_like(self.value_preds, dtype=np.float32)\n        self.returns = np.zeros_like(self.returns, dtype=np.float32)\n        self.rnn_states_actor = np.zeros_like(self.rnn_states_critic)\n        self.rnn_states_critic = np.zeros_like(self.rnn_states_actor)\n    \n    def dict(self):\n        return {'buffer': self.step}\n\n    def compute_returns(self, next_value: np.ndarray):\n        \"\"\"\n        Compute returns either as discounted sum of rewards, or using GAE.\n\n        Args:\n            next_value(np.ndarray): value predictions for the step after the last episode step.\n        \"\"\"\n        self.rewards[self.rewards==1] = self.rewards[self.rewards==1] * self.reward_hyper\n        self.rewards[self.rewards==-1] = self.rewards[self.rewards==-1] / self.reward_hyper\n        if self.use_risk_sensitive:\n            max_depth = min(100, self.buffer_size)\n            max_depth_vec = np.zeros((self.num_agents, 1)) + max_depth\n            depths = np.zeros((self.buffer_size+1, self.num_agents, 1))\n            for i in reversed(range(self.buffer_size)):\n                depths[i] = np.minimum(depths[i+1] + 1, max_depth_vec)\n                depths[i][self.masks[i+1]==0] = 1\n            values_table = np.zeros((max_depth, self.buffer_size, self.num_agents, 1))\n\n            def operator(reward, value, nextvalue, mask):\n                delta = reward + self.gamma * nextvalue * mask - value\n                delta = self.tau * np.maximum(delta, np.zeros_like(delta)) + \\\n                        (1-self.tau) * np.minimum(delta, np.zeros_like(delta))\n                alpha = 1 / (2 * max(self.tau, (1-self.tau)))\n                delta = 2 * alpha * delta\n                return value + delta\n\n            if self.use_gae:\n                self.value_preds[-1] = next_value\n                for t in reversed(range(self.rewards.shape[0])):\n                    values_table[0, t] = operator(self.rewards[t], self.value_preds[t], self.value_preds[t+1], self.masks[t+1])\n                    for h in range(1, int(depths[t].item())):\n                        values_table[h, t] = operator(self.rewards[t], values_table[h-1][t], values_table[h-1][t+1], self.masks[t+1])\n                    for h in reversed(range(int(depths[t].item()))):\n                        self.returns[t] = values_table[h, t] + self.gae_lambda * self.returns[t]\n                    self.returns[t] *= (1-self.gae_lambda) / (1-self.gae_lambda**(int(depths[t].item()))) \n            else:\n                self.value_preds[-1] = next_value\n                for step in reversed(range(self.rewards.shape[0])):\n                    self.returns[step] = operator(self.rewards[step], self.value_preds[step], self.value_preds[step+1], self.masks[step])\n        else:\n            if self.use_gae:\n                self.value_preds[-1] = next_value\n                gae = 0\n                for step in reversed(range(self.rewards.shape[0])):\n                    td_delta = self.rewards[step] + self.gamma * self.value_preds[step + 1] * self.masks[step + 1] - self.value_preds[step]\n                    gae = td_delta + self.gamma * self.gae_lambda * self.masks[step + 1] * gae\n                    self.returns[step] = gae + self.value_preds[step]\n            else:\n                self.returns[-1] = next_value\n                for step in reversed(range(self.rewards.shape[0])):\n                    self.returns[step] = self.returns[step + 1] * self.gamma * self.masks[step + 1] + self.rewards[step]\n\n\n    @staticmethod\n    def recurrent_generator(buffer, num_mini_batch: int, data_chunk_length: int):\n        \"\"\"\n        A recurrent generator that yields training data for chunked RNN training arranged in mini batches.\n        This generator shuffles the data by sequences.\n\n        Args:\n            buffers (Buffer or List[Buffer]) \n            num_mini_batch (int): number of minibatches to split the batch into.\n            data_chunk_length (int): length of sequence chunks with which to train RNN.\n\n        Returns:\n            (obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n                returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch)\n        \"\"\"\n        buffer = [buffer] if isinstance(buffer, PPOBuffer) else buffer  \n        buffer_size = buffer[0].buffer_size\n        num_agents = buffer[0].num_agents\n        assert all([b.buffer_size == buffer_size for b in buffer]) \\\n            and all([b.num_agents == num_agents for b in buffer]) \\\n            and all([isinstance(b, PPOBuffer) for b in buffer]), \\\n            \"Input buffers must has the same type and shape\"\n        buffer_size = buffer_size * len(buffer)\n\n        assert buffer_size >= data_chunk_length, (\n            \"PPO requires the number of buffer size ({}) * num_agents ({})\"\n            \"to be greater than or equal to the number of \"\n            \"data chunk length ({}).\".format(buffer_size, num_agents, data_chunk_length))\n\n        # Transpose and reshape parallel data into sequential data\n        obs = np.vstack([PPOBuffer._cast(buf.obs[:-1]) for buf in buffer])\n        actions = np.vstack([PPOBuffer._cast(buf.actions) for buf in buffer])\n        masks = np.vstack([PPOBuffer._cast(buf.masks[:-1]) for buf in buffer])\n        old_action_log_probs = np.vstack([PPOBuffer._cast(buf.action_log_probs) for buf in buffer])\n        advantages = np.vstack([PPOBuffer._cast(buf.advantages) for buf in buffer])\n        returns = np.vstack([PPOBuffer._cast(buf.returns[:-1]) for buf in buffer])\n        value_preds = np.vstack([PPOBuffer._cast(buf.value_preds[:-1]) for buf in buffer])\n        rnn_states_actor = np.vstack([PPOBuffer._cast(buf.rnn_states_actor[:-1]) for buf in buffer])\n        rnn_states_critic = np.vstack([PPOBuffer._cast(buf.rnn_states_critic[:-1]) for buf in buffer])\n\n        # Get mini-batch size and shuffle chunk data\n        data_chunks = buffer_size // data_chunk_length\n        mini_batch_size = data_chunks // num_mini_batch\n        rand = torch.randperm(data_chunks).numpy()\n        sampler = [rand[i * mini_batch_size:(i + 1) * mini_batch_size] for i in range(num_mini_batch)]\n        for indices in sampler:\n            obs_batch = []\n            actions_batch = []\n            masks_batch = []\n            old_action_log_probs_batch = []\n            advantages_batch = []\n            returns_batch = []\n            value_preds_batch = []\n            rnn_states_actor_batch = []\n            rnn_states_critic_batch = []\n\n            for index in indices:\n\n                ind = index * data_chunk_length\n                # size [T+1, N, Dim] => [T, N, Dim] => [N, T, Dim] => [N * T, Dim] => [L, Dim]\n                obs_batch.append(obs[ind:ind + data_chunk_length])\n                actions_batch.append(actions[ind:ind + data_chunk_length])\n                masks_batch.append(masks[ind:ind + data_chunk_length])\n                old_action_log_probs_batch.append(old_action_log_probs[ind:ind + data_chunk_length])\n                advantages_batch.append(advantages[ind:ind + data_chunk_length])\n                returns_batch.append(returns[ind:ind + data_chunk_length])\n                value_preds_batch.append(value_preds[ind:ind + data_chunk_length])\n                # size [T+1, N, Dim] => [T, N, Dim] => [N, T, Dim] => [N * T, Dim] => [1, Dim]\n                rnn_states_actor_batch.append(rnn_states_actor[ind])\n                rnn_states_critic_batch.append(rnn_states_critic[ind])\n\n            L, N = data_chunk_length, mini_batch_size\n            # These are all from_numpys of size (L, N, Dim)\n            obs_batch = np.stack(obs_batch, axis=1)\n            actions_batch = np.stack(actions_batch, axis=1)\n            masks_batch = np.stack(masks_batch, axis=1)\n            old_action_log_probs_batch = np.stack(old_action_log_probs_batch, axis=1)\n            advantages_batch = np.stack(advantages_batch, axis=1)\n            returns_batch = np.stack(returns_batch, axis=1)\n            value_preds_batch = np.stack(value_preds_batch, axis=1)\n\n            # States is just a (N, -1) from_numpy\n            rnn_states_actor_batch = np.stack(rnn_states_actor_batch).reshape(N, *buffer[0].rnn_states_actor.shape[2:])\n            rnn_states_critic_batch = np.stack(rnn_states_critic_batch).reshape(N, *buffer[0].rnn_states_critic.shape[2:])\n\n            # Flatten the (L, N, ...) from_numpys to (L * N, ...)\n            obs_batch = PPOBuffer._flatten(L, N, obs_batch)\n            actions_batch = PPOBuffer._flatten(L, N, actions_batch)\n            masks_batch = PPOBuffer._flatten(L, N, masks_batch)\n            old_action_log_probs_batch = PPOBuffer._flatten(L, N, old_action_log_probs_batch)\n            advantages_batch = PPOBuffer._flatten(L, N, advantages_batch)\n            returns_batch = PPOBuffer._flatten(L, N, returns_batch)\n            value_preds_batch = PPOBuffer._flatten(L, N, value_preds_batch)\n\n            yield obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n                returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch"]}
{"filename": "ppo/ppo_actor.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom util.util_mlp import MLPBase\nfrom util.util_gru import GRULayer\nfrom util.util_act import ACTLayer\nfrom util.util_util import check\n\n\nclass PPOActor(nn.Module):\n    def __init__(self, args, obs_space, act_space, device=torch.device(\"cpu\")):\n        super(PPOActor, self).__init__()\n        # network config\n        self.gain = args.gain\n        self.hidden_size = args.hidden_size\n        self.act_hidden_size = args.act_hidden_size\n        self.activation_id = args.activation_id\n        self.use_feature_normalization = args.use_feature_normalization\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.use_cnn = args.use_cnn\n        self.tpdv = dict(dtype=torch.float32, device=device)\n        # (1) feature extraction module\n        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n            self.base = nn.Sequential(\n            nn.Conv2d(4, 8, 4, stride=2), ## (40+2-4)/2+1 = 20\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2+1 = 10\n            nn.ReLU(),\n            nn.MaxPool2d(2, 1),\n            nn.Flatten(),\n            nn.Linear(16*7*7, 128),\n            nn.ReLU()\n        )\n            input_size = 128\n        else:\n            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n            input_size = self.base.output_size\n        # (2) rnn module\n        if self.use_recurrent_policy:\n            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n            input_size = self.rnn.output_size\n        # (3) act module\n        self.act = ACTLayer(act_space, input_size, self.act_hidden_size, self.activation_id, self.gain)\n\n        self.to(device)\n\n    def forward(self, obs, rnn_states, masks, deterministic=False):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        actor_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\n        actions, action_log_probs = self.act(actor_features, deterministic)\n\n        return actions, action_log_probs, rnn_states\n\n    def evaluate_actions(self, obs, rnn_states, action, masks, active_masks=None):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        action = check(action).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        if active_masks is not None:\n            active_masks = check(active_masks).to(**self.tpdv)\n\n        actor_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\n        action_log_probs, dist_entropy = self.act.evaluate_actions(actor_features, action, active_masks)\n\n        return action_log_probs, dist_entropy", "\nclass PPOActor(nn.Module):\n    def __init__(self, args, obs_space, act_space, device=torch.device(\"cpu\")):\n        super(PPOActor, self).__init__()\n        # network config\n        self.gain = args.gain\n        self.hidden_size = args.hidden_size\n        self.act_hidden_size = args.act_hidden_size\n        self.activation_id = args.activation_id\n        self.use_feature_normalization = args.use_feature_normalization\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.use_cnn = args.use_cnn\n        self.tpdv = dict(dtype=torch.float32, device=device)\n        # (1) feature extraction module\n        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n            self.base = nn.Sequential(\n            nn.Conv2d(4, 8, 4, stride=2), ## (40+2-4)/2+1 = 20\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2+1 = 10\n            nn.ReLU(),\n            nn.MaxPool2d(2, 1),\n            nn.Flatten(),\n            nn.Linear(16*7*7, 128),\n            nn.ReLU()\n        )\n            input_size = 128\n        else:\n            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n            input_size = self.base.output_size\n        # (2) rnn module\n        if self.use_recurrent_policy:\n            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n            input_size = self.rnn.output_size\n        # (3) act module\n        self.act = ACTLayer(act_space, input_size, self.act_hidden_size, self.activation_id, self.gain)\n\n        self.to(device)\n\n    def forward(self, obs, rnn_states, masks, deterministic=False):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        actor_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\n        actions, action_log_probs = self.act(actor_features, deterministic)\n\n        return actions, action_log_probs, rnn_states\n\n    def evaluate_actions(self, obs, rnn_states, action, masks, active_masks=None):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        action = check(action).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        if active_masks is not None:\n            active_masks = check(active_masks).to(**self.tpdv)\n\n        actor_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\n        action_log_probs, dist_entropy = self.act.evaluate_actions(actor_features, action, active_masks)\n\n        return action_log_probs, dist_entropy", ""]}
{"filename": "ppo/ppo_critic.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom util.util_mlp import MLPBase, MLPLayer\nfrom util.util_gru import GRULayer\nfrom util.util_act import ACTLayer\nfrom util.util_util import check\n\n\nclass PPOCritic(nn.Module):\n    def __init__(self, args, obs_space, device=torch.device(\"cpu\")):\n        super(PPOCritic, self).__init__()\n        # network config\n        self.hidden_size = args.hidden_size\n        self.act_hidden_size = args.act_hidden_size\n        self.activation_id = args.activation_id\n        self.use_feature_normalization = args.use_feature_normalization\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.tpdv = dict(dtype=torch.float32, device=device)\n        self.use_cnn = args.use_cnn\n        # (1) feature extraction module\n        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n            self.base = nn.Sequential(\n            nn.Conv2d(4, 8, 4, stride=2), # (40+2-4)/2+1 = 20\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2 + 1 = 10\n            nn.ReLU(),\n            nn.MaxPool2d(2, 1),\n            nn.Flatten(),\n            nn.Linear(16*7*7, 128),\n            nn.ReLU()\n        )\n            input_size = 128\n        else:\n            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n            input_size = self.base.output_size\n        # (2) rnn module\n        if self.use_recurrent_policy:\n            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n            input_size = self.rnn.output_size\n        # (3) value module\n        if len(self.act_hidden_size) > 0:\n            self.mlp = MLPLayer(input_size, self.act_hidden_size, self.activation_id)\n        self.value_out = nn.Linear(input_size, 1)\n\n        self.to(device)\n\n    def forward(self, obs, rnn_states, masks):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        critic_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            critic_features, rnn_states = self.rnn(critic_features, rnn_states, masks)\n\n        if len(self.act_hidden_size) > 0:\n            critic_features = self.mlp(critic_features)\n\n        values = self.value_out(critic_features)\n\n        return values, rnn_states", "\nclass PPOCritic(nn.Module):\n    def __init__(self, args, obs_space, device=torch.device(\"cpu\")):\n        super(PPOCritic, self).__init__()\n        # network config\n        self.hidden_size = args.hidden_size\n        self.act_hidden_size = args.act_hidden_size\n        self.activation_id = args.activation_id\n        self.use_feature_normalization = args.use_feature_normalization\n        self.use_recurrent_policy = args.use_recurrent_policy\n        self.recurrent_hidden_size = args.recurrent_hidden_size\n        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n        self.tpdv = dict(dtype=torch.float32, device=device)\n        self.use_cnn = args.use_cnn\n        # (1) feature extraction module\n        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n            self.base = nn.Sequential(\n            nn.Conv2d(4, 8, 4, stride=2), # (40+2-4)/2+1 = 20\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2 + 1 = 10\n            nn.ReLU(),\n            nn.MaxPool2d(2, 1),\n            nn.Flatten(),\n            nn.Linear(16*7*7, 128),\n            nn.ReLU()\n        )\n            input_size = 128\n        else:\n            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n            input_size = self.base.output_size\n        # (2) rnn module\n        if self.use_recurrent_policy:\n            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n            input_size = self.rnn.output_size\n        # (3) value module\n        if len(self.act_hidden_size) > 0:\n            self.mlp = MLPLayer(input_size, self.act_hidden_size, self.activation_id)\n        self.value_out = nn.Linear(input_size, 1)\n\n        self.to(device)\n\n    def forward(self, obs, rnn_states, masks):\n        obs = check(obs).to(**self.tpdv)\n        rnn_states = check(rnn_states).to(**self.tpdv)\n        masks = check(masks).to(**self.tpdv)\n\n        critic_features = self.base(obs)\n\n        if self.use_recurrent_policy:\n            critic_features, rnn_states = self.rnn(critic_features, rnn_states, masks)\n\n        if len(self.act_hidden_size) > 0:\n            critic_features = self.mlp(critic_features)\n\n        values = self.value_out(critic_features)\n\n        return values, rnn_states", ""]}
{"filename": "util/util_distributions.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom util.util_util import init\n\"\"\"\nModify standard PyTorch distributions so they are compatible with this code.\n\"\"\"\n\n# Standardize distribution interfaces\n\n", "\n\n# Categorical\nclass FixedCategorical(torch.distributions.Categorical):\n    def sample(self):\n        return super().sample().unsqueeze(-1)\n\n    def log_probs(self, actions):\n        # Single: [1] => [] => [] => [1, 1] => [1] => [1]\n        # Batch: [N]/[N, 1] => [N] => [N] => [N, 1] => [N] => [N, 1]\n        return (\n            super()\n            .log_prob(actions.squeeze(-1))\n            .view(actions.squeeze(-1).unsqueeze(-1).size())\n            .sum(-1, keepdim=True)\n        )\n\n    def mode(self):\n        return self.probs.argmax(dim=-1, keepdim=True)\n\n    def entropy(self):\n        return super().entropy().unsqueeze(-1)", "\n\n# Normal\nclass FixedNormal(torch.distributions.Normal):\n    def log_probs(self, actions):\n        return super().log_prob(actions).sum(-1, keepdim=True)\n\n    def entropy(self):\n        return super().entropy().sum(-1, keepdim=True)\n\n    def mode(self):\n        return self.mean", "\n\n# Bernoulli\nclass FixedBernoulli(torch.distributions.Bernoulli):\n    def log_probs(self, actions):\n        # Single: [K] => [K] => [1]\n        # Batch: [N, K] => [N, K] => [N, 1]\n        return super().log_prob(actions).sum(-1, keepdim=True)\n\n    def entropy(self):\n        return super().entropy().sum(-1, keepdim=True)\n\n    def mode(self):\n        return torch.gt(self.probs, 0.5).float()", "\n\nclass Categorical(nn.Module):\n    def __init__(self, num_inputs, num_outputs, gain=0.01):\n        super(Categorical, self).__init__()\n\n        def init_(m):\n            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\n        self.logits_net = init_(nn.Linear(num_inputs, num_outputs))\n\n    def forward(self, x):\n        x = self.logits_net(x)\n        return FixedCategorical(logits=x)\n\n    @property\n    def output_size(self) -> int:\n        return 1", "\n\nclass DiagGaussian(nn.Module):\n    def __init__(self, num_inputs, num_outputs, gain=0.01):\n        super(DiagGaussian, self).__init__()\n\n        def init_(m):\n            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\n        self.mu_net = init_(nn.Linear(num_inputs, num_outputs))\n        self.log_std = nn.Parameter(torch.zeros(num_outputs))\n        self._num_outputs = num_outputs\n\n    def forward(self, x):\n        action_mean = self.mu_net(x)\n        return FixedNormal(action_mean, self.log_std.exp())\n\n    @property\n    def output_size(self) -> int:\n        return self._num_outputs", "\nclass Bernoulli(nn.Module):\n    def __init__(self, num_inputs, num_outputs, gain=0.01):\n        super(Bernoulli, self).__init__()\n\n        def init_(m):\n            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\n        self.logits_net = init_(nn.Linear(num_inputs, num_outputs))\n        self._num_outputs = num_outputs\n\n    def forward(self, x):\n        x = self.logits_net(x)\n        return FixedBernoulli(logits=x)\n\n    @property\n    def output_size(self) -> int:\n        return self._num_outputs", ""]}
{"filename": "util/util_selfplay.py", "chunked_list": ["from typing import Dict, List, Tuple\nimport numpy as np\nimport random\nfrom abc import ABC, abstractstaticmethod\n\ndef get_algorithm(algo_name):\n    if algo_name == 'sp':\n        return SP()\n    elif algo_name == 'fsp':\n        return FSP()\n    elif algo_name == 'pfsp':\n        return PFSP()\n    else:\n        raise NotImplementedError(\"Unknown algorithm {}\".format(algo_name))", "\n\nclass SelfplayAlgorithm(ABC):\n    def __init__(self) -> None:\n        pass\n\n    def choose_opponents(self, agent_idx: int, agent_elos: dict, num_opponents: int):\n        enm_idxs, enm_history_idxs, enm_elos = [], [], []\n        num_total = 1\n        while True:\n            enm_idx = random.choice([k for k in list(agent_elos.keys())])\n            # 1) choose the opponent agent from populations.\n            enm_idxs.append(enm_idx)\n            # 2) choose the history copy from the current agent according to ELO\n            enm_history_idx = self._choose_history(agent_elos[enm_idx])\n            enm_history_idxs.append(enm_history_idx)\n            enm_elos.append(agent_elos[enm_idx][enm_history_idx])\n            num_total += 1\n            if num_total > num_opponents:\n                break\n        enms = []\n        for agent, itr in zip(enm_idxs, enm_history_idxs):\n                enms.append((agent, itr))\n        return enms, enm_elos\n    \n    @abstractstaticmethod\n    def _choose_history(self, agents_elo: Dict[str, float]):\n        pass", "\nclass SP(SelfplayAlgorithm):\n    def __init__(self) -> None:\n        super().__init__()\n    \n    def _choose_history(self, agents_elo: Dict[str, float]):\n        return list(agents_elo.keys())[-1]\n\n\nclass FSP(SelfplayAlgorithm):\n    def __init__(self) -> None:\n         super().__init__()\n    \n    def _choose_history(self, agents_elo: Dict[str, float]):\n        return np.random.choice(list(agents_elo.keys()))", "\nclass FSP(SelfplayAlgorithm):\n    def __init__(self) -> None:\n         super().__init__()\n    \n    def _choose_history(self, agents_elo: Dict[str, float]):\n        return np.random.choice(list(agents_elo.keys()))\n\n\nclass PFSP(SelfplayAlgorithm):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lam = 1.\n        self.s = 100.\n    \n    def _choose_history(self, agents_elo: Dict[str, float]):\n        history_elo = np.array(list(agents_elo.values()))\n        sample_probs = 1. / (1. + 10. ** (-(history_elo - np.median(history_elo)) / 400.)) * self.s\n        \"\"\" meta-solver \"\"\"\n        k = float(len(sample_probs) + 1)\n        meta_solver_probs = np.exp(self.lam / k * sample_probs) / np.sum(np.exp(self.lam / k * sample_probs))\n        opponent_idx = np.random.choice(a=list(agents_elo.keys()), size=1, p=meta_solver_probs).item()\n        return opponent_idx", "\nclass PFSP(SelfplayAlgorithm):\n    def __init__(self) -> None:\n        super().__init__()\n        self.lam = 1.\n        self.s = 100.\n    \n    def _choose_history(self, agents_elo: Dict[str, float]):\n        history_elo = np.array(list(agents_elo.values()))\n        sample_probs = 1. / (1. + 10. ** (-(history_elo - np.median(history_elo)) / 400.)) * self.s\n        \"\"\" meta-solver \"\"\"\n        k = float(len(sample_probs) + 1)\n        meta_solver_probs = np.exp(self.lam / k * sample_probs) / np.sum(np.exp(self.lam / k * sample_probs))\n        opponent_idx = np.random.choice(a=list(agents_elo.keys()), size=1, p=meta_solver_probs).item()\n        return opponent_idx", "\n\nif __name__ == '__main__':\n    ranks = {\n        0: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n        1: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n        2: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n        3: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n    }\n\n    algo = get_algorithm(\"pfsp\")\n    ret = algo.choose_opponents(agent_idx=0, agent_elos=ranks, num_opponents=4)\n\n    print(ret[0], ret[1])", ""]}
{"filename": "util/util_mlp.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom util.util_util import build_flattener\n\nclass MLPLayer(nn.Module):\n    def __init__(self, input_dim, hidden_size, activation_id):\n        super(MLPLayer, self).__init__()\n        self._size = [input_dim] + list(map(int, hidden_size.split(' ')))\n        self._hidden_layers = len(self._size) - 1\n        active_func = [nn.Tanh(), nn.ReLU(), nn.LeakyReLU(), nn.ELU()][activation_id]\n\n        fc_h = []\n        for j in range(len(self._size) - 1):\n            fc_h += [\n                nn.Linear(self._size[j], self._size[j + 1]), active_func, nn.LayerNorm(self._size[j + 1])\n            ]\n        self.fc = nn.Sequential(*fc_h)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc(x)\n        return x\n\n    @property\n    def output_size(self) -> int:\n        return self._size[-1]", "\n\n# Feature extraction module\nclass MLPBase(nn.Module):\n    def __init__(self, obs_space, hidden_size, activation_id, use_feature_normalization):\n        super(MLPBase, self).__init__()\n        self._hidden_size = hidden_size\n        self._activation_id = activation_id\n        self._use_feature_normalization = use_feature_normalization\n\n        self.obs_flattener = build_flattener(obs_space)\n        input_dim = self.obs_flattener.size\n        if self._use_feature_normalization:\n            self.feature_norm = nn.LayerNorm(input_dim)\n        self.mlp = MLPLayer(input_dim, self._hidden_size, self._activation_id)\n\n    def forward(self, x: torch.Tensor):\n        if self._use_feature_normalization:\n            x = self.feature_norm(x)\n        x = self.mlp(x)\n        return x\n\n    @property\n    def output_size(self) -> int:\n        return self.mlp.output_size", ""]}
{"filename": "util/util_population.py", "chunked_list": ["import os\nimport random\nimport shutil\nimport numpy as np\nfrom copy import deepcopy\n\n\ndef population_based_exploit_explore(epoch: int, elos: dict, hypers: dict, run_dir: str, elo_threshold):\n    topk = int(np.ceil(0.2 * len(elos)))\n    elos_new = deepcopy(elos)\n    hypers_new = deepcopy(hypers)\n    ranks = {agent_id: elos[agent_id][epoch+1] for agent_id in elos.keys()}\n    sorted_ranks_idxs = [k for k in sorted(ranks, key=ranks.__getitem__, reverse=True)]\n    topk_idxs = list(sorted_ranks_idxs[:topk])\n    for agent_id in elos_new.keys():\n        agent_elo = elos[agent_id][epoch+1]\n        better_agent_id = random.sample(topk_idxs, 1)[0]\n        if len(sorted_ranks_idxs) == 1:\n            # population size = 1, no exploit\n            break\n        if ranks[better_agent_id] - agent_elo < elo_threshold or agent_id in topk_idxs:\n            # the agent is already good enough\n            continue\n        elos_new[agent_id][epoch+1] = elos[better_agent_id][epoch+1]\n        os.remove(f\"{run_dir}/agent{agent_id}_history{epoch+1}.pt\")\n        os.remove(f\"{run_dir}/agent{agent_id}_latest.pt\")\n        shutil.copy(f\"{run_dir}/agent{better_agent_id}_latest.pt\", f\"{run_dir}/agent{agent_id}_history{epoch+1}.pt\")\n        shutil.copy(f\"{run_dir}/agent{better_agent_id}_latest.pt\", f\"{run_dir}/agent{agent_id}_latest.pt\")\n        for s in hypers[agent_id].keys():\n            hyper_ = hypers[agent_id][s]\n            new_hyper_ = hypers[better_agent_id][s]\n            inherit_prob = np.random.binomial(1, 0.5, 1)[0]\n            hyper_tmp = (1. - inherit_prob) * hyper_ + inherit_prob * new_hyper_\n            hypers_new[agent_id][s] = np.clip(hyper_tmp + np.random.uniform(-0.2, 0.2), 0., 1.)\n    return elos_new, hypers_new", ""]}
{"filename": "util/util_gru.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\nclass GRULayer(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n        super(GRULayer, self).__init__()\n        self._hidden_size = hidden_size\n        self._num_layers = num_layers\n\n        self.gru = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          num_layers=num_layers)\n        # NOTE: self.gru(x, hxs) needs x=[T, N, input_size] and hxs=[L, N, hidden_size]\n\n        self.norm = nn.LayerNorm(hidden_size)\n\n    def forward(self, x: torch.Tensor, hxs: torch.Tensor, masks: torch.Tensor):\n        # NOTE: N = mini_batch_size; T = recurrent chunk length; L = gru layers\n\n        # (T=1) x: [N, input_size], hxs: [N, L, hidden_size], masks: [N, 1]\n        if x.size(0) == hxs.size(0):\n            # masks: [N, 1] => [N, L] => [N, L, 1]\n            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks.repeat(1, self._num_layers).unsqueeze(-1)).transpose(0, 1).contiguous())\n\n            x = x.squeeze(0)            # [1, N, input_size] => [N, input_size]\n            hxs = hxs.transpose(0, 1)   # [L, N, hidden_size] => [N, L, hidden_size]\n\n        # (T>1): x=[T * N, input_size], hxs=[N, L, hidden_size], masks=[T * N, 1]\n        else:\n            # Mannual reset hxs to zero at ternimal states might be too slow to calculate\n            # We need to tackle the problem more efficiently\n\n            # x is a (T, N, input_size) tensor that has been flatten to (T * N, -1)\n            N = hxs.size(0)\n            T = int(x.size(0) / N)\n            # unflatten x and masks\n            x = x.view(T, N, x.size(1))  # [T * N, input_size] => [T, N, input_size]\n            masks = masks.view(T, N)     # [T * N, 1] => [T, N]\n\n            # Let's figure out which steps in the sequence have a zero for any agent\n            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n            has_zeros = ((masks[1:] == 0.0)\n                         .any(dim=-1)       # [T, N] => [T, 1]\n                         .nonzero(as_tuple=False)\n                         .squeeze(dim=-1)   # [T, 1] => [T]\n                         .cpu())\n            # +1 to correct the masks[1:]\n            has_zeros = (has_zeros + 1).numpy().tolist()\n            # add t=0 and t=T to the list\n            has_zeros = [0] + has_zeros + [T]\n\n            hxs = hxs.transpose(0, 1)   # [N, L, hidden_size] => [L, N, hidden_size]\n            outputs = []\n            for i in range(len(has_zeros) - 1):\n                # We can now process steps that don't have any zeros in masks together!\n                start_idx = has_zeros[i]\n                end_idx = has_zeros[i + 1]\n                # masks[start_idx]: [N] => [1, N, 1] => [L, N, 1]\n                temp = (hxs * masks[start_idx].view(1, -1, 1).repeat(self._num_layers, 1, 1)).contiguous()\n                rnn_scores, hxs = self.gru(x[start_idx:end_idx], temp)\n                outputs.append(rnn_scores)\n            # x is a (T, N, -1) tensor\n            x = torch.cat(outputs, dim=0)\n\n            # flatten\n            x = x.view(T * N, -1)       # [T, N, input_size] => [T * N, input_size]\n            hxs = hxs.transpose(0, 1)   # [L, N, hidden_size] => [N, L, hidden_size]\n\n        x = self.norm(x)\n        return x, hxs\n\n    @property\n    def output_size(self):\n        return self._hidden_size", ""]}
{"filename": "util/util_util.py", "chunked_list": ["import copy\nimport math\nimport gym.spaces\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\n\ndef check(input):\n    output = torch.from_numpy(input) if type(input) == np.ndarray else input\n    return output", "\ndef check(input):\n    output = torch.from_numpy(input) if type(input) == np.ndarray else input\n    return output\n\n\ndef get_shape_from_space(space):\n    if isinstance(space, gym.spaces.Discrete):\n        return (1,)\n    elif isinstance(space, gym.spaces.Box) \\\n            or isinstance(space, gym.spaces.MultiDiscrete) \\\n            or isinstance(space, gym.spaces.MultiBinary):\n        return space.shape\n    elif isinstance(space,gym.spaces.Tuple) and \\\n           isinstance(space[0], gym.spaces.MultiDiscrete) and \\\n               isinstance(space[1], gym.spaces.Discrete):\n        return (space[0].shape[0] + 1,)\n    else:\n        raise NotImplementedError(f\"Unsupported action space type: {type(space)}!\")", "\n\ndef get_gard_norm(it):\n    sum_grad = 0\n    for x in it:\n        if x.grad is None:\n            continue\n        sum_grad += x.grad.norm() ** 2\n    return math.sqrt(sum_grad)\n", "\n\ndef init(module: nn.Module, weight_init, bias_init, gain=1):\n    weight_init(module.weight.data, gain=gain)\n    bias_init(module.bias.data)\n    return module\n\n\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])", "def get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\n\ndef build_flattener(space):\n    if isinstance(space, gym.spaces.Dict):\n        return DictFlattener(space)\n    elif isinstance(space, gym.spaces.Box) \\\n            or isinstance(space, gym.spaces.MultiDiscrete):\n        return BoxFlattener(space)\n    elif isinstance(space, gym.spaces.Discrete):\n        return DiscreteFlattener(space)\n    else:\n        raise NotImplementedError", "\nclass DictFlattener():\n    \"\"\"Dict\u548cVector\u76f4\u63a5\u7684\u8f6c\u6362\n    \"\"\"\n\n    def __init__(self, ori_space):\n        self.space = ori_space\n        assert isinstance(ori_space, gym.spaces.Dict)\n        self.size = 0\n        self.flatteners = OrderedDict()\n        for name, space in self.space.spaces.items():\n            if isinstance(space, gym.spaces.Box):\n                flattener = BoxFlattener(space)\n            elif isinstance(space, gym.spaces.Discrete):\n                flattener = DiscreteFlattener(space)\n            elif isinstance(space, gym.spaces.Dict):\n                flattener = DictFlattener(space)\n            self.flatteners[name] = flattener\n            self.size += flattener.size\n\n    def __call__(self, observation):\n        \"\"\"\u628aDict\u8f6c\u6362\u6210Vector\n        \"\"\"\n        assert isinstance(observation, OrderedDict)\n        batch = self.get_batch(observation, self)\n        if batch == 1:\n            array = np.zeros(self.size,)\n        else:\n            array = np.zeros(self.size)\n\n        self.write(observation, array, 0)\n        return array\n\n    def inv(self, observation):\n        \"\"\"\u628aVector\u89e3\u7801\u6210Dict\n        \"\"\"\n        offset_start, offset_end = 0, 0\n        output = OrderedDict()\n        for n, f in self.flatteners.items():\n            offset_end += f.size\n            output[n] = f.inv(observation[..., offset_start:offset_end])\n            offset_start = offset_end\n        return output\n\n    def write(self, observation, array, offset):\n        for o, f in zip(observation.values(), self.flatteners.values()):\n            f.write(o, array, offset)\n            offset += f.size\n\n    def get_batch(self, observation, flattener):\n        if isinstance(observation, dict):\n            # \u5982\u679c\u662f\u5b57\u5178\u7684\u8bdd\u8fd4\u56de\u7b2c\u4e00\u4e2a\u7684batch\n            for o, f in zip(observation.values(), flattener.flatteners.values()):\n                return self.get_batch(o, f)\n        else:\n            return np.asarray(observation).size // flattener.size", "\n\nclass BoxFlattener():\n    \"\"\"\u628aBox/MultiDiscrete\u7c7b\u578b\u7684\u7a7a\u95f4\u53d8\u6210\u4e00\u4e2aVector\n    \"\"\"\n\n    def __init__(self, ori_space):\n        self.space = ori_space\n        assert isinstance(ori_space, gym.spaces.Box) \\\n            or isinstance(ori_space, gym.spaces.MultiDiscrete)\n        self.size = np.product(ori_space.shape)\n\n    def __call__(self, observation):\n        array = np.array(observation, copy=False)\n        if array.size // self.size == 1:\n            return array.ravel()\n        else:\n            return array.reshape(-1, self.size)\n\n    def inv(self, observation):\n        array = np.array(observation, copy=False)\n        if array.size // self.size == 1:\n            return array.reshape(self.space.shape)\n        else:\n            return array.reshape((-1,) + self.space.shape)\n\n    def write(self, observation, array, offset):\n        array[..., offset:offset + self.size] = self(observation)", "\n\nclass DiscreteFlattener():\n    \"\"\"\u628aDiscrete\u7c7b\u578b\u7684\u7a7a\u95f4\u53d8\u6210\u4e00\u4e2aVector\n    \"\"\"\n\n    def __init__(self, ori_space):\n        self.space = ori_space\n        assert isinstance(ori_space, gym.spaces.Discrete)\n        self.size = 1\n\n    def __call__(self, observation):\n        array = np.array(observation, copy=False)\n        if array.size == 1:\n            return array.item()\n        else:\n            return array.reshape(-1, 1)\n\n    def inv(self, observation):\n        array = np.array(observation, dtype=np.int, copy=False)\n        if array.size == 1:\n            return array.item()\n        else:\n            return array.reshape(-1, 1)\n\n    def write(self, observation, array, offset):\n        array[..., offset:offset + 1] = self(observation)"]}
{"filename": "util/util_act.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport gym.spaces\nfrom util.util_mlp import MLPLayer\nfrom util.util_distributions import Categorical, DiagGaussian, Bernoulli\n\n\nclass ACTLayer(nn.Module):\n    def __init__(self, act_space, input_dim, hidden_size, activation_id, gain):\n        super(ACTLayer, self).__init__()\n        self._mlp_actlayer = False\n        self._continuous_action = False\n        self._multidiscrete_action = False\n        self._mixed_action = False\n\n        if len(hidden_size) > 0:\n            self._mlp_actlayer = True\n            self.mlp = MLPLayer(input_dim, hidden_size, activation_id)\n            input_dim = self.mlp.output_size\n\n        if isinstance(act_space, gym.spaces.Discrete):\n            action_dim = act_space.n\n            self.action_out = Categorical(input_dim, action_dim, gain)\n        elif isinstance(act_space, gym.spaces.Box):\n            self._continuous_action = True\n            action_dim = act_space.shape[0]\n            self.action_out = DiagGaussian(input_dim, action_dim, gain)\n        elif isinstance(act_space, gym.spaces.MultiBinary):\n            action_dim = act_space.shape[0]\n            self.action_out = Bernoulli(input_dim, action_dim, gain)\n        elif isinstance(act_space, gym.spaces.MultiDiscrete):\n            self._multidiscrete_action = True\n            action_dims = act_space.nvec\n            action_outs = []\n            for action_dim in action_dims:\n                action_outs.append(Categorical(input_dim, action_dim, gain))\n            self.action_outs = nn.ModuleList(action_outs)\n        else: \n            raise NotImplementedError(f\"Unsupported action space type: {type(act_space)}!\")\n\n    def forward(self, x, deterministic=False, **kwargs):\n        \"\"\"\n        Compute actions and action logprobs from given input.\n\n        Args:\n            x (torch.Tensor): input to network.\n            deterministic (bool): whether to sample from action distribution or return the mode.\n\n        Returns:\n            actions (torch.Tensor): actions to take.\n            action_log_probs (torch.Tensor): log probabilities of taken actions.\n        \"\"\"\n        if self._mlp_actlayer:\n            x = self.mlp(x)\n\n        if self._multidiscrete_action:\n            actions = []\n            action_log_probs = []\n            for action_out in self.action_outs:\n                action_dist = action_out(x)\n                action = action_dist.mode() if deterministic else action_dist.sample()\n                action_log_prob = action_dist.log_probs(action)\n                actions.append(action)\n                action_log_probs.append(action_log_prob)\n            actions = torch.cat(actions, dim=-1)\n            action_log_probs = torch.cat(action_log_probs, dim=-1).sum(dim=-1, keepdim=True)\n\n        else:\n            action_dists = self.action_out(x)\n            actions = action_dists.mode() if deterministic else action_dists.sample()\n            action_log_probs = action_dists.log_probs(actions)\n        return actions, action_log_probs\n\n    def evaluate_actions(self, x, action, active_masks=None, **kwargs):\n        \"\"\"\n        Compute log probability and entropy of given actions.\n\n        Args:\n            x (torch.Tensor): input to network.\n            action (torch.Tensor): actions whose entropy and log probability to evaluate.\n            active_masks (torch.Tensor): denotes whether an agent is active or dead.\n\n        Returns:\n            action_log_probs (torch.Tensor): log probabilities of the input actions.\n            dist_entropy (torch.Tensor): action distribution entropy for the given inputs.\n        \"\"\"\n        if self._mlp_actlayer:\n            x = self.mlp(x)\n\n        if self._multidiscrete_action:\n            action = torch.transpose(action, 0, 1)\n            action_log_probs = []\n            dist_entropy = []\n            for action_out, act in zip(self.action_outs, action):\n                action_dist = action_out(x)\n                action_log_probs.append(action_dist.log_probs(act.unsqueeze(-1)))\n                if active_masks is not None:\n                    dist_entropy.append((action_dist.entropy() * active_masks) / active_masks.sum())\n                else:\n                    dist_entropy.append(action_dist.entropy() / action_log_probs[-1].size(0))\n            action_log_probs = torch.cat(action_log_probs, dim=-1).sum(dim=-1, keepdim=True)\n            dist_entropy = torch.cat(dist_entropy, dim=-1).sum(dim=-1, keepdim=True)\n\n        else:\n            action_dist = self.action_out(x)\n            action_log_probs = action_dist.log_probs(action)\n            if active_masks is not None:\n                dist_entropy = (action_dist.entropy() * active_masks) / active_masks.sum()\n            else:\n                dist_entropy = action_dist.entropy() / action_log_probs.size(0)\n        return action_log_probs, dist_entropy\n\n    def get_probs(self, x):\n        \"\"\"\n        Compute action probabilities from inputs.\n\n        Args:\n            x (torch.Tensor): input to network.\n\n        Return:\n            action_probs (torch.Tensor):\n        \"\"\"\n        if self._mlp_actlayer:\n            x = self.mlp(x)\n        if self._multidiscrete_action:\n            action_probs = []\n            for action_out in self.action_outs:\n                action_dist = action_out(x)\n                action_prob = action_dist.probs\n                action_probs.append(action_prob)\n            action_probs = torch.cat(action_probs, dim=-1)\n        elif self._continuous_action:\n            raise ValueError(\"Normal distribution has no `probs` attribute!\")\n        else:\n            action_dists = self.action_out(x)\n            action_probs = action_dists.probs\n        return action_probs\n\n    @property\n    def output_size(self) -> int:\n        if self._multidiscrete_action:\n            return len(self.action_outs)\n        else:\n            return self.action_out.output_size", ""]}
{"filename": "envs/__init__.py", "chunked_list": ["from gym.envs.registration import register\nfrom envs.slimevolley.slimevolley_wrapper import VolleyballEnv\n\nregister(\n    id=\"ToyEnv-v0\",\n    entry_point=\"envs.toy:ToyEnv\",\n    kwargs={'wind':True, 'onehot_state':True},\n)\n\nregister(", "\nregister(\n    id='SlimeVolley-v0',\n    entry_point='envs:VolleyballEnv'\n)\n\n\nregister(\n    id='SumoAnts-v0',\n    entry_point='envs.robosumo.envs:SumoEnv',", "    id='SumoAnts-v0',\n    entry_point='envs.robosumo.envs:SumoEnv',\n    kwargs={\n        'agent_names': ['ant', 'ant'],\n        'agent_densities': [13., 13.],\n        'tatami_size': 2.0,\n        'timestep_limit': 500,\n        'reward_shape': 1. \n    },\n)", "    },\n)"]}
{"filename": "envs/toy.py", "chunked_list": ["import numpy as np\nfrom gym import Env, spaces\nfrom gym.utils import seeding\nfrom contextlib import closing\nfrom io import StringIO\nimport sys\n\nUP = 0\nRIGHT = 1\nDOWN = 2", "RIGHT = 1\nDOWN = 2\nLEFT = 3\nDELTA = np.array([\n    (-1, 0),\n    (0, 1),\n    (1, 0),\n    (0, -1)\n])\n\nclass ToyEnv(Env):\n    def __init__(self, wind = True, onehot_state=True) -> None:\n        self.onehot_state = onehot_state\n        self.shape = (4, 4)\n        self.nS = np.prod(self.shape)\n        self.nA = 4\n        # self.observation_space = spaces.Discrete(self.nS)\n        self.observation_space = spaces.Box(low=0, high=1, shape=(16,)) if self.onehot_state else spaces.Discrete(self.nS)\n        self.action_space = spaces.Discrete(self.nA)\n\n        self.use_wind = wind\n        self._wind = np.ones(self.shape)\n        self._water = np.zeros(self.shape)\n        self._water[3, :] = 1\n\n\n        self.start = (2, 0)\n        self.terminate = (2, 3)\n        self.seed()\n    \n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n    \n    def reset(self):\n        self.visited = np.zeros(self.shape)\n        self.current_step = 0\n        self.s = np.array(self.start)\n        self.visited[tuple(self.s)] += 1\n        state = np.ravel_multi_index(self.s, self.shape)\n        if self.onehot_state:\n            state = np.eye(self.nS)[state]\n        return state\n    \n    def step(self, action):\n        self.current_step += 1\n        reward = 0\n        done = False\n\n        if self.use_wind and self._wind[tuple(self.s)]:\n            if self.np_random.random() >= 0.5:\n                action = self.np_random.choice([UP, RIGHT, DOWN, LEFT])\n        new_pos1 = self.s + DELTA[action]\n        new_pos1 = self._limit_coordinates(new_pos1).astype(int)\n\n        if tuple(new_pos1) == self.terminate:\n            reward = 1\n            done = True\n        elif self._water[tuple(new_pos1)]:\n            reward = -1\n        if self.current_step >= 25:\n            done = True\n\n        self.visited[tuple(new_pos1)] += 1\n        self.s = new_pos1\n        state = np.ravel_multi_index(self.s, self.shape)\n        if self.onehot_state:\n            state = np.eye(self.nS)[state]\n        return state, reward, done, {\"step\": self.current_step}\n    \n    def render(self, mode=\"human\"):\n        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n\n        for s in range(self.nS):\n            position = np.unravel_index(s, self.shape)\n            if tuple(self.s) == position:\n                output = \" x \"\n            # Print terminal state\n            elif position == self.terminate:\n                output = \" T \"\n            elif self._water[position]:\n                output = \" W \"\n            else:\n                output = \" o \"\n\n            if position[1] == 0:\n                output = output.lstrip()\n            if position[1] == self.shape[1] - 1:\n                output = output.rstrip()\n                output += \"\\n\"\n\n            outfile.write(output)\n        outfile.write(\"\\n\")\n\n        # No need to return anything for human\n        if mode != \"human\":\n            with closing(outfile):\n                return outfile.getvalue()\n        \n\n    def _limit_coordinates(self, coord):\n        \"\"\"\n        Prevent the agent from falling out of the grid world\n        :param coord:\n        :return:\n        \"\"\"\n        coord[0] = min(coord[0], self.shape[0] - 1)\n        coord[0] = max(coord[0], 0)\n        coord[1] = min(coord[1], self.shape[1] - 1)\n        coord[1] = max(coord[1], 0)\n        return coord", "])\n\nclass ToyEnv(Env):\n    def __init__(self, wind = True, onehot_state=True) -> None:\n        self.onehot_state = onehot_state\n        self.shape = (4, 4)\n        self.nS = np.prod(self.shape)\n        self.nA = 4\n        # self.observation_space = spaces.Discrete(self.nS)\n        self.observation_space = spaces.Box(low=0, high=1, shape=(16,)) if self.onehot_state else spaces.Discrete(self.nS)\n        self.action_space = spaces.Discrete(self.nA)\n\n        self.use_wind = wind\n        self._wind = np.ones(self.shape)\n        self._water = np.zeros(self.shape)\n        self._water[3, :] = 1\n\n\n        self.start = (2, 0)\n        self.terminate = (2, 3)\n        self.seed()\n    \n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n    \n    def reset(self):\n        self.visited = np.zeros(self.shape)\n        self.current_step = 0\n        self.s = np.array(self.start)\n        self.visited[tuple(self.s)] += 1\n        state = np.ravel_multi_index(self.s, self.shape)\n        if self.onehot_state:\n            state = np.eye(self.nS)[state]\n        return state\n    \n    def step(self, action):\n        self.current_step += 1\n        reward = 0\n        done = False\n\n        if self.use_wind and self._wind[tuple(self.s)]:\n            if self.np_random.random() >= 0.5:\n                action = self.np_random.choice([UP, RIGHT, DOWN, LEFT])\n        new_pos1 = self.s + DELTA[action]\n        new_pos1 = self._limit_coordinates(new_pos1).astype(int)\n\n        if tuple(new_pos1) == self.terminate:\n            reward = 1\n            done = True\n        elif self._water[tuple(new_pos1)]:\n            reward = -1\n        if self.current_step >= 25:\n            done = True\n\n        self.visited[tuple(new_pos1)] += 1\n        self.s = new_pos1\n        state = np.ravel_multi_index(self.s, self.shape)\n        if self.onehot_state:\n            state = np.eye(self.nS)[state]\n        return state, reward, done, {\"step\": self.current_step}\n    \n    def render(self, mode=\"human\"):\n        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n\n        for s in range(self.nS):\n            position = np.unravel_index(s, self.shape)\n            if tuple(self.s) == position:\n                output = \" x \"\n            # Print terminal state\n            elif position == self.terminate:\n                output = \" T \"\n            elif self._water[position]:\n                output = \" W \"\n            else:\n                output = \" o \"\n\n            if position[1] == 0:\n                output = output.lstrip()\n            if position[1] == self.shape[1] - 1:\n                output = output.rstrip()\n                output += \"\\n\"\n\n            outfile.write(output)\n        outfile.write(\"\\n\")\n\n        # No need to return anything for human\n        if mode != \"human\":\n            with closing(outfile):\n                return outfile.getvalue()\n        \n\n    def _limit_coordinates(self, coord):\n        \"\"\"\n        Prevent the agent from falling out of the grid world\n        :param coord:\n        :return:\n        \"\"\"\n        coord[0] = min(coord[0], self.shape[0] - 1)\n        coord[0] = max(coord[0], 0)\n        coord[1] = min(coord[1], self.shape[1] - 1)\n        coord[1] = max(coord[1], 0)\n        return coord", "\n\nif __name__ == \"__main__\":\n    env = ToyEnv()\n    env.seed(0)\n    env.action_space.seed(0)\n    state_freq = np.zeros(env.shape)\n    s = env.reset()\n    env.render()\n    pos = np.unravel_index(s, env.shape)\n    while True:\n        a = env.action_space.sample()\n        s,r,done,info = env.step(a)    \n        env.render()\n        pos = np.unravel_index(s, env.shape)\n        if done:\n            state_freq += env.visited\n            print(np.unravel_index(s, env.shape))\n            print(info)\n            break\n    print(state_freq)   "]}
{"filename": "envs/slimevolley/slimevolley_wrapper.py", "chunked_list": ["from .slimevolleygym import SlimeVolleyEnv\nimport numpy as np\n\n\nclass VolleyballEnv(SlimeVolleyEnv):\n    def __init__(self) -> None:\n        super().__init__()\n        self.from_pixels = False # super setting\n        self.atari_mode = True   # super setting\n        self.survival_reward = True\n        self.num_agents = 2\n        self.act_shape = (self.num_agents, 1)\n        self.obs_shape = (self.num_agents, *self.observation_space.shape)\n        self.done_shape = (self.num_agents, 1)\n        self.reward_shape = (self.num_agents, 1)\n        self.is_vector_env = True\n\n    def reset(self):\n        obs = super().reset()\n        return np.array([obs, obs], dtype=np.float32)\n    \n    def step(self, action: np.ndarray):\n        action = action.squeeze()\n        _obs, _reward, _done, info = super().step(action[0], action[1])\n        obs = np.array([_obs, info[\"otherObs\"]], dtype=np.float32)\n        done = np.array([[_done], [_done]], dtype=np.float32)\n        reward = np.array([[_reward], [-_reward]], dtype=np.float32)\n        if self.survival_reward:\n            reward += 0\n        if np.all(done):\n            info['score'] = (np.sign(info['ale.lives'] - info['ale.otherLives'])) / 2 + 0.5\n        return obs, reward, done, info"]}
{"filename": "envs/slimevolley/slimevolleygym.py", "chunked_list": ["\"\"\"\nCode from https://github.com/hardmaru/slimevolleygym\n\nPort of Neural Slime Volleyball to Python Gym Environment\n\nDavid Ha (2020)\n\nOriginal version:\n\nhttps://otoro.net/slimevolley", "\nhttps://otoro.net/slimevolley\nhttps://blog.otoro.net/2015/03/28/neural-slime-volleyball/\nhttps://github.com/hardmaru/neuralslimevolley\n\nNo dependencies apart from Numpy and Gym\n\"\"\"\n\nimport logging\nimport math", "import logging\nimport math\nimport gym\nfrom gym import spaces\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nimport numpy as np\nimport cv2 # installed with gym anyways\nfrom collections import deque\n", "from collections import deque\n\nnp.set_printoptions(threshold=20, precision=3, suppress=True, linewidth=200)\n\n# game settings:\n\nRENDER_MODE = True\n\nREF_W = 24*2\nREF_H = REF_W", "REF_W = 24*2\nREF_H = REF_W\nREF_U = 1.5 # ground height\nREF_WALL_WIDTH = 1.0 # wall width\nREF_WALL_HEIGHT = 3.5\nPLAYER_SPEED_X = 10*1.75\nPLAYER_SPEED_Y = 10*1.35\nMAX_BALL_SPEED = 15*1.5\nTIMESTEP = 1/30.\nNUDGE = 0.1", "TIMESTEP = 1/30.\nNUDGE = 0.1\nFRICTION = 1.0 # 1 means no FRICTION, less means FRICTION\nINIT_DELAY_FRAMES = 30\nGRAVITY = -9.8*2*1.5\n\nMAXLIVES = 5 # game ends when one agent loses this many games\n\nWINDOW_WIDTH = 1200\nWINDOW_HEIGHT = 500", "WINDOW_WIDTH = 1200\nWINDOW_HEIGHT = 500\n\nFACTOR = WINDOW_WIDTH / REF_W\n\n# if set to true, renders using cv2 directly on numpy array\n# (otherwise uses pyglet / opengl -> much smoother for human player)\nPIXEL_MODE = False \nPIXEL_SCALE = 4 # first render at multiple of Pixel Obs resolution, then downscale. Looks better.\n", "PIXEL_SCALE = 4 # first render at multiple of Pixel Obs resolution, then downscale. Looks better.\n\nPIXEL_WIDTH = 84*2*1\nPIXEL_HEIGHT = 84*1\n\ndef setNightColors():\n  ### night time color:\n  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n  BALL_COLOR = (217, 79, 0)\n  AGENT_LEFT_COLOR = (35, 93, 188)\n  AGENT_RIGHT_COLOR = (255, 236, 0)\n  PIXEL_AGENT_LEFT_COLOR = (255, 191, 0) # AMBER\n  PIXEL_AGENT_RIGHT_COLOR = (255, 191, 0) # AMBER\n  \n  BACKGROUND_COLOR = (11, 16, 19)\n  FENCE_COLOR = (102, 56, 35)\n  COIN_COLOR = FENCE_COLOR\n  GROUND_COLOR = (116, 114, 117)", "\ndef setDayColors():\n  ### day time color:\n  ### note: do not use day time colors for pixel-obs training.\n  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n  global PIXEL_SCALE, PIXEL_WIDTH, PIXEL_HEIGHT\n  PIXEL_SCALE = int(4*1.0)\n  PIXEL_WIDTH = int(84*2*1.0)\n  PIXEL_HEIGHT = int(84*1.0)\n  BALL_COLOR = (255, 200, 20)\n  AGENT_LEFT_COLOR = (240, 75, 0)\n  AGENT_RIGHT_COLOR = (0, 150, 255)\n  PIXEL_AGENT_LEFT_COLOR = (240, 75, 0)\n  PIXEL_AGENT_RIGHT_COLOR = (0, 150, 255)\n  \n  BACKGROUND_COLOR = (255, 255, 255)\n  FENCE_COLOR = (240, 210, 130)\n  COIN_COLOR = FENCE_COLOR\n  GROUND_COLOR = (128, 227, 153)", "\nsetNightColors()\n\n# by default, don't load rendering (since we want to use it in headless cloud machines)\nrendering = None\ndef checkRendering():\n  global rendering\n  if rendering is None:\n    from gym.envs.classic_control import rendering as rendering\n\ndef setPixelObsMode():\n  \"\"\"\n  used for experimental pixel-observation mode\n  note: new dim's chosen to be PIXEL_SCALE (2x) as Pixel Obs dims (will be downsampled)\n\n  also, both agent colors are identical, to potentially facilitate multiagent\n  \"\"\"\n  global WINDOW_WIDTH, WINDOW_HEIGHT, FACTOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR, PIXEL_MODE\n  PIXEL_MODE = True\n  WINDOW_WIDTH = PIXEL_WIDTH * PIXEL_SCALE\n  WINDOW_HEIGHT = PIXEL_HEIGHT * PIXEL_SCALE\n  FACTOR = WINDOW_WIDTH / REF_W\n  AGENT_LEFT_COLOR = PIXEL_AGENT_LEFT_COLOR\n  AGENT_RIGHT_COLOR = PIXEL_AGENT_RIGHT_COLOR", "\ndef setPixelObsMode():\n  \"\"\"\n  used for experimental pixel-observation mode\n  note: new dim's chosen to be PIXEL_SCALE (2x) as Pixel Obs dims (will be downsampled)\n\n  also, both agent colors are identical, to potentially facilitate multiagent\n  \"\"\"\n  global WINDOW_WIDTH, WINDOW_HEIGHT, FACTOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR, PIXEL_MODE\n  PIXEL_MODE = True\n  WINDOW_WIDTH = PIXEL_WIDTH * PIXEL_SCALE\n  WINDOW_HEIGHT = PIXEL_HEIGHT * PIXEL_SCALE\n  FACTOR = WINDOW_WIDTH / REF_W\n  AGENT_LEFT_COLOR = PIXEL_AGENT_LEFT_COLOR\n  AGENT_RIGHT_COLOR = PIXEL_AGENT_RIGHT_COLOR", "\ndef upsize_image(img):\n  return cv2.resize(img, (PIXEL_WIDTH * PIXEL_SCALE, PIXEL_HEIGHT * PIXEL_SCALE), interpolation=cv2.INTER_NEAREST)\ndef downsize_image(img):\n  return cv2.resize(img, (PIXEL_WIDTH, PIXEL_HEIGHT), interpolation=cv2.INTER_AREA)\n\n# conversion from space to pixels (allows us to render to diff resolutions)\ndef toX(x):\n  return (x+REF_W/2)*FACTOR\ndef toP(x):\n  return (x)*FACTOR", "def toP(x):\n  return (x)*FACTOR\ndef toY(y):\n  return y*FACTOR\n\nclass DelayScreen:\n  \"\"\" initially the ball is held still for INIT_DELAY_FRAMES(30) frames \"\"\"\n  def __init__(self, life=INIT_DELAY_FRAMES):\n    self.life = 0\n    self.reset(life)\n  def reset(self, life=INIT_DELAY_FRAMES):\n    self.life = life\n  def status(self):\n    if (self.life == 0):\n      return True\n    self.life -= 1\n    return False", "\ndef make_half_circle(radius=10, res=20, filled=True):\n  \"\"\" helper function for pyglet renderer\"\"\"\n  points = []\n  for i in range(res+1):\n    ang = math.pi-math.pi*i / res\n    points.append((math.cos(ang)*radius, math.sin(ang)*radius))\n  if filled:\n    return rendering.FilledPolygon(points)\n  else:\n    return rendering.PolyLine(points, True)", "\ndef _add_attrs(geom, color):\n  \"\"\" help scale the colors from 0-255 to 0.0-1.0 (pyglet renderer) \"\"\"\n  r = color[0]\n  g = color[1]\n  b = color[2]\n  geom.set_color(r/255., g/255., b/255.)\n\ndef create_canvas(canvas, c):\n  if PIXEL_MODE:\n    result = np.ones((WINDOW_HEIGHT, WINDOW_WIDTH, 3), dtype=np.uint8)\n    for channel in range(3):\n      result[:, :, channel] *= c[channel]\n    return result\n  else:\n    rect(canvas, 0, 0, WINDOW_WIDTH, -WINDOW_HEIGHT, color=BACKGROUND_COLOR)\n    return canvas", "def create_canvas(canvas, c):\n  if PIXEL_MODE:\n    result = np.ones((WINDOW_HEIGHT, WINDOW_WIDTH, 3), dtype=np.uint8)\n    for channel in range(3):\n      result[:, :, channel] *= c[channel]\n    return result\n  else:\n    rect(canvas, 0, 0, WINDOW_WIDTH, -WINDOW_HEIGHT, color=BACKGROUND_COLOR)\n    return canvas\n\ndef rect(canvas, x, y, width, height, color):\n  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n  if PIXEL_MODE:\n    canvas = cv2.rectangle(canvas, (round(x), round(WINDOW_HEIGHT-y)),\n      (round(x+width), round(WINDOW_HEIGHT-y+height)),\n      color, thickness=-1, lineType=cv2.LINE_AA)\n    return canvas\n  else:\n    box = rendering.make_polygon([(0,0), (0,-height), (width, -height), (width,0)])\n    trans = rendering.Transform()\n    trans.set_translation(x, y)\n    _add_attrs(box, color)\n    box.add_attr(trans)\n    canvas.add_onetime(box)\n    return canvas", "\ndef rect(canvas, x, y, width, height, color):\n  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n  if PIXEL_MODE:\n    canvas = cv2.rectangle(canvas, (round(x), round(WINDOW_HEIGHT-y)),\n      (round(x+width), round(WINDOW_HEIGHT-y+height)),\n      color, thickness=-1, lineType=cv2.LINE_AA)\n    return canvas\n  else:\n    box = rendering.make_polygon([(0,0), (0,-height), (width, -height), (width,0)])\n    trans = rendering.Transform()\n    trans.set_translation(x, y)\n    _add_attrs(box, color)\n    box.add_attr(trans)\n    canvas.add_onetime(box)\n    return canvas", "\ndef half_circle(canvas, x, y, r, color):\n  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n  if PIXEL_MODE:\n    return cv2.ellipse(canvas, (round(x), WINDOW_HEIGHT-round(y)),\n      (round(r), round(r)), 0, 0, -180, color, thickness=-1, lineType=cv2.LINE_AA)\n  else:\n    geom = make_half_circle(r)\n    trans = rendering.Transform()\n    trans.set_translation(x, y)\n    _add_attrs(geom, color)\n    geom.add_attr(trans)\n    canvas.add_onetime(geom)\n    return canvas", "\ndef circle(canvas, x, y, r, color):\n  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n  if PIXEL_MODE:\n    return cv2.circle(canvas, (round(x), round(WINDOW_HEIGHT-y)), round(r),\n      color, thickness=-1, lineType=cv2.LINE_AA)\n  else:\n    geom = rendering.make_circle(r, res=40)\n    trans = rendering.Transform()\n    trans.set_translation(x, y)\n    _add_attrs(geom, color)\n    geom.add_attr(trans)\n    canvas.add_onetime(geom)\n    return canvas", "\nclass Particle:\n  \"\"\" used for the ball, and also for the round stub above the fence \"\"\"\n  def __init__(self, x, y, vx, vy, r, c):\n    self.x = x\n    self.y = y\n    self.prev_x = self.x\n    self.prev_y = self.y\n    self.vx = vx\n    self.vy = vy\n    self.r = r\n    self.c = c\n  def display(self, canvas):\n    return circle(canvas, toX(self.x), toY(self.y), toP(self.r), color=self.c)\n  def move(self):\n    self.prev_x = self.x\n    self.prev_y = self.y\n    self.x += self.vx * TIMESTEP\n    self.y += self.vy * TIMESTEP\n  def applyAcceleration(self, ax, ay):\n    self.vx += ax * TIMESTEP\n    self.vy += ay * TIMESTEP\n  def checkEdges(self):\n    if (self.x<=(self.r-REF_W/2)):\n      self.vx *= -FRICTION\n      self.x = self.r-REF_W/2+NUDGE*TIMESTEP\n\n    if (self.x >= (REF_W/2-self.r)):\n      self.vx *= -FRICTION;\n      self.x = REF_W/2-self.r-NUDGE*TIMESTEP\n\n    if (self.y<=(self.r+REF_U)):\n      self.vy *= -FRICTION\n      self.y = self.r+REF_U+NUDGE*TIMESTEP\n      if (self.x <= 0):\n        return -1\n      else:\n        return 1\n    if (self.y >= (REF_H-self.r)):\n      self.vy *= -FRICTION\n      self.y = REF_H-self.r-NUDGE*TIMESTEP\n    # fence:\n    if ((self.x <= (REF_WALL_WIDTH/2+self.r)) and (self.prev_x > (REF_WALL_WIDTH/2+self.r)) and (self.y <= REF_WALL_HEIGHT)):\n      self.vx *= -FRICTION\n      self.x = REF_WALL_WIDTH/2+self.r+NUDGE*TIMESTEP\n\n    if ((self.x >= (-REF_WALL_WIDTH/2-self.r)) and (self.prev_x < (-REF_WALL_WIDTH/2-self.r)) and (self.y <= REF_WALL_HEIGHT)):\n      self.vx *= -FRICTION\n      self.x = -REF_WALL_WIDTH/2-self.r-NUDGE*TIMESTEP\n    return 0;\n  def getDist2(self, p): # returns distance squared from p\n    dy = p.y - self.y\n    dx = p.x - self.x\n    return (dx*dx+dy*dy)\n  def isColliding(self, p): # returns true if it is colliding w/ p\n    r = self.r+p.r\n    return (r*r > self.getDist2(p)) # if distance is less than total radius, then colliding.\n  def bounce(self, p): # bounce two balls that have collided (this and that)\n    abx = self.x-p.x\n    aby = self.y-p.y\n    abd = math.sqrt(abx*abx+aby*aby)\n    abx /= abd # normalize\n    aby /= abd\n    nx = abx # reuse calculation\n    ny = aby\n    abx *= NUDGE\n    aby *= NUDGE\n    while(self.isColliding(p)):\n      self.x += abx\n      self.y += aby\n    ux = self.vx - p.vx\n    uy = self.vy - p.vy\n    un = ux*nx + uy*ny\n    unx = nx*(un*2.) # added factor of 2\n    uny = ny*(un*2.) # added factor of 2\n    ux -= unx\n    uy -= uny\n    self.vx = ux + p.vx\n    self.vy = uy + p.vy\n  def limitSpeed(self, minSpeed, maxSpeed):\n    mag2 = self.vx*self.vx+self.vy*self.vy;\n    if (mag2 > (maxSpeed*maxSpeed) ):\n      mag = math.sqrt(mag2)\n      self.vx /= mag\n      self.vy /= mag\n      self.vx *= maxSpeed\n      self.vy *= maxSpeed\n\n    if (mag2 < (minSpeed*minSpeed) ):\n      mag = math.sqrt(mag2)\n      self.vx /= mag\n      self.vy /= mag\n      self.vx *= minSpeed\n      self.vy *= minSpeed", "\nclass Wall:\n  \"\"\" used for the fence, and also the ground \"\"\"\n  def __init__(self, x, y, w, h, c):\n    self.x = x;\n    self.y = y;\n    self.w = w;\n    self.h = h;\n    self.c = c\n  def display(self, canvas):\n    return rect(canvas, toX(self.x-self.w/2), toY(self.y+self.h/2), toP(self.w), toP(self.h), color=self.c)", "\nclass RelativeState:\n  \"\"\"\n  keeps track of the obs.\n  Note: the observation is from the perspective of the agent.\n  an agent playing either side of the fence must see obs the same way\n  \"\"\"\n  def __init__(self):\n    # agent\n    self.x = 0\n    self.y = 0\n    self.vx = 0\n    self.vy = 0\n    # ball\n    self.bx = 0\n    self.by = 0\n    self.bvx = 0\n    self.bvy = 0\n    # opponent\n    self.ox = 0\n    self.oy = 0\n    self.ovx = 0\n    self.ovy = 0\n  def getObservation(self):\n    result = [self.x, self.y, self.vx, self.vy,\n              self.bx, self.by, self.bvx, self.bvy,\n              self.ox, self.oy, self.ovx, self.ovy]\n    scaleFactor = 10.0  # scale inputs to be in the order of magnitude of 10 for neural network.\n    result = np.array(result) / scaleFactor\n    return result", "\nclass Agent:\n  \"\"\" keeps track of the agent in the game. note this is not the policy network \"\"\"\n  def __init__(self, dir, x, y, c):\n    self.dir = dir # -1 means left, 1 means right player for symmetry.\n    self.x = x\n    self.y = y\n    self.r = 1.5\n    self.c = c\n    self.vx = 0\n    self.vy = 0\n    self.desired_vx = 0\n    self.desired_vy = 0\n    self.state = RelativeState()\n    self.emotion = \"happy\"; # hehe...\n    self.life = MAXLIVES\n  def lives(self):\n    return self.life\n  def setAction(self, action):\n    forward = False\n    backward = False\n    jump = False\n    if action[0] > 0:\n      forward = True\n    if action[1] > 0:\n      backward = True\n    if action[2] > 0:\n      jump = True\n    self.desired_vx = 0\n    self.desired_vy = 0\n    if (forward and (not backward)):\n      self.desired_vx = -PLAYER_SPEED_X\n    if (backward and (not forward)):\n      self.desired_vx = PLAYER_SPEED_X\n    if jump:\n      self.desired_vy = PLAYER_SPEED_Y\n  def move(self):\n    self.x += self.vx * TIMESTEP\n    self.y += self.vy * TIMESTEP\n  def step(self):\n    self.x += self.vx * TIMESTEP\n    self.y += self.vy * TIMESTEP\n  def update(self):\n    self.vy += GRAVITY * TIMESTEP\n\n    if (self.y <= REF_U + NUDGE*TIMESTEP):\n      self.vy = self.desired_vy\n\n    self.vx = self.desired_vx*self.dir\n\n    self.move()\n\n    if (self.y <= REF_U):\n      self.y = REF_U;\n      self.vy = 0;\n\n    # stay in their own half:\n    if (self.x*self.dir <= (REF_WALL_WIDTH/2+self.r) ):\n      self.vx = 0;\n      self.x = self.dir*(REF_WALL_WIDTH/2+self.r)\n\n    if (self.x*self.dir >= (REF_W/2-self.r) ):\n      self.vx = 0;\n      self.x = self.dir*(REF_W/2-self.r)\n  def updateState(self, ball, opponent):\n    \"\"\" normalized to side, appears different for each agent's perspective\"\"\"\n    # agent's self\n    self.state.x = self.x*self.dir\n    self.state.y = self.y\n    self.state.vx = self.vx*self.dir\n    self.state.vy = self.vy\n    # ball\n    self.state.bx = ball.x*self.dir\n    self.state.by = ball.y\n    self.state.bvx = ball.vx*self.dir\n    self.state.bvy = ball.vy\n    # opponent\n    self.state.ox = opponent.x*(-self.dir)\n    self.state.oy = opponent.y\n    self.state.ovx = opponent.vx*(-self.dir)\n    self.state.ovy = opponent.vy\n  def getObservation(self):\n    return self.state.getObservation()\n\n  def display(self, canvas, bx, by):\n    x = self.x\n    y = self.y\n    r = self.r\n\n    angle = math.pi * 60 / 180\n    if self.dir == 1:\n      angle = math.pi * 120 / 180\n    eyeX = 0\n    eyeY = 0\n\n    canvas = half_circle(canvas, toX(x), toY(y), toP(r), color=self.c)\n\n    # track ball with eyes (replace with observed info later):\n    c = math.cos(angle)\n    s = math.sin(angle)\n    ballX = bx-(x+(0.6)*r*c);\n    ballY = by-(y+(0.6)*r*s);\n\n    if (self.emotion == \"sad\"):\n      ballX = -self.dir\n      ballY = -3\n\n    dist = math.sqrt(ballX*ballX+ballY*ballY)\n    eyeX = ballX/dist\n    eyeY = ballY/dist\n\n    canvas = circle(canvas, toX(x+(0.6)*r*c), toY(y+(0.6)*r*s), toP(r)*0.3, color=(255, 255, 255))\n    canvas = circle(canvas, toX(x+(0.6)*r*c+eyeX*0.15*r), toY(y+(0.6)*r*s+eyeY*0.15*r), toP(r)*0.1, color=(0, 0, 0))\n\n    # draw coins (lives) left\n    for i in range(1, self.life):\n      canvas = circle(canvas, toX(self.dir*(REF_W/2+0.5-i*2.)), WINDOW_HEIGHT-toY(1.5), toP(0.5), color=COIN_COLOR)\n\n    return canvas", "\nclass BaselinePolicy:\n  \"\"\" Tiny RNN policy with only 120 parameters of otoro.net/slimevolley agent \"\"\"\n  def __init__(self):\n    self.nGameInput = 8 # 8 states for agent\n    self.nGameOutput = 3 # 3 buttons (forward, backward, jump)\n    self.nRecurrentState = 4 # extra recurrent states for feedback.\n\n    self.nOutput = self.nGameOutput+self.nRecurrentState\n    self.nInput = self.nGameInput+self.nOutput\n    \n    # store current inputs and outputs\n    self.inputState = np.zeros(self.nInput)\n    self.outputState = np.zeros(self.nOutput)\n    self.prevOutputState = np.zeros(self.nOutput)\n\n    \"\"\"See training details: https://blog.otoro.net/2015/03/28/neural-slime-volleyball/ \"\"\"\n    self.weight = np.array(\n      [7.5719, 4.4285, 2.2716, -0.3598, -7.8189, -2.5422, -3.2034, 0.3935, 1.2202, -0.49, -0.0316, 0.5221, 0.7026, 0.4179, -2.1689,\n       1.646, -13.3639, 1.5151, 1.1175, -5.3561, 5.0442, 0.8451, 0.3987, -2.9501, -3.7811, -5.8994, 6.4167, 2.5014, 7.338, -2.9887,\n       2.4586, 13.4191, 2.7395, -3.9708, 1.6548, -2.7554, -1.5345, -6.4708, 9.2426, -0.7392, 0.4452, 1.8828, -2.6277, -10.851, -3.2353,\n       -4.4653, -3.1153, -1.3707, 7.318, 16.0902, 1.4686, 7.0391, 1.7765, -1.155, 2.6697, -8.8877, 1.1958, -3.2839, -5.4425, 1.6809,\n       7.6812, -2.4732, 1.738, 0.3781, 0.8718, 2.5886, 1.6911, 1.2953, -9.0052, -4.6038, -6.7447, -2.5528, 0.4391, -4.9278, -3.6695,\n       -4.8673, -1.6035, 1.5011, -5.6124, 4.9747, 1.8998, 3.0359, 6.2983, -4.8568, -2.1888, -4.1143, -3.9874, -0.0459, 4.7134, 2.8952,\n       -9.3627, -4.685, 0.3601, -1.3699, 9.7294, 11.5596, 0.1918, 3.0783, 0.0329, -0.1362, -0.1188, -0.7579, 0.3278, -0.977, -0.9377])\n\n    self.bias = np.array([2.2935,-2.0353,-1.7786,5.4567,-3.6368,3.4996,-0.0685])\n\n    # unflatten weight, convert it into 7x15 matrix.\n    self.weight = self.weight.reshape(self.nGameOutput+self.nRecurrentState,\n      self.nGameInput+self.nGameOutput+self.nRecurrentState)\n  def reset(self):\n    self.inputState = np.zeros(self.nInput)\n    self.outputState = np.zeros(self.nOutput)\n    self.prevOutputState = np.zeros(self.nOutput)\n  def _forward(self):\n    self.prevOutputState = self.outputState\n    self.outputState = np.tanh(np.dot(self.weight, self.inputState)+self.bias)\n  def _setInputState(self, obs):\n    # obs is: (op is opponent). obs is also from perspective of the agent (x values negated for other agent)\n    [x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy, op_x, op_y, op_vx, op_vy] = obs\n    self.inputState[0:self.nGameInput] = np.array([x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy])\n    self.inputState[self.nGameInput:] = self.outputState\n  def _getAction(self):\n    forward = 0\n    backward = 0\n    jump = 0\n    if (self.outputState[0] > 0.75):\n      forward = 1\n    if (self.outputState[1] > 0.75):\n      backward = 1\n    if (self.outputState[2] > 0.75):\n      jump = 1\n    return [forward, backward, jump]\n  def predict(self, obs):\n    \"\"\" take obs, update rnn state, return action \"\"\"\n    self._setInputState(obs)\n    self._forward()\n    return self._getAction()", "\nclass Game:\n  \"\"\"\n  the main slime volley game.\n  can be used in various settings, such as ai vs ai, ai vs human, human vs human\n  \"\"\"\n  def __init__(self, np_random=np.random):\n    self.ball = None\n    self.ground = None\n    self.fence = None\n    self.fenceStub = None\n    self.agent_left = None\n    self.agent_right = None\n    self.delayScreen = None\n    self.np_random = np_random\n    self.reset()\n  def reset(self):\n    self.ground = Wall(0, 0.75, REF_W, REF_U, c=GROUND_COLOR)\n    self.fence = Wall(0, 0.75 + REF_WALL_HEIGHT/2, REF_WALL_WIDTH, (REF_WALL_HEIGHT-1.5), c=FENCE_COLOR)\n    self.fenceStub = Particle(0, REF_WALL_HEIGHT, 0, 0, REF_WALL_WIDTH/2, c=FENCE_COLOR);\n    ball_vx = self.np_random.uniform(low=-20, high=20)\n    ball_vy = self.np_random.uniform(low=10, high=25)\n    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n    self.agent_left = Agent(-1, -REF_W/4, 1.5, c=AGENT_LEFT_COLOR)\n    self.agent_right = Agent(1, REF_W/4, 1.5, c=AGENT_RIGHT_COLOR)\n    self.agent_left.updateState(self.ball, self.agent_right)\n    self.agent_right.updateState(self.ball, self.agent_left)\n    self.delayScreen = DelayScreen()\n  def newMatch(self):\n    ball_vx = self.np_random.uniform(low=-20, high=20)\n    ball_vy = self.np_random.uniform(low=10, high=25)\n    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n    self.delayScreen.reset()\n  def step(self):\n    \"\"\" main game loop \"\"\"\n\n    self.betweenGameControl()\n    self.agent_left.update()\n    self.agent_right.update()\n\n    if self.delayScreen.status():\n      self.ball.applyAcceleration(0, GRAVITY)\n      self.ball.limitSpeed(0, MAX_BALL_SPEED)\n      self.ball.move()\n\n    if (self.ball.isColliding(self.agent_left)):\n      self.ball.bounce(self.agent_left)\n    if (self.ball.isColliding(self.agent_right)):\n      self.ball.bounce(self.agent_right)\n    if (self.ball.isColliding(self.fenceStub)):\n      self.ball.bounce(self.fenceStub)\n\n    # negated, since we want reward to be from the persepctive of right agent being trained.\n    result = -self.ball.checkEdges()\n\n    if (result != 0):\n      self.newMatch() # not reset, but after a point is scored\n      if result < 0: # baseline agent won\n        self.agent_left.emotion = \"happy\"\n        self.agent_right.emotion = \"sad\"\n        self.agent_right.life -= 1\n      else:\n        self.agent_left.emotion = \"sad\"\n        self.agent_right.emotion = \"happy\"\n        self.agent_left.life -= 1\n      return result\n\n    # update internal states (the last thing to do)\n    self.agent_left.updateState(self.ball, self.agent_right)\n    self.agent_right.updateState(self.ball, self.agent_left)\n\n    return result\n  def display(self, canvas):\n    # background color\n    # if PIXEL_MODE is True, canvas is an RGB array.\n    # if PIXEL_MODE is False, canvas is viewer object\n    canvas = create_canvas(canvas, c=BACKGROUND_COLOR)\n    canvas = self.fence.display(canvas)\n    canvas = self.fenceStub.display(canvas)\n    canvas = self.agent_left.display(canvas, self.ball.x, self.ball.y)\n    canvas = self.agent_right.display(canvas, self.ball.x, self.ball.y)\n    canvas = self.ball.display(canvas)\n    canvas = self.ground.display(canvas)\n    return canvas\n  def betweenGameControl(self):\n    agent = [self.agent_left, self.agent_right]\n    if (self.delayScreen.life > 0):\n      pass\n      '''\n      for i in range(2):\n        if (agent[i].emotion == \"sad\"):\n          agent[i].setAction([0, 0, 0]) # nothing\n      '''\n    else:\n      agent[0].emotion = \"happy\"\n      agent[1].emotion = \"happy\"", "\nclass SlimeVolleyEnv(gym.Env):\n  \"\"\"\n  Gym wrapper for Slime Volley game.\n\n  By default, the agent you are training controls the right agent\n  on the right. The agent on the left is controlled by the baseline\n  RNN policy.\n\n  Game ends when an agent loses 5 matches (or at t=3000 timesteps).\n\n  Note: Optional mode for MARL experiments, like self-play which\n  deviates from Gym env. Can be enabled via supplying optional action\n  to override the default baseline agent's policy:\n\n  obs1, reward, done, info = env.step(action1, action2)\n\n  the next obs for the right agent is returned in the optional\n  fourth item from the step() method.\n\n  reward is in the perspective of the right agent so the reward\n  for the left agent is the negative of this number.\n  \"\"\"\n  metadata = {\n    'render.modes': ['human', 'rgb_array', 'state'],\n    'video.frames_per_second' : 50\n  }\n\n  # for compatibility with typical atari wrappers\n  atari_action_meaning = {\n    0: \"NOOP\",\n    1: \"FIRE\",\n    2: \"UP\",\n    3: \"RIGHT\",\n    4: \"LEFT\",\n    5: \"DOWN\",\n    6: \"UPRIGHT\",\n    7: \"UPLEFT\",\n    8: \"DOWNRIGHT\",\n    9: \"DOWNLEFT\",\n    10: \"UPFIRE\",\n    11: \"RIGHTFIRE\",\n    12: \"LEFTFIRE\",\n    13: \"DOWNFIRE\",\n    14: \"UPRIGHTFIRE\",\n    15: \"UPLEFTFIRE\",\n    16: \"DOWNRIGHTFIRE\",\n    17: \"DOWNLEFTFIRE\",\n  }\n  atari_action_set = {\n    0, # NOOP\n    4, # LEFT\n    7, # UPLEFT\n    2, # UP\n    6, # UPRIGHT\n    3, # RIGHT\n  }\n\n  action_table = [[0, 0, 0], # NOOP\n                  [1, 0, 0], # LEFT (forward)\n                  [1, 0, 1], # UPLEFT (forward jump)\n                  [0, 0, 1], # UP (jump)\n                  [0, 1, 1], # UPRIGHT (backward jump)\n                  [0, 1, 0]] # RIGHT (backward)\n\n  from_pixels = False\n  atari_mode = True\n  survival_bonus = False # Depreciated: augment reward, easier to train\n  multiagent = True # optional args anyways\n\n  def __init__(self):\n    \"\"\"\n    Reward modes:\n\n    net score = right agent wins minus left agent wins\n\n    0: returns net score (basic reward)\n    1: returns 0.01 x number of timesteps (max 3000) (survival reward)\n    2: sum of basic reward and survival reward\n\n    0 is suitable for evaluation, while 1 and 2 may be good for training\n\n    Setting multiagent to True puts in info (4th thing returned in stop)\n    the otherObs, the observation for the other agent. See multiagent.py\n\n    Setting self.from_pixels to True makes the observation with multiples\n    of 84, since usual atari wrappers downsample to 84x84\n    \"\"\"\n\n    self.t = 0\n    self.t_limit = 3000\n\n    #self.action_space = spaces.Box(0, 1.0, shape=(3,))\n    if self.atari_mode:\n      self.action_space = spaces.Discrete(6)\n    else:\n      self.action_space = spaces.MultiBinary(3)\n\n    if self.from_pixels:\n      setPixelObsMode()\n      self.observation_space = spaces.Box(low=0, high=255,\n        shape=(PIXEL_HEIGHT, PIXEL_WIDTH, 3), dtype=np.uint8)\n    else:\n      high = np.array([np.finfo(np.float32).max] * 12)\n      self.observation_space = spaces.Box(-high, high)\n    self.canvas = None\n    self.previous_rgbarray = None\n\n    self.game = Game()\n    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n\n    self.policy = BaselinePolicy() # the \u201cbad guy\u201d\n\n    self.viewer = None\n\n    # another avenue to override the built-in AI's action, going past many env wraps:\n    self.otherAction = None\n\n  def seed(self, seed=None):\n    self.np_random, seed = seeding.np_random(seed)\n    self.game = Game(np_random=self.np_random)\n    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n    return [seed]\n\n  def getObs(self):\n    if self.from_pixels:\n      obs = self.render(mode='state')\n      self.canvas = obs\n    else:\n      obs = self.game.agent_right.getObservation()\n    return obs\n\n  def discreteToBox(self, n):\n    # convert discrete action n into the actual triplet action\n    if isinstance(n, (list, tuple, np.ndarray)): # original input for some reason, just leave it:\n      if len(n) == 3:\n        return n\n    assert (int(n) == n) and (n >= 0) and (n < 6)\n    return self.action_table[n]\n\n  def step(self, action, otherAction=None):\n    \"\"\"\n    baseAction is only used if multiagent mode is True\n    note: although the action space is multi-binary, float vectors\n    are fine (refer to setAction() to see how they get interpreted)\n    \"\"\"\n    done = False\n    self.t += 1\n\n    if self.otherAction is not None:\n      otherAction = self.otherAction\n      \n    if otherAction is None: # override baseline policy\n      obs = self.game.agent_left.getObservation()\n      otherAction = self.policy.predict(obs)\n\n    if self.atari_mode:\n      action = self.discreteToBox(action)\n      otherAction = self.discreteToBox(otherAction)\n\n    self.game.agent_left.setAction(otherAction)\n    self.game.agent_right.setAction(action) # external agent is agent_right\n\n    reward = self.game.step()\n\n    obs = self.getObs()\n\n    if self.t >= self.t_limit:\n      done = True\n\n    if self.game.agent_left.life <= 0 or self.game.agent_right.life <= 0:\n      done = True\n\n    otherObs = None\n    if self.multiagent:\n      if self.from_pixels:\n        otherObs = cv2.flip(obs, 1) # horizontal flip\n      else:\n        otherObs = self.game.agent_left.getObservation()\n\n    info = {\n      'ale.lives': self.game.agent_right.lives(),\n      'ale.otherLives': self.game.agent_left.lives(),\n      'otherObs': otherObs,\n      'state': self.game.agent_right.getObservation(),\n      'otherState': self.game.agent_left.getObservation(),\n    }\n\n    if self.survival_bonus:\n      return obs, reward+0.01, done, info\n    return obs, reward, done, info\n\n  def init_game_state(self):\n    self.t = 0\n    self.game.reset()\n\n  def reset(self):\n    self.init_game_state()\n    return self.getObs()\n\n  def checkViewer(self):\n    # for opengl viewer\n    if self.viewer is None:\n      checkRendering()\n      self.viewer = rendering.SimpleImageViewer(maxwidth=2160) # macbook pro resolution\n\n  def render(self, mode='human', close=False):\n\n    if PIXEL_MODE:\n      if self.canvas is not None: # already rendered\n        rgb_array = self.canvas\n        self.canvas = None\n        if mode == 'rgb_array' or mode == 'human':\n          self.checkViewer()\n          larger_canvas = upsize_image(rgb_array)\n          self.viewer.imshow(larger_canvas)\n          if (mode=='rgb_array'):\n            return larger_canvas\n          else:\n            return\n\n      self.canvas = self.game.display(self.canvas)\n      # scale down to original res (looks better than rendering directly to lower res)\n      self.canvas = downsize_image(self.canvas)\n\n      if mode=='state':\n        return np.copy(self.canvas)\n\n      # upsampling w/ nearest interp method gives a retro \"pixel\" effect look\n      larger_canvas = upsize_image(self.canvas)\n      self.checkViewer()\n      self.viewer.imshow(larger_canvas)\n      if (mode=='rgb_array'):\n        return larger_canvas\n\n    else: # pyglet renderer\n      if self.viewer is None:\n        checkRendering()\n        self.viewer = rendering.Viewer(WINDOW_WIDTH, WINDOW_HEIGHT)\n\n      self.game.display(self.viewer)\n      return self.viewer.render(return_rgb_array = mode=='rgb_array')\n\n  def close(self):\n    if self.viewer:\n      self.viewer.close()\n    \n  def get_action_meanings(self):\n    return [self.atari_action_meaning[i] for i in self.atari_action_set]", "\nclass SlimeVolleyPixelEnv(SlimeVolleyEnv):\n  from_pixels = True\n\nclass SlimeVolleyAtariEnv(SlimeVolleyEnv):\n  from_pixels = True\n  atari_mode = True\n\nclass SlimeVolleySurvivalAtariEnv(SlimeVolleyEnv):\n  from_pixels = True\n  atari_mode = True\n  survival_bonus = True", "class SlimeVolleySurvivalAtariEnv(SlimeVolleyEnv):\n  from_pixels = True\n  atari_mode = True\n  survival_bonus = True\n\nclass SurvivalRewardEnv(gym.RewardWrapper):\n  def __init__(self, env):\n    \"\"\"\n    adds 0.01 to the reward for every timestep agent survives\n\n    :param env: (Gym Environment) the environment\n    \"\"\"\n    gym.RewardWrapper.__init__(self, env)\n\n  def reward(self, reward):\n    \"\"\"\n    adds that extra survival bonus for living a bit longer!\n\n    :param reward: (float)\n    \"\"\"\n    return reward + 0.01", "\nclass FrameStack(gym.Wrapper):\n  def __init__(self, env, n_frames):\n    \"\"\"Stack n_frames last frames.\n\n    (don't use lazy frames)\n    modified from:\n    stable_baselines.common.atari_wrappers\n\n    :param env: (Gym Environment) the environment\n    :param n_frames: (int) the number of frames to stack\n    \"\"\"\n    gym.Wrapper.__init__(self, env)\n    self.n_frames = n_frames\n    self.frames = deque([], maxlen=n_frames)\n    shp = env.observation_space.shape\n    self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * n_frames),\n                                        dtype=env.observation_space.dtype)\n\n  def reset(self):\n    obs = self.env.reset()\n    for _ in range(self.n_frames):\n        self.frames.append(obs)\n    return self._get_ob()\n\n  def step(self, action):\n    obs, reward, done, info = self.env.step(action)\n    self.frames.append(obs)\n    return self._get_ob(), reward, done, info\n\n  def _get_ob(self):\n    assert len(self.frames) == self.n_frames\n    return np.concatenate(list(self.frames), axis=2)", "\n#####################\n# helper functions: #\n#####################\n\ndef multiagent_rollout(env, policy_right, policy_left, render_mode=False):\n  \"\"\"\n  play one agent vs the other in modified gym-style loop.\n  important: returns the score from perspective of policy_right.\n  \"\"\"\n  obs_right = env.reset()\n  obs_left = obs_right # same observation at the very beginning for the other agent\n\n  done = False\n  total_reward = 0\n  t = 0\n\n  while not done:\n\n    action_right = policy_right.predict(obs_right)\n    action_left = policy_left.predict(obs_left)\n\n    # uses a 2nd (optional) parameter for step to put in the other action\n    # and returns the other observation in the 4th optional \"info\" param in gym's step()\n    obs_right, reward, done, info = env.step(action_right, action_left)\n    obs_left = info['otherObs']\n\n    total_reward += reward\n    t += 1\n\n    if render_mode:\n      env.render()\n\n  return total_reward, t", "\ndef render_atari(obs):\n  \"\"\"\n  Helper function that takes in a processed obs (84,84,4)\n  Useful for visualizing what an Atari agent actually *sees*\n  Outputs in Atari visual format (Top: resized to orig dimensions, buttom: 4 frames)\n  \"\"\"\n  tempObs = []\n  obs = np.copy(obs)\n  for i in range(4):\n    if i == 3:\n      latest = np.copy(obs[:, :, i])\n    if i > 0: # insert vertical lines\n      obs[:, 0, i] = 141\n    tempObs.append(obs[:, :, i])\n  latest = np.expand_dims(latest, axis=2)\n  latest = np.concatenate([latest*255.0] * 3, axis=2).astype(np.uint8)\n  latest = cv2.resize(latest, (84 * 8, 84 * 4), interpolation=cv2.INTER_NEAREST)\n  tempObs = np.concatenate(tempObs, axis=1)\n  tempObs = np.expand_dims(tempObs, axis=2)\n  tempObs = np.concatenate([tempObs*255.0] * 3, axis=2).astype(np.uint8)\n  tempObs = cv2.resize(tempObs, (84 * 8, 84 * 2), interpolation=cv2.INTER_NEAREST)\n  return np.concatenate([latest, tempObs], axis=0)", "\n####################\n# Reg envs for gym #\n####################\n\n# register(\n#     id='SlimeVolley-v0',\n#     entry_point='slimevolleygym.slimevolley:SlimeVolleyEnv'\n# )\n", "# )\n\n# register(\n#     id='SlimeVolleyPixel-v0',\n#     entry_point='slimevolleygym.slimevolley:SlimeVolleyPixelEnv'\n# )\n\n# register(\n#     id='SlimeVolleyNoFrameskip-v0',\n#     entry_point='slimevolleygym.slimevolley:SlimeVolleyAtariEnv'", "#     id='SlimeVolleyNoFrameskip-v0',\n#     entry_point='slimevolleygym.slimevolley:SlimeVolleyAtariEnv'\n# )\n\n# register(\n#     id='SlimeVolleySurvivalNoFrameskip-v0',\n#     entry_point='slimevolleygym.slimevolley:SlimeVolleySurvivalAtariEnv'\n# )\n\nif __name__==\"__main__\":\n  \"\"\"\n  Example of how to use Gym env, in single or multiplayer setting\n\n  Humans can override controls:\n\n  left Agent:\n  W - Jump\n  A - Left\n  D - Right\n\n  right Agent:\n  Up Arrow, Left Arrow, Right Arrow\n  \"\"\"\n\n  if RENDER_MODE:\n    from pyglet.window import key\n    from time import sleep\n\n  manualAction = [0, 0, 0] # forward, backward, jump\n  otherManualAction = [0, 0, 0]\n  manualMode = False\n  otherManualMode = False\n\n  # taken from https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py\n  def key_press(k, mod):\n    global manualMode, manualAction, otherManualMode, otherManualAction\n    if k == key.LEFT:  manualAction[0] = 1\n    if k == key.RIGHT: manualAction[1] = 1\n    if k == key.UP:    manualAction[2] = 1\n    if (k == key.LEFT or k == key.RIGHT or k == key.UP): manualMode = True\n\n    if k == key.D:     otherManualAction[0] = 1\n    if k == key.A:     otherManualAction[1] = 1\n    if k == key.W:     otherManualAction[2] = 1\n    if (k == key.D or k == key.A or k == key.W): otherManualMode = True\n\n  def key_release(k, mod):\n    global manualMode, manualAction, otherManualMode, otherManualAction\n    if k == key.LEFT:  manualAction[0] = 0\n    if k == key.RIGHT: manualAction[1] = 0\n    if k == key.UP:    manualAction[2] = 0\n    if k == key.D:     otherManualAction[0] = 0\n    if k == key.A:     otherManualAction[1] = 0\n    if k == key.W:     otherManualAction[2] = 0\n\n  policy = BaselinePolicy() # defaults to use RNN Baseline for player\n\n  env = SlimeVolleyEnv()\n  env.seed(np.random.randint(0, 10000))\n  #env.seed(721)\n\n  if RENDER_MODE:\n    env.render()\n    env.viewer.window.on_key_press = key_press\n    env.viewer.window.on_key_release = key_release\n\n  obs = env.reset()\n\n  steps = 0\n  total_reward = 0\n  action = np.array([0, 0, 0])\n\n  done = False\n\n  while not done:\n\n    if manualMode: # override with keyboard\n      action = manualAction\n    else:\n      action = policy.predict(obs)\n\n    if otherManualMode:\n      otherAction = otherManualAction\n      obs, reward, done, _ = env.step(action, otherAction)\n    else:\n      obs, reward, done, _ = env.step(action)\n\n    if reward > 0 or reward < 0:\n      print(\"reward\", reward)\n      manualMode = False\n      otherManualMode = False\n\n    total_reward += reward\n\n    if RENDER_MODE:\n      env.render()\n      sleep(0.01)\n\n    # make the game go slower for human players to be fair to humans.\n    if (manualMode or otherManualMode):\n      if PIXEL_MODE:\n        sleep(0.01)\n      else:\n        sleep(0.02)\n\n  env.close()\n  print(\"cumulative score\", total_reward)", "\nif __name__==\"__main__\":\n  \"\"\"\n  Example of how to use Gym env, in single or multiplayer setting\n\n  Humans can override controls:\n\n  left Agent:\n  W - Jump\n  A - Left\n  D - Right\n\n  right Agent:\n  Up Arrow, Left Arrow, Right Arrow\n  \"\"\"\n\n  if RENDER_MODE:\n    from pyglet.window import key\n    from time import sleep\n\n  manualAction = [0, 0, 0] # forward, backward, jump\n  otherManualAction = [0, 0, 0]\n  manualMode = False\n  otherManualMode = False\n\n  # taken from https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py\n  def key_press(k, mod):\n    global manualMode, manualAction, otherManualMode, otherManualAction\n    if k == key.LEFT:  manualAction[0] = 1\n    if k == key.RIGHT: manualAction[1] = 1\n    if k == key.UP:    manualAction[2] = 1\n    if (k == key.LEFT or k == key.RIGHT or k == key.UP): manualMode = True\n\n    if k == key.D:     otherManualAction[0] = 1\n    if k == key.A:     otherManualAction[1] = 1\n    if k == key.W:     otherManualAction[2] = 1\n    if (k == key.D or k == key.A or k == key.W): otherManualMode = True\n\n  def key_release(k, mod):\n    global manualMode, manualAction, otherManualMode, otherManualAction\n    if k == key.LEFT:  manualAction[0] = 0\n    if k == key.RIGHT: manualAction[1] = 0\n    if k == key.UP:    manualAction[2] = 0\n    if k == key.D:     otherManualAction[0] = 0\n    if k == key.A:     otherManualAction[1] = 0\n    if k == key.W:     otherManualAction[2] = 0\n\n  policy = BaselinePolicy() # defaults to use RNN Baseline for player\n\n  env = SlimeVolleyEnv()\n  env.seed(np.random.randint(0, 10000))\n  #env.seed(721)\n\n  if RENDER_MODE:\n    env.render()\n    env.viewer.window.on_key_press = key_press\n    env.viewer.window.on_key_release = key_release\n\n  obs = env.reset()\n\n  steps = 0\n  total_reward = 0\n  action = np.array([0, 0, 0])\n\n  done = False\n\n  while not done:\n\n    if manualMode: # override with keyboard\n      action = manualAction\n    else:\n      action = policy.predict(obs)\n\n    if otherManualMode:\n      otherAction = otherManualAction\n      obs, reward, done, _ = env.step(action, otherAction)\n    else:\n      obs, reward, done, _ = env.step(action)\n\n    if reward > 0 or reward < 0:\n      print(\"reward\", reward)\n      manualMode = False\n      otherManualMode = False\n\n    total_reward += reward\n\n    if RENDER_MODE:\n      env.render()\n      sleep(0.01)\n\n    # make the game go slower for human players to be fair to humans.\n    if (manualMode or otherManualMode):\n      if PIXEL_MODE:\n        sleep(0.01)\n      else:\n        sleep(0.02)\n\n  env.close()\n  print(\"cumulative score\", total_reward)", ""]}
{"filename": "envs/robosumo/envs/sumo.py", "chunked_list": ["\"\"\"\nMulti-agent sumo environment.\n\"\"\"\nimport os\nimport tempfile\n\nimport numpy as np\n\nimport gym\nfrom gym.spaces import Tuple", "import gym\nfrom gym.spaces import Tuple\nfrom gym.utils import EzPickle\n\n# MuJoCo 1.5+\nfrom mujoco_py import MjViewer\nfrom envs.robosumo.envs import MujocoEnv\n\nfrom . import agents\nfrom .utils import construct_scene", "from . import agents\nfrom .utils import construct_scene\n\n\n_AGENTS = {\n    'ant': os.path.join(os.path.dirname(__file__), \"assets\", \"ant.xml\"),\n    'bug': os.path.join(os.path.dirname(__file__), \"assets\", \"bug.xml\"),\n    'spider': os.path.join(os.path.dirname(__file__), \"assets\", \"spider.xml\"),\n}\n", "}\n\n\nclass SumoEnv(MujocoEnv, EzPickle):\n    \"\"\"\n    Multi-agent sumo environment.\n\n    The goal of each agent is to push the other agent outside the tatami area.\n    The reward is shaped such that agents learn to prefer staying in the center\n    and pushing each other further away from the center. If any of the agents\n    gets outside of the tatami (even accidentially), it gets -WIN_REWARD_COEF\n    and the opponent gets +WIN_REWARD_COEF.\n    \"\"\"\n    STAY_IN_CENTER_COEF = 0.1\n    DRAW_PENALTY = -1000.\n    # MOVE_TO_CENTER_COEF = 0.1\n    MOVE_TO_OPP_COEF = 0.1\n    PUSH_OUT_COEF = 10.0\n\n    def __init__(self, agent_names,\n                 xml_path=None,\n                 init_pos_noise=.1,\n                 init_vel_noise=.1,\n                 agent_kwargs=None,\n                 frame_skip=5,\n                 tatami_size=2.0,\n                 timestep_limit=500,\n                 reward_shape=1.,\n                 **kwargs):\n        EzPickle.__init__(self)\n        self._tatami_size = tatami_size + 0.1\n        self._timestep_limit = timestep_limit\n        self._init_pos_noise = init_pos_noise\n        self._init_vel_noise = init_vel_noise\n        self._n_agents = len(agent_names)\n        self._mujoco_init = False\n        self._num_steps = 0\n        self._spec = None\n        self.WIN_REWARD = 2000 * reward_shape\n        self.LOSE_REWARD = - 2000 / reward_shape\n\n        # Resolve agent scopes\n        agent_scopes = [\n            \"%s%d\" % (name, i)\n            for i, name in enumerate(agent_names)\n        ]\n\n        # Consturct scene XML\n        scene_xml_path = os.path.join(os.path.dirname(__file__),\n                                      \"assets\", \"tatami.xml\")\n        agent_xml_paths = [_AGENTS[name] for name in agent_names]\n        scene = construct_scene(scene_xml_path, agent_xml_paths,\n                                agent_scopes=agent_scopes,\n                                tatami_size=tatami_size,\n                                **kwargs)\n        self.tatami_height = 0.5\n\n        # Init MuJoCo\n        if xml_path is None:\n            with tempfile.TemporaryDirectory() as tmpdir_name:\n                scene_filepath = os.path.join(tmpdir_name, \"scene.xml\")\n                scene.write(scene_filepath)\n                MujocoEnv.__init__(self, scene_filepath, frame_skip)\n        else:\n            with open(xml_path, 'w') as fp:\n                scene.write(fp.name)\n            MujocoEnv.__init__(self, fp.name, frame_skip)\n        self._mujoco_init = True\n\n        # Construct agents\n        agent_kwargs = agent_kwargs or {}\n        self.agents = [\n            agents.get(name, env=self, scope=agent_scopes[i], **agent_kwargs)\n            for i, name in enumerate(agent_names)\n        ]\n\n        # Set opponents\n        for i, agent in enumerate(self.agents):\n            agent.set_opponents([\n                agent for j, agent in enumerate(self.agents) if j != i\n            ])\n\n        # Setup agents\n        for i, agent in enumerate(self.agents):\n            agent.setup_spaces()\n\n        # Set observation and action spaces\n        # self.observation_space = Tuple([\n        #     agent.observation_space for agent in self.agents\n        # ])\n        # self.action_space = Tuple([\n        #     agent.action_space for agent in self.agents\n        # ])\n        self.observation_space = self.agents[0].observation_space\n        self.action_space = self.agents[0].action_space\n        self.num_agents = 2\n        self.is_vector_env = True\n\n    def simulate(self, actions):\n        a = np.concatenate(actions, axis=0)\n        self.do_simulation(a, self.frame_skip)\n\n    def step(self, actions):\n        if not self._mujoco_init:\n            return self._get_obs(), 0, False, None\n\n        dones = [False for _ in range(self._n_agents)]\n        rewards = [0. for _ in range(self._n_agents)]\n        infos = [{} for _ in range(self._n_agents)]\n\n        # Call `before_step` on the agents\n        for i in range(self._n_agents):\n            self.agents[i].before_step()\n\n        # Do simulation\n        self.simulate(actions)\n\n        # Call `after_step` on the agents\n        for i in range(self._n_agents):\n            infos[i]['ctrl_reward'] = self.agents[i].after_step(actions[i])\n\n        # Get obs\n        obs = self._get_obs()\n\n        self._num_steps += 1\n\n        # Compute rewards and dones\n        for i, agent in enumerate(self.agents):\n            self_xyz = agent.get_qpos()[:3]\n            # Loose penalty\n            infos[i]['lose_penalty'] = 0.\n        \n            if (self_xyz[2] < 0.29 + self.tatami_height or\n                    np.max(np.abs(self_xyz[:2])) >= self._tatami_size):\n                infos[i]['lose_penalty'] += self.LOSE_REWARD\n                dones[i] = True\n            # Win reward\n            infos[i]['win_reward'] = 0.\n            for opp in agent._opponents:\n                opp_xyz = opp.get_qpos()[:3]\n                if (opp_xyz[2] < 0.29 + self.tatami_height or\n                        np.max(np.abs(opp_xyz[:2])) >= self._tatami_size):\n                    infos[i]['win_reward'] += self.WIN_REWARD\n                    infos[i]['winner'] = True\n                    dones[i] = True\n            infos[i]['main_reward'] = \\\n                infos[i]['win_reward'] + infos[i]['lose_penalty']\n            # Draw penalty\n            if self._num_steps > self._timestep_limit:\n                infos[i]['main_reward'] += self.DRAW_PENALTY\n                dones[i] = True\n            # Move to opponent(s) and push them out of center\n            infos[i]['move_to_opp_reward'] = 0.\n            infos[i]['push_opp_reward'] = 0.\n            for opp in agent._opponents:\n                infos[i]['move_to_opp_reward'] += \\\n                    self._comp_move_reward(agent, opp.posafter)\n                infos[i]['push_opp_reward'] += \\\n                    self._comp_push_reward(agent, opp.posafter)\n            # Stay in center reward (unused)\n            # infos[i]['stay_in_center'] = self._comp_stay_in_center_reward(agent)\n            # Contact rewards and penalties (unused)\n            infos[i]['contact_reward'] = self._comp_contact_reward(agent)\n            # Reward shaping\n            infos[i]['shaping_reward'] = \\\n                infos[i]['ctrl_reward'] + \\\n                infos[i]['push_opp_reward'] + \\\n                infos[i]['move_to_opp_reward'] + \\\n                infos[i]['contact_reward']\n            # Add up rewards\n            rewards[i] = infos[i]['main_reward'] + infos[i]['shaping_reward']\n\n        rewards = tuple(rewards) # normlize\n        dones = tuple(dones)\n        if np.all(dones):\n            if 'winner' in infos[0]:\n                infos[0]['score'] = 1.\n            elif 'winner' in infos[1]:\n                infos[0]['score'] = 0.\n            else:\n                infos[0]['score'] = 0.5\n        infos = tuple(infos)\n\n        return np.array(obs).reshape(self.num_agents, -1), np.array(rewards).reshape(self.num_agents, -1), np.array(dones).reshape(self.num_agents, -1), infos\n\n    def _comp_move_reward(self, agent, target):\n        move_vec = (agent.posafter - agent.posbefore) / self.dt\n        direction = target - agent.posbefore\n        direction /= np.linalg.norm(direction)\n        return max(np.sum(move_vec * direction), 0.) * self.MOVE_TO_OPP_COEF\n\n    def _comp_push_reward(self, agent, target):\n        dist_to_center = np.linalg.norm(target)\n        return - self.PUSH_OUT_COEF * np.exp(-dist_to_center)\n\n    def _comp_stay_in_center_reward(self, agent):\n        dist_to_center = np.linalg.norm(agent.posafter)\n        return self.STAY_IN_CENTER_COEF * np.exp(-dist_to_center)\n\n    def _comp_contact_reward(self, agent):\n        # Penalty for pain\n        body_ids = [\n            agent.body_name_idx[name]\n            for name in ['head', 'torso'] if name in agent.body_name_idx\n        ]\n        forces = np.clip(agent.get_cfrc_ext(body_ids), -100., 100.)\n        pain = agent.COST_COEFS['pain'] * np.sum(np.abs(forces))\n        # Reward for attacking opponents\n        attack = 0.\n        for other in agent._opponents:\n            body_ids = [\n                other.body_name_idx[name]\n                for name in ['head', 'torso'] if name in other.body_name_idx\n            ]\n            forces = np.clip(other.get_cfrc_ext(body_ids), -100., 100.)\n            attack += agent.COST_COEFS['attack'] * np.sum(np.abs(forces))\n        return attack - pain\n\n    def _get_obs(self):\n        if not self._mujoco_init:\n            return np.concatenate([self.data.qpos.flat, self.data.qvel.flat])\n        return np.array([agent.get_obs() for agent in self.agents])\n\n    def reset_model(self):\n        self._num_steps = 0\n        # Randomize agent positions\n        r, z = 1.15, 1.25\n        delta = (2. * np.pi) / self._n_agents\n        phi = self.np_random.uniform(0., 2. * np.pi)\n        for i, agent in enumerate(self.agents):\n            angle = phi + i * delta\n            x, y = r * np.cos(angle), r * np.sin(angle)\n            agent.set_xyz((x, y, z))\n        # Add noise to all qpos and qvel elements\n        pos_noise = self.np_random.uniform(\n            size=self.model.nq,\n            low=-self._init_pos_noise,\n            high=self._init_pos_noise)\n        vel_noise = self._init_vel_noise * \\\n                    self.np_random.randn(self.model.nv)\n        qpos = self.data.qpos.ravel() + pos_noise\n        qvel = self.data.qvel.ravel() + vel_noise\n        self.init_qpos, self.init_qvel = qpos, qvel\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def viewer_setup(self):\n        if self.viewer is not None:\n            self.viewer._run_speed = 0.5\n            self.viewer.cam.trackbodyid = 0\n            # self.viewer.cam.lookat[2] += .8\n            self.viewer.cam.elevation = -25\n            self.viewer.cam.type = 1\n            self.sim.forward()\n            self.viewer.cam.distance = self.model.stat.extent * 1.0\n        # Make sure that the offscreen context has the same camera setup\n        if self.sim._render_context_offscreen is not None:\n            self.sim._render_context_offscreen.cam.trackbodyid = 0\n            # self.sim._render_context_offscreen.cam.lookat[2] += .8\n            self.sim._render_context_offscreen.cam.elevation = -25\n            self.sim._render_context_offscreen.cam.type = 1\n            self.sim._render_context_offscreen.cam.distance = \\\n                self.model.stat.extent * 1.0\n        self.buffer_size = (1280, 800)\n\n    def render(self, mode='human'):\n        return super(SumoEnv, self).render(mode=mode) # just raise an exception"]}
{"filename": "envs/robosumo/envs/__init__.py", "chunked_list": ["from .mujoco_env import MujocoEnv\nfrom .sumo import SumoEnv\n"]}
{"filename": "envs/robosumo/envs/utils.py", "chunked_list": ["import colorsys\nimport numpy as np\nimport os\nimport xml.etree.ElementTree as ET\n\n\ndef cart2pol(vec):\n    \"\"\"Convert a cartesian 2D vector to polar coordinates.\"\"\"\n    r = np.sqrt(vec[0]**2 + vec[1]**2)\n    theta = np.arctan2(vec[1], vec[0])\n    return np.asarray([r, theta])", "\n\ndef get_distinct_colors(n=2):\n    \"\"\"Source: https://stackoverflow.com/a/876872.\"\"\"\n    HSV_tuples = [(x * 1. / n, .5, .5) for x in range(n)]\n    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n    return RGB_tuples\n\n\ndef _set_class(root, prop, name):\n    if root is None:\n        return\n    if root.tag == prop:\n        root.set('class', name)\n    for child in list(root):\n        _set_class(child, prop, name)", "\ndef _set_class(root, prop, name):\n    if root is None:\n        return\n    if root.tag == prop:\n        root.set('class', name)\n    for child in list(root):\n        _set_class(child, prop, name)\n\n\ndef _add_prefix(root, prop, prefix, force_set=False):\n    if root is None:\n        return\n    root_prop_val = root.get(prop)\n    if root_prop_val is not None:\n        root.set(prop, prefix + '/' + root_prop_val)\n    elif force_set:\n        root.set(prop, prefix + '/' + 'anon' + str(np.random.randint(1, 1e10)))\n    for child in list(root):\n        _add_prefix(child, prop, prefix, force_set)", "\n\ndef _add_prefix(root, prop, prefix, force_set=False):\n    if root is None:\n        return\n    root_prop_val = root.get(prop)\n    if root_prop_val is not None:\n        root.set(prop, prefix + '/' + root_prop_val)\n    elif force_set:\n        root.set(prop, prefix + '/' + 'anon' + str(np.random.randint(1, 1e10)))\n    for child in list(root):\n        _add_prefix(child, prop, prefix, force_set)", "\n\ndef _tuple_to_str(tp):\n    return \" \".join(map(str, tp))\n\n\ndef construct_scene(scene_xml_path, agent_xml_paths,\n                    agent_densities=None,\n                    agent_scopes=None,\n                    init_poses=None,\n                    rgb=None,\n                    tatami_size=None):\n    \"\"\"Construct an XML that represents a MuJoCo scene for sumo.\"\"\"\n    n_agents = len(agent_xml_paths)\n    assert n_agents == 2, \"Only 2-agent sumo is currently supported.\"\n\n    scene = ET.parse(scene_xml_path)\n    scene_root = scene.getroot()\n    scene_default = scene_root.find('default')\n    scene_body = scene_root.find('worldbody')\n    scene_actuator = None\n    scene_sensors = None\n\n    # Set tatami size if specified\n    if tatami_size is not None:\n        for geom in scene_body.findall('geom'):\n            if geom.get('name') == 'tatami':\n                size = tatami_size + 0.3\n                geom.set('size', \"{size:.2f} {size:.2f} 0.25\".format(size=size))\n            if geom.get('name') == 'topborder':\n                fromto = \\\n                    \"-{size:.2f} {size:.2f} 0.5  {size:.2f} {size:.2f} 0.5\" \\\n                    .format(size=tatami_size)\n                geom.set('fromto', fromto)\n            if geom.get('name') == 'rightborder':\n                fromto = \\\n                    \"{size:.2f} -{size:.2f} 0.5  {size:.2f} {size:.2f} 0.5\" \\\n                    .format(size=tatami_size)\n                geom.set('fromto', fromto)\n            if geom.get('name') == 'bottomborder':\n                fromto = \\\n                    \"-{size:.2f} -{size:.2f} 0.5  {size:.2f} -{size:.2f} 0.5\" \\\n                    .format(size=tatami_size)\n                geom.set('fromto', fromto)\n            if geom.get('name') == 'leftborder':\n                fromto = \\\n                    \"-{size:.2f} -{size:.2f} 0.5  -{size:.2f} {size:.2f} 0.5\" \\\n                    .format(size=tatami_size)\n                geom.set('fromto', fromto)\n\n    # Resolve colors\n    if rgb is None:\n        rgb = get_distinct_colors(n_agents)\n    else:\n        assert len(rgb) == n_agents, \"Each agent must have a color.\"\n    RGBA_tuples = list(map(lambda x: _tuple_to_str(x + (1,)), rgb))\n\n    # Resolve densities\n    if agent_densities is None:\n        agent_densities = [10.0] * n_agents\n\n    # Resolve scopes\n    if agent_scopes is None:\n        agent_scopes = ['agent' + str(i) for i in range(n_agents)]\n    else:\n        assert len(agent_scopes) == n_agents, \"Each agent must have a scope.\"\n\n    # Resolve initial positions\n    if init_poses is None:\n        r, phi, z = 1.5, 0., .75\n        delta = (2. * np.pi) / n_agents\n        init_poses = []\n        for i in range(n_agents):\n            angle = phi + i * delta\n            x, y = r * np.cos(angle), r * np.sin(angle)\n            init_poses.append((x, y, z))\n\n    # Build agent XMLs\n    for i in range(n_agents):\n        agent_xml = ET.parse(agent_xml_paths[i])\n        agent_default = ET.SubElement(\n            scene_default, 'default',\n            attrib={'class': agent_scopes[i]}\n        )\n\n        # Set defaults\n        rgba = RGBA_tuples[i]\n        density = str(agent_densities[i])\n        default_set = False\n        for child in list(agent_xml.find('default')):\n            if child.tag == 'geom':\n                child.set('rgba', rgba)\n                child.set('density', density)\n                default_set = True\n            agent_default.append(child)\n        if not default_set:\n            agent_geom = ET.SubElement(\n                agent_default, 'geom',\n                attrib={\n                    'density': density,\n                    'contype': '1',\n                    'conaffinity': '1',\n                    'rgba': rgba,\n                }\n            )\n\n        # Build agent body\n        agent_body = agent_xml.find('body')\n        # set initial position\n        agent_body.set('pos', _tuple_to_str(init_poses[i]))\n        # add class to all geoms\n        _set_class(agent_body, 'geom', agent_scopes[i])\n        # add prefix to all names, important to map joints\n        _add_prefix(agent_body, 'name', agent_scopes[i], force_set=True)\n        # add aggent body to xml\n        scene_body.append(agent_body)\n\n        # Build agent actuators\n        agent_actuator = agent_xml.find('actuator')\n        # add class and prefix to all motor joints\n        _add_prefix(agent_actuator, 'joint', agent_scopes[i])\n        _add_prefix(agent_actuator, 'name', agent_scopes[i])\n        _set_class(agent_actuator, 'motor', agent_scopes[i])\n        # add actuator\n        if scene_actuator is None:\n            scene_root.append(agent_actuator)\n            scene_actuator = scene_root.find('actuator')\n        else:\n            for motor in list(agent_actuator):\n                scene_actuator.append(motor)\n\n        # Build agent sensors\n        agent_sensors = agent_xml.find('sensor')\n        # add same prefix to all sensors\n        _add_prefix(agent_sensors, 'joint', agent_scopes[i])\n        _add_prefix(agent_sensors, 'name', agent_scopes[i])\n        if scene_sensors is None:\n            scene_root.append(agent_sensors)\n            scene_sensors = scene_root.find('sensor')\n        else:\n            for sensor in list(agent_sensors):\n                scene_sensors.append(sensor)\n\n    return scene", ""]}
{"filename": "envs/robosumo/envs/mujoco_env.py", "chunked_list": ["\"\"\"\nThe base class for environments based on MuJoCo 1.5.\n\"\"\"\nimport os\nimport sys\nimport numpy as np\n\nimport gym\nfrom gym import error, spaces\nfrom gym.utils import seeding", "from gym import error, spaces\nfrom gym.utils import seeding\n\ntry:\n    import mujoco_py\n    from mujoco_py import load_model_from_path, MjSim, MjViewer\n    import glfw\nexcept ImportError as e:\n    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n", "\nfrom pkg_resources import parse_version\n\nif parse_version(mujoco_py.__version__) < parse_version('1.5'):\n    raise error.DependencyNotInstalled(\n        \"RoboSumo requires mujoco_py of version 1.5 or higher. \"\n        \"The installed version is {}. Please upgrade mujoco_py.\"\n        .format(mujoco_py.__version__))\n\n\ndef _read_pixels(sim, width=None, height=None, camera_name=None):\n    \"\"\"Reads pixels w/o markers and overlay from the same camera as screen.\"\"\"\n    if width is None or height is None:\n        resolution = glfw.get_framebuffer_size(\n            sim._render_context_window.window)\n        resolution = np.array(resolution)\n        resolution = resolution * min(1000 / np.min(resolution), 1)\n        resolution = resolution.astype(np.int32)\n        resolution -= resolution % 16\n        width, height = resolution\n\n    img = sim.render(width, height, camera_name=camera_name)\n    img = img[::-1, :, :] # Rendered images are upside-down.\n    return img", "\n\ndef _read_pixels(sim, width=None, height=None, camera_name=None):\n    \"\"\"Reads pixels w/o markers and overlay from the same camera as screen.\"\"\"\n    if width is None or height is None:\n        resolution = glfw.get_framebuffer_size(\n            sim._render_context_window.window)\n        resolution = np.array(resolution)\n        resolution = resolution * min(1000 / np.min(resolution), 1)\n        resolution = resolution.astype(np.int32)\n        resolution -= resolution % 16\n        width, height = resolution\n\n    img = sim.render(width, height, camera_name=camera_name)\n    img = img[::-1, :, :] # Rendered images are upside-down.\n    return img", "\n\nclass MujocoEnv(gym.Env):\n    \"\"\"Superclass for all MuJoCo environments.\n    \"\"\"\n    def __init__(self, model_path, frame_skip):\n        if model_path.startswith(\"/\"):\n            fullpath = model_path\n        else:\n            fullpath = os.path.join(os.path.dirname(__file__), \"assets\", model_path)\n        if not os.path.exists(fullpath):\n            raise IOError(\"File %s does not exist\" % fullpath)\n        self.frame_skip = frame_skip\n        self.model = load_model_from_path(fullpath)\n        self.sim = MjSim(self.model)\n        self.data = self.sim.data\n        self.viewer = None\n        self.buffer_size = (1600, 1280)\n\n        self.metadata = {\n            'render.modes': ['human', 'rgb_array'],\n            'video.frames_per_second': 60,\n        }\n\n        self.init_qpos = self.data.qpos.ravel().copy()\n        self.init_qvel = self.data.qvel.ravel().copy()\n        observation, _reward, done, _info = self.step(np.zeros(self.model.nu))\n        assert not done\n        self.obs_dim = np.sum([o.size for o in observation]) if (\n            type(observation) is tuple) else observation.size\n\n        bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)\n        low, high = bounds[:, 0], bounds[:, 1]\n        self.action_space = spaces.Box(low, high)\n\n        high = np.inf * np.ones(self.obs_dim, dtype=np.float32)\n        self.observation_space = spaces.Box(-high, high)\n\n        self._seed()\n\n    def _seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    # methods to override:\n    # ------------------------------------------------------------------------\n\n    def reset_model(self):\n        \"\"\"Reset the robot degrees of freedom (qpos and qvel).\n        Implement this in each subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def viewer_setup(self):\n        \"\"\"Called when the viewer is initialized and after every reset.\n        Optionally implement this method, if you need to tinker with camera\n        position and so forth.\n        \"\"\"\n        pass\n\n    # ------------------------------------------------------------------------\n\n    def reset(self):\n        self.sim.reset()\n        self.sim.forward()\n        ob = self.reset_model()\n        if ob is None: # zihan: added, fix None observation at reset()\n            ob = tuple([np.zeros(self.obs_dim) for _ in self.agents])\n        return ob\n\n    def set_state(self, qpos, qvel):\n        assert qpos.shape == (self.model.nq,)\n        assert qvel.shape == (self.model.nv,)\n        state = self.sim.get_state()\n        for i in range(self.model.nq):\n            state.qpos[i] = qpos[i]\n        for i in range(self.model.nv):\n            state.qvel[i] = qvel[i]\n        self.sim.set_state(state)\n        self.sim.forward()\n\n    @property\n    def dt(self):\n        return self.model.opt.timestep * self.frame_skip\n\n    def do_simulation(self, ctrl, n_frames):\n        for i in range(self.model.nu):\n            self.sim.data.ctrl[i] = ctrl[i]\n        for _ in range(n_frames):\n            self.sim.step()\n\n    def render(self, mode='human', close=False):\n        if close:\n            if self.viewer is not None:\n                self.viewer = None\n            return\n\n        if mode == 'rgb_array':\n            self.viewer_setup()\n            return _read_pixels(self.sim, *self.buffer_size)\n        elif mode == 'human':\n            return self._get_viewer().render()\n\n    def _get_viewer(self, mode='human'):\n        if self.viewer is None and mode == 'human':\n            self.viewer = MjViewer(self.sim)\n            self.viewer_setup()\n        return self.viewer\n\n    def state_vector(self):\n        state = self.sim.get_state()\n        return np.concatenate([state.qpos.flat, state.qvel.flat])", ""]}
{"filename": "envs/robosumo/envs/agents.py", "chunked_list": ["import os\nimport numpy as np\nimport xml.etree.ElementTree as ET\n\nimport gym\n\n\nclass Agent(object):\n    \"\"\"\n    Superclass for all agents in sumo MuJoCo environment.\n    \"\"\"\n\n    CFRC_CLIP = 100.\n\n    COST_COEFS = {\n        'ctrl': 1e-1,\n        'pain': 1e-4,\n        'attack': 1e-1,\n    }\n\n    JNT_NPOS = {\n        0: 7,\n        1: 4,\n        2: 1,\n        3: 1,\n    }\n\n    def __init__(self, env, scope, xml_path, adjust_z=0.):\n        self._env = env\n        self._scope = scope\n        self._xml_path = xml_path\n        self._xml = ET.parse(xml_path)\n        self._adjust_z = adjust_z\n\n        self._set_body()\n        self._set_joint()\n\n    def setup_spaces(self):\n        self._set_observation_space()\n        self._set_action_space()\n\n    def _in_scope(self, name):\n        return name.startswith(self._scope)\n\n    def _set_body(self):\n        self.body_names = list(filter(\n            lambda x: self._in_scope(x), self._env.model.body_names\n        ))\n        self.body_ids = [\n            self._env.model.body_names.index(name) for name in self.body_names\n        ]\n        self.body_name_idx = {\n            name.split('/')[-1]: idx\n            for name, idx in zip(self.body_names, self.body_ids)\n        }\n        # Determine body params\n        self.body_dofnum = self._env.model.body_dofnum[self.body_ids]\n        self.body_dofadr = self._env.model.body_dofadr[self.body_ids]\n        self.nv = self.body_dofnum.sum()\n        # Determine qvel_start_idx and qvel_end_idx\n        dof = list(filter(lambda x: x >= 0, self.body_dofadr))\n        self.qvel_start_idx = int(dof[0])\n        last_dof_body_id = self.body_dofnum.shape[0] - 1\n        while self.body_dofnum[last_dof_body_id] == 0:\n            last_dof_body_id -= 1\n        self.qvel_end_idx = int(dof[-1] + self.body_dofnum[last_dof_body_id])\n\n    def _set_joint(self):\n        self.joint_names = list(filter(\n            lambda x: self._in_scope(x), self._env.model.joint_names\n        ))\n        self.joint_ids = [\n            self._env.model.joint_names.index(name) for name in self.joint_names\n        ]\n\n        # Determine joint params\n        self.jnt_qposadr = self._env.model.jnt_qposadr[self.joint_ids]\n        self.jnt_type = self._env.model.jnt_type[self.joint_ids]\n        self.jnt_nqpos = [self.JNT_NPOS[int(j)] for j in self.jnt_type]\n        self.nq = sum(self.jnt_nqpos)\n        # Determine qpos_start_idx and qpos_end_idx\n        self.qpos_start_idx = int(self.jnt_qposadr[0])\n        self.qpos_end_idx = int(self.jnt_qposadr[-1] + self.jnt_nqpos[-1])\n\n    def _set_observation_space(self):\n        obs = self.get_obs()\n        self.obs_dim = obs.size\n        low = -np.inf * np.ones(self.obs_dim, dtype=np.float32)\n        high = np.inf * np.ones(self.obs_dim, dtype=np.float32)\n        self.observation_space = gym.spaces.Box(low, high)\n\n\n    def _set_action_space(self):\n        acts = self._xml.find('actuator')\n        self.action_dim = len(list(acts))\n        default = self._xml.find('default')\n        range_set = False\n        if default is not None:\n            motor = default.find('motor')\n            if motor is not None:\n                ctrl = motor.get('ctrlrange')\n                if ctrl:\n                    clow, chigh = list(map(float, ctrl.split()))\n                    high = chigh * np.ones(self.action_dim, np.float32)\n                    low = clow * np.ones(self.action_dim, np.float32)\n                    range_set = True\n        if not range_set:\n            high =  np.ones(self.action_dim, dtype=np.float32)\n            low = - np.ones(self.action_dim, dtype=np.float32)\n        for i, motor in enumerate(list(acts)):\n            ctrl = motor.get('ctrlrange')\n            if ctrl:\n                clow, chigh = list(map(float, ctrl.split()))\n                low[i], high[i] = clow, chigh\n        self._low, self._high = low, high\n        self.action_space = gym.spaces.Box(low, high)\n\n    def set_xyz(self, xyz):\n        \"\"\"Set (x, y, z) position of the agent; any element can be None.\"\"\"\n        qpos = self._env.data.qpos.ravel().copy()\n        start = self.qpos_start_idx\n        if xyz[0]: qpos[start]     = xyz[0]\n        if xyz[1]: qpos[start + 1] = xyz[1]\n        if xyz[2]: qpos[start + 2] = xyz[2]\n        qvel = self._env.data.qvel.ravel()\n        self._env.set_state(qpos, qvel)\n\n    def set_euler(self, euler):\n        \"\"\"Set euler angles the agent; any element can be None.\"\"\"\n        qpos = self._env.data.qpos.ravel().copy()\n        start = self.qpos_start_idx\n        if euler[0]: qpos[start + 4] = euler[0]\n        if euler[1]: qpos[start + 5] = euler[1]\n        if euler[2]: qpos[start + 6] = euler[2]\n        qvel = self._env.data.qvel.ravel()\n        self._env.set_state(qpos, qvel)\n\n    def set_opponents(self, opponents):\n        self._opponents = opponents\n\n    def reset(self):\n        pass\n\n    # --------------------------------------------------------------------------\n    # Various getters\n    # --------------------------------------------------------------------------\n\n    def get_body_com(self, body_name):\n        idx = self.body_names.index(self._scope + '/' + body_name)\n        return self._env.data.subtree_com[self.body_ids[idx]]\n\n    def get_cfrc_ext(self, body_ids=None):\n        if body_ids is None:\n            body_ids = self.body_ids\n        return self._env.data.cfrc_ext[body_ids]\n\n    def get_qpos(self):\n        \"\"\"Note: relies on the qpos for one agent being contiguously located.\n        \"\"\"\n        qpos = self._env.data.qpos[self.qpos_start_idx:self.qpos_end_idx].copy()\n        qpos[2] += self._adjust_z\n        return qpos\n\n    def get_qvel(self):\n        \"\"\"Note: relies on the qvel for one agent being contiguously located.\n        \"\"\"\n        qvel = self._env.data.qvel[self.qvel_start_idx:self.qvel_end_idx]\n        return qvel\n\n    def get_qfrc_actuator(self):\n        start, end = self.qvel_start_idx, self.qvel_end_idx\n        qfrc = self._env.data.qfrc_actuator[start:end]\n        return qfrc\n\n    def get_cvel(self):\n        cvel = self._env.data.cvel[self.body_ids]\n        return cvel\n\n    def get_body_mass(self):\n        body_mass = self._env.model.body_mass[self.body_ids]\n        return body_mass\n\n    def get_xipos(self):\n        xipos = self._env.data.xipos[self.body_ids]\n        return xipos\n\n    def get_cinert(self):\n        cinert = self._env.data.cinert[self.body_ids]\n        return cinert\n\n    def get_obs(self):\n        # Observe self\n        self_forces = np.abs(np.clip(\n            self.get_cfrc_ext(), -self.CFRC_CLIP, self.CFRC_CLIP))\n        obs  = [\n            self.get_qpos().flat,           # self all positions\n            self.get_qvel().flat,           # self all velocities\n            self_forces.flat,               # self all forces\n        ]\n        # Observe opponents\n        for opp in self._opponents:\n            body_ids = [\n                opp.body_name_idx[name]\n                for name in ['torso']\n                if name in opp.body_name_idx\n            ]\n            opp_forces = np.abs(np.clip(\n                opp.get_cfrc_ext(body_ids), -self.CFRC_CLIP, self.CFRC_CLIP))\n            obs.extend([\n                opp.get_qpos()[:7].flat,    # opponent torso position\n                opp_forces.flat,            # opponent torso forces\n            ])\n        return np.concatenate(obs)\n\n    def before_step(self):\n        self.posbefore = self.get_qpos()[:2].copy()\n\n    def after_step(self, action):\n        self.posafter = self.get_qpos()[:2].copy()\n        # Control cost\n        reward = - self.COST_COEFS['ctrl'] * np.square(action).sum()\n        return reward", "\n\n# ------------------------------------------------------------------------------\n# Beasts\n# ------------------------------------------------------------------------------\n\nclass Ant(Agent):\n    \"\"\"\n    The 4-leg agent.\n    \"\"\"\n\n    def __init__(self, env, scope=\"ant\", **kwargs):\n        xml_path = os.path.join(os.path.dirname(__file__),\n                                \"assets\", \"ant.xml\")\n        super(Ant, self).__init__(env, scope, xml_path, **kwargs)", "\n\nclass Bug(Agent):\n    \"\"\"\n    The 6-leg agent.\n    \"\"\"\n\n    def __init__(self, env, scope=\"bug\", **kwargs):\n        xml_path = os.path.join(os.path.dirname(__file__),\n                                \"assets\", \"bug.xml\")\n        super(Bug, self).__init__(env, scope, xml_path, **kwargs)", "\n\nclass Spider(Agent):\n    \"\"\"\n    The 8-leg agent.\n    \"\"\"\n\n    def __init__(self, env, scope=\"spider\", **kwargs):\n        xml_path = os.path.join(os.path.dirname(__file__),\n                                \"assets\", \"spider.xml\")\n        super(Spider, self).__init__(env, scope, xml_path, **kwargs)", "\n\n# ------------------------------------------------------------------------------\n\n_available_agents = {\n    'ant': Ant,\n    'bug': Bug,\n    'spider': Spider,\n}\n", "}\n\n\ndef get(name, *args, **kwargs):\n    if name not in _available_agents:\n        raise ValueError(\"Class %s is not available.\" % name)\n    return _available_agents[name](*args, **kwargs)\n"]}
{"filename": "toyexample/rppo.py", "chunked_list": ["import argparse\nimport os\nimport random\nimport time\nfrom distutils.util import strtobool\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nimport gym\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nimport envs\n\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n        help=\"the name of this experiment\")\n    parser.add_argument(\"--seed\", type=int, default=1,\n        help=\"seed of the experiment\")\n    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, cuda will be enabled by default\")\n\n    # Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"ToyEnv-v0\",\n        help=\"the id of the environment\")\n    parser.add_argument(\"--total-timesteps\", type=int, default=1e6,\n        help=\"total timesteps of the experiments\")\n    parser.add_argument(\"--learning-rate\", type=float, default=1e-4,\n        help=\"the learning rate of the optimizer\")\n    parser.add_argument(\"--num-envs\", type=int, default=1,\n        help=\"the number of parallel game environments\")\n    parser.add_argument(\"--num-steps\", type=int, default=200,\n        help=\"the number of steps to run in each environment per policy rollout\")\n    parser.add_argument(\"--gamma\", type=float, default=0.95,\n        help=\"the discount factor gamma\")\n    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n        help=\"the number of mini-batches\")\n    parser.add_argument(\"--update-epochs\", type=int, default=4,\n        help=\"the K epochs to update the policy\")\n    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=False,\n        help=\"Toggles advantages normalization\")\n    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n        help=\"the surrogate clipping coefficient\")\n    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n        help=\"coefficient of the entropy\")\n    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n        help=\"coefficient of the value function\")\n    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n        help=\"the maximum norm for the gradient clipping\")\n\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n        help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Use GAE for advantage computation\")\n    parser.add_argument(\"--risk\", type=lambda x: bool(strtobool(x)), default=True,\n        help=\"the maximum norm for the gradient clipping\")\n    parser.add_argument(\"--tau\", type=float, default=0.5)\n    parser.add_argument(\"--onehot-state\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True)\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    return args", "\n\ndef make_env(args):\n    env = gym.make(args.env_id)\n    env.seed(args.seed)\n    env.action_space.seed(args.seed)\n    env.observation_space.seed(args.seed)\n    return env\n\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer", "\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass Agent(nn.Module):\n    def __init__(self, obs_space, act_space):\n        super().__init__()\n        obs_dim = int(np.prod(obs_space.shape))\n        act_dim = act_space.n\n        hidden_dim = 128\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(obs_dim, hidden_dim)),\n            nn.ReLU(),\n            layer_init(nn.Linear(hidden_dim, 1), std=0.01),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(obs_dim, hidden_dim)),\n            nn.ReLU(),\n            layer_init(nn.Linear(hidden_dim, act_dim), std=0.01),\n        )\n\n    def get_value(self, x):\n        if x.ndim == 1:\n            x = x.view((-1, 1))\n        return self.critic(x)\n\n    def get_action(self, x):\n        if x.ndim == 1:\n            x = x.view((-1, 1))\n        logits = self.actor(x)\n        dist = Categorical(logits=logits)\n        action = dist.probs.argmax(dim=-1)\n        return action\n\n    def get_action_and_value(self, x, action=None):\n        if x.ndim == 1:\n            x = x.view((-1, 1))\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n    \n    def save(self, run_dir):\n        torch.save(self.actor.state_dict(), run_dir + \"/actor.pt\")\n        torch.save(self.critic.state_dict(), run_dir + \"/critic.pt\")\n\n    def restore(self, run_dir):\n        actor_state_dict = torch.load(run_dir + '/actor.pt')\n        # critic_state_dict = torch.load(run_dir + '/critic.pt')\n        # actor_state_dict = torch.load(run_dir)\n        self.actor.load_state_dict(actor_state_dict)", "        # self.critic.load_state_dict(critic_state_dict)\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_dir = f\"runs/{args.env_id}/{args.exp_name}/{args.tau}/\" + time.strftime(\"%b%d-%H%M%S\", time.localtime())\n    if not os.path.exists(run_dir):\n        os.makedirs(run_dir)\n    args.run_dir = run_dir\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\n    # env setup\n    envs = make_env(args)\n    assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    agent = Agent(envs.observation_space, envs.action_space).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, 1) + envs.observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, 1) + envs.action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, 1)).to(device)\n    rewards = torch.zeros((args.num_steps, 1)).to(device)\n    dones = torch.zeros((args.num_steps+1, 1)).to(device)\n    values = torch.zeros((args.num_steps+1, 1)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device).unsqueeze(0)\n    next_done = False\n    num_updates = int(args.total_timesteps // args.batch_size)\n    value_loss = []\n    for update in range(1, num_updates + 1):\n\n        for step in range(0, args.num_steps):\n            global_step += 1\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy().item())\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            if done:\n                next_obs = envs.reset()\n            next_obs = torch.Tensor(next_obs).to(device).unsqueeze(0)\n            next_done = torch.tensor(done).to(device).view(-1)\n\n\n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).view(-1)\n            values[args.num_steps] = next_value\n            dones[args.num_steps] = next_done\n\n            max_depth = 25 # in toy example, max episode length is 25.\n            advantages = torch.zeros_like(rewards).to(device)\n            returns = torch.zeros_like(rewards).to(device)\n            depths = torch.zeros(args.num_steps+1,)\n            masks = 1- dones\n            for i in reversed(range(args.num_steps)):\n                depths[i] = min(depths[i+1]+1, max_depth)\n                if masks[i+1]==0:\n                    depths[i] = 1\n            depths = depths.cpu().numpy().astype(np.int32)\n            values_table = torch.zeros(max_depth, args.num_steps+1).to(device)\n            values_table[0, args.num_steps] = next_value\n\n            def operator(reward, value, nextvalue, mask):\n                delta = reward + args.gamma * nextvalue * mask - value\n                delta = args.tau * torch.maximum(delta, torch.zeros_like(delta)) + \\\n                        (1-args.tau) * torch.minimum(delta, torch.zeros_like(delta))\n                alpha = 1 / (2 * max(args.tau, (1-args.tau)))\n                delta = 2 * alpha * delta\n                return value + delta\n\n            if args.risk:\n                if args.gae:\n                    for step in reversed(range(args.num_steps)):\n                        values_table[0, step] = operator(rewards[step], values[step], values[step+1], masks[step+1])\n                        for d in range(depths[step]):\n                            values_table[d][step] = operator(rewards[step], values_table[d-1][step], values_table[d-1][step+1], masks[step+1])\n                        for d in reversed(range(depths[step])):\n                            returns[step] = values_table[d][step] + args.gae_lambda * returns[step]\n                        returns[step] *= (1-args.gae_lambda) / (1-args.gae_lambda**(depths[step]))\n                else:\n                    for t in reversed(range(args.num_steps)):\n                        returns[t] = operator(rewards[t], values[t], values[t+1], masks[t+1])\n                advantages = returns - values[:-1,...]\n            else:\n                if args.gae:\n                    gae = 0\n                    for t in reversed(range(args.num_steps)):\n                        delta = rewards[t] + args.gamma * values[t+1] * masks[t+1] - values[t]\n                        advantages[t] = gae = delta + args.gamma * args.gae_lambda * masks[t+1] * gae\n                else:\n                    raise NotImplementedError\n                returns = advantages + values[:-1, ...]\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values[:-1, ...].reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n                logratio = newlogprob - b_logprobs[mb_inds]\n                ratio = logratio.exp()\n                mb_advantages = b_advantages[mb_inds]\n                if args.norm_adv:\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\n                # Policy loss\n                pg_loss1 = -mb_advantages * ratio\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n                # pg_loss = pg_loss1.mean()\n\n                # Value loss\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                    v_clipped = b_values[mb_inds] + torch.clamp(\n                        newvalue - b_values[mb_inds],\n                        -args.clip_coef,\n                        args.clip_coef,\n                    )\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                    v_loss = 0.5 * v_loss_max.mean()\n                else:\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\n                entropy_loss = entropy.mean()\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n                optimizer.step()\n\n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n        value_loss.append(v_loss)\n        if update % 1 == 0:\n            print(f\"Updates: {update}/{num_updates}, Returns:{(rewards.sum()/dones.sum()).item()}\")\n        agent.save(run_dir)\n    \n    envs.close()\n\n    from toyexample.eval import eval\n    print(eval(args))"]}
{"filename": "toyexample/eval.py", "chunked_list": ["import argparse\nimport torch\nimport torch.nn as nn\nfrom torch.distributions.categorical import Categorical\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nimport gym\nimport numpy as np\nfrom envs import toy", "import numpy as np\nfrom envs import toy\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.use('Agg')\nfrom rppo import Agent\n\ndef eval(args):\n    ## config\n    seed = 2021\n    num_rollouts = 1000\n    device = torch.device('cuda:0')\n    tpdv = dict(dtype=torch.float32, device=device)\n    env = toy.ToyEnv(wind=True, onehot_state=args.onehot_state)\n    env.seed(seed)\n    policy = Agent(env.observation_space, env.action_space).to(device)\n    policy.restore(args.run_dir)\n\n    ### state frequency\n    state_freq = np.zeros(env.shape)\n    episode_rewards = []\n    violations = []\n    for _ in range(num_rollouts):\n        state = env.reset()\n        episode_reward = 0\n        water_times = 0\n        step = 0\n        while True:\n            act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n            state, reward, done, info = env.step(act.item())\n            episode_reward += reward\n            water_times += 1 if reward == -1 else 0\n            step += 1\n            if done:\n                state_freq += env.visited\n                episode_rewards.append(episode_reward)\n                violations.append(water_times / step)\n                break\n    np.save(args.run_dir + \"/episode_rewards.npy\", np.array(episode_rewards))\n    np.save(args.run_dir + \"/state_freq.npy\", state_freq)\n    state_freq = state_freq / np.sum(state_freq)\n    print(\"state frequency: \", state_freq)\n    plt.figure(figsize=(6,4))\n    sns.heatmap(state_freq, cmap=\"Reds\")\n    plt.savefig(args.run_dir + \"/state_frequency.pdf\")\n\n    # determinstic optimal path for no wind\n    env.use_wind = False\n    state = env.reset()\n    while True:\n        act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n        state, reward, done, info = env.step(act.item())\n        if done:\n            break\n    print(\"policy path for no wind: \", env.visited)\n    plt.figure(figsize=(6,4))\n    sns.heatmap(env.visited, cmap=\"Reds\")\n    plt.savefig(args.run_dir + \"/determinstic_path.pdf\")\n    return np.mean(episode_rewards), np.mean(violations)", "\ndef eval(args):\n    ## config\n    seed = 2021\n    num_rollouts = 1000\n    device = torch.device('cuda:0')\n    tpdv = dict(dtype=torch.float32, device=device)\n    env = toy.ToyEnv(wind=True, onehot_state=args.onehot_state)\n    env.seed(seed)\n    policy = Agent(env.observation_space, env.action_space).to(device)\n    policy.restore(args.run_dir)\n\n    ### state frequency\n    state_freq = np.zeros(env.shape)\n    episode_rewards = []\n    violations = []\n    for _ in range(num_rollouts):\n        state = env.reset()\n        episode_reward = 0\n        water_times = 0\n        step = 0\n        while True:\n            act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n            state, reward, done, info = env.step(act.item())\n            episode_reward += reward\n            water_times += 1 if reward == -1 else 0\n            step += 1\n            if done:\n                state_freq += env.visited\n                episode_rewards.append(episode_reward)\n                violations.append(water_times / step)\n                break\n    np.save(args.run_dir + \"/episode_rewards.npy\", np.array(episode_rewards))\n    np.save(args.run_dir + \"/state_freq.npy\", state_freq)\n    state_freq = state_freq / np.sum(state_freq)\n    print(\"state frequency: \", state_freq)\n    plt.figure(figsize=(6,4))\n    sns.heatmap(state_freq, cmap=\"Reds\")\n    plt.savefig(args.run_dir + \"/state_frequency.pdf\")\n\n    # determinstic optimal path for no wind\n    env.use_wind = False\n    state = env.reset()\n    while True:\n        act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n        state, reward, done, info = env.step(act.item())\n        if done:\n            break\n    print(\"policy path for no wind: \", env.visited)\n    plt.figure(figsize=(6,4))\n    sns.heatmap(env.visited, cmap=\"Reds\")\n    plt.savefig(args.run_dir + \"/determinstic_path.pdf\")\n    return np.mean(episode_rewards), np.mean(violations)", "\n\nif __name__ == \"__main__\":\n    def parse_args():\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"--run-dir\", type=str, default=\"runs/ToyEnv-v0/0.5/Apr07-121744\")\n        args = parser.parse_args()\n        return args\n    args = parse_args()\n    args.onehot_state = True\n    print(eval(args))", "    # returns = []\n    # violations = []\n    # for tau in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n    #     args.run_dir = f\"toyexample/runs/risks/tau{tau}.pt\"\n    #     r, v = eval(args)\n    #     returns.append(r)\n    #     violations.append(v)\n    # returns = np.array(returns)\n    # violations = np.array(violations)\n    # np.save(\"toy_returns.npy\", returns)", "    # violations = np.array(violations)\n    # np.save(\"toy_returns.npy\", returns)\n    # np.save(\"toy_violations.npy\", violations)"]}
{"filename": "toyexample/rqlearning.py", "chunked_list": ["from math import gamma\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\nimport numpy as np\nimport envs\nimport gym\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n", "import seaborn as sns\n\nenv = gym.make('ToyEnv-v0', onehot_state=False)\nnum_epis_train = int(1e4)\ninit_lr = 0.01\ngamma = 0.95\neps = 0.3\n\nlength = 100\nnum_iter = 4 * length", "length = 100\nnum_iter = 4 * length\nnS = env.nS\nnA = env.nA\nQtable = np.zeros((nS, nA))\nrender = False\nvisualize_learning_result = False\nlimited = True\n# tau = 0.5\n\ndef qlearning(tau=0.5):\n    for epis in range(num_epis_train):\n        s = env.reset()\n        # print(f\"--------episode:{epis+1}---------\")\n        for iter in range(1, num_iter+1):\n            if np.random.uniform(0, 1) < eps:\n                a = np.random.choice(nA)\n            else:\n                a = np.argmax(Qtable[s,:])\n            n_s, r, done,_ = env.step(a)\n            # Qtable[s,a] = Qtable[s, a] + lr * (r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a])\n            delta = r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a]\n            alpha = 1 / (2 * max(tau,(1-tau)))\n            if epis >= 1e3:\n                frac = 1 - (epis - 1e3) / (num_epis_train - 1e3)\n                lr = init_lr * frac\n            else:\n                lr = init_lr\n            Qtable[s,a] = Qtable[s, a] + lr * 2 * alpha * (tau * np.maximum(delta, 0) + (1 - tau) * np.minimum(delta, 0))\n            s = n_s\n            if done: \n                break", "# tau = 0.5\n\ndef qlearning(tau=0.5):\n    for epis in range(num_epis_train):\n        s = env.reset()\n        # print(f\"--------episode:{epis+1}---------\")\n        for iter in range(1, num_iter+1):\n            if np.random.uniform(0, 1) < eps:\n                a = np.random.choice(nA)\n            else:\n                a = np.argmax(Qtable[s,:])\n            n_s, r, done,_ = env.step(a)\n            # Qtable[s,a] = Qtable[s, a] + lr * (r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a])\n            delta = r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a]\n            alpha = 1 / (2 * max(tau,(1-tau)))\n            if epis >= 1e3:\n                frac = 1 - (epis - 1e3) / (num_epis_train - 1e3)\n                lr = init_lr * frac\n            else:\n                lr = init_lr\n            Qtable[s,a] = Qtable[s, a] + lr * 2 * alpha * (tau * np.maximum(delta, 0) + (1 - tau) * np.minimum(delta, 0))\n            s = n_s\n            if done: \n                break", "\n\n\nfor tau in range(11):\n    tau = tau / 10\n    env.seed(2000)\n    qlearning(tau)\n    ### state frequency\n    env.use_wind = False\n    num_rollouts = 10000\n    state_freq = np.zeros(env.shape)\n    episode_rewards = []\n    for _ in range(num_rollouts):\n        s = env.reset()\n        episode_reward = 0\n        while True:\n            a  = np.argmax(Qtable[s,:])\n            s, reward, done, info = env.step(a)\n            episode_reward += reward\n            if done:\n                state_freq += env.visited\n                episode_rewards.append(episode_reward)\n                break\n\n    state_freq = state_freq / np.sum(state_freq)\n    print(\"state frequency: \", state_freq)\n    plt.figure(figsize=(6,4))\n    sns.heatmap(state_freq, cmap=\"Reds\")\n    plt.savefig(f\"tau{tau}_state_frequency.png\")"]}
