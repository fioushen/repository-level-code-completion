{"filename": "convert.py", "chunked_list": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport coremltools as ct\nimport numpy as np\nfrom datetime import datetime\nfrom models.gpt2 import GPT as GPT2\nfrom models.pythia import GPT as Pythia\nfrom src.utils.psnr import compute_psnr\nfrom src.utils.trace_warnings import silence_known_trace_warnings\nimport argparse", "from src.utils.trace_warnings import silence_known_trace_warnings\nimport argparse\nimport gc\nimport sys\n\n\"\"\"\nConvert a slightly modified nanoGPT or Huggingface pythia to CoreML.\n\"\"\"\n\nall_names = GPT2.model_names() + Pythia.model_names()", "\nall_names = GPT2.model_names() + Pythia.model_names()\n\nparser = argparse.ArgumentParser(description='Convert a model to CoreML.')\nparser.add_argument('--model_name', choices=all_names, default=\"gpt2\", type=str)\nparser.add_argument('--low_memory', help=\"use less memory at the cost of being slower. useful for large models.\", action=\"store_true\")\nargs = parser.parse_args()\n\nfile_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n", "file_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\nmodel_name = args.model_name\nmodel_filename = model_name.split(\"/\")[-1] + \"_\" + file_suffix\n\nretrace = True\nif retrace:\n    print(f\"Loading model {model_name}...\")\n    model_class = GPT2 if model_filename.startswith(\"gpt2\") else Pythia\n    token_predictor = model_class.from_pretrained(model_name).eval()\n\n    input_ids = torch.randint(10000, (1,512,))\n    output_mask = torch.randint(512, (1,))\n    print(f\"Tracing the model with {input_ids.shape}...\")\n    with silence_known_trace_warnings(model_name):\n        traced_token_predictor = torch.jit.trace(token_predictor, (input_ids, output_mask))\nelse:\n    print(\"Loading from saved file.\")\n    traced_token_predictor = torch.jit.load(f\"{model_filename}.pt\")", "\n# print(traced_token_predictor)\n\nprint(\"Trace finished.\")\nprint(\"Beginning conversion...\")\n\ndef op_selector(op):\n    \"\"\"\n    Return true to use float16 for the op. Must be f16 to run on Neural Engine.\n\n    You can find op_type by looking in Netron and/or print out the op type/name here\n    (usually the names contain a variable name).\n    \"\"\"\n    # LayerNorm is where we lose most of our precision. From experiments\n    # in optimizing for ANE, it's most likely the computing the first mean,\n    # but using the non-optimized version we have to float32 the whole layer norm.\n    return op.op_type not in [\"layer_norm\"]", "\ncompute_precision=ct.precision.FLOAT16\nif model_name in [\"gpt2\"]:\n    print(\"Using float32 for layer_norm otherwise the precision lost is too large.\")\n    print(\"Larger models can use all float16.\") #... and run purely on the neural engine.\n    compute_precision=ct.transform.FP16ComputePrecision(op_selector)\n\nif args.low_memory:\n    del token_predictor\n    gc.collect()", "\nmlmodel = ct.convert(\n    traced_token_predictor,\n    inputs=[\n        ct.TensorType(name=\"input_ids\", shape=[1, 512], dtype=np.int32),\n        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n    ],\n    outputs=[\n        ct.TensorType(name=\"logits\", dtype=np.float32),\n    ],", "        ct.TensorType(name=\"logits\", dtype=np.float32),\n    ],\n    compute_precision=compute_precision,\n    convert_to=\"mlprogram\",\n)\n\nprint(\"Conversion finished.\")\n\nif args.low_memory:\n    del traced_token_predictor\n    gc.collect()\n\n    print(\"Saving...\")\n    mlmodel.save(f\"{model_filename}.mlpackage\")\n\n    del mlmodel\n    gc.collect()\n\n    print(\"Adding metadata...\")\n    mlmodel = ct.models.MLModel(f\"{model_filename}.mlpackage\", skip_model_load=True)", "if args.low_memory:\n    del traced_token_predictor\n    gc.collect()\n\n    print(\"Saving...\")\n    mlmodel.save(f\"{model_filename}.mlpackage\")\n\n    del mlmodel\n    gc.collect()\n\n    print(\"Adding metadata...\")\n    mlmodel = ct.models.MLModel(f\"{model_filename}.mlpackage\", skip_model_load=True)", "\n# TODO: Clean up.\npretty_name = {\n    \"gpt2\": \"gpt2 (124M)\",\n    \"gpt2-medium\": \"gpt2-medium (350M)\",\n    \"gpt2-large\": \"gpt2-large (774M)\",\n    \"gpt2-xl\": \"gpt2-xl (1558M)\",\n}.get(model_name, model_name)\nmodel_family = [x for x in [\"gpt2\", \"pythia\"] if x in model_name][0]\neos_token_id = {\"gpt2\": 50256, \"pythia\": 0}[model_family]", "model_family = [x for x in [\"gpt2\", \"pythia\"] if x in model_name][0]\neos_token_id = {\"gpt2\": 50256, \"pythia\": 0}[model_family]\nbased_on = {\"gpt2\": \"nanoGPT\", \"pythia\": \"the HuggingFace implementation\"}[model_family]\nvocab_size = {\"gpt2\": 50257, \"pythia\": 50304}[model_family] if model_name != \"pythia-6.9b\" else 50432\n\nmlmodel.short_description = f\"{pretty_name} for text generation. Based on {based_on}. Optimized for Apple Neural Engine.\"\nmlmodel.input_description[\"input_ids\"] = f\"Input tokens. e.g. from the huggingface {model_family} tokenizer. Pad to the full length with {eos_token_id} (eos).\"\nmlmodel.input_description[\"output_mask\"] = \"A single element array with the index of your sequence to predict. If your non-padded input length was N, pass [N-1].\"\nmlmodel.output_description[\"logits\"] = f\"Predictions for the element of input_ids specified by output_mask in the shape (1, 1, {vocab_size}). \"\nmlmodel.user_defined_metadata[\"Converted By\"] = \"http://twitter.com/flat\"", "mlmodel.output_description[\"logits\"] = f\"Predictions for the element of input_ids specified by output_mask in the shape (1, 1, {vocab_size}). \"\nmlmodel.user_defined_metadata[\"Converted By\"] = \"http://twitter.com/flat\"\nmlmodel.user_defined_metadata[\"URL\"] = \"https://github.com/smpanaro/more-ane-transformers\"\n\nif not args.low_memory:\n    print(\"Saving...\")\n\n# Workaround to save metadata: https://github.com/apple/coremltools/issues/1680\nto_save = ct.models.MLModel(mlmodel._spec,\n                  weights_dir=mlmodel._weights_dir,", "to_save = ct.models.MLModel(mlmodel._spec,\n                  weights_dir=mlmodel._weights_dir,\n                  is_temp_package=True)\nto_save.save(f\"{model_filename}.mlpackage\")\n\nif args.low_memory:\n    print(\"Skipping model comparison due to low memory mode.\")\n    print(\"Conversion complete.\")\n    sys.exit(0)\n", "\n# Always compare in float32 so we don't overflow.\nwith torch.no_grad():\n    og_out = token_predictor(input_ids, output_mask).to(torch.float32)\n    tr_out = traced_token_predictor(input_ids, output_mask).to(torch.float32)\ninput_ids = input_ids.int()\noutput_mask = output_mask.int()\n# Hanging here? It's very likely your intputs are the wrong shape and/or types.\nprint(\"predicting with mlmodel\")#, input_ids.shape, input_ids.dtype)\ncm_out = mlmodel.predict({\"input_ids\": input_ids.numpy(), \"output_mask\": output_mask.numpy()})", "print(\"predicting with mlmodel\")#, input_ids.shape, input_ids.dtype)\ncm_out = mlmodel.predict({\"input_ids\": input_ids.numpy(), \"output_mask\": output_mask.numpy()})\ncm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n\nassert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\nassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n\ntrace_psnr = compute_psnr(og_out, tr_out)\nif trace_psnr < 200:\n    print(f\"tracing PSNR too low ({trace_psnr}), CoreML model will likely be unusable\")", "if trace_psnr < 200:\n    print(f\"tracing PSNR too low ({trace_psnr}), CoreML model will likely be unusable\")\n\nprint(\"\\nfinished. these should be >60, ideally much higher (inf is perfect). lower and the model may not be usable\")\nprint(\"coreml-traced   psnr:\", compute_psnr(tr_out.numpy(), cm_out.numpy()))\nprint(\"coreml-original psnr:\", compute_psnr(og_out.numpy(), cm_out.numpy()))\n\nif model_name in [\"gpt2-xl\"]:\n    print(\"\\n\ud83d\udc4b This model is big. It will run fast if you have a recent Mac with a fast GPU.\")\n    print(\"If not you can download a version that runs on the Neural Engine from the releases tab on GitHub.\")\n    print(\"If you want to build it yourself follow these steps:\")\n    print(\"1. Install coremltools >= 6.3\")\n    print(f\"2. Run: python -m src.experiments.chunk_model --mlpackage-path {model_filename}.mlpackage -o .\")\n    print(\"3. Edit src/experiments/make_pipeline.py to use the chunked files written by the above command.\")\n    print(\"4. Run: python -m src.experiments.make_pipeline\")\n    print(\"Use the output *-pipeline.mlpackage with generate.py as usual.\")"]}
{"filename": "generate.py", "chunked_list": ["from src.ml_ane_transformers.ane_gpt2 import GPT as AneGPT\nfrom src.utils.model_proxy import MLModelProxy\nfrom transformers import AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport coremltools as ct\nfrom stopwatch import Stopwatch\nfrom models.gpt2 import GPT as GPT2\nfrom models.pythia import GPT as Pythia", "from models.gpt2 import GPT as GPT2\nfrom models.pythia import GPT as Pythia\nimport argparse\nimport sys\nimport os\nimport glob\nfrom collections import OrderedDict\nimport subprocess\n\n\"\"\"", "\n\"\"\"\nLoad a CoreML model and use it to generate text.\n\"\"\"\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\ncompute_unit_by_name = OrderedDict([\n    (\"All\", ct.ComputeUnit.ALL),\n    (\"CPUOnly\", ct.ComputeUnit.CPU_ONLY),", "    (\"All\", ct.ComputeUnit.ALL),\n    (\"CPUOnly\", ct.ComputeUnit.CPU_ONLY),\n    (\"CPUAndGPU\", ct.ComputeUnit.CPU_AND_GPU),\n    (\"CPUAndANE\", ct.ComputeUnit.CPU_AND_NE),\n])\n\nparser = argparse.ArgumentParser(description='Load a CoreML modelpackage and generate some text.')\nparser.add_argument('--model_path', help='path to .mlpackage file', default=\"gpt2.mlpackage\", type=str)\nparser.add_argument('--input_prompt', help='input prompt for the model', default=\"Before boarding your rocket to Mars, remember to pack these items:\", type=str)\nparser.add_argument('--compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")", "parser.add_argument('--input_prompt', help='input prompt for the model', default=\"Before boarding your rocket to Mars, remember to pack these items:\", type=str)\nparser.add_argument('--compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")\nparser.add_argument('--length', help='number of new tokens to generate', type=int, default=40)\nparser.add_argument('--verbose', help='print verbose logs', type=bool, default=False)\nparser.add_argument('--wait', help='wait for confirmation before loading the model (ie to attach a debugger)', action=\"store_true\")\nparser.add_argument('--use-mlpackage', help='don\\'t automatically generate a mlmodelc and use it. dramatically slower but useful for debugging this script.', action=\"store_true\")\n\nargs = parser.parse_args()\n\nif not args.model_path.endswith('.mlpackage') and not args.model_path.endswith('.mlmodelc') :\n    print('Error: Model path must end in .mlpackage (or .mlmodelc if you know what you\\'re doing)')\n    sys.exit(1)", "\nif not args.model_path.endswith('.mlpackage') and not args.model_path.endswith('.mlmodelc') :\n    print('Error: Model path must end in .mlpackage (or .mlmodelc if you know what you\\'re doing)')\n    sys.exit(1)\n\n# Special handling for first-time run.\nif not os.path.exists(args.model_path) and args.model_path == \"gpt2.mlpackage\":\n    files = glob.glob('gpt2*.mlpackage')\n    files = sorted(files, key=lambda x: os.path.getmtime(x))\n    if len(files) == 0:\n        print(f\"Couldn't find {args.model_path}. Either use the --model_path argument or run convert.py to generate one.\")\n        sys.exit(1)\n    args.model_path = files[-1]", "\ncompute_unit = compute_unit_by_name[args.compute_unit]\n\ndef vprint(*pargs, **kwargs):\n    if args.verbose:\n        print(*pargs, **kwargs)\n\ndef get_tokenizer_name(model_path):\n    names = GPT2.model_names() + Pythia.model_names()\n    tokenizer_lookup = {**GPT2.tokenizer_by_name(), **Pythia.tokenizer_by_name()}\n    for n in sorted(names, key=len):\n        if model_path.startswith(n):\n            return tokenizer_lookup[n]\n    print(f\"No tokenizer found for {model_path}\")\n    print(f\"Model name must start with one of:\")\n    print(names)\n    return None", "\ntokenizer_name = get_tokenizer_name(args.model_path)\nif tokenizer_name is None:\n    sys.exit(1)\n\nvprint(\"Loading tokenizer...\")\ntok = AutoTokenizer.from_pretrained(tokenizer_name)\ntok.pad_token_id = tok.eos_token_id\nvprint(\"Loaded tokenizer.\")\n\nif args.wait:\n    print(f\"Current PID: {os.getpid()}\")\n    input(\"Waiting. Press Enter to continue.\")", "vprint(\"Loaded tokenizer.\")\n\nif args.wait:\n    print(f\"Current PID: {os.getpid()}\")\n    input(\"Waiting. Press Enter to continue.\")\n\n# Compile to make generations 2-n much much faster.\nbase_path = args.model_path.replace(\".mlpackage/\", \"\").replace(\".mlmodelc/\", \"\").replace(\".mlpackage\", \"\").replace(\".mlmodelc\", \"\")\nmlpackage_path = base_path + \".mlpackage\"\nmlmodelc_path = base_path + \".mlmodelc\"", "mlpackage_path = base_path + \".mlpackage\"\nmlmodelc_path = base_path + \".mlmodelc\"\nhas_compiled_model = os.path.exists(mlmodelc_path)\nif not has_compiled_model:\n    # Looking to turn this off? As far as I know it's not worth it.\n    # Generating text from a mlpackage does this same compilation every time (slow) and\n    # it doesn't get cached so you will actually use _more_ disk space without this.\n    # It's also much faster to load the model this way. For the xl model this will\n    # take model loading from 1.5 minutes to 2.5 seconds.\n    print(\"Compiling model. This first run will be slow but all subsequent runs will be significantly faster.\")\n    cmd = f\"xcrun coremlcompiler compile {mlpackage_path} .\"\n    compile_result = subprocess.run(cmd, shell=True)\n    has_compiled_model = compile_result.returncode == 0\n    if not has_compiled_model:\n        print(\"Failed to compile. Please open an issue (https://github.com/smpanaro/more-ane-transformers/issues) and include the following:\")\n        print(f\"code: {compile_result.returncode}\\nstdout: {compile_result.stdout}\\nstderr: {compile_result.stderr}\")\n        print(\"Predicting using the (slow) mlpackage method.\")", "\nif has_compiled_model and not os.path.exists(mlpackage_path):\n    # TODO: Dump metadata to disk instead so you can keep just the compiled model.\n    print(f\"No matching mlpackage found for {mlmodelc_path}. Can't predict without that.\")\n    print(f\"It should be at: {mlpackage_path}\")\n    sys.exit(1)\n\n# nano = NanoGPT.from_pretrained(\"gpt2\").eval()\nprint(f\"Loading model from path {mlmodelc_path if has_compiled_model else mlpackage_path} using {compute_unit}...\")\nload_stopwatch = Stopwatch(3)", "print(f\"Loading model from path {mlmodelc_path if has_compiled_model else mlpackage_path} using {compute_unit}...\")\nload_stopwatch = Stopwatch(3)\nmodel, model_with_metadata = None, None\nif has_compiled_model:\n    model = MLModelProxy(mlmodelc_path, compute_unit)\n    # So we can inspect and see what the inputs are.\n    model_with_metadata = ct.models.model.MLModel(mlpackage_path, compute_units=compute_unit, skip_model_load=True)\nelse:\n    model = ct.models.model.MLModel(mlpackage_path, compute_units=compute_unit)\n    model_with_metadata = model", "load_stopwatch.stop()\nprint(f\"Loaded model in {load_stopwatch}.\")\n# print(model)\n\ndef sample(logits, temperature=0.85, top_k=80):\n    if isinstance(logits, np.ndarray):\n        logits = torch.from_numpy(logits)\n    # pluck the logits at the final step and scale by desired temperature\n    logits = logits[:, -1, :] / temperature\n    # optionally crop the logits to only the top k options\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        logits[logits < v[:, [-1]]] = -float('Inf')\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return torch.multinomial(probs.squeeze(), num_samples=1)", "\ntext = args.input_prompt\ninputs = tok(text, return_tensors=\"pt\")\nvprint(\"Tokenized initial inputs:\", inputs[\"input_ids\"].shape)\nane_inputs = AneGPT.build_inputs(inputs['input_ids'], pad_to_length=512, pad_token_id=tok.pad_token_id)\nvprint(\"Generated initial inputs:\")\nvprint({k: v.shape for k,v in ane_inputs.items()})\nvprint({k: v.dtype for k,v in ane_inputs.items()})\n# vprint({k: v.__class__ for k,v in ane_inputs.items()})\n\ndef get_start_idx(ids):\n    ids = ids.tolist()[0]\n    if tok.pad_token_id in ids:\n        return ids.index(tok.pad_token_id)\n    return len(ids)", "# vprint({k: v.__class__ for k,v in ane_inputs.items()})\n\ndef get_start_idx(ids):\n    ids = ids.tolist()[0]\n    if tok.pad_token_id in ids:\n        return ids.index(tok.pad_token_id)\n    return len(ids)\n\ndef from_numpy(d):\n    return {k: torch.from_numpy(v) for k,v in d.items()}", "def from_numpy(d):\n    return {k: torch.from_numpy(v) for k,v in d.items()}\n\ndef without_pad(ids):\n    return ids[ids != tok.pad_token_id].unsqueeze(0)\n\nstopwatch = Stopwatch(3)\nstopwatch.stop()\nstopwatch.reset()\n", "stopwatch.reset()\n\nNUM_INFERENCES = args.length\n\ninput_keys = set([f.name for f in model_with_metadata.input_description._fd_spec])\n\nrelevant_tokens = without_pad(ane_inputs[\"input_ids\"])\nfor i in range(NUM_INFERENCES):\n    next_index = len(relevant_tokens[0]) - 1\n    ane_inputs = AneGPT.build_inputs(relevant_tokens, pad_to_length=512, pad_token_id=tok.pad_token_id)\n    ane_inputs = {k:v for k,v in ane_inputs.items() if k in input_keys}\n\n    # attention_mask = ane_inputs[\"k_mask\"].squeeze().unsqueeze(0)\n    # print(attention_mask.shape)\n    stopwatch.start()\n    # Hanging here? It's very likely your intputs are the wrong shape and/or types.\n    logits = model.predict(ane_inputs)[\"logits\"] # nano\n    # logits = nano(ane_inputs[\"input_ids\"], attention_mask)\n    stopwatch.stop()\n\n    # If the model does not pre-select the next token logits, do so now.\n    if logits.shape[1] > 1:\n        logits = logits[:, [next_index], :]\n\n    ane_next = sample(logits) #ane_inputs['input_ids'], qk_mask=ane_inputs['qk_mask']))\n\n    # Helpful for debugging nonsense generations.\n    # print(torch.topk(torch.from_numpy(logits), 20, dim=-1).indices[:, :20, :])\n    # print(\"chose\", ane_next, \"from idx:\", next_index)\n\n    relevant_tokens = torch.cat((relevant_tokens.squeeze(), torch.tensor([ane_next]))).unsqueeze(0)\n    if i == 0:\n        print(f\"\\n\\033[95m[Prompt] {tok.decode(relevant_tokens.squeeze())}\\033[0m\", end=\"\")\n    else:\n        print(tok.decode(ane_next), end=\"\")\n    sys.stdout.flush()", "\nprint(\"\\n\\n---stats---\")\nper_inference = \"{:.{}f}ms\".format((stopwatch.duration / NUM_INFERENCES) * 1000, 2)\nprint(\"Compute Unit:\", args.compute_unit)\nprint(stopwatch, \"total\")\nprint(f\"{per_inference}/it\")"]}
{"filename": "models/gpt2.py", "chunked_list": ["\"\"\"\nOriginally this was going to be nanoGPT with the bare minimum modifications required\nso that it can be converted to CoreML and compared to the ANE implementation. Turns\nout it's quite fast out of the box, so tweaked it further to gain more speed.\nFrom: https://github.com/karpathy/nanoGPT/blob/a82b33b525ca9855d705656387698e13eb8e8d4b/model.py#L1\n\nSee \"ANE:\" comments for interesting things.\n\nOriginal License:\nMIT License", "Original License:\nMIT License\n\nCopyright (c) 2022 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is", "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE", "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nimport math\nimport inspect", "import math\nimport inspect\nfrom dataclasses import dataclass\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n@dataclass\nclass CacheConfig:\n    kv_cache: torch.Tensor # [num_layers, 1, 2*seqlen, n_embd]\n    kv_mask: torch.BoolTensor # for masking oldk/oldv [1, seqlen, n_embd], can't build in the model (see comments in Attention class)\n    output_mask: torch.Tensor # [1]\n    head_index: int", "\n@dataclass\nclass CacheConfig:\n    kv_cache: torch.Tensor # [num_layers, 1, 2*seqlen, n_embd]\n    kv_mask: torch.BoolTensor # for masking oldk/oldv [1, seqlen, n_embd], can't build in the model (see comments in Attention class)\n    output_mask: torch.Tensor # [1]\n    head_index: int\n\n# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))", "# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "class LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        # Don't need this for ANE.\n        # if not self.flash:\n        #     print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            # self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size, dtype=torch.float16))\n            #                             .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x, attention_mask, kv_config):\n        # kv_cache (B, T*2, C)\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n\n        if kv_config is not None and kv_config.kv_cache is not None:\n            kv_cache = kv_config.kv_cache\n            kv_mask = kv_config.kv_mask\n\n            new_q, new_k, new_v = q,k,v # new_q, new_k, new_v = [B, 1, n_embd]\n            old_k, old_v = kv_cache.chunk(2, dim=1) # each (B, T, C)\n            # replace the row indicated by output_mask with the new values\n\n            # Many ways to overwrite a row in a matrix (old K) with a new value (new K).\n            # One of them works on ANE.\n\n            # op not supported\n            # k = torch.index_copy(old_k, 1, output_mask, new_k)\n            # v = torch.index_copy(old_v, 1, output_mask, new_v)\n\n            # https://yuyangyy.medium.com/understand-torch-scatter-b0fd6275331c\n            # oldk.scatter(1, torch.tensor([3]).repeat(4).unsqueeze(0).unsqueeze(0), newk) # something like this\n            # scatter doesn't run on ANE though :(\n            # idx = output_mask.repeat(old_k.shape[2]).unsqueeze(0).unsqueeze(0)\n            # k = old_k.scatter(1, idx, new_k)\n            # v = old_v.scatter(1, idx, new_v)\n\n            # Like index_copy but supported by coremltools.\n            # Problem is it creates a symbolic variable (the :output_mask slicing) which technically\n            # has a different type than the non-cached case, so can't be used in a branched if/else model.\n            # Also not sure this is numerically accurate.\n            # And also doesn't seem to run on the ANE.\n            # k = torch.cat([old_k[:, :output_mask, :], new_k, old_k[:, output_mask+1:, :]], dim=1)\n            # v = torch.cat([old_v[:, :output_mask, :], new_v, old_v[:, output_mask+1:, :]], dim=1)\n\n            # Fails in index_put AttributeError: 'NoneType' object has no attribute 'sym_type' (the first colon)\n            # k = old_k\n            # k[:, output_mask] = new_k\n            # v = old_v\n            # v[:, output_mask] = new_v\n\n            # Equivalent to cat dim=1, same problems.\n            # k = torch.hstack([old_k[:, :output_mask, :], new_k, old_k[:, output_mask+1:, :]])\n            # v = torch.hstack([old_v[:, :output_mask, :], new_v, old_v[:, output_mask+1:, :]])\n\n            # Fails in index_put AttributeError: 'NoneType' object has no attribute 'sym_type' (the first colon)\n            # This would use scatter_nd under the hood anyways which is not ANE compatible.\n            # mask = torch.zeros_like(old_k)\n            # mask[:, output_mask, :] = 1\n            # k = torch.where(mask.bool(), new_k, old_k)\n            # v = torch.where(mask.bool(), new_v, old_v)\n\n            # This works, but you have to pass the mask along (on the plus side, you only compute it once).\n            k = torch.where(kv_mask, old_k, new_k)\n            v = torch.where(kv_mask, old_v, new_v)\n            q = new_q\n\n            B,T,C = k.size()\n\n        current_cache = torch.cat([k,v], dim=1)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, q.size()[1], self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            # ANE: Using a stored bias makes the model file larger (from a little to a lot)\n            # because it is copied into the model proto for every usage. Additionally, mask\n            # fill only runs on the CPU and moving from ANE <-> CPU is sloooow.\n            # Instead follow the approach from ml-ane-transformers, subtract a large but\n            # float16-friendly value so that masked values are effectively ignored in the softmax.\n            # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-1e4'))\n            att = att + attention_mask\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, -1, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y, current_cache", "\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x, attention_mask=None, kv_config=None):\n        attention_output, new_kv_cache = self.attn(self.ln_1(x), attention_mask, kv_config)\n        x = x + attention_output\n        x = x + self.mlp(self.ln_2(x))\n        return x, new_kv_cache", "\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster", "\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, output_mask=None, kv_cache=None, kv_mask=None, seqlen=512):\n        assert kv_cache is None or idx.shape[1] == 1, f\"kv cache is only supported for single token inputs not {idx.shape}\"\n        return_kv_cache = kv_cache is not None\n\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # ANE: Since we are only inferring and we only care about predicting the next token,\n        # we can use the same triangular mask always. May need to change this to support flexible sizes.\n        attention_mask = (1 - torch.tril(torch.ones((1,1,seqlen,seqlen), dtype=torch.float32))) * -1e4\n\n        # kv_cache [# layers, batch size 1, seqlen * 2 = 512*2, hidden size 768]\n        if kv_cache is None:\n            kv_cache = [None]*len(self.transformer.h)\n        else:\n            pos = output_mask.unsqueeze(0)\n            attention_mask = torch.index_select(attention_mask, 2, output_mask)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n\n        kv_mask_bool = kv_mask.bool() if kv_mask is not None else None\n        new_kv_cache = []\n        head_index = 0\n        for (block, cache) in zip(self.transformer.h, kv_cache):\n            kv_config = CacheConfig(cache, kv_mask_bool, output_mask, head_index)\n            x, block_new_kv_cache = block(x, attention_mask, kv_config)\n            new_kv_cache.append(block_new_kv_cache)\n            head_index += 1\n\n        # No need to compute anything else for the other length-1 tokens.\n        # This shaves ~30% off of gpt2-xl when running on CPU+ANE.\n        if output_mask is not None and t > 1:\n            x = torch.index_select(x, 1, output_mask)\n\n        x = self.transformer.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        # [# layers, batch size 1, seqlen * 2 = 512*2, hidden size 768]\n        # same as input\n        new_kv_cache = torch.stack(new_kv_cache)\n\n        if return_kv_cache:\n            return logits, new_kv_cache\n\n        return logits\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @staticmethod\n    def config_args():\n        # ckiplab/gpt2-tiny-chinese is a good tiny model for experimenting, vocab size 21128\n        return OrderedDict({\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        })\n\n    @staticmethod\n    def model_names():\n        return list(GPT.config_args().keys())\n\n    @staticmethod\n    def tokenizer_by_name():\n        return {n:n for n in GPT.model_names()}\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in GPT.model_names()\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = GPT.config_args()[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx", "\nif __name__ == \"__main__\":\n    import numpy as np\n\n    def build_kv_mask(output_mask, seqlen=512, hidden_size=768):\n        kv_mask = torch.ones(1, seqlen, hidden_size, dtype=torch.int32)\n        kv_mask[:, output_mask, :] = 0\n        return kv_mask\n\n    model = GPT.from_pretrained(\"gpt2\").eval()\n\n    # Check numeric accuracy before converting to CoreML. It's not exact, might be a bug.\n    # if True:\n    if False:\n        input_ids = torch.randint(10_000, (1, 10,))\n\n        with torch.no_grad():\n            base_prediction, _ = model(input_ids, torch.tensor([4]), seqlen=10)\n            print(\"^base ------ first v\")\n\n            out, out_cache = model(input_ids, torch.tensor([3]), seqlen=10)\n\n        print(\"output cache:\", out_cache.shape)\n\n        print(\"----\")\n\n        with torch.no_grad():\n            out_new, out_new_cache = model(input_ids[:,[4]], torch.tensor([4]), out_cache, seqlen=10)\n\n        print(base_prediction[:, 0:6, 0:4])\n        print(out[:, 0, 0:4])\n        print(out_new[:, 0, 0:4])\n        print(\"cache sizes\", out_cache.shape, out_new_cache.shape)\n        print(\"eq?\", torch.equal(out_new[:, 0, :], base_prediction[:, 0, :])) # why aren't these equal?\n        print(\"close?\")\n        np.testing.assert_allclose(out_new[:, 0, :], base_prediction[:, 0 ,:])\n\n    # Work out the pattern what inputs to pass when.\n    # if True:\n    if False:\n        from transformers import AutoTokenizer\n        tok = AutoTokenizer.from_pretrained(\"gpt2\")\n        seq = tok(\"would the real slim\", return_tensors=\"pt\")[\"input_ids\"]\n        input_ids = torch.cat([\n            seq.squeeze(),\n            torch.full((20 - seq.shape[-1],), tok.eos_token_id)\n        ]).unsqueeze(0)\n\n        kvc = None\n        outs = []\n        with torch.no_grad():\n            for i in range(5):\n                seqlen = seq.shape[-1]+i\n                print(\"i\", i, seqlen)\n                inputs = input_ids if kvc is None else input_ids[:, [seqlen-1]]\n                output_mask = torch.tensor([seqlen-1])\n                kv_mask = build_kv_mask(output_mask, seqlen=20, hidden_size=768)\n                out, kvc = model(inputs, output_mask=output_mask, kv_cache=kvc, kv_mask=kv_mask, seqlen=20)\n                input_ids[0][seqlen] = out.argmax()\n                outs.append(out)\n\n        print(tok.decode(input_ids.squeeze().tolist()))\n\n    # Try to build a branching model that can be converted to CoreML.\n    # Doesn't run on the Neural Engine and includes duplicate copies of the weights.\n    # Would also be difficult/impossible to split into a pipeline for gpt2-xl.\n    if True:\n    # if False:\n        import coremltools as ct\n\n        class DoubleGPT(nn.Module):\n            def __init__(self, model_name, seqlen, num_layers, hidden_size):\n                super().__init__()\n                self.cached = GPT.from_pretrained(model_name)\n                self.full = self.cached\n                input_ids = torch.randint(10_000, (1, seqlen,))\n                kv_cache = torch.zeros(num_layers, 1, seqlen*2,hidden_size)\n                kv_mask = torch.zeros(1, seqlen, hidden_size)\n                self.cached = torch.jit.trace(self.cached, (input_ids[:, [3]], torch.tensor([3]), kv_cache, kv_mask))\n                self.full = torch.jit.trace(self.full, (input_ids, torch.tensor([3])))\n\n            def forward(self, x, output_mask, kv_cache, kv_mask):\n                if kv_cache.min() != 0:\n                    ci = torch.index_select(x, 1, output_mask)\n                    y, new_cache = self.cached(ci, output_mask, kv_cache, kv_mask)\n                else:\n                    y, new_cache = self.full(x, output_mask)\n\n                return y, new_cache\n\n        seqlen = 512\n\n        # model_name = \"gpt2\"\n        # num_layers = 12\n        # hidden_size = 768\n\n        model_name = \"gpt2-medium\"\n        num_layers = 24\n        hidden_size = 1024\n\n        input_ids = torch.randint(10_000, (1, seqlen,))\n        # traced_model = torch.jit.trace(model, (input_ids, torch.tensor([3]), torch.zeros(num_layers, 1, seqlen*2,hidden_size)))\n        with torch.no_grad():\n            double = DoubleGPT(model_name, seqlen, num_layers, hidden_size).eval()\n            # double(input_ids, torch.tensor([3]), torch.ones(num_layers, 1, seqlen*2,hidden_size))\n\n            kv_cache_shape = (num_layers, 1, seqlen*2,hidden_size)\n            kv_mask = torch.ones(1, seqlen, hidden_size)\n\n            # Trace just the cached model.\n            traced_cached_model = torch.jit.trace(double, (input_ids, torch.tensor([3]), torch.ones(kv_cache_shape), kv_mask))\n\n            # Trace just the full model.\n            traced_full_model = torch.jit.trace(double, (input_ids, torch.tensor([3]), torch.zeros(kv_cache_shape), kv_mask))\n\n            # Script both models.\n            # traced_model = torch.jit.script(double)\n\n\n        def convert_model(traced_model, name):\n            print(traced_model.code)\n            prog = ct.convert(traced_model,\n                inputs=[\n                    ct.TensorType(name=\"input_ids\", shape=[1, seqlen], dtype=np.int32),\n                    ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n                    ct.TensorType(name=\"kv_cache\", shape=[num_layers, 1, seqlen*2, hidden_size], dtype=np.float32),\n                    ct.TensorType(name=\"kv_mask\", shape=[1, seqlen, hidden_size], dtype=np.int32),\n                ],\n                outputs=[\n                    ct.TensorType(name=\"logits\", dtype=np.float32),\n                    ct.TensorType(name=\"new_kv_cache\", dtype=np.float32),\n                ],\n                minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n                # compute_units=ct.ComputeUnit.CPU_AND_GPU if \"full\" in name else ct.ComputeUnit.CPU_AND_NE,\n                compute_precision=ct.precision.FLOAT32,\n                convert_to=\"milinternal\")\n\n            # print(prog)\n\n            mlmodel = ct.convert(prog,\n                                minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n                                convert_to=\"mlprogram\")\n\n            # print(mlmodel.get_spec().description.input)\n            # spec = mlmodel.get_spec()\n            # spec.description.input[2].type.isOptional = True\n            # mlmodel._spec = spec\n            # print(mlmodel.get_spec().description.input)\n\n            # mlmodel.save(f\"{name}.mlpackage\")\n            return mlmodel\n\n        cached_mlmodel = convert_model(traced_cached_model, \"gpt2kv-cached\")\n        full_mlmodel = convert_model(traced_full_model, \"gpt2kv-full\")\n        # convert_model(traced_model, \"gpt2kv\")\n\n        from transformers import AutoTokenizer\n        tok = AutoTokenizer.from_pretrained(\"gpt2\")\n        seq = tok(\"would the real slim\", return_tensors=\"pt\")[\"input_ids\"]\n        input_ids = torch.cat([\n            seq.squeeze(),\n            torch.full((seqlen - seq.shape[-1],), tok.eos_token_id)\n        ]).unsqueeze(0)\n        original_inputs = {\n            \"input_ids\": input_ids.int().numpy(),\n            \"output_mask\": torch.tensor([seq.shape[1]-1]).int().numpy(),\n            \"kv_cache\": torch.zeros((num_layers, 1, 2*seqlen, hidden_size)).float().numpy(),\n        }\n        original_inputs[\"kv_mask\"] = build_kv_mask(original_inputs[\"output_mask\"], seqlen=512, hidden_size=hidden_size)\n        original_outputs = full_mlmodel.predict(original_inputs)\n\n        # input(\"Press Enter to continue.\")\n\n        from stopwatch import Stopwatch\n        stopwatch = Stopwatch(3)\n        stopwatch.stop()\n        stopwatch.reset()\n\n        input_stopwatch = Stopwatch(3)\n        input_stopwatch.stop()\n        input_stopwatch.reset()\n\n        # print(\"input_ids\", input_ids)\n        # print(\"output_mask\", original_inputs[\"output_mask\"])\n        outputs = original_outputs\n        num_inferences = 0\n        for i in range(min(seqlen, 200) - seq.shape[1]):\n            input_stopwatch.start()\n            input_ids[0, seq.shape[1]+i] = outputs[\"logits\"].argmax()\n            inputs = {\n                \"input_ids\": input_ids.int().numpy(),\n                \"output_mask\": torch.tensor([seq.shape[1]+i]).int().numpy(),\n                \"kv_cache\": outputs[\"new_kv_cache\"], # already numpy floats (from CoreML)\n            }\n            inputs[\"kv_mask\"] = build_kv_mask(inputs[\"output_mask\"], seqlen=512, hidden_size=hidden_size) # probably cheating to not include this in timing...\n            input_stopwatch.stop()\n            # print(\"input_ids\", input_ids)\n            # print(\"output_mask\", inputs[\"output_mask\"])\n\n            stopwatch.start()\n            outputs = cached_mlmodel.predict(inputs)\n            # outputs = full_mlmodel.predict(inputs)\n            stopwatch.stop()\n\n            num_inferences += 1\n\n        input_ids[0, seq.shape[1]+i] = outputs[\"logits\"].argmax()\n        print(\"final input_ids\", input_ids)\n        print(tok.decode(input_ids[0, :]))\n\n        print(\"\\n\\n---stats---\")\n        per_inference = \"{:.{}f}ms\".format((stopwatch.duration / num_inferences) * 1000, 2)\n        print(stopwatch, \"total\")\n        print(f\"{per_inference}/it\")\n\n        input_per_inference = \"{:.{}f}ms\".format((input_stopwatch.duration / num_inferences) * 1000, 2)\n        print(f\"input prep {input_per_inference}/it\")", "\n\n        # print(prog)\n"]}
{"filename": "models/pythia.py", "chunked_list": ["\"\"\"\nnanoGPT + huggingface inspired-implementation of the pythia models from EleutherAI.\nThis includes neox.\n\nPlease read the 'Out-of-scope use' section on the Pythia huggingface pages. Notably:\n\"The Pythia Suite is not intended for deployment. It is not a in itself a product and\ncannot be used for human-facing interactions.\"\n\n\nHybrid of HuggingFace implementation and nanoGPT.", "\nHybrid of HuggingFace implementation and nanoGPT.\nHuggingface License:\nCopyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0", "\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nnanoGPT License:", "\nnanoGPT License:\nMIT License\n\nCopyright (c) 2022 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell", "in the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,", "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nimport math", "\nimport math\nimport inspect\nfrom dataclasses import dataclass\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "from torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.hidden_size % config.n_head == 0\n        self.n_head = config.n_head\n        self.hidden_size = config.hidden_size\n        self.head_size = self.hidden_size // self.n_head\n        self.rotary_ndims = int(self.head_size * config.rotary_pct)\n        self.rotary_emb = RotaryEmbedding(\n            self.rotary_ndims, config.max_position_embeddings, base=config.rotary_emb_base\n        )\n        self.norm_factor = torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32)).to(torch.get_default_dtype())\n        self.query_key_value = nn.Linear(config.hidden_size, 3 * config.hidden_size)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n\n    @classmethod\n    def _split_heads(cls, tensor, num_attention_heads, attn_head_size):\n        \"\"\"\n        Splits hidden dim into attn_head_size and num_attention_heads\n        \"\"\"\n        # tensor: [bs, seq_len, hidden_size]\n        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)\n        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n        tensor = tensor.view(new_shape)\n        # -> [bs, num_attention_heads, seq_len, attn_head_size]\n        tensor = tensor.permute(0, 2, 1, 3)\n        return tensor\n\n    @classmethod\n    def _merge_heads(cls, tensor, num_attention_heads, attn_head_size):\n        \"\"\"\n        Merges attn_head_size dim and num_attn_heads dim into hidden dim\n        \"\"\"\n        # tensor [bs, num_attention_heads, seq_len, attn_head_size]\n        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n        # -> [bs, seq_len, num_attention_heads, attn_head_size]\n        tensor = tensor.view(tensor.size(0), tensor.size(1), num_attention_heads * attn_head_size)\n        # -> [bs, seq_len, hidden_size]\n        return tensor\n\n    def forward(self, x, position_ids, attention_mask):\n        # Compute QKV\n        # Attention heads [batch, seq_len, hidden_size]\n        #   --> [batch, seq_len, (np * 3 * head_size)]\n        qkv = self.query_key_value(x)\n        # assert qkv.shape == torch.Size([1, 11, 512*3]), qkv.shape\n\n        # [batch, seq_len, (num_heads * 3 * head_size)]\n        #   --> [batch, seq_len, num_heads, 3 * head_size]\n        new_qkv_shape = qkv.size()[:-1] + (self.n_head, 3 * self.head_size)\n        qkv = qkv.view(*new_qkv_shape)\n        # assert qkv.shape == torch.Size([1, 11, 8, (512//8)*3]), qkv.shape\n\n        # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n        query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n        key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n        value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n\n        # Compute rotary embeddings on rotary_ndims\n        query_rot = query[..., : self.rotary_ndims]\n        query_pass = query[..., self.rotary_ndims :]\n        key_rot = key[..., : self.rotary_ndims]\n        key_pass = key[..., self.rotary_ndims :]\n\n        # Compute token offset for rotary embeddings (when decoding)\n        seq_len = key.shape[-2]\n        cos, sin = self.rotary_emb(value, seq_len=seq_len)\n        query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n        query = torch.cat((query, query_pass), dim=-1)\n        key = torch.cat((key, key_pass), dim=-1)\n\n        # Compute attention\n        attn_output, _ = self._attn(query, key, value, attention_mask)\n\n        # Reshape outputs\n        attn_output = self._merge_heads(attn_output, self.n_head, self.head_size)\n        attn_output = self.dense(attn_output)\n\n        return attn_output\n\n    def _attn(self, query, key, value, attention_mask=None):\n        # q, k, v: [bs, num_attention_heads, seq_len, attn_head_size]\n        # compute causal mask from causal mask buffer\n        batch_size, num_attention_heads, query_length, attn_head_size = query.size()\n        key_length = key.size(-2)\n\n        # causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n\n        query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)\n        key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)\n        attn_scores = torch.zeros(\n            batch_size * num_attention_heads,\n            query_length,\n            key_length,\n            dtype=query.dtype,\n            device=key.device,\n        )\n        attn_scores = torch.baddbmm(\n            attn_scores,\n            query,\n            key.transpose(1, 2),\n            beta=1.0,\n            alpha=(torch.tensor(1.0, dtype=self.norm_factor.dtype, device=self.norm_factor.device) / self.norm_factor),\n        )\n        attn_scores = attn_scores.view(batch_size, num_attention_heads, query_length, key_length)\n\n        if attention_mask is not None:\n            # Apply the attention mask\n            attn_scores = attn_scores + attention_mask\n\n        attn_weights = nn.functional.softmax(attn_scores, dim=-1)\n        attn_weights = attn_weights.to(value.dtype)\n\n        attn_output = torch.matmul(attn_weights, value)\n        return attn_output, attn_weights", "\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.cos_cached = emb.cos()[None, None, :, :]\n        self.sin_cached = emb.sin()[None, None, :, :]\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, n_head, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.cos_cached = emb.cos()[None, None, :, :]\n            self.sin_cached = emb.sin()[None, None, :, :]\n        return self.cos_cached[:seq_len, ...].to(x.device), self.sin_cached[:seq_len, ...].to(x.device)", "\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.cos_cached = emb.cos()[None, None, :, :]\n        self.sin_cached = emb.sin()[None, None, :, :]\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, n_head, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.cos_cached = emb.cos()[None, None, :, :]\n            self.sin_cached = emb.sin()[None, None, :, :]\n        return self.cos_cached[:seq_len, ...].to(x.device), self.sin_cached[:seq_len, ...].to(x.device)", "\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense_h_to_4h = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.dense_4h_to_h = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.act = nn.GELU() # pretty sure this is the default: approximate=False\n\n    def forward(self, x):\n        x = self.dense_h_to_4h(x)\n        x = self.act(x)\n        x = self.dense_4h_to_h(x)\n        return x", "\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.attention = CausalSelfAttention(config)\n        self.mlp = MLP(config)\n\n    def forward(self, x, position_ids, attention_mask=None):\n        # Computed in parallel.\n        attn_out = self.attention(self.input_layernorm(x), position_ids, attention_mask)\n        mlp_output = self.mlp(self.post_attention_layernorm(x))\n        x = mlp_output + attn_out + x # the order matters...\n        return x", "\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # Almost all pythia models have the same vocab size.\n    n_layer: int = 12\n    n_head: int = 12\n    hidden_size: int = 512\n    intermediate_size: int = 2048\n    max_position_embeddings: int = 2048\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n    layer_norm_eps: float = 1e-05\n    rotary_pct: float = 0.25\n    rotary_emb_base: int = 10000", "\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.embed_in = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, input_ids, output_mask=None):\n        device = input_ids.device\n        b, t = input_ids.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        position_ids = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # ANE: Since we are only inferring and we only care about predicting the next token,\n        # we can use the same triangular mask always. May need to change this to support flexible sizes.\n        attention_mask = (1 - torch.tril(torch.ones((1,1,t,t), dtype=torch.float16))) * -1e4\n\n        inputs_embeds = self.embed_in(input_ids)\n        x = inputs_embeds\n\n        # forward the GPT model itself\n        for l in self.layers:\n            x = l(x, position_ids, attention_mask)\n\n        # No need to compute anything else for the other length-1 tokens.\n        # This shaves ~30% off of gpt2-xl when running on CPU+ANE.\n        if output_mask is not None:\n            x = torch.index_select(x, 1, output_mask)\n\n        x = self.final_layer_norm(x)\n\n        logits = self.lm_head(x)\n\n        return logits\n\n    @staticmethod\n    def config_args():\n        return OrderedDict({\n            'pythia-70m':      dict(n_layer=6,  n_head=8,  hidden_size=512,  intermediate_size=2048),\n            'pythia-160m':     dict(n_layer=12, n_head=12, hidden_size=768,  intermediate_size=3072),\n            'pythia-410m':     dict(n_layer=24, n_head=16, hidden_size=1024, intermediate_size=4096),\n            'pythia-1b':       dict(n_layer=16, n_head=8,  hidden_size=2048, intermediate_size=8192),\n            'pythia-1.4b':     dict(n_layer=24, n_head=16, hidden_size=2048, intermediate_size=8192),\n            'pythia-2.8b':     dict(n_layer=32, n_head=32, hidden_size=2560, intermediate_size=10240),\n            'pythia-6.9b':     dict(n_layer=32, n_head=32, hidden_size=4096, intermediate_size=16384, vocab_size=50432),\n        })\n\n    @staticmethod\n    def model_names():\n        return list(GPT.config_args().keys())\n\n    @staticmethod\n    def tokenizer_by_name():\n        return {n:f\"EleutherAI/{n}\" for n in GPT.model_names()}\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        model_type = model_type.replace('EleutherAI/', '')\n        assert model_type in GPT.model_names()\n        model_type = 'EleutherAI/' + model_type\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPTNeoXForCausalLM\n        print(\"loading weights from pretrained model: %s\" % model_type)\n\n        # n_layer, n_head and hidden_size are determined from model_type\n        config_args = GPT.config_args()[model_type.replace('EleutherAI/', '')]\n\n        # create a from-scratch initialized model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys]\n\n        # init a huggingface/transformers model\n        model_hf = GPTNeoXForCausalLM.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n        for k in list(sd_hf.keys()):\n            if k.startswith(\"gpt_neox.\"):\n                newk = k.replace(\"gpt_neox.\", \"\")\n                assert newk not in sd_hf\n                sd_hf[newk] = sd_hf.pop(k)\n\n        for k in list(sd_hf.keys()):\n            if k.startswith(\"embed_out.\"):\n                newk = k.replace(\"embed_out.\", \"lm_head.\")\n                sd_hf[newk] = sd_hf.pop(k)\n            elif k.endswith(\".masked_bias\"):\n                del sd_hf[k]\n            elif k.endswith(\".attention.bias\"): #FIXME what is this\n                del sd_hf[k]\n\n        in_hf = set(sd_hf.keys()) - set(sd_keys)\n        missing_hf = set(sd_keys) - set(sd_hf.keys())\n        if len(in_hf) != 0 or len(missing_hf) != 0:\n            print([x for x in in_hf if \"\" in x])\n            print([x for x in missing_hf if \"\" in x])\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            assert sd_hf[k].shape == sd[k].shape, f\"{k}: {sd_hf[k].shape} != {sd[k].shape}\"\n            with torch.no_grad():\n                sd[k].copy_(sd_hf[k])\n\n        return model", "\nif __name__ == \"__main__\":\n    import argparse\n    from transformers import GPTNeoXForCausalLM, AutoTokenizer\n    from src.utils.psnr import compute_psnr\n    parser = argparse.ArgumentParser(description='Convert a model to CoreML.')\n    parser.add_argument('--model_name', choices=GPT.model_names(), default=\"pythia-160m\", type=str)\n    args = parser.parse_args()\n\n    model_name = 'EleutherAI/' + args.model_name\n\n    nano = GPT.from_pretrained(model_name).eval()\n    hf = GPTNeoXForCausalLM.from_pretrained(model_name, use_cache=False).eval()\n\n    tok = AutoTokenizer.from_pretrained(model_name)\n\n    inputs = tok(\"this washed coffee comes from huila, colombia\", return_tensors=\"pt\")\n    with torch.no_grad():\n        # print(inputs)\n        # inputs = torch.rand((1,1,512), requires_grad=False)\n        # position_ids = torch.arange(0, inputs.shape[1], dtype=torch.long).unsqueeze(0) # shape (1, t)\n        # attention_mask = (1 - torch.tril(torch.ones((1,1,inputs.shape[1],inputs.shape[1]), dtype=torch.float16))) * -1e4\n        # attention_mask = (1 - torch.tril(torch.ones((1,1,inputs.shape[1],inputs.shape[1]), dtype=torch.float32))) * -1e9\n        # hf_out_a = hf.gpt_neox.layers[0].attention(hf.gpt_neox.layers[0].input_layernorm(inputs), None)[0]\n        # nano_out_a = nano.layers[0].attention(nano.layers[0].input_layernorm(inputs), position_ids, attention_mask)\n        # assert torch.equal(hf_out_a, nano_out_a), \"zzz\"\n        # hf_out_mlp = hf.gpt_neox.layers[0].mlp(hf.gpt_neox.layers[0].post_attention_layernorm(inputs))\n        # nano_out_mlp = nano.layers[0].mlp(nano.layers[0].post_attention_layernorm(inputs))\n        # assert torch.equal(hf_out_mlp, nano_out_mlp), \"lkjlkjlkj\"\n        # hf_out = hf_out_mlp + hf_out_a + inputs\n        # nano_out = nano_out_mlp + nano_out_a + inputs\n        # assert torch.equal(hf_out, nano_out), \"wqwwqwqwq\"\n\n        # hf_out_l = hf.gpt_neox.layers[0](inputs, None)[0]\n        # nano_out_l = nano.layers[0](inputs, position_ids, attention_mask)\n        # wtf = nano_out_a + nano_out_mlp + inputs\n        # print(wtf - nano_out_l)\n        # assert torch.equal(hf_out_l, hf_out)\n        # assert torch.equal(wtf, nano_out_l)\n        # assert torch.equal(hf_out_l, nano_out_l)\n\n\n        inputs = {k:v for k,v in inputs.items() if k in [\"input_ids\"]}\n        hf_out = hf(**inputs)['logits']\n        nano_out = nano(**inputs)\n        # nano = nano.to(device=\"mps\",dtype=torch.float16)\n        # inputs = {k: v.to(device=\"mps\") for k, v in inputs.items()}\n        # nano_out = nano(**inputs).cpu().float()\n\n    assert hf_out.shape == nano_out.shape, f\"{hf_out.shape} != {nano_out.shape}\"\n    # psnr should be ~240 if perfect.\n    print(\"psnr:\", compute_psnr(hf_out, nano_out))\n    print(\"eq\", torch.equal(hf_out, nano_out))", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/utils/psnr.py", "chunked_list": ["import numpy as np\nimport torch\n\ndef compute_psnr(a, b):\n    \"\"\"\n    Compute Peak-Signal-to-Noise-Ratio across two numpy.ndarray objects\n    a: the approximation\n    b: the ground truth\n\n    From ml-ane-transformers.\n\n    Original License:\n    Copyright (C) 2022 Apple Inc. All Rights Reserved.\n\n    IMPORTANT: This Apple software is supplied to you by Apple Inc. (\"Apple\") in consideration of your agreement to the following terms, and your use, installation, modification or redistribution of this Apple software constitutes acceptance of these terms. If you do not agree with these terms, please do not use, install, modify or redistribute this Apple software.\n\n    In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the \"Apple Software\"), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.\n\n    The Apple Software is provided by Apple on an \"AS IS\" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\n    IN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n    \"\"\"\n    if torch.is_tensor(a):\n        a = a.numpy()\n    if torch.is_tensor(b):\n        b = b.numpy()\n\n    max_b = np.abs(b).max()\n    sumdeltasq = 0.0\n\n    sumdeltasq = ((a - b) * (a - b)).sum()\n\n    sumdeltasq /= b.size\n    sumdeltasq = np.sqrt(sumdeltasq)\n\n    eps = 1e-5\n    eps2 = 1e-10\n    psnr = 20 * np.log10((max_b + eps) / (sumdeltasq + eps2))\n\n    return psnr"]}
{"filename": "src/utils/model_proxy.py", "chunked_list": ["import torch\nimport numpy as np\n\nclass MLModelProxy:\n    \"\"\"Just the fun bits from coremltools that allows us to use pre-compiled models.\"\"\"\n    def __init__(self, model_path, compute_unit):\n        from coremltools.libcoremlpython import _MLModelProxy\n        self.__proxy__ = _MLModelProxy(model_path, compute_unit.name)\n\n    def _update_float16_multiarray_input_to_float32(self, input_data):\n        for k, v in input_data.items():\n            if isinstance(v, np.ndarray) and v.dtype == np.float16:\n                input_data[k] = v.astype(np.float32)\n\n    def _convert_tensor_to_numpy(self, input_dict):\n        _HAS_TORCH = True\n        def convert(given_input):\n            if isinstance(given_input, np.ndarray):\n                sanitized_input = given_input\n            elif _HAS_TORCH and isinstance(given_input, torch.Tensor):\n                sanitized_input = given_input.detach().numpy()\n            # elif (_HAS_TF_1 or _HAS_TF_2) and isinstance(given_input, _tf.Tensor):\n            #     sanitized_input = given_input.eval(session=_tf.compat.v1.Session())\n            else:\n                sanitized_input = np.array(given_input)\n            return sanitized_input\n\n        model_input_to_types = {}\n        # Don't pass bad input!\n        # TODO: Steal a spec out of the mlpackage.\n        # for inp in self._spec.description.input:\n        #     type_value = inp.type.multiArrayType.dataType\n        #     type_name = inp.type.multiArrayType.ArrayDataType.Name(type_value)\n        #     if type_name != \"INVALID_ARRAY_DATA_TYPE\":\n        #         model_input_to_types[inp.name] = type_name\n\n        for given_input_name, given_input in input_dict.items():\n            # if not given_input_name in model_input_to_types:\n            #     continue\n            input_dict[given_input_name] = convert(given_input)\n\n    def predict(self, data):\n        \"\"\"\n        You are responsible for not passing bad input! If you're not sure and the\n        model seems to be hanging, try running with a mlpackage\n        \"\"\"\n        # self._verify_input_dict(data)\n        self._convert_tensor_to_numpy(data)\n        self._update_float16_multiarray_input_to_float32(data)\n        return self.__proxy__.predict(data)", "\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": [""]}
{"filename": "src/utils/trace_warnings.py", "chunked_list": ["import warnings\nfrom contextlib import contextmanager\n\ndef _gpt2_warning_filter():\n    patterns = [\n        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        # if this fails the trace will fail\n        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        # TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n        # if output_mask is not None and t > 1:\n        (r\".*Converting a tensor to a Python boolean might cause the trace to be incorrect.*\", [275, 280, 311]),\n\n        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n        #   k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n        #   q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        # UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n        #   v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        # C and n_head will never be negative\n        (r\".*__floordiv__ is deprecated.*\", [155, 156, 157]),\n\n        # TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        # k will always be the same size\n        (r\".*Converting a tensor to a Python float.*\", [165])\n\n    ]\n    for pattern, linenos in patterns:\n        for lineno in linenos:\n            warnings.filterwarnings(\"ignore\", pattern, lineno=lineno)", "\n\n@contextmanager\ndef silence_known_trace_warnings(model_name: str):\n    \"\"\"\n    Silence known and safe to ignore warnings.\n    \"\"\"\n    with warnings.catch_warnings():\n        if model_name in [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]:\n            _gpt2_warning_filter()\n        yield", ""]}
{"filename": "src/ml_ane_transformers/layerwise_comparison.py", "chunked_list": ["from src.ml_ane_transformers.vanilla_gpt2 import GPT as Vanilla\nfrom src.ml_ane_transformers.ane_gpt2 import GPT as ANE\nimport torch\nimport numpy as np\nfrom src.utils.psnr import compute_psnr\nimport os\n\n\"\"\"\nWhat is this file?\nIt's a step-by-step, layer-by-layer process of debugging why the Apple Neural Engine (ANE) optimized", "What is this file?\nIt's a step-by-step, layer-by-layer process of debugging why the Apple Neural Engine (ANE) optimized\nmodel does not match the reference (aka vanilla) gpt2 model.\n\nThe PSNR (peak signal-to-noise ratio) is taken from the ane transformers repo and is a way\nto measure how well the two models match. It seems like 150-200 is good (since these should be mathematically equivalent\n-- e.g. in the Apple repo 60 is the minimum bar they set for the lossy conversion to CoreML).\nDespite some horrific bugs I never got lower than 20 or so. Lower than 60 and something is likely wrong.\n\nRun this file, look at the PSNR logs and if any are <<150, start debugging that layer.", "\nRun this file, look at the PSNR logs and if any are <<150, start debugging that layer.\n\"\"\"\n\nane = ANE.from_pretrained(\"gpt2\")\nvan = Vanilla.from_pretrained(\"gpt2\")\nane.eval()\nvan.eval()\n\nseqlen = 20", "\nseqlen = 20\nidx = torch.randint(30000, (1, seqlen,))\nane_inputs = ane.build_inputs(idx)\nqk_mask, k_mask = [ane_inputs[k] for k in [\"qk_mask\", \"k_mask\"]]\nk_mask = None\n\nprint(\"---\\n\")\n\ndef print_stats(name: str, a, v, should_be_equal=False, end=\"\\n\"):\n    if torch.equal(a,v) and not should_be_equal:\n        print(\"BUG: most likely passed the wrong things\")\n    print(name, a.shape, v.shape, \"psnr\", compute_psnr(a, v), end)", "\ndef print_stats(name: str, a, v, should_be_equal=False, end=\"\\n\"):\n    if torch.equal(a,v) and not should_be_equal:\n        print(\"BUG: most likely passed the wrong things\")\n    print(name, a.shape, v.shape, \"psnr\", compute_psnr(a, v), end)\n\ndef bc1s_to_bsc(x):\n    assert len(x.shape) == 4\n    assert x.shape[0] == 1 # batch is 1\n    assert x.shape[2] == 1 # third dim is always 1\n    return x.permute(0, 3, 1, 2).squeeze(-1)", "\nwith torch.no_grad():\n    ane_embd = ane.transformer.wte(idx)\n    van_embd = van.transformer.wte(idx)\n    print(\"wte\", ane_embd.shape, van_embd.shape, torch.equal(ane_embd, van_embd))\n\n    ane_preb = ane.prepare_for_block(idx)\n    van_preb = van.prepare_for_block(idx)\n    print_stats(\"pre block\", bc1s_to_bsc(ane_preb), van_preb)\n\n    ane_b1 = ane.transformer.h[0](ane.prepare_for_block(idx), qk_mask=qk_mask, k_mask=k_mask)\n    van_b1 = van.transformer.h[0](van.prepare_for_block(idx))\n    print_stats(\"layer 1\", bc1s_to_bsc(ane_b1), van_b1)\n\n    ane_b1_ln_1 = ane.transformer.h[0].ln_1(ane.prepare_for_block(idx))\n    van_b1_ln_1 = van.transformer.h[0].ln_1(van.prepare_for_block(idx))\n    print_stats(\"layer 1 ln1\", bc1s_to_bsc(ane_b1_ln_1), van_b1_ln_1,)\n\n    ane_b1_attn = ane.transformer.h[0].attn(ane_b1_ln_1, qk_mask=qk_mask, k_mask=k_mask)\n    van_b1_attn = van.transformer.h[0].attn(van_b1_ln_1)\n    print_stats(\"layer 1 attn\", bc1s_to_bsc(ane_b1_attn), van_b1_attn)\n\n    ane_b1_attn_qproj = ane.transformer.h[0].attn.q_proj(ane_b1_ln_1)\n    van_b1_attn_qproj = van.transformer.h[0].attn.q_proj(van_b1_ln_1)\n    print_stats(\"layer 1 qproj\", bc1s_to_bsc(ane_b1_attn_qproj), van_b1_attn_qproj)\n\n    ane_b1_attn_kproj = ane.transformer.h[0].attn.k_proj(ane_b1_ln_1)\n    van_b1_attn_kproj = van.transformer.h[0].attn.k_proj(van_b1_ln_1)\n    print_stats(\"layer 1 kproj\", bc1s_to_bsc(ane_b1_attn_kproj), van_b1_attn_kproj)\n\n    ane_b1_attn_vproj = ane.transformer.h[0].attn.v_proj(ane_b1_ln_1)\n    van_b1_attn_vproj = van.transformer.h[0].attn.v_proj(van_b1_ln_1)\n    print_stats(\"layer 1 vproj\", bc1s_to_bsc(ane_b1_attn_vproj), van_b1_attn_vproj)\n\n    ane_b1_attn_atnn = ane.transformer.h[0].attn._attention_fn(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj, qk_mask, k_mask, False)[0].contiguous().view(1, 768, 1, seqlen)\n    van_b1_attn_atnn = van.transformer.h[0].attn._attention_fn(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n    print_stats(\"layer 1 inner attn\", bc1s_to_bsc(ane_b1_attn_atnn), van_b1_attn_atnn)\n\n    # Not sure I fully trust this due to the change to add cat in ane + the permute.\n    ane_b1_attn_qxk = ane.transformer.h[0].attn._qk(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj)\n    van_b1_attn_qxk = van.transformer.h[0].attn._qk(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n    assert len(ane_b1_attn_qxk) == len(van_b1_attn_qxk)\n    print_stats(\"layer 1 q@k\", ane_b1_attn_qxk.permute(0, 2, 3, 1), van_b1_attn_qxk)\n\n    ane_b1_attn_qxk_sm = ane.transformer.h[0].attn._qk_softmax(ane_b1_attn_qproj, ane_b1_attn_kproj, ane_b1_attn_vproj, qk_mask=qk_mask, k_mask=k_mask)\n    van_b1_attn_qxk_sm = van.transformer.h[0].attn._qk_softmax(van_b1_ln_1, van_b1_attn_qproj, van_b1_attn_kproj, van_b1_attn_vproj)\n    assert len(ane_b1_attn_qxk_sm) == len(van_b1_attn_qxk_sm)\n    print_stats(\"layer 1 q@k softmax\", ane_b1_attn_qxk_sm.permute(0, 2, 3, 1), van_b1_attn_qxk_sm)\n\n    # Bubble back up to middle of block.\n    ane_b1_x_post_attn = ane_b1_ln_1 + ane_b1_attn\n    van_b1_x_post_attn = van_b1_ln_1 + van_b1_attn\n    print_stats(\"layer 1 x + attn\", ane_b1_attn_qxk_sm.permute(0, 2, 3, 1), van_b1_attn_qxk_sm)\n\n    ane_b1_ln2 = ane.transformer.h[0].ln_2(ane_b1_x_post_attn)\n    van_b1_ln2 = van.transformer.h[0].ln_2(van_b1_x_post_attn)\n    print_stats(\"layer 1 ln2\", bc1s_to_bsc(ane_b1_ln2), van_b1_ln2)\n\n    ane_b1_mlp = ane.transformer.h[0].mlp(ane_b1_ln2)\n    van_b1_mlp = van.transformer.h[0].mlp(van_b1_ln2)\n    print_stats(\"layer 1 mlp\", bc1s_to_bsc(ane_b1_mlp), van_b1_mlp)\n\n    ane_all_blocks = ane.prepare_for_block(idx)\n    van_all_blocks = van.prepare_for_block(idx)\n    for nh, (ab, vb) in enumerate(zip(ane.transformer.h, van.transformer.h)):\n        ane_all_blocks = ab(ane_all_blocks, qk_mask=qk_mask, k_mask=k_mask)\n        van_all_blocks = vb(van_all_blocks)\n        print_stats(f\"layer {nh}\", bc1s_to_bsc(ane_all_blocks), van_all_blocks, end=\"\")\n\n    ane_lm = ane.lm_head(ane_all_blocks.permute(3, 1, 0, 2)).squeeze().unsqueeze(0)\n    van_lm = van.lm_head(van_all_blocks)\n    print_stats(\"lm\", ane_lm, van_lm)", "\n# E2E - with k_mask to make sure that works.\n\nane_inputs = ane.build_inputs(idx, pad_to_length=40, pad_token_id=350)\nane_idx = ane_inputs[\"input_ids\"].to(\"mps\")\nqk_mask, k_mask, output_mask = [ane_inputs[k] for k in [\"qk_mask\", \"k_mask\", \"output_mask\"]]\n\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\nwith torch.no_grad():\n    ane_out = ane.to(\"mps\", torch.half)(ane_idx, output_mask=output_mask.to(ane_idx.device), qk_mask=None, k_mask=None)[:, [-1], :].float().cpu()\n    van_out = van(idx)[0]", "with torch.no_grad():\n    ane_out = ane.to(\"mps\", torch.half)(ane_idx, output_mask=output_mask.to(ane_idx.device), qk_mask=None, k_mask=None)[:, [-1], :].float().cpu()\n    van_out = van(idx)[0]\n\nassert ane_out.shape == van_out.shape, f\"{ane_out.shape} != {van_out.shape}\"\n\nprint(\"shapes:\", van_out.shape, ane_out.shape)\nprint(\"softmax argmax:\", torch.argmax(van_out.softmax(2)), torch.argmax(ane_out.softmax(2)))\nprint(\"e2e psnr\", compute_psnr(ane_out.softmax(2).numpy(), van_out.softmax(2).numpy()))\n# sim = torch.nn.functional.cosine_similarity(van_out[0], ane_out[0])", "print(\"e2e psnr\", compute_psnr(ane_out.softmax(2).numpy(), van_out.softmax(2).numpy()))\n# sim = torch.nn.functional.cosine_similarity(van_out[0], ane_out[0])\n# print(\"similarity\", sim, torch.mean(sim)) # Turns out this is mostly useless, it's quite easy to get upper 0.9s."]}
{"filename": "src/ml_ane_transformers/trace_psnr.py", "chunked_list": ["import torch\nfrom ane_gpt2 import GPT as ANEGPT\nimport numpy as np\nimport coremltools as ct\nfrom psnr import compute_psnr\n\n\"\"\"\nCompare the PSNR for a saved mlpackage with a model\nand its traced version. Useful for ruling out tracing\nas the source of error.", "and its traced version. Useful for ruling out tracing\nas the source of error.\n\"\"\"\n\ntoken_predictor = ANEGPT.from_pretrained(\"gpt2\").eval()\n\nrandom_tokens = torch.randint(10000, (1,10,))\ninputs_dict = token_predictor.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\nprint(token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"]).shape)\nprint(f\"Tracing the model with {inputs_dict['input_ids']}\")", "print(token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"]).shape)\nprint(f\"Tracing the model with {inputs_dict['input_ids']}\")\ntraced_token_predictor = torch.jit.trace(token_predictor, (inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"]))\n\nprint(\"Traced, diffing...\")\nrandom_tokens = torch.randint(10000, (1,10,))\ninputs_dict = ANEGPT.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n\nwith torch.no_grad():\n    og_out = token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])\n    tr_out = traced_token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])", "with torch.no_grad():\n    og_out = token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])\n    tr_out = traced_token_predictor(inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"])\n\nassert og_out.shape == tr_out.shape\n\npsnr = compute_psnr(og_out.numpy(), tr_out.numpy())\nprint(\"psnr:\", psnr)\n\nprint(\"Comparing with mlpackage\")", "\nprint(\"Comparing with mlpackage\")\nmodel = ct.models.model.MLModel(\"gpt2_2023_03_24-08_02_53_PM.mlpackage\",\n                                compute_units=ct.ComputeUnit.ALL)\n\ncm_out = model.predict(inputs_dict)[\"logits\"]\nassert tr_out.shape == cm_out.shape\n\nprint(\"coreml-traced psnr:\", compute_psnr(tr_out.numpy(), cm_out))\nprint(\"coreml-og     psnr:\", compute_psnr(og_out.numpy(), cm_out))", "print(\"coreml-traced psnr:\", compute_psnr(tr_out.numpy(), cm_out))\nprint(\"coreml-og     psnr:\", compute_psnr(og_out.numpy(), cm_out))"]}
{"filename": "src/ml_ane_transformers/vanilla_gpt2.py", "chunked_list": ["\"\"\"\nAn almost-verbatim unmodified version of nanoGPT. Useful for comparing if ANE-modifications\nare mathematically equivalent.\n\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py", "2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\nOriginal License:\nMIT License\n\nCopyright (c) 2022 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal", "Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n", "copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"", "SOFTWARE.\n\"\"\"\n\nimport math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F", "import torch.nn as nn\nfrom torch.nn import functional as F\n\n# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))", "\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        # self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        # if not self.flash:\n        # coremltools does not support scaled_dot_product_attention.\n        if True:\n            # print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def q_proj(self, x):\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        return q\n\n    def k_proj(self, x):\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        return k\n\n    def v_proj(self, x):\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        return v\n\n    def _qk(self, x, q, k, v):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        return att\n\n    def _qk_softmax(self, x, q, k, v):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float(-1e4))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        return y\n\n    def _attention_fn(self, x, q, k, v):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return y\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # B - batch size\n        # T - sequence length (number of tokens)\n        # C - embedding dimensionality, n_embd, 768\n        # nh - number of heads\n        # hs - head size\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if False:# self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y", "\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x", "\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster", "\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def prepare_for_block(self, idx):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        return self.transformer.drop(tok_emb + pos_emb)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        \"\"\"\n        This long function is unfortunately doing something very simple and is being very defensive:\n        We are separating out all parameters of the model into two buckets: those that will experience\n        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n        We are then returning the PyTorch optimizer object.\n        \"\"\"\n\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (torch.nn.Linear, )\n        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                if pn.endswith('bias'):\n                    # all biases will not be decayed\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    # weights of whitelist modules will be weight decayed\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    # weights of blacklist modules will NOT be weight decayed\n                    no_decay.add(fpn)\n\n        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n        # will appear in the no_decay and decay sets respectively after the above.\n        # In addition, because named_parameters() doesn't return duplicates, it\n        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n        # so let's manually remove 'lm_head.weight' from decay set. This will include\n        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n        decay.remove('lm_head.weight')\n\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n\n        # create the pytorch optimizer object\n        optim_groups = [\n            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n        print(f\"using fused AdamW: {use_fused}\")\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx"]}
{"filename": "src/ml_ane_transformers/__init__.py", "chunked_list": [""]}
{"filename": "src/ml_ane_transformers/ane_gpt2.py", "chunked_list": ["\"\"\"\nA fairly modified version of nanoGPT that attempts to apply the principles\nfrom the ml-ane-transformers repo. Functional, but not as fast as I had hoped.\n\nReferences:\nhttps://github.com/karpathy/nanoGPT/blob/a82b33b525ca9855d705656387698e13eb8e8d4b/model.py#L1\nhttps://github.com/apple/ml-ane-transformers/tree/main/ane_transformers/reference\n\nOriginal nanoGPT License:\nMIT License", "Original nanoGPT License:\nMIT License\n\nCopyright (c) 2022 Andrej Karpathy\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is", "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE", "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\n\nimport math\nimport inspect", "import math\nimport inspect\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom .ane.multihead_attention import SelfAttention as AneSelfAttention\n# from ane.multihead_attention import MultiHeadAttention as AneMultiHeadAttention", "from .ane.multihead_attention import SelfAttention as AneSelfAttention\n# from ane.multihead_attention import MultiHeadAttention as AneMultiHeadAttention\nfrom .ane.layer_norm import LayerNormANE as AneLayerNormx\nfrom .ane.dummy_layer_norm import DummyLayerNormANE\nfrom .ane.kahan_layer_norm import KahanLayerNormANE\nfrom .ane.ffn import FFN as AneMLP\n\n# Torch does not support layer norm over the 1st dimension. But the MIL (almost) does.\n# Override this to use the native 1st dimension MIL layer norm.\nOVERRIDE_LAYER_NORM = True", "# Override this to use the native 1st dimension MIL layer norm.\nOVERRIDE_LAYER_NORM = True\nlayer_norm_cls = DummyLayerNormANE if OVERRIDE_LAYER_NORM else AneLayerNormx\n\nfrom coremltools.converters.mil import register_torch_op\nfrom coremltools.converters.mil.mil import Builder as mb\nfrom coremltools.converters.mil.frontend.torch.ops import _get_inputs\nfrom coremltools.converters.mil.frontend.torch.torch_op_registry import _TORCH_OPS_REGISTRY\n\nif OVERRIDE_LAYER_NORM:\n    if \"batch_norm\" in _TORCH_OPS_REGISTRY:\n        del _TORCH_OPS_REGISTRY[\"batch_norm\"]\n    @register_torch_op\n    def batch_norm(context, node):\n        inputs = _get_inputs(context, node, expected=9)\n        _input = inputs[0]\n        weight = inputs[1]\n        bias = inputs[2]\n        # running_mean = inputs[3]\n        # running_var = inputs[4]\n        # training = inputs[5].val\n        eps = inputs[7]\n\n        # Ideally would not need the transposes, but axes=[1] doesn't work\n        # on the Neural Engine. https://developer.apple.com/forums/thread/728931\n\n        # node.name has to go on the last op, that's also the one that gets added to the context\n        rs1 = mb.transpose(x=_input, perm=[0,3,2,1])\n\n        ln = mb.layer_norm(x=rs1, axes=[3], epsilon=eps, gamma=weight, beta=bias)\n        # context.add(ln)\n\n        rs2 = mb.transpose(x=ln, perm=[0,3,2,1], name=node.name)\n        context.add(rs2)", "\nif OVERRIDE_LAYER_NORM:\n    if \"batch_norm\" in _TORCH_OPS_REGISTRY:\n        del _TORCH_OPS_REGISTRY[\"batch_norm\"]\n    @register_torch_op\n    def batch_norm(context, node):\n        inputs = _get_inputs(context, node, expected=9)\n        _input = inputs[0]\n        weight = inputs[1]\n        bias = inputs[2]\n        # running_mean = inputs[3]\n        # running_var = inputs[4]\n        # training = inputs[5].val\n        eps = inputs[7]\n\n        # Ideally would not need the transposes, but axes=[1] doesn't work\n        # on the Neural Engine. https://developer.apple.com/forums/thread/728931\n\n        # node.name has to go on the last op, that's also the one that gets added to the context\n        rs1 = mb.transpose(x=_input, perm=[0,3,2,1])\n\n        ln = mb.layer_norm(x=rs1, axes=[3], epsilon=eps, gamma=weight, beta=bias)\n        # context.add(ln)\n\n        rs2 = mb.transpose(x=ln, perm=[0,3,2,1], name=node.name)\n        context.add(rs2)", "\n# Note: torch.nn.LayerNorm and ane_transformers.reference.layer_norm.LayerNormANE\n# apply scale and bias terms in opposite orders. In order to accurately restore a\n# state_dict trained using the former into the the latter, we adjust the bias term\ndef correct_for_bias_scale_order_inversion(state_dict, prefix, local_metadata,\n                                           strict, missing_keys,\n                                           unexpected_keys, error_msgs):\n    if not OVERRIDE_LAYER_NORM:\n        state_dict[prefix +'bias'] = state_dict[prefix + 'bias'] / state_dict[prefix +'weight']\n    return state_dict", "\n\n# class AneLayerNorm(AneLayerNormx):\nclass AneLayerNorm(layer_norm_cls):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._register_load_state_dict_pre_hook(\n            correct_for_bias_scale_order_inversion)\n\n# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))", "\n# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)", "\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y", "\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = new_gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x", "\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        # self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.ln_1 = AneLayerNorm(config.n_embd)\n        # self.attn = CausalSelfAttention(config)\n        self.attn = AneSelfAttention(embed_dim=config.n_embd, n_head=config.n_head, dropout=config.dropout, return_weights=False)\n        # self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.ln_2 = AneLayerNorm(config.n_embd)\n        self.mlp = AneMLP(config.n_embd, 4 * config.n_embd, dropout=config.dropout)\n\n    def forward(self, x, qk_mask=None, k_mask=None):\n        \"\"\"\n        x: (batch, embed_dim, 1, seqlen) aka BC1S\n        qk_mask, k_mask as in AneSelfAttention\n        \"\"\"\n        # print(\"xin\", x.shape)\n        # print(\"self.ln_1(x)\", self.ln_1(x).shape)\n        # print(\" self.attn(self.ln_1(x))\",  self.attn(self.ln_1(x)).shape)\n        x = x + self.attn(self.ln_1(x), qk_mask=qk_mask, k_mask=k_mask)\n        x = x + self.mlp(self.ln_2(x))\n        # print(\"xout\", x.shape)\n        return x", "\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster", "\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            # ln_f = LayerNorm(config.n_embd, bias=config.bias),\n            ln_f = AneLayerNorm(config.n_embd),\n        ))\n        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.lm_head = nn.Conv2d(config.n_embd, config.vocab_size, 1, bias=False) # Confirmed that this is equivalent to ^. See: https://sebastianraschka.com/faq/docs/fc-to-conv.html\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        # FIXME: This duplicates these (large) weights.\n        self.transformer.wte.weight = nn.Parameter(self.lm_head.weight.squeeze()) # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        self.qk_mask = ((1 - torch.tril(torch.ones((512,512), dtype=torch.float32))).t() * -1e4).view(1, 512, 1, 512)\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def prepare_for_block(self, idx):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n\n        # print(\"x pre-blocks\", x.shape)\n        # BSC -> BC1S (most conducive to ANE)\n        # Batch=1, Sequence=NumTokens, Channels=NumEmbed aka Hidden Size\n        return x.transpose(1, 2).unsqueeze(2)\n\n    def forward(self, idx, output_mask=None, qk_mask=None, k_mask=None, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n\n        if qk_mask is None:\n            # qk_mask = ((1 - torch.tril(torch.ones((512,512), dtype=torch.float32))).t() * -1e4).view(1, 512, 1, 512)\n            qk_mask = self.qk_mask[:, :t, :, :t].to(device)\n\n        # print(\"forwardz\", idx.shape, self.transformer.wte.weight.shape)\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n\n        # print(\"x pre-blocks\", x.shape)\n        # BSC -> BC1S (most conducive to ANE)\n        # Batch=1, Sequence=NumTokens, Channels=NumEmbed aka Hidden Size\n        x = x.transpose(1, 2).unsqueeze(2)\n\n        for block in self.transformer.h:\n            x = block(x, qk_mask=qk_mask, k_mask=k_mask)\n        x = self.transformer.ln_f(x)\n\n        # No need to pass back the other length-1 results we don't care about.\n        # Big speed boost.\n        if output_mask is not None:\n            x = torch.index_select(x, 3, output_mask)\n            # TODO: Sprinkle in a softmax here? Or does that prevent us from doing top-p/top-k\n            loss = None\n\n        # logits = x\n        old = False\n        if old:\n            # Old slow way.\n            x = x.permute(3, 1, 0, 2) # (S, C, B, 1)\n            logits = self.lm_head(x).squeeze().unsqueeze(0).unsqueeze(0)\n        else:\n            # # New fast way.\n            # This part is actually quite fast. 4ms out of 75ms.\n            x = x.permute(0,2,3,1).squeeze(0)\n\n            # ANE can only load tensors with dim size of at most 16,384 - divide the vocab size to be less than that\n            # gpt2 vocab size is a product of two primes smh\n            splits = self.transformer.wte.weight.split(self.transformer.wte.weight.shape[0]//29, dim=0)\n            logits = torch.cat([torch.einsum('bid,jd->bij', x, split) for split in splits], dim=-1)#.view(*x.shape[:2], -1)\n\n        return logits\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'ckiplab/gpt2-tiny-chinese'}\n        override_args = override_args or {} # default to empty dict\n        # only dropout can be overridden see more notes below\n        assert all(k == 'dropout' for k in override_args)\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'ckiplab/gpt2-tiny-chinese': dict(n_layer=4, n_head=12, n_embd=312), # 18M, vocab size 21128\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n        config_args['vocab_size'] = 21128 if \"chinese\" in model_type else 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        config_args['bias'] = True # always True for GPT model checkpoints\n        # we can override the dropout rate, if desired\n        if 'dropout' in override_args:\n            print(f\"overriding dropout rate to {override_args['dropout']}\")\n            config_args['dropout'] = override_args['dropout']\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        if OVERRIDE_LAYER_NORM:\n            sd_keys = [k for k in sd_keys if not k.endswith(\"running_mean\")]\n            sd_keys = [k for k in sd_keys if not k.endswith(\"running_var\")]\n            sd_keys = [k for k in sd_keys if not k.endswith(\"num_batches_tracked\")]\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        transform_hf_weights(sd_hf)\n\n        # print(\"ane\", sd['transformer.h.0.attn.out_proj.weight'].shape)\n        # print(\"hf\", sd_hf['transformer.h.0.attn.c_attn.weight'].shape)\n\n        # HF uses one weights matrix for QKV (called C), but ANE uses 3.\n        # TODO: Maybe an opportunity to simplify? IDK\n        # for k in list(sd_hf.keys()):\n        #     # attention input\n        #     if \"attn.c_attn\" in k:\n        #         new_k = lambda nk: k.replace(\"c_attn\", nk)\n\n        #         catt = sd_hf[k]\n        #         dim = -1 # bias\n        #         if \"weight\" in k:\n        #             # catt = catt.unsqueeze(-1).unsqueeze(-1)\n        #             dim = -3 # weight\n\n        #         # FIXME: Not sure about the order.\n        #         # From HF: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n        #         qt, kt, vt = catt.chunk(3, dim=dim)\n        #         print(\"catt shape\", catt.shape)\n\n        #         assert qt.shape == sd[new_k(\"k_proj\")].shape,\\\n        #             f\"{k}: {qt.shape} != {sd[new_k('k_proj')].shape}\"\n        #         sd_hf[new_k(\"k_proj\")] = kt\n        #         sd_hf[new_k(\"q_proj\")] = qt\n        #         sd_hf[new_k(\"v_proj\")] = vt\n        #         del sd_hf[k]\n        #     # attention output\n        #     elif \"attn.c_proj\" in k:\n        #         new_k = lambda nk: k.replace(\"c_proj\", nk)\n\n        #         catt = sd_hf[k]\n        #         # if \"weight\" in k:\n        #         #     catt = catt.unsqueeze(-1).unsqueeze(-1)\n\n        #         new_out = catt\n        #         assert new_out.shape == sd[new_k(\"out_proj\")].shape,\\\n        #             f\"{k}: {new_out.shape} != {sd[new_k('out_proj')].shape}\"\n        #         sd_hf[new_k(\"out_proj\")] = new_out\n        #         del sd_hf[k]\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n\n        if len(set(sd_keys) ^ set(sd_keys_hf)) > 0:\n            print(set(sd_keys_hf) - set(sd_keys))\n            print(set(sd_keys) - set(sd_keys_hf))\n\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            # if any(k.endswith(w) for w in transposed):\n            #     print(\"tpose\", k, sd_hf[k].shape)\n            #     # special treatment for the Conv1D weights we need to transpose\n            #     assert sd_hf[k].shape[::-1] == sd[k].shape, f\"{k}: {sd_hf[k].shape[::-1]} == {sd[k].shape}\"\n            #     with torch.no_grad():\n            #         sd[k].copy_(sd_hf[k].t())\n            # if any(k.endswith(w) for w in transposed_and_unsqueezed):\n            #     print(\"tpose+unsq\", k)\n            #     # special treatment for the Conv1D weights we need to transpose and unsqueeze\n            #     # :-2 to drop the last 2 singleton dimensions\n            #     # ::-1 to transpose\n            #     assert sd_hf[k].shape[::-1] == sd[k].shape[:-2], f\"{k}: {sd_hf[k].shape[::-1]} == {sd[k].shape[:-2]}\"\n            #     with torch.no_grad():\n            #         sd[k].copy_(sd_hf[k].t().unsqueeze(-1).unsqueeze(-1))\n            # else:\n                # vanilla copy over the other parameters\n            assert sd_hf[k].shape == sd[k].shape, f\"{k}: {sd_hf[k].shape} != {sd[k].shape}\"\n            with torch.no_grad():\n                sd[k].copy_(sd_hf[k])\n\n        return model\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\n    @staticmethod\n    def build_inputs(seq, pad_to_length=None, pad_token_id=-1, dtype=torch.float32, device=\"cpu\"):\n        seqlen = seq.shape[1]\n\n        if not pad_to_length:\n            pad_to_length = seqlen\n        length = pad_to_length\n\n        assert length == seqlen or pad_token_id != -1, \"pad token must be provided when padding\"\n\n        # Upper triangular mask but in the BC1S format. aka causal mask\n        # Note the transpose! Don't really get it, but easy to see when comparing the results of applying the mask.\n        qk_mask = ((1 - torch.tril(torch.ones((length,length), dtype=dtype))).t() * -1e4).view(1, length, 1, length)\n\n        # We want to attend to the whole sequence, but not the padding. aka attention mask\n        k_mask = torch.cat([\n            torch.zeros(seqlen, dtype=dtype),\n            torch.full((pad_to_length - seqlen,), float(-1e4), dtype=dtype)\n        ]).view(1,length,1,1)\n\n        # Pad the sequence itself too.\n        input_ids = torch.cat([\n            seq.squeeze(),\n            torch.full((pad_to_length - seqlen,), pad_token_id)\n        ]).unsqueeze(0)\n\n        # Used to mask outputs before they exit the model.\n        # input_ids: [0,1,2,3] length = 4, result is in index 3\n        output_mask = torch.tensor([seqlen-1], dtype=torch.int32)\n\n        return {\n            \"input_ids\": input_ids.int().to(device),\n            \"qk_mask\": qk_mask.to(device),\n            \"k_mask\": k_mask.to(device),\n            \"output_mask\": output_mask.to(device),\n        }", "\n# def linear_to_conv2d_map(state_dict):\n#     \"\"\" Unsqueeze twice to map nn.Linear weights to nn.Conv2d weights\n#     \"\"\"\n#     for k in state_dict:\n#         print(k, state_dict[k].shape)\n#         # is_internal_proj = all(substr in k for substr in ['lin', '.weight'])\n#         is_internal_proj = all(substr in k for substr in ['attn', '.weight'])\n#         is_output_proj = all(substr in k\n#                              for substr in ['classifier', '.weight'])", "#         is_output_proj = all(substr in k\n#                              for substr in ['classifier', '.weight'])\n#         is_lm_head = all(substr in k for substr in['lm_head', '.weight'])\n#         if is_internal_proj or is_output_proj or is_lm_head:\n#             if len(state_dict[k].shape) == 2:\n#                 if \"attn.c_proj\" in k: # or \"attn.c_attn\" in k:\n#                     print(\"YOOOOOOOO\", k)\n#                     state_dict[k] = state_dict[k].t()\n#                 state_dict[k] = state_dict[k][:, :, None, None]\n\ndef transform_hf_weights(state_dict):\n    for k in list(state_dict.keys()):\n        # print(k, state_dict[k].shape)\n\n        # In HF defined as:\n        # self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n        # Here:\n        # self.out_proj = nn.Conv2d(self.d_v, self.d_out, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        if \"attn.c_proj.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            newk = k.replace(\"attn.c_proj.weight\", \"attn.out_proj.weight\")\n            state_dict[newk] = state_dict.pop(k).t()[:, :, None, None]\n            # after = state_dict[newk].shape\n            # print(before, \"->\", after)\n\n        # Similar to c_proj.weight\n        elif \"attn.c_proj.bias\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            newk = k.replace(\"attn.c_proj.bias\", \"attn.out_proj.bias\")\n            state_dict[newk] = state_dict.pop(k)\n            # after = state_dict[newk].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n        # Here:\n        # self.q_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        # self.v_proj = nn.Conv2d(embed_dim, self.d_v, 1)\n        # self.k_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"attn.c_attn.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            # From HF: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n            qm,km,vm = state_dict.pop(k).t()[:, :, None, None].chunk(3, dim=0)\n            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n            # afters = [state_dict[newk].shape for newk in newks]\n            # print(before, \"->\", afters)\n\n        # Similar to c_attn.weight\n        elif \"attn.c_attn.bias\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            qm,km,vm = state_dict.pop(k).chunk(3, dim=0)\n            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n            # afters = [state_dict[newk].shape for newk in newks]\n            # print(before, \"->\", afters)\n\n        # In HF defined as:\n        # self.c_fc = Conv1D(intermediate_size, embed_dim)\n        # Here:\n        # self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"mlp.c_fc.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k].t()[:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.c_proj = Conv1D(embed_dim, intermediate_size)\n        # Here:\n        # self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"mlp.c_proj.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k].t()[:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # Here:\n        # self.lm_head = nn.Conv2d(config.n_embd, config.vocab_size, 1, bias=False)\n        # To go from Linear to Conv2d, we need to unsqueeze twice (NO TRANSPOSE).\n        elif \"lm_head.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k][:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # Note: torch.nn.LayerNorm and ane_transformers.reference.layer_norm.LayerNormANE\n        # apply scale and bias terms in opposite orders. In order to accurately restore a\n        # state_dict trained using the former into the the latter, we adjust the bias term\n        elif \".ln_\" in k and \".bias\" in k and not OVERRIDE_LAYER_NORM:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            weight_key = k.replace(\".bias\", \".weight\")\n            state_dict[k] = state_dict[k] / state_dict[weight_key]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        elif \".ln_\" in k and OVERRIDE_LAYER_NORM:\n            newk = k.replace(\".weight\", \".bn.weight\")\n            newk = newk.replace(\".bias\", \".bn.bias\")\n            state_dict[newk] = state_dict.pop(k)", "#                 state_dict[k] = state_dict[k][:, :, None, None]\n\ndef transform_hf_weights(state_dict):\n    for k in list(state_dict.keys()):\n        # print(k, state_dict[k].shape)\n\n        # In HF defined as:\n        # self.c_proj = Conv1D(self.embed_dim, self.embed_dim)\n        # Here:\n        # self.out_proj = nn.Conv2d(self.d_v, self.d_out, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        if \"attn.c_proj.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            newk = k.replace(\"attn.c_proj.weight\", \"attn.out_proj.weight\")\n            state_dict[newk] = state_dict.pop(k).t()[:, :, None, None]\n            # after = state_dict[newk].shape\n            # print(before, \"->\", after)\n\n        # Similar to c_proj.weight\n        elif \"attn.c_proj.bias\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            newk = k.replace(\"attn.c_proj.bias\", \"attn.out_proj.bias\")\n            state_dict[newk] = state_dict.pop(k)\n            # after = state_dict[newk].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim)\n        # Here:\n        # self.q_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        # self.v_proj = nn.Conv2d(embed_dim, self.d_v, 1)\n        # self.k_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"attn.c_attn.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            # From HF: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n            qm,km,vm = state_dict.pop(k).t()[:, :, None, None].chunk(3, dim=0)\n            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n            # afters = [state_dict[newk].shape for newk in newks]\n            # print(before, \"->\", afters)\n\n        # Similar to c_attn.weight\n        elif \"attn.c_attn.bias\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            qm,km,vm = state_dict.pop(k).chunk(3, dim=0)\n            state_dict[k.replace(\"c_attn\", \"q_proj\")] = qm\n            state_dict[k.replace(\"c_attn\", \"k_proj\")] = km\n            state_dict[k.replace(\"c_attn\", \"v_proj\")] = vm\n            # newks = [k.replace(\"c_attn\", f\"{l}_proj\") for l in [\"q\", \"k\", \"v\"]]\n            # afters = [state_dict[newk].shape for newk in newks]\n            # print(before, \"->\", afters)\n\n        # In HF defined as:\n        # self.c_fc = Conv1D(intermediate_size, embed_dim)\n        # Here:\n        # self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"mlp.c_fc.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k].t()[:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.c_proj = Conv1D(embed_dim, intermediate_size)\n        # Here:\n        # self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n        # To go from Conv1D to Conv2d, we need to transpose and unsqueeze twice.\n        elif \"mlp.c_proj.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k].t()[:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # In HF defined as:\n        # self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # Here:\n        # self.lm_head = nn.Conv2d(config.n_embd, config.vocab_size, 1, bias=False)\n        # To go from Linear to Conv2d, we need to unsqueeze twice (NO TRANSPOSE).\n        elif \"lm_head.weight\" in k:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            state_dict[k] = state_dict[k][:, :, None, None]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        # Note: torch.nn.LayerNorm and ane_transformers.reference.layer_norm.LayerNormANE\n        # apply scale and bias terms in opposite orders. In order to accurately restore a\n        # state_dict trained using the former into the the latter, we adjust the bias term\n        elif \".ln_\" in k and \".bias\" in k and not OVERRIDE_LAYER_NORM:\n            # print(k, state_dict[k].shape)\n            # before = state_dict[k].shape\n            weight_key = k.replace(\".bias\", \".weight\")\n            state_dict[k] = state_dict[k] / state_dict[weight_key]\n            # after = state_dict[k].shape\n            # print(before, \"->\", after)\n\n        elif \".ln_\" in k and OVERRIDE_LAYER_NORM:\n            newk = k.replace(\".weight\", \".bn.weight\")\n            newk = newk.replace(\".bias\", \".bn.bias\")\n            state_dict[newk] = state_dict.pop(k)", "\n\n        # else:\n        #     if \"bias\" in k:\n        #         print(k)\n"]}
{"filename": "src/ml_ane_transformers/convert-ane-layers.py", "chunked_list": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport coremltools as ct\nimport numpy as np\nfrom datetime import datetime\nfrom ane_gpt2 import GPT as ANEGPT\n\n\"\"\"\nExperimental setup for going layer-by-layer to see which\nlayers cause a drop in PSNR after conversion to CoreML.", "Experimental setup for going layer-by-layer to see which\nlayers cause a drop in PSNR after conversion to CoreML.\n\nTurns out it's the LayerNorm (specifically computing the sum in float16\nbefore dividing it for the mean).\n\"\"\"\n\ndef compute_psnr(a, b):\n    \"\"\" Compute Peak-Signal-to-Noise-Ratio across two numpy.ndarray objects\n    \"\"\"\n    max_b = np.abs(b).max()\n    sumdeltasq = 0.0\n\n    sumdeltasq = ((a - b) * (a - b)).sum()\n\n    sumdeltasq /= b.size\n    sumdeltasq = np.sqrt(sumdeltasq)\n\n    eps = 1e-5\n    eps2 = 1e-10\n    psnr = 20 * np.log10((max_b + eps) / (sumdeltasq + eps2))\n\n    return psnr", "\nfile_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\nclass TestNet(torch.nn.Module):\n    \"\"\"\n    Wraps a ANEGPT model so individual layers and partial\n    combinations of layers can be tested.\n    \"\"\"\n    def __init__(self, ane: ANEGPT):\n        super().__init__()\n        self.ane = ane\n\n    def forward(self, x, qk_mask=None, k_mask=None, output_mask=None):\n        device = x.device\n        b, t = x.size()\n        # assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n        tok_emb = self.ane.transformer.wte(x) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.ane.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n        x = self.ane.transformer.drop(tok_emb + pos_emb) # psnr: f32:212, f16:99\n\n        x = x.transpose(1, 2).unsqueeze(2)\n        for h in self.ane.transformer.h[:1]:\n            x = h(x, qk_mask, k_mask)\n            # x = h.ln_1(x)\n            # x = h.attn(x, qk_mask=qk_mask, k_mask=k_mask) # psnr f16:89\n            # x = h.ln_2(x) # psnr f16:77\n            # x = h.mlp(x)  # psnr f16:62\n\n        x = x.permute(3, 1, 0, 2)\n        x = self.ane.lm_head(x).squeeze().unsqueeze(0)\n\n        if output_mask is not None:\n            x = torch.index_select(x, 1, output_mask)\n\n        return x", "\nmodel_name = \"gpt2\"\nprint(f\"Loading model {model_name}\")\nane = ANEGPT.from_pretrained(model_name).eval()\n\ntoken_predictor = TestNet(ane).eval()\n# token_predictor = ane # test the whole model\n\nrandom_tokens = torch.randint(30000, (1,10,))\ninputs_dict = ANEGPT.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)", "random_tokens = torch.randint(30000, (1,10,))\ninputs_dict = ANEGPT.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\noutput_mask = torch.tensor([13], dtype=torch.int32)\nprint(\"output_mask\", output_mask)\ninput_ids, qk_mask, k_mask = inputs_dict[\"input_ids\"], inputs_dict[\"qk_mask\"], inputs_dict[\"k_mask\"]\n\nprint(f\"Tracing the model with {input_ids.shape}\")\n\ntraced_token_predictor = torch.jit.trace(token_predictor, (input_ids, qk_mask, k_mask, output_mask))\n", "traced_token_predictor = torch.jit.trace(token_predictor, (input_ids, qk_mask, k_mask, output_mask))\n\nprint(traced_token_predictor)\n\nprint(\"Trace finished\")\nprint(\"Beginning conversion\")\n\nmlmodel = ct.convert(\n    traced_token_predictor,\n    inputs=[", "    traced_token_predictor,\n    inputs=[\n        ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.int32),\n        ct.TensorType(name=\"qk_mask\", shape=[1, 512, 1, 512], dtype=np.float32),\n        ct.TensorType(name=\"k_mask\", shape=[1, 512, 1, 1], dtype=np.float32),\n        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n    ],\n    outputs=[\n        ct.TensorType(name=\"logits\", dtype=np.float32),\n    ],", "        ct.TensorType(name=\"logits\", dtype=np.float32),\n    ],\n    compute_precision=ct.precision.FLOAT16,\n    # minimum_deployment_target=ct.target.macOS13,\n    convert_to=\"mlprogram\",\n)\n\nprint(\"Conversion finished\")\n\nwith torch.no_grad():\n    og_out = token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)\n    tr_out = traced_token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)", "\nwith torch.no_grad():\n    og_out = token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)\n    tr_out = traced_token_predictor(input_ids, qk_mask, k_mask, output_mask).to(torch.float32)\nprint(\"output_mask\", output_mask.shape, output_mask.dtype)\ncm_out = mlmodel.predict({\"input_ids\": input_ids, \"qk_mask\": qk_mask, \"k_mask\": k_mask, \"output_mask\": output_mask})\ncm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n\nassert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\nassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"", "assert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\nassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n\nprint(\"traced-og     psnr:\", compute_psnr(og_out.numpy(), tr_out.numpy()))\nprint(\"coreml-traced psnr:\", compute_psnr(tr_out.numpy(), cm_out.numpy()))\nprint(\"coreml-og     psnr:\", compute_psnr(og_out.numpy(), cm_out.numpy()))\n"]}
{"filename": "src/ml_ane_transformers/convert-ane.py", "chunked_list": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport coremltools as ct\nimport numpy as np\nfrom datetime import datetime\nfrom .ane_gpt2 import GPT as ANEGPT\nfrom src.utils.psnr import compute_psnr\n\n\"\"\"\nConvert a ANE-optimized nanoGPT to CoreML.", "\"\"\"\nConvert a ANE-optimized nanoGPT to CoreML.\n\"\"\"\n\nfile_suffix = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n\nmodel_name = \"gpt2\"\n# model_name = \"ckiplab/gpt2-tiny-chinese\"\nmodel_filename = model_name.split(\"/\")[-1] + \"_\" + file_suffix\n", "model_filename = model_name.split(\"/\")[-1] + \"_\" + file_suffix\n\nretrace = True\nif retrace:\n    print(f\"Loading model {model_name}\")\n    # token_predictor = AutoModelForCausalLM.from_pretrained(model_name, torchscript=True).eval()\n    token_predictor = ANEGPT.from_pretrained(model_name).eval()\n\n    random_tokens = torch.randint(10000, (1,10,))\n    inputs_dict = token_predictor.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\n    print(f\"Tracing the model with {inputs_dict['input_ids']}\")\n    input_ids, qk_mask, k_mask, output_mask = [inputs_dict[k] for k in\\\n                                            [\"input_ids\", \"qk_mask\", \"k_mask\", \"output_mask\"]]\n    del inputs_dict[\"k_mask\"]\n    # Exclude k_mask. It's a no-op for next-token prediction.\n    traced_token_predictor = torch.jit.trace(token_predictor, (input_ids, output_mask))\n\n    #traced_token_predictor.save(f\"{model_filename}.pt\")\nelse:\n    print(\"Loading from saved file\")\n    traced_token_predictor = torch.jit.load(f\"{model_filename}.pt\")", "\nprint(traced_token_predictor)\n\nprint(\"Trace finished\")\nprint(\"Beginning conversion\")\n\n# Going totally F16 is too much. The PSNR drops dramatically and generation is completely garbled.\ndef op_selector(op):\n    \"\"\"\n    Return true to use float16 for the op. Must be f16 to run on Neural Engine.\n\n    You can find op_type by looking in Netron and print out the op names here\n    (usually they contain a variable name).\n    \"\"\"\n    # All the ops involved in LayerNorm. Keep this in f32.\n    # LayerNorm is where we lose most of our precision. Interestingly, it seems\n    # the first mean contributes almost all the error.\n    # TODO: This may only be a problem for \"gpt\", never tried the larger variants.\n    return op.op_type not in [\"reduce_mean\"] or \"channels_mean\" not in op.name", "\ncompute_precision=ct.precision.FLOAT16\n# compute_precision=ct.transform.FP16ComputePrecision(op_selector)\nmlmodel = ct.convert(\n    traced_token_predictor,\n    inputs=[\n        ct.TensorType(name=\"input_ids\", shape=[1, 512], dtype=np.int32),\n        # ct.TensorType(name=\"qk_mask\", shape=[1, 512, 1, 512], dtype=np.float32),\n        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n    ],", "        ct.TensorType(name=\"output_mask\", shape=[1], dtype=np.int32),\n    ],\n    outputs=[\n        ct.TensorType(name=\"logits\", dtype=np.float32),\n    ],\n    compute_precision=compute_precision,\n    # minimum_deployment_target=ct.target.macOS13,\n    convert_to=\"mlprogram\",\n)\n", ")\n\n\nprint(\"Conversion finished\")\nsuffix = \"\"\nif compute_precision == ct.precision.FLOAT32:\n    suffix=\"-f32\"\n\n# Save first, sometimes CoreML segfaults.\nprint(\"Saving\")", "# Save first, sometimes CoreML segfaults.\nprint(\"Saving\")\nmlmodel.save(f\"{model_filename}{suffix}-trash-nosplit-allf16.mlpackage\")\n\n# Always compare in float32 so we don't overflow.\nwith torch.no_grad():\n    og_out = token_predictor(input_ids, output_mask=output_mask).to(torch.float32)\n    tr_out = traced_token_predictor(input_ids, output_mask=output_mask).to(torch.float32)\nprint({k: f\"{v.shape}-{v.dtype}\" for k,v in inputs_dict.items()})\ndel inputs_dict[\"qk_mask\"]", "print({k: f\"{v.shape}-{v.dtype}\" for k,v in inputs_dict.items()})\ndel inputs_dict[\"qk_mask\"]\n# del inputs_dict[\"k_mask\"]\ncm_out = mlmodel.predict(inputs_dict)\ncm_out = torch.from_numpy(cm_out[\"logits\"]).to(torch.float32)\n\nassert og_out.shape == cm_out.shape, f\"{og_out.shape} != {cm_out.shape}\"\nassert og_out.dtype == cm_out.dtype, f\"{og_out.dtype} != {cm_out.dtype}\"\n\n", "\n\nprint(\"this should be quite high. probably >200 or more.\")\nprint(\"traced-original psnr:\", compute_psnr(tr_out.numpy(), og_out.numpy()))\nprint(\"\\nthese should be >60, ideally much higher.\") # otherwise you're only going to generate gibberish\nprint(\"coreml-traced   psnr:\", compute_psnr(cm_out.numpy(), tr_out.numpy()))\nprint(\"coreml-original psnr:\", compute_psnr(cm_out.numpy(), og_out.numpy()))\n# np.testing.assert_allclose(og_out.numpy(), tr_out.numpy(), atol=1e-5, rtol=1e-4)\n# np.testing.assert_allclose(cm_out, tr_out.numpy(), atol=1e-5, rtol=1e-4)\n", "# np.testing.assert_allclose(cm_out, tr_out.numpy(), atol=1e-5, rtol=1e-4)\n"]}
{"filename": "src/ml_ane_transformers/ane/layer_norm.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\nimport torch\nimport torch.nn as nn\n\nclass LayerNormANE(nn.Module):\n    \"\"\" LayerNorm optimized for Apple Neural Engine (ANE) execution\n\n    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        if self.elementwise_affine:\n            self.weight = nn.Parameter(torch.Tensor(num_channels))\n            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def check_overunderflow(self, x):\n        f16_min = torch.finfo(torch.float16).min\n        f16_max = torch.finfo(torch.float16).max\n\n        # Check for values that are outside the range of float16\n        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n        if ouf:\n            print(\"x is out of bounds\", x)\n\n    def forward(self, inputs):\n        input_rank = len(inputs.size())\n\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n        if input_rank == 3 and inputs.size(2) == self.num_channels:\n            inputs = inputs.transpose(1, 2).unsqueeze(2)\n            input_rank = len(inputs.size())\n\n        assert input_rank == self.expected_rank\n        assert inputs.size(1) == self.num_channels\n        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\n        if self.clip_mag is not None:\n            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\n        channels_mean = inputs.mean(dim=1, keepdims=True)\n\n        zero_mean = inputs - channels_mean\n\n        zero_mean_sq = zero_mean * zero_mean\n\n        denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        out = zero_mean * denom\n\n        if self.elementwise_affine:\n            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n                   ) * self.weight.view(1, self.num_channels, 1, 1)\n        # Deviate from ANE implementation, match nn.\n        # if self.elementwise_affine:\n        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n\n        return out", "class LayerNormANE(nn.Module):\n    \"\"\" LayerNorm optimized for Apple Neural Engine (ANE) execution\n\n    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        if self.elementwise_affine:\n            self.weight = nn.Parameter(torch.Tensor(num_channels))\n            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def check_overunderflow(self, x):\n        f16_min = torch.finfo(torch.float16).min\n        f16_max = torch.finfo(torch.float16).max\n\n        # Check for values that are outside the range of float16\n        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n        if ouf:\n            print(\"x is out of bounds\", x)\n\n    def forward(self, inputs):\n        input_rank = len(inputs.size())\n\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n        if input_rank == 3 and inputs.size(2) == self.num_channels:\n            inputs = inputs.transpose(1, 2).unsqueeze(2)\n            input_rank = len(inputs.size())\n\n        assert input_rank == self.expected_rank\n        assert inputs.size(1) == self.num_channels\n        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\n        if self.clip_mag is not None:\n            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\n        channels_mean = inputs.mean(dim=1, keepdims=True)\n\n        zero_mean = inputs - channels_mean\n\n        zero_mean_sq = zero_mean * zero_mean\n\n        denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        out = zero_mean * denom\n\n        if self.elementwise_affine:\n            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n                   ) * self.weight.view(1, self.num_channels, 1, 1)\n        # Deviate from ANE implementation, match nn.\n        # if self.elementwise_affine:\n        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n\n        return out"]}
{"filename": "src/ml_ane_transformers/ane/ffn.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom .layer_norm import LayerNormANE", "\nfrom .layer_norm import LayerNormANE\n\ndef new_gelu(x):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass FFN(nn.Module):\n\n    def __init__(self, embed_dim, ffn_dim, dropout=0.1, **kwargs):\n        super().__init__()\n        self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n        # gelu\n        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n        self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n\n    def _forward_impl(self, x, **kwargs):\n        x = self.c_fc(x)\n        # Match OpenAI GPT implementation, gelu instead of relu.\n        x = new_gelu(x)\n        x = self.dropout(x)\n        x = self.c_proj(x)\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)", "\nclass FFN(nn.Module):\n\n    def __init__(self, embed_dim, ffn_dim, dropout=0.1, **kwargs):\n        super().__init__()\n        self.c_fc = nn.Conv2d(embed_dim, ffn_dim, 1)\n        # gelu\n        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n        self.c_proj = nn.Conv2d(ffn_dim, embed_dim, 1)\n\n    def _forward_impl(self, x, **kwargs):\n        x = self.c_fc(x)\n        # Match OpenAI GPT implementation, gelu instead of relu.\n        x = new_gelu(x)\n        x = self.dropout(x)\n        x = self.c_proj(x)\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)", "\n\nclass ResidualFFN(FFN):\n\n    def __init__(self, embed_dim, dropout=0.1, drop_fn=nn.Dropout, **kwargs):\n        super().__init__(embed_dim, dropout=dropout, **kwargs)\n        self.rdropout = drop_fn(dropout) if dropout > 0. else nn.Identity()\n        self.rnorm = LayerNormANE(embed_dim)\n\n    def forward(self, x):\n        residual = self._forward_impl(x)\n        return self.rnorm(self.rdropout(residual) + x)", "\n\nclass PreNormResidualFFN(ResidualFFN):\n\n    def forward(self, x):\n        residual = self.rdropout(self._forward_impl(self.rnorm(x)))\n        return x + residual"]}
{"filename": "src/ml_ane_transformers/ane/kahan_layer_norm.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\nimport torch\nimport torch.nn as nn\n\nclass KahanLayerNormANE(nn.Module):\n    \"\"\"\n    LayerNorm optimized for Apple Neural Engine (ANE) execution, using Kahan summation\n    to attempt to reduce numerical error. Unfortunately in practice, it slightly increases it.\n\n    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        if self.elementwise_affine:\n            self.weight = nn.Parameter(torch.Tensor(num_channels))\n            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def check_overunderflow(self, x):\n        f16_min = torch.finfo(torch.float16).min\n        f16_max = torch.finfo(torch.float16).max\n\n        # Check for values that are outside the range of float16\n        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n        if ouf:\n            print(\"x is out of bounds\", x)\n\n    @staticmethod\n    def kahan_mean(inputs, size: int = 4):\n        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n        s = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n        c = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n        rc = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n\n        # inputs = torch.sort(inputs, dim=1).values\n        for batch in inputs.chunk(size, dim=1):\n            # print(\"\\n----\\nbatch\", batch)\n            # print(\"batch_sum:\", batch.sum(dim=1, keepdims=True))\n            # print(\"c:\", c)\n            batch_sum = batch.sum(dim=1, keepdims=True)\n            # y = batch_sum - c\n            t = batch_sum\n\n            # KBN sum\n            abss = torch.abs(s)\n            abst = torch.abs(batch_sum)\n            max_st = torch.where(abss >= abst, s, batch_sum)\n            min_st = torch.where(abss < abst, s, batch_sum)\n            # print(max_st, min_st)\n            c += (max_st - batch_sum) + min_st\n\n            # print(\"y = batch_sum - c\\n\", y)\n            # t = s + y\n            # print(\"t = s + y\\n\", t)\n            # print(c.shape, t.shape, s.shape, y.shape)\n            # print(c.dtype, t.dtype, s.dtype, y.dtype)\n            # c = (t - s) - y\n            # rc += c\n            # print(\"c = (t-s) - y\\n\", c)\n            s = t\n            # print(\"s = t\\n\", s)\n\n            # batch_sum = batch.sum(dim=1, keepdims=True)\n            # print(\"batch\")\n            # print(batch)\n            # print(\"batch_sum\")\n            # print(batch_sum)\n            # batch_comp = batch_sum - torch.sum(batch - c)\n            # s += batch_sum\n            # c = batch_comp\n        # print(\"c\", c)\n\n        # print(s, rc)\n        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n        # print(\"kahan mean\", s / inputs.size(1))\n        return (s / inputs.size(1)) + (c / inputs.size(1))\n\n    @staticmethod\n    def stable_mean(inputs, size: int = 4):\n        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n        # (x+y+z) / 3 = (x+y)/3 + z/3\n        m = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n        for batch in inputs.chunk(size, dim=1):\n            m += batch.sum(dim=1, keepdims=True) / inputs.size(1)\n\n        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n        # print(\"stable mean\", m)\n        return m\n\n\n    def forward(self, inputs):\n        input_rank = len(inputs.size())\n\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n        if input_rank == 3 and inputs.size(2) == self.num_channels:\n            inputs = inputs.transpose(1, 2).unsqueeze(2)\n            input_rank = len(inputs.size())\n\n        assert input_rank == self.expected_rank\n        assert inputs.size(1) == self.num_channels\n        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\n        if self.clip_mag is not None:\n            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\n        # Reference implementation.\n        # channels_mean = inputs.mean(dim=1, keepdims=True)\n\n        # zero_mean = inputs - channels_mean\n\n        # zero_mean_sq = zero_mean * zero_mean\n\n        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        # out = zero_mean * denom\n\n        # Kahan implementation.\n        size = 4\n        channels_mean = self.kahan_mean(inputs, size=size)\n        # channels_mean = self.stable_mean(inputs, size=size)\n\n        zero_mean = inputs - channels_mean\n\n        zero_mean_sq = zero_mean * zero_mean\n\n        denom = (self.kahan_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n        # denom = (self.stable_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        out = zero_mean * denom\n\n        if self.elementwise_affine:\n            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n                   ) * self.weight.view(1, self.num_channels, 1, 1)\n        # Deviate from ANE implementation, match nn.\n        # if self.elementwise_affine:\n        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n\n        return out", "class KahanLayerNormANE(nn.Module):\n    \"\"\"\n    LayerNorm optimized for Apple Neural Engine (ANE) execution, using Kahan summation\n    to attempt to reduce numerical error. Unfortunately in practice, it slightly increases it.\n\n    Note: This layer only supports normalization over the final dim. It expects `num_channels`\n    as an argument and not `normalized_shape` which is used by `torch.nn.LayerNorm`.\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        if self.elementwise_affine:\n            self.weight = nn.Parameter(torch.Tensor(num_channels))\n            self.bias = nn.Parameter(torch.Tensor(num_channels))\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def check_overunderflow(self, x):\n        f16_min = torch.finfo(torch.float16).min\n        f16_max = torch.finfo(torch.float16).max\n\n        # Check for values that are outside the range of float16\n        ouf = torch.any(x < f16_min) or torch.any(x > f16_max) or torch.isnan(x).any() or torch.isinf(x).any()\n        if ouf:\n            print(\"x is out of bounds\", x)\n\n    @staticmethod\n    def kahan_mean(inputs, size: int = 4):\n        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n        s = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n        c = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n        rc = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # compensation\n\n        # inputs = torch.sort(inputs, dim=1).values\n        for batch in inputs.chunk(size, dim=1):\n            # print(\"\\n----\\nbatch\", batch)\n            # print(\"batch_sum:\", batch.sum(dim=1, keepdims=True))\n            # print(\"c:\", c)\n            batch_sum = batch.sum(dim=1, keepdims=True)\n            # y = batch_sum - c\n            t = batch_sum\n\n            # KBN sum\n            abss = torch.abs(s)\n            abst = torch.abs(batch_sum)\n            max_st = torch.where(abss >= abst, s, batch_sum)\n            min_st = torch.where(abss < abst, s, batch_sum)\n            # print(max_st, min_st)\n            c += (max_st - batch_sum) + min_st\n\n            # print(\"y = batch_sum - c\\n\", y)\n            # t = s + y\n            # print(\"t = s + y\\n\", t)\n            # print(c.shape, t.shape, s.shape, y.shape)\n            # print(c.dtype, t.dtype, s.dtype, y.dtype)\n            # c = (t - s) - y\n            # rc += c\n            # print(\"c = (t-s) - y\\n\", c)\n            s = t\n            # print(\"s = t\\n\", s)\n\n            # batch_sum = batch.sum(dim=1, keepdims=True)\n            # print(\"batch\")\n            # print(batch)\n            # print(\"batch_sum\")\n            # print(batch_sum)\n            # batch_comp = batch_sum - torch.sum(batch - c)\n            # s += batch_sum\n            # c = batch_comp\n        # print(\"c\", c)\n\n        # print(s, rc)\n        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n        # print(\"kahan mean\", s / inputs.size(1))\n        return (s / inputs.size(1)) + (c / inputs.size(1))\n\n    @staticmethod\n    def stable_mean(inputs, size: int = 4):\n        assert inputs.size(1) % size == 0, \"Batch size must be divisible by channels size.\"\n        # (x+y+z) / 3 = (x+y)/3 + z/3\n        m = torch.zeros((1,1,1,inputs.size(-1)), dtype=inputs.dtype, device=inputs.device) # total\n        for batch in inputs.chunk(size, dim=1):\n            m += batch.sum(dim=1, keepdims=True) / inputs.size(1)\n\n        # print(\"torch.mean\", inputs.mean(dim=1, keepdims=True))\n        # print(\"stable mean\", m)\n        return m\n\n\n    def forward(self, inputs):\n        input_rank = len(inputs.size())\n\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        # Migrate the data format from BSC to BC1S (most conducive to ANE)\n        if input_rank == 3 and inputs.size(2) == self.num_channels:\n            inputs = inputs.transpose(1, 2).unsqueeze(2)\n            input_rank = len(inputs.size())\n\n        assert input_rank == self.expected_rank\n        assert inputs.size(1) == self.num_channels\n        assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\n        if self.clip_mag is not None:\n            inputs.clamp_(-self.clip_mag, self.clip_mag)\n\n        # Reference implementation.\n        # channels_mean = inputs.mean(dim=1, keepdims=True)\n\n        # zero_mean = inputs - channels_mean\n\n        # zero_mean_sq = zero_mean * zero_mean\n\n        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        # out = zero_mean * denom\n\n        # Kahan implementation.\n        size = 4\n        channels_mean = self.kahan_mean(inputs, size=size)\n        # channels_mean = self.stable_mean(inputs, size=size)\n\n        zero_mean = inputs - channels_mean\n\n        zero_mean_sq = zero_mean * zero_mean\n\n        denom = (self.kahan_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n        # denom = (self.stable_mean(zero_mean_sq, size=size) + self.eps).rsqrt()\n        # denom = (zero_mean_sq.mean(dim=1, keepdims=True) + self.eps).rsqrt()\n\n        out = zero_mean * denom\n\n        if self.elementwise_affine:\n            out = (out + self.bias.view(1, self.num_channels, 1, 1)\n                   ) * self.weight.view(1, self.num_channels, 1, 1)\n        # Deviate from ANE implementation, match nn.\n        # if self.elementwise_affine:\n        #     out = (out * self.weight.view(1, self.num_channels, 1, 1)\n        #            ) + self.bias.view(1, self.num_channels, 1, 1)\n\n        return out"]}
{"filename": "src/ml_ane_transformers/ane/dummy_layer_norm.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\nimport torch\nimport torch.nn as nn\n\nclass DummyLayerNormANE(nn.Module):\n    \"\"\"\n    This looks like the ANELayerNorm class, but it just calls through to the\n    torch batch norm (mostly same inputs/outputs as layer norm) and we replace\n    that with a custom coremltools mb (torch layer norm doesn't support\n    the 1st dimension, MIL does).\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        self.bn = nn.BatchNorm2d(num_channels, eps=self.eps, affine=self.elementwise_affine)\n\n    def forward(self, inputs):\n        out = self.bn(inputs)\n        return out", "class DummyLayerNormANE(nn.Module):\n    \"\"\"\n    This looks like the ANELayerNorm class, but it just calls through to the\n    torch batch norm (mostly same inputs/outputs as layer norm) and we replace\n    that with a custom coremltools mb (torch layer norm doesn't support\n    the 1st dimension, MIL does).\n    \"\"\"\n\n    def __init__(self,\n                 num_channels,\n                 clip_mag=None,\n                 eps=1e-5,\n                 elementwise_affine=True):\n        \"\"\"\n        Args:\n            num_channels:       Number of channels (C) where the expected input data format is BC1S. S stands for sequence length.\n            clip_mag:           Optional float value to use for clamping the input range before layer norm is applied.\n                                If specified, helps reduce risk of overflow.\n            eps:                Small value to avoid dividing by zero\n            elementwise_affine: If true, adds learnable channel-wise shift (bias) and scale (weight) parameters\n        \"\"\"\n        super().__init__()\n        # Principle 1: Picking the Right Data Format (machinelearning.apple.com/research/apple-neural-engine)\n        self.expected_rank = len('BC1S')\n\n        self.num_channels = num_channels\n        self.eps = eps\n        self.clip_mag = clip_mag\n        self.elementwise_affine = elementwise_affine\n\n        self.bn = nn.BatchNorm2d(num_channels, eps=self.eps, affine=self.elementwise_affine)\n\n    def forward(self, inputs):\n        out = self.bn(inputs)\n        return out"]}
{"filename": "src/ml_ane_transformers/ane/__init__.py", "chunked_list": [""]}
{"filename": "src/ml_ane_transformers/ane/multihead_attention.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\nimport torch\nimport torch.nn as nn\n\nfrom enum import Enum\n", "from enum import Enum\n\nfrom .layer_norm import LayerNormANE\n\nclass AttentionImplementations(Enum):\n    #ORIGINAL = \"ORIGINAL\"\n    EINSUM = \"EINSUM\"\n    SPLIT_EINSUM = \"SPLIT_EINSUM\"\n\nATTENTION_IMPLEMENTATION_IN_EFFECT = AttentionImplementations.SPLIT_EINSUM", "\nATTENTION_IMPLEMENTATION_IN_EFFECT = AttentionImplementations.SPLIT_EINSUM\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" Multi-Head Attention optimized for efficient ANE deployment\n    \"\"\"\n\n    def __init__(self,\n                 embed_dim,\n                 d_qk=None,\n                 d_v=None,\n                 d_out=None,\n                 n_head=8,\n                 dropout=0.1,\n                 **kwargs):\n        \"\"\"\n        Args:\n            embed_dim:          Dimensionality of the input embeddings\n            d_qk:               Dimensionality of the query and key embeddings. They must match in order to compute\n                                dot product attention. If None, it is set to that of the input tensors, i.e. `embed_dim`\n            d_v:                Dimensionality of the value embeddings. It may differ from that of the query and\n                                key embeddings. If None, it is set to that of the input tensors.\n            d_out:              Dimensionality of the output projection. If None, it is set to that of the input tensors\n            n_head:             The number of different attention heads to compute in parallel, uses :math:`d_qk/n_head`\n                                channel groups in parallel to learn different attention patterns in a single layer\n            dropout:            The probability that each attention weight is zero-masked independent of other weights\n        \"\"\"\n        super().__init__()\n\n        self.d_qk = d_qk or embed_dim\n        self.d_v = d_v or embed_dim\n        self.d_out = d_out or embed_dim\n\n        self.n_head = n_head\n        if self.d_qk % self.n_head != 0 or self.d_v % self.n_head != 0:\n            raise ValueError(\n                f\"Either query-key dimensions ({self.d_qk}) or the value embeddings \"\n                f\"dimensions ({self.d_v}) is not divisible by n_head ({self.n_head})\"\n            )\n        self.q_normalize_fact = float(self.d_qk // self.n_head)**-0.5\n\n        self.q_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        self.v_proj = nn.Conv2d(embed_dim, self.d_v, 1)\n        self.k_proj = nn.Conv2d(embed_dim, self.d_qk, 1)\n        self.out_proj = nn.Conv2d(self.d_v, self.d_out, 1)\n        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n\n        self.apply(self._reset_parameters)\n\n    @staticmethod\n    def _reset_parameters(module):\n        if isinstance(module, nn.Conv2d):\n            nn.init.xavier_uniform_(module.weight)\n            nn.init.constant_(module.bias, 0.)\n\n    def _qk(self, q, k, v):\n        # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n        # Split q, k and v to compute a list of single-head attention functions\n        mh_q = q.split(\n            self.d_qk // self.n_head,\n            dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n        # Principle 3: Minimizing Memory Copies\n        # Avoid as many transposes and reshapes as possible\n        mh_k = k.transpose(1, 3).split(\n            self.d_qk // self.n_head,\n            dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n        mh_v = v.split(\n            self.d_v // self.n_head,\n            dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\n        # `qk = q @ k`\n        attn_weights = [\n            torch.einsum('bchq,bkhc->bkhq', [qi, ki]) * self.q_normalize_fact\n            for qi, ki in zip(mh_q, mh_k)\n        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\n        attn_weights = torch.cat(attn_weights, dim=2)\n        return attn_weights\n\n    def _qk_softmax(self, q, k, v, qk_mask=None, k_mask=None):\n        # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n        # Split q, k and v to compute a list of single-head attention functions\n\n        # Do the scalar multiply before the matmul so it can be folded into the conv.\n        q = q * self.q_normalize_fact\n\n        mh_q = q.split(\n            self.d_qk // self.n_head,\n            dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n        # Principle 3: Minimizing Memory Copies\n        # Avoid as many transposes and reshapes as possible\n        mh_k = k.transpose(1, 3).split(\n            self.d_qk // self.n_head,\n            dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n        mh_v = v.split(\n            self.d_v // self.n_head,\n            dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\n        # `qk = q @ k`\n        attn_weights = [\n            torch.einsum('bchq,bkhc->bkhq', [qi, ki])\n            for qi, ki in zip(mh_q, mh_k)\n        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\n        # Apply attention masking\n        if qk_mask is not None:\n            for head_idx in range(self.n_head):\n                attn_weights[head_idx] = attn_weights[head_idx] + qk_mask\n                # attn_weights[head_idx] = attn_weights[head_idx].masked_fill(torch.tril(torch.ones(5, 5)).t().view(1,5,1,5) == 0, float(\"-inf\"))#-1e4)\n        if k_mask is not None:\n            for head_idx in range(self.n_head):\n                attn_weights[head_idx] = attn_weights[head_idx] + k_mask\n\n        attn_weights = [aw.softmax(dim=1) for aw in attn_weights\n                        ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n        mh_w = [self.dropout(aw) for aw in attn_weights\n                ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\n        # (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n        attn = [\n            torch.einsum('bkhq,bchk->bchq', wi, vi)\n            for wi, vi in zip(mh_w, mh_v)\n        ]  # n_head * (batch_size, d_v/n_head, 1, tgt_seq_len)\n        attn = torch.cat(attn, dim=2)  # (batch_size, d_v, 1, tgt_seq_len)\n        return attn\n\n    def _attention_fn(self, q, k, v, qk_mask, k_mask, return_weights):\n        \"\"\"Core routine for computing multi-head attention\n\n        Args:\n            q:              Projected query embeddings of shape (batch_size, d_qk, 1, tgt_seq_len)\n            k:              Projected key embeddings of shape (batch_size, d_qk, 1, src_seq_len)\n            v:              Projected value embeddings of shape (batch_size, d_v, 1, src_seq_len)\n            qk_mask:        Float tensor of shape (batch_size, src_seq_len, 1, tgt_seq_len).\n                            Indices with the a high negative value, e.g. -1e4, are excluded from attention\n            k_mask:         Float tensor of shape (batch_size, src_seq_len, 1, 1).\n                            Indices with the a high negative value, e.g. -1e4, are excluded from attention\n\n        Returns:\n            attn:           Attention embeddings of shape (batch_size, d_v, 1, tgt_seq_len)\n            attn_weights:   If `return_weights` is True, returns the softmax attention weights used to compute the attention matrix\n        \"\"\"\n        if ATTENTION_IMPLEMENTATION_IN_EFFECT == AttentionImplementations.EINSUM:\n            # Stolen/inspired from ml-stable-diffusion repo. No clue if it works,\n            # had a bug that turned it off by mistake so never tried.\n            bs = q.size(0)\n            dim_head = self.d_qk // self.n_head\n            mh_q = q.view(bs, self.n_head, dim_head, -1)\n            mh_k = k.view(bs, self.n_head, dim_head, -1)\n            mh_v = v.view(bs, self.n_head, dim_head, -1)\n\n            # print(\"qkv\", q.shape, k.shape, v.shape)\n            # print(\"mhqkv\", mh_q.shape, mh_k.shape, mh_v.shape)\n\n            attn_weights = torch.einsum(\"bhcq,bhck->bhqk\", [mh_q, mh_k]) # 1,64,12,20 @ 1,64,20,12 = 1,64,20,20\n            attn_weights.mul_(self.q_normalize_fact)\n\n            # print(\"attn,qk\", attn_weights.shape, qk_mask.shape, k_mask.shape if k_mask is not None else None)\n\n            if qk_mask is not None:\n                qk_mask = qk_mask.squeeze(2).unsqueeze(0)\n                attn_weights = attn_weights + qk_mask\n            if k_mask is not None:\n                k_mask = k_mask.squeeze(2).unsqueeze(0)\n                attn_weights = attn_weights + k_mask\n\n            attn_weights = attn_weights.softmax(dim=3)\n\n            attn = torch.einsum(\"bhqk,bhck->bhcq\", [attn_weights, mh_v])\n            attn = attn.contiguous().view(bs, self.d_out, 1, -1)\n        elif ATTENTION_IMPLEMENTATION_IN_EFFECT == AttentionImplementations.SPLIT_EINSUM:\n            # Do the scalar multiply before the matmul so it can be folded into the conv.\n            q = q * self.q_normalize_fact\n\n            # Principle 2: Chunking Large Intermediate Tensors  (machinelearning.apple.com/research/apple-neural-engine)\n            # Split q, k and v to compute a list of single-head attention functions\n            mh_q = q.split(\n                self.d_qk // self.n_head,\n                dim=1)  # n_head * (batch_size, d_qk/n_head, 1, tgt_seq_len)\n            # Principle 3: Minimizing Memory Copies\n            # Avoid as many transposes and reshapes as possible\n            mh_k = k.transpose(1, 3).split(\n                self.d_qk // self.n_head,\n                dim=3)  # n_head * (batch_size, src_seq_len, 1, d_qk/n_head)\n            mh_v = v.split(\n                self.d_v // self.n_head,\n                dim=1)  # n_head * (batch_size, d_v/n_head, 1, src_seq_len)\n\n            # `qk = q @ k`\n            attn_weights = [\n                torch.einsum('bchq,bkhc->bkhq', [qi, ki])\n                for qi, ki in zip(mh_q, mh_k)\n            ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\n            # Apply attention masking\n            if qk_mask is not None:\n                for head_idx in range(self.n_head):\n                    attn_weights[head_idx] = attn_weights[head_idx] + qk_mask\n            if k_mask is not None:\n                for head_idx in range(self.n_head):\n                    attn_weights[head_idx] = attn_weights[head_idx] + k_mask\n\n            # `w = F.softmax(qk.float(), dim=-1)`\n            attn_weights = [aw.softmax(dim=1) for aw in attn_weights\n                            ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n            mh_w = [self.dropout(aw) for aw in attn_weights\n                    ]  # n_head * (batch_size, src_seq_len, 1, tgt_seq_len)\n\n            # (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n            attn = [\n                torch.einsum('bkhq,bchk->bchq', wi, vi)\n                for wi, vi in zip(mh_w, mh_v)\n            ]  # n_head * (batch_size, d_v/n_head, 1, tgt_seq_len)\n            attn = torch.cat(attn, dim=1)  # (batch_size, d_v, 1, tgt_seq_len)\n\n        if return_weights:\n            return attn, attn_weights\n        return attn, None\n\n    def _forward_impl(\n        self,\n        q,\n        k,\n        v,\n        qpos=None,\n        kpos=None,\n        vpos=None,\n        qk_mask=None,\n        k_mask=None,\n        return_weights=True,\n    ):\n        \"\"\"\n        Args:\n            q:                  Query embeddings of shape (batch_size, embed_dim, 1, tgt_seq_len)\n            k:                  Key embeddings of shape (batch_size, embed_dim, 1, src_seq_len)\n            v:                  Value embeddings of shape (batch_size, embed_dim, 1, src_seq_len)\n            qpos:               Positional encodings for the query embeddings with same shape as `q`\n            kpos:               Positional encodings for the key embeddings with same shape as `k`\n            vpos:               Positional encodings for the key embeddings with same shape as `v`\n            qk_mask:            Float tensor with shape (batch_size, src_seq_len, 1, tgt_seq_len). Example use case: for causal masking\n                                in generative language models (e.g. GPT), fill the upper triangular part with a high negative value (e.g. -1e4).\n                                Indices with the a high negative value, e.g. -1e4, are excluded from attention\n            k_mask:             Float tensor with shape (batch_size, src_seq_len, 1, 1). Example use case: when excluding embeddings that\n                                correspond to zero-padded pixels in an image or unused tokens in a text token sequence from attention.\n                                Indices with the a high negative value, e.g. -1e4, are excluded from attention\n            return_weights:     If True, returns the intermediate attention weights\n\n\n        Note: If any of q,k,v has shape (batch_size, embed_dim, height, width) that represent a 2-d feature map, this will\n        be flattened to (batch_size, embed_dim, 1, height * width)\n\n        Note: `attn_weights` are never passed downstream even when return_weights=True because all the attn_weights\n        are harvested from the outermost module (e.g. ane_transformers.model#Transformer) by means of forward hooks\n        \"\"\"\n        # Parse tensor shapes for source and target sequences\n        assert len(q.size()) == 4 and len(k.size()) == 4 and len(v.size()) == 4, f\"q:{q.size()} k:{k.size()} v:{v.size()}\"\n        b, ct, ht, wt = q.size()\n        b, cs, hs, ws = k.size()\n\n        tgt_seq_len = ht * wt\n        src_seq_len = hs * ws\n\n        # Add positional encodings if any\n        if qpos is not None:\n            q = q + qpos\n        if kpos is not None:\n            k = k + kpos\n        if vpos is not None:\n            v = v + kpos\n\n        # Project q,k,v\n        q = self.q_proj(q)\n        k = self.k_proj(k)\n        v = self.v_proj(v)\n\n        # Validate qk_mask (`attn_mask` in `torch.nn.MultiheadAttention`)\n        expected_qk_mask_shape = [b, src_seq_len, 1, tgt_seq_len]\n        if qk_mask is not None:\n            if qk_mask.dtype != torch.float32:\n                raise RuntimeError(\n                    f\"`qk_mask` must be of type torch.float32, received {qk_mask.dtype}\"\n                )\n            if list(qk_mask.size()) != expected_qk_mask_shape:\n                raise RuntimeError(\n                    f\"Invalid shape for `qk_mask` (Expected {expected_qk_mask_shape}, got {list(qk_mask.size())}\"\n                )\n\n        # Validate k_mask (`key_padding_mask` in `torch.nn.MultiheadAttention`)\n        expected_k_mask_shape = [b, src_seq_len, 1, 1]\n        if k_mask is not None:\n            if k_mask.dtype != torch.float32:\n                raise RuntimeError(\n                    f\"`k_mask` must be of type torch.float32, received {k_mask.dtype}\"\n                )\n            if list(k_mask.size()) != expected_k_mask_shape:\n                raise RuntimeError(\n                    f\"Invalid shape for `k_mask` (Expected {expected_k_mask_shape}, got {list(k_mask.size())}\"\n                )\n\n        # Call the attention function\n        attn, attn_weights = self._attention_fn(q, k, v, qk_mask, k_mask,\n                                                return_weights)\n\n        # Revert to original dimension permutation\n        attn = attn.contiguous().view(b, self.d_v, ht, wt)\n\n        attn = self.out_proj(attn)\n\n        # if return_weights:\n        #     return attn, attn_weights\n        return attn #, None\n\n    def forward(self, q, k, v, **kwargs):\n        return self._forward_impl(q, k, v, **kwargs)", "\n\nclass ResidualMultiHeadAttention(MultiHeadAttention):\n\n    def __init__(self, embed_dim, dropout=0.1, drop_fn=nn.Dropout, **kwargs):\n        super().__init__(embed_dim, dropout=dropout, **kwargs)\n        self.drop_fn = drop_fn(dropout) if dropout > 0. else nn.Identity()\n        self.norm = LayerNormANE(embed_dim)\n\n    def forward(self, q, k, v, **kwargs):\n        attn, attn_weights = self._forward_impl(q, k, v, **kwargs)\n        return self.norm(self.drop_fn(attn) + q), attn_weights", "\n\nclass SelfAttention(MultiHeadAttention):\n\n    def forward(self, qkv, **kwargs):\n        return super()._forward_impl(qkv, qkv, qkv, **kwargs)\n\n\nclass ResidualSelfAttention(ResidualMultiHeadAttention):\n\n    def forward(self, qkv, **kwargs):\n        attn, attn_weights = self._forward_impl(qkv, qkv, qkv, **kwargs)\n        return self.norm(self.drop_fn(attn) + qkv), attn_weights", "class ResidualSelfAttention(ResidualMultiHeadAttention):\n\n    def forward(self, qkv, **kwargs):\n        attn, attn_weights = self._forward_impl(qkv, qkv, qkv, **kwargs)\n        return self.norm(self.drop_fn(attn) + qkv), attn_weights\n\n\nclass PreNormResidualSelfAttention(ResidualSelfAttention):\n\n    def forward(self, qkv, **kwargs):\n        norm_qkv = self.norm(qkv)\n        attn, attn_weights = self._forward_impl(norm_qkv, norm_qkv, norm_qkv,\n                                                **kwargs)\n        result = self.drop_fn(attn) + qkv\n        return result, attn_weights"]}
{"filename": "src/experiments/masks_comparison.py", "chunked_list": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport coremltools as ct\nimport numpy as np\nfrom datetime import datetime\nfrom src.utils.psnr import compute_psnr\nfrom src.ml_ane_transformers.ane_gpt2 import GPT as ANEGPT\n\n\"\"\"\nExperiment to see if we need k, qk or both when we only care about the", "\"\"\"\nExperiment to see if we need k, qk or both when we only care about the\nnext token prediction.\n\nPSNR between just qk + both is very high, so seems that k is unneeded.\n\"\"\"\n\nane = ANEGPT.from_pretrained(\"gpt2\").eval()\n\nrandom_tokens = torch.randint(10000, (1,10,))", "\nrandom_tokens = torch.randint(10000, (1,10,))\ninputs_dict = ane.build_inputs(random_tokens, pad_to_length=512, pad_token_id=350)\ninput_ids, qk_mask, k_mask, output_mask = [inputs_dict[k] for k in\\\n                                            [\"input_ids\", \"qk_mask\", \"k_mask\", \"output_mask\"]]\nassert output_mask[0] == 9, f\"{output_mask} is not as expected\"\n\nwith torch.no_grad():\n    k_only = ane(input_ids, qk_mask=None, k_mask=k_mask, output_mask=output_mask)\n    qk_only = ane(input_ids, qk_mask=qk_mask, k_mask=None, output_mask=output_mask)\n    both = ane(input_ids, qk_mask=qk_mask, k_mask=k_mask, output_mask=output_mask)", "\nassert k_only.shape == qk_only.shape\nassert k_only.shape == both.shape\n\nprint(\"output shape:\", both.shape)\n\nprint(\"k v. qk psnr:\", compute_psnr(k_only, qk_only))\nprint(\"k v. both psnr:\", compute_psnr(k_only, both))\nprint(\"qk v. both psnr:\", compute_psnr(qk_only, both))\n", "print(\"qk v. both psnr:\", compute_psnr(qk_only, both))\n\n\n"]}
{"filename": "src/experiments/replace_row_model.py", "chunked_list": ["import torch\nfrom torch import nn\nimport coremltools as ct\nimport numpy as np\n\n\"\"\"\nTrying to find a way to replace a row in a matrix that can run on the Neural Engine.\n\"\"\"\n\n# new_q, new_k, new_v = q,k,v # new_q, new_k, new_v = [B, 1, n_embd]", "\n# new_q, new_k, new_v = q,k,v # new_q, new_k, new_v = [B, 1, n_embd]\n# old_k, old_v = kv_cache.chunk(2, dim=1) # each (B, T, C)\n\nclass Net(nn.Module):\n    def forward(self, new_k, old_k, mask):\n        # new_k [B, 1, n_embd]\n        # old_k [B, T, n_mbd]\n        # insert_at [1]\n\n        # mask = torch.zeros_like(old_k)\n        # mask[:, insert_at, :] = 1 # Using a symbol here fails an assert, the first : is also not cool.\n\n\n        # ValueError: Type mismatch <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'> vs. <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n        # mask = torch.zeros_like(old_k)\n        # new_mask = torch.ones_like(new_k)\n        # mask = torch.cat([mask[:, :insert_at, :], new_mask, mask[:, insert_at+1:, :]], dim=1)\n\n        # mask = torch.zeros_like(old_k)\n        # new_mask = torch.ones_like(new_k)\n        # mask = torch.cat([mask[:, :2, :], new_mask, mask[:, 2+1:, :]], dim=1)\n\n        return torch.where(mask.bool(), new_k, old_k)", "\nnet = Net().eval()\n\nnew_k = torch.tensor([[[99.,98,97]]])\nold_k = torch.tensor([[[0.,1,2],[3,4,5],[6,7,8],[9,10,11]]])\ninsert_at = torch.tensor([2])\n\nmask = torch.zeros_like(old_k)\nmask[:, insert_at, :] = 1\nprint(mask.shape)", "mask[:, insert_at, :] = 1\nprint(mask.shape)\n\nresult = net(new_k, old_k, mask)\nprint(result)\n# assert torch.equal(result[0, 2, :], new_k.squeeze())\n\ntraced = torch.jit.trace(net, (new_k, old_k, mask))\n\nprog = ct.convert(traced,", "\nprog = ct.convert(traced,\n    inputs=[\n        ct.TensorType(name=\"new_k\", shape=new_k.shape, dtype=np.float16),\n        ct.TensorType(name=\"old_k\", shape=old_k.shape, dtype=np.float16),\n        # ct.TensorType(name=\"insert_at\", shape=insert_at.shape, dtype=np.int32),\n        ct.TensorType(name=\"mask\", shape=insert_at.shape, dtype=np.float16),\n    ],\n    outputs=[\n        ct.TensorType(name=\"result\", dtype=np.float16),", "    outputs=[\n        ct.TensorType(name=\"result\", dtype=np.float16),\n    ],\n    minimum_deployment_target=ct.target.iOS16, # float16 for less casting\n    convert_to=\"milinternal\")\nprint(prog)\n\nmlmodel = ct.convert(prog,\n    minimum_deployment_target=ct.target.iOS16, # float16 for less casting\n    convert_to=\"mlprogram\")", "    minimum_deployment_target=ct.target.iOS16, # float16 for less casting\n    convert_to=\"mlprogram\")\n\nmlmodel.save(\"replace-row-model.mlpackage\")"]}
{"filename": "src/experiments/embedding_conv.py", "chunked_list": ["import torch\nfrom torch import nn\nimport numpy as np\n\n\"\"\"\nFigure if/how to replace embedding + lm head layers with conv so we can\nget to the preferred ANE shape earlier/faster.\n\nResult is the embedding is definitely not possible and the split einsum a la\nWhisper works well enough.", "Result is the embedding is definitely not possible and the split einsum a la\nWhisper works well enough.\n\"\"\"\n\n# input BT\n# gather weight TC\n# gather output BSC\n# block input (ANE optimal) BC1S\n# output from blocks: BC1S\n# ...", "# output from blocks: BC1S\n# ...\n# desired output BST\n\nB,C,S,T = 1, 312, 512, 2000\n# B,C,S,T = 1, 4, 512, 2000\n# B,C,S,T = 1, 4, 6, 10\n\nassert C % 2 == 0\nassert T % 2 == 0", "assert C % 2 == 0\nassert T % 2 == 0\n\ntorch.manual_seed(42)\n\nconv = nn.Conv2d(C, T, 1, bias=False) # lm_head\nemb = nn.Embedding(T, C)\nemb.weight = nn.Parameter(conv.weight.squeeze())\nlin = nn.Linear(C,T, bias=False)\nlin.weight = nn.Parameter(conv.weight.squeeze())", "lin = nn.Linear(C,T, bias=False)\nlin.weight = nn.Parameter(conv.weight.squeeze())\n\nx = torch.randn((B, S, C))\nlin_res = lin(x)\nx = x.transpose(1, 2).unsqueeze(2)\nassert x.shape == torch.Size([B,C,1,S])\n# print(\"lin_res\", lin_res)\n# conv_res = conv(x)\n# print(\"conv_res.shape\", conv_res.shape)", "# conv_res = conv(x)\n# print(\"conv_res.shape\", conv_res.shape)\n# conv_res = conv_res.permute(0,2,3,1).squeeze(0)\n# print(\"conv_res\", conv_res)\n# print(\"lin vs conv\", torch.equal(lin_res, conv_res))\n# print(lin_res - conv_res)\n# conv2_res = conv(x.permute(3,1,0,2)).squeeze().unsqueeze(0)\n# print(\"conv_res2\", conv_res)\n# print(\"lin vs conv2\", torch.equal(lin_res, conv2_res))\n# print(lin_res - conv_res)", "# print(\"lin vs conv2\", torch.equal(lin_res, conv2_res))\n# print(lin_res - conv_res)\n\nassert x.shape == torch.Size([B,C,1,S])\nx = x.permute(0,2,3,1).squeeze(0)\nassert x.shape == torch.Size([B,S,C])\n\n# ANE can only load tensors with dim size of at most 16,384 - divide the vocab size to be less than that\n# gpt2 vocab size is a product of two primes smh\nprint(lin.weight.shape)", "# gpt2 vocab size is a product of two primes smh\nprint(lin.weight.shape)\nsplits = emb.weight.split(emb.weight.shape[0]//2, dim=0)\necat = torch.cat([torch.einsum('bid,jd->bij', x, split) for split in splits], dim=-1)#.view(*x.shape[:2], -1)\nprint(ecat.shape)\n# ecat = torch.einsum('bid,jd->bij', x, emb.weight)\nprint(ecat.shape)\nprint(lin_res - ecat)\nprint(torch.allclose(lin_res, ecat, atol=1e-6))\n", "print(torch.allclose(lin_res, ecat, atol=1e-6))\n\n\n\n# @torch.no_grad()\n# def naive_way(x):\n#     assert x.shape == torch.Size([B,C,1,S])\n#     x = x.permute(3, 1, 0, 2) # shape = (S, C, B, 1)\n#     logits = conv(x) # shape = (S, T, B, 1)\n#     logits = logits.squeeze().unsqueeze(0) # shape = (B, S, T)", "#     logits = conv(x) # shape = (S, T, B, 1)\n#     logits = logits.squeeze().unsqueeze(0) # shape = (B, S, T)\n#     return logits\n\n# # no faster than naive way\n# @torch.no_grad()\n# def post_permute_way(x):\n#     assert x.shape == torch.Size([B,C,1,S])\n#     logits = conv(x) # shape = (B, T, 1, S)\n#     assert logits.shape == torch.Size([B,T,1,S])", "#     logits = conv(x) # shape = (B, T, 1, S)\n#     assert logits.shape == torch.Size([B,T,1,S])\n#     logits = logits.permute(0,2,3,1).squeeze(1) # shape = (B, S, T)\n#      # .permute(0,3,1,2).squeeze(-1)\n#     return logits\n\n# x = torch.randn((B, C, 1, S))\n\n# print(\"naive\", naive_way(x).shape)\n# print(\"post_permute\", post_permute_way(x).shape)", "# print(\"naive\", naive_way(x).shape)\n# print(\"post_permute\", post_permute_way(x).shape)\n# # print(naive_way(x))\n# # print(post_permute_way(x))\n# print(torch.allclose(naive_way(x), post_permute_way(x), atol=1e-6))\n# print(\"default tol:\", torch.allclose(naive_way(x), post_permute_way(x)))\n\n# # @torch.no_grad()\n# # def emb_way(x):\n# #     assert x.shape == torch.Size([B,S])", "# # def emb_way(x):\n# #     assert x.shape == torch.Size([B,S])\n# #     return emb(x)\n\n# # @torch.no_grad()\n# # def conv_emb_way(x):\n# #     assert x.shape == torch.Size([B,S])\n# #     return conv(x)\n\n# # x = torch.randint(2000, (B, S))", "\n# # x = torch.randint(2000, (B, S))\n# # print(\"emb\", emb_way(x).shape)\n# # print(\"conv emb\", conv_emb_way(x).shape)\n# # print(torch.allclose(emb_way(x), conv_emb_way(x), atol=1e-6))\n\n"]}
{"filename": "src/experiments/chaining_models.py", "chunked_list": ["import torch\nimport coremltools as ct\nimport numpy as np\nimport argparse\nfrom stopwatch import Stopwatch\nfrom collections import OrderedDict\nimport time\n\n\"\"\"\nTry loading multiple models at the same time and then predict on", "\"\"\"\nTry loading multiple models at the same time and then predict on\nthem in sequence.\n\"\"\"\n\ncompute_unit_by_name = OrderedDict([\n    (\"All\", ct.ComputeUnit.ALL),\n    (\"CPUOnly\", ct.ComputeUnit.CPU_ONLY),\n    (\"CPUAndGPU\", ct.ComputeUnit.CPU_AND_GPU),\n    (\"CPUAndANE\", ct.ComputeUnit.CPU_AND_NE),", "    (\"CPUAndGPU\", ct.ComputeUnit.CPU_AND_GPU),\n    (\"CPUAndANE\", ct.ComputeUnit.CPU_AND_NE),\n])\n\nall_models = [\n    # \"test-net-22-loops.mlpackage\",\n    # \"test-net-22-loops-2.mlpackage\",\n    \"test-net-21-loops.mlpackage\",\n    \"test-net-21-loops-2.mlpackage\",\n    \"test-net-20-loops.mlpackage\",", "    \"test-net-21-loops-2.mlpackage\",\n    \"test-net-20-loops.mlpackage\",\n    \"test-net-20-loops-2.mlpackage\",\n    \"test-net-20-loops-3.mlpackage\",\n    \"test-net-20-loops-4.mlpackage\",\n]\n\nparser = argparse.ArgumentParser(description='Load many models and predict on them in sequence.')\nparser.add_argument('total_models', help='total models', type=int, choices=[x+1 for x in range(len(all_models))])\nparser.add_argument('compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")", "parser.add_argument('total_models', help='total models', type=int, choices=[x+1 for x in range(len(all_models))])\nparser.add_argument('compute_unit', help='compute unit', type=str, choices=list(compute_unit_by_name.keys()), default=\"All\")\nargs = parser.parse_args()\n\ncompute_unit = compute_unit_by_name[args.compute_unit]\nprint(\"Compute Unit:\", args.compute_unit)\n\nmodel_paths = all_models[:args.total_models]\nprint(f\"Running with {len(model_paths)} models.\")\n", "print(f\"Running with {len(model_paths)} models.\")\n\ninput_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n\nmodels = []\nload_times = []\ninitial_predict_times = []\ngroup_predict_times = []\n\n# Load each model and predict on it once.\nfor p in model_paths:\n    print(f\"Loading {p}\")\n    load_sw = Stopwatch(2)\n    mlmodel = ct.models.MLModel(p, compute_units=compute_unit)\n    load_sw.stop()\n    load_times.append(load_sw.duration)\n\n    models.append(mlmodel)\n\n    inital_pred_sw = Stopwatch(2)\n    mlmodel.predict({\"input_ids\": input_ids})\n    inital_pred_sw.stop()\n    initial_predict_times.append(inital_pred_sw.duration)", "\n# Load each model and predict on it once.\nfor p in model_paths:\n    print(f\"Loading {p}\")\n    load_sw = Stopwatch(2)\n    mlmodel = ct.models.MLModel(p, compute_units=compute_unit)\n    load_sw.stop()\n    load_times.append(load_sw.duration)\n\n    models.append(mlmodel)\n\n    inital_pred_sw = Stopwatch(2)\n    mlmodel.predict({\"input_ids\": input_ids})\n    inital_pred_sw.stop()\n    initial_predict_times.append(inital_pred_sw.duration)", "\nprint(\"Sleeping...\")\ntime.sleep(5)\n\nprint(\"Warm up group predict...\")\nfor m in models:\n    m.predict({\"input_ids\": input_ids})\nfor m in models:\n    m.predict({\"input_ids\": input_ids})\n", "\nprint(\"Predicting back to back.\")\ngroup_predict_sw = Stopwatch(2)\nfor m in models:\n    m.predict({\"input_ids\": input_ids})\n\ngroup_predict_sw.stop()\n\n\nprint(f\"{(np.average(load_times) * 1000):.2f}ms avg load time ({load_times})\")", "\nprint(f\"{(np.average(load_times) * 1000):.2f}ms avg load time ({load_times})\")\nprint(f\"{(np.average(initial_predict_times) * 1000):.2f}ms avg initial predict ({initial_predict_times})\")\nprint(f\"{(group_predict_sw.duration * 1000):.2f}ms total group predict (avg: {1000*group_predict_sw.duration / float(len(models)):.2f}ms)\")\n\n"]}
{"filename": "src/experiments/chunk_model.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE.md file.\n# Copyright (C) 2022 Apple Inc. All Rights Reserved.\n#\n\n\"\"\"\nBased on Apple's ml-stable-diffusion script to split the unet model into chunks,\nbut adapted to support splitting models into >2 chunks.\n\nOriginal License:", "\nOriginal License:\nCopyright (C) 2022 Apple Inc. All Rights Reserved.\n\nIMPORTANT: This Apple software is supplied to you by Apple Inc. (\"Apple\") in consideration of your agreement to the following terms, and your use, installation, modification or redistribution of this Apple software constitutes acceptance of these terms. If you do not agree with these terms, please do not use, install, modify or redistribute this Apple software.\n\nIn consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the \"Apple Software\"), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.\n\nThe Apple Software is provided by Apple on an \"AS IS\" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n", "The Apple Software is provided by Apple on an \"AS IS\" basis. APPLE MAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND OPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\nIN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION, MODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED AND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport argparse\nfrom collections import OrderedDict\n\nimport coremltools as ct\nfrom coremltools.converters.mil import Block, Program, Var", "import coremltools as ct\nfrom coremltools.converters.mil import Block, Program, Var\nfrom coremltools.converters.mil.frontend.milproto.load import load as _milproto_to_pymil\nfrom coremltools.converters.mil.mil import Builder as mb\nfrom coremltools.converters.mil.mil import Placeholder\nfrom coremltools.converters.mil.mil import types as types\nfrom coremltools.converters.mil.mil.passes.helper import block_context_manager\nfrom coremltools.converters.mil.mil.passes.pass_registry import PASS_REGISTRY\n# from coremltools.converters.mil.testing_utils import random_gen_input_feature_type\n", "# from coremltools.converters.mil.testing_utils import random_gen_input_feature_type\n\nimport gc\n\nimport logging\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n", "logger.setLevel(logging.INFO)\n\nimport numpy as np\nimport os\nimport shutil\nimport time\n\n\ndef _verify_output_correctness_of_chunks(full_model, first_chunk_model,\n                                         second_chunk_model):\n    \"\"\" Verifies the end-to-end output correctness of full (original) model versus chunked models\n    \"\"\"\n    # Generate inputs for first chunk and full model\n    # input_dict = {}\n    # for input_desc in full_model._spec.description.input:\n    #     input_dict[input_desc.name] = random_gen_input_feature_type(input_desc)\n\n    # # Generate outputs for first chunk and full model\n    # outputs_from_full_model = full_model.predict(input_dict)\n    # outputs_from_first_chunk_model = first_chunk_model.predict(input_dict)\n\n    # # Prepare inputs for second chunk model from first chunk's outputs and regular inputs\n    # second_chunk_input_dict = {}\n    # for input_desc in second_chunk_model._spec.description.input:\n    #     if input_desc.name in outputs_from_first_chunk_model:\n    #         second_chunk_input_dict[\n    #             input_desc.name] = outputs_from_first_chunk_model[\n    #                 input_desc.name]\n    #     else:\n    #         second_chunk_input_dict[input_desc.name] = input_dict[\n    #             input_desc.name]\n\n    # # Generate output for second chunk model\n    # outputs_from_second_chunk_model = second_chunk_model.predict(\n    #     second_chunk_input_dict)\n\n    # Verify correctness across all outputs from second chunk and full model\n    print(\"Skipping output correctness check.\")", "def _verify_output_correctness_of_chunks(full_model, first_chunk_model,\n                                         second_chunk_model):\n    \"\"\" Verifies the end-to-end output correctness of full (original) model versus chunked models\n    \"\"\"\n    # Generate inputs for first chunk and full model\n    # input_dict = {}\n    # for input_desc in full_model._spec.description.input:\n    #     input_dict[input_desc.name] = random_gen_input_feature_type(input_desc)\n\n    # # Generate outputs for first chunk and full model\n    # outputs_from_full_model = full_model.predict(input_dict)\n    # outputs_from_first_chunk_model = first_chunk_model.predict(input_dict)\n\n    # # Prepare inputs for second chunk model from first chunk's outputs and regular inputs\n    # second_chunk_input_dict = {}\n    # for input_desc in second_chunk_model._spec.description.input:\n    #     if input_desc.name in outputs_from_first_chunk_model:\n    #         second_chunk_input_dict[\n    #             input_desc.name] = outputs_from_first_chunk_model[\n    #                 input_desc.name]\n    #     else:\n    #         second_chunk_input_dict[input_desc.name] = input_dict[\n    #             input_desc.name]\n\n    # # Generate output for second chunk model\n    # outputs_from_second_chunk_model = second_chunk_model.predict(\n    #     second_chunk_input_dict)\n\n    # Verify correctness across all outputs from second chunk and full model\n    print(\"Skipping output correctness check.\")", "    # for out_name in outputs_from_full_model.keys():\n    #     torch2coreml.report_correctness(\n    #         original_outputs=outputs_from_full_model[out_name],\n    #         final_outputs=outputs_from_second_chunk_model[out_name],\n    #         log_prefix=f\"{out_name}\")\n\n\ndef _load_prog_from_mlmodel(model):\n    \"\"\" Load MIL Program from an MLModel\n    \"\"\"\n    model_spec = model.get_spec()\n    start_ = time.time()\n    logger.info(\n        \"Loading MLModel object into a MIL Program object (including the weights)..\"\n    )\n    prog = _milproto_to_pymil(\n        model_spec=model_spec,\n        specification_version=model_spec.specificationVersion,\n        file_weights_dir=model.weights_dir,\n    )\n    logger.info(f\"Program loaded in {time.time() - start_:.1f} seconds\")\n\n    return prog", "\nclass Chunk():\n    def __init__(self, start_op_idx, end_op_idx, cumulative_size_in_mb):\n        self.start_op_idx = start_op_idx\n        self.end_op_idx = end_op_idx\n        self.cumulative_size_in_mb = cumulative_size_in_mb\n\n    def save(self, prog, name):\n        mlmodel = ct.convert(\n            prog,\n            convert_to=\"mlprogram\",\n            compute_units=ct.ComputeUnit.CPU_ONLY,\n            minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n        )\n        mlmodel.save(name)\n\n    def __repr__(self):\n        return f\"<Chunk start={self.start_op_idx} end={self.end_op_idx} cumulative_size={self.cumulative_size_in_mb}MB>\"", "\ndef _get_op_idx_split_locations(prog: Program):\n    \"\"\" Find the op that approximately bisects the graph as measure by weights size on each side\n    \"\"\"\n    main_block = prog.functions[\"main\"]\n    total_size_in_mb = 0\n\n    for op in main_block.operations:\n        if op.op_type == \"const\" and isinstance(op.val.val, np.ndarray):\n            size_in_mb = op.val.val.size * op.val.val.itemsize / (1024 * 1024)\n            total_size_in_mb += size_in_mb\n    if total_size_in_mb < 1800:\n        print(\"You /may/ not need to split this model in order to get it to run on the Neural Engine.\")\n    chunk_size = 1800 # Under XXMB. 2400 too high. 1800 works for gpt2-xl.\n    # 670 for 8 chunks of 2.8b\n    next_split_size = chunk_size\n    print(f\"Total size: {total_size_in_mb}MB\")\n    print(f\"Target split size: {chunk_size}MB\")\n\n    # Find the first non const op (single child), where the total cumulative size exceeds\n    # the half size for the first time\n    chunks = []\n    cumulative_size_in_mb = 0\n    for op in main_block.operations:\n        if op.op_type == \"const\" and isinstance(op.val.val, np.ndarray):\n            size_in_mb = op.val.val.size * op.val.val.itemsize / (1024 * 1024)\n            cumulative_size_in_mb += size_in_mb\n\n        # TODO: Maybe reverse this order?\n        if (cumulative_size_in_mb > next_split_size and op.op_type != \"const\"\n                and len(op.outputs) == 1\n                and len(op.outputs[0].child_ops) == 1):\n            op_idx = main_block.operations.index(op)\n            start_op_idx = 0 if len(chunks) == 0 else chunks[-1].end_op_idx\n            chunks.append(Chunk(start_op_idx, op_idx, cumulative_size_in_mb))\n            next_split_size = cumulative_size_in_mb + chunk_size\n            logger.info(f\"current_size = {cumulative_size_in_mb}MB next split size = {next_split_size}MB\")\n            # return op_idx, cumulative_size_in_mb, total_size_in_mb\n\n    # TODO: Handle more gracefully.\n    op_idx = main_block.operations.index(main_block.operations[-1])\n    start_op_idx = 0 if len(chunks) == 0 else chunks[-1].end_op_idx\n    chunks.append(Chunk(start_op_idx, op_idx, cumulative_size_in_mb))\n    next_split_size = cumulative_size_in_mb + chunk_size\n    logger.info(f\"current_size = {cumulative_size_in_mb}MB next split size = {next_split_size}MB\")\n\n    return chunks", "\n\ndef _get_first_chunk_outputs(block, start_op_idx, op_idx):\n    # Get the list of all vars that go across from first program (all ops from 0 to op_idx (inclusive))\n    # to the second program (all ops from op_idx+1 till the end). These all vars need to be made the output\n    # of the first program and the input of the second program\n    boundary_vars = set()\n    for i in range(start_op_idx, op_idx + 1):\n        op = block.operations[i]\n        for var in op.outputs:\n            if var.val is None:  # only consider non const vars\n                for child_op in var.child_ops:\n                    child_op_idx = block.operations.index(child_op)\n                    if child_op_idx > op_idx:\n                        boundary_vars.add(var)\n    return list(boundary_vars)", "\n\n@block_context_manager\ndef _add_fp32_casts(block, boundary_vars):\n    new_boundary_vars = []\n    for var in boundary_vars:\n        if var.dtype != types.fp16:\n            new_boundary_vars.append(var)\n        else:\n            fp32_var = mb.cast(x=var, dtype=\"fp32\", name=var.name)\n            new_boundary_vars.append(fp32_var)\n    return new_boundary_vars", "\n\ndef _make_first_chunk_prog(prog, op_idx):\n    \"\"\" Build first chunk by declaring early outputs and removing unused subgraph\n    \"\"\"\n    block = prog.functions[\"main\"]\n    boundary_vars = _get_first_chunk_outputs(block, 0, op_idx)\n\n    # Due to possible numerical issues, cast any fp16 var to fp32\n    new_boundary_vars = _add_fp32_casts(block, boundary_vars)\n\n    block.outputs.clear()\n    block.set_outputs(new_boundary_vars)\n    PASS_REGISTRY[\"common::dead_code_elimination\"](prog)\n    return prog", "\n\ndef _make_second_chunk_prog(prog, previous_start_op_idx, start_op_idx, end_op_idx, is_last=False):\n    \"\"\" Build second chunk by rebuilding a pristine MIL Program from MLModel\n    \"\"\"\n    block = prog.functions[\"main\"]\n    block.opset_version = ct.target.iOS16\n\n    # First chunk outputs are second chunk inputs (e.g. skip connections)\n    boundary_vars = _get_first_chunk_outputs(block, 0, start_op_idx)\n\n    output_boundary_vars = _get_first_chunk_outputs(block, start_op_idx, end_op_idx)\n    if not is_last:\n        new_boundary_vars = _add_fp32_casts(block, output_boundary_vars)\n        block.outputs.clear()\n        block.set_outputs(new_boundary_vars)\n\n    # This op will not be included in this program. Its output var will be made into an input\n    boundary_op = block.operations[start_op_idx]\n\n    logger.info(f\"boundary vars (inputs) {previous_start_op_idx}-{start_op_idx} {[v.name for v in boundary_vars]}\")\n    logger.info(f\"boundary op: {boundary_op}\".rstrip())\n    logger.info(f\"boundary vars (outputs) {start_op_idx}-{end_op_idx} {[v.name for v in output_boundary_vars]}\")\n\n    # Add all boundary ops as inputs\n    with block:\n        for var in boundary_vars:\n            new_placeholder = Placeholder(\n                sym_shape=var.shape,\n                dtype=var.dtype if var.dtype != types.fp16 else types.fp32,\n                name=var.name,\n            )\n            # logger.info(f\"New placeholder: {var.name}\")\n\n            block._input_dict[\n                new_placeholder.outputs[0].name] = new_placeholder.outputs[0]\n\n            block.function_inputs = tuple(block._input_dict.values())\n            new_var = None\n            if var.dtype == types.fp16:\n                new_var = mb.cast(x=new_placeholder.outputs[0],\n                                  dtype=\"fp16\",\n                                  before_op=var.op)\n            else:\n                new_var = new_placeholder.outputs[0]\n\n            # logger.info(f\"Replacing {var.name} with {new_var.name}\")\n            block.replace_uses_of_var_after_op(\n                anchor_op=boundary_op,\n                old_var=var,\n                new_var=new_var,\n                # force_replace=True # For quantized models.\n            )\n\n    PASS_REGISTRY[\"common::dead_code_elimination\"](prog)\n\n    # Remove any unused inputs\n    new_input_dict = OrderedDict()\n    for k, v in block._input_dict.items():\n        if len(v.child_ops) > 0:\n            new_input_dict[k] = v\n    block._input_dict = new_input_dict\n    block.function_inputs = tuple(block._input_dict.values())\n    logger.info(f\"final inputs: {[v.name for v in block.function_inputs]}\")\n\n    return prog", "\ndef save_chunk(prog_chunk, filename):\n    model_chunk = ct.convert(\n        prog_chunk,\n        convert_to=\"mlprogram\",\n        compute_units=ct.ComputeUnit.CPU_ONLY,\n        minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n    )\n    model_chunk.save(filename)\n\ndef main(args):\n    os.makedirs(args.o, exist_ok=True)\n\n    # Check filename extension\n    mlpackage_name = os.path.basename(args.mlpackage_path)\n    name, ext = os.path.splitext(mlpackage_name)\n    assert ext == \".mlpackage\", f\"`--mlpackage-path` (args.mlpackage_path) is not an .mlpackage file\"\n\n    # Load CoreML model\n    logger.info(\"Loading model from {}\".format(args.mlpackage_path))\n    start_ = time.time()\n    model = ct.models.MLModel(\n        args.mlpackage_path,\n        compute_units=ct.ComputeUnit.CPU_ONLY,\n    )\n    logger.info(\n        f\"Loading {args.mlpackage_path} took {time.time() - start_:.1f} seconds\"\n    )\n\n    # Load the MIL Program from MLModel\n    prog = _load_prog_from_mlmodel(model)\n\n    logger.info(f\"Total ops: {len(prog.functions['main'].operations)}\")\n\n    chunks: list[Chunk] = _get_op_idx_split_locations(prog)\n    logger.info(f\"{args.mlpackage_path} will chunked into {len(chunks)} pieces.\")\n    for c in chunks:\n        logger.info(c)\n\n    # From start to first chunk op idx.\n    # prog_chunk1 = _make_first_chunk_prog(prog, chunks[0].end_op_idx)\n    # logger.info(f\"Converting and saving first chunk up to op {chunks[0].end_op_idx}\")\n    # filename = f\"{name}_chunk1.mlpackage\"\n    # chunks[0].save(prog_chunk1, filename)\n    # logger.info(f\"Saved first chunk {filename} of {len(chunks)} total\")\n\n    # del prog_chunk1\n    # gc.collect()\n\n    prev_start_index = 0\n    for idx, chunk in [(i+1, c) for i,c in enumerate(chunks)]:\n        filename = f\"{name}_chunk{idx}.mlpackage\"\n        full_prog = _load_prog_from_mlmodel(model)\n        logger.info(f\"Converting and saving chunk #{idx} from op {chunk.start_op_idx}-{chunk.end_op_idx}\")\n        is_last = chunk.end_op_idx == chunks[-1].end_op_idx\n        prog_chunkn = _make_second_chunk_prog(full_prog, prev_start_index, chunk.start_op_idx, chunk.end_op_idx, is_last=is_last)\n        chunk.save(prog_chunkn, filename)\n\n        del full_prog\n        del prog_chunkn\n        gc.collect()\n        logger.info(f\"Saved chunk {filename} of {len(chunks)} total\")\n\n        prev_start_index = chunk.start_op_idx\n\n\n    # Compute the incision point by bisecting the program based on weights size\n    # op_idx, first_chunk_weights_size, total_weights_size = _get_op_idx_split_location(\n    #     prog)\n    # main_block = prog.functions[\"main\"]\n    # incision_op = main_block.operations[op_idx]\n    # logger.info(f\"{args.mlpackage_path} will chunked into two pieces.\")\n    # logger.info(\n    #     f\"The incision op: name={incision_op.name}, type={incision_op.op_type}, index={op_idx}/{len(main_block.operations)}\"\n    # )\n    # logger.info(f\"First  chunk size = {first_chunk_weights_size:.2f} MB\")\n    # logger.info(\n    #     f\"Second chunk size = {total_weights_size - first_chunk_weights_size:.2f} MB\"\n    # )\n\n    # # Build first chunk (in-place modifies prog by declaring early exits and removing unused subgraph)\n    # prog_chunk1 = _make_first_chunk_prog(prog, op_idx)\n\n    # # Build the second chunk\n    # prog_chunk2 = _make_second_chunk_prog(_load_prog_from_mlmodel(model),\n    #                                       op_idx)\n\n    # if not args.check_output_correctness:\n    #     # Original model no longer needed in memory\n    #     del model\n    #     gc.collect()\n\n    # # Convert the MIL Program objects into MLModels\n    # logger.info(\"Converting the two programs\")\n    # model_chunk1 = ct.convert(\n    #     prog_chunk1,\n    #     convert_to=\"mlprogram\",\n    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n    # )\n    # del prog_chunk1\n    # gc.collect()\n    # logger.info(\"Conversion of first chunk done.\")\n\n    # model_chunk2 = ct.convert(\n    #     prog_chunk2,\n    #     convert_to=\"mlprogram\",\n    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n    # )\n    # del prog_chunk2\n    # gc.collect()\n    # logger.info(\"Conversion of second chunk done.\")\n\n    # # Verify output correctness\n    # if args.check_output_correctness:\n    #     logger.info(\"Verifying output correctness of chunks\")\n    #     _verify_output_correctness_of_chunks(\n    #         full_model=model,\n    #         first_chunk_model=model_chunk1,\n    #         second_chunk_model=model_chunk2,\n    #     )\n\n    # # Remove original (non-chunked) model if requested\n    # if args.remove_original:\n    #     logger.info(\n    #         \"Removing original (non-chunked) model at {args.mlpackage_path}\")\n    #     shutil.rmtree(args.mlpackage_path)\n    #     logger.info(\"Done.\")\n\n    # # Save the chunked models to disk\n    # out_path_chunk1 = os.path.join(args.o, name + \"_chunk1.mlpackage\")\n    # out_path_chunk2 = os.path.join(args.o, name + \"_chunk2.mlpackage\")\n\n    # logger.info(\n    #     f\"Saved chunks in {args.o} with the suffix _chunk1.mlpackage and _chunk2.mlpackage\"\n    # )\n    # model_chunk1.save(out_path_chunk1)\n    # model_chunk2.save(out_path_chunk2)\n    logger.info(\"Done.\")", "\ndef main(args):\n    os.makedirs(args.o, exist_ok=True)\n\n    # Check filename extension\n    mlpackage_name = os.path.basename(args.mlpackage_path)\n    name, ext = os.path.splitext(mlpackage_name)\n    assert ext == \".mlpackage\", f\"`--mlpackage-path` (args.mlpackage_path) is not an .mlpackage file\"\n\n    # Load CoreML model\n    logger.info(\"Loading model from {}\".format(args.mlpackage_path))\n    start_ = time.time()\n    model = ct.models.MLModel(\n        args.mlpackage_path,\n        compute_units=ct.ComputeUnit.CPU_ONLY,\n    )\n    logger.info(\n        f\"Loading {args.mlpackage_path} took {time.time() - start_:.1f} seconds\"\n    )\n\n    # Load the MIL Program from MLModel\n    prog = _load_prog_from_mlmodel(model)\n\n    logger.info(f\"Total ops: {len(prog.functions['main'].operations)}\")\n\n    chunks: list[Chunk] = _get_op_idx_split_locations(prog)\n    logger.info(f\"{args.mlpackage_path} will chunked into {len(chunks)} pieces.\")\n    for c in chunks:\n        logger.info(c)\n\n    # From start to first chunk op idx.\n    # prog_chunk1 = _make_first_chunk_prog(prog, chunks[0].end_op_idx)\n    # logger.info(f\"Converting and saving first chunk up to op {chunks[0].end_op_idx}\")\n    # filename = f\"{name}_chunk1.mlpackage\"\n    # chunks[0].save(prog_chunk1, filename)\n    # logger.info(f\"Saved first chunk {filename} of {len(chunks)} total\")\n\n    # del prog_chunk1\n    # gc.collect()\n\n    prev_start_index = 0\n    for idx, chunk in [(i+1, c) for i,c in enumerate(chunks)]:\n        filename = f\"{name}_chunk{idx}.mlpackage\"\n        full_prog = _load_prog_from_mlmodel(model)\n        logger.info(f\"Converting and saving chunk #{idx} from op {chunk.start_op_idx}-{chunk.end_op_idx}\")\n        is_last = chunk.end_op_idx == chunks[-1].end_op_idx\n        prog_chunkn = _make_second_chunk_prog(full_prog, prev_start_index, chunk.start_op_idx, chunk.end_op_idx, is_last=is_last)\n        chunk.save(prog_chunkn, filename)\n\n        del full_prog\n        del prog_chunkn\n        gc.collect()\n        logger.info(f\"Saved chunk {filename} of {len(chunks)} total\")\n\n        prev_start_index = chunk.start_op_idx\n\n\n    # Compute the incision point by bisecting the program based on weights size\n    # op_idx, first_chunk_weights_size, total_weights_size = _get_op_idx_split_location(\n    #     prog)\n    # main_block = prog.functions[\"main\"]\n    # incision_op = main_block.operations[op_idx]\n    # logger.info(f\"{args.mlpackage_path} will chunked into two pieces.\")\n    # logger.info(\n    #     f\"The incision op: name={incision_op.name}, type={incision_op.op_type}, index={op_idx}/{len(main_block.operations)}\"\n    # )\n    # logger.info(f\"First  chunk size = {first_chunk_weights_size:.2f} MB\")\n    # logger.info(\n    #     f\"Second chunk size = {total_weights_size - first_chunk_weights_size:.2f} MB\"\n    # )\n\n    # # Build first chunk (in-place modifies prog by declaring early exits and removing unused subgraph)\n    # prog_chunk1 = _make_first_chunk_prog(prog, op_idx)\n\n    # # Build the second chunk\n    # prog_chunk2 = _make_second_chunk_prog(_load_prog_from_mlmodel(model),\n    #                                       op_idx)\n\n    # if not args.check_output_correctness:\n    #     # Original model no longer needed in memory\n    #     del model\n    #     gc.collect()\n\n    # # Convert the MIL Program objects into MLModels\n    # logger.info(\"Converting the two programs\")\n    # model_chunk1 = ct.convert(\n    #     prog_chunk1,\n    #     convert_to=\"mlprogram\",\n    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n    # )\n    # del prog_chunk1\n    # gc.collect()\n    # logger.info(\"Conversion of first chunk done.\")\n\n    # model_chunk2 = ct.convert(\n    #     prog_chunk2,\n    #     convert_to=\"mlprogram\",\n    #     compute_units=ct.ComputeUnit.CPU_ONLY,\n    #     minimum_deployment_target=ct.target.iOS16, # TODO: Is this needed?\n    # )\n    # del prog_chunk2\n    # gc.collect()\n    # logger.info(\"Conversion of second chunk done.\")\n\n    # # Verify output correctness\n    # if args.check_output_correctness:\n    #     logger.info(\"Verifying output correctness of chunks\")\n    #     _verify_output_correctness_of_chunks(\n    #         full_model=model,\n    #         first_chunk_model=model_chunk1,\n    #         second_chunk_model=model_chunk2,\n    #     )\n\n    # # Remove original (non-chunked) model if requested\n    # if args.remove_original:\n    #     logger.info(\n    #         \"Removing original (non-chunked) model at {args.mlpackage_path}\")\n    #     shutil.rmtree(args.mlpackage_path)\n    #     logger.info(\"Done.\")\n\n    # # Save the chunked models to disk\n    # out_path_chunk1 = os.path.join(args.o, name + \"_chunk1.mlpackage\")\n    # out_path_chunk2 = os.path.join(args.o, name + \"_chunk2.mlpackage\")\n\n    # logger.info(\n    #     f\"Saved chunks in {args.o} with the suffix _chunk1.mlpackage and _chunk2.mlpackage\"\n    # )\n    # model_chunk1.save(out_path_chunk1)\n    # model_chunk2.save(out_path_chunk2)\n    logger.info(\"Done.\")", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--mlpackage-path\",\n        required=True,\n        help=\n        \"Path to the mlpackage file to be split into two mlpackages of approximately same file size.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        required=True,\n        help=\n        \"Path to output directory where the two model chunks should be saved.\",\n    )\n    parser.add_argument(\n        \"--remove-original\",\n        action=\"store_true\",\n        help=\n        \"If specified, removes the original (non-chunked) model to avoid duplicating storage.\"\n    )\n    parser.add_argument(\n        \"--check-output-correctness\",\n        action=\"store_true\",\n        help=\n        (\"If specified, compares the outputs of original Core ML model with that of pipelined CoreML model chunks and reports PSNR in dB. \",\n         \"Enabling this feature uses more memory. Disable it if your machine runs out of memory.\"\n         ))\n\n    args = parser.parse_args()\n    main(args)", ""]}
{"filename": "src/experiments/kahan_layer_norm.py", "chunked_list": ["import torch\nfrom torch import nn\nimport numpy as np\nfrom src.ml_ane_transformers.ane.layer_norm import LayerNormANE as LayerNorm\nfrom src.ml_ane_transformers.ane.kahan_layer_norm import KahanLayerNormANE as KahanLayerNorm\nimport coremltools as ct\nfrom src.utils.psnr import compute_psnr\nfrom coremltools.converters.mil import Builder as mb\nimport sys\n", "import sys\n\n\"\"\"\nCompare and test a Kahan summation implementation of layer norm vs.\nthe default ANE-optimized one.\n\nSeems that for some reason my Kahan implementation is slightly less accurate.\n\nEnded up finding that the model builder layer_norm can take a 4D input and do\nthe norm on the channel dimension. Unfortunately doesn't work on the Neural Engine afaict.", "Ended up finding that the model builder layer_norm can take a 4D input and do\nthe norm on the channel dimension. Unfortunately doesn't work on the Neural Engine afaict.\n\"\"\"\n\ntorch.manual_seed(42)\n\nB,C,S = 1, 1024, 512\n# B,C,S = 1, 6, 1\n# B,C,S = 1,3,1\n# B,C,S = 1,3,2", "# B,C,S = 1,3,1\n# B,C,S = 1,3,2\n# x = torch.FloatTensor(B,C,1,S).uniform_(torch.finfo(torch.half).min*0.9, torch.finfo(torch.half).max*0.9)\nx = torch.randn((B,C,1,S), dtype=torch.float16).float().cpu()\n# x = torch.tensor([[[[10000.0]], [[3.14159]], [[2.71828]]]], dtype=torch.float16).float().cpu()\n# x = torch.tensor([[[[10000.0, 2.71828]], [[3.14159, 10000.0]], [[2.71828, 3.14159]]]], dtype=torch.float16).float().cpu()\n# print(x.shape, x.to(\"mps\").half().cumsum(dim=1))\n\n# Ignore learnable params.\nclip_mag = None#1e7", "# Ignore learnable params.\nclip_mag = None#1e7\nln = LayerNorm(C, clip_mag=clip_mag, elementwise_affine=False)\nkln = KahanLayerNorm(C, clip_mag=clip_mag, elementwise_affine=False)\nnnln = nn.LayerNorm(C, elementwise_affine=False)\n\ndef print_stats(normal, kahan):\n    assert normal.shape == kahan.shape\n    print(\"all close?\", torch.allclose(normal, kahan))\n    print(\"equal?\", torch.equal(normal, kahan))\n    print(\"mean diff\", torch.mean(normal - kahan))\n    print(\"max diff\", torch.max(torch.abs(normal - kahan)))\n    # print(\"psnr\", compute_psnr(normal, kahan))\n    print(\"psnr\", compute_psnr(kahan, normal))\n    print(\"num close:\", torch.sum(torch.isclose(normal, kahan)))", "\nwith torch.no_grad():\n    km = kln.kahan_mean(x.to(\"mps\").half(), 4).float().cpu()\n    hm = x.to(\"mps\").half().mean(dim=1, keepdim=True).float().cpu()\n    m = x.to(\"mps\").float().mean(dim=1, keepdim=True).float().cpu()\n    dm = x.double().mean(dim=1, keepdim=True)\n\nprint(\"mean vs kahan mean half\\n----\")\nprint_stats(m, km)\nprint_stats(m, hm)", "print_stats(m, km)\nprint_stats(m, hm)\n# print(\"kahan\", km)\n# print(\"exactly:\", m)\n\nwith torch.no_grad():\n    ln_res = ln(x.float())\n    kln_res = kln(x.float())\n# print(\"float32\\n----\")\n# print_stats(ln_res, kln_res)", "# print(\"float32\\n----\")\n# print_stats(ln_res, kln_res)\n\nwith torch.no_grad():\n    y = x.half().to(\"mps\")\n    ln_res_half = ln(y).float().cpu()\n    kln_res_half = kln(y).float().cpu()\n# print(\"\\nfloat16\\n----\")\n# print_stats(ln_res_half, kln_res_half)\n", "# print_stats(ln_res_half, kln_res_half)\n\nprint(\"\\nfloat16 normal v float32 normal\\n----\")\nprint_stats(ln_res, ln_res_half)\n\nprint(\"\\nfloat16 kahan v float32 normal\\n----\")\nprint_stats(ln_res, kln_res_half)\n\ndef convert_bc1s_norm(n, f32=False, skip_trace=False):\n    if not skip_trace:\n        traced = torch.jit.trace(n, (x,))\n        mlp = ct.convert(traced,\n                        inputs=[ct.TensorType(name=\"x\", shape=(B,C,1,S), dtype=np.float32)],\n                        outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n                        compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                        compute_units=ct.ComputeUnit.CPU_AND_NE,\n                        convert_to=\"milinternal\")\n    else:\n        mlp = n\n    print(n.__class__)\n    print(mlp)\n    return ct.convert(mlp,\n                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n                    convert_to=\"mlprogram\")", "def convert_bc1s_norm(n, f32=False, skip_trace=False):\n    if not skip_trace:\n        traced = torch.jit.trace(n, (x,))\n        mlp = ct.convert(traced,\n                        inputs=[ct.TensorType(name=\"x\", shape=(B,C,1,S), dtype=np.float32)],\n                        outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n                        compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                        compute_units=ct.ComputeUnit.CPU_AND_NE,\n                        convert_to=\"milinternal\")\n    else:\n        mlp = n\n    print(n.__class__)\n    print(mlp)\n    return ct.convert(mlp,\n                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n                    convert_to=\"mlprogram\")", "\ndef convert_bsc_norm(n, f32=False):\n    traced = torch.jit.trace(n, (x.permute(0,3,1,2).squeeze(-1),))\n    mlp = ct.convert(traced,\n                    inputs=[ct.TensorType(name=\"x\", shape=(B,S,C), dtype=np.float32)],\n                    outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n                    convert_to=\"milinternal\")\n    print(n.__class__)\n    print(mlp)\n    return ct.convert(mlp,\n                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                    compute_units=ct.ComputeUnit.CPU_AND_NE,\n                    convert_to=\"mlprogram\")", "\n# Interesting...\n@mb.program(input_specs=[mb.TensorSpec(shape=(B,C,1,S)),])\ndef ln_prog(x):\n    # x = mb.squeeze(x=x, axes=[2], name='squeeze')\n    x = mb.layer_norm(x=x, axes=[1], name=\"y\")\n    # x = mb.expand_dims(x=x, axes=[2], name=\"y\")\n    return x\n\n", "\n\ncln = convert_bc1s_norm(ln, False)\n# ckln = convert_bc1s_norm(kln, False)\nlnp = convert_bc1s_norm(ln_prog, False, skip_trace=True)\n# half_nnln = convert_bsc_norm(nn.LayerNorm(C, elementwise_affine=False))\n# nnln = convert_bsc_norm(nn.LayerNorm(C, elementwise_affine=False),f32=True)\n\ninp = {\"x\": x.float().numpy()}\ncoreml_ln = torch.from_numpy(cln.predict(inp)[\"y\"])", "inp = {\"x\": x.float().numpy()}\ncoreml_ln = torch.from_numpy(cln.predict(inp)[\"y\"])\ncoreml_kln = torch.from_numpy(ckln.predict(inp)[\"y\"])\nprint(lnp.predict(inp))\ncoreml_lnp = torch.from_numpy(lnp.predict(inp)[\"y\"])\ncoreml_half_nnln = half_nnln.predict({\"x\": x.permute(0,3,1,2).squeeze(-1).float().numpy()})[\"y\"]\ncoreml_half_nnln = torch.from_numpy(coreml_half_nnln).permute(0,2,1).unsqueeze(2)\ncoreml_nnln = nnln.predict({\"x\": x.permute(0,3,1,2).squeeze(-1).float().numpy()})[\"y\"]\ncoreml_nnln = torch.from_numpy(coreml_nnln).permute(0,2,1).unsqueeze(2)\n", "coreml_nnln = torch.from_numpy(coreml_nnln).permute(0,2,1).unsqueeze(2)\n\nprint(\"\\coreml nn ln vs kln\\n----\")\nprint_stats(coreml_nnln, coreml_kln)\nprint(\"\\coreml nn ln vs ln\\n----\")\nprint_stats(coreml_nnln, coreml_ln)\nprint(\"\\coreml nn ln vs half nn\\n----\")\nprint_stats(coreml_nnln, coreml_half_nnln)\nprint(\"\\coreml nn ln vs ln prog\\n----\")\nprint_stats(coreml_nnln, coreml_lnp)", "print(\"\\coreml nn ln vs ln prog\\n----\")\nprint_stats(coreml_nnln, coreml_lnp)\n\n\n# Output of coreml norms for 1x1024x1x512 input with a 512 chunks.\n# Took forever to run and I think basically shows that Kahan accumulates too much error.\n# \\coreml nn ln vs kln\n# ----\n# all close? False\n# equal? False", "# all close? False\n# equal? False\n# mean diff tensor(2.1594e-06)\n# max diff tensor(0.0187)\n# psnr 67.48296284743999\n# num close: tensor(2398)\n# \\coreml nn ln vs ln\n# ----\n# all close? False\n# equal? False", "# all close? False\n# equal? False\n# mean diff tensor(2.3021e-06)\n# max diff tensor(0.0057)\n# psnr 77.59771092144952\n# num close: tensor(7922)\n\n"]}
{"filename": "src/experiments/dummy_layer_norm.py", "chunked_list": ["import torch\nfrom torch import nn\nimport numpy as np\nfrom src.ml_ane_transformers.ane.dummy_layer_norm import DummyLayerNormANE as DummyLN\nfrom src.ml_ane_transformers.ane.layer_norm import LayerNormANE\nimport coremltools as ct\nfrom src.utils.psnr import compute_psnr\nfrom coremltools.converters.mil import Builder as mb\nimport sys\n", "import sys\n\n\"\"\"\nTest out the DummyLayerNormANE and see if it's possible to hot swap\nby overriding the batch_norm op.\n\nIt is.\n\"\"\"\n\ntorch.manual_seed(42)", "\ntorch.manual_seed(42)\n\nfrom coremltools.converters.mil import register_torch_op\nfrom coremltools.converters.mil.mil import Builder as mb\nfrom coremltools.converters.mil.frontend.torch.ops import _get_inputs\nfrom coremltools.converters.mil.frontend.torch.torch_op_registry import _TORCH_OPS_REGISTRY\nif \"batch_norm\" in _TORCH_OPS_REGISTRY:\n    del _TORCH_OPS_REGISTRY[\"batch_norm\"]\n", "\n@register_torch_op\ndef batch_norm(context, node):\n    inputs = _get_inputs(context, node, expected=9)\n    _input = inputs[0]\n    weight = inputs[1]\n    bias = inputs[2]\n    running_mean = inputs[3]\n    running_var = inputs[4]\n    training = inputs[5].val\n    eps = inputs[7]\n\n    context.add(eps)\n    context.add(weight)\n    context.add(bias)\n    context.add(_input)\n    print(\"weight.shape\", weight.shape)\n\n    ln = mb.layer_norm(x=_input, axes=[1], epsilon=eps, gamma=weight, beta=bias, name=node.name)\n    context.add(ln)", "\n\nB,C,S = 1, 5, 3\ndummy = DummyLN(C)\nln = nn.LayerNorm(C, elementwise_affine=False)\nane = LayerNormANE(C, elementwise_affine=False)\n\ndummy_trace = torch.jit.trace(dummy, torch.randn((B,C,1,S)))\nprog = ct.convert(dummy_trace,\n                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],", "prog = ct.convert(dummy_trace,\n                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],\n                outputs=[ct.TensorType(\"y\")],\n                # pass_pipeline=ct.PassPipeline(pipeline_name=\"empty\"),\n                compute_precision=ct.precision.FLOAT32, # The float16 loss is real.\n                convert_to=\"milinternal\")\nprint(prog)\n\nmlmodel = ct.convert(prog,\n                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],", "mlmodel = ct.convert(prog,\n                inputs=[ct.TensorType(\"x\", shape=(B,C,1,S))],\n                outputs=[ct.TensorType(\"y\")],\n                compute_precision=ct.precision.FLOAT32,\n                convert_to=\"mlprogram\")\n\nx = torch.randn((B,C,1,S))\nprint(\"x\", x)\nprint(\"y\", mlmodel.predict({\"x\": x.numpy()})[\"y\"])\nprint(\"ane\", ane(x))", "print(\"y\", mlmodel.predict({\"x\": x.numpy()})[\"y\"])\nprint(\"ane\", ane(x))\nprint(\"ln\", ane(x.permute(0,3,1,2).squeeze(-1)))\n"]}
{"filename": "src/experiments/diff_chunked_models.py", "chunked_list": ["import coremltools as ct\nimport argparse\nimport os\nimport sys\nimport torch\nfrom src.utils.psnr import compute_psnr\n\n\"\"\"\nImport a chunked pipeline model and an original model, perform predictions\non both and ensure that they are identical.", "Import a chunked pipeline model and an original model, perform predictions\non both and ensure that they are identical.\n\"\"\"\n\nparser = argparse.ArgumentParser(description='Load a pipeline CoreML modelpackage and compare it with it\\'s non-pipeline version.')\nparser.add_argument('pipeline_model_path', help='path to *-pipeline.mlpackage file', type=str)\n# parser.add_argument('normal_model_path', help='path to non-pipelined *.mlpackage file', type=str)\nargs = parser.parse_args()\n\nmodel_path = args.pipeline_model_path.replace(\"-pipeline\", \"\")", "\nmodel_path = args.pipeline_model_path.replace(\"-pipeline\", \"\")\n# if os.path.exists(model_path):\n#     print(f\"non-pipelined model not found at {model_path}\")\n#     sys.exit(1)\n\npipeline_path = args.pipeline_model_path\n# TODO: Need to add the model proxy for this.\n# if os.path.exists(pipeline_path.replace('.mlpackage', '.mlmodelc')):\n#     pipeline_path = pipeline_path.replace('.mlpackage', '.mlmodelc')", "# if os.path.exists(pipeline_path.replace('.mlpackage', '.mlmodelc')):\n#     pipeline_path = pipeline_path.replace('.mlpackage', '.mlmodelc')\n# if os.path.exists(model_path.replace('.mlpackage', '.mlmodelc')):\n#     model_path = model_path.replace('.mlpackage', '.mlmodelc')\n\nprint(f\"Loading pipeline from {pipeline_path}...\")\npipeline = ct.models.MLModel(args.pipeline_model_path, compute_units=ct.ComputeUnit.CPU_ONLY)\nprint(f\"Loading normal from {model_path}...\")\nmodel = ct.models.MLModel(model_path, compute_units=ct.ComputeUnit.CPU_ONLY)\nprint(\"Loaded both models.\")", "model = ct.models.MLModel(model_path, compute_units=ct.ComputeUnit.CPU_ONLY)\nprint(\"Loaded both models.\")\n\n# input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\ninput_ids = torch.randint(10000, (1,512,)).int()\noutput_mask = torch.tensor([2]).int()\nprint(\"input_ids.shape\", input_ids.shape)\nprint(\"output_mask.shape\", output_mask.shape)\n\npo = pipeline.predict({\"input_ids\": input_ids, \"output_mask\": output_mask})[\"logits\"]", "\npo = pipeline.predict({\"input_ids\": input_ids, \"output_mask\": output_mask})[\"logits\"]\nprint(\"Predicted on pipeline.\")\nmo = model.predict({\"input_ids\": input_ids, \"output_mask\": output_mask})[\"logits\"]\n\nprint(\"psnr: \", compute_psnr(po, mo))\nprint(\"equal:\",  torch.equal(torch.from_numpy(mo), torch.from_numpy(po)))"]}
{"filename": "src/experiments/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/norm_chunk_test.py", "chunked_list": ["import torch\n\n\"\"\"\nExperiment with splitting the input into chunks and computing\nthe mean on each chunk separately. If overflowing was a problem,\nit might help but you lose a lot of precision.\n\"\"\"\n\nn_embd = num_channels = 768\nseqlen = 512", "n_embd = num_channels = 768\nseqlen = 512\neps = 1e-5\n\nweight = torch.rand(n_embd)\nbias = torch.rand(n_embd)\n\nx = torch.rand([1, n_embd, 1, seqlen])\n\n# torch.Size([1, 768, 1, 512]) torch.Size([768]) torch.Size([768])", "\n# torch.Size([1, 768, 1, 512]) torch.Size([768]) torch.Size([768])\n# x, weight, bias ^\ndef baseline(inputs):\n    input_rank = len(inputs.size())\n\n    assert input_rank == 4\n    assert inputs.size(1) == num_channels\n    assert inputs.dtype == torch.float16 or inputs.dtype == torch.float32\n\n    # if self.clip_mag is not None:\n    #     inputs.clamp_(-self.clip_mag, self.clip_mag)\n\n    channels_mean = inputs.mean(dim=1, keepdims=True) # shape [1,1,1,S]\n    zero_mean = inputs - channels_mean # shape [1,C,1,S]\n    zero_mean_sq = zero_mean * zero_mean # shape [1,C,1,S]\n    denom = (zero_mean_sq.mean(dim=1, keepdims=True) + eps).rsqrt() # [1,1,1,S]\n    out = zero_mean * denom # shape [1,C,1,S]\n    out = (out + bias.view(1, num_channels, 1, 1)\n            ) * weight.view(1, num_channels, 1, 1)\n    return out", "\ndef chunked_mean(inputs, num_chunks):\n    \"\"\"\n    Compute a mean in chunks to avoid overflows while summing. Assumes BC1S format.\n    NOTE: This causes a large loss of precision in float16. Not worth it as far as I can tell.\n    \"\"\"\n    num_channels = inputs.shape[1]\n    assert num_chunks > 0, \"num_chunks must be positive\"\n    assert num_channels % num_chunks == 0, \"num_chunks must evenly divide num_channels\"\n\n    if num_chunks == 1:\n        return inputs.mean(dim=1, keepdim=True)\n\n    channel_chunks = inputs.split(num_channels//num_chunks, dim=1)\n    mean = 0\n    for chunk in channel_chunks:\n        chunk_sum = chunk.sum(dim=1, keepdim=True) # shape [1,1,1,S]\n        chunk_mean = chunk_sum / num_channels\n        mean = mean + chunk_mean\n    return mean", "\ndef chunked(inputs, num_chunks, scale_factor=1e3):\n    channels_mean = chunked_mean(inputs / scale_factor, num_chunks) # shape [1,1,1,S]\n    channels_mean *= scale_factor\n\n    zero_mean = inputs - channels_mean # shape [1,C,1,S]\n    zero_mean_sq = zero_mean * zero_mean # shape [1,C,1,S]\n\n    mean_zero_mean_sq = chunked_mean(zero_mean_sq / scale_factor, num_chunks) * scale_factor\n    denom = (mean_zero_mean_sq + eps).rsqrt() # [1,1,1,S]\n\n    # denom = (chunked_mean(zero_mean_sq, num_chunks) + eps).rsqrt() # [1,1,1,S]\n    out = zero_mean * denom # shape [1,C,1,S]\n    out = (out + bias.view(1, num_channels, 1, 1)\n            ) * weight.view(1, num_channels, 1, 1)\n    return out", "\nbl = baseline(x)\nch = chunked(x, 1)\nprint(\"baseline:\", bl.shape)\nprint(\"chunked :\", ch.shape)\nprint(\"equal?  :\", torch.equal(bl, ch))\ndelta = bl - ch\nprint(\"max delta:\", delta.max(), \"median:\", delta.median(), \"mean:\", delta.mean())\n", ""]}
{"filename": "src/experiments/conv_test.py", "chunked_list": ["import torch\nfrom torch import nn\n\n\"\"\"\nExperiment to convince myself about how to appropriately translate\nweights for a Conv1D to Conv2d.\n\"\"\"\n\nEMBED_DIM = 768\n\nclass Conv1D(nn.Module):\n    \"\"\"\n    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n    Basically works like a linear layer but the weights are transposed.\n    Args:\n        nf (`int`): The number of output features.\n        nx (`int`): The number of input features.\n    \"\"\"\n\n    def __init__(self, nf, nx):\n        super().__init__()\n        self.nf = nf\n        self.weight = nn.Parameter(torch.empty(nx, nf))\n        self.bias = nn.Parameter(torch.zeros(nf))\n        nn.init.normal_(self.weight, std=0.02)\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(size_out)\n        return x", "EMBED_DIM = 768\n\nclass Conv1D(nn.Module):\n    \"\"\"\n    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n    Basically works like a linear layer but the weights are transposed.\n    Args:\n        nf (`int`): The number of output features.\n        nx (`int`): The number of input features.\n    \"\"\"\n\n    def __init__(self, nf, nx):\n        super().__init__()\n        self.nf = nf\n        self.weight = nn.Parameter(torch.empty(nx, nf))\n        self.bias = nn.Parameter(torch.zeros(nf))\n        nn.init.normal_(self.weight, std=0.02)\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(size_out)\n        return x", "\nc_proj = Conv1D(EMBED_DIM, EMBED_DIM)\n#c_proj = nn.Conv1d(EMBED_DIM, EMBED_DIM, 1)\nprint(\"1d weight shape:\", c_proj.weight.shape)\nprint(\"1d bias shape:\", c_proj.bias.shape)\n\nout_proj = nn.Conv2d(EMBED_DIM, EMBED_DIM, 1)\nprint(\"2d weight shape:\", out_proj.weight.shape)\nprint(\"2d bias shape:\", out_proj.bias.shape)\n\nwith torch.no_grad():\n    out_proj.weight.copy_(c_proj.weight.t().unsqueeze(-1).unsqueeze(-1))\n    out_proj.bias.copy_(c_proj.bias)", "print(\"2d bias shape:\", out_proj.bias.shape)\n\nwith torch.no_grad():\n    out_proj.weight.copy_(c_proj.weight.t().unsqueeze(-1).unsqueeze(-1))\n    out_proj.bias.copy_(c_proj.bias)\n\nt = torch.rand(EMBED_DIM, EMBED_DIM)\noned_out = c_proj(t)\ntwod_out = out_proj(t.unsqueeze(-1).unsqueeze(-1))\nprint(\"1d out\", oned_out.shape)", "twod_out = out_proj(t.unsqueeze(-1).unsqueeze(-1))\nprint(\"1d out\", oned_out.shape)\nprint(\"2d out\", twod_out.shape)\n\ntwod_out = twod_out.squeeze(-1).squeeze(-1)\n# TODO: cosine similarity is kind of garbage, did I mess this conversion up in the ane-optimized model?\nprint(nn.CosineSimilarity(dim=-1)(oned_out, twod_out).mean())\nprint(torch.equal(oned_out, twod_out))\n", ""]}
{"filename": "src/experiments/quant_chunk_test.py", "chunked_list": ["import torch\nfrom torch import nn\nimport coremltools as ct\nimport numpy as np\n\n\"\"\"\nBuild a test model, quantize it and use it for debugging\nchunking.\n\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ls = nn.ModuleList([\n            nn.Linear(1600, 6400),\n            nn.GELU(),\n            nn.Linear(6400, 1600),\n            nn.GELU(),\n            nn.Linear(1600, 6400),\n            nn.GELU(),\n            nn.Linear(6400, 1600),\n        ])\n\n    def forward(self, x):\n        for l in self.ls:\n            x = l(x)\n        return x", "\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ls = nn.ModuleList([\n            nn.Linear(1600, 6400),\n            nn.GELU(),\n            nn.Linear(6400, 1600),\n            nn.GELU(),\n            nn.Linear(1600, 6400),\n            nn.GELU(),\n            nn.Linear(6400, 1600),\n        ])\n\n    def forward(self, x):\n        for l in self.ls:\n            x = l(x)\n        return x", "\nif __name__ == \"__main__\":\n    model = Net().eval()\n\n    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n    traced = torch.jit.trace(model, (input_ids))\n\n    mlmodel = ct.convert(\n        traced,\n        inputs=[\n            ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.float32),\n        ],\n        outputs=[\n            ct.TensorType(name=\"logits\", dtype=np.float32),\n        ],\n        compute_precision=ct.precision.FLOAT16,\n        convert_to=\"mlprogram\",\n    )\n\n    quantized = ct.compression_utils.palettize_weights(mlmodel, nbits=2, mode=\"kmeans\")\n    quantized.save(f\"test-quant-net.mlpackage\")"]}
{"filename": "src/experiments/make_pipeline.py", "chunked_list": ["import coremltools as ct\nimport argparse\nimport glob\nimport re\nimport sys\n\n\"\"\"\nCompose multiple test networks into a single pipeline to see if\nit will do anything clever when running on the ANE.\n", "it will do anything clever when running on the ANE.\n\n** Requires coremltools 6.3 **\n\"\"\"\n\nparser = argparse.ArgumentParser(description='Stitch multiple chunked models into a single pipeline')\nparser.add_argument('model_path', help='path to any of the chunked model .mlpackage files', type=str)\nparser.add_argument('--range', help=\"comma-separated range of models to include\", type=str)\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\nmodel_range = None\nif args.range is not None:\n    model_range = [int(x) for x in args.range.split(\",\")]\n    assert len(model_range) == 2, f\"range must have two elements: {model_range}\"\n\ndef chunk_num(path):\n    match = re.search(r'_chunk(\\d+)\\.mlpackage$', path)\n    if match:\n        return int(match.group(1))\n    return None", "\npattern = re.sub(r'_chunk\\d+', '*', args.model_path)\nmatching_files = [(chunk_num(path), path) for path in glob.glob(pattern)]\nmatching_files = [x for x in matching_files if x[0] is not None]\nmodel_paths = [x[1] for x in sorted(matching_files, key=lambda x: x[0])]\n\noutput_filename =  model_paths[0].replace(\"_chunk1\", \"-pipeline\")\n\nif model_range is not None:\n    # Shift left since chunks are 1-indexed.\n    start, end = model_range[0]-1, model_range[1]\n    model_paths = model_paths[start:end]", "if model_range is not None:\n    # Shift left since chunks are 1-indexed.\n    start, end = model_range[0]-1, model_range[1]\n    model_paths = model_paths[start:end]\n\nprint(\"Will create a pipeline from the following models in this order:\")\nfor mp in model_paths:\n    print(mp)\ncorrect = input(\"Is that correct? (y/n) \") in [\"y\", \"Y\"]\n\nif correct:\n    print(\"Creating pipeline.\")\nelse:\n    sys.exit(1)", "correct = input(\"Is that correct? (y/n) \") in [\"y\", \"Y\"]\n\nif correct:\n    print(\"Creating pipeline.\")\nelse:\n    sys.exit(1)\n\nmodels = [ct.models.MLModel(model_path, skip_model_load=True) for model_path in model_paths]\n\nfilename =  model_paths[0].replace(\"_chunk1\", \"-pipeline\")\nif model_range is not None:\n    output_filename = output_filename.replace(\".mlpackage\", f\"-{model_range[0]}to{model_range[1]}.mlpackage\")", "\nfilename =  model_paths[0].replace(\"_chunk1\", \"-pipeline\")\nif model_range is not None:\n    output_filename = output_filename.replace(\".mlpackage\", f\"-{model_range[0]}to{model_range[1]}.mlpackage\")\n\n# Erroring here? You need coremltools >= 6.3\npipeline = ct.utils.make_pipeline(*models)\nprint(f\"Saving pipeline model to {output_filename}\")\npipeline.save(output_filename)\n", "pipeline.save(output_filename)\n"]}
{"filename": "src/experiments/repro_channel_layer_norm.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport coremltools as ct\nfrom coremltools.converters.mil import Builder as mb\nimport numpy as np\n\n\"\"\"\nReproduce layer norm on the first dimension of a 4D tensor that\nworks on CPU but fails on Neural Engine.\n\"\"\"", "works on CPU but fails on Neural Engine.\n\"\"\"\n\neps = 1e-5\n\n# B,C,S = 1,4,2 # This will look like it works because it's too small for the ANE.\nB,C,S = 1,1800,16 # This will fail since it's large enough for the ANE. Make it bigger if it doesn't.\n\ng,b = 1, 0 # It's not these, the results change predictably with them.\n@mb.program(input_specs=[mb.TensorSpec(shape=(B,C,1,S)),])\ndef ln_prog(x):\n    gamma = (torch.ones((C,), dtype=torch.float32) * g).tolist()\n    beta = (torch.ones((C), dtype=torch.float32) * b).tolist()\n    x = mb.layer_norm(x=x, axes=[1], gamma=gamma, beta=beta, name=\"y\")\n    return x", "g,b = 1, 0 # It's not these, the results change predictably with them.\n@mb.program(input_specs=[mb.TensorSpec(shape=(B,C,1,S)),])\ndef ln_prog(x):\n    gamma = (torch.ones((C,), dtype=torch.float32) * g).tolist()\n    beta = (torch.ones((C), dtype=torch.float32) * b).tolist()\n    x = mb.layer_norm(x=x, axes=[1], gamma=gamma, beta=beta, name=\"y\")\n    return x\n\nx = torch.arange(B*C*S).reshape(B,C,1,S).float()\n# x = torch.cat([torch.ones((1,1,1,S)).float()] + [torch.zeros((1,1,1,S)).float() for i in range(C-1)], dim=1)", "x = torch.arange(B*C*S).reshape(B,C,1,S).float()\n# x = torch.cat([torch.ones((1,1,1,S)).float()] + [torch.zeros((1,1,1,S)).float() for i in range(C-1)], dim=1)\n# x = torch.cat([torch.ones((1,C,1,1)).float() * C*2] + [torch.zeros((1,C,1,1)).float() for i in range(S-1)], dim=3)\n# print(x.squeeze(2).permute(0,2,1))\n# x = x[torch.randperm(x.shape[0])]\n# x = torch.randn((B,C,1,S)).float() + 1000\n\ndef make_model(prog, compute_units):\n    return ct.convert(\n        prog,\n        inputs=[ct.TensorType(name=\"x\", shape=(B,C,1,S), dtype=np.float32)],\n        outputs=[ct.TensorType(name=\"y\", dtype=np.float32)],\n        compute_units=compute_units,\n        # iOS 16 doesn't help.\n        convert_to=\"mlprogram\",\n    )", "\ndef predict(model, x):\n    return torch.from_numpy(model.predict({\"x\": x.numpy()})[\"y\"])\n\ncpu_model = make_model(ln_prog, ct.ComputeUnit.CPU_ONLY)\nane_model = make_model(ln_prog, ct.ComputeUnit.CPU_AND_NE)\n\nprint(\"input\\n-----\\n\",x,\"\\n\")\n\ndef ane_layer_norm(x):\n    channels_mean = x.mean(dim=1, keepdims=True)\n    zero_mean = x - channels_mean\n    zero_mean_sq = zero_mean * zero_mean\n    denom = (zero_mean_sq.mean(dim=1, keepdims=True) + eps).rsqrt()\n    return zero_mean * denom", "\ndef ane_layer_norm(x):\n    channels_mean = x.mean(dim=1, keepdims=True)\n    zero_mean = x - channels_mean\n    zero_mean_sq = zero_mean * zero_mean\n    denom = (zero_mean_sq.mean(dim=1, keepdims=True) + eps).rsqrt()\n    return zero_mean * denom\nane_layer_norm_out = ane_layer_norm(x)\nprint(\"ml-ane-transformers LayerNorm result\\n-----\\n\",ane_layer_norm_out,\"\\n\")\n", "print(\"ml-ane-transformers LayerNorm result\\n-----\\n\",ane_layer_norm_out,\"\\n\")\n\ncpu_out = predict(cpu_model, x)\nprint(\"mb.program CPU_ONLY result\\n-----\\n\",cpu_out,\"\\n\")\n\n# Will be slightly different because of float16.\nane_out = predict(ane_model, x)\nprint(\"mb.program CPU_AND_NE result\\n-----\\n\",ane_out,\"\\n\")\n\n\ndef custom_norm(x, dim):\n    channels_mean = x.mean(dim=dim, keepdims=True)  # B11S\n    zero_mean = x - channels_mean                   # BC1S\n    zero_mean_sq = zero_mean * zero_mean            # BC1S\n    denom = (zero_mean_sq.mean(dim=dim, keepdims=True) + eps).rsqrt() # B11S\n    return zero_mean * denom", "\n\ndef custom_norm(x, dim):\n    channels_mean = x.mean(dim=dim, keepdims=True)  # B11S\n    zero_mean = x - channels_mean                   # BC1S\n    zero_mean_sq = zero_mean * zero_mean            # BC1S\n    denom = (zero_mean_sq.mean(dim=dim, keepdims=True) + eps).rsqrt() # B11S\n    return zero_mean * denom\n\n# for i in range(len(x.shape)):", "\n# for i in range(len(x.shape)):\n#     print(i, \"result\\n-----\\n\",custom_norm(x, i),\"\\n\")\n#     break\n\n# x_norms = torch.norm(x, p=2, dim=1, keepdim=True)\n# print(x_norms.shape)\n# x_norm = x / ((x_norms + eps) * 2 * C)\n# print(x_norms)\n", "# print(x_norms)\n\n# Trying to find the pattern in how the ANE output is different.\nprint(cpu_out.mean())\nprint(B,C,S, (ane_out / cpu_out).mean())\n# 1 1 2048 tensor(inf) (ane_out = inf, cpu_out = 0)\n# 1 2 2048 tensor(3.9978)\n# 1 3 2048 tensor(nan) (eyeballing it looks like ~8 but there's a bunch of zeros)\n# 1 4 2048 tensor(8.9392)\n# 1 5 2048 tensor(nan) (eyeballing it looks like ~1.26 but there's a bunch of zeros)", "# 1 4 2048 tensor(8.9392)\n# 1 5 2048 tensor(nan) (eyeballing it looks like ~1.26 but there's a bunch of zeros)\n# 1 6 2048 tensor(13.6535)\n# 1 7 2048 tensor(17.2813) # median\n# 1 8 2048 tensor(18.3189)\n# 1 10 2048 tensor(22.9630)\n# 1 14 2048 tensor(32.2279)\n# 1 15 2048 tensor(7249.1562) (maybe median is a better measure, that's 35)\n# 1 16 2048 tensor(36.8549)\n# 1 20 2048 tensor(46.0989)", "# 1 16 2048 tensor(36.8549)\n# 1 20 2048 tensor(46.0989)\n# 1 30 2048 tensor(69.1986)\n\n# 1 20 1024 tensor(23.0494)\n\n# 1 1800 1 tensor(2.2537)\n# 1 1800 2 tensor(4.2657)\n# 1 1800 3 tensor(6.2848)\n# 1 1800 4 tensor(8.3176)", "# 1 1800 3 tensor(6.2848)\n# 1 1800 4 tensor(8.3176)\n# 1 1800 5 tensor(10.3583)\n# 1 1800 16 tensor(32.6669)\n\n# 1 3600 8 tensor(32.6650) # BCS (no 1)\n\n# 1 3600 1 tensor(4.2657)"]}
{"filename": "src/experiments/repro_pipeline_compilation_bug.py", "chunked_list": ["import coremltools as ct\nfrom coremltools.converters.mil.mil import Function, Program\nfrom coremltools.converters.mil.mil import Builder as mb\nimport numpy as np\n\n\"\"\"\nFor filing a GH issue about how compiled pipelines include\nmultiple copies of the weights.\n\"\"\"\n\ndef _make_model(input_name, input_length,\n                output_name, output_length,\n                convert_to):\n\n    weight_tensor = np.arange(input_length * output_length, dtype='float32')\n    weight_tensor = weight_tensor.reshape(output_length, input_length)\n\n    prog = Program()\n    func_inputs = {input_name: mb.placeholder(shape=(input_length,))}\n    with Function(func_inputs) as ssa_fun:\n        input = ssa_fun.inputs[input_name]\n        y = mb.linear(x=input, weight=weight_tensor, name=output_name)\n        ssa_fun.set_outputs([y])\n        prog.add_function(\"main\", ssa_fun)\n\n    return ct.convert(prog, convert_to=convert_to)", "\"\"\"\n\ndef _make_model(input_name, input_length,\n                output_name, output_length,\n                convert_to):\n\n    weight_tensor = np.arange(input_length * output_length, dtype='float32')\n    weight_tensor = weight_tensor.reshape(output_length, input_length)\n\n    prog = Program()\n    func_inputs = {input_name: mb.placeholder(shape=(input_length,))}\n    with Function(func_inputs) as ssa_fun:\n        input = ssa_fun.inputs[input_name]\n        y = mb.linear(x=input, weight=weight_tensor, name=output_name)\n        ssa_fun.set_outputs([y])\n        prog.add_function(\"main\", ssa_fun)\n\n    return ct.convert(prog, convert_to=convert_to)", "\n\nif __name__ == \"__main__\":\n    # Create models\n    m1 = _make_model(\"x\", 20, \"y1\", 10, \"mlprogram\")\n    m2 = _make_model(\"y1\", 10, \"y2\", 2, \"mlprogram\")\n\n    pipeline_model = ct.utils.make_pipeline(m1, m2)\n\n    pipeline_model.save(\"test-pipeline.mlpackage\")"]}
{"filename": "src/experiments/print_model_weight_stats.py", "chunked_list": ["import numpy as np\nfrom models.pythia import GPT as Pythia\nimport torch\nimport plotille\n\n\"\"\"\nPrint out statistics about model weights.\n\"\"\"\n\nmodels = [", "\nmodels = [\n    # \"pythia-70m\",\n    # \"pythia-160m\",\n    \"pythia-410m\",\n    \"pythia-1b\",\n    \"pythia-1.4b\",\n    \"pythia-2.8b\",\n]\n", "]\n\n@torch.no_grad()\ndef print_weight_stats(model_name):\n    print(\"model\", model_name)\n    model = Pythia.from_pretrained(model_name).eval()\n\n    # Print the min/max/mean across all params.\n    # for name, param in model.named_parameters():\n    #     print(name, param.shape, param.dtype, param.min(), param.max(), param.mean())\n\n    all_params = []\n    for name, param in model.named_parameters():\n        all_params.append(param.flatten()) # a tensor\n    all_params = torch.cat(all_params, dim=0)\n\n    print(\"all params\", all_params.shape, all_params.dtype, all_params.min(), all_params.max(), all_params.mean())\n\n    counts, bins = np.histogram(all_params.numpy(), bins=10)\n    print(plotille.hist_aggregated(\n        counts,\n        bins,\n        width=80,\n        log_scale=True,\n        linesep='\\n',\n        lc=None,\n        bg=None,\n        color_mode='names',\n    ))\n\n    print(\"------\\n\")", "\nfor m in models:\n    print_weight_stats(m)\n"]}
{"filename": "src/experiments/memory_speedup.py", "chunked_list": ["import torch\nfrom torch import nn\nimport numpy as np\nimport coremltools as ct\nimport time\n\n\"\"\"\nGenerate a large-ish model for evaluating the speed of\nct.convert on different versions of coremltools.\n\"\"\"", "ct.convert on different versions of coremltools.\n\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self, num_loops):\n        super().__init__()\n        self.ls = nn.ModuleList([\n            l for _ in range(num_loops)\n            for l in [nn.Linear(1600, 6400), nn.GELU(), nn.Linear(6400, 1600)]\n        ])\n\n    def forward(self, x):\n        for l in self.ls:\n            x = l(x)\n        return x", "\nif __name__ == \"__main__\":\n    # 1600*6400*2 = 20.4M params + 40.96MB per loop\n    num_loops = 20\n    net = Net(num_loops).eval()\n\n    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n    traced = torch.jit.trace(net, (input_ids))\n\n    total_params = sum(p.numel() for p in traced.parameters())\n    print(f\"{total_params / 1000 / 1000:0.3f}M params\")\n    print(f\"{(total_params * 16 / 8) / 1024 / 1024:0.1f} MB @ f16\")\n\n    start = time.perf_counter()\n    ct.convert(\n        traced,\n        inputs=[\n            ct.TensorType(name=\"inputs\", shape=input_ids.shape, dtype=np.float32),\n        ],\n        outputs=[\n            ct.TensorType(name=\"outputs\", dtype=np.float32),\n        ],\n        compute_precision=ct.precision.FLOAT16, # FLOAT32 is faster.\n        convert_to=\"mlprogram\",\n    )\n    end = time.perf_counter()\n    print(f\"convert took: {end - start:0.4f}s\")"]}
{"filename": "src/experiments/max_size_model.py", "chunked_list": ["import torch\nfrom torch import nn\nimport numpy as np\nimport coremltools as ct\nimport sys\n\n\"\"\"\nGenerate large models (either number of parameters or number of ops) to\ntry and find the limits of the neural engine. It seems to be size-based.\n\"\"\"", "try and find the limits of the neural engine. It seems to be size-based.\n\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self, num_loops):\n        super().__init__()\n        # Many weights.\n        self.ls = nn.ModuleList([\n            l for _ in range(num_loops)\n            for l in [nn.Linear(1600, 6400), nn.GELU(), nn.Linear(6400, 1600)]\n        ])\n        # Many ops.\n        # self.ls = nn.ModuleList([\n        #     l for _ in range(num_loops)\n        #     for l in [nn.Linear(20, 10), nn.GELU(), nn.Linear(10, 20)]\n        # ])\n\n    def forward(self, x):\n        for l in self.ls:\n            x = l(x)\n\n        return x", "\nif __name__ == \"__main__\":\n    size = 20 # 20 gets you about 800MB.\n    net = Net(size).eval()\n\n    input_ids = torch.rand((1,512,1600,), dtype=torch.float32)\n\n    if input_ids.shape[-1] > 20 and size > 200:\n        print(\"Comment this out if you really want to make a 10GB+ model.\")\n        sys.exit(1)\n\n    traced = torch.jit.trace(net, (input_ids))\n\n    total_params = sum(p.numel() for p in traced.parameters())\n    print(f\"{total_params} params\")\n    print(f\"{(total_params * 16 / 8) / 1024 / 1024} MB @ f16\")\n\n    mlmodel = ct.convert(\n        traced,\n        inputs=[\n            ct.TensorType(name=\"input_ids\", shape=input_ids.shape, dtype=np.float32),\n        ],\n        outputs=[\n            ct.TensorType(name=\"logits\", dtype=np.float32),\n        ],\n        compute_precision=ct.precision.FLOAT16,\n        convert_to=\"mlprogram\",\n    )\n    mlmodel.save(f\"test-net-{size}-loops.mlpackage\")"]}
{"filename": "src/experiments/custom_softmax.py", "chunked_list": ["import torch\nimport coremltools as ct\nfrom coremltools.converters.mil import Builder as mb\nimport numpy as np\nfrom coremltools.converters.mil.mil import types\n\n\"\"\"\nConfirm that coremltools uses the softmax subtraction trick (it must, right?).\n\"\"\"\n", "\"\"\"\n\nS = 3\n@mb.program(input_specs=[mb.TensorSpec(shape=(S,), dtype=types.fp16),], opset_version=ct.target.iOS16)\ndef sm_prog(x):\n    return mb.softmax(x=x, axis=-1, name=\"y\")\n\nf32 = False\nmlmodel = ct.convert(sm_prog,\n                    inputs=[ct.TensorType(name=\"x\", shape=(S,), dtype=types.fp16)],", "mlmodel = ct.convert(sm_prog,\n                    inputs=[ct.TensorType(name=\"x\", shape=(S,), dtype=types.fp16)],\n                    outputs=[ct.TensorType(name=\"y\", dtype=types.fp16)],\n                    compute_precision=ct.precision.FLOAT32 if f32 else ct.precision.FLOAT16,\n                    compute_units=ct.ComputeUnit.CPU_ONLY,\n                    minimum_deployment_target=ct.target.iOS16,\n                    convert_to=\"mlprogram\")\n# print(mlmodel)\n\nx = torch.tensor([700_000, 700_000, 30_000]).float()", "\nx = torch.tensor([700_000, 700_000, 30_000]).float()\ninputs = {\"x\": x.numpy()}\nml_y = mlmodel.predict(inputs)[\"y\"]\n\ntorch_y = x.softmax(-1)\n\nmanual_y = torch.exp(x - x.max()) / torch.exp(x-x.max()).sum(-1, keepdim=True)\n\nprint(\"ml_y\", ml_y)", "\nprint(\"ml_y\", ml_y)\nprint(\"torch_y\", torch_y)\nprint(\"manual_y\", manual_y)\n"]}
{"filename": "src/experiments/dump_weights.py", "chunked_list": ["import coremltools as ct\nimport argparse\nimport glob\nimport re\nimport sys\nimport os\nimport numpy as np\n\nfrom coremltools.libmilstoragepython import _BlobStorageReader as BlobReader\nimport coremltools as ct", "from coremltools.libmilstoragepython import _BlobStorageReader as BlobReader\nimport coremltools as ct\nfrom coremltools.converters.mil.mil import types\nfrom coremltools.converters.mil.frontend.milproto.helper import proto_to_types\n\n\"\"\"\nParse out the ops that use weights from weight.bin from a .proto mlpackage proto file.\n\"\"\"\n\nparser = argparse.ArgumentParser(description='Parse .mlpackage to inspect weights')", "\nparser = argparse.ArgumentParser(description='Parse .mlpackage to inspect weights')\nparser.add_argument('model_path', help='path to a .mlmodelc file', type=str)\nargs = parser.parse_args()\n\ndef get_nn(spec):\n    if spec.WhichOneof(\"Type\") == \"neuralNetwork\":\n        nn_spec = spec.neuralNetwork\n    elif spec.WhichOneof(\"Type\") in \"neuralNetworkClassifier\":\n        nn_spec = spec.neuralNetworkClassifier\n    elif spec.WhichOneof(\"Type\") in \"neuralNetworkRegressor\":\n        nn_spec = spec.neuralNetworkRegressor\n    elif spec.WhichOneof(\"Type\") in \"mlProgram\":\n        nn_spec = spec.mlProgram\n    else:\n        raise ValueError(f\"Invalid neural network specification for the model: {spec.WhichOneof('Type')}\")\n    return nn_spec", "\nspec =  ct.utils.load_spec(args.model_path)\nnn_spec = get_nn(spec)\n# print(dir(nn_spec))\nprint(f\"attributes: {nn_spec.attributes}\\ndocString: {nn_spec.docString}\\nversion: {nn_spec.version}\")\nprint(f\"{len(nn_spec.functions)} functions\")\nassert len(nn_spec.functions) == 1, \"haven't seen a spec with > 1 function\"\nfn_name, fn = list(nn_spec.functions.items())[0]\nprint(f\"{fn_name} function\\n------\")\nprint(f\"opset: {fn.opset}\")", "print(f\"{fn_name} function\\n------\")\nprint(f\"opset: {fn.opset}\")\nprint(f\"attributes: {fn.attributes}\")\nprint(f\"block_specializations: {list(fn.block_specializations.keys())}\")\nblock = fn.block_specializations[fn.opset]\nprint(f\"{fn.opset} block\\n------\")\nprint(f\"attributes: {block.attributes}\")\nprint(f\"operations count: {len(block.operations)}\")\n\ntotal_bits = 0", "\ntotal_bits = 0\nblob_reader = BlobReader(os.path.join(args.model_path, \"Data/com.apple.CoreML/weights/weight.bin\"))\nfor op in block.operations:\n    for name, att in op.attributes.items():\n        if att.WhichOneof(\"value\") != \"blobFileValue\":\n            continue\n\n        valuetype = proto_to_types(att.type)\n        is_tensor = types.is_tensor(valuetype)\n        if not is_tensor:\n            print(f\"Skipping non-tensor type: {att.type.WhichOneof('type')}\")\n            continue\n\n        if op.type in [\"const\"]: # constexpr too?\n            # print(op.type, att.blobFileValue.fileName, att.blobFileValue.offset, op.outputs[0].name)\n            offset = att.blobFileValue.offset\n            dtype = valuetype if not is_tensor else valuetype.get_primitive()\n            shape = () if not is_tensor else valuetype.get_shape()\n\n\n            if dtype == types.uint8:\n                value_bits = 8\n            elif dtype == types.int8:\n                value_bits = 8\n            elif dtype == types.fp16:\n                value_bits = 16\n            elif dtype == types.fp32:\n                value_bits = 32\n            else:\n                raise ValueError(f\"Invalid dtype for blob file value type: {dtype}\")\n\n            num_values = np.product(shape)\n            print(op.outputs[0].name, num_values, value_bits)\n            total_bits += (num_values * value_bits)", "\n            # if dtype == types.uint8:\n            #     np_value = np.array(blob_reader.read_uint8_data(offset), np.uint8)\n            # elif dtype == types.int8:\n            #     np_value = np.array(blob_reader.read_int8_data(offset), np.int8)\n            # elif dtype == types.fp16:\n            #     np_value_uint16 = np.array(blob_reader.read_fp16_data(offset), np.uint16)\n            #     np_value = np.frombuffer(np_value_uint16.tobytes(), np.float16)\n            # elif dtype == types.fp32:\n            #     np_value = np.array(blob_reader.read_float_data(offset), np.float32)", "            # elif dtype == types.fp32:\n            #     np_value = np.array(blob_reader.read_float_data(offset), np.float32)\n            # else:\n            #     raise ValueError(f\"Invalid dtype for blob file value type: {dtype}\")\n\n            # value = np_value\n            # if dtype in (types.fp16, types.int8, types.uint8, types.uint32):\n            #     value = np.frombuffer(value, types.nptype_from_builtin(dtype)).reshape(\n            #         shape\n            #     )", "            #         shape\n            #     )\n            # elif dtype == types.str and shape == ():\n            #     value = str(value[0])\n            # elif dtype in (types.fp32, types.str, types.bool, types.int32, types.int64):\n            #     value = (\n            #         np.array(value).astype(types.nptype_from_builtin(dtype)).reshape(shape)\n            #     )\n            # else:\n            #     raise ValueError(f\"Invalid dtype for tensor value: {dtype}\")", "            # else:\n            #     raise ValueError(f\"Invalid dtype for tensor value: {dtype}\")\n\n            # print(value)\n\nprint(f\"total: {total_bits} bits\")"]}
{"filename": "src/experiments/dump_mil_weights.py", "chunked_list": ["import coremltools as ct\nimport argparse\nimport glob\nimport re\nimport sys\n\nfrom coremltools.libmilstoragepython import _BlobStorageReader as BlobReader\n\n\"\"\"\nParse out the ops that use weights from weight.bin from a .mil file.", "\"\"\"\nParse out the ops that use weights from weight.bin from a .mil file.\n\"\"\"\n\nparser = argparse.ArgumentParser(description='Parse .mil to inspect weights')\nparser.add_argument('model_path', help='path to a .mlmodelc file', type=str)\nargs = parser.parse_args()\n\nmil_path = f\"{args.model_path}/model.mil\"\n", "mil_path = f\"{args.model_path}/model.mil\"\n\nweight_lines = []\nwith open(mil_path, 'r') as f:\n    for line in f:\n        if \"BLOBFILE\" in line:\n            weight_lines.append(line)\n\nweight_names = []\nfor l in weight_lines:\n    name = re.search(r'>\\s*(.*?)\\s*=', l).group(1)\n    weight_names.append(name)", "weight_names = []\nfor l in weight_lines:\n    name = re.search(r'>\\s*(.*?)\\s*=', l).group(1)\n    weight_names.append(name)\n\nfor n in weight_names:\n    print(n)\n"]}
{"filename": "src/experiments/check_psnr.py", "chunked_list": ["import coremltools as ct\nimport argparse\nimport torch\nimport numpy as np\nfrom src.utils.model_proxy import MLModelProxy\nfrom models.gpt2 import GPT as GPT2\nfrom models.pythia import GPT as Pythia\nfrom src.utils.psnr import compute_psnr\n\n\"\"\"", "\n\"\"\"\nCheck PSNR between a CoreML model and a non-CoreML model.\nOver 60 means there was little loss in the conversion process.\n\"\"\"\n\nall_names = GPT2.model_names() + Pythia.model_names()\n\nparser = argparse.ArgumentParser(description='Load a CoreML modelpackage and generate some text.')\nparser.add_argument('mlmodelc_path', help='path to .mlpackage file', default=\"gpt2.mlpackage\", type=str)", "parser = argparse.ArgumentParser(description='Load a CoreML modelpackage and generate some text.')\nparser.add_argument('mlmodelc_path', help='path to .mlpackage file', default=\"gpt2.mlpackage\", type=str)\nparser.add_argument('model_name', choices=all_names, default=\"gpt2\", type=str)\n\nargs = parser.parse_args()\n\n\nmodel_class = GPT2 if \"gpt2\" in args.model_name else Pythia\nbaseline_model = model_class.from_pretrained(args.model_name)\n", "baseline_model = model_class.from_pretrained(args.model_name)\n\n# Not all models work on CPU_ONLY (e.g. pythia-70m)\nmlmodel = MLModelProxy(args.mlmodelc_path, ct.ComputeUnit.CPU_AND_NE)\n\ndef jaccard(x,y):\n    z=set(x).intersection(set(y))\n    a=float(len(z))/(len(x)+len(y)-len(z))\n    return a\n", "\npsnrs = []\nfor i in range(5):\n    input_ids = torch.randint(10_000, (1,512,))\n    output_mask = torch.randint(512, (1,))\n    with torch.no_grad():\n        baseline_out = baseline_model(input_ids, output_mask).to(torch.float32)\n    input_ids = input_ids.int()\n    output_mask = output_mask.int()\n    # Hanging here? It's very likely your intputs are the wrong shape and/or types.\n    print(\"predicting with mlmodel\")#, input_ids.shape, input_ids.dtype)\n    mlmodel_out = mlmodel.predict({\"input_ids\": input_ids.numpy(), \"output_mask\": output_mask.numpy()})\n    mlmodel_out = torch.from_numpy(mlmodel_out[\"logits\"]).to(torch.float32)\n\n    assert baseline_out.shape == mlmodel_out.shape, f\"{baseline_out.shape} != {mlmodel_out.shape}\"\n    assert baseline_out.dtype == mlmodel_out.dtype, f\"{baseline_out.dtype} != {mlmodel_out.dtype}\"\n\n    psnr = compute_psnr(baseline_out, mlmodel_out)\n    print(\"PSNR:\", psnr)\n    psnrs.append(psnr)\n\n    for k in [80]: #range(40, 400, 40):\n        print(\"k:\", k)\n        baseline_topk = torch.topk(torch.nn.functional.softmax(baseline_out, dim=-1), k)\n        coreml_topk = torch.topk(torch.nn.functional.softmax(mlmodel_out, dim=-1), k)\n        # print(\"baseline_topk:\", baseline_topk.indices)\n        # print(\"coreml_topk:\", coreml_topk.indices)\n\n        topk_psnr = compute_psnr(baseline_out[:, :, baseline_topk.indices], mlmodel_out[:, :, baseline_topk.indices])\n        print(\"topk PSNR:\", topk_psnr)\n        # closer to 1 is better\n        print(\"jaccard topk\", jaccard(baseline_topk.indices.flatten().tolist(), coreml_topk.indices.flatten().tolist()))\n\n        kl_div = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(mlmodel_out, dim=-1), torch.nn.functional.softmax(baseline_out, dim=-1), reduction=\"batchmean\")\n        # clsoer to 0 is better\n        print(\"kl div\", kl_div.item())\n    print(\"\")", "\nprint(\"Mean PSNR:\", np.average(psnrs))\nprint(\"Median PSNR:\", np.median(psnrs))"]}
