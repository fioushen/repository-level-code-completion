{"filename": "covid-optimize.py", "chunked_list": ["import os\nimport argparse\nfrom fairseq_models import AntibodyRobertaModel\n\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils", "import torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.data import encoders\nfrom fairseq import checkpoint_utils\nfrom tqdm import tqdm\nfrom fairseq.data import Dictionary\n\nfrom fairseq_models.data.abgen_dataset import AntibodyComplexDataset\nfrom fairseq_models.data.ab_dictionary import ALPHABET\nfrom fairseq_models.modules.utils import compute_rmsd", "from fairseq_models.data.ab_dictionary import ALPHABET\nfrom fairseq_models.modules.utils import compute_rmsd\nfrom fairseq.data import data_utils\n\nimport json\nimport csv\nimport math, random, sys\nimport numpy as np\nimport argparse\nimport os", "import argparse\nimport os\nfrom copy import deepcopy\nfrom tqdm import tqdm, trange\n\nfrom neut_model import *\nfrom structgen import *\n\nfrom igfold import IgFoldRunner\n", "from igfold import IgFoldRunner\n\n\nclass CovidNeutralizationModel():\n    \n    def __init__(self):\n        MODEL_PATH = \"checkpoints/pretrained/ckpts/covid/neut_model1.ckpt\"\n        MODEL_ARGS = {\n            \"hidden_dim\": 256,\n            \"n_layers\": 2,\n            \"use_srupp\": False\n        }\n        MODEL_ARGS.update(MultiABOnlyCoronavirusModel.add_extra_args())\n        self.model = MultiABOnlyCoronavirusModel.load_from_checkpoint(\n            MODEL_PATH,\n            **MODEL_ARGS,  # type: ignore\n        )\n        self.model.cuda()\n        self.model.eval()\n\n    def predict(self, vh, vl):\n        ab_sequence = vh\n        ab_sequence = torch.tensor([AA_VOCAB[aa] for aa in ab_sequence]).long()\n        ab_sequence = ab_sequence.unsqueeze(0).cuda()\n        neut_logits = self.model(ab_sequence)\n        prob = torch.sigmoid(neut_logits)\n        return prob[0, 1].item()", "\ndef data_curate(batch, seq_vocab, tag_vocab, mask_idx):\n    entry = batch\n    # construct data for ABGNN\n    surface = torch.tensor(\n            [i for i,v in enumerate(entry['cdr']) if v in ['3']]\n    )\n    paratope_surface = surface\n    \n    seq_len = len(entry['seq'])\n    cdr_len = len(surface)\n    antibody_coords = torch.zeros((seq_len, 4, 3))\n    mask_cdr_coords = torch.zeros((cdr_len, 4, 3))\n    paratope_coords = mask_cdr_coords.clone()\n\n    paratope_seq = ''.join([entry['seq'][i] for i in surface.tolist()])\n    paratope_seq = torch.tensor([ALPHABET.index(a) for a in paratope_seq])\n\n    antibody_seq = torch.tensor([seq_vocab.index(a) for a in batch['seq']])\n    antibody_cdr = torch.tensor([tag_vocab.index(a) for a in entry['cdr']])\n\n    # data for AbBERT\n    cdr_mask = antibody_cdr == tag_vocab.index('3')\n\n    masked_antibody_seq = torch.full_like(antibody_seq, mask_idx)\n    masked_antibody_seq[~cdr_mask] = antibody_seq[~cdr_mask]\n    \n    label_antibody_seq = torch.full_like(antibody_seq, seq_vocab.pad())\n    label_antibody_seq[cdr_mask] = antibody_seq[cdr_mask]\n\n    antibody_seq = antibody_seq - 3\n    \n    cdr_string = entry['cdr']\n    \n    return masked_antibody_seq.cuda(), antibody_cdr.cuda(), label_antibody_seq.cuda(), \\\n            paratope_seq.cuda(), paratope_coords.cuda(), \\\n            paratope_surface.cuda(), mask_cdr_coords.cuda(), \\\n            antibody_coords.cuda(), antibody_seq.cuda(), cdr_string", "\n\ndef collater(batch, pad_idx):\n    try:\n        masked_antibody_seq, antibody_cdr, label_antibody_seq, \\\n        paratope_seq, paratope_coords, \\\n        paratope_surface, mask_cdr_coords, \\\n        antibody_coords, antibody_seq, cdr_string \\\n        = tuple(zip(*batch))\n    except:\n        return None\n\n    # for bert\n    batched_seq = data_utils.collate_tokens(masked_antibody_seq, pad_idx, left_pad=False)\n    batched_tag = data_utils.collate_tokens(antibody_cdr, pad_idx, left_pad=False)\n    batched_label = data_utils.collate_tokens(label_antibody_seq, pad_idx, left_pad=False)\n\n    # for decoder \n    def featurize_paratope(seq_batch, coords_batch, mask_cdr_coords_batch):\n        X = pad_sequence(coords_batch, batch_first=True, padding_value=0)\n        S = pad_sequence(seq_batch, batch_first=True, padding_value=0)\n        X_init = torch.cat([i for i in mask_cdr_coords_batch])\n        cont_S = torch.cat([i for i in seq_batch])\n        return X, S, cont_S, X_init\n\n    def feature_framework(antibody_seq, antibody_coords, antibody_cdr):\n        X = pad_sequence(antibody_coords, batch_first=True, padding_value=0)\n        S = pad_sequence(antibody_seq, batch_first=True, padding_value=0)\n        antibody_cdr = list(antibody_cdr)\n        mask = S.bool().float()\n        return X, S, antibody_cdr, mask\n\n    paratope = featurize_paratope(paratope_seq, paratope_coords, mask_cdr_coords)\n    antibody = feature_framework(antibody_seq, antibody_coords, cdr_string)\n    return batched_seq, batched_tag, batched_label, paratope, antibody", "\n\ndef extract_fn(query):\n    all_viruses = [\n        x.replace(\"(weak)\", \"\").strip()\n        for x in query.replace(\" and \", \";\").replace(\",\", \";\").split(\";\")\n    ]\n    filtered = [virus for virus in all_viruses if virus in RELEVANT_VIRUSES]\n    return set(filtered)\n", "\n\ndef load_data():\n    # Load antibody data\n    with open(\"dataset/exp3-covabdab/CoV-AbDab_050821.csv\", \"r\") as f:\n        full_data = list(csv.DictReader(f))\n\n    # First filter to relevant AB's\n    full_data = [\n        item\n        for item in full_data\n        if any(\n            virus in item[key]\n            for virus in RELEVANT_VIRUSES\n            for key in RELEVANT_KEYS\n        )\n        and item[\"Ab or Nb\"] == \"Ab\"  # remove nanobodies\n        and len(item[\"VH or VHH\"].strip()) > 2  # ensure sequence available\n        and len(item[\"VL\"].strip()) > 2  # ensure sequence available\n        and \"S\" in item[\"Protein + Epitope\"]  # ensure binds to S protein\n        and TYPE_MAP[item[\"Protein + Epitope\"]] == \"rbd\"\n    ]\n\n    sarscov2_ab_data = []\n    for item in full_data:\n        bindings = set()\n        if item[\"Binds to\"]:\n            bindings = extract_fn(item[\"Binds to\"])\n\n        non_bindings = set()\n        if item[\"Doesn't Bind to\"]:\n            non_bindings = extract_fn(item[\"Doesn't Bind to\"])\n\n        neutralizing = set()\n        if item[\"Neutralising Vs\"]:\n            neutralizing = extract_fn(item[\"Neutralising Vs\"])\n\n        non_neutralizing = set()\n        if item[\"Not Neutralising Vs\"]:\n            non_neutralizing = extract_fn(item[\"Not Neutralising Vs\"])\n\n        all_viruses = bindings | non_bindings | neutralizing | non_neutralizing\n        full_label = [0, 0]\n        full_mask = [0, 0]\n        for virus in all_viruses:\n            label = [0, 0]\n            mask = [0, 0]\n            if virus in bindings and neutralizing:\n                label = [1, 1]\n                mask = [1, 1]\n            elif virus in bindings and non_neutralizing:\n                label = [0, 1]\n                mask = [1, 1]\n            elif virus in non_bindings:\n                label = [0, 0]\n                mask = [1, 1]\n            elif virus in neutralizing:\n                label = [1, 1]\n                mask = [1, 1]\n            elif virus in bindings:\n                label = [0, 1]\n                mask = [0, 1]\n            elif virus in non_neutralizing:\n                label = [0, 1]\n                mask = [1, 0]\n\n            idx = virus == \"SARS-CoV2\"\n            full_label[idx] = label[0]\n            full_mask[idx] = mask[0]\n\n        # Save sars-cov2 data for later\n        sarscov2_ab_data.append({\n            'name': item[\"\\ufeffName\"],\n            'label': full_label[1],\n            'mask': full_mask[1],\n            'epitope': TYPE_MAP[item[\"Protein + Epitope\"]],\n            'ab': item[\"VH or VHH\"].replace(\" \", \"\") + \"-\" + item[\"VL\"].replace(\" \", \"\"),\n            'hcdr3': item[\"CDRH3\"],\n            'lcdr3': item[\"CDRL3\"],\n        })\n    return sarscov2_ab_data", "\ndef make_entry(d, args, igfold):\n    entry = {k: d[k] for k in ['name', 'hcdr3', 'lcdr3']}\n    entry['VH'], entry['VL'] = d['ab'].split('-')\n    assert entry['VH'].count(entry['hcdr3']) == 1\n    entry['context'] = entry['VH'].replace(entry['hcdr3'], '#' * len(entry['hcdr3']))\n    fw1, fw2 = entry['context'].replace('#', ' ').split()\n    entry['cdr'] = '0' * len(fw1) + '3' * len(entry['hcdr3']) + '0' * len(fw2)\n    \n    sequences = {\n        \"H\": entry['VH'],\n        \"L\": entry['VL'],\n    }\n    pred_pdb = \"checkpoints/exp3-igfold-structs/\" + entry['name'] + \".pdb\" # save dir\n\n    predicted_pdb = igfold.fold(\n        pred_pdb, # Output PDB file\n        sequences=sequences, # Antibody sequences\n        do_refine=False, # Refine the antibody structure with PyRosetta\n        do_renum=False, # Renumber predicted antibody structure (Chothia)\n    )\n    coords = np.array(predicted_pdb.coords[0, :len(entry['VH'])].cpu()) # (N, 5, 3)\n    \n    entry['coords'] = {\n            \"N\": coords[:, 0],\n            \"CA\":coords[:, 1],\n            \"C\": coords[:, 2],\n            \"O\": coords[:, 4],\n    }\n    entry['label'] = d['label']\n    entry['seq'] = entry['VH'] if args.architecture == 'hierarchical' else entry['hcdr3']\n    return entry", "\n\n# Decode new sequences\ndef decode(model, ab, args, seq_vocab, tag_vocab, mask_idx):\n    ab = data_curate(ab, seq_vocab, tag_vocab, mask_idx)\n    batch = [ab] * args.batch_size\n    model.eval()\n    model.inference = True\n    with torch.no_grad():\n        batched_seq, batched_tag, batched_label, paratope, antibody = collater(batch, seq_vocab.pad())\n        \n        masked_tokens = batched_label.ne(seq_vocab.pad())\n        sample_size = masked_tokens.int().sum()\n        \n        out = model(\n            src_tokens=batched_seq, \n            tag_tokens=batched_tag,\n            paratope=paratope,\n            epitope=None,\n            antibody=antibody,\n            masked_tokens=masked_tokens,\n            num_decode=1\n            )[0]\n        new_seqs = out.handle\n            \n    return new_seqs, out.ppl.exp()", "\n\ndef evaluate(model, predictor, evaluator, data, args, seq_vocab, tag_vocab, mask_idx):\n    succ, tot = 0, 0\n    sum_ppl, tot_aa = 0., 0.\n    model.eval()\n    with torch.no_grad():\n        for ab in tqdm(data):\n            # print(ab)\n            new_seqs, ppl = decode(model, ab, args, seq_vocab, tag_vocab, mask_idx)\n            tot = tot + len(new_seqs)\n            prior_ppl = is_natural_seq(evaluator, ab, new_seqs)\n            for new_cdr, ppl in zip(new_seqs, prior_ppl):\n                if is_valid_seq(new_cdr) and ppl <= args.max_prior_ppl:\n                    VH = ab['VH'].replace(ab['hcdr3'], new_cdr)\n                    prob = predictor.predict(VH, ab['VL'])\n                    succ = succ + prob\n                else:\n                    succ = succ + ab['score']  # not valid, improvement=0\n            sum_ppl += ppl * len(new_seqs)\n            tot_aa += len(new_seqs)\n    return succ / tot, sum_ppl / tot_aa", "\n\n# Glycoslation is bad\ndef is_valid_seq(cdr):\n    if '#' in cdr: return False\n    charge = [CHARGE[x] for x in cdr]\n    if sum(charge) >= 3 or sum(charge) <= -3:\n        return False\n    for a in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n        if ('N' + a + 'T') in cdr:\n            return False\n        if ('N' + a + 'S') in cdr:\n            return False\n        if (a * 4) in cdr:\n            return False\n    return True", "\n\ndef is_natural_seq(evaluator, ab, cand_cdrs):\n    with torch.no_grad():\n        batch = []\n        for cdr in cand_cdrs:\n            ab = deepcopy(ab)\n            ab['seq'] = ab['VH'].replace(ab['hcdr3'], cdr)\n            batch.append(ab)\n\n        hX, hS, hL, hmask = completize(batch)\n        cand_ppl1 = evaluator[0].log_prob(hS, hL, hmask).ppl.exp()\n\n        batch = []\n        for cdr in cand_cdrs:\n            ab = deepcopy(ab)\n            ab['seq'] = cdr\n            batch.append(ab)\n\n        (hX, hS, hL, hmask), context = featurize(batch, context=True)\n        cand_ppl2 = evaluator[1].log_prob(hS, hmask, context=context).ppl.exp()\n        cand_ppl3 = evaluator[2].log_prob(hS, hmask, context=context).ppl.exp()\n\n        cand_ppl = torch.maximum(cand_ppl1, torch.maximum(cand_ppl2, cand_ppl3))\n    return cand_ppl.tolist()", "\ndef main(args):\n    \n    modelhub = AntibodyRobertaModel.from_pretrained(\n        model_name_or_path=args.cktpath,\n        inference=True,\n        fix_bert_param=args.fix_bert_param,\n    )\n    optimizer = modelhub.optimizer\n    model = modelhub.model\n    seq_vocab = modelhub.task.source_dictionary\n    tag_vocab = modelhub.task.tag_source_dictionary\n    mask_idx = modelhub.task.mask_idx\n    model.cuda()\n\n    def print_parameter_number(net):\n        net_name = type(net).__name__\n        total_num = sum(p.numel() for p in net.parameters())\n        trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n        print(f'{net_name} total parameters:{total_num}, trainable: {trainable_num}')\n    # print_parameter_number(model.classification_heads)\n    print_parameter_number(model)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n    split_map = {}\n    with open(args.cluster) as f:\n        for line in f:\n            cdr3, fold = line.strip(\"\\r\\n \").split()\n            split_map[cdr3] = fold\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    predictor = CovidNeutralizationModel()\n    \n    optimizer.set_lr(args.lr)\n\n    # Language model ensemble (ensure CDR naturalness) \n    evaluator = [None, None, None]\n    eval_args = deepcopy(args)\n    eval_args.hidden_size = 256\n    eval_args.depth = 4\n    eval_args.block_size = 8\n    evaluator[0] = HierarchicalDecoder(eval_args).cuda()\n    evaluator[0].eval()\n    model_ckpt = torch.load(\"checkpoints/pretrained/ckpts/covid/hieratt.ckpt\")[0]\n    evaluator[0].load_state_dict(model_ckpt)\n\n    eval_args = deepcopy(args)\n    eval_args.hidden_size = 128\n    eval_args.depth = 1\n    evaluator[1] = Seq2Seq(eval_args).cuda()\n    evaluator[1].eval()\n    model_ckpt = torch.load(\"checkpoints/pretrained/ckpts/covid/lstm.ckpt\")[0]\n    evaluator[1].load_state_dict(model_ckpt)\n\n    eval_args = deepcopy(args)\n    eval_args.hidden_size = 256\n    eval_args.depth = 3\n    evaluator[2] = Decoder(eval_args, return_coords=False).cuda()\n    evaluator[2].eval()\n    model_ckpt = torch.load(\"checkpoints/pretrained/ckpts/covid/autoreg.ckpt\")[0]\n    evaluator[2].load_state_dict(model_ckpt)\n\n    # data preparation\n    igfold = IgFoldRunner()\n    all_ab = [make_entry(d, args, igfold) for d in load_data() if d['hcdr3'] in d['ab'] and d['mask'] == 1]\n    for entry in tqdm(all_ab):\n        entry['score'] = predictor.predict(entry['VH'], entry['VL'])\n\n    train_ab = [d for d in all_ab if split_map[d['hcdr3']] == 'train' and d['label'] == 1]\n    val_ab = [d for d in all_ab if split_map[d['hcdr3']] == 'val' and d['label'] == 1]\n    test_ab = [d for d in all_ab if split_map[d['hcdr3']] == 'test' and d['label'] == 1]\n    print(\"train/val/test:\", len(train_ab), len(val_ab), len(test_ab))\n\n    train_data = {entry['name'] : [entry] for entry in train_ab}\n    best_score, best_epoch = -10.0, 0\n    cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=seq_vocab.pad())\n    for e in trange(args.epochs):\n        # Decode new cdrs\n        ab = random.choice(train_ab)\n        name = ab['name']\n        new_seqs, _ = decode(model, ab, args, seq_vocab, tag_vocab, mask_idx)\n\n        prior_ppl = is_natural_seq(evaluator, ab, new_seqs)\n        for new_cdr, ppl in zip(new_seqs, prior_ppl):\n            if is_valid_seq(new_cdr) and ppl <= args.max_prior_ppl:\n                entry = deepcopy(ab)\n                entry['VH'] = entry['VH'].replace(entry['hcdr3'], new_cdr)\n                prob = predictor.predict(entry['VH'], entry['VL'])\n                #print(name, entry['hcdr3'], entry['score'], '-->', new_cdr, prob)\n                if prob > entry['score']: # first round: about 24 in 200\n                    entry['score'] = prob\n                    entry['hcdr3'] = new_cdr\n                    entry['seq'] = entry['VH'] if args.architecture == 'hierarchical' else entry['hcdr3']\n                    train_data[name].append(entry)\n    \n        if name in train_data:\n            dlist = sorted(train_data[name], key=lambda d:d['score'], reverse=True)\n            train_data[name] = dlist[:args.topk]\n        else:\n            exit(0)\n\n        # Train model\n        model.train()\n        model.inference = False\n        optimizer.zero_grad()\n        train_keys = sorted(train_data.keys())\n        \n        name = random.choice(train_keys)\n        batch = train_data[name]\n        name_samples = []\n        for name_sample in batch:\n            name_samples.append(data_curate(name_sample, seq_vocab, tag_vocab, mask_idx))\n\n        batched_seq, batched_tag, batched_label, paratope, antibody = collater(name_samples, seq_vocab.pad())\n        \n        masked_tokens = batched_label.ne(seq_vocab.pad())\n        sample_size = masked_tokens.int().sum()\n        \n        out, transformer_logits, _ = model(\n            src_tokens=batched_seq, \n            tag_tokens=batched_tag,\n            paratope=paratope,\n            epitope=None,\n            antibody=antibody,\n            masked_tokens=masked_tokens\n            )\n        \n        # compute encoder loss\n        if out.nll == 0.: # bertonly ablation\n            targets = batched_label\n            if masked_tokens is not None:\n                targets = targets[masked_tokens]\n            loss = cross_entropy(\n                transformer_logits.view(-1, transformer_logits.size(-1)),\n                targets.view(-1) - 3, \n            ) / sample_size / math.log(2)\n        else:\n            loss = out.nll\n\n        loss.backward()\n        optimizer.step()\n\n        if (e + 1) % args.valid_iter == 0:\n            ckpt = (model.state_dict(), optimizer.state_dict())\n            torch.save(ckpt, os.path.join(args.save_dir, f\"model.ckpt.{e+1}\"))\n            val_score, val_ppl = evaluate(model, predictor, evaluator, val_ab, args, seq_vocab, tag_vocab, mask_idx)\n            test_score, test_ppl = evaluate(model, predictor, evaluator, test_ab, args, seq_vocab, tag_vocab, mask_idx)\n            print(f'Epoch {e+1}: average valid neutralization score: {val_score:.3f}')\n            print(f'             average valid ppl: {val_ppl:.3f}')\n            print(f'Epoch {e+1}: average test neutralization score: {test_score:.3f}')\n            print(f'             average test ppl: {test_ppl:.3f}')\n            if val_score > best_score:\n                best_epoch = e + 1\n                best_score = val_score\n\n    # best_epoch = 10000\n    if best_epoch > 0:\n        best_ckpt = os.path.join(args.save_dir, f\"model.ckpt.{best_epoch}\")\n        model.load_state_dict(torch.load(best_ckpt)[0])\n\n    test_score, test_ppl = evaluate(model, predictor, evaluator, test_ab, args, seq_vocab, tag_vocab, mask_idx)\n    print(f'Test average neutralization score: {test_score:.3f}')\n    print(f'             average test ppl: {test_ppl:.3f}')", "\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    # data and ckpts\n    parser.add_argument(\"--cluster\", type=str, default=\"dataset/exp3-covabdab/cdrh3_split.txt\")\n    parser.add_argument(\"--cktpath\", type=str, default=\"checkpoints/0301-exp3-sabdab/checkpoint_best.pt\")\n    parser.add_argument(\"--save_dir\", type=str, default='checkpoints/exp3-ckpts/temp')\n    # training settings    \n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--batch_size\", type=int, default=100)\n    parser.add_argument(\"--lr\", type=float, default=1e-4)\n    parser.add_argument(\"--epochs\", type=int, default=10000)\n    parser.add_argument(\"--valid_iter\", type=int, default=1000)\n    parser.add_argument(\"--topk\", type=int, default=16)\n    parser.add_argument(\"--fix_bert_param\", action=\"store_true\", default=False)\n    # evaluate naturalness (and relevant refinegnn setting)  ===  fixed\n    parser.add_argument(\"--load_model\", type=str, default=\"checkpoints/pretrained/RefineGNN-rabd/model.best\")\n    parser.add_argument(\"--architecture\", type=str, default=\"hierarchical\")\n    parser.add_argument(\"--cdr_type\", type=str, default=\"3\")\n    parser.add_argument(\"--hidden_size\", type=int, default=256)\n    parser.add_argument(\"--k_neighbors\", type=int, default=9)\n    parser.add_argument(\"--update_freq\", type=int, default=1)\n    parser.add_argument(\"--augment_eps\", type=float, default=3.0)\n    parser.add_argument(\"--block_size\", type=int, default=8)\n    parser.add_argument(\"--depth\", type=int, default=4)\n    parser.add_argument(\"--vocab_size\", type=int, default=21)\n    parser.add_argument(\"--num_rbf\", type=int, default=16)\n    parser.add_argument(\"--dropout\", type=float, default=0.1)\n    parser.add_argument(\"--max_prior_ppl\", type=int, default=10)\n    parser.add_argument(\"--context\", type=bool, default=True)\n    \n    args = parser.parse_args()\n    \n    assert os.path.exists(args.cktpath)\n    \n    main(args)", ""]}
{"filename": "inference.py", "chunked_list": ["import os\nimport argparse\nfrom fairseq_models import AntibodyRobertaModel\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.data import encoders", "from fairseq import utils\nfrom fairseq.data import encoders\nfrom fairseq import checkpoint_utils\nfrom tqdm import tqdm\nfrom fairseq.data import Dictionary\n\nfrom fairseq_models.data.abgen_dataset import AntibodyComplexDataset\nfrom fairseq_models.data.ab_dictionary import ALPHABET\nfrom fairseq_models.modules.utils import compute_rmsd\ndef main(args):\n    \n    modelhub = AntibodyRobertaModel.from_pretrained(\n        model_name_or_path=args.cktpath,\n        inference=True,\n    )\n    modelhub.cuda()\n    modelhub.eval()\n    \n    num_decode=args.num_decode\n    succ, tot = 0, 0\n    tot_ppl = 0.\n    topk=args.topk\n    dataset = AntibodyComplexDataset(\n        data_path=args.data_path,\n        split=args.split,\n        seq_vocab=modelhub.task.source_dictionary,\n        tag_vocab=modelhub.task.tag_source_dictionary,\n        cdr_types=[args.cdr_type],\n        L_target=20,\n        pad_idx=modelhub.task.source_dictionary.pad(),\n        mask_idx=modelhub.task.mask_idx,\n        max_len=256\n    )\n    print('PDB', 'Native', 'Designed', 'Perplexity', 'RMSD')\n    sum_rmsd = 0.\n    with torch.no_grad():\n        for ab in tqdm(dataset):\n            new_cdrs, new_ppl, new_rmsd = [], [], []\n            sample = dataset.collater([ab] * 1)\n\n            batched_seq, batched_tag, batched_label, paratope, epitope, antibody = sample\n                                \n            batched_seq, batched_tag, batched_label = [item.to(modelhub.device) for item in [batched_seq, batched_tag, batched_label]]\n            paratope = [item.to(modelhub.device) for item in paratope]\n            epitope = [item.to(modelhub.device) for item in epitope]\n            antibody_X, antibody_S, antibody_cdr, padding_mask = antibody\n            antibody_X, antibody_S, antibody_cdr, padding_mask = antibody_X.to(modelhub.device), antibody_S.to(modelhub.device), antibody_cdr, padding_mask.to(modelhub.device)\n            antibody = (antibody_X, antibody_S, antibody_cdr, padding_mask)\n\n            masked_tokens = batched_label.ne(modelhub.task.source_dictionary.pad())\n            sample_size = masked_tokens.int().sum()\n            \n            out = modelhub.model(\n                src_tokens=batched_seq, \n                tag_tokens=batched_tag,\n                paratope=paratope,\n                epitope=epitope,\n                antibody=antibody,\n                masked_tokens=masked_tokens,\n                num_decode=num_decode\n            )[0]\n\n            bind_X, bind_S, _, _ = paratope\n            bind_mask = bind_S > 0\n\n            out_X = out.bind_X.unsqueeze(0)\n            rmsd = compute_rmsd(\n                    out_X[:, :, 1], bind_X[:, :, 1], bind_mask\n                )\n            new_rmsd.extend([rmsd] * num_decode)\n            new_cdrs.extend(out.handle)\n            new_ppl.extend(out.ppl.tolist())\n            \n            orig_cdr = ''.join([ALPHABET[i] for i in ab[3]])\n            new_res = sorted(zip(new_cdrs, new_ppl, new_rmsd), key=lambda x:x[1])\n            \n            for cdr,ppl,rmsd in new_res[:topk]:\n                # print(orig_cdr, cdr, '%.3f' % ppl, '%.4f' % rmsd)\n                match = [int(a == b) for a,b in zip(orig_cdr, cdr)]\n                succ += sum(match) \n                tot += len(match)\n                tot_ppl += ppl\n\n                sum_rmsd += rmsd\n    \n    print(f'PPL = {tot_ppl / len(dataset) / topk:.4f}')\n    print(f'RMSD = {sum_rmsd.item() / len(dataset) / topk:.4f}')\n    print(f'Amino acid recovery rate = {succ / tot:.4f}') ", "from fairseq_models.modules.utils import compute_rmsd\ndef main(args):\n    \n    modelhub = AntibodyRobertaModel.from_pretrained(\n        model_name_or_path=args.cktpath,\n        inference=True,\n    )\n    modelhub.cuda()\n    modelhub.eval()\n    \n    num_decode=args.num_decode\n    succ, tot = 0, 0\n    tot_ppl = 0.\n    topk=args.topk\n    dataset = AntibodyComplexDataset(\n        data_path=args.data_path,\n        split=args.split,\n        seq_vocab=modelhub.task.source_dictionary,\n        tag_vocab=modelhub.task.tag_source_dictionary,\n        cdr_types=[args.cdr_type],\n        L_target=20,\n        pad_idx=modelhub.task.source_dictionary.pad(),\n        mask_idx=modelhub.task.mask_idx,\n        max_len=256\n    )\n    print('PDB', 'Native', 'Designed', 'Perplexity', 'RMSD')\n    sum_rmsd = 0.\n    with torch.no_grad():\n        for ab in tqdm(dataset):\n            new_cdrs, new_ppl, new_rmsd = [], [], []\n            sample = dataset.collater([ab] * 1)\n\n            batched_seq, batched_tag, batched_label, paratope, epitope, antibody = sample\n                                \n            batched_seq, batched_tag, batched_label = [item.to(modelhub.device) for item in [batched_seq, batched_tag, batched_label]]\n            paratope = [item.to(modelhub.device) for item in paratope]\n            epitope = [item.to(modelhub.device) for item in epitope]\n            antibody_X, antibody_S, antibody_cdr, padding_mask = antibody\n            antibody_X, antibody_S, antibody_cdr, padding_mask = antibody_X.to(modelhub.device), antibody_S.to(modelhub.device), antibody_cdr, padding_mask.to(modelhub.device)\n            antibody = (antibody_X, antibody_S, antibody_cdr, padding_mask)\n\n            masked_tokens = batched_label.ne(modelhub.task.source_dictionary.pad())\n            sample_size = masked_tokens.int().sum()\n            \n            out = modelhub.model(\n                src_tokens=batched_seq, \n                tag_tokens=batched_tag,\n                paratope=paratope,\n                epitope=epitope,\n                antibody=antibody,\n                masked_tokens=masked_tokens,\n                num_decode=num_decode\n            )[0]\n\n            bind_X, bind_S, _, _ = paratope\n            bind_mask = bind_S > 0\n\n            out_X = out.bind_X.unsqueeze(0)\n            rmsd = compute_rmsd(\n                    out_X[:, :, 1], bind_X[:, :, 1], bind_mask\n                )\n            new_rmsd.extend([rmsd] * num_decode)\n            new_cdrs.extend(out.handle)\n            new_ppl.extend(out.ppl.tolist())\n            \n            orig_cdr = ''.join([ALPHABET[i] for i in ab[3]])\n            new_res = sorted(zip(new_cdrs, new_ppl, new_rmsd), key=lambda x:x[1])\n            \n            for cdr,ppl,rmsd in new_res[:topk]:\n                # print(orig_cdr, cdr, '%.3f' % ppl, '%.4f' % rmsd)\n                match = [int(a == b) for a,b in zip(orig_cdr, cdr)]\n                succ += sum(match) \n                tot += len(match)\n                tot_ppl += ppl\n\n                sum_rmsd += rmsd\n    \n    print(f'PPL = {tot_ppl / len(dataset) / topk:.4f}')\n    print(f'RMSD = {sum_rmsd.item() / len(dataset) / topk:.4f}')\n    print(f'Amino acid recovery rate = {succ / tot:.4f}') ", "\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cktpath\", type=str, default='checkpoints/baseline/checkpoint_best.pt')\n    parser.add_argument(\"--num_decode\", type=int, default=10000)\n    parser.add_argument(\"--topk\", type=int, default=100)\n    parser.add_argument(\"--cdr_type\", type=str, default='3')\n    parser.add_argument(\"--data_path\", type=str, default=\"dataset/exp2-hern\")\n    parser.add_argument(\"--split\", type=str, default=\"test\")\n    \n    args = parser.parse_args()\n    \n    assert os.path.exists(args.cktpath)\n    \n    main(args)", ""]}
{"filename": "neut_model.py", "chunked_list": ["from typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch import Tensor\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nAA_VOCAB = {", "\nAA_VOCAB = {\n    \"#\": 0,\n    \"A\": 1,\n    \"R\": 2,\n    \"N\": 3,\n    \"D\": 4,\n    \"C\": 5,\n    \"Q\": 6,\n    \"E\": 7,", "    \"Q\": 6,\n    \"E\": 7,\n    \"G\": 8,\n    \"H\": 9,\n    \"I\": 10,\n    \"L\": 11,\n    \"K\": 12,\n    \"M\": 13,\n    \"F\": 14,\n    \"P\": 15,", "    \"F\": 14,\n    \"P\": 15,\n    \"S\": 16,\n    \"T\": 17,\n    \"W\": 18,\n    \"Y\": 19,\n    \"V\": 20,\n    \"X\": 21,\n    \"-\": 22,\n    \"O\": 23,", "    \"-\": 22,\n    \"O\": 23,\n    \"*\": 24,\n}\nRELEVANT_VIRUSES = {\"SARS-CoV1\", \"SARS-CoV2\"}\nRELEVANT_KEYS = {\n    \"Neutralising Vs\",\n    \"Not Neutralising Vs\",\n    \"Binds to\",\n    \"Doesn't Bind to\",", "    \"Binds to\",\n    \"Doesn't Bind to\",\n}\nTYPE_MAP = {\n    \"S1; non-RBD\": \"ntd\",\n    \"S2 (quaternary glycan epitope)\": \"s2\",\n    \"S: NTD\": \"ntd\",\n    \"S: RBD\": \"rbd\",\n    \"S; NTD\": \"ntd\",\n    \"S; Possibly RBD\": \"rbd\",", "    \"S; NTD\": \"ntd\",\n    \"S; Possibly RBD\": \"rbd\",\n    \"S; RBD\": \"rbd\",\n    \"S; RBD/non-RBD\": \"unk\",\n    \"S; S1\": \"unk\",\n    \"S; S1 non-RBD\": \"ntd\",\n    \"S; S1/S2\": \"unk\",\n    \"S; S1/S2 Cleavage Site\": \"unk\",\n    \"S; S2\": \"s2\",\n    \"S; S2 (quaternary glycan epitope)\": \"s2\",", "    \"S; S2\": \"s2\",\n    \"S; S2 (quaternary glycan epitope)\": \"s2\",\n    \"S; S2 Stem Helix\": \"s2\",\n    \"S; Unk\": \"unk\",\n    \"S; non-RBD\": \"unk\",\n    \"S; non-S1\": \"s2\",\n    \"S; probably RBD (implied by clustering)\": \"rbd\",\n}\n\n\nclass RNNEncoder(nn.Module):\n    \"\"\"Implements a multi-layer RNN.\n\n    This module can be used to create multi-layer RNN models, and\n    provides a way to reduce to output of the RNN to a single hidden\n    state by pooling the encoder states either by taking the maximum,\n    average, or by taking the last hidden state before padding.\n    Padding is dealt with by using torch's PackedSequence.\n\n    Attributes\n    ----------\n    rnn: nn.Module\n        The rnn submodule\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        n_layers: int = 1,\n        rnn_type: str = \"lstm\",\n        dropout: float = 0,\n        attn_dropout: float = 0,\n        attn_heads: int = 1,\n        bidirectional: bool = False,\n        layer_norm: bool = False,\n        highway_bias: float = -2,\n        rescale: bool = True,\n        enforce_sorted: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initializes the RNNEncoder object.\n        Parameters\n        ----------\n        input_size : int\n            The dimension the input data\n        hidden_size : int\n            The hidden dimension to encode the data in\n        n_layers : int, optional\n            The number of rnn layers, defaults to 1\n        rnn_type : str, optional\n           The type of rnn cell, one of: `lstm`, `gru`, `sru`\n           defaults to `lstm`\n        dropout : float, optional\n            Amount of dropout to use between RNN layers, defaults to 0\n        bidirectional : bool, optional\n            Set to use a bidrectional encoder, defaults to False\n        layer_norm : bool, optional\n            [SRU only] whether to use layer norm\n        highway_bias : float, optional\n            [SRU only] value to use for the highway bias\n        rescale : bool, optional\n            [SRU only] whether to use rescaling\n        enforce_sorted: bool\n            Whether rnn should enforce that sequences are ordered by\n            length. Requires True for ONNX support. Defaults to False.\n        kwargs\n            Additional parameters to be passed to SRU when building\n            the rnn.\n        Raises\n        ------\n        ValueError\n            The rnn type should be one of: `lstm`, `gru`, `sru`\n\n        \"\"\"\n        super().__init__()\n\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.enforce_sorted = enforce_sorted\n        self.output_size = 2 * hidden_size if bidirectional else hidden_size\n\n        if rnn_type in [\"lstm\", \"gru\"]:\n            rnn_fn = nn.LSTM if rnn_type == \"lstm\" else nn.GRU\n            self.rnn = rnn_fn(\n                input_size=input_size,\n                hidden_size=hidden_size,\n                num_layers=n_layers,\n                dropout=dropout,\n                bidirectional=bidirectional,\n            )\n        elif rnn_type == \"sru\":\n            from sru import SRU\n\n            try:\n                self.rnn = SRU(\n                    input_size,\n                    hidden_size,\n                    num_layers=n_layers,\n                    dropout=dropout,\n                    bidirectional=bidirectional,\n                    layer_norm=layer_norm,\n                    rescale=rescale,\n                    highway_bias=highway_bias,\n                    **kwargs,\n                )\n            except TypeError:\n                raise ValueError(f\"Unkown kwargs passed to SRU: {kwargs}\")\n        elif rnn_type == \"srupp\":\n            from sru import SRUpp\n\n            try:\n                self.rnn = SRUpp(\n                    input_size,\n                    hidden_size,\n                    hidden_size // 2,\n                    num_layers=n_layers,\n                    highway_bias=highway_bias,\n                    dropout=dropout,\n                    attn_dropout=attn_dropout,\n                    num_heads=attn_heads,\n                    layer_norm=layer_norm,\n                    attn_layer_norm=True,\n                    bidirectional=bidirectional,\n                    **kwargs,\n                )\n            except TypeError:\n                raise ValueError(f\"Unkown kwargs passed to SRU: {kwargs}\")\n        else:\n            raise ValueError(f\"Unkown rnn type: {rnn_type}, use of of: gru, sru, lstm\")\n\n    def forward(\n        self,\n        data: Tensor,\n        state: Optional[Tensor] = None,\n        padding_mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"Performs a forward pass through the network.\n        Parameters\n        ----------\n        data : Tensor\n            The input data, as a float tensor of shape [B x S x E]\n        state: Tensor\n            An optional previous state of shape [L x B x H]\n        padding_mask: Tensor, optional\n            The padding mask of shape [B x S], dtype should be bool\n        Returns\n        -------\n        Tensor\n            The encoded output, as a float tensor of shape [B x S x H]\n        Tensor\n            The encoded state, as a float tensor of shape [L x B x H]\n\n        \"\"\"\n        data = data.transpose(0, 1)\n        if padding_mask is not None:\n            padding_mask = padding_mask.transpose(0, 1)\n\n        if padding_mask is None:\n            # Default RNN behavior\n            output, state = self.rnn(data, state)\n        elif self.rnn_type == \"sru\":\n            # SRU takes a mask instead of PackedSequence objects\n            # ~ operator negates bool tensor in torch 1.3\n            output, state = self.rnn(data, state, mask_pad=(~padding_mask))\n        elif self.rnn_type == \"srupp\":\n            # SRU takes a mask instead of PackedSequence objects\n            # ~ operator negates bool tensor in torch 1.3\n            output, state, _ = self.rnn(data, state, mask_pad=(~padding_mask))\n        else:\n            # Deal with variable length sequences\n            lengths = padding_mask.long().sum(dim=0)\n            # Pass through the RNN\n            packed = nn.utils.rnn.pack_padded_sequence(\n                data, lengths.cpu(), enforce_sorted=self.enforce_sorted\n            )\n            output, state = self.rnn(packed, state)\n            output, _ = nn.utils.rnn.pad_packed_sequence(output, total_length=data.size(0))\n\n        # TODO investigate why PyTorch returns type Any for output\n        return output.transpose(0, 1).contiguous(), state  # type: ignore", "\n\nclass RNNEncoder(nn.Module):\n    \"\"\"Implements a multi-layer RNN.\n\n    This module can be used to create multi-layer RNN models, and\n    provides a way to reduce to output of the RNN to a single hidden\n    state by pooling the encoder states either by taking the maximum,\n    average, or by taking the last hidden state before padding.\n    Padding is dealt with by using torch's PackedSequence.\n\n    Attributes\n    ----------\n    rnn: nn.Module\n        The rnn submodule\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        hidden_size: int,\n        n_layers: int = 1,\n        rnn_type: str = \"lstm\",\n        dropout: float = 0,\n        attn_dropout: float = 0,\n        attn_heads: int = 1,\n        bidirectional: bool = False,\n        layer_norm: bool = False,\n        highway_bias: float = -2,\n        rescale: bool = True,\n        enforce_sorted: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Initializes the RNNEncoder object.\n        Parameters\n        ----------\n        input_size : int\n            The dimension the input data\n        hidden_size : int\n            The hidden dimension to encode the data in\n        n_layers : int, optional\n            The number of rnn layers, defaults to 1\n        rnn_type : str, optional\n           The type of rnn cell, one of: `lstm`, `gru`, `sru`\n           defaults to `lstm`\n        dropout : float, optional\n            Amount of dropout to use between RNN layers, defaults to 0\n        bidirectional : bool, optional\n            Set to use a bidrectional encoder, defaults to False\n        layer_norm : bool, optional\n            [SRU only] whether to use layer norm\n        highway_bias : float, optional\n            [SRU only] value to use for the highway bias\n        rescale : bool, optional\n            [SRU only] whether to use rescaling\n        enforce_sorted: bool\n            Whether rnn should enforce that sequences are ordered by\n            length. Requires True for ONNX support. Defaults to False.\n        kwargs\n            Additional parameters to be passed to SRU when building\n            the rnn.\n        Raises\n        ------\n        ValueError\n            The rnn type should be one of: `lstm`, `gru`, `sru`\n\n        \"\"\"\n        super().__init__()\n\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.enforce_sorted = enforce_sorted\n        self.output_size = 2 * hidden_size if bidirectional else hidden_size\n\n        if rnn_type in [\"lstm\", \"gru\"]:\n            rnn_fn = nn.LSTM if rnn_type == \"lstm\" else nn.GRU\n            self.rnn = rnn_fn(\n                input_size=input_size,\n                hidden_size=hidden_size,\n                num_layers=n_layers,\n                dropout=dropout,\n                bidirectional=bidirectional,\n            )\n        elif rnn_type == \"sru\":\n            from sru import SRU\n\n            try:\n                self.rnn = SRU(\n                    input_size,\n                    hidden_size,\n                    num_layers=n_layers,\n                    dropout=dropout,\n                    bidirectional=bidirectional,\n                    layer_norm=layer_norm,\n                    rescale=rescale,\n                    highway_bias=highway_bias,\n                    **kwargs,\n                )\n            except TypeError:\n                raise ValueError(f\"Unkown kwargs passed to SRU: {kwargs}\")\n        elif rnn_type == \"srupp\":\n            from sru import SRUpp\n\n            try:\n                self.rnn = SRUpp(\n                    input_size,\n                    hidden_size,\n                    hidden_size // 2,\n                    num_layers=n_layers,\n                    highway_bias=highway_bias,\n                    dropout=dropout,\n                    attn_dropout=attn_dropout,\n                    num_heads=attn_heads,\n                    layer_norm=layer_norm,\n                    attn_layer_norm=True,\n                    bidirectional=bidirectional,\n                    **kwargs,\n                )\n            except TypeError:\n                raise ValueError(f\"Unkown kwargs passed to SRU: {kwargs}\")\n        else:\n            raise ValueError(f\"Unkown rnn type: {rnn_type}, use of of: gru, sru, lstm\")\n\n    def forward(\n        self,\n        data: Tensor,\n        state: Optional[Tensor] = None,\n        padding_mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor]:\n        \"\"\"Performs a forward pass through the network.\n        Parameters\n        ----------\n        data : Tensor\n            The input data, as a float tensor of shape [B x S x E]\n        state: Tensor\n            An optional previous state of shape [L x B x H]\n        padding_mask: Tensor, optional\n            The padding mask of shape [B x S], dtype should be bool\n        Returns\n        -------\n        Tensor\n            The encoded output, as a float tensor of shape [B x S x H]\n        Tensor\n            The encoded state, as a float tensor of shape [L x B x H]\n\n        \"\"\"\n        data = data.transpose(0, 1)\n        if padding_mask is not None:\n            padding_mask = padding_mask.transpose(0, 1)\n\n        if padding_mask is None:\n            # Default RNN behavior\n            output, state = self.rnn(data, state)\n        elif self.rnn_type == \"sru\":\n            # SRU takes a mask instead of PackedSequence objects\n            # ~ operator negates bool tensor in torch 1.3\n            output, state = self.rnn(data, state, mask_pad=(~padding_mask))\n        elif self.rnn_type == \"srupp\":\n            # SRU takes a mask instead of PackedSequence objects\n            # ~ operator negates bool tensor in torch 1.3\n            output, state, _ = self.rnn(data, state, mask_pad=(~padding_mask))\n        else:\n            # Deal with variable length sequences\n            lengths = padding_mask.long().sum(dim=0)\n            # Pass through the RNN\n            packed = nn.utils.rnn.pack_padded_sequence(\n                data, lengths.cpu(), enforce_sorted=self.enforce_sorted\n            )\n            output, state = self.rnn(packed, state)\n            output, _ = nn.utils.rnn.pad_packed_sequence(output, total_length=data.size(0))\n\n        # TODO investigate why PyTorch returns type Any for output\n        return output.transpose(0, 1).contiguous(), state  # type: ignore", "\n\nclass SRUppModel(nn.Module):\n\n    def __init__(\n        self,\n        num_aa: int,\n        num_tokens: int,\n        n_layers: int = 1,\n        hidden_dim: int = 256,\n        dropout: float = 0,\n        ab_pad_id: int = 0,\n        virus_pad_id: int = 0,\n        use_srupp: bool = False,\n    ):\n        super().__init__()\n\n        # Virus encoder\n        self.hidden_dim = hidden_dim\n        self.seq_embedding = nn.Embedding(num_aa, hidden_dim // 4)\n\n        # Antibody encoder\n        rnn_type = \"srupp\" if use_srupp else \"sru\"\n        self.dropout = nn.Dropout(dropout)\n        self.rnn_ab = RNNEncoder(\n            hidden_dim // 4,\n            hidden_dim // 2,\n            n_layers=n_layers,\n            rnn_type=rnn_type,\n            bidirectional=True,\n            dropout=dropout,\n        )\n        self.ab_pad_id = ab_pad_id\n        self.virus_pad_id = virus_pad_id\n        self.num_tokens = num_tokens\n\n    @property\n    def output_dim(self):\n        return self.hidden_dim\n\n    def forward(self, ab_seq):\n        # Compute padding mask\n        padding_ab = ab_seq != self.ab_pad_id\n\n        # Compute token embeddings\n        ab_emb = self.dropout(self.seq_embedding(ab_seq))\n\n        # Pass through SRUpp layers\n        ab_encodings, _ = self.rnn_ab(ab_emb, padding_mask=padding_ab)\n        return ab_encodings", "\n\nclass MultiABOnlyCoronavirusModel(pl.LightningModule):\n\n    def __init__(\n        self,\n        num_aa: int,\n        num_tokens: int,\n        n_layers: int = 1,\n        hidden_dim: int = 128,\n        dropout: float = 0,\n        lr: float = 1e-3,\n        ab_pad_id: int = 0,\n        virus_pad_id: int = 0,\n        neut_lambda: float = 0.5,\n        use_srupp: bool = False,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.ab_pad_id = ab_pad_id\n        self.virus_pad_id = virus_pad_id\n        self.neut_lambda = neut_lambda\n        self.dropout = nn.Dropout(dropout)\n        self.encoder = SRUppModel(  # type: ignore\n            num_aa=num_aa,\n            num_tokens=num_tokens,\n            n_layers=n_layers,\n            hidden_dim=hidden_dim,\n            dropout=dropout,\n            ab_pad_id=ab_pad_id,\n            virus_pad_id=virus_pad_id,\n            use_srupp=use_srupp,\n        )\n        encoder_dim = self.encoder.output_dim\n        self.fc_neut = nn.Sequential(\n            nn.Linear(encoder_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(hidden_dim // 2, 2),\n        )\n        #self.neut_auc_sars2 = AUROCWithMask(\n        #    num_classes=2, average=None, compute_on_step=False\n        #)\n        #self.neut_auc_sars1 = AUROCWithMask(\n        #    num_classes=2, average=None, compute_on_step=False\n        #)\n\n    @classmethod\n    def add_extra_args(cls) -> Dict:\n        extra_args = {\n            \"num_aa\": len(AA_VOCAB),\n            \"num_tokens\": 1024,\n            \"ab_pad_id\": AA_VOCAB[\"#\"],\n            \"virus_pad_id\": AA_VOCAB[\"#\"],\n        }\n        return extra_args\n\n    def average(self, data, padding):\n        data = (data * padding.unsqueeze(2)).sum(dim=1)\n        padding_sum = padding.sum(dim=1)\n        padding_sum[padding_sum == 0] = 1.0\n        avg = data / padding_sum.unsqueeze(1)\n        return avg\n\n    def forward(self, ab_seq):\n        padding_mask_ab = (ab_seq != self.ab_pad_id).float()\n        ab_encodings = self.encoder(ab_seq)\n        output_encoding = self.average(ab_encodings, padding_mask_ab)\n        output_encoding = self.dropout(output_encoding)\n        neut_logits = self.fc_neut(output_encoding).squeeze(1)\n\n        return neut_logits\n\n    def configure_callbacks(self):\n        return [ModelCheckpoint(monitor=\"auc\", save_top_k=1, mode=\"max\")]\n\n    def compute_metrics(self, batch):\n        ab_seq = batch[\"ab\"]\n        neut_label = batch[\"neut_label\"]\n        neut_mask = batch[\"neut_mask\"]\n        neut_logits = self(ab_seq)\n        neut_loss = F.binary_cross_entropy_with_logits(\n            neut_logits, neut_label.float(), reduction=\"none\"\n        )\n        neut_mask_sum = neut_mask.sum()\n        neut_mask_sum = neut_mask_sum if neut_mask_sum > 0 else 1.0\n        neut_loss = (neut_loss * neut_mask).sum() / neut_mask_sum\n\n        # Final loss\n        loss = neut_loss\n\n        # Compute metrics (ignore neg label 0)\n        self.neut_auc_sars1(\n            torch.sigmoid(neut_logits[:, 0]),\n            neut_label[:, 0].long(),\n            neut_mask[:, 0].bool(),\n        )\n        self.neut_auc_sars2(\n            torch.sigmoid(neut_logits[:, 1]),\n            neut_label[:, 1].long(),\n            neut_mask[:, 1].bool(),\n        )\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_metrics(batch)\n        self.log(\"loss\", loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self.compute_metrics(batch)\n        self.log(\"val_loss\", loss, prog_bar=True)\n\n    def training_epoch_end(self, output):\n        try:\n            neut_auc_sars1 = self.neut_auc_sars1.compute()\n        except Exception:\n            neut_auc_sars1 = 0.5\n\n        try:\n            neut_auc_sars2 = self.neut_auc_sars2.compute()\n        except Exception:\n            neut_auc_sars2 = 0.5\n\n        self.log(\"train_auc_sars_cov_1\", neut_auc_sars1, prog_bar=False)\n        self.log(\"train_auc_sars_cov_2\", neut_auc_sars2, prog_bar=False)\n        self.neut_auc_sars1.reset()\n        self.neut_auc_sars2.reset()\n\n    def validation_epoch_end(self, output):\n        try:\n            neut_auc_sars1 = self.neut_auc_sars1.compute()\n        except Exception as e:\n            print(e)\n            neut_auc_sars1 = 0.5\n\n        try:\n            neut_auc_sars2 = self.neut_auc_sars2.compute()\n        except Exception:\n            neut_auc_sars2 = 0.5\n\n        self.log(\"auc\", (neut_auc_sars1 + neut_auc_sars2) / 2, prog_bar=True)\n        self.log(\"auc_sars_cov_1\", neut_auc_sars1, prog_bar=True)\n        self.log(\"auc_sars_cov_2\", neut_auc_sars2, prog_bar=True)\n        self.neut_auc_sars1.reset()\n        self.neut_auc_sars2.reset()\n\n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, output):\n        return self.validation_epoch_end(output)\n\n    def configure_optimizers(self):\n        return RAdam((p for p in self.parameters() if p.requires_grad), lr=self.lr)"]}
{"filename": "fairseq_models/__init__.py", "chunked_list": ["from .tasks.abbert_mlm_task import AntibodyMaskedLMTask\nfrom .tasks.abgen_task import AntibodyGenerationTask\nfrom .criterions.abbert_masked_lm import AntibodyMaskedLmLoss\nfrom .criterions.abgen_criterions import AntibodyGenerationLoss\nfrom .models.ab_roberta_base import AntibodyRobertaModel\n"]}
{"filename": "fairseq_models/data/abgen_dataset_noantigen.py", "chunked_list": ["import torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport json, copy\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom fairseq_models.modules.utils import full_square_dist\nfrom functools import lru_cache\nfrom pathlib import Path", "from functools import lru_cache\nfrom pathlib import Path\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    FairseqDataset\n)\n\nfrom .ab_dictionary import (", "\nfrom .ab_dictionary import (\n    ALPHABET,\n    ALPHABET_FULL,\n    RESTYPE_1to3,\n    ATOM_TYPES,\n    RES_ATOM14,\n    UNK_LIST\n)\n", ")\n\n\nclass AntibodyOnlyDataset(FairseqDataset):\n\n    def __init__(self, data_path, split, seq_vocab, tag_vocab, cdr_types, L_target, pad_idx, mask_idx, max_len):\n        if split not in ('train', 'valid', 'test'):\n            raise ValueError(f\"Unrecognized split: {split}. \"\n                             f\"Must be one of ['train', 'valid', 'test']\")\n\n        \n        data_path = Path(data_path)\n        data_file = f'{split}_data.jsonl'\n        jsonl_file = data_path / data_file\n\n        self.seq_vocab = seq_vocab\n        self.tag_vocab = tag_vocab\n        self.cdr_types = cdr_types\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n\n        self.cdr_types = cdr_types\n        self.data = []\n        with open(jsonl_file) as f:\n            all_lines = f.readlines()\n            for line in tqdm(all_lines):\n                entry = json.loads(line)\n                assert len(entry['antibody_coords']) == len(entry['antibody_seq'])\n                for cdr in cdr_types:\n                    if entry['antibody_cdr'].count(cdr) <= 4:\n                        continue\n                if entry['pdb'] in UNK_LIST:\n                    continue\n                if len(entry['antibody_seq']) > max_len:\n                    continue\n                if entry['antibody_cdr'][-1] != '0':\n                    # print('no fwr4')\n                    continue\n                \n                # zero struct and cdr3 only\n                # seq_len = len(entry['antibody_seq'])\n                # entry['antibody_coords'] = torch.zeros((seq_len, 4, 3))\n                entry['antibody_cdr'] = entry['antibody_cdr'].replace(\"1\", \"0\")\n                entry['antibody_cdr'] = entry['antibody_cdr'].replace(\"2\", \"0\")\n                # paratope region\n                surface = torch.tensor(\n                        [i for i,v in enumerate(entry['antibody_cdr']) if v in cdr_types]\n                )\n                entry['paratope_surface'] = surface\n                entry['cdr_string'] = entry['antibody_cdr']\n\n                l_coord, r_coord = torch.tensor(entry['antibody_coords'])[surface[0] - 1], torch.tensor(entry['antibody_coords'])[surface[-1] + 1]\n                n_span = len(surface) + 1\n                coord_offsets = (r_coord - l_coord).unsqueeze(0).expand(n_span - 1, 4, 3) \n                coord_offsets = torch.cumsum(coord_offsets, dim=0)\n                mask_cdr_coords = l_coord + coord_offsets / n_span # [cdr_len, 4, 3]\n                entry['mask_cdr_coords'] = mask_cdr_coords\n                \n                entry['paratope_seq'] = ''.join([entry['antibody_seq'][i] for i in surface.tolist()])\n                entry['paratope_coords'] = torch.tensor(entry['antibody_coords'])[surface]\n\n                if len(entry['paratope_coords']) > 4 and entry['antibody_cdr'].count('001') <= 1:\n                    # string to list\n                    entry['antibody_seq_str'] = entry['antibody_seq']\n                    entry['antibody_seq'] = torch.tensor([ALPHABET_FULL.index(a) for a in entry['antibody_seq']])\n                    entry['paratope_seq'] = torch.tensor([ALPHABET.index(a) for a in entry['paratope_seq']])\n                    entry['antibody_cdr'] = torch.tensor([self.tag_vocab.index(a) for a in entry['antibody_cdr']])\n                    entry['antibody_coords'] = torch.tensor(entry['antibody_coords'])\n\n                    # make masked dataset\n                    # only implement one cdr predicting\n                    for cdr in self.cdr_types:\n                        cdr_mask = entry['antibody_cdr'] == self.tag_vocab.index(cdr)\n                        entry['prefix_len'] = entry['cdr_string'].index(cdr)\n                    # print(cdr_mask)\n                    label_seq = torch.full_like(entry['antibody_seq'], self.pad_idx)\n                    label_seq[cdr_mask] = entry['antibody_seq'][cdr_mask]\n\n                    entry['label_antibody_seq'] = label_seq\n                    \n                    masked_seq = torch.full_like(entry['antibody_seq'], self.mask_idx)\n                    masked_seq[~cdr_mask] = entry['antibody_seq'][~cdr_mask]\n                    entry['masked_antibody_seq'] = masked_seq\n\n                    # from bert vocab to decoder vocab\n                    entry['antibody_seq'] = entry['antibody_seq'] - 3\n\n                    self.data.append(entry)\n\n        self.sizes = np.array([len(item['paratope_seq']) for item in self.data])\n        self.prefix_len = np.array([item['prefix_len'] for item in self.data])\n        \n\n    def __len__(self):\n        return len(self.data)\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        return item['masked_antibody_seq'], item['antibody_cdr'], item['label_antibody_seq'], \\\n               item['paratope_seq'], item['paratope_coords'], \\\n               item['paratope_surface'], item['mask_cdr_coords'], \\\n               item['antibody_coords'], item['antibody_seq'], item['cdr_string']\n\n\n\n    def collater(self, samples):\n        return self.collate_fn(samples)\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n    \n    def collate_fn(self, batch):\n        try:\n            masked_antibody_seq, antibody_cdr, label_antibody_seq, \\\n            paratope_seq, paratope_coords, \\\n            paratope_surface, mask_cdr_coords, \\\n            antibody_coords, antibody_seq, cdr_string \\\n            = tuple(zip(*batch))\n        except:\n            return None\n\n        # for bert\n        batched_seq = data_utils.collate_tokens(masked_antibody_seq, self.pad_idx, left_pad=False)\n        batched_tag = data_utils.collate_tokens(antibody_cdr, self.pad_idx, left_pad=False)\n        batched_label = data_utils.collate_tokens(label_antibody_seq, self.pad_idx, left_pad=False)\n\n        # for decoder \n        def featurize_paratope(seq_batch, coords_batch, mask_cdr_coords_batch):\n            X = pad_sequence(coords_batch, batch_first=True, padding_value=0)\n            S = pad_sequence(seq_batch, batch_first=True, padding_value=0)\n            X_init = torch.cat([i for i in mask_cdr_coords_batch])\n            cont_S = torch.cat([i for i in seq_batch])\n            return X, S, cont_S, X_init\n\n        def feature_framework(antibody_seq, antibody_coords, antibody_cdr):\n            X = pad_sequence(antibody_coords, batch_first=True, padding_value=0)\n            S = pad_sequence(antibody_seq, batch_first=True, padding_value=0)\n            antibody_cdr = list(antibody_cdr)\n            mask = S.bool().float()\n            return X, S, antibody_cdr, mask\n\n        paratope = featurize_paratope(paratope_seq, paratope_coords, mask_cdr_coords)\n        antibody = feature_framework(antibody_seq, antibody_coords, cdr_string)\n        return batched_seq, batched_tag, batched_label, paratope, None, antibody", "\n        \n\n"]}
{"filename": "fairseq_models/data/ab_dictionary.py", "chunked_list": ["from fairseq.data import Dictionary\n\nALPHABET_FOR_INIT = '#SGTYVALRDKQWINFEPCMH'\n\nclass AbgenDictionary(Dictionary):\n    def __init__(self):\n        self.symbols = [\n            '#', 'S', 'G', 'T', 'Y', 'V', 'A', 'L', 'R', 'D', 'K', 'Q', 'W', 'I', 'N', 'F', 'E', 'P', 'C', 'M', 'H'\n        ]\n        self.count = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        self.indices = {\n            '#': 0,  'S': 1, 'G': 2, 'T': 3, 'Y': 4, 'V': 5, 'A': 6, 'L': 7, 'R': 8, 'D': 9, 'K': 10, \n            'Q': 11, 'W': 12, 'I': 13, 'N': 14, 'F': 15, 'E': 16, 'P': 17, 'C': 18, 'M': 19, 'H': 20\n        }\n        self.nspecial = len(self.symbols) ", "\n\nclass AbbertDictionary(Dictionary):\n    def __init__(self):\n        bos=\"<s>\"\n        pad=\"<pad>\"\n        eos=\"</s>\"\n        unk=\"<unk>\"\n        self.bos_word, self.unk_word, self.pad_word, self.eos_word = bos, unk, pad, eos\n        self.symbols = [\n            '<s>', '<pad>', '</s>', '<unk>', 'S', 'G', 'T', 'Y', 'V', 'A', 'L', 'R', 'D', 'K', 'Q', 'W', 'I', 'N', 'F', 'E', 'P', 'C', 'M', 'H', \n            'X', '*', 'madeupword0000', 'madeupword0001', 'madeupword0002', 'madeupword0003', 'madeupword0004', 'madeupword0005'\n        ]\n        self.count = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        self.indices = {\n            '<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3, 'S': 4, 'G': 5, 'T': 6, 'Y': 7, 'V': 8, 'A': 9, 'L': 10, 'R': 11, 'D': 12, \n            'K': 13, 'Q': 14, 'W': 15, 'I': 16, 'N': 17, 'F': 18, 'E': 19, 'P': 20, 'C': 21, 'M': 22, 'H': 23, 'X': 24, '*': 25, \n            'madeupword0000': 26, 'madeupword0001': 27, 'madeupword0002': 28, 'madeupword0003': 29, 'madeupword0004': 30, 'madeupword0005': 31}\n        self.bos_index = self.indices[bos]\n        self.pad_index = self.indices[pad]\n        self.eos_index = self.indices[eos]\n        self.unk_index = self.indices[unk]\n\n        self.nspecial = len(self.symbols) ", "\nclass TagDictionary(Dictionary):\n    def __init__(self):\n        bos=\"<s>\"\n        pad=\"<pad>\"\n        eos=\"</s>\"\n        unk=\"<unk>\"\n        self.bos_word, self.unk_word, self.pad_word, self.eos_word = bos, unk, pad, eos\n        self.symbols = [\n            '<s>', '<pad>', '</s>', '<unk>', '0', '3', '1', '2'\n        ]\n        self.count = [1, 1, 1, 1, 0, 0, 0, 0]\n        self.indices = {\n            '<s>': 0, '<pad>': 1, '</s>': 2, '<unk>': 3, '0': 4, '3': 5, '1': 6, '2': 7\n        }\n        self.bos_index = self.indices[bos]\n        self.pad_index = self.indices[pad]\n        self.eos_index = self.indices[eos]\n        self.unk_index = self.indices[unk]\n\n        self.nspecial = len(self.symbols) ", "\n\nALPHABET = AbgenDictionary()\nALPHABET_FULL = AbbertDictionary()\nTAG_FULL = TagDictionary()\n\nRESTYPE_1to3 = {\n     \"A\": \"ALA\", \"R\": \"ARG\", \"N\": \"ASN\", \"D\": \"ASP\", \"C\": \"CYS\", \"Q\": \"GLN\",\"E\": \"GLU\", \"G\": \"GLY\", \"H\": \"HIS\", \"I\": \"ILE\", \"L\": \"LEU\", \"K\": \"LYS\", \"M\": \"MET\", \"F\": \"PHE\", \"P\": \"PRO\", \"S\": \"SER\", \"T\": \"THR\", \"W\": \"TRP\", \"Y\": \"TYR\", \"V\": \"VAL\",\n}\n", "}\n\n# ALPHABET = ['#', 'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\nATOM_TYPES = [\n    '', 'N', 'CA', 'C', 'O', 'CB', 'CG', 'CG1', 'CG2', 'OG', 'OG1', 'SG', 'CD',\n    'CD1', 'CD2', 'ND1', 'ND2', 'OD1', 'OD2', 'SD', 'CE', 'CE1', 'CE2', 'CE3',\n    'NE', 'NE1', 'NE2', 'OE1', 'OE2', 'CH2', 'NH1', 'NH2', 'OH', 'CZ', 'CZ2',\n    'CZ3', 'NZ', 'OXT'\n]\n", "]\n\nRES_ATOM14 = [\n    ['', '', '', '', '', '', '', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'OG', '', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', '', '', '', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'OG1', 'CG2', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ', 'OH', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG1', 'CG2', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', '', '', '', '', '', '', '', '', ''],", "    ['N', 'CA', 'C', 'O', 'CB', 'CG1', 'CG2', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', '', '', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD1', 'CD2', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', 'NE', 'CZ', 'NH1', 'NH2', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'OD1', 'OD2', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', 'CE', 'NZ', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', 'OE1', 'NE2', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD1', 'CD2', 'NE1', 'CE2', 'CE3', 'CZ2', 'CZ3', 'CH2'],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG1', 'CG2', 'CD1', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'OD1', 'ND2', '', '', '', '', '', ''],", "    ['N', 'CA', 'C', 'O', 'CB', 'CG1', 'CG2', 'CD1', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'OD1', 'ND2', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', 'OE1', 'OE2', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'CD', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'SG', '', '', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'SD', 'CE', '', '', '', '', '', ''],\n    ['N', 'CA', 'C', 'O', 'CB', 'CG', 'ND1', 'CD2', 'CE1', 'NE2', '', '', '', ''],\n]\n", "]\n\nUNK_LIST = [\n    '6u8k',\n    '6db8',\n    '5e08',\n    '5gkr',\n    '4xwo',\n    '4kzd',\n    '6x5n',", "    '4kzd',\n    '6x5n',\n    '6db9',\n    '1keg',\n    '2ok0',\n    '2fr4',\n    '6b3k',\n    '6b14',\n    '2r8s',\n    '1xf2',", "    '2r8s',\n    '1xf2',\n    '1i8m',\n    '6mwn',\n    '6x5m',\n    '1cbv',\n    '4kze',\n    '1ehl',\n    '3ivk',\n    '6u8d'", "    '3ivk',\n    '6u8d'\n]\n"]}
{"filename": "fairseq_models/data/abgen_dataset.py", "chunked_list": ["import torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport json, copy\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom fairseq_models.modules.utils import full_square_dist\nfrom functools import lru_cache\nfrom pathlib import Path", "from functools import lru_cache\nfrom pathlib import Path\n\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,\n    FairseqDataset\n)\n\nfrom .ab_dictionary import (", "\nfrom .ab_dictionary import (\n    ALPHABET,\n    ALPHABET_FULL,\n    RESTYPE_1to3,\n    ATOM_TYPES,\n    RES_ATOM14,\n    UNK_LIST\n)\n", ")\n\n\nclass AntibodyComplexDataset(FairseqDataset):\n\n    def __init__(self, data_path, split, seq_vocab, tag_vocab, cdr_types, L_target, pad_idx, mask_idx, max_len):\n        if split not in ('train', 'valid', 'test'):\n            raise ValueError(f\"Unrecognized split: {split}. \"\n                             f\"Must be one of ['train', 'valid', 'test']\")\n\n        \n        data_path = Path(data_path)\n        data_file = f'{split}_data.jsonl'\n        jsonl_file = data_path / data_file\n\n        self.seq_vocab = seq_vocab\n        self.tag_vocab = tag_vocab\n        self.cdr_types = cdr_types\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n\n        self.cdr_types = cdr_types\n        self.data = []\n        with open(jsonl_file) as f:\n            all_lines = f.readlines()\n            for line in tqdm(all_lines):\n                entry = json.loads(line)\n                assert len(entry['antibody_coords']) == len(entry['antibody_seq'])\n                assert len(entry['antigen_coords']) == len(entry['antigen_seq'])\n                for cdr in cdr_types:\n                    if entry['antibody_cdr'].count(cdr) <= 4:\n                        continue\n                if entry['pdb'] in UNK_LIST:\n                    continue\n                if len(entry['antibody_seq']) > max_len:\n                    continue\n                \n                # paratope region\n                surface = torch.tensor(\n                        [i for i,v in enumerate(entry['antibody_cdr']) if v in cdr_types]\n                )\n                entry['paratope_surface'] = surface\n                entry['cdr_string'] = entry['antibody_cdr']\n\n                l_coord, r_coord = torch.tensor(entry['antibody_coords'])[surface[0] - 1], torch.tensor(entry['antibody_coords'])[surface[-1] + 1]\n                n_span = len(surface) + 1\n                coord_offsets = (r_coord - l_coord).unsqueeze(0).expand(n_span - 1, 14, 3) \n                coord_offsets = torch.cumsum(coord_offsets, dim=0)\n                mask_cdr_coords = l_coord + coord_offsets / n_span # [cdr_len, 14, 3]\n                mask_cdr_coords[:, 4:] = mask_cdr_coords[:, 1].unsqueeze(1) # regularize side chain atoms\n                entry['mask_cdr_coords'] = mask_cdr_coords\n                \n                entry['paratope_seq'] = ''.join([entry['antibody_seq'][i] for i in surface.tolist()])\n                entry['paratope_coords'] = torch.tensor(entry['antibody_coords'])[surface]\n                entry['paratope_atypes'] = torch.tensor(\n                        [[ATOM_TYPES.index(a) for a in RES_ATOM14[ALPHABET.index(s)]] for s in entry['paratope_seq']]\n                )\n                if entry['paratope_coords'][0][0].norm(dim=-1) < 1e-6:\n                    # print('here')\n                    continue\n                mask = (entry['paratope_coords'].norm(dim=-1) > 1e-6).long()\n                entry['paratope_atypes'] *= mask\n\n                # Create epitope\n                entry['antigen_seq'] = entry['antigen_seq']\n                entry['antigen_coords'] = torch.tensor(entry['antigen_coords'])\n                entry['antigen_atypes'] = torch.tensor(\n                        [[ATOM_TYPES.index(a) for a in RES_ATOM14[ALPHABET.index(s)]] for s in entry['antigen_seq']]\n                )\n                mask = (entry['antigen_coords'].norm(dim=-1) > 1e-6).long()\n                entry['antigen_atypes'] *= mask\n\n                # Find epitope surface\n                dist, _ = full_square_dist(\n                        entry['antigen_coords'][None,...], \n                        entry['paratope_coords'][None,...], \n                        entry['antigen_atypes'][None,...], \n                        entry['paratope_atypes'][None,...], \n                        contact=True\n                )\n                K = min(len(dist[0]), L_target)\n                epitope = dist[0].amin(dim=-1).topk(k=K, largest=False).indices\n                entry['epitope_surface'] = torch.sort(epitope).values\n\n                if len(entry['paratope_coords']) > 4 and len(entry['antigen_coords']) > 4 and entry['antibody_cdr'].count('001') <= 1:\n                    # string to list\n                    entry['antibody_seq_str'] = entry['antibody_seq']\n                    entry['antigen_seq_str'] = entry['antigen_seq']\n                    entry['antibody_seq'] = torch.tensor([ALPHABET_FULL.index(a) for a in entry['antibody_seq']])\n                    entry['paratope_seq'] = torch.tensor([ALPHABET.index(a) for a in entry['paratope_seq']])\n                    entry['antigen_seq'] = torch.tensor([ALPHABET.index(a) for a in entry['antigen_seq']])\n                    entry['antibody_cdr'] = torch.tensor([self.tag_vocab.index(a) for a in entry['antibody_cdr']])\n                    entry['antibody_coords'] = torch.tensor(entry['antibody_coords'])\n\n                    # make masked dataset\n                    # only implement one cdr predicting\n                    for cdr in self.cdr_types:\n                        cdr_mask = entry['antibody_cdr'] == self.tag_vocab.index(cdr)\n                        entry['prefix_len'] = entry['cdr_string'].index(cdr)\n                    # print(cdr_mask)\n                    label_seq = torch.full_like(entry['antibody_seq'], self.pad_idx)\n                    label_seq[cdr_mask] = entry['antibody_seq'][cdr_mask]\n\n                    entry['label_antibody_seq'] = label_seq\n                    \n                    masked_seq = torch.full_like(entry['antibody_seq'], self.mask_idx)\n                    masked_seq[~cdr_mask] = entry['antibody_seq'][~cdr_mask]\n                    entry['masked_antibody_seq'] = masked_seq\n\n                    # from bert vocab to decoder vocab\n                    entry['antibody_seq'] = entry['antibody_seq'] - 3\n\n                    self.data.append(entry)\n\n        self.sizes = np.array([len(item['paratope_seq']) for item in self.data])\n        self.prefix_len = np.array([item['prefix_len'] for item in self.data])\n        \n\n    def __len__(self):\n        return len(self.data)\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        return item['masked_antibody_seq'], item['antibody_cdr'], item['label_antibody_seq'], \\\n               item['paratope_seq'], item['paratope_coords'], item['paratope_atypes'], \\\n               item['antigen_seq'], item['antigen_coords'], item['antigen_atypes'], \\\n               item['paratope_surface'], item['epitope_surface'], item['mask_cdr_coords'], \\\n               item['antibody_coords'], item['antibody_seq'], item['cdr_string']\n\n\n\n    def collater(self, samples):\n        return self.collate_fn(samples)\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n    \n    def collate_fn(self, batch):\n        try:\n            masked_antibody_seq, antibody_cdr, label_antibody_seq, \\\n            paratope_seq, paratope_coords, paratope_atypes, \\\n            antigen_seq, antigen_coords, antigen_atypes, \\\n            paratope_surface, epitope_surface, mask_cdr_coords, \\\n            antibody_coords, antibody_seq, cdr_string \\\n            = tuple(zip(*batch))\n        except:\n            return None\n\n        # for bert\n        batched_seq = data_utils.collate_tokens(masked_antibody_seq, self.pad_idx, left_pad=False)\n        batched_tag = data_utils.collate_tokens(antibody_cdr, self.pad_idx, left_pad=False)\n        batched_label = data_utils.collate_tokens(label_antibody_seq, self.pad_idx, left_pad=False)\n\n        # for decoder \n        def featurize_antigen(seq_batch, coords_batch, atypes_batch):\n            S = pad_sequence(seq_batch, batch_first=True, padding_value=0)\n            X = pad_sequence(coords_batch, batch_first=True, padding_value=0)\n            A = pad_sequence(atypes_batch, batch_first=True, padding_value=0)\n            return X, S, A\n\n        def featurize_paratope(seq_batch, coords_batch, mask_cdr_coords_batch):\n            X = pad_sequence(coords_batch, batch_first=True, padding_value=0)\n            S = pad_sequence(seq_batch, batch_first=True, padding_value=0)\n            X_init = torch.cat([i for i in mask_cdr_coords_batch])\n            cont_S = torch.cat([i for i in seq_batch])\n            return X, S, cont_S, X_init\n\n        def feature_framework(antibody_seq, antibody_coords, antibody_cdr):\n            X = pad_sequence(antibody_coords, batch_first=True, padding_value=0)\n            S = pad_sequence(antibody_seq, batch_first=True, padding_value=0)\n            antibody_cdr = list(antibody_cdr)\n            mask = S.bool().float()\n            return X, S, antibody_cdr, mask\n\n        def select_target(tgt_X, tgt_S, tgt_A, tgt_pos):\n            max_len = max([len(pos) for pos in tgt_pos])\n            xlist = [tgt_X[i, pos] for i,pos in enumerate(tgt_pos)]\n            slist = [tgt_S[i, pos] for i,pos in enumerate(tgt_pos)]\n            alist = [tgt_A[i, pos] for i,pos in enumerate(tgt_pos)]\n            tgt_X = [F.pad(x, (0,0,0,0,0,max_len-len(x))) for x in xlist]\n            tgt_S = [F.pad(s, (0,max_len-len(s))) for s in slist]\n            tgt_A = [F.pad(a, (0,0,0,max_len-len(a))) for a in alist]\n            return torch.stack(tgt_X, dim=0), torch.stack(tgt_S, dim=0), torch.stack(tgt_A, dim=0)\n\n        antigen = featurize_antigen(antigen_seq, antigen_coords, antigen_atypes)\n        epitope = select_target(antigen[0], antigen[1], antigen[2], [t_s for t_s in epitope_surface]) # select epitope from antigen\n        paratope = featurize_paratope(paratope_seq, paratope_coords, mask_cdr_coords)\n        antibody = feature_framework(antibody_seq, antibody_coords, cdr_string)\n        return batched_seq, batched_tag, batched_label, paratope, epitope, antibody", "\n        \n\n"]}
{"filename": "fairseq_models/models/ab_decoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom fairseq_models.data.abgen_dataset import ALPHABET, ATOM_TYPES, RES_ATOM14\nfrom fairseq_models.modules.hmpn_encoder import *\nfrom fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * ", "from fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * \nfrom fairseq_models.modules.utils import *\n\nclass FullshotRefineDecoder(ABModel):\n\n    def __init__(self, args):\n        super(FullshotRefineDecoder, self).__init__(args)\n        self.args = args\n        self.hierarchical = args.hierarchical\n        self.residue_atom14 = torch.tensor([\n                [ATOM_TYPES.index(a) for a in atoms] for atoms in RES_ATOM14\n        ]).cuda()\n\n        self.W_s = nn.Linear(args.hidden_size, len(ALPHABET))\n        self.W_t = nn.Linear(self.embedding.dim(), args.hidden_size)\n        self.U_i = nn.Linear(self.embedding.dim(), args.hidden_size)\n\n        # self.W_trans = nn.Linear(args.hidden_size, self.embedding.dim())\n        self.coord_loss = nn.SmoothL1Loss(reduction='sum')\n\n        if args.hierarchical:\n            self.struct_mpn = HierEGNNEncoder(args)\n            self.seq_mpn = HierEGNNEncoder(args, update_X=False, backbone_CA_only=False)\n        else:\n            self.struct_mpn = EGNNEncoder(args)\n            self.seq_mpn = EGNNEncoder(args, update_X=False)\n\n        self.framework_encoder = HierarchicalDecoder(args)\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def struct_loss(self, antibody_X, epitope_X, true_V, true_R, true_D, inter_D, true_C):\n        # dihedral loss\n        antibody_V = self.features._dihedrals(antibody_X)\n        vloss = self.mse_loss(antibody_V, true_V).sum(dim=-1)\n        # local loss\n        rdist = antibody_X.unsqueeze(2) - antibody_X.unsqueeze(3)\n        rdist = torch.sum(rdist ** 2, dim=-1)\n        rloss = self.huber_loss(rdist, true_R) + 10 * F.relu(1.5 - rdist)\n        # full loss\n        cdist, _ = full_square_dist(antibody_X, antibody_X, torch.ones_like(antibody_X)[..., 0], torch.ones_like(antibody_X)[..., 0])\n        closs = self.huber_loss(cdist, true_C) + 10 * F.relu(1.5 - cdist)\n        # alpha carbon\n        antibody_X, epitope_X = antibody_X[:, :, 1], epitope_X[:, :, 1]\n        # CDR self distance\n        dist = antibody_X.unsqueeze(1) - antibody_X.unsqueeze(2)\n        dist = torch.sum(dist ** 2, dim=-1)\n        dloss = self.huber_loss(dist, true_D) + 10 * F.relu(14.4 - dist)\n        # inter distance\n        idist = antibody_X.unsqueeze(2) - epitope_X.unsqueeze(1)\n        idist = torch.sum(idist ** 2, dim=-1)\n        iloss = self.huber_loss(idist, inter_D) + 10 * F.relu(14.4 - idist)\n        return dloss, vloss, rloss, iloss, closs\n\n    def forward(\n        self, init_prob, paratope, epitope, antibody,\n        pretrained_embedding=None, masked_tokens=None\n    ):\n        # return ReturnType(xloss=3.0, nll=2., bind_X=torch.ones(masked_tokens.sum().item(), 14, 3), handle=(None, None))\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        epitope_X, epitope_S, epitope_A = epitope\n        antibody_X, antibody_S, antibody_cdr, padding_mask = antibody\n\n\n        # Encode target\n        epitope_h = self.U_i(self.embedding(epitope_S))\n        epitope_V = self.features._dihedrals(epitope_X)\n        epitope_mask = epitope_A[:,:,1].clamp(max=1).float()\n        \n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, true_antibody_X, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, padding_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 14).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        dloss = vloss = rloss = iloss = sloss = closs = 0\n        for t in range(self.args.refine_iteration):\n            if t < self.args.refine_iteration - 1:\n                with torch.no_grad():\n                    # sequence update\n                    antibody_V = self.features._dihedrals(antibody_X.detach())\n                    complex_V = torch.cat([antibody_V, epitope_V], dim=1).detach()\n                    complex_X = torch.cat([antibody_X, epitope_X], dim=1).detach()\n                    complex_A = torch.cat([antibody_A, epitope_A], dim=1).detach()\n\n                    # sequence message passing\n                    complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n                    complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                    logits = self.W_s(complex_h[:, :antibody_N])\n                    logits = logits[paratope_mask]\n                    antibody_h_0 = antibody_h_0.clone()\n\n                    # calculate sequence loss\n                    snll = self.ce_loss(logits, flatten_cdr_S)\n                    sloss = sloss + torch.sum(snll)\n\n                    # update paratope embedding\n                    probs = F.softmax(logits, dim=-1)\n                    antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n                    # structrue message passing\n                    complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n                    complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                    antibody_X = antibody_X.clone()\n                    antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n\n                    ratio = (t + 1) / self.args.refine_iteration\n                    label_X = true_antibody_X * ratio + antibody_X * (1 - ratio)\n                \n                    true_V = self.features._dihedrals(label_X)\n                    true_R, rmask_2D = inner_square_dist(label_X, antibody_A.clamp(max=1).float())\n                    true_D, mask_2D = self_square_dist(label_X, paratope_mask)\n                    true_C, cmask_2D = full_square_dist(label_X, label_X, antibody_A, antibody_A)\n                    inter_D, imask_2D = cross_square_dist(label_X, epitope_X, paratope_mask, epitope_mask)\n\n                    dloss_t, vloss_t, rloss_t, iloss_t, closs_t = self.struct_loss(\n                            antibody_X, epitope_X, true_V, true_R, true_D, inter_D, true_C\n                    )\n                    vloss = vloss + vloss_t * paratope_mask\n                    dloss = dloss + dloss_t * mask_2D\n                    iloss = iloss + iloss_t * imask_2D\n                    rloss = rloss + rloss_t * rmask_2D\n                    closs = closs + closs_t * cmask_2D\n            else:\n                # sequence update\n                antibody_V = self.features._dihedrals(antibody_X.detach())\n                complex_V = torch.cat([antibody_V, epitope_V], dim=1).detach()\n                complex_X = torch.cat([antibody_X, epitope_X], dim=1).detach()\n                complex_A = torch.cat([antibody_A, epitope_A], dim=1).detach()\n\n                # sequence message passing\n                complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n                complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                logits = self.W_s(complex_h[:, :antibody_N])\n                logits = logits[paratope_mask]\n                antibody_h_0 = antibody_h_0.clone()\n\n                # calculate sequence loss\n                snll = self.ce_loss(logits, flatten_cdr_S)\n                sloss = sloss + torch.sum(snll)\n\n                # update paratope embedding\n                probs = F.softmax(logits, dim=-1)\n                antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n                # structrue message passing\n                complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n                complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                antibody_X = antibody_X.clone()\n                antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n\n                ratio = (t + 1) / self.args.refine_iteration\n                label_X = true_antibody_X * ratio + antibody_X * (1 - ratio)\n            \n                true_V = self.features._dihedrals(label_X)\n                true_R, rmask_2D = inner_square_dist(label_X, antibody_A.clamp(max=1).float())\n                true_D, mask_2D = self_square_dist(label_X, paratope_mask)\n                true_C, cmask_2D = full_square_dist(label_X, label_X, antibody_A, antibody_A)\n                inter_D, imask_2D = cross_square_dist(label_X, epitope_X, paratope_mask, epitope_mask)\n\n                dloss_t, vloss_t, rloss_t, iloss_t, closs_t = self.struct_loss(\n                        antibody_X, epitope_X, true_V, true_R, true_D, inter_D, true_C\n                )\n                vloss = vloss + vloss_t * paratope_mask\n                dloss = dloss + dloss_t * mask_2D\n                iloss = iloss + iloss_t * imask_2D\n                rloss = rloss + rloss_t * rmask_2D\n                closs = closs + closs_t * cmask_2D\n        \n\n        sloss = sloss / paratope_mask.sum() # / self.args.refine_iteration\n        dloss = torch.sum(dloss) / mask_2D.sum() \n        iloss = torch.sum(iloss) / imask_2D.sum() \n        vloss = torch.sum(vloss) / paratope_mask.sum() \n        # print('mask_2D', mask_2D.sum())\n        if self.hierarchical:\n            rloss = torch.sum(rloss) / rmask_2D.sum()\n            closs = torch.sum(closs) / cmask_2D.sum()\n        else:\n            rloss = torch.sum(rloss[:,:,:4,:4]) / rmask_2D[:,:,:4,:4].sum()\n            closs = 0\n\n\n        struct_loss = (dloss + iloss + vloss + rloss + closs) / paratope_mask.sum() # / self.args.refine_iteration\n        seq_loss = sloss\n        return ReturnType(xloss=struct_loss, nll=seq_loss, bind_X=antibody_X[paratope_mask].detach(), handle=(epitope_X, epitope_A))\n\n    def generate(\n        self, init_prob, paratope, epitope, antibody,\n        pretrained_embedding=None, masked_tokens=None, num_decode=1\n    ):\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        epitope_X, epitope_S, epitope_A = epitope\n        antibody_X, antibody_S, antibody_cdr, antibody_mask = antibody\n\n\n        # Encode target (assumes same target)\n        epitope_h = self.U_i(self.embedding(epitope_S))\n        epitope_V = self.features._dihedrals(epitope_X)\n        epitope_mask = epitope_A[:,:,1].clamp(max=1).float()\n\n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, _, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, antibody_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 14).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        for t in range(self.args.refine_iteration):\n            # sequence update\n            antibody_V = self.features._dihedrals(antibody_X.detach())\n            complex_V = torch.cat([antibody_V, epitope_V], dim=1).detach()\n            complex_X = torch.cat([antibody_X, epitope_X], dim=1).detach()\n            complex_A = torch.cat([antibody_A, epitope_A], dim=1).detach()\n\n            # sequence message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            logits = self.W_s(complex_h[:, :antibody_N])\n            logits = logits[paratope_mask]\n            antibody_h_0 = antibody_h_0.clone()\n\n            # update paratope embedding\n            probs = F.softmax(logits, dim=-1)\n            antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n            # structrue message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            antibody_X = antibody_X.clone()\n            antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n    \n        # sample new sequences\n        prob = F.softmax(logits.view(-1, len(ALPHABET)), dim=-1)\n        bind_I = torch.multinomial(prob, num_samples=num_decode, replacement=True)\n        bind_I = torch.transpose(bind_I, 0, 1)\n        repeat_logits = logits.view(-1, len(ALPHABET)).repeat(num_decode, 1)\n        snll = self.ce_loss(repeat_logits, bind_I.reshape(-1))\n        sloss = snll.view(num_decode, paratope_N).mean(dim=1)\n\n        S = bind_I.tolist()\n        S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(num_decode)]\n        ppl = torch.exp(sloss / paratope_N)\n        return ReturnType(handle=S, ppl=ppl, bind_X=antibody_X[paratope_mask].detach())", ""]}
{"filename": "fairseq_models/models/ab_roberta_base.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nRoBERTa: A Robustly Optimized BERT Pretraining Approach.\n\"\"\"\n\nimport logging\n", "import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.models import (\n    FairseqEncoder,\n    FairseqEncoderModel,\n    register_model,", "    FairseqEncoderModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.modules import LayerNorm\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom fairseq.modules.transformer_sentence_encoder import init_bert_params\n\nfrom fairseq_models.modules.ab_transformer_sentence_encoder import AntibodyTransformerSentenceEncoder\nfrom fairseq_models.models.ab_decoder import FullshotRefineDecoder", "from fairseq_models.modules.ab_transformer_sentence_encoder import AntibodyTransformerSentenceEncoder\nfrom fairseq_models.models.ab_decoder import FullshotRefineDecoder\nfrom fairseq_models.models.ab_decoder_ablation import FullshotRefineDecoderStack, BertonlyDecoder\nfrom fairseq_models.models.ab_decoder_noantigen import NoAntigenFullshotRefineDecoder\nfrom fairseq_models.data.ab_dictionary import ALPHABET, ALPHABET_FULL, TAG_FULL\n\nfrom fairseq.data import Dictionary\n\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass PrefixEncoder(torch.nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.prefix_projection = args.prefix_projection\n        prefix_hidden_size = args.encoder_embed_dim\n        if self.prefix_projection:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(args.pre_seq_len, args.encoder_embed_dim)\n            self.trans = torch.nn.Sequential(\n                torch.nn.Linear(args.encoder_embed_dim, prefix_hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(prefix_hidden_size, args.encoder_layers * 2 * args.encoder_embed_dim)\n            )\n        else:\n            self.embedding = torch.nn.Embedding(args.pre_seq_len, args.encoder_layers * 2 * args.encoder_embed_dim)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.trans(prefix_tokens)\n            # print(self.trans[0].weight)\n            # exit(0)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values", "\n\n@register_model(\"antibody_roberta\")\nclass AntibodyRobertaModel(FairseqEncoderModel):\n    def __init__(self, args, encoder, dictionary, inference=False):\n        super().__init__(encoder)\n        self.args = args\n        self.dictionary = dictionary\n        self.inference=inference\n        \n        if self.args.finetune:\n            if self.args.finetune_bert_scheme == 'prefix_tuning' and self.args.pre_seq_len > 0:\n                for param in self.encoder.parameters():\n                    param.requires_grad = False\n                self.pre_seq_len = args.pre_seq_len\n                self.n_layer = args.encoder_layers\n                self.n_head = args.encoder_attention_heads\n                self.n_embd = args.encoder_embed_dim // args.encoder_attention_heads\n                self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n                self.prefix_encoder = PrefixEncoder(args)\n                # print(self.n_layer)\n            elif self.args.finetune_bert_scheme == 'fixed':\n                for param in self.encoder.parameters():\n                    param.requires_grad = False\n\n            if self.args.bertonly:\n                self.structure_decoder = BertonlyDecoder(self.args)\n            elif self.args.noantigen:\n                self.structure_decoder = NoAntigenFullshotRefineDecoder(self.args)\n            elif self.args.stack:\n                self.structure_decoder = FullshotRefineDecoderStack(self.args)\n            else:\n                self.structure_decoder = FullshotRefineDecoder(self.args)\n    \n        # We follow BERT's random weight initialization\n        self.apply(init_bert_params)\n\n\n\n\n    def print_parameter_number(self, net):\n        net_name = type(net).__name__\n        total_num = sum(p.numel() for p in net.parameters())\n        trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n        print(f'{net_name} total parameters:{total_num}, trainable: {trainable_num}')\n\n    @staticmethod\n    def add_args(parser):\n        \"\"\"Add model-specific arguments to the parser.\"\"\"\n        parser.add_argument(\n            \"--encoder-layers\", type=int, metavar=\"L\", help=\"num encoder layers\"\n        )\n        parser.add_argument(\n            \"--encoder-embed-dim\",\n            type=int,\n            metavar=\"H\",\n            help=\"encoder embedding dimension\",\n        )\n        parser.add_argument(\n            \"--encoder-ffn-embed-dim\",\n            type=int,\n            metavar=\"F\",\n            help=\"encoder embedding dimension for FFN\",\n        )\n        parser.add_argument(\n            \"--encoder-attention-heads\",\n            type=int,\n            metavar=\"A\",\n            help=\"num encoder attention heads\",\n        )\n        parser.add_argument(\n            \"--activation-fn\",\n            choices=utils.get_available_activation_fns(),\n            help=\"activation function to use\",\n        )\n        parser.add_argument(\n            \"--pooler-activation-fn\",\n            choices=utils.get_available_activation_fns(),\n            help=\"activation function to use for pooler layer\",\n        )\n        parser.add_argument(\n            \"--encoder-normalize-before\",\n            action=\"store_true\",\n            help=\"apply layernorm before each encoder block\",\n        )\n        parser.add_argument(\n            \"--dropout\", type=float, metavar=\"D\", help=\"dropout probability\"\n        )\n        parser.add_argument(\n            \"--attention-dropout\",\n            type=float,\n            metavar=\"D\",\n            help=\"dropout probability for attention weights\",\n        )\n        parser.add_argument(\n            \"--activation-dropout\",\n            type=float,\n            metavar=\"D\",\n            help=\"dropout probability after activation in FFN\",\n        )\n        parser.add_argument(\n            \"--pooler-dropout\",\n            type=float,\n            metavar=\"D\",\n            help=\"dropout probability in the masked_lm pooler layers\",\n        )\n        parser.add_argument(\n            \"--max-positions\", type=int, help=\"number of positional embeddings to learn\"\n        )\n        parser.add_argument(\n            \"--load-checkpoint-heads\",\n            action=\"store_true\",\n            help=\"(re-)register and load heads when loading checkpoints\",\n        )\n        # args for \"Reducing Transformer Depth on Demand with Structured Dropout\" (Fan et al., 2019)\n        parser.add_argument(\n            \"--encoder-layerdrop\",\n            type=float,\n            metavar=\"D\",\n            default=0,\n            help=\"LayerDrop probability for encoder\",\n        )\n        parser.add_argument(\n            \"--encoder-layers-to-keep\",\n            default=None,\n            help=\"which layers to *keep* when pruning as a comma-separated list\",\n        )\n        # args for Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)\n        parser.add_argument(\n            \"--quant-noise-pq\",\n            type=float,\n            metavar=\"D\",\n            default=0,\n            help=\"iterative PQ quantization noise at training time\",\n        )\n        parser.add_argument(\n            \"--quant-noise-pq-block-size\",\n            type=int,\n            metavar=\"D\",\n            default=8,\n            help=\"block size of quantization noise at training time\",\n        )\n        parser.add_argument(\n            \"--quant-noise-scalar\",\n            type=float,\n            metavar=\"D\",\n            default=0,\n            help=\"scalar quantization noise and scalar quantization at training time\",\n        )\n        parser.add_argument(\n            \"--untie-weights-roberta\",\n            action=\"store_true\",\n            help=\"Untie weights between embeddings and classifiers in RoBERTa\",\n        )\n        parser.add_argument(\n            \"--spectral-norm-classification-head\",\n            action=\"store_true\",\n            default=False,\n            help=\"Apply spectral normalization on the classification head\",\n        )\n        # args for Hierarchical Equivariant Refinement Network (HERN)\n        parser.add_argument('--hidden_size', type=int, default=256)\n        parser.add_argument('--k_neighbors', type=int, default=9)\n        parser.add_argument('--depth', type=int, default=4)\n        parser.add_argument('--clash_step', type=int, default=10)\n        parser.add_argument('--num_rbf', type=int, default=16)\n        parser.add_argument('--hierarchical', type=bool, default=True)\n        # new addedargs\n        parser.add_argument('--finetune-bert-scheme', type=str, default='fixed', choices=['prefix_tuning', 'fixed', 'all_tuning'])\n        parser.add_argument('--pre-seq-len', type=int, default=1)\n        parser.add_argument('--prefix-projection', type=bool, default=True)\n\n        parser.add_argument('--refine-iteration', type=int, default=5)\n\n        parser.add_argument('--block_size', type=int, default=8)\n\n        parser.add_argument('--use-no-pretrain', type=bool, default=False)\n\n        # finetune or pretrain\n        parser.add_argument('--finetune', action='store_true', default=False)\n        parser.add_argument('--stack', action='store_true', default=False)\n        parser.add_argument('--bertonly', action='store_true', default=False)\n\n    @classmethod\n    def build_model(cls, args, task, inference=False):\n        \"\"\"Build a new model instance.\"\"\"\n\n        # make sure all arguments are present\n        base_architecture(args)\n\n        if not hasattr(args, \"max_positions\"):\n            args.max_positions = args.tokens_per_sample\n\n        encoder = RobertaEncoder(args, task.source_dictionary, task.tag_source_dictionary)\n        return cls(args, encoder, task.source_dictionary, inference)\n\n    def get_prompt(self, batch_size, data_device):\n        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(data_device)\n        past_key_values = self.prefix_encoder(prefix_tokens)\n        past_key_values = past_key_values.view(\n            batch_size,\n            self.pre_seq_len,\n            self.n_layer * 2, \n            self.n_head,\n            self.n_embd\n        )\n        # past_key_values = self.dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2) # (key, value) n_layer x batch_size x n_head x pre_seq_len x n_embd\n        # n_layer * [2, bsz, head, len, dim//head]\n        return past_key_values\n\n\n    def forward(\n        self,\n        src_tokens,\n        tag_tokens,\n        paratope=None,\n        epitope=None,\n        antibody=None,\n        features_only=False, \n        return_all_hiddens=False,\n        masked_tokens=None,\n        num_decode=1,\n        **kwargs\n    ):\n        if self.args.finetune:\n            batch_size = src_tokens.shape[0]\n            if self.args.finetune_bert_scheme == 'prefix_tuning':\n                past_key_values = self.get_prompt(batch_size=batch_size, data_device=src_tokens.device)\n            else:\n                past_key_values = None\n            enc_out, pt_emb, extra = self.encoder(        \n                src_tokens=src_tokens,\n                tag_tokens=tag_tokens,\n                features_only=features_only,\n                return_all_hiddens=return_all_hiddens,\n                past_key_values=past_key_values, \n                masked_tokens=masked_tokens,\n                **kwargs\n            )\n            # for ablation\n            # extra = None\n            # enc_out = torch.zeros((masked_tokens.sum(), 21), device=masked_tokens.device)\n            # pt_emb = torch.zeros((masked_tokens.sum(), 768), device=masked_tokens.device)\n            if self.inference:\n                decoder_out = self.structure_decoder.generate(\n                    enc_out, paratope, epitope, antibody, pt_emb, masked_tokens, num_decode\n                )\n            else:\n                decoder_out = self.structure_decoder( # feed into the generation model\n                    enc_out, paratope, epitope, antibody, pt_emb, masked_tokens\n                )\n            \n            return decoder_out, enc_out, extra\n        \n        else:\n            x, _, extra = self.encoder(src_tokens, tag_tokens, features_only, return_all_hiddens, masked_tokens=masked_tokens, **kwargs)\n            return x, extra\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n        logits = net_output[0].float()\n        if log_probs:\n            return F.log_softmax(logits, dim=-1)\n        else:\n            return F.softmax(logits, dim=-1)\n    \n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + '.' if name != \"\" else \"\"\n\n        super().upgrade_state_dict_named(state_dict, name)\n\n        # Copy any newly-added classification heads into the state dict\n        # with their current weights.\n        if hasattr(self, \"structure_decoder\"):\n            cur_state = self.structure_decoder.state_dict()\n            for k, v in cur_state.items():\n                if prefix + \"structure_decoder.\" + k not in state_dict:\n                    logger.info(\"Overwriting \" + prefix + \"structure_decoder.\" + k)\n                    state_dict[prefix + \"structure_decoder.\" + k] = v\n            \n        if self.args.finetune_bert_scheme == 'prefix_tuning' and self.args.pre_seq_len > 0:\n            cur_state = self.prefix_encoder.state_dict()\n            for k, v in cur_state.items():\n                if prefix + \"prefix_encoder.\" + k not in state_dict:\n                    logger.info(\"Overwriting \" + prefix + \"prefix_encoder.\" + k)\n                    state_dict[prefix + \"prefix_encoder.\" + k] = v\n\n    @property\n    def supported_targets(self):\n        return {\"self\"}\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        inference=False,\n        fix_bert_param=False,\n        **kwargs\n    ):\n        from fairseq.checkpoint_utils import load_checkpoint_to_cpu\n        import os\n        \n        state = load_checkpoint_to_cpu(model_name_or_path, arg_overrides=None)\n        \n        args = state[\"args\"]\n        from fairseq import models, quantization_utils\n\n        class TaskConfig:\n            def __init__(self):\n                self.source_dictionary = ALPHABET_FULL\n                self.mask_idx = self.source_dictionary.add_symbol(\"<mask>\")\n                self.tag_source_dictionary =  TAG_FULL\n        task = TaskConfig()\n        model = cls.build_model(args, task, inference)\n        \n        # model.add_state_dict_named(state['model'], \"\")\n        model.load_state_dict(state[\"model\"], strict=True, args=args)\n        \n        from itertools import chain\n        from fairseq.optim.adam import FairseqAdam\n        params = list(\n            filter(\n                lambda p: p.requires_grad,\n                chain(model.parameters()),\n            )\n        )\n        optimizer = FairseqAdam(args, params)\n        \n        last_optim_state = state[\"last_optimizer_state\"]\n        optimizer.load_state_dict(last_optim_state)\n        if fix_bert_param:\n            for param in model.prefix_encoder.parameters():\n                param.requires_grad = False\n\n        cls.upgrade_args(args)\n        \n        logger.info(args)\n        return HubInterface(args, model, task, optimizer)", "\nclass HubInterface(nn.Module):\n    \"\"\"A simple PyTorch Hub interface to RoBERTa.\n\n    Usage: https://github.com/pytorch/fairseq/tree/master/examples/roberta\n    \"\"\"\n\n    def __init__(self, args, model, task, optimizer):\n        super().__init__()\n        self.args = args\n        self.model = model\n        self.task = task\n        self.optimizer = optimizer\n\n\n        # this is useful for determining the device\n        self.register_buffer(\"_float_tensor\", torch.tensor([0], dtype=torch.float))\n\n    @property\n    def device(self):\n        return self._float_tensor.device", "\n\n\nclass RobertaLMHead(nn.Module):\n    \"\"\"Head for masked language modeling.\"\"\"\n\n    def __init__(self, embed_dim, output_dim, activation_fn, weight=None, finetune=False):\n        super().__init__()\n        self.dense = nn.Linear(embed_dim, embed_dim)\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.layer_norm = LayerNorm(embed_dim)\n        self.finetune = finetune\n\n\n        if weight is None:\n            weight = nn.Linear(embed_dim, output_dim, bias=False).weight\n        self.weight = weight\n        self.bias = nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, features, masked_tokens=None, **kwargs):\n        # Only project the masked tokens while training,\n        # saves both memory and computation\n        if masked_tokens is not None:\n            features = features[masked_tokens, :]\n\n        x = self.dense(features)\n        x = self.activation_fn(x)\n        x = self.layer_norm(x)\n        # project back to size of vocabulary with bias\n        x = F.linear(x, self.weight) + self.bias\n        \n        if self.finetune:\n            x = torch.cat((torch.zeros((x.shape[0], 1), device=x.device), x[:, 4:24]), dim=1)\n        \n        return x", "\n\nclass RobertaEncoder(FairseqEncoder):\n    \"\"\"RoBERTa encoder.\"\"\"\n\n    def __init__(self, args, dictionary, tag_dict):\n        super().__init__(dictionary)\n        self.args = args\n\n        tt = torch.tensor([i for i in range(len(dictionary))])\n        print('dictionary string: ', dictionary.string(tt))\n\n        if args.encoder_layers_to_keep:\n            args.encoder_layers = len(args.encoder_layers_to_keep.split(\",\"))\n\n        self.sentence_encoder = AntibodyTransformerSentenceEncoder(\n            padding_idx=dictionary.pad(),\n            vocab_size=len(dictionary),\n            num_encoder_layers=args.encoder_layers,\n            embedding_dim=args.encoder_embed_dim,\n            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n            num_attention_heads=args.encoder_attention_heads,\n            dropout=args.dropout,\n            attention_dropout=args.attention_dropout,\n            activation_dropout=args.activation_dropout,\n            layerdrop=args.encoder_layerdrop,\n            max_seq_len=args.max_positions,\n            segments_vocab=tag_dict,\n            encoder_normalize_before=True,\n            apply_bert_init=True,\n            activation_fn=args.activation_fn,\n            q_noise=args.quant_noise_pq,\n            qn_block_size=args.quant_noise_pq_block_size,\n        )\n        args.untie_weights_roberta = getattr(args, \"untie_weights_roberta\", False)\n\n        self.lm_head = RobertaLMHead(\n            embed_dim=args.encoder_embed_dim,\n            output_dim=len(dictionary),\n            activation_fn=args.activation_fn,\n            weight=(\n                self.sentence_encoder.embed_tokens.weight\n                if not args.untie_weights_roberta\n                else None\n            ),\n            finetune=args.finetune\n        )\n\n    def forward(\n        self,\n        src_tokens,\n        tag_tokens,\n        features_only=False,\n        return_all_hiddens=False,\n        masked_tokens=None,\n        past_key_values=None,\n        **unused\n    ):\n        \"\"\"\n        Args:\n            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`\n            features_only (bool, optional): skip LM head and just return\n                features. If True, the output will be of shape\n                `(batch, src_len, embed_dim)`.\n            return_all_hiddens (bool, optional): also return all of the\n                intermediate hidden states (default: False).\n\n        Returns:\n            tuple:\n                - the LM output of shape `(batch, src_len, vocab)`\n                - a dictionary of additional data, where 'inner_states'\n                  is a list of hidden states. Note that the hidden\n                  states have shape `(src_len, batch, vocab)`.\n        \"\"\"\n        x, extra = self.extract_features(\n            src_tokens, tag_tokens, return_all_hiddens=return_all_hiddens, past_key_values=past_key_values,\n        )\n\n        if self.args.finetune:\n            pt_emb = x.clone().detach()\n            if masked_tokens is not None:\n                pt_emb = pt_emb[masked_tokens, :]\n        else:\n            pt_emb = None\n        \n        if not features_only:\n            x = self.output_layer(x, masked_tokens=masked_tokens)\n\n        return x, pt_emb, extra\n\n    def extract_features(self, src_tokens, tag_tokens, return_all_hiddens=False, past_key_values=None, **kwargs):\n        inner_states, _ = self.sentence_encoder(\n            src_tokens,\n            segment_labels=tag_tokens,\n            last_state_only=not return_all_hiddens,\n            token_embeddings=kwargs.get(\"token_embeddings\", None),\n            past_key_values=past_key_values,\n        )\n        features = inner_states[-1].transpose(0, 1)  # T x B x C -> B x T x C\n        return features, {\"inner_states\": inner_states if return_all_hiddens else None}\n\n    def output_layer(self, features, masked_tokens=None, **unused):\n        return self.lm_head(features, masked_tokens)\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the encoder.\"\"\"\n        return self.args.max_positions", "\n\n@register_model_architecture(\"antibody_roberta\", \"antibody_roberta\")\ndef base_architecture(args):\n    args.encoder_layers = getattr(args, \"encoder_layers\", 12)\n    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 768)\n    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 3072)\n    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 12)\n\n    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n    args.pooler_activation_fn = getattr(args, \"pooler_activation_fn\", \"tanh\")\n\n    args.dropout = getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n    args.activation_dropout = getattr(args, \"activation_dropout\", 0.0)\n    args.pooler_dropout = getattr(args, \"pooler_dropout\", 0.0)\n    args.encoder_layers_to_keep = getattr(args, \"encoder_layers_to_keep\", None)\n    args.encoder_layerdrop = getattr(args, \"encoder_layerdrop\", 0.0)\n    args.encoder_layerdrop = getattr(args, \"encoder_layerdrop\", 0.0)\n    args.spectral_norm_classification_head = getattr(\n        args, \"spectral_nrom_classification_head\", False\n    )", "\n\n@register_model_architecture(\"antibody_roberta\", \"antibody_roberta_base\")\ndef roberta_base_architecture(args):\n    base_architecture(args)\n\n"]}
{"filename": "fairseq_models/models/ab_decoder_ablation.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom fairseq_models.data.abgen_dataset import ALPHABET, ATOM_TYPES, RES_ATOM14\nfrom fairseq_models.modules.hmpn_encoder import *\nfrom fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * ", "from fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * \nfrom fairseq_models.modules.utils import *\n\nclass BertonlyDecoder(nn.Module):\n    def __init__(self, args):\n        super(BertonlyDecoder, self).__init__()\n\n        self.args = args\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n    \n    def forward(self, enc_out, paratope, epitope, antibody, pt_emb, masked_tokens):\n        bind_X, _, _, _ = paratope\n        bind_X = torch.zeros((bind_X.size(0), bind_X.size(1), 4, 3), device=enc_out.device)\n        \n        return ReturnType(xloss=0., nll=0., bind_X=bind_X)\n    \n    def generate(self, enc_out, paratope, epitope, antibody, pt_emb, masked_tokens, num_decode):\n        \n        B, paratope_N = paratope[1].size(0), paratope[1].size(1)\n        prob = F.softmax(enc_out.view(-1, len(ALPHABET)), dim=-1)\n        bind_I = torch.multinomial(prob, num_samples=1).squeeze(-1)\n        snll = self.ce_loss(enc_out.view(-1, len(ALPHABET)), bind_I)\n        sloss = snll.view(B, paratope_N).mean(dim=1)\n\n        S = bind_I.view(B, paratope_N).tolist()\n        S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(B)]\n        ppl = torch.exp(sloss / paratope_N)\n        return ReturnType(handle=S, ppl=ppl, bind_X=None)", "        # B, paratope_N = paratope[1].size(0), paratope[1].size(1)\n        # prob = F.softmax(enc_out.view(-1, len(ALPHABET)), dim=-1)\n        # bind_I = torch.multinomial(prob, num_samples=num_decode, replacement=True)\n        # bind_I = torch.transpose(bind_I, 0, 1)\n        # repeat_logits = enc_out.view(-1, len(ALPHABET)).repeat(num_decode, 1)\n        # snll = self.ce_loss(repeat_logits, bind_I.reshape(-1))\n        # sloss = snll.view(num_decode, paratope_N).mean(dim=1)\n\n        # S = bind_I.tolist()\n        # S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(num_decode)]", "        # S = bind_I.tolist()\n        # S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(num_decode)]\n        # ppl = torch.exp(sloss / paratope_N)\n        # return ReturnType(handle=S, ppl=ppl, bind_X=None)\n\n        # B, N = paratope[1].size(0), paratope[1].size(1)\n        # bind_I = torch.zeros(B* N).cuda().long()\n        # prob = F.softmax(enc_out.view(-1, len(ALPHABET)), dim=-1)\n\n", "\n\n        # bind_I = torch.multinomial(prob, num_samples=1).squeeze(-1)\n        \n        # sloss = self.ce_loss(enc_out.view(-1, len(ALPHABET)), bind_I)\n        # S = bind_I.view(B, N).tolist()\n        # S = [''.join([ALPHABET[S[i][j]] for j in range(N)]) for i in range(B)]\n        # ppl = torch.exp(sloss / N)\n        # ReturnType(handle=S, ppl=ppl, bind_X=None)\n", "        # ReturnType(handle=S, ppl=ppl, bind_X=None)\n\n\n\nclass FullshotRefineDecoderStack(ABModel):\n\n    def __init__(self, args):\n        super(FullshotRefineDecoderStack, self).__init__(args)\n        self.args = args\n        self.hierarchical = args.hierarchical\n        self.residue_atom14 = torch.tensor([\n                [ATOM_TYPES.index(a) for a in atoms] for atoms in RES_ATOM14\n        ]).cuda()\n\n        self.W_s = nn.Linear(args.hidden_size, len(ALPHABET))\n        self.W_t = nn.Linear(self.embedding.dim(), args.hidden_size)\n        self.U_i = nn.Linear(self.embedding.dim(), args.hidden_size)\n\n        # self.W_trans = nn.Linear(args.hidden_size, self.embedding.dim())\n        self.coord_loss = nn.SmoothL1Loss(reduction='sum')\n\n        if args.hierarchical:\n            self.struct_mpn = HierEGNNEncoder(args)\n            self.seq_mpn = HierEGNNEncoder(args, update_X=False, backbone_CA_only=False)\n        else:\n            self.struct_mpn = EGNNEncoder(args)\n            self.seq_mpn = EGNNEncoder(args, update_X=False)\n\n        self.framework_encoder = HierarchicalDecoder(args)\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def struct_loss(self, antibody_X, epitope_X, true_V, true_R, true_D, inter_D, true_C):\n        # dihedral loss\n        antibody_V = self.features._dihedrals(antibody_X)\n        vloss = self.mse_loss(antibody_V, true_V).sum(dim=-1)\n        # local loss\n        rdist = antibody_X.unsqueeze(2) - antibody_X.unsqueeze(3)\n        rdist = torch.sum(rdist ** 2, dim=-1)\n        rloss = self.huber_loss(rdist, true_R) + 10 * F.relu(1.5 - rdist)\n        # full loss\n        cdist, _ = full_square_dist(antibody_X, antibody_X, torch.ones_like(antibody_X)[..., 0], torch.ones_like(antibody_X)[..., 0])\n        closs = self.huber_loss(cdist, true_C) + 10 * F.relu(1.5 - cdist)\n        # alpha carbon\n        antibody_X, epitope_X = antibody_X[:, :, 1], epitope_X[:, :, 1]\n        # CDR self distance\n        dist = antibody_X.unsqueeze(1) - antibody_X.unsqueeze(2)\n        dist = torch.sum(dist ** 2, dim=-1)\n        dloss = self.huber_loss(dist, true_D) + 10 * F.relu(14.4 - dist)\n        # inter distance\n        idist = antibody_X.unsqueeze(2) - epitope_X.unsqueeze(1)\n        idist = torch.sum(idist ** 2, dim=-1)\n        iloss = self.huber_loss(idist, inter_D) + 10 * F.relu(14.4 - idist)\n        return dloss, vloss, rloss, iloss, closs\n\n    def forward(\n        self, init_prob, paratope, epitope, antibody,\n        pretrained_embedding=None, masked_tokens=None\n    ):\n        # return ReturnType(xloss=3.0, nll=2., bind_X=torch.ones(masked_tokens.sum().item(), 14, 3), handle=(None, None))\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        epitope_X, epitope_S, epitope_A = epitope\n        antibody_X, antibody_S, antibody_cdr, padding_mask = antibody\n\n\n        # Encode target\n        epitope_h = self.U_i(self.embedding(epitope_S))\n        epitope_V = self.features._dihedrals(epitope_X)\n        epitope_mask = epitope_A[:,:,1].clamp(max=1).float()\n        \n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, true_antibody_X, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, padding_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 14).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        dloss = vloss = rloss = iloss = sloss = closs = 0\n        for t in range(self.args.refine_iteration):\n            # sequence update\n            antibody_V = self.features._dihedrals(antibody_X.detach())\n            complex_V = torch.cat([antibody_V, epitope_V], dim=1).detach()\n            complex_X = torch.cat([antibody_X, epitope_X], dim=1).detach()\n            complex_A = torch.cat([antibody_A, epitope_A], dim=1).detach()\n\n            # sequence message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            logits = self.W_s(complex_h[:, :antibody_N])\n            logits = logits[paratope_mask]\n            antibody_h_0 = antibody_h_0.clone()\n\n            # calculate sequence loss\n            snll = self.ce_loss(logits, flatten_cdr_S)\n            sloss = sloss + torch.sum(snll)\n\n            # update paratope embedding\n            probs = F.softmax(logits, dim=-1)\n            antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n            # structrue message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            antibody_X = antibody_X.clone()\n            antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n\n            ratio = (t + 1) / self.args.refine_iteration\n            label_X = true_antibody_X * ratio + antibody_X * (1 - ratio)\n        \n            true_V = self.features._dihedrals(label_X)\n            true_R, rmask_2D = inner_square_dist(label_X, antibody_A.clamp(max=1).float())\n            true_D, mask_2D = self_square_dist(label_X, paratope_mask)\n            true_C, cmask_2D = full_square_dist(label_X, label_X, antibody_A, antibody_A)\n            inter_D, imask_2D = cross_square_dist(label_X, epitope_X, paratope_mask, epitope_mask)\n\n            dloss_t, vloss_t, rloss_t, iloss_t, closs_t = self.struct_loss(\n                    antibody_X, epitope_X, true_V, true_R, true_D, inter_D, true_C\n            )\n            vloss = vloss + vloss_t * paratope_mask\n            dloss = dloss + dloss_t * mask_2D\n            iloss = iloss + iloss_t * imask_2D\n            rloss = rloss + rloss_t * rmask_2D\n            closs = closs + closs_t * cmask_2D\n    \n\n        sloss = sloss / paratope_mask.sum() # / self.args.refine_iteration\n        dloss = torch.sum(dloss) / mask_2D.sum() \n        iloss = torch.sum(iloss) / imask_2D.sum() \n        vloss = torch.sum(vloss) / paratope_mask.sum() \n        # print('mask_2D', mask_2D.sum())\n        if self.hierarchical:\n            rloss = torch.sum(rloss) / rmask_2D.sum()\n            closs = torch.sum(closs) / cmask_2D.sum()\n        else:\n            rloss = torch.sum(rloss[:,:,:4,:4]) / rmask_2D[:,:,:4,:4].sum()\n            closs = 0\n\n\n        struct_loss = (dloss + iloss + vloss + rloss + closs) / paratope_mask.sum() # / self.args.refine_iteration\n        seq_loss = sloss\n        return ReturnType(xloss=struct_loss, nll=seq_loss, bind_X=antibody_X[paratope_mask].detach(), handle=(epitope_X, epitope_A))\n\n    def generate(\n        self, init_prob, paratope, epitope, antibody,\n        pretrained_embedding=None, masked_tokens=None, num_decode=1\n    ):\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        epitope_X, epitope_S, epitope_A = epitope\n        antibody_X, antibody_S, antibody_cdr, antibody_mask = antibody\n\n\n        # Encode target (assumes same target)\n        epitope_h = self.U_i(self.embedding(epitope_S))\n        epitope_V = self.features._dihedrals(epitope_X)\n        epitope_mask = epitope_A[:,:,1].clamp(max=1).float()\n\n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, _, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, antibody_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 14).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        for t in range(self.args.refine_iteration):\n            # sequence update\n            antibody_V = self.features._dihedrals(antibody_X.detach())\n            complex_V = torch.cat([antibody_V, epitope_V], dim=1).detach()\n            complex_X = torch.cat([antibody_X, epitope_X], dim=1).detach()\n            complex_A = torch.cat([antibody_A, epitope_A], dim=1).detach()\n\n            # sequence message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            logits = self.W_s(complex_h[:, :antibody_N])\n            logits = logits[paratope_mask]\n            antibody_h_0 = antibody_h_0.clone()\n\n            # update paratope embedding\n            probs = F.softmax(logits, dim=-1)\n            antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n            # structrue message passing\n            complex_h = self.W_i(torch.cat([antibody_h_0, epitope_h], dim=1))\n            complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            antibody_X = antibody_X.clone()\n            antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n    \n        # sample new sequences\n        prob = F.softmax(logits.view(-1, len(ALPHABET)), dim=-1)\n        bind_I = torch.multinomial(prob, num_samples=num_decode, replacement=True)\n        bind_I = torch.transpose(bind_I, 0, 1)\n        repeat_logits = logits.view(-1, len(ALPHABET)).repeat(num_decode, 1)\n        snll = self.ce_loss(repeat_logits, bind_I.reshape(-1))\n        sloss = snll.view(num_decode, paratope_N).mean(dim=1)\n\n        S = bind_I.tolist()\n        S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(num_decode)]\n        ppl = torch.exp(sloss / paratope_N)\n        return ReturnType(handle=S, ppl=ppl, bind_X=antibody_X[paratope_mask].detach())", ""]}
{"filename": "fairseq_models/models/ab_decoder_noantigen.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom fairseq_models.data.abgen_dataset import ALPHABET, ATOM_TYPES, RES_ATOM14\nfrom fairseq_models.modules.hmpn_encoder import *\nfrom fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * ", "from fairseq_models.modules.framework_encoder import HierarchicalDecoder\nfrom fairseq_models.modules.nnutils import * \nfrom fairseq_models.modules.utils import *\n\nclass NoAntigenFullshotRefineDecoder(ABModel):\n\n    def __init__(self, args):\n        super(NoAntigenFullshotRefineDecoder, self).__init__(args)\n        self.args = args\n        self.hierarchical = args.hierarchical\n        self.residue_atom14 = torch.tensor([\n                [ATOM_TYPES.index(a) for a in atoms] for atoms in RES_ATOM14\n        ]).cuda()\n\n        self.W_s = nn.Linear(args.hidden_size, len(ALPHABET))\n        self.W_t = nn.Linear(self.embedding.dim(), args.hidden_size)\n        self.U_i = nn.Linear(self.embedding.dim(), args.hidden_size)\n\n        # self.W_trans = nn.Linear(args.hidden_size, self.embedding.dim())\n        self.coord_loss = nn.SmoothL1Loss(reduction='sum')\n\n        if args.hierarchical:\n            self.struct_mpn = HierEGNNEncoder(args)\n            self.seq_mpn = HierEGNNEncoder(args, update_X=False, backbone_CA_only=False)\n        else:\n            self.struct_mpn = EGNNEncoder(args)\n            self.seq_mpn = EGNNEncoder(args, update_X=False)\n\n        self.framework_encoder = HierarchicalDecoder(args)\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def struct_loss(self, antibody_X, true_V, true_R, true_D, true_C):\n        # dihedral loss\n        antibody_V = self.features._dihedrals(antibody_X)\n        vloss = self.mse_loss(antibody_V, true_V).sum(dim=-1)\n        # local loss\n        rdist = antibody_X.unsqueeze(2) - antibody_X.unsqueeze(3)\n        rdist = torch.sum(rdist ** 2, dim=-1)\n        rloss = self.huber_loss(rdist, true_R) + 10 * F.relu(1.5 - rdist)\n        # full loss\n        cdist, _ = full_square_dist(antibody_X, antibody_X, torch.ones_like(antibody_X)[..., 0], torch.ones_like(antibody_X)[..., 0])\n        closs = self.huber_loss(cdist, true_C) + 10 * F.relu(1.5 - cdist)\n        # alpha carbon\n        antibody_X = antibody_X[:, :, 1]\n        # CDR self distance\n        dist = antibody_X.unsqueeze(1) - antibody_X.unsqueeze(2)\n        dist = torch.sum(dist ** 2, dim=-1)\n        dloss = self.huber_loss(dist, true_D) + 10 * F.relu(14.4 - dist)\n        return dloss, vloss, rloss, closs\n\n    def forward(\n        self, init_prob, paratope, epitope=None, antibody=None,\n        pretrained_embedding=None, masked_tokens=None\n    ):\n        # return ReturnType(xloss=3.0, nll=2., bind_X=torch.ones(masked_tokens.sum().item(), 14, 3), handle=(None, None))\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        antibody_X, antibody_S, antibody_cdr, padding_mask = antibody\n\n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, true_antibody_X, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, padding_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n        # antibody_X = torch.zeros_like(antibody_X)\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 4).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        dloss = vloss = rloss = sloss = closs = 0\n        for t in range(self.args.refine_iteration):\n            if t < self.args.refine_iteration - 1:\n                with torch.no_grad():\n                    # sequence update\n                    antibody_V = self.features._dihedrals(antibody_X.detach())\n                    complex_V = antibody_V.detach()\n                    complex_X = antibody_X.detach()\n                    complex_A = antibody_A.detach()\n\n                    # sequence message passing\n                    complex_h = self.W_i(antibody_h_0)\n                    complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                    logits = self.W_s(complex_h[:, :antibody_N])\n                    logits = logits[paratope_mask]\n                    antibody_h_0 = antibody_h_0.clone()\n\n                    # calculate sequence loss\n                    snll = self.ce_loss(logits, flatten_cdr_S)\n                    sloss = sloss + torch.sum(snll)\n\n                    # update paratope embedding\n                    probs = F.softmax(logits, dim=-1)\n                    antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n                    # structrue message passing\n                    complex_h = self.W_i(antibody_h_0)\n                    complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                    antibody_X = antibody_X.clone()\n                    antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n\n                    ratio = (t + 1) / self.args.refine_iteration\n                    label_X = true_antibody_X * ratio + antibody_X * (1 - ratio)\n                \n                    true_V = self.features._dihedrals(label_X)\n                    true_R, rmask_2D = inner_square_dist(label_X, antibody_A.clamp(max=1).float())\n                    true_D, mask_2D = self_square_dist(label_X, paratope_mask)\n                    true_C, cmask_2D = full_square_dist(label_X, label_X, antibody_A, antibody_A)\n\n                    dloss_t, vloss_t, rloss_t, closs_t = self.struct_loss(\n                            antibody_X, true_V, true_R, true_D, true_C\n                    )\n                    vloss = vloss + vloss_t * paratope_mask\n                    dloss = dloss + dloss_t * mask_2D\n                    rloss = rloss + rloss_t * rmask_2D\n                    closs = closs + closs_t * cmask_2D\n            else:\n                # sequence update\n                antibody_V = self.features._dihedrals(antibody_X.detach())\n                complex_V = antibody_V.detach()\n                complex_X = antibody_X.detach()\n                complex_A = antibody_A.detach()\n\n                # sequence message passing\n                complex_h = self.W_i(antibody_h_0)\n                complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                logits = self.W_s(complex_h[:, :antibody_N])\n                logits = logits[paratope_mask]\n                antibody_h_0 = antibody_h_0.clone()\n\n                # calculate sequence loss\n                snll = self.ce_loss(logits, flatten_cdr_S)\n                sloss = sloss + torch.sum(snll)\n\n                # update paratope embedding\n                probs = F.softmax(logits, dim=-1)\n                antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n                # structrue message passing\n                complex_h = self.W_i(antibody_h_0)\n                complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n                antibody_X = antibody_X.clone()\n                antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n\n                ratio = (t + 1) / self.args.refine_iteration\n                label_X = true_antibody_X * ratio + antibody_X * (1 - ratio)\n            \n                true_V = self.features._dihedrals(label_X)\n                true_R, rmask_2D = inner_square_dist(label_X, antibody_A.clamp(max=1).float())\n                true_D, mask_2D = self_square_dist(label_X, paratope_mask)\n                true_C, cmask_2D = full_square_dist(label_X, label_X, antibody_A, antibody_A)\n\n                dloss_t, vloss_t, rloss_t, closs_t = self.struct_loss(\n                        antibody_X, true_V, true_R, true_D, true_C\n                )\n                vloss = vloss + vloss_t * paratope_mask\n                dloss = dloss + dloss_t * mask_2D\n                rloss = rloss + rloss_t * rmask_2D\n                closs = closs + closs_t * cmask_2D\n\n    \n\n        sloss = sloss / paratope_mask.sum() / self.args.refine_iteration\n        dloss = torch.sum(dloss) / mask_2D.sum() \n        vloss = torch.sum(vloss) / paratope_mask.sum() \n        # print('mask_2D', mask_2D.sum())\n        if self.hierarchical:\n            rloss = torch.sum(rloss) / rmask_2D.sum()\n            closs = torch.sum(closs) / cmask_2D.sum()\n        else:\n            rloss = torch.sum(rloss[:,:,:4,:4]) / rmask_2D[:,:,:4,:4].sum()\n            closs = 0\n\n\n        struct_loss = (dloss + vloss + rloss + closs) / paratope_N / self.args.refine_iteration\n        seq_loss = sloss\n        return ReturnType(xloss=struct_loss, nll=seq_loss, bind_X=antibody_X[paratope_mask].detach())\n\n    def generate(\n        self, init_prob, paratope, epitope=None, antibody=None,\n        pretrained_embedding=None, masked_tokens=None, num_decode=1\n    ):\n        _, true_cdr_S, flatten_cdr_S, flatten_init_X = paratope\n        antibody_X, antibody_S, antibody_cdr, antibody_mask = antibody\n\n        # antibody_h_0: initialized antibody embedding with shape [B, antibody_N, 256]\n        # paratope_mask: mask for paratope nodes with shape [B, antibody_N]\n        # padding_mask: mask for existing nodes with shape [B, antibody_N]\n        init_prob = F.softmax(init_prob, dim=-1)\n        antibody_h_0, _, paratope_mask, antibody_X, padding_mask = self.framework_encoder(\n            antibody_X, antibody_S, antibody_cdr, antibody_mask, init_prob, masked_tokens, flatten_init_X.detach().clone()\n        )\n\n        B, paratope_N = true_cdr_S.size(0), true_cdr_S.size(1)\n        antibody_N = paratope_mask.size(1)\n        \n        antibody_A = torch.zeros(B, antibody_N, 4).cuda().long()\n        antibody_A[padding_mask>0] = torch.tensor(\n            [1, 2, 3, 4], \n        ).cuda().expand(int(padding_mask.sum()), -1)  # backbone atoms\n        \n        # Refine\n        for t in range(self.args.refine_iteration):\n            # sequence update\n            antibody_V = self.features._dihedrals(antibody_X.detach())\n            complex_V = antibody_V.detach()\n            complex_X = antibody_X.detach()\n            complex_A = antibody_A.detach()\n\n            # sequence message passing\n            complex_h = self.W_i(antibody_h_0)\n            complex_h, _ = self.seq_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            logits = self.W_s(complex_h[:, :antibody_N])\n            logits = logits[paratope_mask]\n            antibody_h_0 = antibody_h_0.clone()\n\n            # update paratope embedding\n            probs = F.softmax(logits, dim=-1)\n            antibody_h_0[paratope_mask] = self.W_t(self.embedding.soft_forward(probs))\n\n            # structrue message passing\n            complex_h = self.W_i(antibody_h_0)\n            complex_h, complex_X = self.struct_mpn(complex_X, complex_V, complex_h, complex_A, antibody_N=antibody_N, paratope_mask=paratope_mask, pretrained_embedding=pretrained_embedding)\n            antibody_X = antibody_X.clone()\n            antibody_X[paratope_mask] = complex_X[:, :antibody_N][paratope_mask]\n    \n        # sample new sequences\n        prob = F.softmax(logits.view(-1, len(ALPHABET)), dim=-1)\n        bind_I = torch.multinomial(prob, num_samples=1).squeeze(-1)\n        snll = self.ce_loss(logits.view(-1, len(ALPHABET)), bind_I)\n        sloss = snll.view(B, paratope_N).mean(dim=1)\n\n        S = bind_I.view(B, paratope_N).tolist()\n        S = [''.join([ALPHABET[S[i][j]] for j in range(paratope_N)]) for i in range(B)]\n        ppl = torch.exp(sloss / paratope_N)\n        return ReturnType(handle=S, ppl=ppl, bind_X=antibody_X[paratope_mask].detach())", ""]}
{"filename": "fairseq_models/modules/protein_features.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\n# from matplotlib import pyplot as plt\nfrom .utils import gather_edges, gather_nodes\n\n\nclass PositionalEncodings(nn.Module):\n\n    def __init__(self, num_embeddings, period_range=[2,1000]):\n        super(PositionalEncodings, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.period_range = period_range \n\n    def forward(self, E_idx):\n        # i-j\n        N_batch = E_idx.size(0)\n        N_nodes = E_idx.size(1)\n        N_neighbors = E_idx.size(2)\n        ii = torch.arange(N_nodes).view((1, -1, 1)).to(E_idx.device)\n        d = (E_idx.float() - ii).unsqueeze(-1)\n        # Original Transformer frequencies\n        frequency = torch.exp(\n            torch.arange(0, self.num_embeddings, 2)\n            * -(np.log(10000.0) / self.num_embeddings)\n        ).to(E_idx.device)\n        # Grid-aligned\n        # frequency = 2. * np.pi * torch.exp(\n        #     -torch.linspace(\n        #         np.log(self.period_range[0]), \n        #         np.log(self.period_range[1]),\n        #         self.num_embeddings / 2\n        #     )\n        # )\n        angles = d * frequency.view((1,1,1,-1))\n        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n        return E", "\n\nclass PositionalEncodings(nn.Module):\n\n    def __init__(self, num_embeddings, period_range=[2,1000]):\n        super(PositionalEncodings, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.period_range = period_range \n\n    def forward(self, E_idx):\n        # i-j\n        N_batch = E_idx.size(0)\n        N_nodes = E_idx.size(1)\n        N_neighbors = E_idx.size(2)\n        ii = torch.arange(N_nodes).view((1, -1, 1)).to(E_idx.device)\n        d = (E_idx.float() - ii).unsqueeze(-1)\n        # Original Transformer frequencies\n        frequency = torch.exp(\n            torch.arange(0, self.num_embeddings, 2)\n            * -(np.log(10000.0) / self.num_embeddings)\n        ).to(E_idx.device)\n        # Grid-aligned\n        # frequency = 2. * np.pi * torch.exp(\n        #     -torch.linspace(\n        #         np.log(self.period_range[0]), \n        #         np.log(self.period_range[1]),\n        #         self.num_embeddings / 2\n        #     )\n        # )\n        angles = d * frequency.view((1,1,1,-1))\n        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n        return E", "\n\nclass ProteinFeatures(nn.Module):\n\n    def __init__(self, num_positional_embeddings=16, num_rbf=16, top_k=30, features_type='backbone', direction='forward'):\n        \"\"\" Extract protein features \"\"\"\n        super(ProteinFeatures, self).__init__()\n        self.top_k = top_k\n        self.num_rbf = num_rbf\n        self.num_positional_embeddings = num_positional_embeddings\n        self.direction = direction\n\n        # Feature types\n        self.features_type = features_type\n        self.feature_dimensions = {\n            'atom': (0, num_positional_embeddings + num_rbf),\n            'backbone': (6, num_positional_embeddings + num_rbf + 7),\n        }\n\n        # Positional encoding\n        self.embeddings = PositionalEncodings(num_positional_embeddings)\n        \n    \n    def _dist(self, X, mask, eps=1E-6):\n        \"\"\" Pairwise euclidean distances \"\"\"\n        N = X.size(1)\n        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n        if self.direction == 'bidirectional':\n            mask_2D = mask_2D - torch.eye(N).unsqueeze(0).cuda()  # remove self\n            mask_2D = mask_2D.clamp(min=0)\n        elif self.direction == 'forward':\n            nmask = torch.arange(X.size(1)).cuda()\n            nmask = nmask.view(1,-1,1) > nmask.view(1,1,-1)\n            mask_2D = nmask.float() * mask_2D  # [B, N, N]\n        else:\n            raise ValueError('invalid direction', direction)\n\n        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n\n        # Identify k nearest neighbors (not including self)\n        D_adjust = D + (1. - mask_2D) * 10000\n        top_k = min(self.top_k, N)\n        D_neighbors, E_idx = torch.topk(D_adjust, top_k, dim=-1, largest=False)\n        mask_neighbors = gather_edges(mask_2D.unsqueeze(-1), E_idx)\n\n        # Debug plot KNN\n        # print(E_idx[:10,:10])\n        # D_simple = mask_2D * torch.zeros(D.size()).scatter(-1, E_idx, torch.ones_like(knn_D))\n        # print(D_simple)\n        # fig = plt.figure(figsize=(4,4))\n        # ax = fig.add_subplot(111)\n        # D_simple = D.data.numpy()[0,:,:]\n        # plt.imshow(D_simple, aspect='equal')\n        # plt.axis('off')\n        # plt.tight_layout()\n        # plt.savefig('D_knn.pdf')\n        # exit(0)\n        return D_neighbors, E_idx, mask_neighbors\n\n    \n    def _rbf(self, D):\n        # Distance radial basis function\n        D_min, D_max, D_count = 0., 20., self.num_rbf\n        D_mu = torch.linspace(D_min, D_max, D_count).cuda()\n        D_mu = D_mu.view([1,1,1,-1])\n        D_sigma = (D_max - D_min) / D_count\n        D_expand = torch.unsqueeze(D, -1)\n        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n\n        # for i in range(D_count):\n        #     fig = plt.figure(figsize=(4,4))\n        #     ax = fig.add_subplot(111)\n        #     rbf_i = RBF.data.numpy()[0,i,:,:]\n        #     # rbf_i = D.data.numpy()[0,0,:,:]\n        #     plt.imshow(rbf_i, aspect='equal')\n        #     plt.axis('off')\n        #     plt.tight_layout()\n        #     plt.savefig('rbf{}.pdf'.format(i))\n        #     print(np.min(rbf_i), np.max(rbf_i), np.mean(rbf_i))\n        # exit(0)\n        return RBF\n\n    \n    def _quaternions(self, R):\n        \"\"\" Convert a batch of 3D rotations [R] to quaternions [Q]\n            R [...,3,3]\n            Q [...,4]\n        \"\"\"\n        # Simple Wikipedia version\n        # en.wikipedia.org/wiki/Rotation_matrix#Quaternion\n        # For other options see math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n        diag = torch.diagonal(R, dim1=-2, dim2=-1)\n        Rxx, Ryy, Rzz = diag.unbind(-1)\n        magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n              Rxx - Ryy - Rzz, \n            - Rxx + Ryy - Rzz, \n            - Rxx - Ryy + Rzz\n        ], -1)))\n        _R = lambda i,j: R[:,:,:,i,j]\n        signs = torch.sign(torch.stack([\n            _R(2,1) - _R(1,2),\n            _R(0,2) - _R(2,0),\n            _R(1,0) - _R(0,1)\n        ], -1))\n        xyz = signs * magnitudes\n        # The relu enforces a non-negative trace\n        w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n        Q = torch.cat((xyz, w), -1)\n        Q = F.normalize(Q, dim=-1)\n\n        # Axis of rotation\n        # Replace bad rotation matrices with identity\n        # I = torch.eye(3).view((1,1,1,3,3))\n        # I = I.expand(*(list(R.shape[:3]) + [-1,-1]))\n        # det = (\n        #     R[:,:,:,0,0] * (R[:,:,:,1,1] * R[:,:,:,2,2] - R[:,:,:,1,2] * R[:,:,:,2,1])\n        #     - R[:,:,:,0,1] * (R[:,:,:,1,0] * R[:,:,:,2,2] - R[:,:,:,1,2] * R[:,:,:,2,0])\n        #     + R[:,:,:,0,2] * (R[:,:,:,1,0] * R[:,:,:,2,1] - R[:,:,:,1,1] * R[:,:,:,2,0])\n        # )\n        # det_mask = torch.abs(det.unsqueeze(-1).unsqueeze(-1))\n        # R = det_mask * R + (1 - det_mask) * I\n\n        # DEBUG\n        # https://math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n        # Columns of this are in rotation plane\n        # A = R - I\n        # v1, v2 = A[:,:,:,:,0], A[:,:,:,:,1]\n        # axis = F.normalize(torch.cross(v1, v2), dim=-1)\n        return Q\n\n    \n    def _contacts(self, D_neighbors, E_idx, mask_neighbors, cutoff=8):\n        \"\"\" Contacts \"\"\"\n        D_neighbors = D_neighbors.unsqueeze(-1)\n        neighbor_C = mask_neighbors * (D_neighbors < cutoff)\n        return neighbor_C\n\n    \n    def _hbonds(self, X, E_idx, mask_neighbors, eps=1E-3):\n        \"\"\" Hydrogen bonds and contact map\n        \"\"\"\n        X_atoms = dict(zip(['N', 'CA', 'C', 'O'], torch.unbind(X, 2)))\n\n        # Virtual hydrogens\n        X_atoms['C_prev'] = F.pad(X_atoms['C'][:,1:,:], (0,0,0,1), 'constant', 0)\n        X_atoms['H'] = X_atoms['N'] + F.normalize(\n             F.normalize(X_atoms['N'] - X_atoms['C_prev'], -1)\n          +  F.normalize(X_atoms['N'] - X_atoms['CA'], -1)\n        , -1)\n\n        def _distance(X_a, X_b):\n            return torch.norm(X_a[:,None,:,:] - X_b[:,:,None,:], dim=-1)\n\n        def _inv_distance(X_a, X_b):\n            return 1. / (_distance(X_a, X_b) + eps)\n\n        # DSSP vacuum electrostatics model\n        U = (0.084 * 332) * (\n              _inv_distance(X_atoms['O'], X_atoms['N'])\n            + _inv_distance(X_atoms['C'], X_atoms['H'])\n            - _inv_distance(X_atoms['O'], X_atoms['H'])\n            - _inv_distance(X_atoms['C'], X_atoms['N'])\n        )\n\n        HB = (U < -0.5)\n        neighbor_HB = mask_neighbors * gather_edges(HB.unsqueeze(-1),  E_idx)\n        # print(HB)\n        # HB = F.sigmoid(U)\n        # U_np = U.cpu().data.numpy()\n        # # plt.matshow(np.mean(U_np < -0.5, axis=0))\n        # plt.matshow(HB[0,:,:])\n        # plt.colorbar()\n        # plt.show()\n        # D_CA = _distance(X_atoms['CA'], X_atoms['CA'])\n        # D_CA = D_CA.cpu().data.numpy()\n        # plt.matshow(D_CA[0,:,:] < contact_D)\n        # # plt.colorbar()\n        # plt.show()\n        # exit(0)\n        return neighbor_HB\n\n    \n    def _AD_features(self, X, eps=1e-6):\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Bond angle calculation\n        cosA = -(u_1 * u_0).sum(-1)\n        cosA = torch.clamp(cosA, -1+eps, 1-eps)\n        A = torch.acos(cosA)\n        # Angle between normals\n        cosD = (n_2 * n_1).sum(-1)\n        cosD = torch.clamp(cosD, -1+eps, 1-eps)\n        D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n        # Backbone features\n        AD_features = torch.stack((torch.cos(A), torch.sin(A) * torch.cos(D), torch.sin(A) * torch.sin(D)), 2)\n        return F.pad(AD_features, (0,0,1,2), 'constant', 0)\n\n    \n    def _orientations_coarse(self, X, E_idx, eps=1e-6):\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Build relative orientations\n        o_1 = F.normalize(u_2 - u_1, dim=-1)\n        O = torch.stack((o_1, n_2, torch.cross(o_1, n_2)), 2)\n        O = O.view(list(O.shape[:2]) + [9])\n        O = F.pad(O, (0,0,1,2), 'constant', 0)\n\n        O_neighbors = gather_nodes(O, E_idx)\n        X_neighbors = gather_nodes(X, E_idx)\n        \n        # Re-view as rotation matrices\n        O = O.view(list(O.shape[:2]) + [3,3])\n        O_neighbors = O_neighbors.view(list(O_neighbors.shape[:3]) + [3,3])\n\n        # Rotate into local reference frames\n        dX = X_neighbors - X.unsqueeze(-2)\n        dU = torch.matmul(O.unsqueeze(2), dX.unsqueeze(-1)).squeeze(-1)\n        dU = F.normalize(dU, dim=-1)\n        R = torch.matmul(O.unsqueeze(2).transpose(-1,-2).contiguous(), O_neighbors)\n        Q = self._quaternions(R)\n        return torch.cat((dU,Q), dim=-1)\n\n    \n    def _dihedrals(self, X, eps=1e-7):\n        # First 3 coordinates are N, CA, C\n        X = X[:,:,:3,:].reshape(X.shape[0], 3*X.shape[1], 3)\n\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Angle between normals\n        cosD = (n_2 * n_1).sum(-1)\n        cosD = torch.clamp(cosD, -1+eps, 1-eps)\n        D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n\n        D = F.pad(D, (3,0), 'constant', 0)\n        D = D.view((D.size(0), int(D.size(1)/3), 3))\n        phi, psi, omega = torch.unbind(D,-1)\n\n        # print(cosD.cpu().data.numpy().flatten())\n        # print(omega.sum().cpu().data.numpy().flatten())\n\n        # Bond angle calculation\n        # A = torch.acos(-(u_1 * u_0).sum(-1))\n\n        # DEBUG: Ramachandran plot\n        # x = phi.cpu().data.numpy().flatten()\n        # y = psi.cpu().data.numpy().flatten()\n        # plt.scatter(x * 180 / np.pi, y * 180 / np.pi, s=1, marker='.')\n        # plt.xlabel('phi')\n        # plt.ylabel('psi')\n        # plt.axis('square')\n        # plt.grid()\n        # plt.axis([-180,180,-180,180])\n        # plt.show()\n\n        # Lift angle representations to the circle\n        D_features = torch.cat((torch.cos(D), torch.sin(D)), 2)\n        return D_features\n\n    def forward(self, X, mask):\n        \"\"\" Featurize coordinates as an attributed graph \"\"\"\n        if self.features_type == 'backbone':\n            X_ca = X[:,:,1,:]\n            D_neighbors, E_idx, mask_neighbors = self._dist(X_ca, mask)\n            RBF = self._rbf(D_neighbors)\n            E_positional = self.embeddings(E_idx)\n            O_features = self._orientations_coarse(X_ca, E_idx)\n            E = torch.cat((E_positional, RBF, O_features), -1)\n            V = self._dihedrals(X)\n\n        elif self.features_type == 'atom':\n            D_neighbors, E_idx, mask_neighbors = self._dist(X, mask)\n            RBF = self._rbf(D_neighbors)\n            E_positional = self.embeddings(E_idx)\n            E = torch.cat((E_positional, RBF), -1)\n            V = None\n\n        return V, E, E_idx", "\n"]}
{"filename": "fairseq_models/modules/framework_encoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom .utils import *\nfrom .nnutils import * \nfrom .protein_features import ProteinFeatures\n\n", "\n\n\nclass HierarchicalDecoder(nn.Module):\n\n    def __init__(self, args):\n        super(HierarchicalDecoder, self).__init__()\n        self.cdr_type = args.cdr_type\n        self.k_neighbors = args.k_neighbors\n        self.block_size = args.block_size\n        self.hidden_size = args.hidden_size\n        self.args = args\n\n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type='full',\n                direction='bidirectional'\n        )\n        # self.node_in, self.edge_in = self.features.feature_dimensions['full']\n        self.embedding = AAEmbedding()\n        self.W_s = nn.Linear(self.embedding.dim(), args.hidden_size)\n        \n        self.rnn = nn.GRU(\n                args.hidden_size, args.hidden_size, batch_first=True, \n                num_layers=1, bidirectional=True\n        )\n\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n                \n    def mask_mean(self, X, mask, i):\n        # [B, N, 4, 3] -> [B, 1, 4, 3] / [B, 1, 1, 1]\n        X = X[:, i:i+self.block_size]\n        if X.dim() == 4:\n            mask = mask[:, i:i+self.block_size].unsqueeze(-1).unsqueeze(-1)\n        else:\n            mask = mask[:, i:i+self.block_size].unsqueeze(-1)\n        return torch.sum(X * mask, dim=1, keepdims=True) / (mask.sum(dim=1, keepdims=True) + 1e-8)\n\n    def make_X_blocks(self, X, l, r, mask):\n        N = X.size(1)\n        lblocks = [self.mask_mean(X, mask, i) for i in range(0, l, self.block_size)]\n        rblocks = [self.mask_mean(X, mask, i) for i in range(r + 1, N, self.block_size)]\n        bX = torch.cat(lblocks + [X[:, l:r+1]] + rblocks, dim=1)\n        return bX.detach()\n\n    def make_S_blocks(self, hS, l, r, mask):\n        N = hS.size(1) # 130\n        # l=T_min=96, r=T_max=109, range of cdr-3\n        # block_size=8\n        # ==> len(LS)=12, LS[i].shape=[7, 1, 256]\n        # ==> len(RS)=3, RS[i].shape=[7, 1, 256]\n        lseqs = [self.mask_mean(hS, mask, i) for i in range(0, l, self.block_size)] # mask_mean: calculate mean without mask\n        rseqs = [self.mask_mean(hS, mask, i) for i in range(r + 1, N, self.block_size)]\n        bS = torch.cat(lseqs + [hS[:, l:r+1]] + rseqs, dim=1) # 12 + 14 + 3 = 29 ==> [7, 29, 256]\n        return bS, len(lseqs), len(rseqs)\n    \n    def make_mask_blocks(self, mask, l, r):\n        N = mask.size(1)\n        lmask = [mask[:, i:i+self.block_size].amax(dim=1, keepdims=True) for i in range(0, l, self.block_size)] # amax: max val of each row\n        rmask = [mask[:, i:i+self.block_size].amax(dim=1, keepdims=True) for i in range(r + 1, N, self.block_size)] # if one elem masked, the whole block masked\n        bmask = torch.cat(lmask + [mask[:, l:r+1]] + rmask, dim=1)\n        return bmask\n\n    def get_completion_mask(self, B, N, cdr_range):\n        cmask = torch.zeros(B, N).cuda()\n        for i, (l,r) in enumerate(cdr_range): \n            cmask[i, l:r+1] = 1\n        return cmask\n\n    def remove_cdr_coords(self, X, cdr_range):\n        X = X.clone()\n        for i, (l,r) in enumerate(cdr_range):\n            X[i, l:r+1, :, :] = 0\n        return X.clone()\n\n    def forward(self, antibody_X, antibody_S, antibody_cdr, padding_mask, c_init_prob, paratope_mask, init_X):\n        B, N = padding_mask.size(0), padding_mask.size(1)\n        \n        cdr_range = [(cdr.index(self.cdr_type), cdr.rindex(self.cdr_type)) for cdr in antibody_cdr]\n        T_min = min([l for l,r in cdr_range])\n        T_max = max([r for l,r in cdr_range])\n        \n        antibody_init_X = antibody_X.clone()\n        # print(init_X.shape) [8, 13, 14, 3]\n        # print(antibody_init_X.shape) [8, 221, 14, 3]\n        # print(padding_mask)\n        # print(padding_mask.shape)\n        # print(antibody_init_X[padding_mask>0].shape)\n        antibody_init_X[paratope_mask>0] = init_X\n        antibody_init_X = self.make_X_blocks(antibody_init_X, T_min, T_max, padding_mask)\n\n        # make blocks and encode framework\n        S = antibody_S.clone() * (1 - paratope_mask.long()) # type_id, 0 for cdr-3 [7, 130]\n        seq_emb = self.embedding(S)\n        \n        seq_emb[paratope_mask>0] = self.embedding.soft_forward(c_init_prob)\n        hS = self.W_s(seq_emb)\n        hS, offset, suffix = self.make_S_blocks(hS, T_min, T_max, padding_mask)\n        paratope_mask = torch.cat([paratope_mask.new_zeros(B, offset), paratope_mask[:, T_min:T_max+1], paratope_mask.new_zeros(B, suffix)], dim=1) # [7, 12+14+3]\n\n        \n        # Ground truth \n        antibody_X = self.make_X_blocks(antibody_X, T_min, T_max, padding_mask) # [7, 130, 4, 3] ==> [7, 29, 4, 3]\n        \n        padding_mask = self.make_mask_blocks(padding_mask, T_min, T_max)\n\n        return hS, antibody_X, paratope_mask, antibody_init_X, padding_mask", ""]}
{"filename": "fairseq_models/modules/utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import namedtuple\n\nReturnType = namedtuple('ReturnType',('xloss','nll','ppl','bind_X','handle'), defaults=(None, None, None, None, None))\n\n\ndef kabsch(A, B):\n    a_mean = A.mean(dim=1, keepdims=True)\n    b_mean = B.mean(dim=1, keepdims=True)\n    A_c = A - a_mean\n    B_c = B - b_mean\n    # Covariance matrix\n    H = torch.bmm(A_c.transpose(1,2), B_c)  # [B, 3, 3]\n    U, S, V = torch.svd(H)\n    # Rotation matrix\n    R = torch.bmm(V, U.transpose(1,2))  # [B, 3, 3]\n    # Translation vector\n    t = b_mean - torch.bmm(R, a_mean.transpose(1,2)).transpose(1,2)\n    A_aligned = torch.bmm(R, A.transpose(1,2)).transpose(1,2) + t\n    return A_aligned, R, t", "\ndef kabsch(A, B):\n    a_mean = A.mean(dim=1, keepdims=True)\n    b_mean = B.mean(dim=1, keepdims=True)\n    A_c = A - a_mean\n    B_c = B - b_mean\n    # Covariance matrix\n    H = torch.bmm(A_c.transpose(1,2), B_c)  # [B, 3, 3]\n    U, S, V = torch.svd(H)\n    # Rotation matrix\n    R = torch.bmm(V, U.transpose(1,2))  # [B, 3, 3]\n    # Translation vector\n    t = b_mean - torch.bmm(R, a_mean.transpose(1,2)).transpose(1,2)\n    A_aligned = torch.bmm(R, A.transpose(1,2)).transpose(1,2) + t\n    return A_aligned, R, t", "\n# X: [B, N, 4, 3], R: [B, 3, 3], t: [B, 3]\ndef rigid_transform(X, R, t):\n    B, N, L = X.size(0), X.size(1), X.size(2)\n    X = X.reshape(B, N * L, 3)\n    X = torch.bmm(R, X.transpose(1,2)).transpose(1,2) + t\n    return X.view(B, N, L, 3)\n\n# A: [B, N, 3], B: [B, N, 3], mask: [B, N]\ndef compute_rmsd(A, B, mask):\n    A_aligned, _, _ = kabsch(A, B)\n    rmsd = ((A_aligned - B) ** 2).sum(dim=-1)\n    rmsd = torch.sum(rmsd * mask, dim=-1) / (mask.sum(dim=-1) + 1e-6)\n    return rmsd.sqrt()", "# A: [B, N, 3], B: [B, N, 3], mask: [B, N]\ndef compute_rmsd(A, B, mask):\n    A_aligned, _, _ = kabsch(A, B)\n    rmsd = ((A_aligned - B) ** 2).sum(dim=-1)\n    rmsd = torch.sum(rmsd * mask, dim=-1) / (mask.sum(dim=-1) + 1e-6)\n    return rmsd.sqrt()\n\n# A: [B, N, 3], B: [B, N, 3], mask: [B, N]\ndef compute_rmsd_no_align(A, B, mask):\n    rmsd = ((A - B) ** 2).sum(dim=-1)\n    rmsd = torch.sum(rmsd * mask, dim=-1) / (mask.sum(dim=-1) + 1e-6)\n    return rmsd.sqrt()", "def compute_rmsd_no_align(A, B, mask):\n    rmsd = ((A - B) ** 2).sum(dim=-1)\n    rmsd = torch.sum(rmsd * mask, dim=-1) / (mask.sum(dim=-1) + 1e-6)\n    return rmsd.sqrt()\n\ndef eig_coord(X, mask):\n    D, mask_2D = self_square_dist(X, torch.ones_like(mask))\n    return eig_coord_from_dist(D)\n\ndef eig_coord_from_dist(D):\n    M = (D[:, :1, :] + D[:, :, :1] - D) / 2\n    L, V = torch.linalg.eigh(M)\n    L = torch.diag_embed(L)\n    X = torch.matmul(V, L.clamp(min=0).sqrt())\n    return X[:, :, -3:].detach()", "\ndef eig_coord_from_dist(D):\n    M = (D[:, :1, :] + D[:, :, :1] - D) / 2\n    L, V = torch.linalg.eigh(M)\n    L = torch.diag_embed(L)\n    X = torch.matmul(V, L.clamp(min=0).sqrt())\n    return X[:, :, -3:].detach()\n\ndef inner_square_dist(X, mask):\n    L = mask.size(2)\n    dX = X.unsqueeze(2) - X.unsqueeze(3)  # [B,N,1,L,3] - [B,N,L,1,3]\n    mask_2D = mask.unsqueeze(2) * mask.unsqueeze(3)\n    mask_2D = mask_2D * (1 - torch.eye(L)[None,None,:,:]).to(mask_2D)\n    D = torch.sum(dX**2, dim=-1)\n    return D * mask_2D, mask_2D", "def inner_square_dist(X, mask):\n    L = mask.size(2)\n    dX = X.unsqueeze(2) - X.unsqueeze(3)  # [B,N,1,L,3] - [B,N,L,1,3]\n    mask_2D = mask.unsqueeze(2) * mask.unsqueeze(3)\n    mask_2D = mask_2D * (1 - torch.eye(L)[None,None,:,:]).to(mask_2D)\n    D = torch.sum(dX**2, dim=-1)\n    return D * mask_2D, mask_2D\n\ndef self_square_dist(X, mask):\n    X = X[:, :, 1] \n    dX = X.unsqueeze(1) - X.unsqueeze(2)  # [B, 1, N, 3] - [B, N, 1, 3]\n    D = torch.sum(dX**2, dim=-1)\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)  # [B, 1, N] x [B, N, 1]\n    mask_2D = mask_2D * (1 - torch.eye(mask.size(1))[None,:,:]).to(mask_2D)\n    return D, mask_2D", "def self_square_dist(X, mask):\n    X = X[:, :, 1] \n    dX = X.unsqueeze(1) - X.unsqueeze(2)  # [B, 1, N, 3] - [B, N, 1, 3]\n    D = torch.sum(dX**2, dim=-1)\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)  # [B, 1, N] x [B, N, 1]\n    mask_2D = mask_2D * (1 - torch.eye(mask.size(1))[None,:,:]).to(mask_2D)\n    return D, mask_2D\n\ndef cross_square_dist(X, Y, xmask, ymask):\n    X, Y = X[:, :, 1], Y[:, :, 1]\n    dxy = X.unsqueeze(2) - Y.unsqueeze(1)  # [B, N, 1, 3] - [B, 1, M, 3]\n    D = torch.sum(dxy ** 2, dim=-1)\n    mask_2D = xmask.unsqueeze(2) * ymask.unsqueeze(1)  # [B, N, 1] x [B, 1, M]\n    return D, mask_2D", "def cross_square_dist(X, Y, xmask, ymask):\n    X, Y = X[:, :, 1], Y[:, :, 1]\n    dxy = X.unsqueeze(2) - Y.unsqueeze(1)  # [B, N, 1, 3] - [B, 1, M, 3]\n    D = torch.sum(dxy ** 2, dim=-1)\n    mask_2D = xmask.unsqueeze(2) * ymask.unsqueeze(1)  # [B, N, 1] x [B, 1, M]\n    return D, mask_2D\n\ndef full_square_dist(X, Y, XA, YA, contact=False, remove_diag=False):\n    B, N, M, L = X.size(0), X.size(1), Y.size(1), Y.size(2)\n    X = X.view(B, N * L, 3)\n    Y = Y.view(B, M * L, 3)\n    dxy = X.unsqueeze(2) - Y.unsqueeze(1)  # [B, NL, 1, 3] - [B, 1, ML, 3]\n    D = torch.sum(dxy ** 2, dim=-1)\n    D = D.view(B, N, L, M, L)\n    D = D.transpose(2, 3).reshape(B, N, M, L*L)\n\n    xmask = XA.clamp(max=1).float().view(B, N * L)\n    ymask = YA.clamp(max=1).float().view(B, M * L)\n    mask = xmask.unsqueeze(2) * ymask.unsqueeze(1)  # [B, NL, 1] x [B, 1, ML]\n    mask = mask.view(B, N, L, M, L)\n    mask = mask.transpose(2, 3).reshape(B, N, M, L*L)\n    if remove_diag:\n        mask = mask * (1 - torch.eye(N)[None,:,:,None]).to(mask)\n\n    if contact:\n        D = D + 1e6 * (1 - mask)\n        return D.amin(dim=-1), mask.amax(dim=-1)\n    else:\n        return D, mask", "\n\"\"\" Quaternion functions \"\"\"\n\ndef quaternion_to_matrix(quaternions):\n    r, i, j, k = torch.unbind(quaternions, -1)\n    two_s = 2.0 / (1e-4 + (quaternions * quaternions).sum(-1))\n    o = torch.stack(\n        (\n            1 - two_s * (j * j + k * k),\n            two_s * (i * j - k * r),\n            two_s * (i * k + j * r),\n            two_s * (i * j + k * r),\n            1 - two_s * (i * i + k * k),\n            two_s * (j * k - i * r),\n            two_s * (i * k - j * r),\n            two_s * (j * k + i * r),\n            1 - two_s * (i * i + j * j),\n        ),\n        -1,\n    )\n    return o.reshape(quaternions.shape[:-1] + (3, 3))", "\ndef matrix_to_quaternion(rot):\n    if(rot.shape[-2:] != (3, 3)):\n        raise ValueError(\"Input rotation is incorrectly shaped\")\n\n    rot = [[rot[..., i, j] for j in range(3)] for i in range(3)]\n    [[xx, xy, xz], [yx, yy, yz], [zx, zy, zz]] = rot \n\n    k = [\n        [ xx + yy + zz,      zy - yz,      xz - zx,      yx - xy,],\n        [      zy - yz, xx - yy - zz,      xy + yx,      xz + zx,],\n        [      xz - zx,      xy + yx, yy - xx - zz,      yz + zy,],\n        [      yx - xy,      xz + zx,      yz + zy, zz - xx - yy,]\n    ]\n\n    k = (1./3.) * torch.stack([torch.stack(t, dim=-1) for t in k], dim=-2)\n    _, vectors = torch.linalg.eigh(k)\n    return vectors[..., -1]", "\n\"\"\" Graph functions \"\"\"\n\ndef autoregressive_mask(E_idx):\n    N_nodes = E_idx.size(1)\n    ii = torch.arange(N_nodes).cuda()\n    ii = ii.view((1, -1, 1))\n    mask = E_idx - ii < 0\n    return mask.float()\n", "\n# The following gather functions\ndef gather_edges(edges, neighbor_idx):\n    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n    edge_features = torch.gather(edges, 2, neighbors)\n    return edge_features\n\ndef gather_nodes(nodes, neighbor_idx):\n    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n    # Gather and re-pack\n    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n    return neighbor_features", "def gather_nodes(nodes, neighbor_idx):\n    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n    # Gather and re-pack\n    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n    return neighbor_features\n\ndef cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n    h_nodes = gather_nodes(h_nodes, E_idx)\n    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n    return h_nn", "\ndef cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n    h_nodes = gather_nodes(h_nodes, E_idx)\n    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n    return h_nn\n\n\ndef pairwise_distance(X, mask):\n    X_ca = X[:, :, 1, :]  # alpha carbon\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)\n    dX = X_ca.unsqueeze(1) - X_ca.unsqueeze(2)\n    D = mask_2D * torch.sqrt(torch.sum(dX**2, dim=3))\n    return D, mask_2D", ""]}
{"filename": "fairseq_models/modules/nnutils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom .protein_features import ProteinFeatures\nfrom .utils import *\nfrom fairseq_models.data.ab_dictionary import ALPHABET_FOR_INIT\n\nclass Normalize(nn.Module):\n\n    def __init__(self, features, epsilon=1e-6):\n        super(Normalize, self).__init__()\n        self.gain = nn.Parameter(torch.ones(features))\n        self.bias = nn.Parameter(torch.zeros(features))\n        self.epsilon = epsilon\n\n    def forward(self, x, dim=-1):\n        mu = x.mean(dim, keepdim=True)\n        sigma = torch.sqrt(x.var(dim, keepdim=True) + self.epsilon)\n        gain = self.gain\n        bias = self.bias\n        # Reshape\n        if dim != -1:\n            shape = [1] * len(mu.size())\n            shape[dim] = self.gain.size()[0]\n            gain = gain.view(shape)\n            bias = bias.view(shape)\n\n        return gain * (x - mu) / (sigma + self.epsilon) + bias", "\nclass Normalize(nn.Module):\n\n    def __init__(self, features, epsilon=1e-6):\n        super(Normalize, self).__init__()\n        self.gain = nn.Parameter(torch.ones(features))\n        self.bias = nn.Parameter(torch.zeros(features))\n        self.epsilon = epsilon\n\n    def forward(self, x, dim=-1):\n        mu = x.mean(dim, keepdim=True)\n        sigma = torch.sqrt(x.var(dim, keepdim=True) + self.epsilon)\n        gain = self.gain\n        bias = self.bias\n        # Reshape\n        if dim != -1:\n            shape = [1] * len(mu.size())\n            shape[dim] = self.gain.size()[0]\n            gain = gain.view(shape)\n            bias = bias.view(shape)\n\n        return gain * (x - mu) / (sigma + self.epsilon) + bias", "\n\nclass MPNNLayer(nn.Module):\n\n    def __init__(self, num_hidden, num_in, dropout):\n        super(MPNNLayer, self).__init__()\n        self.num_hidden = num_hidden\n        self.num_in = num_in\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.Identity() #Normalize(num_hidden)\n        self.W = nn.Sequential(\n                nn.Linear(num_hidden + num_in, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n        )\n\n    def forward(self, h_V, h_E, mask_attend):\n        # h_V: [B, N, H]; h_E: [B, N, K, H]\n        # mask_attend: [B, N, K]\n        h_V_expand = h_V.unsqueeze(-2).expand(-1, -1, h_E.size(-2), -1)\n        h_EV = torch.cat([h_V_expand, h_E], dim=-1)  # [B, N, K, H]\n        h_message = self.W(h_EV) * mask_attend.unsqueeze(-1)\n        dh = torch.mean(h_message, dim=-2)\n        h_V = self.norm(h_V + self.dropout(dh))\n        return h_V", "\n\nclass PosEmbedding(nn.Module):\n\n    def __init__(self, num_embeddings):\n        super(PosEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n\n    # E_idx: [B, N]\n    def forward(self, E_idx):\n        frequency = torch.exp(\n            torch.arange(0, self.num_embeddings, 2, dtype=torch.float32)\n            * -(np.log(10000.0) / self.num_embeddings)\n        ).cuda()\n        angles = E_idx.unsqueeze(-1) * frequency.view((1,1,-1))\n        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n        return E", "\n\nclass AAEmbedding(nn.Module):\n\n    def __init__(self):\n        super(AAEmbedding, self).__init__()\n        self.hydropathy = {'#': 0, \"I\":4.5, \"V\":4.2, \"L\":3.8, \"F\":2.8, \"C\":2.5, \"M\":1.9, \"A\":1.8, \"W\":-0.9, \"G\":-0.4, \"T\":-0.7, \"S\":-0.8, \"Y\":-1.3, \"P\":-1.6, \"H\":-3.2, \"N\":-3.5, \"D\":-3.5, \"Q\":-3.5, \"E\":-3.5, \"K\":-3.9, \"R\":-4.5}\n        self.volume = {'#': 0, \"G\":60.1, \"A\":88.6, \"S\":89.0, \"C\":108.5, \"D\":111.1, \"P\":112.7, \"N\":114.1, \"T\":116.1, \"E\":138.4, \"V\":140.0, \"Q\":143.8, \"H\":153.2, \"M\":162.9, \"I\":166.7, \"L\":166.7, \"K\":168.6, \"R\":173.4, \"F\":189.9, \"Y\":193.6, \"W\":227.8}\n        self.charge = {**{'R':1, 'K':1, 'D':-1, 'E':-1, 'H':0.1}, **{x:0 for x in 'ABCFGIJLMNOPQSTUVWXYZ#'}}\n        self.polarity = {**{x:1 for x in 'RNDQEHKSTY'}, **{x:0 for x in \"ACGILMFPWV#\"}}\n        self.acceptor = {**{x:1 for x in 'DENQHSTY'}, **{x:0 for x in \"RKWACGILMFPV#\"}}\n        self.donor = {**{x:1 for x in 'RKWNQHSTY'}, **{x:0 for x in \"DEACGILMFPV#\"}}\n\n        alphabet = ALPHABET_FOR_INIT\n\n        self.embedding = torch.tensor([\n            [self.hydropathy[alphabet[i]], self.volume[alphabet[i]] / 100, self.charge[alphabet[i]],\n            self.polarity[alphabet[i]], self.acceptor[alphabet[i]], self.donor[alphabet[i]]]\n            for i in range(len(alphabet))\n        ]).cuda()\n\n    def to_rbf(self, D, D_min, D_max, stride):\n        D_count = int((D_max - D_min) / stride)\n        D_mu = torch.linspace(D_min, D_max, D_count).cuda()\n        D_mu = D_mu.view(1,1,-1)  # [1, 1, K]\n        D_expand = torch.unsqueeze(D, -1)  # [B, N, 1]\n        return torch.exp(-((D_expand - D_mu) / stride) ** 2)\n\n    def transform(self, aa_vecs):\n        return torch.cat([\n            self.to_rbf(aa_vecs[:, :, 0], -4.5, 4.5, 0.1),\n            self.to_rbf(aa_vecs[:, :, 1], 0, 2.2, 0.1),\n            self.to_rbf(aa_vecs[:, :, 2], -1.0, 1.0, 0.25),\n            torch.sigmoid(aa_vecs[:, :, 3:] * 6 - 3),\n        ], dim=-1)\n\n    def dim(self):\n        return 90 + 22 + 8 + 3\n\n    def forward(self, x, raw=False):\n        B, N = x.size(0), x.size(1)\n        aa_vecs = self.embedding[x.view(-1)].view(B, N, -1)\n        rbf_vecs = self.transform(aa_vecs)\n        return aa_vecs if raw else rbf_vecs\n\n    def to_rbf_(self, D, D_min, D_max, stride):\n        D_count = int((D_max - D_min) / stride)\n        D_mu = torch.linspace(D_min, D_max, D_count).cuda()\n        D_mu = D_mu.view(1,-1)  # [1, 1, K]\n        D_expand = torch.unsqueeze(D, -1)  # [B, N, 1]\n        return torch.exp(-((D_expand - D_mu) / stride) ** 2)  \n    \n    def soft_forward(self, x):\n        aa_vecs = torch.matmul(x, self.embedding)\n        rbf_vecs = torch.cat([\n            self.to_rbf_(aa_vecs[:, 0], -4.5, 4.5, 0.1),\n            self.to_rbf_(aa_vecs[:, 1], 0, 2.2, 0.1),\n            self.to_rbf_(aa_vecs[:, 2], -1.0, 1.0, 0.25),\n            torch.sigmoid(aa_vecs[:, 3:] * 6 - 3),\n        ], dim=-1)\n        return rbf_vecs", "\n\nclass ABModel(nn.Module):\n\n    def __init__(self, args):\n        \n        super(ABModel, self).__init__()\n\n        self.k_neighbors = args.k_neighbors\n        self.hidden_size = args.hidden_size\n        self.embedding = AAEmbedding()\n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type='full',\n                direction='bidirectional'\n        )\n        \n        self.W_i = nn.Linear(args.hidden_size, args.hidden_size)\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n        self.mse_loss = nn.MSELoss(reduction='none')\n        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n\n    def select_target(self, tgt_X, tgt_h, tgt_A, tgt_pos):\n        max_len = max([len(pos) for pos in tgt_pos])\n        xlist = [tgt_X[i, pos] for i,pos in enumerate(tgt_pos)]\n        hlist = [tgt_h[i, pos] for i,pos in enumerate(tgt_pos)]\n        alist = [tgt_A[i, pos] for i,pos in enumerate(tgt_pos)]\n        tgt_X = [F.pad(x, (0,0,0,0,0,max_len-len(x))) for x in xlist]\n        tgt_h = [F.pad(h, (0,0,0,max_len-len(h))) for h in hlist]\n        tgt_A = [F.pad(a, (0,0,0,max_len-len(a))) for a in alist]\n        return torch.stack(tgt_X, dim=0), torch.stack(tgt_h, dim=0), torch.stack(tgt_A, dim=0)", ""]}
{"filename": "fairseq_models/modules/ab_transformer_sentence_encoder.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom fairseq.modules import (", "import torch.nn as nn\nfrom fairseq.modules import (\n    FairseqDropout,\n    LayerDropModuleList,\n    LayerNorm,\n    MultiheadAttention,\n    PositionalEmbedding,\n)\nfrom fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom fairseq.data import Dictionary", "from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_\nfrom fairseq.data import Dictionary\n\nfrom .ab_transformer_sentence_encoder_layer import TransformerSentenceEncoderLayer\n\ndef init_bert_params(module):\n    \"\"\"\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of\n           in_project_weight for MultiHeadAttention initialized using\n           the normal distribution (to be validated).\n    \"\"\"\n\n    if isinstance(module, nn.Linear):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    if isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.padding_idx is not None:\n            module.weight.data[module.padding_idx].zero_()\n    if isinstance(module, MultiheadAttention):\n        module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n        module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n        module.v_proj.weight.data.normal_(mean=0.0, std=0.02)", "\n\nclass AntibodyTransformerSentenceEncoder(nn.Module):\n    \"\"\"\n    Implementation for a Bi-directional Transformer based Sentence Encoder used\n    in BERT/XLM style pre-trained models.\n\n    This first computes the token embedding using the token embedding matrix,\n    position embeddings (if specified) and segment embeddings\n    (if specified). After applying the specified number of\n    TransformerEncoderLayers, it outputs all the internal states of the\n    encoder as well as the final representation associated with the first\n    token (usually CLS token).\n\n    Input:\n        - tokens: B x T matrix representing sentences\n        - segment_labels: B x T matrix representing segment label for tokens\n\n    Output:\n        - a tuple of the following:\n            - a list of internal model states used to compute the\n              predictions where each tensor has shape T x B x C\n            - sentence representation associated with first input token\n              in format B x C.\n    \"\"\"\n\n    def __init__(\n        self,\n        padding_idx: int,\n        vocab_size: int,\n        num_encoder_layers: int = 6,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        layerdrop: float = 0.0,\n        max_seq_len: int = 256,\n        segments_vocab: Dictionary = None,\n        use_position_embeddings: bool = True,\n        offset_positions_by_padding: bool = True,\n        encoder_normalize_before: bool = False,\n        apply_bert_init: bool = False,\n        activation_fn: str = \"relu\",\n        learned_pos_embedding: bool = True,\n        embed_scale: float = None,\n        freeze_embeddings: bool = False,\n        n_trans_layers_to_freeze: int = 0,\n        export: bool = False,\n        traceable: bool = False,\n        q_noise: float = 0.0,\n        qn_block_size: int = 8,\n    ) -> None:\n\n        super().__init__()\n        self.padding_idx = padding_idx\n        self.vocab_size = vocab_size\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n        self.layerdrop = layerdrop\n        self.max_seq_len = max_seq_len\n        self.embedding_dim = embedding_dim\n        self.num_segments = len(segments_vocab)\n        self.use_position_embeddings = use_position_embeddings\n        self.apply_bert_init = apply_bert_init\n        self.learned_pos_embedding = learned_pos_embedding\n        self.traceable = traceable\n        self.tpu = False  # whether we're on TPU\n\n        self.embed_tokens = self.build_embedding(\n            self.vocab_size, self.embedding_dim, self.padding_idx\n        )\n        self.embed_scale = embed_scale\n\n        if q_noise > 0:\n            self.quant_noise = apply_quant_noise_(\n                nn.Linear(self.embedding_dim, self.embedding_dim, bias=False),\n                q_noise,\n                qn_block_size,\n            )\n        else:\n            self.quant_noise = None\n\n        self.segment_embeddings = (\n            nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=segments_vocab.pad())\n            if self.num_segments > 0\n            else None\n        )\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                self.max_seq_len,\n                self.embedding_dim,\n                padding_idx=(self.padding_idx if offset_positions_by_padding else None),\n                learned=self.learned_pos_embedding,\n            )\n            if self.use_position_embeddings\n            else None\n        )\n\n        if self.layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n        self.layers.extend(\n            [\n                self.build_transformer_sentence_encoder_layer(\n                    embedding_dim=self.embedding_dim,\n                    ffn_embedding_dim=ffn_embedding_dim,\n                    num_attention_heads=num_attention_heads,\n                    dropout=self.dropout_module.p,\n                    attention_dropout=attention_dropout,\n                    activation_dropout=activation_dropout,\n                    activation_fn=activation_fn,\n                    export=export,\n                    q_noise=q_noise,\n                    qn_block_size=qn_block_size,\n                )\n                for _ in range(num_encoder_layers)\n            ]\n        )\n\n        if encoder_normalize_before:\n            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)\n        else:\n            self.emb_layer_norm = None\n\n        # Apply initialization of model params after building the model\n        if self.apply_bert_init:\n            self.apply(init_bert_params)\n\n        def freeze_module_params(m):\n            if m is not None:\n                for p in m.parameters():\n                    p.requires_grad = False\n\n        if freeze_embeddings:\n            freeze_module_params(self.embed_tokens)\n            freeze_module_params(self.segment_embeddings)\n            freeze_module_params(self.embed_positions)\n            freeze_module_params(self.emb_layer_norm)\n\n        for layer in range(n_trans_layers_to_freeze):\n            freeze_module_params(self.layers[layer])\n\n    def build_embedding(self, vocab_size, embedding_dim, padding_idx):\n        return nn.Embedding(vocab_size, embedding_dim, padding_idx)\n\n    def build_transformer_sentence_encoder_layer(\n        self,\n        embedding_dim,\n        ffn_embedding_dim,\n        num_attention_heads,\n        dropout,\n        attention_dropout,\n        activation_dropout,\n        activation_fn,\n        export,\n        q_noise,\n        qn_block_size,\n    ):\n        return TransformerSentenceEncoderLayer(\n            embedding_dim=embedding_dim,\n            ffn_embedding_dim=ffn_embedding_dim,\n            num_attention_heads=num_attention_heads,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            activation_dropout=activation_dropout,\n            activation_fn=activation_fn,\n            export=export,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n    def prepare_for_tpu_(self, **kwargs):\n        self.tpu = True\n\n    def forward(\n        self,\n        tokens: torch.Tensor,\n        segment_labels: torch.Tensor = None,\n        last_state_only: bool = False,\n        positions: Optional[torch.Tensor] = None,\n        token_embeddings: Optional[torch.Tensor] = None,\n        past_key_values=None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        # compute padding mask. This is needed for multi-head attention\n        padding_mask = tokens.eq(self.padding_idx)\n        if not self.traceable and not self.tpu and not padding_mask.any():\n            padding_mask = None\n\n        if token_embeddings is not None:\n            x = token_embeddings\n        else:\n            x = self.embed_tokens(tokens)\n\n        if self.embed_scale is not None:\n            x = x * self.embed_scale\n\n        if self.embed_positions is not None:\n            x = x + self.embed_positions(tokens, positions=positions)\n\n        if self.segment_embeddings is not None and segment_labels is not None:\n            x = x + self.segment_embeddings(segment_labels)\n\n        if self.quant_noise is not None:\n            x = self.quant_noise(x)\n\n        if self.emb_layer_norm is not None:\n            x = self.emb_layer_norm(x)\n\n        x = self.dropout_module(x)\n\n        # account for padding while computing the representation\n        if padding_mask is not None:\n            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n\n        # if past_key_values is not None:\n        #     prefix_attention_mask = torch.zeros(padding_mask.shape[0], 1).to(padding_mask.device)\n        #     padding_mask = torch.cat((prefix_attention_mask, padding_mask), dim=1)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        inner_states = []\n        if not last_state_only:\n            inner_states.append(x)\n\n        for i, layer in enumerate(self.layers):\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n            x, _ = layer(x, past_key_value=past_key_value)\n            if not last_state_only:\n                inner_states.append(x)\n\n        sentence_rep = x[0, :, :]\n\n        if last_state_only:\n            inner_states = [x]\n\n        if self.traceable:\n            return torch.stack(inner_states), sentence_rep\n        else:\n            return inner_states, sentence_rep", ""]}
{"filename": "fairseq_models/modules/ab_multihead_attention.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.incremental_decoding_utils import with_incremental_state\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\nfrom torch import Tensor, nn\nfrom torch.nn import Parameter\n\n", "\n\n@with_incremental_state\nclass MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        add_bias_kv=False,\n        add_zero_attn=False,\n        self_attention=False,\n        encoder_decoder_attention=False,\n        q_noise=0.0,\n        qn_block_size=8,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim ** -0.5\n\n        self.self_attention = self_attention\n        self.encoder_decoder_attention = encoder_decoder_attention\n\n        assert not self.self_attention or self.qkv_same_dim, (\n            \"Self-attention requires query, key and \" \"value to be of the same size\"\n        )\n\n        self.k_proj = quant_noise(\n            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.v_proj = quant_noise(\n            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n        self.q_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        self.out_proj = quant_noise(\n            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size\n        )\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self.reset_parameters()\n\n        self.onnx_trace = False\n        self.tpu = False\n\n    def prepare_for_onnx_export_(self):\n        self.onnx_trace = True\n\n    def prepare_for_tpu_(self, **kwargs):\n        self.tpu = True\n\n    def reset_parameters(self):\n        if self.qkv_same_dim:\n            # Empirically observed the convergence to be much better with\n            # the scaled initialization\n            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n        else:\n            nn.init.xavier_uniform_(self.k_proj.weight)\n            nn.init.xavier_uniform_(self.v_proj.weight)\n            nn.init.xavier_uniform_(self.q_proj.weight)\n\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.0)\n        if self.bias_k is not None:\n            nn.init.xavier_normal_(self.bias_k)\n        if self.bias_v is not None:\n            nn.init.xavier_normal_(self.bias_v)\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        need_weights: bool = True,\n        static_kv: bool = False,\n        attn_mask: Optional[Tensor] = None,\n        before_softmax: bool = False,\n        need_head_weights: bool = False,\n        past_key_value: Optional[tuple] = None,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            need_weights (bool, optional): return the attention weights,\n                averaged over heads (default: False).\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n            before_softmax (bool, optional): return the raw attention\n                weights and values before the attention softmax.\n            need_head_weights (bool, optional): return the attention\n                weights for each head. Implies *need_weights*. Default:\n                return the average attention weights over all heads.\n        \"\"\"\n        if need_head_weights:\n            need_weights = True\n\n        tgt_len, bsz, embed_dim = query.size()\n        # print(query.shape)\n        # print(attn_mask)\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\n        if (\n            not self.onnx_trace\n            and not self.tpu  # don't use PyTorch version on TPUs\n            and incremental_state is None\n            and not static_kv\n            # A workaround for quantization to work. Otherwise JIT compilation\n            # treats bias in linear module as method.\n            and not torch.jit.is_scripting()\n            and past_key_value is None\n        ):\n            assert key is not None and value is not None\n            return F.multi_head_attention_forward(\n                query,\n                key,\n                value,\n                self.embed_dim,\n                self.num_heads,\n                torch.empty([0]),\n                torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n                self.bias_k,\n                self.bias_v,\n                self.add_zero_attn,\n                self.dropout_module.p,\n                self.out_proj.weight,\n                self.out_proj.bias,\n                self.training or self.dropout_module.apply_during_inference,\n                key_padding_mask,\n                need_weights,\n                attn_mask,\n                use_separate_proj_weight=True,\n                q_proj_weight=self.q_proj.weight,\n                k_proj_weight=self.k_proj.weight,\n                v_proj_weight=self.v_proj.weight,\n            )\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n            if saved_state is not None and \"prev_key\" in saved_state:\n                # previous time steps are cached - no need to recompute\n                # key and value if they are static\n                if static_kv:\n                    assert self.encoder_decoder_attention and not self.self_attention\n                    key = value = None\n        else:\n            saved_state = None\n\n        if self.self_attention:\n            q = self.q_proj(query)\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n        elif self.encoder_decoder_attention:\n            # encoder-decoder attention\n            q = self.q_proj(query)\n            if key is None:\n                assert value is None\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n\n        else:\n            assert key is not None and value is not None\n            q = self.q_proj(query)\n            k = self.k_proj(key)\n            v = self.v_proj(value)\n        q *= self.scaling\n\n        if self.bias_k is not None:\n            assert self.bias_v is not None\n            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        key_padding_mask.new_zeros(key_padding_mask.size(0), 1),\n                    ],\n                    dim=1,\n                )\n\n        q = (\n            q.contiguous()\n            .view(tgt_len, bsz * self.num_heads, self.head_dim)\n            .transpose(0, 1)\n        )\n        if k is not None:\n            k = (\n                k.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n        if v is not None:\n            v = (\n                v.contiguous()\n                .view(-1, bsz * self.num_heads, self.head_dim)\n                .transpose(0, 1)\n            )\n\n        if past_key_value is not None:\n            # print(q.shape)\n            # print(past_key_value[0].contiguous().view(bsz * self.num_heads, -1, self.head_dim).shape)\n            # print(past_key_value[0].shape) # torch.Size([8, 12, 1, 64])\n            # # print(past_key_value[1].shape)\n            # print(k.shape) # 96, 232, 64\n            # # print(v.shape)\n            # # (key, value) n_layer x batch_size x n_head x pre_seq_len x n_embd\n            k = torch.cat([past_key_value[0].contiguous().view(bsz * self.num_heads, -1, self.head_dim), k], dim=1)\n            v = torch.cat([past_key_value[1].contiguous().view(bsz * self.num_heads, -1, self.head_dim), v], dim=1)\n        \n\n        if saved_state is not None:\n            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n            if \"prev_key\" in saved_state:\n                _prev_key = saved_state[\"prev_key\"]\n                assert _prev_key is not None\n                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    k = prev_key\n                else:\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n            if \"prev_value\" in saved_state:\n                _prev_value = saved_state[\"prev_value\"]\n                assert _prev_value is not None\n                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n                if static_kv:\n                    v = prev_value\n                else:\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n            prev_key_padding_mask: Optional[Tensor] = None\n            if \"prev_key_padding_mask\" in saved_state:\n                prev_key_padding_mask = saved_state[\"prev_key_padding_mask\"]\n            assert k is not None and v is not None\n            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(\n                key_padding_mask=key_padding_mask,\n                prev_key_padding_mask=prev_key_padding_mask,\n                batch_size=bsz,\n                src_len=k.size(1),\n                static_kv=static_kv,\n            )\n\n            saved_state[\"prev_key\"] = k.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_value\"] = v.view(bsz, self.num_heads, -1, self.head_dim)\n            saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n            # In this branch incremental_state is never None\n            assert incremental_state is not None\n            incremental_state = self._set_input_buffer(incremental_state, saved_state)\n        assert k is not None\n        src_len = k.size(1)\n\n        # This is part of a workaround to get around fork/join parallelism\n        # not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n\n        if key_padding_mask is not None:\n            assert key_padding_mask.size(0) == bsz\n            assert key_padding_mask.size(1) == src_len\n\n        if self.add_zero_attn:\n            assert v is not None\n            src_len += 1\n            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n            if attn_mask is not None:\n                attn_mask = torch.cat(\n                    [attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1\n                )\n            if key_padding_mask is not None:\n                key_padding_mask = torch.cat(\n                    [\n                        key_padding_mask,\n                        torch.zeros(key_padding_mask.size(0), 1).type_as(\n                            key_padding_mask\n                        ),\n                    ],\n                    dim=1,\n                )\n\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)\n\n        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n\n        if attn_mask is not None:\n            attn_mask = attn_mask.unsqueeze(0)\n            if self.onnx_trace:\n                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n            attn_weights += attn_mask\n\n        if key_padding_mask is not None:\n            # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            if not self.tpu:\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n            else:\n                attn_weights = attn_weights.transpose(0, 2)\n                attn_weights = attn_weights.masked_fill(key_padding_mask, float(\"-inf\"))\n                attn_weights = attn_weights.transpose(0, 2)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if before_softmax:\n            return attn_weights, v\n\n        attn_weights_float = utils.softmax(\n            attn_weights, dim=-1, onnx_trace=self.onnx_trace\n        )\n        attn_weights = attn_weights_float.type_as(attn_weights)\n        attn_probs = self.dropout_module(attn_weights)\n\n        assert v is not None\n        attn = torch.bmm(attn_probs, v)\n        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n        if self.onnx_trace and attn.size(1) == 1:\n            # when ONNX tracing a single decoder step (sequence length == 1)\n            # the transpose is a no-op copy before view, thus unnecessary\n            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n        else:\n            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n        attn = self.out_proj(attn)\n        attn_weights: Optional[Tensor] = None\n        if need_weights:\n            attn_weights = attn_weights_float.view(\n                bsz, self.num_heads, tgt_len, src_len\n            ).transpose(1, 0)\n            if not need_head_weights:\n                # average attention weights over heads\n                attn_weights = attn_weights.mean(dim=0)\n\n        return attn, attn_weights\n\n    @staticmethod\n    def _append_prev_key_padding_mask(\n        key_padding_mask: Optional[Tensor],\n        prev_key_padding_mask: Optional[Tensor],\n        batch_size: int,\n        src_len: int,\n        static_kv: bool,\n    ) -> Optional[Tensor]:\n        # saved key padding masks have shape (bsz, seq_len)\n        if prev_key_padding_mask is not None and static_kv:\n            new_key_padding_mask = prev_key_padding_mask\n        elif prev_key_padding_mask is not None and key_padding_mask is not None:\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), key_padding_mask.float()], dim=1\n            )\n        # During incremental decoding, as the padding token enters and\n        # leaves the frame, there will be a time when prev or current\n        # is None\n        elif prev_key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - prev_key_padding_mask.size(1)),\n                device=prev_key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [prev_key_padding_mask.float(), filler.float()], dim=1\n            )\n        elif key_padding_mask is not None:\n            filler = torch.zeros(\n                (batch_size, src_len - key_padding_mask.size(1)),\n                device=key_padding_mask.device,\n            )\n            new_key_padding_mask = torch.cat(\n                [filler.float(), key_padding_mask.float()], dim=1\n            )\n        else:\n            new_key_padding_mask = prev_key_padding_mask\n        return new_key_padding_mask\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n        input_buffer = self._get_input_buffer(incremental_state)\n        if input_buffer is not None:\n            for k in input_buffer.keys():\n                input_buffer_k = input_buffer[k]\n                if input_buffer_k is not None:\n                    if self.encoder_decoder_attention and input_buffer_k.size(\n                        0\n                    ) == new_order.size(0):\n                        break\n                    input_buffer[k] = input_buffer_k.index_select(0, new_order)\n            incremental_state = self._set_input_buffer(incremental_state, input_buffer)\n        return incremental_state\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):\n        return attn_weights\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        prefix = name + \".\" if name != \"\" else \"\"\n        items_to_add = {}\n        keys_to_remove = []\n        for k in state_dict.keys():\n            if k.endswith(prefix + \"in_proj_weight\"):\n                # in_proj_weight used to be q + k + v with same dimensions\n                dim = int(state_dict[k].shape[0] / 3)\n                items_to_add[prefix + \"q_proj.weight\"] = state_dict[k][:dim]\n                items_to_add[prefix + \"k_proj.weight\"] = state_dict[k][dim : 2 * dim]\n                items_to_add[prefix + \"v_proj.weight\"] = state_dict[k][2 * dim :]\n\n                keys_to_remove.append(k)\n\n                k_bias = prefix + \"in_proj_bias\"\n                if k_bias in state_dict.keys():\n                    dim = int(state_dict[k].shape[0] / 3)\n                    items_to_add[prefix + \"q_proj.bias\"] = state_dict[k_bias][:dim]\n                    items_to_add[prefix + \"k_proj.bias\"] = state_dict[k_bias][\n                        dim : 2 * dim\n                    ]\n                    items_to_add[prefix + \"v_proj.bias\"] = state_dict[k_bias][2 * dim :]\n\n                    keys_to_remove.append(prefix + \"in_proj_bias\")\n\n        for k in keys_to_remove:\n            del state_dict[k]\n\n        for key, value in items_to_add.items():\n            state_dict[key] = value", ""]}
{"filename": "fairseq_models/modules/ab_transformer_sentence_encoder_layer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Callable, Optional\n\nimport torch\nimport torch.nn as nn\nfrom fairseq import utils", "import torch.nn as nn\nfrom fairseq import utils\nfrom fairseq.modules import LayerNorm\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.quant_noise import quant_noise\n\nfrom .ab_multihead_attention import MultiheadAttention\n\nclass TransformerSentenceEncoderLayer(nn.Module):\n    \"\"\"\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        activation_fn: str = \"relu\",\n        export: bool = False,\n        q_noise: float = 0.0,\n        qn_block_size: int = 8,\n        init_fn: Callable = None,\n    ) -> None:\n        super().__init__()\n\n        if init_fn is not None:\n            init_fn()\n\n        # Initialize parameters\n        self.embedding_dim = embedding_dim\n        self.num_attention_heads = num_attention_heads\n        self.attention_dropout = attention_dropout\n        self.q_noise = q_noise\n        self.qn_block_size = qn_block_size\n\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n        self.activation_dropout_module = FairseqDropout(\n            activation_dropout, module_name=self.__class__.__name__\n        )\n\n        # Initialize blocks\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.self_attn = self.build_self_attention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n        self.fc1 = self.build_fc1(\n            self.embedding_dim,\n            ffn_embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            ffn_embedding_dim,\n            self.embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(\n        self,\n        embed_dim,\n        num_attention_heads,\n        dropout,\n        self_attention,\n        q_noise,\n        qn_block_size,\n    ):\n        return MultiheadAttention(\n            embed_dim,\n            num_attention_heads,\n            dropout=dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        past_key_value=None,\n    ):\n        \"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer implementation.\n        \"\"\"\n        residual = x\n        x, attn = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=self_attn_padding_mask,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n            past_key_value=past_key_value\n        )\n        x = self.dropout_module(x)\n        x = residual + x\n        x = self.self_attn_layer_norm(x)\n\n        residual = x\n        x = self.activation_fn(self.fc1(x))\n        # print(self.fc1.weight)\n        # exit(0)\n        x = self.activation_dropout_module(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        x = residual + x\n        x = self.final_layer_norm(x)\n        return x, attn", "class TransformerSentenceEncoderLayer(nn.Module):\n    \"\"\"\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int = 768,\n        ffn_embedding_dim: int = 3072,\n        num_attention_heads: int = 8,\n        dropout: float = 0.1,\n        attention_dropout: float = 0.1,\n        activation_dropout: float = 0.1,\n        activation_fn: str = \"relu\",\n        export: bool = False,\n        q_noise: float = 0.0,\n        qn_block_size: int = 8,\n        init_fn: Callable = None,\n    ) -> None:\n        super().__init__()\n\n        if init_fn is not None:\n            init_fn()\n\n        # Initialize parameters\n        self.embedding_dim = embedding_dim\n        self.num_attention_heads = num_attention_heads\n        self.attention_dropout = attention_dropout\n        self.q_noise = q_noise\n        self.qn_block_size = qn_block_size\n\n        self.dropout_module = FairseqDropout(\n            dropout, module_name=self.__class__.__name__\n        )\n        self.activation_dropout_module = FairseqDropout(\n            activation_dropout, module_name=self.__class__.__name__\n        )\n\n        # Initialize blocks\n        self.activation_fn = utils.get_activation_fn(activation_fn)\n        self.self_attn = self.build_self_attention(\n            self.embedding_dim,\n            num_attention_heads,\n            dropout=attention_dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the self attention layer\n        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n        self.fc1 = self.build_fc1(\n            self.embedding_dim,\n            ffn_embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n        self.fc2 = self.build_fc2(\n            ffn_embedding_dim,\n            self.embedding_dim,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n        # layer norm associated with the position wise feed-forward NN\n        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n\n    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):\n        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)\n\n    def build_self_attention(\n        self,\n        embed_dim,\n        num_attention_heads,\n        dropout,\n        self_attention,\n        q_noise,\n        qn_block_size,\n    ):\n        return MultiheadAttention(\n            embed_dim,\n            num_attention_heads,\n            dropout=dropout,\n            self_attention=True,\n            q_noise=q_noise,\n            qn_block_size=qn_block_size,\n        )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        past_key_value=None,\n    ):\n        \"\"\"\n        LayerNorm is applied either before or after the self-attention/ffn\n        modules similar to the original Transformer implementation.\n        \"\"\"\n        residual = x\n        x, attn = self.self_attn(\n            query=x,\n            key=x,\n            value=x,\n            key_padding_mask=self_attn_padding_mask,\n            need_weights=False,\n            attn_mask=self_attn_mask,\n            past_key_value=past_key_value\n        )\n        x = self.dropout_module(x)\n        x = residual + x\n        x = self.self_attn_layer_norm(x)\n\n        residual = x\n        x = self.activation_fn(self.fc1(x))\n        # print(self.fc1.weight)\n        # exit(0)\n        x = self.activation_dropout_module(x)\n        x = self.fc2(x)\n        x = self.dropout_module(x)\n        x = residual + x\n        x = self.final_layer_norm(x)\n        return x, attn", ""]}
{"filename": "fairseq_models/modules/hmpn_encoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom .utils import *\nfrom .nnutils import *\nfrom .protein_features import ProteinFeatures\n", "from .protein_features import ProteinFeatures\n\n\nclass EGNNEncoder(nn.Module):\n    \n    def __init__(self, args, node_hdim=0, features_type='backbone', update_X=True):\n        super(EGNNEncoder, self).__init__()\n        self.update_X = update_X\n        self.features_type = features_type\n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type=features_type,\n                direction='bidirectional'\n        )\n        self.node_in, self.edge_in = self.features.feature_dimensions[features_type]\n        self.node_in += node_hdim\n        \n        self.W_v = nn.Linear(self.node_in, args.hidden_size)\n        self.W_e = nn.Linear(self.edge_in, args.hidden_size)\n        self.layers = nn.ModuleList([\n                MPNNLayer(args.hidden_size, args.hidden_size * 3, dropout=args.dropout)\n                for _ in range(args.depth)\n        ])\n        if self.update_X:\n            self.W_x = nn.Linear(args.hidden_size, args.hidden_size)\n            self.U_x = nn.Linear(args.hidden_size, args.hidden_size)\n            self.T_x = nn.Sequential(nn.ReLU(), nn.Linear(args.hidden_size, 14))\n\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    # [backbone] X: [B,N,L,3], V/S: [B,N,H], A: [B,N,L]\n    # [atom] X: [B,N*L,3], V/S: [B,N*L,H], A: [B,N*L]\n    def forward(self, X, V, S, A):\n        mask = A.clamp(max=1).float() # if position in A has value => Set to 1 = True\n        vmask = mask[:,:,1] if self.features_type == 'backbone' else mask\n        _, E, E_idx = self.features(X, vmask)\n        # E = torch.ones(X.shape[0], X.shape[1], 9, self.features.feature_dimensions[self.features_type][1]).to(X.device)\n        # E_idx = torch.ones(X.shape[0], X.shape[1], 9).to(X.device).type(torch.int64)\n\n        h = self.W_v(V)    # [B, N, H] \n        h_e = self.W_e(E)  # [B, N, K, H] \n        nei_s = gather_nodes(S, E_idx)  # [B, N, K, H]\n        emask = gather_nodes(vmask[...,None], E_idx).squeeze(-1)\n\n        # message passing\n        for layer in self.layers:\n            nei_v = gather_nodes(h, E_idx)  # [B, N, K, H]\n            nei_h = torch.cat([nei_v, nei_s, h_e], dim=-1) # [B, N, K, H*3]\n            h = layer(h, nei_h, mask_attend=emask)  # [B, N, H]\n            h = h * vmask.unsqueeze(-1)  # [B, N, H]\n\n        if self.update_X and self.features_type == 'backbone':\n            ca_mask = mask[:,:,1]  # [B, N]\n            mij = self.W_x(h).unsqueeze(2) + self.U_x(h).unsqueeze(1)  # [B,N,N,H]\n            xij = X.unsqueeze(2) - X.unsqueeze(1)  # [B,N,N,L,3]\n            xij = xij * self.T_x(mij).unsqueeze(-1)  # [B,N,N,L,3]\n            f = torch.sum(xij * ca_mask[:,None,:,None,None], dim=2)  # [B,N,N,L,3] * [B,1,N,1,1]\n            f = f / (1e-6 + ca_mask.sum(dim=1)[:,None,None,None])    # [B,N,L,3] / [B,1,1,1]\n            X = X + f.clamp(min=-20.0, max=20.0)\n\n        return h, X * mask[...,None]", "\n\nclass HierEGNNEncoder(nn.Module):\n\n    def __init__(self, args, update_X=True, backbone_CA_only=True):\n        super(HierEGNNEncoder, self).__init__()\n        self.args = args\n        self.update_X = update_X\n        self.backbone_CA_only = backbone_CA_only\n        self.clash_step = args.clash_step\n        self.residue_mpn = EGNNEncoder(\n                args, features_type='backbone',\n                node_hdim=args.hidden_size,\n                update_X=False,\n        )\n        self.atom_mpn = EGNNEncoder(\n                args, features_type='atom',\n                node_hdim=args.hidden_size,\n                update_X=False,\n        )\n        if self.update_X:\n            # backbone coord update\n            self.W_x = nn.Linear(args.hidden_size, args.hidden_size)\n            self.U_x = nn.Linear(args.hidden_size, args.hidden_size)\n            self.T_x = nn.Sequential(nn.ReLU(), nn.Linear(args.hidden_size, 4))\n            # side chain coord update\n            self.W_a = nn.Linear(args.hidden_size, args.hidden_size)\n            self.U_a = nn.Linear(args.hidden_size, args.hidden_size)\n            self.T_a = nn.Sequential(nn.ReLU(), nn.Linear(args.hidden_size, 1))\n\n        self.embedding = nn.Embedding(38, args.hidden_size) # 38: len(ATOM_TYPES)\n\n\n        self.W_fusion = nn.Sequential(\n            nn.Linear(args.hidden_size + args.encoder_embed_dim, args.hidden_size),\n            nn.ReLU(),\n            nn.Linear(args.hidden_size, args.hidden_size)\n        )\n\n        # self.node_in = 6\n        # self.node_in += args.hidden_size\n        # self.LLiner = nn.Linear(self.node_in, args.hidden_size)\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    # X: [B,N,L,3], V: [B,N,6], S: [B,N,H], A: [B,N,L]\n    def forward(self, X, V, S, A, antibody_N=0, paratope_mask=None, pretrained_embedding=None):\n        B, N, L = X.size()[:3]\n        X_atom = X.view(B, N*L, 3)\n        mask = A.clamp(max=1).float()\n\n        # atom message passing\n        h_atom = self.embedding(A).view(B, N*L, -1)\n        h_atom, _ = self.atom_mpn(X_atom, h_atom, h_atom, A.view(B,-1))\n        h_atom = h_atom.view(B,N,L,-1)\n        h_atom = h_atom * mask[...,None]\n        h_A = h_atom.sum(dim=-2) / (1e-6 + mask.sum(dim=-1)[...,None])\n\n        # residue message passing\n        h_V = torch.cat([V, h_A], dim=-1)\n        # h_res = self.LLiner(h_V)\n        h_res, _ = self.residue_mpn(X, h_V, S, A)\n\n        # pretrained embedding fusion\n        if antibody_N > 0:\n            new_item = h_res[:, :antibody_N].clone()\n            new_item[paratope_mask] = self.W_fusion(\n                torch.cat((h_res[:, :antibody_N][paratope_mask], pretrained_embedding), dim=1)\n            )\n            h_res[:, :antibody_N] = new_item\n\n\n        if self.update_X:\n            # backbone update\n            bb_mask = mask[:,:,:4]  # [B, N, 4]\n            X_bb = X[:,:,:4]  # backbone atoms\n            mij = self.W_x(h_res).unsqueeze(2) + self.U_x(h_res).unsqueeze(1)  # [B,N,N,H]\n            xij = X_bb.unsqueeze(2) - X_bb.unsqueeze(1)  # [B,N,N,4,3]\n            dij = xij.norm(dim=-1)  # [B,N,N,4]\n            fij = torch.maximum(self.T_x(mij), 3.8 - dij)  # break term [B,N,N,4]\n            xij = xij * fij.unsqueeze(-1)\n            f_res = torch.sum(xij * bb_mask[:,None,:,:,None], dim=2)  # [B,N,N,4,3] * [B,1,N,4,1] -> [B,N,4,3]\n            f_res = f_res / (1e-6 + bb_mask.sum(dim=1, keepdims=True)[...,None])  # [B,N,4,3]\n            X_bb = X_bb + f_res.clamp(min=-20.0, max=20.0)\n\n            # Clash correction\n            for _ in range(self.clash_step):\n                xij = X_bb.unsqueeze(2) - X_bb.unsqueeze(1)  # [B,N,N,4,3]\n                dij = xij.norm(dim=-1)  # [B,N,N,4]\n                fij = F.relu(3.8 - dij)  # repulsion term [B,N,N,4]\n                xij = xij * fij.unsqueeze(-1)\n                f_res = torch.sum(xij * bb_mask[:,None,:,:,None], dim=2)  # [B,N,N,4,3] * [B,1,N,4,1] -> [B,N,4,3]\n                f_res = f_res / (1e-6 + bb_mask.sum(dim=1, keepdims=True)[...,None])  # [B,N,4,3]\n                X_bb = X_bb + f_res.clamp(min=-20.0, max=20.0)\n\n            # side chain update\n            mij = self.W_a(h_atom).unsqueeze(3) + self.U_a(h_atom).unsqueeze(2)  # [B,N,L,1,H] + [B,N,1,L,H]\n            xij = X.unsqueeze(3) - X.unsqueeze(2)  # [B,N,L,1,3] - [B,N,1,L,3]\n            dij = xij.norm(dim=-1)  # [B,N,L,L]\n            fij = torch.maximum(self.T_a(mij).squeeze(-1), 1.5 - dij)  # break term [B,N,L,L]\n            xij = xij * fij.unsqueeze(-1)  # [B,N,L,L,3]\n            f_atom = torch.sum(xij * mask[:,:,None,:,None], dim=3)  # [B,N,L,L,3] * [B,N,1,L,1] -> [B,N,L,3]\n            X_sc = X + 0.1 * f_atom\n\n            if self.backbone_CA_only:\n                X = torch.cat((X_sc[:,:,:1], X_bb[:,:,1:2], X_sc[:,:,2:]), dim=2)\n            else:\n                X = torch.cat((X_bb[:,:,:4], X_sc[:,:,4:]), dim=2)\n\n        return h_res, X * mask[...,None]", ""]}
{"filename": "fairseq_models/tasks/abbert_mlm_task.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\n\nimport numpy as np\nfrom fairseq import utils", "import numpy as np\nfrom fairseq import utils\nfrom fairseq.data import (\n    Dictionary,\n    IdDataset,\n    NestedDictionaryDataset,\n    NumelDataset,\n    NumSamplesDataset,\n    PrependTokenDataset,\n    RightPadDataset,", "    PrependTokenDataset,\n    RightPadDataset,\n    SortDataset,\n    TokenBlockDataset,\n    data_utils,\n)\nfrom fairseq.data.shorten_dataset import maybe_shorten_dataset\nfrom fairseq.tasks import LegacyFairseqTask, register_task\n\nfrom fairseq_models.tasks.abbert_mask_tokens_dataset import AntibodyMaskTokensDataset", "\nfrom fairseq_models.tasks.abbert_mask_tokens_dataset import AntibodyMaskTokensDataset\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\"antibody_masked_lm\")\nclass AntibodyMaskedLMTask(LegacyFairseqTask):\n    \"\"\"Task for training masked language models (e.g., BERT, RoBERTa).\"\"\"\n\n    @staticmethod\n    def add_args(parser):\n        \"\"\"Add task-specific arguments to the parser.\"\"\"\n        # data: seq data-bin\n        parser.add_argument(\n            \"data\",\n            help=\"colon separated path to data directories list, \\\n                            will be iterated upon during epochs in round-robin manner\",\n        )\n        parser.add_argument(\n            \"--tag-data\", \n            help='name of the tag data'\n        )\n        parser.add_argument(\n            \"--sample-break-mode\",\n            default=\"complete\",\n            choices=[\"none\", \"complete\", \"complete_doc\", \"eos\"],\n            help='If omitted or \"none\", fills each sample with tokens-per-sample '\n            'tokens. If set to \"complete\", splits samples only at the end '\n            \"of sentence, but may include multiple sentences per sample. \"\n            '\"complete_doc\" is similar but respects doc boundaries. '\n            'If set to \"eos\", includes only one sentence per sample.',\n        )\n        parser.add_argument(\n            \"--tokens-per-sample\",\n            default=512,\n            type=int,\n            help=\"max number of total tokens over all segments \"\n            \"per sample for BERT dataset\",\n        )\n        parser.add_argument(\n            \"--mask-prob\",\n            default=0.15,\n            type=float,\n            help=\"probability of replacing a token with mask\",\n        )\n        parser.add_argument(\n            \"--leave-unmasked-prob\",\n            default=0.1,\n            type=float,\n            help=\"probability that a masked token is unmasked\",\n        )\n        parser.add_argument(\n            \"--random-token-prob\",\n            default=0.1,\n            type=float,\n            help=\"probability of replacing a token with a random token\",\n        )\n        parser.add_argument(\n            \"--freq-weighted-replacement\",\n            default=False,\n            action=\"store_true\",\n            help=\"sample random replacement words based on word frequencies\",\n        )\n\n        # parser.add_argument(\n        #     \"--mask-aa-pieces\",\n        #     default=False,\n        #     action=\"store_true\",\n        #     help=\"mask whole A.A. pieces\",\n        # )\n\n        parser.add_argument(\n            \"--mask-multiple-length\",\n            default=1,\n            type=int,\n            help=\"repeat the mask indices multiple times\",\n        )\n        parser.add_argument(\n            \"--mask-stdev\", default=0.0, type=float, help=\"stdev of the mask length\"\n        )\n        parser.add_argument(\n            \"--shorten-method\",\n            default=\"none\",\n            choices=[\"none\", \"truncate\", \"random_crop\"],\n            help=\"if not none, shorten sequences that exceed --tokens-per-sample\",\n        )\n        parser.add_argument(\n            \"--shorten-data-split-list\",\n            default=\"\",\n            help=\"comma-separated list of dataset splits to apply shortening to, \"\n            'e.g., \"train,valid\" (default: all dataset splits)',\n        )\n\n        # parser.add_argument(\n        #     \"--sentencepiece-model-path\",\n        #     type=str,\n        #     default=None,\n        #     help=\"path to the used sentencepiece tokenizer\",\n        # )\n\n\n    def __init__(self, args, seq_dict, tag_dict):\n        super().__init__(args)\n        self.seq_dict = seq_dict\n        self.tag_dict = tag_dict\n        self.seed = args.seed\n\n        # add mask token\n        self.mask_idx = seq_dict.add_symbol(\"<mask>\")\n        # self.mask_idx = tag_dict.add_symbol('<mask>')\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        seq_dict = Dictionary.load(os.path.join(args.data, 'dict.txt'))\n        tag_dict = Dictionary.load(os.path.join(args.tag_data, 'dict.txt'))\n\n        logger.info(\"[input] dictionary: {} types\".format(len(seq_dict)))\n        logger.info(\"[input] dictionary: {} types\".format(len(tag_dict)))\n\n        # AAPieceFromUniprot21.set_model_path(args.sentencepiece_model_path)\n        return cls(args, seq_dict, tag_dict)\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"\n        \n        def curate_dataset(dataset_path, dictionary):\n            paths = utils.split_paths(dataset_path)\n            assert len(paths) > 0\n            data_path = paths[(epoch - 1) % len(paths)]\n            split_path = os.path.join(data_path, split)\n\n            dataset = data_utils.load_indexed_dataset(\n                split_path,\n                dictionary,\n                self.args.dataset_impl,\n                combine=combine,\n            )\n            if dataset is None:\n                raise FileNotFoundError(\n                    \"Dataset not found: {} ({})\".format(split, split_path)\n                )\n\n\n            dataset = maybe_shorten_dataset(\n                dataset,\n                split,\n                self.args.shorten_data_split_list,\n                self.args.shorten_method,\n                self.args.tokens_per_sample,\n                self.args.seed,\n            )\n\n            # create continuous blocks of tokens\n            dataset = TokenBlockDataset(\n                dataset,\n                dataset.sizes,\n                self.args.tokens_per_sample - 1,  # one less for <s>\n                pad=dictionary.pad(),\n                eos=dictionary.eos(),\n                break_mode=self.args.sample_break_mode,\n            )\n            logger.info(\"loaded {} blocks from: {}\".format(len(dataset), split_path))\n\n            # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n            dataset = PrependTokenDataset(dataset, dictionary.bos())\n\n            return dataset\n\n        seq_dataset = curate_dataset(self.args.data, self.source_dictionary)\n        tag_dataset = curate_dataset(self.args.tag_data, self.tag_source_dictionary)\n        # print(max([len(seq_dataset[i]) for i in range(900)]))\n        # print(tag_dataset[5])\n        # exit(0)\n\n        # aa_piece_dataset=AAPieceDataset(dataset,enabled=self.args.mask_aa_pieces)\n\n        # homology_dataset=HomologyDataset(dataset,enabled=self.args.contrastive_learning,negative_sample=(self.args.criterion=='masked_lm_with_msa'))\n\n        # create masked input and targets\n\n        src_dataset, tgt_dataset = AntibodyMaskTokensDataset.apply_mask(\n            seq_dataset,\n            tag_dataset,\n            self.source_dictionary,\n            self.tag_source_dictionary,\n            pad_idx=self.source_dictionary.pad(),\n            mask_idx=self.mask_idx,\n            seed=self.args.seed,\n            mask_prob=self.args.mask_prob,\n            leave_unmasked_prob=self.args.leave_unmasked_prob,\n            random_token_prob=self.args.random_token_prob,\n            # random_token_prob=0,     ###   self.args.random_token_prob,   \u4e0d\u968f\u673a\u66ff\u6362token, for BPE training\n            freq_weighted_replacement=self.args.freq_weighted_replacement,\n\n            # mask_aa_pieces=self.args.mask_aa_pieces,\n            # aa_piece_dataset=aa_piece_dataset,\n\n            mask_multiple_length=self.args.mask_multiple_length,\n            mask_stdev=self.args.mask_stdev,\n        )\n\n        with data_utils.numpy_seed(self.args.seed):\n            shuffle = np.random.permutation(len(src_dataset))\n\n        self.datasets[split] = SortDataset(\n            NestedDictionaryDataset(\n                {\n                    \"id\": IdDataset(),\n                    \"net_input0\": {\n                        \"src_tokens\": RightPadDataset(\n                            src_dataset,\n                            pad_idx=self.source_dictionary.pad(),\n                        ),\n                        \"src_lengths\": NumelDataset(src_dataset, reduce=False),\n                    },\n                    \"net_input1\": {\n                        \"src_tokens\": RightPadDataset(\n                            tag_dataset,\n                            pad_idx=self.tag_source_dictionary.pad(),\n                        ),\n                        \"src_lengths\": NumelDataset(src_dataset, reduce=False),\n                    },\n                    \"target\": RightPadDataset(\n                        tgt_dataset,\n                        pad_idx=self.source_dictionary.pad(),\n                    ),\n                    \"nsentences\": NumSamplesDataset(),\n                    \"ntokens\": NumelDataset(src_dataset, reduce=True),\n                },\n                sizes=[src_dataset.sizes],\n            ),\n            sort_order=[\n                shuffle,\n                src_dataset.sizes,\n            ],\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, sort=True):\n        src_dataset=TokenBlockDataset(\n                src_tokens,\n                src_lengths,\n                self.args.tokens_per_sample - 1,  # one less for <s>\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=\"eos\",\n            )\n\n        src_dataset = PrependTokenDataset(src_dataset, self.source_dictionary.bos())\n\n        # aa_piece_dataset = AAPieceDataset(src_dataset,enabled=self.args.mask_aa_pieces)\n\n        src_dataset = RightPadDataset(\n            src_dataset,\n            pad_idx=self.source_dictionary.pad(),\n        )\n\n\n        src_dataset = NestedDictionaryDataset(\n            {\n                \"id\": IdDataset(),\n                \"net_input\": {\n                    \"src_tokens\": src_dataset,\n                    \"src_lengths\": NumelDataset(src_dataset, reduce=False),\n\n                    # \"aa_piece_handlers\": aa_piece_dataset,\n                },\n            },\n            sizes=src_lengths,\n        )\n        if sort:\n            src_dataset = SortDataset(src_dataset, sort_order=[src_lengths])\n        return src_dataset\n\n    @property\n    def source_dictionary(self):\n        return self.seq_dict\n    \n    @property\n    def tag_source_dictionary(self):\n        return self.tag_dict\n\n    @property\n    def target_dictionary(self):\n        return self.seq_dict", "\n\n"]}
{"filename": "fairseq_models/tasks/abgen_task.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nimport logging\nimport os\n\nimport fairseq\nimport numpy as np", "import fairseq\nimport numpy as np\nimport torch\nimport warnings\n\n\nfrom fairseq import utils\nfrom fairseq.data import (\n    data_utils,\n    Dictionary,", "    data_utils,\n    Dictionary,\n    SortDataset,\n)\nfrom fairseq.tasks import LegacyFairseqTask, register_task\n\nfrom fairseq_models.data.abgen_dataset import AntibodyComplexDataset\nfrom fairseq_models.data.abgen_dataset_noantigen import AntibodyOnlyDataset\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\n@register_task('antibody_generation_task')\nclass AntibodyGenerationTask(LegacyFairseqTask):\n    \"\"\"\n    Notice: till now, one of the MLM/PSSM Pred. is must needed for build & load the dataset\n\n\n    Sentence (or sentence pair) prediction (classification or regression) task.\n\n    Args:\n        dictionary (Dictionary): the dictionary for the input of the task\n    \"\"\"\n\n    @staticmethod\n    def add_args(parser):\n        \"\"\"Add task-specific arguments to the parser.\"\"\"\n        parser.add_argument('--sabdab-data', help=\"sabdab data path\")\n        parser.add_argument(\n            '--cdr-type', \n            default='3', \n            choices=['1', '2', '3'],\n            help=\"which part to predict\"\n        )\n        parser.add_argument(\n            '--L-target', \n            default=20,\n            type=int,\n            help=\"number of antigen residues to be considered as epitope\"\n        )\n\n        parser.add_argument('--noantigen', action='store_true', default=False)\n        \n\n    def __init__(self, args, seq_dict, tag_dict):\n        super().__init__(args)\n        self.args = args\n        self.seq_dict = seq_dict\n        self.tag_dict = tag_dict\n        self.seed = args.seed\n\n        self.mask_idx = seq_dict.add_symbol(\"<mask>\")\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        seq_dict = Dictionary.load(os.path.join('data', 'seq_dict.txt'))\n        tag_dict = Dictionary.load(os.path.join('data', 'tag_dict.txt'))\n\n        logger.info(\"[input] dictionary: {} types\".format(len(seq_dict)))\n        logger.info(\"[input] dictionary: {} types\".format(len(tag_dict)))\n\n        \n        return cls(args, seq_dict, tag_dict)  # Done: needs refine to TAPE's tokenizer\n        \n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"\n        ### downstream tasks\n        if self.args.noantigen:\n            dataset = AntibodyOnlyDataset(\n                data_path=self.args.sabdab_data,\n                split=split,\n                seq_vocab=self.source_dictionary,\n                tag_vocab=self.tag_source_dictionary,\n                cdr_types=[*self.args.cdr_type],\n                L_target=self.args.L_target,\n                pad_idx=self.source_dictionary.pad(),\n                mask_idx=self.mask_idx,\n                max_len=self.args.max_positions\n            )\n        else:\n            dataset = AntibodyComplexDataset(\n                data_path=self.args.sabdab_data,\n                split=split,\n                seq_vocab=self.source_dictionary,\n                tag_vocab=self.tag_source_dictionary,\n                cdr_types=[*self.args.cdr_type],\n                L_target=self.args.L_target,\n                pad_idx=self.source_dictionary.pad(),\n                mask_idx=self.mask_idx,\n                max_len=self.args.max_positions\n            )\n        \n\n        with data_utils.numpy_seed(self.args.seed + epoch):\n            shuffle = np.random.permutation(len(dataset))\n            \n        self.datasets[split] = SortDataset(dataset, sort_order=[shuffle, dataset.prefix_len, dataset.sizes])\n        \n        return None  # return in advance\n\n    def build_model(self, args):\n \n        model = super().build_model(args)\n\n        def inplace_relu(m):\n            classname = m.__class__.__name__\n            if classname.find('ReLU') != -1:\n                m.inplace=True\n        model.apply(inplace_relu)\n\n        return model\n\n    @property\n    def source_dictionary(self):\n        return self.seq_dict\n    \n    @property\n    def tag_source_dictionary(self):\n        return self.tag_dict\n\n    @property\n    def target_dictionary(self):\n        return self.seq_dict"]}
{"filename": "fairseq_models/tasks/abbert_mask_tokens_dataset.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import lru_cache\n\nimport numpy as np\nimport torch\nfrom fairseq.data import Dictionary, data_utils", "import torch\nfrom fairseq.data import Dictionary, data_utils\nfrom fairseq.data import BaseWrapperDataset, LRUCacheDataset\n\n# from fairseq_models.aa_piece_dataset import AAPieceDataset\n\n\nclass AntibodyMaskTokensDataset(BaseWrapperDataset):\n    \"\"\"\n    A wrapper Dataset for masked language modeling.\n\n    Input items are masked according to the specified masking probability.\n\n    Args:\n        dataset: Dataset to wrap.\n        sizes: Sentence lengths\n        vocab: Dictionary with the vocabulary and special tokens.\n        pad_idx: Id of pad token in vocab\n        mask_idx: Id of mask token in vocab\n        return_masked_tokens: controls whether to return the non-masked tokens\n            (the default) or to return a tensor with the original masked token\n            IDs (and *pad_idx* elsewhere). The latter is useful as targets for\n            masked LM training.\n        seed: Seed for random number generator for reproducibility.\n        mask_prob: probability of replacing a token with *mask_idx*.\n        leave_unmasked_prob: probability that a masked token is unmasked.\n        random_token_prob: probability of replacing a masked token with a\n            random token from the vocabulary.\n        freq_weighted_replacement: sample random replacement words based on\n            word frequencies in the vocab.\n        mask_whole_words: only mask whole words. This should be a byte mask\n            over vocab indices, indicating whether it is the beginning of a\n            word. We will extend any mask to encompass the whole word.\n        bpe: BPE to use for whole-word masking.\n        mask_multiple_length : repeat each mask index multiple times. Default\n            value is 1.\n        mask_stdev : standard deviation of masks distribution in case of\n            multiple masking. Default value is 0.\n    \"\"\"\n\n    @classmethod\n    def apply_mask(cls, dataset1: torch.utils.data.Dataset, dataset2: torch.utils.data.Dataset, *args, **kwargs):\n        \"\"\"Return the source and target datasets for masked LM training.\"\"\"\n        dataset1 = LRUCacheDataset(dataset1)\n        dataset2 = LRUCacheDataset(dataset2)\n        return (\n            LRUCacheDataset(cls(dataset1, dataset2, *args, **kwargs, return_masked_tokens=False)),\n            LRUCacheDataset(cls(dataset1, dataset2, *args, **kwargs, return_masked_tokens=True)),\n        )\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.Dataset,\n        tag_dataset: torch.utils.data.Dataset,\n        seq_vocab: Dictionary,\n        tag_vocab: Dictionary,\n        pad_idx: int,\n        mask_idx: int,\n        return_masked_tokens: bool = False,\n        seed: int = 1,\n        mask_prob: float = 0.15,\n        leave_unmasked_prob: float = 0.1,\n        random_token_prob: float = 0.1,\n        freq_weighted_replacement: bool = False,\n        # mask_aa_pieces:bool=False,\n        # aa_piece_dataset:AAPieceDataset=None,\n        mask_multiple_length: int = 1,\n        mask_stdev: float = 0.0,\n    ):\n        assert 0.0 < mask_prob <= 1.0\n        assert 0.0 <= random_token_prob <= 1.0\n        assert 0.0 <= leave_unmasked_prob <= 1.0\n        assert random_token_prob + leave_unmasked_prob <= 1.0\n        assert mask_multiple_length >= 1\n        assert mask_stdev >= 0.0\n\n        self.dataset = dataset\n        self.tag_dataset = tag_dataset\n        self.seq_vocab = seq_vocab\n        self.tag_vocab = tag_vocab\n        self.pad_idx = pad_idx\n        self.mask_idx = mask_idx\n        self.return_masked_tokens = return_masked_tokens\n        self.seed = seed\n        self.mask_prob = mask_prob\n        self.leave_unmasked_prob = leave_unmasked_prob\n        self.random_token_prob = random_token_prob\n\n        # self.mask_aa_pieces=mask_aa_pieces\n        # self.aa_piece_dataset=aa_piece_dataset\n\n        self.mask_multiple_length = mask_multiple_length\n        self.mask_stdev = mask_stdev\n\n        if random_token_prob > 0.0:\n            if freq_weighted_replacement:\n                weights = np.array(self.seq_vocab.count)\n            else:\n                weights = np.ones(len(self.seq_vocab))\n            weights[: self.seq_vocab.nspecial] = 0\n            self.weights = weights / weights.sum()\n\n        # if mask_aa_pieces:\n        #     # self.aa_pieces=AAPieceFromUniprot21()\n        #     self.aa_piece_dataset=aa_piece_dataset\n\n        self.epoch = 0\n\n    @property\n    def can_reuse_epoch_itr_across_epochs(self):\n        return True  # only the noise changes, not item sizes\n\n    def set_epoch(self, epoch, **unused):\n        super().set_epoch(epoch)\n        self.epoch = epoch\n\n    def __getitem__(self, index: int):\n        return self.__getitem_cached__(self.seed, self.epoch, index)\n\n    @lru_cache(maxsize=8)\n    def __getitem_cached__(self, seed: int, epoch: int, index: int):\n        with data_utils.numpy_seed(self.seed, self.epoch, index):\n            item = self.dataset[index]\n            tag = self.tag_dataset[index]\n            cdr1_idx = self.tag_vocab.index('1')\n            cdr2_idx = self.tag_vocab.index('2')\n            cdr3_idx = self.tag_vocab.index('3')\n            cdr_mask = torch.logical_or(\n                torch.logical_or(tag==cdr1_idx, tag==cdr2_idx),\n                tag==cdr3_idx\n            )\n            sz = cdr_mask.sum()\n            \n\n            assert (\n                self.mask_idx not in item\n            ), \"Dataset contains mask_idx (={}), this is not expected!\".format(\n                self.mask_idx,\n            )\n\n\n            # if self.mask_aa_pieces:\n            #     # word_begins_mask = self.mask_whole_words.gather(0, item)\n            #     # word_begins_idx = word_begins_mask.nonzero().view(-1)\n            #     # sz = len(word_begins_idx)\n            #     # words = np.split(word_begins_mask, word_begins_idx)[1:]\n            #     # assert len(words) == sz\n            #     # word_lens = list(map(len, words))\n\n            #     # \u5bf9item\u505aBPE\u7c92\u5ea6\u7684mask\uff0c\u6ce8\u610f1\u4e2aitem\u53ea\u542b1\u6761\u5e8f\u5217\uff0c\u5373sample-break-mode\u9009\u9879\u5fc5\u987b\u4e3aeos\n            #     # pieces = self.aa_pieces.encode_pieces(item)\n            #     pieces = self.aa_piece_dataset[index].get_pieces()\n\n            #     word_lens = list(map(len, pieces))\n            #     word_lens=[1]+word_lens+[1]\n            #     sz = len(word_lens)\n\n            if self.mask_prob == 1.0:\n                mask = np.full(sz, True)\n                if self.return_masked_tokens:\n                    # exit early if we're just returning the masked tokens\n                    # (i.e., the targets for masked LM training)\n                    # if self.mask_aa_pieces:\n                    #     mask = np.repeat(mask, word_lens)\n                        \n                    new_item = np.full(len(item), self.pad_idx)\n                    pre_mask = np.full(sz, self.pad_idx)\n                    pre_mask[mask] = item[cdr_mask][torch.from_numpy(mask.astype(np.uint8)) == 1]\n                    new_item[cdr_mask] = pre_mask\n\n                    return torch.from_numpy(new_item)\n\n                # decide unmasking and random replacement\n                rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob\n                if rand_or_unmask_prob > 0.0:\n                    rand_or_unmask = mask & (np.random.rand(sz) < rand_or_unmask_prob)\n                    if self.random_token_prob == 0.0:\n                        unmask = rand_or_unmask\n                        rand_mask = None\n                    elif self.leave_unmasked_prob == 0.0:\n                        unmask = None\n                        rand_mask = rand_or_unmask\n                    else:\n                        unmask_prob = self.leave_unmasked_prob / rand_or_unmask_prob\n                        decision = np.random.rand(sz) < unmask_prob\n                        unmask = rand_or_unmask & decision\n                        rand_mask = rand_or_unmask & (~decision)\n                else:\n                    unmask = rand_mask = None\n\n                if unmask is not None:\n                    mask = mask ^ unmask\n\n                # if self.mask_aa_pieces:\n                #     mask = np.repeat(mask, word_lens)\n\n                new_item = np.copy(item)\n                pre_mask = np.full(sz, self.pad_idx)\n                pre_mask[mask] = self.mask_idx\n                if rand_mask is not None:\n                    num_rand = rand_mask.sum()\n                    if num_rand > 0:\n                        # if self.mask_aa_pieces:\n                        #     rand_mask = np.repeat(rand_mask, word_lens)\n                        #     num_rand = rand_mask.sum()\n\n                        pre_mask[rand_mask] = np.random.choice(\n                            len(self.seq_vocab),\n                            num_rand,\n                            p=self.weights,\n                        )\n                new_item[cdr_mask] = pre_mask\n\n                # print(\"sentence: \", item)\n                # print(\"tag: \", tag)\n                # print(\"tag seq\", self.tag_vocab.string(tag))\n                # print(\"new sent: \", torch.from_numpy(new_item))\n                # print('tag_vocab: ', self.tag_vocab)\n                # exit(0)\n\n                return torch.from_numpy(new_item)\n                \n\n\n            # decide elements to mask\n            mask = np.full(sz, False)\n            num_mask = int(\n                # add a random number for probabilistic rounding\n                self.mask_prob * sz\n                + np.random.rand()\n            )\n\n            # multiple masking as described in the vq-wav2vec paper (https://arxiv.org/abs/1910.05453)\n            mask_idc = np.random.choice(sz, num_mask, replace=False)\n            \n            mask_idc = mask_idc[mask_idc < len(mask)]\n            try:\n                mask[mask_idc] = True\n            except:  # something wrong\n                print(\n                    \"Assigning mask indexes {} to mask {} failed!\".format(\n                        mask_idc, mask\n                    )\n                )\n                raise\n\n            if self.return_masked_tokens:\n                # exit early if we're just returning the masked tokens\n                # (i.e., the targets for masked LM training)\n                # if self.mask_aa_pieces:\n                #     mask = np.repeat(mask, word_lens)\n                \n                new_item = np.full(len(item), self.pad_idx)\n                pre_mask = np.full(sz, self.pad_idx)\n                pre_mask[mask] = item[cdr_mask][torch.from_numpy(mask.astype(np.uint8)) == 1]\n                new_item[cdr_mask] = pre_mask\n\n                return torch.from_numpy(new_item)\n\n            # decide unmasking and random replacement\n            rand_or_unmask_prob = self.random_token_prob + self.leave_unmasked_prob\n            if rand_or_unmask_prob > 0.0:\n                rand_or_unmask = mask & (np.random.rand(sz) < rand_or_unmask_prob)\n                if self.random_token_prob == 0.0:\n                    unmask = rand_or_unmask\n                    rand_mask = None\n                elif self.leave_unmasked_prob == 0.0:\n                    unmask = None\n                    rand_mask = rand_or_unmask\n                else:\n                    unmask_prob = self.leave_unmasked_prob / rand_or_unmask_prob\n                    decision = np.random.rand(sz) < unmask_prob\n                    unmask = rand_or_unmask & decision\n                    rand_mask = rand_or_unmask & (~decision)\n            else:\n                unmask = rand_mask = None\n\n            if unmask is not None:\n                mask = mask ^ unmask\n\n            # if self.mask_aa_pieces:\n            #     mask = np.repeat(mask, word_lens)\n\n            new_item = np.copy(item)\n            pre_mask = np.full(sz, self.pad_idx)\n            pre_mask[mask] = self.mask_idx\n            if rand_mask is not None:\n                num_rand = rand_mask.sum()\n                if num_rand > 0:\n                    # if self.mask_aa_pieces:\n                    #     rand_mask = np.repeat(rand_mask, word_lens)\n                    #     num_rand = rand_mask.sum()\n\n                    pre_mask[rand_mask] = np.random.choice(\n                        len(self.seq_vocab),\n                        num_rand,\n                        p=self.weights,\n                    )\n            new_item[cdr_mask] = pre_mask\n\n            # print(\"sentence: \", item)\n            # print(\"tag: \", tag)\n            # print(\"tag seq\", self.tag_vocab.string(tag))\n            # print(\"new sent: \", torch.from_numpy(new_item))\n            # print('tag_vocab: ', self.tag_vocab)\n            # exit(0)\n\n            return torch.from_numpy(new_item)", ""]}
{"filename": "fairseq_models/criterions/abbert_masked_lm.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom fairseq import metrics, modules, utils", "import torch.nn.functional as F\nfrom fairseq import metrics, modules, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\n\n\n@register_criterion(\"antibody_masked_lm\")\nclass AntibodyMaskedLmLoss(FairseqCriterion):\n    \"\"\"\n    Implementation for the loss used in masked language model (MLM) training.\n    \"\"\"\n\n    def __init__(self, task, tpu=False):\n        super().__init__(task)\n        self.tpu = tpu\n\n    def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        masked_tokens = sample[\"target\"].ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum()\n\n        logits = model(src_tokens=sample[\"net_input0\"][\"src_tokens\"], tag_tokens=sample[\"net_input1\"][\"src_tokens\"], masked_tokens=masked_tokens)[0]\n        targets = model.get_targets(sample, [logits])\n        if masked_tokens is not None:\n            targets = targets[masked_tokens]\n\n        loss = modules.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            reduction=\"sum\",\n            ignore_index=self.padding_idx,\n        )\n\n\n        logging_output = {\n            \"loss\": loss if self.tpu else loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"nsentences\"],\n            \"sample_size\": sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n        metrics.log_scalar(\n            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n        )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return True", ""]}
{"filename": "fairseq_models/criterions/abgen_criterions.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom fairseq import utils", "import torch.nn as nn\nfrom fairseq import utils\nfrom fairseq import metrics\nfrom fairseq.criterions import LegacyFairseqCriterion, register_criterion\n\nfrom fairseq_models.modules.utils import compute_rmsd\n\n@register_criterion(\"antibody_generation_loss\")\nclass AntibodyGenerationLoss(LegacyFairseqCriterion):\n    \"\"\" \n    Implementation for the loss used in masked language model (MLM) training.\n    \"\"\"\n\n    def __init__(self, args, task, tpu=False):\n        super().__init__(args, task)\n        self.tpu = tpu\n        self.cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=self.padding_idx)\n        self.update_num = 0\n        self.args = args\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument('--loss-scale-enc', type=float, default=1.,\n                            help='loss scale of encoder loss')\n        parser.add_argument('--loss-scale-dec-sloss', type=float, default=1.,\n                            help='loss scale of decoder sloss')\n        parser.add_argument('--loss-scale-dec-xloss', type=float, default=1.,\n                            help='loss scale of decoder xloss')\n\n    def forward(self, model, sample):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        batched_seq, batched_tag, batched_label, paratope, epitope, antibody = sample\n\n        masked_tokens = batched_label.ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum()\n        \n        out, transformer_logits, _ = model(\n            src_tokens=batched_seq, \n            tag_tokens=batched_tag,\n            paratope=paratope,\n            epitope=epitope,\n            antibody=antibody,\n            masked_tokens=masked_tokens,\n        )\n\n        # compute encoder loss\n        targets = batched_label\n        if masked_tokens is not None:\n            targets = targets[masked_tokens]\n        if transformer_logits is not None:\n            sloss = self.cross_entropy(\n                transformer_logits.view(-1, transformer_logits.size(-1)),\n                targets.view(-1) - 3, \n            ) / sample_size / math.log(2)\n        else:\n            sloss = 0\n\n        # compute RMSD score\n        bind_X, bind_S, _, _ = paratope\n        bind_mask = bind_S > 0\n        if self.args.noantigen: # noantigen version has no side chains\n            ret = torch.zeros((bind_X.size(0), bind_X.size(1), 4, 3), device=batched_seq.device)\n        else:\n            ret = torch.zeros((bind_X.size(0), bind_X.size(1), 14, 3), device=batched_seq.device)\n        cnt = 0\n        for i, v in enumerate([sum(mask) for mask in masked_tokens]):\n            ret[i, :v] = out.bind_X[cnt:cnt+v]\n            cnt += v\n        rmsd = compute_rmsd(\n                ret[:, :, 1], bind_X[:, :, 1], bind_mask\n            )\n        rmsd = sum(rmsd) / len(rmsd)\n\n        # loss sum\n        loss =  self.args.loss_scale_enc * sloss + \\\n                self.args.loss_scale_dec_xloss * out.xloss + \\\n                self.args.loss_scale_dec_sloss * out.nll\n        \n        logging_output = {\n            \"encoder_sloss\": sloss.item(),\n            \"decoder_sloss\": out.nll,\n            \"decoder_xloss\": out.xloss,\n            \"sample_size\": sample_size.item(),\n            \"loss\": loss.item(),\n            \"rmsd\": rmsd.item()\n        }\n        return loss, sample_size.item(), logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        encoder_sloss = sum(log.get(\"encoder_sloss\", 0) for log in logging_outputs)\n        decoder_sloss = sum(log.get(\"decoder_sloss\", 0) for log in logging_outputs)\n        decoder_xloss = sum(log.get(\"decoder_xloss\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n        loss = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        rmsd = sum(log.get(\"rmsd\", 0) for log in logging_outputs)\n        \n        metrics.log_scalar(\n            \"encoder_sloss\", encoder_sloss, sample_size, round=3\n        )\n        metrics.log_derived(\n            \"encoder_ppl\", lambda meters: utils.get_perplexity(meters[\"encoder_sloss\"].avg)\n        )\n        metrics.log_scalar(\n            \"decoder_sloss\", decoder_sloss, sample_size, round=3\n        )\n        metrics.log_derived(\n            \"decoder_ppl\", lambda meters: utils.get_perplexity(meters[\"decoder_sloss\"].avg)\n        )\n        metrics.log_scalar(\n            \"decoder_xloss\", decoder_xloss, sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"loss\", loss, sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"rmsd\", rmsd, sample_size, round=3\n        )\n\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return True", "class AntibodyGenerationLoss(LegacyFairseqCriterion):\n    \"\"\" \n    Implementation for the loss used in masked language model (MLM) training.\n    \"\"\"\n\n    def __init__(self, args, task, tpu=False):\n        super().__init__(args, task)\n        self.tpu = tpu\n        self.cross_entropy = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=self.padding_idx)\n        self.update_num = 0\n        self.args = args\n\n    @staticmethod\n    def add_args(parser):\n        parser.add_argument('--loss-scale-enc', type=float, default=1.,\n                            help='loss scale of encoder loss')\n        parser.add_argument('--loss-scale-dec-sloss', type=float, default=1.,\n                            help='loss scale of decoder sloss')\n        parser.add_argument('--loss-scale-dec-xloss', type=float, default=1.,\n                            help='loss scale of decoder xloss')\n\n    def forward(self, model, sample):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        batched_seq, batched_tag, batched_label, paratope, epitope, antibody = sample\n\n        masked_tokens = batched_label.ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum()\n        \n        out, transformer_logits, _ = model(\n            src_tokens=batched_seq, \n            tag_tokens=batched_tag,\n            paratope=paratope,\n            epitope=epitope,\n            antibody=antibody,\n            masked_tokens=masked_tokens,\n        )\n\n        # compute encoder loss\n        targets = batched_label\n        if masked_tokens is not None:\n            targets = targets[masked_tokens]\n        if transformer_logits is not None:\n            sloss = self.cross_entropy(\n                transformer_logits.view(-1, transformer_logits.size(-1)),\n                targets.view(-1) - 3, \n            ) / sample_size / math.log(2)\n        else:\n            sloss = 0\n\n        # compute RMSD score\n        bind_X, bind_S, _, _ = paratope\n        bind_mask = bind_S > 0\n        if self.args.noantigen: # noantigen version has no side chains\n            ret = torch.zeros((bind_X.size(0), bind_X.size(1), 4, 3), device=batched_seq.device)\n        else:\n            ret = torch.zeros((bind_X.size(0), bind_X.size(1), 14, 3), device=batched_seq.device)\n        cnt = 0\n        for i, v in enumerate([sum(mask) for mask in masked_tokens]):\n            ret[i, :v] = out.bind_X[cnt:cnt+v]\n            cnt += v\n        rmsd = compute_rmsd(\n                ret[:, :, 1], bind_X[:, :, 1], bind_mask\n            )\n        rmsd = sum(rmsd) / len(rmsd)\n\n        # loss sum\n        loss =  self.args.loss_scale_enc * sloss + \\\n                self.args.loss_scale_dec_xloss * out.xloss + \\\n                self.args.loss_scale_dec_sloss * out.nll\n        \n        logging_output = {\n            \"encoder_sloss\": sloss.item(),\n            \"decoder_sloss\": out.nll,\n            \"decoder_xloss\": out.xloss,\n            \"sample_size\": sample_size.item(),\n            \"loss\": loss.item(),\n            \"rmsd\": rmsd.item()\n        }\n        return loss, sample_size.item(), logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        encoder_sloss = sum(log.get(\"encoder_sloss\", 0) for log in logging_outputs)\n        decoder_sloss = sum(log.get(\"decoder_sloss\", 0) for log in logging_outputs)\n        decoder_xloss = sum(log.get(\"decoder_xloss\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n        loss = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        rmsd = sum(log.get(\"rmsd\", 0) for log in logging_outputs)\n        \n        metrics.log_scalar(\n            \"encoder_sloss\", encoder_sloss, sample_size, round=3\n        )\n        metrics.log_derived(\n            \"encoder_ppl\", lambda meters: utils.get_perplexity(meters[\"encoder_sloss\"].avg)\n        )\n        metrics.log_scalar(\n            \"decoder_sloss\", decoder_sloss, sample_size, round=3\n        )\n        metrics.log_derived(\n            \"decoder_ppl\", lambda meters: utils.get_perplexity(meters[\"decoder_sloss\"].avg)\n        )\n        metrics.log_scalar(\n            \"decoder_xloss\", decoder_xloss, sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"loss\", loss, sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"rmsd\", rmsd, sample_size, round=3\n        )\n\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return True", ""]}
{"filename": "structgen/decoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom structgen.encoder import MPNEncoder\nfrom structgen.data import alphabet\nfrom structgen.utils import *\nfrom structgen.protein_features import ProteinFeatures\n", "from structgen.protein_features import ProteinFeatures\n\n\nclass Decoder(nn.Module):\n    \n    def __init__(self, args, return_coords=True):\n        super(Decoder, self).__init__()\n        self.k_neighbors = args.k_neighbors\n        self.depth = args.depth\n        self.hidden_size = args.hidden_size\n        self.augment_eps = args.augment_eps\n        self.context = args.context\n        self.return_coords = return_coords\n\n        self.pos_embedding = PosEmbedding(16)\n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type='dist',\n                direction='forward'\n        )\n        self.node_in, self.edge_in = self.features.feature_dimensions['dist']\n        self.O_nei = nn.Sequential(\n                nn.Linear(args.hidden_size * 2 + 16, args.hidden_size),\n                nn.ReLU(),\n                nn.Linear(args.hidden_size, 1),\n        )\n        self.O_dist = nn.Sequential(\n                nn.Linear(args.hidden_size * 2 + 16, args.hidden_size),\n                nn.ReLU(),\n                nn.Linear(args.hidden_size, 1),\n        )\n        self.O_s = nn.Linear(args.hidden_size, args.vocab_size)\n        self.O_v = nn.Linear(args.hidden_size, self.node_in)\n        self.O_e = nn.Linear(args.hidden_size, self.edge_in - self.features.num_positional_embeddings)\n\n        self.struct_mpn = MPNEncoder(args, self.node_in, self.edge_in)\n        self.seq_mpn = MPNEncoder(args, self.node_in, self.edge_in)\n\n        if args.context:\n            self.W_stc = nn.Sequential(\n                    nn.Linear(args.hidden_size * 2, args.hidden_size),\n                    nn.ReLU(),\n            )\n            self.W_seq = nn.Sequential(\n                    nn.Linear(args.hidden_size * 2, args.hidden_size),\n                    nn.ReLU(),\n            )\n            self.crnn = nn.GRU(\n                    len(alphabet), args.hidden_size, \n                    batch_first=True, num_layers=1,\n                    dropout=args.dropout\n            )\n\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n        self.mse_loss = nn.MSELoss(reduction='none')\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    # Q: [B, N, H], K, V: [B, M, H]\n    def attention(self, Q, context, W):\n        context, cmask = context  # cmask: [B, M]\n        att = torch.bmm(Q, context.transpose(1, 2))  # [B, N, M]\n        att = att - 1e6 * (1 - cmask.unsqueeze(1))\n        att = F.softmax(att, dim=-1)\n        out = torch.bmm(att, context)  # [B, N, M] * [B, M, H]\n        out = torch.cat([Q, out], dim=-1)\n        return W(out)\n\n    def encode_context(self, context):\n        cS, cmask, crange = context\n        cS = F.one_hot(cS, num_classes=len(alphabet)).float()\n        cH, _ = self.crnn(cS)\n        return (cH, cmask)\n\n    def forward(self, X, S, L, mask, context=None, debug=False):\n        # X: [B, N, 4, 3], S: [B, N], mask: [B, N] \n        true_V, _, _ = self.features(X, mask)\n        N, K = S.size(1), self.k_neighbors\n\n        # data augmentation\n        V, E, E_idx = self.features(\n                X + self.augment_eps * torch.randn_like(X), \n                mask\n        )\n\n        # run struct MPN\n        h = self.struct_mpn(V, E, S, E_idx, mask)\n        if self.context:\n            context = self.encode_context(context)\n            h = self.attention(h, context, self.W_stc)\n\n        # predict node feature with h_{v-1}\n        vout = self.O_v(h[:, :-1])\n        vloss = self.mse_loss(vout, true_V[:, 1:]).mean(dim=-1)\n        vloss = torch.sum(vloss * mask[:, 1:]) / mask[:, 1:].sum()\n\n        # predict neighbors with h_{v-1}, h_u, E_pos\n        E_next, nlabel, dlabel, nmask = get_nei_label(X, mask, K)  # [B, N-1, N]\n        h_cur = h[:, :-1].unsqueeze(2).expand(-1,-1,N,-1)  # [B, N-1, N, H]\n        h_pre = gather_nodes(h, E_next)  # [B, N-1, N, H]\n        pos = torch.arange(1, N).cuda().view(1, -1, 1) - E_next  # [B, N-1, N]\n        E_pos = self.pos_embedding(pos)  # [B, N-1, N, H]\n        h_nei = torch.cat([h_cur, h_pre, E_pos], dim=-1)\n        nout = self.O_nei(h_nei).squeeze(-1)  # [B, N-1, N]\n        nloss = self.bce_loss(nout, nlabel.float())\n        nloss = torch.sum(nloss * nmask) / nmask.sum()\n\n        # predict neighbors distance\n        dout = self.O_dist(h_nei).squeeze(-1)  # [B, N-1, N]\n        dout = dout[:, :, :K]  # [B, N-1, K]\n        dmask = nmask[:, :, :K]  # [B, N-1, K]\n        dlabel = dlabel.clamp(max=20)\n        dlabel = (dlabel[:, :, :K] - 10) / 10  # D in [0, 20]\n        dloss = self.mse_loss(dout, dlabel)\n        dloss = torch.sum(dloss * dmask) / dmask.sum()\n\n        # sequence prediction\n        h = self.seq_mpn(V, E, S, E_idx, mask)\n        if self.context:\n            h = self.attention(h, context, self.W_seq)\n\n        sout = self.O_s(h)\n        sloss = self.ce_loss(sout.view(-1, sout.size(-1)), S.view(-1))\n        sloss = torch.sum(sloss * mask.view(-1)) / mask.sum()\n\n        loss = sloss + nloss + vloss + dloss\n        dout = dout * 10 + 10\n        return (sout, vout, nout, dout) if debug else loss\n    \n    def expand_one_residue(self, h, V, E, E_idx, t):\n        # predict node feature for t+1\n        B, K = len(h), self.k_neighbors\n        V[:, t+1] = self.O_v(h[:, t])\n\n        # predict neighbors for t+1\n        h_cur = h[:, t:t+1].expand(-1, t+1, -1)  # [B, t+1, H]\n        h_pre = h[:, :t+1]  # [B, t+1, H]\n        pos = t + 1 - torch.arange(t + 1).view(1, -1, 1).expand(B, -1, -1)  # [B, t+1, 1]\n        E_pos = self.pos_embedding(pos.cuda()).squeeze(2)  # [B, t+1, H]\n        h_nei = torch.cat([h_cur, h_pre, E_pos], dim=-1)\n        nout = self.O_nei(h_nei).squeeze(-1)  # [B, t+1]\n\n        if K <= t + 1:\n            _, E_idx[:, t+1] = nout.topk(dim=-1, k=K, largest=True)\n            nei_topk = E_idx[:, t+1]  # [B, K]\n        else:\n            E_idx[:, t+1, :t+1] *= 0\n            E_idx[:, t+1, :t+1] += torch.arange(t, -1, -1).view(1,-1).cuda()\n            nei_topk = E_idx[:, t+1, :t+1]  # [B, t+1]\n\n        # predict neighbors distance\n        # Positional encoding is relative!\n        dout = self.O_dist(h_nei).squeeze(-1)  # [B, t+1]\n        dout = dout * 10 + 10\n        dout = gather_2d(dout, nei_topk)  # [B, t+1]\n        rbf_vecs = self.features._rbf(dout.unsqueeze(1))  # [B, 1, t+1, H]\n        pos_vecs = self.pos_embedding(nei_topk.unsqueeze(1) - t - 1)  # [B, 1, t+1] => [B, 1, t+1, H]\n        E[:, t+1, :t+1] = torch.cat([pos_vecs, rbf_vecs], dim=-1).squeeze(1)  # [B, t+1, H]\n        return nout, dout\n\n    def log_prob(self, S, mask, context=None, debug=None):\n        B, N = S.size(0), S.size(1)\n        K = self.k_neighbors\n\n        V = torch.zeros(B, N+1, self.node_in).cuda()\n        V[:, :, :self.node_in // 2] = 1.  # cos(0) = 1\n        E = torch.zeros(B, N+1, K, self.edge_in).cuda()\n        E_idx = torch.zeros(B, N+1, K).long().cuda() + N - 1\n        h_stc = [torch.zeros(B, N, self.hidden_size, requires_grad=True).cuda() for _ in range(self.depth + 1)]\n        h_seq = [torch.zeros(B, N, self.hidden_size, requires_grad=True).cuda() for _ in range(self.depth + 1)]\n\n        D = torch.zeros(B, N+1, K).cuda()\n        log_prob = []\n        if self.context:\n            context = self.encode_context(context)\n\n        for t in range(N):\n            # run MPN\n            h_seq = self.seq_mpn.inc_forward(V, E, S, E_idx, mask, h_seq, t)\n            h_stc = self.struct_mpn.inc_forward(V, E, S, E_idx, mask, h_stc, t)\n\n            h = h_seq[-1][:, t:t+1]\n            if self.context:\n                h = self.attention(h, context, self.W_seq)\n\n            # predict residue for t\n            logits = self.O_s(h.squeeze(1))\n            lprob = F.log_softmax(logits, dim=-1)\n            nll = F.nll_loss(lprob, S[:, t], reduction='none')\n            log_prob.append(nll)\n\n            # predict position for t + 1\n            h = self.attention(h_stc[-1], context, self.W_stc) if self.context else h_stc[-1]\n            V, E, E_idx = V.clone(), E.clone(), E_idx.clone()  # avoid inplace autograd error\n            nout, dout = self.expand_one_residue(h, V, E, E_idx, t)\n            V, E, E_idx = V.clone(), E.clone(), E_idx.clone()  # avoid inplace autograd error\n            D[:, t+1, :dout.size(-1)] = dout\n\n            if debug and t < N - 1:\n                self.debug_decode(debug, logits, V, E, E_idx, mask, nout, dout, t)\n\n        log_prob = torch.stack(log_prob, dim=1)  # [B, N]\n        ppl = torch.sum(log_prob * mask, dim=-1) / mask.sum(dim=-1)\n        log_prob = torch.sum(log_prob * mask) / mask.sum()\n        if self.return_coords:\n            X = fit_coords(D[:, :-1, :].detach(), E_idx[:, :-1, :].detach(), mask)\n            X = X.unsqueeze(2).expand(-1,-1,4,-1)\n            return ReturnType(nll=log_prob, ppl=ppl, X_cdr=X)\n        else:\n            return ReturnType(nll=log_prob, ppl=ppl, X_cdr=None)\n\n    def generate(self, B, N, context=None, return_ppl=False):\n        K = self.k_neighbors\n        S = torch.zeros(B, N).long().cuda()\n        mask = torch.ones(B, N).cuda()\n\n        V = torch.zeros(B, N+1, self.node_in).cuda()\n        V[:, :, :self.node_in // 2] = 1.  # cos(0) = 1\n        E = torch.zeros(B, N+1, K, self.edge_in).cuda()\n        E_idx = torch.zeros(B, N+1, K).long().cuda() + N - 1\n        h_stc = [torch.zeros(B, N, self.hidden_size).cuda() for _ in range(self.depth + 1)]\n        h_seq = [torch.zeros(B, N, self.hidden_size).cuda() for _ in range(self.depth + 1)]\n\n        if self.context:\n            context = self.encode_context(context)\n\n        sloss = 0.\n        for t in range(N):\n            # run MPN\n            h_seq = self.seq_mpn.inc_forward(V, E, S, E_idx, mask, h_seq, t)\n            h_stc = self.struct_mpn.inc_forward(V, E, S, E_idx, mask, h_stc, t)\n\n            h = h_seq[-1][:, t:t+1]\n            if self.context:\n                h = self.attention(h, context, self.W_seq)\n\n            # predict residue for t\n            logits = self.O_s(h.squeeze(1))\n            prob = F.softmax(logits, dim=-1)  # [B, 20]\n            S[:, t] = torch.multinomial(prob, num_samples=1).squeeze(-1)  # [B, 1]\n            sloss = sloss + self.ce_loss(logits, S[:, t])\n\n            # predict position for t + 1\n            h = self.attention(h_stc[-1], context, self.W_stc) if self.context else h_stc[-1]\n            nout, dout = self.expand_one_residue(h, V, E, E_idx, t)\n\n        S = S.tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(N)]) for i in range(B)]\n        ppl = torch.exp(sloss / N)\n        return (S, ppl) if return_ppl else S\n\n    def debug_decode(self, debug_info, logits, V, E, E_idx, mask, nout, dout, t):\n        X, L, true_logits, true_vout, true_nout, true_dout = debug_info[:7]\n        true_V, true_E, true_E_idx = self.features(X, mask)\n\n        print(t)\n        ll = min(t + 1, self.k_neighbors)\n        print('-------S-------')\n        print(logits - true_logits[:, t])\n        print('-------N-------')\n        print(E_idx[:, t+1])\n        print(true_E_idx[:, t+1])\n        print(nout[:, :ll].sum() - true_nout[:, t, :ll].sum())\n        print('-------V-------')\n        print(V[:, t+1] - true_vout[:, t])\n        print('-------E-------')\n        print(dout[:, :ll].sum() - true_dout[:, t, :ll].sum())\n        #print(E[:, t+1] - true_E[:, t+1])\n        print('---------------')\n\n        V[:, t+1] = true_V[:, t+1]\n        E[:, t+1] = true_E[:, t+1]\n        E_idx[:, t+1] = true_E_idx[:, t+1]\n        input(\"Press Enter to continue...\")", ""]}
{"filename": "structgen/hierarchical.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom structgen.encoder import MPNEncoder\nfrom structgen.data import alphabet\nfrom structgen.utils import *\nfrom structgen.protein_features import ProteinFeatures\n", "from structgen.protein_features import ProteinFeatures\n\n\nclass HierarchicalEncoder(nn.Module):\n    \n    def __init__(self, args, node_in, edge_in):\n        super(HierarchicalEncoder, self).__init__()\n        self.node_in, self.edge_in = node_in, edge_in\n        self.W_v = nn.Sequential(\n                nn.Linear(self.node_in, args.hidden_size, bias=True),\n                Normalize(args.hidden_size)\n        )\n        self.W_e = nn.Sequential(\n                nn.Linear(self.edge_in, args.hidden_size, bias=True),\n                Normalize(args.hidden_size)\n        )\n        self.layers = nn.ModuleList([\n                MPNNLayer(args.hidden_size, args.hidden_size * 3, dropout=args.dropout)\n                for _ in range(args.depth)\n        ])\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def forward(self, V, E, hS, E_idx, mask):\n        h_v = self.W_v(V)  # [B, N, H] \n        h_e = self.W_e(E)  # [B, N, K, H] \n        nei_s = gather_nodes(hS, E_idx)  # [B, N, K, H]\n\n        # [B, N, 1] -> [B, N, K, 1] -> [B, N, K]\n        vmask = gather_nodes(mask.unsqueeze(-1), E_idx).squeeze(-1)\n        h = h_v\n        for layer in self.layers:\n            nei_v = gather_nodes(h, E_idx)  # [B, N, K, H]\n            nei_h = torch.cat([nei_v, nei_s, h_e], dim=-1)\n            h = layer(h, nei_h, mask_attend=vmask)  # [B, N, H]\n            h = h * mask.unsqueeze(-1)  # [B, N, H]\n        return h", "\n\nclass HierarchicalDecoder(nn.Module):\n\n    def __init__(self, args):\n        super(HierarchicalDecoder, self).__init__()\n        self.cdr_type = args.cdr_type\n        self.k_neighbors = args.k_neighbors\n        self.block_size = args.block_size\n        self.update_freq = args.update_freq\n        self.hidden_size = args.hidden_size\n        self.pos_embedding = PosEmbedding(16)\n        \n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type='full',\n                direction='bidirectional'\n        )\n        self.node_in, self.edge_in = self.features.feature_dimensions['full']\n        self.O_d0 = nn.Linear(args.hidden_size, 12)\n        self.O_d = nn.Linear(args.hidden_size, 12)\n        self.O_s = nn.Linear(args.hidden_size, args.vocab_size)\n        self.W_s = nn.Embedding(args.vocab_size, args.hidden_size)\n\n        self.struct_mpn = HierarchicalEncoder(args, self.node_in, self.edge_in)\n        self.seq_mpn = HierarchicalEncoder(args, self.node_in, self.edge_in)\n        self.init_mpn = HierarchicalEncoder(args, 16, 32)\n        self.rnn = nn.GRU(\n                args.hidden_size, args.hidden_size, batch_first=True, \n                num_layers=1, bidirectional=True\n        ) \n        self.W_stc = nn.Sequential(\n                nn.Linear(args.hidden_size * 2, args.hidden_size),\n                nn.ReLU(),\n        )\n        self.W_seq = nn.Sequential(\n                nn.Linear(args.hidden_size * 2, args.hidden_size),\n                nn.ReLU(),\n        )\n\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n        self.mse_loss = nn.MSELoss(reduction='none')\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def init_struct(self, B, N, K):\n        # initial V\n        pos = torch.arange(N).cuda()\n        V = self.pos_embedding(pos.view(1, N, 1))  # [1, N, 1, 16]\n        V = V.squeeze(2).expand(B, -1, -1)  # [B, N, 6]\n        # initial E_idx\n        pos = pos.unsqueeze(0) - pos.unsqueeze(1)     # [N, N]\n        D_idx, E_idx = pos.abs().topk(k=K, dim=-1, largest=False)    # [N, K]\n        E_idx = E_idx.unsqueeze(0).expand(B, -1, -1)  # [B, N, K]\n        D_idx = D_idx.unsqueeze(0).expand(B, -1, -1)  # [B, N, K]\n        # initial E\n        E_rbf = self.features._rbf(3 * D_idx)\n        E_pos = self.features.embeddings(E_idx)\n        E = torch.cat((E_pos, E_rbf), dim=-1)\n        return V, E, E_idx\n\n    def init_coords(self, S, mask):\n        B, N = S.size(0), S.size(1)\n        K = min(self.k_neighbors, N)\n        V, E, E_idx = self.init_struct(B, N, K)\n        h = self.init_mpn(V, E, S, E_idx, mask)\n        return self.predict_dist(self.O_d0(h))\n\n    # Q: [B, N, H], K, V: [B, M, H]\n    def attention(self, Q, context, cmask, W):\n        att = torch.bmm(Q, context.transpose(1, 2))  # [B, N, M]\n        att = att - 1e6 * (1 - cmask.unsqueeze(1))\n        att = F.softmax(att, dim=-1)\n        out = torch.bmm(att, context)  # [B, N, M] * [B, M, H]\n        out = torch.cat([Q, out], dim=-1)\n        return W(out)\n\n    def predict_dist(self, X):\n        X = X.view(X.size(0), X.size(1), 4, 3)\n        X_ca = X[:, :, 1, :]\n        dX = X_ca[:, None, :, :] - X_ca[:, :, None, :]\n        D = torch.sum(dX ** 2, dim=-1)\n        V = self.features._dihedrals(X)\n        AD = self.features._AD_features(X[:,:,1,:])\n        return X.detach().clone(), D, V, AD\n\n    def mask_mean(self, X, mask, i):\n        # [B, N, 4, 3] -> [B, 1, 4, 3] / [B, 1, 1, 1]\n        X = X[:, i:i+self.block_size]\n        if X.dim() == 4:\n            mask = mask[:, i:i+self.block_size].unsqueeze(-1).unsqueeze(-1)\n        else:\n            mask = mask[:, i:i+self.block_size].unsqueeze(-1)\n        return torch.sum(X * mask, dim=1, keepdims=True) / (mask.sum(dim=1, keepdims=True) + 1e-8)\n\n    def make_X_blocks(self, X, l, r, mask):\n        N = X.size(1)\n        lblocks = [self.mask_mean(X, mask, i) for i in range(0, l, self.block_size)]\n        rblocks = [self.mask_mean(X, mask, i) for i in range(r + 1, N, self.block_size)]\n        bX = torch.cat(lblocks + [X[:, l:r+1]] + rblocks, dim=1)\n        return bX.detach()\n\n    def make_S_blocks(self, LS, S, RS, l, r, mask):\n        N = S.size(1)\n        hS = self.W_s(S)\n        LS = [self.mask_mean(hS, mask, i) for i in range(0, l, self.block_size)]\n        RS = [self.mask_mean(hS, mask, i) for i in range(r + 1, N, self.block_size)]\n        bS = torch.cat(LS + [hS[:, l:r+1]] + RS, dim=1)\n        lmask = [mask[:, i:i+self.block_size].amax(dim=1, keepdims=True) for i in range(0, l, self.block_size)]\n        rmask = [mask[:, i:i+self.block_size].amax(dim=1, keepdims=True) for i in range(r + 1, N, self.block_size)]\n        bmask = torch.cat(lmask + [mask[:, l:r+1]] + rmask, dim=1)\n        return bS, bmask, len(LS), len(RS)\n\n    def get_completion_mask(self, B, N, cdr_range):\n        cmask = torch.zeros(B, N).cuda()\n        for i, (l,r) in enumerate(cdr_range):\n            cmask[i, l:r+1] = 1\n        return cmask\n\n    def remove_cdr_coords(self, X, cdr_range):\n        X = X.clone()\n        for i, (l,r) in enumerate(cdr_range):\n            X[i, l:r+1, :, :] = 0\n        return X.clone()\n\n    def forward(self, true_X, true_S, true_cdr, mask):\n        B, N = mask.size(0), mask.size(1)\n        K = min(self.k_neighbors, N)\n\n        cdr_range = [(cdr.index(self.cdr_type), cdr.rindex(self.cdr_type)) for cdr in true_cdr]\n        T_min = min([l for l,r in cdr_range])\n        T_max = max([r for l,r in cdr_range])\n        cmask = self.get_completion_mask(B, N, cdr_range)\n        smask = mask.clone()\n\n        # make blocks and encode framework\n        S = true_S.clone() * (1 - cmask.long())\n        hS, _ = self.rnn(self.W_s(S))\n        LS, RS = hS[:, :, :self.hidden_size], hS[:, :, self.hidden_size:]\n        hS, mask, offset, suffix = self.make_S_blocks(LS, S, RS, T_min, T_max, mask)\n        cmask = torch.cat([cmask.new_zeros(B, offset), cmask[:, T_min:T_max+1], cmask.new_zeros(B, suffix)], dim=1)\n\n        # Ground truth \n        true_X = self.make_X_blocks(true_X, T_min, T_max, smask)\n        true_V = self.features._dihedrals(true_X)\n        true_AD = self.features._AD_features(true_X[:,:,1,:])\n        true_D, mask_2D = pairwise_distance(true_X, mask)\n        true_D = true_D ** 2\n\n        # initial loss\n        sloss = 0.\n        X, D, V, AD = self.init_coords(hS, mask)\n        X = X.detach().clone()\n        dloss = self.huber_loss(D, true_D)\n        vloss = self.mse_loss(V, true_V)\n        aloss = self.mse_loss(AD, true_AD)\n\n        for t in range(T_min, T_max + 1):\n            # Prepare input\n            V, E, E_idx = self.features(X, mask)\n            hS = self.make_S_blocks(LS, S, RS, T_min, T_max, smask)[0]\n\n            # Predict residue t\n            h = self.seq_mpn(V, E, hS, E_idx, mask)\n            h = self.attention(h, LS, smask, self.W_seq)\n            logits = self.O_s(h[:, offset + t - T_min])\n            snll = self.ce_loss(logits, true_S[:, t])\n            sloss = sloss + torch.sum(snll * cmask[:, offset + t - T_min])\n\n            # Teacher forcing on S\n            S = S.clone()\n            S[:, t] = true_S[:, t]\n            S = S.clone()\n\n            # Iterative refinement\n            if t % self.update_freq == 0:\n                h = self.struct_mpn(V, E, hS, E_idx, mask)\n                h = self.attention(h, LS, smask, self.W_stc)\n                X, D, V, AD = self.predict_dist(self.O_d(h))\n                X = X.detach().clone()\n                dloss = dloss + self.huber_loss(D, true_D)\n                vloss = vloss + self.mse_loss(V, true_V)\n                aloss = aloss + self.mse_loss(AD, true_AD)\n\n        dloss = torch.sum(dloss * mask_2D) / mask_2D.sum()\n        vloss = torch.sum(vloss * mask.unsqueeze(-1)) / mask.sum()\n        aloss = torch.sum(aloss * mask.unsqueeze(-1)) / mask.sum()\n        sloss = sloss.sum() / cmask.sum()\n        loss = sloss + dloss + vloss + aloss\n        return loss, sloss\n\n    def log_prob(self, true_S, true_cdr, mask):\n        B, N = mask.size(0), mask.size(1)\n        K = min(self.k_neighbors, N)\n\n        cdr_range = [(cdr.index(self.cdr_type), cdr.rindex(self.cdr_type)) for cdr in true_cdr]\n        T_min = min([l for l,r in cdr_range])\n        T_max = max([r for l,r in cdr_range])\n        cmask = self.get_completion_mask(B, N, cdr_range)\n        smask = mask.clone()\n\n        # initialize\n        S = true_S.clone() * (1 - cmask.long())\n        hS, _ = self.rnn(self.W_s(S))\n        LS, RS = hS[:, :, :self.hidden_size], hS[:, :, self.hidden_size:]\n        hS, mask, offset, suffix = self.make_S_blocks(LS, S, RS, T_min, T_max, mask)\n        cmask = torch.cat([cmask.new_zeros(B, offset), cmask[:, T_min:T_max+1], cmask.new_zeros(B, suffix)], dim=1)\n\n        sloss = 0.\n        X = self.init_coords(hS, mask)[0]\n        X = X.detach().clone()\n\n        for t in range(T_min, T_max + 1):\n            # Prepare input\n            V, E, E_idx = self.features(X, mask)\n            hS = self.make_S_blocks(LS, S, RS, T_min, T_max, smask)[0]\n\n            # Predict residue t\n            h = self.seq_mpn(V, E, hS, E_idx, mask)\n            h = self.attention(h, LS, smask, self.W_seq)\n            logits = self.O_s(h[:, offset + t - T_min])\n            snll = self.ce_loss(logits, true_S[:, t])\n            sloss = sloss + snll * cmask[:, offset + t - T_min]\n\n            # Teacher forcing on S\n            S = S.clone()\n            S[:, t] = true_S[:, t]\n            S = S.clone()\n\n            # Iterative refinement\n            if t % self.update_freq == 0:\n                h = self.struct_mpn(V, E, hS, E_idx, mask)\n                h = self.attention(h, LS, smask, self.W_stc)\n                X = self.predict_dist(self.O_d(h))[0]\n                X = X.detach().clone()\n\n        ppl = sloss / cmask.sum(dim=-1)\n        sloss = sloss.sum() / cmask.sum()\n        return ReturnType(nll=sloss, ppl=ppl, X=X, X_cdr=X[:, offset:offset+T_max-T_min+1])\n\n    def generate(self, true_S, true_cdr, mask, return_ppl=False):\n        B, N = mask.size(0), mask.size(1)\n        K = min(self.k_neighbors, N)\n\n        cdr_range = [(cdr.index(self.cdr_type), cdr.rindex(self.cdr_type)) for cdr in true_cdr]\n        T_min = min([l for l,r in cdr_range])\n        T_max = max([r for l,r in cdr_range])\n        cmask = self.get_completion_mask(B, N, cdr_range)\n        smask = mask.clone()\n\n        # initialize\n        S = true_S.clone() * (1 - cmask.long())\n        hS, _ = self.rnn(self.W_s(S))\n        LS, RS = hS[:, :, :self.hidden_size], hS[:, :, self.hidden_size:]\n        hS, mask, offset, suffix = self.make_S_blocks(LS, S, RS, T_min, T_max, mask)\n        cmask = torch.cat([cmask.new_zeros(B, offset), cmask[:, T_min:T_max+1], cmask.new_zeros(B, suffix)], dim=1)\n\n        X = self.init_coords(hS, mask)[0]\n        X = X.detach().clone()\n        sloss = 0\n\n        for t in range(T_min, T_max + 1):\n            # Prepare input\n            V, E, E_idx = self.features(X, mask)\n            hS = self.make_S_blocks(LS, S, RS, T_min, T_max, smask)[0]\n\n            # Predict residue t\n            h = self.seq_mpn(V, E, hS, E_idx, mask)\n            h = self.attention(h, LS, smask, self.W_seq)\n            logits = self.O_s(h[:, offset + t - T_min])\n            prob = F.softmax(logits, dim=-1)  # [B, 20]\n            S[:, t] = torch.multinomial(prob, num_samples=1).squeeze(-1)  # [B, 1]\n            sloss = sloss + self.ce_loss(logits, S[:, t]) * cmask[:, offset + t - T_min]\n\n            # Iterative refinement\n            h = self.struct_mpn(V, E, hS, E_idx, mask)\n            h = self.attention(h, LS, smask, self.W_stc)\n            X = self.predict_dist(self.O_d(h))[0]\n            X = X.detach().clone()\n\n        S = S.tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(cdr_range[i][0], cdr_range[i][1] + 1)]) for i in range(B)]\n        ppl = torch.exp(sloss / cmask.sum(dim=-1))\n        return (S, ppl, X[:, offset:offset+T_max-T_min+1]) if return_ppl else S", ""]}
{"filename": "structgen/protein_features.py", "chunked_list": ["from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport copy\n\nfrom matplotlib import pyplot as plt", "\nfrom matplotlib import pyplot as plt\n\nfrom structgen.utils import gather_edges, gather_nodes, Normalize\n\n\nclass PositionalEncodings(nn.Module):\n\n    def __init__(self, num_embeddings, period_range=[2,1000]):\n        super(PositionalEncodings, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.period_range = period_range \n\n    def forward(self, E_idx):\n        # i-j\n        N_batch = E_idx.size(0)\n        N_nodes = E_idx.size(1)\n        N_neighbors = E_idx.size(2)\n        ii = torch.arange(N_nodes, dtype=torch.float32).view((1, -1, 1)).cuda()\n        d = (E_idx.float() - ii).unsqueeze(-1)\n        # Original Transformer frequencies\n        frequency = torch.exp(\n            torch.arange(0, self.num_embeddings, 2, dtype=torch.float32)\n            * -(np.log(10000.0) / self.num_embeddings)\n        ).cuda()\n        # Grid-aligned\n        # frequency = 2. * np.pi * torch.exp(\n        #     -torch.linspace(\n        #         np.log(self.period_range[0]), \n        #         np.log(self.period_range[1]),\n        #         self.num_embeddings / 2\n        #     )\n        # )\n        angles = d * frequency.view((1,1,1,-1))\n        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n        return E", "\n\nclass ProteinFeatures(nn.Module):\n\n    def __init__(self, num_positional_embeddings=16, num_rbf=16, top_k=30, features_type='full', direction='forward'):\n        \"\"\" Extract protein features \"\"\"\n        super(ProteinFeatures, self).__init__()\n        self.top_k = top_k\n        self.num_rbf = num_rbf\n        self.num_positional_embeddings = num_positional_embeddings\n        self.direction = direction\n\n        # Feature types\n        self.features_type = features_type\n        self.feature_dimensions = {\n            'frame': (6, num_positional_embeddings + num_rbf + 7),\n            'full': (6, num_positional_embeddings + num_rbf + 7),\n            'dist': (6, num_positional_embeddings + num_rbf),\n            'hbonds': (3, 2 * num_positional_embeddings),\n        }\n\n        # Positional encoding\n        self.embeddings = PositionalEncodings(num_positional_embeddings)\n        \n    def _dist(self, X, mask, eps=1E-6):\n        \"\"\" Pairwise euclidean distances \"\"\"\n        N = X.size(1)\n        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n        if self.direction == 'bidirectional':\n            mask_2D = mask_2D - torch.eye(N).unsqueeze(0).cuda()  # remove self\n            mask_2D = mask_2D.clamp(min=0)\n        elif self.direction == 'forward':\n            nmask = torch.arange(X.size(1)).cuda()\n            nmask = nmask.view(1,-1,1) > nmask.view(1,1,-1)\n            mask_2D = nmask.float() * mask_2D  # [B, N, N]\n        else:\n            raise ValueError('invalid direction', direction)\n\n        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n\n        # Identify k nearest neighbors (not including self)\n        D_adjust = D + (1. - mask_2D) * 10000\n        top_k = min(self.top_k, N)\n        D_neighbors, E_idx = torch.topk(D_adjust, top_k, dim=-1, largest=False)\n        mask_neighbors = gather_edges(mask_2D.unsqueeze(-1), E_idx)\n\n        # Debug plot KNN\n        # print(E_idx[:10,:10])\n        # D_simple = mask_2D * torch.zeros(D.size()).scatter(-1, E_idx, torch.ones_like(knn_D))\n        # print(D_simple)\n        # fig = plt.figure(figsize=(4,4))\n        # ax = fig.add_subplot(111)\n        # D_simple = D.data.numpy()[0,:,:]\n        # plt.imshow(D_simple, aspect='equal')\n        # plt.axis('off')\n        # plt.tight_layout()\n        # plt.savefig('D_knn.pdf')\n        # exit(0)\n        return D_neighbors, E_idx, mask_neighbors\n\n    def _rbf(self, D):\n        # Distance radial basis function\n        D_min, D_max, D_count = 0., 20., self.num_rbf\n        D_mu = torch.linspace(D_min, D_max, D_count).cuda()\n        D_mu = D_mu.view([1,1,1,-1])\n        D_sigma = (D_max - D_min) / D_count\n        D_expand = torch.unsqueeze(D, -1)\n        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n\n        # for i in range(D_count):\n        #     fig = plt.figure(figsize=(4,4))\n        #     ax = fig.add_subplot(111)\n        #     rbf_i = RBF.data.numpy()[0,i,:,:]\n        #     # rbf_i = D.data.numpy()[0,0,:,:]\n        #     plt.imshow(rbf_i, aspect='equal')\n        #     plt.axis('off')\n        #     plt.tight_layout()\n        #     plt.savefig('rbf{}.pdf'.format(i))\n        #     print(np.min(rbf_i), np.max(rbf_i), np.mean(rbf_i))\n        # exit(0)\n        return RBF\n\n    def _quaternions(self, R):\n        \"\"\" Convert a batch of 3D rotations [R] to quaternions [Q]\n            R [...,3,3]\n            Q [...,4]\n        \"\"\"\n        # Simple Wikipedia version\n        # en.wikipedia.org/wiki/Rotation_matrix#Quaternion\n        # For other options see math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n        diag = torch.diagonal(R, dim1=-2, dim2=-1)\n        Rxx, Ryy, Rzz = diag.unbind(-1)\n        magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n              Rxx - Ryy - Rzz, \n            - Rxx + Ryy - Rzz, \n            - Rxx - Ryy + Rzz\n        ], -1)))\n        _R = lambda i,j: R[:,:,:,i,j]\n        signs = torch.sign(torch.stack([\n            _R(2,1) - _R(1,2),\n            _R(0,2) - _R(2,0),\n            _R(1,0) - _R(0,1)\n        ], -1))\n        xyz = signs * magnitudes\n        # The relu enforces a non-negative trace\n        w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n        Q = torch.cat((xyz, w), -1)\n        Q = F.normalize(Q, dim=-1)\n\n        # Axis of rotation\n        # Replace bad rotation matrices with identity\n        # I = torch.eye(3).view((1,1,1,3,3))\n        # I = I.expand(*(list(R.shape[:3]) + [-1,-1]))\n        # det = (\n        #     R[:,:,:,0,0] * (R[:,:,:,1,1] * R[:,:,:,2,2] - R[:,:,:,1,2] * R[:,:,:,2,1])\n        #     - R[:,:,:,0,1] * (R[:,:,:,1,0] * R[:,:,:,2,2] - R[:,:,:,1,2] * R[:,:,:,2,0])\n        #     + R[:,:,:,0,2] * (R[:,:,:,1,0] * R[:,:,:,2,1] - R[:,:,:,1,1] * R[:,:,:,2,0])\n        # )\n        # det_mask = torch.abs(det.unsqueeze(-1).unsqueeze(-1))\n        # R = det_mask * R + (1 - det_mask) * I\n\n        # DEBUG\n        # https://math.stackexchange.com/questions/2074316/calculating-rotation-axis-from-rotation-matrix\n        # Columns of this are in rotation plane\n        # A = R - I\n        # v1, v2 = A[:,:,:,:,0], A[:,:,:,:,1]\n        # axis = F.normalize(torch.cross(v1, v2), dim=-1)\n        return Q\n\n    def _contacts(self, D_neighbors, E_idx, mask_neighbors, cutoff=8):\n        \"\"\" Contacts \"\"\"\n        D_neighbors = D_neighbors.unsqueeze(-1)\n        neighbor_C = mask_neighbors * (D_neighbors < cutoff).type(torch.float32)\n        return neighbor_C\n\n    def _hbonds(self, X, E_idx, mask_neighbors, eps=1E-3):\n        \"\"\" Hydrogen bonds and contact map\n        \"\"\"\n        X_atoms = dict(zip(['N', 'CA', 'C', 'O'], torch.unbind(X, 2)))\n\n        # Virtual hydrogens\n        X_atoms['C_prev'] = F.pad(X_atoms['C'][:,1:,:], (0,0,0,1), 'constant', 0)\n        X_atoms['H'] = X_atoms['N'] + F.normalize(\n             F.normalize(X_atoms['N'] - X_atoms['C_prev'], -1)\n          +  F.normalize(X_atoms['N'] - X_atoms['CA'], -1)\n        , -1)\n\n        def _distance(X_a, X_b):\n            return torch.norm(X_a[:,None,:,:] - X_b[:,:,None,:], dim=-1)\n\n        def _inv_distance(X_a, X_b):\n            return 1. / (_distance(X_a, X_b) + eps)\n\n        # DSSP vacuum electrostatics model\n        U = (0.084 * 332) * (\n              _inv_distance(X_atoms['O'], X_atoms['N'])\n            + _inv_distance(X_atoms['C'], X_atoms['H'])\n            - _inv_distance(X_atoms['O'], X_atoms['H'])\n            - _inv_distance(X_atoms['C'], X_atoms['N'])\n        )\n\n        HB = (U < -0.5).type(torch.float32)\n        neighbor_HB = mask_neighbors * gather_edges(HB.unsqueeze(-1),  E_idx)\n        # print(HB)\n        # HB = F.sigmoid(U)\n        # U_np = U.cpu().data.numpy()\n        # # plt.matshow(np.mean(U_np < -0.5, axis=0))\n        # plt.matshow(HB[0,:,:])\n        # plt.colorbar()\n        # plt.show()\n        # D_CA = _distance(X_atoms['CA'], X_atoms['CA'])\n        # D_CA = D_CA.cpu().data.numpy()\n        # plt.matshow(D_CA[0,:,:] < contact_D)\n        # # plt.colorbar()\n        # plt.show()\n        # exit(0)\n        return neighbor_HB\n\n    def _AD_features(self, X, eps=1e-6):\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Bond angle calculation\n        cosA = -(u_1 * u_0).sum(-1)\n        cosA = torch.clamp(cosA, -1+eps, 1-eps)\n        A = torch.acos(cosA)\n        # Angle between normals\n        cosD = (n_2 * n_1).sum(-1)\n        cosD = torch.clamp(cosD, -1+eps, 1-eps)\n        D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n        # Backbone features\n        AD_features = torch.stack((torch.cos(A), torch.sin(A) * torch.cos(D), torch.sin(A) * torch.sin(D)), 2)\n        return F.pad(AD_features, (0,0,1,2), 'constant', 0)\n\n    def _orientations_coarse(self, X, E_idx, eps=1e-6):\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Build relative orientations\n        o_1 = F.normalize(u_2 - u_1, dim=-1)\n        O = torch.stack((o_1, n_2, torch.cross(o_1, n_2)), 2)\n        O = O.view(list(O.shape[:2]) + [9])\n        O = F.pad(O, (0,0,1,2), 'constant', 0)\n\n        O_neighbors = gather_nodes(O, E_idx)\n        X_neighbors = gather_nodes(X, E_idx)\n        \n        # Re-view as rotation matrices\n        O = O.view(list(O.shape[:2]) + [3,3])\n        O_neighbors = O_neighbors.view(list(O_neighbors.shape[:3]) + [3,3])\n\n        # Rotate into local reference frames\n        dX = X_neighbors - X.unsqueeze(-2)\n        dU = torch.matmul(O.unsqueeze(2), dX.unsqueeze(-1)).squeeze(-1)\n        dU = F.normalize(dU, dim=-1)\n        R = torch.matmul(O.unsqueeze(2).transpose(-1,-2), O_neighbors)\n        Q = self._quaternions(R)\n\n        # Orientation features\n        O_features = torch.cat((dU,Q), dim=-1)\n\n        # Frame features (neighbors sorted)\n        X_neighbors = gather_nodes(X, E_idx.sort(dim=-1).values)\n        dX = X_neighbors - X.unsqueeze(-2)\n        F_vectors = torch.cross(dX[:, :, 1:], dX[:, :, :-1], dim=-1)\n        F_norms = torch.norm(F_vectors, dim=-1)\n        F_vectors = F.normalize(F_vectors, dim=-1)\n        F_vectors = torch.matmul(O.unsqueeze(2), F_vectors.unsqueeze(-1)).squeeze(-1)\n\n        return O_features, F_norms, F_vectors\n\n    def _dihedrals(self, X, eps=1e-7):\n        # First 3 coordinates are N, CA, C\n        X = X[:,:,:3,:].reshape(X.shape[0], 3*X.shape[1], 3)\n\n        # Shifted slices of unit vectors\n        dX = X[:,1:,:] - X[:,:-1,:]\n        U = F.normalize(dX, dim=-1)\n        u_2 = U[:,:-2,:]\n        u_1 = U[:,1:-1,:]\n        u_0 = U[:,2:,:]\n        # Backbone normals\n        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n\n        # Angle between normals\n        cosD = (n_2 * n_1).sum(-1)\n        cosD = torch.clamp(cosD, -1+eps, 1-eps)\n        D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n\n        D = F.pad(D, (3,0), 'constant', 0)\n        D = D.view((D.size(0), int(D.size(1)/3), 3))\n        phi, psi, omega = torch.unbind(D,-1)\n\n        # print(cosD.cpu().data.numpy().flatten())\n        # print(omega.sum().cpu().data.numpy().flatten())\n\n        # Bond angle calculation\n        # A = torch.acos(-(u_1 * u_0).sum(-1))\n\n        # DEBUG: Ramachandran plot\n        # x = phi.cpu().data.numpy().flatten()\n        # y = psi.cpu().data.numpy().flatten()\n        # plt.scatter(x * 180 / np.pi, y * 180 / np.pi, s=1, marker='.')\n        # plt.xlabel('phi')\n        # plt.ylabel('psi')\n        # plt.axis('square')\n        # plt.grid()\n        # plt.axis([-180,180,-180,180])\n        # plt.show()\n\n        # Lift angle representations to the circle\n        D_features = torch.cat((torch.cos(D), torch.sin(D)), 2)\n        return D_features\n\n    def forward(self, X, mask):\n        \"\"\" Featurize coordinates as an attributed graph \"\"\"\n        # Build k-Nearest Neighbors graph\n        X_ca = X[:,:,1,:]\n        D_neighbors, E_idx, mask_neighbors = self._dist(X_ca, mask)\n        RBF = self._rbf(D_neighbors)\n        E_positional = self.embeddings(E_idx)\n\n        # Pairwise and triplet features\n        O_features, F_norms, F_features = self._orientations_coarse(X_ca, E_idx)\n        F_RBF = self._rbf(F_norms)\n\n        if self.features_type == 'frame':\n            # Coarse backbone features\n            V = self._dihedrals(X)\n            E2 = torch.cat((E_positional, RBF, O_features), -1)\n            E_sorted = self.embeddings(E_idx.sort(dim=-1).values)\n            E3 = torch.cat((E_sorted[:,:,:-1], F_RBF, F_features), -1)\n            E = (E2, E3)\n        elif self.features_type == 'hbonds':\n            # Hydrogen bonds and contacts\n            neighbor_HB = self._hbonds(X, E_idx, mask_neighbors)\n            neighbor_C = self._contacts(D_neighbors, E_idx, mask_neighbors)\n            # Pack\n            V = mask.unsqueeze(-1) * torch.ones_like(AD_features)\n            neighbor_C = neighbor_C.expand(-1,-1,-1, int(self.num_positional_embeddings / 2))\n            neighbor_HB = neighbor_HB.expand(-1,-1,-1, int(self.num_positional_embeddings / 2))\n            E = torch.cat((E_positional, neighbor_C, neighbor_HB), -1)\n        elif self.features_type == 'full':\n            # Full backbone angles\n            V = self._dihedrals(X)\n            E = torch.cat((E_positional, RBF, O_features), -1)\n        elif self.features_type == 'dist':\n            # Full backbone angles\n            V = self._dihedrals(X)\n            E = torch.cat((E_positional, RBF), -1)\n\n        return V, E, E_idx", "\n        # DEBUG\n        # U = (np.nan * torch.zeros(X.size(0),X.size(1),X.size(1),3)).scatter(2, E_idx.unsqueeze(-1).expand(-1,-1,-1,3), E[:,:,:,:3])\n        # plt.imshow(U.data.numpy()[0,:,:,0])\n        # plt.show()\n        # exit(0)\n"]}
{"filename": "structgen/sequence.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom structgen.data import alphabet\nfrom structgen.utils import ReturnType\n\n\nclass SeqModel(nn.Module):\n\n    def __init__(self, args):\n        super(SeqModel, self).__init__()\n        self.hidden_size = args.hidden_size\n        self.W_s = nn.Embedding(args.vocab_size, args.hidden_size)\n\n        self.lstm = nn.LSTM(\n                args.hidden_size, args.hidden_size, \n                batch_first=True, num_layers=args.depth,\n                dropout=args.dropout\n        )\n        self.W_out = nn.Linear(args.hidden_size, args.vocab_size, bias=True)\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, S, mask):\n        h_S = self.W_s(S)\n        h_S_shift = F.pad(h_S[:,0:-1], (0,0,1,0), 'constant', 0)\n        h_V, _ = self.lstm(h_S_shift)\n        logits = self.W_out(h_V)\n        loss = self.ce_loss(logits.view(-1, logits.size(-1)), S.view(-1))\n        loss = torch.sum(loss * mask.view(-1)) / mask.sum()\n        return loss\n\n    def log_prob(self, S, mask):\n        return ReturnType(nll=self(S, mask))\n\n    def generate(self, B, N):\n        h = torch.zeros(self.lstm.num_layers, B, self.hidden_size).cuda()\n        c = torch.zeros(self.lstm.num_layers, B, self.hidden_size).cuda()\n        S = torch.zeros(B, N + 1).long().cuda()\n        for t in range(N):\n            h_S = self.W_s(S[:, t:t+1])\n            h_V, (h, c) = self.lstm(h_S, (h, c))\n            logits = self.W_out(h_V)\n            prob = F.softmax(logits, dim=-1).squeeze(1)\n            S[:, t+1] = torch.multinomial(prob, num_samples=1).squeeze(-1)\n        \n        S = S[:, 1:].tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(N)]) for i in range(B)]\n        return S", "class SeqModel(nn.Module):\n\n    def __init__(self, args):\n        super(SeqModel, self).__init__()\n        self.hidden_size = args.hidden_size\n        self.W_s = nn.Embedding(args.vocab_size, args.hidden_size)\n\n        self.lstm = nn.LSTM(\n                args.hidden_size, args.hidden_size, \n                batch_first=True, num_layers=args.depth,\n                dropout=args.dropout\n        )\n        self.W_out = nn.Linear(args.hidden_size, args.vocab_size, bias=True)\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, S, mask):\n        h_S = self.W_s(S)\n        h_S_shift = F.pad(h_S[:,0:-1], (0,0,1,0), 'constant', 0)\n        h_V, _ = self.lstm(h_S_shift)\n        logits = self.W_out(h_V)\n        loss = self.ce_loss(logits.view(-1, logits.size(-1)), S.view(-1))\n        loss = torch.sum(loss * mask.view(-1)) / mask.sum()\n        return loss\n\n    def log_prob(self, S, mask):\n        return ReturnType(nll=self(S, mask))\n\n    def generate(self, B, N):\n        h = torch.zeros(self.lstm.num_layers, B, self.hidden_size).cuda()\n        c = torch.zeros(self.lstm.num_layers, B, self.hidden_size).cuda()\n        S = torch.zeros(B, N + 1).long().cuda()\n        for t in range(N):\n            h_S = self.W_s(S[:, t:t+1])\n            h_V, (h, c) = self.lstm(h_S, (h, c))\n            logits = self.W_out(h_V)\n            prob = F.softmax(logits, dim=-1).squeeze(1)\n            S[:, t+1] = torch.multinomial(prob, num_samples=1).squeeze(-1)\n        \n        S = S[:, 1:].tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(N)]) for i in range(B)]\n        return S", "\n\nclass Seq2Seq(nn.Module):\n\n    def __init__(self, args):\n        super(Seq2Seq, self).__init__()\n        self.hidden_size = args.hidden_size\n        self.W_s = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.W_a = nn.Embedding(args.vocab_size, args.hidden_size)\n\n        self.encoder = nn.LSTM(\n                args.hidden_size, args.hidden_size, \n                batch_first=True, num_layers=args.depth,\n                dropout=args.dropout\n        )\n        self.decoder = nn.LSTM(\n                args.hidden_size, args.hidden_size, \n                batch_first=True, num_layers=args.depth,\n                dropout=args.dropout\n        )\n        self.W_out = nn.Linear(args.hidden_size * 2, args.vocab_size, bias=True)\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def RL_parameters(self):\n        return self.parameters()\n\n    def encode(self, aS, amask):\n        h_S = self.W_a(aS)\n        h_V, _ = self.encoder(h_S)  # [B, M, H]\n        return h_V * amask.unsqueeze(-1)\n\n    def forward(self, S, mask, context):\n        h_S = self.W_s(S)\n        h_S_shift = F.pad(h_S[:,0:-1], (0,0,1,0), 'constant', 0)\n        h_V, _ = self.decoder(h_S_shift)  # [B, N, H]\n\n        # attention\n        aS, amask, _ = context\n        h_A = self.encode(aS, amask)  # [B, M, H]\n        att = torch.bmm(h_V, h_A.transpose(1, 2))  # [B, N, M]\n        att_mask = mask.unsqueeze(2) * amask.unsqueeze(1)  # [B, N, 1] * [B, 1, M]\n        att = att - (1 - att_mask) * 1e6  # attention mask\n        att = F.softmax(att, dim=-1)  # [B, N, M]\n        h_att = torch.bmm(att, h_A)\n        h_out = torch.cat([h_V, h_att], dim=-1)\n\n        logits = self.W_out(h_out)\n        loss = self.ce_loss(logits.view(-1, logits.size(-1)), S.view(-1))\n        loss = torch.sum(loss * mask.view(-1)) / mask.sum()\n        return loss\n\n    def log_prob(self, S, mask, context):\n        B, N = S.size(0), S.size(1)\n        h_S = self.W_s(S)\n        h_S_shift = F.pad(h_S[:,0:-1], (0,0,1,0), 'constant', 0)\n        h_V, _ = self.decoder(h_S_shift)  # [B, N, H]\n\n        # attention\n        aS, amask, _ = context\n        h_A = self.encode(aS, amask)  # [B, M, H]\n        att = torch.bmm(h_V, h_A.transpose(1, 2))  # [B, N, M]\n        att_mask = mask.unsqueeze(2) * amask.unsqueeze(1)  # [B, N, 1] * [B, 1, M]\n        att = att - (1 - att_mask) * 1e6  # attention mask\n        att = F.softmax(att, dim=-1)  # [B, N, M]\n        h_att = torch.bmm(att, h_A)\n        h_out = torch.cat([h_V, h_att], dim=-1)\n\n        logits = self.W_out(h_out)\n        loss = self.ce_loss(logits.view(-1, logits.size(-1)), S.view(-1))\n        loss = loss.view(B, N)\n        ppl = torch.sum(loss * mask, dim=-1) / mask.sum(dim=-1)\n        nll = torch.sum(loss * mask) / mask.sum()\n\n        return ReturnType(nll=nll, ppl=ppl)\n\n    def generate(self, B, N, context, return_ppl=False):\n        aS, amask, _ = context\n        h_A = self.encode(aS, amask)  # [B, M, H]\n\n        h = torch.zeros(self.decoder.num_layers, B, self.hidden_size).cuda()\n        c = torch.zeros(self.decoder.num_layers, B, self.hidden_size).cuda()\n        S = torch.zeros(B, N + 1).long().cuda()\n        sloss = 0.\n\n        for t in range(N):\n            h_S = self.W_s(S[:, t:t+1])\n            h_V, (h, c) = self.decoder(h_S, (h, c))\n\n            att = torch.bmm(h_V, h_A.transpose(1, 2))  # [B, 1, M]\n            att_mask = amask.unsqueeze(1)  # [B, 1, M]\n            att = att - (1 - att_mask) * 1e6  # attention mask\n            att = F.softmax(att, dim=-1)  # [B, 1, M]\n            h_att = torch.bmm(att, h_A)   # [B, 1, H]\n            h_out = torch.cat([h_V, h_att], dim=-1)\n\n            logits = self.W_out(h_out).squeeze(1)\n            prob = F.softmax(logits, dim=-1)\n            S[:, t+1] = torch.multinomial(prob, num_samples=1).squeeze(-1)\n            sloss = sloss + self.ce_loss(logits, S[:, t+1])\n        \n        S = S[:, 1:].tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(N)]) for i in range(B)]\n        ppl = torch.exp(sloss / N)\n        return (S, ppl) if return_ppl else S", "\n\n"]}
{"filename": "structgen/__init__.py", "chunked_list": ["from structgen.data import *\nfrom structgen.decoder import Decoder\nfrom structgen.revision import RevisionDecoder\nfrom structgen.encoder import MPNEncoder\nfrom structgen.hierarchical import HierarchicalDecoder\nfrom structgen.sequence import SeqModel, Seq2Seq\nfrom structgen.utils import compute_rmsd\n"]}
{"filename": "structgen/utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import namedtuple\n\nReturnType = namedtuple('ReturnType',('nll','ppl','X','X_cdr'), defaults=(None, None, None, None))\n\n# problem: hard to add mask in SVD\ndef kabsch(A, B):\n    a_mean = A.mean(dim=1, keepdims=True)\n    b_mean = B.mean(dim=1, keepdims=True)\n    A_c = A - a_mean\n    B_c = B - b_mean\n    # Covariance matrix\n    H = torch.bmm(A_c.transpose(1,2), B_c)  # [B, 3, 3]\n    U, S, V = torch.svd(H)\n    # Rotation matrix\n    R = torch.bmm(V, U.transpose(1,2))  # [B, 3, 3]\n    # Translation vector\n    t = b_mean - torch.bmm(R, a_mean.transpose(1,2)).transpose(1,2)\n    A_aligned = torch.bmm(R, A.transpose(1,2)).transpose(1,2) + t\n    return A_aligned, R, t", "# problem: hard to add mask in SVD\ndef kabsch(A, B):\n    a_mean = A.mean(dim=1, keepdims=True)\n    b_mean = B.mean(dim=1, keepdims=True)\n    A_c = A - a_mean\n    B_c = B - b_mean\n    # Covariance matrix\n    H = torch.bmm(A_c.transpose(1,2), B_c)  # [B, 3, 3]\n    U, S, V = torch.svd(H)\n    # Rotation matrix\n    R = torch.bmm(V, U.transpose(1,2))  # [B, 3, 3]\n    # Translation vector\n    t = b_mean - torch.bmm(R, a_mean.transpose(1,2)).transpose(1,2)\n    A_aligned = torch.bmm(R, A.transpose(1,2)).transpose(1,2) + t\n    return A_aligned, R, t", "\n# A: [B, N, 3], B: [B, N, 3], mask: [B, N]\ndef compute_rmsd(A, B, mask):\n    A_aligned, _, _ = kabsch(A, B)\n    rmsd = ((A_aligned - B) ** 2).sum(dim=-1)\n    rmsd = torch.sum(rmsd * mask, dim=-1) / (mask.sum(dim=-1) + 1e-6)\n    return rmsd.sqrt()\n\ndef autoregressive_mask(E_idx):\n    N_nodes = E_idx.size(1)\n    ii = torch.arange(N_nodes).cuda()\n    ii = ii.view((1, -1, 1))\n    mask = E_idx - ii < 0\n    return mask.float()", "def autoregressive_mask(E_idx):\n    N_nodes = E_idx.size(1)\n    ii = torch.arange(N_nodes).cuda()\n    ii = ii.view((1, -1, 1))\n    mask = E_idx - ii < 0\n    return mask.float()\n\ndef fit_coords(D, E_idx, mask, lr=2.0, num_steps=200):\n    with torch.enable_grad():\n        pred_xyz = torch.randn(D.size(0), D.size(1), 3).cuda()\n        pred_xyz = pred_xyz.requires_grad_()\n        optimizer = torch.optim.Adam([pred_xyz], lr=lr)\n        vmask = autoregressive_mask(E_idx) * mask.unsqueeze(-1)\n\n        for _ in range(num_steps):\n            optimizer.zero_grad()\n            cur_D = (pred_xyz.unsqueeze(2) - pred_xyz.unsqueeze(1)).norm(dim=-1, p=2)  # [B, N, N]\n            cur_D = gather_edges(cur_D.unsqueeze(-1), E_idx).squeeze(-1)\n            loss = (D - cur_D) ** 2\n            loss = torch.sum(loss * vmask) / vmask.sum()\n            loss.backward()\n            optimizer.step()\n\n    return pred_xyz.detach().clone()", "\n# The following gather functions\ndef gather_edges(edges, neighbor_idx):\n    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n    edge_features = torch.gather(edges, 2, neighbors)\n    return edge_features\n\ndef gather_nodes(nodes, neighbor_idx):\n    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n    # Gather and re-pack\n    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n    return neighbor_features", "def gather_nodes(nodes, neighbor_idx):\n    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n    # Gather and re-pack\n    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n    return neighbor_features\n\ndef cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n    h_nodes = gather_nodes(h_nodes, E_idx)\n    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n    return h_nn", "\ndef cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n    h_nodes = gather_nodes(h_nodes, E_idx)\n    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n    return h_nn\n\ndef gather_2d(x, idx):\n    # x: [B, N], idx: [B, K]\n    batch_size, nei_size = idx.size(0), idx.size(1)\n    idx_flat1 = idx.reshape(-1)\n    idx_flat0 = torch.cat([torch.arange(batch_size)] * nei_size, dim=0)\n    new_x = x[idx_flat0, idx_flat1]\n    return new_x.view(batch_size, nei_size)", "\n# h: [B, N, H], x: [B, 1, H]\ndef insert_tensor(h, x, t):\n    if t == 0:\n        return torch.cat((x, h[:, 1:]), dim=1)\n    elif t == h.size(1) - 1:\n        return torch.cat((h[:, :-1], x), dim=1)\n    else:\n        return torch.cat((h[:, :t], x, h[:, t+1:]), dim=1)\n\ndef pairwise_distance(X, mask):\n    X_ca = X[:, :, 1, :]  # alpha carbon\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)\n    dX = X_ca.unsqueeze(1) - X_ca.unsqueeze(2)\n    D = mask_2D * torch.sqrt(torch.sum(dX**2, dim=3))\n    return D, mask_2D", "\ndef pairwise_distance(X, mask):\n    X_ca = X[:, :, 1, :]  # alpha carbon\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)\n    dX = X_ca.unsqueeze(1) - X_ca.unsqueeze(2)\n    D = mask_2D * torch.sqrt(torch.sum(dX**2, dim=3))\n    return D, mask_2D\n\ndef self_square_dist(X, mask):\n    X = X[:, :, 1] \n    dX = X.unsqueeze(1) - X.unsqueeze(2)  # [B, 1, N, 3] - [B, N, 1, 3]\n    D = torch.sum(dX**2, dim=-1)\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)  # [B, 1, N] x [B, N, 1]\n    mask_2D = mask_2D - torch.eye(mask.size(1))[None,:,:].cuda()\n    return D, mask_2D", "def self_square_dist(X, mask):\n    X = X[:, :, 1] \n    dX = X.unsqueeze(1) - X.unsqueeze(2)  # [B, 1, N, 3] - [B, N, 1, 3]\n    D = torch.sum(dX**2, dim=-1)\n    mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)  # [B, 1, N] x [B, N, 1]\n    mask_2D = mask_2D - torch.eye(mask.size(1))[None,:,:].cuda()\n    return D, mask_2D\n\ndef get_nei_label(X, mask, K):\n    D, mask_2D = pairwise_distance(X, mask)\n    nmask = torch.arange(X.size(1)).cuda()\n    nmask = nmask.view(1,-1,1) > nmask.view(1,1,-1)\n    nmask = nmask.float() * mask_2D  # [B, N, N]\n\n    # Identify k nearest neighbors (including self)\n    D_adjust = D + (1. - nmask) * 100000\n    D_sorted, E_idx = torch.topk(D_adjust, D.size(1), dim=-1, largest=False)\n    E_next = E_idx[:, 1:]\n    D_next = D_sorted[:, 1:]\n    nmask = gather_edges(nmask.unsqueeze(-1), E_next)  # [B, N-1, N, 1]\n    nlabel = torch.zeros_like(E_next)\n    nlabel[:, :, :K] = 1\n    return E_next, nlabel, D_next, nmask.squeeze(-1)", "def get_nei_label(X, mask, K):\n    D, mask_2D = pairwise_distance(X, mask)\n    nmask = torch.arange(X.size(1)).cuda()\n    nmask = nmask.view(1,-1,1) > nmask.view(1,1,-1)\n    nmask = nmask.float() * mask_2D  # [B, N, N]\n\n    # Identify k nearest neighbors (including self)\n    D_adjust = D + (1. - nmask) * 100000\n    D_sorted, E_idx = torch.topk(D_adjust, D.size(1), dim=-1, largest=False)\n    E_next = E_idx[:, 1:]\n    D_next = D_sorted[:, 1:]\n    nmask = gather_edges(nmask.unsqueeze(-1), E_next)  # [B, N-1, N, 1]\n    nlabel = torch.zeros_like(E_next)\n    nlabel[:, :, :K] = 1\n    return E_next, nlabel, D_next, nmask.squeeze(-1)", "\n\nclass Normalize(nn.Module):\n\n    def __init__(self, features, epsilon=1e-6):\n        super(Normalize, self).__init__()\n        self.gain = nn.Parameter(torch.ones(features))\n        self.bias = nn.Parameter(torch.zeros(features))\n        self.epsilon = epsilon\n\n    def forward(self, x, dim=-1):\n        mu = x.mean(dim, keepdim=True)\n        sigma = torch.sqrt(x.var(dim, keepdim=True) + self.epsilon)\n        gain = self.gain\n        bias = self.bias\n        # Reshape\n        if dim != -1:\n            shape = [1] * len(mu.size())\n            shape[dim] = self.gain.size()[0]\n            gain = gain.view(shape)\n            bias = bias.view(shape)\n\n        return gain * (x - mu) / (sigma + self.epsilon) + bias", "\n\nclass MPNNLayer(nn.Module):\n\n    def __init__(self, num_hidden, num_in, dropout):\n        super(MPNNLayer, self).__init__()\n        self.num_hidden = num_hidden\n        self.num_in = num_in\n        self.dropout = nn.Dropout(dropout)\n        self.norm = Normalize(num_hidden)\n        self.W = nn.Sequential(\n                nn.Linear(num_hidden + num_in, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n        )\n\n    def forward(self, h_V, h_E, mask_attend):\n        # h_V: [B, N, H]; h_E: [B, N, K, H]\n        # mask_attend: [B, N, K]\n        h_V_expand = h_V.unsqueeze(-2).expand(-1, -1, h_E.size(-2), -1)\n        h_EV = torch.cat([h_V_expand, h_E], dim=-1)  # [B, N, K, H]\n        h_message = self.W(h_EV) * mask_attend.unsqueeze(-1)\n        dh = torch.mean(h_message, dim=-2)\n        h_V = self.norm(h_V + self.dropout(dh))\n        return h_V", "\n\nclass FrameMPNNLayer(nn.Module):\n\n    def __init__(self, num_hidden, num_in, dropout):\n        super(FrameMPNNLayer, self).__init__()\n        self.num_hidden = num_hidden\n        self.num_in = num_in\n        self.dropout = nn.Dropout(dropout)\n        self.norm = Normalize(num_hidden)\n        self.W = nn.Sequential(\n                nn.Linear(num_hidden + num_in, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n        )\n        self.U = nn.Sequential(\n                nn.Linear(num_hidden + num_in, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n                nn.ReLU(),\n                nn.Linear(num_hidden, num_hidden),\n        )\n\n    def forward(self, h_V, h_E2, h_E3, mask2, mask3):\n        # h_V: [B, N, H]; h_E: [B, N, K, H]\n        # mask_attend: [B, N, K]\n        h_V_expand = h_V.unsqueeze(-2).expand(-1, -1, h_E2.size(-2), -1)\n        h_EV = torch.cat([h_V_expand, h_E2], dim=-1)  # [B, N, K, H]\n        h_message = self.W(h_EV) * mask2.unsqueeze(-1)\n        dh = torch.mean(h_message, dim=-2)\n\n        h_V_expand = h_V.unsqueeze(-2).expand(-1, -1, h_E3.size(-2), -1)\n        h_EV = torch.cat([h_V_expand, h_E3], dim=-1)  # [B, N, K, H]\n        mask3 = mask3[:,:,:-1] * mask3[:,:,1:]\n        u_message = self.U(h_EV) * mask3.unsqueeze(-1)\n        du = torch.mean(u_message, dim=-2)\n\n        h_V = self.norm(h_V + self.dropout(dh) + self.dropout(du))\n        return h_V", "\n\nclass PosEmbedding(nn.Module):\n\n    def __init__(self, num_embeddings):\n        super(PosEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n\n    def forward(self, E_idx):\n        # Original Transformer frequencies\n        frequency = torch.exp(\n            torch.arange(0, self.num_embeddings, 2, dtype=torch.float32)\n            * -(np.log(10000.0) / self.num_embeddings)\n        ).cuda()\n        angles = E_idx.unsqueeze(-1) * frequency.view((1,1,1,-1))\n        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n        return E", "\n\nif __name__ == \"__main__\":\n    # Test RMSD calculation\n    A = torch.tensor([[1., 1.], [2., 2.], [1.5, 3.]], dtype=torch.float32)\n    R0 = torch.tensor([[np.cos(60), -np.sin(60)], [np.sin(60), np.cos(60)]], dtype=torch.float32)\n    B = (R0.mm(A.T)).T\n    t0 = torch.tensor([3., 3.])\n    B += t0\n    C = B * torch.tensor([-1., 1.])\n    rmsd = compute_rmsd(\n            torch.stack([A, A], dim=0),\n            torch.stack([B, C], dim=0),\n            mask=torch.ones(2, 3)\n    )\n    print(rmsd)", "\n"]}
{"filename": "structgen/data.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport json, time, copy\nimport random\n\nDUMMY = {'pdb': None, 'seq': '#' * 10,\n        'coords': {\n            \"N\": np.zeros((10, 3)) + np.nan,\n            \"CA\":np.zeros((10, 3)) + np.nan,", "            \"N\": np.zeros((10, 3)) + np.nan,\n            \"CA\":np.zeros((10, 3)) + np.nan,\n            \"C\": np.zeros((10, 3)) + np.nan,\n            \"O\": np.zeros((10, 3)) + np.nan,\n        }\n}\nalphabet = '#ACDEFGHIKLMNPQRSTVWY'  # 0 is padding\nHYDROPATHY = {'#': 0, \"I\":4.5, \"V\":4.2, \"L\":3.8, \"F\":2.8, \"C\":2.5, \"M\":1.9, \"A\":1.8, \"W\":-0.9, \"G\":-0.4, \"T\":-0.7, \"S\":-0.8, \"Y\":-1.3, \"P\":-1.6, \"H\":-3.2, \"N\":-3.5, \"D\":-3.5, \"Q\":-3.5, \"E\":-3.5, \"K\":-3.9, \"R\":-4.5}\nVOLUME = {'#': 0, \"G\":60.1, \"A\":88.6, \"S\":89.0, \"C\":108.5, \"D\":111.1, \"P\":112.7, \"N\":114.1, \"T\":116.1, \"E\":138.4, \"V\":140.0, \"Q\":143.8, \"H\":153.2, \"M\":162.9, \"I\":166.7, \"L\":166.7, \"K\":168.6, \"R\":173.4, \"F\":189.9, \"Y\":193.6, \"W\":227.8}\nCHARGE = {**{'R':1, 'K':1, 'D':-1, 'E':-1, 'H':0.1}, **{x:0 for x in 'ABCFGIJLMNOPQSTUVWXYZ#'}}", "VOLUME = {'#': 0, \"G\":60.1, \"A\":88.6, \"S\":89.0, \"C\":108.5, \"D\":111.1, \"P\":112.7, \"N\":114.1, \"T\":116.1, \"E\":138.4, \"V\":140.0, \"Q\":143.8, \"H\":153.2, \"M\":162.9, \"I\":166.7, \"L\":166.7, \"K\":168.6, \"R\":173.4, \"F\":189.9, \"Y\":193.6, \"W\":227.8}\nCHARGE = {**{'R':1, 'K':1, 'D':-1, 'E':-1, 'H':0.1}, **{x:0 for x in 'ABCFGIJLMNOPQSTUVWXYZ#'}}\nPOLARITY = {**{x:1 for x in 'RNDQEHKSTY'}, **{x:0 for x in \"ACGILMFPWV#\"}}\nACCEPTOR = {**{x:1 for x in 'DENQHSTY'}, **{x:0 for x in \"RKWACGILMFPV#\"}}\nDONOR = {**{x:1 for x in 'RKWNQHSTY'}, **{x:0 for x in \"DEACGILMFPV#\"}}\nPMAP = lambda x: [HYDROPATHY[x] / 5, VOLUME[x] / 200, CHARGE[x], POLARITY[x], ACCEPTOR[x], DONOR[x]]\n\n\nclass AntibodyDataset():\n\n    def __init__(self, jsonl_file, cdr_type='3', max_len=130):\n        alphabet_set = set([a for a in alphabet])\n        self.data = []\n        with open(jsonl_file) as f:\n            lines = f.readlines()\n            for i in range(len(lines)):\n                entry = json.loads(lines[i])\n                if entry['cdr'] is None or cdr_type not in entry['cdr']:\n                    continue\n\n                last_cdr = entry['cdr'].rindex(cdr_type)\n                if last_cdr >= max_len - 1:\n                    entry['seq'] = entry['seq'][last_cdr - max_len + 10 : last_cdr + 10]\n                    entry['cdr'] = entry['cdr'][last_cdr - max_len + 10 : last_cdr + 10]\n                    for key, val in entry['coords'].items():\n                        entry['coords'][key] = np.asarray(val)[last_cdr - max_len + 10 : last_cdr + 10]\n                else:\n                    entry['seq'] = entry['seq'][:max_len]\n                    entry['cdr'] = entry['cdr'][:max_len]\n                    for key, val in entry['coords'].items():\n                        entry['coords'][key] = np.asarray(val)[:max_len]\n\n                if entry is not None and len(entry['seq']) > 0:\n                    self.data.append(entry)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]", "class AntibodyDataset():\n\n    def __init__(self, jsonl_file, cdr_type='3', max_len=130):\n        alphabet_set = set([a for a in alphabet])\n        self.data = []\n        with open(jsonl_file) as f:\n            lines = f.readlines()\n            for i in range(len(lines)):\n                entry = json.loads(lines[i])\n                if entry['cdr'] is None or cdr_type not in entry['cdr']:\n                    continue\n\n                last_cdr = entry['cdr'].rindex(cdr_type)\n                if last_cdr >= max_len - 1:\n                    entry['seq'] = entry['seq'][last_cdr - max_len + 10 : last_cdr + 10]\n                    entry['cdr'] = entry['cdr'][last_cdr - max_len + 10 : last_cdr + 10]\n                    for key, val in entry['coords'].items():\n                        entry['coords'][key] = np.asarray(val)[last_cdr - max_len + 10 : last_cdr + 10]\n                else:\n                    entry['seq'] = entry['seq'][:max_len]\n                    entry['cdr'] = entry['cdr'][:max_len]\n                    for key, val in entry['coords'].items():\n                        entry['coords'][key] = np.asarray(val)[:max_len]\n\n                if entry is not None and len(entry['seq']) > 0:\n                    self.data.append(entry)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]", "\n\nclass CDRDataset():\n\n    def __init__(self, jsonl_file, hcdr):\n        alphabet_set = set([a for a in alphabet])\n        self.cdrs = []\n        self.atgs = []\n\n        with open(jsonl_file) as f:\n            lines = f.readlines()\n            for i in range(len(lines)):\n                for cdr_type in hcdr:\n                    entry = self.get_cdr(lines[i], cdr_type)\n                    if entry is not None and len(entry['seq']) > 0:\n                        self.cdrs.append(entry)\n                        self.atgs.append(None)\n\n    def get_cdr(self, s, cdr_type):\n        entry = json.loads(s)\n        seq = entry['seq']\n        if seq is None or len(cdr_type) == 0: \n            return None\n\n        if 'cdr' in entry:\n            cdr = entry['cdr']\n            entry['chain'] = entry['seq']\n            entry['context'] = ''.join(\n                    [(alphabet[0] if y == cdr_type else x) for x,y in zip(seq, cdr)]\n            )\n            entry['seq'] = ''.join(\n                    [x for x,y in zip(seq, cdr) if y == cdr_type]\n            )\n            cdr_mask = np.array([(y == cdr_type) for y in cdr])\n        else:\n            cdr_mask = np.array([True] * len(seq))\n\n        for key, val in entry['coords'].items():\n            val = np.asarray(val)\n            val = val[:len(cdr_mask)]\n            entry['coords'][key] = val[cdr_mask] if len(cdr_mask) <= len(val) else val\n\n        return entry\n\n    def __len__(self):\n        return len(self.cdrs)\n\n    def __getitem__(self, idx):\n        return (self.cdrs[idx], self.atgs[idx])", "\n\nclass StructureDataset():\n\n    def __init__(self, jsonl_file, max_length=100):\n        alphabet_set = set([a for a in alphabet])\n        with open(jsonl_file) as f:\n            self.data = []\n            lines = f.readlines()\n            for i, line in enumerate(lines):\n                entry = json.loads(line)\n                seq = entry['seq']\n                for key, val in entry['coords'].items():\n                    entry['coords'][key] = np.asarray(val)\n\n                # Check if in alphabet\n                bad_chars = set([s for s in seq]).difference(alphabet_set)\n                if len(bad_chars) == 0 and len(seq) <= max_length:\n                    self.data.append(entry)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]", "\n\nclass StructureLoader():\n\n    def __init__(self, dataset, batch_tokens, binder_data=None, interval_sort=0):\n        self.dataset = dataset\n        self.size = len(dataset)\n        self.lengths = [len(dataset[i]['seq']) for i in range(self.size)]\n        self.batch_tokens = batch_tokens\n        self.binder_data = binder_data\n\n        if interval_sort > 0:\n            cdr_type = str(interval_sort)\n            self.lengths = [dataset[i]['cdr'].count(cdr_type) for i in range(self.size)]\n            self.intervals = [(dataset[i]['cdr'].index(cdr_type), dataset[i]['cdr'].rindex(cdr_type)) for i in range(self.size)]\n            sorted_ix = sorted(range(self.size), key=self.intervals.__getitem__)\n        else:\n            sorted_ix = np.argsort(self.lengths)\n\n        # Cluster into batches of similar sizes\n        clusters, batch = [], []\n        for ix in sorted_ix:\n            size = self.lengths[ix]\n            if size * (len(batch) + 1) <= self.batch_tokens:\n                batch.append(ix)\n            else:\n                clusters.append(batch)\n                batch = [ix]\n        if len(batch) > 0:\n            clusters.append(batch)\n        self.clusters = clusters\n\n    def __len__(self):\n        return len(self.clusters)\n\n    def __iter__(self):\n        np.random.shuffle(self.clusters)\n        for b_idx in self.clusters:\n            batch = [self.dataset[i] for i in b_idx]\n            if self.binder_data:\n                abatch = [self.binder_data[i] for i in b_idx]\n                yield (batch, abatch)\n            else:\n                yield batch", "\n\ndef completize(batch):\n    B = len(batch)\n    L = [b['cdr'] for b in batch]\n    L_max = max([len(b['seq']) for b in batch])\n    X = np.zeros([B, L_max, 4, 3])\n    S = np.zeros([B, L_max], dtype=np.int32)\n    mask = np.zeros([B, L_max], dtype=np.float32)\n\n    # Build the batch\n    for i, b in enumerate(batch):\n        x = np.stack([b['coords'][c] for c in ['N', 'CA', 'C', 'O']], 1)\n        X[i,:len(x),:,:] = x\n        l = len(b['seq'])\n        indices = np.asarray([alphabet.index(a) for a in b['seq']], dtype=np.int32)\n        S[i, :l] = indices\n        mask[i, :l] = 1.\n\n    # Remove NaN coords\n    mask = mask * np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n    isnan = np.isnan(X)\n    X[isnan] = 0.\n\n    # Conversion\n    S = torch.from_numpy(S).long().cuda()\n    X = torch.from_numpy(X).float().cuda()\n    mask = torch.from_numpy(mask).float().cuda()\n    return X, S, L, mask", "\n\ndef featurize(batch, context=True):\n    B = len(batch)\n    L_max = max([len(b['seq']) for b in batch])\n    X = np.zeros([B, L_max, 4, 3])\n    S = np.zeros([B, L_max], dtype=np.int32)\n    P = np.zeros([B, L_max, 6])\n    mask = np.zeros([B, L_max], dtype=np.float32)\n\n    # Build the batch\n    for i, b in enumerate(batch):\n        x = np.stack([b['coords'][c] for c in ['N', 'CA', 'C', 'O']], 1)\n        if len(x) <= L_max:\n            X[i,:len(x),:,:] = x\n\n        l = len(b['seq'])\n        indices = np.asarray([alphabet.index(a) for a in b['seq']], dtype=np.int32)\n        S[i, :l] = indices\n        P[i, :l] = np.array([PMAP(a) for a in b['seq']])\n        mask[i, :l] = 1.\n\n    # Remove NaN coords\n    mask = mask * np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n    isnan = np.isnan(X)\n    X[isnan] = 0.\n\n    # Conversion\n    S = torch.from_numpy(S).long().cuda()\n    X = torch.from_numpy(X).float().cuda()\n    P = torch.from_numpy(P).float().cuda()\n    mask = torch.from_numpy(mask).float().cuda()\n\n    if context:  # extract context\n        L_max = max([len(b['context']) for b in batch])\n        cS = np.zeros([B, L_max], dtype=np.int32)\n        cmask = np.zeros([B, L_max], dtype=np.float32)\n        crange = [None] * B\n        for i, b in enumerate(batch):\n            l = len(b['context'])\n            indices = np.asarray([alphabet.index(a) for a in b['context']], dtype=np.int32)\n            cS[i, :l] = indices\n            cmask[i, :l] = 1.\n            crange[i] = (b['context'].index('#'), b['context'].rindex('#'))\n\n        cmask = torch.from_numpy(cmask).float().cuda()\n        cS = torch.from_numpy(cS).long().cuda()\n        context = (cS, cmask, crange)\n\n    return (X, S, P, mask), context", ""]}
{"filename": "structgen/revision.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom structgen.encoder import MPNEncoder\nfrom structgen.data import alphabet\nfrom structgen.utils import *\nfrom structgen.protein_features import ProteinFeatures\n", "from structgen.protein_features import ProteinFeatures\n\n\nclass RevisionDecoder(nn.Module):\n\n    def __init__(self, args):\n        super(RevisionDecoder, self).__init__()\n        self.k_neighbors = args.k_neighbors\n        self.hidden_size = args.hidden_size\n        self.pos_embedding = PosEmbedding(16)\n        self.context = args.context\n\n        self.features = ProteinFeatures(\n                top_k=args.k_neighbors, num_rbf=args.num_rbf,\n                features_type='full',\n                direction='bidirectional'\n        )\n        self.node_in, self.edge_in = self.features.feature_dimensions['full']\n        self.O_d0 = nn.Linear(args.hidden_size, 12)\n        self.O_d = nn.Linear(args.hidden_size, 12)\n        self.O_s = nn.Linear(args.hidden_size, args.vocab_size)\n\n        self.struct_mpn = MPNEncoder(args, self.node_in, self.edge_in, direction='bidirectional')\n        self.seq_mpn = MPNEncoder(args, self.node_in, self.edge_in, direction='bidirectional')\n\n        if args.context:\n            self.crnn = nn.GRU(\n                    len(alphabet), args.hidden_size, \n                    batch_first=True, num_layers=1,\n                    dropout=args.dropout\n            )\n            self.W_stc = nn.Sequential(\n                    nn.Linear(args.hidden_size * 2, args.hidden_size),\n                    nn.ReLU(),\n            )\n            self.W_seq = nn.Sequential(\n                    nn.Linear(args.hidden_size * 2, args.hidden_size),\n                    nn.ReLU(),\n            )\n        else:\n            self.init_mpn = MPNEncoder(args, 16, 32, direction='bidirectional')\n            self.W_stc = self.W_seq = None\n\n        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n        self.mse_loss = nn.MSELoss(reduction='none')\n\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def init_struct(self, B, N, K):\n        # initial S and V\n        S = torch.zeros(B, N).cuda().long()\n        pos = torch.arange(N).cuda()\n        V = self.pos_embedding(pos.view(1, N, 1))  # [1, N, 1, 16]\n        V = V.squeeze(2).expand(B, -1, -1)  # [B, N, 6]\n        # initial E_idx\n        pos = pos.unsqueeze(0) - pos.unsqueeze(1)     # [N, N]\n        D_idx, E_idx = pos.abs().topk(k=K, dim=-1, largest=False)    # [N, K]\n        E_idx = E_idx.unsqueeze(0).expand(B, -1, -1)  # [B, N, K]\n        D_idx = D_idx.unsqueeze(0).expand(B, -1, -1)  # [B, N, K]\n        # initial E\n        E_rbf = self.features._rbf(3 * D_idx)\n        E_pos = self.features.embeddings(E_idx)\n        E = torch.cat((E_pos, E_rbf), dim=-1)\n        return V, E, S, E_idx\n\n    def encode_context(self, cS, cmask, crange):\n        cS = F.one_hot(cS, num_classes=len(alphabet)).float()\n        cH, _ = self.crnn(cS)\n        max_len = max([right - left + 1 for left,right in crange])\n        cdr_h = [cH[i, left:right+1] for i,(left,right) in enumerate(crange)]\n        cdr_h = [F.pad(h, (0,0,0,max_len-len(h))) for h in cdr_h]\n        return torch.stack(cdr_h, dim=0), cH, cmask, crange\n\n    def init_coords(self, mask):\n        B, N = mask.size(0), mask.size(1)\n        K = min(self.k_neighbors, N)\n        V, E, S, E_idx = self.init_struct(B, N, K)\n        h = self.init_mpn(V, E, S, E_idx, mask)\n        return self.predict_dist(self.O_d0(h))\n\n    # Q: [B, N, H], K, V: [B, M, H]\n    def attention(self, Q, context, cmask, W):\n        if self.context:\n            att = torch.bmm(Q, context.transpose(1, 2))  # [B, N, M]\n            att = att - 1e6 * (1 - cmask.unsqueeze(1))\n            att = F.softmax(att, dim=-1)\n            out = torch.bmm(att, context)  # [B, N, M] * [B, M, H]\n            out = torch.cat([Q, out], dim=-1)\n            return W(out)\n        else:\n            return Q\n\n    def predict_dist(self, X):\n        X = X.view(X.size(0), X.size(1), 4, 3)\n        X_ca = X[:, :, 1, :]\n        dX = X_ca[:, None, :, :] - X_ca[:, :, None, :]\n        D = torch.sum(dX ** 2, dim=-1)\n        V = self.features._dihedrals(X)\n        AD = self.features._AD_features(X[:,:,1,:])\n        return X.detach().clone(), D, V, AD\n\n    def forward(self, true_X, true_S, L, mask, context=None):\n        B, N = mask.size(0), mask.size(1)\n        K = min(self.k_neighbors, N)\n\n        # Ground truth \n        true_V = self.features._dihedrals(true_X)\n        true_AD = self.features._AD_features(true_X[:,:,1,:])\n        true_D, mask_2D = pairwise_distance(true_X, mask)\n        true_D = true_D ** 2\n\n        # initialize\n        sloss = 0.\n        S = torch.zeros(B, N).cuda().long()\n        if self.context:\n            h, cH, cmask, crange = self.encode_context(*context)\n            X, D, V, AD = self.predict_dist(self.O_d0(h))\n        else:\n            X, D, V, AD = self.init_coords(mask)\n            cH = cmask = None\n\n        dloss = self.huber_loss(D, true_D)\n        vloss = self.mse_loss(V, true_V)\n        aloss = self.mse_loss(AD, true_AD)\n\n        for t in range(N):\n            # Predict residue t\n            V, E, E_idx = self.features(X, mask)\n            h = self.seq_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_seq)\n            logits = self.O_s(h[:, t])\n            snll = self.ce_loss(logits, true_S[:, t])\n            sloss = sloss + torch.sum(snll * mask[:, t])\n\n            # Teacher forcing on S\n            S = S.clone()\n            S[:, t] = true_S[:, t]\n            S = S.clone()\n\n            # Iterative refinement\n            h = self.struct_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_stc)\n            X, D, V, AD = self.predict_dist(self.O_d(h))\n            dloss = dloss + self.huber_loss(D, true_D)\n            vloss = vloss + self.mse_loss(V, true_V)\n            aloss = aloss + self.mse_loss(AD, true_AD)\n\n        dloss = torch.sum(dloss * mask_2D) / mask_2D.sum()\n        vloss = torch.sum(vloss * mask.unsqueeze(-1)) / mask.sum()\n        aloss = torch.sum(aloss * mask.unsqueeze(-1)) / mask.sum()\n        sloss = sloss.sum() / mask.sum()\n        return sloss + dloss + vloss + aloss\n\n    def log_prob(self, true_S, mask, context=None):\n        B, N = true_S.size(0), true_S.size(1)\n        K = min(self.k_neighbors, N)\n        mask_2D = mask.unsqueeze(1) * mask.unsqueeze(2)\n\n        # initialize\n        sloss = 0.\n        S = torch.zeros(B, N).cuda().long()\n        if self.context:\n            h, cH, cmask, crange = self.encode_context(*context)\n            X = self.predict_dist(self.O_d0(h))[0]\n        else:\n            X = self.init_coords(mask)[0]\n            cH = cmask = None\n        \n        for t in range(N):\n            # Predict residue t\n            V, E, E_idx = self.features(X, mask)\n            h = self.seq_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_seq)\n            logits = self.O_s(h[:, t])\n            prob = F.softmax(logits, dim=-1)  # [B, 20]\n            snll = self.ce_loss(logits, true_S[:, t])\n            sloss = sloss + torch.sum(snll * mask[:, t])\n\n            # Teacher forcing on S\n            S = S.clone()\n            S[:, t] = true_S[:, t]\n            S = S.clone()\n\n            # Iterative refinement\n            h = self.struct_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_stc)\n            X = self.predict_dist(self.O_d(h))[0]\n\n        sloss = sloss / mask.sum()\n        return ReturnType(nll=sloss, X_cdr=X)\n\n    def generate(self, B, N, context=None, return_ppl=False):\n        K = min(self.k_neighbors, N)\n        mask = torch.ones(B, N).cuda()\n\n        S = torch.zeros(B, N).cuda().long()\n        if self.context:\n            h, cH, cmask, crange = self.encode_context(*context)\n            X = self.predict_dist(self.O_d0(h))[0]\n        else:\n            X = self.init_coords(mask)[0]\n            cH = cmask = None\n\n        sloss = 0.\n        for t in range(N):\n            # Predict residue t\n            V, E, E_idx = self.features(X, mask)\n            h = self.seq_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_seq)\n            logits = self.O_s(h[:, t])\n            prob = F.softmax(logits, dim=-1)  # [B, 20]\n            S[:, t] = torch.multinomial(prob, num_samples=1).squeeze(-1)  # [B, 1]\n            sloss = sloss + self.ce_loss(logits, S[:, t])\n\n            # Iterative refinement\n            h = self.struct_mpn(V, E, S, E_idx, mask)\n            h = self.attention(h, cH, cmask, self.W_stc)\n            X = self.predict_dist(self.O_d(h))[0]\n\n        S = S.tolist()\n        S = [''.join([alphabet[S[i][j]] for j in range(N)]) for i in range(B)]\n        ppl = torch.exp(sloss / N)\n        return (S, ppl) if return_ppl else S", ""]}
{"filename": "structgen/encoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom structgen.utils import *\n\n\nclass MPNEncoder(nn.Module):\n    \n    def __init__(self, args, node_in, edge_in, direction='forward'):\n        super(MPNEncoder, self).__init__()\n        self.node_in, self.edge_in = node_in, edge_in\n        self.direction = direction\n        self.W_v = nn.Sequential(\n                nn.Linear(self.node_in, args.hidden_size, bias=True),\n                Normalize(args.hidden_size)\n        )\n        self.W_e = nn.Sequential(\n                nn.Linear(self.edge_in, args.hidden_size, bias=True),\n                Normalize(args.hidden_size)\n        )\n        self.W_s = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = nn.ModuleList([\n                MPNNLayer(args.hidden_size, args.hidden_size * 3, dropout=args.dropout)\n                for _ in range(args.depth)\n        ])\n        for param in self.parameters():\n            if param.dim() > 1:\n                nn.init.xavier_uniform_(param)\n\n    def autoregressive_mask(self, E_idx):\n        N_nodes = E_idx.size(1)\n        ii = torch.arange(N_nodes).cuda()\n        ii = ii.view((1, -1, 1))\n        mask = E_idx - ii < 0\n        return mask.float()\n\n    def forward(self, V, E, S, E_idx, mask):\n        h_v = self.W_v(V)  # [B, N, H] \n        h_e = self.W_e(E)  # [B, N, K, H] \n        h_s = self.W_s(S)  # [B, N, H] \n        nei_s = gather_nodes(h_s, E_idx)  # [B, N, K, H]\n\n        if self.direction == 'forward':\n            vmask = self.autoregressive_mask(E_idx)  # [B, N, K]\n            vmask = mask.unsqueeze(-1) * vmask\n        elif self.direction == 'bidirectional':\n            # [B, N, 1] -> [B, N, K, 1] -> [B, N, K]\n            vmask = gather_nodes(mask.unsqueeze(-1), E_idx).squeeze(-1)\n        else:\n            raise ValueError('invalid direction', self.direction)\n\n        h = h_v\n        for layer in self.layers:\n            nei_v = gather_nodes(h, E_idx)  # [B, N, K, H]\n            nei_h = torch.cat([nei_v, nei_s, h_e], dim=-1)\n            h = layer(h, nei_h, mask_attend=vmask)  # [B, N, H]\n            h = h * mask.unsqueeze(-1)  # [B, N, H]\n        return h\n\n    # incremental forward for unidirectional model\n    def inc_forward(self, V, E, S, E_idx, mask, h_all, t):\n        assert self.direction == 'forward'\n        h_v = self.W_v(V[:, t:t+1])  # [B, 1, H] \n        h_e = self.W_e(E[:, t:t+1])  # [B, 1, K, H] \n        nei_s = gather_nodes(S.unsqueeze(-1), E_idx[:, t:t+1])  # [B, 1, K, 1]\n        nei_s = self.W_s(nei_s.squeeze(-1))  # [B, 1, K, H]\n\n        # sequence prediction\n        h_all[0] = insert_tensor(h_all[0], h_v, t)  # h_all[0][:, t:t+1] = h_v\n        for i, layer in enumerate(self.layers):\n            nei_v = gather_nodes(h_all[i], E_idx[:, t:t+1])  # [B, 1, K, H]\n            vmask = (E_idx[:, t:t+1] < t).float()  # [B, 1, K]\n            nei_h = torch.cat([nei_v, nei_s, h_e], dim=-1)\n            cur_h = h_all[i][:, t:t+1]\n            h = layer(cur_h, nei_h, mask_attend=vmask)  # [B, 1, H]\n            new_h = h * mask[:, t:t+1].unsqueeze(-1)  # [B, 1, H]\n            h_all[i + 1] = insert_tensor(h_all[i + 1], new_h, t)\n        return h_all", "\n"]}
