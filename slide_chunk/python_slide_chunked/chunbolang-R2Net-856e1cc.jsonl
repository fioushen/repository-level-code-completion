{"filename": "train_base.py", "chunked_list": ["import os\nimport datetime\nimport random\nimport time\nimport cv2\nfrom cv2 import mean\nimport numpy as np\nimport logging\nimport argparse\nfrom visdom import Visdom", "import argparse\nfrom visdom import Visdom\nimport os.path as osp\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.optim", "import torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\n\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda import amp\nfrom torch.utils.data.distributed import DistributedSampler\n", "from torch.utils.data.distributed import DistributedSampler\n\nfrom model.util import PSPNet\n            \nfrom dataset import iSAID, iSAID_1\n\nfrom util import config\nfrom util.util import AverageMeter, intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n                             fix_bn, check_makedirs,lr_decay, Special_characters\n", "                             fix_bn, check_makedirs,lr_decay, Special_characters\n\ncv2.ocl.setUseOpenCL(False)\ncv2.setNumThreads(0)\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n    parser.add_argument('--arch', type=str, default='PSPNet', help='') # \n    parser.add_argument('--split', type=int, default=1, help='') # \n    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n    parser.add_argument('--variable1', type=str, default='', help='') #\n    parser.add_argument('--variable2', type=str, default='', help='') #\n    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n    base_config = 'config/pretrain/{}.yaml'.format(args.dataset)\n    # data_config = 'config/dataset/{}.yaml'.format(args.dataset)\n\n    # assert args.config is not None\n    cfg = config.load_cfg_from_cfg_file([base_config])\n    cfg = config.merge_cfg_from_args(cfg, args)\n    if args.opts is not None:\n        cfg = config.merge_cfg_from_list(cfg, args.opts)\n\n    cfg.snapshot_path = 'initmodel/PSPNet/{}/split{}/{}/'.format(args.dataset,  args.split, args.backbone)\n    cfg.result_path = 'initmodel/PSPNet/{}/split{}/{}/result/'.format(args.dataset,  args.split, args.backbone)\n    return cfg", "\n\ndef get_model(args):\n\n    model = eval(args.arch).OneModel(args)\n    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n\n    if args.distributed:\n        # Initialize Process Group\n        dist.init_process_group(backend='nccl')\n        print('args.local_rank: ', args.local_rank)\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device('cuda', args.local_rank)\n        model.to(device)\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n    else:\n        model = model.cuda()\n\n    # Resume\n    check_makedirs(args.snapshot_path)\n    check_makedirs(args.result_path)\n\n    if args.resume:\n        resume_path = osp.join(args.snapshot_path, args.resume)\n        if os.path.isfile(resume_path):\n            if main_process():\n                logger.info(\"=> loading checkpoint '{}'\".format(resume_path))\n            checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n            args.start_epoch = checkpoint['epoch']\n            new_param = checkpoint['state_dict']\n            try: \n                model.load_state_dict(new_param)\n            except RuntimeError:                   # 1GPU loads mGPU model\n                for key in list(new_param.keys()):\n                    new_param[key[7:]] = new_param.pop(key)\n                model.load_state_dict(new_param)\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            if main_process():\n                logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n        else:\n            if main_process():       \n                logger.info(\"=> no checkpoint found at '{}'\".format(resume_path))\n\n\n    # Get model para.\n    total_number, learnable_number = get_model_para_number(model)\n    if main_process():\n        print('Number of Parameters: %d' % (total_number))\n        print('Number of Learnable Parameters: %d' % (learnable_number))\n\n    time.sleep(5)\n    return model, optimizer", "\ndef main_process():\n    return not args.distributed or (args.distributed and (args.local_rank == 0))\n\ndef main():\n    global args, logger\n    args = get_parser()\n \n    logger = get_logger()\n    args.logger = logger\n\n    # args.distributed = False # Debug\n    args.distributed = True if torch.cuda.device_count() > 1 else False\n    shuffle = False if args.distributed else True\n\n    if main_process():\n        print(args)\n\n    if args.manual_seed is not None:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n\n    if main_process():\n        logger.info(\"=> creating dataset ...\")\n# ----------------------  DATASET  ----------------------\n    train_data = eval('{}.{}_base_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n                                             mode='train', transform_dict=args.train_transform)\n    train_sampler = DistributedSampler(train_data) if args.distributed else None\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=shuffle, \\\n                                            num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n    # Val\n\n    val_data =  eval('{}.{}_base_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n                                             mode='val', transform_dict=args.val_transform)            \n    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n\n    args.base_class_num = len(train_data.list)\n    \n    logger.info('train_list: {}'.format(train_data.list))\n    logger.info('num_train_data: {}'.format(len(train_data)))\n    logger.info('val_list: {}'.format(val_data.list))\n    logger.info('num_val_data: {}'.format(len(val_data)))\n    time.sleep(2)\n\n    if main_process():\n        logger.info(\"=> creating model ...\")\n    model, optimizer = get_model(args)\n    \n    logger.info(model)\n\n# ----------------------  TRAINVAL  ----------------------\n    global best_miou, best_FBiou, best_epoch, keep_epoch, val_num\n    global best_name, grow_name, all_name, latest_name\n\n    best_miou = 0.\n    best_FBiou = 0.\n\n    best_epoch = 0\n    keep_epoch = 0\n    val_num = 0\n\n    start_time = time.time()\n    scaler = amp.GradScaler()\n\n#--------------------------- FilenamePrepare -----------------------------\n\n    latest_name = args.snapshot_path + 'latest.pth'\n    best_name = args.snapshot_path + 'best.pth'\n    grow_name = args.snapshot_path + 'grow.txt'\n    all_name = args.snapshot_path + 'all.txt'\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if keep_epoch == args.stop_interval:\n            break\n        if args.fix_random_seed_val:\n            setup_seed(args.manual_seed + epoch, args.seed_deterministic)\n\n        epoch_log = epoch + 1\n        keep_epoch += 1\n        \n        # ----------------------  TRAIN  ----------------------\n        train(train_loader, val_loader, model, optimizer, epoch, scaler)\n        # save model for <resuming>\n        if ((epoch + 1) % args.save_freq == 0) and main_process():\n\n            logger.info('Saving checkpoint to: ' + latest_name)\n            if osp.exists(latest_name):\n                os.remove(latest_name)            \n            torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, latest_name)\n\n\n        # -----------------------  VAL  -----------------------\n        if args.evaluate and (epoch + 1)% args.val_freq == 0:\n            _,fbIou, _,_, mIoU,_ , recall, precision = validate(val_loader, model)   \n            val_num += 1\n\n            with open(all_name, 'a') as f:\n                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, mIoU, fbIou, recall, precision))\n\n        # save model for <testing> and <fine-tuning>\n            if mIoU > best_miou:\n                best_miou, best_epoch = mIoU, epoch\n                keep_epoch = 0\n                with open(grow_name, 'a') as f:\n                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch , best_miou, fbIou, recall, precision)) \n                logger.info('Saving checkpoint to: ' + best_name)\n                if osp.exists(best_name):\n                    os.remove(best_name)    \n                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name)  \n\n    \n    total_time = time.time() - start_time\n    t_m, t_s = divmod(total_time, 60)\n    t_h, t_m = divmod(t_m, 60)\n    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\n    print('\\nEpoch: {}/{} \\t Total running time: {}'.format(epoch_log, args.epochs, total_time))\n    print('The number of models validated: {}'.format(val_num))            \n    print('\\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Final Best Result   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n    print(args.arch + '\\t Group:{} \\t Best_mIoU:{:.4f} \\t Best_FBIoU:{:.4f} \\t Best_step:{}'.format(args.split, best_miou, best_FBiou, best_epoch))\n    print('>'*80)\n    print ('\u5f53\u524d\u7684\u65e5\u671f\u548c\u65f6\u95f4\u662f %s' % datetime.datetime.now())", "\n\ndef train(train_loader, val_loader, model, optimizer, epoch ,scaler):\n    global best_miou, best_epoch, keep_epoch, val_num\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    main_loss_meter = AverageMeter()\n    aux_loss_meter_1 = AverageMeter()\n    aux_loss_meter_2 = AverageMeter()\n    loss_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    target_meter = AverageMeter()\n    tmp_num = 0\n    model.train()\n    if args.fix_bn:\n        model.apply(fix_bn) # fix batchnorm\n\n    end = time.time()\n    val_time = 0.\n    max_iter = args.epochs * len(train_loader)\n    current_characters = Special_characters[random.randint(1,len(Special_characters)-1)]\n\n    current_GPU = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n    for i, (input, target) in enumerate(train_loader):\n\n        data_time.update(time.time() - end - val_time)\n        current_iter = epoch * len(train_loader) + i + 1\n        lr_decay(optimizer, args.base_lr, current_iter, max_iter, args.lr_decay, current_characters )\n        if current_iter % 50 == 0 and main_process():\n            print(current_characters[0]*3 +' '*5 + '{}_{}_{}_split{} Pretrain: {} GPU_id: {}'.format(args.arch,\\\n                             args.dataset ,args.backbone, args.split, args.pretrain, current_GPU) + ' '*5 + current_characters[1]*3)\n\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n        optimizer.zero_grad()\n\n        output, main_loss, aux_loss_1, aux_loss_2= model(x=input, y=target)\n        loss = main_loss + aux_loss_1 * args.aux_weight1 +aux_loss_2 * args.aux_weight2\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        n = input.size(0) # batch_size\n\n        intersection, union, target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n        \n        Iou = sum(intersection_meter.val[1:]) / (sum(union_meter.val[1:]) + 1e-10)  # allAcc\n        main_loss_meter.update(main_loss.item(), n)\n        if isinstance(aux_loss_1, torch.Tensor):\n            aux_loss_meter_1.update(aux_loss_1.item(), n)\n        if isinstance(aux_loss_2, torch.Tensor):\n            aux_loss_meter_2.update(aux_loss_2.item(), n)\n\n        loss_meter.update(loss.item(), n)\n        batch_time.update(time.time() - end - val_time)\n        end = time.time()\n\n        remain_iter = max_iter - current_iter\n        remain_time = remain_iter * batch_time.avg\n        t_m, t_s = divmod(remain_time, 60)\n        t_h, t_m = divmod(t_m, 60)\n        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))\n\n        if (i + 1) % args.print_freq == 0 and main_process():\n            logger.info('Epoch: [{}/{}][{}/{}] '\n                        'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                        'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                        'Remain {remain_time} '\n                        'MainLoss {main_loss_meter.val:.4f} '\n                        'AuxLoss_1 {aux_loss_meter_1.val:.4f} '  \n                        'AuxLoss_2 {aux_loss_meter_2.val:.4f} ' \n                        'Loss {loss_meter.val:.4f} '\n                        'Iou {Iou:.4f}.'.format(epoch+1, args.epochs, i + 1, len(train_loader),\n                                                        batch_time=batch_time,\n                                                        data_time=data_time,\n                                                        remain_time=remain_time,\n                                                        main_loss_meter=main_loss_meter,\n                                                        aux_loss_meter_1=aux_loss_meter_1,\n                                                        aux_loss_meter_2=aux_loss_meter_2,\n                                                        loss_meter=loss_meter,\n                                                        Iou=Iou))\n\n        \n        # -----------------------  SubEpoch VAL  -----------------------\n        if args.evaluate and args.SubEpoch_val and (args.epochs<=100 and (epoch + 1)%args.val_freq==0) and (i==round(len(train_loader)/2)): # max_epoch<=100\u65f6\u8fdb\u884chalf_epoch Val\n            _,fbIou, _,_, mIoU,_ , recall, precision = validate(val_loader, model)   \n            model.train()\n            val_num += 1 \n            # save model for <testing> and <fine-tuning>\n\n            with open(all_name, 'a') as f:\n                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, mIoU, fbIou, recall, precision))\n\n            if mIoU > best_miou:\n                best_miou, best_epoch = mIoU, (epoch-0.5)\n                keep_epoch = 0\n                with open(grow_name, 'a') as f:\n                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch , best_miou, fbIou, recall, precision)) \n             \n                logger.info('Saving checkpoint to: ' + best_name)\n                if osp.exists(best_name):\n                    os.remove(best_name) \n                torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name) \n            tmp_num += 1\n\n\n    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n    mIoU = np.mean(iou_class)\n    mAcc = np.mean(accuracy_class)\n    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\n    logger.info('Train result at epoch [{}/{}]: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(epoch, args.epochs, mIoU, mAcc, allAcc))\n    for i in range(2):\n        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\n    return main_loss_meter.avg, mIoU, mAcc, allAcc", "\ndef validate(val_loader, model):\n    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n    batch_time = AverageMeter()\n    model_time = AverageMeter()\n    data_time = AverageMeter()\n    loss_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    target_meter = AverageMeter()\n\n    split_gap = len(val_loader.dataset.list)\n    test_num = min(len(val_loader), 1000) # 20000 \n\n    class_intersection_meter = [0]*split_gap\n    class_union_meter = [0]*split_gap   \n    class_target_meter = [0]*split_gap \n\n    if args.manual_seed is not None and args.fix_random_seed_val:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\n    model.eval()\n    end = time.time()\n    val_start = end\n\n    iter_num = 0\n    total_time = 0\n    for e in range(10):\n        for i, (input, target) in enumerate(val_loader):\n            if iter_num * args.batch_size_val >= test_num:\n                break\n            iter_num += 1\n            data_time.update(time.time() - end)\n\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n            start_time = time.time()\n\n            # with autocast():\n            with torch.no_grad():\n                output = model(x=input, y=target)\n            total_time = total_time + 1\n            model_time.update(time.time() - start_time)\n\n            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n            output = output.float()\n            loss = criterion(output, target)    \n\n            n = input.size(0)\n            loss = torch.mean(loss)\n\n            output = output.max(1)[1]\n\n            intersection, union, new_target = intersectionAndUnionGPU(output, target, split_gap+1, args.ignore_label)\n            intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n            intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\n            for idx in range(1,len(intersection)):\n                class_intersection_meter[idx-1] += intersection[idx]\n                class_union_meter[idx-1] += union[idx]\n                class_target_meter[idx-1] += new_target[idx]\n\n            Iou = np.mean(intersection_meter.val[1:] / (union_meter.val[1:] + 1e-10))\n            recall = np.mean(intersection_meter.val[1:] / (target_meter.val[1:]+ 1e-10))\n            precision = np.mean(intersection_meter.val[1:] /(union_meter.val[1:] - target_meter.val[1:] + intersection_meter.val[1:] + 1e-10) )\n\n            loss_meter.update(loss.item(), input.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if ((i + 1) % round((test_num/40)) == 0):\n                logger.info('Test: [{}/{}] '\n                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n                            'recall {recall:.4f} '\n                            'precision {precision:.4f} '\n                            'Iou {Iou:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n                                                            data_time=data_time,\n                                                            batch_time=batch_time,\n                                                            loss_meter=loss_meter,\n                                                            recall=recall,\n                                                            precision=precision,\n                                                            Iou=Iou))\n    val_time = time.time()-val_start\n\n    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n    mIoU = np.mean(iou_class)\n    mAcc = np.mean(accuracy_class)\n    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\n    \n    class_iou_class = []\n    class_miou = 0\n    class_recall_class = []\n    class_mrecall = 0\n    class_precisoin_class = []\n    class_mprecision = 0\n\n    for i in range(len(class_intersection_meter)):\n        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n        class_iou_class.append(class_iou)\n        class_miou += class_iou\n        \n        class_recall = class_intersection_meter[i]/(class_target_meter[i]+ 1e-10)\n        class_recall_class.append(class_recall)\n        class_mrecall += class_recall\n\n        class_precision = class_intersection_meter[i]/(class_union_meter[i] - class_target_meter[i] + class_intersection_meter[i]+ 1e-10)\n        class_precisoin_class.append(class_precision)\n        class_mprecision += class_precision\n\n    class_mrecall = class_mrecall*1.0 / len(class_intersection_meter)  \n    class_miou = class_miou*1.0 / len(class_intersection_meter)\n    class_mprecision = class_mprecision*1.0 / len(class_intersection_meter)\n\n\n    logger.info('mean IoU---Val result: mIoU {:.4f}.'.format(class_miou))\n    logger.info('mean recall---Val result: mrecall {:.4f}.'.format(class_mrecall))\n    logger.info('mean precisoin---Val result: mprecisoin {:.4f}.'.format(class_mprecision))\n\n    for i in range(split_gap):\n        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t recall {:.4f}. \\t precision {:.4f}. \\t {}'.format(i+1, class_iou_class[i], class_recall_class[i], class_precisoin_class[i],\\\n                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))     \n     \n\n    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n    for i in range(2):\n        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\n    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\n    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_mrecall, class_mprecision", "\n\n\n\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "train.py", "chunked_list": ["import os\nimport datetime\nimport random\nimport time\nimport cv2\nimport numpy as np\nimport logging\nimport argparse\nfrom visdom import Visdom\nimport os.path as osp", "from visdom import Visdom\nimport os.path as osp\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data", "import torch.optim\nimport torch.utils.data\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\n\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda import amp\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom model.few_seg import R2Net", "\nfrom model.few_seg import R2Net\n# from model.workdir import\n\nfrom dataset import iSAID, iSAID_1\n\nfrom util import config\nfrom util.util import AverageMeter,  intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n                                     fix_bn, check_makedirs,freeze_modules,lr_decay, Special_characters\n", "                                     fix_bn, check_makedirs,freeze_modules,lr_decay, Special_characters\n\ncv2.ocl.setUseOpenCL(False)\ncv2.setNumThreads(0)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n    parser.add_argument('--arch', type=str, default='R2Net', help='') # \n    parser.add_argument('--shot', type=int, default=1, help='') # \n    parser.add_argument('--split', type=int, default=0, help='') # \n    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n    parser.add_argument('--variable1', type=str, default='', help='') #\n    parser.add_argument('--variable2', type=str, default='', help='') #\n    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n    base_config = 'config/base.yaml'\n    data_config = 'config/dataset/{}.yaml'.format(args.dataset)\n    if args.arch in ['R2Net']:\n        model_config = 'config/model/few_seg/{}.yaml'.format(args.arch)\n    else:\n        model_config = 'config/model/workdir/{}.yaml'.format(args.arch)\n\n    if os.path.exists(model_config):\n        cfg = config.load_cfg_from_cfg_file([base_config, data_config, model_config])\n    else:\n        cfg = config.load_cfg_from_cfg_file([base_config, data_config])\n\n    cfg = config.merge_cfg_from_args(cfg, args)\n    if args.opts is not None:\n        cfg = config.merge_cfg_from_list(cfg, args.opts)\n    return cfg", "\n\ndef get_model(args):\n\n    model = eval(args.arch).OneModel(args, cls_type='Base')\n    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n\n    freeze_modules(model, args.freeze_layer)\n    time.sleep(2)\n\n    if args.distributed:\n        # Initialize Process Group\n        device = torch.device('cuda', args.local_rank)\n        model.to(device)\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    else:\n        model = model.cuda()\n\n    # Resume\n    if args.resume:\n        resume_path = osp.join(args.snapshot_path, args.resume)\n        if os.path.isfile(resume_path):\n            if main_process():\n                logger.info(\"=> loading checkpoint '{}'\".format(resume_path))\n            checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n            args.start_epoch = checkpoint['epoch']\n            new_param = checkpoint['state_dict']\n            try: \n                model.load_state_dict(new_param)\n            except RuntimeError:                   # 1GPU loads mGPU model\n                for key in list(new_param.keys()):\n                    new_param[key[7:]] = new_param.pop(key)\n                model.load_state_dict(new_param)\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            if main_process():\n                logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n        else:\n            if main_process():       \n                logger.info(\"=> no checkpoint found at '{}'\".format(resume_path))\n\n\n    # Get model para.\n    total_number, learnable_number = get_model_para_number(model)\n    if main_process():\n        print('Number of Parameters: %d' % (total_number))\n        print('Number of Learnable Parameters: %d' % (learnable_number))\n\n    time.sleep(5)\n    return model, optimizer", "\ndef main_process():\n    return not args.distributed or (args.distributed and (args.local_rank == 0))\n\ndef main():\n\n    global args, logger\n    args = get_parser()\n\n    logger = get_logger()\n    args.logger = logger\n    args.distributed = False # Debug\n    # args.distributed = True if torch.cuda.device_count() > 1 else False\n    \n    shuffle = False if args.distributed else True\n    \n\n    get_save_path(args)\n    check_makedirs(args.snapshot_path)\n    check_makedirs(args.result_path)\n\n    if main_process():\n        print(args)\n\n    if args.manual_seed is not None:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n        \n    if args.distributed:\n        dist.init_process_group(backend='nccl')\n        print('args.local_rank: ', args.local_rank)\n        torch.cuda.set_device(args.local_rank)\n\n    if main_process():\n        logger.info(\"=> creating dataset ...\")\n\n# ----------------------  DATASET  ----------------------\n    train_data = eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n                                            shot=args.shot, mode='train', transform_dict=args.train_transform)\n    train_sampler = DistributedSampler(train_data) if args.distributed else None\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=shuffle, \\\n                                            num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True)\n    # Val\n\n    val_data =  eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n                                            shot=args.shot, mode='val', transform_dict=args.val_transform)            \n    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n    logger.info('train_list: {}'.format(train_data.list))\n    logger.info('num_train_data: {}'.format(len(train_data)))\n    logger.info('val_list: {}'.format(val_data.list))\n    logger.info('num_val_data: {}'.format(len(val_data)))\n\n    args.base_class_num = len(train_data.list)\n    args.novel_class_num = len(val_data.list)\n\n    config_file = args.snapshot_path + 'config.yaml'\n\n    with open(config_file, 'w', encoding='utf-8') as f:\n        f.write(str(args))\n    time.sleep(2)\n\n    if main_process():\n        logger.info(\"=> creating model ...\")\n    model, optimizer = get_model(args)\n    \n    logger.info(model)\n\n\n# ----------------------  TRAINVAL  ----------------------\n    global best_miou, best_FBiou, best_epoch, keep_epoch, val_num\n    global best_name, grow_name, all_name, latest_name, best_class_iou\n\n    best_miou = 0.\n    best_FBiou = 0.\n    best_class_iou = []\n    best_epoch = 0\n    keep_epoch = 0\n    val_num = 0\n\n    start_time = time.time()\n    scaler = amp.GradScaler()\n\n#--------------------------- FilenamePrepare -----------------------------\n\n    latest_name = args.snapshot_path + 'latest.pth'\n    best_name = args.snapshot_path + 'best.pth'\n    grow_name = args.snapshot_path + 'grow.txt'\n    all_name = args.snapshot_path + 'all.txt'\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if keep_epoch == args.stop_interval:\n            break\n        if args.fix_random_seed_val:\n            setup_seed(args.manual_seed + epoch, args.seed_deterministic)\n\n        epoch_log = epoch + 1\n        keep_epoch += 1\n        \n        # ----------------------  TRAIN  ----------------------\n        train(train_loader, val_loader, model, optimizer, epoch, scaler)\n        torch.cuda.empty_cache()\n        # save model for <resuming>\n        if ((epoch + 1) % args.save_freq == 0) and main_process():\n\n            logger.info('Saving checkpoint to: ' + latest_name)\n            if osp.exists(latest_name):\n                os.remove(latest_name)            \n            torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, latest_name)\n\n\n        # -----------------------  VAL  -----------------------\n        if args.evaluate and (epoch + 1)% args.val_freq == 0:\n            _,fbIou, _,_, mIoU,_ , recall, precision, class_miou = validate(val_loader, model)   \n            torch.cuda.empty_cache()\n            val_num += 1\n\n            with open(all_name, 'a') as f:\n                f.write('[{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch+1, mIoU, fbIou, recall, precision))\n\n        # save model for <testing> and <fine-tuning>\n            if mIoU > best_miou:\n                best_miou, best_epoch, best_class_iou, best_FBiou = mIoU, epoch, class_miou, fbIou\n                keep_epoch = 0\n                with open(grow_name, 'a') as f:\n                    f.write('Best_epoch:{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch+1 , best_miou, fbIou, recall, precision)) \n                logger.info('Saving checkpoint to: ' + best_name + '  miou: {:.4f}'.format(best_miou))\n                if osp.exists(best_name):\n                    os.remove(best_name)    \n                torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name)  \n\n    with open(args.snapshot_path + 'class.txt', 'a') as f:\n        for i in range(len(best_class_iou)):\n            f.write('{:.4f}\\t'.format(best_class_iou[i]))\n        f.write('\\nmiou: {:.4f}, fb_iou: {:.4f}'.format(best_miou, best_FBiou))\n    total_time = time.time() - start_time\n    t_m, t_s = divmod(total_time, 60)\n    t_h, t_m = divmod(t_m, 60)\n    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\n    print('\\nEpoch: {}/{} \\t Total running time: {}'.format(epoch_log, args.epochs, total_time))\n    print('The number of models validated: {}'.format(val_num))            \n    print('\\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  Final Best Result   <<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n    print(args.arch + '\\t Group:{} \\t Best_mIoU:{:.4f} \\t Best_FBIoU:{:.4f} \\t Best_step:{}'.format(args.split, best_miou, best_FBiou, best_epoch + 1 ))\n    print('>'*80)\n    print ('\u5f53\u524d\u7684\u65e5\u671f\u548c\u65f6\u95f4\u662f %s' % datetime.datetime.now())", "\n\ndef train(train_loader, val_loader, model, optimizer, epoch ,scaler):\n    global best_miou, best_epoch, keep_epoch, val_num\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    main_loss_meter = AverageMeter()\n    aux_loss_meter_1 = AverageMeter()\n    aux_loss_meter_2 = AverageMeter()\n    loss_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    target_meter = AverageMeter()\n    recall_meter = AverageMeter()\n    acc_meter = AverageMeter()\n    tmp_num = 0\n    model.train()\n    if args.fix_bn:\n        model.apply(fix_bn) # fix batchnorm\n\n    end = time.time()\n    val_time = 0.\n    max_iter = args.epochs * len(train_loader)\n    \n    current_characters = Special_characters[random.randint(0,len(Special_characters)-1)]\n\n    current_GPU = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n    # GPU_name = torch.cuda.get_device_name()\n    for i, (input, target, s_input, s_mask, subcls) in enumerate(train_loader):\n\n        data_time.update(time.time() - end - val_time)\n        current_iter = epoch * len(train_loader) + i + 1\n\n        lr_decay(optimizer, args.base_lr, current_iter, max_iter, args.lr_decay, current_characters )\n        if current_iter % 50 == 0 and main_process():\n            print(current_characters[0]*3 +' '*5 + '{}_{}_{}_split{}_{}shot Pretrain: {} GPU_id: {}'.format(args.arch,\\\n                             args.dataset ,args.backbone, args.split, args.shot, args.pretrain, current_GPU) + ' '*5 + current_characters[1]*3)\n   \n        s_input = s_input.cuda(non_blocking=True)\n        s_mask = s_mask.cuda(non_blocking=True)\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n        optimizer.zero_grad()\n        # with torch.autograd.set_detect_anomaly(True):      # debug\n        output, main_loss, aux_loss_1, aux_loss_2= model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n        loss = main_loss + aux_loss_1 * args.aux_weight1 +aux_loss_2 * args.aux_weight2\n    \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        if 'para_limit' in args.keys():\n            for item_id in range(len(args.para_limit.name)):\n                item = args.para_limit.name[item_id]\n                tmp_limit = args.para_limit.limit[item_id]\n                eval('model.{}'.format(item)).data.clamp_(tmp_limit[0], tmp_limit[1])\n\n        n = input.size(0) # batch_size\n\n        intersection, union, target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n        \n        \n        accuracy = sum(intersection_meter.val[1:]) / (sum(target_meter.val[1:]) + 1e-10)  # allAcc\n        main_loss_meter.update(main_loss.item(), n)\n        if isinstance(aux_loss_1, torch.Tensor):\n            aux_loss_meter_1.update(aux_loss_1.item(), n)\n        if isinstance(aux_loss_2, torch.Tensor):\n            aux_loss_meter_2.update(aux_loss_2.item(), n)\n\n        loss_meter.update(loss.item(), n)\n        batch_time.update(time.time() - end - val_time)\n        end = time.time()\n\n        remain_iter = max_iter - current_iter\n        remain_time = remain_iter * batch_time.avg\n        t_m, t_s = divmod(remain_time, 60)\n        t_h, t_m = divmod(t_m, 60)\n        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))\n\n        if (i + 1) % args.print_freq == 0 and main_process():\n            logger.info('Epoch: [{}/{}][{}/{}] '\n                        'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                        'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                        'Remain {remain_time} '\n                        'MainLoss {main_loss_meter.val:.4f} '\n                        'AuxLoss_1 {aux_loss_meter_1.val:.4f} '  \n                        'AuxLoss_2 {aux_loss_meter_2.val:.4f} ' \n                        'Loss {loss_meter.val:.4f} '\n                        'Accuracy {accuracy:.4f}.'.format(epoch+1, args.epochs, i + 1, len(train_loader),\n                                                        batch_time=batch_time,\n                                                        data_time=data_time,\n                                                        remain_time=remain_time,\n                                                        main_loss_meter=main_loss_meter,\n                                                        aux_loss_meter_1=aux_loss_meter_1,\n                                                        aux_loss_meter_2=aux_loss_meter_2,\n                                                        loss_meter=loss_meter,\n                                                        accuracy=accuracy))\n\n\n        \n        # -----------------------  SubEpoch VAL  -----------------------\n        if args.evaluate and args.SubEpoch_val and ((epoch + 1)%args.val_freq==0) and (i in torch.arange(1,args.sub_freq)*round(len(train_loader)/args.sub_freq)): # max_epoch<=100\u65f6\u8fdb\u884chalf_epoch Val\n            _,fbIou, _,_, mIoU,_ , recall, precision, class_miou = validate(val_loader, model)    \n            torch.cuda.empty_cache()\n            model.train()\n            if args.fix_bn:\n                model.apply(fix_bn)\n            val_num += 1 \n            tmp_num += 1\n            # save model for <testing> and <fine-tuning>\n\n            with open(all_name, 'a') as f:\n                f.write('[{}_{},miou:{:.4f}, fbIou:{:.4f}, recall:{:.4f}, precision:{:.4f},]\\n'.format(epoch, tmp_num, mIoU, fbIou, recall, precision))\n\n            if mIoU > best_miou:\n                best_miou, best_epoch, best_class_iou, best_FBiou = mIoU, epoch+(1/args.sub_freq)*tmp_num, class_miou, fbIou\n                keep_epoch = 0\n                with open(grow_name, 'a') as f:\n                    f.write('Best_epoch:{}_{} , Best_miou:{:.4f} , fbIou:{:.4f} , recall:{:.4f}, precision:{:.4f}, \\n'.format(epoch, tmp_num, best_miou, fbIou, recall, precision)) \n             \n                logger.info('Saving checkpoint to: ' + best_name + '  miou: {:.4f}'.format(best_miou))\n                if osp.exists(best_name):\n                    os.remove(best_name) \n                torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, best_name) \n            \n    if hasattr(model, 'out_data'):\n        with open(args.snapshot_path + 'out_data.txt', 'a') as f:\n            for item in model.out_data:\n                f.write(item + '\\n')\n            model.out_data = []\n\n    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n    mIoU = np.mean(iou_class)\n    mAcc = np.mean(accuracy_class)\n    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\n    logger.info('Train result at epoch [{}/{}]: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(epoch+1, args.epochs, mIoU, mAcc, allAcc))\n    for i in range(2):\n        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n\n    return main_loss_meter.avg, mIoU, mAcc, allAcc", "\ndef validate(val_loader, model):\n    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n    batch_time = AverageMeter()\n    model_time = AverageMeter()\n    data_time = AverageMeter()\n    loss_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    target_meter = AverageMeter()\n\n    split_gap = len(val_loader.dataset.list)\n    test_num = 1000 # 20000 \n\n    class_intersection_meter = [0]*split_gap\n    class_union_meter = [0]*split_gap   \n    class_target_meter = [0]*split_gap   \n\n    if args.manual_seed is not None and args.fix_random_seed_val:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\n    model.eval()\n    end = time.time()\n    val_start = end\n\n    assert test_num % args.batch_size_val == 0\n    alpha = round(test_num / args.batch_size_val)\n    iter_num = 0\n    total_time = 0\n    for e in range(10):\n        for i, (input, target, s_input, s_mask, subcls, ori_label) in enumerate(val_loader):\n            if iter_num * args.batch_size_val >= test_num:\n                break\n            iter_num += 1\n            data_time.update(time.time() - end)\n\n            s_input = s_input.cuda(non_blocking=True)\n            s_mask = s_mask.cuda(non_blocking=True)\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n            ori_label = ori_label.cuda(non_blocking=True)\n            start_time = time.time()\n\n            # with autocast():\n            with torch.no_grad():\n                output = model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n            total_time = total_time + 1\n            model_time.update(time.time() - start_time)\n\n            if args.ori_resize:\n                longerside = max(ori_label.size(1), ori_label.size(2))\n                backmask = torch.ones(ori_label.size(0), longerside, longerside, device='cuda')*255\n                backmask[:, :ori_label.size(1), :ori_label.size(2)] = ori_label\n                target = backmask.clone().long()\n\n            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n            output = output.float()\n            loss = criterion(output, target)    \n\n            n = input.size(0)\n            loss = torch.mean(loss)\n\n            output = output.max(1)[1]\n\n            # for b_id in range(output.size(0)):\n            intersection, union, new_target = intersectionAndUnionGPU(output, target, 2, args.ignore_label)\n            intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n            intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\n            tmp_id = subcls[0].cpu().numpy()[0]\n            class_intersection_meter[tmp_id] += intersection[1]\n            class_union_meter[tmp_id] += union[1] \n            class_target_meter[tmp_id] += new_target[1]\n\n            recall = np.mean(intersection_meter.val[1:] / (target_meter.val[1:]+ 1e-10))\n            precision = np.mean(intersection_meter.val[1:] /(union_meter.val[1:] - target_meter.val[1:] + intersection_meter.val[1:] + 1e-10) ) \n            Iou = sum(intersection_meter.val[1:]) / (sum(union_meter.val[1:]) + 1e-10)\n\n            loss_meter.update(loss.item(), input.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if ((iter_num) % round((alpha/20)) == 0):\n                logger.info('Test: [{}/{}] '\n                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n                            'recall {recall:.4f} '\n                            'precision {precision:.4f} '\n                            'Iou {Iou:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n                                                            data_time=data_time,\n                                                            batch_time=batch_time,\n                                                            loss_meter=loss_meter,\n                                                            recall=recall,\n                                                            precision=precision,\n                                                            Iou=Iou))\n    val_time = time.time()-val_start\n\n    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n    mIoU = np.mean(iou_class)\n    mAcc = np.mean(accuracy_class)\n    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\n    \n    class_iou_class = []\n    class_miou = 0\n    class_recall_class = []\n    class_mrecall = 0\n    class_precisoin_class = []\n    class_mprecision = 0\n\n    for i in range(len(class_intersection_meter)):\n        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n        class_iou_class.append(class_iou)\n        class_miou += class_iou\n        \n        class_recall = class_intersection_meter[i]/(class_target_meter[i]+ 1e-10)\n        class_recall_class.append(class_recall)\n        class_mrecall += class_recall\n\n        class_precision = class_intersection_meter[i]/(class_union_meter[i] - class_target_meter[i] + class_intersection_meter[i]+ 1e-10)\n        class_precisoin_class.append(class_precision)\n        class_mprecision += class_precision\n\n    class_mrecall = class_mrecall*1.0 / len(class_intersection_meter)  \n    class_miou = class_miou*1.0 / len(class_intersection_meter)\n    class_mprecision = class_mprecision*1.0 / len(class_intersection_meter)\n\n\n    logger.info('mean IoU---Val result: mIoU {:.4f}.'.format(class_miou))\n    logger.info('mean recall---Val result: mrecall {:.4f}.'.format(class_mrecall))\n    logger.info('mean precisoin---Val result: mprecisoin {:.4f}.'.format(class_mprecision))\n\n    for i in range(split_gap):\n        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t recall {:.4f}. \\t precision {:.4f}. \\t {}'.format(i+1, \\\n                        class_iou_class[i], class_recall_class[i], class_precisoin_class[i],\\\n                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))     \n\n     \n\n    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n    for i in range(2):\n        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\n    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\n    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_mrecall, class_mprecision, class_iou_class", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "test.py", "chunked_list": ["import os\nimport datetime\nimport random\nimport time\nimport cv2\nimport numpy as np\nimport logging\nimport argparse\nfrom visdom import Visdom\nimport os.path as osp", "from visdom import Visdom\nimport os.path as osp\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data", "import torch.optim\nimport torch.utils.data\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\n\nfrom torch.cuda.amp import autocast as autocast\nfrom torch.cuda import amp\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom model.few_seg import R2Net", "\nfrom model.few_seg import R2Net\n# from model.workdir import\nfrom dataset import iSAID, iSAID_1\n\nfrom util import config\nfrom util.util import AverageMeter, poly_learning_rate, intersectionAndUnionGPU, get_model_para_number, setup_seed, get_logger, get_save_path, \\\n                                    is_same_model, fix_bn, sum_list, check_makedirs,freeze_modules, adjust_learning_rate_poly,lr_decay\n\ncv2.ocl.setUseOpenCL(False)", "\ncv2.ocl.setUseOpenCL(False)\ncv2.setNumThreads(0)\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='PyTorch Few-Shot Semantic Segmentation')\n    parser.add_argument('--arch', type=str, default='R2Net', help='') # \n    parser.add_argument('--shot', type=int, default=1, help='') # \n    parser.add_argument('--split', type=int, default=0, help='') # \n    parser.add_argument('--dataset', type=str, default='iSAID', help='') # \n    parser.add_argument('--backbone', type=str, default='vgg', help='') # \n    parser.add_argument('--s_q', default='False', help='') #\n    parser.add_argument('--cross_domain', default=None, help='') #\n    parser.add_argument('--variable1', type=str, default='', help='') #\n    parser.add_argument('--variable2', type=str, default='', help='') #\n    parser.add_argument('--local_rank', type=int, default=-1, help='number of cpu threads to use during batch generation')    \n    parser.add_argument('--opts', help='see config/ade20k/ade20k_pspnet50.yaml for all options', default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n    base_config = 'config/base.yaml'\n    data_config = 'config/dataset/{}.yaml'.format(args.cross_domain)\n    if args.arch in ['R2Net']:\n        model_config = 'config/model/few_seg/{}.yaml'.format(args.arch)\n    else:\n        model_config = 'config/model/workdir/{}.yaml'.format(args.arch)\n\n    if os.path.exists(model_config):\n        cfg = config.load_cfg_from_cfg_file([base_config, data_config, model_config])\n    else:\n        cfg = config.load_cfg_from_cfg_file([base_config, data_config])\n\n    cfg = config.merge_cfg_from_args(cfg, args)\n    if args.opts is not None:\n        cfg = config.merge_cfg_from_list(cfg, args.opts)\n    return cfg", "\n\ndef get_model(args):\n\n    model = eval(args.arch).OneModel(args, cls_type='Base')\n    optimizer = model.get_optim(model, args.lr_decay, LR=args.base_lr)\n\n    model = model.cuda()\n\n    # Resume\n    get_save_path(args)\n    check_makedirs(args.snapshot_path)\n    check_makedirs(args.result_path)\n\n    weight_path = osp.join(args.snapshot_path, 'best.pth')\n    if os.path.isfile(weight_path):\n        logger.info(\"=> loading checkpoint '{}'\".format(weight_path))\n        checkpoint = torch.load(weight_path, map_location=torch.device('cpu'))\n        args.start_epoch = checkpoint['epoch']\n        new_param = checkpoint['state_dict']\n        try: \n            model.load_state_dict(new_param)\n        except RuntimeError:                   # 1GPU loads mGPU model\n            for key in list(new_param.keys()):\n                new_param[key[7:]] = new_param.pop(key)\n            model.load_state_dict(new_param)\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        logger.info(\"=> loaded checkpoint '{}' (epoch {})\".format(weight_path, checkpoint['epoch']))\n    else:\n        logger.info(\"=> no checkpoint found at '{}'\".format(weight_path))\n\n    # Get model para.\n    total_number, learnable_number = get_model_para_number(model)\n    print('Number of Parameters: %d' % (total_number))\n    print('Number of Learnable Parameters: %d' % (learnable_number))\n\n    time.sleep(5)\n    return model, optimizer", "\ndef main_process():\n    return not args.distributed or (args.distributed and (args.local_rank == 0))\n\ndef main():\n    global args, logger\n    args = get_parser()\n    logger = get_logger()\n    args.logger = logger\n    args.distributed = True if torch.cuda.device_count() > 1 else False\n\n    if main_process():\n        print(args)\n\n    if args.manual_seed is not None:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n\n    if main_process():\n        logger.info(\"=> creating dataset ...\")\n        \n    train_data = eval('{}.{}_few_dataset'.format(args.dataset, args.dataset))(split=args.split, \\\n                                        shot=args.shot, mode='train', transform_dict=args.train_transform)\n    train_id = []\n    for i in range(len(train_data.list)):\n        train_id.append(train_data.class_id[train_data.list[i]])\n    val_data =  eval('{}.{}_few_dataset'.format(args.cross_domain, args.cross_domain))(split=args.split, \\\n                                        shot=args.shot, mode='val', transform_dict=args.val_transform)\n    val_data.list = []\n    for id in val_data.all_class:\n        if val_data.class_id[id] not in train_id:\n            val_data.list.append(id)\n    tmp = set() \n    for item in val_data.list:\n        tmp = tmp | set(val_data.sub_class_file_list[item])\n    val_data.data_list = list(tmp)\n        \n    val_sampler = DistributedSampler(val_data) if args.distributed else None                  \n    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size_val, shuffle=False, \\\n                                            num_workers=args.workers, pin_memory=False, sampler=val_sampler)\n\n    args.base_class_num =len(train_data.list)\n    args.novel_class_num = len(val_data.list)\n\n    # if args.cross_dataset:\n    #     train_data = eval('{}.{}_few_dataset'.format(args.train_dataset, args.train_dataset)).class_id\n\n    logger.info('val_list: {}'.format(val_data.list))\n    logger.info('num_val_data: {}'.format(len(val_data)))\n\n    if main_process():\n        logger.info(\"=> creating model ...\")\n    model, _ = get_model(args)\n    \n    logger.info(model)\n\n    val_manual_seed = args.manual_seed\n    if eval(args.s_q):\n        val_num = 2\n    else:\n        val_num = 5\n\n    setup_seed(val_manual_seed, False)\n    seed_array = np.random.randint(0,1000,val_num) \n\n    start_time = time.time()\n    FBIoU_array = np.zeros(val_num)\n    mIoU_array = np.zeros(val_num)\n    pIoU_array = np.zeros(val_num)\n    class_array = np.zeros([val_num, len(val_data.list)])\n\n    txt_root = 'exp/{}/test_result/'.format(args.arch)\n    check_makedirs(txt_root)\n\n    for val_id in range(val_num):\n        val_seed = seed_array[val_id]\n        print('Val: [{}/{}] \\t Seed: {}'.format(val_id+1, val_num, val_seed))\n        loss_val, mIoU_val, mAcc_val, allAcc_val, class_miou, pIoU, class_iou = validate(val_loader, model, val_seed) \n\n        FBIoU_array[val_id], mIoU_array[val_id], pIoU_array[val_id] = mIoU_val, class_miou, pIoU\n        for class_id in range(len(class_iou)):\n            class_array[val_id, class_id] = class_iou[class_id]\n    class_marray = np.mean(class_array, 0)\n    total_time = time.time() - start_time\n    t_m, t_s = divmod(total_time, 60)\n    t_h, t_m = divmod(t_m, 60)\n    total_time = '{:02d}h {:02d}m {:02d}s'.format(int(t_h), int(t_m), int(t_s))\n\n    print('\\nTotal running time: {}'.format(total_time))\n    print('Seed0: {}'.format(val_manual_seed))\n    print('mIoU:  {}'.format(np.round(mIoU_array, 4)))\n    print('FBIoU: {}'.format(np.round(FBIoU_array, 4)))\n    print('pIoU:  {}'.format(np.round(pIoU_array, 4)))\n    print('-'*43)\n    print('Best_Seed_m: {} \\t Best_Seed_F: {} \\t Best_Seed_p: {}'.format(seed_array[mIoU_array.argmax()], seed_array[FBIoU_array.argmax()], seed_array[pIoU_array.argmax()]))\n    print('Best_mIoU: {:.4f} \\t Best_FBIoU: {:.4f} \\t Best_pIoU: {:.4f}'.format(mIoU_array.max(), FBIoU_array.max(), pIoU_array.max()))\n    print('Mean_mIoU: {:.4f} \\t Mean_FBIoU: {:.4f} \\t Mean_pIoU: {:.4f}'.format(mIoU_array.mean(), FBIoU_array.mean(), pIoU_array.mean()))\n\n    with open(txt_root + '{}_{}.txt'.format(args.dataset, args.cross_domain), 'a') as f:\n        f.write('\\nsupp=query : {}  '.format(args.s_q)+ '\\n')\n        f.write('{} {} split{} {} shot'.format(args.arch, args.backbone, args.split, args.shot) + '\\n')\n        f.write('Seed0: {}\\n'.format(val_manual_seed))\n        f.write('Seed:  {}\\n'.format(seed_array))\n        f.write('mIoU:  {}\\n'.format(np.round(mIoU_array, 4)))\n        f.write('FBIoU: {}\\n'.format(np.round(FBIoU_array, 4)))\n        f.write('pIoU:  {}\\n'.format(np.round(pIoU_array, 4)))\n        f.write('Best_Seed_m: {} \\t Best_Seed_F: {} \\t Best_Seed_p: {} \\n'.format(seed_array[mIoU_array.argmax()], seed_array[FBIoU_array.argmax()], seed_array[pIoU_array.argmax()]))\n        f.write('Best_mIoU: {:.4f} \\t Best_FBIoU: {:.4f} \\t Best_pIoU: {:.4f} \\n'.format(mIoU_array.max(), FBIoU_array.max(), pIoU_array.max()))\n        f.write('Mean_mIoU: {:.4f} \\t Mean_FBIoU: {:.4f} \\t Mean_pIoU: {:.4f} \\n'.format(mIoU_array.mean(), FBIoU_array.mean(), pIoU_array.mean()))\n        for id in range(len(val_data.list)):\n            f.write('{}\\t'.format(val_data.list[id]))\n        f.write('\\n')\n        for id in range(len(val_data.list)):\n            f.write('{:.2f}\\t'.format(class_marray[id]*100))\n        f.write('\\n' + '-'*47 + '\\n')\n        f.write(str(datetime.datetime.now()) + '\\n')", "\ndef validate(val_loader, model, val_seed):\n    logger.info('>>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>')\n    batch_time = AverageMeter()\n    model_time = AverageMeter()\n    data_time = AverageMeter()\n    loss_meter = AverageMeter()\n    intersection_meter = AverageMeter()\n    union_meter = AverageMeter()\n    target_meter = AverageMeter()\n\n    split_gap = len(val_loader.dataset.list)\n    if args.s_q:\n        test_num = min(len(val_loader)*2, 1000)\n    else:\n        test_num = 1000\n\n    class_intersection_meter = [0]*split_gap\n    class_union_meter = [0]*split_gap   \n\n    if args.manual_seed is not None and args.fix_random_seed_val:\n        setup_seed(args.manual_seed, args.seed_deterministic)\n\n    setup_seed(val_seed, args.seed_deterministic)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n\n    model.eval()\n    end = time.time()\n    val_start = end\n\n    alpha = round(test_num / args.batch_size_val)\n    iter_num = 0\n    total_time = 0\n    for e in range(10):\n        for i, (input, target, s_input, s_mask, subcls, ori_label) in enumerate(val_loader):\n            if iter_num * args.batch_size_val >= test_num:\n                break\n            iter_num += 1\n            data_time.update(time.time() - end)\n\n            s_input = s_input.cuda(non_blocking=True)\n            s_mask = s_mask.cuda(non_blocking=True)\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n            ori_label = ori_label.cuda(non_blocking=True)\n            start_time = time.time()\n\n            # with autocast():\n            if eval(args.s_q):\n                assert (args.shot==1)\n                with torch.no_grad():\n                    output = model(s_x=input.unsqueeze(1), s_y=target.unsqueeze(1), x=input, y=target, cat_idx=subcls)\n            else:\n                with torch.no_grad():\n                    output = model(s_x=s_input, s_y=s_mask, x=input, y=target, cat_idx=subcls)\n            total_time = total_time + 1\n            model_time.update(time.time() - start_time)\n\n            if args.ori_resize:\n                longerside = max(ori_label.size(1), ori_label.size(2))\n                backmask = torch.ones(ori_label.size(0), longerside, longerside, device='cuda')*255\n                backmask[:, :ori_label.size(1), :ori_label.size(2)] = ori_label\n                target = backmask.clone().long()\n\n            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n            output = output.float()\n            loss = criterion(output, target)    \n\n            n = input.size(0)\n            loss = torch.mean(loss)\n\n            output = output.max(1)[1]\n            for b_id in range(output.size(0)):\n                intersection, union, new_target = intersectionAndUnionGPU(output[b_id,], target[b_id,], 2, args.ignore_label)\n                intersection, union, new_target = intersection.cpu().numpy(), union.cpu().numpy(), new_target.cpu().numpy()\n                intersection_meter.update(intersection), union_meter.update(union), target_meter.update(new_target)\n\n                tmp_id = subcls[0].cpu().numpy()[b_id]\n                class_intersection_meter[tmp_id] += intersection[1]\n                class_union_meter[tmp_id] += union[1] \n\n            accuracy = sum(intersection_meter.val[1:]) / (sum(target_meter.val[1:]) + 1e-10)\n\n            loss_meter.update(loss.item(), input.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if ((iter_num ) % round((alpha/20)) == 0):\n                logger.info('Test: [{}/{}] '\n                            'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                            'Batch {batch_time.val:.3f} ({batch_time.avg:.3f}) '\n                            'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n                            'Accuracy {accuracy:.4f}.'.format(iter_num* args.batch_size_val, test_num,\n                                                            data_time=data_time,\n                                                            batch_time=batch_time,\n                                                            loss_meter=loss_meter,\n                                                            accuracy=accuracy))\n    val_time = time.time()-val_start\n\n    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n    mIoU = np.mean(iou_class)\n    mAcc = np.mean(accuracy_class)\n    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n\n    \n    class_iou_class = []\n    class_miou = 0\n    for i in range(len(class_intersection_meter)):\n        class_iou = class_intersection_meter[i]/(class_union_meter[i]+ 1e-10)\n        class_iou_class.append(class_iou)\n        class_miou += class_iou\n    class_miou = class_miou*1.0 / len(class_intersection_meter)\n    logger.info('meanIoU---Val result: mIoU {:.4f}.'.format(class_miou))\n    for i in range(split_gap):\n        logger.info('Class_{}: \\t Result: iou {:.4f}. \\t {}'.format(i+1, class_iou_class[i],\\\n                         val_loader.dataset.class_id[val_loader.dataset.list[i]]))            \n     \n\n    logger.info('FBIoU---Val result: mIoU/mAcc/allAcc {:.4f}/{:.4f}/{:.4f}.'.format(mIoU, mAcc, allAcc))\n    for i in range(2):\n        logger.info('Class_{} Result: iou/accuracy {:.4f}/{:.4f}.'.format(i, iou_class[i], accuracy_class[i]))\n    logger.info('<<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<')\n\n    print('total time: {:.4f}, avg inference time: {:.4f}, count: {}'.format(val_time, model_time.avg, test_num))\n\n    return loss_meter.avg, mIoU, mAcc, allAcc, class_miou, iou_class[1], class_iou_class", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "lists/iSAID/gen_list.py", "chunked_list": ["import numpy as np\nimport cv2\nimport os\nimport time\nfrom tqdm import tqdm\n# from dataset.base import classId2className\nclassId2className = {\n                    'coco': {\n                         0: 'background',\n                         1: 'person',", "                         0: 'background',\n                         1: 'person',\n                         2: 'bicycle',\n                         3: 'car',\n                         4: 'motorcycle',\n                         5: 'airplane',\n                         6: 'bus',\n                         7: 'train',\n                         8: 'truck',\n                         9: 'boat',", "                         8: 'truck',\n                         9: 'boat',\n                         10: 'traffic light',\n                         11: 'fire hydrant',\n                         12: 'stop sign',\n                         13: 'parking meter',\n                         14: 'bench',\n                         15: 'bird',\n                         16: 'cat',\n                         17: 'dog',", "                         16: 'cat',\n                         17: 'dog',\n                         18: 'horse',\n                         19: 'sheep',\n                         20: 'cow',\n                         21: 'elephant',\n                         22: 'bear',\n                         23: 'zebra',\n                         24: 'giraffe',\n                         25: 'backpack',", "                         24: 'giraffe',\n                         25: 'backpack',\n                         26: 'umbrella',\n                         27: 'handbag',\n                         28: 'tie',\n                         29: 'suitcase',\n                         30: 'frisbee',\n                         31: 'skis',\n                         32: 'snowboard',\n                         33: 'sports ball',", "                         32: 'snowboard',\n                         33: 'sports ball',\n                         34: 'kite',\n                         35: 'baseball bat',\n                         36: 'baseball glove',\n                         37: 'skateboard',\n                         38: 'surfboard',\n                         39: 'tennis racket',\n                         40: 'bottle',\n                         41: 'wine glass',", "                         40: 'bottle',\n                         41: 'wine glass',\n                         42: 'cup',\n                         43: 'fork',\n                         44: 'knife',\n                         45: 'spoon',\n                         46: 'bowl',\n                         47: 'banana',\n                         48: 'apple',\n                         49: 'sandwich',", "                         48: 'apple',\n                         49: 'sandwich',\n                         50: 'orange',\n                         51: 'broccoli',\n                         52: 'carrot',\n                         53: 'hot dog',\n                         54: 'pizza',\n                         55: 'donut',\n                         56: 'cake',\n                         57: 'chair',", "                         56: 'cake',\n                         57: 'chair',\n                         58: 'sofa',\n                         59: 'pottedplant',\n                         60: 'bed',\n                         61: 'diningtable',\n                         62: 'toilet',\n                         63: 'tv',\n                         64: 'laptop',\n                         65: 'mouse',", "                         64: 'laptop',\n                         65: 'mouse',\n                         66: 'remote',\n                         67: 'keyboard',\n                         68: 'cell phone',\n                         69: 'microwave',\n                         70: 'oven',\n                         71: 'toaster',\n                         72: 'sink',\n                         73: 'refrigerator',", "                         72: 'sink',\n                         73: 'refrigerator',\n                         74: 'book',\n                         75: 'clock',\n                         76: 'vase',\n                         77: 'scissors',\n                         78: 'teddy bear',\n                         79: 'hair drier',\n                         80: 'toothbrush'},\n", "                         80: 'toothbrush'},\n\n                    'pascal': {\n                        0: 'background',\n                        1: 'airplane',\n                        2: 'bicycle',\n                        3: 'bird',\n                        4: 'boat',\n                        5: 'bottle',\n                        6: 'bus',", "                        5: 'bottle',\n                        6: 'bus',\n                        7: 'cat',\n                        8: 'car',\n                        9: 'chair',\n                        10: 'cow',\n                        11: 'diningtable',\n                        12: 'dog',\n                        13: 'horse',\n                        14: 'motorcycle',", "                        13: 'horse',\n                        14: 'motorcycle',\n                        15: 'person',\n                        16: 'pottedplant',\n                        17: 'sheep',\n                        18: 'sofa',\n                        19: 'train',\n                        20: 'tv'\n                        },\n                    ", "                        },\n                    \n                    'iSAID':{\n                        0: 'unlabeled',\n                        1: 'ship',\n                        2: 'storage_tank',\n                        3: 'baseball_diamond',\n                        4: 'tennis_court',\n                        5: 'basketball_court',\n                        6: 'Ground_Track_Field',", "                        5: 'basketball_court',\n                        6: 'Ground_Track_Field',\n                        7: 'Bridge',\n                        8: 'Large_Vehicle',\n                        9: 'Small_Vehicle',\n                        10: 'Helicopter',\n                        11: 'Swimming_pool',\n                        12: 'Roundabout',\n                        13: 'Soccer_ball_field',\n                        14: 'plane',", "                        13: 'Soccer_ball_field',\n                        14: 'plane',\n                        15: 'Harbor'\n                    },\n\n                    'few_shot':{\n                        0: 'Background',\n                        1: 'Foreground',\n\n                    }", "\n                    }\n                     }\n\n\ndataset = 'iSAID'\ndata_root = '/disk2/lcb/datasets/{}/ann_dir/'.format(dataset)\n\nlist_dir = 'lists/{}/'.format(dataset)\nif not os.path.exists(list_dir):\n    os.makedirs(list_dir)", "list_dir = 'lists/{}/'.format(dataset)\nif not os.path.exists(list_dir):\n    os.makedirs(list_dir)\n\nclass_num = len(classId2className[dataset]) -1\n\ndef gen_list(dataset, data_root, list_dir, class_num):\n\n    for mode in ['train', 'val']:\n        mutli_class_num = np.zeros(class_num)\n        pot_num = np.zeros(class_num)\n        pic_num= np.zeros(class_num)\n\n        tmp_root = data_root + mode\n        new_root = tmp_root #.replace('ann_dir', 'ann_dir_1')\n\n        file_list = os.listdir(new_root)\n        pair_list = []\n        for idx in tqdm(range(len(file_list))):\n            file = file_list[idx]\n            label_name = 'ann_dir/{}/'.format(mode) +  file\n            image_name = ('img_dir/{}/'.format(mode) + file.replace('_instance_color_RGB', ''))\n            label = cv2.imread(new_root + '/' + file, cv2.IMREAD_GRAYSCALE)\n            label_list = np.unique(label).tolist()\n            if 0 in label_list:\n                label_list.remove(0)\n            if 255 in label_list:\n                label_list.remove(255)\n\n            if len(label_list) != 0 :  #and len(label_list) <3\n                mutli_class_num[len(label_list)-1] += 1\n\n                for cls in label_list:\n                    if len(np.where(label == cls)[0]) > 2*32*32 :\n                        pair_list.append(image_name + ' ' + label_name)\n                        \n                        pic_num[cls-1] += 1\n                        pot_num[cls-1] += len(np.where(label == cls)[0])/1000000\n\n        pair_list = list(set(pair_list))\n\n        with open(list_dir + '{}_num.txt'.format(mode), 'a') as f:\n            f.write('-'*23 +'{}_{}'.format( dataset, mode) + '-'*23  + '\\n')\n            for i in range(class_num):\n                f.write('\u540c\u65f6\u542b\u6709{}\u4e2a\u7c7b\u522b\u7684\u56fe\u50cf\u6570\u76ee\uff1a{}'.format(i+1, mutli_class_num[i]) + '\\n')\n            f.write('\u56fe\u50cf\u603b\u6570\uff1a{}'.format(np.sum(mutli_class_num) )+ '\\n')\n\n            for i in range(class_num):\n                f.write('\u7c7b\u522b{} \\t \u56fe\u50cf\u6570\u76ee\uff1a{} \\t {} '.format(i+1,  pic_num[i], classId2className[dataset][i+1]) + '\\n')\n\n            f.write('\u56fe\u50cf\u6570\u76ee\u7c7b\u522b\u6bd4\u5217\u4e3a\uff1a{}'. format(np.round(pic_num/np.min(pic_num), 2))  + '\\n')\n\n            for i in range(class_num):\n                f.write('\u7c7b\u522b{} \\t \u50cf\u7d20\u70b9\u6570\u76ee\uff1a{:.1f} million \\t {}'.format(i+1, pot_num[i], classId2className[dataset][i+1]) + '\\n')\n\n            f.write('\u50cf\u7d20\u70b9\u7c7b\u522b\u6bd4\u5217\u4e3a\uff1a{}'. format(np.round(pot_num/np.min(pot_num), 2))  + '\\n')\n        \n        with open(list_dir + '{}.txt'.format(mode), 'a') as f:\n            for pair in pair_list:\n                f.write(pair + '\\n')", "                \ndef change_label(data_root):\n    for mode in ['train', 'val']:\n        tmp_root = data_root + mode\n        new_root = tmp_root.replace('ann_dir', 'ann_dir_1')\n        if not os.path.exists(new_root):\n            os.makedirs(new_root)\n        file_list = os.listdir(tmp_root)\n        for idx in tqdm(range(len(file_list))):\n            file = file_list[idx]\n            label_name = new_root + '/' + file\n            label = cv2.imread(tmp_root + '/' + file, cv2.IMREAD_GRAYSCALE)\n            label[label==0] = 255\n            label[label==6] = 0\n            cv2.imwrite(label_name, label)", "\n\n# change_label(data_root)\ngen_list(dataset, data_root, list_dir, class_num)\n"]}
{"filename": "model/util/ASPP.py", "chunked_list": ["\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\nclass ASPP(nn.Module):\n    def __init__(self, out_channels=256):\n        super(ASPP, self).__init__()\n        self.layer6_0 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(),\n        )\n        self.layer6_1 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(),\n            )\n        self.layer6_2 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n            nn.ReLU(),\n            )\n        self.layer6_3 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n            nn.ReLU(),\n            )\n        self.layer6_4 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n            nn.ReLU(),\n            )\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \n    def forward(self, x):\n        feature_size = x.shape[-2:]\n        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\n        global_feature = self.layer6_0(global_feature)\n\n        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n        out = torch.cat(\n            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n        return out", "\n\nclass ASPP_Drop(nn.Module):\n    def __init__(self, out_channels=256):\n        super(ASPP_Drop, self).__init__()\n        self.layer6_0 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.5),\n        )\n        self.layer6_1 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.5),\n            )\n        self.layer6_2 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.5)\n            )\n        self.layer6_3 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.5)\n            )\n        self.layer6_4 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.5)\n            )\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        feature_size = x.shape[-2:]\n        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\n        global_feature = self.layer6_0(global_feature)\n\n        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n        out = torch.cat(\n            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n        return out", "\n\nclass ASPP_BN(nn.Module):\n    def __init__(self, out_channels=256):\n        super(ASPP_BN, self).__init__()\n        self.layer6_0 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()            \n        )\n        self.layer6_1 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()  \n            )\n        self.layer6_2 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=6,dilation=6, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()  \n            )\n        self.layer6_3 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()  \n            )\n        self.layer6_4 = nn.Sequential(\n            nn.Conv2d(out_channels , out_channels , kernel_size=3, stride=1, padding=18, dilation=18, bias=True),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()  \n            )\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        feature_size = x.shape[-2:]\n        global_feature = F.avg_pool2d(x, kernel_size=feature_size)\n\n        global_feature = self.layer6_0(global_feature)\n\n        global_feature = global_feature.expand(-1, -1, feature_size[0], feature_size[1])\n        out = torch.cat(\n            [global_feature, self.layer6_1(x), self.layer6_2(x), self.layer6_3(x), self.layer6_4(x)], dim=1)\n        return out"]}
{"filename": "model/util/cls.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import BatchNorm2d as BatchNorm        \n\nfrom torch.cuda.amp import autocast as autocast\nfrom model.backbone.layer_extrator import layer_extrator\n\n\nclass cls(nn.Module):\n    def __init__(self, backbone, fp16=True):\n        super(cls, self).__init__()\n\n\n        self.backbone = backbone\n        self.criterion = nn.CrossEntropyLoss()\n        self.pretrained = True\n        self.classes = 46\n\n        self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=self.backbone, pretrained=True)\n        self.fp16 = fp16\n\n        if self.backbone == 'resnet50' or self.backbone == 'resnet101':\n            self.avgpool = nn.AdaptiveAvgPool2d(1)\n            self.fc = nn.Linear(512 * 4, self.classes )\n        else:\n            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n            self.fc = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, self.classes),\n        )\n\n        self.GAP = nn.AdaptiveAvgPool2d(1)\n\n    def get_optim(self, model, lr_dict, LR):\n        optimizer = torch.optim.SGD(model.parameters(),\\\n            lr=LR, momentum=lr_dict['momentum'], weight_decay=lr_dict['weight_decay'])\n        \n        return optimizer\n    \n    def forward(self, x,  y):\n        with autocast(enabled=self.fp16):\n            x = self.layer0(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n\n            x = self.avgpool(x)\n            x = x.view(x.size(0), -1)\n            x = self.fc(x)\n            # x = F.log_softmax(x, 1,)\n            if self.training:\n                loss = self.criterion(x, y.long())\n                # loss_1 = F.nll_loss(x,y.long())\n                return x, loss\n            else:\n                return x", "\nclass cls(nn.Module):\n    def __init__(self, backbone, fp16=True):\n        super(cls, self).__init__()\n\n\n        self.backbone = backbone\n        self.criterion = nn.CrossEntropyLoss()\n        self.pretrained = True\n        self.classes = 46\n\n        self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=self.backbone, pretrained=True)\n        self.fp16 = fp16\n\n        if self.backbone == 'resnet50' or self.backbone == 'resnet101':\n            self.avgpool = nn.AdaptiveAvgPool2d(1)\n            self.fc = nn.Linear(512 * 4, self.classes )\n        else:\n            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n            self.fc = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, self.classes),\n        )\n\n        self.GAP = nn.AdaptiveAvgPool2d(1)\n\n    def get_optim(self, model, lr_dict, LR):\n        optimizer = torch.optim.SGD(model.parameters(),\\\n            lr=LR, momentum=lr_dict['momentum'], weight_decay=lr_dict['weight_decay'])\n        \n        return optimizer\n    \n    def forward(self, x,  y):\n        with autocast(enabled=self.fp16):\n            x = self.layer0(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n\n            x = self.avgpool(x)\n            x = x.view(x.size(0), -1)\n            x = self.fc(x)\n            # x = F.log_softmax(x, 1,)\n            if self.training:\n                loss = self.criterion(x, y.long())\n                # loss_1 = F.nll_loss(x,y.long())\n                return x, loss\n            else:\n                return x", "\n    #   \n    "]}
{"filename": "model/util/PSPNet.py", "chunked_list": ["import torch\nfrom torch import nn\nfrom torch._C import device\nimport torch.nn.functional as F\nfrom torch.nn import BatchNorm2d as BatchNorm        \n\nfrom model.backbone.layer_extrator import layer_extrator\nfrom torch.cuda.amp import autocast\n\nclass PPM(nn.Module):\n    def __init__(self, in_dim, reduction_dim, bins):\n        super(PPM, self).__init__()\n        self.features = []\n        for bin in bins:\n            self.features.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(bin),\n                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n                nn.BatchNorm2d(reduction_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.features = nn.ModuleList(self.features)\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for f in self.features:\n            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n        return torch.cat(out, 1)", "\nclass PPM(nn.Module):\n    def __init__(self, in_dim, reduction_dim, bins):\n        super(PPM, self).__init__()\n        self.features = []\n        for bin in bins:\n            self.features.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(bin),\n                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n                nn.BatchNorm2d(reduction_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.features = nn.ModuleList(self.features)\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for f in self.features:\n            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n        return torch.cat(out, 1)", "\n\nclass OneModel(nn.Module):\n    def __init__(self, args):\n        super(OneModel, self).__init__()\n\n        self.criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n        self.classes = args.base_class_num +1\n        self.backbone = args.backbone\n\n        self.fp16 = args.fp16\n        \n        if args.backbone in ['vgg', 'resnet50', 'resnet101']:\n            self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=args.backbone, pretrained = True)\n            self.encoder = nn.Sequential(self.layer0, self.layer1, self.layer2, self.layer3, self.layer4)\n            fea_dim = 512 if args.backbone == 'vgg' else 2048\n\n        # Base Learner\n        bins=(1, 2, 3, 6)\n        self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)\n        self.cls = nn.Sequential(\n            nn.Conv2d(fea_dim*2, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p=0.1),\n            nn.Conv2d(512, self.classes, kernel_size=1))\n\n    def get_optim(self, model, args, LR):\n        optimizer = torch.optim.SGD(\n            [     \n            {'params': model.encoder.parameters(), 'lr' : LR},\n            {'params': model.ppm.parameters(), 'lr' : LR*10},\n            {'params': model.cls.parameters(), 'lr' : LR*10},\n            ], lr=LR, momentum=args.momentum, weight_decay=args.weight_decay)\n        return optimizer\n\n\n    def forward(self, x, y):\n        with autocast(enabled=self.fp16):\n            x_size = x.size()\n            h = x_size[2]\n            w = x_size[3]\n\n            x = self.encoder(x)\n            if self.backbone == 'swin':\n                x = self.ppm(x.permute(0, 3, 1, 2))\n            else:\n                x = self.ppm(x)\n            x = self.cls(x)\n\n            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n\n            if self.training:\n                main_loss = self.criterion(x, y.long())\n                return x.max(1)[1], main_loss, 0, 0\n            else:\n                return x"]}
{"filename": "model/few_seg/R2Net.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import BatchNorm2d as BatchNorm        \n\nimport numpy as np\nimport random\nimport time\nimport cv2\n", "import cv2\n\nfrom model.backbone.layer_extrator import layer_extrator\n\nfrom model.util.ASPP import ASPP, ASPP_Drop ,ASPP_BN\nfrom model.util.PSPNet import OneModel as PSPNet\nfrom torch.cuda.amp import autocast as autocast\n\ndef Cor_Map(query_feat, supp_feat_list, mask_list):\n    corr_query_mask_list = []\n    cosine_eps = 1e-7\n    for i, tmp_supp_feat in enumerate(supp_feat_list):\n        resize_size = tmp_supp_feat.size(2)\n        tmp_mask = F.interpolate(mask_list[i], size=(resize_size, resize_size), mode='bilinear', align_corners=True)\n\n        tmp_supp_feat_4 = tmp_supp_feat * tmp_mask\n        q = query_feat\n        s = tmp_supp_feat_4\n        bsize, ch_sz, sp_sz, _ = q.size()[:]\n\n        tmp_query = q\n        tmp_query = tmp_query.contiguous().view(bsize, ch_sz, -1)\n        tmp_query_norm = torch.norm(tmp_query, 2, 1, True)\n\n        tmp_supp = s               \n        tmp_supp = tmp_supp.contiguous().view(bsize, ch_sz, -1) \n        tmp_supp = tmp_supp.contiguous().permute(0, 2, 1)\n        tmp_supp_norm = torch.norm(tmp_supp, 2, 2, True) \n\n        similarity = torch.bmm(tmp_supp, tmp_query)/(torch.bmm(tmp_supp_norm, tmp_query_norm) + cosine_eps)   \n        similarity = similarity.max(1)[0].view(bsize, sp_sz*sp_sz)\n        similarity = (similarity - similarity.min(1)[0].unsqueeze(1))/(similarity.max(1)[0].unsqueeze(1) - similarity.min(1)[0].unsqueeze(1) + cosine_eps)\n        corr_query = similarity.view(bsize, 1, sp_sz, sp_sz)\n        corr_query_mask_list.append(corr_query)  \n    corr_query_mask = torch.cat(corr_query_mask_list, 1).mean(1).unsqueeze(1)\n    return corr_query_mask", "def Cor_Map(query_feat, supp_feat_list, mask_list):\n    corr_query_mask_list = []\n    cosine_eps = 1e-7\n    for i, tmp_supp_feat in enumerate(supp_feat_list):\n        resize_size = tmp_supp_feat.size(2)\n        tmp_mask = F.interpolate(mask_list[i], size=(resize_size, resize_size), mode='bilinear', align_corners=True)\n\n        tmp_supp_feat_4 = tmp_supp_feat * tmp_mask\n        q = query_feat\n        s = tmp_supp_feat_4\n        bsize, ch_sz, sp_sz, _ = q.size()[:]\n\n        tmp_query = q\n        tmp_query = tmp_query.contiguous().view(bsize, ch_sz, -1)\n        tmp_query_norm = torch.norm(tmp_query, 2, 1, True)\n\n        tmp_supp = s               \n        tmp_supp = tmp_supp.contiguous().view(bsize, ch_sz, -1) \n        tmp_supp = tmp_supp.contiguous().permute(0, 2, 1)\n        tmp_supp_norm = torch.norm(tmp_supp, 2, 2, True) \n\n        similarity = torch.bmm(tmp_supp, tmp_query)/(torch.bmm(tmp_supp_norm, tmp_query_norm) + cosine_eps)   \n        similarity = similarity.max(1)[0].view(bsize, sp_sz*sp_sz)\n        similarity = (similarity - similarity.min(1)[0].unsqueeze(1))/(similarity.max(1)[0].unsqueeze(1) - similarity.min(1)[0].unsqueeze(1) + cosine_eps)\n        corr_query = similarity.view(bsize, 1, sp_sz, sp_sz)\n        corr_query_mask_list.append(corr_query)  \n    corr_query_mask = torch.cat(corr_query_mask_list, 1).mean(1).unsqueeze(1)\n    return corr_query_mask", "\ndef Weighted_GAP(supp_feat, mask):\n    supp_feat = supp_feat * mask\n    feat_h, feat_w = supp_feat.shape[-2:][0], supp_feat.shape[-2:][1]\n    area = F.avg_pool2d(mask, (supp_feat.size()[2], supp_feat.size()[3])) * feat_h * feat_w + 0.0005\n    supp_feat = F.avg_pool2d(input=supp_feat, kernel_size=supp_feat.shape[-2:]) * feat_h * feat_w / area\n    return supp_feat\n\ndef pro_select(query_feat, pro_list):\n    \"\"\"\n    query_feat  b*c*h*w\n    pro_list  [b*1*c*1*1]*n  \n    \"\"\"\n    query_base = query_feat.unsqueeze(1) # b*1*c*h*w\n    pro_gather = torch.cat(pro_list, 1)  # b*n*c*1*1\n    pro_gather = pro_gather.expand(-1,-1,-1,query_base.size(3), query_base.size(4)) # b*n*c*h*w\n\n    index_map = nn.CosineSimilarity(2)(pro_gather, query_base).unsqueeze(2) # b*n*1*h*w\n    index_pro = index_map.max(1)[1].unsqueeze(1).expand_as(query_base)  # b*1*c*h*w\n    out_pro = torch.gather(pro_gather, 1, index_pro).squeeze(1)\n    out_map = torch.sum(index_map, 1)\n\n    return out_pro, out_map", "def pro_select(query_feat, pro_list):\n    \"\"\"\n    query_feat  b*c*h*w\n    pro_list  [b*1*c*1*1]*n  \n    \"\"\"\n    query_base = query_feat.unsqueeze(1) # b*1*c*h*w\n    pro_gather = torch.cat(pro_list, 1)  # b*n*c*1*1\n    pro_gather = pro_gather.expand(-1,-1,-1,query_base.size(3), query_base.size(4)) # b*n*c*h*w\n\n    index_map = nn.CosineSimilarity(2)(pro_gather, query_base).unsqueeze(2) # b*n*1*h*w\n    index_pro = index_map.max(1)[1].unsqueeze(1).expand_as(query_base)  # b*1*c*h*w\n    out_pro = torch.gather(pro_gather, 1, index_pro).squeeze(1)\n    out_map = torch.sum(index_map, 1)\n\n    return out_pro, out_map", "\nclass feat_decode(nn.Module):\n    def __init__(self, inchannel):\n        super(feat_decode, self).__init__()\n\n        self.ASPP = ASPP()\n\n        self.res1 = nn.Sequential(\n            nn.Conv2d(inchannel*5, inchannel, kernel_size=1, padding=0, bias=False),\n            nn.ReLU(inplace=True),                          \n        )\n        \n        self.res2 = nn.Sequential(\n            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n            nn.ReLU(inplace=True),   \n            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n            nn.ReLU(inplace=True),                             \n        )    \n\n        self.cls = nn.Sequential(\n            nn.Conv2d(inchannel, inchannel, kernel_size=3, padding=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p=0.1),                 \n            nn.Conv2d(inchannel, 1, kernel_size=1)\n        )\n\n    def forward(self,x):\n        x =self.ASPP(x)\n        x = self.res1(x)\n        x = self.res2(x) + x\n        x = self.cls(x)\n\n        return x", "\nclass OneModel(nn.Module):\n    def __init__(self, args, cls_type=None):\n        super(OneModel, self).__init__()\n\n\n        self.shot = args.shot\n        self.criterion = nn.CrossEntropyLoss(ignore_index=args.ignore_label)\n        self.pretrained = args.pretrain\n        self.classes = 2\n        self.fp16 = args.fp16\n        self.backbone = args.backbone\n        self.base_class_num = args.base_class_num\n\n        self.alpha = torch.nn.Parameter(torch.FloatTensor(self.base_class_num+1,1), requires_grad=True)\n        self.beta = torch.nn.Parameter(torch.FloatTensor(self.base_class_num+1,1), requires_grad=True)\n        nn.init.normal_(self.alpha, mean=1.0)\n        nn.init.normal_(self.beta)\n        \n        self.pro_global = torch.FloatTensor(self.base_class_num+1, 256).cuda()\n        nn.init.normal_(self.pro_global)\n        # self.pro_global.data.fill_(0.0)\n\n        if self.pretrained:\n            BaseNet = PSPNet(args)\n            weight_path = 'initmodel/PSPNet/{}/split{}/{}/best.pth'.format(args.dataset, args.split, args.backbone)\n            new_param = torch.load(weight_path, map_location=torch.device('cpu'))['state_dict']\n            print('load <base> weights from: {}'.format(weight_path))\n            try: \n                BaseNet.load_state_dict(new_param)\n            except RuntimeError:                   # 1GPU loads mGPU model\n                for key in list(new_param.keys()):\n                    new_param[key[7:]] = new_param.pop(key)\n                BaseNet.load_state_dict(new_param)\n            \n            self.layer0, self.layer1, self.layer2, \\\n                self.layer3, self.layer4 = BaseNet.layer0, BaseNet.layer1, BaseNet.layer2, BaseNet.layer3, BaseNet.layer4\n\n            self.base_layer = nn.Sequential(BaseNet.ppm, BaseNet.cls)\n        else:\n            self.layer0, self.layer1, self.layer2, self.layer3, self.layer4 = layer_extrator(backbone=args.backbone, pretrained=True)\n\n        reduce_dim = 256\n        if self.backbone == 'vgg':\n            fea_dim = 512 + 256\n        else:\n            fea_dim = 1024 + 512       \n\n        self.down_query = nn.Sequential(\n            nn.Conv2d(fea_dim, reduce_dim, kernel_size=1, padding=0, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p=0.5)                  \n        )\n        self.down_supp = nn.Sequential(\n            nn.Conv2d(fea_dim, reduce_dim, kernel_size=1, padding=0, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(p=0.5)                   \n        )  \n\n        self.init_merge = nn.Sequential(\n            nn.Conv2d(reduce_dim*2 +  1 , reduce_dim, kernel_size=1, padding=0, bias=False),\n            nn.ReLU(inplace=True))\n\n\n        self.init_merge_bg = nn.Sequential(\n            nn.Conv2d(reduce_dim*2 , reduce_dim, kernel_size=1, padding=0, bias=False),\n            nn.ReLU(inplace=True))\n\n\n        self.GAP = nn.AdaptiveAvgPool2d(1)\n\n        self.decode = feat_decode(inchannel= reduce_dim)\n        \n        self.iter = 0\n\n    def get_optim(self, model, lr_dict, LR):\n        optimizer = torch.optim.SGD(\n            [\n            {'params': model.alpha},\n            {'params': model.beta},\n            {'params': model.pro_global},\n            {'params': model.down_query.parameters()},\n            {'params': model.down_supp.parameters()},\n            {'params': model.init_merge.parameters()},\n            {'params': model.init_merge_bg.parameters()},\n            {'params': model.decode.parameters()},\n            ],\n\n            lr=LR, momentum=lr_dict['momentum'], weight_decay=lr_dict['weight_decay'])\n        \n        return optimizer\n\n\n    def forward(self, x, s_x, s_y, y, cat_idx=None):\n        with autocast(enabled=self.fp16):\n            x_size = x.size()\n            h = x_size[2]\n            w = x_size[3]\n\n            # Query Feature\n            with torch.no_grad():\n                query_feat_0 = self.layer0(x)\n                query_feat_1 = self.layer1(query_feat_0)\n                query_feat_2 = self.layer2(query_feat_1)\n                query_feat_3 = self.layer3(query_feat_2)\n                query_feat_4 = self.layer4(query_feat_3)\n                query_out = self.base_layer(query_feat_4)\n                query_out = nn.Softmax2d()(query_out)\n            \n                if self.backbone == 'vgg':\n                    query_feat_2 = F.interpolate(query_feat_2, size=(query_feat_3.size(2),query_feat_3.size(3)), mode='bilinear', align_corners=True)\n\n            query_feat = torch.cat([query_feat_3, query_feat_2], 1)\n            query_feat = self.down_query(query_feat)\n            \n            no_base_map = self.get_no_base_map(query_out, cat_idx)\n\n            # Support Feature     \n            final_supp_list = []\n            mask_list = []\n            act_fg_list = []\n            act_bg_list = []\n            feat_fg_list = []\n            feat_bg_list = []\n            aux_loss_1 = 0\n            aux_loss_2 = 0\n            for i in range(self.shot):\n                pro_fg_list = []\n                pro_bg_list = []\n                mask = (s_y[:,i,:,:] == 1).float().unsqueeze(1)\n                mask_list.append(mask)\n                with torch.no_grad():\n                    supp_feat_0 = self.layer0(s_x[:,i,:,:,:])\n                    supp_feat_1 = self.layer1(supp_feat_0)\n                    supp_feat_2 = self.layer2(supp_feat_1)\n                    supp_feat_3 = self.layer3(supp_feat_2)\n                    supp_feat_4_true = self.layer4(supp_feat_3)\n\n                    mask = F.interpolate(mask, size=(supp_feat_3.size(2), supp_feat_3.size(3)), mode='bilinear', align_corners=True)\n                    supp_feat_4 = self.layer4(supp_feat_3*mask)\n                    final_supp_list.append(supp_feat_4)\n                    if self.backbone == 'vgg':\n                        supp_feat_2 = F.interpolate(supp_feat_2, size=(supp_feat_3.size(2),supp_feat_3.size(3)), mode='bilinear', align_corners=True)\n                \n                    supp_base_out = self.base_layer(supp_feat_4_true.clone())\n                    supp_base_out = nn.Softmax2d()(supp_base_out) # b*(c+1)*h*w\n                    \n\n                supp_feat = torch.cat([supp_feat_3, supp_feat_2], 1)\n                supp_feat_tmp = self.down_supp(supp_feat)\n\n                # gen pro\n                pro_fg = Weighted_GAP(supp_feat_tmp , mask)\n                pro_fg_list.append(pro_fg.unsqueeze(1) )\n                pro_bg = Weighted_GAP(supp_feat_tmp , 1-mask)\n                pro_bg_list.append(pro_bg.unsqueeze(1))\n                # gen pro by act_map\n                pro_list = []\n                for j in range(self.base_class_num +1 ):\n                    pro = Weighted_GAP(supp_feat_tmp , supp_base_out[:,j,].unsqueeze(1)).unsqueeze(1) # b*1*256*1*1\n                    pro_list.append(pro)\n                    if not self.training and j ==0 :\n                        pro_fg_list.append(pro)\n\n                local_pro = torch.cat(pro_list, 1)  #b*(c+1)*256*1*1\n\n                # global 2 local\n\n                cur_local_pro = ((self.alpha * self.pro_global).unsqueeze(0) + (self.beta).unsqueeze(0) * local_pro.squeeze(3).squeeze(3)).unsqueeze(3).unsqueeze(3) # b*(c+1)*256*1*1\n                # local 2 global\n                # with torch.no_grad():\n                # new_pro_global = self.pro_global * self.beta + torch.mean(local_pro, 0).squeeze(2).squeeze(2) * (1-self.beta)\n                # with torch.no_grad():\n                #     self.pro_global = new_pro_global.clone()\n\n                base_fg_list = []\n                base_bg_list = []\n\n                # select pro\n                for b_id in range(query_feat.size(0)):\n                    c_id_array = torch.arange(self.base_class_num+1, device='cuda')\n                    c_id = cat_idx[0][b_id] + 1\n                    c_mask = (c_id_array!=c_id)\n\n                    if self.training:\n                        base_fg_list.append(cur_local_pro[b_id, c_id,:,: ].unsqueeze(0))  # b*256*1*1\n                        base_bg_list.append(cur_local_pro[b_id,c_mask,:,:].unsqueeze(0)) # b*c*256*1*1\n                    else:\n                        base_bg_list.append(cur_local_pro[b_id,:,:,:].unsqueeze(0))  # b*(c+1)*1*1\n\n                if self.training:\n                    base_fg = torch.cat(base_fg_list, 0)  # b*1*256*1*1\n                    base_bg = torch.cat(base_bg_list, 0)  # b*c(c+1)*256*1*1\n                    pro_fg_list.append(base_fg.unsqueeze(1))\n                    pro_bg_list.append(base_bg)\n                    tmp_pro = torch.mean(cur_local_pro, 0 ).squeeze(2)\n                    # tmp_pro = self.pro_global.unsqueeze(2) # (c+1)*256*1\n                    crs = nn.CosineSimilarity(1)(tmp_pro, tmp_pro.transpose(0,2))  # (c+1)*(c+1)\n                    crs[crs==1] = 0\n                    crs_1 = 1-nn.CosineSimilarity(1)(torch.mean(local_pro, 0).squeeze(2), tmp_pro)\n                    gamma = (self.base_class_num+1) * self.base_class_num  \n                    aux_loss_1 +=  (torch.sum(crs) / gamma )\n                    aux_loss_2 += torch.mean(crs_1)\n                else:\n                    base_bg = torch.cat(base_bg_list, 0)  # b*c(c+1)*256*1*1\n                    pro_bg_list.append(base_bg)\n                \n                fg_feat, fg_map = pro_select(query_feat, pro_fg_list)\n                bg_feat, bg_map = pro_select(query_feat, pro_bg_list)\n                \n                feat_bg_list.append(bg_feat.unsqueeze(1))\n                feat_fg_list.append(fg_feat.unsqueeze(1))\n                act_bg_list.append(bg_map.unsqueeze(1))\n                act_fg_list.append(fg_map.unsqueeze(1))\n                \n            if self.shot>1:\n                aux_loss_1 /= self.shot\n                aux_loss_2 /= self.shot\n                fg_dis = torch.cat(act_fg_list, 1)\n                bg_dis = torch.cat(act_bg_list, 1)  # b*k*1*h*w\n                fg_dis =F.softmax(fg_dis, 1)\n                bg_dis =F.softmax(bg_dis, 1)\n                fg_feat = torch.mean(torch.cat(feat_fg_list, 1)*fg_dis, 1)\n                bg_feat = torch.mean(torch.cat(feat_bg_list, 1)*bg_dis, 1)\n                # bg_map = torch.mean(torch.cat(act_bg_list, 1), 1)\n                # fg_map = torch.mean(torch.cat(act_fg_list, 1), 1)\n                \n\n            corr_query_mask = Cor_Map(query_feat_4, final_supp_list, mask_list )\n            corr_query_mask = F.interpolate(corr_query_mask, size=(query_feat.size(2), query_feat.size(3)), mode='bilinear', align_corners=True)\n\n            query_feat_bin = query_feat\n\n            merge_feat_bin = torch.cat([query_feat_bin, fg_feat, corr_query_mask], 1)\n            merge_feat_bin = self.init_merge(merge_feat_bin)     \n\n            merge_feat_bg_bin = torch.cat([query_feat_bin, bg_feat], 1)\n            merge_feat_bg_bin = self.init_merge_bg(merge_feat_bg_bin)   \n\n            out_fg = self.decode(merge_feat_bin * no_base_map)\n            out_bg = self.decode(merge_feat_bg_bin)\n            output_fin = torch.cat([out_bg, out_fg], 1)\n\n            #   Output Part\n            output_fin = F.interpolate(output_fin, size=(h, w), mode='bilinear', align_corners=True)\n                \n            if self.training:\n                act_map = nn.Softmax(1)(output_fin)\n                alpha = self.GAP(act_map[:,1].unsqueeze(1))\n                main_loss = self.criterion(output_fin, y.long()) \n\n                mask_y = (y==1).float().unsqueeze(1)\n                alpha_1 = self.GAP(mask_y)\n                beta = (alpha - alpha_1)**2\n                \n                aux_loss = -(1-alpha)*torch.log(alpha) - beta * torch.log(1-beta)\n                return output_fin.max(1)[1], main_loss, torch.mean(aux_loss), aux_loss_1 + aux_loss_2\n            else:\n                return output_fin\n\n    def get_no_base_map(self, query_out, cat_idx):\n        map_list = []\n        if self.training:\n            for b_id in range(query_out.size(0)):\n                c_id = cat_idx[0][b_id] + 1\n                current_map = query_out[b_id,c_id,]\n                bg_map = query_out[b_id,0,]\n                map_list.append((bg_map + current_map).unsqueeze(0))\n                \n            out_map = torch.cat(map_list, 0).unsqueeze(1)\n        else:\n            out_map = query_out[:,0,]\n        \n        return out_map"]}
{"filename": "model/backbone/transform.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nimport math, copy\nfrom torch.autograd import Variable\n\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 2)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key))]\n        value = value.repeat(self.h, 1, 1).transpose(0, 1).contiguous().unsqueeze(-1)\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(query, key, value, mask=mask,\n                                 dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        return torch.mean(x, -3)", "class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 2)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key))]\n        value = value.repeat(self.h, 1, 1).transpose(0, 1).contiguous().unsqueeze(-1)\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(query, key, value, mask=mask,\n                                 dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        return torch.mean(x, -3)", "\n\nclass PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n\n    def __init__(self, d_model, dropout, max_len=10000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)],\n                         requires_grad=False)\n        return self.dropout(x)", "\n\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn", "\n\ndef clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"]}
{"filename": "model/backbone/utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom itertools import repeat\nimport collections.abc\n\nimport math\nimport warnings\n", "import warnings\n\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\n\n\ndef _trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n                      \"The distribution of values may be incorrect.\",\n                      stacklevel=2)\n\n    # Values are generated by using a truncated uniform distribution and\n    # then using the inverse CDF for the normal distribution.\n    # Get upper and lower cdf values\n    l = norm_cdf((a - mean) / std)\n    u = norm_cdf((b - mean) / std)\n\n    # Uniformly fill tensor with values from [l, u], then translate to\n    # [2l-1, 2u-1].\n    tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n    # Use inverse cdf transform for normal distribution to get truncated\n    # standard normal\n    tensor.erfinv_()\n\n    # Transform to proper mean, std\n    tensor.mul_(std * math.sqrt(2.))\n    tensor.add_(mean)\n\n    # Clamp to ensure it's in the proper range\n    tensor.clamp_(min=a, max=b)\n    return tensor", "\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n    applied while sampling the normal with mean/std applied, therefore a, b args\n    should be adjusted to match the range of mean, std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    with torch.no_grad():\n        return _trunc_normal_(tensor, mean, std, a, b)", "\n\ndef trunc_normal_tf_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n\n    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n    and the result is subsquently scaled and shifted by the mean and std args.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    with torch.no_grad():\n        _trunc_normal_(tensor, 0, 1.0, a, b)\n        tensor.mul_(std).add_(mean)\n    return tensor", "\n\ndef variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    if mode == 'fan_in':\n        denom = fan_in\n    elif mode == 'fan_out':\n        denom = fan_out\n    elif mode == 'fan_avg':\n        denom = (fan_in + fan_out) / 2\n\n    variance = scale / denom\n\n    if distribution == \"truncated_normal\":\n        # constant is stddev of standard normal truncated to (-2, 2)\n        trunc_normal_tf_(tensor, std=math.sqrt(variance) / .87962566103423978)\n    elif distribution == \"normal\":\n        with torch.no_grad():\n            tensor.normal_(std=math.sqrt(variance))\n    elif distribution == \"uniform\":\n        bound = math.sqrt(3 * variance)\n        with torch.no_grad():\n            tensor.uniform_(-bound, bound)\n    else:\n        raise ValueError(f\"invalid distribution {distribution}\")", "\n\ndef lecun_normal_(tensor):\n    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse", "\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\n\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor", "\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor", "\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n\n    def extra_repr(self):\n        return f'drop_prob={round(self.drop_prob,3):0.3f}'", ""]}
{"filename": "model/backbone/vgg.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nBatchNorm = nn.BatchNorm2d\n\n__all__ = [\n    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n    'vgg19_bn', 'vgg19',\n]", "    'vgg19_bn', 'vgg19',\n]\n\n\nmodel_urls = {\n    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',", "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, BatchNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)", "class VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, BatchNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)", "\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, BatchNorm(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)", "\n\ncfg = {\n    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    \"\"\"VGG 11-layer model (configuration \"A\")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n    return model", "\n\ndef vgg11(pretrained=False, **kwargs):\n    \"\"\"VGG 11-layer model (configuration \"A\")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n    return model", "\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n    return model", "\n\ndef vgg13(pretrained=False, **kwargs):\n    \"\"\"VGG 13-layer model (configuration \"B\")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['B']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n    return model", "\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n    return model", "\n\ndef vgg16(pretrained=False, **kwargs):\n    \"\"\"VGG 16-layer model (configuration \"D\")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['D']), **kwargs)\n    if pretrained:\n        #model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n        model_path = './initmodel/vgg16.pth'\n        model.load_state_dict(torch.load(model_path), strict=False) \n    return model", "\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n    if pretrained:\n        #model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n        model_path = './initmodel/vgg16_bn.pth'\n        model.load_state_dict(torch.load(model_path), strict=False)        \n    return model", "\n\ndef vgg19(pretrained=False, **kwargs):\n    \"\"\"VGG 19-layer model (configuration \"E\")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['E']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n    return model", "\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n    return model", "\nif __name__ =='__main__':\n    import os\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n    input = torch.rand(4, 3, 473, 473).cuda()\n    target = torch.rand(4, 473, 473).cuda()*1.0\n    model = vgg16_bn(pretrained=False).cuda()\n    model.train()\n    layer0_idx = range(0,6)\n    layer1_idx = range(6,13)\n    layer2_idx = range(13,23)\n    layer3_idx = range(23,33)\n    layer4_idx = range(34,43)\n    #layer4_idx = range(34,43)\n    print(model.features)\n    layers_0 = []\n    layers_1 = []\n    layers_2 = []\n    layers_3 = []\n    layers_4 = []\n    for idx in layer0_idx:\n        layers_0 += [model.features[idx]]\n    for idx in layer1_idx:\n        layers_1 += [model.features[idx]]\n    for idx in layer2_idx:\n        layers_2 += [model.features[idx]]\n    for idx in layer3_idx:\n        layers_3 += [model.features[idx]] \n    for idx in layer4_idx:\n        layers_4 += [model.features[idx]]         \n\n    layer0 = nn.Sequential(*layers_0) \n    layer1 = nn.Sequential(*layers_1) \n    layer2 = nn.Sequential(*layers_2) \n    layer3 = nn.Sequential(*layers_3) \n    layer4 = nn.Sequential(*layers_4) \n\n    output = layer0(input)\n    print(layer0)\n    print('layer 0: {}'.format(output.size()))\n    output = layer1(output)\n    print(layer1)\n    print('layer 1: {}'.format(output.size()))\n    output = layer2(output)\n    print(layer2)\n    print('layer 2: {}'.format(output.size()))\n    output = layer3(output)\n    print(layer3)\n    print('layer 3: {}'.format(output.size()))\n    output = layer4(output)\n    print(layer4)\n    print('layer 4: {}'.format(output.size()))", " "]}
{"filename": "model/backbone/resnet.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nBatchNorm = nn.BatchNorm2d\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152']\n", "           'resnet152']\n\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}", "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                            padding=1, bias=False)\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, deep_base=True):\n        super(ResNet, self).__init__()\n        self.deep_base = deep_base\n        if not self.deep_base:\n            self.inplanes = 64\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.bn1 = BatchNorm(64)\n            self.relu = nn.ReLU(inplace=True)\n        else:\n            self.inplanes = 128\n            self.conv1 = conv3x3(3, 64, stride=2)\n            self.bn1 = BatchNorm(64)\n            self.relu1 = nn.ReLU(inplace=True)\n            self.conv2 = conv3x3(64, 64)\n            self.bn2 = BatchNorm(64)\n            self.relu2 = nn.ReLU(inplace=True)\n            self.conv3 = conv3x3(64, 128)\n            self.bn3 = BatchNorm(128)\n            self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, BatchNorm):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                        kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        if self.deep_base:\n            x = self.relu2(self.bn2(self.conv2(x)))\n            x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x", "\n\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    return model", "\n\ndef resnet34(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n    return model", "\n\ndef resnet50(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n        model_path = './initmodel/resnet50_v2.pth'\n        model.load_state_dict(torch.load(model_path), strict=False)\n    return model", "\n\ndef resnet101(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n        model_path = './initmodel/resnet101_v2.pth'\n        model.load_state_dict(torch.load(model_path), strict=False)\n    return model", "\n\ndef resnet152(pretrained=False, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n        model_path = './initmodel/resnet152_v2.pth'\n        model.load_state_dict(torch.load(model_path), strict=False)\n    return model", ""]}
{"filename": "model/backbone/layer_extrator.py", "chunked_list": ["from turtle import forward\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import BatchNorm2d as BatchNorm        \n\nimport numpy as np\nimport random\nimport time\nimport cv2", "import time\nimport cv2\n\nimport model.backbone.resnet as models\nimport model.backbone.vgg as vgg_models\n\ndef get_vgg16_layer(model):\n    layer0_idx = range(0,7)\n    layer1_idx = range(7,14)\n    layer2_idx = range(14,24)\n    layer3_idx = range(24,34)\n    layer4_idx = range(34,43)\n    layers_0 = []\n    layers_1 = []\n    layers_2 = []\n    layers_3 = []\n    layers_4 = []\n    for idx in layer0_idx:\n        layers_0 += [model.features[idx]]\n    for idx in layer1_idx:\n        layers_1 += [model.features[idx]]\n    for idx in layer2_idx:\n        layers_2 += [model.features[idx]]\n    for idx in layer3_idx:\n        layers_3 += [model.features[idx]]\n    for idx in layer4_idx:\n        layers_4 += [model.features[idx]]  \n    layer0 = nn.Sequential(*layers_0) \n    layer1 = nn.Sequential(*layers_1) \n    layer2 = nn.Sequential(*layers_2) \n    layer3 = nn.Sequential(*layers_3) \n    layer4 = nn.Sequential(*layers_4)\n    return layer0,layer1,layer2,layer3,layer4", "\n\ndef layer_extrator(backbone = None, pretrained = True,):\n    \"\"\"\n    \n    \"\"\"\n    if backbone == 'vgg':\n        print('INFO: Using VGG_16 bn')\n        vgg_models.BatchNorm = BatchNorm\n        vgg16 = vgg_models.vgg16_bn(pretrained=pretrained)\n        print(vgg16)\n        layer0, layer1, layer2, layer3, layer4 = get_vgg16_layer(vgg16)\n    else:\n        print('INFO: Using {}'.format(backbone))\n        if backbone == 'resnet50':\n            resnet = models.resnet50(pretrained=pretrained)\n        elif backbone == 'resnet101':\n            resnet = models.resnet101(pretrained=pretrained)\n        else:\n            resnet = models.resnet152(pretrained=pretrained)\n        layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu1, resnet.conv2, resnet.bn2, resnet.relu2, resnet.conv3, resnet.bn3, resnet.relu3, resnet.maxpool)\n        layer1, layer2, layer3, layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n\n        for n, m in layer3.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n        for n, m in layer4.named_modules():\n            if 'conv2' in n:\n                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n            elif 'downsample.0' in n:\n                m.stride = (1, 1)\n\n    return layer0, layer1, layer2, layer3, layer4"]}
{"filename": "dataset/base_data.py", "chunked_list": ["from lib2to3.pgen2.token import N_TOKENS\nimport os\nimport os.path as osp\nimport cv2\nimport numpy as np\nimport copy\n\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\nimport torch", "import torch.nn.functional as F\nimport torch\nimport random\nfrom PIL import Image\n\n\nfrom util.util import  make_dict, gen_list\nfrom util.get_transform import get_transform\nfrom util.get_weak_anns import transform_anns\n", "from util.get_weak_anns import transform_anns\n\n\ndef label_trans(label, target_cls, mode):\n    label_class = np.unique(label).tolist()\n    if 0 in label_class:\n        label_class.remove(0)\n\n    target_pix = np.where(label == target_cls)\n    new_label = np.zeros_like(label)\n    new_label[target_pix[0],target_pix[1]] = 1 \n\n    if mode == 'ignore':\n        for cls in label_class:\n            if cls != target_cls:\n                ignore_pix = np.where(label == cls)\n                new_label[ignore_pix[0],ignore_pix[1]] = 255\n    elif mode == 'To_zero':\n        ignore_pix = np.where(label == 255)\n        new_label[ignore_pix[0],ignore_pix[1]] = 255\n\n    return new_label", "\nclass Few_Data(Dataset):\n\n    class_id = None\n    all_class = None\n    val_class = None\n\n    data_root = None\n    val_list =None\n    train_list =None\n\n    def __init__(self, split=0, shot=1, dataset=None, mode='train', ann_type='mask', transform_dict=None, ori_resize=False):\n\n        assert mode in ['train', 'val', 'demo']\n\n        self.mode = mode\n        self.shot = shot\n        self.ann_type = ann_type\n\n        self.sample_mode = transform_dict.pop('sample_mode')\n        self.fliter_mode = transform_dict.pop('fliter_mode')\n\n        if self.mode == 'train':\n            self.list = list(set(self.all_class) - set(self.val_class[split]))\n        else:\n            self.list = self.val_class[split]\n        \n        if self.mode == 'demo':\n            dict_name = './lists/{}/train_dict.txt'.format(dataset)\n            self.sample_mode = 'class'\n        else:\n            dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n        if not os.path.exists(dict_name):\n            make_dict(data_root=self.data_root, data_list=eval('self.{}_list'.format(self.mode)), \\\n                        all_class= self.all_class, dataset=dataset, mode=self.mode)\n        \n        self.data_list, self.sub_class_file_list = gen_list(dict_name, self.list, fliter=self.fliter_mode)\n\n        self.transform_dict = transform_dict\n        self.AUG = get_transform(transform_dict)\n\n    def transform(self, image, label):\n        if self.transform_dict['type'] == 'albumentations':\n            aug = self.AUG(image=image, mask=label)\n            return aug['image'], aug['mask']\n        else:\n            image, label = self.AUG(image=image, label=label)\n            return image, label\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, index):\n        label_class = []\n        if self.sample_mode == 'rand':\n            image_path, label_path = self.data_list[index]\n        elif self.sample_mode == 'class':\n            tmp_class = self.list[random.randint(1, len(self.list) ) -1]\n            file_all = list(set(self.sub_class_file_list[tmp_class]) & set(self.data_list))\n            image_path, label_path = file_all[random.randint(1,len(file_all))-1]\n\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR) \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n        image = np.float32(image)\n        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n\n        label_class = np.unique(label).tolist()\n        if 0 in label_class:\n            label_class.remove(0)\n        if 255 in label_class:\n            label_class.remove(255) \n\n        new_label_class = []       \n        for c in label_class:\n            if c in self.list:\n                new_label_class.append(c)\n\n        label_class = new_label_class    \n        assert len(label_class) > 0\n\n        # supp_query dataset\n        class_chosen = label_class[random.randint(1,len(label_class))-1]\n\n        label = label_trans(label, class_chosen, mode='To_zero')\n\n        file_class_chosen = self.sub_class_file_list[class_chosen]\n        num_file = len(file_class_chosen)\n\n        \n        \n        if self.mode == 'demo':\n            s_x_list = []\n            s_y_list = []\n            s_ori_x_list = []\n            s_ori_y_list = []\n            \n            raw_image = image.copy()\n            raw_label = label.copy()\n            if self.transform is not None:\n                image, label = self.transform(image, label)\n\n            for i in range(10):\n\n                support_image_path_list = []\n                support_label_path_list = []\n                support_idx_list = []\n                for k in range(self.shot):\n                    support_idx = random.randint(1,num_file)-1\n                    support_image_path = image_path\n                    support_label_path = label_path\n                    while((support_image_path == image_path and support_label_path == label_path) or support_idx in support_idx_list):\n                        support_idx = random.randint(1,num_file)-1\n                        support_image_path, support_label_path = file_class_chosen[support_idx]                \n                    support_idx_list.append(support_idx)\n                    support_image_path_list.append(support_image_path)\n                    support_label_path_list.append(support_label_path)\n\n                support_image_list_ori = []\n                support_label_list_ori = [] \n                support_label_list_ori_mask = []\n\n\n                subcls_list = []\n                for k in range(self.shot):\n                    subcls_list.append(self.list.index(class_chosen))\n                    support_image_path = support_image_path_list[k]\n                    support_label_path = support_label_path_list[k] \n                    support_image = cv2.imread(support_image_path, cv2.IMREAD_COLOR)      \n                    support_image = cv2.cvtColor(support_image, cv2.COLOR_BGR2RGB)\n                    support_image = np.float32(support_image)\n                    support_label = cv2.imread(support_label_path, cv2.IMREAD_GRAYSCALE)\n                    target_pix = np.where(support_label == class_chosen)\n                    ignore_pix = np.where(support_label == 255)\n                    support_label[:,:] = 0\n                    support_label[target_pix[0],target_pix[1]] = 1 \n                    \n                    support_label, support_label_mask = transform_anns(support_label, self.ann_type)\n                    support_label[ignore_pix[0],ignore_pix[1]] = 255\n                    support_label_mask[ignore_pix[0],ignore_pix[1]] = 255\n\n                    support_image_list_ori.append(support_image)\n                    support_label_list_ori.append(support_label)\n                    support_label_list_ori_mask.append(support_label_mask)\n                assert len(support_label_list_ori) == self.shot and len(support_image_list_ori) == self.shot    \n\n                support_image_list = [[] for _ in range(self.shot)]\n                support_label_list = [[] for _ in range(self.shot)]\n\n                if self.transform is not None:\n                    for k in range(self.shot):\n                        support_image_list[k], support_label_list[k] = self.transform(support_image_list_ori[k], support_label_list_ori[k])\n            \n                s_xs = support_image_list\n                s_ys = support_label_list\n                \n                s_x = s_xs[0].unsqueeze(0)\n                for i in range(1, self.shot):\n                    s_x = torch.cat([s_xs[i].unsqueeze(0), s_x], 0)\n                s_y = s_ys[0].unsqueeze(0)\n                for i in range(1, self.shot):\n                    s_y = torch.cat([s_ys[i].unsqueeze(0), s_y], 0)\n\n                s_x_list.append(s_x)\n                s_y_list.append(s_y)\n                s_ori_x_list.append(support_image_list_ori)\n                s_ori_y_list.append(support_label_list_ori)\n        else:\n            support_image_path_list = []\n            support_label_path_list = []\n            support_idx_list = []\n            for k in range(self.shot):\n                support_idx = random.randint(1,num_file)-1\n                support_image_path = image_path\n                support_label_path = label_path\n                while((support_image_path == image_path and support_label_path == label_path) or support_idx in support_idx_list):  # \u4fdd\u8bc1\u4e0equery\u4e0d\u540c\n                    support_idx = random.randint(1,num_file)-1\n                    support_image_path, support_label_path = file_class_chosen[support_idx]                \n                support_idx_list.append(support_idx)\n                support_image_path_list.append(support_image_path)\n                support_label_path_list.append(support_label_path)\n\n            support_image_list_ori = []\n            support_label_list_ori = []\n            support_label_list_ori_mask = []\n\n            subcls_list = []\n            for k in range(self.shot):\n\n                subcls_list.append(self.list.index(class_chosen))\n                support_image_path = support_image_path_list[k]\n                support_label_path = support_label_path_list[k] \n                support_image = cv2.imread(support_image_path, cv2.IMREAD_COLOR)      \n                support_image = cv2.cvtColor(support_image, cv2.COLOR_BGR2RGB)\n                support_image = np.float32(support_image)\n\n                support_label = cv2.imread(support_label_path, cv2.IMREAD_GRAYSCALE)\n                support_label = label_trans(support_label, class_chosen, mode='To_zero')\n\n                support_label, support_label_mask = transform_anns(support_label, self.ann_type)\n                support_label_mask = label_trans(support_label_mask, class_chosen, mode='To_zero')\n\n                if support_image.shape[0] != support_label.shape[0] or support_image.shape[1] != support_label.shape[1]:\n                    raise (RuntimeError(\"Support Image & label shape mismatch: \" + support_image_path + \" \" + support_label_path + \"\\n\"))            \n                support_image_list_ori.append(support_image)\n                support_label_list_ori.append(support_label)\n                support_label_list_ori_mask.append(support_label_mask)\n            assert len(support_label_list_ori) == self.shot and len(support_image_list_ori) == self.shot    \n\n            raw_image = image.copy()\n            raw_label = label.copy()\n            support_image_list = [[] for _ in range(self.shot)]\n            support_label_list = [[] for _ in range(self.shot)]\n\n            image, label = self.transform(image, label)\n            for k in range(self.shot):\n                support_image_list[k], support_label_list[k] = self.transform(support_image_list_ori[k], support_label_list_ori[k])\n        \n            s_xs = support_image_list\n            s_ys = support_label_list\n            \n            s_x = s_xs[0].unsqueeze(0)\n            for i in range(1, self.shot):\n                s_x = torch.cat([s_xs[i].unsqueeze(0), s_x], 0)\n            s_y = s_ys[0].unsqueeze(0)\n            for i in range(1, self.shot):\n                s_y = torch.cat([s_ys[i].unsqueeze(0), s_y], 0)\n\n            total_image_list = support_image_list_ori.copy()\n            total_image_list.append(raw_image)\n\n        # Return\n        if self.mode == 'train':\n            return image, label, s_x, s_y, subcls_list\n        elif self.mode == 'val':\n            return image, label, s_x, s_y, subcls_list, raw_label\n        elif self.mode == 'demo':\n            return image, label, s_x_list, s_y_list, subcls_list, s_ori_x_list, s_ori_y_list, raw_image, raw_label", "        \nclass Base_Data(Dataset):\n\n    class_id = None\n    all_class = None\n    val_class = None\n\n    data_root = None\n    val_list =None\n    train_list =None\n\n    def __init__(self, split=0,  data_root=None, dataset=None, mode='train', transform_dict=None):\n\n        assert mode in ['train', 'val']\n\n        self.mode = mode\n        \n        self.transform_dict = transform_dict\n        self.AUG = get_transform(transform_dict)\n\n        if split == -1 :\n            self.list = self.all_class\n        else:\n            self.list = list(set(self.all_class) - set(self.val_class[split]))\n\n        dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n        if not os.path.exists(dict_name):\n            make_dict(data_root=self.data_root, data_list=eval('self.{}_list'.format(self.mode)), \\\n                        all_class= self.all_class, dataset=dataset, mode=self.mode)\n\n        self.data_list, _ = gen_list(dict_name, self.list, fliter=False)\n\n        # if split == -1 :\n        #     self.list = self.all_class\n        #     list_path = './lists/{}/{}.txt'.format(dataset, self.mode)\n        #     with open(list_path, 'r') as f:\n        #         f_str = f.readlines()\n        #     self.data_list = []\n        #     for line in f_str:\n        #         img, mask = line.split(' ')\n        #         img = '../data/{}/'.format(dataset) + img\n        #         mask = '../data/{}/'.format(dataset) + mask\n        #         self.data_list.append((img, mask.strip()))\n        # else:\n        #     self.list = list(set(self.all_class) - set(self.val_class[split]))\n        #     list_root = './lists/{}/fss_list/{}/'.format(dataset, self.mode)\n        #     # dict_path = list_root + '{}_dict.txt'.format(self.mode)\n\n        #     if self.mode == 'train':\n        #         list_path = list_root + 'train_split{}.txt'.format(split)\n        #     elif self.mode == 'val':\n        #         list_path = list_root + 'val_base{}.txt'.format(split)\n\n        #     with open(list_path, 'r') as f:\n        #         f_str = f.readlines()\n        #     self.data_list = []\n        #     for line in f_str:\n        #         img, mask = line.split(' ')\n        #         self.data_list.append((img, mask.strip()))\n            \n\n    def transform(self, image, label):\n        if self.transform_dict['type'] == 'albumentations':\n            aug = self.AUG(image=image, mask=label)\n            return aug['image'], aug['mask']\n        else:\n            image, label = self.AUG(image=image, label=label)\n            return image, label\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, index):\n        image_path, label_path = self.data_list[index]\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR) \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n        image = np.float32(image)\n        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n        label_tmp = label.copy()\n\n        label_class = np.unique(label).tolist()\n        if 0 in label_class:\n            label_class.remove(0)   \n        if 255 in label_class:\n            label_class.remove(255) \n\n        for cls in label_class:\n            select_pix = np.where(label_tmp == cls)\n            if cls in self.list:\n                label[select_pix[0],select_pix[1]] = self.list.index(cls) + 1\n            else:\n                label[select_pix[0],select_pix[1]] = 0\n                \n        raw_label = label.copy()\n\n        image, label = self.transform(image, label)\n\n        # Return\n        if self.mode == 'val':\n            # return image, label, raw_label\n            return image, label\n        else:\n            return image, label"]}
{"filename": "dataset/iSAID.py", "chunked_list": ["from dataset.base_data import Few_Data, Base_Data\n\n\nclass iSAID_few_dataset(Few_Data):\n\n    class_id = {\n                0: 'unlabeled',\n                1: 'ship',\n                2: 'storage_tank',\n                3: 'baseball_diamond',  \n                4: 'tennis_court',\n                5: 'basketball_court',\n                6: 'Ground_Track_Field',\n                7: 'Bridge',\n                8: 'Large_Vehicle',\n                9: 'Small_Vehicle',\n                10: 'Helicopter',\n                11: 'Swimming_pool',\n                12: 'Roundabout',\n                13: 'Soccer_ball_field',\n                14: 'plane',\n                15: 'Harbor'\n                    }\n    \n    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n    \n    all_class = list(range(1, 16))\n    val_class = [list(range(1, 6)), list(range(6, 11)), list(range(11, 16))]\n\n    data_root = '../data/iSAID'\n    train_list ='./lists/iSAID/train.txt'\n    val_list ='./lists/iSAID/val.txt'\n\n    def __init__(self, split=0, shot=1, dataset='iSAID', mode='train', ann_type='mask', transform_dict=None):\n        super().__init__(split, shot, dataset, mode, ann_type, transform_dict)", "\n\nclass iSAID_base_dataset(Base_Data):\n    class_id = {\n                0: 'unlabeled',\n                1: 'ship',\n                2: 'storage_tank',\n                3: 'baseball_diamond',\n                4: 'tennis_court',\n                5: 'basketball_court',\n                6: 'Ground_Track_Field',\n                7: 'Bridge',\n                8: 'Large_Vehicle',\n                9: 'Small_Vehicle',\n                10: 'Helicopter',\n                11: 'Swimming_pool',\n                12: 'Roundabout',\n                13: 'Soccer_ball_field',\n                14: 'plane',\n                15: 'Harbor'\n                    }\n    \n    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n    \n    all_class = list(range(1, 16))\n    val_class = [list(range(1, 6)), list(range(6, 11)), list(range(11, 16))]\n\n    data_root = '../data/iSAID'\n    train_list ='./lists/iSAID/train.txt'\n    val_list ='./lists/iSAID/val.txt'\n\n    def __init__(self, split=0, shot=1, data_root=None, dataset='iSAID', mode='train', transform_dict=None):\n        super().__init__(split,  data_root, dataset, mode, transform_dict)"]}
{"filename": "dataset/iSAID_1.py", "chunked_list": ["from dataset.base_data import Few_Data, Base_Data\n\n\nclass iSAID_1_few_dataset(Few_Data):\n\n    class_id = {\n                0: 'unlabeled',\n                1: 'ship',\n                2: 'storage_tank',\n                3: 'baseball_diamond',  \n                4: 'tennis_court',\n                5: 'basketball_court',\n                6: 'Ground_Track_Field',\n                7: 'Bridge',\n                8: 'Large_Vehicle',\n                9: 'Small_Vehicle',\n                10: 'Helicopter',\n                11: 'Swimming_pool',\n                12: 'Roundabout',\n                13: 'Soccer_ball_field',\n                14: 'plane',\n                15: 'Harbor'\n                    }\n    \n    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n    \n    all_class = list(range(1, 16))\n    val_class = [list(range(1, 16, 3)), list(range(2, 16, 3)), list(range(3, 16, 3))]\n\n    data_root = '../data/iSAID'\n    train_list ='./lists/iSAID/train.txt'\n    val_list ='./lists/iSAID/val.txt'\n\n    def __init__(self, split=0, shot=1, dataset='iSAID', mode='train', ann_type='mask', transform_dict=None):\n        super().__init__(split, shot, dataset, mode, ann_type, transform_dict)", "\n\nclass iSAID_1_base_dataset(Base_Data):\n    class_id = {\n                0: 'unlabeled',\n                1: 'ship',\n                2: 'storage_tank',\n                3: 'baseball_diamond',\n                4: 'tennis_court',\n                5: 'basketball_court',\n                6: 'Ground_Track_Field',\n                7: 'Bridge',\n                8: 'Large_Vehicle',\n                9: 'Small_Vehicle',\n                10: 'Helicopter',\n                11: 'Swimming_pool',\n                12: 'Roundabout',\n                13: 'Soccer_ball_field',\n                14: 'plane',\n                15: 'Harbor'\n                    }\n    \n    PALETTE = [[0, 0, 0], [0, 0, 63], [0, 63, 63], [0, 63, 0], [0, 63, 127],\n               [0, 63, 191], [0, 63, 255], [0, 127, 63], [0, 127, 127],\n               [0, 0, 127], [0, 0, 191], [0, 0, 255], [0, 191, 127],\n               [0, 127, 191], [0, 127, 255], [0, 100, 155]]\n    \n    all_class = list(range(1, 16))\n    val_class = [list(range(1, 16, 3)), list(range(2, 16, 3)), list(range(3, 16, 3))]\n\n    data_root = '../data/iSAID'\n    train_list ='./lists/iSAID/train.txt'\n    val_list ='./lists/iSAID/val.txt'\n\n    def __init__(self, split=0, shot=1, data_root=None, dataset='iSAID', mode='train', transform_dict=None):\n        super().__init__(split,  data_root, dataset, mode, transform_dict)"]}
{"filename": "util/config.py", "chunked_list": ["# -----------------------------------------------------------------------------\n# Functions for parsing args\n# -----------------------------------------------------------------------------\nimport yaml\nimport os\nfrom ast import literal_eval\nimport copy\n\n\nclass CfgNode(dict):\n    \"\"\"\n    CfgNode represents an internal node in the configuration tree. It's a simple\n    dict-like container that allows for attribute-based access to keys.\n    \"\"\"\n\n    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n        # Recursively convert nested dictionaries in init_dict into CfgNodes\n        init_dict = {} if init_dict is None else init_dict\n        key_list = [] if key_list is None else key_list\n        for k, v in init_dict.items():\n            if type(v) is dict:\n                # Convert dict to CfgNode\n                init_dict[k] = CfgNode(v, key_list=key_list + [k])\n        super(CfgNode, self).__init__(init_dict)\n\n    def __getattr__(self, name):\n        if name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        self[name] = value\n\n    def __str__(self):\n        def _indent(s_, num_spaces):\n            s = s_.split(\"\\n\")\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(num_spaces * \" \") + line for line in s]\n            s = \"\\n\".join(s)\n            s = first + \"\\n\" + s\n            return s\n\n        r = \"\"\n        s = []\n        for k, v in sorted(self.items()):\n            seperator = \"\\n\" if isinstance(v, CfgNode) else \" \"\n            attr_str = \"{}:{}{}\".format(str(k), seperator, str(v))\n            attr_str = _indent(attr_str, 2)\n            s.append(attr_str)\n        r += \"\\n\".join(s)\n        return r\n\n    def __repr__(self):                # print\n        return \"{}({})\".format(self.__class__.__name__, super(CfgNode, self).__repr__())", "\nclass CfgNode(dict):\n    \"\"\"\n    CfgNode represents an internal node in the configuration tree. It's a simple\n    dict-like container that allows for attribute-based access to keys.\n    \"\"\"\n\n    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n        # Recursively convert nested dictionaries in init_dict into CfgNodes\n        init_dict = {} if init_dict is None else init_dict\n        key_list = [] if key_list is None else key_list\n        for k, v in init_dict.items():\n            if type(v) is dict:\n                # Convert dict to CfgNode\n                init_dict[k] = CfgNode(v, key_list=key_list + [k])\n        super(CfgNode, self).__init__(init_dict)\n\n    def __getattr__(self, name):\n        if name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        self[name] = value\n\n    def __str__(self):\n        def _indent(s_, num_spaces):\n            s = s_.split(\"\\n\")\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(num_spaces * \" \") + line for line in s]\n            s = \"\\n\".join(s)\n            s = first + \"\\n\" + s\n            return s\n\n        r = \"\"\n        s = []\n        for k, v in sorted(self.items()):\n            seperator = \"\\n\" if isinstance(v, CfgNode) else \" \"\n            attr_str = \"{}:{}{}\".format(str(k), seperator, str(v))\n            attr_str = _indent(attr_str, 2)\n            s.append(attr_str)\n        r += \"\\n\".join(s)\n        return r\n\n    def __repr__(self):                # print\n        return \"{}({})\".format(self.__class__.__name__, super(CfgNode, self).__repr__())", "\n\ndef load_cfg_from_cfg_file(file_list):\n    cfg = {}\n    for file in file_list:\n        assert os.path.isfile(file) and file.endswith('.yaml'), \\\n            '{} is not a yaml file'.format(file)\n\n        with open(file, 'r') as f:\n            cfg_from_file = yaml.safe_load(f)\n        \n        for key in cfg_from_file:\n            cfg[key] = cfg_from_file[key]\n    # for key in cfg_from_file:\n    #     for k, v in cfg_from_file[key].items():\n    #         cfg[k] = v\n    \n    cfg = CfgNode(cfg)\n    return cfg", "\ndef merge_cfg_from_args(cfg, args):\n    args_dict = args.__dict__\n    for k ,v in args_dict.items():\n        if not k == 'config' or k == 'opts':\n            cfg[k] = v\n            \n    return cfg\n\ndef merge_cfg_from_list(cfg, cfg_list):\n    new_cfg = copy.deepcopy(cfg)\n    assert len(cfg_list) % 2 == 0\n    for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        subkey = full_key.split('.')[-1]\n        assert subkey in cfg, 'Non-existent key: {}'.format(full_key)\n        value = _decode_cfg_value(v)\n        value = _check_and_coerce_cfg_value_type(\n            value, cfg[subkey], subkey, full_key\n        )\n        setattr(new_cfg, subkey, value)\n\n    return new_cfg", "\ndef merge_cfg_from_list(cfg, cfg_list):\n    new_cfg = copy.deepcopy(cfg)\n    assert len(cfg_list) % 2 == 0\n    for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        subkey = full_key.split('.')[-1]\n        assert subkey in cfg, 'Non-existent key: {}'.format(full_key)\n        value = _decode_cfg_value(v)\n        value = _check_and_coerce_cfg_value_type(\n            value, cfg[subkey], subkey, full_key\n        )\n        setattr(new_cfg, subkey, value)\n\n    return new_cfg", "\n\n\ndef _decode_cfg_value(v):\n    \"\"\"Decodes a raw config value (e.g., from a yaml config files or command\n    line argument) into a Python object.\n    \"\"\"\n    # All remaining processing is only applied to strings\n    if not isinstance(v, str):\n        return v\n    # Try to interpret `v` as a:\n    #   string, number, tuple, list, dict, boolean, or None\n    try:\n        v = literal_eval(v)\n    # The following two excepts allow v to pass through when it represents a\n    # string.\n    #\n    # Longer explanation:\n    # The type of v is always a string (before calling literal_eval), but\n    # sometimes it *represents* a string and other times a data structure, like\n    # a list. In the case that v represents a string, what we got back from the\n    # yaml parser is 'foo' *without quotes* (so, not '\"foo\"'). literal_eval is\n    # ok with '\"foo\"', but will raise a ValueError if given 'foo'. In other\n    # cases, like paths (v = 'foo/bar' and not v = '\"foo/bar\"'), literal_eval\n    # will raise a SyntaxError.\n    except ValueError:\n        pass\n    except SyntaxError:\n        pass\n    return v", "\n\ndef _check_and_coerce_cfg_value_type(replacement, original, key, full_key):\n    \"\"\"Checks that `replacement`, which is intended to replace `original` is of\n    the right type. The type is correct if it matches exactly or is one of a few\n    cases in which the type can be easily coerced.\n    \"\"\"\n    original_type = type(original)\n    replacement_type = type(replacement)\n\n    # The types must match (with some exceptions)\n    if replacement_type == original_type:\n        return replacement\n\n    # Cast replacement from from_type to to_type if the replacement and original\n    # types match from_type and to_type\n    def conditional_cast(from_type, to_type):\n        if replacement_type == from_type and original_type == to_type:\n            return True, to_type(replacement)\n        else:\n            return False, None\n\n    # Conditionally casts\n    # list <-> tuple\n    casts = [(tuple, list), (list, tuple)]\n    # For py2: allow converting from str (bytes) to a unicode string\n    try:\n        casts.append((str, unicode))  # noqa: F821\n    except Exception:\n        pass\n\n    for (from_type, to_type) in casts:\n        converted, converted_value = conditional_cast(from_type, to_type)\n        if converted:\n            return converted_value\n\n    raise ValueError(\n        \"Type mismatch ({} vs. {}) with values ({} vs. {}) for config \"\n        \"key: {}\".format(\n            original_type, replacement_type, original, replacement, full_key\n        )\n    )", "\n\n# def _assert_with_logging(cond, msg):\n#     if not cond:\n#         logger.debug(msg)\n#     assert cond, msg\n\n"]}
{"filename": "util/get_weak_anns.py", "chunked_list": ["from __future__ import absolute_import, division\n\nimport networkx as nx\nimport numpy as np\nfrom scipy.ndimage import binary_dilation, binary_erosion, maximum_filter\nfrom scipy.special import comb\nfrom skimage.filters import rank\nfrom skimage.morphology import dilation, disk, erosion, medial_axis\nfrom sklearn.neighbors import radius_neighbors_graph\nimport cv2", "from sklearn.neighbors import radius_neighbors_graph\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom scipy import ndimage\n\ndef bezier_curve(points, nb_points=1000):\n    \"\"\" Given a list of points compute a bezier curve from it.\n    # Arguments\n        points: ndarray. Array of points with shape (N, 2) with N being the\n            number of points and the second dimension representing the\n            (x, y) coordinates.\n        nb_points: Integer. Number of points to sample from the bezier curve.\n            This value must be larger than the number of points given in\n            `points`. Maximum value 10000.\n    # Returns\n        ndarray: Array of shape (1000, 2) with the bezier curve of the\n            given path of points.\n    \"\"\"\n    nb_points = min(nb_points, 1000)\n\n    points = np.asarray(points, dtype=np.float)\n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\n            '`points` should be two dimensional and have shape: (N, 2)')\n\n    n_points = len(points)\n    if n_points > nb_points:\n        # We are downsampling points\n        return points\n\n    t = np.linspace(0., 1., nb_points).reshape(1, -1)\n\n    # Compute the Bernstein polynomial of n, i as a function of t\n    i = np.arange(n_points).reshape(-1, 1)\n    n = n_points - 1\n    polynomial_array = comb(n, i) * (t**(n - i)) * (1 - t)**i\n\n    bezier_curve_points = polynomial_array.T.dot(points)\n\n    return bezier_curve_points", "\n\ndef bresenham(points):\n    \"\"\" Apply Bresenham algorithm for a list points.\n    More info: https://en.wikipedia.org/wiki/Bresenham's_line_algorithm\n    # Arguments\n        points: ndarray. Array of points with shape (N, 2) with N being the number\n            if points and the second coordinate representing the (x, y)\n            coordinates.\n    # Returns\n        ndarray: Array of points after having applied the bresenham algorithm.\n    \"\"\"\n\n    points = np.asarray(points, dtype=np.int)\n\n    def line(x0, y0, x1, y1):\n        \"\"\" Bresenham line algorithm.\n        \"\"\"\n        d_x = x1 - x0\n        d_y = y1 - y0\n\n        x_sign = 1 if d_x > 0 else -1\n        y_sign = 1 if d_y > 0 else -1\n\n        d_x = np.abs(d_x)\n        d_y = np.abs(d_y)\n\n        if d_x > d_y:\n            xx, xy, yx, yy = x_sign, 0, 0, y_sign\n        else:\n            d_x, d_y = d_y, d_x\n            xx, xy, yx, yy = 0, y_sign, x_sign, 0\n\n        D = 2 * d_y - d_x\n        y = 0\n\n        line = np.empty((d_x + 1, 2), dtype=points.dtype)\n        for x in range(d_x + 1):\n            line[x] = [x0 + x * xx + y * yx, y0 + x * xy + y * yy]\n            if D >= 0:\n                y += 1\n                D -= 2 * d_x\n            D += 2 * d_y\n\n        return line\n\n    nb_points = len(points)\n    if nb_points < 2:\n        return points\n\n    new_points = []\n\n    for i in range(nb_points - 1):\n        p = points[i:i + 2].ravel().tolist()\n        new_points.append(line(*p))\n\n    new_points = np.concatenate(new_points, axis=0)\n\n    return new_points", "\n\ndef scribbles2mask(scribbles,\n                   output_resolution,\n                   bezier_curve_sampling=False,\n                   nb_points=1000,\n                   compute_bresenham=True,\n                   default_value=0):\n    \"\"\" Convert the scribbles data into a mask.\n    # Arguments\n        scribbles: Dictionary. Scribbles in the default format.\n        output_resolution: Tuple. Output resolution (H, W).\n        bezier_curve_sampling: Boolean. Weather to sample first the returned\n            scribbles using bezier curve or not.\n        nb_points: Integer. If `bezier_curve_sampling` is `True` set the number\n            of points to sample from the bezier curve.\n        compute_bresenham: Boolean. Whether to compute bresenham algorithm for the\n            scribbles lines.\n        default_value: Integer. Default value for the pixels which do not belong\n            to any scribble.\n    # Returns\n        ndarray: Array with the mask of the scribbles with the index of the\n            object ids. The shape of the returned array is (B x H x W) by\n            default or (H x W) if `only_annotated_frame==True`.\n    \"\"\"\n    if len(output_resolution) != 2:\n        raise ValueError(\n            'Invalid output resolution: {}'.format(output_resolution))\n    for r in output_resolution:\n        if r < 1:\n            raise ValueError(\n                'Invalid output resolution: {}'.format(output_resolution))\n\n    size_array = np.asarray(output_resolution[::-1], dtype=np.float) - 1\n    m = np.full(output_resolution, default_value, dtype=np.int)\n    \n    for p in scribbles:\n        p /= output_resolution[::-1]\n        path = p.tolist()\n        path = np.asarray(path, dtype=np.float)\n        if bezier_curve_sampling:\n            path = bezier_curve(path, nb_points=nb_points)\n        path *= size_array\n        path = path.astype(np.int)\n\n        if compute_bresenham:\n            path = bresenham(path)\n        m[path[:, 1], path[:, 0]] = 1\n\n    return m", "\nclass ScribblesRobot(object):\n    \"\"\"Robot that generates realistic scribbles simulating human interaction.\n    \n    # Attributes:\n        kernel_size: Float. Fraction of the square root of the area used\n            to compute the dilation and erosion before computing the\n            skeleton of the error masks.\n        max_kernel_radius: Float. Maximum kernel radius when applying\n            dilation and erosion. Default 16 pixels.\n        min_nb_nodes: Integer. Number of nodes necessary to keep a connected\n            graph and convert it into a scribble.\n        nb_points: Integer. Number of points to sample the bezier curve\n            when converting the final paths into curves.\n\n    Reference:\n    [1] Sergi et al., \"The 2018 DAVIS Challenge on Video Object Segmentation\", arxiv 2018\n    [2] Jordi et al., \"The 2017 DAVIS Challenge on Video Object Segmentation\", arxiv 2017\n    \n    \"\"\"\n    def __init__(self,\n                 kernel_size=.15,\n                 max_kernel_radius=16,\n                 min_nb_nodes=4,\n                 nb_points=1000):\n        if kernel_size >= 1. or kernel_size < 0:\n            raise ValueError('kernel_size must be a value between [0, 1).')\n\n        self.kernel_size = kernel_size\n        self.max_kernel_radius = max_kernel_radius\n        self.min_nb_nodes = min_nb_nodes\n        self.nb_points = nb_points\n\n    def _generate_scribble_mask(self, mask):\n        \"\"\" Generate the skeleton from a mask\n        Given an error mask, the medial axis is computed to obtain the\n        skeleton of the objects. In order to obtain smoother skeleton and\n        remove small objects, an erosion and dilation operations are performed.\n        The kernel size used is proportional the squared of the area.\n        # Arguments\n            mask: Numpy Array. Error mask\n        Returns:\n            skel: Numpy Array. Skeleton mask\n        \"\"\"\n        mask = np.asarray(mask, dtype=np.uint8)\n        side = np.sqrt(np.sum(mask > 0))\n\n        mask_ = mask\n        # kernel_size = int(self.kernel_size * side)\n        kernel_radius = self.kernel_size * side * .5\n        kernel_radius = min(kernel_radius, self.max_kernel_radius)\n        # logging.verbose(\n        #     'Erosion and dilation with kernel radius: {:.1f}'.format(\n        #         kernel_radius), 2)\n        compute = True\n        while kernel_radius > 1. and compute:\n            kernel = disk(kernel_radius)\n            mask_ = rank.minimum(mask.copy(), kernel)\n            mask_ = rank.maximum(mask_, kernel)\n            compute = False\n            if mask_.astype(np.bool).sum() == 0:\n                compute = True\n                prev_kernel_radius = kernel_radius\n                kernel_radius *= .9\n                # logging.verbose('Reducing kernel radius from {:.1f} '.format(\n                #     prev_kernel_radius) +\n                #                 'pixels to {:.1f}'.format(kernel_radius), 1)\n\n        mask_ = np.pad(\n            mask_, ((1, 1), (1, 1)), mode='constant', constant_values=False)\n        skel = medial_axis(mask_.astype(np.bool))\n        skel = skel[1:-1, 1:-1]\n        return skel\n\n    def _mask2graph(self, skeleton_mask):\n        \"\"\" Transforms a skeleton mask into a graph\n        Args:\n            skeleton_mask (ndarray): Skeleton mask\n        Returns:\n            tuple(nx.Graph, ndarray): Returns a tuple where the first element\n                is a Graph and the second element is an array of xy coordinates\n                indicating the coordinates for each Graph node.\n                If an empty mask is given, None is returned.\n        \"\"\"\n        mask = np.asarray(skeleton_mask, dtype=np.bool)\n        if np.sum(mask) == 0:\n            return None\n\n        h, w = mask.shape\n        x, y = np.arange(w), np.arange(h)\n        X, Y = np.meshgrid(x, y)\n\n        X, Y = X.ravel(), Y.ravel()\n        M = mask.ravel()\n\n        X, Y = X[M], Y[M]\n        points = np.c_[X, Y]\n        G = radius_neighbors_graph(points, np.sqrt(2), mode='distance')\n        T = nx.from_scipy_sparse_matrix(G)\n\n        return T, points\n\n    def _acyclics_subgraphs(self, G):\n        \"\"\" Divide a graph into connected components subgraphs\n        Divide a graph into connected components subgraphs and remove its\n        cycles removing the edge with higher weight inside the cycle. Also\n        prune the graphs by number of nodes in case the graph has not enought\n        nodes.\n        Args:\n            G (nx.Graph): Graph\n        Returns:\n            list(nx.Graph): Returns a list of graphs which are subgraphs of G\n                with cycles removed.\n        \"\"\"\n        if not isinstance(G, nx.Graph):\n            raise TypeError('G must be a nx.Graph instance')\n        S = []  # List of subgraphs of G\n\n        for g in nx.connected_component_subgraphs(G):  # Make sure the version of package \"networkx\" < 2.4\n        # for g in (G.subgraph(c) for c in nx.connected_components(G)):\n\n            # Remove all cycles that we may find\n            has_cycles = True\n            while has_cycles:\n                try:\n                    cycle = nx.find_cycle(g)\n                    weights = np.asarray([G[u][v]['weight'] for u, v in cycle])\n                    idx = weights.argmax()\n                    # Remove the edge with highest weight at cycle\n                    g.remove_edge(*cycle[idx])\n                except nx.NetworkXNoCycle:\n                    has_cycles = False\n\n            if len(g) < self.min_nb_nodes:\n                # Prune small subgraphs\n                # logging.verbose('Remove a small line with {} nodes'.format(\n                #     len(g)), 1)\n                continue\n\n            S.append(g)\n\n        return S\n\n    def _longest_path_in_tree(self, G):\n        \"\"\" Given a tree graph, compute the longest path and return it\n        Given an undirected tree graph, compute the longest path and return it.\n        The approach use two shortest path transversals (shortest path in a\n        tree is the same as longest path). This could be improve but would\n        require implement it:\n        https://cs.stackexchange.com/questions/11263/longest-path-in-an-undirected-tree-with-only-one-traversal\n        Args:\n            G (nx.Graph): Graph which should be an undirected tree graph\n        Returns:\n            list(int): Returns a list of indexes of the nodes belonging to the\n                longest path.\n        \"\"\"\n        if not isinstance(G, nx.Graph):\n            raise TypeError('G must be a nx.Graph instance')\n        if not nx.is_tree(G):\n            raise ValueError('Graph G must be a tree (graph without cycles)')\n\n        # Compute the furthest node to the random node v\n        v = list(G.nodes())[0]\n        distance = nx.single_source_shortest_path_length(G, v)\n        vp = max(distance.items(), key=lambda x: x[1])[0]\n        # From this furthest point v' find again the longest path from it\n        distance = nx.single_source_shortest_path(G, vp)\n        longest_path = max(distance.values(), key=len)\n        # Return the longest path\n\n        return list(longest_path)\n\n\n    def generate_scribbles(self, mask):\n        \"\"\"Given a binary mask, the robot will return a scribble in the region\"\"\"\n\n        # generate scribbles\n        skel_mask = self._generate_scribble_mask(mask)\n        G, P = self._mask2graph(skel_mask)\n        S = self._acyclics_subgraphs(G)\n        longest_paths_idx = [self._longest_path_in_tree(s) for s in S]\n        longest_paths = [P[idx] for idx in longest_paths_idx]\n        scribbles_paths = [\n                bezier_curve(p, self.nb_points) for p in longest_paths\n        ]\n\n        output_resolution = tuple([mask.shape[0], mask.shape[1]])\n        scribble_mask = scribbles2mask(scribbles_paths, output_resolution)\n\n        return scribble_mask", "\n\ndef find_bbox(mask):\n    _, labels, stats, centroids = cv2.connectedComponentsWithStats(mask.astype(np.uint8))\n    return stats[1:]  # remove bg stat\n\n\ndef transform_anns(mask, ann_type):\n    mask_ori = mask.copy()\n    if ann_type == 'scribble':\n        dilated_size = 20\n        Scribble_Expert = ScribblesRobot()\n        scribble_mask = Scribble_Expert.generate_scribbles(mask)\n        scribble_mask = ndimage.maximum_fliter(scribble_mask, size=dilated_size)\n        return scribble_mask.astype(np.uint8), mask_ori\n\n    elif ann_type == 'bbox':\n        bboxs = find_bbox(mask)\n        for j in bboxs: \n            cv2.rectangle(mask, (j[0], j[1]), (j[0] + j[2], j[1] + j[3]), 1, -1) # -1->fill; 2->draw_rec        \n        return mask, mask_ori\n    \n    elif ann_type == 'mask':\n        return mask, mask_ori", "\n\nif __name__ == '__main__':\n    label_path = '2008_001227.png'\n    Scribble_Expert = ScribblesRobot()\n    mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n\n    dilated_size = 20\n    scribble_mask = Scribble_Expert.generate_scribbles(mask)\n    scribble_mask = ndimage.maximum_fliter(scribble_mask, size=dilated_size) # \n    scribble_mask[scribble_mask==1] = 255\n    cv2.imwrite('scribble.png', scribble_mask)\n\n    bboxs = find_bbox(mask)\n    mask_color = cv2.imread(label_path, cv2.IMREAD_COLOR)\n    for j in bboxs: \n        cv2.rectangle(mask_color, (j[0], j[1]), (j[0] + j[2], j[1] + j[3]), (0,255,0), -1) # -1->fill; 2->draw_rec\n    cv2.imwrite('bbox.png', mask_color)\n\n    print('done')", ""]}
{"filename": "util/get_transform.py", "chunked_list": ["import random\nimport math\n\nimport numpy as np\nimport numbers\nimport collections\nimport cv2\n\nimport torch\nimport albumentations as Albu", "import torch\nimport albumentations as Albu\nfrom albumentations.pytorch import ToTensorV2\nimport util.transform as base\nfrom torchvision import transforms as pytorch\n\n\"\"\"\nalbumentations \n\n", "\n\n\"\"\"\n\ndef get_transform(transform_dict):\n    \"\"\"\n    a dict \n    \"\"\"\n    pip_line = []\n\n    if transform_dict['type'] == 'albumentations':\n        tmp = 'Albu.'\n    elif transform_dict['type'] == 'pytorch':\n        tmp = 'pytorch.'\n    else:\n        tmp = 'base.'\n    \n    for key in transform_dict:\n        if key != 'type':\n            if key == 'OneOf' or key == 'SomeOf':\n                tmp_pip_line = []\n                for item in transform_dict[key]['transforms']:\n                    tmp_pip_line.append(eval(tmp+ item.pop('type'))(**item))\n                pip_line.append(eval(tmp+ key)(transforms=tmp_pip_line, p=transform_dict[key]['p']))\n            elif key == 'ToTensorV2':\n                pip_line.append(eval(key)())\n            elif key == 'ToTensor' and tmp == 'pytorch.':\n                pip_line.append(eval(tmp+ key)())\n            else:\n                pip_line.append(eval(tmp+ key)(**transform_dict[key]))\n\n    transformer = eval(tmp + 'Compose')(pip_line)\n\n    return transformer", ""]}
{"filename": "util/transform.py", "chunked_list": ["import random\nimport math\nimport numpy as np\nimport numbers\nimport collections\nimport cv2\nimport time\nimport torch\n\nmanual_seed = 123", "\nmanual_seed = 123\ntorch.manual_seed(manual_seed)\nnp.random.seed(manual_seed)\ntorch.manual_seed(manual_seed)\ntorch.cuda.manual_seed_all(manual_seed)\nrandom.seed(manual_seed)\n\nclass Compose(object):\n    # Composes segtransforms: segtransform.Compose([segtransform.RandScale([0.5, 2.0]), segtransform.ToTensor()])\n    def __init__(self, segtransform):\n        self.segtransform = segtransform\n\n    def __call__(self, image, label):\n        for t in self.segtransform:\n            image, label = t(image, label)\n        return image, label", "class Compose(object):\n    # Composes segtransforms: segtransform.Compose([segtransform.RandScale([0.5, 2.0]), segtransform.ToTensor()])\n    def __init__(self, segtransform):\n        self.segtransform = segtransform\n\n    def __call__(self, image, label):\n        for t in self.segtransform:\n            image, label = t(image, label)\n        return image, label\n\nclass ToTensor(object):\n    def __init__(self, enabled = True):\n        self.enabled = enabled\n    # Converts numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n    def __call__(self, image, label):\n        if not isinstance(image, np.ndarray) or not isinstance(label, np.ndarray):\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n                                \"[eg: data readed by cv2.imread()].\\n\"))\n        if len(image.shape) > 3 or len(image.shape) < 2:\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray with 3 dims or 2 dims.\\n\"))\n        if len(image.shape) == 2:\n            image = np.expand_dims(image, axis=2)\n        if not len(label.shape) == 2:\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray labellabel with 2 dims.\\n\"))\n\n        image = torch.from_numpy(image.transpose((2, 0, 1)))\n        if not isinstance(image, torch.FloatTensor):\n            image = image.float()\n        label = torch.from_numpy(label)\n        if not isinstance(label, torch.LongTensor):\n            label = label.long()\n        return image, label", "\nclass ToTensor(object):\n    def __init__(self, enabled = True):\n        self.enabled = enabled\n    # Converts numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n    def __call__(self, image, label):\n        if not isinstance(image, np.ndarray) or not isinstance(label, np.ndarray):\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray\"\n                                \"[eg: data readed by cv2.imread()].\\n\"))\n        if len(image.shape) > 3 or len(image.shape) < 2:\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray with 3 dims or 2 dims.\\n\"))\n        if len(image.shape) == 2:\n            image = np.expand_dims(image, axis=2)\n        if not len(label.shape) == 2:\n            raise (RuntimeError(\"segtransform.ToTensor() only handle np.ndarray labellabel with 2 dims.\\n\"))\n\n        image = torch.from_numpy(image.transpose((2, 0, 1)))\n        if not isinstance(image, torch.FloatTensor):\n            image = image.float()\n        label = torch.from_numpy(label)\n        if not isinstance(label, torch.LongTensor):\n            label = label.long()\n        return image, label", "\nclass ToNumpy(object):\n    # Converts torch.FloatTensor of shape (C x H x W) to a numpy.ndarray (H x W x C).\n    def __call__(self, image, label):\n        if not isinstance(image, torch.Tensor) or not isinstance(label, torch.Tensor):\n            raise (RuntimeError(\"segtransform.ToNumpy() only handle torch.tensor\"))\n\n        image = image.cpu().numpy().transpose((1, 2, 0))\n        if not image.dtype == np.uint8:\n            image = image.astype(np.uint8)\n        label = label.cpu().numpy().transpose((1, 2, 0))\n        if not label.dtype == np.uint8:\n            label = label.astype(np.uint8)\n        return image, label", "\nclass Normalize(object):\n    # Normalize tensor with mean and standard deviation along channel: channel = (channel - mean) / std\n    def __init__(self, mean, std=None):\n        if std is None:\n            assert len(mean) > 0\n        else:\n            assert len(mean) == len(std)\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, label):\n        if self.std is None:\n            for t, m in zip(image, self.mean):\n                t.sub_(m)\n        else:\n            for t, m, s in zip(image, self.mean, self.std):\n                t.sub_(m).div_(s)\n        return image, label", "\nclass UnNormalize(object):\n    # UnNormalize tensor with mean and standard deviation along channel: channel = (channel * std) + mean\n    def __init__(self, mean, std=None):\n        if std is None:\n            assert len(mean) > 0\n        else:\n            assert len(mean) == len(std)\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, label):\n        if self.std is None:\n            for t, m in zip(image, self.mean):\n                t.add_(m)\n        else:\n            for t, m, s in zip(image, self.mean, self.std):\n                t.mul_(s).add_(m)\n        return image, label", "\nclass Resize(object):\n    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, label):\n\n        value_scale = 255\n        mean = [0.485, 0.456, 0.406]\n        mean = [item * value_scale for item in mean]\n        std = [0.229, 0.224, 0.225]\n        std = [item * value_scale for item in std]\n\n        def find_new_hw(ori_h, ori_w, test_size):\n            if ori_h >= ori_w:\n                ratio = test_size*1.0 / ori_h\n                new_h = test_size\n                new_w = int(ori_w * ratio)\n            elif ori_w > ori_h:\n                ratio = test_size*1.0 / ori_w\n                new_h = int(ori_h * ratio)\n                new_w = test_size\n\n            if new_h % 8 != 0:\n                new_h = (int(new_h /8))*8\n            else:\n                new_h = new_h\n            if new_w % 8 != 0:\n                new_w = (int(new_w /8))*8\n            else:\n                new_w = new_w    \n            return new_h, new_w           \n\n        test_size = self.size\n        new_h, new_w = find_new_hw(image.shape[0], image.shape[1], test_size)\n        #new_h, new_w = test_size, test_size\n        image_crop = cv2.resize(image, dsize=(int(new_w), int(new_h)), interpolation=cv2.INTER_LINEAR)\n        back_crop = np.zeros((test_size, test_size, 3)) \n        # back_crop[:,:,0] = mean[0]\n        # back_crop[:,:,1] = mean[1]\n        # back_crop[:,:,2] = mean[2]\n        back_crop[:new_h, :new_w, :] = image_crop\n        image = back_crop \n\n        s_mask = label\n        new_h, new_w = find_new_hw(s_mask.shape[0], s_mask.shape[1], test_size)\n        #new_h, new_w = test_size, test_size\n        s_mask = cv2.resize(s_mask.astype(np.float32), dsize=(int(new_w), int(new_h)),interpolation=cv2.INTER_NEAREST)\n        back_crop_s_mask = np.ones((test_size, test_size)) * 255\n        back_crop_s_mask[:new_h, :new_w] = s_mask\n        label = back_crop_s_mask\n\n        return image, label", "\nclass test_Resize(object):\n    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, label):\n\n        value_scale = 255\n        mean = [0.485, 0.456, 0.406]\n        mean = [item * value_scale for item in mean]\n        std = [0.229, 0.224, 0.225]\n        std = [item * value_scale for item in std]\n\n        def find_new_hw(ori_h, ori_w, test_size):\n            if max(ori_h, ori_w) > test_size:\n                if ori_h >= ori_w:\n                    ratio = test_size*1.0 / ori_h\n                    new_h = test_size\n                    new_w = int(ori_w * ratio)\n                elif ori_w > ori_h:\n                    ratio = test_size*1.0 / ori_w\n                    new_h = int(ori_h * ratio)\n                    new_w = test_size\n\n                if new_h % 8 != 0:\n                    new_h = (int(new_h /8))*8\n                else:\n                    new_h = new_h\n                if new_w % 8 != 0:\n                    new_w = (int(new_w /8))*8\n                else:\n                    new_w = new_w    \n                return new_h, new_w     \n            else:\n                return ori_h, ori_w      \n\n        test_size = self.size\n        new_h, new_w = find_new_hw(image.shape[0], image.shape[1], test_size)\n        if new_w != image.shape[0] or new_h != image.shape[1]:\n            image_crop = cv2.resize(image, dsize=(int(new_w), int(new_h)), interpolation=cv2.INTER_LINEAR)\n        else:\n            image_crop = image.copy()\n        back_crop = np.zeros((test_size, test_size, 3)) \n        back_crop[:new_h, :new_w, :] = image_crop\n        image = back_crop \n\n        s_mask = label\n        new_h, new_w = find_new_hw(s_mask.shape[0], s_mask.shape[1], test_size)\n        if new_w != s_mask.shape[0] or new_h != s_mask.shape[1]:\n            s_mask = cv2.resize(s_mask.astype(np.float32), dsize=(int(new_w), int(new_h)),interpolation=cv2.INTER_NEAREST)\n        back_crop_s_mask = np.ones((test_size, test_size)) * 255\n        back_crop_s_mask[:new_h, :new_w] = s_mask\n        label = back_crop_s_mask\n\n        return image, label", "\nclass Direct_Resize(object):\n    # Resize the input to the given size, 'size' is a 2-element tuple or list in the order of (h, w).\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image, label):\n\n        test_size = self.size\n\n        image = cv2.resize(image, dsize=(test_size, test_size), interpolation=cv2.INTER_LINEAR)\n        label = cv2.resize(label.astype(np.float32), dsize=(test_size, test_size),interpolation=cv2.INTER_NEAREST)\n\n        return image, label", "\nclass RandScale(object):\n    # Randomly resize image & label with scale factor in [scale_min, scale_max]\n    def __init__(self, scale, aspect_ratio=None):\n        assert (isinstance(scale, collections.Iterable) and len(scale) == 2)\n        if isinstance(scale, collections.Iterable) and len(scale) == 2 \\\n                and isinstance(scale[0], numbers.Number) and isinstance(scale[1], numbers.Number) \\\n                and 0 < scale[0] < scale[1]:\n            self.scale = scale\n        else:\n            raise (RuntimeError(\"segtransform.RandScale() scale param error.\\n\"))\n        if aspect_ratio is None:\n            self.aspect_ratio = aspect_ratio\n        elif isinstance(aspect_ratio, collections.Iterable) and len(aspect_ratio) == 2 \\\n                and isinstance(aspect_ratio[0], numbers.Number) and isinstance(aspect_ratio[1], numbers.Number) \\\n                and 0 < aspect_ratio[0] < aspect_ratio[1]:\n            self.aspect_ratio = aspect_ratio\n        else:\n            raise (RuntimeError(\"segtransform.RandScale() aspect_ratio param error.\\n\"))\n\n    def __call__(self, image, label):\n        temp_scale = self.scale[0] + (self.scale[1] - self.scale[0]) * random.random()\n        temp_aspect_ratio = 1.0\n        if self.aspect_ratio is not None:\n            temp_aspect_ratio = self.aspect_ratio[0] + (self.aspect_ratio[1] - self.aspect_ratio[0]) * random.random()\n            temp_aspect_ratio = math.sqrt(temp_aspect_ratio)\n        scale_factor_x = temp_scale * temp_aspect_ratio\n        scale_factor_y = temp_scale / temp_aspect_ratio\n        image = cv2.resize(image, None, fx=scale_factor_x, fy=scale_factor_y, interpolation=cv2.INTER_LINEAR)\n        label = cv2.resize(label, None, fx=scale_factor_x, fy=scale_factor_y, interpolation=cv2.INTER_NEAREST)\n        return image, label", "\nclass Crop(object):\n    \"\"\"Crops the given ndarray image (H*W*C or H*W).\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n        int instead of sequence like (h, w), a square crop (size, size) is made.\n    \"\"\"\n    def __init__(self, size, crop_type='center', padding=None, ignore_label=255):\n        self.size = size\n        if isinstance(size, int):\n            self.crop_h = size\n            self.crop_w = size\n        elif isinstance(size, collections.Iterable) and len(size) == 2 \\\n                and isinstance(size[0], int) and isinstance(size[1], int) \\\n                and size[0] > 0 and size[1] > 0:\n            self.crop_h = size[0]\n            self.crop_w = size[1]\n        else:\n            raise (RuntimeError(\"crop size error.\\n\"))\n        if crop_type == 'center' or crop_type == 'rand':\n            self.crop_type = crop_type\n        else:\n            raise (RuntimeError(\"crop type error: rand | center\\n\"))\n        if padding is None:\n            self.padding = padding\n        elif isinstance(padding, list):\n            if all(isinstance(i, numbers.Number) for i in padding):\n                self.padding = padding\n            else:\n                raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n            if len(padding) != 3:\n                raise (RuntimeError(\"padding channel is not equal with 3\\n\"))\n        else:\n            raise (RuntimeError(\"padding in Crop() should be a number list\\n\"))\n        if isinstance(ignore_label, int):\n            self.ignore_label = ignore_label\n        else:\n            raise (RuntimeError(\"ignore_label should be an integer number\\n\"))\n\n    def __call__(self, image, label):\n        h, w = label.shape\n\n        \n        pad_h = max(self.crop_h - h, 0)\n        pad_w = max(self.crop_w - w, 0)\n        pad_h_half = int(pad_h / 2)\n        pad_w_half = int(pad_w / 2)\n        if pad_h > 0 or pad_w > 0:\n            if self.padding is None:\n                raise (RuntimeError(\"segtransform.Crop() need padding while padding argument is None\\n\"))\n            image = cv2.copyMakeBorder(image, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.padding)\n            label = cv2.copyMakeBorder(label, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=self.ignore_label)\n        h, w = label.shape\n        raw_label = label\n        raw_image = image\n\n        if self.crop_type == 'rand':\n            h_off = random.randint(0, h - self.crop_h)\n            w_off = random.randint(0, w - self.crop_w)\n        else:\n            h_off = int((h - self.crop_h) / 2)\n            w_off = int((w - self.crop_w) / 2)\n        image = image[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n        label = label[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n        raw_pos_num = np.sum(raw_label == 1)\n        pos_num = np.sum(label == 1)\n        crop_cnt = 0\n        while(pos_num < 0.85*raw_pos_num and crop_cnt<=30):\n            image = raw_image\n            label = raw_label\n            if self.crop_type == 'rand':\n                h_off = random.randint(0, h - self.crop_h)\n                w_off = random.randint(0, w - self.crop_w)\n            else:\n                h_off = int((h - self.crop_h) / 2)\n                w_off = int((w - self.crop_w) / 2)\n            image = image[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]\n            label = label[h_off:h_off+self.crop_h, w_off:w_off+self.crop_w]   \n            raw_pos_num = np.sum(raw_label == 1)\n            pos_num = np.sum(label == 1)  \n            crop_cnt += 1\n        if crop_cnt >= 50:\n            image = cv2.resize(raw_image, (self.size[0], self.size[0]), interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(raw_label, (self.size[0], self.size[0]), interpolation=cv2.INTER_NEAREST)            \n\n        if image.shape != (self.size[0], self.size[0], 3):\n            image = cv2.resize(image, (self.size[0], self.size[0]), interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(label, (self.size[0], self.size[0]), interpolation=cv2.INTER_NEAREST)\n\n        return image, label", "\nclass RandRotate(object):\n    # Randomly rotate image & label with rotate factor in [rotate_min, rotate_max]\n    def __init__(self, rotate, padding, ignore_label=255, p=0.5):\n        assert (isinstance(rotate, collections.Iterable) and len(rotate) == 2)\n        if isinstance(rotate[0], numbers.Number) and isinstance(rotate[1], numbers.Number) and rotate[0] < rotate[1]:\n            self.rotate = rotate\n        else:\n            raise (RuntimeError(\"segtransform.RandRotate() scale param error.\\n\"))\n        assert padding is not None\n        assert isinstance(padding, list) and len(padding) == 3\n        if all(isinstance(i, numbers.Number) for i in padding):\n            self.padding = padding\n        else:\n            raise (RuntimeError(\"padding in RandRotate() should be a number list\\n\"))\n        assert isinstance(ignore_label, int)\n        self.ignore_label = ignore_label\n        self.p = p\n\n    def __call__(self, image, label):\n        if random.random() < self.p:\n            angle = self.rotate[0] + (self.rotate[1] - self.rotate[0]) * random.random()\n            h, w = label.shape\n            matrix = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n            image = cv2.warpAffine(image, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=self.padding)\n            label = cv2.warpAffine(label, matrix, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=self.ignore_label)\n        return image, label", "\nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, image, label):\n        if random.random() < self.p:\n            image = cv2.flip(image, 1)\n            label = cv2.flip(label, 1)\n        return image, label", "\nclass RandomVerticalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, image, label):\n        if random.random() < self.p:\n            image = cv2.flip(image, 0)\n            label = cv2.flip(label, 0)\n        return image, label", "\nclass RandomGaussianBlur(object):\n    def __init__(self, radius=5):\n        self.radius = radius\n\n    def __call__(self, image, label):\n        if random.random() < 0.5:\n            image = cv2.GaussianBlur(image, (self.radius, self.radius), 0)\n        return image, label\n\nclass RGB2BGR(object):\n    # Converts image from RGB order to BGR order, for model initialized from Caffe\n    def __call__(self, image, label):\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        return image, label", "\nclass RGB2BGR(object):\n    # Converts image from RGB order to BGR order, for model initialized from Caffe\n    def __call__(self, image, label):\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        return image, label\n\nclass BGR2RGB(object):\n    # Converts image from BGR order to RGB order, for model initialized from Pytorch\n    def __call__(self, image, label):\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image, label", ""]}
{"filename": "util/util.py", "chunked_list": ["import os\nimport numpy as np\nfrom PIL import Image\nimport random\nimport logging\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import MultipleLocator\nfrom matplotlib.ticker import FuncFormatter, FormatStrFormatter\nfrom matplotlib import font_manager", "from matplotlib.ticker import FuncFormatter, FormatStrFormatter\nfrom matplotlib import font_manager\nfrom matplotlib import rcParams\nimport seaborn as sns\nimport pandas as pd\nimport math\nfrom seaborn.distributions import distplot\nfrom tqdm import tqdm\nfrom scipy import ndimage\n", "from scipy import ndimage\n\n# from get_weak_anns import find_bbox, ScribblesRobot\n\nimport torch\nfrom torch import nn\nimport torch.backends.cudnn as cudnn\nimport torch.nn.init as initer\n\nSpecial_characters = [", "\nSpecial_characters = [\n    ['\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588', '\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581'],\n    ['( * ^ _ ^ * )', '( * ^ _ ^ * )'],\n    ['( $ _ $ )', '( $ _ $ )'],\n    ['(\uff1f \u2027 _ \u2027 )', '( \u2027 _ \u2027 \uff1f)'],\n    ['( T___T )', '( T___T )'],\n    [' \u2312 _ \u2312 \u2606 ', ' \u2606 \u2312 _ \u2312 '],\n    ['( = ^ o ^ = )', '( = ^ o ^ = )'],\n    [' \u32a3 \u32a3 \u32a3 ', ' \u32a3 \u32a3 \u32a3 '],", "    ['( = ^ o ^ = )', '( = ^ o ^ = )'],\n    [' \u32a3 \u32a3 \u32a3 ', ' \u32a3 \u32a3 \u32a3 '],\n    ['.\u00b8.\u00b7\u00b4\u00af`\u00b7', '.\u00b8.\u00b7\u00b4\u00af`\u00b7'],\n    ['( \u00af \u25a1 \u00af )', '( \u00af \u25a1 \u00af )'],\n    ['( \u2299 o \u2299 )', '( \u2299 o \u2299 )'],\n    [' \u25d5 \u203f \u25d5 \uff61 ', ' \uff61 \u25d5 \u203f \u25d5 '],\n    ['( \u25e1 \u203f \u25e1 \u273f)', '(\u273f \u25e1 \u203f \u25e1 )']\n]\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", "\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", "\ndef lr_decay(optimizer, base_lr, curr_iter, max_iter, decay_dict, current_characters=None):\n    if decay_dict['type'] == 'poly_learning_rate':\n        scale_lr=10.\n        lr = base_lr * (1 - float(curr_iter) / max_iter) ** decay_dict['power']\n        for index, param_group in enumerate(optimizer.param_groups):\n            if index <= decay_dict['index_split']:\n                param_group['lr'] = lr\n            else:\n                param_group['lr'] = lr * scale_lr\n\n    elif decay_dict['type'] == 'adjust_learning_rate_poly':\n        reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n        lr = base_lr * reduce\n        optimizer.param_groups[0]['lr'] = lr * 1\n        optimizer.param_groups[1]['lr'] = lr * 2\n        optimizer.param_groups[2]['lr'] = lr * 10\n        optimizer.param_groups[3]['lr'] = lr * 20\n    \n    elif decay_dict['type'] == 'half_learning_rate':\n        scale_lr=10.\n        lr = base_lr * (1 - float(decay_dict['rate']*curr_iter) / (max_iter)) ** decay_dict['power']\n\n        for index, param_group in enumerate(optimizer.param_groups):\n            if index <= decay_dict['index_split']:\n                param_group['lr'] = lr\n            else:\n                param_group['lr'] = lr * scale_lr\n\n    elif decay_dict['type'] == 'split_learning_rate_poly':\n        reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n        lr = base_lr * reduce\n        for i in range(len(optimizer.param_groups)):\n            optimizer.param_groups[i]['lr'] = lr * decay_dict['scale_lr'][i]\n\n    elif decay_dict['type'] == 'step_learning_rate_poly':\n        # reduce = ((1-float(curr_iter)/max_iter)**(decay_dict['power']))\n        \n        # tmp_lr =base_lr\n        for i in len(decay_dict['step']):\n\n            if float(curr_iter)/max_iter > decay_dict['step'][i]:\n                tmp_lr = base_lr*decay_dict['step_rate'][i]\n        # lr = tmp_lr * reduce\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = tmp_lr\n    elif decay_dict['type'] == 'adam':\n        lr = base_lr\n        if curr_iter % 50 == 0:   \n            print(' Using Adam optim')\n\n        \n    if curr_iter % 50 == 0:   \n        print(' '*len(current_characters[0])*3 +' '*10  + 'Base LR: {:.8f}, Curr LR: {:.8f}'.format(base_lr, lr) )", "\ndef step_learning_rate(optimizer, base_lr, epoch, step_epoch, multiplier=0.1):\n    \"\"\"Sets the learning rate to the base LR decayed by 10 every step epochs\"\"\"\n    lr = base_lr * (multiplier ** (epoch // step_epoch))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef poly_learning_rate(optimizer, base_lr, curr_iter, max_iter, power=0.9, index_split=-1, scale_lr=10., warmup=False, warmup_step=500):\n    \"\"\"poly learning rate policy\"\"\"\n    if warmup and curr_iter < warmup_step:\n        lr = base_lr * (0.1 + 0.9 * (curr_iter/warmup_step))\n    else:\n        lr = base_lr * (1 - float(curr_iter) / max_iter) ** power\n\n    if curr_iter % 50 == 0:   \n        print('Base LR: {:.4f}, Curr LR: {:.4f}, Warmup: {}.'.format(base_lr, lr, (warmup and curr_iter < warmup_step)))     \n\n    for index, param_group in enumerate(optimizer.param_groups):\n        if index <= index_split:\n            param_group['lr'] = lr\n        else:\n            param_group['lr'] = lr * scale_lr", "\ndef adjust_learning_rate_poly(optimizer, base_lr, curr_iter, max_iter,  power=0.9):\n    # base_lr = 3.5e-4\n    # max_iter = args.max_steps\n    reduce = ((1-float(curr_iter)/max_iter)**(power))\n    lr = base_lr * reduce\n    optimizer.param_groups[0]['lr'] = lr * 1\n    optimizer.param_groups[1]['lr'] = lr * 2\n    optimizer.param_groups[2]['lr'] = lr * 10\n    optimizer.param_groups[3]['lr'] = lr * 20\n    if curr_iter % 50 == 0:   \n        print('Base LR: {:.8f}, Curr LR: {:.8f}, '.format(base_lr, lr)) ", "\ndef intersectionAndUnion(output, target, K, ignore_index=255):\n    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n    assert (output.ndim in [1, 2, 3])\n    assert output.shape == target.shape\n    output = output.reshape(output.size).copy()\n    target = target.reshape(target.size)\n    output[np.where(target == ignore_index)[0]] = ignore_index\n    intersection = output[np.where(output == target)[0]]\n    area_intersection, _ = np.histogram(intersection, bins=np.arange(K+1))\n    area_output, _ = np.histogram(output, bins=np.arange(K+1))\n    area_target, _ = np.histogram(target, bins=np.arange(K+1))\n    area_union = area_output + area_target - area_intersection\n    return area_intersection, area_union, area_target", "\ndef intersectionAndUnionGPU(output, target, K, ignore_index=255):\n    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n    assert (output.dim() in [1, 2, 3])\n    assert output.shape == target.shape\n    output = output.view(-1)\n    # target = target.view(-1)\n    target = target.reshape(-1)\n    output[target == ignore_index] = ignore_index\n    intersection = output[output == target]\n    area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n    area_union = area_output + area_target - area_intersection\n    return area_intersection, area_union, area_target", "\ndef check_mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n\ndef check_makedirs(dir_name):\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n\ndef del_file(path):\n    for i in os.listdir(path):\n        path_file = os.path.join(path,i)\n        if os.path.isfile(path_file):\n            os.remove(path_file)\n        else:\n            del_file(path_file)", "\ndef del_file(path):\n    for i in os.listdir(path):\n        path_file = os.path.join(path,i)\n        if os.path.isfile(path_file):\n            os.remove(path_file)\n        else:\n            del_file(path_file)\n\ndef init_weights(model, conv='kaiming', batchnorm='normal', linear='kaiming', lstm='kaiming'):\n    \"\"\"\n    :param model: Pytorch Model which is nn.Module\n    :param conv:  'kaiming' or 'xavier'\n    :param batchnorm: 'normal' or 'constant'\n    :param linear: 'kaiming' or 'xavier'\n    :param lstm: 'kaiming' or 'xavier'\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n            if conv == 'kaiming':\n                initer.kaiming_normal_(m.weight)\n            elif conv == 'xavier':\n                initer.xavier_normal_(m.weight)\n            else:\n                raise ValueError(\"init type of conv error.\\n\")\n            if m.bias is not None:\n                initer.constant_(m.bias, 0)\n\n        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):#, BatchNorm1d, BatchNorm2d, BatchNorm3d)):\n            if batchnorm == 'normal':\n                initer.normal_(m.weight, 1.0, 0.02)\n            elif batchnorm == 'constant':\n                initer.constant_(m.weight, 1.0)\n            else:\n                raise ValueError(\"init type of batchnorm error.\\n\")\n            initer.constant_(m.bias, 0.0)\n\n        elif isinstance(m, nn.Linear):\n            if linear == 'kaiming':\n                initer.kaiming_normal_(m.weight)\n            elif linear == 'xavier':\n                initer.xavier_normal_(m.weight)\n            else:\n                raise ValueError(\"init type of linear error.\\n\")\n            if m.bias is not None:\n                initer.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.LSTM):\n            for name, param in m.named_parameters():\n                if 'weight' in name:\n                    if lstm == 'kaiming':\n                        initer.kaiming_normal_(param)\n                    elif lstm == 'xavier':\n                        initer.xavier_normal_(param)\n                    else:\n                        raise ValueError(\"init type of lstm error.\\n\")\n                elif 'bias' in name:\n                    initer.constant_(param, 0)", "\ndef init_weights(model, conv='kaiming', batchnorm='normal', linear='kaiming', lstm='kaiming'):\n    \"\"\"\n    :param model: Pytorch Model which is nn.Module\n    :param conv:  'kaiming' or 'xavier'\n    :param batchnorm: 'normal' or 'constant'\n    :param linear: 'kaiming' or 'xavier'\n    :param lstm: 'kaiming' or 'xavier'\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n            if conv == 'kaiming':\n                initer.kaiming_normal_(m.weight)\n            elif conv == 'xavier':\n                initer.xavier_normal_(m.weight)\n            else:\n                raise ValueError(\"init type of conv error.\\n\")\n            if m.bias is not None:\n                initer.constant_(m.bias, 0)\n\n        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):#, BatchNorm1d, BatchNorm2d, BatchNorm3d)):\n            if batchnorm == 'normal':\n                initer.normal_(m.weight, 1.0, 0.02)\n            elif batchnorm == 'constant':\n                initer.constant_(m.weight, 1.0)\n            else:\n                raise ValueError(\"init type of batchnorm error.\\n\")\n            initer.constant_(m.bias, 0.0)\n\n        elif isinstance(m, nn.Linear):\n            if linear == 'kaiming':\n                initer.kaiming_normal_(m.weight)\n            elif linear == 'xavier':\n                initer.xavier_normal_(m.weight)\n            else:\n                raise ValueError(\"init type of linear error.\\n\")\n            if m.bias is not None:\n                initer.constant_(m.bias, 0)\n\n        elif isinstance(m, nn.LSTM):\n            for name, param in m.named_parameters():\n                if 'weight' in name:\n                    if lstm == 'kaiming':\n                        initer.kaiming_normal_(param)\n                    elif lstm == 'xavier':\n                        initer.xavier_normal_(param)\n                    else:\n                        raise ValueError(\"init type of lstm error.\\n\")\n                elif 'bias' in name:\n                    initer.constant_(param, 0)", "\ndef colorize(gray, palette):\n    # gray: numpy array of the label and 1*3N size list palette\n    color = Image.fromarray(gray.astype(np.uint8)).convert('P')\n    color.putpalette(palette)\n    return color\n\n# ------------------------------------------------------\ndef get_model_para_number(model):\n    total_number = 0\n    learnable_number = 0 \n    for para in model.parameters():\n        total_number += torch.numel(para)\n        if para.requires_grad == True:\n            learnable_number+= torch.numel(para)\n    return total_number, learnable_number", "def get_model_para_number(model):\n    total_number = 0\n    learnable_number = 0 \n    for para in model.parameters():\n        total_number += torch.numel(para)\n        if para.requires_grad == True:\n            learnable_number+= torch.numel(para)\n    return total_number, learnable_number\n\ndef setup_seed(seed=2021, deterministic=False):\n    if deterministic:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)", "\ndef setup_seed(seed=2021, deterministic=False):\n    if deterministic:\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)", "    \ndef get_logger():\n    logger_name = \"main-logger\"\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    fmt = \"[%(asctime)s %(levelname)s %(filename)s line %(lineno)d %(process)d] %(message)s\"\n    handler.setFormatter(logging.Formatter(fmt))\n    logger.addHandler(handler)\n    return logger", "\ndef get_metirc(output, target, K, ignore_index=255):\n    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n    #\u4ea4\u5e76\u6bd4\n    assert (output.dim() in [1, 2, 3])\n    assert output.shape == target.shape\n    output = output.view(-1)\n    target = target.view(-1)\n    output[target == ignore_index] = ignore_index\n    intersection = output[output == target]\n    if intersection.shape[0] == 0:\n        area_intersection = torch.tensor([0.,0.],device='cuda')\n    else:\n        area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n    # area_union = area_output + area_target - area_intersection\n    Pre = area_intersection / (area_output + 1e-10)\n    Rec = area_intersection / (area_target + 1e-10)\n    return Pre, Rec", "\ndef get_save_path_1(args):\n    if len(args.variable1) != 0 :\n        variable1 = eval('args.{}'.format(args.variable1))\n        args[args.variable1] = eval(args.aux1)\n    else :\n        variable1 = 0\n    if len(args.variable2) != 0 :\n        variable2 = eval('args.{}'.format(args.variable2))\n        args[args.variable2] = eval(args.aux2)\n    else :\n        variable2 = 0\n\n    args.snapshot_path = 'exp/{}/{}/{}/split{}/{}shot/'.format( args.arch,  args.dataset, args.backbone, args.split, args.shot)\n    args.result_path = 'exp/{}/{}/{}/split{}/result/'.format( args.arch,  args.dataset, args.backbone, args.split)\n    if len(args.variable1) != 0 or len(args.variable2) != 0  :\n        args.snapshot_path = args.snapshot_path+'{}_{}/'.format(eval(args.aux1), eval(args.aux2))", "        \ndef get_save_path(args):\n    if len(args.variable1) != 0 :\n        variable1 = eval('args.{}'.format(args.variable1))\n\n    else :\n        variable1 = 0\n    if len(args.variable2) != 0 :\n        variable2 = eval('args.{}'.format(args.variable2))\n\n    else :\n        variable2 = 0\n\n    args.snapshot_path = 'exp/{}/{}/{}/split{}/{}shot/'.format( args.arch,  args.dataset, args.backbone, args.split, args.shot)\n    args.result_path = 'exp/{}/{}/{}/split{}/result/'.format( args.arch,  args.dataset, args.backbone, args.split)\n    if len(args.variable1) != 0 or len(args.variable2) != 0  :\n        args.snapshot_path = args.snapshot_path+'{}_{}/'.format(variable1, variable2)", "        \n\ndef is_same_model(model1, model2):\n    flag = 0\n    count = 0\n    for k, v in model1.state_dict().items():\n        model1_val = v\n        model2_val = model2.state_dict()[k]\n        if (model1_val==model2_val).all():\n            pass\n        else:\n            flag+=1\n            print('value of key <{}> mismatch'.format(k))\n        count+=1\n\n    return True if flag==0 else False", "\ndef fix_bn(m):\n    classname = m.__class__.__name__\n    if classname.find('BatchNorm') != -1:\n        m.eval()\n    if classname.find('LayerNorm') != -1:\n        m.eval()\n\ndef freeze_modules(model, freeze_layer):\n\n    for item in freeze_layer:\n        if hasattr(model, item):\n            for param in eval('model.' + item).parameters():\n                param.requires_grad = False\n            print('model.{} has been frozen'.format(item))\n        else:\n            print('model has no part named {} , please check the input'.format(item))", "def freeze_modules(model, freeze_layer):\n\n    for item in freeze_layer:\n        if hasattr(model, item):\n            for param in eval('model.' + item).parameters():\n                param.requires_grad = False\n            print('model.{} has been frozen'.format(item))\n        else:\n            print('model has no part named {} , please check the input'.format(item))\n\ndef sum_list(list):\n    sum = 0\n    for item in list:\n        sum += item\n    return sum", "\ndef sum_list(list):\n    sum = 0\n    for item in list:\n        sum += item\n    return sum\n\ndef make_dataset(data_root=None, data_list=None, all_class=None, split_list=None, fliter_intersection=True):    \n    # if not os.path.isfile(data_list):\n    #     raise (RuntimeError(\"Image list file do not exist: \" + data_list + \"\\n\"))\n\n    # Shaban uses these lines to remove small objects:\n    # if util.change_coordinates(mask, 32.0, 0.0).sum() > 2:\n    #    flitered_item.append(item)\n    # which means the mask will be downsampled to 1/32 of the original size and the valid area should be larger than 2, \n    # therefore the area in original size should be accordingly larger than 2 * 32 * 32    \n    image_label_list_0 = []  \n    image_label_list_1 = [] \n    image_label_list_2 = [] \n    image_label_list_3 = [] \n    fin_list = []\n    list_read = open(data_list).readlines()\n\n    sub_class_file_list = {}\n    for sub_c in all_class:\n        sub_class_file_list[sub_c] = []\n\n    for l_idx in tqdm(range(len(list_read))):\n        line = list_read[l_idx]\n        line = line.strip()\n        line_split = line.split(' ')\n        image_name = os.path.join(data_root, line_split[0])\n        label_name = os.path.join(data_root, line_split[1])\n        item = (image_name, label_name)\n        label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n        label_class = np.unique(label).tolist()\n\n        if 0 in label_class:\n            label_class.remove(0)\n        if 255 in label_class:\n            label_class.remove(255)\n        # all_label_list = []\n        new_label_class_0 = []\n        new_label_class_1 = []\n        new_label_class_2 = []\n        new_label_class_3 = []\n        all_label_class = []\n        \n        if fliter_intersection:\n\n            for c in label_class:\n                tmp_label = np.zeros_like(label)\n                target_pix = np.where(label == c)\n                tmp_label[target_pix[0],target_pix[1]] = 1 \n                \n                if tmp_label.sum() >= 2 * 32 * 32:   \n                    all_label_class.append(c)\n                    for i in range(len(split_list)):\n                        sub_list = split_list[i]\n                        tmp_label_class = eval('new_label_class_{}'.format(i))\n                        if set(label_class).issubset(set(sub_list)):\n                            if c in sub_list:\n                                tmp_label_class.append(c)\n        else:\n            for c in label_class:\n                tmp_label = np.zeros_like(label)\n                target_pix = np.where(label == c)\n                tmp_label[target_pix[0],target_pix[1]] = 1 \n                \n                if tmp_label.sum() >= 2 * 32 * 32:   \n                    all_label_class.append(c)\n                    for i in range(len(split_list)):\n                        sub_list = split_list[i]\n                        tmp_label_class = eval('new_label_class_{}'.format(i))\n                        if c in sub_list:\n                            tmp_label_class.append(c)    \n   \n        for i in range(len(split_list)):\n            new_label_class = eval('new_label_class_{}'.format(i))\n            image_label_list = eval('image_label_list_{}'.format(i))\n            if len(new_label_class) > 0:\n                image_label_list.append(item)\n\n        for c in all_label_class:\n            sub_class_file_list[c].append(item)\n            \n    for i in range(len(split_list)):\n        image_label_list = eval('image_label_list_{}'.format(i))\n        fin_list.append(image_label_list)  \n    # print(\"Checking image&label pair {} list done! \".format(split))\n    return fin_list, sub_class_file_list", "\ndef make_data_list(data_root=None, dataset=None, train_list=None, val_list=None, all_class=None, val_class=None, ):\n    list_root = './lists/{}/fss_list/'.format(dataset)\n    if not os.path.exists(list_root):\n        train_root = list_root + 'train/'\n        val_root = list_root + 'val/'\n        print('{} has been created!'.format(list_root))\n        os.makedirs(train_root)\n        os.makedirs(val_root)\n\n    data_root = data_root\n\n    train_class_list = []\n    for i in range(len(val_class)):\n        tmp_list = list(set(all_class) - set(val_class[i]))\n        train_class_list.append(tmp_list)\n\n\n    print('Processing train_split')\n    train_txt, train_dict = make_dataset(data_root=data_root, data_list=train_list, all_class=all_class, split_list=train_class_list, fliter_intersection=True)\n    with open (train_root + 'train_dict.txt', 'w') as f:\n        f.write(str(train_dict))\n\n    for i in range(len(train_txt)):\n        data_list = train_txt[i]\n        with open(train_root + 'train_split{}.txt'.format(i), 'w')as f:\n            for item in data_list:\n                img, label = item\n                f.write(img + ' ')\n                f.write(label + '\\n')\n            \n    print('Processing val_split')\n    val_txt, val_dict = make_dataset(data_root=data_root, data_list=val_list, all_class=all_class, split_list=val_class, fliter_intersection=False)\n    with open (val_root + 'val_dict.txt', 'w') as f:\n        f.write(str(val_dict))\n\n    for i in range(len(val_txt)):\n        data_list = val_txt[i]\n        with open(val_root + 'val_split{}.txt'.format(i), 'w')as f:\n            for item in data_list:\n                img, label = item\n                f.write(img + ' ')\n                f.write(label + '\\n')\n\n    print('Processing val_base')\n    base_txt, _ = make_dataset(data_root=data_root, data_list=val_list, all_class=all_class, split_list=val_class, fliter_intersection=False)\n\n    for i in range(len(base_txt)):\n        data_list = base_txt[i]\n        with open(val_root + 'val_base{}.txt'.format(i), 'w')as f:\n            for item in data_list:\n                img, label = item\n                f.write(img + ' ')\n                f.write(label + '\\n')", "\ndef make_dict(data_root=None, data_list=None, all_class=None, dataset=None, mode=None):    \n\n    \"\"\"\n    data_list: all data in train/val\n    all_class: all class in dataset\n    \n    return the dict that contains the data_list of each classes\n    \"\"\"\n\n    dict_name = './lists/{}/{}_dict.txt'.format(dataset, mode)\n    list_read = open(data_list).readlines()\n\n    sub_class_file_list = {}\n    for sub_c in all_class:\n        sub_class_file_list[sub_c] = []\n    print('Processing {} data' .format(mode))\n    for l_idx in tqdm(range(len(list_read))):\n        line = list_read[l_idx]\n        line = line.strip()\n        line_split = line.split(' ')\n        image_name = os.path.join(data_root, line_split[0])\n        label_name = os.path.join(data_root, line_split[1])\n        item = (image_name, label_name)\n        label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n        label_class = np.unique(label).tolist()\n\n        if 0 in label_class:\n            label_class.remove(0)\n        if 255 in label_class:\n            label_class.remove(255)\n        all_label_class = []\n        for c in label_class:\n            tmp_label = np.zeros_like(label)\n            target_pix = np.where(label == c)\n            tmp_label[target_pix[0],target_pix[1]] = 1 \n            \n            if tmp_label.sum() >= 2 * 32 * 32:   \n                all_label_class.append(c)\n\n        for c in all_label_class:\n            sub_class_file_list[c].append(item)\n\n    with open (dict_name, 'w') as f:\n        f.write(str(sub_class_file_list))", "\ndef gen_list(class_dict_name, class_list, fliter):\n    with open(class_dict_name, 'r') as f:\n        f_str = f.read()\n    class_dict = eval(f_str)\n\n    Discard = set()\n    Adopt = set()\n    for id in class_dict:\n        if id in class_list:\n            Adopt = set(class_dict[id]) | Adopt\n        else:\n            Discard = set(class_dict[id]) | Discard\n    if fliter:\n        out_list = Adopt - Discard\n    else:\n        out_list = Adopt\n    \n    return list(out_list), class_dict", "\n    # return sub_class_file_list\n\n"]}
