{"filename": "NC/methods/SlotGAT/conv.py", "chunked_list": ["\"\"\"Torch modules for graph attention networks(GAT).\"\"\"\n# pylint: disable= no-member, arguments-differ, invalid-name\nfrom shutil import ExecError\nimport torch as th\nfrom torch import nn\nimport torch\nfrom dgl import function as fn\nfrom dgl.nn.pytorch import edge_softmax\nfrom dgl._ffi.base import DGLError\nfrom dgl.nn.pytorch.utils import Identity", "from dgl._ffi.base import DGLError\nfrom dgl.nn.pytorch.utils import Identity\nfrom dgl.utils import expand_as_pair\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.profiler import profile, record_function, ProfilerActivity\n# pylint: disable=W0235\nclass slotGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.,\n                 num_ntype=None, eindexer=None,inputhead=False, dataRecorder=None):\n        super(slotGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None\n        self.eindexer=eindexer\n        self.num_ntype=num_ntype \n        \n        self.attentions=None\n        self.dataRecorder=dataRecorder\n\n        if isinstance(in_feats, tuple):\n            raise NotImplementedError()\n        else:\n            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None\n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        self.alpha = alpha\n        self.inputhead=inputhead\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc, gain=gain)\n            \n        else:\n            raise NotImplementedError()\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.attn_e, gain=gain) \n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        elif isinstance(self.res_fc, Identity):\n            pass\n        elif isinstance(self.res_fc, nn.Parameter):\n            nn.init.xavier_normal_(self.res_fc, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n        with graph.local_scope():\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                raise NotImplementedError()\n            else:\n                #feature transformation first\n                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n\n                if self.inputhead:\n                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n                else:\n                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n                if \"getEmb\" in get_out:\n                    self.emb=h_dst.cpu().detach()\n                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads)\n                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n                e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n                e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n                ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n                el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n                er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n                graph.srcdata.update({'ft': feat_src, 'el': el})\n                graph.dstdata.update({'er': er})\n                graph.edata.update({'ee': ee}) if self._edge_feats else None\n                graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n                e_=graph.edata.pop('e')\n                ee=graph.edata.pop('ee') if self._edge_feats else 0\n                e=e_+ee\n                \n                e = self.leaky_relu(e)\n            # compute softmax\n            a=self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                a=a * (1-self.alpha) + res_attn * self.alpha \n            if self.dataRecorder[\"status\"]==\"FinalTesting\":\n                if \"attention\" not in self.dataRecorder[\"data\"]:\n                    self.dataRecorder[\"data\"][\"attention\"]=[]\n                self.dataRecorder[\"data\"][\"attention\"].append(a)\n            graph.edata['a'] = a\n            # then message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n                             \n            rst = graph.dstdata['ft'] \n            # residual\n            if self.res_fc is not None:\n                \n                if self._in_dst_feats != self._out_feats:\n                    resval =torch.bmm(h_src,self.res_fc)\n                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                else:\n                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            self.attentions=graph.edata.pop('a').detach()\n            torch.cuda.empty_cache()\n            return rst, self.attentions", "class slotGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.,\n                 num_ntype=None, eindexer=None,inputhead=False, dataRecorder=None):\n        super(slotGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None\n        self.eindexer=eindexer\n        self.num_ntype=num_ntype \n        \n        self.attentions=None\n        self.dataRecorder=dataRecorder\n\n        if isinstance(in_feats, tuple):\n            raise NotImplementedError()\n        else:\n            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None\n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        self.alpha = alpha\n        self.inputhead=inputhead\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc, gain=gain)\n            \n        else:\n            raise NotImplementedError()\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.attn_e, gain=gain) \n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        elif isinstance(self.res_fc, Identity):\n            pass\n        elif isinstance(self.res_fc, nn.Parameter):\n            nn.init.xavier_normal_(self.res_fc, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n        with graph.local_scope():\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                raise NotImplementedError()\n            else:\n                #feature transformation first\n                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n\n                if self.inputhead:\n                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n                else:\n                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n                if \"getEmb\" in get_out:\n                    self.emb=h_dst.cpu().detach()\n                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads)\n                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n                e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n                e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n                ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n                el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n                er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n                graph.srcdata.update({'ft': feat_src, 'el': el})\n                graph.dstdata.update({'er': er})\n                graph.edata.update({'ee': ee}) if self._edge_feats else None\n                graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n                e_=graph.edata.pop('e')\n                ee=graph.edata.pop('ee') if self._edge_feats else 0\n                e=e_+ee\n                \n                e = self.leaky_relu(e)\n            # compute softmax\n            a=self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                a=a * (1-self.alpha) + res_attn * self.alpha \n            if self.dataRecorder[\"status\"]==\"FinalTesting\":\n                if \"attention\" not in self.dataRecorder[\"data\"]:\n                    self.dataRecorder[\"data\"][\"attention\"]=[]\n                self.dataRecorder[\"data\"][\"attention\"].append(a)\n            graph.edata['a'] = a\n            # then message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n                             \n            rst = graph.dstdata['ft'] \n            # residual\n            if self.res_fc is not None:\n                \n                if self._in_dst_feats != self._out_feats:\n                    resval =torch.bmm(h_src,self.res_fc)\n                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                else:\n                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            self.attentions=graph.edata.pop('a').detach()\n            torch.cuda.empty_cache()\n            return rst, self.attentions", "\n\nclass changedGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.,\n                 num_ntype=None,  eindexer=None):\n        super(changedGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats)  \n        self.eindexer=eindexer\n        if isinstance(in_feats, tuple):\n            self.fc_src = nn.Linear(\n                self._in_src_feats, out_feats * num_heads, bias=False)\n            self.fc_dst = nn.Linear(\n                self._in_dst_feats, out_feats * num_heads, bias=False)\n            raise Exception(\"!!!\")\n        else:\n            self.fc = nn.Linear(\n                    self._in_src_feats, out_feats * num_heads, bias=False) \n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats,num_etypes)))\n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc = nn.Linear(\n                    self._in_dst_feats, num_heads * out_feats, bias=False)\n                    \n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        if bias:\n            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats))) \n        self.alpha = alpha\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        else:\n            raise NotImplementedError()\n            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain)\n        nn.init.xavier_normal_(self.attn_e, gain=gain)\n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat, res_attn=None):\n        with graph.local_scope():\n            \n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                h_src = self.feat_drop(feat[0])\n                h_dst = self.feat_drop(feat[1])\n                if not hasattr(self, 'fc_src'):\n                    self.fc_src, self.fc_dst = self.fc, self.fc\n                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n                raise Exception(\"!!!\")\n            else:\n                #feature transformation first\n                h_src = h_dst = self.feat_drop(feat)\n\n                feat_src = feat_dst = self.fc(h_src).view(\n                        -1, self._num_heads, self._out_feats)\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\n\n            e_feat = self.edge_emb(e_feat)\n            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n            graph.srcdata.update({'ft': feat_src, 'el': el})\n            graph.dstdata.update({'er': er})\n            graph.edata.update({'ee': ee})\n            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n            # compute softmax\n            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n            # then message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n            \n            rst = graph.dstdata['ft']\n            # residual\n            if self.res_fc is not None:\n                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            return rst, graph.edata.pop('a').detach()", "\n\n\n# pylint: enable=W0235\nclass myGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.):\n        super(myGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats)\n        if isinstance(in_feats, tuple):\n            self.fc_src = nn.Linear(\n                self._in_src_feats, out_feats * num_heads, bias=False)\n            self.fc_dst = nn.Linear(\n                self._in_dst_feats, out_feats * num_heads, bias=False)\n            raise Exception(\"!!!\")\n        else:\n            self.fc = nn.Linear(\n                self._in_src_feats, out_feats * num_heads, bias=False)\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc = nn.Linear(\n                    self._in_dst_feats, num_heads * out_feats, bias=False)\n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        if bias:\n            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats)))\n        self.alpha = alpha\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        else:\n            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain)\n        nn.init.xavier_normal_(self.attn_e, gain=gain)\n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat, res_attn=None):\n        with graph.local_scope():\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                h_src = self.feat_drop(feat[0])\n                h_dst = self.feat_drop(feat[1])\n                if not hasattr(self, 'fc_src'):\n                    self.fc_src, self.fc_dst = self.fc, self.fc\n                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n                raise Exception(\"!!!\")\n            else:\n                h_src = h_dst = self.feat_drop(feat)\n                feat_src = feat_dst = self.fc(h_src).view(\n                    -1, self._num_heads, self._out_feats)\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n            e_feat = self.edge_emb(e_feat)\n            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n            graph.srcdata.update({'ft': feat_src, 'el': el})\n            graph.dstdata.update({'er': er})\n            graph.edata.update({'ee': ee})\n            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n            # compute softmax\n            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n            # message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n                             \n            rst = graph.dstdata['ft']\n            # residual\n            if self.res_fc is not None:\n                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            return rst, graph.edata.pop('a').detach()", ""]}
{"filename": "NC/methods/SlotGAT/GNN.py", "chunked_list": ["import torch\nimport torch as th\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport dgl\nfrom dgl.nn.pytorch import GraphConv\nimport math\nimport dgl.function as fn\nfrom dgl.nn.pytorch import edge_softmax, GATConv\nfrom conv import myGATConv,changedGATConv,slotGATConv", "from dgl.nn.pytorch import edge_softmax, GATConv\nfrom conv import myGATConv,changedGATConv,slotGATConv\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom torch_geometric.typing import OptPairTensor, OptTensor, Size, Tensor\nfrom typing import Callable, Tuple, Union\nfrom torch_scatter import scatter_add\nfrom torch_geometric.utils import add_remaining_self_loops\nfrom dgl._ffi.base import DGLError\nfrom typing import List, NamedTuple, Optional, Tuple, Union\n", "from typing import List, NamedTuple, Optional, Tuple, Union\n\n\n\nfrom torch.nn import Linear\nfrom torch_geometric.nn.conv import MessagePassing, GCNConv\n\nclass Adj(NamedTuple):\n    edge_index: torch.Tensor\n    edge_features: torch.Tensor\n    size: Tuple[int, int]\n    target_size: int\n\n    def to(self, *args, **kwargs):\n        return Adj(\n            self.edge_index.to(*args, **kwargs),\n            self.edge_features.to(*args, **kwargs),\n            self.size,\n            self.target_size\n        )", "    \n\n\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 g,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 activation,\n                 dropout):\n        super(MLP, self).__init__()\n        self.num_classes=num_classes\n        self.layers = nn.ModuleList()\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input layer\n        self.layers.append(nn.Linear(num_hidden, num_hidden, bias=True))\n        # hidden layers\n        for i in range(num_layers - 1):\n            self.layers.append(nn.Linear(num_hidden, num_hidden))\n        # output layer\n        self.layers.append(nn.Linear(num_hidden, num_classes))\n        for ly in self.layers:\n            nn.init.xavier_normal_(ly.weight, gain=1.414)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, features_list, e_feat):\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n\n        for i, layer in enumerate(self.layers):\n            encoded_embeddings=h\n            h = self.dropout(h)\n            h = layer(h)\n            h=F.relu(h) if i<len(self.layers) else h\n\n        return h,encoded_embeddings", "\n\n\nclass LabelPropagation(nn.Module):\n    r\"\"\"\n    Description\n    -----------\n    Introduced in `Learning from Labeled and Unlabeled Data with Label Propagation <https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3864&rep=rep1&type=pdf>`_\n    .. math::\n        \\mathbf{Y}^{\\prime} = \\alpha \\cdot \\mathbf{D}^{-1/2} \\mathbf{A}\n        \\mathbf{D}^{-1/2} \\mathbf{Y} + (1 - \\alpha) \\mathbf{Y},\n    where unlabeled data is inferred by labeled data via propagation.\n    Parameters\n    ----------\n        num_layers: int\n            The number of propagations.\n        alpha: float\n            The :math:`\\alpha` coefficient.\n    \"\"\"\n    def __init__(self, num_layers, alpha):\n        super(LabelPropagation, self).__init__()\n\n        self.num_layers = num_layers\n        self.alpha = alpha\n    \n    @torch.no_grad()\n    def forward(self, g, labels, mask,get_out=\"False\"):    # labels.shape=(number of nodes of type 0)  may contain false labels, therefore here the mask argument which provides the training nodes' idx is important\n        with g.local_scope():\n            if labels.dtype == torch.long:\n                labels = F.one_hot(labels.view(-1)).to(torch.float32)\n            y=torch.zeros((g.num_nodes(),labels.shape[1])).to(labels.device)\n            y[mask] = labels[mask]\n            \n            last = (1 - self.alpha) * y\n            degs = g.in_degrees().float().clamp(min=1)\n            norm = torch.pow(degs, -0.5).to(labels.device).unsqueeze(1)\n\n            for _ in range(self.num_layers):\n                # Assume the graphs to be undirected\n                g.ndata['h'] = y * norm\n                g.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h'))\n                y = last + self.alpha * g.ndata.pop('h') * norm\n                y=F.normalize(y,p=1,dim=1)   #normalize y by row with p-1-norm\n                y[mask] = labels[mask]\n                last = (1 - self.alpha) * y\n            \n            return y,None", "\n\n\n\n\n       \nclass slotGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha,\n                 num_ntype,\n                 eindexer, aggregator=\"SA\",predicted_by_slot=\"None\", addLogitsTrain=\"None\",  SAattDim=32,dataRecorder=None,targetTypeAttention=\"False\",vis_data_saver=None):\n        super(slotGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims]) \n        self.num_ntype=num_ntype\n        self.num_classes=num_classes\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        self.predicted_by_slot=predicted_by_slot \n        self.addLogitsTrain=addLogitsTrain \n        self.SAattDim=SAattDim \n        self.vis_data_saver=vis_data_saver\n        self.dataRecorder=dataRecorder\n        \n        if aggregator==\"SA\":\n            last_dim=num_classes\n                \n            self.macroLinear=nn.Linear(last_dim, self.SAattDim, bias=True);nn.init.xavier_normal_(self.macroLinear.weight, gain=1.414);nn.init.normal_(self.macroLinear.bias, std=1.414*math.sqrt(1/(self.macroLinear.bias.flatten().shape[0])))\n            self.macroSemanticVec=nn.Parameter(torch.FloatTensor(self.SAattDim,1));nn.init.normal_(self.macroSemanticVec,std=1)\n            \n \n \n        self.last_fc = nn.Parameter(th.FloatTensor(size=(num_classes*self.num_ntype, num_classes))) ;nn.init.xavier_normal_(self.last_fc, gain=1.414)\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer,inputhead=True, dataRecorder=dataRecorder))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n                num_hidden* heads[l-1] , num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer, dataRecorder=dataRecorder))\n        # output projection\n        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n            num_hidden* heads[-2] , num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer, dataRecorder=dataRecorder))\n        self.aggregator=aggregator\n        self.by_slot=[f\"by_slot_{nt}\" for nt in range(g.num_ntypes)]\n        assert aggregator in ([\"onedimconv\",\"average\",\"last_fc\",\"max\",\"SA\"]+self.by_slot)\n        if self.aggregator==\"onedimconv\":\n            self.nt_aggr=nn.Parameter(torch.FloatTensor(1,1,self.num_ntype,1));nn.init.normal_(self.nt_aggr,std=1) \n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n    def l2byslot(self,x):\n        \n        x=x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))\n        x=x / (torch.max(torch.norm(x, dim=2, keepdim=True), self.epsilon))\n        x=x.flatten(1)\n        return x\n\n    def forward(self, features_list,e_feat, get_out=\"False\"):\n        with record_function(\"model_forward\"):\n            encoded_embeddings=None\n            h = []\n            for nt_id,(fc, feature) in enumerate(zip(self.fc_list, features_list)):\n                nt_ft=fc(feature)\n                emsen_ft=torch.zeros([nt_ft.shape[0],nt_ft.shape[1]*self.num_ntype]).to(feature.device)\n                emsen_ft[:,nt_ft.shape[1]*nt_id:nt_ft.shape[1]*(nt_id+1)]=nt_ft\n                h.append(emsen_ft)   # the id is decided by the node types\n            h = torch.cat(h, 0)        #  num_nodes*(num_type*hidden_dim)\n            res_attn = None\n            for l in range(self.num_layers):\n                h, res_attn = self.gat_layers[l](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)   #num_nodes*num_heads*(num_ntype*hidden_dim)\n                h = h.flatten(1)    #num_nodes*(num_heads*num_ntype*hidden_dim)\n                encoded_embeddings=h\n            # output projection\n            logits, _ = self.gat_layers[-1](self.g, h, e_feat,get_out=get_out, res_attn=None)   #num_nodes*num_heads*num_ntype*hidden_dim\n        \n        if self.aggregator==\"SA\" :\n            logits=logits.squeeze(1)\n            logits=self.l2byslot(logits)\n            logits=logits.view(-1, self.num_ntype,int(logits.shape[1]/self.num_ntype))\n            \n            if \"getSlots\" in get_out:\n                self.logits=logits.detach()\n\n             \n            \n            slot_scores=(F.tanh(self.macroLinear(logits))@self.macroSemanticVec).mean(0,keepdim=True)  #num_slots\n            self.slot_scores=F.softmax(slot_scores,dim=1)\n            logits=(logits*self.slot_scores).sum(1)\n            if  self.dataRecorder[\"meta\"][\"getSAAttentionScore\"]==\"True\":\n                self.dataRecorder[\"data\"][f\"{self.dataRecorder['status']}_SAAttentionScore\"]=self.slot_scores.flatten().tolist() #count dist\n\n\n        #average across the ntype info\n        if self.predicted_by_slot!=\"None\" and self.training==False:\n            with record_function(\"predict_by_slot\"):\n                logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n                if self.predicted_by_slot==\"max\":\n                    if \"getMaxSlot\" in  get_out:\n                        maxSlotIndexesWithLabels=logits.max(2)[1].squeeze(1)\n                        logits_indexer=logits.max(2)[0].max(2)[1]\n                        self.maxSlotIndexes=torch.gather(maxSlotIndexesWithLabels,1,logits_indexer)\n                    logits=logits.max(2)[0]\n                elif self.predicted_by_slot==\"all\":\n                    if \"getSlots\" in get_out:\n                        self.logits=logits.detach()\n                    logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)  #average??\n\n                else:\n                    target_slot=int(self.predicted_by_slot)\n                    logits=logits[:,:,target_slot,:].squeeze(2)\n        else:\n            #with record_function(\"slot_aggregation\"):\n            if self.aggregator==\"average\":\n                logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)\n            elif self.aggregator==\"onedimconv\":\n                logits=(logits.view(-1,1,self.num_ntype,self.num_classes)*F.softmax(self.leaky_relu(self.nt_aggr),dim=2)).sum(2)\n            elif self.aggregator==\"last_fc\":\n                logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n                logits=logits.flatten(1)\n                logits=logits.matmul(self.last_fc).unsqueeze(1)\n            elif self.aggregator==\"max\":\n                logits=logits.view(-1,1,self.num_ntype,self.num_classes).max(2)[0]\n        \n            elif self.aggregator==\"None\":\n            \n                logits=logits.view(-1,1, self.num_ntype,self.num_classes).flatten(2)\n            elif  self.aggregator== \"SA\":\n                logits=logits.view(-1,1, 1,self.num_classes).flatten(2)\n\n\n\n            else:\n                raise NotImplementedError()\n        #average across the heads\n        ### logits = [num_nodes *  num_of_heads *num_classes]\n        self.logits_mean=logits.flatten().mean()\n        logits = logits.mean(1)\n        \n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n        return logits, encoded_embeddings    #hidden_logits", "\n\n\n\n\n\n  \nclass changedGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha,\n                 num_ntype,\n                 eindexer, ):\n        super(changedGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims]) \n        #self.ae_drop=nn.Dropout(feat_drop)\n        #if ae_layer==\"last_hidden\":\n            #self.lc_ae=nn.ModuleList([nn.Linear(num_hidden * heads[-2],num_hidden, bias=True),nn.Linear(num_hidden,num_ntype, bias=True)])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n                num_hidden * heads[l-1], num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer))\n        # output projection\n        self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n            num_hidden * heads[-2], num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype,  eindexer=eindexer))\n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\n    def forward(self, features_list, e_feat,get_out=\"False\"):\n\n        hidden_logits=None\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))   # the id is decided by the node types\n        h = torch.cat(h, 0)\n        res_attn = None\n        for l in range(self.num_layers):\n            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n            h = h.flatten(1)\n            #if self.ae_layer==\"last_hidden\":\n            encoded_embeddings=h\n            \"\"\"for i in range(len(self.lc_ae)):\n                _h=self.lc_ae[i](_h)\n                if i==0:\n                    _h=self.ae_drop(_h)\n                    _h=F.relu(_h)\n            hidden_logits=_h\"\"\"\n        # output projection\n        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=None)\n        logits = logits.mean(1)\n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n        return logits, encoded_embeddings    #hidden_logits", "\n\n\n\n\n\nclass myGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha, dataRecorder=None):\n        super(myGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n                num_hidden * heads[l-1], num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))\n        # output projection\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden * heads[-2], num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))\n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n        self.dataRecorder=dataRecorder\n\n    def forward(self, features_list, e_feat, get_out=\"False\"):\n\n\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))   # the id is decided by the node types\n        h = torch.cat(h, 0)\n        res_attn = None\n        for l in range(self.num_layers):\n            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n            h = h.flatten(1)\n            encoded_embeddings=h\n        # output projection\n        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=None)\n        logits = logits.mean(1)\n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n        return logits,encoded_embeddings", "\nclass RGAT(nn.Module):\n    def __init__(self,\n                 gs,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual ):\n        super(GAT, self).__init__()\n        self.gs = gs\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList([nn.ModuleList() for i in range(len(gs))])\n        self.activation = activation\n        self.weights = nn.Parameter(torch.zeros((len(in_dims), num_layers+1, len(gs))))\n        self.sm = nn.Softmax(2)\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        for i in range(len(gs)):\n            # input projection (no residual)\n            self.gat_layers[i].append(GATConv(\n                num_hidden, num_hidden, heads[0],\n                feat_drop, attn_drop, negative_slope, False, self.activation))\n            # hidden layers\n            for l in range(1, num_layers):\n                # due to multi-head, the in_dim = num_hidden * num_heads\n                self.gat_layers[i].append(GATConv(\n                    num_hidden * heads[l-1], num_hidden, heads[l],\n                    feat_drop, attn_drop, negative_slope, residual, self.activation))\n            # output projection\n            self.gat_layers[i].append(GATConv(\n                num_hidden * heads[-2], num_classes, heads[-1],\n                feat_drop, attn_drop, negative_slope, residual, None))\n\n    def forward(self, features_list):\n        nums = [feat.size(0) for feat in features_list]\n        weights = self.sm(self.weights)\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n        for l in range(self.num_layers):\n            out = []\n            for i in range(len(self.gs)):\n                out.append(torch.split(self.gat_layers[i][l](self.gs[i], h).flatten(1), nums))\n            h = []\n            for k in range(len(nums)):\n                tmp = []\n                for i in range(len(self.gs)):\n                    tmp.append(out[i][k]*weights[k,l,i])\n                h.append(sum(tmp))\n            h = torch.cat(h, 0)\n        out = []\n        for i in range(len(self.gs)):\n            out.append(torch.split(self.gat_layers[i][-1](self.gs[i], h).mean(1), nums))\n        logits = []\n        for k in range(len(nums)):\n            tmp = []\n            for i in range(len(self.gs)):\n                tmp.append(out[i][k]*weights[k,-1,i])\n            logits.append(sum(tmp))\n        logits = torch.cat(logits, 0)\n        return logits", "\nclass GAT(nn.Module):\n    def __init__(self,\n                 g,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,dataRecorder=None ):\n        super(GAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        self.dataRecorder=dataRecorder\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(GATConv(\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(GATConv(\n                num_hidden * heads[l-1], num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation))\n        # output projection\n        self.gat_layers.append(GATConv(\n            num_hidden * heads[-2], num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None))\n\n    def forward(self, features_list, e_feat,get_out=\"False\"):\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n        for l in range(self.num_layers):\n            h = self.gat_layers[l](self.g, h).flatten(1)\n            encoded_embeddings=h\n        # output projection\n        logits = self.gat_layers[-1](self.g, h).mean(1)\n        return logits,encoded_embeddings", "\n\nclass GCN(nn.Module):\n    def __init__(self,\n                 g,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 activation,\n                 dropout, dataRecorder=None):\n        super(GCN, self).__init__()\n        self.g = g\n        self.layers = nn.ModuleList()\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        self.dataRecorder=dataRecorder\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input layer\n        self.layers.append(GraphConv(num_hidden, num_hidden, activation=activation, weight=False))\n        # hidden layers\n        for i in range(num_layers - 1):\n            self.layers.append(GraphConv(num_hidden, num_hidden, activation=activation))\n        # output layer\n        self.layers.append(GraphConv(num_hidden, num_classes))\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, features_list, e_feat,get_out=\"False\"):\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n        for i, layer in enumerate(self.layers):\n            encoded_embeddings=h\n            h = self.dropout(h)\n            h = layer(self.g, h)\n        return h,encoded_embeddings", "\n\n\n\ndef gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n             add_self_loops=True, dtype=None,norm=\"D^{-1/2}(A+I)D^{-1/2}\",attn_drop=None):\n\n    fill_value = 2. if improved else 1.\n    num_nodes = int(edge_index.max()) + 1 if num_nodes is None else num_nodes\n    if edge_weight is None:\n        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n                                 device=edge_index.device)\n\n    if add_self_loops:\n        edge_index, tmp_edge_weight = add_remaining_self_loops(\n            edge_index, edge_weight, fill_value, num_nodes)\n        assert tmp_edge_weight is not None\n        edge_weight = tmp_edge_weight\n        \n    row, col = edge_index[0], edge_index[1]\n    deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n    if norm==\"D^{-1/2}(A+I)D^{-1/2}\":\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        return edge_index, attn_drop(deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col])\n    elif norm==\"D^{-1}(A+I)\":\n        deg_inv_sqrt = deg.pow_(-1)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        return edge_index, attn_drop(deg_inv_sqrt[row] * edge_weight )\n    elif norm==\"(A+I)D^{-1}\":\n        deg_inv_sqrt = deg.pow_(-1)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        return edge_index, attn_drop(deg_inv_sqrt[col] * edge_weight )\n    elif norm==\"(A+I)\":\n        return edge_index, attn_drop(edge_weight )\n    else:\n        raise Exception(f\"No specified norm: {norm}\")", "\n\n\n\n"]}
{"filename": "NC/methods/SlotGAT/pipeline_utils.py", "chunked_list": ["\nimport random\nimport queue\nimport time\nimport subprocess\nimport multiprocessing\nfrom threading import main_thread\nimport os\nimport pandas as pd\n", "import pandas as pd\n\nimport copy\n\ndef get_tasks_for_online(dataset_and_hypers):\n    pass\n\n\ndef get_tasks_linear_around(task_space,best_hyper):\n    tasks=[]\n    for param_in_space,param_values in task_space.items():\n        assert param_in_space in best_hyper.keys()\n        for param_value in param_values:\n            temp_t={}\n            #copy best hyper except specified param\n            for param_in_best,value_in_best in best_hyper.items():\n                if \"search_\" in param_in_best:\n                    \n                    temp_t[param_in_best]= f\"[{value_in_best}]\"  if param_in_best!=param_in_space else f\"[{param_value}]\" \n                else:\n                    \n                    temp_t[param_in_best]=value_in_best if param_in_best!=param_in_space else param_value\n            tasks.append(temp_t)\n\n    \n    return tasks", "def get_tasks_linear_around(task_space,best_hyper):\n    tasks=[]\n    for param_in_space,param_values in task_space.items():\n        assert param_in_space in best_hyper.keys()\n        for param_value in param_values:\n            temp_t={}\n            #copy best hyper except specified param\n            for param_in_best,value_in_best in best_hyper.items():\n                if \"search_\" in param_in_best:\n                    \n                    temp_t[param_in_best]= f\"[{value_in_best}]\"  if param_in_best!=param_in_space else f\"[{param_value}]\" \n                else:\n                    \n                    temp_t[param_in_best]=value_in_best if param_in_best!=param_in_space else param_value\n            tasks.append(temp_t)\n\n    \n    return tasks", "\n\ndef get_tasks(task_space):\n    tasks=[{}]\n    for k,v in task_space.items():\n        tasks=expand_task(tasks,k,v)\n    return tasks\n\ndef expand_task(tasks,k,v):\n    temp_tasks=[]\n    if type(v) is str and type(eval(v)) is list:\n        for value in eval(v):\n            if k.startswith(\"search_\"):\n                value=str([value])\n            for t in tasks:\n                temp_t=copy.deepcopy(t)\n                temp_t[k]=value\n                temp_tasks.append(temp_t)\n    elif type(v) is list:\n        for value in v:\n            for t in tasks:\n                temp_t=copy.deepcopy(t)\n                temp_t[k]=value\n                temp_tasks.append(temp_t)\n    else:\n        for t in tasks:\n            temp_t=copy.deepcopy(t)\n            temp_t[k]=v\n            temp_tasks.append(temp_t)\n    return temp_tasks", "def expand_task(tasks,k,v):\n    temp_tasks=[]\n    if type(v) is str and type(eval(v)) is list:\n        for value in eval(v):\n            if k.startswith(\"search_\"):\n                value=str([value])\n            for t in tasks:\n                temp_t=copy.deepcopy(t)\n                temp_t[k]=value\n                temp_tasks.append(temp_t)\n    elif type(v) is list:\n        for value in v:\n            for t in tasks:\n                temp_t=copy.deepcopy(t)\n                temp_t[k]=value\n                temp_tasks.append(temp_t)\n    else:\n        for t in tasks:\n            temp_t=copy.deepcopy(t)\n            temp_t[k]=v\n            temp_tasks.append(temp_t)\n    return temp_tasks", "\n\n \n\ndef proc_yes(yes,args_dict):\n    temp_yes=[]\n    for name in yes:\n        temp_yes.append(f\"{name}_{args_dict[name]}\")\n    return temp_yes\n\ndef get_best_hypers_from_csv(dataset,net,yes,no,metric=\"2_valAcc\"):\n    print(f\"yes: {yes}, no: {no}\")\n    #get search best hypers\n    fns=[]\n    for root, dirs, files in os.walk(\"./log\", topdown=False):\n        for name in files:\n            FLAG=1\n            if \"old\" in root:\n                continue\n            if \".py\" in name:\n                continue\n            if \".txt\" in name:\n                continue\n            if \".csv\" not in name:\n                continue\n            for n in no:\n                if n in name:\n                    FLAG=0\n            for y in yes:\n                if y not in name:\n                    FLAG=0\n            if FLAG==0:\n                continue\n\n            if dataset in name:\n                name0=name.replace(\"_GTN\",\"\",1) if \"kdd\" not in name else name\n                if net in name0 :\n\n                    fn=os.path.join(root, name)\n                    fns.append(fn)\n    score_max=0\n    print(fns)\n    if fns==[]:\n        raise Exception\n    for fn in fns:\n\n        param_data=pd.read_csv(fn)\n        param_data_sorted=param_data.sort_values(by=metric,ascending=False).head(1)\n        #print(param_data_sorted.columns)\n        param_mapping={\"1_Lr\":\"search_lr\",\n        \"1_Wd\":\"search_weight_decay\",\n        \"1_featType\":\"feats-type\",\n        \"1_hiddenDim\":\"search_hidden_dim\",\n        \"1_numLayers\":\"search_num_layers\",\n        \"1_numOfHeads\":\"search_num_heads\",}\n        score=param_data_sorted[metric].iloc[0]\n        if score>score_max:\n            print(   f\"score:{score}\\t {param_data_sorted} bigger than current score {score_max} \"  )\n            best_hypers={}\n            score_max=score\n            best_param_data_sorted=param_data_sorted\n            for col_name in param_data_sorted.columns:\n                if col_name.startswith(\"1_\"):\n                    if param_mapping[col_name].startswith(\"search_\"):\n                        best_hypers[param_mapping[col_name]]=f\"[{param_data_sorted[col_name].iloc[0]}]\"\n                    else:\n                        best_hypers[param_mapping[col_name]]=f\"{param_data_sorted[col_name].iloc[0]}\"\n        print(f\"Best Score:{score_max}\\t {best_param_data_sorted}\")\n        \n\n    return best_hypers", "\ndef get_best_hypers_from_csv(dataset,net,yes,no,metric=\"2_valAcc\"):\n    print(f\"yes: {yes}, no: {no}\")\n    #get search best hypers\n    fns=[]\n    for root, dirs, files in os.walk(\"./log\", topdown=False):\n        for name in files:\n            FLAG=1\n            if \"old\" in root:\n                continue\n            if \".py\" in name:\n                continue\n            if \".txt\" in name:\n                continue\n            if \".csv\" not in name:\n                continue\n            for n in no:\n                if n in name:\n                    FLAG=0\n            for y in yes:\n                if y not in name:\n                    FLAG=0\n            if FLAG==0:\n                continue\n\n            if dataset in name:\n                name0=name.replace(\"_GTN\",\"\",1) if \"kdd\" not in name else name\n                if net in name0 :\n\n                    fn=os.path.join(root, name)\n                    fns.append(fn)\n    score_max=0\n    print(fns)\n    if fns==[]:\n        raise Exception\n    for fn in fns:\n\n        param_data=pd.read_csv(fn)\n        param_data_sorted=param_data.sort_values(by=metric,ascending=False).head(1)\n        #print(param_data_sorted.columns)\n        param_mapping={\"1_Lr\":\"search_lr\",\n        \"1_Wd\":\"search_weight_decay\",\n        \"1_featType\":\"feats-type\",\n        \"1_hiddenDim\":\"search_hidden_dim\",\n        \"1_numLayers\":\"search_num_layers\",\n        \"1_numOfHeads\":\"search_num_heads\",}\n        score=param_data_sorted[metric].iloc[0]\n        if score>score_max:\n            print(   f\"score:{score}\\t {param_data_sorted} bigger than current score {score_max} \"  )\n            best_hypers={}\n            score_max=score\n            best_param_data_sorted=param_data_sorted\n            for col_name in param_data_sorted.columns:\n                if col_name.startswith(\"1_\"):\n                    if param_mapping[col_name].startswith(\"search_\"):\n                        best_hypers[param_mapping[col_name]]=f\"[{param_data_sorted[col_name].iloc[0]}]\"\n                    else:\n                        best_hypers[param_mapping[col_name]]=f\"{param_data_sorted[col_name].iloc[0]}\"\n        print(f\"Best Score:{score_max}\\t {best_param_data_sorted}\")\n        \n\n    return best_hypers", "\ndef get_best_hypers(dataset,net,yes,no):\n    print(f\"yes: {yes}, no: {no}\")\n    #get search best hypers\n    best={}\n    fns=[]\n    for root, dirs, files in os.walk(\"./log\", topdown=False):\n        for name in files:\n            FLAG=1\n            if \"old\" in root:\n                continue\n            if \".py\" in name:\n                continue\n            if \".txt\" in name:\n                continue\n            for n in no:\n                if n in name:\n                    FLAG=0\n            for y in yes:\n                if y not in name:\n                    FLAG=0\n            if FLAG==0:\n                continue\n\n            if dataset in name:\n                name0=name.replace(\"_GTN\",\"\",1) if \"kdd\" not in name else name\n                if net in name0 :\n\n                    fn=os.path.join(root, name)\n                    fns.append(fn)\n    score_max=0\n    print(fns)\n    if fns==[]:\n        raise Exception\n    for fn in fns:\n        path=fn\n        FLAG0=False\n        FLAG1=False\n        with open(fn,\"r\") as f:\n            for line in f:\n                if \"Best trial\" in line and FLAG0==False:\n                    FLAG0=True\n                    FLAG1=False\n                    continue\n                if FLAG0==True:\n                    if \"Value\" in line:\n                        _,score=line.strip(\"\\n\").replace(\" \",\"\").split(\":\")\n                        score=float(score)\n                        continue\n                    if \"Params:\" in line:\n                        FLAG1=True\n                        count=0\n                        continue\n                if FLAG1==True and score>=score_max and \"    \" in line and count<=5:\n\n                    param,value=line.strip(\"\\n\").replace(\" \",\"\").split(\":\")\n                    best[param]=value\n                    score_max=score\n                    FLAG0=False\n                    count+=1\n        print(best)\n        best_hypers={}\n        for key in best.keys():\n            best_hypers[\"search_\"+key]=f\"\"\"[{best[key]}]\"\"\"\n    return best_hypers", "\n\nclass Run( multiprocessing.Process):\n    def __init__(self,task,pool=0,idx=0,tc=0,start_time=0):\n        super().__init__()\n        self.task=task\n        self.log=os.path.join(task['study_name'])\n        self.idx=idx\n        self.pool=pool\n        self.device=None\n        self.tc=tc\n        self.start_time=start_time\n        #self.pbar=pbar\n    def run(self):\n        print(f\"{'*'*10} study  {self.log} no.{self.idx} waiting for device\")\n        count=0\n        device_units=[]\n        while True:\n            if len(device_units)>0:\n                try:\n                    unit=self.pool.get(timeout=10*random.random())\n                except queue.Empty:\n                    for unit in device_units:\n                            self.pool.put(unit)\n                    print(f\"Hold {str(device_units)} and waiting for too long! Throw back and go to sleep\")\n                    time.sleep(10*random.random())\n                    device_units=[]\n                    count=0\n                    continue\n            else:\n                unit=self.pool.get()\n            if len(device_units)>0:  # consistency check\n                if unit[0]!=device_units[-1][0]:\n                    print(f\"Get {str(device_units)} and {unit} not consistent devices and throw back it\")\n                    self.pool.put(unit)\n                    time.sleep(10*random.random())\n                    continue\n            count+=1\n            device_units.append(unit)\n            if count==self.task['cost']:\n                break\n\n\n        print(f\"{'-'*10}  study  {self.log} no.{self.idx} get the devices {str(device_units)} and start working\")\n        self.device=device_units[0][0]\n        try:\n            exit_command=get_command_from_argsDict(self.task,self.device,self.idx)\n            \n            print(f\"running: {exit_command}\")\n            subprocess.run(exit_command,shell=True)\n        finally:\n            for unit in device_units:\n                self.pool.put(unit)\n            #localtime = time.asctime( time.localtime(time.time()) )\n        \n        end_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        print(f\"Start time: {self.start_time}\\nEnd time: {end_time}\\nwith {self.idx}/{self.tc} tasks\")\n\n        print(f\"  {'<'*10} end  study  {self.log} no.{self.idx} of command \")", "\n\n\ndef get_command_from_argsDict(args_dict,gpu,idx):\n    command='python -W ignore run_analysis.py  '\n    for key in args_dict.keys():\n        command+=f\" --{key} {args_dict[key]} \"\n\n\n    command+=f\" --gpu {gpu} \"\n    if os.name!=\"nt\":\n        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n    return command", "\n\n\ndef run_command_in_parallel(args_dict,gpus,worker_num):\n\n\n    command='python -W ignore run_dist.py  '\n    for key in args_dict.keys():\n        command+=f\" --{key} {args_dict[key]} \"\n\n\n    process_queue=[]\n    for gpu in gpus:\n        \n        command+=f\" --gpu {gpu} \"\n        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n        for _ in range(worker_num):\n            \n            print(f\"running: {command}\")\n            p=Run(command)\n            p.daemon=True\n            p.start()\n            process_queue.append(p)\n            time.sleep(5)\n\n    for p in process_queue:\n        p.join()", "\n\n\n\ndef config_study_name(prefix,specified_args,extract_dict):\n    study_name=prefix\n    for k in specified_args:\n        v=extract_dict[k]\n        study_name+=f\"_{k}_{v}\"\n    if study_name[0]==\"_\":\n        study_name=study_name.replace(\"_\",\"\",1)\n    study_storage=f\"sqlite:///db/{study_name}.db\"\n    return study_name,study_storage", ""]}
{"filename": "NC/methods/SlotGAT/run_analysis.py", "chunked_list": ["from re import I\nimport sys\nimport pickle\nfrom numpy.core.numeric import identity\nsys.path.append('../../')\nimport time\nimport argparse\nimport os\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nfrom utils.pytorchtools import EarlyStopping\nfrom utils.data import load_data\nfrom utils.tools import func_args_parse,single_feat_net,vis_data_collector,blank_profile,writeIntoCsvLogger,count_torch_tensor\n#from utils.tools import index_generator, evaluate_results_nc, parse_minibatch\nfrom GNN import myGAT,changedGAT,GAT,GCN,slotGAT,LabelPropagation,MLP", "#from utils.tools import index_generator, evaluate_results_nc, parse_minibatch\nfrom GNN import myGAT,changedGAT,GAT,GCN,slotGAT,LabelPropagation,MLP\nimport dgl\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom torch.profiler import tensorboard_trace_handler\nfrom sklearn.manifold import TSNE\n#import wandb\nimport threading\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nimport json\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    np.random.seed(seed)  # Numpy module.\n    random.seed(seed)  # Python random module.\n\n    torch.use_deterministic_algorithms(True,warn_only=True)\n    torch.backends.cudnn.enabled = False \n    torch.backends.cudnn.benchmark = False\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n    os.environ['PYTHONHASHSEED'] = str(seed)", "set_seed(2022)\nfeature_usage_dict={0:\"loaded features\",\n1:\"only target node features (zero vec for others)\",\n2:\"only target node features (id vec for others)\",\n3:\"all id vec. Default is 2\",\n4:\"only term features (id vec for others)\",\n5:\"only term features (zero vec for others)\",\n}\nap = argparse.ArgumentParser(description='MRGNN testing for the DBLP dataset')\nap.add_argument('--feats-type', type=int, default=0,", "ap = argparse.ArgumentParser(description='MRGNN testing for the DBLP dataset')\nap.add_argument('--feats-type', type=int, default=0,\n                help='Type of the node features used. ' +\n                        '0 - loaded features; ' +\n                        '1 - only target node features (zero vec for others); ' +\n                        '2 - only target node features (id vec for others); ' +\n                        '3 - all id vec. Default is 2;' +\n                    '4 - only term features (id vec for others);' + \n                    '5 - only term features (zero vec for others).')\n", "                    '5 - only term features (zero vec for others).')\n\nap.add_argument('--use_trained', type=str, default=\"False\")\nap.add_argument('--trained_dir', type=str, default=\"outputs\")\nap.add_argument('--save_trained', type=str, default=\"False\")\nap.add_argument('--save_dir', type=str, default=\"outputs\")\n#ap.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\nap.add_argument('--num-heads', type=int, default=8, help='Number of the attention heads. Default is 8.')\nap.add_argument('--epoch', type=int, default=300, help='Number of epochs.')\nap.add_argument('--patience', type=int, default=30, help='Patience.')", "ap.add_argument('--epoch', type=int, default=300, help='Number of epochs.')\nap.add_argument('--patience', type=int, default=30, help='Patience.')\nap.add_argument('--repeat', type=int, default=30, help='Repeat the training and testing for N times. Default is 1.')\n#ap.add_argument('--num-layers', type=int, default=2)\n#ap.add_argument('--lr', type=float, default=5e-4)\nap.add_argument('--dropout_feat', type=float, default=0.5)\nap.add_argument('--dropout_attn', type=float, default=0.5)\n#ap.add_argument('--weight-decay', type=float, default=1e-4)\nap.add_argument('--slope', type=float, default=0.05)\nap.add_argument('--residual', type=str, default=\"True\")", "ap.add_argument('--slope', type=float, default=0.05)\nap.add_argument('--residual', type=str, default=\"True\")\nap.add_argument('--dataset', type=str)\nap.add_argument('--edge-feats', type=int, default=64)\nap.add_argument('--run', type=int, default=1)\nap.add_argument('--cost', type=int, default=1)\nap.add_argument('--gpu', type=str, default=\"0\")\n#ap.add_argument('--hiddens', type=str, default=\"64_32\")\nap.add_argument('--activation', type=str, default=\"elu\")\nap.add_argument('--bias', type=str, default=\"true\")", "ap.add_argument('--activation', type=str, default=\"elu\")\nap.add_argument('--bias', type=str, default=\"true\")\nap.add_argument('--net', type=str, default=\"myGAT\")\nap.add_argument('--L2_norm', type=str, default=\"False\")\nap.add_argument('--task_property', type=str, default=\"notSpecified\")\nap.add_argument('--study_name', type=str, default=\"temp\")\nap.add_argument('--verbose', type=str, default=\"False\") \nap.add_argument('--slot_aggregator', type=str, default=\"None\") \nap.add_argument('--SAattDim', type=int, default=3) \nap.add_argument('--LP_alpha', type=float, default=0.5)  #1,0.99,0.5", "ap.add_argument('--SAattDim', type=int, default=3) \nap.add_argument('--LP_alpha', type=float, default=0.5)  #1,0.99,0.5\nap.add_argument('--get_out', default=\"\")  \n#ap.add_argument('--get_out_tasks', default=\"\")  \nap.add_argument('--profile', default=\"False\")  \nap.add_argument('--get_out_tsne', default=\"False\")  \nap.add_argument('--normalize', default=\"True\")  \n# to search\nap.add_argument('--search_num_heads', type=str, default=\"[8]\")\nap.add_argument('--search_lr', type=str, default=\"[1e-3,5e-4,1e-4]\")", "ap.add_argument('--search_num_heads', type=str, default=\"[8]\")\nap.add_argument('--search_lr', type=str, default=\"[1e-3,5e-4,1e-4]\")\nap.add_argument('--search_weight_decay', type=str, default=\"[5e-4,1e-4,1e-5]\")\nap.add_argument('--search_hidden_dim', type=str, default=\"[64,128]\")\nap.add_argument('--search_num_layers', type=str, default=\"[2]\")\n\ntorch.set_num_threads(4)\nargs = ap.parse_args()\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\ntry:\n    torch.cuda.set_device(int(args.gpu))\nexcept :\n    pass", "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\ntry:\n    torch.cuda.set_device(int(args.gpu))\nexcept :\n    pass\n\ndef sp_to_spt(mat):\n    coo = mat.tocoo()\n    values = coo.data\n    indices = np.vstack((coo.row, coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = coo.shape\n\n    return torch.sparse.FloatTensor(i, v, torch.Size(shape))", "\ndef mat2tensor(mat):\n    if type(mat) is np.ndarray:\n        return torch.from_numpy(mat).type(torch.FloatTensor)\n    return sp_to_spt(mat)\n\ndef run_model_DBLP(trial=None):\n    #data preparation\n\n    num_heads=int(eval(args.search_num_heads)[0]);assert len(eval(args.search_num_heads))==1\n    lr=float(eval(args.search_lr)[0]);assert len(eval(args.search_lr))==1\n    weight_decay=float(eval(args.search_weight_decay)[0]);assert len(eval(args.search_weight_decay))==1\n    hidden_dim=int(eval(args.search_hidden_dim)[0]);assert len(eval(args.search_hidden_dim))==1\n    num_layers=int(eval(args.search_num_layers)[0]);assert len(eval(args.search_num_layers))==1\n    \n    if True:\n\n        get_out=args.get_out.split(\"_\")\n        getSAAttentionScore=\"True\" if \"getSAAttentionScore\" in args.get_out else \"False\"\n        dataRecorder={\"meta\":{\n            \"getSAAttentionScore\":getSAAttentionScore,\n        },\"data\":{},\"status\":\"None\"}\n        feats_type = args.feats_type\n        slot_aggregator=args.slot_aggregator   \n        \n        multi_labels=True if args.dataset in [\"IMDB\",\"IMDB_hgb\"] else False  #imdb online\n        dl_mode='multi' if multi_labels else 'bi'\n        features_list, adjM, labels, train_val_test_idx, dl = load_data(args.dataset,multi_labels=multi_labels)\n        class_num=max(labels)+1 if not multi_labels else len(labels[0])   \n        vis_data_saver=vis_data_collector() \n        running_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        print(running_time)\n        device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n        features_list = [mat2tensor(features).to(device) for features in features_list] \n        if feats_type == 0:\n            in_dims = [features.shape[1] for features in features_list]\n        elif feats_type == 1 or feats_type == 5:\n            save = 0 if feats_type == 1 else 2\n            in_dims = []#[features_list[0].shape[1]] + [10] * (len(features_list) - 1)\n            for i in range(0, len(features_list)):\n                if i == save:\n                    in_dims.append(features_list[i].shape[1])\n                else:\n                    in_dims.append(10)\n                    features_list[i] = torch.zeros((features_list[i].shape[0], 10)).to(device)\n        elif feats_type == 2 or feats_type == 4:\n            save = feats_type - 2\n            in_dims = [features.shape[0] for features in features_list]\n            for i in range(0, len(features_list)):\n                if i == save:\n                    in_dims[i] = features_list[i].shape[1]\n                    continue\n                dim = features_list[i].shape[0]\n                indices = np.vstack((np.arange(dim), np.arange(dim)))\n                indices = torch.LongTensor(indices)\n                values = torch.FloatTensor(np.ones(dim))\n                features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n        elif feats_type == 3:\n            in_dims = [features.shape[0] for features in features_list]\n            for i in range(len(features_list)):\n                dim = features_list[i].shape[0]\n                indices = np.vstack((np.arange(dim), np.arange(dim)))\n                indices = torch.LongTensor(indices)\n                values = torch.FloatTensor(np.ones(dim))\n                features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n        labels = torch.LongTensor(labels).to(device)  if not multi_labels else  torch.FloatTensor(labels).to(device)\n        train_idx = train_val_test_idx['train_idx']\n        train_idx = np.sort(train_idx)\n        val_idx = train_val_test_idx['val_idx']\n        val_idx = np.sort(val_idx)\n        test_idx = train_val_test_idx['test_idx']\n        test_idx = np.sort(test_idx)\n        edge2type = {}\n        for k in dl.links['data']:\n            for u,v in zip(*dl.links['data'][k].nonzero()):\n                edge2type[(u,v)] = k\n        count_self=0\n        for i in range(dl.nodes['total']):\n            FLAG=0\n            if (i,i) not in edge2type:\n                edge2type[(i,i)] = len(dl.links['count'])\n                FLAG=1\n        count_self+=FLAG\n        count_reverse=0\n        for k in dl.links['data']:\n            FLAG=0\n            for u,v in zip(*dl.links['data'][k].nonzero()):\n                if (v,u) not in edge2type:\n                    edge2type[(v,u)] = count_reverse+1+len(dl.links['count'])\n                    FLAG=1\n            count_reverse+=FLAG\n        num_etype=len(dl.links['count'])+count_self+count_reverse\n        g = dgl.DGLGraph(adjM+(adjM.T))\n        g = dgl.remove_self_loop(g)\n        g = dgl.add_self_loop(g)\n        g = g.to(device)\n        e_feat = []\n        count=0\n        count_mappings={}\n        counted_dict={}\n        eid=0\n        etype_ids={}\n        g_=g.cpu()\n        for u, v in tqdm(zip(*g_.edges())):\n            u =u.item() #u.cpu().item()\n            v =v.item() #v.cpu().item()\n            if not counted_dict.setdefault(edge2type[(u,v)],False) :\n                count_mappings[edge2type[(u,v)]]=count\n                counted_dict[edge2type[(u,v)]]=True\n                count+=1\n            e_feat.append(count_mappings[edge2type[(u,v)]])\n            if edge2type[(u,v)] in etype_ids.keys():\n                etype_ids[edge2type[(u,v)]].append(eid)\n            else:\n                etype_ids[edge2type[(u,v)]]=[eid]\n            eid+=1\n        e_feat = torch.tensor(e_feat, dtype=torch.long).to(device)\n        g.etype_ids=etype_ids\n        reduc=\"mean\"\n        loss = nn.BCELoss(reduction=reduc) if multi_labels else F.nll_loss\n        loss_val = nn.BCELoss() if multi_labels else F.nll_loss\n        g.edge_type_indexer=F.one_hot(e_feat).to(device)\n        num_ntypes=len(features_list)\n        num_nodes=dl.nodes['total']\n        g.node_idx_by_ntype=[]\n        g.num_ntypes=num_ntypes\n        g.node_ntype_indexer=torch.zeros(num_nodes,num_ntypes).to(device)\n        ntype_dims=[]\n        idx_count=0\n        ntype_count=0\n        for feature in features_list:\n            temp=[]\n            for _ in feature:\n                temp.append(idx_count)\n                g.node_ntype_indexer[idx_count][ntype_count]=1\n                idx_count+=1\n\n            g.node_idx_by_ntype.append(temp)\n            ntype_dims.append(feature.shape[1])\n            ntype_count+=1 \n        eindexer=None\n    LP_alpha=args.LP_alpha\n    ma_F1s=[]\n    mi_F1s=[]\n    val_accs=[]\n    val_losses_neg=[]\n    toCsvRepetition=[]\n    for re in range(args.repeat):\n        training_times=[]\n        inference_times=[]\n        #re-id the train-validation in each repeat\n        tr_len,val_len=len(train_idx),len(val_idx)\n        total_idx=np.concatenate([train_idx,val_idx])\n        total_idx=np.random.permutation(total_idx)\n        train_idx,val_idx=total_idx[0:tr_len],total_idx[tr_len:tr_len+val_len] \n        net_wrapper=single_feat_net\n        t_re0=time.time()\n        num_classes = dl.labels_train['num_classes']\n        heads = [num_heads] * num_layers + [1]\n        if args.net=='myGAT':\n            GNN=myGAT\n            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,dataRecorder = dataRecorder)\n        elif args.net=='changedGAT':\n            GNN=changedGAT\n            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,num_ntype=num_ntypes, eindexer=eindexer, dataRecorder = dataRecorder)\n        elif args.net=='slotGAT':\n            GNN=slotGAT\n            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,num_ntype=num_ntypes, eindexer=eindexer, aggregator=slot_aggregator ,SAattDim=args.SAattDim,dataRecorder=dataRecorder,vis_data_saver=vis_data_saver)\n        elif args.net=='GAT':\n            GNN=GAT\n            fargs,fkargs=func_args_parse(g, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, dataRecorder = dataRecorder)\n        elif args.net=='GCN':\n            GNN=GCN\n            fargs,fkargs=func_args_parse(g, in_dims, hidden_dim, num_classes, num_layers, F.relu, args.dropout_feat, dataRecorder = dataRecorder)\n        elif args.net=='LabelPropagation':\n            #net=LabelPropagation(num_layers, LP_alpha)\n            GNN=LabelPropagation\n            fargs,fkargs=func_args_parse(num_layers, LP_alpha)\n        elif args.net=='MLP':\n            GNN=MLP\n            fargs,fkargs=func_args_parse(g,in_dims,hidden_dim,num_classes,num_layers,F.relu,args.dropout_feat)\n        else:\n            raise NotImplementedError()\n\n        net=net_wrapper(GNN,*fargs,**fkargs)\n            \n        print(f\"model using: {net.__class__.__name__}\")  if args.verbose==\"True\" else None\n        net.to(device)\n \n\n        if args.use_trained==\"True\":\n            ckp_fname=os.path.join(args.trained_dir,args.net,args.dataset,str(re),\"model.pt\")\n        else:\n            if args.net==\"LabelPropagation\":\n                pass\n            else:\n                optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n            net.train()\n            if args.save_trained==\"True\":\n                # files in save_dir should be considered ***important***\n                ckp_fname=os.path.join(args.save_dir,args.net,args.dataset,str(re),\"model.pt\")\n                os.makedirs(os.path.dirname(ckp_fname),exist_ok=True)\n            else:\n                # files in checkpoint could be considered to be deleted\n                t=time.localtime()\n                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n                ckp_dname=os.path.join('checkpoint',str_t)\n                os.mkdir(ckp_dname)\n                ckp_fname=os.path.join(ckp_dname,'checkpoint_{}_{}_re_{}_feat_{}_heads_{}_{}.pt'.format(args.dataset, num_layers,re,args.feats_type,num_heads,net.__class__.__name__))\n            early_stopping = EarlyStopping(patience=args.patience, verbose=False, save_path=ckp_fname)\n                \n            \n            if args.profile==\"True\":\n                profile_func=profile\n            elif args.profile==\"False\":\n                profile_func=blank_profile\n            with profile_func(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,schedule=torch.profiler.schedule(\n                    wait=2,\n                    warmup=2,\n                    active=6,\n                    repeat=1),on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler/trace_\"+args.study_name)) as prof:\n                for epoch in range(args.epoch):\n                    training_time_start=time.time()\n                    if args.net==\"LabelPropagation\"  :\n                        continue\n                    t_0_start = time.time()\n                    # training\n                    net.train()\n                    with record_function(\"model_inference\"):\n                        net.dataRecorder[\"status\"]=\"Training\"\n                        logits,_ = net(features_list, e_feat) \n                        net.dataRecorder[\"status\"]=\"None\"\n                    logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n                    train_loss = loss(logp[train_idx], labels[train_idx])# if not multi_labels else loss(logp[train_idx], labels[train_idx])\n                    # autograd\n                    optimizer.zero_grad()\n                    with record_function(\"model_backward\"):\n                        train_loss.backward()\n                        optimizer.step()\n                    t_0_end = time.time()\n                    training_time_end=time.time()\n                    training_times.append(training_time_end-training_time_start)\n                    t_1_start = time.time()\n                    #validation\n                    net.eval()\n                    with torch.no_grad():\n                        net.dataRecorder[\"status\"]=\"Validation\"\n                        logits,_ = net(features_list, e_feat)\n                        net.dataRecorder[\"status\"]=\"None\"\n                        logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n                        val_loss = loss_val(logp[val_idx], labels[val_idx])\n                    t_1_end = time.time()\n                    # print validation info\n                    print('Epoch {:05d} | Train_Loss: {:.4f} | train Time: {:.4f} | Val_Loss {:.4f} | val Time(s) {:.4f}'.format(\n                        epoch, train_loss.item(), t_0_end-t_0_start,val_loss.item(), t_1_end - t_1_start)) if args.verbose==\"True\" else None\n                    # early stopping\n                    early_stopping(val_loss, net)\n                    if epoch>args.epoch/2 and early_stopping.early_stop:\n                        #print('Early stopping!')\n                        break\n                    prof.step()\n        # validation with evaluate_results_nc\n        if args.net!=\"LabelPropagation\":\n            net.load_state_dict(torch.load(ckp_fname),strict=False)\n        net.eval()\n        with torch.no_grad():            \n            net.dataRecorder[\"status\"]=\"FinalValidation\"\n            infer_time_start=time.time()\n            logits,_ = net(features_list, e_feat,get_out=get_out) if args.net!=\"LabelPropagation\"  else net(g,labels,mask=train_idx)\n            logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n            val_loss = loss_val(logp[val_idx], labels[val_idx])\n            net.dataRecorder[\"status\"]=\"None\"\n            val_logits = logits[val_idx]\n            pred=val_logits.argmax(axis=1) if not multi_labels else (val_logits>0).int()\n            all_pred=logits.argmax(axis=1) if not multi_labels else (logits>0).int()           \n        val_results=dl.evaluate_by_group(all_pred,val_idx,train=True,mode=dl_mode)\n        test_results=dl.evaluate_by_group(all_pred,test_idx,train=False,mode=dl_mode)\n        infer_time_end=time.time()\n        inference_times.append(infer_time_end-infer_time_start)\n        vis_data_saver.collect_in_run(test_results[\"micro-f1\"],\"micro-f1\",re=re)\n        vis_data_saver.collect_in_run(test_results[\"macro-f1\"],\"macro-f1\",re=re)\n        training_time_mean=sum(training_times)/(len(training_times)+1e-10)\n        training_time_total=sum(training_times)\n        inference_time_mean=sum(inference_times)/(len(inference_times)+1e-10)\n        inference_time_total=sum(inference_times)\n        global peak_gpu_memory\n        peak_gpu_memory_by_torch=torch.cuda.max_memory_allocated()/1024/1024/1024\n        toCsv={ \"0_dataset\":args.dataset,\n                \"0_net\":args.net,\n                \"0_aggregator\":args.slot_aggregator,\n                \"0_getout\":args.get_out,\n                \"1_featType\":feats_type,\n                \"1_numOfEpoch\":args.epoch,\n                \"1_numLayers\":num_layers,\n                \"1_hiddenDim\":hidden_dim,\n                \"1_SAattDim\":args.SAattDim,\n                \"1_numOfHeads\":num_heads,\n                \"1_Lr\":lr,\n                \"1_Wd\":weight_decay,\n                \"1_dropoutFeat\":args.dropout_feat,\n                \"1_dropoutAttn\":args.dropout_attn,\n                \"1_L2_norm\":args.L2_norm,\n                \"2_valAcc_mean\":val_results[\"acc\"],\n                \"2_valAcc_std\":val_results[\"acc\"],\n                \"2_valMiPre_mean\":val_results[\"micro-pre\"],\n                \"2_valMiPre_std\":val_results[\"micro-pre\"],\n                \"2_valMaPre_mean\":val_results[\"macro-pre\"],\n                \"2_valMaPre_std\":val_results[\"macro-pre\"],\n                \"2_valMiRec_mean\":val_results[\"micro-rec\"],\n                \"2_valMiRec_std\":val_results[\"micro-rec\"],\n                \"2_valMaRec_mean\":val_results[\"macro-rec\"],\n                \"2_valMaRec_std\":val_results[\"macro-rec\"],\n                \"2_valMiF1_mean\":val_results[\"micro-f1\"],\n                \"2_valMiF1_std\":val_results[\"micro-f1\"],\n                \"2_valMaF1_mean\":val_results[\"macro-f1\"],\n                \"2_valMaF1_std\":val_results[\"macro-f1\"],\n                \"3_testAcc_mean\":test_results[\"acc\"],\n                \"3_testAcc_std\":test_results[\"acc\"],\n                \"3_testMiPre_mean\":test_results[\"micro-pre\"],\n                \"3_testMiPre_std\":test_results[\"micro-pre\"],\n                \"3_testMaPre_mean\":test_results[\"macro-pre\"],\n                \"3_testMaPre_std\":test_results[\"macro-pre\"],\n                \"3_testMiRec_mean\":test_results[\"micro-rec\"],\n                \"3_testMiRec_std\":test_results[\"micro-rec\"],\n                \"3_testMaRec_mean\":test_results[\"macro-rec\"],\n                \"3_testMaRec_std\":test_results[\"macro-rec\"],\n                \"3_testMiF1_mean\":test_results[\"micro-f1\"],\n                \"3_testMiF1_std\":test_results[\"micro-f1\"],\n                \"3_testMaF1_mean\":test_results[\"macro-f1\"],\n                \"3_testMaF1_std\":test_results[\"macro-f1\"], \n                \"3_training_time_per_epoch_mean\":training_time_mean,\n                \"3_training_time_per_epoch_total\":training_time_total,\n                \"3_inference_time_per_epoch_mean\":inference_time_mean,\n                \"3_inference_time_per_epoch_total\":inference_time_total,\n                \"3_peak_memory\":peak_gpu_memory_by_torch,\n                }\n        toCsvRepetition.append(toCsv)\n        if not multi_labels:\n            val_acc=val_results[\"acc\"]\n            val_accs.append(val_acc)\n            score=sum(val_accs)/len(val_accs)\n        else:\n            val_losses_neg.append(1/(1+val_loss))\n            score=sum(val_losses_neg)/len(val_losses_neg)\n            \n        # testing with evaluate_results_nc\n        if args.net!=\"LabelPropagation\":\n            net.load_state_dict(torch.load(ckp_fname),strict=False) \n        net.eval()\n        test_logits = []\n        \n        with torch.no_grad():\n            net.dataRecorder[\"status\"]=\"FinalTesting\"\n            logits,_ = net(features_list, e_feat,get_out=get_out) if args.net!=\"LabelPropagation\"  else net(g,labels,mask=train_idx)\n            net.dataRecorder[\"status\"]=\"None\" \n            test_logits = logits[test_idx]\n            pred = test_logits.cpu().numpy().argmax(axis=1) if not multi_labels else (test_logits.cpu().numpy()>0).astype(int)\n            onehot = np.eye(num_classes, dtype=np.int32)\n            pred = onehot[pred] if not multi_labels else  pred\n            d=dl.evaluate(pred,mode=dl_mode)\n            print(d) if args.verbose==\"True\" else None\n            \n        ma_F1s.append(d[\"macro-f1\"])\n        mi_F1s.append(d[\"micro-f1\"])\n        t_re1=time.time();t_re=t_re1-t_re0\n    vis_data_saver.collect_whole_process(round(float(100*np.mean(np.array(ma_F1s)) ),2),name=\"macro-f1-mean\");vis_data_saver.collect_whole_process(round(float(100*np.std(np.array(ma_F1s)) ),2),name=\"macro-f1-std\");vis_data_saver.collect_whole_process(round(float(100*np.mean(np.array(mi_F1s)) ),2),name=\"micro-f1-mean\");vis_data_saver.collect_whole_process(round(float(100*np.std(np.array(mi_F1s)) ),2),name=\"micro-f1-std\")\n    print(f\"mean and std of macro-f1: {  100*np.mean(np.array(ma_F1s)) :.1f}\\u00B1{  100*np.std(np.array(ma_F1s)) :.1f}\");print(f\"mean and std of micro-f1: {  100*np.mean(np.array(mi_F1s)) :.1f}\\u00B1{  100*np.std(np.array(mi_F1s)) :.1f}\") \n    #print(optimizer) if args.verbose==\"True\" else None\n    toCsvAveraged={}\n    for tocsv in toCsvRepetition:\n        for name in tocsv.keys():\n            if name.startswith(\"1_\"):\n                toCsvAveraged[name]=tocsv[name]\n            else:\n                if name not in toCsvAveraged.keys():\n                    toCsvAveraged[name]=[]\n                toCsvAveraged[name].append(tocsv[name])\n    \n    for name in toCsvAveraged.keys():\n        if not name.startswith(\"1_\") :\n            if type(toCsvAveraged[name][0]) is str:\n                toCsvAveraged[name]=toCsvAveraged[name][0]\n            elif \"_mean\" in name:\n                toCsvAveraged[name]=sum(toCsvAveraged[name])/len(toCsvAveraged[name])\n            elif \"_total\" in name:\n                toCsvAveraged[name]=sum(toCsvAveraged[name])/len(toCsvAveraged[name])\n            elif \"_std\" in name:\n                toCsvAveraged[name]= np.std(np.array(toCsvAveraged[name])) \n            elif \"peak\" in name:\n                toCsvAveraged[name]=max(toCsvAveraged[name])\n            else:\n                raise Exception()\n            \n\n    writeIntoCsvLogger(toCsvAveraged,f\"./log/{args.study_name}.csv\")\n\n\n\n\n\n    fn=os.path.join(\"log\",args.study_name)\n    if os.path.exists(fn):\n        m=\"a\"\n    else:\n        m=\"w\"\n    \n    with open(fn,m) as f:\n         \n        f.write(f\"score {  score :.4f}  mean and std of macro-f1: {  100*np.mean(np.array(ma_F1s)) :.1f}\\u00B1{  100*np.std(np.array(ma_F1s)) :.1f} micro-f1: {  100*np.mean(np.array(mi_F1s)) :.1f}\\u00B1{  100*np.std(np.array(mi_F1s)) :.1f}\\n\") \n        if trial:\n            f.write(f\"trial.params: {str(trial.params)}\"+\"\\n\")\n            \n\n    return score", "\ndef remove_ckp_files(ckp_dname):\n    import shutil\n    shutil.rmtree(ckp_dname) \n\nif __name__ == '__main__':\n    \n    peak_gpu_memory=0\n    \n    \n    run_model_DBLP(trial=None)", "\n\n\n\n    \n"]}
{"filename": "NC/methods/SlotGAT/run_train_slotGAT_on_all_dataset.py", "chunked_list": ["import time\nimport subprocess\nimport multiprocessing\nfrom threading import main_thread\nfrom pipeline_utils import get_best_hypers,run_command_in_parallel,config_study_name,Run,get_tasks,get_tasks_linear_around,get_tasks_for_online\nimport os\nimport copy\n#time.sleep(60*60*4)\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))", "\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\nif not os.path.exists(\"./log\"):\n    os.mkdir(\"./log\")\nif not os.path.exists(\"./checkpoint\"):\n    os.mkdir(\"./checkpoint\")\nif not os.path.exists(\"./analysis\"):\n    os.mkdir(\"./analysis\")\nif not os.path.exists(\"./model_files\"):\n    os.mkdir(\"./model_files\")", "if not os.path.exists(\"./model_files\"):\n    os.mkdir(\"./model_files\")\n\nresources_dict={\"0\":1,\"1\":1}   #id:load\n\nstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\nresources=resources_dict.keys()\npool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j))", "pool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j))\n\nprefix=\"get_results\";specified_args=[\"dataset\",  \"net\", ]\n\nnets=[\"slotGAT\"]\ndataset_restrict=[]\n", "dataset_restrict=[]\n\nfixed_info_by_net={\"slotGAT\":\n                        {\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\", \"verbose\":\"True\",\n                         \"use_trained\":\"False\",\n                        #\"trained_dir\":\"outputs\",\n                        \"save_trained\":\"True\",\n                        \"save_dir\":\"outputs\",},\n}\n", "}\n\n\ndataset_and_hypers_by_net={\n        \"slotGAT\":\n            {\n            (\"IMDB\",1,5):\n                {\"search_hidden_dim\":128,\"search_num_layers\":3,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.8,\"dropout_attn\":0.2},\n            (\"ACM\",1,5):\n                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},", "            (\"ACM\",1,5):\n                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},\n            (\"DBLP\",1,5):\n                {\"search_hidden_dim\":64,\"search_num_layers\":4,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.5,\"dropout_attn\":0.5},\n            (\"Freebase\",1,5):\n                {\"search_hidden_dim\":16,\"search_num_layers\":2,\"search_lr\":0.0005,\"search_weight_decay\":0.001,\"feats-type\":2,\"num-heads\":8,\"epoch\":300,\"SAattDim\":8,\"dropout_feat\":0.5,\"dropout_attn\":0.5,\"edge-feats\":\"0\"},\n            (\"PubMed_NC\",1,5):\n                {\"search_hidden_dim\":128,\"search_num_layers\":2,\"search_lr\":0.005,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.2,\"dropout_attn\":0.8,}\n             },\n    }", "             },\n    }\n\n\n\n\n\ndef getTasks(fixed_info,dataset_and_hypers):\n    \n    for k,v in dataset_and_hypers.items():\n        for k1,v1 in v.items():\n            if \"search_\" in k1:\n                if type(v1)!=str:\n                    v[k1]=f\"[{v1}]\"\n        \n\n    tasks_list=[]\n    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n        if len(dataset_restrict)>0 and dataset not in dataset_restrict:\n            continue\n        args_dict={}\n        for dict_to_add in [task,fixed_info]:\n            for k,v in dict_to_add.items():\n                args_dict[k]=v \n        args_dict['dataset']=dataset\n        #args_dict['trial_num']=trial_num\n        args_dict['repeat']=repeat\n        study_name,study_storage=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n        args_dict['study_name']=study_name\n        #args_dict['study_storage']=study_storage\n        args_dict['cost']=cost\n        tasks_list.append(args_dict)\n\n    print(\"tasks_list:\", tasks_list)\n\n    return tasks_list ", "\ntasks_list=[]\nfor net in nets:\n    tasks_list.extend(getTasks(fixed_info_by_net[net],dataset_and_hypers_by_net[net]))\n\n\n\n\n\n", "\n\n\n\n\nsub_queues=[]\nitems=len(tasks_list)%60\nfor i in range(items):\n    sub_queues.append(tasks_list[60*i:(60*i+60)])\nsub_queues.append(tasks_list[(60*items+60):])", "sub_queues.append(tasks_list[(60*items+60):])\n\nif items==0:\n    sub_queues.append(tasks_list)\n\n## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\nidx=0\ntc=len(tasks_list)\nfor sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "for sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "    \n\nprint('end all')\n\n\n\n\nend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"]}
{"filename": "NC/methods/SlotGAT/run_use_slotGAT_on_all_dataset.py", "chunked_list": ["import time\nimport subprocess\nimport multiprocessing\nfrom threading import main_thread\nfrom pipeline_utils import get_best_hypers,run_command_in_parallel,config_study_name,Run,get_tasks,get_tasks_linear_around,get_tasks_for_online\nimport os\nimport copy\n#time.sleep(60*60*4)\n\nos.chdir(os.path.dirname(os.path.abspath(__file__)))", "\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\nif not os.path.exists(\"./log\"):\n    os.mkdir(\"./log\")\nif not os.path.exists(\"./checkpoint\"):\n    os.mkdir(\"./checkpoint\")\nif not os.path.exists(\"./analysis\"):\n    os.mkdir(\"./analysis\")\nif not os.path.exists(\"./outputs\"):\n    os.mkdir(\"./outputs\")", "if not os.path.exists(\"./outputs\"):\n    os.mkdir(\"./outputs\")\n\nresources_dict={\"0\":1,\"1\":1}   #id:load \n\nstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\nresources=resources_dict.keys()\npool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j)) ", "pool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j)) \nprefix=\"get_results_use_trained\";specified_args=[\"dataset\",  \"net\", ]\n\nnets=[\"slotGAT\"] \ndataset_restrict=[]\n\nfixed_info_by_net={\"slotGAT\":", "\nfixed_info_by_net={\"slotGAT\":\n                        {\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\", \"verbose\":\"True\",\n                        \"use_trained\":\"True\",\n                        \"trained_dir\":\"outputs\",\n                        \"save_trained\":\"False\",\n                        #\"save_dir\":\"outputs\",\n                        },\n}\n", "}\n\n\ndataset_and_hypers_by_net={\n        \"slotGAT\":\n            {\n             (\"IMDB\",1,5):\n                {\"search_hidden_dim\":128,\"search_num_layers\":3,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.8,\"dropout_attn\":0.2},\n            (\"ACM\",1,5):\n                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},", "            (\"ACM\",1,5):\n                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},\n            (\"DBLP\",1,5): \n                {\"search_hidden_dim\":64,\"search_num_layers\":4,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.5,\"dropout_attn\":0.5},\n            (\"Freebase\",1,5):\n                {\"search_hidden_dim\":16,\"search_num_layers\":2,\"search_lr\":0.0005,\"search_weight_decay\":0.001,\"feats-type\":2,\"num-heads\":8,\"epoch\":300,\"SAattDim\":8,\"dropout_feat\":0.5,\"dropout_attn\":0.5,\"edge-feats\":\"0\"},\n            (\"PubMed_NC\",1,5):\n                {\"search_hidden_dim\":128,\"search_num_layers\":2,\"search_lr\":0.005,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.2,\"dropout_attn\":0.8,}\n            },\n    }", "            },\n    }\n\n\n\n\n\ndef getTasks(fixed_info,dataset_and_hypers):\n    \n    for k,v in dataset_and_hypers.items():\n        for k1,v1 in v.items():\n            if \"search_\" in k1:\n                if type(v1)!=str:\n                    v[k1]=f\"[{v1}]\"\n        \n\n    tasks_list=[]\n    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n        if len(dataset_restrict)>0 and dataset not in dataset_restrict:\n            continue\n        args_dict={}\n        for dict_to_add in [task,fixed_info]:\n            for k,v in dict_to_add.items():\n                args_dict[k]=v \n        args_dict['dataset']=dataset\n        #args_dict['trial_num']=trial_num\n        args_dict['repeat']=repeat\n        study_name,study_storage=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n        args_dict['study_name']=study_name\n        #args_dict['study_storage']=study_storage\n        args_dict['cost']=cost\n        tasks_list.append(args_dict)\n\n    print(\"tasks_list:\", tasks_list)\n\n    return tasks_list ", "\ntasks_list=[]\nfor net in nets:\n    tasks_list.extend(getTasks(fixed_info_by_net[net],dataset_and_hypers_by_net[net]))\n\n\n\n\n\n", "\n\n\n\n\nsub_queues=[]\nitems=len(tasks_list)%60\nfor i in range(items):\n    sub_queues.append(tasks_list[60*i:(60*i+60)])\nsub_queues.append(tasks_list[(60*items+60):])", "sub_queues.append(tasks_list[(60*items+60):])\n\nif items==0:\n    sub_queues.append(tasks_list)\n\n## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\nidx=0\ntc=len(tasks_list)\nfor sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "for sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "    \n\nprint('end all')\n\n\n\n\nend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"]}
{"filename": "NC/methods/SlotGAT/utils/pytorchtools.py", "chunked_list": ["import numpy as np\nimport torch\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience, verbose=False, delta=0, save_path='checkpoint.pt'):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.save_path = save_path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score - self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model to {self.save_path}...')\n        torch.save(model.state_dict(), self.save_path)\n        self.val_loss_min = val_loss", ""]}
{"filename": "NC/methods/SlotGAT/utils/__init__.py", "chunked_list": [""]}
{"filename": "NC/methods/SlotGAT/utils/tools.py", "chunked_list": ["import torch\nimport dgl\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import LinearSVC\nimport os\nimport torch.nn.functional as F\nimport torch.nn as nn", "import torch.nn.functional as F\nimport torch.nn as nn\nimport copy\nimport json\nimport pickle\nfrom matplotlib import pyplot as plt\n\n\n\n\ndef count_torch_tensor(t):\n    t=t.flatten(0).cpu()\n    c={}\n    for n in t:\n        n=n.item()\n        if n not in c:\n            c[n]=0\n        c[n]+=1\n    c=sorted(list(c.items()),key=lambda x:x[0])\n    return c", "\n\ndef count_torch_tensor(t):\n    t=t.flatten(0).cpu()\n    c={}\n    for n in t:\n        n=n.item()\n        if n not in c:\n            c[n]=0\n        c[n]+=1\n    c=sorted(list(c.items()),key=lambda x:x[0])\n    return c", "\n\n\n\ndef strList(l):\n    return [str(x) for x in l]\n\ndef writeIntoCsvLogger(dictToWrite,file_name):\n    #read file\n    to_write_names=sorted(list(dictToWrite.keys()))\n    if not os.path.exists(file_name):\n        to_write_line=[]\n        for n in to_write_names:\n            to_write_line.append(dictToWrite[n])\n        with open(file_name,\"w\") as f:\n            f.write(  \",\".join(strList(to_write_names)) +\"\\n\")\n            f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n    else:\n        with open(file_name,\"r\") as f:\n            rec=[]\n            for line in f:\n                line=line.strip(\"\\n\").split(\",\")\n                rec.append(line)\n        #ensure they have same names\n        row_names=sorted(rec[0])\n        if to_write_names!=row_names:\n            collected_names_not_in=[]\n            for n in to_write_names:\n                if n not in row_names:\n                    for i,n_r in enumerate(rec):\n                        if i==0:\n                            rec[0].append(n)\n                        else:\n                            rec[i].append(\"\")\n                row_names.append(n)\n            for n_r in row_names:\n                if n_r not in to_write_names:\n                    dictToWrite[n_r]=\"\"\n                    to_write_names.append(n_r)\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            rec.append(to_write_line)\n            with open(file_name,\"w\") as f:\n                for line_list in rec:\n                    f.write(  \",\".join(strList(line_list)) +\"\\n\")\n        else:\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            with open(file_name,\"a\") as f:\n                f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n        re_order_csv(file_name)", "\n        \ndef re_order_csv(file_name):\n    with open(file_name,\"r\") as f:\n        rec=[]\n        for line in f:\n            line=line.strip(\"\\n\").split(\",\")\n            rec.append(line)\n    row_names=sorted(enumerate(rec[0]),key=lambda x:x[1])\n    row_names_idx=[i[0] for i in row_names]    \n    row_names_=[i[1] for i in row_names]    \n    if row_names_idx==sorted(row_names_idx):\n        print(\"No need to reorder\")\n        return None\n    else:\n        print(\"reordering\")\n        with open(file_name,\"w\") as f:\n            for line_list in rec:\n                to_write_line=[ line_list[row_names_idx[i]]  for i in range(len(line_list))  ]\n                f.write(  \",\".join(to_write_line) +\"\\n\")", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\ndef func_args_parse(*args,**kargs):\n    return args,kargs\n\nclass blank_profile():\n    def __init__(self,*args,**kwargs):\n        pass\n    def __enter__(self,*args,**kwargs):\n        return self\n    def __exit__(self,*args,**kwargs):\n        pass\n    def step(self):\n        pass", "class blank_profile():\n    def __init__(self,*args,**kwargs):\n        pass\n    def __enter__(self,*args,**kwargs):\n        return self\n    def __exit__(self,*args,**kwargs):\n        pass\n    def step(self):\n        pass\n\nclass vis_data_collector():\n    #all data must be simple python objects like int or 'str'\n    def __init__(self):\n        self.data_dict={}\n        self.tensor_dict={}\n        #formatting:\n        #\n        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\n    def save_meta(self,meta_data,meta_name):\n        self.data_dict[\"meta\"]={meta_name:meta_data}\n        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n        \n\n    def collect_in_training(self,data,name,re,epoch,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\n    def collect_in_run(self,data,name,re,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][name]=data\n\n    def collect_whole_process(self,data,name):\n        self.data_dict[name]=data\n    def collect_whole_process_tensor(self,data,name):\n        self.tensor_dict[name]=data\n        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\n\n    def save(self,fn):\n        \n        f = open(fn+\".json\", 'w')\n        json.dump(self.data_dict, f, indent=4)\n        f.close()\n\n        for k,v in self.tensor_dict.items():\n\n            torch.save(v,fn+\"_\"+k+\".pt\")\n    \n    def load(self,fn):\n        f = open(fn, 'r')\n        self.data_dict= json.load(f)\n        f.close()\n        for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n            self.tensor_dict[name]=torch.load(name+\".pt\")\n\n\n\n    def trans_to_numpy(self,name,epoch_range=None):\n        data=[]\n        re=0\n        while f\"re-{re}\" in self.data_dict.keys():\n            data.append([])\n            for i in range(epoch_range):\n                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n            re+=1\n        data=np.array(data)\n        return np.mean(data,axis=0),np.std(data,axis=0)\n\n    def visualize_tsne(self,dn,node_idx_by_ntype):\n        from matplotlib.pyplot import figure\n\n        figure(figsize=(16, 9), dpi=80)\n        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n        print(dn)\n        layers=[]\n        heads=[]\n        ets=[]\n        #\"tsne_emb_layer_0_slot_0\"\n        for k,v in self.data_dict.items():\n            if \"tsne_emb_layer\" in k:\n                temp=k.split(\"_\")\n                if int(temp[3]) not in layers:\n                    layers.append(int(temp[3]))\n                if temp[4]==\"slot\":\n                    if int(temp[5]) not in ets:\n                        ets.append(int(temp[5]))\n        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n        print(layers,heads,ets)\n        #heads plot\n        for layer in layers:\n            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n            fig.set_size_inches(16,9)\n            fig.set_dpi(100)\n            nts=list(range(len(node_idx_by_ntype)))\n            for nt in nts:\n                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n                subax.cla()\n                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n                print(datas.shape)\n                for nt_j in nts:\n                    x=datas[0][ node_idx_by_ntype[nt_j]]\n                    y=datas[1][ node_idx_by_ntype[nt_j]]\n                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n                subax.set_xlim(-100,100)\n                subax.set_ylim(-100,100)\n                subax.set_title(f\"layer_{layer}_slot_{nt}\")\n                plt.title(f\"layer_{layer}_slot_{nt}\")\n                lgnd=subax.legend()\n                for lh in lgnd.legendHandles:\n                    lh._sizes=[10]\n            fig.suptitle(f\"embedding_tsne_layer_{layer}\")\n            plt.savefig(os.path.join(dn,f\"slot_embeddings_layer_{layer}.png\"))", "\nclass vis_data_collector():\n    #all data must be simple python objects like int or 'str'\n    def __init__(self):\n        self.data_dict={}\n        self.tensor_dict={}\n        #formatting:\n        #\n        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\n    def save_meta(self,meta_data,meta_name):\n        self.data_dict[\"meta\"]={meta_name:meta_data}\n        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n        \n\n    def collect_in_training(self,data,name,re,epoch,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\n    def collect_in_run(self,data,name,re,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][name]=data\n\n    def collect_whole_process(self,data,name):\n        self.data_dict[name]=data\n    def collect_whole_process_tensor(self,data,name):\n        self.tensor_dict[name]=data\n        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\n\n    def save(self,fn):\n        \n        f = open(fn+\".json\", 'w')\n        json.dump(self.data_dict, f, indent=4)\n        f.close()\n\n        for k,v in self.tensor_dict.items():\n\n            torch.save(v,fn+\"_\"+k+\".pt\")\n    \n    def load(self,fn):\n        f = open(fn, 'r')\n        self.data_dict= json.load(f)\n        f.close()\n        for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n            self.tensor_dict[name]=torch.load(name+\".pt\")\n\n\n\n    def trans_to_numpy(self,name,epoch_range=None):\n        data=[]\n        re=0\n        while f\"re-{re}\" in self.data_dict.keys():\n            data.append([])\n            for i in range(epoch_range):\n                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n            re+=1\n        data=np.array(data)\n        return np.mean(data,axis=0),np.std(data,axis=0)\n\n    def visualize_tsne(self,dn,node_idx_by_ntype):\n        from matplotlib.pyplot import figure\n\n        figure(figsize=(16, 9), dpi=80)\n        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n        print(dn)\n        layers=[]\n        heads=[]\n        ets=[]\n        #\"tsne_emb_layer_0_slot_0\"\n        for k,v in self.data_dict.items():\n            if \"tsne_emb_layer\" in k:\n                temp=k.split(\"_\")\n                if int(temp[3]) not in layers:\n                    layers.append(int(temp[3]))\n                if temp[4]==\"slot\":\n                    if int(temp[5]) not in ets:\n                        ets.append(int(temp[5]))\n        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n        print(layers,heads,ets)\n        #heads plot\n        for layer in layers:\n            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n            fig.set_size_inches(16,9)\n            fig.set_dpi(100)\n            nts=list(range(len(node_idx_by_ntype)))\n            for nt in nts:\n                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n                subax.cla()\n                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n                print(datas.shape)\n                for nt_j in nts:\n                    x=datas[0][ node_idx_by_ntype[nt_j]]\n                    y=datas[1][ node_idx_by_ntype[nt_j]]\n                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n                subax.set_xlim(-100,100)\n                subax.set_ylim(-100,100)\n                subax.set_title(f\"layer_{layer}_slot_{nt}\")\n                plt.title(f\"layer_{layer}_slot_{nt}\")\n                lgnd=subax.legend()\n                for lh in lgnd.legendHandles:\n                    lh._sizes=[10]\n            fig.suptitle(f\"embedding_tsne_layer_{layer}\")\n            plt.savefig(os.path.join(dn,f\"slot_embeddings_layer_{layer}.png\"))", "\n\n\n                \n                    \n\n\n\n\n\ndef single_feat_net(net,*args,**kargs):\n    return net(*args,**kargs)", "\n\ndef single_feat_net(net,*args,**kargs):\n    return net(*args,**kargs)\n\n\n\n\ndef idx_to_one_hot(idx_arr):\n    one_hot = np.zeros((idx_arr.shape[0], idx_arr.max() + 1))\n    one_hot[np.arange(idx_arr.shape[0]), idx_arr] = 1\n    return one_hot", "def idx_to_one_hot(idx_arr):\n    one_hot = np.zeros((idx_arr.shape[0], idx_arr.max() + 1))\n    one_hot[np.arange(idx_arr.shape[0]), idx_arr] = 1\n    return one_hot\n\n\ndef kmeans_test(X, y, n_clusters, repeat=10):\n    nmi_list = []\n    ari_list = []\n    for _ in range(repeat):\n        kmeans = KMeans(n_clusters=n_clusters)\n        y_pred = kmeans.fit_predict(X)\n        nmi_score = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n        ari_score = adjusted_rand_score(y, y_pred)\n        nmi_list.append(nmi_score)\n        ari_list.append(ari_score)\n    return np.mean(nmi_list), np.std(nmi_list), np.mean(ari_list), np.std(ari_list)", "\n\ndef svm_test(X, y, test_sizes=(0.2, 0.4, 0.6, 0.8), repeat=10):\n    random_states = [182318 + i for i in range(repeat)]\n    result_macro_f1_list = []\n    result_micro_f1_list = []\n    for test_size in test_sizes:\n        macro_f1_list = []\n        micro_f1_list = []\n        for i in range(repeat):\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=test_size, shuffle=True, random_state=random_states[i])\n            svm = LinearSVC(dual=False)\n            svm.fit(X_train, y_train)\n            y_pred = svm.predict(X_test)\n            macro_f1 = f1_score(y_test, y_pred, average='macro')\n            micro_f1 = f1_score(y_test, y_pred, average='micro')\n            macro_f1_list.append(macro_f1)\n            micro_f1_list.append(micro_f1)\n        result_macro_f1_list.append((np.mean(macro_f1_list), np.std(macro_f1_list)))\n        result_micro_f1_list.append((np.mean(micro_f1_list), np.std(micro_f1_list)))\n    return result_macro_f1_list, result_micro_f1_list", "\n\ndef evaluate_results_nc(embeddings, labels, num_classes):\n    print('SVM test')\n    svm_macro_f1_list, svm_micro_f1_list = svm_test(embeddings, labels)\n    print('Macro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(macro_f1_mean, macro_f1_std, train_size) for\n                                    (macro_f1_mean, macro_f1_std), train_size in\n                                    zip(svm_macro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n    print('Micro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(micro_f1_mean, micro_f1_std, train_size) for\n                                    (micro_f1_mean, micro_f1_std), train_size in\n                                    zip(svm_micro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n    print('K-means test')\n    nmi_mean, nmi_std, ari_mean, ari_std = kmeans_test(embeddings, labels, num_classes)\n    print('NMI: {:.6f}~{:.6f}'.format(nmi_mean, nmi_std))\n    print('ARI: {:.6f}~{:.6f}'.format(ari_mean, ari_std))\n\n    return svm_macro_f1_list, svm_micro_f1_list, nmi_mean, nmi_std, ari_mean, ari_std", "\n\ndef parse_adjlist(adjlist, edge_metapath_indices, samples=None):\n    edges = []\n    nodes = set()\n    result_indices = []\n    for row, indices in zip(adjlist, edge_metapath_indices):\n        row_parsed = list(map(int, row.split(' ')))\n        nodes.add(row_parsed[0])\n        if len(row_parsed) > 1:\n            # sampling neighbors\n            if samples is None:\n                neighbors = row_parsed[1:]\n                result_indices.append(indices)\n            else:\n                # undersampling frequent neighbors\n                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n                p = []\n                for count in counts:\n                    p += [(count ** (3 / 4)) / count] * count\n                p = np.array(p)\n                p = p / p.sum()\n                samples = min(samples, len(row_parsed) - 1)\n                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n                neighbors = [row_parsed[i + 1] for i in sampled_idx]\n                result_indices.append(indices[sampled_idx])\n        else:\n            neighbors = []\n            result_indices.append(indices)\n        for dst in neighbors:\n            nodes.add(dst)\n            edges.append((row_parsed[0], dst))\n    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n    result_indices = np.vstack(result_indices)\n    return edges, result_indices, len(nodes), mapping", "\n\ndef parse_minibatch(adjlists, edge_metapath_indices_list, idx_batch, device, samples=None):\n    g_list = []\n    result_indices_list = []\n    idx_batch_mapped_list = []\n    for adjlist, indices in zip(adjlists, edge_metapath_indices_list):\n        edges, result_indices, num_nodes, mapping = parse_adjlist(\n            [adjlist[i] for i in idx_batch], [indices[i] for i in idx_batch], samples)\n\n        g = dgl.DGLGraph(multigraph=True)\n        g.add_nodes(num_nodes)\n        if len(edges) > 0:\n            sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n            g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n            result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n        else:\n            result_indices = torch.LongTensor(result_indices).to(device)\n        #g.add_edges(*list(zip(*[(dst, src) for src, dst in sorted(edges)])))\n        #result_indices = torch.LongTensor(result_indices).to(device)\n        g_list.append(g)\n        result_indices_list.append(result_indices)\n        idx_batch_mapped_list.append(np.array([mapping[idx] for idx in idx_batch]))\n\n    return g_list, result_indices_list, idx_batch_mapped_list", "\n\ndef parse_adjlist_LastFM(adjlist, edge_metapath_indices, samples=None, exclude=None, offset=None, mode=None):\n    edges = []\n    nodes = set()\n    result_indices = []\n    for row, indices in zip(adjlist, edge_metapath_indices):\n        row_parsed = list(map(int, row.split(' ')))\n        nodes.add(row_parsed[0])\n        if len(row_parsed) > 1:\n            # sampling neighbors\n            if samples is None:\n                if exclude is not None:\n                    if mode == 0:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[:, [0, 1, -1, -2]]]\n                    else:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[:, [0, 1, -1, -2]]]\n                    neighbors = np.array(row_parsed[1:])[mask]\n                    result_indices.append(indices[mask])\n                else:\n                    neighbors = row_parsed[1:]\n                    result_indices.append(indices)\n            else:\n                # undersampling frequent neighbors\n                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n                p = []\n                for count in counts:\n                    p += [(count ** (3 / 4)) / count] * count\n                p = np.array(p)\n                p = p / p.sum()\n                samples = min(samples, len(row_parsed) - 1)\n                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n                if exclude is not None:\n                    if mode == 0:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n                    else:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n                    neighbors = np.array([row_parsed[i + 1] for i in sampled_idx])[mask]\n                    result_indices.append(indices[sampled_idx][mask])\n                else:\n                    neighbors = [row_parsed[i + 1] for i in sampled_idx]\n                    result_indices.append(indices[sampled_idx])\n        else:\n            neighbors = [row_parsed[0]]\n            indices = np.array([[row_parsed[0]] * indices.shape[1]])\n            if mode == 1:\n                indices += offset\n            result_indices.append(indices)\n        for dst in neighbors:\n            nodes.add(dst)\n            edges.append((row_parsed[0], dst))\n    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n    result_indices = np.vstack(result_indices)\n    return edges, result_indices, len(nodes), mapping", "\n\ndef parse_minibatch_LastFM(adjlists_ua, edge_metapath_indices_list_ua, user_artist_batch, device, samples=None, use_masks=None, offset=None):\n    g_lists = [[], []]\n    result_indices_lists = [[], []]\n    idx_batch_mapped_lists = [[], []]\n    for mode, (adjlists, edge_metapath_indices_list) in enumerate(zip(adjlists_ua, edge_metapath_indices_list_ua)):\n        for adjlist, indices, use_mask in zip(adjlists, edge_metapath_indices_list, use_masks[mode]):\n            if use_mask:\n                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, user_artist_batch, offset, mode)\n            else:\n                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, offset=offset, mode=mode)\n\n            g = dgl.DGLGraph(multigraph=True)\n            g.add_nodes(num_nodes)\n            if len(edges) > 0:\n                sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n                g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n                result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n            else:\n                result_indices = torch.LongTensor(result_indices).to(device)\n            g_lists[mode].append(g)\n            result_indices_lists[mode].append(result_indices)\n            idx_batch_mapped_lists[mode].append(np.array([mapping[row[mode]] for row in user_artist_batch]))\n\n    return g_lists, result_indices_lists, idx_batch_mapped_lists", "\n\nclass index_generator:\n    def __init__(self, batch_size, num_data=None, indices=None, shuffle=True):\n        if num_data is not None:\n            self.num_data = num_data\n            self.indices = np.arange(num_data)\n        if indices is not None:\n            self.num_data = len(indices)\n            self.indices = np.copy(indices)\n        self.batch_size = batch_size\n        self.iter_counter = 0\n        self.shuffle = shuffle\n        if shuffle:\n            np.random.shuffle(self.indices)\n\n    def next(self):\n        if self.num_iterations_left() <= 0:\n            self.reset()\n        self.iter_counter += 1\n        return np.copy(self.indices[(self.iter_counter - 1) * self.batch_size:self.iter_counter * self.batch_size])\n\n    def num_iterations(self):\n        return int(np.ceil(self.num_data / self.batch_size))\n\n    def num_iterations_left(self):\n        return self.num_iterations() - self.iter_counter\n\n    def reset(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.iter_counter = 0", ""]}
{"filename": "NC/methods/SlotGAT/utils/data.py", "chunked_list": ["import networkx as nx\nimport numpy as np\nimport scipy\nimport pickle\nimport scipy.sparse as sp\n\ndef load_data(prefix='DBLP',multi_labels=False ):\n    from scripts.data_loader import data_loader\n    dl = data_loader('../../data/'+prefix)\n    features = []\n    for i in range(len(dl.nodes['count'])):\n        th = dl.nodes['attr'][i]\n        if th is None:\n            features.append(sp.eye(dl.nodes['count'][i]))\n        else:\n            features.append(th)\n    adjM = sum(dl.links['data'].values())\n    labels = np.zeros((dl.nodes['count'][0], dl.labels_train['num_classes']), dtype=int)\n    val_ratio = 0.2\n    train_idx = np.nonzero(dl.labels_train['mask'])[0]\n    np.random.shuffle(train_idx)\n    split = int(train_idx.shape[0]*val_ratio)\n    val_idx = train_idx[:split]\n    train_idx = train_idx[split:]\n    train_idx = np.sort(train_idx)\n    val_idx = np.sort(val_idx)\n    test_idx = np.nonzero(dl.labels_test['mask'])[0]\n    labels[train_idx] = dl.labels_train['data'][train_idx]\n    labels[val_idx] = dl.labels_train['data'][val_idx]\n    #if prefix != 'IMDB':\n    if not multi_labels:\n        labels = labels.argmax(axis=1)\n    train_val_test_idx = {}\n    train_val_test_idx['train_idx'] = train_idx\n    train_val_test_idx['val_idx'] = val_idx\n    train_val_test_idx['test_idx'] = test_idx\n    return features,\\\n           adjM, \\\n           labels,\\\n           train_val_test_idx,\\\n            dl", ""]}
{"filename": "NC/methods/SlotGAT/utils/preprocess.py", "chunked_list": ["import numpy as np\nimport scipy.sparse\nimport networkx as nx\n\n\ndef get_metapath_adjacency_matrix(adjM, type_mask, metapath):\n    \"\"\"\n    :param M: the raw adjacency matrix\n    :param type_mask: an array of types of all node\n    :param metapath\n    :return: a list of metapath-based adjacency matrices\n    \"\"\"\n    out_adjM = scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[0], type_mask == metapath[1])])\n    for i in range(1, len(metapath) - 1):\n        out_adjM = out_adjM.dot(scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])]))\n    return out_adjM.toarray()", "\n\n# networkx.has_path may search too\ndef get_metapath_neighbor_pairs(M, type_mask, expected_metapaths):\n    \"\"\"\n    :param M: the raw adjacency matrix\n    :param type_mask: an array of types of all node\n    :param expected_metapaths: a list of expected metapaths\n    :return: a list of python dictionaries, consisting of metapath-based neighbor pairs and intermediate paths\n    \"\"\"\n    outs = []\n    for metapath in expected_metapaths:\n        # consider only the edges relevant to the expected metapath\n        mask = np.zeros(M.shape, dtype=bool)\n        for i in range((len(metapath) - 1) // 2):\n            temp = np.zeros(M.shape, dtype=bool)\n            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n            mask = np.logical_or(mask, temp)\n        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n\n        # only need to consider the former half of the metapath\n        # e.g., we only need to consider 0-1-2 for the metapath 0-1-2-1-0\n        metapath_to_target = {}\n        for source in (type_mask == metapath[0]).nonzero()[0]:\n            for target in (type_mask == metapath[(len(metapath) - 1) // 2]).nonzero()[0]:\n                # check if there is a possible valid path from source to target node\n                has_path = False\n                single_source_paths = nx.single_source_shortest_path(\n                    partial_g_nx, source, cutoff=(len(metapath) + 1) // 2 - 1)\n                if target in single_source_paths:\n                    has_path = True\n\n                #if nx.has_path(partial_g_nx, source, target):\n                if has_path:\n                    shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n                                 len(p) == (len(metapath) + 1) // 2]\n                    if len(shortests) > 0:\n                        metapath_to_target[target] = metapath_to_target.get(target, []) + shortests\n        metapath_neighbor_paris = {}\n        for key, value in metapath_to_target.items():\n            for p1 in value:\n                for p2 in value:\n                    metapath_neighbor_paris[(p1[0], p2[0])] = metapath_neighbor_paris.get((p1[0], p2[0]), []) + [\n                        p1 + p2[-2::-1]]\n        outs.append(metapath_neighbor_paris)\n    return outs", "\n\ndef get_networkx_graph(neighbor_pairs, type_mask, ctr_ntype):\n    indices = np.where(type_mask == ctr_ntype)[0]\n    idx_mapping = {}\n    for i, idx in enumerate(indices):\n        idx_mapping[idx] = i\n    G_list = []\n    for metapaths in neighbor_pairs:\n        edge_count = 0\n        sorted_metapaths = sorted(metapaths.items())\n        G = nx.MultiDiGraph()\n        G.add_nodes_from(range(len(indices)))\n        for (src, dst), paths in sorted_metapaths:\n            for _ in range(len(paths)):\n                G.add_edge(idx_mapping[src], idx_mapping[dst])\n                edge_count += 1\n        G_list.append(G)\n    return G_list", "\n\ndef get_edge_metapath_idx_array(neighbor_pairs):\n    all_edge_metapath_idx_array = []\n    for metapath_neighbor_pairs in neighbor_pairs:\n        sorted_metapath_neighbor_pairs = sorted(metapath_neighbor_pairs.items())\n        edge_metapath_idx_array = []\n        for _, paths in sorted_metapath_neighbor_pairs:\n            edge_metapath_idx_array.extend(paths)\n        edge_metapath_idx_array = np.array(edge_metapath_idx_array, dtype=int)\n        all_edge_metapath_idx_array.append(edge_metapath_idx_array)\n        print(edge_metapath_idx_array.shape)\n    return all_edge_metapath_idx_array", ""]}
{"filename": "NC/methods/SlotGAT/analysis/get_good_tsnes.py", "chunked_list": ["\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport os\nimport json\nimport sys\nimport argparse\nimport time\nimport math\nimport random", "import math\nimport random\nimport torch\nimport torch.nn as nn\nimport matplotlib as mpl\nplt.rcParams[\"font.family\"] = \"Times New Roman\"\nmpl.rcParams['xtick.labelsize']=0\nmpl.rcParams['ytick.labelsize']=0\nabcd=\"abcdefghijklmnopqrstuvwxyz\"\nclass vis_data_collector():\n    #all data must be simple python objects like int or 'str'\n    def __init__(self):\n        self.data_dict={}\n        self.tensor_dict={}\n        #formatting:\n        #\n        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\n    def save_meta(self,meta_data,meta_name):\n        self.data_dict[\"meta\"]={meta_name:meta_data}\n        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n        \n\n    def collect_in_training(self,data,name,re,epoch,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\n    def collect_in_run(self,data,name,re,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][name]=data\n\n    def collect_whole_process(self,data,name):\n        self.data_dict[name]=data\n    def collect_whole_process_tensor(self,data,name):\n        self.tensor_dict[name]=data\n        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\n\n    def save(self,fn):\n        \n        f = open(fn+\".json\", 'w')\n        json.dump(self.data_dict, f, indent=4)\n        f.close()\n\n        for k,v in self.tensor_dict.items():\n\n            torch.save(v,fn+\"_\"+k+\".pt\")\n    \n    def load(self,fn):\n        f = open(fn, 'r')\n        self.data_dict= json.load(f)\n        f.close()\n        if \"meta\" in self.data_dict.keys():\n            for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n                self.tensor_dict[name]=torch.load(name+\".pt\")\n\n\n\n    def trans_to_numpy(self,name,epoch_range=None):\n        data=[]\n        re=0\n        while f\"re-{re}\" in self.data_dict.keys():\n            data.append([])\n            for i in range(epoch_range):\n                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n            re+=1\n        data=np.array(data)\n        return np.mean(data,axis=0),np.std(data,axis=0)\n\n    def visualize_tsne(self,dn,node_idx_by_ntype,dataset):\n        from matplotlib.pyplot import figure\n\n        figure(figsize=(16, 12), dpi=300)\n        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n        print(dn)\n        layers=[]\n        heads=[]\n        ets=[]\n        #\"tsne_emb_layer_0_slot_0\"\n        for k,v in self.data_dict.items():\n            if \"tsne_emb_layer\" in k:\n                temp=k.split(\"_\")\n                if int(temp[3]) not in layers:\n                    layers.append(int(temp[3]))\n                if temp[4]==\"slot\":\n                    if int(temp[5]) not in ets:\n                        ets.append(int(temp[5]))\n        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n        print(layers,heads,ets)\n        #heads plot\n        for layer in layers:\n            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n            fig.set_size_inches(16,12)\n            fig.set_dpi(300)\n            nts=list(range(len(node_idx_by_ntype)))\n            for nt in nts:\n                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n                subax.cla()\n                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n                print(datas.shape)\n                x_lower,x_upper=0,0\n                y_lower,y_upper=0,0\n                for nt_j in nts:\n                    x=datas[0][ node_idx_by_ntype[nt_j]]\n                    y=datas[1][ node_idx_by_ntype[nt_j]]\n                   # subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n                    #if nt==1:\n                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=3,label=f\"Type {nt_j}\")\n                    #else:\n                    #    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1)\n                    #set 90% pencentile of lim\n                    xs=sorted(x)\n                    ys=sorted(y)\n                    x_lower,x_upper=min(xs[int(len(xs)*0.05)],x_lower),max(xs[int(len(xs)*0.95)],x_upper)\n                    y_lower,y_upper=min(ys[int(len(ys)*0.05)],y_lower),max(ys[int(len(ys)*0.95)],y_upper)\n                #subax.set_xticks(fontsize=12)    \n                #subax.set_yticks(fontsize=12)    \n                subax.set_xlim(x_lower,x_upper)\n                subax.set_ylim(y_lower,y_upper)\n                subax.tick_params(direction='in', length=0, width=0)\n                #subax.set_xlim(-200,200)\n                #subax.set_ylim(-200,200)\n                #subax.set_title(f\" Layer {layer} Slot {nt}\")\n                subax.set_xlabel(f\"({abcd[nt]}) Slot {nt}\",fontsize=55)\n                #plt.title(f\"Layer {layer} Slot {nt}\")\n                #if  nt==1:\n                    #lgnd=subax.legend(loc=\"lower right\", bbox_to_anchor=(1.3, 0.6))\n                    \n                    #for lh in lgnd.legendHandles:\n                        #lh._sizes=[16]\n            #fig.legend()\n            handles, labels = subax.get_legend_handles_labels()\n            lgnd=fig.legend(handles, labels, loc='right',fontsize=50,bbox_to_anchor=(1.09, 1.04), ncol=4,handletextpad=0.01)\n            #lgnd=fig.legend(bbox_to_anchor=(1.3, 0.6))\n            #lgnd=fig.legend()\n            for lh in lgnd.legendHandles:\n                lh._sizes=[100]\n            fig.tight_layout() \n            #fig.suptitle(f\"Tsne Embedding of Layer {layer}\")\n            plt.savefig(os.path.join(dn,f\"{dataset}_slot_embeddings_layer_{layer}.png\"),bbox_inches=\"tight\")", "abcd=\"abcdefghijklmnopqrstuvwxyz\"\nclass vis_data_collector():\n    #all data must be simple python objects like int or 'str'\n    def __init__(self):\n        self.data_dict={}\n        self.tensor_dict={}\n        #formatting:\n        #\n        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\n    def save_meta(self,meta_data,meta_name):\n        self.data_dict[\"meta\"]={meta_name:meta_data}\n        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n        \n\n    def collect_in_training(self,data,name,re,epoch,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\n    def collect_in_run(self,data,name,re,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][name]=data\n\n    def collect_whole_process(self,data,name):\n        self.data_dict[name]=data\n    def collect_whole_process_tensor(self,data,name):\n        self.tensor_dict[name]=data\n        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\n\n    def save(self,fn):\n        \n        f = open(fn+\".json\", 'w')\n        json.dump(self.data_dict, f, indent=4)\n        f.close()\n\n        for k,v in self.tensor_dict.items():\n\n            torch.save(v,fn+\"_\"+k+\".pt\")\n    \n    def load(self,fn):\n        f = open(fn, 'r')\n        self.data_dict= json.load(f)\n        f.close()\n        if \"meta\" in self.data_dict.keys():\n            for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n                self.tensor_dict[name]=torch.load(name+\".pt\")\n\n\n\n    def trans_to_numpy(self,name,epoch_range=None):\n        data=[]\n        re=0\n        while f\"re-{re}\" in self.data_dict.keys():\n            data.append([])\n            for i in range(epoch_range):\n                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n            re+=1\n        data=np.array(data)\n        return np.mean(data,axis=0),np.std(data,axis=0)\n\n    def visualize_tsne(self,dn,node_idx_by_ntype,dataset):\n        from matplotlib.pyplot import figure\n\n        figure(figsize=(16, 12), dpi=300)\n        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n        print(dn)\n        layers=[]\n        heads=[]\n        ets=[]\n        #\"tsne_emb_layer_0_slot_0\"\n        for k,v in self.data_dict.items():\n            if \"tsne_emb_layer\" in k:\n                temp=k.split(\"_\")\n                if int(temp[3]) not in layers:\n                    layers.append(int(temp[3]))\n                if temp[4]==\"slot\":\n                    if int(temp[5]) not in ets:\n                        ets.append(int(temp[5]))\n        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n        print(layers,heads,ets)\n        #heads plot\n        for layer in layers:\n            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n            fig.set_size_inches(16,12)\n            fig.set_dpi(300)\n            nts=list(range(len(node_idx_by_ntype)))\n            for nt in nts:\n                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n                subax.cla()\n                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n                print(datas.shape)\n                x_lower,x_upper=0,0\n                y_lower,y_upper=0,0\n                for nt_j in nts:\n                    x=datas[0][ node_idx_by_ntype[nt_j]]\n                    y=datas[1][ node_idx_by_ntype[nt_j]]\n                   # subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n                    #if nt==1:\n                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=3,label=f\"Type {nt_j}\")\n                    #else:\n                    #    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1)\n                    #set 90% pencentile of lim\n                    xs=sorted(x)\n                    ys=sorted(y)\n                    x_lower,x_upper=min(xs[int(len(xs)*0.05)],x_lower),max(xs[int(len(xs)*0.95)],x_upper)\n                    y_lower,y_upper=min(ys[int(len(ys)*0.05)],y_lower),max(ys[int(len(ys)*0.95)],y_upper)\n                #subax.set_xticks(fontsize=12)    \n                #subax.set_yticks(fontsize=12)    \n                subax.set_xlim(x_lower,x_upper)\n                subax.set_ylim(y_lower,y_upper)\n                subax.tick_params(direction='in', length=0, width=0)\n                #subax.set_xlim(-200,200)\n                #subax.set_ylim(-200,200)\n                #subax.set_title(f\" Layer {layer} Slot {nt}\")\n                subax.set_xlabel(f\"({abcd[nt]}) Slot {nt}\",fontsize=55)\n                #plt.title(f\"Layer {layer} Slot {nt}\")\n                #if  nt==1:\n                    #lgnd=subax.legend(loc=\"lower right\", bbox_to_anchor=(1.3, 0.6))\n                    \n                    #for lh in lgnd.legendHandles:\n                        #lh._sizes=[16]\n            #fig.legend()\n            handles, labels = subax.get_legend_handles_labels()\n            lgnd=fig.legend(handles, labels, loc='right',fontsize=50,bbox_to_anchor=(1.09, 1.04), ncol=4,handletextpad=0.01)\n            #lgnd=fig.legend(bbox_to_anchor=(1.3, 0.6))\n            #lgnd=fig.legend()\n            for lh in lgnd.legendHandles:\n                lh._sizes=[100]\n            fig.tight_layout() \n            #fig.suptitle(f\"Tsne Embedding of Layer {layer}\")\n            plt.savefig(os.path.join(dn,f\"{dataset}_slot_embeddings_layer_{layer}.png\"),bbox_inches=\"tight\")", "\n\n\n\nfile_names=[] # to specify!!!\n\nfor fn,dataset_info_fn in file_names:\n    vis=vis_data_collector()\n    vis_dataset_info=vis_data_collector()\n    vis_dataset_info.load(dataset_info_fn)\n    node_idx_by_ntype=vis_dataset_info.data_dict[\"node_idx_by_ntype\"]\n    vis.load(fn)\n    vis.visualize_tsne(os.path.dirname(fn),node_idx_by_ntype,dataset=dataset_info_fn.split(\"_\")[2])", "                "]}
{"filename": "NC/scripts/NC_F1.py", "chunked_list": ["import json\nfrom collections import Counter\nfrom sklearn.metrics import f1_score\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport os\nimport sys\n\nclass F1:\n    def __init__(self, data_name, pred_files, label_classes, args):\n        self.label_classes = label_classes\n        self.F1_list = {'macro': [], 'micro': []}\n        if len(pred_files)==0:\n            self.F1_mean = {'macro_mean': 0, 'micro_mean': 0}\n            self.F1_std = {'macro_std': 0, 'micro_std': 0}\n        else:\n            true_file = os.path.join(args.ground_dir, data_name, 'label.dat.test')\n            self.ture_label = self.load_labels(true_file)\n\n            for pred_file in pred_files:\n                pred_label = self.load_labels(pred_file)\n                ans = self.evaluate_F1(pred_label)\n                self.F1_list['macro'].append(ans['macro'])\n                self.F1_list['micro'].append(ans['micro'])\n            self.F1_mean = {'macro_mean':np.mean(self.F1_list['macro']), 'micro_mean':np.mean(self.F1_list['micro'])}\n            self.F1_std = {'macro_std': np.std(self.F1_list['macro']), 'micro_std': np.std(self.F1_list['micro'])}\n\n    def load_labels(self, name):\n        \"\"\"\n        return labels dict\n            num_classes: total number of labels\n            total: total number of labeled data\n            count: number of labeled data for each node type\n            data: a numpy matrix with shape (self.nodes['total'], self.labels['num_classes'])\n        \"\"\"\n        labels = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': {}}\n        nc = 0\n        with open(name, 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                node_id, node_name, node_type, node_label = int(th[0]), th[1], int(th[2]), th[3]\n                if node_label.strip()==\"\":\n                    node_label = []\n                else:\n                    node_label = list(map(float, node_label.rstrip().split(',')))\n\n                for i in range(len(node_label)):\n                    node_label[i]=int(node_label[i])\n                    nc = max(nc, node_label[i]+1)\n                labels['data'][node_id] = node_label\n                labels['count'][node_type] += 1\n                labels['total'] += 1\n        labels['num_classes'] = nc\n\n        return labels\n\n    def evaluate_F1(self,pred_label):\n        y_true, y_pred =[], []\n        ans={'macro':0, 'micro':0}\n        for k in self.ture_label['data'].keys():\n            y_true.append(self.ture_label['data'][k])\n            if k not in pred_label['data']:\n                return ans\n            y_pred.append(pred_label['data'][k])\n        if self.label_classes>2:\n            mlp = MultiLabelBinarizer([i for i in range(self.label_classes)])\n            y_pred = mlp.fit_transform(y_pred)\n            y_true = mlp.fit_transform(y_true)\n        ans['macro'] = f1_score(y_true, y_pred, average='macro')\n        ans['micro'] = f1_score(y_true, y_pred, average='micro')\n        return ans", "class F1:\n    def __init__(self, data_name, pred_files, label_classes, args):\n        self.label_classes = label_classes\n        self.F1_list = {'macro': [], 'micro': []}\n        if len(pred_files)==0:\n            self.F1_mean = {'macro_mean': 0, 'micro_mean': 0}\n            self.F1_std = {'macro_std': 0, 'micro_std': 0}\n        else:\n            true_file = os.path.join(args.ground_dir, data_name, 'label.dat.test')\n            self.ture_label = self.load_labels(true_file)\n\n            for pred_file in pred_files:\n                pred_label = self.load_labels(pred_file)\n                ans = self.evaluate_F1(pred_label)\n                self.F1_list['macro'].append(ans['macro'])\n                self.F1_list['micro'].append(ans['micro'])\n            self.F1_mean = {'macro_mean':np.mean(self.F1_list['macro']), 'micro_mean':np.mean(self.F1_list['micro'])}\n            self.F1_std = {'macro_std': np.std(self.F1_list['macro']), 'micro_std': np.std(self.F1_list['micro'])}\n\n    def load_labels(self, name):\n        \"\"\"\n        return labels dict\n            num_classes: total number of labels\n            total: total number of labeled data\n            count: number of labeled data for each node type\n            data: a numpy matrix with shape (self.nodes['total'], self.labels['num_classes'])\n        \"\"\"\n        labels = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': {}}\n        nc = 0\n        with open(name, 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                node_id, node_name, node_type, node_label = int(th[0]), th[1], int(th[2]), th[3]\n                if node_label.strip()==\"\":\n                    node_label = []\n                else:\n                    node_label = list(map(float, node_label.rstrip().split(',')))\n\n                for i in range(len(node_label)):\n                    node_label[i]=int(node_label[i])\n                    nc = max(nc, node_label[i]+1)\n                labels['data'][node_id] = node_label\n                labels['count'][node_type] += 1\n                labels['total'] += 1\n        labels['num_classes'] = nc\n\n        return labels\n\n    def evaluate_F1(self,pred_label):\n        y_true, y_pred =[], []\n        ans={'macro':0, 'micro':0}\n        for k in self.ture_label['data'].keys():\n            y_true.append(self.ture_label['data'][k])\n            if k not in pred_label['data']:\n                return ans\n            y_pred.append(pred_label['data'][k])\n        if self.label_classes>2:\n            mlp = MultiLabelBinarizer([i for i in range(self.label_classes)])\n            y_pred = mlp.fit_transform(y_pred)\n            y_true = mlp.fit_transform(y_true)\n        ans['macro'] = f1_score(y_true, y_pred, average='macro')\n        ans['micro'] = f1_score(y_true, y_pred, average='micro')\n        return ans", "\nimport argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate F1 for NC result.\")\n    parser.add_argument('--pred_zip', type=str, default=\"nc.zip\",\n                        help='Compressed pred files.')\n    parser.add_argument('--ground_dir', type=str, default=\"../data\",\n                        help='Dir of ground files.')\n    parser.add_argument('--log', type=str, default=\"nc.log\",\n                        help='output file')\n    return parser.parse_args()", "\nimport zipfile\n\ndef extract_zip(zip_path, extract_path):\n    zip = zipfile.ZipFile(zip_path, 'r')\n    zip.extractall(extract_path)\n    return zip.namelist()\n\ndef write_log(log_file, log_msg):\n    with open(log_file, 'w') as log_handle:\n        log_handle.write(log_msg)", "def write_log(log_file, log_msg):\n    with open(log_file, 'w') as log_handle:\n        log_handle.write(log_msg)\n\ndef delete_files(files_):\n    for f in files_:\n        if os.path.exists(f):\n            os.remove(f)\n\nif __name__ == '__main__':\n    # get argument settings.\n    args = parse_args()\n    zip_path = args.pred_zip\n    log_msg = ''\n    if not os.path.exists(zip_path):\n        log_msg = 'ERROR: No such zip file!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    extract_path = 'nc'\n    extract_file_list = extract_zip(zip_path, extract_path)\n    extract_file_list = [os.path.join(extract_path, f_) for f_ in extract_file_list]\n\n    data_list = ['DBLP', 'IMDB', 'ACM', 'Freebase']\n    class_count = {'DBLP':2, 'IMDB':5,'ACM':2,'Freebase':2}\n\n    res={}\n    detect_data_files = []\n    for data_name in data_list:\n        pred_files = []\n        for i in range(1,6):\n            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n            if not os.path.exists(file_name):\n                continue\n            pred_files.append(file_name)\n            detect_data_files.append(file_name)\n        if len(pred_files)>0 and len(pred_files)!=5:\n            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n            write_log(args.log, log_msg)\n            delete_files(extract_file_list)\n            sys.exit()\n        res[data_name] = F1(data_name,pred_files,class_count[data_name],args)\n    if len(detect_data_files) == 0:\n        log_msg = f'ERROR: No file detected, please confirm that ' \\\n                  f'the data file is in the top directory of the compressed package!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    delete_files(extract_file_list)\n\n    hgb_score_list = []\n    for data_name in data_list:\n        hgb_score_list.append(res[data_name].F1_mean['macro_mean'])\n        hgb_score_list.append(res[data_name].F1_mean['micro_mean'])\n    hgb_score = np.mean(hgb_score_list)\n\n    detail_json = {}\n    log_msg = f'{hgb_score}###'\n    for data_name in data_list:\n        detail_json[data_name] = {}\n        detail_json[data_name][\"F1 mean\"] = res[data_name].F1_mean\n        detail_json[data_name][\"F1 std\"] = res[data_name].F1_std\n    log_msg += json.dumps(detail_json)\n    write_log(args.log, log_msg)\n    sys.exit()", "\nif __name__ == '__main__':\n    # get argument settings.\n    args = parse_args()\n    zip_path = args.pred_zip\n    log_msg = ''\n    if not os.path.exists(zip_path):\n        log_msg = 'ERROR: No such zip file!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    extract_path = 'nc'\n    extract_file_list = extract_zip(zip_path, extract_path)\n    extract_file_list = [os.path.join(extract_path, f_) for f_ in extract_file_list]\n\n    data_list = ['DBLP', 'IMDB', 'ACM', 'Freebase']\n    class_count = {'DBLP':2, 'IMDB':5,'ACM':2,'Freebase':2}\n\n    res={}\n    detect_data_files = []\n    for data_name in data_list:\n        pred_files = []\n        for i in range(1,6):\n            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n            if not os.path.exists(file_name):\n                continue\n            pred_files.append(file_name)\n            detect_data_files.append(file_name)\n        if len(pred_files)>0 and len(pred_files)!=5:\n            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n            write_log(args.log, log_msg)\n            delete_files(extract_file_list)\n            sys.exit()\n        res[data_name] = F1(data_name,pred_files,class_count[data_name],args)\n    if len(detect_data_files) == 0:\n        log_msg = f'ERROR: No file detected, please confirm that ' \\\n                  f'the data file is in the top directory of the compressed package!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    delete_files(extract_file_list)\n\n    hgb_score_list = []\n    for data_name in data_list:\n        hgb_score_list.append(res[data_name].F1_mean['macro_mean'])\n        hgb_score_list.append(res[data_name].F1_mean['micro_mean'])\n    hgb_score = np.mean(hgb_score_list)\n\n    detail_json = {}\n    log_msg = f'{hgb_score}###'\n    for data_name in data_list:\n        detail_json[data_name] = {}\n        detail_json[data_name][\"F1 mean\"] = res[data_name].F1_mean\n        detail_json[data_name][\"F1 std\"] = res[data_name].F1_std\n    log_msg += json.dumps(detail_json)\n    write_log(args.log, log_msg)\n    sys.exit()", "\n\n\n"]}
{"filename": "NC/scripts/__init__.py", "chunked_list": [""]}
{"filename": "NC/scripts/data_loader.py", "chunked_list": ["import os\nimport numpy as np\nimport scipy.sparse as sp\nfrom collections import Counter, defaultdict\nfrom sklearn.metrics import f1_score,multilabel_confusion_matrix,accuracy_score,auc,precision_score,recall_score\nimport time\nimport copy\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'", "class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'", "\n\nclass data_loader:\n    def __init__(self, path ): \n        self.path = path\n        self.nodes = self.load_nodes()\n        self.links = self.load_links()\n        self.labels_train = self.load_labels('label.dat')\n        self.labels_test = self.load_labels('label.dat.test')\n\n\n    def get_sub_graph(self, node_types_tokeep):\n        \"\"\"\n        node_types_tokeep is a list or set of node types that you want to keep in the sub-graph\n        We only support whole type sub-graph for now.\n        This is an in-place update function!\n        return: old node type id to new node type id dict, old edge type id to new edge type id dict\n        \"\"\"\n        keep = set(node_types_tokeep)\n        new_node_type = 0\n        new_node_id = 0\n        new_nodes = {'total':0, 'count':Counter(), 'attr':{}, 'shift':{}}\n        new_links = {'total':0, 'count':Counter(), 'meta':{}, 'data':defaultdict(list)}\n        new_labels_train = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n        new_labels_test = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n        old_nt2new_nt = {}\n        old_idx = []\n        for node_type in self.nodes['count']:\n            if node_type in keep:\n                nt = node_type\n                nnt = new_node_type\n                old_nt2new_nt[nt] = nnt\n                cnt = self.nodes['count'][nt]\n                new_nodes['total'] += cnt\n                new_nodes['count'][nnt] = cnt\n                new_nodes['attr'][nnt] = self.nodes['attr'][nt]\n                new_nodes['shift'][nnt] = new_node_id\n                beg = self.nodes['shift'][nt]\n                old_idx.extend(range(beg, beg+cnt))\n                \n                cnt_label_train = self.labels_train['count'][nt]\n                new_labels_train['count'][nnt] = cnt_label_train\n                new_labels_train['total'] += cnt_label_train\n                cnt_label_test = self.labels_test['count'][nt]\n                new_labels_test['count'][nnt] = cnt_label_test\n                new_labels_test['total'] += cnt_label_test\n                \n                new_node_type += 1\n                new_node_id += cnt\n\n        new_labels_train['num_classes'] = self.labels_train['num_classes']\n        new_labels_test['num_classes'] = self.labels_test['num_classes']\n        for k in ['data', 'mask']:\n            new_labels_train[k] = self.labels_train[k][old_idx]\n            new_labels_test[k] = self.labels_test[k][old_idx]\n\n        old_et2new_et = {}\n        new_edge_type = 0\n        for edge_type in self.links['count']:\n            h, t = self.links['meta'][edge_type]\n            if h in keep and t in keep:\n                et = edge_type\n                net = new_edge_type\n                old_et2new_et[et] = net\n                new_links['total'] += self.links['count'][et]\n                new_links['count'][net] = self.links['count'][et]\n                new_links['meta'][net] = tuple(map(lambda x:old_nt2new_nt[x], self.links['meta'][et]))\n                new_links['data'][net] = self.links['data'][et][old_idx][:, old_idx]\n                new_edge_type += 1\n\n        self.nodes = new_nodes\n        self.links = new_links\n        self.labels_train = new_labels_train\n        self.labels_test = new_labels_test\n        return old_nt2new_nt, old_et2new_et\n\n    def get_meta_path(self, meta=[]):\n        \"\"\"\n        Get meta path matrix\n            meta is a list of edge types (also can be denoted by a pair of node types)\n            return a sparse matrix with shape [node_num, node_num]\n        \"\"\"\n        ini = sp.eye(self.nodes['total'])\n        meta = [self.get_edge_type(x) for x in meta]\n        for x in meta:\n            ini = ini.dot(self.links['data'][x]) if x >= 0 else ini.dot(self.links['data'][-x - 1].T)\n        return ini\n\n    def dfs(self, now, meta, meta_dict):\n        if len(meta) == 0:\n            meta_dict[now[0]].append(now)\n            return\n        th_mat = self.links['data'][meta[0]] if meta[0] >= 0 else self.links['data'][-meta[0] - 1].T\n        th_node = now[-1]\n        for col in th_mat[th_node].nonzero()[1]:\n            self.dfs(now+[col], meta[1:], meta_dict)\n\n    def get_full_meta_path(self, meta=[], symmetric=False):\n        \"\"\"\n        Get full meta path for each node\n            meta is a list of edge types (also can be denoted by a pair of node types)\n            return a dict of list[list] (key is node_id)\n        \"\"\"\n        meta = [self.get_edge_type(x) for x in meta]\n        if len(meta) == 1:\n            meta_dict = {}\n            start_node_type = self.links['meta'][meta[0]][0] if meta[0]>=0 else self.links['meta'][-meta[0]-1][1]\n            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n                meta_dict[i] = []\n                self.dfs([i], meta, meta_dict)\n        else:\n            meta_dict1 = {}\n            meta_dict2 = {}\n            mid = len(meta) // 2\n            meta1 = meta[:mid]\n            meta2 = meta[mid:]\n            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0]>=0 else self.links['meta'][-meta1[0]-1][1]\n            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n                meta_dict1[i] = []\n                self.dfs([i], meta1, meta_dict1)\n            start_node_type = self.links['meta'][meta2[0]][0] if meta2[0]>=0 else self.links['meta'][-meta2[0]-1][1]\n            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n                meta_dict2[i] = []\n            if symmetric:\n                for k in meta_dict1:\n                    paths = meta_dict1[k]\n                    for x in paths:\n                        meta_dict2[x[-1]].append(list(reversed(x)))\n            else:\n                for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n                    self.dfs([i], meta2, meta_dict2)\n            meta_dict = {}\n            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0]>=0 else self.links['meta'][-meta1[0]-1][1]\n            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n                meta_dict[i] = []\n                for beg in meta_dict1[i]:\n                    for end in meta_dict2[beg[-1]]:\n                        meta_dict[i].append(beg + end[1:])\n        return meta_dict\n\n    def gen_file_for_evaluate(self, test_idx, label, file_path, mode='bi'):\n        if test_idx.shape[0] != label.shape[0]:\n            return\n        if mode == 'multi':\n            multi_label=[]\n            for i in range(label.shape[0]):\n                label_list = [str(j) for j in range(label[i].shape[0]) if label[i][j]==1]\n                multi_label.append(','.join(label_list))\n            label=multi_label\n        elif mode=='bi':\n            label = np.array(label)\n        else:\n            return\n        with open(file_path, \"w\") as f:\n            for nid, l in zip(test_idx, label):\n                f.write(f\"{nid}\\t\\t{self.get_node_type(nid)}\\t{l}\\n\")\n\n    def evaluate(self, pred, mode='bi'):\n        #print(f\"{bcolors.WARNING}Warning: If you want to obtain test score, please submit online on biendata.{bcolors.ENDC}\")\n        y_true = self.labels_test['data'][self.labels_test['mask']]\n        micro = f1_score(y_true, pred, average='micro')\n        macro = f1_score(y_true, pred, average='macro')\n        result = {\n            'micro-f1': micro,\n            'macro-f1': macro,\n        }\n        if  mode=='multi':\n            mcm=multilabel_confusion_matrix(y_true, pred)\n            result.update({\"mcm\":mcm})\n        return result\n\n    def evaluate_by_group(self,pred,group_ids,train=False,mode=\"bi\"):\n        #pred should be all-prediction\n        if len(group_ids)<1:\n            return {'micro-f1': \"NoNodes\",'macro-f1': \"NoNodes\",}\n        if train:\n            labels=self.labels_train['data']\n        else:\n            labels=self.labels_test['data']\n        labels=labels[group_ids]\n        if mode==\"bi\":\n            labels=labels.argmax(-1)\n        y_true=labels\n        pred=pred[group_ids].cpu().detach()\n        micro = f1_score(labels, pred, average='micro')\n        macro = f1_score(labels, pred, average='macro')\n        result = {\n            'micro-f1': micro,\n            'macro-f1': macro,\n            'num':len(group_ids),\n            \"acc\":\"\"\n        }\n        #if  mode=='multi':\n            #mcm=multilabel_confusion_matrix(labels, pred)\n            #result.update({\"mcm\":str(mcm)})\n            #pass\n        if mode==\"bi\":\n            result[\"acc\"]=accuracy_score(labels,pred)\n        result[\"micro-pre\"]=precision_score(y_true,pred, average='micro')\n        result[\"macro-pre\"]=precision_score(y_true,pred, average='macro')\n        result[\"micro-rec\"]=recall_score(y_true, pred, average='micro')\n        result[\"macro-rec\"]=recall_score(y_true, pred, average='macro')\n\n        return result\n        \n\n    def load_labels(self, name):\n        \"\"\"\n        return labels dict\n            num_classes: total number of labels\n            total: total number of labeled data\n            count: number of labeled data for each node type\n            data: a numpy matrix with shape (self.nodes['total'], self.labels['num_classes'])\n            mask: to indicate if that node is labeled, if False, that line of data is masked\n        \"\"\"\n        labels = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n        nc = 0\n        mask = np.zeros(self.nodes['total'], dtype=bool)\n        data = [None for i in range(self.nodes['total'])]\n        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                node_id, node_name, node_type, node_label = int(th[0]), th[1], int(th[2]), list(map(int, th[3].split(',')))\n                for label in node_label:\n                    nc = max(nc, label+1)\n                mask[node_id] = True\n                data[node_id] = node_label\n                labels['count'][node_type] += 1\n                labels['total'] += 1\n        labels['num_classes'] = nc\n        new_data = np.zeros((self.nodes['total'], labels['num_classes']), dtype=int)\n        for i,x in enumerate(data):\n            if x is not None:\n                for j in x:\n                    new_data[i, j] = 1\n        labels['data'] = new_data\n        labels['mask'] = mask\n        return labels\n\n    def get_node_type(self, node_id):\n        for i in range(len(self.nodes['shift'])):\n            if node_id < self.nodes['shift'][i]+self.nodes['count'][i]:\n                return i\n\n    def get_edge_type(self, info):\n        if type(info) is int or len(info) == 1:\n            return info\n        for i in range(len(self.links['meta'])):\n            if self.links['meta'][i] == info:\n                return i\n        info = (info[1], info[0])\n        for i in range(len(self.links['meta'])):\n            if self.links['meta'][i] == info:\n                return -i - 1\n        raise Exception('No available edge type')\n\n    def get_edge_info(self, edge_id):\n        return self.links['meta'][edge_id]\n    \n    def list_to_sp_mat(self, li):\n        data = [x[2] for x in li]\n        i = [x[0] for x in li]\n        j = [x[1] for x in li]\n        return sp.coo_matrix((data, (i,j)), shape=(self.nodes['total'], self.nodes['total'])).tocsr()\n    \n    def load_links(self):\n        \"\"\"\n        return links dict\n            total: total number of links\n            count: a dict of int, number of links for each type\n            meta: a dict of tuple, explaining the link type is from what type of node to what type of node\n            data: a dict of sparse matrices, each link type with one matrix. Shapes are all (nodes['total'], nodes['total'])\n        \"\"\"\n        links = {'total':0, 'count':Counter(), 'meta':{}, 'data':defaultdict(list)}\n        r_ids=[]\n        with open(os.path.join(self.path, 'link.dat'), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n                \n                if h_id in self.old_to_new_id_mapping.keys() and t_id in self.old_to_new_id_mapping.keys() :\n                    h_id=self.old_to_new_id_mapping[h_id]\n                    t_id=self.old_to_new_id_mapping[t_id]\n                    if r_id not in links['meta']:\n                        h_type = self.get_node_type(h_id)\n                        t_type = self.get_node_type(t_id)\n                        links['meta'][r_id] = (h_type, t_type)\n                    links['data'][r_id].append((h_id, t_id, link_weight))\n                    links['count'][r_id] += 1\n                    links['total'] += 1\n                    if r_id not in r_ids:\n                        r_ids.append(r_id)\n        r_ids=sorted(r_ids)\n\n        temp_meta={}\n        for i in range(len(links['meta'].keys())):\n            temp_meta[i]=links['meta'][r_ids[i]]\n        links['meta']=temp_meta\n\n        temp_count={}\n        for i in range(len(links['count'].keys())):\n            temp_count[i]=links['count'][r_ids[i]]\n        links['count']=temp_count\n\n        temp_data={}\n        for i in range(len(links['data'].keys())):\n            temp_data[i]=links['data'][r_ids[i]]\n        links['data']=temp_data\n\n\n\n\n\n        new_data = {}\n        for r_id in links['data']:\n            new_data[r_id] = self.list_to_sp_mat(links['data'][r_id])\n        links['data'] = new_data\n        return links\n\n    def load_nodes(self):\n        \"\"\"\n        return nodes dict\n            total: total number of nodes\n            count: a dict of int, number of nodes for each type\n            attr: a dict of np.array (or None), attribute matrices for each type of nodes\n            shift: node_id shift for each type. You can get the id range of a type by \n                        [ shift[node_type], shift[node_type]+count[node_type] )\n        \"\"\"\n        nodes = {'total':0, 'count':Counter(), 'attr':{}, 'shift':{}}\n        node_ids=[]\n        print(self.path)\n        #with open(os.path.join(self.path, 'new.txt'), 'w') as f:\n            #f.write(\"1\")\n\n\n        with open(os.path.join(self.path, 'node.dat'), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                if len(th) == 4:\n                    # Then this line of node has attribute\n                    node_id, node_name, node_type, node_attr = th\n                    node_id = int(node_id)\n                    node_type = int(node_type)\n                    node_attr = list(map(float, node_attr.split(',')))\n                    node_ids.append(node_id)\n                    nodes['count'][node_type] += 1\n                    nodes['attr'][node_id] = node_attr\n                    nodes['total'] += 1\n                elif len(th) == 3:\n                    # Then this line of node doesn't have attribute\n                    node_id, node_name, node_type = th\n                    node_id = int(node_id)\n                    node_type = int(node_type)\n                    node_ids.append(node_id)\n                    nodes['count'][node_type] += 1\n                    nodes['total'] += 1\n                else:\n                    raise Exception(\"Too few information to parse!\")\n        \n        #type_id mapping\n        temp_count=Counter()\n        for k in nodes['count']:\n            \n            temp_count[k]=nodes['count'][k]\n        nodes['count']=temp_count\n\n        #node_id mapping\n        self.old_to_new_id_mapping={}\n        self.new_to_old_id_mapping={}\n        node_ids=sorted(node_ids)\n        for new_id in range(len(node_ids)):\n            self.old_to_new_id_mapping[node_ids[new_id]]=new_id\n            self.new_to_old_id_mapping[new_id]=node_ids[new_id]\n        temp_attr={}\n        for old_id in node_ids:\n            if old_id in nodes['attr'].keys():\n                temp_attr[self.old_to_new_id_mapping[old_id]]=nodes['attr'][old_id]\n        nodes['attr']=temp_attr\n\n        shift = 0\n        attr = {}\n        for i in range(len(nodes['count'])):\n            nodes['shift'][i] = shift\n            if shift in nodes['attr']:\n                mat = []\n                for j in range(shift, shift+nodes['count'][i]):\n                    mat.append(nodes['attr'][j])\n                attr[i] = np.array(mat)\n            else:\n                attr[i] = None\n            shift += nodes['count'][i]\n        nodes['attr'] = attr\n        return nodes", "\n"]}
{"filename": "LP/methods/slotGAT/conv.py", "chunked_list": ["\"\"\"Torch modules for graph attention networks(GAT).\"\"\"\n# pylint: disable= no-member, arguments-differ, invalid-name\nimport torch as th\nimport torch\nfrom torch import nn\n\nfrom dgl import function as fn\nfrom dgl.nn.pytorch import edge_softmax\nfrom dgl._ffi.base import DGLError\nfrom dgl.nn.pytorch.utils import Identity", "from dgl._ffi.base import DGLError\nfrom dgl.nn.pytorch.utils import Identity\nfrom dgl.utils import expand_as_pair\n\n\n\n\n\nclass slotGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.,\n                 num_ntype=None,eindexer=None,inputhead=False):\n        super(slotGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None \n        self.eindexer=eindexer\n        self.num_ntype=num_ntype \n        self.attentions=None   \n\n        if isinstance(in_feats, tuple):\n            raise NotImplementedError()\n        else:\n            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n            \"\"\"else:\n                self.fc =nn.ModuleList([nn.Linear(\n                    self._in_src_feats, out_feats * num_heads, bias=False)  for _ in range(num_ntype)] )\n                raise Exception(\"!!!\")\"\"\"\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None \n        \n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n                \n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        if bias:\n            raise NotImplementedError()\n        self.alpha = alpha\n        self.inputhead=inputhead\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc, gain=gain)\n            \n        else:\n            raise Exception(\"!!!\")\n            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain) \n        \n        if  self._edge_feats:\n            nn.init.xavier_normal_(self.attn_e, gain=gain) \n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        elif isinstance(self.res_fc, Identity):\n            pass\n        elif isinstance(self.res_fc, nn.Parameter):\n            nn.init.xavier_normal_(self.res_fc, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n        with graph.local_scope():\n            node_idx_by_ntype=graph.node_idx_by_ntype\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                raise Exception(\"!!!\")\n                h_src = self.feat_drop(feat[0])\n                h_dst = self.feat_drop(feat[1])\n                if not hasattr(self, 'fc_src'):\n                    self.fc_src, self.fc_dst = self.fc, self.fc\n                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n            else:\n                #feature transformation first\n                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n                \n\n\n                if self.inputhead:\n                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n                else:\n                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n                if \"getEmb\" in get_out:\n                    self.emb=h_dst.cpu().detach()\n                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads) \n                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n                \n                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                    \n\n\n\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n        \n            \n            \n            e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n            \n            \n            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n            graph.srcdata.update({'ft': feat_src, 'el': el})\n            graph.dstdata.update({'er': er})\n            graph.edata.update({'ee': ee}) if self._edge_feats else None\n            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n            e_=graph.edata.pop('e')\n            ee=graph.edata.pop('ee') if self._edge_feats else 0\n            e=e_+ee\n            \n            e = self.leaky_relu(e)\n            # compute softmax\n            a=self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                a=a * (1-self.alpha) + res_attn * self.alpha \n\n            graph.edata['a'] = a\n            # then message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n                             \n            rst = graph.dstdata['ft']\n            # residual\n            if self.res_fc is not None:\n                if self._in_dst_feats != self._out_feats:\n                    resval =torch.bmm(h_src,self.res_fc)\n                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                    #resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n                else:\n                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n                \n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            self.attentions=graph.edata.pop('a').detach()\n            torch.cuda.empty_cache()\n            return rst, self.attentions", "class slotGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.,\n                 num_ntype=None,eindexer=None,inputhead=False):\n        super(slotGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None \n        self.eindexer=eindexer\n        self.num_ntype=num_ntype \n        self.attentions=None   \n\n        if isinstance(in_feats, tuple):\n            raise NotImplementedError()\n        else:\n            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n            \"\"\"else:\n                self.fc =nn.ModuleList([nn.Linear(\n                    self._in_src_feats, out_feats * num_heads, bias=False)  for _ in range(num_ntype)] )\n                raise Exception(\"!!!\")\"\"\"\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None \n        \n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n                \n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        if bias:\n            raise NotImplementedError()\n        self.alpha = alpha\n        self.inputhead=inputhead\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc, gain=gain)\n            \n        else:\n            raise Exception(\"!!!\")\n            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain) \n        \n        if  self._edge_feats:\n            nn.init.xavier_normal_(self.attn_e, gain=gain) \n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        elif isinstance(self.res_fc, Identity):\n            pass\n        elif isinstance(self.res_fc, nn.Parameter):\n            nn.init.xavier_normal_(self.res_fc, gain=gain)\n        if self._edge_feats:\n            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n        with graph.local_scope():\n            node_idx_by_ntype=graph.node_idx_by_ntype\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                raise Exception(\"!!!\")\n                h_src = self.feat_drop(feat[0])\n                h_dst = self.feat_drop(feat[1])\n                if not hasattr(self, 'fc_src'):\n                    self.fc_src, self.fc_dst = self.fc, self.fc\n                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n            else:\n                #feature transformation first\n                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n                \n\n\n                if self.inputhead:\n                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n                else:\n                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n                if \"getEmb\" in get_out:\n                    self.emb=h_dst.cpu().detach()\n                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads) \n                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n                \n                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                    \n\n\n\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n        \n            \n            \n            e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n            \n            \n            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n            graph.srcdata.update({'ft': feat_src, 'el': el})\n            graph.dstdata.update({'er': er})\n            graph.edata.update({'ee': ee}) if self._edge_feats else None\n            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n            e_=graph.edata.pop('e')\n            ee=graph.edata.pop('ee') if self._edge_feats else 0\n            e=e_+ee\n            \n            e = self.leaky_relu(e)\n            # compute softmax\n            a=self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                a=a * (1-self.alpha) + res_attn * self.alpha \n\n            graph.edata['a'] = a\n            # then message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n                             \n            rst = graph.dstdata['ft']\n            # residual\n            if self.res_fc is not None:\n                if self._in_dst_feats != self._out_feats:\n                    resval =torch.bmm(h_src,self.res_fc)\n                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n                    #resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n                else:\n                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n                \n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            self.attentions=graph.edata.pop('a').detach()\n            torch.cuda.empty_cache()\n            return rst, self.attentions", "\n\n\n\n\n\n\n\n\n", "\n\n# pylint: enable=W0235\nclass myGATConv(nn.Module):\n    \"\"\"\n    Adapted from\n    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n    \"\"\"\n    def __init__(self,\n                 edge_feats,\n                 num_etypes,\n                 in_feats,\n                 out_feats,\n                 num_heads,\n                 feat_drop=0.,\n                 attn_drop=0.,\n                 negative_slope=0.2,\n                 residual=False,\n                 activation=None,\n                 allow_zero_in_degree=False,\n                 bias=False,\n                 alpha=0.):\n        super(myGATConv, self).__init__()\n        self._edge_feats = edge_feats\n        self._num_heads = num_heads\n        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n        self._out_feats = out_feats\n        self._allow_zero_in_degree = allow_zero_in_degree\n        self.edge_emb = nn.Embedding(num_etypes, edge_feats)\n        if isinstance(in_feats, tuple):\n            self.fc_src = nn.Linear(\n                self._in_src_feats, out_feats * num_heads, bias=False)\n            self.fc_dst = nn.Linear(\n                self._in_dst_feats, out_feats * num_heads, bias=False)\n        else:\n            self.fc = nn.Linear(\n                self._in_src_feats, out_feats * num_heads, bias=False)\n        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n        self.feat_drop = nn.Dropout(feat_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        if residual:\n            if self._in_dst_feats != out_feats:\n                self.res_fc = nn.Linear(\n                    self._in_dst_feats, num_heads * out_feats, bias=False)\n            else:\n                self.res_fc = Identity()\n        else:\n            self.register_buffer('res_fc', None)\n        self.reset_parameters()\n        self.activation = activation\n        self.bias = bias\n        if bias:\n            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats)))\n        self.alpha = alpha\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        if hasattr(self, 'fc'):\n            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        else:\n            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_l, gain=gain)\n        nn.init.xavier_normal_(self.attn_r, gain=gain)\n        nn.init.xavier_normal_(self.attn_e, gain=gain)\n        if isinstance(self.res_fc, nn.Linear):\n            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\n    def set_allow_zero_in_degree(self, set_value):\n        self._allow_zero_in_degree = set_value\n\n    def forward(self, graph, feat, e_feat, res_attn=None):\n        with graph.local_scope():\n            if not self._allow_zero_in_degree:\n                if (graph.in_degrees() == 0).any():\n                    raise DGLError('There are 0-in-degree nodes in the graph, '\n                                   'output for those nodes will be invalid. '\n                                   'This is harmful for some applications, '\n                                   'causing silent performance regression. '\n                                   'Adding self-loop on the input graph by '\n                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n                                   'the issue. Setting ``allow_zero_in_degree`` '\n                                   'to be `True` when constructing this module will '\n                                   'suppress the check and let the code run.')\n\n            if isinstance(feat, tuple):\n                h_src = self.feat_drop(feat[0])\n                h_dst = self.feat_drop(feat[1])\n                if not hasattr(self, 'fc_src'):\n                    self.fc_src, self.fc_dst = self.fc, self.fc\n                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n            else:\n                h_src = h_dst = self.feat_drop(feat)\n                feat_src = feat_dst = self.fc(h_src).view(\n                    -1, self._num_heads, self._out_feats)\n                if graph.is_block:\n                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n            e_feat = self.edge_emb(e_feat)\n            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n            graph.srcdata.update({'ft': feat_src, 'el': el})\n            graph.dstdata.update({'er': er})\n            graph.edata.update({'ee': ee})\n            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n            # compute softmax\n            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n            if res_attn is not None:\n                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n            # message passing\n            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n                             fn.sum('m', 'ft'))\n            rst = graph.dstdata['ft']\n            # residual\n            if self.res_fc is not None:\n                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n                rst = rst + resval\n            # bias\n            if self.bias:\n                rst = rst + self.bias_param\n            # activation\n            if self.activation:\n                rst = self.activation(rst)\n            return rst, graph.edata.pop('a').detach()", "\n"]}
{"filename": "LP/methods/slotGAT/GNN.py", "chunked_list": ["import torch\nimport torch as th\nimport torch.nn as nn\nimport dgl\nfrom dgl.nn.pytorch import GraphConv\nimport math\nimport torch.nn.functional as F\nimport dgl.function as fn\nfrom dgl.nn.pytorch import edge_softmax, GATConv\nfrom conv import myGATConv,slotGATConv", "from dgl.nn.pytorch import edge_softmax, GATConv\nfrom conv import myGATConv,slotGATConv\n\nfrom dgl._ffi.base import DGLError\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\"\"\"\nclass DistMult(nn.Module):\n    def __init__(self, num_rel, dim):\n        super(DistMult, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(size=(num_rel, dim, dim)))", "        super(DistMult, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(size=(num_rel, dim, dim)))\n        nn.init.xavier_normal_(self.W, gain=1.414)\n\n    def forward(self, left_emb, right_emb, r_id):\n        thW = self.W[r_id]\n        left_emb = torch.unsqueeze(left_emb, 1)\n        right_emb = torch.unsqueeze(right_emb, 2)\n        return torch.bmm(torch.bmm(left_emb, thW), right_emb).squeeze()\"\"\"\n", "        return torch.bmm(torch.bmm(left_emb, thW), right_emb).squeeze()\"\"\"\n\n\nclass DistMult(nn.Module):\n    def __init__(self, num_rel, dim):\n        super(DistMult, self).__init__()\n        self.W = nn.Parameter(torch.FloatTensor(size=(num_rel, dim, dim)))\n        nn.init.xavier_normal_(self.W, gain=1.414)\n    def forward(self, left_emb, right_emb, r_id,slot_num=None,prod_aggr=None,sigmoid=\"after\"):\n        if not prod_aggr:\n            thW = self.W[r_id]\n            left_emb = torch.unsqueeze(left_emb, 1)\n            right_emb = torch.unsqueeze(right_emb, 2)\n            #return torch.bmm(torch.bmm(left_emb, thW), right_emb).squeeze()\n            scores=torch.zeros(right_emb.shape[0]).to(right_emb.device)\n            for i in range(int(max(r_id))+1):\n                scores[r_id==i]=torch.bmm(torch.matmul(left_emb[r_id==i], self.W[i]), right_emb[r_id==i]).squeeze()\n            return scores\n        else:\n            raise Exception", "\n\nclass Dot(nn.Module):\n    def __init__(self):\n        super(Dot, self).__init__()\n    def forward(self, left_emb, right_emb, r_id,slot_num=None,prod_aggr=None,sigmoid=\"after\"):\n        if not prod_aggr:\n            left_emb = torch.unsqueeze(left_emb, 1)\n            right_emb = torch.unsqueeze(right_emb, 2)\n            return torch.bmm(left_emb, right_emb).squeeze()\n        else:\n            left_emb = left_emb.view(-1,slot_num,int(left_emb.shape[1]/slot_num))\n            right_emb = right_emb.view(-1,int(right_emb.shape[1]/slot_num),slot_num)\n            x=torch.bmm(left_emb, right_emb)# num_sampled_edges* num_slot*num_slot\n            if prod_aggr==\"all\":\n                x=x.flatten(1)\n                x=x.sum(1)\n                return x\n            x=torch.diagonal(x,0,1,2) # num_sampled_edges* num_slot\n            if sigmoid==\"before\":\n                x=F.sigmoid(x)\n            \n            if prod_aggr==\"mean\":\n                x=x.mean(1)\n                \n            elif prod_aggr==\"max\":\n                x=x.max(1)[0]\n            elif prod_aggr==\"sum\":\n                x=x.sum(1)\n            else:\n                raise Exception()\n            return x", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\nclass myGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha,\n                 decode='distmult',inProcessEmb=\"True\",l2use=\"True\",dataRecorder=None,get_out=None):\n        super(myGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.inProcessEmb=inProcessEmb\n        self.l2use=l2use\n        self.dataRecorder=dataRecorder\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n                num_hidden * heads[l-1], num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))\n        # output projection\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden * heads[-2], num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))\n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n        if decode == 'distmult':\n            self.decoder = DistMult(num_etypes, num_classes*(num_layers+2))\n        elif decode == 'dot':\n            self.decoder = Dot()\n        self.get_out=get_out\n\n    def l2_norm(self, x):\n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        return x / (torch.max(torch.norm(x, dim=1, keepdim=True), self.epsilon))\n\n    def forward(self, features_list, e_feat, left, right, mid):\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n        emb = [self.l2_norm(h)]\n        res_attn = None\n        for l in range(self.num_layers):\n            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n            emb.append(self.l2_norm(h.mean(1)))\n            h = h.flatten(1)\n        # output projection\n        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=res_attn)#None)\n        logits = logits.mean(1)\n        logits = self.l2_norm(logits)\n        #emb.append(logits)\n        if self.inProcessEmb==\"True\":\n            emb.append(logits)\n        else:\n            emb=[logits]\n        logits = torch.cat(emb, 1)\n        left_emb = logits[left]\n        right_emb = logits[right]\n        return F.sigmoid(self.decoder(left_emb, right_emb, mid))", "class myGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha,\n                 decode='distmult',inProcessEmb=\"True\",l2use=\"True\",dataRecorder=None,get_out=None):\n        super(myGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.activation = activation\n        self.inProcessEmb=inProcessEmb\n        self.l2use=l2use\n        self.dataRecorder=dataRecorder\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n        # input projection (no residual)\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n                num_hidden * heads[l-1], num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))\n        # output projection\n        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n            num_hidden * heads[-2], num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))\n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n        if decode == 'distmult':\n            self.decoder = DistMult(num_etypes, num_classes*(num_layers+2))\n        elif decode == 'dot':\n            self.decoder = Dot()\n        self.get_out=get_out\n\n    def l2_norm(self, x):\n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        return x / (torch.max(torch.norm(x, dim=1, keepdim=True), self.epsilon))\n\n    def forward(self, features_list, e_feat, left, right, mid):\n        h = []\n        for fc, feature in zip(self.fc_list, features_list):\n            h.append(fc(feature))\n        h = torch.cat(h, 0)\n        emb = [self.l2_norm(h)]\n        res_attn = None\n        for l in range(self.num_layers):\n            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n            emb.append(self.l2_norm(h.mean(1)))\n            h = h.flatten(1)\n        # output projection\n        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=res_attn)#None)\n        logits = logits.mean(1)\n        logits = self.l2_norm(logits)\n        #emb.append(logits)\n        if self.inProcessEmb==\"True\":\n            emb.append(logits)\n        else:\n            emb=[logits]\n        logits = torch.cat(emb, 1)\n        left_emb = logits[left]\n        right_emb = logits[right]\n        return F.sigmoid(self.decoder(left_emb, right_emb, mid))", "\n\n       \nclass slotGAT(nn.Module):\n    def __init__(self,\n                 g,\n                 edge_dim,\n                 num_etypes,\n                 in_dims,\n                 num_hidden,\n                 num_classes,\n                 num_layers,\n                 heads,\n                 activation,\n                 feat_drop,\n                 attn_drop,\n                 negative_slope,\n                 residual,\n                 alpha,\n                 num_ntype,\n                 eindexer,aggregator=\"average\",predicted_by_slot=\"None\", get_out=[\"\"],\n                 decode='distmult',inProcessEmb=\"True\",l2BySlot=\"False\",prod_aggr=None,sigmoid=\"after\",l2use=\"True\",SAattDim=128,dataRecorder=None):\n        super(slotGAT, self).__init__()\n        self.g = g\n        self.num_layers = num_layers\n        self.gat_layers = nn.ModuleList()\n        self.heads=heads\n        self.activation = activation\n        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n        self.num_ntype=num_ntype\n        self.num_classes=num_classes\n        self.leaky_relu = nn.LeakyReLU(negative_slope)\n        self.predicted_by_slot=predicted_by_slot  \n        self.inProcessEmb=inProcessEmb \n        self.l2BySlot=l2BySlot\n        self.prod_aggr=prod_aggr\n        self.sigmoid=sigmoid\n        self.l2use=l2use\n        self.SAattDim=SAattDim\n        self.dataRecorder=dataRecorder\n        \n        self.get_out=get_out \n        self.num_etypes=num_etypes\n        self.num_hidden=num_hidden\n        self.last_fc = nn.Parameter(th.FloatTensor(size=(num_classes*self.num_ntype, num_classes))) ;nn.init.xavier_normal_(self.last_fc, gain=1.414)\n        \n        for fc in self.fc_list:\n            nn.init.xavier_normal_(fc.weight, gain=1.414)\n            \n        # input projection (no residual)\n        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n            num_hidden, num_hidden, heads[0],\n            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer,inputhead=True))\n        # hidden layers\n        for l in range(1, num_layers):\n            # due to multi-head, the in_dim = num_hidden * num_heads\n            self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n                num_hidden* heads[l-1] , num_hidden, heads[l],\n                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer))\n        # output projection\n        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n            num_hidden* heads[-2] , num_classes, heads[-1],\n            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer))\n        self.aggregator=aggregator\n        if aggregator==\"SA\":\n            if self.inProcessEmb==\"True\":\n                last_dim=num_hidden*(2+num_layers)\n            else:\n                last_dim=num_hidden\n                \n            \n            self.macroLinear=nn.Linear(last_dim, self.SAattDim, bias=True);nn.init.xavier_normal_(self.macroLinear.weight, gain=1.414);nn.init.normal_(self.macroLinear.bias, std=1.414*math.sqrt(1/(self.macroLinear.bias.flatten().shape[0])))\n            self.macroSemanticVec=nn.Parameter(torch.FloatTensor(self.SAattDim,1));nn.init.normal_(self.macroSemanticVec,std=1)\n        \n\n        self.by_slot=[f\"by_slot_{nt}\" for nt in range(num_ntype)]\n        assert aggregator in ([\"average\",\"last_fc\",\"max\",\"None\",\"SA\"]+self.by_slot)\n        #self.get_out=get_out\n        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n        if decode == 'distmult':\n            if self.aggregator==\"None\":\n                num_classes=num_classes*num_ntype\n            self.decoder = DistMult(num_etypes, num_classes*(num_layers+2))\n        elif decode == 'dot':\n            self.decoder = Dot()\n\n\n    def forward(self, features_list,e_feat, left, right, mid, get_out=\"False\"): \n        encoded_embeddings=None\n        \n        h = []\n        for nt_id,(fc, feature) in enumerate(zip(self.fc_list, features_list)):\n            nt_ft=fc(feature)\n            emsen_ft=torch.zeros([nt_ft.shape[0],nt_ft.shape[1]*self.num_ntype]).to(feature.device)\n            emsen_ft[:,nt_ft.shape[1]*nt_id:nt_ft.shape[1]*(nt_id+1)]=nt_ft\n            h.append(emsen_ft)   # the id is decided by the node types\n        h = torch.cat(h, 0)        #  num_nodes*(num_type*hidden_dim)\n        \n        emb = [self.aggr_func(self.l2_norm(h,l2BySlot=self.l2BySlot))]\n        res_attn = None\n        for l in range(self.num_layers):\n            h, res_attn = self.gat_layers[l](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)   #num_nodes*num_heads*(num_ntype*hidden_dim)\n            emb.append(self.aggr_func(self.l2_norm(h.mean(1),l2BySlot=self.l2BySlot)))\n            h = h.flatten(1)#num_nodes*(num_heads*num_ntype*hidden_dim)\n            \n        # output projection\n        logits, _ = self.gat_layers[-1](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)#None)   #num_nodes*num_heads*num_ntype*hidden_dim\n        \n        \n        logits = logits.mean(1)\n        if self.predicted_by_slot!=\"None\" and self.training==False:\n            logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n            \n            if self.predicted_by_slot==\"max\":\n                if \"getMaxSlot\" in  get_out:\n                    maxSlotIndexesWithLabels=logits.max(2)[1].squeeze(1)\n                    logits_indexer=logits.max(2)[0].max(2)[1]\n                    self.maxSlotIndexes=torch.gather(maxSlotIndexesWithLabels,1,logits_indexer)\n                logits=logits.max(2)[0]\n            elif self.predicted_by_slot==\"all\":\n                if \"getSlots\" in get_out:\n                    self.logits=logits.detach()\n                logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)\n\n            else:\n                target_slot=int(self.predicted_by_slot)\n                logits=logits[:,:,target_slot,:].squeeze(2)\n        else:\n            logits=self.aggr_func(self.l2_norm(logits,l2BySlot=self.l2BySlot))\n            \n        \n        if self.inProcessEmb==\"True\":\n            emb.append(logits)\n        else:\n            emb=[logits]\n        if self.aggregator==\"None\" and self.inProcessEmb==\"True\":\n            emb=[ x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))   for x in emb]\n            o = torch.cat(emb, 2).flatten(1)\n        else:\n            o = torch.cat(emb, 1)\n        if self.aggregator==\"SA\" :\n            o=o.view(-1, self.num_ntype,int(o.shape[1]/self.num_ntype))\n            \n            slot_scores=(F.tanh( self.macroLinear(o))  @  self.macroSemanticVec).mean(0,keepdim=True)  #num_slots\n            self.slot_scores=F.softmax(slot_scores,dim=1)\n            o=(o*self.slot_scores).sum(1)  \n\n        left_emb = o[left]\n        right_emb = o[right]\n        if self.sigmoid==\"after\":\n            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr)\n            logits=F.sigmoid(logits)\n        elif self.sigmoid==\"before\":\n            \n            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr,sigmoid=self.sigmoid)\n        elif self.sigmoid==\"None\":\n            left_emb=self.l2_norm(left_emb,l2BySlot=self.l2BySlot)\n            right_emb=self.l2_norm(right_emb,l2BySlot=self.l2BySlot)\n            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr)\n        else:\n            raise Exception()\n        return logits\n\n\n    def l2_norm(self, x,l2BySlot=\"False\"):\n        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n        if self.l2use==\"True\":\n            if l2BySlot==\"False\":\n                return x / (torch.max(torch.norm(x, dim=1, keepdim=True), self.epsilon))\n            elif l2BySlot==\"True\":\n                x=x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))\n                x=x / (torch.max(torch.norm(x, dim=2, keepdim=True), self.epsilon))\n                x=x.flatten(1)\n                return x\n        elif self.l2use==\"False\":\n            return x\n        else:\n            raise Exception()\n\n\n    def aggr_func(self,logits):\n        if self.aggregator==\"average\":\n            logits=logits.view(-1, self.num_ntype,self.num_classes).mean(1)\n        elif self.aggregator==\"last_fc\":\n            logits=logits.view(-1,self.num_ntype,self.num_classes)\n            logits=logits.flatten(1)\n            logits=logits.matmul(self.last_fc).unsqueeze(1)\n        elif self.aggregator==\"max\":\n            logits=logits.view(-1,self.num_ntype,self.num_classes).max(1)[0]\n        \n        elif self.aggregator==\"None\" or \"SA\":\n            logits=logits.view(-1, self.num_ntype,self.num_classes).flatten(1)\n\n\n\n        else:\n            raise NotImplementedError()\n        \n        return logits", "\n"]}
{"filename": "LP/methods/slotGAT/pipeline_utils.py", "chunked_list": ["\nimport random\nimport queue\nimport time\nimport subprocess\nimport multiprocessing\nfrom threading import main_thread\nimport os\nimport pandas as pd\n", "import pandas as pd\n\nimport copy\n\n\n\n\n\n\n", "\n\n\nclass Run( multiprocessing.Process):\n    def __init__(self,task,pool=0,idx=0,tc=0,start_time=0):\n        super().__init__()\n        self.task=task\n        self.log=os.path.join(task['study_name'])\n        self.idx=idx\n        self.pool=pool\n        self.device=None\n        self.tc=tc\n        self.start_time=start_time\n        #self.pbar=pbar\n    def run(self):\n        print(f\"{'*'*10} study  {self.log} no.{self.idx} waiting for device\")\n        count=0\n        device_units=[]\n        while True:\n            if len(device_units)>0:\n                try:\n                    unit=self.pool.get(timeout=10*random.random())\n                except queue.Empty:\n                    for unit in device_units:\n                            self.pool.put(unit)\n                    print(f\"Hold {str(device_units)} and waiting for too long! Throw back and go to sleep\")\n                    time.sleep(10*random.random())\n                    device_units=[]\n                    count=0\n                    continue\n            else:\n                unit=self.pool.get()\n            if len(device_units)>0:  # consistency check\n                if unit[0]!=device_units[-1][0]:\n                    print(f\"Get {str(device_units)} and {unit} not consistent devices and throw back it\")\n                    self.pool.put(unit)\n                    time.sleep(10*random.random())\n                    continue\n            count+=1\n            device_units.append(unit)\n            if count==self.task['cost']:\n                break\n\n\n        print(f\"{'-'*10}  study  {self.log} no.{self.idx} get the devices {str(device_units)} and start working\")\n        self.device=device_units[0][0]\n        try:\n            exit_command=get_command_from_argsDict(self.task,self.device,self.idx)\n            \n            print(f\"running: {exit_command}\")\n            subprocess.run(exit_command,shell=True)\n        finally:\n            for unit in device_units:\n                self.pool.put(unit)\n            #localtime = time.asctime( time.localtime(time.time()) )\n        \n        end_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        print(f\"Start time: {self.start_time}\\nEnd time: {end_time}\\nwith {self.idx}/{self.tc} tasks\")\n\n        print(f\"  {'<'*10} end  study  {self.log} no.{self.idx} of command \")", "\n\n\ndef get_command_from_argsDict(args_dict,gpu,idx):\n    command='python -W ignore run_dist.py  '\n    for key in args_dict.keys():\n        command+=f\" --{key} {args_dict[key]} \"\n\n\n    command+=f\" --gpu {gpu} \"\n    if os.name!=\"nt\":\n        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n    return command", "\n\n\n\n\ndef config_study_name(prefix,specified_args,extract_dict):\n    study_name=prefix\n    for k in specified_args:\n        v=extract_dict[k]\n        study_name+=f\"_{k}_{v}\"\n    if study_name[0]==\"_\":\n        study_name=study_name.replace(\"_\",\"\",1) \n    return study_name ", ""]}
{"filename": "LP/methods/slotGAT/run_dist.py", "chunked_list": ["import sys\nsys.path.append('../../')\nimport time\nimport argparse\nimport json\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np", "import torch.nn.functional as F\nimport numpy as np\nfrom utils.tools import writeIntoCsvLogger,vis_data_collector,blank_profile\nfrom utils.pytorchtools import EarlyStopping\nfrom utils.data import load_data\nfrom GNN import myGAT,slotGAT\nfrom matplotlib import pyplot as plt\nimport dgl\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom torch.profiler import tensorboard_trace_handler", "from torch.profiler import profile, record_function, ProfilerActivity\nfrom torch.profiler import tensorboard_trace_handler\nimport os\nimport random\ntorch.set_num_threads(4)\n\n\ndef sp_to_spt(mat):\n    coo = mat.tocoo()\n    values = coo.data\n    indices = np.vstack((coo.row, coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = coo.shape\n\n    return torch.sparse.FloatTensor(i, v, torch.Size(shape))", "\ndef mat2tensor(mat):\n    if type(mat) is np.ndarray:\n        return torch.from_numpy(mat).type(torch.FloatTensor)\n    return sp_to_spt(mat)\n\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    np.random.seed(seed)  # Numpy module.\n    random.seed(seed)  # Python random module.\n\n    torch.use_deterministic_algorithms(True)\n    torch.backends.cudnn.enabled = False \n    torch.backends.cudnn.benchmark = False\n    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n    os.environ['PYTHONHASHSEED'] = str(seed)", "\n\ndef run_model_DBLP(args):\n\n    set_seed(args.seed)\n    get_out=args.get_out.split(\"_\") \n    dataRecorder={\"meta\":{},\"data\":{},\"status\":\"None\"}\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n    exp_info=f\"exp setting: {vars(args)}\"\n    vis_data_saver=vis_data_collector()\n    vis_data_saver.save_meta(exp_info,\"exp_info\")\n    try:\n        torch.cuda.set_device(int(args.gpu))\n    except :\n        pass\n    feats_type = args.feats_type\n    features_list, adjM, dl = load_data(args.dataset)\n    device = torch.device('cuda' if (torch.cuda.is_available() and args.gpu!=\"cpu\") else 'cpu')\n    features_list = [mat2tensor(features).to(device) for features in features_list]\n    if feats_type == 0:\n        in_dims = [features.shape[1] for features in features_list]\n    elif feats_type == 1 or feats_type == 5:\n        save = 0 if feats_type == 1 else 2\n        in_dims = []#[features_list[0].shape[1]] + [10] * (len(features_list) - 1)\n        for i in range(0, len(features_list)):\n            if i == save:\n                in_dims.append(features_list[i].shape[1])\n            else:\n                in_dims.append(10)\n                features_list[i] = torch.zeros((features_list[i].shape[0], 10)).to(device)\n    elif feats_type == 2 or feats_type == 4:\n        save = feats_type - 2\n        in_dims = [features.shape[0] for features in features_list]\n        for i in range(0, len(features_list)):\n            if i == save:\n                in_dims[i] = features_list[i].shape[1]\n                continue\n            dim = features_list[i].shape[0]\n            indices = np.vstack((np.arange(dim), np.arange(dim)))\n            indices = torch.LongTensor(indices)\n            values = torch.FloatTensor(np.ones(dim))\n            features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n    elif feats_type == 3:\n        in_dims = [features.shape[0] for features in features_list]\n        for i in range(len(features_list)):\n            dim = features_list[i].shape[0]\n            indices = np.vstack((np.arange(dim), np.arange(dim)))\n            indices = torch.LongTensor(indices)\n            values = torch.FloatTensor(np.ones(dim))\n            features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n    \n    edge2type = {}\n    for k in dl.links['data']:\n        for u,v in zip(*dl.links['data'][k].nonzero()):\n            edge2type[(u,v)] = k\n    for i in range(dl.nodes['total']):\n        if (i,i) not in edge2type:\n            edge2type[(i,i)] = len(dl.links['count'])\n    for k in dl.links['data']:\n        for u,v in zip(*dl.links['data'][k].nonzero()):\n            if (v,u) not in edge2type:\n                edge2type[(v,u)] = k+1+len(dl.links['count'])\n\n    g = dgl.DGLGraph(adjM+(adjM.T))\n    g = dgl.remove_self_loop(g)\n    g = dgl.add_self_loop(g)\n    g = g.to(device)\n    e_feat = []\n    for u, v in zip(*g.edges()):\n        u = u.cpu().item()\n        v = v.cpu().item()\n        e_feat.append(edge2type[(u,v)])\n    e_feat = torch.tensor(e_feat, dtype=torch.long).to(device)\n    g.edge_type_indexer=F.one_hot(e_feat).to(device)\n    total = len(list(dl.links_test['data'].keys()))\n    num_ntypes=len(features_list)\n    #num_layers=len(hiddens)-1\n    num_nodes=dl.nodes['total']\n    num_etype=len(dl.links['count'])\n    \n\n    g.node_idx_by_ntype=[]\n\n\n    g.node_ntype_indexer=torch.zeros(num_nodes,num_ntypes).to(device)\n    ntype_dims=[]\n    idx_count=0\n    ntype_count=0\n    for feature in features_list:\n        temp=[]\n        for _ in feature:\n            temp.append(idx_count)\n            g.node_ntype_indexer[idx_count][ntype_count]=1\n            idx_count+=1\n\n        g.node_idx_by_ntype.append(temp)\n        ntype_dims.append(feature.shape[1])\n        ntype_count+=1\n    ntypes=g.node_ntype_indexer.argmax(1)\n    ntype_indexer=g.node_ntype_indexer\n    res_2hops=[]\n    res_randoms=[]\n    \n    toCsvRepetition=[]\n    for re in range(args.repeat):\n        print(f\"re : {re} starts!\\n\\n\\n\")\n        res_2hop = defaultdict(float)\n        res_random = defaultdict(float)\n        train_pos, valid_pos = dl.get_train_valid_pos()#edge_types=[test_edge_type])\n        # train_pos {etype0: [[...], [...]], etype1: [[...], [...]]}\n        # valid_pos {etype0: [[...], [...]], etype1: [[...], [...]]}\n        num_classes = args.hidden_dim\n        heads = [args.num_heads] * args.num_layers + [args.num_heads]\n        num_ntype=len(features_list)\n        g.num_ntypes=num_ntype \n        eindexer=None\n        prod_aggr=args.prod_aggr if args.prod_aggr!=\"None\" else None\n        if args.net==\"myGAT\":\n            net = myGAT(g, args.edge_feats, len(dl.links['count'])*2+1, in_dims, args.hidden_dim, num_classes, args.num_layers, heads, F.elu , args.dropout_feat,args.dropout_attn, args.slope, eval(args.residual), args.residual_att,decode=args.decoder,dataRecorder=dataRecorder,get_out=get_out) \n        elif args.net==\"slotGAT\":\n            net = slotGAT(g, args.edge_feats, len(dl.links['count'])*2+1, in_dims, args.hidden_dim, num_classes, args.num_layers, heads, F.elu,  args.dropout_feat,args.dropout_attn, args.slope, eval(args.residual), args.residual_att,  num_ntype,   \n                 eindexer,decode=args.decoder,aggregator=args.slot_aggregator,inProcessEmb=args.inProcessEmb,l2BySlot=args.l2BySlot,prod_aggr=prod_aggr,sigmoid=args.sigmoid,l2use=args.l2use,SAattDim=args.SAattDim,dataRecorder=dataRecorder,get_out=get_out,predicted_by_slot=args.predicted_by_slot)\n        print(net) if args.verbose==\"True\" else None\n        \n            \n        net.to(device)\n        epoch_val_loss=0\n        val_res_RocAucRandom=0\n        val_res_MRRRandom=0\n\n        if args.use_trained==\"True\":\n            ckp_fname=os.path.join(args.trained_dir,args.net,args.dataset,str(re),\"model.pt\")\n        else:\n            optimizer = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n\n            # training loop\n            net.train()\n            try:\n                if not os.path.exists('checkpoint'):\n                    os.mkdir('checkpoint')\n                t=time.localtime()\n                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_mday:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n                ckp_dname=os.path.join('checkpoint',str_t)\n                os.mkdir(ckp_dname)\n            except:\n                time.sleep(1)\n                t=time.localtime()\n                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_mday:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n                ckp_dname=os.path.join('checkpoint',str_t)\n                os.mkdir(ckp_dname)\n            if args.save_trained==\"True\":\n                # files in save_dir should be considered ***important***\n                ckp_fname=os.path.join(args.save_dir,args.net,args.dataset,str(re),\"model.pt\")\n                #ckp_fname=os.path.join(args.save_dir,args.study_name,args.net,args.dataset,str(re),\"model.pt\")\n                os.makedirs(os.path.dirname(ckp_fname),exist_ok=True)\n            else:\n                ckp_fname=os.path.join(ckp_dname,'checkpoint_{}_{}_re_{}_feat_{}_heads_{}_{}.pt'.format(args.dataset, args.num_layers,re,args.feats_type,args.num_heads,net.__class__.__name__))\n            \n            early_stopping = EarlyStopping(patience=args.patience, verbose=True, save_path=ckp_fname)\n            loss_func = nn.BCELoss()\n            train_losses=[]\n            val_losses=[]\n            \n            if args.profile==\"True\":\n                profile_func=profile\n            elif args.profile==\"False\":\n                profile_func=blank_profile\n\n            \n            with profile_func(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,schedule=torch.profiler.schedule(\n                    wait=2,\n                    warmup=2,\n                    active=6,\n                    repeat=1),on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler/trace_\"+args.study_name)) as prof:    \n                for epoch in range(args.epoch):\n                    train_pos_head_full = np.array([])\n                    train_pos_tail_full = np.array([])\n                    train_neg_head_full = np.array([])\n                    train_neg_tail_full = np.array([])\n                    r_id_full = np.array([])\n                    for test_edge_type in dl.links_test['data'].keys():\n                        train_neg = dl.get_train_neg(edge_types=[test_edge_type])[test_edge_type]\n                        # train_neg [[0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [9294, 8277, 4484, 7413, 1883, 5117, 9256, 3106, 636, ...]]\n                        train_pos_head_full = np.concatenate([train_pos_head_full, np.array(train_pos[test_edge_type][0])])\n                        train_pos_tail_full = np.concatenate([train_pos_tail_full, np.array(train_pos[test_edge_type][1])])\n                        train_neg_head_full = np.concatenate([train_neg_head_full, np.array(train_neg[0])])\n                        train_neg_tail_full = np.concatenate([train_neg_tail_full, np.array(train_neg[1])])\n                        r_id_full = np.concatenate([r_id_full, np.array([test_edge_type]*len(train_pos[test_edge_type][0]))])\n                    train_idx = np.arange(len(train_pos_head_full))\n                    np.random.shuffle(train_idx)\n                    batch_size = args.batch_size\n                    epoch_val_loss=0\n                    c=0\n                    val_res_RocAucRandom=0\n                    val_res_MRRRandom=0\n                    for step, start in enumerate(range(0, len(train_pos_head_full), args.batch_size)):\n\n                    \n\n                        t_start = time.time()\n                        # training\n                        net.train()\n                        train_pos_head = train_pos_head_full[train_idx[start:start+batch_size]]\n                        train_neg_head = train_neg_head_full[train_idx[start:start+batch_size]]\n                        train_pos_tail = train_pos_tail_full[train_idx[start:start+batch_size]]\n                        train_neg_tail = train_neg_tail_full[train_idx[start:start+batch_size]]\n                        r_id = r_id_full[train_idx[start:start+batch_size]]\n                        left = np.concatenate([train_pos_head, train_neg_head])  #to get heads embeddings\n                        right = np.concatenate([train_pos_tail, train_neg_tail])   #to get tail embeddings\n                        mid = np.concatenate([r_id, r_id])  #specify edge types\n                        labels = torch.FloatTensor(np.concatenate([np.ones(train_pos_head.shape[0]), np.zeros(train_neg_head.shape[0])])).to(device)\n                        \n                        with record_function(\"model_inference\"):\n                            net.dataRecorder[\"status\"]=\"Training\"\n                            logits = net(features_list, e_feat, left, right, mid)\n                            net.dataRecorder[\"status\"]=\"None\"\n                        logp = logits\n                        train_loss = loss_func(logp, labels)\n\n                        # autograd\n                        optimizer.zero_grad()\n                        \n                        with record_function(\"model_backward\"):\n                            train_loss.backward()\n                            optimizer.step()\n\n                        t_end = time.time()\n\n                        # print training info\n                        print('Epoch {:05d}, Step{:05d} | Train_Loss: {:.4f} | Time: {:.4f}'.format(epoch, step, train_loss.item(), t_end-t_start))  if args.verbose==\"True\" else None\n                        train_losses.append(train_loss.item())\n                        t_start = time.time()\n                        # validation\n                        net.eval()\n                        #print(\"validation!\")\n                        with torch.no_grad():\n                            valid_pos_head = np.array([])\n                            valid_pos_tail = np.array([])\n                            valid_neg_head = np.array([])\n                            valid_neg_tail = np.array([])\n                            valid_r_id = np.array([])\n                            for test_edge_type in dl.links_test['data'].keys():\n                                valid_neg = dl.get_valid_neg(edge_types=[test_edge_type])[test_edge_type]\n                                valid_pos_head = np.concatenate([valid_pos_head, np.array(valid_pos[test_edge_type][0])])\n                                valid_pos_tail = np.concatenate([valid_pos_tail, np.array(valid_pos[test_edge_type][1])])\n                                valid_neg_head = np.concatenate([valid_neg_head, np.array(valid_neg[0])])\n                                valid_neg_tail = np.concatenate([valid_neg_tail, np.array(valid_neg[1])])\n                                valid_r_id = np.concatenate([valid_r_id, np.array([test_edge_type]*len(valid_pos[test_edge_type][0]))])\n                            left = np.concatenate([valid_pos_head, valid_neg_head])\n                            right = np.concatenate([valid_pos_tail, valid_neg_tail])\n                            mid = np.concatenate([valid_r_id, valid_r_id])\n                            labels = torch.FloatTensor(np.concatenate([np.ones(valid_pos_head.shape[0]), np.zeros(valid_neg_head.shape[0])])).to(device)\n                            logits = net(features_list, e_feat, left, right, mid)\n                            logp = logits\n                            val_loss = loss_func(logp, labels)\n\n                            pred = logits.cpu().numpy()\n                            edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n                            labels = labels.cpu().numpy()\n                            val_res = dl.evaluate(edge_list, pred, labels)\n\n                            val_res_RocAucRandom+=val_res[\"roc_auc\"]*left.shape[0]\n                            val_res_MRRRandom+=val_res[\"MRR\"]*left.shape[0]\n                            epoch_val_loss+=val_loss.item()*left.shape[0]\n                            c+=left.shape[0]\n                        \n                        t_end = time.time()\n                        # print validation info\n                        print('Epoch {:05d} | Val_Loss {:.4f} | Time(s) {:.4f}'.format(\n                            epoch, val_loss.item(), t_end - t_start))  if args.verbose==\"True\" else None\n                        val_losses.append(val_loss.item())\n                        # early stopping\n                        early_stopping(val_loss, net)\n                        if early_stopping.early_stop:\n                            print('Early stopping!')  if args.verbose==\"True\" else None\n                            break\n                        prof.step()\n                    epoch_val_loss=epoch_val_loss/c\n                    val_res_RocAucRandom=val_res_RocAucRandom/c\n                    val_res_MRRRandom=val_res_MRRRandom/c\n                \n\n        first_flag = True\n        for test_edge_type in dl.links_test['data'].keys():\n            # testing with evaluate_results_nc\n            net.load_state_dict(torch.load(ckp_fname))\n            net.eval()\n            test_logits = []\n            with torch.no_grad():\n                test_neigh, test_label = dl.get_test_neigh()\n                test_neigh = test_neigh[test_edge_type]\n                test_label = test_label[test_edge_type]\n                left = np.array(test_neigh[0])\n                right = np.array(test_neigh[1])\n                mid = np.zeros(left.shape[0], dtype=np.int32)\n                mid[:] = test_edge_type\n                labels = torch.FloatTensor(test_label).to(device)\n                net.dataRecorder[\"status\"]=\"Test2Hop\"\n                logits = net(features_list, e_feat, left, right, mid)\n                net.dataRecorder[\"status\"]=\"None\"\n                pred = logits.cpu().numpy()\n                edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n                labels = labels.cpu().numpy()\n                \n                first_flag = False\n                res = dl.evaluate(edge_list, pred, labels)\n                #print(f\"res {res}\")\n                for k in res:\n                    res_2hop[k] += res[k]\n            with torch.no_grad():\n                test_neigh, test_label = dl.get_test_neigh_w_random()\n                test_neigh = test_neigh[test_edge_type]\n                test_label = test_label[test_edge_type]\n                left = np.array(test_neigh[0])\n                right = np.array(test_neigh[1])\n                mid = np.zeros(left.shape[0], dtype=np.int32)\n                mid[:] = test_edge_type\n                labels = torch.FloatTensor(test_label).to(device)\n                net.dataRecorder[\"status\"]=\"TestRandom\"\n                logits = net(features_list, e_feat, left, right, mid)\n                net.dataRecorder[\"status\"]=\"None\"\n                pred = logits.cpu().numpy()\n                edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n                labels = labels.cpu().numpy()\n                res = dl.evaluate(edge_list, pred, labels)\n                #print(f\"res {res}\")\n                for k in res:\n                    res_random[k] += res[k]\n        for k in res_2hop:\n            res_2hop[k] /= total\n        for k in res_random:\n            res_random[k] /= total\n        res_2hops.append(res_2hop)\n        res_randoms.append(res_random)\n\n        toCsv={ \"1_featType\":feats_type,\n            \"1_numLayers\":args.num_layers,\n            \"1_hiddenDim\":args.hidden_dim,\n            \"1_SAAttDim\":args.SAattDim,\n            \"1_numOfHeads\":args.num_heads,\n            \"1_numOfEpoch\":args.epoch,\n            \"1_Lr\":args.lr,\n            \"1_Wd\":args.weight_decay,\n            \"1_decoder\":args.decoder,\n            \"1_batchSize\":args.batch_size,\n            \"1_residual-att\":args.residual_att,\n            \"1_residual\":args.residual,\n            \"1_dropoutFeat\":args.dropout_feat,\n            \"1_dropoutAttn\":args.dropout_attn,\n            #\"2_valAcc\":val_results[\"acc\"],\n            #\"2_valMiPre\":val_results[\"micro-pre\"],\n            \"2_valLossNeg_mean\":-epoch_val_loss,\n            \"2_valRocAucRandom_mean\":val_res_RocAucRandom,\n            \"2_valMRRRandom_mean\":val_res_MRRRandom,\n            \"3_testRocAuc2hop_mean\":res_2hop[\"roc_auc\"],\n            \"3_testMRR2hop_mean\":res_2hop[\"MRR\"],\n            \"3_testRocAucRandom_mean\":res_random[\"roc_auc\"],\n            \"3_testMRRRandom_mean\":res_random[\"MRR\"],\n            \"2_valLossNeg_std\":-epoch_val_loss,\n            \"2_valRocAucRandom_std\":val_res_RocAucRandom,\n            \"2_valMRRRandom_std\":val_res_MRRRandom,\n            \"3_testRocAuc2hop_std\":res_2hop[\"roc_auc\"],\n            \"3_testMRR2hop_std\":res_2hop[\"MRR\"],\n            \"3_testRocAucRandom_std\":res_random[\"roc_auc\"],\n            \"3_testMRRRandom_std\":res_random[\"MRR\"],}\n        toCsvRepetition.append(toCsv)\n        \n    print(f\"res_2hops {res_2hops}\")  if args.verbose==\"True\" else None\n    print(f\"res_randoms {res_randoms}\")  if args.verbose==\"True\" else None\n\n\n    toCsvAveraged={}\n    for tocsv in toCsvRepetition:\n        for name in tocsv.keys():\n            if name.startswith(\"1_\"):\n                toCsvAveraged[name]=tocsv[name]\n            else:\n                if name not in toCsvAveraged.keys():\n                    toCsvAveraged[name]=[]\n                toCsvAveraged[name].append(tocsv[name])\n    print(toCsvAveraged)\n    for name in toCsvAveraged.keys():\n        if not name.startswith(\"1_\") :\n            if type(toCsvAveraged[name][0]) is str:\n                toCsvAveraged[name]=toCsvAveraged[name][0]\n            else:\n                \n                if \"_mean\" in name:\n                    toCsvAveraged[name]=np.mean(np.array(toCsvAveraged[name])) \n                elif \"_std\" in name:\n                    toCsvAveraged[name]= np.std(np.array(toCsvAveraged[name])) \n    #toCsvAveraged[\"5_expInfo\"]=exp_info\n\n    writeIntoCsvLogger(toCsvAveraged,f\"./log/{args.study_name}.csv\")\n\n\n\n    #########################################\n    #####        data preprocess\n    #########################################\n\n\n    if not os.path.exists(f\"./analysis\"):\n        os.mkdir(\"./analysis\")\n    if not os.path.exists(f\"./analysis/{args.study_name}\"):\n        os.mkdir(f\"./analysis/{args.study_name}\")\n    \n    if get_out !=['']:\n        vis_data_saver.save(os.path.join(f\"./analysis/{args.study_name}\",args.study_name+\".visdata\"))", "        #vis_data_saver.save(os.path.join(f\"./analysis/{args.study_name}\",args.study_name+\".visdata\"))\n\n    \n\n\nif __name__ == '__main__':\n    ap = argparse.ArgumentParser(description='Run Model on LP tasks.')\n    ap.add_argument('--feats-type', type=int, default=3,\n                    help='Type of the node features used. ' +\n                         '0 - loaded features; ' +\n                         '1 - only target node features (zero vec for others); ' +\n                         '2 - only target node features (id vec for others); ' +\n                         '3 - all id vec. Default is 2;' +\n                        '4 - only term features (id vec for others);' + \n                        '5 - only term features (zero vec for others).')\n    ap.add_argument('--seed', type=int, default=111, help='Random seed.')\n    ap.add_argument('--use_trained', type=str, default=\"False\")\n    ap.add_argument('--trained_dir', type=str, default=\"outputs\")\n    ap.add_argument('--save_trained', type=str, default=\"False\")\n    ap.add_argument('--save_dir', type=str, default=\"outputs\")\n    ap.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\n    ap.add_argument('--num-heads', type=int, default=2, help='Number of the attention heads. Default is 8.')\n    ap.add_argument('--epoch', type=int, default=300, help='Number of epochs.')\n    ap.add_argument('--patience', type=int, default=40, help='Patience.')\n    ap.add_argument('--num-layers', type=int, default=2)\n    ap.add_argument('--lr', type=float, default=5e-4)\n    ap.add_argument('--dropout_feat', type=float, default=0.5)\n    ap.add_argument('--dropout_attn', type=float, default=0.5)\n    ap.add_argument('--weight-decay', type=float, default=1e-4)\n    ap.add_argument('--slope', type=float, default=0.01)\n    ap.add_argument('--dataset', type=str)\n    ap.add_argument('--net', type=str, default='myGAT')\n    ap.add_argument('--gpu', type=str, default=\"0\")\n    ap.add_argument('--verbose', type=str, default='False')\n    ap.add_argument('--edge-feats', type=int, default=32)\n    ap.add_argument('--batch-size', type=int, default=1024)\n    ap.add_argument('--decoder', type=str, default='distmult')  \n    ap.add_argument('--inProcessEmb', type=str, default='True')\n    ap.add_argument('--l2BySlot', type=str, default='True')\n    ap.add_argument('--l2use', type=str, default='True')\n    ap.add_argument('--prod_aggr', type=str, default='None')\n    ap.add_argument('--sigmoid', type=str, default='after') \n\n \n    \n    ap.add_argument('--get_out', default=\"\")  \n\n\n    ap.add_argument('--profile', default=\"False\")  \n    ap.add_argument('--run', type=int, default=1)\n    ap.add_argument('--cost', type=int, default=1)\n    ap.add_argument('--repeat', type=int, default=10, help='Repeat the training and testing for N times. Default is 1.')\n    \n    ap.add_argument('--task_property', type=str, default=\"notSpecified\")\n    ap.add_argument('--study_name', type=str, default=\"temp\")\n    \n    ap.add_argument('--residual_att', type=float, default=0.05)\n    ap.add_argument('--residual', type=str, default=\"True\")\n\n    ap.add_argument('--SAattDim', type=int, default=128)\n    ap.add_argument('--slot_aggregator', type=str, default=\"SA\") \n    ap.add_argument('--predicted_by_slot', type=str, default=\"None\") \n\n\n    args = ap.parse_args()\n    run_model_DBLP(args)", ""]}
{"filename": "LP/methods/slotGAT/run_train_slotGAT_on_all_dataset.py", "chunked_list": ["\n\nimport time\nimport subprocess\nimport multiprocessing\nfrom threading import main_thread\nfrom pipeline_utils import config_study_name,Run\nimport os \n\nresources_dict={\"0\":1,\"1\":1}   #id:load", "\nresources_dict={\"0\":1,\"1\":1}   #id:load\n#dataset_to_evaluate=[(\"pubmed_HNE_LP\",1,5)]   # dataset,cost,repeat\nprefix=\"get_results\";specified_args=[\"dataset\",   \"net\"]\n\nstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\nfixed_info_by_selected_keys={\n    \"slotGAT\":{\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\",\"inProcessEmb\":\"True\",\n                        \"use_trained\":\"False\",", "    \"slotGAT\":{\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\",\"inProcessEmb\":\"True\",\n                        \"use_trained\":\"False\",\n                        #\"trained_dir\":\"outputs\",\n                        \"save_trained\":\"True\",\n                        \"save_dir\":\"outputs\",\n                        } }\n\n\ndataset_and_hypers={\n    (\"PubMed_LP\",1,5):", "dataset_and_hypers={\n    (\"PubMed_LP\",1,5):\n        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[4]\",\"lr\":\"[1e-3]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"distmult\"],\"batch-size\":[8192,],\"dropout_feat\":[0.5],\"dropout_attn\":[0.5],\"residual_att\":[0.2],\"residual\":[\"True\"],\"SAattDim\":[32]},\n    (\"LastFM\",1,5):\n        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[8]\",\"lr\":\"[5e-4]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"dot\"],\"batch-size\":[8192,],\"SAattDim\":[64],\"dropout_feat\":[0.2],\"dropout_attn\":[0.9],\"residual_att\":[0.5],\"residual\":[\"True\"]}\n    }\n\n\ndef getTasks(fixed_info,dataset_and_hypers):\n    \n    for k,v in dataset_and_hypers.items():\n        for k1,v1 in v.items():\n            if \"search_\" in k1:\n                if type(v1)!=str:\n                    v[k1]=f\"[{v1}]\"\n            # if list str, get the first one\n            if  type(v1)==list:\n                v[k1]=v1[0]\n            if type(v1)==str:\n                if v1[0]==\"[\" and v1[-1]==\"]\":\n                    v[k1]=eval(v1)[0]\n        \n\n    tasks_list=[]\n    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n        args_dict={}\n        for dict_to_add in [task,fixed_info]:\n            for k,v in dict_to_add.items():\n                args_dict[k]=v\n        net=args_dict['net']\n        args_dict['dataset']=dataset\n        #args_dict['trial_num']=trial_num\n        args_dict['repeat']=repeat\n        study_name=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n        args_dict['study_name']=study_name\n        args_dict['cost']=cost\n        tasks_list.append(args_dict)\n\n    print(\"tasks_list:\", tasks_list)\n\n    return tasks_list ", "def getTasks(fixed_info,dataset_and_hypers):\n    \n    for k,v in dataset_and_hypers.items():\n        for k1,v1 in v.items():\n            if \"search_\" in k1:\n                if type(v1)!=str:\n                    v[k1]=f\"[{v1}]\"\n            # if list str, get the first one\n            if  type(v1)==list:\n                v[k1]=v1[0]\n            if type(v1)==str:\n                if v1[0]==\"[\" and v1[-1]==\"]\":\n                    v[k1]=eval(v1)[0]\n        \n\n    tasks_list=[]\n    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n        args_dict={}\n        for dict_to_add in [task,fixed_info]:\n            for k,v in dict_to_add.items():\n                args_dict[k]=v\n        net=args_dict['net']\n        args_dict['dataset']=dataset\n        #args_dict['trial_num']=trial_num\n        args_dict['repeat']=repeat\n        study_name=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n        args_dict['study_name']=study_name\n        args_dict['cost']=cost\n        tasks_list.append(args_dict)\n\n    print(\"tasks_list:\", tasks_list)\n\n    return tasks_list ", "\ntasks_list=[]\nfor k in fixed_info_by_selected_keys.keys():\n        \n    tasks_list.extend(getTasks(fixed_info_by_selected_keys[k],dataset_and_hypers))\n\n\n\n\n", "\n\n\nresources=resources_dict.keys()\npool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j))\n\n", "\n\nsub_queues=[]\nitems=len(tasks_list)%60\nfor i in range(items):\n    sub_queues.append(tasks_list[60*i:(60*i+60)])\nsub_queues.append(tasks_list[(60*items+60):])\n\nif items==0:\n    sub_queues.append(tasks_list)", "if items==0:\n    sub_queues.append(tasks_list)\n\n## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\nidx=0\ntc=len(tasks_list)\nfor sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "    \n\nprint('end all')\n\n\n\n\nend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\nprint(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n", "print(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n\n\n\n"]}
{"filename": "LP/methods/slotGAT/run_use_slotGAT_on_all_dataset.py", "chunked_list": ["\n\nimport time \nimport multiprocessing\nfrom threading import main_thread\nfrom pipeline_utils import config_study_name,Run\nimport os \n#time.sleep(60*60*4)\n\n\nif not os.path.exists(\"outputs\"):\n    os.mkdir(\"outputs\")", "\n\nif not os.path.exists(\"outputs\"):\n    os.mkdir(\"outputs\")\nif not os.path.exists(\"log\"):\n    os.mkdir(\"log\")\nif not os.path.exists(\"checkpoint\"):\n    os.mkdir(\"checkpoint\")\n\n", "\n\n\n\nresources_dict={\"0\":1,\"1\":1}   #id:load\n#dataset_to_evaluate=[(\"pubmed_HNE_LP\",1,5)]   # dataset,cost,repeat\nprefix=\"get_results_trained\";specified_args=[\"dataset\",   \"net\"]\n\nstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n", "start_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\nfixed_info_by_selected_keys={\n    \"slotGAT\":{\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\",\"inProcessEmb\":\"True\",\n                        \"use_trained\":\"True\",\n                        \"trained_dir\":\"outputs\",\n                        \"save_trained\":\"False\",\n                        #\"save_dir\":\"outputs\",\n                        } }\n", "                        } }\n\n\n\ndataset_and_hypers={\n    (\"PubMed_LP\",1,5):\n        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[4]\",\"lr\":\"[1e-3]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"distmult\"],\"batch-size\":[8192,],\"dropout_feat\":[0.5],\"dropout_attn\":[0.5],\"residual_att\":[0.2],\"residual\":[\"True\"],\"SAattDim\":[32]},\n    (\"LastFM\",1,5):\n        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[8]\",\"lr\":\"[5e-4]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"dot\"],\"batch-size\":[8192,],\"SAattDim\":[64],\"dropout_feat\":[0.2],\"dropout_attn\":[0.9],\"residual_att\":[0.5],\"residual\":[\"True\"]}\n    }", "        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[8]\",\"lr\":\"[5e-4]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"dot\"],\"batch-size\":[8192,],\"SAattDim\":[64],\"dropout_feat\":[0.2],\"dropout_attn\":[0.9],\"residual_att\":[0.5],\"residual\":[\"True\"]}\n    }\n\ndef getTasks(fixed_info,dataset_and_hypers):\n    \n    for k,v in dataset_and_hypers.items():\n        for k1,v1 in v.items():\n            if \"search_\" in k1:\n                if type(v1)!=str:\n                    v[k1]=f\"[{v1}]\"\n            # if list str, get the first one\n            if  type(v1)==list:\n                v[k1]=v1[0]\n            if type(v1)==str:\n                if v1[0]==\"[\" and v1[-1]==\"]\":\n                    v[k1]=eval(v1)[0]\n        \n\n    tasks_list=[]\n    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n        args_dict={}\n        for dict_to_add in [task,fixed_info]:\n            for k,v in dict_to_add.items():\n                args_dict[k]=v\n        net=args_dict['net']\n        args_dict['dataset']=dataset\n        #args_dict['trial_num']=trial_num\n        args_dict['repeat']=repeat\n        study_name =config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n        args_dict['study_name']=study_name \n        args_dict['cost']=cost\n        tasks_list.append(args_dict)\n\n    print(\"tasks_list:\", tasks_list)\n\n    return tasks_list ", "\ntasks_list=[]\nfor k in fixed_info_by_selected_keys.keys():\n        \n    tasks_list.extend(getTasks(fixed_info_by_selected_keys[k],dataset_and_hypers))\n\n\n\n\n", "\n\n\nresources=resources_dict.keys()\npool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\nfor i in resources:\n    for j in range(resources_dict[i]):\n        pool.put(i+str(j))\n\n", "\n\nsub_queues=[]\nitems=len(tasks_list)%60\nfor i in range(items):\n    sub_queues.append(tasks_list[60*i:(60*i+60)])\nsub_queues.append(tasks_list[(60*items+60):])\n\nif items==0:\n    sub_queues.append(tasks_list)", "if items==0:\n    sub_queues.append(tasks_list)\n\n## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\nidx=0\ntc=len(tasks_list)\nfor sub_tasks_list in sub_queues:\n    process_queue=[]\n    for i in range(len(sub_tasks_list)):\n        idx+=1\n        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n        p.daemon=True\n        p.start()\n        process_queue.append(p)\n\n    for p in process_queue:\n        p.join()", "    \n\nprint('end all')\n\n\n\n\nend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\nprint(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n", "print(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n\n\n\n"]}
{"filename": "LP/methods/slotGAT/utils/pytorchtools.py", "chunked_list": ["import numpy as np\nimport torch\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience, verbose=False, delta=0, save_path='checkpoint.pt'):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.save_path = save_path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score - self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.save_path)\n        self.val_loss_min = val_loss", ""]}
{"filename": "LP/methods/slotGAT/utils/__init__.py", "chunked_list": [""]}
{"filename": "LP/methods/slotGAT/utils/tools.py", "chunked_list": ["import torch\nimport dgl\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import LinearSVC\nimport os\n\n", "\n\nimport json\n\nclass blank_profile():\n    def __init__(self,*args,**kwargs):\n        pass\n    def __enter__(self,*args,**kwargs):\n        return self\n    def __exit__(self,*args,**kwargs):\n        pass\n    def step(self):\n        pass", "\n\n\nclass vis_data_collector():\n    #all data must be simple python objects like int or 'str'\n    def __init__(self):\n        self.data_dict={}\n        self.tensor_dict={}\n        #formatting:\n        #\n        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\n    def save_meta(self,meta_data,meta_name):\n        self.data_dict[\"meta\"]={meta_name:meta_data}\n        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n        \n\n    def collect_in_training(self,data,name,re,epoch,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\n    def collect_in_run(self,data,name,re,r=4):\n        if f\"re-{re}\" not in self.data_dict.keys():\n            self.data_dict[f\"re-{re}\" ]={}\n        if type(data)==float:\n            data=round(data,r)\n        self.data_dict[f\"re-{re}\" ][name]=data\n\n    def collect_whole_process(self,data,name):\n        self.data_dict[name]=data\n    def collect_whole_process_tensor(self,data,name):\n        self.tensor_dict[name]=data\n        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\n\n    def save(self,fn):\n        \n        f = open(fn+\".json\", 'w')\n        json.dump(self.data_dict, f, indent=4)\n        f.close()\n\n        for k,v in self.tensor_dict.items():\n\n            torch.save(v,fn+\"_\"+k+\".pt\")\n    \n    def load(self,fn):\n        f = open(fn, 'r')\n        self.data_dict= json.load(f)\n        f.close()\n        for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n            self.tensor_dict[name]=torch.load(name+\".pt\")\n\n\n\n    def trans_to_numpy(self,name,epoch_range=None):\n        data=[]\n        re=0\n        while f\"re-{re}\" in self.data_dict.keys():\n            data.append([])\n            for i in range(epoch_range):\n                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n            re+=1\n        data=np.array(data)\n        return np.mean(data,axis=0),np.std(data,axis=0)\n\n    def visualize_tsne(self,dn,node_idx_by_ntype):\n        from matplotlib.pyplot import figure\n\n        figure(figsize=(16, 9), dpi=80)\n        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n        print(dn)\n        layers=[]\n        heads=[]\n        ets=[]\n        for k,v in self.data_dict.items():\n            if \"attention_hist_layer\" in k:\n                temp=k.split(\"_\")\n                if int(temp[3]) not in layers:\n                    layers.append(int(temp[3]))\n                if temp[4]==\"head\":\n                    if int(temp[5]) not in heads:\n                        heads.append(int(temp[5]))\n                if temp[4]==\"et\":\n                    if int(temp[5]) not in ets:\n                        ets.append(int(temp[5]))\n        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n        #print(layers,heads,ets)\n        #heads plot\n        for layer in layers:\n            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n            fig.set_size_inches(16,9)\n            fig.set_dpi(100)\n            nts=list(range(len(node_idx_by_ntype)))\n            for nt in nts:\n                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n                subax.cla()\n                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n                print(datas.shape)\n                for nt_j in nts:\n                    x=datas[0][ node_idx_by_ntype[nt_j]]\n                    y=datas[1][ node_idx_by_ntype[nt_j]]\n                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n                subax.set_xlim(-100,100)\n                subax.set_ylim(-100,100)\n                subax.set_title(f\"layer_{layer}_slot_{nt}\")\n                plt.title(f\"layer_{layer}_slot_{nt}\")\n                lgnd=subax.legend()\n                for lh in lgnd.legendHandles:\n                    lh._sizes=[10]\n            fig.suptitle(f\"embedding_tsne_layer_{layer}\")\n            plt.savefig(os.path.join(dn,f\"slot_embeddings_layer_{layer}.png\"))", "\n\n\n                \n                    \n\ndef count_torch_tensor(t):\n    t=t.flatten(0).cpu()\n    c={}\n    for n in t:\n        n=n.item()\n        if n not in c:\n            c[n]=0\n        c[n]+=1\n    c=sorted(list(c.items()),key=lambda x:x[0])\n    return c", "\n\n\n\ndef strList(l):\n    return [str(x) for x in l]\n\n    \ndef writeIntoCsvLogger(dictToWrite,file_name):\n    #read file\n    to_write_names=sorted(list(dictToWrite.keys()))\n    if not os.path.exists(file_name):\n        to_write_line=[]\n        for n in to_write_names:\n            to_write_line.append(dictToWrite[n])\n        with open(file_name,\"w\") as f:\n            f.write(  \",\".join(strList(to_write_names)) +\"\\n\")\n            f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n    else:\n        with open(file_name,\"r\") as f:\n            rec=[]\n            for line in f:\n                line=line.strip(\"\\n\").split(\",\")\n                rec.append(line)\n        #ensure they have same names\n        row_names=sorted(rec[0])\n        if to_write_names!=row_names:\n            collected_names_not_in=[]\n            for n in to_write_names:\n                if n not in row_names:\n                    for i,n_r in enumerate(rec):\n                        if i==0:\n                            rec[0].append(n)\n                        else:\n                            rec[i].append(\"\")\n                row_names.append(n)\n            for n_r in row_names:\n                if n_r not in to_write_names:\n                    dictToWrite[n_r]=\"\"\n                    to_write_names.append(n_r)\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            rec.append(to_write_line)\n            with open(file_name,\"w\") as f:\n                for line_list in rec:\n                    f.write(  \",\".join(strList(line_list)) +\"\\n\")\n        else:\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            with open(file_name,\"a\") as f:\n                f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n        re_order_csv(file_name)", "def writeIntoCsvLogger(dictToWrite,file_name):\n    #read file\n    to_write_names=sorted(list(dictToWrite.keys()))\n    if not os.path.exists(file_name):\n        to_write_line=[]\n        for n in to_write_names:\n            to_write_line.append(dictToWrite[n])\n        with open(file_name,\"w\") as f:\n            f.write(  \",\".join(strList(to_write_names)) +\"\\n\")\n            f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n    else:\n        with open(file_name,\"r\") as f:\n            rec=[]\n            for line in f:\n                line=line.strip(\"\\n\").split(\",\")\n                rec.append(line)\n        #ensure they have same names\n        row_names=sorted(rec[0])\n        if to_write_names!=row_names:\n            collected_names_not_in=[]\n            for n in to_write_names:\n                if n not in row_names:\n                    for i,n_r in enumerate(rec):\n                        if i==0:\n                            rec[0].append(n)\n                        else:\n                            rec[i].append(\"\")\n                row_names.append(n)\n            for n_r in row_names:\n                if n_r not in to_write_names:\n                    dictToWrite[n_r]=\"\"\n                    to_write_names.append(n_r)\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            rec.append(to_write_line)\n            with open(file_name,\"w\") as f:\n                for line_list in rec:\n                    f.write(  \",\".join(strList(line_list)) +\"\\n\")\n        else:\n            to_write_line=[]\n            for n in rec[0]:\n                to_write_line.append(dictToWrite[n])\n            with open(file_name,\"a\") as f:\n                f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n        re_order_csv(file_name)", "\n        \ndef re_order_csv(file_name):\n    with open(file_name,\"r\") as f:\n        rec=[]\n        for line in f:\n            line=line.strip(\"\\n\").split(\",\")\n            rec.append(line)\n    row_names=sorted(enumerate(rec[0]),key=lambda x:x[1])\n    row_names_idx=[i[0] for i in row_names]    \n    row_names_=[i[1] for i in row_names]    \n    if row_names_idx==sorted(row_names_idx):\n        print(\"No need to reorder\")\n        return None\n    else:\n        print(\"reordering\")\n        with open(file_name,\"w\") as f:\n            for line_list in rec:\n                to_write_line=[ line_list[row_names_idx[i]]  for i in range(len(line_list))  ]\n                f.write(  \",\".join(to_write_line) +\"\\n\")", "\n\n\n\ndef idx_to_one_hot(idx_arr):\n    one_hot = np.zeros((idx_arr.shape[0], idx_arr.max() + 1))\n    one_hot[np.arange(idx_arr.shape[0]), idx_arr] = 1\n    return one_hot\n\n\ndef kmeans_test(X, y, n_clusters, repeat=10):\n    nmi_list = []\n    ari_list = []\n    for _ in range(repeat):\n        kmeans = KMeans(n_clusters=n_clusters)\n        y_pred = kmeans.fit_predict(X)\n        nmi_score = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n        ari_score = adjusted_rand_score(y, y_pred)\n        nmi_list.append(nmi_score)\n        ari_list.append(ari_score)\n    return np.mean(nmi_list), np.std(nmi_list), np.mean(ari_list), np.std(ari_list)", "\n\ndef kmeans_test(X, y, n_clusters, repeat=10):\n    nmi_list = []\n    ari_list = []\n    for _ in range(repeat):\n        kmeans = KMeans(n_clusters=n_clusters)\n        y_pred = kmeans.fit_predict(X)\n        nmi_score = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n        ari_score = adjusted_rand_score(y, y_pred)\n        nmi_list.append(nmi_score)\n        ari_list.append(ari_score)\n    return np.mean(nmi_list), np.std(nmi_list), np.mean(ari_list), np.std(ari_list)", "\n\ndef svm_test(X, y, test_sizes=(0.2, 0.4, 0.6, 0.8), repeat=10):\n    random_states = [182318 + i for i in range(repeat)]\n    result_macro_f1_list = []\n    result_micro_f1_list = []\n    for test_size in test_sizes:\n        macro_f1_list = []\n        micro_f1_list = []\n        for i in range(repeat):\n            X_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=test_size, shuffle=True, random_state=random_states[i])\n            svm = LinearSVC(dual=False)\n            svm.fit(X_train, y_train)\n            y_pred = svm.predict(X_test)\n            macro_f1 = f1_score(y_test, y_pred, average='macro')\n            micro_f1 = f1_score(y_test, y_pred, average='micro')\n            macro_f1_list.append(macro_f1)\n            micro_f1_list.append(micro_f1)\n        result_macro_f1_list.append((np.mean(macro_f1_list), np.std(macro_f1_list)))\n        result_micro_f1_list.append((np.mean(micro_f1_list), np.std(micro_f1_list)))\n    return result_macro_f1_list, result_micro_f1_list", "\n\ndef evaluate_results_nc(embeddings, labels, num_classes):\n    print('SVM test')\n    svm_macro_f1_list, svm_micro_f1_list = svm_test(embeddings, labels)\n    print('Macro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(macro_f1_mean, macro_f1_std, train_size) for\n                                    (macro_f1_mean, macro_f1_std), train_size in\n                                    zip(svm_macro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n    print('Micro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(micro_f1_mean, micro_f1_std, train_size) for\n                                    (micro_f1_mean, micro_f1_std), train_size in\n                                    zip(svm_micro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n    print('K-means test')\n    nmi_mean, nmi_std, ari_mean, ari_std = kmeans_test(embeddings, labels, num_classes)\n    print('NMI: {:.6f}~{:.6f}'.format(nmi_mean, nmi_std))\n    print('ARI: {:.6f}~{:.6f}'.format(ari_mean, ari_std))\n\n    return svm_macro_f1_list, svm_micro_f1_list, nmi_mean, nmi_std, ari_mean, ari_std", "\n\ndef parse_adjlist(adjlist, edge_metapath_indices, samples=None):\n    edges = []\n    nodes = set()\n    result_indices = []\n    for row, indices in zip(adjlist, edge_metapath_indices):\n        row_parsed = list(map(int, row.split(' ')))\n        nodes.add(row_parsed[0])\n        if len(row_parsed) > 1:\n            # sampling neighbors\n            if samples is None:\n                neighbors = row_parsed[1:]\n                result_indices.append(indices)\n            else:\n                # undersampling frequent neighbors\n                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n                p = []\n                for count in counts:\n                    p += [(count ** (3 / 4)) / count] * count\n                p = np.array(p)\n                p = p / p.sum()\n                samples = min(samples, len(row_parsed) - 1)\n                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n                neighbors = [row_parsed[i + 1] for i in sampled_idx]\n                result_indices.append(indices[sampled_idx])\n        else:\n            neighbors = []\n            result_indices.append(indices)\n        for dst in neighbors:\n            nodes.add(dst)\n            edges.append((row_parsed[0], dst))\n    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n    result_indices = np.vstack(result_indices)\n    return edges, result_indices, len(nodes), mapping", "\n\ndef parse_minibatch(adjlists, edge_metapath_indices_list, idx_batch, device, samples=None):\n    g_list = []\n    result_indices_list = []\n    idx_batch_mapped_list = []\n    for adjlist, indices in zip(adjlists, edge_metapath_indices_list):\n        edges, result_indices, num_nodes, mapping = parse_adjlist(\n            [adjlist[i] for i in idx_batch], [indices[i] for i in idx_batch], samples)\n\n        g = dgl.DGLGraph(multigraph=True)\n        g.add_nodes(num_nodes)\n        if len(edges) > 0:\n            sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n            g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n            result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n        else:\n            result_indices = torch.LongTensor(result_indices).to(device)\n        #g.add_edges(*list(zip(*[(dst, src) for src, dst in sorted(edges)])))\n        #result_indices = torch.LongTensor(result_indices).to(device)\n        g_list.append(g)\n        result_indices_list.append(result_indices)\n        idx_batch_mapped_list.append(np.array([mapping[idx] for idx in idx_batch]))\n\n    return g_list, result_indices_list, idx_batch_mapped_list", "\n\ndef parse_adjlist_LastFM(adjlist, edge_metapath_indices, samples=None, exclude=None, offset=None, mode=None):\n    edges = []\n    nodes = set()\n    result_indices = []\n    for row, indices in zip(adjlist, edge_metapath_indices):\n        row_parsed = list(map(int, row.split(' ')))\n        nodes.add(row_parsed[0])\n        if len(row_parsed) > 1:\n            # sampling neighbors\n            if samples is None:\n                if exclude is not None:\n                    if mode == 0:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[:, [0, 1, -1, -2]]]\n                    else:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[:, [0, 1, -1, -2]]]\n                    neighbors = np.array(row_parsed[1:])[mask]\n                    result_indices.append(indices[mask])\n                else:\n                    neighbors = row_parsed[1:]\n                    result_indices.append(indices)\n            else:\n                # undersampling frequent neighbors\n                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n                p = []\n                for count in counts:\n                    p += [(count ** (3 / 4)) / count] * count\n                p = np.array(p)\n                p = p / p.sum()\n                samples = min(samples, len(row_parsed) - 1)\n                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n                if exclude is not None:\n                    if mode == 0:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n                    else:\n                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n                    neighbors = np.array([row_parsed[i + 1] for i in sampled_idx])[mask]\n                    result_indices.append(indices[sampled_idx][mask])\n                else:\n                    neighbors = [row_parsed[i + 1] for i in sampled_idx]\n                    result_indices.append(indices[sampled_idx])\n        else:\n            neighbors = [row_parsed[0]]\n            indices = np.array([[row_parsed[0]] * indices.shape[1]])\n            if mode == 1:\n                indices += offset\n            result_indices.append(indices)\n        for dst in neighbors:\n            nodes.add(dst)\n            edges.append((row_parsed[0], dst))\n    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n    result_indices = np.vstack(result_indices)\n    return edges, result_indices, len(nodes), mapping", "\n\ndef parse_minibatch_LastFM(adjlists_ua, edge_metapath_indices_list_ua, user_artist_batch, device, samples=None, use_masks=None, offset=None):\n    g_lists = [[], []]\n    result_indices_lists = [[], []]\n    idx_batch_mapped_lists = [[], []]\n    for mode, (adjlists, edge_metapath_indices_list) in enumerate(zip(adjlists_ua, edge_metapath_indices_list_ua)):\n        for adjlist, indices, use_mask in zip(adjlists, edge_metapath_indices_list, use_masks[mode]):\n            if use_mask:\n                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, user_artist_batch, offset, mode)\n            else:\n                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, offset=offset, mode=mode)\n\n            g = dgl.DGLGraph(multigraph=True)\n            g.add_nodes(num_nodes)\n            if len(edges) > 0:\n                sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n                g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n                result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n            else:\n                result_indices = torch.LongTensor(result_indices).to(device)\n            g_lists[mode].append(g)\n            result_indices_lists[mode].append(result_indices)\n            idx_batch_mapped_lists[mode].append(np.array([mapping[row[mode]] for row in user_artist_batch]))\n\n    return g_lists, result_indices_lists, idx_batch_mapped_lists", "\n\nclass index_generator:\n    def __init__(self, batch_size, num_data=None, indices=None, shuffle=True):\n        if num_data is not None:\n            self.num_data = num_data\n            self.indices = np.arange(num_data)\n        if indices is not None:\n            self.num_data = len(indices)\n            self.indices = np.copy(indices)\n        self.batch_size = batch_size\n        self.iter_counter = 0\n        self.shuffle = shuffle\n        if shuffle:\n            np.random.shuffle(self.indices)\n\n    def next(self):\n        if self.num_iterations_left() <= 0:\n            self.reset()\n        self.iter_counter += 1\n        return np.copy(self.indices[(self.iter_counter - 1) * self.batch_size:self.iter_counter * self.batch_size])\n\n    def num_iterations(self):\n        return int(np.ceil(self.num_data / self.batch_size))\n\n    def num_iterations_left(self):\n        return self.num_iterations() - self.iter_counter\n\n    def reset(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.iter_counter = 0", ""]}
{"filename": "LP/methods/slotGAT/utils/data.py", "chunked_list": ["import networkx as nx\nimport numpy as np\nimport scipy\nimport pickle\nimport scipy.sparse as sp\n\ndef load_data(prefix='DBLP'):\n    from scripts.data_loader import data_loader\n    dl = data_loader('../../data/'+prefix)\n    features = []\n    for i in range(len(dl.nodes['count'])):\n        th = dl.nodes['attr'][i]\n        if th is None:\n            features.append(sp.eye(dl.nodes['count'][i]))\n        else:\n            features.append(th)\n    adjM = sum(dl.links['data'].values())\n    return features,\\\n           adjM, \\\n            dl", ""]}
{"filename": "LP/methods/slotGAT/utils/preprocess.py", "chunked_list": ["import numpy as np\nimport scipy.sparse\nimport networkx as nx\n\n\ndef get_metapath_adjacency_matrix(adjM, type_mask, metapath):\n    \"\"\"\n    :param M: the raw adjacency matrix\n    :param type_mask: an array of types of all node\n    :param metapath\n    :return: a list of metapath-based adjacency matrices\n    \"\"\"\n    out_adjM = scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[0], type_mask == metapath[1])])\n    for i in range(1, len(metapath) - 1):\n        out_adjM = out_adjM.dot(scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])]))\n    return out_adjM.toarray()", "\n\n# networkx.has_path may search too\ndef get_metapath_neighbor_pairs(M, type_mask, expected_metapaths):\n    \"\"\"\n    :param M: the raw adjacency matrix\n    :param type_mask: an array of types of all node\n    :param expected_metapaths: a list of expected metapaths\n    :return: a list of python dictionaries, consisting of metapath-based neighbor pairs and intermediate paths\n    \"\"\"\n    outs = []\n    for metapath in expected_metapaths:\n        # consider only the edges relevant to the expected metapath\n        mask = np.zeros(M.shape, dtype=bool)\n        for i in range((len(metapath) - 1) // 2):\n            temp = np.zeros(M.shape, dtype=bool)\n            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n            mask = np.logical_or(mask, temp)\n        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n\n        # only need to consider the former half of the metapath\n        # e.g., we only need to consider 0-1-2 for the metapath 0-1-2-1-0\n        metapath_to_target = {}\n        for source in (type_mask == metapath[0]).nonzero()[0]:\n            for target in (type_mask == metapath[(len(metapath) - 1) // 2]).nonzero()[0]:\n                # check if there is a possible valid path from source to target node\n                has_path = False\n                single_source_paths = nx.single_source_shortest_path(\n                    partial_g_nx, source, cutoff=(len(metapath) + 1) // 2 - 1)\n                if target in single_source_paths:\n                    has_path = True\n\n                #if nx.has_path(partial_g_nx, source, target):\n                if has_path:\n                    shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n                                 len(p) == (len(metapath) + 1) // 2]\n                    if len(shortests) > 0:\n                        metapath_to_target[target] = metapath_to_target.get(target, []) + shortests\n        metapath_neighbor_paris = {}\n        for key, value in metapath_to_target.items():\n            for p1 in value:\n                for p2 in value:\n                    metapath_neighbor_paris[(p1[0], p2[0])] = metapath_neighbor_paris.get((p1[0], p2[0]), []) + [\n                        p1 + p2[-2::-1]]\n        outs.append(metapath_neighbor_paris)\n    return outs", "\n\ndef get_networkx_graph(neighbor_pairs, type_mask, ctr_ntype):\n    indices = np.where(type_mask == ctr_ntype)[0]\n    idx_mapping = {}\n    for i, idx in enumerate(indices):\n        idx_mapping[idx] = i\n    G_list = []\n    for metapaths in neighbor_pairs:\n        edge_count = 0\n        sorted_metapaths = sorted(metapaths.items())\n        G = nx.MultiDiGraph()\n        G.add_nodes_from(range(len(indices)))\n        for (src, dst), paths in sorted_metapaths:\n            for _ in range(len(paths)):\n                G.add_edge(idx_mapping[src], idx_mapping[dst])\n                edge_count += 1\n        G_list.append(G)\n    return G_list", "\n\ndef get_edge_metapath_idx_array(neighbor_pairs):\n    all_edge_metapath_idx_array = []\n    for metapath_neighbor_pairs in neighbor_pairs:\n        sorted_metapath_neighbor_pairs = sorted(metapath_neighbor_pairs.items())\n        edge_metapath_idx_array = []\n        for _, paths in sorted_metapath_neighbor_pairs:\n            edge_metapath_idx_array.extend(paths)\n        edge_metapath_idx_array = np.array(edge_metapath_idx_array, dtype=int)\n        all_edge_metapath_idx_array.append(edge_metapath_idx_array)\n        print(edge_metapath_idx_array.shape)\n    return all_edge_metapath_idx_array", ""]}
{"filename": "LP/scripts/__init__.py", "chunked_list": [""]}
{"filename": "LP/scripts/rm.py", "chunked_list": ["import random\n\n\n\nprint(random.randrange(0,4))"]}
{"filename": "LP/scripts/data_loader.py", "chunked_list": ["import os\nimport numpy as np\nimport scipy.sparse as sp\nfrom collections import Counter, defaultdict, OrderedDict\nfrom sklearn.metrics import f1_score, auc, roc_auc_score, precision_recall_curve\nimport random\nimport copy\n\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'", "\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'", "\n\nclass data_loader:\n    def __init__(self, path, edge_types=[]):\n        self.path = path\n        self.splited = False\n        self.nodes = self.load_nodes()\n        self.links = self.load_links('link.dat')\n        self.links_test = self.load_links('link.dat.test')\n        self.test_types = list(self.links_test['data'].keys()) if edge_types == [] else edge_types\n        self.types = self.load_types('node.dat')\n        self.train_pos, self.valid_pos = self.get_train_valid_pos()\n        self.train_neg, self.valid_neg = self.get_train_neg(), self.get_valid_neg()\n        self.gen_transpose_links()\n        self.nonzero = False\n\n    def get_train_valid_pos(self, train_ratio=0.9):\n        if self.splited:\n            return self.train_pos, self.valid_pos\n        else:\n            edge_types = self.links['data'].keys()\n            train_pos, valid_pos = dict(), dict()\n            for r_id in edge_types:\n                train_pos[r_id] = [[], []]\n                valid_pos[r_id] = [[], []]\n                row, col = self.links['data'][r_id].nonzero()\n                last_h_id = -1\n                for (h_id, t_id) in zip(row, col):\n                    if h_id != last_h_id:\n                        train_pos[r_id][0].append(h_id)\n                        train_pos[r_id][1].append(t_id)\n                        last_h_id = h_id\n\n                    else:\n                        if random.random() < train_ratio:\n                            train_pos[r_id][0].append(h_id)\n                            train_pos[r_id][1].append(t_id)\n                        else:\n                            valid_pos[r_id][0].append(h_id)\n                            valid_pos[r_id][1].append(t_id)\n                            self.links['data'][r_id][h_id, t_id] = 0\n                            self.links['count'][r_id] -= 1\n                            self.links['total'] -= 1\n                self.links['data'][r_id].eliminate_zeros()\n            self.splited = True\n            return train_pos, valid_pos\n\n    def get_sub_graph(self, node_types_tokeep):\n        \"\"\"\n        node_types_tokeep is a list or set of node types that you want to keep in the sub-graph\n        We only support whole type sub-graph for now.\n        This is an in-place update function!\n        return: old node type id to new node type id dict, old edge type id to new edge type id dict\n        \"\"\"\n        keep = set(node_types_tokeep)\n        new_node_type = 0\n        new_node_id = 0\n        new_nodes = {'total': 0, 'count': Counter(), 'attr': {}, 'shift': {}}\n        new_links = {'total': 0, 'count': Counter(), 'meta': {}, 'data': defaultdict(list)}\n        new_labels_train = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': None, 'mask': None}\n        new_labels_test = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': None, 'mask': None}\n        old_nt2new_nt = {}\n        old_idx = []\n        for node_type in self.nodes['count']:\n            if node_type in keep:\n                nt = node_type\n                nnt = new_node_type\n                old_nt2new_nt[nt] = nnt\n                cnt = self.nodes['count'][nt]\n                new_nodes['total'] += cnt\n                new_nodes['count'][nnt] = cnt\n                new_nodes['attr'][nnt] = self.nodes['attr'][nt]\n                new_nodes['shift'][nnt] = new_node_id\n                beg = self.nodes['shift'][nt]\n                old_idx.extend(range(beg, beg + cnt))\n\n                cnt_label_train = self.labels_train['count'][nt]\n                new_labels_train['count'][nnt] = cnt_label_train\n                new_labels_train['total'] += cnt_label_train\n                cnt_label_test = self.labels_test['count'][nt]\n                new_labels_test['count'][nnt] = cnt_label_test\n                new_labels_test['total'] += cnt_label_test\n\n                new_node_type += 1\n                new_node_id += cnt\n\n        new_labels_train['num_classes'] = self.labels_train['num_classes']\n        new_labels_test['num_classes'] = self.labels_test['num_classes']\n        for k in ['data', 'mask']:\n            new_labels_train[k] = self.labels_train[k][old_idx]\n            new_labels_test[k] = self.labels_test[k][old_idx]\n\n        old_et2new_et = {}\n        new_edge_type = 0\n        for edge_type in self.links['count']:\n            h, t = self.links['meta'][edge_type]\n            if h in keep and t in keep:\n                et = edge_type\n                net = new_edge_type\n                old_et2new_et[et] = net\n                new_links['total'] += self.links['count'][et]\n                new_links['count'][net] = self.links['count'][et]\n                new_links['meta'][net] = tuple(map(lambda x: old_nt2new_nt[x], self.links['meta'][et]))\n                new_links['data'][net] = self.links['data'][et][old_idx][:, old_idx]\n                new_edge_type += 1\n\n        self.nodes = new_nodes\n        self.links = new_links\n        self.labels_train = new_labels_train\n        self.labels_test = new_labels_test\n        return old_nt2new_nt, old_et2new_et\n\n    def get_meta_path(self, meta=[]):\n        \"\"\"\n        Get meta path matrix\n            meta is a list of edge types (also can be denoted by a pair of node types)\n            return a sparse matrix with shape [node_num, node_num]\n        \"\"\"\n        ini = sp.eye(self.nodes['total'])\n        meta = [self.get_edge_type(x) for x in meta]\n        for x in meta:\n            ini = ini.dot(self.links['data'][x]) if x >= 0 else ini.dot(self.links['data_trans'][-x - 1])\n        return ini\n\n    def get_nonzero(self):\n        self.nonzero = True\n        self.re_cache = defaultdict(dict)\n        for k in self.links['data']:\n            th_mat = self.links['data'][k]\n            for i in range(th_mat.shape[0]):\n                th = th_mat[i].nonzero()[1]\n                self.re_cache[k][i] = th\n        for k in self.links['data_trans']:\n            th_mat = self.links['data_trans'][k]\n            for i in range(th_mat.shape[0]):\n                th = th_mat[i].nonzero()[1]\n                self.re_cache[-k - 1][i] = th\n\n    def dfs(self, now, meta, meta_dict):\n        if len(meta) == 0:\n            meta_dict[now[0]].append(now)\n            return\n        # th_mat = self.links['data'][meta[0]] if meta[0] >= 0 else self.links['data_trans'][-meta[0] - 1]\n        th_node = now[-1]\n        for col in self.re_cache[meta[0]][th_node]:  # th_mat[th_node].nonzero()[1]:\n            self.dfs(now + [col], meta[1:], meta_dict)\n\n    def get_full_meta_path(self, meta=[], symmetric=False):\n        \"\"\"\n        Get full meta path for each node\n            meta is a list of edge types (also can be denoted by a pair of node types)\n            return a dict of list[list] (key is node_id)\n        \"\"\"\n        if not self.nonzero:\n            self.get_nonzero()\n        meta = [self.get_edge_type(x) for x in meta]\n        if len(meta) == 1:\n            meta_dict = {}\n            start_node_type = self.links['meta'][meta[0]][0] if meta[0] >= 0 else self.links['meta'][-meta[0] - 1][1]\n            trav = range(self.nodes['shift'][start_node_type],\n                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n            for i in trav:\n                meta_dict[i] = []\n                self.dfs([i], meta, meta_dict)\n        else:\n            meta_dict1 = {}\n            meta_dict2 = {}\n            mid = len(meta) // 2\n            meta1 = meta[:mid]\n            meta2 = meta[mid:]\n            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0] >= 0 else self.links['meta'][-meta1[0] - 1][1]\n            trav = range(self.nodes['shift'][start_node_type],\n                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n            for i in trav:\n                meta_dict1[i] = []\n                self.dfs([i], meta1, meta_dict1)\n            start_node_type = self.links['meta'][meta2[0]][0] if meta2[0] >= 0 else self.links['meta'][-meta2[0] - 1][1]\n            trav = range(self.nodes['shift'][start_node_type],\n                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n            for i in trav:\n                meta_dict2[i] = []\n            if symmetric:\n                for k in meta_dict1:\n                    paths = meta_dict1[k]\n                    for x in paths:\n                        meta_dict2[x[-1]].append(list(reversed(x)))\n            else:\n                for i in trav:\n                    self.dfs([i], meta2, meta_dict2)\n            meta_dict = {}\n            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0] >= 0 else self.links['meta'][-meta1[0] - 1][1]\n            for i in range(self.nodes['shift'][start_node_type],\n                           self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type]):\n                meta_dict[i] = []\n                for beg in meta_dict1[i]:\n                    for end in meta_dict2[beg[-1]]:\n                        meta_dict[i].append(beg + end[1:])\n        return meta_dict\n\n    def gen_file_for_evaluate(self, edge_list, confidence, edge_type, file_path, flag):\n        \"\"\"\n        :param edge_list: shape(2, edge_num)\n        :param confidence: shape(edge_num,)\n        :param edge_type: shape(1)\n        :param file_path: string\n        \"\"\"\n        op = \"w\" if flag else \"a\"\n        with open(file_path, op) as f:\n            for l,r,c in zip(edge_list[0], edge_list[1], confidence):\n                f.write(f\"{l}\\t{r}\\t{edge_type}\\t{c}\\n\")\n\n\n    @staticmethod\n    def evaluate(edge_list, confidence, labels):\n        \"\"\"\n        :param edge_list: shape(2, edge_num)\n        :param confidence: shape(edge_num,)\n        :param labels: shape(edge_num,)\n        :return: dict with all scores we need\n        \"\"\"\n        \n        confidence = np.array(confidence)\n        labels = np.array(labels)\n        roc_auc = roc_auc_score(labels, confidence)\n        mrr_list, cur_mrr = [], 0\n        t_dict, labels_dict, conf_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n        for i, h_id in enumerate(edge_list[0]):\n            t_dict[h_id].append(edge_list[1][i])\n            labels_dict[h_id].append(labels[i])\n            conf_dict[h_id].append(confidence[i])\n        for h_id in t_dict.keys():\n            conf_array = np.array(conf_dict[h_id])\n            rank = np.argsort(-conf_array)\n            sorted_label_array = np.array(labels_dict[h_id])[rank]\n            pos_index = np.where(sorted_label_array == 1)[0]\n            if len(pos_index) == 0:\n                continue\n            pos_min_rank = np.min(pos_index)\n            cur_mrr = 1 / (1 + pos_min_rank)\n            mrr_list.append(cur_mrr)\n        mrr = np.mean(mrr_list)\n\n        return {'roc_auc': roc_auc, 'MRR': mrr}\n\n    def get_node_type(self, node_id):\n        for i in range(len(self.nodes['shift'])):\n            if node_id < self.nodes['shift'][i] + self.nodes['count'][i]:\n                return i\n\n    def get_edge_type(self, info):\n        if type(info) is int or len(info) == 1:\n            return info\n        for i in range(len(self.links['meta'])):\n            if self.links['meta'][i] == info:\n                return i\n        info = (info[1], info[0])\n        for i in range(len(self.links['meta'])):\n            if self.links['meta'][i] == info:\n                return -i - 1\n        raise Exception('No available edge type')\n\n    def get_edge_info(self, edge_id):\n        return self.links['meta'][edge_id]\n\n    def list_to_sp_mat(self, li):\n        data = [x[2] for x in li]\n        i = [x[0] for x in li]\n        j = [x[1] for x in li]\n        return sp.coo_matrix((data, (i, j)), shape=(self.nodes['total'], self.nodes['total'])).tocsr()\n\n    def load_types(self, name):\n        \"\"\"\n        return types dict\n            types: list of types\n            total: total number of nodes\n            data: a dictionary of type of all nodes)\n        \"\"\"\n        types = {'types': list(), 'total': 0, 'data': dict()}\n        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.strip().split('\\t')\n                node_id, node_name, node_type = int(th[0]), th[1], int(th[2])\n                types['data'][node_id] = node_type\n                types['types'].append(node_type)\n                types['total'] += 1\n        types['types'] = list(set(types['types']))\n        return types\n\n    def get_train_neg(self, edge_types=[]):\n        edge_types = self.test_types if edge_types == [] else edge_types\n        train_neg = dict()\n        for r_id in edge_types:\n            h_type, t_type = self.links['meta'][r_id]\n            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n            '''get neg_neigh'''\n            train_neg[r_id] = [[], []]\n            for h_id in self.train_pos[r_id][0]:\n                train_neg[r_id][0].append(h_id)\n                neg_t = random.randrange(t_range[0], t_range[1])\n                train_neg[r_id][1].append(neg_t)\n        return train_neg\n\n    def get_valid_neg(self, edge_types=[]):\n        edge_types = self.test_types if edge_types == [] else edge_types\n        valid_neg = dict()\n        for r_id in edge_types:\n            h_type, t_type = self.links['meta'][r_id]\n            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n            '''get neg_neigh'''\n            valid_neg[r_id] = [[], []]\n            for h_id in self.valid_pos[r_id][0]:\n                valid_neg[r_id][0].append(h_id)\n                neg_t = random.randrange(t_range[0], t_range[1])\n                valid_neg[r_id][1].append(neg_t)\n        return valid_neg\n\n    def get_test_neigh_2hop(self):\n        return self.get_test_neigh()\n\n    def get_test_neigh(self):\n        random.seed(1)\n        neg_neigh, pos_neigh, test_neigh, test_label = dict(), dict(), dict(), dict()\n        edge_types = self.test_types\n        '''get sec_neigh'''\n        pos_links = 0\n        for r_id in self.links['data'].keys():\n            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n        for r_id in self.links_test['data'].keys():\n            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n        for r_id in self.valid_pos.keys():\n            values = [1] * len(self.valid_pos[r_id][0])\n            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n            pos_links += valid_of_rel\n\n        r_double_neighs = np.dot(pos_links, pos_links)\n        data = r_double_neighs.data\n        data[:] = 1\n        r_double_neighs = \\\n            sp.coo_matrix((data, r_double_neighs.nonzero()), shape=np.shape(pos_links), dtype=int) \\\n            - sp.coo_matrix(pos_links, dtype=int) \\\n            - sp.lil_matrix(np.eye(np.shape(pos_links)[0], dtype=int))\n        data = r_double_neighs.data\n        pos_count_index = np.where(data > 0)\n        row, col = r_double_neighs.nonzero()\n        r_double_neighs = sp.coo_matrix((data[pos_count_index], (row[pos_count_index], col[pos_count_index])),\n                                        shape=np.shape(pos_links))\n\n        row, col = r_double_neighs.nonzero()\n        data = r_double_neighs.data\n        sec_index = np.where(data > 0)\n        row, col = row[sec_index], col[sec_index]\n\n        relation_range = [self.nodes['shift'][k] for k in range(len(self.nodes['shift']))] + [self.nodes['total']]\n        for r_id in self.links_test['data'].keys():\n            neg_neigh[r_id] = defaultdict(list)\n            h_type, t_type = self.links_test['meta'][r_id]\n            r_id_index = np.where((row >= relation_range[h_type]) & (row < relation_range[h_type + 1])\n                                  & (col >= relation_range[t_type]) & (col < relation_range[t_type + 1]))[0]\n            # r_num = np.zeros((3, 3))\n            # for h_id, t_id in zip(row, col):\n            #     r_num[self.get_node_type(h_id)][self.get_node_type(t_id)] += 1\n            r_row, r_col = row[r_id_index], col[r_id_index]\n            for h_id, t_id in zip(r_row, r_col):\n                neg_neigh[r_id][h_id].append(t_id)\n\n        for r_id in edge_types:\n            '''get pos_neigh'''\n            pos_neigh[r_id] = defaultdict(list)\n            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n            for h_id, t_id in zip(row, col):\n                pos_neigh[r_id][h_id].append(t_id)\n\n            '''sample neg as same number as pos for each head node'''\n            test_neigh[r_id] = [[], []]\n            pos_list = [[], []]\n            test_label[r_id] = []\n            for h_id in sorted(list(pos_neigh[r_id].keys())):\n                pos_list[0] = [h_id] * len(pos_neigh[r_id][h_id])\n                pos_list[1] = pos_neigh[r_id][h_id]\n                test_neigh[r_id][0].extend(pos_list[0])\n                test_neigh[r_id][1].extend(pos_list[1])\n                test_label[r_id].extend([1] * len(pos_list[0]))\n\n                neg_list = random.choices(neg_neigh[r_id][h_id], k=len(pos_list[0])) if len(\n                    neg_neigh[r_id][h_id]) != 0 else []\n                test_neigh[r_id][0].extend([h_id] * len(neg_list))\n                test_neigh[r_id][1].extend(neg_list)\n                test_label[r_id].extend([0] * len(neg_list))\n        return test_neigh, test_label\n\n    def get_test_neigh_w_random(self):\n        random.seed(1)\n        all_had_neigh = defaultdict(list)\n        neg_neigh, pos_neigh, neigh, label = dict(), dict(), dict(), dict()\n        edge_types = self.test_types\n        '''get pos_links of train and test data'''\n        pos_links = 0\n        for r_id in self.links['data'].keys():\n            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n        for r_id in self.links_test['data'].keys():\n            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n        for r_id in self.valid_pos.keys():\n            values = [1] * len(self.valid_pos[r_id][0])\n            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n            pos_links += valid_of_rel\n\n        row, col = pos_links.nonzero()\n        for h_id, t_id in zip(row, col):\n            all_had_neigh[h_id].append(t_id)\n        for h_id in all_had_neigh.keys():\n            all_had_neigh[h_id] = set(all_had_neigh[h_id])\n        for r_id in edge_types:\n            h_type, t_type = self.links_test['meta'][r_id]\n            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n            '''get pos_neigh and neg_neigh'''\n            pos_neigh[r_id], neg_neigh[r_id] = defaultdict(list), defaultdict(list)\n            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n            for h_id, t_id in zip(row, col):\n                pos_neigh[r_id][h_id].append(t_id)\n                neg_t = random.randrange(t_range[0], t_range[1])\n                while neg_t in all_had_neigh[h_id]:\n                    neg_t = random.randrange(t_range[0], t_range[1])\n                neg_neigh[r_id][h_id].append(neg_t)\n            '''get the neigh'''\n            neigh[r_id] = [[], []]\n            pos_list = [[], []]\n            neg_list = [[], []]\n            label[r_id] = []\n            for h_id in sorted(list(pos_neigh[r_id].keys())):\n                pos_list[0] = [h_id] * len(pos_neigh[r_id][h_id])\n                pos_list[1] = pos_neigh[r_id][h_id]\n                neigh[r_id][0].extend(pos_list[0])\n                neigh[r_id][1].extend(pos_list[1])\n                label[r_id].extend([1] * len(pos_neigh[r_id][h_id]))\n                neg_list[0] = [h_id] * len(neg_neigh[r_id][h_id])\n                neg_list[1] = neg_neigh[r_id][h_id]\n                neigh[r_id][0].extend(neg_list[0])\n                neigh[r_id][1].extend(neg_list[1])\n                label[r_id].extend([0] * len(neg_neigh[r_id][h_id]))\n        return neigh, label\n\n    def get_test_neigh_full_random(self):\n        edge_types = self.test_types\n        random.seed(1)\n        '''get pos_links of train and test data'''\n        all_had_neigh = defaultdict(list)\n        pos_links = 0\n        for r_id in self.links['data'].keys():\n            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n        for r_id in self.links_test['data'].keys():\n            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n        for r_id in self.valid_pos.keys():\n            values = [1] * len(self.valid_pos[r_id][0])\n            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n            pos_links += valid_of_rel\n\n        row, col = pos_links.nonzero()\n        for h_id, t_id in zip(row, col):\n            all_had_neigh[h_id].append(t_id)\n        for h_id in all_had_neigh.keys():\n            all_had_neigh[h_id] = set(all_had_neigh[h_id])\n        test_neigh, test_label = dict(), dict()\n        for r_id in edge_types:\n            test_neigh[r_id] = [[], []]\n            test_label[r_id] = []\n            h_type, t_type = self.links_test['meta'][r_id]\n            h_range = (self.nodes['shift'][h_type], self.nodes['shift'][h_type] + self.nodes['count'][h_type])\n            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n            for h_id, t_id in zip(row, col):\n                test_neigh[r_id][0].append(h_id)\n                test_neigh[r_id][1].append(t_id)\n                test_label[r_id].append(1)\n                neg_h = random.randrange(h_range[0], h_range[1])\n                neg_t = random.randrange(t_range[0], t_range[1])\n                while neg_t in all_had_neigh[neg_h]:\n                    neg_h = random.randrange(h_range[0], h_range[1])\n                    neg_t = random.randrange(t_range[0], t_range[1])\n                test_neigh[r_id][0].append(neg_h)\n                test_neigh[r_id][1].append(neg_t)\n                test_label[r_id].append(0)\n\n        return test_neigh, test_label\n\n    def gen_transpose_links(self):\n        self.links['data_trans'] = defaultdict()\n        for r_id in self.links['data'].keys():\n            self.links['data_trans'][r_id] = self.links['data'][r_id].T\n\n    def load_links(self, name):\n        \"\"\"\n        return links dict\n            total: total number of links\n            count: a dict of int, number of links for each type\n            meta: a dict of tuple, explaining the link type is from what type of node to what type of node\n            data: a dict of sparse matrices, each link type with one matrix. Shapes are all (nodes['total', nodes['total'])\n        \"\"\"\n        links = {'total': 0, 'count': Counter(), 'meta': {}, 'data': defaultdict(list)}\n        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n                if r_id not in links['meta']:\n                    h_type = self.get_node_type(h_id)\n                    t_type = self.get_node_type(t_id)\n                    links['meta'][r_id] = (h_type, t_type)\n                links['data'][r_id].append((h_id, t_id, link_weight))\n                links['count'][r_id] += 1\n                links['total'] += 1\n        new_data = {}\n        for r_id in links['data']:\n            new_data[r_id] = self.list_to_sp_mat(links['data'][r_id])\n        links['data'] = new_data\n        return links\n\n    def load_nodes(self):\n        \"\"\"\n        return nodes dict\n        total: total number of nodes\n        count: a dict of int, number of nodes for each type\n        attr: a dict of np.array (or None), attribute matrices for each type of nodes\n        shift: node_id shift for each type. You can get the id range of a type by\n                    [ shift[node_type], shift[node_type]+count[node_type] )\n        \"\"\"\n        nodes = {'total': 0, 'count': Counter(), 'attr': {}, 'shift': {}}\n        with open(os.path.join(self.path, 'node.dat'), 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                if len(th) == 4:\n                    # Then this line of node has attribute\n                    node_id, node_name, node_type, node_attr = th\n                    node_id = int(node_id)\n                    node_type = int(node_type)\n                    node_attr = list(map(float, node_attr.split(',')))\n                    nodes['count'][node_type] += 1\n                    nodes['attr'][node_id] = node_attr\n                    nodes['total'] += 1\n                elif len(th) == 3:\n                    # Then this line of node doesn't have attribute\n                    node_id, node_name, node_type = th\n                    node_id = int(node_id)\n                    node_type = int(node_type)\n                    nodes['count'][node_type] += 1\n                    nodes['total'] += 1\n                else:\n                    raise Exception(\"Too few information to parse!\")\n        shift = 0\n        attr = {}\n        for i in range(len(nodes['count'])):\n            nodes['shift'][i] = shift\n            if shift in nodes['attr']:\n                mat = []\n                for j in range(shift, shift + nodes['count'][i]):\n                    mat.append(nodes['attr'][j])\n                attr[i] = np.array(mat)\n            else:\n                attr[i] = None\n            shift += nodes['count'][i]\n        nodes['attr'] = attr\n        return nodes", ""]}
{"filename": "LP/scripts/LP_AUC_MRR.py", "chunked_list": ["from collections import Counter\nfrom sklearn.metrics import f1_score,roc_auc_score\nimport numpy as np\nimport os\nfrom collections import defaultdict\nimport sys\nimport json\n\nclass AUC_MRR:\n    def __init__(self, data_name, pred_files):\n        self.AUC_list = []\n        self.MRR_list = []\n        if len(pred_files)==0:\n            self.AUC_mean = np.mean(0)\n            self.MRR_mean = np.mean(0)\n            self.AUC_std = np.std(0)\n            self.MRR_std = np.std(0)\n        else:\n            true_file = os.path.join(args.ground_dir, data_name, 'link.dat.test')\n            self.links_true = self.load_links(true_file)\n            for pred_file in pred_files:\n                self.links_test = self.load_links(pred_file)\n                ans = self.evaluate_AUC_MRR()\n                self.AUC_list.append(ans['AUC'])\n                self.MRR_list.append(ans['MRR'])\n            self.AUC_mean = np.mean(self.AUC_list)\n            self.MRR_mean = np.mean(self.MRR_list)\n            self.AUC_std = np.std(self.AUC_list)\n            self.MRR_std = np.std(self.MRR_list)\n\n    def load_links(self, file_name):\n        \"\"\"\n        return links dict\n            total: total number of links\n            count: a dict of int, number of links for each type\n            data: a list, where each link type have a dict with {(head_id, tail_id): confidence}\n        \"\"\"\n        links = {'total': 0, 'count': Counter(), 'data': defaultdict(dict)}\n        with open(file_name, 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n                links['data'][r_id][(h_id, t_id)] = link_weight\n                links['count'][r_id] += 1\n                links['total'] += 1\n        return links\n\n\n    def evaluate_AUC_MRR(self):\n        ans_all_link_type = {'AUC':[],'MRR':[]}\n        for link_type in  self.links_test['count'].keys():\n            edge_list = {0: [], 1: []}\n            confidence = []\n            labels = []\n            for h_t in self.links_test['data'][link_type].keys():\n                edge_list[0].append(h_t[0])\n                edge_list[1].append(h_t[1])\n                confidence.append(self.links_test['data'][link_type][h_t])\n                if h_t in self.links_true['data'][link_type]:\n                    labels.append(1.0)\n                else:\n                    labels.append(0.)\n            ans = self.evaluate(edge_list, confidence, labels)\n            ans_all_link_type['AUC'].append(ans['AUC'])\n            ans_all_link_type['MRR'].append(ans['MRR'])\n        ans_all_link_type['AUC'] = np.mean(ans_all_link_type['AUC'])\n        ans_all_link_type['MRR'] = np.mean(ans_all_link_type['MRR'])\n        return ans_all_link_type\n\n    @staticmethod\n    def evaluate(edge_list, confidence, labels):\n        \"\"\"\n        :param edge_list: shape(2, edge_num)\n        :param confidence: shape(edge_num,)\n        :param labels: shape(edge_num,)\n        :return: dict with all scores we need\n        \"\"\"\n        confidence = np.array(confidence)\n        labels = np.array(labels)\n        roc_auc = roc_auc_score(labels, confidence)\n        mrr_list, cur_mrr = [], 0\n        t_dict, labels_dict, conf_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n        for i, h_id in enumerate(edge_list[0]):\n            t_dict[h_id].append(edge_list[1][i])\n            labels_dict[h_id].append(labels[i])\n            conf_dict[h_id].append(confidence[i])\n        for h_id in t_dict.keys():\n            conf_array = np.array(conf_dict[h_id])\n            rank = np.argsort(-conf_array)\n            sorted_label_array = np.array(labels_dict[h_id])[rank]\n            pos_index = np.where(sorted_label_array == 1)[0]\n            if len(pos_index) == 0:\n                continue\n            pos_min_rank = np.min(pos_index)\n            cur_mrr = 1 / (1 + pos_min_rank)\n            mrr_list.append(cur_mrr)\n        mrr = np.mean(mrr_list)\n\n        return {'AUC': roc_auc, 'MRR': mrr}", "class AUC_MRR:\n    def __init__(self, data_name, pred_files):\n        self.AUC_list = []\n        self.MRR_list = []\n        if len(pred_files)==0:\n            self.AUC_mean = np.mean(0)\n            self.MRR_mean = np.mean(0)\n            self.AUC_std = np.std(0)\n            self.MRR_std = np.std(0)\n        else:\n            true_file = os.path.join(args.ground_dir, data_name, 'link.dat.test')\n            self.links_true = self.load_links(true_file)\n            for pred_file in pred_files:\n                self.links_test = self.load_links(pred_file)\n                ans = self.evaluate_AUC_MRR()\n                self.AUC_list.append(ans['AUC'])\n                self.MRR_list.append(ans['MRR'])\n            self.AUC_mean = np.mean(self.AUC_list)\n            self.MRR_mean = np.mean(self.MRR_list)\n            self.AUC_std = np.std(self.AUC_list)\n            self.MRR_std = np.std(self.MRR_list)\n\n    def load_links(self, file_name):\n        \"\"\"\n        return links dict\n            total: total number of links\n            count: a dict of int, number of links for each type\n            data: a list, where each link type have a dict with {(head_id, tail_id): confidence}\n        \"\"\"\n        links = {'total': 0, 'count': Counter(), 'data': defaultdict(dict)}\n        with open(file_name, 'r', encoding='utf-8') as f:\n            for line in f:\n                th = line.split('\\t')\n                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n                links['data'][r_id][(h_id, t_id)] = link_weight\n                links['count'][r_id] += 1\n                links['total'] += 1\n        return links\n\n\n    def evaluate_AUC_MRR(self):\n        ans_all_link_type = {'AUC':[],'MRR':[]}\n        for link_type in  self.links_test['count'].keys():\n            edge_list = {0: [], 1: []}\n            confidence = []\n            labels = []\n            for h_t in self.links_test['data'][link_type].keys():\n                edge_list[0].append(h_t[0])\n                edge_list[1].append(h_t[1])\n                confidence.append(self.links_test['data'][link_type][h_t])\n                if h_t in self.links_true['data'][link_type]:\n                    labels.append(1.0)\n                else:\n                    labels.append(0.)\n            ans = self.evaluate(edge_list, confidence, labels)\n            ans_all_link_type['AUC'].append(ans['AUC'])\n            ans_all_link_type['MRR'].append(ans['MRR'])\n        ans_all_link_type['AUC'] = np.mean(ans_all_link_type['AUC'])\n        ans_all_link_type['MRR'] = np.mean(ans_all_link_type['MRR'])\n        return ans_all_link_type\n\n    @staticmethod\n    def evaluate(edge_list, confidence, labels):\n        \"\"\"\n        :param edge_list: shape(2, edge_num)\n        :param confidence: shape(edge_num,)\n        :param labels: shape(edge_num,)\n        :return: dict with all scores we need\n        \"\"\"\n        confidence = np.array(confidence)\n        labels = np.array(labels)\n        roc_auc = roc_auc_score(labels, confidence)\n        mrr_list, cur_mrr = [], 0\n        t_dict, labels_dict, conf_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n        for i, h_id in enumerate(edge_list[0]):\n            t_dict[h_id].append(edge_list[1][i])\n            labels_dict[h_id].append(labels[i])\n            conf_dict[h_id].append(confidence[i])\n        for h_id in t_dict.keys():\n            conf_array = np.array(conf_dict[h_id])\n            rank = np.argsort(-conf_array)\n            sorted_label_array = np.array(labels_dict[h_id])[rank]\n            pos_index = np.where(sorted_label_array == 1)[0]\n            if len(pos_index) == 0:\n                continue\n            pos_min_rank = np.min(pos_index)\n            cur_mrr = 1 / (1 + pos_min_rank)\n            mrr_list.append(cur_mrr)\n        mrr = np.mean(mrr_list)\n\n        return {'AUC': roc_auc, 'MRR': mrr}", "\nimport argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate AUC and MRR for LP result.\")\n    parser.add_argument('--pred_zip', type=str, default=\"lp.zip\",\n                        help='Compressed pred files.')\n    parser.add_argument('--ground_dir', type=str, default=\"../data\",\n                        help='Dir of ground files.')\n    parser.add_argument('--log', type=str, default=\"lp.log\",\n                        help='output file')\n    return parser.parse_args()", "\nimport zipfile\n\ndef extract_zip(zip_path, extract_path):\n    zip = zipfile.ZipFile(zip_path, 'r')\n    zip.extractall(extract_path)\n    return zip.namelist()\n\ndef write_log(log_file, log_msg):\n    with open(log_file, 'w') as log_handle:\n        log_handle.write(log_msg)", "def write_log(log_file, log_msg):\n    with open(log_file, 'w') as log_handle:\n        log_handle.write(log_msg)\n\ndef delete_files(files_):\n    for f in files_:\n        if os.path.exists(f):\n            os.remove(f)\n\nif __name__ == '__main__':\n    # get argument settings.\n    args = parse_args()\n\n    zip_path = args.pred_zip\n    if not os.path.exists(zip_path):\n        log_msg = 'ERROR: No such zip file!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    extract_path='lp'\n    extract_file_list = extract_zip(zip_path, extract_path)\n    extract_file_list = [os.path.join(extract_path,f_) for f_ in extract_file_list]\n\n    data_list = ['amazon', 'LastFM', 'PubMed']\n\n    res={}\n    detect_data_files = []\n    for data_name in data_list:\n        pred_files = []\n        for i in range(1, 6):\n            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n            if not os.path.exists(file_name):\n                continue\n            pred_files.append(file_name)\n            detect_data_files.append(file_name)\n        if len(pred_files) > 0 and len(pred_files) != 5:\n            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n            write_log(args.log, log_msg)\n            delete_files(extract_file_list)\n            sys.exit()\n        res[data_name] = AUC_MRR(data_name, pred_files)\n    if len(detect_data_files) == 0:\n        log_msg = f'ERROR: No file detected, please confirm that ' \\\n                  f'the data file is in the top directory of the compressed package!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    delete_files(extract_file_list)\n\n    hgb_score_list = []\n    for data_name in data_list:\n        hgb_score_list.append(res[data_name].AUC_mean)\n        hgb_score_list.append(res[data_name].MRR_mean)\n    hgb_score = np.mean(hgb_score_list)\n\n    detail_json = {}\n    log_msg = f'{hgb_score}###'\n    for data_name in data_list:\n        detail_json[data_name] = {}\n        detail_json[data_name]['AUC_mean'] = res[data_name].AUC_mean\n        detail_json[data_name]['AUC_std'] = res[data_name].AUC_std\n        detail_json[data_name]['MRR_mean'] = res[data_name].MRR_mean\n        detail_json[data_name]['MRR_std'] = res[data_name].MRR_std\n    log_msg += json.dumps(detail_json)\n    write_log(args.log, log_msg)\n    sys.exit()", "\nif __name__ == '__main__':\n    # get argument settings.\n    args = parse_args()\n\n    zip_path = args.pred_zip\n    if not os.path.exists(zip_path):\n        log_msg = 'ERROR: No such zip file!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    extract_path='lp'\n    extract_file_list = extract_zip(zip_path, extract_path)\n    extract_file_list = [os.path.join(extract_path,f_) for f_ in extract_file_list]\n\n    data_list = ['amazon', 'LastFM', 'PubMed']\n\n    res={}\n    detect_data_files = []\n    for data_name in data_list:\n        pred_files = []\n        for i in range(1, 6):\n            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n            if not os.path.exists(file_name):\n                continue\n            pred_files.append(file_name)\n            detect_data_files.append(file_name)\n        if len(pred_files) > 0 and len(pred_files) != 5:\n            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n            write_log(args.log, log_msg)\n            delete_files(extract_file_list)\n            sys.exit()\n        res[data_name] = AUC_MRR(data_name, pred_files)\n    if len(detect_data_files) == 0:\n        log_msg = f'ERROR: No file detected, please confirm that ' \\\n                  f'the data file is in the top directory of the compressed package!'\n        write_log(args.log, log_msg)\n        sys.exit()\n    delete_files(extract_file_list)\n\n    hgb_score_list = []\n    for data_name in data_list:\n        hgb_score_list.append(res[data_name].AUC_mean)\n        hgb_score_list.append(res[data_name].MRR_mean)\n    hgb_score = np.mean(hgb_score_list)\n\n    detail_json = {}\n    log_msg = f'{hgb_score}###'\n    for data_name in data_list:\n        detail_json[data_name] = {}\n        detail_json[data_name]['AUC_mean'] = res[data_name].AUC_mean\n        detail_json[data_name]['AUC_std'] = res[data_name].AUC_std\n        detail_json[data_name]['MRR_mean'] = res[data_name].MRR_mean\n        detail_json[data_name]['MRR_std'] = res[data_name].MRR_std\n    log_msg += json.dumps(detail_json)\n    write_log(args.log, log_msg)\n    sys.exit()", "\n"]}
