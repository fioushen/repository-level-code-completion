{"filename": "main.py", "chunked_list": ["import argparse\nimport os\n\n# Local/application specific imports\nfrom src.utils import utils\nfrom src.audio import audio_utils\n\n#TODO:\n# remove hardcoded file related variables\n\ndef validate_args(args):\n    \"\"\"\n    Validates the file path arguments.\n\n    Args:\n        args: The command line arguments.\n    \"\"\"\n    if not os.path.isfile(args.audio_link):\n        raise ValueError(f\"File not found: {args.audio_link}\")\n    if not os.path.isfile(args.vid_link):\n        raise ValueError(f\"File not found: {args.vid_link}\")", "# remove hardcoded file related variables\n\ndef validate_args(args):\n    \"\"\"\n    Validates the file path arguments.\n\n    Args:\n        args: The command line arguments.\n    \"\"\"\n    if not os.path.isfile(args.audio_link):\n        raise ValueError(f\"File not found: {args.audio_link}\")\n    if not os.path.isfile(args.vid_link):\n        raise ValueError(f\"File not found: {args.vid_link}\")", "\n\ndef main():\n    \"\"\"\n    Main function to handle command line arguments and initiate the video generation.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Generate video with subtitles.')\n    parser.add_argument('--audio_link', type=str, required=True,\n                        help='Path to the audio file.')\n    parser.add_argument('--vid_link', type=str, required=True,\n                        help='Path to the video file.')\n    parser.add_argument('--swear_word_list', type=str, required=False, default=\"\",\n                        help='Path to the text file with a list of swear words to be filtered out.')\n    parser.add_argument('--video_output', type=str, required=True,\n                        help='Path for the output video file.')\n    parser.add_argument('--srtFilename', type=str, required=False, default=\"\",\n                        help='Path for the subtitle file. If not provided, no subtitle file will be saved.')\n    \n    args = parser.parse_args()\n\n    # Validate the arguments\n    validate_args(args)\n\n    # If no swear word list is provided, default to the predefined list\n    if args.swear_word_list:\n        with open(args.swear_word_list, 'r') as file:\n            args.swear_word_list = [word.strip() for word in file.readlines()]\n    else:\n        args.swear_word_list = audio_utils.get_swear_word_list().keys()\n\n\n    utils.generate_video_with_subtitles(\n        args.audio_link, \n        args.vid_link, \n        args.swear_word_list, \n        args.video_output,\n        args.srtFilename\n        )", "\n\n    \nif __name__ == '__main__':\n    try:\n        main()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")"]}
{"filename": "gui_upload_video.py", "chunked_list": ["import tkinter as tk\nfrom tkinter import filedialog\nfrom argparse import Namespace\n\nfrom src.utils.upload_video import get_authenticated_service, initialize_upload, validate_shorts\nfrom googleapiclient.errors import HttpError\n\ndef browse_files(entry):\n    filename = filedialog.askopenfilename(initialdir = \"/\", title = \"Select a File\", filetypes = ((\"Text files\", \"*.mp4*\"), (\"all files\", \"*.*\")))\n    entry.delete(0, tk.END)\n    entry.insert(tk.END, filename)", "\ndef upload():\n    options = Namespace(\n        file=file_entry.get(),\n        title=title_entry.get(),\n        description=description_entry.get(),\n        category=category_entry.get(),\n        keywords=keywords_entry.get(),\n        privacyStatus=privacy_status_var.get(),\n        thumbnail=thumbnail_entry.get(),\n        madeForKids=bool(made_for_kids_var.get()),\n        youtubeShort=bool(youtube_short_var.get())\n    )\n\n    if options.youtubeShort:\n        validate_shorts(options)\n\n    youtube = get_authenticated_service()\n    try:\n        initialize_upload(youtube, options)\n    except HttpError as e:\n        print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')", "\nroot = tk.Tk()\n\nfile_label = tk.Label(root, text=\"Video file\")\nfile_label.pack()\nfile_entry = tk.Entry(root)\nfile_entry.pack()\nbrowse_button = tk.Button(root, text=\"Browse\", command=lambda: browse_files(file_entry))\nbrowse_button.pack()\n", "browse_button.pack()\n\ntitle_label = tk.Label(root, text=\"Title\")\ntitle_label.pack()\ntitle_entry = tk.Entry(root)\ntitle_entry.insert(tk.END, 'Test Title')\ntitle_entry.pack()\n\ndescription_label = tk.Label(root, text=\"Description\")\ndescription_label.pack()", "description_label = tk.Label(root, text=\"Description\")\ndescription_label.pack()\ndescription_entry = tk.Entry(root)\ndescription_entry.insert(tk.END, 'Test Description')\ndescription_entry.pack()\n\ncategory_label = tk.Label(root, text=\"Category\")\ncategory_label.pack()\ncategory_entry = tk.Entry(root)\ncategory_entry.insert(tk.END, '27')", "category_entry = tk.Entry(root)\ncategory_entry.insert(tk.END, '27')\ncategory_entry.pack()\n\nkeywords_label = tk.Label(root, text=\"Keywords\")\nkeywords_label.pack()\nkeywords_entry = tk.Entry(root)\nkeywords_entry.insert(tk.END, '')\nkeywords_entry.pack()\n", "keywords_entry.pack()\n\nprivacy_status_var = tk.StringVar(root)\nprivacy_status_var.set(\"private\") \nprivacy_status_option = tk.OptionMenu(root, privacy_status_var, \"public\", \"private\", \"unlisted\")\nprivacy_status_option.pack()\n\nthumbnail_label = tk.Label(root, text=\"Thumbnail\")\nthumbnail_label.pack()\nthumbnail_entry = tk.Entry(root)", "thumbnail_label.pack()\nthumbnail_entry = tk.Entry(root)\nthumbnail_entry.insert(tk.END, '')\nthumbnail_entry.pack()\nthumbnail_browse_button = tk.Button(root, text=\"Browse\", command=lambda: browse_files(thumbnail_entry))\nthumbnail_browse_button.pack()\n\nmade_for_kids_var = tk.BooleanVar(root)\nmade_for_kids_check = tk.Checkbutton(root, text='Made for Kids', variable=made_for_kids_var)\nmade_for_kids_check.pack()", "made_for_kids_check = tk.Checkbutton(root, text='Made for Kids', variable=made_for_kids_var)\nmade_for_kids_check.pack()\n\nyoutube_short_var = tk.BooleanVar(root)\nyoutube_short_check = tk.Checkbutton(root, text='YouTube Shorts', variable=youtube_short_var)\nyoutube_short_check.pack()\n\nupload_button = tk.Button(root, text=\"Upload\", command=upload)\nupload_button.pack()\n", "upload_button.pack()\n\nroot.mainloop()\n"]}
{"filename": "examples/reddit_to_video.py", "chunked_list": ["\"\"\"\nThis module fetches posts from a subreddit, converts one of the posts into audio,\nthen generates a video with subtitles from the audio and a sample video.\n\"\"\"\n\nimport os\nimport sys\nimport logging\nfrom dotenv import load_dotenv\nfrom pathlib import Path", "from dotenv import load_dotenv\nfrom pathlib import Path\n\n# Third party imports\nfrom elevenlabs import set_api_key, generate, save\n\n# Local/application specific imports\nfrom src.utils import reddit_api, utils\nfrom src.audio import audio_utils\n\ndef check_environment_variables():\n    \"\"\"\n    Checks if necessary environment variables are set.\n    \n    This function gets the 'ELEVENLABS_API_KEY' and 'STRING_AUDIO_FILE_LOCATION' \n    from the environment variables and checks if they are set.\n    \n    Returns:\n        ELEVENLABS_API_KEY (str): The API key for elevenlabs.\n        STRING_AUDIO_FILE_LOCATION (str): The location to store the audio file.\n        \n    Raises:\n        SystemExit: If the environment variables are not set.\n    \"\"\"\n    ELEVENLABS_API_KEY = os.getenv('ELEVENLABS_API_KEY')\n    if not ELEVENLABS_API_KEY:\n        logging.error(\"The ELEVENLABS_API_KEY environment variable is not set.\")\n        sys.exit(1)\n\n    STRING_AUDIO_FILE_LOCATION = os.getenv(\"STRING_AUDIO_FILE_LOCATION\")\n    if not STRING_AUDIO_FILE_LOCATION:\n        logging.error(\"The STRING_AUDIO_FILE_LOCATION environment variable is not set.\")\n        sys.exit(1)\n\n    return ELEVENLABS_API_KEY, STRING_AUDIO_FILE_LOCATION", "from src.audio import audio_utils\n\ndef check_environment_variables():\n    \"\"\"\n    Checks if necessary environment variables are set.\n    \n    This function gets the 'ELEVENLABS_API_KEY' and 'STRING_AUDIO_FILE_LOCATION' \n    from the environment variables and checks if they are set.\n    \n    Returns:\n        ELEVENLABS_API_KEY (str): The API key for elevenlabs.\n        STRING_AUDIO_FILE_LOCATION (str): The location to store the audio file.\n        \n    Raises:\n        SystemExit: If the environment variables are not set.\n    \"\"\"\n    ELEVENLABS_API_KEY = os.getenv('ELEVENLABS_API_KEY')\n    if not ELEVENLABS_API_KEY:\n        logging.error(\"The ELEVENLABS_API_KEY environment variable is not set.\")\n        sys.exit(1)\n\n    STRING_AUDIO_FILE_LOCATION = os.getenv(\"STRING_AUDIO_FILE_LOCATION\")\n    if not STRING_AUDIO_FILE_LOCATION:\n        logging.error(\"The STRING_AUDIO_FILE_LOCATION environment variable is not set.\")\n        sys.exit(1)\n\n    return ELEVENLABS_API_KEY, STRING_AUDIO_FILE_LOCATION", "\n# This function fetches posts from the specified subreddit\ndef fetch_reddit_posts(subreddit):\n    return reddit_api.fetch_reddit_posts(subreddit)\n\n# This function creates a directory in the specified location to store the audio file\ndef create_audio_directory(audio_file_location):\n    audio_directory = utils.create_next_dir(audio_file_location)\n    current_directory = os.getcwd()\n    directory_path = os.path.join(current_directory, Path(audio_directory))\n    return directory_path", "\n# This function generates an audio file from the specified script using the Eleven Labs API\ndef generate_audio(script, voice, model):\n    return generate(text=script, voice=voice, model=model)\n\ndef main(api_key, audio_file_location):\n    subreddit = 'dndstories'\n\n    # Fetch posts from the subreddit\n    posts_dict = fetch_reddit_posts(subreddit)\n    first_story = posts_dict[-1]\n\n    # Convert the first post into a script\n    script_first_story = reddit_api.turn_post_into_script(\n        first_story['body'], first_story['title'])\n\n    # Create a directory to store the audio file\n    directory_path = create_audio_directory(audio_file_location)\n    complete_audio_path = os.path.join(directory_path, \"story_part_0.wav\")\n\n    # Fetch the list of swear words to filter\n    swear_word_list = [*audio_utils.get_swear_word_list().keys()]\n\n    # Generate the audio from the script\n    audio = generate_audio(script_first_story, voice=\"Bella\", model=\"eleven_monolingual_v1\")\n    save(audio, complete_audio_path)\n\n    input_video_file = r'sample_video.mp4'\n    output_video_file = r\"sample_0.mp4\"\n\n    # Generate the final video with subtitles, filtering out any swear words\n    utils.generate_video_with_subtitles(\n        complete_audio_path, input_video_file, swear_word_list, output_video_file)", "\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO)\n    load_dotenv()\n\n    # Check environment variables and proceed if all necessary variables are set\n    ELEVENLABS_API_KEY, STRING_AUDIO_FILE_LOCATION = check_environment_variables()\n    set_api_key(ELEVENLABS_API_KEY)\n    main(ELEVENLABS_API_KEY, STRING_AUDIO_FILE_LOCATION)\n", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/audio/audio_utils.py", "chunked_list": ["from typing import List, Tuple, Dict, Union\nimport re\nimport os\n\nimport csv\nfrom pydub import AudioSegment\n\nfrom src.audio import concate_audio\nfrom src.utils import (generate_subtitles, text_utils)\n", "from src.utils import (generate_subtitles, text_utils)\n\n\nSWEAR_WORD_LIST_FILE_LOCATION = os.getenv('SWEAR_WORD_LIST_FILE_LOCATION_FILE_LOCATION')\n\ndef silence_segments(input_file, output_file, segments):\n    '''silences all selected segments'''\n    # Load audio file\n    audio = AudioSegment.from_file(input_file)\n\n    # Loop over the list of segments\n    for segment in segments:\n        # Calculate the start and end times in milliseconds\n        start_ms = segment['start'] * 1000\n        end_ms = segment['end'] * 1000\n\n        # Create a silent audio segment with the same duration as the specified segment\n        duration = end_ms - start_ms\n        silent_segment = AudioSegment.silent(duration=duration)\n\n        # Replace the segment with the silent audio\n        audio = audio[:start_ms] + silent_segment + audio[end_ms:]\n\n    # Export the modified audio to a file\n    audio.export(output_file, format=\"wav\")", "\ndef make_family_friendly(input_data:str,swear_word_list:List[str],output_data:str=\"output0.wav\"):\n    x = generate_subtitles.transcribe_and_align(input_data)\n    x_word_segments = x['word_segments']\n\n    swear_word_segements = text_utils.filter_text_by_list(x_word_segments,swear_word_list)\n\n    silence_segments(input_data, output_data, swear_word_segements)\n\ndef mask_swear_segments(word_list: List[str], x_word_segments: List[Dict[str, Union[str, float]]]) -> List[Dict[str, Union[str, float]]]:\n    x_word_segments_copy = []\n    for i in x_word_segments:\n        segment_copy = i.copy()\n        segment_copy['text'] = mask_specific_words(word_list, i['text'])\n        x_word_segments_copy.append(segment_copy)\n    return x_word_segments_copy", "\ndef mask_swear_segments(word_list: List[str], x_word_segments: List[Dict[str, Union[str, float]]]) -> List[Dict[str, Union[str, float]]]:\n    x_word_segments_copy = []\n    for i in x_word_segments:\n        segment_copy = i.copy()\n        segment_copy['text'] = mask_specific_words(word_list, i['text'])\n        x_word_segments_copy.append(segment_copy)\n    return x_word_segments_copy\n\ndef remove_swears(audio_script:str) ->str:\n    links_dict = get_swear_word_list()\n\n    for word, replacement in links_dict.items():\n        audio_script = audio_script.replace(word, replacement)\n\n    return audio_script", "\ndef remove_swears(audio_script:str) ->str:\n    links_dict = get_swear_word_list()\n\n    for word, replacement in links_dict.items():\n        audio_script = audio_script.replace(word, replacement)\n\n    return audio_script\n\ndef get_swear_word_list():\n        with open(SWEAR_WORD_LIST_FILE_LOCATION, 'r') as f:\n            reader = csv.reader(f)\n            # create a dictionary with the first column as the keys and the second column as the values\n            links_dict = {rows[0]: rows[1] for rows in reader}\n        return links_dict", "\ndef get_swear_word_list():\n        with open(SWEAR_WORD_LIST_FILE_LOCATION, 'r') as f:\n            reader = csv.reader(f)\n            # create a dictionary with the first column as the keys and the second column as the values\n            links_dict = {rows[0]: rows[1] for rows in reader}\n        return links_dict\n\ndef mask_word(match):\n    word = match.group(0)\n    return word[0] + \"*\" * (len(word) - 2) + word[-1]", "def mask_word(match):\n    word = match.group(0)\n    return word[0] + \"*\" * (len(word) - 2) + word[-1]\n\ndef mask_specific_words(words_to_mask: List[str], string_to_mask: str) -> str:\n    \"\"\"\n    Mask specific words in a given string by replacing them with asterisks, while preserving the first and last characters.\n\n    Args:\n        words_to_mask (List[str]): List of words to mask.\n        string_to_mask (str): String to be masked.\n\n    Returns:\n        str: Masked string.\n    \"\"\"\n    # Create a set of unique words to mask for faster lookup\n    words_set = set(words_to_mask)\n\n    # Compile the regex pattern to match any of the words to mask\n    pattern = re.compile(r\"\\b(?:{})\\b\".format(\"|\".join(re.escape(word) for word in words_set)), flags=re.IGNORECASE)\n\n    # Replace the matched words with asterisks, preserving the first and last characters\n\n    # Perform the replacement using the compiled pattern and mask_word function\n    masked_string = pattern.sub(mask_word, string_to_mask)\n\n    return masked_string"]}
{"filename": "src/audio/concate_audio.py", "chunked_list": ["#!/usr/bin/env python3\n\nimport os\nimport random\nimport sys\nimport argparse\nfrom typing import List\nfrom natsort import natsorted\nfrom pydub import AudioSegment\n", "from pydub import AudioSegment\n\n\ndef get_sorted_audio_files(directory: str) -> List[str]:\n    return [os.path.join(directory, f) for f in natsorted(os.listdir(directory)) if f.endswith('.wav')]\n\n\ndef combine_audio_files_directory(directory: str, output: str) -> AudioSegment:\n    \"\"\"\n    Combines all .wav audio files in a given directory into a single audio file and exports it to the specified output file.\n\n    :param directory: Path to the directory containing the audio files to be combined.\n    :param output: Path to the output file where the combined audio will be saved.\n    :return: The combined audio as a PyDub AudioSegment object.\n    \"\"\"\n    audio_files = get_sorted_audio_files(directory)\n    combined_audio = AudioSegment.empty()\n    for audio_file in audio_files:\n        audio = AudioSegment.from_file(audio_file)\n        combined_audio += audio\n    combined_audio.export(output, format='wav')\n    return combined_audio", "\n\ndef combine_audio_files_with_random_pause(directory:str, output:str) -> AudioSegment:\n    \"\"\"\n    Combines all .wav audio files in a given directory into a single audio file with a random pause (300ms to 500ms) between\n    each file and exports it to the specified output file.\n    \n    :param directory: Path to the directory containing the audio files to be combined.\n    :param output: Path to the output file where the combined audio will be saved.\n    :return: The combined audio as a PyDub AudioSegment object.\n    \"\"\"\n    combined_audio = AudioSegment.empty()\n    audio_files = get_sorted_audio_files(directory)\n    combined_audio = AudioSegment.from_file(audio_files[0])\n    for audio_file in audio_files[1:]:\n        pause_duration = random.randint(300, 500)\n        combined_audio += AudioSegment.silent(duration=pause_duration)\n        audio = AudioSegment.from_file(audio_file)\n        combined_audio += audio\n    combined_audio.export(output, format='wav')\n    return combined_audio", "\n\ndef main(args: List[str]) -> None:\n    parser = argparse.ArgumentParser(description='Combine multiple audio files into one')\n    parser.add_argument('directory', metavar='DIRECTORY', help='directory containing the audio files to combine')\n    parser.add_argument('-o', '--output', default='combined_audio.wav', help='output filename (default: combined_audio.wav)')\n    parser.add_argument('--pause', action='store_true', help='add random pause between files')\n    args = parser.parse_args(args)\n\n    if args.pause:\n        combine_audio_files_with_random_pause(args.directory, args.output)\n    else:\n        combine_audio_files_directory(args.directory, args.output)", "\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n"]}
{"filename": "src/audio/__init__.py", "chunked_list": [""]}
{"filename": "src/test/test_audio.py", "chunked_list": ["import os\nimport tempfile\nimport pytest\nfrom pydub import AudioSegment\nfrom pyttsx3 import init\nimport shutil\nfrom ..audio.concate_audio import (\n    get_sorted_audio_files,\n    combine_audio_files_directory,\n    combine_audio_files_with_random_pause,", "    combine_audio_files_directory,\n    combine_audio_files_with_random_pause,\n)\n\n\n@pytest.fixture\ndef audio_files_directory():\n    # Create a temporary directory for the audio files\n    temp_dir = tempfile.mkdtemp()\n\n    # Create temporary audio files in the directory\n    audio_files = ['file1.wav', 'file2.wav', 'file3.wav']\n    for audio_file in audio_files:\n        file_path = os.path.join(temp_dir, audio_file)\n        generate_audio(\"Sample audio\", file_path)  # Generate sample audio using TTS\n\n    yield temp_dir\n\n    # Clean up the temporary directory and files after the test\n    for audio_file in audio_files:\n        file_path = os.path.join(temp_dir, audio_file)\n        os.remove(file_path)\n    shutil.rmtree(temp_dir, ignore_errors=True)", "\ndef generate_audio(text, output_file):\n    engine = init()\n    engine.save_to_file(text, output_file)\n    engine.runAndWait()\n\ndef test_get_sorted_audio_files(audio_files_directory):\n    expected_files = [\n        os.path.join(audio_files_directory, 'file1.wav'),\n        os.path.join(audio_files_directory, 'file2.wav'),\n        os.path.join(audio_files_directory, 'file3.wav'),\n    ]\n    assert get_sorted_audio_files(audio_files_directory) == expected_files", "\n\ndef test_combine_audio_files_directory(audio_files_directory):\n    output_file = os.path.join(audio_files_directory, 'combined_audio.wav')\n    combined_audio = combine_audio_files_directory(audio_files_directory, output_file)\n    assert isinstance(combined_audio, AudioSegment)\n    assert os.path.isfile(output_file)\n\n\ndef test_combine_audio_files_with_random_pause(audio_files_directory):\n    output_file = os.path.join(audio_files_directory, 'combined_audio.wav')\n    combined_audio = combine_audio_files_with_random_pause(audio_files_directory, output_file)\n    assert isinstance(combined_audio, AudioSegment)\n    assert os.path.isfile(output_file)", "\ndef test_combine_audio_files_with_random_pause(audio_files_directory):\n    output_file = os.path.join(audio_files_directory, 'combined_audio.wav')\n    combined_audio = combine_audio_files_with_random_pause(audio_files_directory, output_file)\n    assert isinstance(combined_audio, AudioSegment)\n    assert os.path.isfile(output_file)\n\n\n# Additional tests can be added as needed\n", "# Additional tests can be added as needed\n"]}
{"filename": "src/test/__init__.py", "chunked_list": [""]}
{"filename": "src/metadata/keyword_analysis.py", "chunked_list": ["import os\nimport matplotlib.pyplot as plt\nfrom pytrends.request import TrendReq\nimport pandas as pd\nimport datetime\nfrom typing import List, Optional\n\ndef validate_inputs(keywords: List[str], timeframe: str) -> None:\n    \"\"\"\n    Validate the inputs for the get_trends function.\n\n    Parameters\n    ----------\n    keywords : list of str\n        List of keywords to get trends data for.\n    timeframe : str\n        Timeframe for the trends data.\n    \"\"\"\n    if not isinstance(keywords, list):\n        raise ValueError(\"Keywords should be a list of strings.\")\n    \n    valid_timeframes = ['now 1-H', 'now 4-H', 'now 1-d', 'now 7-d', 'today 1-m', 'today 3-m', 'today 12-m', 'today 5-y', 'all']\n    if timeframe not in valid_timeframes:\n        raise ValueError(\"Invalid timeframe. Check the Google Trends API for valid timeframes.\")", "\n\ndef get_trends(keywords: List[str], timeframe: str = 'today 5-y') -> pd.DataFrame:\n    \"\"\"\n    Get Google Trends data for a list of keywords.\n\n    Args:\n        keywords (List[str]): List of keywords to get trends data for.\n        timeframe (str, optional): Timeframe for the trends data, defaults to 'today 5-y'.\n\n    Returns:\n        pd.DataFrame: DataFrame with the trends data.\n    \"\"\"\n    pytrends = TrendReq(hl='en-US', tz=360)\n\n    # Build the payload\n    pytrends.build_payload(keywords, timeframe=timeframe)\n\n    # Get Google Trends data\n    trends_data = pytrends.interest_over_time()\n\n    return trends_data", "\n\ndef plot_trends(trend_data: pd.DataFrame, save: bool = False, filename: Optional[str] = None) -> None:\n    \"\"\"\n    Plot Google Trends data.\n\n    Parameters\n    ----------\n    trend_data : pandas.DataFrame\n        DataFrame with the trends data.\n    save : bool, optional\n        Whether to save the plot as a PNG image, defaults to False.\n    filename : str, optional\n        Filename for the PNG image, defaults to None.\n    \"\"\"\n    plt.figure(figsize=(14, 8))\n    for keyword in trend_data.columns[:-1]:  # Exclude the 'isPartial' column\n        plt.plot(trend_data.index, trend_data[keyword], label=keyword)\n    plt.xlabel('Date')\n    plt.ylabel('Trends Index')\n    plt.title('Google Search Trends over time')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \n    if save:\n        if filename is None:\n            # Create filename with current datetime if not specified\n            filename = f\"graph_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.png\"\n        plt.savefig(filename, format='png')  # Save the plot as a PNG image", "\ndef main() -> None:\n    \"\"\"\n    Main function to get and plot Google Trends data.\n    \"\"\"\n    # List of keywords to get trends data for\n    keywords = ['Blockchain', 'pizza', 'Australian Cattle Dog','AI',\"Chinese Food\"]\n\n    # Get the trends data\n    trend_data = get_trends(keywords)\n\n    # Plot the trends data\n    plot_trends(trend_data)", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metadata/video_engagement_metrics.py", "chunked_list": ["import os\nimport argparse\nfrom googleapiclient.discovery import build\nfrom dotenv import load_dotenv\n\ndef get_video_engagement_metrics(video_id: str, api_key: str) -> dict:\n    \"\"\"\n    Fetch engagement metrics for a specific YouTube video.\n\n    Args:\n        video_id (str): The ID of the YouTube video.\n        api_key (str): The Google API key.\n\n    Returns:\n        dict: A dictionary containing engagement metrics and video metadata.\n    \"\"\"\n    # Build the YouTube API client\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    # Make the API request to get video statistics and snippet (for metadata)\n    response = youtube.videos().list(\n        part='statistics,snippet',\n        id=video_id\n    ).execute()\n\n    # Extract engagement metrics and metadata from the response\n    engagement_metrics = response['items'][0]['statistics']\n    metadata = response['items'][0]['snippet']\n\n    # Add metadata to the engagement metrics dictionary\n    engagement_metrics.update(metadata)\n\n    return engagement_metrics", "\n\ndef get_video_comments(video_id: str, api_key: str, max_results: int = 20, include_replies: bool = True) -> list:\n    \"\"\"\n    Fetch comments for a specific YouTube video.\n\n    Args:\n        video_id (str): The ID of the YouTube video.\n        api_key (str): The Google API key.\n        max_results (int, optional): Maximum number of comments to return. Defaults to 20.\n        include_replies (bool, optional): If True, includes replies to comments. Defaults to True.\n\n    Returns:\n        list: A list containing comments, booleans indicating if it is a reply, the comment's publish datetime, \n              like count, and author channel Id.\n    \"\"\"\n    # Build the YouTube API client\n    youtube = build('youtube', 'v3', developerKey=api_key)\n\n    # Fetch comments for the video\n    response = youtube.commentThreads().list(\n        part='snippet,replies',\n        videoId=video_id,\n        textFormat='plainText',\n        maxResults=max_results\n    ).execute()\n\n    # Extract comments from the response\n    comments = []\n    for item in response['items']:\n        comment = item['snippet']['topLevelComment']['snippet']\n        comments.append(\n            {\n                'text': comment['textDisplay'],\n                'is_reply': False,\n                'like_count': comment['likeCount'],\n                'author_channel_id': comment['authorChannelId']['value'],\n                'publish_time': comment['publishedAt']\n            }\n        )\n\n        # Extract replies if any\n        if include_replies and 'replies' in item:\n            for reply in item['replies']['comments']:\n                reply_comment = reply['snippet']\n                comments.append(\n                    {\n                        'text': reply_comment['textDisplay'],\n                        'is_reply': True,\n                        'like_count': reply_comment['likeCount'],\n                        'author_channel_id': reply_comment['authorChannelId']['value'],\n                        'publish_time': reply_comment['publishedAt']\n                    }\n                )\n\n    return comments", "\n\ndef main():\n    # Load environment variables\n    load_dotenv()\n\n    # Get the API key from the environment variables\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\n\n    # Setup argument parser\n    parser = argparse.ArgumentParser(description='Fetch engagement metrics or comments for a YouTube video.')\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument('--metrics', action='store_true', help='Get engagement metrics')\n    group.add_argument('--comments', type=int, nargs='?', const=20, help='Get comments')\n    parser.add_argument('--no-replies', action='store_true', help='Exclude replies to comments')\n    parser.add_argument('--order', type=str, choices=['relevance', 'time', 'rating', 'videoLikes', 'videoRelevance'], default='relevance', help='Order of comments')\n    parser.add_argument('video_id', type=str, help='The ID of the YouTube video')\n\n    # Parse command-line arguments\n    args = parser.parse_args()\n\n    if args.metrics:\n        # Get and print engagement metrics\n        engagement_metrics = get_video_engagement_metrics(args.video_id, api_key)\n        print(engagement_metrics)\n    elif args.comments is not None:\n        # Get and print comments\n        comments = get_video_comments(args.video_id, api_key, args.comments, not args.no_replies, args.order)\n        for comment, is_reply, datetime in comments:\n            print(f'{\"Reply\" if is_reply else \"Comment\"} ({datetime}): {comment}')", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metadata/text_clustering_and_keyword_extraction.py", "chunked_list": ["from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# Assume descriptions contains the text data of the video descriptions\ndescriptions = [\"Python tutorial\", \"How to bake a cake\", \"Machine learning basics\", \"...\"]\n\n# Number of clusters\nnum_clusters = 3\n\n# Vectorize the descriptions using TF-IDF", "\n# Vectorize the descriptions using TF-IDF\nvectorizer = TfidfVectorizer(max_features=10000)\ntfidf = vectorizer.fit_transform(descriptions)\n\n# Perform KMeans clustering\nkmeans = KMeans(n_clusters=num_clusters)\nkmeans.fit(tfidf)\n\n# For each cluster, print the top keywords\nfor i in range(num_clusters):\n    print(f\"Niche #{i + 1}:\")\n    \n    # Get the descriptions in this cluster\n    cluster_descriptions = tfidf[kmeans.labels_ == i]\n    \n    # Sum the TF-IDF scores for each keyword\n    sum_tfidf = cluster_descriptions.sum(axis=0)\n\n    # Get the top 10 keywords in this cluster\n    top_keywords_indices = sum_tfidf.argsort()[0, ::-1][:10]\n    top_keywords = [vectorizer.get_feature_names_out()[index] for index in top_keywords_indices.flat]\n    \n    print(top_keywords)", "\n# For each cluster, print the top keywords\nfor i in range(num_clusters):\n    print(f\"Niche #{i + 1}:\")\n    \n    # Get the descriptions in this cluster\n    cluster_descriptions = tfidf[kmeans.labels_ == i]\n    \n    # Sum the TF-IDF scores for each keyword\n    sum_tfidf = cluster_descriptions.sum(axis=0)\n\n    # Get the top 10 keywords in this cluster\n    top_keywords_indices = sum_tfidf.argsort()[0, ::-1][:10]\n    top_keywords = [vectorizer.get_feature_names_out()[index] for index in top_keywords_indices.flat]\n    \n    print(top_keywords)", ""]}
{"filename": "src/metadata/youtube_search.py", "chunked_list": ["import os\nimport urllib.request\nimport re\nimport requests\nfrom dotenv import load_dotenv\nfrom typing import Union, List\n\ndef get_unique_video_ids(search_query: str) -> List[str]:\n    \"\"\"\n    Get the unique video IDs from YouTube search results.\n\n    Args:\n        search_query (str): The search query to use on YouTube.\n\n    Returns:\n        List[str]: A list of unique video IDs from the search results.\n    \"\"\"\n    # Replace white spaces with a \"+\" symbol\n    search_query = search_query.replace(\" \", \"+\")\n    \n    # Create a URL to search on YouTube using the search query\n    url = f\"https://www.youtub.com/results?search_query={search_query}&sp=CAMSBAgEEAE%253D\"\n    \n    # Open the URL and read the HTML response\n    with urllib.request.urlopen(url) as html:\n        html_content = html.read().decode()\n    \n    # Find all video IDs in the HTML content using a regular expression\n    video_ids = re.findall(r\"watch\\?v=(\\S{11})\", html_content)\n    \n    # Convert the list of video IDs to a set to remove duplicates and then convert it back to a list\n    unique_video_ids = list(set(video_ids))\n    return unique_video_ids", "\n\ndef get_video_views(video_id: str, api_key: str) -> Union[str, None]:\n    \"\"\"\n    Get the view count for a specific YouTube video.\n\n    Args:\n        video_id (str): The ID of the YouTube video.\n        api_key (str): The Google API key.\n\n    Returns:\n        Union[str, None]: The view count for the video as a string, or None if an error occurred.\n    \"\"\"\n    # Construct the API URL\n    url = f'https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key={api_key}'\n\n    # Make a GET request to the API\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Load the JSON response into a Python dictionary\n        data = response.json()\n    \n        # Get the view count from the statistics object\n        view_count = data['items'][0]['statistics']['viewCount']\n    \n        # Return the view count\n        return view_count\n    else:\n        # If the request was not successful, raise an exception\n        raise Exception(f'Error retrieving video data: {response.text}')", "\n\ndef main():\n    # Load environment variables from .env file\n    load_dotenv()\n\n    # Get the API key from the environment variable GOOGLE_API_KEY\n    API_KEY = os.getenv('GOOGLE_API_KEY')\n\n    # Check if the API key was successfully loaded\n    if not API_KEY:\n        raise Exception('Missing API key. Please set the environment variable GOOGLE_API_KEY in the .env file.')\n\n    # Example usage\n    search_query = \"Mozart\"\n    video_ids = get_unique_video_ids(search_query)\n    for video_id in video_ids:\n        view_count = get_video_views(video_id, API_KEY)\n        print(f'The video with ID {video_id} has {view_count} views.')", "\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metadata/check_channel_monitization.py", "chunked_list": ["\"\"\"\nThis module provides a function to check whether a given YouTube channel is monetized. \n\nThe function `is_monetized` sends a GET request to a specified YouTube channel URL and then \nparses the HTML of the page looking for the 'is_monetization_enabled' key in the script tags. \nIt then returns `True` if the key's value is set to `true` and `False` otherwise.\n\"\"\"\n\nimport argparse\nimport requests", "import argparse\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef is_monetized(url):\n    \"\"\"\n    Determines if the specified YouTube channel is monetized.\n\n    Args:\n        url (str): The URL of the YouTube channel.\n\n    Returns:\n        bool: True if the channel is monetized, False otherwise.\n    \"\"\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    monetization_key = '{\"key\":\"is_monetization_enabled\",\"value\":\"true\"}'\n\n    scripts = soup.find_all('script')\n    for script in scripts:\n        # Make sure script.string is not None before checking for the monetization key\n        if script.string and monetization_key in script.string:\n            return True\n    return False", "\ndef main():\n    parser = argparse.ArgumentParser(description='Check if a YouTube channel is monetized.')\n    parser.add_argument('url', type=str, help='The URL of the YouTube channel to check.')\n\n    args = parser.parse_args()\n\n    print(is_monetized(args.url))\n\nif __name__ == \"__main__\":\n    main()", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metadata/data_collection.py", "chunked_list": ["from metadata.get_youtube_video_tags import get_video_tags\nfrom metadata.video_engagement_metrics import get_video_engagement_metrics\n\ndef collect_data(video_id, api_key):\n    # Collect metadata about the video\n    tags = get_video_tags(video_id, api_key)\n    engagement_metrics = get_video_engagement_metrics(video_id, api_key)\n    \n    # Combine the tags and engagement metrics into one dictionary\n    data = {**tags, **engagement_metrics}\n    \n    return data", ""]}
{"filename": "src/metadata/get_youtube_video_tags.py", "chunked_list": ["import argparse\nfrom YoutubeTags import videotags\n\ndef get_video_tags(video_url: str) -> list:\n    \"\"\"Get a list of tags for a YouTube video.\n\n    Args:\n        video_url (str): URL of the YouTube video.\n\n    Returns:\n        list: List of tags for the video, or an empty list if tags could not be retrieved.\n    \"\"\"\n    try:\n        findtags = videotags(video_url)\n        return [tag.strip() for tag in findtags.split(\",\")]\n    except Exception as e:\n        print(f\"An error occurred while retrieving tags for the video: {e}\")\n        return []", "\ndef main():\n    parser = argparse.ArgumentParser(description='Get YouTube video tags')\n    parser.add_argument('url', type=str, help='YouTube video URL')\n\n    args = parser.parse_args()\n\n    tags = get_video_tags(args.url)\n    print(tags)\n\nif __name__ == \"__main__\":\n    main()", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/metadata/__init__.py", "chunked_list": ["from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Assume we have some dataset of videos with their metadata and labels\nvideos = [\n    {\"title\": \"Python Tutorial for Beginners\", \"description\": \"Learn Python programming\", \"tags\": [\"Python\", \"Programming\"], \"niche\": \"technology\"},\n    {\"title\": \"Easy Chocolate Cake Recipe\", \"description\": \"Delicious chocolate cake\", \"tags\": [\"Baking\", \"Cake\"], \"niche\": \"cooking\"},\n]", "    {\"title\": \"Easy Chocolate Cake Recipe\", \"description\": \"Delicious chocolate cake\", \"tags\": [\"Baking\", \"Cake\"], \"niche\": \"cooking\"},\n]\n\nmetadata = [video['title'] + ' ' + video['description'] + ' ' + ' '.join(video['tags']) for video in videos]\nlabels = [video['niche'] for video in videos]\n\n# Split the data into training and testing sets\nmetadata_train, metadata_test, labels_train, labels_test = train_test_split(metadata, labels, test_size=0.2, random_state=42)\n\n# Create a pipeline for data preprocessing and training", "\n# Create a pipeline for data preprocessing and training\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', LogisticRegression()),\n])\n\n# Train the model\npipeline.fit(metadata_train, labels_train)\n", "pipeline.fit(metadata_train, labels_train)\n\n# Test the model\naccuracy = pipeline.score(metadata_test, labels_test)\nprint(f\"Model accuracy: {accuracy}\")\n\n# Now we can use the trained model to categorize new videos\nvideo = {\"title\": \"How to lose weight\", \"description\": \"Effective workout routines\", \"tags\": [\"Fitness\", \"Workout\"]}\nmetadata = video['title'] + ' ' + video['description'] + ' ' + ' '.join(video['tags'])\nprint(f\"Predicted niche: {pipeline.predict([metadata])[0]}\")", "metadata = video['title'] + ' ' + video['description'] + ' ' + ' '.join(video['tags'])\nprint(f\"Predicted niche: {pipeline.predict([metadata])[0]}\")"]}
{"filename": "src/video/__init__.py", "chunked_list": [""]}
{"filename": "src/video/utils.py", "chunked_list": ["from typing import Tuple\nfrom moviepy.editor import VideoFileClip\n\ndef get_video_size(filename: str) -> Tuple[int, int]:\n    \"\"\"\n    Get the dimensions (width and height) of a video file.\n\n    Args:\n        filename (str): The path to the video file.\n\n    Returns:\n        Tuple[int, int]: A tuple containing the width and height of the video, in the format (width, height).\n    \"\"\"\n    video = VideoFileClip(filename)\n    return (video.w, video.h)", ""]}
{"filename": "src/video/cut_video.py", "chunked_list": ["import time\nimport argparse\nfrom pathlib import Path\n\nimport moviepy.editor as mp\n\n\ndef split_video(input_path: Path, output_path: Path, start_time: int, end_time: int) -> None:\n    \"\"\"\n    Split a video file into a specified section.\n\n    Args:\n        input_path (Path): The path to the input video file.\n        output_path (Path): The path to the output video file.\n        start_time (int): The starting time in seconds.\n        end_time (int): The ending time in seconds.\n\n    Raises:\n        ValueError: If the input video file does not exist or if the start or end times are invalid.\n\n    \"\"\"\n    if not input_path.exists():\n        raise ValueError(f\"Input file {input_path} does not exist.\")\n\n    if (\n        not isinstance(start_time, int) \n        or not isinstance(end_time, int) \n        or start_time < 0 or end_time < 0\n        ):\n        raise ValueError(\"Start and end times must be non-negative integers.\")\n\n    video = mp.VideoFileClip(str(input_path))\n\n    video_clip = video.subclip(start_time, end_time)\n\n    try:\n        video_clip.write_videofile(str(output_path), fps=24)\n    except OSError:\n        print(\"Error: Unable to write output file.\")\n        time.sleep(10)\n        output_path.unlink()\n        video_clip.write_videofile(str(output_path), fps=24)", "\n\ndef main():\n    # Parse the command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input_path\", type=Path, help=\"The path to the input video file\")\n    parser.add_argument(\"output_path\", type=Path, help=\"The path to the output video file\")\n    parser.add_argument(\"start_time\", type=int, help=\"The starting time in seconds\")\n    parser.add_argument(\"end_time\", type=int, help=\"The ending time in seconds\")\n    args = parser.parse_args()\n\n    # Call the cut_video function with the parsed arguments\n    input_path = args.input_path\n    output_path = args.output_path\n    start_time = args.start_time\n    end_time = args.end_time\n    split_video(input_path, output_path, start_time, end_time)", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/video/random_sample_clip.py", "chunked_list": ["import random\nimport argparse\n\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom moviepy.audio.io.AudioFileClip import AudioFileClip\n\n\n\ndef create_clip_with_matching_audio(video_path: str, audio_path: str, output_path: str) -> None:\n    \"\"\"\n    Create a video clip with the same duration as the provided audio file.\n    The video clip is extracted from the input video file and the audio is set to the provided audio file.\n    The resulting clip is saved to the output path.\n\n    Args:\n        video_path (str): The path to the input video file.\n        audio_path (str): The path to the input audio file.\n        output_path (str): The path to the output file where the resulting video clip will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Load video and audio files\n    video = VideoFileClip(video_path)\n    audio = AudioFileClip(audio_path)\n\n    # Set duration of clip to match audio file\n    duration = audio.duration\n\n    # Get a random start time for the clip\n    start_time = random.uniform(0, video.duration - duration)\n\n    # Extract clip from the video\n    clip = video.subclip(start_time, start_time + duration)\n\n    # Set the audio of the clip to the audio file\n    clip = clip.set_audio(audio)\n\n    # Save the clip\n    clip.write_videofile(output_path, audio_codec=\"aac\")", "def create_clip_with_matching_audio(video_path: str, audio_path: str, output_path: str) -> None:\n    \"\"\"\n    Create a video clip with the same duration as the provided audio file.\n    The video clip is extracted from the input video file and the audio is set to the provided audio file.\n    The resulting clip is saved to the output path.\n\n    Args:\n        video_path (str): The path to the input video file.\n        audio_path (str): The path to the input audio file.\n        output_path (str): The path to the output file where the resulting video clip will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Load video and audio files\n    video = VideoFileClip(video_path)\n    audio = AudioFileClip(audio_path)\n\n    # Set duration of clip to match audio file\n    duration = audio.duration\n\n    # Get a random start time for the clip\n    start_time = random.uniform(0, video.duration - duration)\n\n    # Extract clip from the video\n    clip = video.subclip(start_time, start_time + duration)\n\n    # Set the audio of the clip to the audio file\n    clip = clip.set_audio(audio)\n\n    # Save the clip\n    clip.write_videofile(output_path, audio_codec=\"aac\")", "\n\ndef main():\n    parser = argparse.ArgumentParser(description='Create a video clip with matching audio')\n    parser.add_argument('video_path', type=str, help='path to video file')\n    parser.add_argument('audio_path', type=str, help='path to audio file')\n    parser.add_argument('output_path', type=str, help='path to output file')\n    args = parser.parse_args()\n    create_clip_with_matching_audio(args.video_path, args.audio_path, args.output_path)\n", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/video/create_video_from_single_image.py", "chunked_list": ["import argparse\nimport pathlib\nimport subprocess\nfrom typing import Tuple\nfrom PIL import Image\n\ndef create_video_from_single_image(input_img: str, input_audio: str, output_file: str, output_extension: str = \"mp4\", img_size: Tuple[int, int] = (1080, 1920)) -> None:\n    \"\"\"\n    Creates a video using an input image and audio, with the specified output extension and image size.\n    \n    :param input_img: Path to the input image file.\n    :param input_audio: Path to the input audio file.\n    :param output_file: Name of the output video file.\n    :param output_extension: Extension of the output video file. Default is 'mp4'.\n    :param img_size: Tuple containing the desired image width and height. Default is (1080, 1920).\n    \"\"\"\n    # Resize the input image to the specified resolution\n    image = Image.open(input_img)\n    resized_image = image.resize(img_size)\n    resized_image.save(input_img)\n\n    # Create the video using the resized image and specified output extension\n    output_path = pathlib.Path(output_file)\n    output_path = output_path.with_suffix(f\".{output_extension}\")\n    command = ['ffmpeg', '-loop', '1', '-i', input_img, '-i', input_audio, \"-vcodec\", \"mpeg4\", \"-acodec\", \"aac\", '-shortest', str(output_path), \"-y\", \"-r\", \"2\"]\n    print(command)\n    subprocess.run(command)", "\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Create a video using an input image and audio.\")\n    parser.add_argument(\"input_img\", help=\"Path to the input image\")\n    parser.add_argument(\"input_audio\", help=\"Path to the input audio\")\n    parser.add_argument(\"output_file\", help=\"Name of the output video file\")\n    parser.add_argument(\"-e\", \"--output_extension\", help=\"Extension of the output video file\", default=\"mp4\")\n\n    args = parser.parse_args()\n    create_video_from_single_image(args.input_img, args.input_audio, args.output_file, args.output_extension)", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/video/concatenate_videos.py", "chunked_list": ["import argparse\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips\nfrom typing import List\n\ndef concatenate_videos(videos: List[str], output: str) -> None:\n    \"\"\"\n    Concatenate a list of video files and save the resulting video to an output file.\n\n    Args:\n        videos (List[str]): A list of paths to the video files to be concatenated.\n        output (str): The path to the output file where the concatenated video will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # list of video clips\n    video_clips = []\n\n    for path in videos:\n        # load video\n        clip = VideoFileClip(path)\n        clip = clip.subclip()\n\n        # append clip to the list\n        video_clips.append(clip)\n\n    # concatenate the clips\n    final_clip = concatenate_videoclips(video_clips)\n\n    # write the final clip to file\n    final_clip.write_videofile(output)", "\n\ndef main():\n    # create a parser object\n    parser = argparse.ArgumentParser(description=\"Concatenate a list of videos\")\n\n    # add an argument for the list of videos\n    parser.add_argument(\"videos\", nargs=\"+\", type=str, help=\"the videos to concatenate\")\n\n    # add an argument for the output file name\n    parser.add_argument(\"-o\", \"--output\", type=str, default=\"final_video.mp4\", help=\"the output file name\")\n\n    # parse the arguments\n    args = parser.parse_args()\n\n    # call the concatenate_videos function with the parsed arguments\n    concatenate_videos(args.videos, args.output)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/utils/text_utils.py", "chunked_list": ["import re\nfrom typing import List, Dict, Union\n\n\ndef replace_caps_with_hyphens(sentence):\n    pattern = r'\\b([A-Z]+)\\b'\n    replacement = lambda match: '-'.join(list(match.group(1)))\n    return re.sub(pattern, replacement, sentence)\n\n\ndef remove_parenthesis(text:str):\n    # define the pattern to match\n    pattern = r'\\(([^\\s()]+)\\)'\n    # remove the tokens from the string using regular expressions\n    # remove any text enclosed in parentheses if it contains only one word\n    text_without_single_word_parentheses = re.sub(\n        pattern, lambda m: m.group(1) if ' ' in m.group(1) else '', text\n        )\n    # return text_without_tokens\n    return text_without_single_word_parentheses", "\n\ndef remove_parenthesis(text:str):\n    # define the pattern to match\n    pattern = r'\\(([^\\s()]+)\\)'\n    # remove the tokens from the string using regular expressions\n    # remove any text enclosed in parentheses if it contains only one word\n    text_without_single_word_parentheses = re.sub(\n        pattern, lambda m: m.group(1) if ' ' in m.group(1) else '', text\n        )\n    # return text_without_tokens\n    return text_without_single_word_parentheses", "    \n\ndef replace_hyphens_with_single_space(text):\n\n    return re.sub(r'-\\B|\\B-', ' ', text)\n\ndef add_spaces_around_hyphen(words):\n\n    return re.sub(r'([a-zA-Z])-([a-zA-Z])', r'\\1 - \\2', words)\n\ndef add_spaces_around_hyphens(input_str):\n    # Replace all hyphens with a space followed by a hyphen followed by another space\n    # Example: 'A-I-T-A' -> 'A - I - T - A'\n    \n    output_str = re.sub(r'-', ' - ', input_str)\n    \n    return output_str", "\ndef add_spaces_around_hyphens(input_str):\n    # Replace all hyphens with a space followed by a hyphen followed by another space\n    # Example: 'A-I-T-A' -> 'A - I - T - A'\n    \n    output_str = re.sub(r'-', ' - ', input_str)\n    \n    return output_str\n\ndef clean_up(text:str) -> str:\n    \n    text = \" \".join(text.split())\n\n    text = remove_parenthesis(text)\n\n    text = text.replace(\"\\\\\",\" slash \")\n\n    if \"r/\" in text:\n        text = text.replace(\"r/\", \" R slash \")\n\n    if \"AITA\" in text:\n        text = text.replace(\"AITA\", \" am i the asshole  \")\n\n\n    text = replace_hyphens_with_single_space(text)\n    text = replace_caps_with_hyphens(text)\n    text =  add_spaces_around_hyphen(text)\n\n\n    return text", "\ndef clean_up(text:str) -> str:\n    \n    text = \" \".join(text.split())\n\n    text = remove_parenthesis(text)\n\n    text = text.replace(\"\\\\\",\" slash \")\n\n    if \"r/\" in text:\n        text = text.replace(\"r/\", \" R slash \")\n\n    if \"AITA\" in text:\n        text = text.replace(\"AITA\", \" am i the asshole  \")\n\n\n    text = replace_hyphens_with_single_space(text)\n    text = replace_caps_with_hyphens(text)\n    text =  add_spaces_around_hyphen(text)\n\n\n    return text", "\n\n\ndef join_sentences(sentences: List[str]) -> List[str]:\n    '''splits body of text such that it never surpases maximum token 250'''\n    result = []\n    current_sentence = \"\"\n    current_word_count = 0\n    \n    for sentence in sentences:\n        # split sentence into words and add to current word count\n        words = sentence.split()\n        current_word_count += len(words)\n        \n        # if adding the current sentence would result in too many words, add the current sentence to the result\n        if current_word_count > 249:\n            result.append(current_sentence)\n            current_sentence = \"\"\n            current_word_count = 0\n        \n        # add current sentence and a space to the result\n        if len(current_sentence) > 0:\n            current_sentence += \" \"\n        \n        # add current sentence to the result\n        current_sentence += sentence\n    \n    # add final sentence to the result\n    result.append(current_sentence)\n    \n    return result", "\n\n\ndef turn_post_into_script(reddit_post,reddit_title):\n    ending = \" . Ever been in a situation like this? Leave it in the comment section. Like and subscribe if you enjoyed this video and want to see more like them. Thank you for watching my video. I hope you enjoyed it, and please have a wonderful day.\"\n    opening = f\"Today's story from reddit - - ... {reddit_title} ... let's get into the story ... \"\n\n    total_script = opening + reddit_post + ending\n    return total_script\n", "\n\n\ndef filter_text_by_list(text_list: List[Dict[str, Union[str, float]]], word_list: List[str]) -> List[Dict[str, Union[str, float]]]:\n    '''returns segments of swear words'''\n    filtered_list = []\n    for item in text_list:\n        # Remove all non-alphanumeric characters from the item's text\n        cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]', '', item['text'])\n        if cleaned_text.lower() in word_list:\n            filtered_list.append(item)\n    return filtered_list", ""]}
{"filename": "src/utils/generate_subtitles.py", "chunked_list": ["from pathlib import Path\nimport argparse\nfrom typing import List, Dict, Union\n\nimport moviepy.editor as mp\nimport whisperx\nimport pandas as pd\nfrom moviepy.video.tools.subtitles import SubtitlesClip\nfrom moviepy.editor import VideoFileClip\n", "from moviepy.editor import VideoFileClip\n\n\nfrom src.video import utils\n\nTextSegmentList = [List[Dict[str, Union[str, float]]]]\n\n\ndef transcribe_and_align(input_path: Path, device: str = \"cpu\", model_type: str = \"medium\") -> dict:\n    \"\"\"Transcribe and align audio file.\n\n    Args:\n        input_path (Path): Path to audio file.\n        device (str, optional): Device to use for transcription and alignment.\n            Defaults to \"cpu\".\n        model_type (str, optional): Type of model to use for transcription.\n            Defaults to \"medium\".\n\n    Returns:\n        dict: Aligned transcriptions.\n    \"\"\"\n    model = whisperx.load_model(model_type,device)\n    result = model.transcribe(input_path)\n    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n    result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, input_path, device)\n    return result_aligned", "def transcribe_and_align(input_path: Path, device: str = \"cpu\", model_type: str = \"medium\") -> dict:\n    \"\"\"Transcribe and align audio file.\n\n    Args:\n        input_path (Path): Path to audio file.\n        device (str, optional): Device to use for transcription and alignment.\n            Defaults to \"cpu\".\n        model_type (str, optional): Type of model to use for transcription.\n            Defaults to \"medium\".\n\n    Returns:\n        dict: Aligned transcriptions.\n    \"\"\"\n    model = whisperx.load_model(model_type,device)\n    result = model.transcribe(input_path)\n    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n    result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, input_path, device)\n    return result_aligned", "\n\ndef segment_text_by_word_length(\n    my_list: list,\n    word_length_max: int = 5\n) -> TextSegmentList:\n    \"\"\"\n    Segments a list of dictionaries containing text and timestamps into groups of a specified maximum word length.\n\n    Args:\n        my_list (TextSegmentList): A list of dictionaries containing 'text', 'start', and 'end' keys.\n        word_length_max (int, optional): The maximum number of words per segment. Defaults to 5.\n\n    Returns:\n        TextSegmentList: A list of dictionaries containing the segmented text and corresponding start and end timestamps.\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be a list of dictionaries.\")\n\n    if not all(isinstance(item, dict) for item in my_list):\n        raise TypeError(\"Each item in 'my_list' must be a dictionary.\")\n\n    if not all(\n        all(key in item for key in [\"text\", \"start\", \"end\"])\n        for item in my_list\n    ):\n        raise ValueError(\"Each dictionary in 'my_list' must have 'text', 'start', and 'end' keys.\")\n\n    if not isinstance(word_length_max, int) or word_length_max < 1:\n        raise ValueError(\"Invalid value for 'word_length_max'. It must be a positive integer.\")\n\n    segmented_text = []\n    temp_segment = []\n\n    for item in my_list:\n        temp_segment.append(item)\n        if len(temp_segment) == word_length_max:\n            segmented_text.append(temp_segment)\n            temp_segment = []\n\n    if temp_segment:\n        segmented_text.append(temp_segment)\n\n    complete_segments = []\n    for segment in segmented_text:\n        start_time = segment[0]['start']\n        end_time = segment[-1]['end']\n        text = \" \".join(item['text'] for item in segment)\n        complete_segments.append({\"text\": text, \"start\": start_time, \"end\": end_time})\n\n    return complete_segments", "\ndef add_subtitles_to_video(input_path: str, output_path: str, word_segments: TextSegmentList) -> None:\n    \"\"\"\n    Add subtitles to a video file based on word segments with start and end times.\n\n    Args:\n        input_path (str): The path to the input video file.\n        output_path (str): The path to the output video file with subtitles added.\n        word_segments (TextSegmentList): A list of dictionaries containing 'text', 'start', and 'end' keys\n            for each word segment.\n\n    Returns:\n        None\n    \"\"\"\n    text_clip_data = {\n        'start': [segment['start'] for segment in word_segments],\n        'end': [segment['end'] for segment in word_segments],\n        'text': [segment['text'] for segment in word_segments]\n        }\n\n    df = pd.DataFrame.from_dict(text_clip_data)\n\n    movie_width, movie_height = utils.get_video_size(input_path)\n    # Write the video file\n    video = VideoFileClip(input_path)\n    generator = lambda txt: mp.TextClip(txt, fontsize=80, color='black', align='center', font='P052-Bold', stroke_width=3, bg_color=\"white\",method='caption',size=(movie_width, movie_height))\n    generator = lambda txt: mp.TextClip(txt, fontsize=80, color='white', align='center', font='P052-Bold', stroke_width=3, method='caption',size=(movie_width/2, movie_height),stroke_color=\"black\",)\n    subs = tuple(zip(tuple(zip(df['start'].values, df['end'].values)), df['text'].values))\n    subtitles = SubtitlesClip(subs, generator,)\n\n\n    final_clip = mp.CompositeVideoClip([video, subtitles.set_pos(('center','center')),])\n\n    try:\n        final_clip.write_videofile(output_path, fps=24)\n    except OSError:\n        Path(output_path).unlink()\n        final_clip.write_videofile(output_path, fps=24)\n        \n    return output_path", "\n\ndef main():\n    # Set up the argument parser\n    parser = argparse.ArgumentParser(description=\"Create a webm video using an input image and audio.\")\n    parser.add_argument(\"input_path\", type=Path, help=\"Path to the input audio file.\")\n    parser.add_argument(\"output_path\",  type=Path, default=None, help=\"Path to the output video file. If not provided, the input path will be used with a different file extension.\")\n    parser.add_argument(\"--device\", type=str, default=\"cpu\", help=\"Device to use for transcription and alignment (default: 'cpu')\")\n    parser.add_argument(\"--model_type\", type=str, default=\"medium\", help=\"Type of model to use for transcription (default: 'medium')\")\n    args = parser.parse_args()\n    \n    # Set the output path\n    if args.output_path is None:\n        output_path = args.input_path\n    else:\n        output_path = args.output_path\n\n    input_path = args.input_path\n    input_path = str(input_path)\n    output_path = str(output_path)\n\n    #  Transcribe the audio file and align the transcript\n    word_segments = transcribe_and_align(input_path, args.device, args.model_type)\n    word_segments = word_segments['word_segments']\n    \n    # Add the subtitles to the video\n    add_subtitles_to_video(input_path, output_path, word_segments)", "\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n"]}
{"filename": "src/utils/play_ht_api.py", "chunked_list": ["import requests\nimport json\nimport os\nimport time\nimport urllib.request\nfrom dotenv import load_dotenv\nimport math\n\n# Load the .env file\nload_dotenv()", "# Load the .env file\nload_dotenv()\n\n# Get the value of the PLAYHT_API_KEY variable\nPLAYHT_API_KEY = os.getenv('PLAYHT_API_KEY')\nPLAYHT_API_USER_ID = os.getenv('PLAYHT_API_USER_ID')\n\n\ndef generate_ultra_track(body, voice=\"Larry\", speed=\"0.85\"):\n    \"\"\"\n    Generate an audio track using Play.ht API with the given text, voice, and speed.\n\n    Args:\n        body (str): The text content to be converted to audio.\n        voice (str, optional): The voice to be used for the audio. Defaults to \"Larry\".\n        speed (str, optional): The speed of the audio playback. Defaults to \"0.85\".\n\n    Returns:\n        str: The transcription ID of the generated audio track.\n    \"\"\"\n    url = \"https://play.ht/api/v1/convert\"\n\n    payload = json.dumps({\n    \"voice\": voice,\n    \"content\": [\n        body,\n    ],\n    \"speed\": speed,\n    \"preset\": \"balanced\"\n    })\n    headers = {\n    'Authorization': PLAYHT_API_KEY,\n    'X-User-ID': PLAYHT_API_USER_ID,\n    'Content-Type': 'application/json'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    print(response.text)\n    return json.loads(response.text)['transcriptionId']", "def generate_ultra_track(body, voice=\"Larry\", speed=\"0.85\"):\n    \"\"\"\n    Generate an audio track using Play.ht API with the given text, voice, and speed.\n\n    Args:\n        body (str): The text content to be converted to audio.\n        voice (str, optional): The voice to be used for the audio. Defaults to \"Larry\".\n        speed (str, optional): The speed of the audio playback. Defaults to \"0.85\".\n\n    Returns:\n        str: The transcription ID of the generated audio track.\n    \"\"\"\n    url = \"https://play.ht/api/v1/convert\"\n\n    payload = json.dumps({\n    \"voice\": voice,\n    \"content\": [\n        body,\n    ],\n    \"speed\": speed,\n    \"preset\": \"balanced\"\n    })\n    headers = {\n    'Authorization': PLAYHT_API_KEY,\n    'X-User-ID': PLAYHT_API_USER_ID,\n    'Content-Type': 'application/json'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    print(response.text)\n    return json.loads(response.text)['transcriptionId']", "\n\ndef download_file(file_url, file_name, directory):\n    \"\"\"\n    Download a file from the specified URL and save it to the given directory with the given file name.\n\n    Args:\n        file_url (str): The URL of the file to download.\n        file_name (str): The name to save the downloaded file as.\n        directory (str): The directory to save the file in.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    file_path = os.path.join(directory, file_name)\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}  \n    r = requests.get(file_url,headers=headers)\n\n\n    url = 'https://example.com/myfile.txt'\n    local_filename, headers = urllib.request.urlretrieve(file_url, file_path)", "\n\ndef ultra_play_ht_get_id(transaction_id: str):\n    \"\"\"\n    Get the audio URL for the given transcription ID using Play.ht API.\n\n    Args:\n        transaction_id (str): The transcription ID to get the audio URL for.\n\n    Returns:\n        str: The audio URL of the generated audio track.\n    \"\"\"\n    url = f\"https://play.ht/api/v1/articleStatus?transcriptionId={transaction_id}&ultra=true\"\n    payload = json.dumps({\n    'transcriptionId': transaction_id,\n\n    })\n\n    headers = {\n    'Authorization': PLAYHT_API_KEY,\n    'X-User-ID': PLAYHT_API_USER_ID,\n    'Content-Type': 'application/json'\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, data=payload)\n    print(response.text)\n    return json.loads(response.text)['audioUrl'][0]", "\n\ndef generate_track_on_machine(body, file_name, directory, voice=\"Larry\", speed=\"0.85\"):\n    \"\"\"\n    Generate an audio track on a local machine with the given text, voice, and speed.\n\n    Args:\n        body (str): The text content to be converted to audio.\n        file_name (str): The name to save the generated audio file as.\n        directory (str): The directory to save the audio file in.\n        voice (str, optional): The voice to be used for the audio. Defaults to \"Larry\".\n        speed (str, optional): The speed of the audio playback. Defaults to \"0.85\".\n\n    Returns:\n        None\n    \"\"\"\n    id = generate_ultra_track(body,voice,speed)\n\n    audio_url = ultra_play_ht_get_id(id)\n    print(audio_url)\n    time.sleep(45)\n    try:\n        download_file(audio_url,file_name,directory)\n    except:\n        print(\"Issue downloading audio\")", "        \n\nif __name__ == \"__main__\":\n    text = 'This was a few weeks ago. I was flying to visit my best friend across the USA FL-CA. I get on and am in the back of the plane in an aisle seat 23C. Upon arrival I see a 20 something (f) sitting in my seat so I point out \"Hey sorry you are probably in the wrong seat\" and show her my ticket. With an eye roll that could have sounded like she was playing Yahtzee she says \"oh I\\'m 24C.\" I look at 24C right behind her and see why she took my seat. There is a 300-400lb (f) sitting in the middle seat. I\\'m a 6\\'1 230lb (m) ...not ideal. After a 15 second stare down I say \"well?\" and she says she is \\'comfortable already\\' and \\'not moving\\' and \\'wants to sleep\\' blah blah. OK I see how it is....real dumb to put someone upset with you in the seat behind you...I proceeded to set a silent timer on my phone that went off every two minutes to remind myself to kick her seat, violently, and then every time the seat belt sign went off I\\'d get up, grabbing the top of the seat to lift myself up pulling her seat back and forth and one time (accidental but worth) pulled her hair she put over the back of the seat. Safe to say she had lots of extra \\'turbulence\\' and got absolutely no sleep. There were MANY death stares and head turns. Each time I would just smile and wave. I knew she wouldn\\'t say anything either because she is not even supposed to be in that seat anyways. Happy travels.'\n    words = text.split()\n    chunk_size = 10\n    num_chunks = math.ceil(len(words) / chunk_size)\n    time.sleep(400)\n\n    for i in range(num_chunks):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        chunk = \" \".join(words[start:end])\n        filename = f\"track_{i}.wav\"\n        generate_track_on_machine(chunk, filename, r\"\\sample\")\n        time.sleep(400)"]}
{"filename": "src/utils/reddit_api.py", "chunked_list": ["import os\nfrom dotenv import load_dotenv\n\nimport praw\n\nfrom src.utils import text_utils\n\n# Load the .env file\nload_dotenv()\n", "load_dotenv()\n\nREDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\nREDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n\ndef fetch_reddit_posts(subreddit_name:str, top_posts_limit:int=3) -> dict:\n    # Get the data from a selected subreddit\n    reddit_subreddit = get_subreddit(subreddit_name)\n\n    # Query the top posts of the given subreddit\n    hot_subreddit_posts = reddit_subreddit.top(\"all\", limit=top_posts_limit)\n    hot_subreddit_posts = [*hot_subreddit_posts]\n\n    posts_dict = [{\"title\": text_utils.clean_up(post.title), \"body\": text_utils.clean_up(post.selftext)} for post in hot_subreddit_posts]\n\n    return posts_dict", "\n\ndef get_subreddit(sub:str):\n    reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,         \n    client_secret=REDDIT_CLIENT_SECRET,      \n    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\")\n\n    # get the subreddit\n    subreddit = reddit.subreddit(sub)\n    return subreddit", "\n\ndef turn_post_into_script(reddit_post,reddit_title):\n    ending = \" . Ever been in a situation like this? Leave it in the comment section. Like and subscribe if you enjoyed this video and want to see more like them. Thank you for watching my video. I hope you enjoyed it, and please have a wonderful day.\"\n    opening = f\"Today's story from reddit - - ... {reddit_title} ... let's get into the story ... \"\n\n    total_script = opening + reddit_post + ending\n    return total_script\n\n\ndef get_sub_comments(comment, allComments, verbose=True):\n    allComments.append(comment)\n    if not hasattr(comment, \"replies\"):\n        replies = comment.comments()\n        if verbose: print(\"fetching (\" + str(len(allComments)) + \" comments fetched total)\")\n\n        else:\n            replies = comment.replies\n        for child in replies:\n            get_sub_comments(child, allComments, verbose=verbose)", "\n\ndef get_sub_comments(comment, allComments, verbose=True):\n    allComments.append(comment)\n    if not hasattr(comment, \"replies\"):\n        replies = comment.comments()\n        if verbose: print(\"fetching (\" + str(len(allComments)) + \" comments fetched total)\")\n\n        else:\n            replies = comment.replies\n        for child in replies:\n            get_sub_comments(child, allComments, verbose=verbose)", "\ndef get_all(r, submissionId, verbose=True):\n    submission = r.submission(submissionId)\n    comments = submission.comments\n    commentsList = []\n    for comment in comments:\n        get_sub_comments(comment, commentsList, verbose=verbose)\n        return commentsList\n\n", "\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": [""]}
{"filename": "src/utils/utils.py", "chunked_list": ["import os\nimport subprocess\nimport re\nfrom typing import List, Dict, Any\n\nfrom datetime import timedelta\nfrom pathlib import Path\nimport argparse\n\nfrom src.video import random_sample_clip", "\nfrom src.video import random_sample_clip\nfrom src.utils import generate_subtitles, text_utils\nfrom src.audio import audio_utils\n\n\ndef combine_audio_and_video(\n        video_path: str, \n        audio_path: str, \n        output_path: str) -> None:\n    \"\"\"\n    Combine the given audio and video files into a single output video file.\n\n    Args:\n        video_path (str): The path to the input video file.\n        audio_path (str): The path to the input audio file.\n        output_path (str): The path to save the combined output video file.\n\n    Returns:\n        None\n    \"\"\"\n\n    ffmpeg_cmd = [\n        \"ffmpeg\",\n        \"-i\", video_path,\n        \"-i\", audio_path,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-map\", \"0:v:0\",\n        \"-map\", \"1:a:0\",\n        output_path,\n        \"-y\"\n    ]\n    \n    subprocess.run(ffmpeg_cmd, check=True)", "\ndef generate_video_with_subtitles(\n        uncensored_audio_file: str, \n        source_video: str, \n        swear_word_list: List[str], \n        video_output_location: str, \n        srtFilename: str = \"\", \n        whisper_model: str = \"medium\") -> None:\n    \"\"\"\n    Generate a censored video with masked audio and subtitles.\n\n    Args:\n        uncensored_audio_file (str): The path to the uncensored audio file.\n        source_video (str): The path to the source video file.\n        swear_word_list (List[str]): A list of swear words to be censored.\n        video_output_location (str): The path to save the generated video.\n        srtFilename (str, optional): The path to save the subtitle file. If not provided, no subtitle file will be saved.\n        whisper_model (str, optional): The Whisper ASR model type. Defaults to \"medium\".\n\n    Returns:\n        None\n    \"\"\"\n    \n    parent_folder = os.path.dirname(video_output_location)\n    srtFilename = os.path.join(parent_folder, srtFilename) if srtFilename else \"\"\n    video_clip = Path(\"sample.mp4\")\n    family_friendly_audio = Path(uncensored_audio_file).with_name(\"uncensored.wav\")\n    \n    #complete script generated from audio file\n\n    raw_transcript = generate_subtitles.transcribe_and_align(\n        uncensored_audio_file,\n        model_type=whisper_model\n        )\n    \n    segments = raw_transcript['segments']\n\n    segments = audio_utils.mask_swear_segments(\n        swear_word_list,\n        segments\n        )\n    \n    \n    if srtFilename:\n        if os.path.exists(srtFilename):\n            os.remove(srtFilename)\n\n        #generate srt file from segments\n        write_srt_file(segments, srtFilename)\n\n    raw_word_segments  = raw_transcript['word_segments']\n\n    #adds mask to existing script\n    masked_script = audio_utils.mask_swear_segments(\n        swear_word_list,\n        raw_word_segments\n        )\n    \n    #find times when the speaker swears\n    swear_segments = text_utils.filter_text_by_list(\n        raw_word_segments,\n        swear_word_list\n        )\n    \n    n_segment = generate_subtitles.segment_text_by_word_length(masked_script,)\n\n    audio_utils.silence_segments(\n        uncensored_audio_file,\n        str(family_friendly_audio),\n        swear_segments\n        )\n    \n    random_sample_clip.create_clip_with_matching_audio(\n        source_video,\n        str(family_friendly_audio),\n        str(video_clip)\n        )\n\n    generate_subtitles.add_subtitles_to_video(\n        str(video_clip),\n        video_output_location,\n        n_segment\n        )\n    \n    #remove temp files\n    os.remove(video_clip)\n    os.remove(family_friendly_audio)", "\n\ndef create_next_dir(input_directory: str) -> str:\n    input_directory = Path(input_directory)\n    is_absolute = input_directory.is_absolute()\n\n    # pattern to match directories\n    dir_pattern = r'story_\\d+'\n\n    current_directory = Path(os.getcwd())\n    if not is_absolute:\n        directory_path = Path.joinpath(current_directory, input_directory)\n        if directory_path.exists():\n            dirs = [d for d in os.listdir(directory_path) if re.match(dir_pattern, d)]\n            # extract the numbers from the directory names and convert them to integers\n            dir_numbers = [int(re.search(r'\\d+', d).group()) for d in dirs]\n            # get the maximum number\n            next_num = max(dir_numbers) + 1\n            # create the new directory name\n            new_dir = f'story_{next_num}'\n            directory_path = Path.joinpath(current_directory, input_directory, Path(new_dir))\n\n        else:\n            directory_path = Path.joinpath(current_directory, input_directory, Path('story_1'))\n\n    os.makedirs(directory_path)\n\n    return directory_path", "\ndef write_srt_file(segments: List[Dict[str, Any]], srt_filename: str) -> None:\n    \"\"\"\n    Write an SRT file from a list of video segments.\n\n    This function writes the given segments into an SRT (SubRip Text) file,\n    which is a common format for subtitles. Each segment includes start and end times\n    and the associated text.\n\n    Args:\n        segments: A list of dictionaries representing video segments, where each\n                  dictionary includes 'start', 'end', and 'text' keys.\n        srt_filename: The filename for the resulting SRT file.\n\n    Returns:\n        None\n    \"\"\"\n\n    for i, segment in enumerate(segments):\n        # Convert start and end times to SRT time format (hh:mm:ss,ms)\n        start_time = str(0)+str(timedelta(seconds=int(segment['start'])))+',000'\n        end_time = str(0)+str(timedelta(seconds=int(segment['end'])))+',000'\n\n        # Get the text associated with this segment\n        text = segment['text']\n\n        # Create the SRT-formatted string for this segment\n        srt_segment = f\"{i+1}\\n{start_time} --> {end_time}\\n{text[1:] if text[0] == ' ' else text}\\n\\n\"\n\n        # Append this segment to the SRT file\n        with open(srt_filename, 'a', encoding='utf-8') as srt_file:\n            srt_file.write(srt_segment)", "\n\nif __name__ == \"__main__\":\n    # swear_word_list = [*audio.audio_utils.get_swear_word_list().keys()]\n    swear_word_list = []\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"uncensored_audio_file\", type=str, help=\"Path to the uncensored audio file\")\n    parser.add_argument(\"source_video\", type=str, help=\"Path to the source video file\")\n    parser.add_argument(\"video_output_location\", type=str, help=\"Path to the output video file\")\n    parser.add_argument(\"--swear_word_list\", type=str, nargs=\"+\", help=\"List of swear words to mask\", default=swear_word_list)\n    args = parser.parse_args()\n\n    generate_video_with_subtitles(args.uncensored_audio_file, args.source_video, args.swear_word_list, args.video_output_location)"]}
{"filename": "src/utils/upload_video.py", "chunked_list": ["from argparse import Namespace\nimport argparse\nimport os\n\nfrom googleapiclient.http import MediaFileUpload\nfrom googleapiclient.errors import HttpError\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\n\nfrom moviepy.editor import VideoFileClip", "\nfrom moviepy.editor import VideoFileClip\n\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\nSCOPES = ['https://www.googleapis.com/auth/youtube.upload']\nAPI_SERVICE_NAME = 'youtube'\nAPI_VERSION = 'v3'\n\ndef get_authenticated_service() -> build:\n    \"\"\"\n    Authenticate the user using OAuth2.\n    \n    Returns:\n        The authenticated service that can be used to interact with the YouTube API.\n    \"\"\"\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_console()\n    return build(API_SERVICE_NAME, API_VERSION, credentials=credentials)", "\ndef get_authenticated_service() -> build:\n    \"\"\"\n    Authenticate the user using OAuth2.\n    \n    Returns:\n        The authenticated service that can be used to interact with the YouTube API.\n    \"\"\"\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_console()\n    return build(API_SERVICE_NAME, API_VERSION, credentials=credentials)", "\n\ndef validate_shorts(options: Namespace) -> None:\n    \"\"\"\n    Validate that the video file meets the requirements for YouTube Shorts.\n\n    Parameters:\n        options (Namespace): Command line arguments.\n    \"\"\"\n    # Check the video format and duration\n    video = VideoFileClip(options.file)\n    width, height = video.size\n    duration = video.duration\n\n    # Check if video is vertical (aspect ratio of 9:16)\n    if width / height != 9 / 16:\n        raise ValueError(\"Video is not in the correct aspect ratio for YouTube Shorts. It must be a vertical video (aspect ratio 9:16).\")\n\n    # Check if video is no longer than 60 seconds\n    if duration > 60:\n        raise ValueError(\"Video is too long for YouTube Shorts. It must be 60 seconds or less.\")", "\n\ndef initialize_upload(youtube: build, options: Namespace) -> None:\n    \"\"\"\n    Initialize the video upload to YouTube.\n    \n    Parameters:\n        youtube (build): The authenticated YouTube API service.\n        options (Namespace): Command line arguments.\n    \"\"\"\n    tags = None\n    if options.keywords:\n        tags = options.keywords.split(',')\n\n    # Check if the video is a YouTube short and append \"#Shorts\" to the title\n    title = options.title\n    if options.youtubeShort:\n        title += \" #Shorts\"\n\n    body=dict(\n        snippet=dict(\n            title=title,\n            description=options.description,\n            tags=tags,\n            categoryId=options.category\n        ),\n        status=dict(\n            privacyStatus=options.privacyStatus,\n            madeForKids=options.madeForKids  \n        )\n    )\n\n    insert_request = youtube.videos().insert(\n        part=\",\".join(body.keys()),\n        body=body,\n        media_body=MediaFileUpload(options.file, chunksize=-1, resumable=True)\n    )\n    \n    resumable_upload(youtube, insert_request, options)", "\ndef resumable_upload(youtube: build, insert_request: object, options: Namespace) -> None:\n    \"\"\"\n    Upload the video file to YouTube and track its progress.\n    \n    Parameters:\n        youtube (build): The authenticated YouTube API service.\n        insert_request (object): The insert request object.\n        options (Namespace): Command line arguments.\n    \"\"\"\n    response = None\n    while response is None:\n        status, response = insert_request.next_chunk()\n        if 'id' in response:\n            print(f\"Video id {response['id']} was successfully uploaded.\")\n            set_thumbnail(youtube, options, response['id'])", "\ndef set_thumbnail(youtube: build, options: Namespace, video_id: str) -> None:\n    \"\"\"\n    Set the thumbnail of the uploaded video.\n    \n    Parameters:\n        youtube (build): The authenticated YouTube API service.\n        options (Namespace): Command line arguments.\n        video_id (str): The ID of the uploaded video.\n    \"\"\"\n    youtube.thumbnails().set(\n        videoId=video_id,\n        media_body=MediaFileUpload(options.thumbnail)\n    ).execute()", "\ndef main():\n    \"\"\"\n    Parse command line arguments and upload a video to YouTube.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--file', required=True, help='Video file to upload')\n    parser.add_argument('--title', help='Video title', default='Test Title')\n    parser.add_argument('--description', help='Video description', default='Test Description')\n    parser.add_argument('--category', default='27', help='Numeric video category. See https://developers.google.com/youtube/v3/docs/videoCategories/list')\n    parser.add_argument('--keywords', help='Video keywords, comma separated', default='')\n    parser.add_argument('--privacyStatus', choices=['public', 'private', 'unlisted'], default='private', help='Video privacy status.')\n    parser.add_argument('--thumbnail', help='Thumbnail image file', default='')\n    parser.add_argument('--madeForKids', type=bool, default=False, help='Made for kids field.')\n    parser.add_argument('--youtubeShort', type=bool, default=False, help='Is this a YouTube short, if so it must have a aspect ratio of 9:16?')  # Added 'youtubeShort' argument\n    args = parser.parse_args()\n\n    if args.youtubeShort:\n        validate_shorts(args)\n\n    youtube = get_authenticated_service()\n    try:\n        initialize_upload(youtube, args)\n    except HttpError as e:\n        print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')", "\nif __name__ == '__main__':\n    main()"]}
{"filename": "src/images/__init__.py", "chunked_list": [""]}
{"filename": "src/images/thumbnail.py", "chunked_list": ["import os\nimport requests\nfrom typing import List, Dict, Optional\n\nfrom PIL import Image, ImageDraw, ImageFont\nfrom rembg import remove\n\n\ndef create_thumbnail(segmented_image:str,text:List[Dict],output_image=\"thumbnail.png\"):\n    # load the background image\n    background_dimensions = (1280, 720)\n    background = Image.new(\"RGB\", background_dimensions, (0, 0, 0))\n\n    # load the image to be placed on the right\n    image = Image.open(segmented_image)\n    \n    # resize the image to fit on the background\n    max_width = 6000\n    max_height = background.height\n\n    width, height = image.size\n    if width > max_width or height > max_height:\n        ratio = min(max_width/width, max_height/height)\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        image = image.resize((new_width, new_height))\n\n\n    padding = 0 \n\n    image_y = background.height - image.height - padding\n    x_edge = background.width - image.width\n\n    # place the image on the right side of the background\n    background.paste(image, (x_edge, image_y))\n\n    # create a draw object\n    draw = ImageDraw.Draw(background)\n\n    # iterate over the list of text objects and draw them on the left side of the background\n    for item in text:\n        # get the text and formatting information\n        text = item[\"text\"]\n        color = item[\"color\"]\n        spacingTop = item[\"spacingTop\"]\n        size = item[\"size\"]\n        font_location = item['font']\n\n        # set the font and color\n        draw.text((30, spacingTop), text, font=ImageFont.truetype(font_location, size), fill=color,)\n\n    # save the resulting image\n    background.save(output_image)", "def create_thumbnail(segmented_image:str,text:List[Dict],output_image=\"thumbnail.png\"):\n    # load the background image\n    background_dimensions = (1280, 720)\n    background = Image.new(\"RGB\", background_dimensions, (0, 0, 0))\n\n    # load the image to be placed on the right\n    image = Image.open(segmented_image)\n    \n    # resize the image to fit on the background\n    max_width = 6000\n    max_height = background.height\n\n    width, height = image.size\n    if width > max_width or height > max_height:\n        ratio = min(max_width/width, max_height/height)\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        image = image.resize((new_width, new_height))\n\n\n    padding = 0 \n\n    image_y = background.height - image.height - padding\n    x_edge = background.width - image.width\n\n    # place the image on the right side of the background\n    background.paste(image, (x_edge, image_y))\n\n    # create a draw object\n    draw = ImageDraw.Draw(background)\n\n    # iterate over the list of text objects and draw them on the left side of the background\n    for item in text:\n        # get the text and formatting information\n        text = item[\"text\"]\n        color = item[\"color\"]\n        spacingTop = item[\"spacingTop\"]\n        size = item[\"size\"]\n        font_location = item['font']\n\n        # set the font and color\n        draw.text((30, spacingTop), text, font=ImageFont.truetype(font_location, size), fill=color,)\n\n    # save the resulting image\n    background.save(output_image)", "\ndef create_thumbnail(segmented_image: str, \n                     text_items: List[Dict[str, object]], \n                     output_image: Optional[str] = None, \n                     text_x_pos: int = 0, \n                     default_font_location: str = \"arial.ttf\",\n                     y_spacing: int = 1) -> Image.Image:\n    \"\"\"\n    Create a thumbnail image from a background, segmented image, and text.\n\n    This function generates a thumbnail image by combining a provided segmented image with text\n    drawn on the left side. The text_items parameter is a list of dictionaries, each containing\n    the text to be drawn and its formatting options. The resulting image can be saved to disk\n    or returned as a PIL.Image object.\n\n    Args:\n        segmented_image (str): Path to the segmented image file.\n        text_items (List[Dict[str, object]]): A list of dictionaries containing the text and\n            its formatting options. Each dictionary should have the following keys:\n            - \"text\": The text string to be drawn.\n            - \"color\": The text color in RGB format.\n            - \"size\": The font size.\n            - \"font\" (optional): Path to the font file. Defaults to arial.ttf.\n            - \"y_spacing\" (optional): Vertical spacing between text lines. Defaults to 1.\n        output_image (Optional[str], optional): Path to save the output image. If not provided,\n            the function will return the generated image as a PIL.Image object.\n        text_x_pos (int, optional): The x-coordinate of the starting position for the text.\n            Defaults to 0.\n        default_font_location (str, optional): Path to the default font file. Defaults to \"arial.ttf\".\n        y_spacing (int, optional): Default vertical spacing between text lines. Defaults to 1.\n\n    Returns:\n        Image.Image: The generated thumbnail image. Returned only if output_image is not provided.\n    \"\"\"\n\n    # Load the background image\n    background_dimensions = (1280, 720)\n    background_color = (0, 0, 0)\n    background = Image.new(\"RGB\", background_dimensions, background_color)\n\n    # Load the image to be placed on the right\n    image = Image.open(segmented_image)\n\n    # Resize the image to fit on the background\n    max_width = 6000  # This value seems too high for a thumbnail\n    max_height = background.height\n\n    width, height = image.size\n    if width > max_width or height > max_height:\n        ratio = min(max_width/width, max_height/height)\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        image = image.resize((new_width, new_height))\n\n    # Calculate the y-coordinate of the image\n    image_y = background.height - image.height\n\n    # Place the image on the right side of the background\n    background.paste(image, (background.width - image.width, image_y))\n\n    # Create a draw object\n    draw = ImageDraw.Draw(background)\n\n    # Set default text position\n    text_y_pos = 0\n\n    # Iterate over the list of text objects and draw them on the left side of the background\n    for item in text:\n        # Get the text and formatting information\n        text = item[\"text\"]\n        color = item[\"color\"]\n        y_spacing = item.get(\"y_spacing\", y_spacing)\n        size = item[\"size\"]\n        font_location = item.get(\"font\", default_font_location)\n\n        # Set the font and color\n        font = ImageFont.truetype(font_location, size)\n        text_width, text_height = draw.textsize(text, font=font)\n        draw.text((text_x_pos, text_y_pos + y_spacing), text, font=font, fill=color)\n\n        # Update the text position\n        text_y_pos += text_height + y_spacing\n\n    # Save the resulting image\n    if output_image:\n        background.save(output_image)\n    else:\n        return background", "\ndef segment_image(input_path:str,output_path:str):\n    input = Image.open(input_path)\n    output = remove(input)\n    output.save(output_path)\n\n\ndef crop_png(input_data: str, output_data: str) -> None:\n    \"\"\"\n    Crop a PNG image to the non-transparent area and save it to a file.\n\n    Args:\n        input_data (str): The path to the input image file.\n        output_data (str): The path to save the cropped image file.\n\n    Returns:\n        None\n    \"\"\"\n    # Open the image\n    image = Image.open(input_data)\n\n    # Get the size of the image\n    width, height = image.size\n\n    # Get the alpha channel of the image\n    alpha = image.split()[-1]\n\n    # Find the bounding box of the non-transparent part of the image\n    bbox = alpha.getbbox()\n\n    # Crop the image to the bounding box\n    cropped_image = image.crop(bbox)\n\n    # Save the cropped image\n    cropped_image.save(output_data)", "\n\ndef crop_transparent(image_path: str, output_path: str):\n    \"\"\"Crop a transparent image and save to a file.\n\n    Args:\n        image_path (str): The path to the input image.\n        output_path (str): The path to save the cropped image.\n\n    Returns:\n        None\n    \"\"\"\n    # Open the image\n    image = Image.open(image_path)\n\n    # Get the size of the image\n    width, height = image.size\n\n    # Find the dimensions of the non-transparent part of the image\n    left, top, right, bottom = width, height, 0, 0\n    for x in range(width):\n        for y in range(height):\n            alpha = image.getpixel((x,y))[3]\n            if alpha != 0:\n                left = min(left, x)\n                top = min(top, y)\n                right = max(right, x)\n                bottom = max(bottom, y)\n\n    # Crop the image to the non-transparent part\n    image = image.crop((left, top, right, bottom))\n\n    # Save the cropped image\n    image.save(output_path)", "\n\n\ndef download_image(url: str, directory: str = \".\") -> str:\n    \"\"\"\n    Download an image from a URL and save it to a directory.\n\n    Args:\n        url (str): The URL of the image to download.\n        directory (str): The directory to save the image in. Defaults to the current directory.\n\n    Returns:\n        str: The file path of the downloaded image if successful, else None.\n    \"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        filename = os.path.basename(url)\n        filepath = os.path.join(directory, filename)\n        with open(filepath, \"wb\") as f:\n            f.write(response.content)\n        return filepath\n    else:\n        return None", "\n    \ndef convert_to_png(sample:str) -> str:\n    \"\"\"\n    Converts the input image file to PNG format if it is a JPEG file.\n\n    Args:\n        sample (str): The path to the input image file.\n\n    Returns:\n        str: The path to the output image file. If the input image file is already in PNG format, returns the original file path.\n    \"\"\"\n    img = Image.open(sample)\n    if img.format == \"JPEG\":\n        img = img.convert(\"RGBA\")\n        os.remove(sample)\n        # get the base file name and old extension\n        base_name, old_extension = os.path.splitext(sample)\n\n        # replace the old extension with \".png\"\n        new_file_path = base_name + \".png\"\n        img.save(new_file_path)\n    else:\n        new_file_path = sample\n    return new_file_path", "\n\nif __name__ == '__main__':\n\n    data = \"https://th.bing.com/th/id/R.3d79e075f692870894fc41d6304eb4f2?rik=GfJgXZ5%2b5MJCVQ&riu=http%3a%2f%2fwww.pixelstalk.net%2fwp-content%2fuploads%2f2016%2f05%2fReally-Cool-Image.jpg\"\n    data = \"https://www.lockheedmartin.com/content/dam/lockheed-martin/eo/photo/news/features/2021/ai/ai-small-1920.jpg.pc-adaptive.768.medium.jpeg\"\n    data = \"https://images.girlslife.com/posts/009/9250/shutterstock_406983616.jpg\"\n    data = \"https://www.aaa.com/AAA/common/AAR/images/deice1.png\"\n\n        \n    sample = download_image(data)\n    ff = convert_to_png(sample)", "    # segment_image(ff,ff)\n    # crop_png(ff,ff)\n\n\n\n    # default_font_location = r\"C:\\Users\\isaya\\Downloads\\Press_Start_2P\\PressStart2P-Regular.ttf\"\n    # default_font_location = \"arial.ttf\"\n    # text = [\n    #     {\"text\": \"r/Petty revenge\", \"color\" : (255,255,255), \"spacingTop\": 0, \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"\", \"color\" : (255,0,0), \"spacingTop\": 90, \"size\" : 90, \"font\": default_font_location},", "    #     {\"text\": \"r/Petty revenge\", \"color\" : (255,255,255), \"spacingTop\": 0, \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"\", \"color\" : (255,0,0), \"spacingTop\": 90, \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"taken? I'll go to \", \"color\" : (255,255,255), \"spacingTop\": 180, \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"first class\", \"color\" : (255, 215, 0), \"spacingTop\": 270, \"size\" : 90, \"font\": default_font_location}\n    # ]\n\n    # input_data = r\"c:\\Users\\isaya\\code_examples\\Machine_Learning\\img_manipulation\\japanese_robot.jpg\"\n    # output_data = r\"c:\\Users\\isaya\\code_examples\\Machine_Learning\\img_manipulation\\_robot.png\"\n    # segment_image(input_data,output_data)\n    # crop_png(output_data,output_data)", "    # segment_image(input_data,output_data)\n    # crop_png(output_data,output_data)\n\n    # default_font_location = \"arial.ttf\"\n    # ff = r\"c:\\Users\\isaya\\code_examples\\Machine_Learning\\img_manipulation\\toy_boat_0.jpg\"\n    # text = [\n    #     {\"text\": \"r/Petty revenge\", \"color\" : (255,255,255), \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"Allow my seat to be\", \"color\" : (255,0,0), \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"taken? I'll go to \", \"color\" : (255,255,255), \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"first class\", \"color\" : (255,255,255), \"size\" : 90, \"font\": default_font_location},", "    #     {\"text\": \"taken? I'll go to \", \"color\" : (255,255,255), \"size\" : 90, \"font\": default_font_location},\n    #     {\"text\": \"first class\", \"color\" : (255,255,255), \"size\" : 90, \"font\": default_font_location},\n    # ]\n    # create_thumbnail(ff, text, \"thumbnail.png\",text_x_pos=20)\n\n\n    # create_thumbnail(ff,text,\"fuck.png\")\n    # create_thumbnail(output_data,text,\"fuck.png\")"]}
{"filename": "src/youtube/__init__.py", "chunked_list": [""]}
