{"filename": "testRes.py", "chunked_list": ["import os\nimport time\n#To import Grpc APIs\nfrom client.client import rpc_adjust_res\n\n#Node-Ip relationship\nNodeIpMapper={\n    \"cpu-03\":\"10.2.64.3:50052\",\n    \"cpu-04\":\"10.2.64.4:50052\",\n    \"cpu-07\":\"10.2.64.7:50052\",", "    \"cpu-04\":\"10.2.64.4:50052\",\n    \"cpu-07\":\"10.2.64.7:50052\",\n    \"cpu-08\":\"10.2.64.8:50052\",\n}\n\n#pod-node relationship\npod_node_mapper={'check': 'cpu-04', 'consul': 'cpu-06', 'entering-ms': 'cpu-03', 'frontend-recommend': 'cpu-07', 'frontend-reserve': 'cpu-08', 'frontend-search': 'cpu-04', 'geo': 'cpu-04', 'jaeger': 'cpu-06', 'memcached-check': 'cpu-04', 'memcached-profile': 'cpu-07', 'memcached-rate': 'cpu-04', 'memcached-reservation': 'cpu-08', 'mongodb-check': 'cpu-04', 'mongodb-geo': 'cpu-04', 'mongodb-history': 'cpu-07', 'mongodb-profile': 'cpu-07', 'mongodb-rate': 'cpu-04', 'mongodb-recommendation': 'cpu-07', 'mongodb-reservation': 'cpu-08', 'mongodb-user': 'cpu-08', 'profile': 'cpu-07', 'rank-category': 'cpu-07', 'rank-overall': 'cpu-07', 'rate': 'cpu-04', 'recommendation': 'cpu-07', 'reservation': 'cpu-08', 'search': 'cpu-04', 'user': 'cpu-08'}\n\n#pod-uid relationship\npod_uids={", "#pod-uid relationship\npod_uids={\n    \"entering-ms\":['c95afad8-75ea-4cf6-b0a4-07982fad4cf7\\n'],\n    \"frontend-search\":['07b00721-06e6-47a6-abae-c6b6cb6512e0\\n'],\n    \"frontend-recommend\":['249ce56f-f436-411d-841a-1430135f8cd9\\n'],\n    \"frontend-reserve\":['1a545e65-91a4-4f99-96be-993366e7924d\\n'],\n    \"search\": ['41be1c59-1531-478f-8e4b-74b826e00c20\\n'],\n    \"check\": ['c2c39aef-516c-469d-b31c-c0e9193cea39\\n'],\n    \"recommendation\": ['75b476d8-759a-42ec-b02d-9f79ec0c3e99\\n'],\n    \"profile\": ['2fdee85b-2f09-4c96-922b-2f22800c1f53\\n'],", "    \"recommendation\": ['75b476d8-759a-42ec-b02d-9f79ec0c3e99\\n'],\n    \"profile\": ['2fdee85b-2f09-4c96-922b-2f22800c1f53\\n'],\n    \"user\": ['153e0a9a-8cd5-439d-96e2-a21db4985b44\\n'],\n    \"reservation\": ['12ae4870-ceca-46c5-a8de-74992794251f\\n'],\n    \"geo\": ['80a37e50-bfbb-4a13-84e7-79a3a3e61879\\n'],\n    \"rate\":['1477e6df-edce-435f-8985-5a9d24359693\\n'],\n    \"memcached-check\":['bf8c4d95-e879-4116-82e4-75f0aed3469f\\n'],\n    \"rank-category\": ['4a0e4a06-74bd-485b-9108-cda1b0bb2cdb\\n'],\n    \"rank-overall\": ['604e15ce-e6b7-45d6-8347-9f51954db5cf\\n'],\n    \"memcached-profile\": ['5cb477da-db24-4cee-97eb-c0668a6ef455\\n'],", "    \"rank-overall\": ['604e15ce-e6b7-45d6-8347-9f51954db5cf\\n'],\n    \"memcached-profile\": ['5cb477da-db24-4cee-97eb-c0668a6ef455\\n'],\n    \"memcached-reservation\": ['b295189c-b513-4ad5-99b6-ff8e6dd4ff09\\n'],\n    \"mongodb-reservation\":['67273aa4-7f56-45fc-a721-04ac453833bb\\n'],\n    \"memcached-rate\":['a2630e43-dd25-47f6-b14c-8ac73ba386b1\\n']\n}\n\n#get pod-node relationship\ndef pod_node():\n    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n    pod_node_mapper={}\n    for i in range(len(pods)):\n        pod=pods[i].replace(\"\\n\",\"\")\n        node=nodes[i].replace(\"\\n\",\"\")\n        if(pod==\"NAME\"):\n            continue\n        temp=pod.split(\"-\")\n        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n        pod_name=pod.replace(extra,\"\")\n        pod_node_mapper[pod_name]=node\n    print(pod_node_mapper)", "def pod_node():\n    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n    pod_node_mapper={}\n    for i in range(len(pods)):\n        pod=pods[i].replace(\"\\n\",\"\")\n        node=nodes[i].replace(\"\\n\",\"\")\n        if(pod==\"NAME\"):\n            continue\n        temp=pod.split(\"-\")\n        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n        pod_name=pod.replace(extra,\"\")\n        pod_node_mapper[pod_name]=node\n    print(pod_node_mapper)", "\n#get pod-uid relationship\ndef get_pod_uids():\n    for svc in pod_uids.keys():\n        service=str(svc)\n        os.system(\"./get_pod_uid.sh \"+service+\" >/dev/null 2>&1\")\n        uids=os.popen(\"cat uid.txt\").readlines()\n        print(svc, uids)\n\n#Call Grpc APIs to adjust resources\ndef set_cpu(service,cpu,replicas_number=1):\n    node=pod_node_mapper[service]\n    ip_str=NodeIpMapper[node]\n    uids=pod_uids[service]\n    res=rpc_adjust_res(ip_str,uids,cpu)", "\n#Call Grpc APIs to adjust resources\ndef set_cpu(service,cpu,replicas_number=1):\n    node=pod_node_mapper[service]\n    ip_str=NodeIpMapper[node]\n    uids=pod_uids[service]\n    res=rpc_adjust_res(ip_str,uids,cpu)\n\n#Adjust the initial state using profiling on own cluster\ndef set_QoS_violations():\n    set_cpu(\"entering-ms\",24,1)\n    set_cpu(\"frontend-search\",9.1,1)\n    set_cpu(\"frontend-recommend\",10.5,1)\n    set_cpu(\"frontend-reserve\",5.1,1)\n    set_cpu(\"search\",6.5,1)\n    set_cpu(\"check\",10,1)\n    set_cpu(\"recommendation\",6,1)\n    set_cpu(\"profile\",6,1)\n    set_cpu(\"user\",1.6,1)\n    set_cpu(\"reservation\",3.5,1)\n    set_cpu(\"geo\",2.5,1)\n    set_cpu(\"rate\",4.2,1)\n    set_cpu(\"memcached-check\",4,1)\n    set_cpu(\"rank-category\",2,1)\n    set_cpu(\"rank-overall\",3,1)\n    set_cpu(\"memcached-profile\",1.5,1)\n    set_cpu(\"memcached-rate\",1.2,1)\n    set_cpu(\"memcached-reservation\",1.5,1)", "#Adjust the initial state using profiling on own cluster\ndef set_QoS_violations():\n    set_cpu(\"entering-ms\",24,1)\n    set_cpu(\"frontend-search\",9.1,1)\n    set_cpu(\"frontend-recommend\",10.5,1)\n    set_cpu(\"frontend-reserve\",5.1,1)\n    set_cpu(\"search\",6.5,1)\n    set_cpu(\"check\",10,1)\n    set_cpu(\"recommendation\",6,1)\n    set_cpu(\"profile\",6,1)\n    set_cpu(\"user\",1.6,1)\n    set_cpu(\"reservation\",3.5,1)\n    set_cpu(\"geo\",2.5,1)\n    set_cpu(\"rate\",4.2,1)\n    set_cpu(\"memcached-check\",4,1)\n    set_cpu(\"rank-category\",2,1)\n    set_cpu(\"rank-overall\",3,1)\n    set_cpu(\"memcached-profile\",1.5,1)\n    set_cpu(\"memcached-rate\",1.2,1)\n    set_cpu(\"memcached-reservation\",1.5,1)", "\n#Adjust the enough resources for the  on own cluster\ndef set_enough():\n    set_cpu(\"entering-ms\",48,1)\n    set_cpu(\"frontend-search\",19,1)\n    set_cpu(\"frontend-recommend\",21,1)\n    set_cpu(\"frontend-reserve\",11,1)\n    set_cpu(\"search\",13.2,1)\n    set_cpu(\"check\",20,1)\n    set_cpu(\"recommendation\",13.2,1)\n    set_cpu(\"profile\",13.5,1)\n    set_cpu(\"user\",3.5,1)\n    set_cpu(\"reservation\",9,1)\n    set_cpu(\"geo\",5,1)\n    set_cpu(\"rate\",8.8,1)\n    set_cpu(\"memcached-check\",6.6,1)\n    set_cpu(\"rank-category\",4,1)\n    set_cpu(\"rank-overall\",6,1)\n    set_cpu(\"memcached-profile\",3,1)\n    set_cpu(\"memcached-rate\",2.4,1)\n    set_cpu(\"memcached-reservation\",2,1)", "\n\nif __name__ == \"__main__\":\n    pod_node()\n    get_pod_uids()\n    set_enough()\n    set_QoS_violations()\n    print(time.time())"]}
{"filename": "Agent.py", "chunked_list": ["import os\nimport scipy\nimport pandas\nimport numpy as np\nimport time\nimport sys\nfrom MSDAG import *\nfrom LoadMonitor.NetTrafficMonitor import initialize,runOneTime\nfrom LoadUpdator.Update import Update_Traffic_Tree,Update_Traffic_Graph\nfrom Predict.Predictor import predict_net_to_load, predict_load_to_CPU", "from LoadUpdator.Update import Update_Traffic_Tree,Update_Traffic_Graph\nfrom Predict.Predictor import predict_net_to_load, predict_load_to_CPU\nfrom QueryDrainer.AdjustRes import run_set_cpu\nfrom QueryDrainer.Compensator import Compensate\nfrom testRes import set_QoS_violations, set_enough\nfrom multiprocessing import Process\n\nif __name__ == \"__main__\":\n    \n    # set enough resources\n    set_enough()\n    #Traffic monitoritor interval & overhead\n    time_interval=1.05\n    #Initial DAG\n    root=Node(0,\"test\",0,0,0,0,[],[],0,0,0,0,0,0,0)\n    ms_dag=MSDAG(root,0, np.zeros((1,1)),np.zeros((1,1)),[],{})\n    ms_dag.buildDAG()\n    ms_dag.postOrder(ms_dag.root)\n    ms_dag.ms_interface()\n    ms_dag.display()\n    # Run 5 seconds normally\n    print(\"begin_time=\",time.time())\n    time.sleep(5)\n    # Adjust to the resources with initial resouces\n    time_monitor_start=time.time()\n    print(\"QoS_violaiton_time=\",time_monitor_start)\n    set_QoS_violations()\n    #Initial monitor\n    initialize(ms_dag)\n    # Monitor 1 seconds\n    time.sleep(1)\n    # 1.Get network traffic\n    sampling_duration=runOneTime(ms_dag)\n    # 2.Get monitor load\n    predict_net_to_load(ms_dag)\n    ms_dag.root.realLoad=ms_dag.root.monitorLoad\n    # 3.Update real load\n    Update_Traffic_Graph(ms_dag)\n    # 4.Queued query drain\n    Compensate(ms_dag,time_interval)\n    # 5.Get CPU need\n    predict_load_to_CPU(ms_dag)\n    # 6.CPU allocation\n    run_set_cpu(ms_dag)\n    time_set_done=time.time()\n    print(\"Adjustment done=\",time_set_done)\n    # Run extra time, then set enough\n    time.sleep(3-(time.time()-time_monitor_start))\n    set_enough()"]}
{"filename": "MSDAG.py", "chunked_list": ["import os\nimport numpy as np\nfrom queue import Queue\n\n#Node refers to each MS\nclass Node:\n    def __init__(self,index,name,in_traffic,out_traffic,upper_traffic,back_traffic,children,pressure_children,realLoad,monitorLoad,handleLoad,CPU_Allocated,CPU_Need,overLoad,SLAtime):\n        self.index=index\n        self.name=name\n        self.in_traffic=in_traffic\n        self.out_traffic=out_traffic\n        self.upper_traffic=upper_traffic\n        self.back_traffic=back_traffic\n        self.children=children\n        self.pressure_children=pressure_children\n        self.realLoad=realLoad\n        self.monitorLoad=monitorLoad\n        self.handleLoad=handleLoad\n        self.CPU_Allocated=CPU_Allocated\n        self.CPU_Need=CPU_Need\n        self.overLoad=overLoad\n        self.SLAtime=SLAtime\n\n    def display(self):\n        print(self.index,self.name,self.in_traffic,self.out_traffic,self.upper_traffic,self.back_traffic,self.children,self.pressure_children,self.realLoad,self.monitorLoad,self.handleLoad,self.CPU_Allocated,self.CPU_Need,self.overLoad,self.SLAtime)", "\n#DAG for all MSs\nclass MSDAG:\n    def __init__(self, root, MS_number, Traffic_matrix, Load_matrix, Access_Record, ms_interface_mapper, PodNodeMapper={},NodeIpMapper={}):\n        self.root=root\n        self.MS_number=MS_number\n        self.Traffic_matrix=Traffic_matrix\n        self.Load_matrix=Load_matrix\n        self.Access_Record=Access_Record\n        self.ms_interface_mapper=ms_interface_mapper\n        self.PodNodeMapper=PodNodeMapper\n        self.NodeIpMapper=NodeIpMapper\n\n    #Initial microservices\n    def buildDAG(self):\n        checkmmc=Node(12,\"memcached-check\",0,0,0,0,[],[],0,0,1000,0,0,0,0)\n        check=Node(5,\"check\",0,0,0,0,[checkmmc],[checkmmc],0,0,1000,0,0,0,0)\n        ratemmc=Node(18,\"memcached-rate\",0,0,0,0,[],[check],0,0,500,0,0,0,0)\n        geo=Node(10,\"geo\",0,0,0,0,[],[check],0,0,500,0,0,0,0)\n        rate=Node(11,\"rate\",0,0,0,0,[ratemmc],[ratemmc],0,0,500,0,0,0,0)\n        search=Node(4,\"search\",0,0,0,0,[geo,rate],[rate,geo],0,0,1000,0,0,0,0)\n        frontendSearch=Node(1,\"frontend-search\",0,0,0,0,[search,check],[search],0,0,1000,0,0,0,0)\n        profilemmc=Node(15,\"memcached-profile\",0,0,0,0,[],[],0,0,1000,0,0,0,0)\n        profile=Node(7,\"profile\",0,0,0,0,[profilemmc],[profilemmc],0,0,1000,0,0,0,0)\n        rank_overall=Node(14,\"rank-overall\",0,0,0,0,[],[profile],0,0,500,0,0,0,0)\n        rank_category=Node(13,\"rank-category\",0,0,0,0,[],[profile],0,0,500,0,0,0,0)\n        recommend=Node(6,\"recommendation\",0,0,0,0,[rank_category,rank_overall],[rank_category,rank_overall],0,0,1000,0,0,0,0)\n        frontendRecommend=Node(2,\"frontend-recommend\",0,0,0,0,[recommend,profile],[recommend],0,0,1000,0,0,0,0)\n        reservemongodb=Node(17,\"mongodb-reservation\",0,0,0,0,[],[],0,0,500,0,0,0,0)\n        reservemmc=Node(16,\"memcached-reservation\",0,0,0,0,[],[],0,0,500,0,0,0,0)\n        reserve=Node(9,\"reservation\",0,0,0,0,[reservemmc,reservemongodb],[reservemmc,reservemongodb],0,0,500,0,0,0,0)\n        user=Node(8,\"user\",0,0,0,0,[],[reserve],0,0,500,0,0,0,0)\n        frontendReserve=Node(3,\"frontend-reserve\",0,0,0,0,[user,reserve],[user],0,0,500,0,0,0,0)\n        EnteringMS=Node(0,\"entering-ms\",0,0,0,0,[frontendSearch,frontendRecommend,frontendReserve],[frontendSearch,frontendRecommend,frontendReserve],3675.0,3675.0,2500,0,0,0,0)\n        self.root=EnteringMS\n        #Traffic matrix\n        self.MS_number=19\n        self.Traffic_matrix=np.zeros((self.MS_number,self.MS_number))\n        #Load matrix\n        self.Load_matrix=np.zeros((self.MS_number,self.MS_number))\n        #Node-IP relationship\n        self.NodeIpMapper={\n            \"cpu-03\":\"10.2.64.3:50052\",\n            \"cpu-04\":\"10.2.64.4:50052\",\n            \"cpu-07\":\"10.2.64.7:50052\",\n            \"cpu-08\":\"10.2.64.8:50052\",\n        }\n\n    #Post order microservices\n    def postOrder(self,root):\n        if not root:\n            return\n        for target in root.children:\n            self.postOrder(target)\n        self.Access_Record.append(root)\n\n    #pod-interface relationship\n    def ms_interface(self):\n        pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n        nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n        for i in range(len(pods)):\n            pod=pods[i].replace(\"\\n\",\"\")\n            node=nodes[i].replace(\"\\n\",\"\")\n            if(pod==\"NAME\"):\n                continue\n            id=os.popen(\"kubectl exec -i \"+pod+\" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n            print(pod,id)\n            idtarget=\"^'\"+id+\":\"+\" \"+\"'\"\n            if(node==\"cpu-06\"):\n                temp=os.popen(\"ip link show | grep \"+idtarget+\" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n            else:\n                temp=os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget+\"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n            index=temp.find('@')\n            iplink=temp[:index]\n            temp=pod.split(\"-\")\n            extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n            pod_name=pod.replace(extra,\"\")\n            self.ms_interface_mapper[iplink]=pod_name\n            self.PodNodeMapper[pod_name]=node\n    \n    #display all\n    def display(self):\n        print(\"==========DAG Root:==========\")\n        self.root.display()\n        print(\"==========Microservice Number:==========\")\n        print(self.MS_number)\n        print(\"==========Load_matrix:==========\")\n        for i in range(len(self.Load_matrix)):\n            print(i,list(self.Load_matrix[i]))\n        print(\"==========All Nodes(post order):==========\")\n        for target in self.Access_Record:\n            target.display()\n        print(\"==========Pod<->Interfaces==========\")\n        for key in self.ms_interface_mapper:\n            print(key,self.ms_interface_mapper[key])\n        print(\"==========Pod<->Node:==========\")\n        for key in self.PodNodeMapper:\n            print(key,self.PodNodeMapper[key])", "\nif __name__ == \"__main__\":\n    root=Node(0,\"test\",0,0,0,0,[],[],0,0,0,0,0,0,0)\n    ms_dag=MSDAG(root,0, np.zeros((1,1)),np.zeros((1,1)),[],{})\n    ms_dag.buildDAG()\n    ms_dag.postOrder(ms_dag.root)\n    ms_dag.ms_interface()\n    ms_dag.display()"]}
{"filename": "getLatencyRes.py", "chunked_list": ["import os\nimport numpy as np\nimport time\nimport requests\nimport json\nimport pandas\nimport datetime\n\ndef get_time_stamp16(inputTime):\n    date_stamp = str(int(time.mktime(inputTime.timetuple())))\n    data_microsecond = str(\"%06d\"%inputTime.microsecond)\n    date_stamp = date_stamp+data_microsecond\n    return int(date_stamp)", "def get_time_stamp16(inputTime):\n    date_stamp = str(int(time.mktime(inputTime.timetuple())))\n    data_microsecond = str(\"%06d\"%inputTime.microsecond)\n    date_stamp = date_stamp+data_microsecond\n    return int(date_stamp)\n\ndef get_latency(startTs,endTs,period):\n    data=requests.get(url='http://127.0.0.1:30910/api/traces?service=enteringMS&start='+str(startTs)+'&end='+str(endTs)+'&prettyPrint=true&limit=6000000').json()\n    counter=0\n    latency = []\n    errorCount=0\n    for element in data['data']:\n        counter+=1\n        spanCount=0\n        minTs=0.0\n        maxTs=0.0\n        flag=1\n        for a in element['spans']:\n            for b in a['tags']:\n                if(b['key']==\"error\" and b['value']==True):\n                    flag=0\n            if(flag==0):\n                errorCount+=1\n                break\n            spanCount+=1\n            if(spanCount==1):\n                minTs=float(a['startTime'])\n                maxTs=float(a['startTime'])+float(a['duration'])\n            else:\n                if(minTs>float(a['startTime'])):\n                    minTs=float(a['startTime'])\n                else:\n                    minTs=minTs\n                if(maxTs<(float(a['startTime'])+float(a['duration']))):\n                    maxTs=float(a['startTime'])+float(a['duration'])\n                else:\n                    maxTs=maxTs\n        if(flag==1):\n            time=(maxTs-minTs)/1000\n            latency.append(time)\n    if(counter==0 or len(latency)==0):\n        return [0,0,0,0,0,0]\n    return [np.mean(latency),np.percentile(latency,50),np.percentile(latency,99),len(latency)/period,errorCount]", "\ndef latency_analyze(startTime,duration):\n    t_start=int(startTime*1000000)\n    t_end=int((startTime+duration)*1000000)\n    interval=100000#every 100ms requests\n    index=t_start\n    counter=0\n    while(index<=t_end):\n        res=get_latency(index,index+interval,0.1)\n        print(counter,index,index+interval,res[0],res[1],res[2],res[3],res[4])#average,50th,99th,99.9th,throughput,errorCount\n        index+=interval\n        counter+=1", "\nif __name__ == \"__main__\":\n    latency_analyze(1683713739.2187307,35)\n"]}
{"filename": "profile_example.py", "chunked_list": ["import client.ProfileGet as cc\nimport os,json\nfrom collections import defaultdict\nimport pandas\nimport time\nfrom multiprocessing import Process\nimport datetime\n\n\n", "\n\nNodeIpMapper = {\n    \"cpu-03\": \"10.2.64.3:50052\",\n    \"cpu-04\": \"10.2.64.4:50052\",\n    \"cpu-07\": \"10.2.64.7:50052\",\n    \"cpu-08\": \"10.2.64.8:50052\",\n}\ndef execute_load(cmd):\n    print(cmd)\n    os.system(cmd)", "def execute_load(cmd):\n    print(cmd)\n    os.system(cmd)\n\n# analyse the string  format  docker stats \ndef getComputeRes(res1):\n    Res_mapper={}\n    \n    res_name,res_CPU,res_MEM=[],[],[]\n    for i in range(len(res1)):\n        array_temp1=res1[i].split(\" \")\n        counter1=0\n        for j in range(1,len(array_temp1)):\n            if(array_temp1[j]!=''):\n                counter1+=1\n                if(counter1==1):\n                    res_name.append(array_temp1[j])\n                elif(counter1==2):\n                    res_CPU.append([array_temp1[j]])\n                elif(counter1==3):\n                    res_MEM.append([array_temp1[j]])\n                    break\n    for i in range(len(res_name)):\n        ms=res_name[i]\n        temp=ms.replace(\"\\n\",\"\").split(\"_\")[1].replace(\"ebc1-\",\"\")\n        cpu=float(res_CPU[i][0].replace(\"%\",\"\"))\n        if(\"GiB\" in res_MEM[i][0].replace(\"\\n\",\"\")):\n            memory=float(float(res_MEM[i][0].replace(\"GiB\",\"\"))*1000)\n        else:\n            memory=float(res_MEM[i][0].replace(\"MiB\",\"\"))\n        if(temp not in Res_mapper.keys()):\n            Res_mapper[temp]=[cpu,memory]\n        else:\n            Res_mapper[temp][0]+=cpu\n            Res_mapper[temp][1]+=memory\n    return Res_mapper", "\n\n\n# run loadgenerator\ncmd_load = \"python3 LoadGenerator.py -q 2000'\"\np_load=Process(target=execute_load,args=(cmd_load,))\np_load.start()\ndt_time2 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\n", "\n\n# get profile\ncount=0\nmax_count=8\nlist_profile=[]\nwhile(count<max_count):\n    lines_str = ''\n    for k, v in NodeIpMapper.items():\n        str_res = cc.getProfile(v)\n        lines_str += str_res", "    for k, v in NodeIpMapper.items():\n        str_res = cc.getProfile(v)\n        lines_str += str_res\n    lines_get_proc = lines_str.split('\\n')\n    # print(lines_get_proc)\n    profile_map = getComputeRes(lines_get_proc)\n    list_profile.append(profile_map)\n    count+=1\n    # time.sleep(3)\ndt_time3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')", "    # time.sleep(3)\ndt_time3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n \n \n# compute the mean value \nnew_map=defaultdict(list)\nkey_new=set()\nfor profile_map in list_profile:\n    for k,v in profile_map.items():\n        key_new.add(k)", "l_key_new=list(key_new)\nfor k in l_key_new:\n    new_map[k].append(0.0)\n    new_map[k].append(0.0)\n# cpu_usage_all=defaultdict(float)\n# mem_usage_all=defaultdict(float)\nc_all=0\n\n\nlist_frame=[]\nfor profile_map in list_profile:\n    for k,v in profile_map.items():\n        new_map[k][0]+=v[0]\n        new_map[k][1]+=v[1]\n        list_frame.append([c_all,k,v[0],v[1]])\n    c_all+=1", "\nlist_frame=[]\nfor profile_map in list_profile:\n    for k,v in profile_map.items():\n        new_map[k][0]+=v[0]\n        new_map[k][1]+=v[1]\n        list_frame.append([c_all,k,v[0],v[1]])\n    c_all+=1\n\nfor k,v in new_map.items():\n    new_map[k][0]=new_map[k][0]/c_all\n    new_map[k][1]=new_map[k][1]/c_all", "\nfor k,v in new_map.items():\n    new_map[k][0]=new_map[k][0]/c_all\n    new_map[k][1]=new_map[k][1]/c_all\n\n\nres_profile_json=json.dumps(new_map)\nwith open('profile_test.json',mode='w',encoding='utf-8') as f:\n    f.write(res_profile_json)\n", "\n\ndt_time4 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\nprint(\"2 \",dt_time2)\nprint(\"3 \",dt_time3)\nprint(\"4 \",dt_time4)\n\nprint(\"end...\")\n", "print(\"end...\")\n"]}
{"filename": "LoadUpdator/Update.py", "chunked_list": ["import os\nimport scipy\nimport pandas\nimport numpy as np\nimport time\nimport sys\nimport copy\nfrom queue import Queue\n\n#For tree structure, all in-degree is 1\ndef Update_Traffic_Tree(ms_dag):\n    print(\"Entering in......\")\n    BFS_name=list()\n    q=Queue()\n    q.put(ms_dag.root)\n    BFS_name.append(ms_dag.root.name)\n    while(not q.empty()):\n        target=q.get()\n        rate=target.realLoad/(target.handleLoad-target.overLoad)\n        if(rate<1):\n            rate=1\n        print(target.name)\n        for child in target.children:\n            if(child.name not in BFS_name):\n                child.realLoad=(child.monitorLoad-child.overLoad)*rate\n                BFS_name.append(child.name)\n                q.put(child)\n    print(\"done\")\n    ms_dag.display()", "\n#For tree structure, all in-degree is 1\ndef Update_Traffic_Tree(ms_dag):\n    print(\"Entering in......\")\n    BFS_name=list()\n    q=Queue()\n    q.put(ms_dag.root)\n    BFS_name.append(ms_dag.root.name)\n    while(not q.empty()):\n        target=q.get()\n        rate=target.realLoad/(target.handleLoad-target.overLoad)\n        if(rate<1):\n            rate=1\n        print(target.name)\n        for child in target.children:\n            if(child.name not in BFS_name):\n                child.realLoad=(child.monitorLoad-child.overLoad)*rate\n                BFS_name.append(child.name)\n                q.put(child)\n    print(\"done\")\n    ms_dag.display()", "\n#For graph structure, some in-degrees are above 1\ndef Update_Traffic_Graph(ms_dag):\n    print(\"Entering in......\")\n    # mark visit times\n    BFS_name={\n        \"frontend-search\":1,\n        \"frontend-recommend\":1,\n        \"frontend-reserve\":1,\n        \"search\":1,\n        \"check\":2,\n        \"recommendation\":1,\n        \"profile\":2,\n        \"user\":1,\n        \"reservation\":1,\n        \"geo\":1,\n        \"rate\":1,\n        \"memcached-check\":1,\n        \"rank-category\":1,\n        \"rank-overall\":1,\n        \"memcached-profile\":1,\n        \"memcached-reservation\":1,\n        \"mongodb-reservation\":1,\n        \"memcached-rate\":1\n    }\n    q=Queue()\n    q.put(ms_dag.root)\n    while(not q.empty()):\n        target=q.get()\n        idx1=target.index\n        if(target.index==0):\n            rate=target.realLoad/target.handleLoad\n        else:\n            target.realLoad=np.sum(ms_dag.Load_matrix[:,idx1])\n            rate=target.realLoad/min(target.handleLoad,target.monitorLoad)\n        if(rate<1):\n            rate=1\n        for child in target.pressure_children:\n            if(BFS_name[child.name]!=0):\n                idx2=child.index\n                ms_dag.Load_matrix[idx1][idx2]=ms_dag.Load_matrix[idx1][idx2]*rate\n                BFS_name[child.name]-=1\n                if(BFS_name[child.name]==0):\n                    q.put(child)\n    print(\"==========done==========\")\n    ms_dag.display()", "\nif __name__ == \"__main__\":\n    Update_Traffic_Graph()"]}
{"filename": "QueryDrainer/Compensator.py", "chunked_list": ["import os\nimport pandas\nimport numpy as np\nimport time\n\nSLA=3#QoS reovery time, can be changed\n\n#time_interval: monitor interval\ndef Compensate(ms_dag,time_interval):\n    # Compensate for every microservice\n    for ms in ms_dag.Access_Record:\n        # this queue\n        this_overLoad=ms.realLoad-min(ms.monitorLoad-ms.overLoad,ms.handleLoad-ms.overLoad)\n        this_queueRequests=this_overLoad*time_interval\n        # history queue\n        history_overLoad=ms.overLoad\n        history_surplusTime=ms.SLAtime-time_interval\n        history_queueRequests=history_overLoad*history_surplusTime\n        # all queue\n        all_queueRequests=this_queueRequests+history_queueRequests\n        # needed total load\n        ms.SLAtime=SLA-time_interval\n        all_Requests=all_queueRequests+ms.realLoad*ms.SLAtime\n        tot_load=all_Requests/ms.SLAtime\n        # cal overload\n        ms.overLoad=tot_load-ms.realLoad\n        \n    print(\"==========Compensate Done.==========\")\n    ms_dag.display()", "def Compensate(ms_dag,time_interval):\n    # Compensate for every microservice\n    for ms in ms_dag.Access_Record:\n        # this queue\n        this_overLoad=ms.realLoad-min(ms.monitorLoad-ms.overLoad,ms.handleLoad-ms.overLoad)\n        this_queueRequests=this_overLoad*time_interval\n        # history queue\n        history_overLoad=ms.overLoad\n        history_surplusTime=ms.SLAtime-time_interval\n        history_queueRequests=history_overLoad*history_surplusTime\n        # all queue\n        all_queueRequests=this_queueRequests+history_queueRequests\n        # needed total load\n        ms.SLAtime=SLA-time_interval\n        all_Requests=all_queueRequests+ms.realLoad*ms.SLAtime\n        tot_load=all_Requests/ms.SLAtime\n        # cal overload\n        ms.overLoad=tot_load-ms.realLoad\n        \n    print(\"==========Compensate Done.==========\")\n    ms_dag.display()", "\nif __name__ == \"__main__\":\n    print()"]}
{"filename": "QueryDrainer/AdjustRes.py", "chunked_list": ["import os\nimport time\nimport sys\nsys.path.append(\".\") \n# To import grpc APIs\nfrom client.client import rpc_adjust_res\n\n#Node-Ip relationship\npod_node_mapper={'check': 'cpu-04', 'consul': 'cpu-06', 'entering-ms': 'cpu-03', 'frontend-recommend': 'cpu-07', 'frontend-reserve': 'cpu-08', 'frontend-search': 'cpu-04', 'geo': 'cpu-04', 'jaeger': 'cpu-06', 'memcached-check': 'cpu-04', 'memcached-profile': 'cpu-07', 'memcached-rate': 'cpu-04', 'memcached-reservation': 'cpu-08', 'mongodb-check': 'cpu-04', 'mongodb-geo': 'cpu-04', 'mongodb-history': 'cpu-07', 'mongodb-profile': 'cpu-07', 'mongodb-rate': 'cpu-04', 'mongodb-recommendation': 'cpu-07', 'mongodb-reservation': 'cpu-08', 'mongodb-user': 'cpu-08', 'profile': 'cpu-07', 'rank-category': 'cpu-07', 'rank-overall': 'cpu-07', 'rate': 'cpu-04', 'recommendation': 'cpu-07', 'reservation': 'cpu-08', 'search': 'cpu-04', 'user': 'cpu-08'}\n", "pod_node_mapper={'check': 'cpu-04', 'consul': 'cpu-06', 'entering-ms': 'cpu-03', 'frontend-recommend': 'cpu-07', 'frontend-reserve': 'cpu-08', 'frontend-search': 'cpu-04', 'geo': 'cpu-04', 'jaeger': 'cpu-06', 'memcached-check': 'cpu-04', 'memcached-profile': 'cpu-07', 'memcached-rate': 'cpu-04', 'memcached-reservation': 'cpu-08', 'mongodb-check': 'cpu-04', 'mongodb-geo': 'cpu-04', 'mongodb-history': 'cpu-07', 'mongodb-profile': 'cpu-07', 'mongodb-rate': 'cpu-04', 'mongodb-recommendation': 'cpu-07', 'mongodb-reservation': 'cpu-08', 'mongodb-user': 'cpu-08', 'profile': 'cpu-07', 'rank-category': 'cpu-07', 'rank-overall': 'cpu-07', 'rate': 'cpu-04', 'recommendation': 'cpu-07', 'reservation': 'cpu-08', 'search': 'cpu-04', 'user': 'cpu-08'}\n\n#pod-node relationship\npod_uids={\n    \"entering-ms\":['c95afad8-75ea-4cf6-b0a4-07982fad4cf7\\n'],\n    \"frontend-search\":['07b00721-06e6-47a6-abae-c6b6cb6512e0\\n'],\n    \"frontend-recommend\":['249ce56f-f436-411d-841a-1430135f8cd9\\n'],\n    \"frontend-reserve\":['1a545e65-91a4-4f99-96be-993366e7924d\\n'],\n    \"search\": ['41be1c59-1531-478f-8e4b-74b826e00c20\\n'],\n    \"check\": ['c2c39aef-516c-469d-b31c-c0e9193cea39\\n'],", "    \"search\": ['41be1c59-1531-478f-8e4b-74b826e00c20\\n'],\n    \"check\": ['c2c39aef-516c-469d-b31c-c0e9193cea39\\n'],\n    \"recommendation\": ['75b476d8-759a-42ec-b02d-9f79ec0c3e99\\n'],\n    \"profile\": ['2fdee85b-2f09-4c96-922b-2f22800c1f53\\n'],\n    \"user\": ['153e0a9a-8cd5-439d-96e2-a21db4985b44\\n'],\n    \"reservation\": ['12ae4870-ceca-46c5-a8de-74992794251f\\n'],\n    \"geo\": ['80a37e50-bfbb-4a13-84e7-79a3a3e61879\\n'],\n    \"rate\":['1477e6df-edce-435f-8985-5a9d24359693\\n'],\n    \"memcached-check\":['bf8c4d95-e879-4116-82e4-75f0aed3469f\\n'],\n    \"rank-category\": ['4a0e4a06-74bd-485b-9108-cda1b0bb2cdb\\n'],", "    \"memcached-check\":['bf8c4d95-e879-4116-82e4-75f0aed3469f\\n'],\n    \"rank-category\": ['4a0e4a06-74bd-485b-9108-cda1b0bb2cdb\\n'],\n    \"rank-overall\": ['604e15ce-e6b7-45d6-8347-9f51954db5cf\\n'],\n    \"memcached-profile\": ['5cb477da-db24-4cee-97eb-c0668a6ef455\\n'],\n    \"memcached-reservation\": ['b295189c-b513-4ad5-99b6-ff8e6dd4ff09\\n'],\n    \"mongodb-reservation\":['67273aa4-7f56-45fc-a721-04ac453833bb\\n'],\n    \"memcached-rate\":['a2630e43-dd25-47f6-b14c-8ac73ba386b1\\n']\n}\n\n#pod-uid relationship\ndef get_pod_uids():\n    for svc in pod_uids.keys():\n        service=str(svc)\n        os.system(\"./get_pod_uid.sh \"+service+\" >/dev/null 2>&1\")\n        uids=os.popen(\"cat uid.txt\").readlines()\n        print(svc, uids)", "\n#pod-uid relationship\ndef get_pod_uids():\n    for svc in pod_uids.keys():\n        service=str(svc)\n        os.system(\"./get_pod_uid.sh \"+service+\" >/dev/null 2>&1\")\n        uids=os.popen(\"cat uid.txt\").readlines()\n        print(svc, uids)\n\n#Call Grpc APIs to adjust resources\ndef set_cpu(ms_dag,service,cpu,replicas_number=1):\n    node=ms_dag.PodNodeMapper[service]\n    ip_str=ms_dag.NodeIpMapper[node]\n    uids=pod_uids[service]\n    res=rpc_adjust_res(ip_str,uids,cpu)", "\n#Call Grpc APIs to adjust resources\ndef set_cpu(ms_dag,service,cpu,replicas_number=1):\n    node=ms_dag.PodNodeMapper[service]\n    ip_str=ms_dag.NodeIpMapper[node]\n    uids=pod_uids[service]\n    res=rpc_adjust_res(ip_str,uids,cpu)\n\n#Call set_CPU\ndef run_set_cpu(ms_dag):\n    for target in ms_dag.Access_Record:\n        # if(target.name==\"mongodb-reservation\"):\n            # continue\n        print(target.name,target.CPU_Allocated)\n        set_cpu(ms_dag,target.name,target.CPU_Allocated,1)\n        target.handleLoad=target.realLoad+target.overLoad\n    print(\"Set all CPU done\")", "#Call set_CPU\ndef run_set_cpu(ms_dag):\n    for target in ms_dag.Access_Record:\n        # if(target.name==\"mongodb-reservation\"):\n            # continue\n        print(target.name,target.CPU_Allocated)\n        set_cpu(ms_dag,target.name,target.CPU_Allocated,1)\n        target.handleLoad=target.realLoad+target.overLoad\n    print(\"Set all CPU done\")\n\nif __name__ == \"__main__\":\n    get_pod_uids()", "\nif __name__ == \"__main__\":\n    get_pod_uids()"]}
{"filename": "workerServer/distributed_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: distributed.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n", "# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x11\\x64istributed.proto\\\"\\x1e\\n\\x0eTrafficRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fTrafficResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\")\\n\\nResRequest\\x12\\x0c\\n\\x04uids\\x18\\x01 \\x03(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01\\\"\\x1d\\n\\x0bResResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\\"\\x1e\\n\\x0eProfileRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fProfileResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\"\\x1e\\n\\x0eNetProcRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fNetProcResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\x32\\xd4\\x01\\n\\x0bGrpcService\\x12\\x32\\n\\x0bget_traffic\\x12\\x0f.TrafficRequest\\x1a\\x10.TrafficResponse\\\"\\x00\\x12(\\n\\tadjustRes\\x12\\x0b.ResRequest\\x1a\\x0c.ResResponse\\\"\\x00\\x12\\x32\\n\\x0bget_profile\\x12\\x0f.ProfileRequest\\x1a\\x10.ProfileResponse\\\"\\x00\\x12\\x33\\n\\x0cget_net_proc\\x12\\x0f.NetProcRequest\\x1a\\x10.NetProcResponse\\\"\\x00\\x42\\x03\\x80\\x01\\x01\\x62\\x06proto3')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())", "\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'distributed_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  DESCRIPTOR._serialized_options = b'\\200\\001\\001'\n  _TRAFFICREQUEST._serialized_start=21\n  _TRAFFICREQUEST._serialized_end=51\n  _TRAFFICRESPONSE._serialized_start=53\n  _TRAFFICRESPONSE._serialized_end=106\n  _RESREQUEST._serialized_start=108\n  _RESREQUEST._serialized_end=149\n  _RESRESPONSE._serialized_start=151\n  _RESRESPONSE._serialized_end=180\n  _PROFILEREQUEST._serialized_start=182\n  _PROFILEREQUEST._serialized_end=212\n  _PROFILERESPONSE._serialized_start=214\n  _PROFILERESPONSE._serialized_end=267\n  _NETPROCREQUEST._serialized_start=269\n  _NETPROCREQUEST._serialized_end=299\n  _NETPROCRESPONSE._serialized_start=301\n  _NETPROCRESPONSE._serialized_end=354\n  _GRPCSERVICE._serialized_start=357\n  _GRPCSERVICE._serialized_end=569", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "workerServer/server.py", "chunked_list": ["#! /usr/bin/env python\n# coding=utf8\n\nimport time\nfrom concurrent import futures\nimport grpc\nimport os\n\nimport distributed_pb2_grpc,distributed_pb2\n", "import distributed_pb2_grpc,distributed_pb2\n\nfrom datetime import datetime,timedelta\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\n\n# service name:corresponds the keys of pod_uids\n# cpu: resource of cpu,1 is 0.1 CPU core\n# replica_number: replica number of MS\ndef set_cpu(uids,cpu):\n    cpu=cpu*10000\n    cpu=int(cpu)\n    \n    cpu_every=cpu//len(uids)\n    # print(cpu_every)\n    for uid in uids:\n        uid=uid.replace(\"\\n\",\"\")\n        path = '/sys/fs/cgroup/cpu/kubepods/besteffort/pod' + uid + '/cpu.cfs_quota_us'\n        print(path,cpu_every)\n        # f = open(path,\"r\")\n        # original = int(f.read())\n        # f.close()\n        if cpu_every<1000:\n            cpu_every=1000\n        curr_value = str(cpu_every)\n        with open(path, \"w+\") as f:\n            f.write(curr_value)", "# replica_number: replica number of MS\ndef set_cpu(uids,cpu):\n    cpu=cpu*10000\n    cpu=int(cpu)\n    \n    cpu_every=cpu//len(uids)\n    # print(cpu_every)\n    for uid in uids:\n        uid=uid.replace(\"\\n\",\"\")\n        path = '/sys/fs/cgroup/cpu/kubepods/besteffort/pod' + uid + '/cpu.cfs_quota_us'\n        print(path,cpu_every)\n        # f = open(path,\"r\")\n        # original = int(f.read())\n        # f.close()\n        if cpu_every<1000:\n            cpu_every=1000\n        curr_value = str(cpu_every)\n        with open(path, \"w+\") as f:\n            f.write(curr_value)", "\n\n\n\nclass TestService(distributed_pb2_grpc.GrpcServiceServicer):\n\n    def __init__(self):\n        \n        pass\n    \n    def adjustRes(self, request, context):\n        '''\n        adjust resource\n        '''\n        uids=request.uids\n        cpu_value=float(request.value)\n        \n       \n        print(uids,cpu_value)\n        set_cpu(uids,cpu_value)\n        result='1'\n        return distributed_pb2.ResResponse(result=str(result))\n    def get_profile(self, request, context):\n        '''\n        get the cpu use of mircoservices\n        '''\n        svc_name = request.data\n        timestampf=datetime.now().timestamp()\n        cmd=\"docker stats --no-stream | grep \"+svc_name\n        res1=os.popen(cmd).readlines()\n        res_net=''.join(res1)\n        # res_net=\"success\"\n        return distributed_pb2.ProfileResponse(result=res_net,time_stamp=timestampf)\n    def get_net_proc(self, request, context):\n        '''\n        get the total traffic of interface of net\n        '''\n\n        src_ip = request.data\n        timestampf=datetime.now().timestamp()\n        # print(timestampf)\n        \n        \n        lines = os.popen(\"cat /proc/net/dev\").readlines()\n        res_net=','.join(lines)\n\n        # print(res_net)\n        \n        return distributed_pb2.NetProcResponse(result=res_net,time_stamp=timestampf)", "    \n    \n    \ndef run():\n    '''\n    start service\n    '''\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=70))\n    distributed_pb2_grpc.add_GrpcServiceServicer_to_server(TestService(),server)\n    server.add_insecure_port('[::]:50052')\n    server.start()\n    print(\"start service...\")\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        server.stop(0)", "if __name__ == '__main__':\n    run()\n"]}
{"filename": "workerServer/distributed_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\n\nimport distributed_pb2 as distributed__pb2\n\n\nclass GrpcServiceStub(object):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.get_traffic = channel.unary_unary(\n                '/GrpcService/get_traffic',\n                request_serializer=distributed__pb2.TrafficRequest.SerializeToString,\n                response_deserializer=distributed__pb2.TrafficResponse.FromString,\n                )\n        self.adjustRes = channel.unary_unary(\n                '/GrpcService/adjustRes',\n                request_serializer=distributed__pb2.ResRequest.SerializeToString,\n                response_deserializer=distributed__pb2.ResResponse.FromString,\n                )\n        self.get_profile = channel.unary_unary(\n                '/GrpcService/get_profile',\n                request_serializer=distributed__pb2.ProfileRequest.SerializeToString,\n                response_deserializer=distributed__pb2.ProfileResponse.FromString,\n                )\n        self.get_net_proc = channel.unary_unary(\n                '/GrpcService/get_net_proc',\n                request_serializer=distributed__pb2.NetProcRequest.SerializeToString,\n                response_deserializer=distributed__pb2.NetProcResponse.FromString,\n                )", "\n\nclass GrpcServiceServicer(object):\n    \"\"\"\n    \"\"\"\n\n    def get_traffic(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def adjustRes(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def get_profile(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def get_net_proc(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')", "\n\ndef add_GrpcServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'get_traffic': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_traffic,\n                    request_deserializer=distributed__pb2.TrafficRequest.FromString,\n                    response_serializer=distributed__pb2.TrafficResponse.SerializeToString,\n            ),\n            'adjustRes': grpc.unary_unary_rpc_method_handler(\n                    servicer.adjustRes,\n                    request_deserializer=distributed__pb2.ResRequest.FromString,\n                    response_serializer=distributed__pb2.ResResponse.SerializeToString,\n            ),\n            'get_profile': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_profile,\n                    request_deserializer=distributed__pb2.ProfileRequest.FromString,\n                    response_serializer=distributed__pb2.ProfileResponse.SerializeToString,\n            ),\n            'get_net_proc': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_net_proc,\n                    request_deserializer=distributed__pb2.NetProcRequest.FromString,\n                    response_serializer=distributed__pb2.NetProcResponse.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'GrpcService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))", "\n\n # This class is part of an EXPERIMENTAL API.\nclass GrpcService(object):\n    \"\"\"\n    \"\"\"\n\n    @staticmethod\n    def get_traffic(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_traffic',\n            distributed__pb2.TrafficRequest.SerializeToString,\n            distributed__pb2.TrafficResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def adjustRes(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/adjustRes',\n            distributed__pb2.ResRequest.SerializeToString,\n            distributed__pb2.ResResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def get_profile(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_profile',\n            distributed__pb2.ProfileRequest.SerializeToString,\n            distributed__pb2.ProfileResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def get_net_proc(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_net_proc',\n            distributed__pb2.NetProcRequest.SerializeToString,\n            distributed__pb2.NetProcResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)", ""]}
{"filename": "Predict/Predictor.py", "chunked_list": ["import csv\nimport scipy.stats as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\n#Record the slope and interceptes\n#(Examples, need to fit on own clusters)", "#Record the slope and interceptes\n#(Examples, need to fit on own clusters)\nparams_net_to_load={\n    \"entering-ms\":[0.06,0.1],\n    \"frontend-search\":[0.06,0.1],\n    \"frontend-recommend\":[0.06,0.1],\n    \"frontend-reserve\":[0.06,0.1],\n    \"search\":[0.06,0.1],\n    \"check\":[0.06,0.1],\n    \"recommendation\":[0.06,0.1],", "    \"check\":[0.06,0.1],\n    \"recommendation\":[0.06,0.1],\n    \"profile\":[0.06,0.1],\n    \"user\":[0.06,0.1],\n    \"reservation\":[0.06,0.1],\n    \"geo\":[0.06,0.1],\n    \"rate\":[0.06,0.1],\n    \"memcached-check\":[0.06,0.1],\n    \"rank-category\":[0.06,0.1],\n    \"rank-overall\":[0.06,0.1],", "    \"rank-category\":[0.06,0.1],\n    \"rank-overall\":[0.06,0.1],\n    \"memcached-profile\":[0.06,0.1],\n    \"memcached-reservation\":[0.06,0.1],\n    \"mongodb-reservation\":[0.06,0.1],\n    \"memcached-rate\":[0.06,0.1]\n}\n\n#Record the slope and interceptes\n#(Examples, need to fit on own clusters)", "#Record the slope and interceptes\n#(Examples, need to fit on own clusters)\nparams_load_to_CPU={\n    \"entering-ms\":[0.06,0.1],\n    \"frontend-search\":[0.06,0.1],\n    \"frontend-recommend\":[0.06,0.1],\n    \"frontend-reserve\":[0.06,0.1],\n    \"search\":[0.06,0.1],\n    \"check\":[0.06,0.1],\n    \"recommendation\":[0.06,0.1],", "    \"check\":[0.06,0.1],\n    \"recommendation\":[0.06,0.1],\n    \"profile\":[0.06,0.1],\n    \"user\":[0.06,0.1],\n    \"reservation\":[0.06,0.1],\n    \"geo\":[0.06,0.1],\n    \"rate\":[0.06,0.1],\n    \"memcached-check\":[0.06,0.1],\n    \"rank-category\":[0.06,0.1],\n    \"rank-overall\":[0.06,0.1],", "    \"rank-category\":[0.06,0.1],\n    \"rank-overall\":[0.06,0.1],\n    \"memcached-profile\":[0.06,0.1],\n    \"memcached-reservation\":[0.06,0.1],\n    \"mongodb-reservation\":[0.06,0.1],\n    \"memcached-rate\":[0.06,0.1]\n}\n\n# Record corresponding edges\nedge_mapper={", "# Record corresponding edges\nedge_mapper={\n    \"geo\": [[4,10]],\n    \"memcached-rate\": [[11,18]],\n    \"rate\": [[4,11]],\n    \"search\": [[1,4]],\n    \"memcached-check\": [[5,12]],\n    \"check\": [[10,5],[18,5]], #2 in-degrees\n    \"frontend-search\": [[0,1]],\n    \"rank-category\": [[6,13]],", "    \"frontend-search\": [[0,1]],\n    \"rank-category\": [[6,13]],\n    \"rank-overall\": [[6,14]],\n    \"recommendation\": [[2,6]],\n    \"memcached-profile\": [[7,15]],\n    \"profile\": [[13,7],[14,7]], #2 in-degrees\n    \"frontend-recommend\": [[0,2]],\n    \"user\": [[3,8]],\n    \"memcached-reservation\": [[9,16]],\n    \"mongodb-reservation\": [[9,17]],", "    \"memcached-reservation\": [[9,16]],\n    \"mongodb-reservation\": [[9,17]],\n    \"reservation\": [[8,9]],\n    \"frontend-reserve\": [[0,3]]\n}\n\n#Get the monitored load\ndef predict_net_to_load(ms_dag):\n    print(\"Monitor Load......\")\n    for target in ms_dag.Access_Record:\n        slope=params_net_to_load[target.name][0]\n        intercept=params_net_to_load[target.name][1]\n        target.monitorLoad=target.upper_traffic*slope+intercept\n        '''\n        # Profile values with traffic error to avoid network fluctuations to evaluate\n        target.monitorLoad=Base_MonitoredLoad[target.name]*TrafficMonitorError[target.name]\n        '''\n        print(target.name,target.monitorLoad)\n        # no in-degrees of entering ms\n        if(target.name==\"entering-ms\"):\n            continue\n        # Update edges in the Load_matrix\n        edges=edge_mapper[target.name]\n        if(len(edges)==1):#1 in-degree\n            ms_dag.Load_matrix[edges[0][0],edges[0][1]]=target.monitorLoad\n        else:# multiple in-degrees\n            ratios=[0,0]\n            if(target.name==\"check\"):\n                load1=min(ms_dag.Access_Record[0].handleLoad,ms_dag.Access_Record[0].monitorLoad)\n                load2=min(ms_dag.Access_Record[2].handleLoad,ms_dag.Access_Record[2].monitorLoad)\n                ratios[0]=load1/(load1+load2)\n                ratios[1]=1-ratios[0]\n            elif(target.name==\"profile\"):\n                load1=min(ms_dag.Access_Record[7].handleLoad,ms_dag.Access_Record[7].monitorLoad)\n                load2=min(ms_dag.Access_Record[8].handleLoad,ms_dag.Access_Record[8].monitorLoad)\n                ratios[0]=load1/(load1+load2)\n                ratios[1]=1-ratios[0]\n            for i in range(len(edges)):\n                edge=edges[i]\n                ratio=ratios[i]\n                ms_dag.Load_matrix[edge[0],edge[1]]=target.monitorLoad*ratio", "\n#Obtain the CPU allocation\ndef predict_load_to_CPU(ms_dag):\n    for target in ms_dag.Access_Record:\n        slope=params_load_to_CPU[target.name][0]\n        intercept=params_load_to_CPU[target.name][1]\n        target.CPU_Need=target.realLoad*slope+intercept\n        target.CPU_Allocated=(target.realLoad+target.overLoad)*slope+intercept\n        over_CPU=target.CPU_Allocated-target.CPU_Need\n        '''\n        # Profile CPUs with real-load error to avoid prediction interference to evaluate\n        ratio=target.realLoad/Base_RealLoad[target.name]\n        target.CPU_Need=Profile_CPUs[target.name]*ratio\n        over_CPU=target.overLoad/target.realLoad*target.CPU_Need\n        target.CPU_Allocated=over_CPU+target.CPU_Need\n        '''\n        print(target.name,target.realLoad,target.overLoad,over_CPU,target.CPU_Allocated)", "\nif __name__ == \"__main__\":\n    predict_load_to_CPU()"]}
{"filename": "client/TrafficGet_hb.py", "chunked_list": ["import os\nfrom collections import defaultdict\nfrom client.client import rpc_adjust_res, rpc_get_traffic\nimport pandas\nimport time\nfrom multiprocessing import Process\nimport datetime\n\n\nms_interface_mapper = defaultdict(list)  # {calicoxxx:[10.244.xx.xx,ms-0]}", "\nms_interface_mapper = defaultdict(list)  # {calicoxxx:[10.244.xx.xx,ms-0]}\nms_ip_mapper = defaultdict(list)\norder_mapper_hb = {\"entering-ms\": [], \"frontend-search\": [\"entering-ms\"], \"frontend-recommend\": [\"entering-ms\"], \"frontend-reserve\": [\n    \"entering-ms\"], \"search\": [\"frontend-search\"], \"check\": [\"frontend-search\"], \"recommendation\": [\"frontend-recommend\"], \"profile\": [\"frontend-recommend\"],  \"user\": [\"frontend-reserve\"], \"reservation\": [\"frontend-reserve\"], \"geo\": [\"search\"], \"rate\": [\"search\"], \"memcached-check\": [\"check\"], \"rank-category\": [\"recommendation\"], \"rank-overall\": [\"recommendation\"], \"memcached-profile\": [\"profile\"], \"memcached-reservation\": [\n    \"reservation\"], \"mongodb-reservation\": [\"reservation\"], \"memcached-rate\": [\"rate\"]}\n# {cpu-04:[calicoxxx,calicoxxx,......]} \nNode_interface_mapper = defaultdict(list)\nNodeIpMapper = {\n    \"cpu-03\": \"10.2.64.3:50052\",", "NodeIpMapper = {\n    \"cpu-03\": \"10.2.64.3:50052\",\n    \"cpu-04\": \"10.2.64.4:50052\",\n    \"cpu-07\": \"10.2.64.7:50052\",\n    \"cpu-08\": \"10.2.64.8:50052\",\n}\n\n\n\n\ndef analyse_lines(lines_get_proc):\n    global ms_interface_mapper, ms_ip_mapper, order_mapper_ebc2, order_mapper_ebc1, order_mapper_hb\n    now_ms = ''\n    now_ip = ''\n    map_res_traffic = defaultdict(int)\n    map_res_num = defaultdict(int)\n    for l in lines_get_proc:\n        line = l.rstrip('\\n')\n        if line == '':\n            continue\n        if line.startswith('+'):\n            ca_name = line[1:]\n            if ca_name not in ms_interface_mapper.keys():\n                continue\n            now_ip = ms_interface_mapper[ca_name][0]\n            now_ms = ms_interface_mapper[ca_name][1]\n            continue\n        if (now_ms not in order_mapper_hb.keys()) or (now_ms == ''):\n            continue\n\n        list_line = line.split(':')\n        if len(list_line) < 3:\n            print(l, \"num is not correct!\")\n            return defaultdict(int), defaultdict(int)\n        src_ip = list_line[0].split(';')[0]\n        dst_ip = list_line[0].split(';')[1]\n        traffic_loc = int(list_line[1])\n        bag_num = int(list_line[2])\n        if now_ms == \"entering-ms\" and (src_ip not in ms_ip_mapper.keys()):\n            map_res_traffic[now_ms] += traffic_loc\n            map_res_num[now_ms] += bag_num\n            continue\n\n        if (src_ip not in ms_ip_mapper) or (dst_ip not in ms_ip_mapper):\n            # print(l,\"src or dst is not in json\")\n            continue\n        src_ms = ms_ip_mapper[src_ip][1]\n        dst_ms = ms_ip_mapper[dst_ip][1]\n\n        if dst_ms != now_ms:\n            # print(l,\"dst is not now_dst!\")\n            continue\n\n        if src_ms in order_mapper_hb[now_ms]:\n            map_res_traffic[now_ms] += traffic_loc\n            map_res_num[now_ms] += bag_num\n    return map_res_traffic, map_res_num", "\n\ndef analyse_lines(lines_get_proc):\n    global ms_interface_mapper, ms_ip_mapper, order_mapper_ebc2, order_mapper_ebc1, order_mapper_hb\n    now_ms = ''\n    now_ip = ''\n    map_res_traffic = defaultdict(int)\n    map_res_num = defaultdict(int)\n    for l in lines_get_proc:\n        line = l.rstrip('\\n')\n        if line == '':\n            continue\n        if line.startswith('+'):\n            ca_name = line[1:]\n            if ca_name not in ms_interface_mapper.keys():\n                continue\n            now_ip = ms_interface_mapper[ca_name][0]\n            now_ms = ms_interface_mapper[ca_name][1]\n            continue\n        if (now_ms not in order_mapper_hb.keys()) or (now_ms == ''):\n            continue\n\n        list_line = line.split(':')\n        if len(list_line) < 3:\n            print(l, \"num is not correct!\")\n            return defaultdict(int), defaultdict(int)\n        src_ip = list_line[0].split(';')[0]\n        dst_ip = list_line[0].split(';')[1]\n        traffic_loc = int(list_line[1])\n        bag_num = int(list_line[2])\n        if now_ms == \"entering-ms\" and (src_ip not in ms_ip_mapper.keys()):\n            map_res_traffic[now_ms] += traffic_loc\n            map_res_num[now_ms] += bag_num\n            continue\n\n        if (src_ip not in ms_ip_mapper) or (dst_ip not in ms_ip_mapper):\n            # print(l,\"src or dst is not in json\")\n            continue\n        src_ms = ms_ip_mapper[src_ip][1]\n        dst_ms = ms_ip_mapper[dst_ip][1]\n\n        if dst_ms != now_ms:\n            # print(l,\"dst is not now_dst!\")\n            continue\n\n        if src_ms in order_mapper_hb[now_ms]:\n            map_res_traffic[now_ms] += traffic_loc\n            map_res_num[now_ms] += bag_num\n    return map_res_traffic, map_res_num", "\n\n\ndef ms_interace():\n    global ms_interface_mapper, ms_ip_mapper, Node_interface_mapper\n    # MS-name, pod-ip, node-name\n    pods_ips_nodes = os.popen(\n        \"kubectl get pods -o wide | awk '{print $1,$6,$7}'\").readlines()\n    for p_ip in pods_ips_nodes:\n        p_ip = p_ip.rstrip('\\n')\n        l_pip = p_ip.split(' ')\n        if len(l_pip) < 3:\n            continue\n        pod = l_pip[0]\n        ip = l_pip[1]\n        node = l_pip[2]\n        if (pod == \"NAME\"):\n            continue\n        if (pod[:6] == \"jaeger\"):\n            continue\n        id = -1\n        # Obatin the no of network interface\n        try:\n            id = os.popen(\"kubectl exec -i \"+pod +\n                          \" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n        except IndexError:\n            print(\"kubectl error\", pod, id)\n            continue\n        idtarget = \"'^\"+id+\":\"+\" \"+\"'\"\n        temp = \"\"\n        # Obtain the network interface name\n        if (node == \"cpu-06\"):  \n            temp = os.popen(\"ip link show | grep \"+idtarget +\n                            \" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n        else: \n            temp = os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget +\n                            \"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n\n        index = temp.find('@')\n        iplink = temp[:index]\n        temp = pod.split(\"-\")\n        extra = \"-\"+temp[-2]+\"-\"+temp[-1]\n        value = pod.replace(extra, \"\")\n\n        # print(iplink, ip, value)\n        ms_interface_mapper[iplink] = [ip, value]\n        ms_ip_mapper[ip] = [iplink, value]\n        if node != 'cpu-06':\n            Node_interface_mapper[node].append(iplink)\n    pods_ips_nodes_svc = os.popen(\n        \"kubectl get svc | awk '{print $1,$3}'\").readlines()\n    for p_ip in pods_ips_nodes_svc:\n        p_ip = p_ip.rstrip('\\n')\n        l_pip = p_ip.split(' ')\n        if len(l_pip) < 2:\n            continue\n        pod = l_pip[0]\n        ip = l_pip[1]\n\n        if (pod == \"NAME\"):\n            continue\n        if (pod[:6] == \"jaeger\" or pod == \"kubernetes\"):\n            continue\n        ms_ip_mapper[ip] = [\"\", pod]\n    for k, v in ms_interface_mapper.items():\n        print(k, v)\n    for k, v in ms_ip_mapper.items():\n        print(k, v)", "\ndef execute_cmd(node, list_interface, ip_list):\n    str_interface = ' '.join(list_interface)\n    str_iplist = ' '.join(ip_list)\n    cmd = \"ssh \"+node+\" '/state/partition/zxtong_2/TrafficMonitor/TrafficGet/get_traffic_new \" + \\\n        str_interface+\" \"+str_iplist+\" 25 300 600'\"\n    print(cmd)\n    os.system(cmd)\n\n", "\n\n\n\ndef start_capture():\n    global Node_interface_mapper, ms_interface_mapper\n    print(\"Node_interface_mapper\", Node_interface_mapper)\n    process_list = []\n    for k, v in Node_interface_mapper.items():\n        if len(v) == 0:\n            continue\n        ip_list = []\n        for interface_name in v:\n            ip_list.append(ms_interface_mapper[interface_name][0])\n        p = Process(target=execute_cmd, args=(k, v, ip_list,))\n        # print(list_ms_4)\n        process_list.append(p)\n    for p in process_list:\n        p.start()\n    return process_list", "\n\n\ndef getTraffic():\n    global ms_interface_mapper, ms_ip_mapper\n    traffic_mapper = defaultdict(int)\n    lines_str = ''\n    for k, v in NodeIpMapper.items():\n        if k not in Node_interface_mapper.keys():\n            continue\n        str_res = rpc_get_traffic(v)\n        lines_str += str_res\n    lines_get_proc = lines_str.split('\\n')\n    # print(lines_get_proc)\n    traffic_mapper, res_tmp = analyse_lines(lines_get_proc)\n    return traffic_mapper, res_tmp", "\n\nif __name__ == '__main__':\n    dt_time0 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n    ms_interace()\n    dt_time1 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n    process_list = start_capture()\n\n    list_frame = []\n    count = 0\n    max_count = 20\n    lines = []\n\n\n    old_map_traffic = defaultdict(int)\n    old_map_num = defaultdict(int)\n    dt_time2 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n    while (count < max_count):\n        time.sleep(0.5)\n        print(count)\n        new_map_traffic, new_map_num = getTraffic()\n        for k, v in new_map_traffic.items():\n            l_tmp = [count, k, v-old_map_traffic[k],\n                     new_map_num[k]-old_map_num[k]]\n            list_frame.append(l_tmp)\n        old_map_traffic = new_map_traffic\n        old_map_num = new_map_num\n        count += 1\n    dt_time3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n    for p in process_list:\n        p.terminate()\n        p.join()\n    '''\n    '''\n\n    dt_time4 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n    print(dt_time0)\n    print(dt_time1)\n    print(dt_time2)\n    print(dt_time3)\n    print(dt_time4)\n\n    df = pandas.DataFrame(list_frame, columns=[\n                          'time', 'direction', 'traffic', 'num'])\n    df.to_csv('speed.csv')", ""]}
{"filename": "client/ProfileGet.py", "chunked_list": ["\nfrom client.client import rpc_get_profile\n\n\n\n\ndef getProfile(v,svc_name):\n    return rpc_get_profile(v,svc_name)\n\n\nif __name__ == '__main__':\n    getProfile(\"10.2.64.8:50052\",\"ebc1\")", "\n\nif __name__ == '__main__':\n    getProfile(\"10.2.64.8:50052\",\"ebc1\")\n    "]}
{"filename": "client/distributed_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: distributed.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n", "# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x11\\x64istributed.proto\\\"\\x1e\\n\\x0eTrafficRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fTrafficResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\")\\n\\nResRequest\\x12\\x0c\\n\\x04uids\\x18\\x01 \\x03(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01\\\"\\x1d\\n\\x0bResResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\\"\\x1e\\n\\x0eProfileRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fProfileResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\\"\\x1e\\n\\x0eNetProcRequest\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"5\\n\\x0fNetProcResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\ntime_stamp\\x18\\x02 \\x01(\\x01\\x32\\xd4\\x01\\n\\x0bGrpcService\\x12\\x32\\n\\x0bget_traffic\\x12\\x0f.TrafficRequest\\x1a\\x10.TrafficResponse\\\"\\x00\\x12(\\n\\tadjustRes\\x12\\x0b.ResRequest\\x1a\\x0c.ResResponse\\\"\\x00\\x12\\x32\\n\\x0bget_profile\\x12\\x0f.ProfileRequest\\x1a\\x10.ProfileResponse\\\"\\x00\\x12\\x33\\n\\x0cget_net_proc\\x12\\x0f.NetProcRequest\\x1a\\x10.NetProcResponse\\\"\\x00\\x42\\x03\\x80\\x01\\x01\\x62\\x06proto3')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())", "\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'distributed_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  DESCRIPTOR._serialized_options = b'\\200\\001\\001'\n  _TRAFFICREQUEST._serialized_start=21\n  _TRAFFICREQUEST._serialized_end=51\n  _TRAFFICRESPONSE._serialized_start=53\n  _TRAFFICRESPONSE._serialized_end=106\n  _RESREQUEST._serialized_start=108\n  _RESREQUEST._serialized_end=149\n  _RESRESPONSE._serialized_start=151\n  _RESRESPONSE._serialized_end=180\n  _PROFILEREQUEST._serialized_start=182\n  _PROFILEREQUEST._serialized_end=212\n  _PROFILERESPONSE._serialized_start=214\n  _PROFILERESPONSE._serialized_end=267\n  _NETPROCREQUEST._serialized_start=269\n  _NETPROCREQUEST._serialized_end=299\n  _NETPROCRESPONSE._serialized_start=301\n  _NETPROCRESPONSE._serialized_end=354\n  _GRPCSERVICE._serialized_start=357\n  _GRPCSERVICE._serialized_end=569", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "client/client.py", "chunked_list": ["#! /usr/bin/env python\n# coding=utf8\nimport grpc\nimport time,json\nimport client.distributed_pb2_grpc as distributed_pb2_grpc\nimport  client.distributed_pb2  as distributed_pb2\n\ndef rpc_get_traffic(ip_str):\n    '''\n    get the traffic \n    '''\n    time1=time.time()\n    conn=grpc.insecure_channel(ip_str)\n    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n    request = distributed_pb2.TrafficRequest(data=ip_str)\n    response = client.get_traffic(request)\n    res_net=response.result\n\n    return res_net", "\ndef rpc_adjust_res(ip_str,uids,value):\n    '''\n    adjust resource from client\n    :return:\n    '''\n    conn=grpc.insecure_channel(ip_str)\n    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n    # map_res is the resource allocated\n    \n    request = distributed_pb2.ResRequest(uids=uids,value=value)\n    response = client.adjustRes(request)\n    return response.result", "    # print(\"func2 received:\",response.result)\ndef rpc_get_profile(ip_str,svc_name):\n    '''\n    get the cpu use from client\n    '''\n    time1=time.time()\n    conn=grpc.insecure_channel(ip_str)\n    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n    request = distributed_pb2.ProfileRequest(data=svc_name)\n    response = client.get_profile(request)\n    res_net=response.result\n    return res_net", "def rpc_get_net_proc(ip_str):\n    '''\n    get the in traffic of every interface of network\n    :return:\n    '''\n    time1=time.time()\n    conn=grpc.insecure_channel(ip_str)\n    client = distributed_pb2_grpc.GrpcServiceStub(channel=conn)\n    request = distributed_pb2.NetProcRequest(data=ip_str)\n    response = client.get_net_proc(request)\n    res_str=response.result\n    res_net=res_str.split(',')\n\n    return res_net", "def run():\n    # test function\n    count=0\n    test=10\n    ip_str='10.3.64.4:50052'\n    # time.sleep(2)\n    while count<test:\n        count+=1\n        str1=rpc_get_traffic(ip_str)\n        print(str1)\n        time.sleep(1)", "    # func3(1)\nif __name__ == '__main__':\n    run()\n"]}
{"filename": "client/__init__.py", "chunked_list": [""]}
{"filename": "client/Resset.py", "chunked_list": ["import os\nfrom collections import defaultdict\nfrom client.client import rpc_adjust_res\nimport pandas\nimport time\nfrom multiprocessing import Process\nimport datetime\n\n\n\ndef setProfile(ip_str,uids,value):\n    return rpc_adjust_res(ip_str,uids,value)", "\n\ndef setProfile(ip_str,uids,value):\n    return rpc_adjust_res(ip_str,uids,value)\n\n\nif __name__ == '__main__':\n    # setProfile(\"10.2.64.8:50052\")\n    print(\"Resset\")"]}
{"filename": "client/distributed_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\n\nimport client.distributed_pb2 as distributed__pb2\n\n\nclass GrpcServiceStub(object):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.get_traffic = channel.unary_unary(\n                '/GrpcService/get_traffic',\n                request_serializer=distributed__pb2.TrafficRequest.SerializeToString,\n                response_deserializer=distributed__pb2.TrafficResponse.FromString,\n                )\n        self.adjustRes = channel.unary_unary(\n                '/GrpcService/adjustRes',\n                request_serializer=distributed__pb2.ResRequest.SerializeToString,\n                response_deserializer=distributed__pb2.ResResponse.FromString,\n                )\n        self.get_profile = channel.unary_unary(\n                '/GrpcService/get_profile',\n                request_serializer=distributed__pb2.ProfileRequest.SerializeToString,\n                response_deserializer=distributed__pb2.ProfileResponse.FromString,\n                )\n        self.get_net_proc = channel.unary_unary(\n                '/GrpcService/get_net_proc',\n                request_serializer=distributed__pb2.NetProcRequest.SerializeToString,\n                response_deserializer=distributed__pb2.NetProcResponse.FromString,\n                )", "\n\nclass GrpcServiceServicer(object):\n    \"\"\"\n    \"\"\"\n\n    def get_traffic(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def adjustRes(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def get_profile(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def get_net_proc(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')", "\n\ndef add_GrpcServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'get_traffic': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_traffic,\n                    request_deserializer=distributed__pb2.TrafficRequest.FromString,\n                    response_serializer=distributed__pb2.TrafficResponse.SerializeToString,\n            ),\n            'adjustRes': grpc.unary_unary_rpc_method_handler(\n                    servicer.adjustRes,\n                    request_deserializer=distributed__pb2.ResRequest.FromString,\n                    response_serializer=distributed__pb2.ResResponse.SerializeToString,\n            ),\n            'get_profile': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_profile,\n                    request_deserializer=distributed__pb2.ProfileRequest.FromString,\n                    response_serializer=distributed__pb2.ProfileResponse.SerializeToString,\n            ),\n            'get_net_proc': grpc.unary_unary_rpc_method_handler(\n                    servicer.get_net_proc,\n                    request_deserializer=distributed__pb2.NetProcRequest.FromString,\n                    response_serializer=distributed__pb2.NetProcResponse.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'GrpcService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))", "\n\n # This class is part of an EXPERIMENTAL API.\nclass GrpcService(object):\n    \"\"\"\n    \"\"\"\n\n    @staticmethod\n    def get_traffic(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_traffic',\n            distributed__pb2.TrafficRequest.SerializeToString,\n            distributed__pb2.TrafficResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def adjustRes(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/adjustRes',\n            distributed__pb2.ResRequest.SerializeToString,\n            distributed__pb2.ResResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def get_profile(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_profile',\n            distributed__pb2.ProfileRequest.SerializeToString,\n            distributed__pb2.ProfileResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def get_net_proc(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/GrpcService/get_net_proc',\n            distributed__pb2.NetProcRequest.SerializeToString,\n            distributed__pb2.NetProcResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)", ""]}
{"filename": "LoadGenerator/LoadGenerator-dis-hr.py", "chunked_list": ["import requests,os\nimport math\nimport random\nimport time\nimport numpy as np\nimport sys\nfrom threading import Thread\nfrom multiprocessing import Process,Manager\nimport argparse\nimport subprocess", "import argparse\nimport subprocess\nimport pandas\n\nparser = argparse.ArgumentParser(description='--head,--qps,--node_size')\n \nparser.add_argument('-head','--head', type=int, default=1)\nparser.add_argument('-q','--qps', type=int, default=3750)\nparser.add_argument('-n','--node_size', type=int, default=3)\nparser.add_argument('-d','--duration', type=int, default=30)", "parser.add_argument('-n','--node_size', type=int, default=3)\nparser.add_argument('-d','--duration', type=int, default=30)\nparser.add_argument('-p','--process', type=int, default=20)\nparser.add_argument('-t','--types', type=int, default=4)\nparser.add_argument('-nodeName','--nodeName', type=str, default='cpu-02')\n\ncg_rate_list_all=[  \n                  [],\n                  [1,1,1,1,1],\n                  [0.5,0.5,1,1,2],", "                  [1,1,1,1,1],\n                  [0.5,0.5,1,1,2],\n                  [0.5,0.5,1.5,0.5,2],\n                  [1,0.5,1,0.5,2],\n                  [1.5,0.5,0.5,0.5,2]]\ncg_rate_list=[]\n\ndata=\"http://10.102.103.173:5000/\"\nnode_list=['cpu-02','cpu-09','cpu-10']\n\ndef search(dynamic_rate):\n    # random data generation\n    in_date=random.randint(9,23)\n    # out_date=random.randint(in_date+1,24)\n    out_date=in_date+1\n    in_date_str=str(in_date)\n    if(in_date<=9):\n        in_date_str=\"2015-04-0\"+in_date_str\n    else:\n        in_date_str=\"2015-04-\"+in_date_str\n    out_date_str=str(out_date)\n    if(out_date<=9):\n        out_date_str=\"2015-04-0\"+out_date_str\n    else:\n        out_date_str=\"2015-04-\"+out_date_str\n    lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n    lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n    # req_param=\"rates\"\n    coin=random.random()\n    if(coin<dynamic_rate):# search by distance\n        req_param=\"nearby\"\n    else:#search by rate\n        req_param=\"rates\"\n    # else:           #search by rate in certain disctance\n    #     req_param=\"all\"\n    # generate url and params\n    url=data+\"hotels\"\n    params={\n        \"inDate\":in_date_str,\n        \"outDate\":out_date_str,\n        \"lat\":str(lat),\n        \"lon\":str(lon),\n        \"require\":req_param,\n    }\n    return url,params", "node_list=['cpu-02','cpu-09','cpu-10']\n\ndef search(dynamic_rate):\n    # random data generation\n    in_date=random.randint(9,23)\n    # out_date=random.randint(in_date+1,24)\n    out_date=in_date+1\n    in_date_str=str(in_date)\n    if(in_date<=9):\n        in_date_str=\"2015-04-0\"+in_date_str\n    else:\n        in_date_str=\"2015-04-\"+in_date_str\n    out_date_str=str(out_date)\n    if(out_date<=9):\n        out_date_str=\"2015-04-0\"+out_date_str\n    else:\n        out_date_str=\"2015-04-\"+out_date_str\n    lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n    lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n    # req_param=\"rates\"\n    coin=random.random()\n    if(coin<dynamic_rate):# search by distance\n        req_param=\"nearby\"\n    else:#search by rate\n        req_param=\"rates\"\n    # else:           #search by rate in certain disctance\n    #     req_param=\"all\"\n    # generate url and params\n    url=data+\"hotels\"\n    params={\n        \"inDate\":in_date_str,\n        \"outDate\":out_date_str,\n        \"lat\":str(lat),\n        \"lon\":str(lon),\n        \"require\":req_param,\n    }\n    return url,params", "\n#hotel-benchmark \"recommend\" generate url with random data\ndef recommend(dynamic_rate):\n    # random data generation\n    coin=random.random()\n    if coin<dynamic_rate:\n        coin_=random.random()\n        if(coin_<0.33):# best fit shortest distance\n            req_param=\"dis\"\n        elif(coin_<0.66):#best fit hightest rate\n            req_param=\"rate\"\n        else:#best fit lowest price\n            req_param=\"price\"\n        lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n        lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n        # generate url and params\n        url=data+\"recommendations\"\n        params={\n            \"require\":str(req_param),\n            \"lat\":str(lat),\n            \"lon\":str(lon)\n        }\n    else:\n        req_param=\"overall\"\n        lat = 38.0235 + (random.uniform(0, 481) - 240.5)/1000.0\n        lon = -122.095 + (random.uniform(0, 325) - 157.0)/1000.0\n        id = random.randint(0, 500)\n        user_name = \"Cornell_\"+str(id)\n        # generate url and params\n        url=data+\"recommendations\"\n        params={\n            \"require\":str(req_param),\n            \"lat\":str(lat),\n            \"lon\":str(lon),\n            \"username\":user_name\n        }        \n    return url, params", "\n#hotel-benchmark \"user\" generate url with random data\ndef user():\n    # random data generation\n    id = random.randint(0, 500)\n    user_name = \"Cornell_\"+str(id)\n    pass_word = \"\"\n    for i in range(3):\n        pass_word=pass_word+str(id)\n    # generate url and params\n    url=data+\"user\"\n    params={\n        \"username\":str(user_name),\n        \"password\":str(pass_word)\n    }\n    return url, params", "\n#hotel-benchmark \"reserve\" generate url with random data\ndef reserve():\n    # random data generation\n    in_date=random.randint(9,23)\n    out_date=in_date+random.randint(1,5)\n    in_date_str=str(in_date)\n    if(in_date<=9):\n        in_date_str = \"2015-04-0\"+in_date_str \n    else:\n        in_date_str = \"2015-04-\"+in_date_str\n    out_date_str=str(out_date)\n    if(out_date<=9):\n        out_date_str = \"2015-04-0\"+out_date_str \n    else:\n        out_date_str = \"2015-04-\"+out_date_str\n    hotel_id=str(random.randint(1,80))\n    id = random.randint(0, 500)\n    user_name = \"Cornell_\"+str(id)\n    pass_word = \"\"\n    for i in range(10):\n        pass_word=pass_word+str(id)\n    cust_name=user_name\n    num_room=\"1\"\n    # generate url and params\n    url=data+\"reservation\"\n    params={\n        \"inDate\":in_date_str,\n        \"outDate\":out_date_str,\n        \"lat\":\"\",\n        \"lon\":\"\",\n        \"hotelId\":hotel_id,\n        \"customerName\":cust_name,\n        \"username\":user_name,\n        \"password\":pass_word,\n        \"number\":num_room\n    }\n    return url, params", "\ndef generate_gaussian_arraival():\n    # np.random.normal(4,0.08,250)#mu,sigma,sampleNo\n    dis_list=[]\n    with open(\"GD_norm.csv\") as f:#use already generated data before\n        for line in f:\n            dis_list.append(float(line.replace(\"\\n\",\"\")))\n    return dis_list\n\n#post a request\ndef post_request(url, params,list_99):\n    response=requests.get(url=url,params=params,headers={'Connection':'close'},timeout=(10,10))\n    list_99.append([time.time(),response.elapsed.total_seconds()*1000])", "\n#post a request\ndef post_request(url, params,list_99):\n    response=requests.get(url=url,params=params,headers={'Connection':'close'},timeout=(10,10))\n    list_99.append([time.time(),response.elapsed.total_seconds()*1000])\n\n# generate requests with dynamic graphs\ndef dynamic_graph_rate(dr0,dr1,dr2,dr3,dr4):\n    # ratio for 3 kinds of call graphs\n    cnt=dr0+dr1+dr2+dr3+dr4\n    search_ratio=float(dr0+dr1)/cnt\n    recommend_ratio=float(dr2+dr3)/cnt\n    reserve_ratio=float(dr4)/cnt\n    # for each request, random call graph\n    # print(search_ratio,recommend_ratio,reserve_ratio)\n    coin=random.random()\n    if(coin<search_ratio):\n        url, params=search(float(dr0)/(dr0+dr1))\n    elif(coin<search_ratio+recommend_ratio):\n        url, params=recommend(float(dr2)/(dr2+dr3))\n    else:\n        url, params=reserve()\n    return url, params", "\ndef threads_generation(QPS_list,duraion,process_number,list_99_all): #250,20\n    plist = []\n    query_list = []\n    dis_list = []\n    list_99=[]\n    gs_inter=generate_gaussian_arraival()\n    for j in range(0,duraion):\n        QOS_this_s=int(QPS_list[j]/process_num)\n        for i in range(0,QOS_this_s):\n            url, params=dynamic_graph_rate(cg_rate_list[0],cg_rate_list[1],\n            cg_rate_list[2],cg_rate_list[3],cg_rate_list[4])#determine dynamic call graph\n            p = Thread(target=post_request,args=(url,params,list_99))\n            plist.append(p)\n            dis_list.append(gs_inter[i%250]* 250/QOS_this_s)\n    # print(\"Total %d thread in %d s\"%(len(dis_list),duraion))\n    fun_sleep_overhead=0.000075# overhead to call sleep()function\n    # print(\"begin\")\n    # For each process, control the QPS=250query/s, and apply Gaussian distribution\n    waste_time=0\n    t1=time.time()\n    for i in range(len(plist)):\n        t_s=time.time()#10^-3ms, negligible\n        plist[i].start()\n        t_e=time.time()\n        #[thread start time] + [sleep() funtion time] + [sleep time] = dis_list[i]\n        sleep_time=dis_list[i]/1000-((t_e-t_s)+fun_sleep_overhead)\n        if(sleep_time>0):\n            # can compensate\n            if(sleep_time+waste_time>=0):\n                time.sleep(sleep_time+waste_time)\n                waste_time=0\n            else:\n                waste_time+=sleep_time\n        else:\n            waste_time+=sleep_time\n    t2=time.time()\n    print(\"done, time count is %f,should be %d, waste_time %f\\n\"%(t2-t1,duraion,waste_time))\n    #Control all the requests ends before statistics\n    for item in plist:\n        item.join()\n    list_99_all.append(list_99)", "    # print(np.percentile(np.array(list_99),99))\n\ndef request_test(QPS_list,duraion=20,process_number=20):\n    time1=time.time()\n    assert len(QPS_list)==int(duraion)\n    process_list=[]\n    list_99_all=Manager().list()\n    for i in range(process_number):\n        p=Process(target=threads_generation,args=(QPS_list,duraion,process_number,list_99_all))\n        process_list.append(p)\n    # start processes\n    for p in process_list:\n        p.start()\n    # print(\"All processes start.\")\n    for p in process_list:\n        p.join()\n    print(\"All processes done.Total is %f\"%(time.time()-time1))    \n    latency_this_node=[]\n    for i in list_99_all:\n        latency_this_node.extend(i)\n    for i in range(len(latency_this_node)):\n        latency_this_node[i][0]-=time1\n    latency_this_node=sorted(latency_this_node,key=(lambda x:x[0]),reverse=False)  \n    df=pandas.DataFrame(latency_this_node,columns=['time','latency'])\n    df.to_csv(f'hr_{args.nodeName}_latency.csv') ", "            \ndef run_on_node(cmd):\n    os.system(cmd)\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    if args.head:\n        o_process_list=[]\n        for i in range(args.node_size):\n            cmd=\" ssh {} 'ulimit -n 100000;cd /home/jcshi/LoadGenerator;python3 LoadGenerator-dis-hr.py --head {} --qps {} --node_size {} -p {} -t {} -nodeName {} -d {}' \"\\\n                .format(node_list[i],0,args.qps,args.node_size,args.process,args.types,node_list[i],args.duration)\n            print(cmd)\n            # run_on_node(cmd)\n            p=Process(target=run_on_node,args=(cmd,))\n            o_process_list.append(p)\n        for p in o_process_list:\n            p.start()\n        for p in o_process_list:\n            p.join()\n    else:\n        print(args)\n        os.system(\"ulimit -n 100000\")\n        QPS=int(args.qps/args.node_size)\n        duraion=args.duration\n        process_num=args.process\n        cg_rate_list=cg_rate_list_all[args.types]\n        QPS_list=[]\n        for i in range(duraion):\n            QPS_list+=[QPS]\n        print(QPS_list)\n        request_test(QPS_list,duraion,process_num)"]}
{"filename": "LoadMonitor/NetTrafficMonitor_temp.py", "chunked_list": ["import os\nimport time\nimport copy\nimport math\nimport sys\nsys.path.append(\"..\") \nfrom client.client import rpc_get_net_proc\n\nPodNodeMapper={}\nNodeIpMapper={", "PodNodeMapper={}\nNodeIpMapper={\n    \"cpu-03\":\"10.2.64.3:50052\",\n    \"cpu-04\":\"10.2.64.4:50052\",\n    \"cpu-07\":\"10.2.64.7:50052\",\n    \"cpu-08\":\"10.2.64.8:50052\",\n}\n\nclass Node:\n    def __init__(self,name,in_traffic,out_traffic,upper_traffic,back_traffic,children):\n        self.name=name\n        self.in_traffic=in_traffic\n        self.out_traffic=out_traffic\n        self.upper_traffic=upper_traffic\n        self.back_traffic=back_traffic\n        self.children=children\n    def display(self):\n        print(self.name,self.in_traffic,self.out_traffic,self.upper_traffic,self.back_traffic,self.children)", "class Node:\n    def __init__(self,name,in_traffic,out_traffic,upper_traffic,back_traffic,children):\n        self.name=name\n        self.in_traffic=in_traffic\n        self.out_traffic=out_traffic\n        self.upper_traffic=upper_traffic\n        self.back_traffic=back_traffic\n        self.children=children\n    def display(self):\n        print(self.name,self.in_traffic,self.out_traffic,self.upper_traffic,self.back_traffic,self.children)", "\n\ndef buildDAG():\n    profilemmc=Node(\"memcached-profile\",0,0,0,0,[])\n    ratemmc=Node(\"memcached-rate\",0,0,0,0,[])\n    reservemmc=Node(\"memcached-reserve\",0,0,0,0,[])\n    reservemongodb=Node(\"mongodb-reservation\",0,0,0,0,[])\n    checkmmc=Node(\"memcached-check\",0,0,0,0,[])\n    rank_category=Node(\"rank-category\",0,0,0,0,[])\n    rank_overall=Node(\"rank-overall\",0,0,0,0,[])\n    recommend=Node(\"recommendation\",0,0,0,0,[rank_category,rank_overall])\n    geo=Node(\"geo\",0,0,0,0,[])\n    user=Node(\"user\",0,0,0,0,[])    \n    profile=Node(\"profile\",0,0,0,0,[profilemmc])\n    rate=Node(\"rate\",0,0,0,0,[ratemmc])\n    search=Node(\"search\",0,0,0,0,[geo,rate])\n    check=Node(\"check\",0,0,0,0,[checkmmc])\n    reserve=Node(\"reservation\",0,0,0,0,[reservemmc,reservemongodb])\n    frontendSearch=Node(\"frontend-search\",0,0,0,0,[search,check])\n    frontendRecommend=Node(\"frontend-recommend\",0,0,0,0,[recommend,profile])\n    frontendReserve=Node(\"frontend-reserve\",0,0,0,0,[user,reserve])\n    EnteringMS=Node(\"entering-ms\",0,0,0,0,[frontendSearch,frontendRecommend,frontendReserve])\n    return EnteringMS", "\ndef postOrder(Access_Record,root):\n    if not root:\n        return\n    for target in root.children:\n        postOrder(Access_Record,target)\n    Access_Record.append(root)\n\nms_interace_mapper={}\n\ndef ms_interace():\n    global ms_interace_mapper, PodNodeMapper\n    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n    for i in range(len(pods)):\n        pod=pods[i].replace(\"\\n\",\"\")\n        node=nodes[i].replace(\"\\n\",\"\")\n        if(pod==\"NAME\"):\n            continue\n        id=os.popen(\"kubectl exec -i \"+pod+\" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n        print(pod,id)\n        idtarget=\"^'\"+id+\":\"+\" \"+\"'\"\n        if(node==\"cpu-06\"):\n            temp=os.popen(\"ip link show | grep \"+idtarget+\" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n        else:\n            temp=os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget+\"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n        index=temp.find('@')\n        iplink=temp[:index]\n        temp=pod.split(\"-\")\n        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n        value=pod.replace(extra,\"\")\n        if(value==\"memcached-reservation\"):\n            value=\"memcached-reserve\"\n        ms_interace_mapper[iplink]=value\n        PodNodeMapper[value]=node", "ms_interace_mapper={}\n\ndef ms_interace():\n    global ms_interace_mapper, PodNodeMapper\n    pods=os.popen(\"kubectl get pods -o wide | awk '{print $1}'\").readlines()\n    nodes=os.popen(\"kubectl get pods -o wide | awk '{print $7}'\").readlines()\n    for i in range(len(pods)):\n        pod=pods[i].replace(\"\\n\",\"\")\n        node=nodes[i].replace(\"\\n\",\"\")\n        if(pod==\"NAME\"):\n            continue\n        id=os.popen(\"kubectl exec -i \"+pod+\" -- cat /sys/class/net/eth0/iflink\").readlines()[0].replace(\"\\n\", \"\")\n        print(pod,id)\n        idtarget=\"^'\"+id+\":\"+\" \"+\"'\"\n        if(node==\"cpu-06\"):\n            temp=os.popen(\"ip link show | grep \"+idtarget+\" | awk '{print $2}'\").readlines()[0].replace(\"\\n\", \"\")\n        else:\n            temp=os.popen(\"ssh \"+node+\" 'ip link show | grep \"+idtarget+\"'\").readlines()[0].replace(\"\\n\", \"\").split(\" \")[1]\n        index=temp.find('@')\n        iplink=temp[:index]\n        temp=pod.split(\"-\")\n        extra=\"-\"+temp[-2]+\"-\"+temp[-1]\n        value=pod.replace(extra,\"\")\n        if(value==\"memcached-reservation\"):\n            value=\"memcached-reserve\"\n        ms_interace_mapper[iplink]=value\n        PodNodeMapper[value]=node", "\ndef getTraffic():\n    global ms_interace_mapper\n    traffic_mapper={}\n    res=[]\n    for k,v in NodeIpMapper.items():\n        res_rpc=rpc_get_net_proc(v)\n        res.extend(res_rpc)\n    for i in range(2,len(res)):\n        line=res[i].replace(\"\\n\",\"\").split(\" \")\n        while '' in line:\n            line.remove('')\n        interface=line[0].replace(\":\",\"\")\n        if(interface in ms_interace_mapper.keys()):\n            if(ms_interace_mapper[interface] not in traffic_mapper.keys()):\n                traffic_mapper[ms_interace_mapper[interface]]=[int(line[9]),int(line[1])]\n            else:\n                traffic_mapper[ms_interace_mapper[interface]][0]+=int(line[9])\n                traffic_mapper[ms_interace_mapper[interface]][1]+=int(line[1])\n    return traffic_mapper", "\ndef calTrafficRate(traffic_mapper_old,traffic_mapper_new,duration):\n    this_sample_rate={}\n    for key in traffic_mapper_old:\n        in_traffic_rate=(traffic_mapper_new[key][0]-traffic_mapper_old[key][0])/duration/1000000*8\n        out_traffic_rate=(traffic_mapper_new[key][1]-traffic_mapper_old[key][1])/duration/1000000*8\n        this_sample_rate[key]=[in_traffic_rate,out_traffic_rate,duration]\n    return this_sample_rate\n\ndef calUpperandBacktrafic(this_Access_Record,this_sample_rate):\n    for target in this_Access_Record:\n        target.in_traffic=this_sample_rate[target.name][0]\n        target.out_traffic=this_sample_rate[target.name][1]\n        target.upper_traffic=target.in_traffic\n        target.back_traffic=target.out_traffic\n        for child in target.children:\n            target.upper_traffic-=child.back_traffic\n            target.back_traffic-=child.upper_traffic\n    return this_Access_Record", "\ndef calUpperandBacktrafic(this_Access_Record,this_sample_rate):\n    for target in this_Access_Record:\n        target.in_traffic=this_sample_rate[target.name][0]\n        target.out_traffic=this_sample_rate[target.name][1]\n        target.upper_traffic=target.in_traffic\n        target.back_traffic=target.out_traffic\n        for child in target.children:\n            target.upper_traffic-=child.back_traffic\n            target.back_traffic-=child.upper_traffic\n    return this_Access_Record", "\ndef runTrafficMonitor(Access_Record):\n    traffic_mapper_old=getTraffic()\n    t_old=time.time()\n    while(True):\n        time.sleep(1)\n        traffic_mapper_new=getTraffic()\n        t_new=time.time()\n        this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n        this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n        for i in range(len(this_Access_Record)):\n            print(this_Access_Record[i].name,this_Access_Record[i].upper_traffic,this_sample_rate['entering-ms'][2])\n        print()\n        traffic_mapper_old=traffic_mapper_new\n        t_old=t_new", "\ndef runOneTime():\n    root=buildDAG()\n    Access_Record=[]\n    postOrder(Access_Record,root)\n    ms_interace()\n    traffic_mapper_old=getTraffic()\n    t_old=time.time()\n    time.sleep(5)\n    traffic_mapper_new=getTraffic()\n    t_new=time.time()\n    this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n    this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n    return this_Access_Record", "\nif __name__ == \"__main__\":\n    root=buildDAG()\n    Access_Record=[]\n    postOrder(Access_Record,root)\n    for target in Access_Record:\n        print(target.name)\n    ms_interace()\n    runTrafficMonitor(Access_Record)\n", ""]}
{"filename": "LoadMonitor/NetTrafficMonitor.py", "chunked_list": ["import os\nimport time\nimport copy\nimport math\nimport sys\nsys.path.append(\"..\") \nfrom client.client import rpc_get_net_proc\n\ntraffic_mapper_old={}\nt_old=0", "traffic_mapper_old={}\nt_old=0\n\n#get traffic data based on network interface\n#every 1 second\ndef getTraffic(ms_dag):\n    ms_interface_mapper=ms_dag.ms_interface_mapper\n    traffic_mapper={}\n    res=[]\n    for k,v in ms_dag.NodeIpMapper.items():\n        res_rpc=rpc_get_net_proc(v)\n        res.extend(res_rpc)\n    for i in range(2,len(res)):\n        line=res[i].replace(\"\\n\",\"\").split(\" \")\n        while '' in line:\n            line.remove('')\n        interface=line[0].replace(\":\",\"\")\n        if(interface in ms_interface_mapper.keys()):\n            if(ms_interface_mapper[interface] not in traffic_mapper.keys()):\n                traffic_mapper[ms_interface_mapper[interface]]=[int(line[9]),int(line[1])]\n            else:\n                traffic_mapper[ms_interface_mapper[interface]][0]+=int(line[9])\n                traffic_mapper[ms_interface_mapper[interface]][1]+=int(line[1])\n    return traffic_mapper", "\n#call traffic difference\ndef calTrafficRate(traffic_mapper_old,traffic_mapper_new,duration):\n    this_sample_rate={}\n    for key in traffic_mapper_old:\n        in_traffic_rate=(traffic_mapper_new[key][0]-traffic_mapper_old[key][0])/duration/1000000*8\n        out_traffic_rate=(traffic_mapper_new[key][1]-traffic_mapper_old[key][1])/duration/1000000*8\n        this_sample_rate[key]=[in_traffic_rate,out_traffic_rate,duration]\n    return this_sample_rate\n", "\n#Post order update based on structure\ndef calUpperandBacktrafic(ms_dag,this_sample_rate):\n    for target in ms_dag.Access_Record:\n        target.in_traffic=this_sample_rate[target.name][0]\n        target.out_traffic=this_sample_rate[target.name][1]\n        target.upper_traffic=target.in_traffic\n        target.back_traffic=target.out_traffic\n        for child in target.children:\n            target.upper_traffic-=child.back_traffic\n            target.back_traffic-=child.upper_traffic\n            #deal with fluncations\n            if(target.upper_traffic<0):\n                target.upper_traffic=0\n            if(target.back_traffic<0):\n                target.back_traffic=0", "\n#For initialization\ndef initialize(ms_dag):\n    global traffic_mapper_old,t_old\n    traffic_mapper_old=getTraffic(ms_dag)\n    t_old=time.time()\n    print(t_old,traffic_mapper_old)\n    # print(\"==========\")\n\ndef runOneTime(ms_dag):\n    global t_old,traffic_mapper_old\n    # print(t_old,traffic_mapper_old)\n    traffic_mapper_new=getTraffic(ms_dag)\n    t_new=time.time()\n    # print(t_new,traffic_mapper_new)\n    this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n    # this_Access_Record=calUpperandBacktrafic(copy.deepcopy(ms_dag.Access_Record),this_sample_rate)\n    calUpperandBacktrafic(ms_dag,this_sample_rate)\n    # print(ms_dag.Access_Record[11].upper_traffic,this_sample_rate['entering-ms'][2])\n    # update the last record\n    t_old=t_new\n    traffic_mapper_old=traffic_mapper_new\n    return this_sample_rate['entering-ms'][2]", "\ndef runOneTime(ms_dag):\n    global t_old,traffic_mapper_old\n    # print(t_old,traffic_mapper_old)\n    traffic_mapper_new=getTraffic(ms_dag)\n    t_new=time.time()\n    # print(t_new,traffic_mapper_new)\n    this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n    # this_Access_Record=calUpperandBacktrafic(copy.deepcopy(ms_dag.Access_Record),this_sample_rate)\n    calUpperandBacktrafic(ms_dag,this_sample_rate)\n    # print(ms_dag.Access_Record[11].upper_traffic,this_sample_rate['entering-ms'][2])\n    # update the last record\n    t_old=t_new\n    traffic_mapper_old=traffic_mapper_new\n    return this_sample_rate['entering-ms'][2]", "\n#For separable test\ndef runTrafficMonitor(Access_Record):\n    traffic_mapper_old=getTraffic()\n    t_old=time.time()\n    while(True):\n        time.sleep(1)\n        traffic_mapper_new=getTraffic()\n        t_new=time.time()\n        this_sample_rate=calTrafficRate(traffic_mapper_old,traffic_mapper_new,t_new-t_old)\n        this_Access_Record=calUpperandBacktrafic(copy.deepcopy(Access_Record),this_sample_rate)\n        # obtain upper traffic\n        print(this_Access_Record[11].name,this_Access_Record[11].upper_traffic,this_sample_rate['entering-ms'][2])\n        # temp_Record.append(this_Access_Record[11].upper_traffic)\n        traffic_mapper_old=traffic_mapper_new\n        t_old=t_new", "    # print(len(temp_Record))"]}
