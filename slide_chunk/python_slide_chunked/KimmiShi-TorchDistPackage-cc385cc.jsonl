{"filename": "setup.py", "chunked_list": ["# from distutils.core import setup,find_packages\nfrom setuptools import find_packages, setup\n\nsetup(name='torchdistpackage',\n      version='0.1',\n      description='TorchDistPackage',\n      author='KimmiShi',\n      author_email='',\n      url='https://github.com/KimmiShi/TorchDistPackage',\n      packages=find_packages(include=['torchdistpackage', 'torchdistpackage.*'])", "      url='https://github.com/KimmiShi/TorchDistPackage',\n      packages=find_packages(include=['torchdistpackage', 'torchdistpackage.*'])\n     )"]}
{"filename": "torchdistpackage/__init__.py", "chunked_list": ["from .ddp.naive_ddp import NaiveDDP, moe_dp_iter_step, create_moe_dp_hooks\nfrom .ddp.zero_optim import Bf16ZeroOptimizer\n\n# from .ddp.torch_py_ddp import PythonDDP\n\nfrom .dist.launch_from_slurm import setup_distributed_slurm\n\nfrom .dist.process_topo import torch_parallel_context as tpc\nfrom .dist.process_topo import test_comm, is_using_pp\n", "from .dist.process_topo import test_comm, is_using_pp\n\nfrom .utils import fix_rand\n"]}
{"filename": "torchdistpackage/utils.py", "chunked_list": ["import torch\nimport os\n\ndef fix_rand(rank=0):\n    import numpy as np\n    import random\n\n    seed = 2222 + rank\n    print(f\"Setting random seed to {seed}\")\n\n    # PyTorch random number generator (for cpu and cuda)\n    torch.manual_seed(seed)\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    # python random\n    random.seed(seed)\n\n    # numpy RNG\n    np.random.seed(seed)\n\n    # cuda benchmarking\n    torch.backends.cudnn.benchmark = False\n\n    # deterministic algos\n    # torch.use_deterministic_algorithms(True)\n\n    # cudnn conv deterministic\n    torch.backends.cudnn.deterministic = True\n\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.enabled = False\n    torch.random.manual_seed(seed)", ""]}
{"filename": "torchdistpackage/ddp/naive_ddp.py", "chunked_list": ["import os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed import ReduceOp\n\n\n__all__ = [\"NaiveDDP\"]", "\n__all__ = [\"NaiveDDP\"]\n\n\nclass NaiveDDP(torch.nn.Module):\n    r\"\"\" NaiveDDP wraps torch.nn.Module with distribued data parallel support\n\n    Args:\n        module (torch.nn.Module, required):\n            model used for computing.\n        sync (boolean, default: False):\n            True -> the gradient allreduce will happen after backward;\n            False -> the gradient allreduce will overlap with backward.\n        gradient_as_bucket_view: bucket grads\n        process_group: DP process group\n        dp_rank0: the first rank (rank0) of dp process group, when used with Model Parallel, rank0 is not always equal to '0'\n        reduce_op: 'avg' or 'sum\n        kwargs:\n            num_grad_acc_iter: only do reduce grad after backward for num_grad_acc_iter times\n\n    Note: for bucket, a warmup iter is needed to build up bucket, and correctly reduce grads\n    \"\"\"\n\n    def __init__(\n        self,\n        module,\n        sync=False,\n        bucket_cap_mb=25,\n        gradient_as_bucket_view=False,\n        process_group=None,\n        dp_rank0=0,\n        reduce_op=\"avg\",\n        **kwargs,\n    ):\n        super(NaiveDDP, self).__init__()\n        self.module = module\n\n        if hasattr(module, \"_ddp_params_and_buffers_to_ignore\"):\n            self.parameters_to_ignore = module._ddp_params_and_buffers_to_ignore\n        else:\n            self.parameters_to_ignore = []\n\n        self.group = process_group or None\n        self.dp_rank0 = dp_rank0\n        self.reduce_op = ReduceOp.SUM if reduce_op.lower == \"sum\" else ReduceOp.AVG\n\n        # Holds all_reduce handles, used when async_reduction is True\n        # self.async_handles = set()\n\n        self.broadcast_params()\n\n        self.sync = sync\n        self.gradient_as_bucket_view = gradient_as_bucket_view\n        self.bucket_cap_bytes = bucket_cap_mb * 1024 * 1024\n        self.buckets = {}\n        self.buckets_idx = 0\n\n        self.num_iter = 0\n        # self.lock = Lock()\n\n        self.reduce_time = 0.0\n        self.hook_time = 0.0\n        self.verbose = kwargs.get(\"verbose\", False)\n\n        self.num_grad_acc_iter = kwargs.get(\"num_grad_acc_iter\", 1)\n        self.grad_reduce_cnts = {}\n\n        self.reduce_stream = torch.cuda.Stream()\n        if not sync and dist.get_world_size(self.group) > 1:\n            self._grad_accs = []\n            self._register_hooks()\n\n    def forward(self, *inputs, **kwargs):\n        return self.module(*inputs, **kwargs)\n\n    def _register_hooks(self):\n        for i, (name, p) in enumerate(self.module.named_parameters()):\n            if p.requires_grad and name not in self.parameters_to_ignore:\n                if dist.get_world_size(self._get_group(name, p)) <= 1:\n                    continue\n\n                p_tmp = p.expand_as(p)\n                grad_acc = p_tmp.grad_fn.next_functions[0][0]\n                grad_acc.register_hook(self._make_hook(name, p, i))\n                self._grad_accs.append(grad_acc)  # ! very important\n\n    def _get_group(self, name, param):\n        return self.group\n\n    def sync_comm(self):\n        beg = time.perf_counter()\n        self.reduce_stream.synchronize()\n\n        self.reduce_time += time.perf_counter() - beg\n\n    def _reduce_grads(self, grad, group, name):\n        if self.sync:\n            dist.all_reduce(grad, group=group, op=self.reduce_op)\n        else:\n            if self.grad_reduce_cnts.get(name, 0) < self.num_grad_acc_iter - 1:\n                self.grad_reduce_cnts[name] = self.grad_reduce_cnts.get(name, 0) + 1\n                return\n            # beg = time.perf_counter()\n            stream = self.reduce_stream\n            stream.wait_stream(torch.cuda.current_stream())\n            # self.reduce_time += time.perf_counter()-beg\n\n            # handle = dist.all_reduce(grad, group=self.group, async_op=True, op=self.reduce_op)\n            # self.async_handles.add(handle)\n            with torch.cuda.stream(self.reduce_stream):\n                try:\n                    dist.all_reduce(\n                        grad, group=self.group, async_op=False, op=self.reduce_op\n                    )\n                except Exception as e:\n                    import pdb\n\n                    pdb.set_trace()\n                    print(\"Exception at _reduce_grads\")\n\n    def reduce_dispatch(self, name, p, idx=None):\n        should_bucket = (\n            lambda grad: grad.element_size() * grad.numel()\n            < self.bucket_cap_bytes * 4 // 5\n        )\n        if self.gradient_as_bucket_view and should_bucket(p.grad):\n            # if param has no \"bucket\", assign a 'bucket' to this param\n            if not hasattr(p, \"grad_bucket\"):\n                bucket_info = (p.grad.dtype, p.grad.device, self._get_group(name, p))\n\n                bucket = None\n                # if bucket_info exists, get a existing bucket\n                if bucket_info in self.buckets:\n                    bucket = self.buckets[bucket_info]\n\n                # if no existing bucket or bucket cannot hold current param, create a new bucket\n                if not bucket or not bucket.can_fit(p.grad):\n                    bucket = self.buckets[bucket_info] = GradBucket(\n                        f\"grad_bucket_{self.buckets_idx}\",\n                        self.bucket_cap_bytes,\n                        p.grad.element_size(),\n                        bucket_info,\n                    )\n                    self.buckets_idx += 1\n\n                p.grad = bucket.push(name, p.grad)\n                p.grad_bucket = bucket\n\n                # launch a reduce every time a new tensor comes\n                self._reduce_grads(\n                    p.grad.data, self._get_group(name, p), \"bucket_warmup\"\n                )\n\n                # we should remove the full buckets from self to make sure that bucket is not resued?\n                #       not needed, since the bucket will be full, and will be replaced by next new bucket\n\n            else:  # if param already has a 'bucket', mark current param ready, and if bucket is ready, reduce the bucket\n                bucket = p.grad_bucket\n                if bucket.grad_ready():\n                    self._reduce_grads(bucket.data, bucket.group, bucket.name)\n                    bucket.grad_reset()\n        else:\n            self._reduce_grads(p.grad.data, self._get_group(name, p), name)\n\n    def _make_hook(self, name, p, i):\n        def hook(*ignore):\n            # make grad thread safe\n            # with self.lock:\n\n            self.reduce_dispatch(name, p, i)\n\n        return hook\n\n    def _reset_iter(self):\n        if self.verbose and dist.get_rank(self.group) == 0:\n            print(\n                \"rank:\",\n                dist.get_rank(),\n                \" Total Reduce of last iter: \",\n                self.reduce_time,\n            )\n        self.num_iter += 1\n        self.reduce_time = 0.0\n\n        # clear grad reduce cnt every iter\n        for key in self.grad_reduce_cnts:\n            self.grad_reduce_cnts[key] = 0\n\n    def reduce_gradients(self):\n        \"\"\" call this after a iter, to reudce grads and sync \"\"\"\n\n        # no need sync when not distributed\n        if dist.get_world_size(self.group) <= 1:\n            return\n\n        beg = time.perf_counter()\n\n        if self.sync:\n            for i, (name, param) in enumerate(self.module.named_parameters()):\n                if (\n                    name not in self.parameters_to_ignore\n                    and param.requires_grad\n                    and param.grad is not None\n                ):\n                    if dist.get_world_size(self._get_group(name, param)) <= 1:\n                        continue\n                    self.reduce_dispatch(name, param, i)\n        # else:\n        #     for handle in self.async_handles:\n        #         handle.wait()\n        #     self.async_handles.clear()\n\n        torch.cuda.synchronize()\n        self.reduce_time += time.perf_counter() - beg\n\n        self._reset_iter()\n\n    def broadcast_params(self):\n        \"\"\" broadcast model parameters \"\"\"\n        for name, param in self.module.state_dict().items():\n            if name not in self.parameters_to_ignore:\n                dist.broadcast(param, self.dp_rank0, group=self._get_group(name, param))", "\n\nclass MoEDP(torch.nn.Module):\n    r\"\"\" NaiveDDP wraps torch.nn.Module with distribued data parallel support\n\n    Args:\n        module (torch.nn.Module, required):\n            model used for computing.\n        sync (boolean, default: False):\n            True -> the gradient allreduce will happen after backward;\n            False -> the gradient allreduce will overlap with backward.\n        gradient_as_bucket_view: bucket grads\n        process_group: DP process group\n        dp_rank0: the first rank (rank0) of dp process group, when used with Model Parallel, rank0 is not always equal to '0'\n        reduce_op: 'avg' or 'sum\n        kwargs:\n            num_grad_acc_iter: only do reduce grad after backward for num_grad_acc_iter times\n\n    Note: for bucket, a warmup iter is needed to build up bucket, and correctly reduce grads\n    \"\"\"\n\n    def __init__(\n        self,\n        expert_params,\n        sync=True,\n        bucket_cap_mb=25,\n        gradient_as_bucket_view=False,\n        process_group=None,\n        dp_rank0=0,\n        reduce_op=\"avg\",\n        **kwargs,\n    ):\n        super(MoEDP, self).__init__()\n        self.expert_params = expert_params\n\n        self.group = process_group\n        self.dp_rank0 = dp_rank0\n        self.reduce_op = ReduceOp.SUM if reduce_op.lower == \"sum\" else ReduceOp.AVG\n\n        self.broadcast_params()\n\n        self.sync = sync\n        self.gradient_as_bucket_view = gradient_as_bucket_view\n        self.bucket_cap_bytes = bucket_cap_mb * 1024 * 1024\n        self.buckets = {}\n        self.buckets_idx = 0\n\n        self.num_iter = 0\n        # self.lock = Lock()\n\n        # Holds all_reduce handles, used when async_reduction is True\n        self.async_handles = set()\n        self.use_sync_handle = True\n\n        self.reduce_time = 0.0\n        self.hook_time = 0.0\n        self.verbose = kwargs.get(\"verbose\", False)\n\n        self.num_grad_acc_iter = kwargs.get(\"num_grad_acc_iter\", 1)\n        self.grad_reduce_cnts = {}\n\n        self.reduce_stream = torch.cuda.Stream()\n        if dist.get_world_size(self.group) > 1:\n            self._grad_accs = []\n            self._register_hooks()\n\n    def forward(self, *inputs, **kwargs):\n        return self.module(*inputs, **kwargs)\n\n    def broadcast_params(self):\n        \"\"\" broadcast model parameters \"\"\"\n        for param in self.expert_params.values():\n            dist.broadcast(param, self.dp_rank0, group=self.group)\n\n    def _register_hooks(self):\n        for name, p in self.expert_params.items():\n            if p.requires_grad:\n                p_tmp = p.expand_as(p)\n                grad_acc = p_tmp.grad_fn.next_functions[0][0]\n                grad_acc.register_hook(self._make_hook(name, p))\n                self._grad_accs.append(grad_acc)  # ! very important\n\n    def _reduce_grads(self, grad, group, name):\n        if self.sync:\n            dist.all_reduce(grad, group=group, op=self.reduce_op)\n        elif self.use_sync_handle:\n            if self.grad_reduce_cnts.get(name, 0) < self.num_grad_acc_iter - 1:\n                self.grad_reduce_cnts[name] = self.grad_reduce_cnts.get(name, 0) + 1\n                return\n\n            # handle = dist.all_reduce(grad, group=group, async_op=True, op=self.reduce_op)\n            # self.async_handles.add(handle)\n        else:\n            stream = self.reduce_stream\n            stream.wait_stream(torch.cuda.current_stream())\n\n            with torch.cuda.stream(self.reduce_stream):\n                try:\n                    dist.all_reduce(\n                        grad, group=self.group, async_op=False, op=self.reduce_op\n                    )\n                except Exception as e:\n                    import pdb\n\n                    pdb.set_trace()\n                    print(\"Exception at _reduce_grads\")\n\n    def reduce_dispatch(self, name, p):\n        should_bucket = (\n            lambda grad: grad.element_size() * grad.numel()\n            < self.bucket_cap_bytes * 4 // 5\n        )\n        if self.gradient_as_bucket_view and should_bucket(p.grad):\n            # if param has no \"bucket\", assign a 'bucket' to this param\n            if not hasattr(p, \"grad_bucket\"):\n                bucket_info = (p.grad.dtype, p.grad.device, self.group)\n\n                bucket = None\n                # if bucket_info exists, get a existing bucket\n                if bucket_info in self.buckets:\n                    bucket = self.buckets[bucket_info]\n\n                # if no existing bucket or bucket cannot hold current param, create a new bucket\n                if not bucket or not bucket.can_fit(p.grad):\n                    bucket = self.buckets[bucket_info] = GradBucket(\n                        f\"grad_bucket_{self.buckets_idx}\",\n                        self.bucket_cap_bytes,\n                        p.grad.element_size(),\n                        bucket_info,\n                    )\n                    self.buckets_idx += 1\n\n                p.grad = bucket.push(name, p.grad)\n                p.grad_bucket = bucket\n\n                # launch a reduce every time a new tensor comes\n                self._reduce_grads(p.grad.data, self.group, \"bucket_warmup\")\n\n                # we should remove the full buckets from self to make sure that bucket is not resued?\n                #       not needed, since the bucket will be full, and will be replaced by next new bucket\n\n            else:  # if param already has a 'bucket', mark current param ready, and if bucket is ready, reduce the bucket\n                bucket = p.grad_bucket\n                if bucket.grad_ready():\n                    self._reduce_grads(bucket.data, bucket.group, bucket.name)\n                    bucket.grad_reset()\n        else:\n            self._reduce_grads(p.grad.data, self.group, name)\n\n    def _make_hook(self, name, p):\n        def hook(*ignore):\n            # make grad thread safe\n            # with self.lock:\n\n            self.reduce_dispatch(name, p)\n\n        return hook\n\n    def reduce_gradients(self):\n        \"\"\" call this after a iter, to reudce grads and sync \"\"\"\n\n        # no need sync when not distributed\n        if dist.get_world_size(self.group) <= 1:\n            return\n\n        if self.sync:\n            for name, param in self.expert_params.items():\n                if param.requires_grad and param.grad is not None:\n                    if dist.get_world_size(self.group) <= 1:\n                        continue\n                    self.reduce_dispatch(name, param, i)\n        else:\n            for handle in self.async_handles:\n                handle.wait()\n            self.async_handles.clear()\n\n        # clear grad reduce cnt every iter\n        for key in self.grad_reduce_cnts:\n            self.grad_reduce_cnts[key] = 0\n\n        torch.cuda.synchronize()", "\n\nmoe_dp_mod = None\n\n\ndef moe_dp_iter_step():\n    global moe_dp_mod\n    moe_dp_mod.reduce_gradients()\n\n\ndef create_moe_dp_hooks(\n    params: dict,\n    moe_dp_group,\n    moe_dp_rank0,\n    overlap_comm=True,\n    reduce_op=\"avg\",\n    sync=False,\n    num_grad_acc_iter=1,\n):\n    global moe_dp_mod\n    moe_dp_mod = MoEDP(\n        params,\n        sync=sync,\n        process_group=moe_dp_group,\n        dp_rank0=moe_dp_rank0,\n        reduce_op=reduce_op,\n        gradient_as_bucket_view=True,\n        num_grad_acc_iter=num_grad_acc_iter,\n    )\n    return moe_dp_mod", "\n\ndef create_moe_dp_hooks(\n    params: dict,\n    moe_dp_group,\n    moe_dp_rank0,\n    overlap_comm=True,\n    reduce_op=\"avg\",\n    sync=False,\n    num_grad_acc_iter=1,\n):\n    global moe_dp_mod\n    moe_dp_mod = MoEDP(\n        params,\n        sync=sync,\n        process_group=moe_dp_group,\n        dp_rank0=moe_dp_rank0,\n        reduce_op=reduce_op,\n        gradient_as_bucket_view=True,\n        num_grad_acc_iter=num_grad_acc_iter,\n    )\n    return moe_dp_mod", "\n\nclass GradBucket(object):\n    def __init__(self, name, size, element_size, bucket_info):\n        self.element_size = element_size\n        self.numel = size // self.element_size\n        self.name = name\n        self.dtype, self.device, self.group = bucket_info\n        self.data = torch.zeros(self.numel, dtype=self.dtype, device=self.device)\n\n        self.offset = 0\n        self.grads = []\n        self.ready = 0\n\n    def get_aligned_size(self, tensor):\n        aligned_size = (tensor.element_size() * tensor.numel() + 2 ** 9 - 1) & ~(\n            2 ** 9 - 1\n        )\n        assert aligned_size % tensor.element_size() == 0\n        return aligned_size // tensor.element_size()\n\n    def grad_ready(self):\n        self.ready += 1\n        return self.ready >= len(self.grads)\n\n    def grad_reset(self):\n        self.ready = 0\n\n    def can_fit(self, grad):\n        return self.offset + self.get_aligned_size(grad) <= self.numel\n\n    def push(self, name, grad):\n        new_grad = self.data.narrow(0, self.offset, grad.numel()).view_as(grad)\n        new_grad.copy_(grad)\n        self.offset += self.get_aligned_size(grad)\n        self.grads.append(name)\n        return new_grad", ""]}
{"filename": "torchdistpackage/ddp/zero_optim.py", "chunked_list": ["# A simple zero impl that:\n#  1. shards the opt states\n#  2. shards the grads\n#  Supports bf16 only\n\n\n# work flow of bf16 optim:\n#   model param in bf16\n#   -> grads in bf16\n#   reduce and remove grads not needed in current partition", "#   -> grads in bf16\n#   reduce and remove grads not needed in current partition\n#   copy grads to fp32\n#   optim updates fp32 copy of param using fp32 grad\n#   update fp16 param using fp32 param\n\nimport torch\nimport torch.distributed as dist\nimport math\n\ndef partition_params(params, num_partitions, numel_per_partition):\n    \"\"\"partitions params\n\n    Args:\n        params (list): the complete list of params to partition\n        num_partitions (int): zero dp world size\n        numel_per_partition (int): max number of param cnt\n\n    Returns:\n        list: list of partitions\n    \"\"\"\n    partitions = []\n    elcnt = 0\n    partition_id = 0\n    for ind in range(num_partitions):\n        partitions.append([])\n    for param in params:\n        partitions[partition_id].append(param)\n        elcnt+=param.numel()\n        if elcnt > numel_per_partition:\n            partition_id+=1\n            elcnt=0\n    return partitions", "import math\n\ndef partition_params(params, num_partitions, numel_per_partition):\n    \"\"\"partitions params\n\n    Args:\n        params (list): the complete list of params to partition\n        num_partitions (int): zero dp world size\n        numel_per_partition (int): max number of param cnt\n\n    Returns:\n        list: list of partitions\n    \"\"\"\n    partitions = []\n    elcnt = 0\n    partition_id = 0\n    for ind in range(num_partitions):\n        partitions.append([])\n    for param in params:\n        partitions[partition_id].append(param)\n        elcnt+=param.numel()\n        if elcnt > numel_per_partition:\n            partition_id+=1\n            elcnt=0\n    return partitions", "\nFREE_BUFFERS = []\n\nclass Bucket():\n    def __init__(self, dtype, size, reduce_stream, dp_group, reduce_op) -> None:\n        self.capacity = size\n        self.reduce_stream = reduce_stream#torch.cuda.Stream()\n        self.dp_group = dp_group\n        self.reduce_op = reduce_op\n        self.params_in_bucket = []\n        self.numel_in_bucket = 0\n\n        global FREE_BUFFERS\n        if len(FREE_BUFFERS) > 0:\n            self.buffer = FREE_BUFFERS.pop(0)\n        else:\n            self.buffer = torch.empty(self.capacity, dtype=dtype).cuda()\n\n    def try_hold(self, param, param_hook):\n        self.param_hook = param_hook\n        if param.grad.numel() > self.capacity - self.numel_in_bucket:\n            self.reduce()\n            return False\n        else:\n            self.push(param)\n            return True\n\n    def push(self, param):\n        self.params_in_bucket.append(param)\n        self.numel_in_bucket+=param.numel()\n\n    def reduce(self):\n        param_list = self.params_in_bucket\n        self.reduce_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(self.reduce_stream):\n            pos = 0\n            for param in param_list:\n                slice = self.buffer.narrow(0, pos, param.grad.numel())\n                slice.copy_(param.grad.view(-1))\n                # set param.grad back to avoid copy after allreduce\n                param.grad = slice.view_as(param.grad)\n                pos+=param.grad.numel()\n            dist.all_reduce(\n                self.buffer, group=self.dp_group, async_op=False, op=self.reduce_op\n            )\n            pos=0\n            # copy reduced grads back, and do master grad update\n            for param in param_list:\n                self.param_hook(param)\n\n            # clean up\n            global FREE_BUFFERS\n            FREE_BUFFERS.append(self.buffer.zero_())\n            self.buffer = None", "\n\nclass Bf16ZeroOptimizer():\n    \"\"\"Usage:\n        1. wrap original optimizer:\n            `optimizer = Bf16ZeroOptimizer(optimizer, bf16_master_weights=True, overlap_comm=True)`\n        2. use wrapped optim like orignal one\n\n       **Note**:\n            1. bf16_master_weights=True is not compatible with bucketize,\n                since bucketize requires an copy of master grad\n    \"\"\"\n    def __init__(self, optim, dp_group=None, bf16_master_weights=False, overlap_comm=False, stage=2,\n                 bucket_size=5e8, bucketize=True) -> None:\n        self.optim = optim\n        self.dp_group = dp_group\n        self.bf16_master_weights = bf16_master_weights\n        self.partition_grad = stage==2\n        self.overlap_comm=overlap_comm\n        self.reduce_bucket_size = int(bucket_size)\n        self.bucketize = bucketize\n\n        self.reduce_stream = torch.cuda.Stream() if overlap_comm else torch.cuda.current_stream()\n        self.reduce_op = dist.ReduceOp.AVG\n        self.grad_accs = []\n\n        if torch.distributed.is_initialized():\n            self.partition_id = dist.get_rank(self.dp_group)\n            num_partitions = dist.get_world_size(self.dp_group)\n        else:\n            self.partition_id = 0\n            num_partitions = 1\n        self.num_partitions = num_partitions\n\n        self.all_param_groups_partitions = []\n        self.bit16_params_shard_groups = []\n        self.master_weight_shard_groups = []\n        self.bf16_param_id_in_partition = set()\n        self.bf16_param_to_master_weight_map = dict()\n        self.param2rank = dict()\n\n        self.original_dtype = optim.param_groups[0]['params'][0].dtype\n\n        if self.bucketize:\n            self.working_bucket = self.create_bucket()\n\n        for param_group in self.optim.param_groups:\n            trainable_parameters = [param for param in param_group['params'] if param.requires_grad]\n            total_num_elements = sum([p.numel() for p in trainable_parameters])\n            target_partition_numel = math.ceil(total_num_elements//num_partitions)\n            all_partitions = partition_params(trainable_parameters, num_partitions, target_partition_numel)\n            self.all_param_groups_partitions.append(all_partitions)\n            params_in_cur_partition = all_partitions[self.partition_id]\n            self.bit16_params_shard_groups.append(params_in_cur_partition)\n\n            # build param id to rank map\n            for rank, partition in enumerate(all_partitions):\n                for param in partition:\n                    self.param2rank[id(param)] = rank\n\n            for param in params_in_cur_partition:\n                self.bf16_param_id_in_partition.add(id(param))\n\n            if bf16_master_weights:\n                self.master_weight_shard_groups.append(params_in_cur_partition)\n            else:\n                fp32_params_shard = [p.clone().detach().float() for p in params_in_cur_partition]\n\n                for ind in range(len(params_in_cur_partition)):\n                    self.bf16_param_to_master_weight_map[id(params_in_cur_partition[ind])] = fp32_params_shard[ind]\n                #in case the internal optimizer needs it\n                for p in fp32_params_shard:\n                    p.requires_grad = True\n\n                self.master_weight_shard_groups.append(fp32_params_shard)\n\n\n            # update optim's param group\n            param_group['params'] = self.master_weight_shard_groups[-1]\n\n            for ind,param in enumerate(trainable_parameters):\n                    def wrapper(param, ind):\n                        param_tmp = param.expand_as(param)\n                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\n                        def reduce_partition_and_remove_grads(*notneeded):\n                            if self.bucketize:\n                                reduce_and_remove_grad_bucketized(param)\n                            else:\n                                reduce_and_remove_grad(param)\n\n                        grad_acc.register_hook(reduce_partition_and_remove_grads)\n                        self.grad_accs.append(grad_acc)\n                    wrapper(param, ind)\n\n        print(\"Bf16ZeroOptimizer initialized.\")\n        # create hook that does reduce & remove grad\n        def reduce_and_remove_grad(param):\n            if self.num_partitions > 1:\n                self.reduce_stream.wait_stream(torch.cuda.current_stream())\n                dst_rank = 0\n                with torch.cuda.stream(self.reduce_stream):\n                    # dist.all_reduce(\n                    #     param.grad.data, group=self.dp_group, async_op=False, op=self.reduce_op\n                    # )\n\n                    # single reduce, might be more efficientcy than all-reduce\n                    dst_rank = self.param2rank[id(param)]\n                    dist.reduce(param.grad.data, dst_rank, group=self.dp_group, async_op=False, op=self.reduce_op)\n\n                    self.copy2master_or_free(param)\n\n        def reduce_and_remove_grad_bucketized(param):\n            self.bucket_reduce_helper(param)\n\n    def create_bucket(self):\n        return Bucket(self.original_dtype, self.reduce_bucket_size, self.reduce_stream,\n                      self.dp_group, self.reduce_op)\n\n    def copy2master_or_free(self, param):\n        # copy to master if needed and free 16bit grad\n        if id(param) in self.bf16_param_id_in_partition:\n            if not self.bf16_master_weights:\n                master_weight = self.bf16_param_to_master_weight_map[id(param)]\n                if master_weight.grad is None:\n                    master_weight.grad = param.grad.clone().detach().to(master_weight.dtype)\n                else:\n                    master_weight.grad.data.copy_(param.grad.data)\n                if self.partition_grad:\n                    # free 16bit grad\n                    param.grad = None\n        elif self.partition_grad:\n            param.grad = None\n\n\n    def single_reduce_and_remove(self, param):\n        if self.overlap_comm:\n            self.reduce_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(self.reduce_stream):\n            dist.all_reduce(\n                param.grad, group=self.dp_group, async_op=False, op=self.reduce_op\n            )\n            self.copy2master_or_free(param)\n\n\n    def bucket_reduce_helper(self, param):\n        # for extra large param, just launch reduce\n        if param.numel() > self.reduce_bucket_size:\n            self.single_reduce_and_remove(param)\n        else:\n            # let bucket to the reduce\n            if self.working_bucket.try_hold(param, self.copy2master_or_free):\n                pass\n            else:\n                self.working_bucket = self.create_bucket()\n                assert self.working_bucket.try_hold(param, self.copy2master_or_free)\n\n    def finish_bucket(self):\n        self.working_bucket.reduce()\n        # reset working bucket for next iter\n        self.working_bucket = self.create_bucket()\n\n    def step(self):\n        # 0. finish not reduced bucket\n        if self.bucketize:\n            self.finish_bucket()\n\n        self.reduce_stream.synchronize()\n\n        # 1. param update of single partition\n        self.optim.step()\n\n        # and relase master grad\n        if not self.bf16_master_weights:\n            for pg in self.master_weight_shard_groups:\n                for master_param in pg:\n                    master_param.grad=None\n\n        # 2. update bf16 param with fp32 param in current partition\n        if not self.bf16_master_weights:\n            for ind in range(len(self.bit16_params_shard_groups)):\n                for param_ind in range(len(self.bit16_params_shard_groups[ind])):\n                    self.bit16_params_shard_groups[ind][param_ind].data.copy_(self.master_weight_shard_groups[ind][param_ind])\n        # 3. all-gather bit16 params\n        #    do this by broadcast\n        if self.num_partitions ==1:\n            return\n        for param_partitions in self.all_param_groups_partitions:\n            for partition_id in range(self.num_partitions):\n                partition = param_partitions[partition_id]\n                # broadcast partition from rank partition_id to the rest\n                for param in partition:\n                    dist.broadcast(param.data, partition_id, self.dp_group)\n\n    def zero_grad(self):\n        # self.optim.zero_grad()\n        for pg_partitions in self.all_param_groups_partitions:\n            for parition in pg_partitions:\n                for p in parition:\n                    if p.grad is not None:\n                        p.grad.zero_()\n\n\n    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n    def _get_state(self):\n        return self.optim.state\n\n    def _set_state(self, value):\n        self.optim.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optim.param_groups\n\n    def _set_param_groups(self, value):\n        self.optim.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)", "\n\n\n"]}
{"filename": "torchdistpackage/ddp/__init__.py", "chunked_list": [""]}
{"filename": "torchdistpackage/parallel/__init__.py", "chunked_list": ["from .pipeline_parallel.pipeline_sched import forward_backward, forward_eval\nfrom .pipeline_parallel.pipeline_helper import partition_uniform, flatten_model\n\n\nfrom .tensor_parallel.transformer import ParallelBlock,Block\nfrom .tensor_parallel.attn import Attention, TpAttention\nfrom .tensor_parallel.mlp import Mlp, TpMlp\nfrom .tensor_parallel.tp_utils import *"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/attn.py", "chunked_list": ["import torch\nfrom torch import nn as nn\n\n\nfrom .tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n    gather_from_sequence_parallel_region\n\ndef _split_heads(tensor, num_heads, attn_head_size):\n    \"\"\"\n    Splits hidden_size dim into attn_head_size and num_heads\n    \"\"\"\n    new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n    tensor = tensor.view(new_shape)\n    return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)", "\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        self.qkv = TpLinear(dim, dim * 3, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = TpLinear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n\n    def _naive_attn(self, x):\n        B, N, DIM = x.shape\n\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim=-1)  # make torchscript happy (cannot use tensor as tuple)\n        q= _split_heads(q, self.num_heads, self.head_dim)\n        k= _split_heads(k, self.num_heads, self.head_dim)\n        v= _split_heads(v, self.num_heads, self.head_dim)\n\n        attn = ((q * self.scale) @ k.transpose(-2, -1))\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, DIM)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def forward(self, x):\n        x = self._naive_attn(x)\n        return x", "\n\nclass TpAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.,\n                 tp_group = None, sequence_parallel=False):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n\n        set_tp_group(tp_group)\n        self.tp_size = torch.distributed.get_world_size(get_tp_group())\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.head_num_per_partition = self.num_heads//self.tp_size\n\n        self.qkv = ColParallelLinear(dim, dim * 3, bias=qkv_bias)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n\n        self.proj = RowParallelLinear(dim, dim, sequence_parallel=sequence_parallel)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.sequence_parallel = sequence_parallel\n\n\n    def _naive_attn(self, x):\n        B, N, DIM = x.shape     # DIM=self.head_dim*self.num_heads\n        qkv_out = self.qkv(x)       # B,N,3*DIM\n\n        q, k, v = qkv_out.chunk(3, dim=-1) # B.N.DIM\n        q= _split_heads(q, self.head_num_per_partition, self.head_dim)\n        k= _split_heads(k, self.head_num_per_partition, self.head_dim)\n        v= _split_heads(v, self.head_num_per_partition, self.head_dim)\n\n\n        attn = ((q * self.scale) @ k.transpose(-2, -1))\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, DIM//self.tp_size)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def forward(self, x):\n        if self.sequence_parallel:\n            # assume input tensor is sequence parallel\n            x = gather_from_sequence_parallel_region(x)\n\n        x = self._naive_attn(x)\n        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/mlp.py", "chunked_list": ["import torch\nfrom torch import nn as nn\n\n# from torchdistpackage.parallel import *\nfrom .tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n    gather_from_sequence_parallel_region\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\n        From timm, but modified to run with tensor parallel and sequence parallel.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            tp_group = None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = bias\n\n        set_tp_group(tp_group)\n        self.fc1 = TpLinear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = TpLinear(hidden_features, out_features, bias=bias)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x", "\nclass TpMlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\n        From timm, but modified to run with tensor parallel and sequence parallel.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            tp_group = None,\n            bias=True,\n            drop=0.,\n            sequence_parallel=False\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = bias\n        self.sequence_parallel = sequence_parallel\n\n        set_tp_group(tp_group)\n        self.fc1 = ColParallelLinear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = RowParallelLinear(hidden_features, out_features, bias=bias, sequence_parallel=sequence_parallel)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        if self.sequence_parallel:\n            # assume input tensor is sequence parallel\n            x = gather_from_sequence_parallel_region(x)\n\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/transformer.py", "chunked_list": ["from typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn as nn\n\nfrom .attn import TpAttention,Attention\nfrom .mlp import TpMlp,Mlp\n\nfrom .tp_utils import set_sequence_parallel_attr, gather_from_sequence_parallel_region, maybe_split_into_sequence_parallel\n\nclass Block(nn.Module):\n    def __init__(self, dim, mlp_ratio=4,num_heads=8, **not_used):\n        super().__init__()\n\n        self.ln_1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim, num_heads=num_heads)\n        self.ln_2 = nn.LayerNorm(dim)\n        self.mlp = Mlp(dim, hidden_features=int(dim*mlp_ratio))\n\n    def forward(self, hidden_states):\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_output = self.attn(\n            hidden_states\n        )\n        # residual connection\n        hidden_states = attn_output + residual\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n\n        return hidden_states", "from .tp_utils import set_sequence_parallel_attr, gather_from_sequence_parallel_region, maybe_split_into_sequence_parallel\n\nclass Block(nn.Module):\n    def __init__(self, dim, mlp_ratio=4,num_heads=8, **not_used):\n        super().__init__()\n\n        self.ln_1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim, num_heads=num_heads)\n        self.ln_2 = nn.LayerNorm(dim)\n        self.mlp = Mlp(dim, hidden_features=int(dim*mlp_ratio))\n\n    def forward(self, hidden_states):\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_output = self.attn(\n            hidden_states\n        )\n        # residual connection\n        hidden_states = attn_output + residual\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n\n        return hidden_states", "\n\nclass ParallelBlock(nn.Module):\n    def __init__(self, dim, mlp_ratio=4, num_heads=8, sequence_parallel=False):\n        super().__init__()\n\n        self.ln_1 = nn.LayerNorm(dim)\n        self.attn = TpAttention(dim, num_heads=num_heads, sequence_parallel=sequence_parallel)\n        self.ln_2 = nn.LayerNorm(dim)\n        self.mlp = TpMlp(dim, hidden_features=int(dim*mlp_ratio), sequence_parallel=sequence_parallel)\n        self.sequence_parallel = sequence_parallel\n\n    def forward(self, hidden_states):\n        if self.sequence_parallel:\n            hidden_states = maybe_split_into_sequence_parallel(hidden_states)\n\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n\n        # tp block: gather from seq-p and output seq-p\n        attn_output = self.attn(\n            hidden_states\n        )\n        # residual connection\n        hidden_states = attn_output + residual\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n\n        # tp block: gather from seq-p and output seq-p\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n\n        if self.sequence_parallel:\n            set_sequence_parallel_attr(hidden_states)\n        return hidden_states\n\n    @torch.no_grad()\n    def init_from_full(self, blk):\n        self.mlp.fc2.init_weight_from_full(blk.mlp.fc2.weight)\n        self.mlp.fc1.init_weight_from_full(blk.mlp.fc1.weight)\n        self.attn.qkv.init_weight_from_full_attn(blk.attn.qkv.weight)\n        self.attn.proj.init_weight_from_full(blk.attn.proj.weight)\n\n        # lns\n        self.ln_1.weight.copy_(blk.ln_1.weight)\n        self.ln_1.bias.copy_(blk.ln_1.bias)\n        self.ln_2.weight.copy_(blk.ln_2.weight)\n        self.ln_2.bias.copy_(blk.ln_2.bias)", "\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, mlp_ratio=4, num_heads=8, depth=12, tensor_parallel=True, sequence_parallel=True):\n        super().__init__()\n        blk_type = Block if not tensor_parallel else ParallelBlock\n        self.blocks = nn.ModuleList([blk_type(dim, mlp_ratio=mlp_ratio, num_heads=num_heads, sequence_parallel=sequence_parallel) for _ in range(depth)])\n        self.sequence_parallel = sequence_parallel\n\n    def forward(self, x):\n        for blk in self.blocks:\n            x = blk(x)\n        if self.sequence_parallel:\n            x = gather_from_sequence_parallel_region(x)\n        return x"]}
{"filename": "torchdistpackage/parallel/tensor_parallel/__init__.py", "chunked_list": [""]}
{"filename": "torchdistpackage/parallel/tensor_parallel/tp_utils.py", "chunked_list": ["import torch\nfrom torch import nn as nn\nfrom torch.nn.parameter import Parameter\n\nimport torch.distributed as dist\n\nTP_GROUP=None\ndef get_tp_group():\n    global TP_GROUP\n    return TP_GROUP", "\ndef set_tp_group(group):\n    if group is not None:\n        global TP_GROUP\n        TP_GROUP=group\n\ndef get_tensor_model_parallel_world_size():\n    return dist.get_world_size(get_tp_group())\n\ndef maybe_gather_from_sequence_parallel(inp):\n    if is_squence_parallel_tensor(inp):\n        return gather_from_sequence_parallel_region(inp)\n    return inp", "\ndef maybe_gather_from_sequence_parallel(inp):\n    if is_squence_parallel_tensor(inp):\n        return gather_from_sequence_parallel_region(inp)\n    return inp\n\ndef maybe_split_into_sequence_parallel(inp):\n    if not is_squence_parallel_tensor(inp):\n        return _split_along_first_dim(inp)\n    return inp", "\ndef set_sequence_parallel_attr(inp, value=True):\n    setattr(inp, \"sequence_parallel\", value)\n    return inp\n\ndef is_squence_parallel_tensor(inp):\n    return hasattr(inp, \"sequence_parallel\") and inp.sequence_parallel==True\n\n\n# the following Reduce and Gather functions are adopted from https://github.com/NVIDIA/Megatron-LM\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        torch.distributed.all_reduce(input_, group=get_tp_group())\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output", "\n# the following Reduce and Gather functions are adopted from https://github.com/NVIDIA/Megatron-LM\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        torch.distributed.all_reduce(input_, group=get_tp_group())\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output", "\n\ndef _reduce_scatter_along_first_dim(input_):\n    \"\"\"Reduce-scatter the input tensor across model parallel group.\"\"\"\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    assert dim_size[0] % world_size == 0, \\\n        \"First dimension of the tensor should be divisible by tensor parallel size\"\n\n    dim_size[0] = dim_size[0] // world_size\n\n    output = torch.empty(dim_size, dtype=input_.dtype,\n                         device=torch.cuda.current_device())\n    torch.distributed._reduce_scatter_base(output, input_.contiguous(),\n                                           group=get_tp_group())\n    return output", "\ndef _gather_along_first_dim(input_):\n    \"\"\"Gather tensors and concatinate along the first dimension.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    dim_size[0] = dim_size[0] * world_size\n\n    output = torch.empty(dim_size, dtype=input_.dtype,\n                         device=torch.cuda.current_device())\n    torch.distributed._all_gather_base(output, input_.contiguous(),\n                                       group=get_tp_group())\n\n    return output", "def _split_along_first_dim(input_):\n    \"\"\"Split the tensor along its first dimension and keep the\n    corresponding slice.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along first dimension.\n    dim_size = input_.size()[0]\n    assert dim_size % world_size == 0, \\\n        \"First dimension of the tensor should be divisible by tensor parallel size\"\n    local_dim_size = dim_size // world_size\n    rank = torch.distributed.get_rank(get_tp_group())\n    dim_offset = rank * local_dim_size\n\n    output = input_[dim_offset:dim_offset+local_dim_size].contiguous()\n\n    set_sequence_parallel_attr(output, True)\n    return output", "\nclass _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Reduce scatter the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _reduce_scatter_along_first_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _reduce_scatter_along_first_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather_along_first_dim(grad_output)", "\n\nclass _GatherFromSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from sequence parallel region and concatinate.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, tensor_parallel_output_grad=True):\n        return _gather_along_first_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_, tensor_parallel_output_grad=True):\n        ctx.tensor_parallel_output_grad = tensor_parallel_output_grad\n        return _gather_along_first_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        tensor_parallel_output_grad = ctx.tensor_parallel_output_grad\n\n        # If the computation graph after the gather operation is\n        # in the tensor parallel mode, output gradients need to reduce\n        # scattered and whereas if the computation is duplicated,\n        # output gradients need to be scattered.\n        if tensor_parallel_output_grad:\n            return _reduce_scatter_along_first_dim(grad_output), None\n        else:\n            return _split_along_first_dim(grad_output), None", "\ndef gather_from_sequence_parallel_region(input_, tensor_parallel_output_grad=True):\n    output = _GatherFromSequenceParallelRegion.apply(input_, tensor_parallel_output_grad)\n    set_sequence_parallel_attr(output, False)\n    return output\n\ndef reduce_scatter_to_sequence_parallel_region(input_):\n    out = _ReduceScatterToSequenceParallelRegion.apply(input_)\n    set_sequence_parallel_attr(out)\n    return out", "\n\nclass TpLinear(nn.Module):\n    def __init__(self, fin, fout, bias=True):\n        super(TpLinear, self).__init__()\n        self.weight = Parameter(torch.rand((fin, fout)))\n        self.bias =None\n        if bias:\n            self.bias = Parameter(torch.zeros(fout))\n\n    def forward(self, x):\n        out = torch.matmul(x, self.weight)\n        if self.bias is not None:\n            out +=self.bias\n        return out", "\nclass ColParallelLinear(nn.Module):\n    def __init__(self, fin, fout, bias=True):\n        super(ColParallelLinear, self).__init__()\n        tp_group=get_tp_group()\n        self.tp_world_size = torch.distributed.get_world_size(tp_group)\n        assert fout%self.tp_world_size==0\n        self.fout = int(fout/self.tp_world_size)\n\n        self.linear = TpLinear(fin, self.fout, bias)\n\n    def forward(self, x):\n        \"\"\"\n        1. automatically split fout\n        2. do linear compute\n        3. the output is split , no communication\n        \"\"\"\n        out = self.linear(x)\n        return out\n\n    def init_weight_from_full(self, fullwt):\n        cur_rank = torch.distributed.get_rank(get_tp_group())\n        start_ind = cur_rank*self.fout\n        end_ind = (cur_rank+1)*self.fout\n        slice = fullwt[:,start_ind:end_ind]\n        with torch.no_grad():\n            self.linear.weight.copy_(slice)\n\n    def init_weight_from_full_attn(self, fullwt):\n        cur_rank = torch.distributed.get_rank(get_tp_group())\n        ws = torch.distributed.get_world_size(get_tp_group())\n        dim=fullwt.shape[0]\n        dim3=fullwt.shape[1]\n        fullwts = fullwt.split(dim3//3, dim=-1) # (q,k,v)\n        splits = []\n        for wt in fullwts:\n            splits.append(wt.split(wt.shape[-1]//ws, dim=-1)[cur_rank])\n\n        cat_full = torch.cat(splits, dim=-1)\n\n        with torch.no_grad():\n            self.linear.weight.copy_(cat_full)", "\nclass RowParallelLinear(nn.Module):\n    def __init__(self, fin, fout, bias=True, sequence_parallel=False):\n        super(RowParallelLinear, self).__init__()\n        tp_group=get_tp_group()\n        self.tp_world_size = torch.distributed.get_world_size(tp_group)\n        assert fin%self.tp_world_size==0\n        self.fin = int(fin/self.tp_world_size)\n        self.linear = TpLinear(self.fin, fout, bias)\n        self.sequence_parallel = sequence_parallel\n\n\n    def forward(self, x):\n        \"\"\"\n        1. automatically split fout\n        2. do linear compute\n        3. the output is allreduced\n        \"\"\"\n        out = self.linear(x)\n        if not self.sequence_parallel:\n            out = _ReduceFromModelParallelRegion.apply(out)\n        else:\n            out = reduce_scatter_to_sequence_parallel_region(out)\n        return out\n\n    def init_weight_from_full(self, fullwt):\n        cur_rank = torch.distributed.get_rank(get_tp_group())\n        start_ind = cur_rank*self.fin\n        end_ind = (cur_rank+1)*self.fin\n        slice = fullwt[start_ind:end_ind]\n        with torch.no_grad():\n            self.linear.weight.copy_(slice)", ""]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/comm.py", "chunked_list": ["# Adapted from https://github.com/hpcaitech/ColossalAI\n\nfrom typing import List, Tuple, Union\nimport torch\nimport torch.distributed as dist\n\nfrom torchdistpackage import tpc\nfrom functools import reduce\nimport operator\n", "import operator\n\n\nTensorShape = Union[torch.Size, List[int], Tuple[int]]\n\n\ndef get_current_device() -> torch.device:\n    \"\"\"\n    Returns currently selected device (gpu/cpu).\n    If cuda available, return gpu, otherwise return cpu.\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(f\"cuda:{torch.cuda.current_device()}\")\n    else:\n        return torch.device(\"cpu\")", "\n\ndef send_meta_helper(obj, next_rank, tensor_kwargs):\n    send_shape = torch.tensor(obj.size(), **tensor_kwargs)\n    send_ndims = torch.tensor(len(obj.size()), **tensor_kwargs)\n    dist.send(send_ndims, next_rank)\n    dist.send(send_shape, next_rank)\n\n\ndef send_obj_meta(obj, need_meta=True, next_rank=None) -> bool:\n    \"\"\"Sends obj meta information before sending a specific obj.\n    Since the recipient must know the shape of the obj in p2p communications,\n    meta information of the obj should be sent before communications. This function\n    synchronizes with :func:`recv_obj_meta`.\n\n    Args:\n        obj (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): obj to be sent.\n        need_meta (bool, optional): If False, meta information won't be sent.\n        next_rank (int): The rank of the next member in pipeline parallel group.\n\n    Returns:\n        bool: False\n    \"\"\"\n    if need_meta:\n        if next_rank is None:\n            next_rank = tpc.get_next_global_rank(\"pipe\")\n        # import pdb;pdb.set_trace()\n\n        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n        if isinstance(obj, torch.Tensor):\n            send_obj_nums = torch.tensor(1, **tensor_kwargs)\n            dist.send(send_obj_nums, next_rank)\n            send_meta_helper(obj, next_rank, tensor_kwargs)\n        else:\n            send_obj_nums = torch.tensor(len(obj), **tensor_kwargs)\n            dist.send(send_obj_nums, next_rank)\n            for tensor_to_send in obj:\n                send_meta_helper(tensor_to_send, next_rank, tensor_kwargs)\n\n    return False", "\ndef send_obj_meta(obj, need_meta=True, next_rank=None) -> bool:\n    \"\"\"Sends obj meta information before sending a specific obj.\n    Since the recipient must know the shape of the obj in p2p communications,\n    meta information of the obj should be sent before communications. This function\n    synchronizes with :func:`recv_obj_meta`.\n\n    Args:\n        obj (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): obj to be sent.\n        need_meta (bool, optional): If False, meta information won't be sent.\n        next_rank (int): The rank of the next member in pipeline parallel group.\n\n    Returns:\n        bool: False\n    \"\"\"\n    if need_meta:\n        if next_rank is None:\n            next_rank = tpc.get_next_global_rank(\"pipe\")\n        # import pdb;pdb.set_trace()\n\n        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n        if isinstance(obj, torch.Tensor):\n            send_obj_nums = torch.tensor(1, **tensor_kwargs)\n            dist.send(send_obj_nums, next_rank)\n            send_meta_helper(obj, next_rank, tensor_kwargs)\n        else:\n            send_obj_nums = torch.tensor(len(obj), **tensor_kwargs)\n            dist.send(send_obj_nums, next_rank)\n            for tensor_to_send in obj:\n                send_meta_helper(tensor_to_send, next_rank, tensor_kwargs)\n\n    return False", "\n\ndef recv_meta_helper(prev_rank, tensor_kwargs):\n    recv_ndims = torch.empty((), **tensor_kwargs)\n    dist.recv(recv_ndims, prev_rank)\n    recv_shape = torch.empty(recv_ndims, **tensor_kwargs)\n    dist.recv(recv_shape, prev_rank)\n    return recv_shape\n\n\ndef recv_obj_meta(obj_shape, prev_rank=None) -> torch.Size:\n    \"\"\"Receives obj meta information before receiving a specific obj.\n    Since the recipient must know the shape of the obj in p2p communications,\n    meta information of the obj should be received before communications. This function\n    synchronizes with :func:`send_obj_meta`.\n\n    Args:\n        obj_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the obj to be received.\n        prev_rank (int): The rank of the source of the obj.\n\n    Returns:\n        Union[:class:`torch.Size`, List[:class:`torch.Size`]]: The shape of the obj to be received.\n    \"\"\"\n    if obj_shape is None:\n        if prev_rank is None:\n            prev_rank = tpc.get_prev_global_rank(\"pipe\")\n\n        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n        recv_obj_nums = torch.empty((), **tensor_kwargs)\n        # import pdb;pdb.set_trace()\n\n        dist.recv(recv_obj_nums, prev_rank)\n        if recv_obj_nums.item() == 1:\n            recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n            obj_shape = torch.Size(recv_shape)\n        else:\n            obj_shape = []\n            for i in range(recv_obj_nums.item()):\n                recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n                obj_shape.append(torch.Size(recv_shape))\n\n    return obj_shape", "\n\ndef recv_obj_meta(obj_shape, prev_rank=None) -> torch.Size:\n    \"\"\"Receives obj meta information before receiving a specific obj.\n    Since the recipient must know the shape of the obj in p2p communications,\n    meta information of the obj should be received before communications. This function\n    synchronizes with :func:`send_obj_meta`.\n\n    Args:\n        obj_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the obj to be received.\n        prev_rank (int): The rank of the source of the obj.\n\n    Returns:\n        Union[:class:`torch.Size`, List[:class:`torch.Size`]]: The shape of the obj to be received.\n    \"\"\"\n    if obj_shape is None:\n        if prev_rank is None:\n            prev_rank = tpc.get_prev_global_rank(\"pipe\")\n\n        tensor_kwargs = {\"dtype\": torch.long, \"device\": get_current_device()}\n        recv_obj_nums = torch.empty((), **tensor_kwargs)\n        # import pdb;pdb.set_trace()\n\n        dist.recv(recv_obj_nums, prev_rank)\n        if recv_obj_nums.item() == 1:\n            recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n            obj_shape = torch.Size(recv_shape)\n        else:\n            obj_shape = []\n            for i in range(recv_obj_nums.item()):\n                recv_shape = recv_meta_helper(prev_rank, tensor_kwargs)\n                obj_shape.append(torch.Size(recv_shape))\n\n    return obj_shape", "\n\ndef split_tensor_into_1d_equal_chunks(\n    tensor: torch.Tensor, new_buffer=False\n) -> torch.Tensor:\n    \"\"\"Break a tensor into equal 1D chunks.\n\n    Args:\n        tensor (:class:`torch.Tensor`): Tensor to be split before communication.\n        new_buffer (bool, optional): Whether to use a new buffer to store sliced tensor.\n\n    Returns:\n        :class:`torch.Tensor`: The split tensor\n    \"\"\"\n    partition_size = torch.numel(tensor) // tpc.get_group_size(\"tensor\")\n    start_index = partition_size * tpc.get_group_rank(\"tensor\")\n    end_index = start_index + partition_size\n    if new_buffer:\n        data = torch.empty(\n            partition_size,\n            dtype=tensor.dtype,\n            device=torch.cuda.current_device(),\n            requires_grad=False,\n        )\n        data.copy_(tensor.view(-1)[start_index:end_index])\n    else:\n        data = tensor.view(-1)[start_index:end_index]\n    return data", "\n\ndef gather_split_1d_tensor(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"Opposite of above function, gather values from model parallel ranks.\n\n    Args:\n        tensor (:class:`torch.Tensor`): Tensor to be gathered after communication.\n    Returns:\n        :class:`torch.Tensor`: The gathered tensor.\n    \"\"\"\n    world_size = tpc.get_group_size(\"tensor\")\n    numel = torch.numel(tensor)\n    numel_gathered = world_size * numel\n    gathered = torch.empty(\n        numel_gathered,\n        dtype=tensor.dtype,\n        device=torch.cuda.current_device(),\n        requires_grad=False,\n    )\n    chunks = [gathered[i * numel : (i + 1) * numel] for i in range(world_size)]\n    dist.all_gather(chunks, tensor, group=tpc.get_group(\"tensor\"))\n    return gathered", "\n\ndef _get_tensor_shape(\n    tensor_shape: TensorShape, chunk_tensor: bool = False\n) -> Tuple[TensorShape, bool]:\n    \"\"\"get the exact tensor shape when communicating and return whether the tensor is a chunk\n\n    Args:\n        tensor_shape (:class:`torch.Size`): shape of tensor\n        chunk_tensor (bool, optional): whether to chunk tensor, defaults to False\n\n    Returns:\n        Tuple[Union[:class:`torch.Size`, List[int], Tuple[int]], bool]: exact tensor shape, whether to chunk tensor\n    \"\"\"\n    if chunk_tensor:\n        tensor_chunk_shape = reduce(operator.mul, tensor_shape, 1)\n        tensor_parallel_world_size = tpc.get_group_size(\"tensor\")\n        if tensor_chunk_shape % tensor_parallel_world_size == 0:\n            tensor_chunk_shape = tensor_chunk_shape // tensor_parallel_world_size\n        else:\n            tensor_chunk_shape = tensor_shape\n            chunk_tensor = False\n    else:\n        tensor_chunk_shape = tensor_shape\n    return tensor_chunk_shape, chunk_tensor", "\n\ndef create_recv_buffer_with_shapes(recv_shapes, dtype, scatter_gather_tensors):\n    if isinstance(recv_shapes, torch.Size):\n        recv_chunk_shape, recv_split = _get_tensor_shape(\n            recv_shapes, scatter_gather_tensors\n        )\n        buffer_recv = torch.empty(\n            recv_chunk_shape,\n            requires_grad=True,\n            device=get_current_device(),\n            dtype=dtype,\n        )\n        return buffer_recv, recv_split\n    buffer_recv = []\n    for recv_shape in recv_shapes:\n        recv_chunk_shape, recv_split = _get_tensor_shape(\n            recv_shape, scatter_gather_tensors\n        )\n        tensor_recv = torch.empty(\n            recv_chunk_shape,\n            requires_grad=True,\n            device=get_current_device(),\n            dtype=dtype,\n        )\n        buffer_recv.append(tensor_recv)\n    return buffer_recv, recv_split", "\n\ndef process_object_to_send(object_send, scatter_gather_tensors):\n    if isinstance(object_send, torch.Tensor):\n        send_split = _get_tensor_shape(object_send.shape, scatter_gather_tensors)[1]\n        if send_split:\n            object_send = split_tensor_into_1d_equal_chunks(object_send)\n        return object_send\n\n    object_send_list = []\n    for tensor_send in object_send:\n        send_split = _get_tensor_shape(tensor_send.shape, scatter_gather_tensors)[1]\n        if send_split:\n            object_send_list.append(split_tensor_into_1d_equal_chunks(tensor_send))\n        else:\n            object_send_list.append(tensor_send)\n    object_send = tuple(object_send_list)\n\n    return object_send", "\n\ndef filling_ops_queue(obj, comm_op, comm_rank, ops_queue):\n    if isinstance(obj, torch.Tensor):\n        op_to_add = dist.P2POp(comm_op, obj, comm_rank)\n        ops_queue.append(op_to_add)\n    else:\n        for tensor_to_comm in obj:\n            op_to_add = dist.P2POp(comm_op, tensor_to_comm, comm_rank)\n            ops_queue.append(op_to_add)", "\n\ndef _communicate(\n    object_send_next: Union[torch.Tensor, List[torch.Tensor]] = None,\n    object_send_prev: Union[torch.Tensor, List[torch.Tensor]] = None,\n    recv_prev: bool = False,\n    recv_next: bool = False,\n    recv_prev_shape: Union[torch.Size, List[torch.Size]] = None,\n    recv_next_shape: Union[torch.Size, List[torch.Size]] = None,\n    prev_rank: int = None,\n    next_rank: int = None,\n    dtype: torch.dtype = None,\n    scatter_gather_tensors: bool = False,\n) -> Tuple[Union[torch.Tensor, List[torch.Tensor]]]:\n    \"\"\"\n    Adapted from megatron.p2p_communication.\n    Communicate tensors between stages. Used as helper method in other\n    communication methods that are used in pipeline schedule.\n    Takes the following arguments:\n        object_send_next (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): tensor to send to next rank (no tensor sent if\n                          set to None).\n        object_send_prev (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): tensor to send to prev rank (no tensor sent if\n                          set to None).\n        recv_prev (bool): boolean for whether tensor should be received from\n                   previous rank.\n        recv_next (bool): boolean for whether tensor should be received from\n                   next rank.\n        recv_prev_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the previous stage, defaults to None.\n        recv_next_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): shape of the tensor to be received from the next stage, defaults to None.\n        prev_rank (int): the rank of the previous pipeline stage, defaults to None,\n        next_rank (int): the rank of the next pipeline stage, defaults to None,\n        dtype (torch.dtype): data type of intermediate buffers, defaults to None\n        scatter_gather_tensors (bool): whether to scatter and gather tensor between pipeline stages, defaults to False\n    Returns:\n        Tuple[Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]]: returns tensor_recv_prev, tensor_recv_next\n    \"\"\"\n\n    # Create placeholder tensors for receive in forward and backward directions\n    # if needed.\n    tensor_recv_prev = None\n    tensor_recv_next = None\n\n    if recv_prev:\n        assert recv_prev_shape is not None\n        tensor_recv_prev, recv_prev_split = create_recv_buffer_with_shapes(\n            recv_prev_shape, dtype, scatter_gather_tensors\n        )\n\n    if recv_next:\n        assert recv_next_shape is not None\n        tensor_recv_next, recv_next_split = create_recv_buffer_with_shapes(\n            recv_next_shape, dtype, scatter_gather_tensors\n        )\n\n    if object_send_prev is not None or recv_prev:\n        if prev_rank is None:\n            prev_rank = tpc.get_prev_global_rank(\"pipe\")\n\n    if object_send_next is not None or recv_next:\n        if next_rank is None:\n            next_rank = tpc.get_next_global_rank(\"pipe\")\n\n    if object_send_prev is not None:\n        object_send_prev = process_object_to_send(\n            object_send_prev, scatter_gather_tensors\n        )\n\n    if object_send_next is not None:\n        object_send_next = process_object_to_send(\n            object_send_next, scatter_gather_tensors\n        )\n\n    ops = []\n    if object_send_prev is not None:\n        filling_ops_queue(object_send_prev, dist.isend, prev_rank, ops)\n\n    if tensor_recv_prev is not None:\n        filling_ops_queue(tensor_recv_prev, dist.irecv, prev_rank, ops)\n\n    if tensor_recv_next is not None:\n        filling_ops_queue(tensor_recv_next, dist.irecv, next_rank, ops)\n\n    if object_send_next is not None:\n        filling_ops_queue(object_send_next, dist.isend, next_rank, ops)\n\n    if len(ops) > 0:\n        reqs = dist.batch_isend_irecv(ops)\n        for req in reqs:\n            req.wait()\n    # To protect against race condition when using batch_isend_irecv().\n    torch.cuda.synchronize()\n\n    if recv_prev and recv_prev_split:\n        if isinstance(tensor_recv_prev, torch.Tensor):\n            tensor_recv_prev = (\n                gather_split_1d_tensor(tensor_recv_prev)\n                .view(recv_prev_shape)\n                .requires_grad_()\n            )\n        else:\n            for index in range(len(tensor_recv_prev)):\n                tensor_recv_prev[index] = (\n                    gather_split_1d_tensor(tensor_recv_prev[index])\n                    .view(recv_prev_shape[index])\n                    .requires_grad_()\n                )\n\n    if recv_next and recv_next_split:\n        if isinstance(tensor_recv_next, torch.Tensor):\n            tensor_recv_next = (\n                gather_split_1d_tensor(tensor_recv_next)\n                .view(recv_next_shape)\n                .requires_grad_()\n            )\n        else:\n            for index in range(len(tensor_recv_next)):\n                tensor_recv_next[index] = (\n                    gather_split_1d_tensor(tensor_recv_next[index])\n                    .view(recv_next_shape[index])\n                    .requires_grad_()\n                )\n\n    return tensor_recv_prev, tensor_recv_next", "\n\ndef recv_forward(\n    input_tensor_shape, prev_rank=None, dtype=torch.float, scatter_gather_tensors=False\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Copy the forward output from the previous stage in pipeline as the input tensor of this stage.\n    Args:\n        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n        prev_rank (int, optional): The rank of the source of the tensor.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor or input tensor list.\n    \"\"\"\n    if tpc.is_first_in_pipeline_group():\n        input_tensor = None\n    else:\n        input_tensor, _ = _communicate(\n            recv_prev=True,\n            recv_prev_shape=input_tensor_shape,\n            prev_rank=prev_rank,\n            dtype=dtype,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )\n    return input_tensor", "\n\ndef recv_backward(\n    output_grad_shape, next_rank=None, dtype=torch.float, scatter_gather_tensors=False\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Copy the gradient tensor from the next stage in pipeline as the input gradient of this stage.\n    Args:\n        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n        next_rank (int, optional): The rank of the source of the tensor.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor or gradident tensor list.\n    \"\"\"\n    if tpc.is_last_in_pipeline_group():\n        output_tensor_grad = None\n    else:\n        _, output_tensor_grad = _communicate(\n            recv_next=True,\n            recv_next_shape=output_grad_shape,\n            next_rank=next_rank,\n            dtype=dtype,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )\n    return output_tensor_grad", "\n\ndef send_forward(output_tensor, next_rank=None, scatter_gather_tensors=False) -> None:\n    \"\"\"Sends the input tensor to the next stage in pipeline.\n    Args:\n        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n        next_rank (int, optional): The rank of the recipient of the tensor.\n    \"\"\"\n    if not tpc.is_last_in_pipeline_group():\n        _communicate(\n            object_send_next=output_tensor,\n            next_rank=next_rank,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )", "\n\ndef send_backward(\n    input_tensor_grad, prev_rank=None, scatter_gather_tensors=False\n) -> None:\n    \"\"\"Sends the gradient tensor to the previous stage in pipeline.\n    Args:\n        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent\n        prev_rank (int, optional): The rank of the recipient of the tensor\n    \"\"\"\n    if not tpc.is_first_in_pipeline_group():\n        _communicate(\n            object_send_prev=input_tensor_grad,\n            prev_rank=prev_rank,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )", "\n\ndef send_forward_recv_backward(\n    output_tensor,\n    output_grad_shape,\n    recv_next=True,\n    next_rank=None,\n    dtype=torch.float,\n    scatter_gather_tensors=False,\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Batched communication operation. Sends the input tensor to the\n    next stage in pipeline, while receives the gradient tensor from the\n    next stage in pipeline as the input gradient tensor of this stage.\n    Args:\n        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor.\n    \"\"\"\n    if tpc.is_last_in_pipeline_group():\n        output_tensor_grad = None\n    else:\n        _, output_tensor_grad = _communicate(\n            object_send_next=output_tensor,\n            recv_next=recv_next,\n            recv_next_shape=output_grad_shape,\n            next_rank=next_rank,\n            dtype=dtype,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )\n    return output_tensor_grad", "\n\ndef send_backward_recv_forward(\n    input_tensor_grad,\n    input_tensor_shape,\n    recv_prev=True,\n    prev_rank=None,\n    dtype=torch.float,\n    scatter_gather_tensors=False,\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Batched communication operation. Sends the gradient tensor to the\n    previous stage in pipeline, while receives the output tensor from the\n    previous stage in pipeline as the input of this stage.\n    Args:\n        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor.\n    \"\"\"\n    if tpc.is_first_in_pipeline_group():\n        input_tensor = None\n    else:\n        input_tensor, _ = _communicate(\n            object_send_prev=input_tensor_grad,\n            recv_prev=recv_prev,\n            recv_prev_shape=input_tensor_shape,\n            prev_rank=prev_rank,\n            dtype=dtype,\n            scatter_gather_tensors=scatter_gather_tensors,\n        )\n    return input_tensor", "\n\ndef send_forward_recv_forward(\n    output_tensor,\n    input_tensor_shape,\n    recv_prev=True,\n    prev_rank=None,\n    next_rank=None,\n    dtype=torch.float,\n    scatter_gather_tensors=False,\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Batched communication operation. Sends the input tensor to the\n    next stage in pipeline, while receives the output tensor from the\n    previous stage in pipeline as the input of this stage.\n    Args:\n        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input tensor.\n    \"\"\"\n    input_tensor, _ = _communicate(\n        object_send_next=output_tensor,\n        recv_prev=recv_prev,\n        recv_prev_shape=input_tensor_shape,\n        prev_rank=prev_rank,\n        next_rank=next_rank,\n        dtype=dtype,\n        scatter_gather_tensors=scatter_gather_tensors,\n    )\n    return input_tensor", "\n\ndef send_backward_recv_backward(\n    input_tensor_grad,\n    output_grad_shape,\n    recv_next=True,\n    prev_rank=None,\n    next_rank=None,\n    dtype=torch.float,\n    scatter_gather_tensors=False,\n) -> Union[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"Batched communication operation. Sends the gradient tensor to the\n    previous stage in pipeline, while receives the gradient tensor from the\n    next member in pipeline as the input of this stage.\n    Args:\n        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor to be sent.\n        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor to be received.\n    Returns:\n        Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]: The input gradient tensor.\n    \"\"\"\n    _, output_tensor_grad = _communicate(\n        object_send_prev=input_tensor_grad,\n        recv_next=recv_next,\n        recv_next_shape=output_grad_shape,\n        prev_rank=prev_rank,\n        next_rank=next_rank,\n        dtype=dtype,\n        scatter_gather_tensors=scatter_gather_tensors,\n    )\n    return output_tensor_grad", "\n\ndef send_forward_backward_recv_forward_backward(\n    output_tensor,\n    input_tensor_grad,\n    input_tensor_shape,\n    output_grad_shape,\n    recv_prev=True,\n    recv_next=True,\n    prev_rank=None,\n    next_rank=None,\n    dtype=torch.float,\n    scatter_gather_tensors=False,\n) -> Tuple[Union[torch.Tensor, List[torch.Tensor]]]:\n    \"\"\"Batched communication operation. Sends the input tensor to the next stage in pipeline and\n    the gradient tensor to the previous stage, while receives the input gradient tensor from the\n    next stage and the input tensor from the previous stage.\n    Args:\n        output_tensor (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor sent to the next.\n        input_tensor_grad (Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): Tensor sent to the previous.\n        input_tensor_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor received from the previous.\n        output_grad_shape (Union[:class:`torch.Size`, List[:class:`torch.Size`]]): The shape of the tensor received from the next.\n    Returns:\n        Tuple(Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]], Union[:class:`torch.Tensor`, List[:class:`torch.Tensor`]]): (the input tensor, the input gradient tensor)\n    \"\"\"\n    input_tensor, output_tensor_grad = _communicate(\n        object_send_next=output_tensor,\n        object_send_prev=input_tensor_grad,\n        recv_prev=recv_prev,\n        recv_next=recv_next,\n        recv_prev_shape=input_tensor_shape,\n        recv_next_shape=output_grad_shape,\n        prev_rank=prev_rank,\n        next_rank=next_rank,\n        dtype=dtype,\n        scatter_gather_tensors=scatter_gather_tensors,\n    )\n    return input_tensor, output_tensor_grad", ""]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/pipeline_helper.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom torchdistpackage import tpc\n\n\ndef partition_uniform(flat_sequence, extra_len=0) -> list:\n    \"\"\"\n        accept a list and return a list of Modules\n    \"\"\"\n    rank = tpc.get_group_rank(\"pipe\")\n    world_size = tpc.get_group_size(\"pipe\")\n    leng = len(flat_sequence) + extra_len\n    length = leng // world_size\n    beg = rank * length\n    end = (rank + 1) * length if rank != world_size - 1 else len(flat_sequence)\n    kept_flat_sequence = flat_sequence[beg:end]\n    return kept_flat_sequence", "\n\ndef _heap_addition(weights: list, intervals: list, add_cnt: int):\n    import heapq\n\n    def _heap_push(heap, st, ed):\n        value = weights[ed - 1]\n        if st > 0:\n            value -= weights[st - 1]\n        heapq.heappush(heap, (-value, st, ed))\n\n    ret_intervals = []\n    heap = []\n\n    for st, ed in intervals:\n        _heap_push(heap, st, ed)\n\n    while add_cnt > 0:\n        _, st, ed = heapq.heappop(heap)\n        if ed - st == 1:\n            ret_intervals.append((st, ed))\n        else:\n            l, m, r = _binary_partition(weights, st, ed)\n            _heap_push(heap, l, m)\n            _heap_push(heap, m, r)\n            add_cnt -= 1\n\n    while heap:\n        _, st, ed = heapq.heappop(heap)\n        ret_intervals.append((st, ed))\n\n    ret_intervals.sort()\n    return ret_intervals", "\n\ndef _calc_partitions(weights, value):\n    prev = 0\n    prefix = 0\n    num_block = 0\n    intervals = []\n\n    for idx, w in enumerate(weights):\n        if weights[idx] - prefix > value:\n            intervals.append((prev, idx))\n            prev = idx\n            prefix = weights[idx - 1]\n            num_block += 1\n\n    intervals.append((prev, len(weights)))\n    return num_block + 1, intervals", "\n\ndef _binary_search(weights, num):\n    length = len(weights)\n    prefix = [1 if w == 0 else w for w in weights]\n    for i in range(1, length):\n        prefix[i] += prefix[i - 1]\n\n    lower_bound = max(weights)\n    upper_bound = prefix[length - 1]\n\n    while upper_bound > lower_bound:\n        mid = (upper_bound + lower_bound) // 2\n        number, _ = _calc_partitions(prefix, mid)\n        if number <= num:\n            upper_bound = mid\n        else:\n            lower_bound = mid + 1\n\n    num_block, intervals = _calc_partitions(prefix, upper_bound)\n    if num_block < num:\n        intervals = _heap_addition(prefix, intervals, num - num_block)\n\n    return intervals", "\n\ndef partition_balanced(flat_sequence, sequence, **kwargs):\n    rank = tpc.get_group_rank(\"pipe\")\n    world_size = tpc.get_group_size(\"pipe\")\n    assert len(flat_sequence) >= world_size\n\n    def count_params(model):\n        param_count = 0\n        for param in model.parameters():\n            param_count += param.numel()\n        if param_count == 0:\n            param_count = 1\n        return param_count\n\n    weight_sequence = [count_params(item) for item in flat_sequence]\n    del count_params\n    intervals = _binary_search(weight_sequence, world_size)\n    sequence = flat_sequence[intervals[rank][0] : intervals[rank][1]]\n    return sequence", "\n\ndef flatten_sequence(sequence, level=1):\n    \"\"\"\n        Flatten a give model of type nn.Sequential, since the child module maybe nn.Sequential\n    \"\"\"\n    if level == 0:\n        if isinstance(sequence, list):\n            return sequence\n        elif isinstance(sequence, torch.nn.Sequential):\n            return [i for i in sequence]\n        else:\n            return [sequence]\n    res = []\n    for element in sequence:\n        res += flatten_sequence(element, level - 1)\n    return res", "\n\nclass CallableModule(torch.nn.Module):\n    \"\"\"\n        wraps a Callable into a nn.Module\n    \"\"\"\n\n    def __init__(self, fn):\n        super(CallableModule, self).__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x)", "\n\ndef flatten_model(model, layer_list, return_list=False):\n    \"\"\"\n        flatten a model that is not a nn.Sequential, but according to a list of layer name\n        Example:\n            exec_seq = [\n                'conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool',\n                lambda x: torch.flatten(x, 1), 'fc'\n            ]\n            seq_model = flatten_model(model, exec_seq)\n    \"\"\"\n    module_list = []\n    for layer_name in layer_list:\n        if isinstance(layer_name, str):\n            sub_mod = getattr(model, layer_name)\n            if isinstance(sub_mod, torch.nn.modules.container.Sequential) or isinstance(\n                sub_mod, torch.nn.ModuleList\n            ):\n                for op in sub_mod:\n                    module_list.append(op)\n            else:\n                module_list.append(sub_mod)\n        elif isinstance(layer_name, torch.nn.Module):\n            module_list.append(layer_name)\n        elif callable(layer_name):\n            # maybe lambda, or functions, this maynot work for nn.Sequential\n            module_list.append(CallableModule(layer_name))\n        else:\n            # unknown\n            print(\"flatten_model do not support layer: \", layer_name, type(layer_name))\n            raise NotImplementedError()\n    if return_list:\n        return module_list\n    return nn.Sequential(*module_list)", "\n\ndef flat_and_partition(sequence, flat_level=1, partition_policy=\"uniform\", **kwargs):\n    flattened = flatten_sequence(sequence, flat_level)\n    partition_fn = eval(f\"partition_{partition_policy}\")\n    cur_partition = partition_fn(flattened, **kwargs)\n    return cur_partition\n"]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/clip_grad_parallel.py", "chunked_list": ["# adapted from https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_\nimport torch\nimport torch.distributed as dist\nfrom torchdistpackage import tpc, is_using_pp\n\nimport warnings\nfrom typing import Union, Iterable, List, Dict, Tuple, Optional\n\nimport torch\nfrom torch import Tensor, inf", "import torch\nfrom torch import Tensor, inf\nfrom collections import defaultdict\nfrom torch.autograd.grad_mode import no_grad\n\n_tensor_or_tensors = Union[torch.Tensor, Iterable[torch.Tensor]]\n\ndef clip_grad_norm_(\n        parameters: _tensor_or_tensors, max_norm: float, norm_type: float = 2.0,\n        error_if_nonfinite: bool = False) -> torch.Tensor:\n    r\"\"\"Clips gradient norm of an iterable of parameters.\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified in-place.\n    Args:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n        error_if_nonfinite (bool): if True, an error is thrown if the total\n            norm of the gradients from :attr:`parameters` is ``nan``,\n            ``inf``, or ``-inf``. Default: False (will switch to True in the future)\n    Returns:\n        Total norm of the parameter gradients (viewed as a single vector).\n    \"\"\"\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == inf:\n        norms = [p.grad.detach().abs().max().to(device) for p in parameters]\n        total_norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))\n    else:\n        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n    if error_if_nonfinite and torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(\n            f'The total norm of order {norm_type} for gradients from '\n            '`parameters` is non-finite, so it cannot be clipped. To disable '\n            'this error and scale the gradients by the non-finite norm anyway, '\n            'set `error_if_nonfinite=False`')\n    # for model parallel, and ZeRO(>=2), we need to collect total_norm from all MP ranks\n    if is_using_pp():\n        dist.all_reduce(\n            total_norm, op=torch.distributed.ReduceOp.SUM, group=tpc.get_group(\"pipe\")\n        )\n    # TODO(sdx): other parallel mode to be supported\n\n    clip_coef = max_norm / (total_norm + 1e-6)\n    # Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\n    # avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\n    # when the gradients do not reside in CPU memory.\n    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n    for p in parameters:\n        p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n    return total_norm", "\n# This util function splits tensors into groups by device and dtype, which is useful before sending\n# tensors off to a foreach implementation, which requires tensors to be on one device and dtype.\n# If tensorlistlist contains more than one tensorlist, the following assumptions are made BUT NOT verified:\n#   - tensorlists CAN be None\n#   - all tensors in the first specified list cannot be None\n#   - given an index i, all specified tensorlist[i]s match in dtype and device\n# with_indices (bool, optional): whether to track previous indices as the last list per dictionary entry.\n#   It comes in handy if there are Nones or literals in the tensorlists that are getting scattered out.\n#   Whereas mutating a tensor in the resulting split-up tensorlists WILL propagate changes back to the", "#   It comes in handy if there are Nones or literals in the tensorlists that are getting scattered out.\n#   Whereas mutating a tensor in the resulting split-up tensorlists WILL propagate changes back to the\n#   original input tensorlists, changing up Nones/literals WILL NOT propagate, and manual propagation\n#   may be necessary. Check out torch/optim/sgd.py for an example.\n@no_grad()\ndef _group_tensors_by_device_and_dtype(tensorlistlist: List[List[Tensor]],\n                                       with_indices: Optional[bool] = False) -> \\\n        Dict[Tuple[torch.device, torch.dtype], List[List[Union[Tensor, int]]]]:\n    assert all(not x or len(x) == len(tensorlistlist[0]) for x in tensorlistlist), (\n           \"all specified tensorlists must match in length\")\n    per_device_and_dtype_tensors: Dict[Tuple[torch.device, torch.dtype], List[List[Union[Tensor, int]]]] = defaultdict(\n        lambda: [[] for _ in range(len(tensorlistlist) + (1 if with_indices else 0))])\n    for i, t in enumerate(tensorlistlist[0]):\n        key = (t.device, t.dtype)\n        for j in range(len(tensorlistlist)):\n            # a tensorlist may be empty/None\n            if tensorlistlist[j]:\n                per_device_and_dtype_tensors[key][j].append(tensorlistlist[j][i])\n        if with_indices:\n            # tack on previous index\n            per_device_and_dtype_tensors[key][j + 1].append(i)\n    return per_device_and_dtype_tensors", "\n\nclass NativeScalerPP:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(\n            self,\n            loss,\n            optimizer,\n            clip_grad=None,\n            clip_mode='norm',\n            parameters=None,\n            create_graph=False,\n            need_update=True,\n    ):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        # TODO: how to broadcast the scale?\n        # if is_using_pp():\n        #     cur_scale = self._scaler.get_scale()\n        #     if tpc.is_last_in_pipeline_group():\n        #         dist.broadcast()\n        if need_update:\n            if clip_grad is not None:\n                assert parameters is not None\n                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n                clip_grad_norm_(parameters, clip_grad)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)", ""]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/__init__.py", "chunked_list": [""]}
{"filename": "torchdistpackage/parallel/pipeline_parallel/pipeline_sched.py", "chunked_list": ["import torch\nfrom torchdistpackage import tpc\nfrom . import comm\n\n\ndef _forward_step_in_forward_backward(\n    input_obj_from_prev, ind, micro_bs, fwd_fn, extra_inputs=[]\n):\n    \"\"\"\n        params:\n            input_obj_from_prev: the output of prev stage\n            ind: current micro-batch index\n            micro_bs: the micro-batch batchsize\n            fwd_fn: the fwd func of current stage\n            extra_inputs: extra inputs for current stage, will be split into micro batches\n    \"\"\"\n    cur_inputs = []\n    if input_obj_from_prev is not None:\n        if isinstance(input_obj_from_prev, torch.Tensor):\n            cur_inputs.append(input_obj_from_prev)\n        elif isinstance(input_obj_from_prev, list) or isinstance(\n            input_obj_from_prev, tuple\n        ):\n            cur_inputs.append(*input_obj_from_prev)\n\n    for inp in extra_inputs:\n        cur_inputs.append(inp[ind * micro_bs : (ind + 1) * micro_bs])\n\n    # inputs made of two parts: the prev stage output, and given mini-batch ( that should be split into micro-batches)\n    if len(cur_inputs) == 1:\n        return fwd_fn(cur_inputs[0])\n    else:\n        return fwd_fn(cur_inputs)", "\n\ndef _backward_step_in_forward_backward(\n    input_obj, output_obj, output_obj_grad, backward_fn\n):\n    \"\"\"\n        runs backward using user given function, supports `optimizer.backward(loss)`\n    \"\"\"\n\n    # Retain the grad on the input_obj.\n    if input_obj is not None:\n        if isinstance(input_obj, torch.Tensor):\n            input_obj.retain_grad()\n        else:\n            for in_tensor in input_obj:\n                if in_tensor is not None:\n                    in_tensor.retain_grad()\n    if backward_fn is None:\n        if output_obj_grad is None:\n            output_obj.backward()  # equal to loss.backward\n        else:\n            torch.autograd.backward(tensors=output_obj, grad_tensors=output_obj_grad)\n    else:\n        backward_fn(output_obj, output_obj_grad)\n\n    # Collect the grad of the input_obj.\n    input_obj_grad = None\n    if input_obj is not None:\n        if isinstance(input_obj, torch.Tensor):\n            input_obj_grad = input_obj.grad\n        else:\n            input_obj_grad = []\n            for in_tensor in input_obj:\n                input_obj_grad.append(in_tensor.grad)\n\n    return input_obj_grad", "\n\ndef forward_backward(\n    optimizer,\n    fwd_fn,\n    bwd_fn,\n    inputs,\n    num_microbatches=1,\n    forward_only=False,\n    dtype=torch.bfloat16,\n    scatter_gather_tensors=False,\n):\n    \"\"\"\n        params:\n            optimizer: the optimizer, used to call optimizer.zero_grad()\n            fwd_fn & bwd_fn: the fwd/bwd func of current stage\n            inputs: inputs for current stage, for the first stage this must not be None,\n                    for other stages, this could be None, and could also have extra inputs\n            num_microbatches: the micro-batch number\n            forward_only: if run forward_only, no backward is run\n            dtype: tensor dtype\n            scatter_gather_tensors: for communication\n    \"\"\"\n\n    num_warmup_microbatches = (\n        tpc.get_group_size(\"pipe\") - tpc.get_group_rank(\"pipe\") - 1\n    )\n\n    num_warmup_microbatches = min(num_warmup_microbatches, num_microbatches)\n    num_microbatches_remaining = num_microbatches - num_warmup_microbatches\n    if isinstance(inputs, torch.Tensor):\n        inputs = [inputs]\n    elif inputs == None:\n        assert (\n            not tpc.is_first_in_pipeline_group()\n        ), \"pipeline 1st stage should have valid inputs!\"\n        inputs = []\n\n    micro_bs = 0\n    if len(inputs) > 0:\n        mini_bs = inputs[0].size(0)\n        micro_bs = int(mini_bs / num_microbatches)\n\n    # Input, output tensors only need to be saved when doing backward passes\n    input_objs = None\n    output_objs = None\n    if not forward_only:\n        input_objs = []\n        output_objs = []\n\n    # Used for tensor meta information communication\n    ft_shapes = None\n    bt_shapes = None\n    fs_checker = True\n    if optimizer:\n        optimizer.zero_grad()\n\n    # Run warmup forward passes.\n    for i in range(num_warmup_microbatches):\n        if not tpc.is_first_in_pipeline_group:\n            ft_shapes = comm.recv_obj_meta(ft_shapes)\n        input_obj = comm.recv_forward(\n            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n        )\n\n        output_obj = _forward_step_in_forward_backward(\n            input_obj, i, micro_bs, fwd_fn, inputs\n        )\n        if not tpc.is_last_in_pipeline_group():\n            if isinstance(output_obj, torch.Tensor):\n                bt_shapes = output_obj.shape\n            else:\n                bt_shapes = []\n                for out_tensor in output_obj:\n                    bt_shapes.append(out_tensor.shape)\n            fs_checker = comm.send_obj_meta(output_obj, fs_checker)\n        comm.send_forward(output_obj, scatter_gather_tensors=scatter_gather_tensors)\n\n        if not forward_only:\n            input_objs.append(input_obj)\n            output_objs.append(output_obj)\n\n    # Before running 1F1B, need to receive first forward tensor.\n    # If all microbatches are run in warmup / cooldown phase, then no need to\n    # receive this tensor here.\n    if num_microbatches_remaining > 0:\n        if not tpc.is_first_in_pipeline_group():\n            ft_shapes = comm.recv_obj_meta(ft_shapes)\n        input_obj = comm.recv_forward(\n            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n        )\n\n    # Run 1F1B in steady state.\n    for i in range(num_microbatches_remaining):\n        last_iteration = i == (num_microbatches_remaining - 1)\n\n        output_obj = _forward_step_in_forward_backward(\n            input_obj, i + num_warmup_microbatches, micro_bs, fwd_fn, inputs\n        )\n        if forward_only:\n            comm.send_forward(output_obj, scatter_gather_tensors=scatter_gather_tensors)\n\n            if not last_iteration:\n                input_obj = comm.recv_forward(\n                    ft_shapes,\n                    dtype=dtype,\n                    scatter_gather_tensors=scatter_gather_tensors,\n                )\n\n        else:\n            output_obj_grad = comm.send_forward_recv_backward(\n                output_obj,\n                bt_shapes,\n                dtype=dtype,\n                scatter_gather_tensors=scatter_gather_tensors,\n            )\n\n            # Add input_obj and output_obj to end of list.\n            input_objs.append(input_obj)\n            output_objs.append(output_obj)\n\n            # Pop output_obj and output_obj from the start of the list for\n            # the backward pass.\n            input_obj = input_objs.pop(0)\n            output_obj = output_objs.pop(0)\n\n            input_obj_grad = _backward_step_in_forward_backward(\n                input_obj, output_obj, output_obj_grad, bwd_fn\n            )\n\n            if last_iteration:\n                input_obj = None\n                comm.send_backward(\n                    input_obj_grad, scatter_gather_tensors=scatter_gather_tensors\n                )\n            else:\n                input_obj = comm.send_backward_recv_forward(\n                    input_obj_grad,\n                    ft_shapes,\n                    dtype=dtype,\n                    scatter_gather_tensors=scatter_gather_tensors,\n                )\n\n    # Run cooldown backward passes.\n    if not forward_only:\n        for i in range(num_warmup_microbatches):\n            input_obj = input_objs.pop(0)\n            output_obj = output_objs.pop(0)\n            output_obj_grad = comm.recv_backward(\n                bt_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n            )\n\n            input_obj_grad = _backward_step_in_forward_backward(\n                input_obj, output_obj, output_obj_grad, bwd_fn\n            )\n\n            comm.send_backward(\n                input_obj_grad, scatter_gather_tensors=scatter_gather_tensors\n            )\n\n    return output_obj", "\n\ndef forward_eval(fwd_fn, inputs, dtype, **kwargs):\n    \"\"\"\n        params:\n            fwd_fn: the fwd func of current stage\n            inputs: inputs for current stage, for the first stage this must not be None,\n                    for other stages, this could be None, and could also have extra inputs\n            dtype: tensor dtype\n    \"\"\"\n\n    scatter_gather_tensors = False\n    fwd_inputs = []\n    # receve output from prev stage except for 1st stage\n    if not tpc.is_first_in_pipeline_group():\n        ft_shapes = None\n        ft_shapes = comm.recv_obj_meta(ft_shapes)\n        output_from_prev = comm.recv_forward(\n            ft_shapes, dtype=dtype, scatter_gather_tensors=scatter_gather_tensors\n        )\n        fwd_inputs.append(output_from_prev)\n\n    # create input\n    if isinstance(inputs, torch.Tensor):\n        fwd_inputs.append(inputs)\n    elif isinstance(inputs, list):\n        for inp in inputs:\n            fwd_inputs.append(inp)\n\n    if len(fwd_inputs) == 1:\n        fwd_inputs = fwd_inputs[0]\n    # run forward\n    fwd_output = None\n    fwd_output = fwd_fn(fwd_inputs)\n    if not tpc.is_last_in_pipeline_group():\n        comm.send_obj_meta(fwd_output)\n        comm.send_forward(fwd_output, scatter_gather_tensors=scatter_gather_tensors)\n\n    return fwd_output", ""]}
{"filename": "torchdistpackage/dist/py_comm_test.py", "chunked_list": ["import os\n\nimport torch\nimport torch.distributed as dist\nimport time\nimport functools\n\nfrom torchdistpackage import setup_distributed_slurm\n\n# reference: https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md", "\n# reference: https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md\n# algbw = Size/time\n# bus_bw = algbw * fraction * (n-1)/n\nmode_2_frac = dict(\n    all_reduce = 2,\n    all_gather = 1,\n    reduce_scatter = 1,\n)\n\ndef test_collection(ele_num_total, mode = 'all_reduce', group=None):\n    comm_op = eval(f\"dist.{mode}\")\n    ws = dist.get_world_size(group)\n\n    ele_num = ele_num_total\n    if mode == \"all_gather\":\n        ele_num = int(ele_num_total // dist.get_world_size(group))\n    tensor = torch.randn(ele_num).half().cuda()\n    if mode == \"all_gather\":\n        tensor_list = [torch.randn(ele_num).half().cuda() for _ in range(ws)]\n        # import pdb;pdb.set_trace()\n        comm_op = functools.partial(comm_op, tensor_list)\n\n    comm_op(tensor, group=group)\n    comm_op(tensor, group=group)\n    dist.barrier()\n    torch.cuda.synchronize()\n    bw=0\n    frac = mode_2_frac[mode]\n    num_repeat = 1\n\n    dist.barrier()\n    torch.cuda.synchronize()\n    beg=time.perf_counter()\n    for _ in range(num_repeat):\n        comm_op(tensor, group=group)\n    dist.barrier()\n    torch.cuda.synchronize()\n\n    time_avg = (time.perf_counter()-beg)/num_repeat\n    algbw = (ele_num_total*2/1e9)/time_avg # GB/s\n    bw = algbw * frac * ((ws-1)/ws)\n    bw = round(bw, 3)\n    time_avg = round(time_avg, 3)\n    if dist.get_rank()==0:\n        print(f\"{mode} repeat={num_repeat}, bandwidth:{bw} GB/s time_avg:{time_avg} s, numel={tensor.numel()}\")\n\n    torch.cuda.synchronize()\n    return bw,time_avg", ")\n\ndef test_collection(ele_num_total, mode = 'all_reduce', group=None):\n    comm_op = eval(f\"dist.{mode}\")\n    ws = dist.get_world_size(group)\n\n    ele_num = ele_num_total\n    if mode == \"all_gather\":\n        ele_num = int(ele_num_total // dist.get_world_size(group))\n    tensor = torch.randn(ele_num).half().cuda()\n    if mode == \"all_gather\":\n        tensor_list = [torch.randn(ele_num).half().cuda() for _ in range(ws)]\n        # import pdb;pdb.set_trace()\n        comm_op = functools.partial(comm_op, tensor_list)\n\n    comm_op(tensor, group=group)\n    comm_op(tensor, group=group)\n    dist.barrier()\n    torch.cuda.synchronize()\n    bw=0\n    frac = mode_2_frac[mode]\n    num_repeat = 1\n\n    dist.barrier()\n    torch.cuda.synchronize()\n    beg=time.perf_counter()\n    for _ in range(num_repeat):\n        comm_op(tensor, group=group)\n    dist.barrier()\n    torch.cuda.synchronize()\n\n    time_avg = (time.perf_counter()-beg)/num_repeat\n    algbw = (ele_num_total*2/1e9)/time_avg # GB/s\n    bw = algbw * frac * ((ws-1)/ws)\n    bw = round(bw, 3)\n    time_avg = round(time_avg, 3)\n    if dist.get_rank()==0:\n        print(f\"{mode} repeat={num_repeat}, bandwidth:{bw} GB/s time_avg:{time_avg} s, numel={tensor.numel()}\")\n\n    torch.cuda.synchronize()\n    return bw,time_avg", "\n\ndef test_all2all_balanced(ele_num, group=None):\n    tensor = torch.ones(ele_num).cuda() * dist.get_rank()\n\n    output = torch.empty_like(tensor)\n    dist.all_to_all_single(output, tensor, group=group)\n\n    dist.barrier()\n    torch.cuda.synchronize()\n\n    num_repeat = 1\n    beg=time.perf_counter()\n    for _ in range(num_repeat):\n        dist.all_to_all_single(output, tensor, group=group)\n    dist.barrier()\n    torch.cuda.synchronize()\n    time_avg = (time.perf_counter()-beg)/num_repeat\n\n    if dist.get_rank()==0:\n        print(f\"all2all_balanced repeat={num_repeat}, time_avg:{time_avg} s, numel={tensor.numel()}\")", "\n\nif __name__==\"__main__\":\n    setup_distributed_slurm()\n\n    test_collection(1801705472*2, mode='all_gather')\n"]}
{"filename": "torchdistpackage/dist/launch_from_slurm.py", "chunked_list": ["import os\nimport subprocess\n\nimport torch\nimport torch.distributed as dist\n\n# TODO: this func is not exmamined\ndef find_free_port():\n    import socket\n    from contextlib import closing\n\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        s.bind((\"\", 0))\n        return str(s.getsockname()[1])", "\ndef setup_distributed_slurm(backend=\"nccl\", port=None):\n    \"\"\"Initialize distributed training environment.\n    support both slurm and torch.distributed.launch\n    see torch.distributed.init_process_group() for more details\n    \"\"\"\n\n    import os\n    import subprocess\n    import torch\n    import torch.distributed as dist\n\n    num_gpus = torch.cuda.device_count()\n\n    if \"SLURM_JOB_ID\" in os.environ:\n        rank = int(os.environ[\"SLURM_PROCID\"])\n        world_size = int(os.environ[\"SLURM_NTASKS\"])\n        node_list = os.environ[\"SLURM_NODELIST\"]\n\n        addr = subprocess.getoutput(\n            f\"scontrol show hostname {node_list} | head -n1\")\n        if \"MASTER_ADDR\" not in os.environ:\n            os.environ[\"MASTER_ADDR\"] = addr\n\n        # specify master port\n        if port is not None:\n            os.environ[\"MASTER_PORT\"] = str(port)\n        elif \"MASTER_PORT\" not in os.environ:\n            port = 54647\n            os.environ[\"MASTER_PORT\"] = str(port)\n        else:\n            port = int(os.environ[\"MASTER_PORT\"])\n        os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n        local_rank = rank % num_gpus\n        os.environ[\"LOCAL_RANK\"] = str(local_rank)\n        os.environ[\"RANK\"] = str(rank)\n    else:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ[\"WORLD_SIZE\"])\n        local_rank = rank % num_gpus\n\n    dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n\n    device = rank % torch.cuda.device_count()\n    torch.cuda.set_device(device)\n    print(f\"dist init done, world_size = {dist.get_world_size()}\")\n    return rank, world_size, port, addr"]}
{"filename": "torchdistpackage/dist/__init__.py", "chunked_list": [""]}
{"filename": "torchdistpackage/dist/utils.py", "chunked_list": ["import ctypes\nimport time\nimport math\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import init\n\n", "\n\n_cudart = ctypes.CDLL('libcudart.so')\n\ndef cu_prof_start():\n    \"\"\"\n    This function and cu_prof_stop  are two functions used to do multi process Nsight Profiling.\n    Example::\n        if self.iter == 20 and torch.distributed.get_rank() == 0:\n            cu_prof_start()\n        if self.iter >= 40:\n            cu_prof_stop()\n            exit()\n    \"\"\"\n    ret = _cudart.cudaProfilerStart()\n    if ret != 0:\n        raise Exception('cudaProfilerStart() returned %d' % ret)\n    else:\n        print('cudaProfilerStart() returned %d' % ret)", "\ndef cu_prof_stop():\n    ret = _cudart.cudaProfilerStop()\n    if ret != 0:\n        raise Exception('cudaProfilerStop() returned %d' % ret)\n    else:\n        print('cudaProfilerStop() returned %d' % ret)\n        exit()\n\ndef nvtx_decorator(func):\n    \"\"\"decorator that causes an NVTX range to be recorded for the duration of the\n    function call.\"\"\"\n\n    def wrapped_fn(*args, **kwargs):\n        torch.cuda.nvtx.range_push(func.__qualname__)\n        ret_val = func(*args, **kwargs)\n        torch.cuda.nvtx.range_pop()\n        return ret_val\n\n    return wrapped_fn", "\ndef nvtx_decorator(func):\n    \"\"\"decorator that causes an NVTX range to be recorded for the duration of the\n    function call.\"\"\"\n\n    def wrapped_fn(*args, **kwargs):\n        torch.cuda.nvtx.range_push(func.__qualname__)\n        ret_val = func(*args, **kwargs)\n        torch.cuda.nvtx.range_pop()\n        return ret_val\n\n    return wrapped_fn", "\nclass NVTX_Context(object):\n    \"\"\" A simple context used to record performance info.\n    \"\"\"\n\n    def __init__(self, context_name, record_time=False):\n        self.context_name = context_name\n        self.start_time = None\n        self.exit_time = None\n        self.record_time = record_time\n        self.synchronize = False\n\n    def __enter__(self):\n        torch.cuda.nvtx.range_push(self.context_name)\n        if self.record_time:\n            torch.cuda.synchronize()\n            self.start_time = time.time()\n\n    def __exit__(self, type, value, traceback):\n        if self.record_time:\n            torch.cuda.synchronize()\n            self.exit_time = time.time()\n            print(f\"{self.context_name} duration is {self.exit_time - self.start_time}\")\n        torch.cuda.nvtx.range_pop()", "\ndef _has_inf_or_nan(x, j=None):\n    try:\n        # if x is half, the .float() incurs an additional deep copy, but it's necessary if\n        # Pytorch's .sum() creates a one-element tensor of the same type as x\n        # (which is true for some recent version of pytorch).\n        cpu_sum = float(x.float().sum())\n        # More efficient version that can be used if .sum() returns a Python scalar\n        # cpu_sum = float(x.sum())\n    except RuntimeError as instance:\n        # We want to check if inst is actually an overflow exception.\n        # RuntimeError could come from a different error.\n        # If so, we still want the exception to propagate.\n        if \"value cannot be converted\" not in instance.args[0]:\n            raise\n        return True\n    else:\n        if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n            return True\n        return False", "\ndef disable_non_master_print(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print"]}
{"filename": "torchdistpackage/dist/process_topo.py", "chunked_list": ["from functools import partial\nfrom collections import defaultdict\nimport numpy\nimport torch.distributed as dist\n\nclass SingletonMeta(type):\n    \"\"\"\n    The Singleton class can be implemented in different ways in Python. Some\n    possible methods include: base class, decorator, metaclass. We will use the\n    metaclass because it is best suited for this purpose.\n    \"\"\"\n\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        \"\"\"\n        Possible changes to the value of the `__init__` argument do not affect\n        the returned instance.\n        \"\"\"\n        if cls not in cls._instances:\n            instance = super().__call__(*args, **kwargs)\n            cls._instances[cls] = instance\n        else:\n            assert len(args) == 0 and len(\n                kwargs) == 0, f'{cls.__name__} is a singleton class and a instance has been created.'\n        return cls._instances[cls]", "\ndef gen_inner_ranks(world_size, group_size):\n    num_groups = int(world_size//group_size)\n    return [list(range(g*group_size, (g+1)*group_size)) for g in range(num_groups)]\n\ndef gen_groups(world_size, group_size, strides, hook):\n    group_size = int(group_size)\n    num_groups = int(world_size//group_size)\n    if strides is None or len(strides)==0:\n        # most inner\n        list_of_ranks = gen_inner_ranks(world_size, group_size)\n        # print(list_of_ranks)\n        for ranks in list_of_ranks:\n            hook(ranks)\n    else:\n        inner_group_size = numpy.prod(strides)\n        list_of_inner_ranks = gen_inner_ranks(world_size, inner_group_size)\n        # print(list_of_inner_ranks)\n\n        for ind in range(inner_group_size):\n            chunks = [list_of_inner_ranks[x:x+group_size] for x in range(0, len(list_of_inner_ranks), group_size)]\n            # print(chunks)\n            for chunk in chunks:\n                cur_ranks = [ranks[ind] for ranks in chunk]\n                hook(cur_ranks)", "\nclass ProcessTopology(metaclass=SingletonMeta):\n    def __init__(self):\n        self._groups = dict()\n        self._ranks_in_group = dict()\n        self._ranks_all = defaultdict(list)\n\n    def _build_group(self, type, ranks):\n        self._ranks_all[type].append(ranks)\n        from datetime import timedelta\n        grp = dist.new_group(ranks, timeout=timedelta(seconds=100))\n        if dist.get_rank() in ranks:\n            self._groups[type] = grp\n            self._ranks_in_group[type] = ranks\n\n            if dist.get_rank() == ranks[0]:\n                print(f\"group {type}, ranks: {ranks}\")\n\n    def setup_process_groups(self, config:list):\n        \"\"\"\n            Example: setup_process_groups([('data',4), ('pipe',2), ('tensor',2)])   # world_size=16\n\n            Result:\n                tensor parallel groups:\n                    [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]\n                pipeline parallel groups:\n                    [0, 2]\n                    [4, 6]\n                    [8, 10]\n                    [12, 14]\n                    [1, 3]\n                    [5, 7]\n                    [9, 11]\n                    [13, 15]\n                data parallel groups:\n                    [0, 4, 8, 12]\n                    [1, 5, 9, 13]\n                    [2, 6, 10, 14]\n                    [3, 7, 11, 15]\n            Usage:\n                # setup\n                dist_init_slurm()\n                dist_config = [('data',world_size/(2*pp_size)), ('pipe',pp_size), ('tensor',2)]\n                torch_parallel_context.setup_process_groups(dist_config)\n                # api example\n                test_comm()\n\n        \"\"\"\n        dims = [item[0] for item in config]\n        sizes = [int(item[1]) for item in config]\n\n        self._groups['global'] = None\n        self._ranks_in_group['global'] = list(range(dist.get_world_size()))\n\n        for (dim, size) in config:\n            cur_dim_ind = dims.index(dim)\n            strides=sizes[cur_dim_ind+1:] if cur_dim_ind+1 < len(sizes) else []\n\n            gen_groups(dist.get_world_size(), size, strides, partial(self._build_group, dim))\n\n        # build Model Parallel Group Automatically\n        if \"tensor\" in dims or \"pipe\" in dims:\n            for g in range(len(self._ranks_all['data'][0])):\n                model_ranks = [dp_ranks[g] for dp_ranks in self._ranks_all['data']]\n                self._build_group(\"model\", model_ranks)\n\n    def build_moe_groups(self, moe_dp_size=None, moe_ep_size=None):\n        # build for moe: moe_data_parallel, moe_expert_parallel\n        # default: moe_expert_parallel group = DDP group\n        dp_ranks_all = self._ranks_all['data']\n\n        if moe_dp_size and not moe_ep_size:\n            moe_ep_size = int(self.get_dp_size()//moe_dp_size)\n        elif moe_ep_size and not moe_dp_size:\n            moe_dp_size = int(self.get_dp_size()//moe_ep_size)\n        elif moe_dp_size and moe_ep_size:\n            assert moe_dp_size*moe_ep_size == self.get_dp_size()\n        else:\n            print(\"invalid args: \", moe_dp_size, moe_ep_size)\n        print(f\"MoE group config: moe_dp_size={moe_dp_size}, moe_ep_size={moe_ep_size}\")\n        num_ep_groups = int(self.get_dp_size() // moe_ep_size)\n        num_dp_groups = int(self.get_dp_size() // moe_dp_size)\n\n        for dp_ranks in dp_ranks_all:\n            for ep_g_id in range(num_ep_groups):\n                moe_ep_ranks_id = list(range(ep_g_id*moe_ep_size, (ep_g_id+1)*moe_ep_size))\n                moe_ep_ranks = [dp_ranks[i] for i in moe_ep_ranks_id]\n                self._build_group(\"moe_ep\", moe_ep_ranks)\n            for dp_g_id in range(num_dp_groups):\n                moe_dp_ranks_id = list(range(dp_g_id, len(dp_ranks) ,moe_ep_size))\n                moe_dp_ranks = [dp_ranks[i] for i in moe_dp_ranks_id]\n                self._build_group(\"moe_dp\", moe_dp_ranks)\n\n\n\n    def _is_inited(self, mode):\n        return mode in self._groups\n\n    def get_group(self, mode):\n        if not self._is_inited(mode):\n            assert False, f\"{mode} is not initialized!\"\n        return self._groups[mode]\n\n    def get_group_rank(self, mode):\n        return dist.get_rank(group=self.get_group(mode))\n\n    def get_ranks_in_group(self, mode):\n        if not self._is_inited(mode):\n            assert False, f\"{mode} is not initialized!\"\n        return self._ranks_in_group[mode]\n\n    def get_tp_rank(self):\n        return self.get_group_rank('tensor')\n\n    def get_pp_rank(self):\n        return self.get_group_rank('pipe')\n\n    def get_dp_rank(self):\n        return self.get_group_rank('data')\n\n    def get_mp_rank(self):\n        return self.get_group_rank('model')\n\n    def get_group_size(self, mode):\n        if not self._is_inited(mode):\n            assert False, f\"{mode} is not initialized!\"\n        return len(self._ranks_in_group[mode])\n\n    def get_tp_size(self):\n        return self.get_group_size('tensor')\n\n    def get_pp_size(self):\n        return self.get_group_size('pipe')\n\n    def get_dp_size(self):\n        return self.get_group_size('data')\n\n    def get_mp_size(self):\n        return self.get_group_size('model')\n\n    def is_first_in_group(self, mode):\n        return self.get_group_rank(mode) == 0\n\n    def is_last_in_group(self, mode):\n        return dist.get_rank() == self._ranks_in_group[mode][-1]\n\n    def is_first_in_tensor_group(self):\n        return self.is_first_in_group('tensor')\n\n    def is_last_in_tensor_group(self):\n        return self.is_last_in_group('tensor')\n\n    def is_first_in_pipeline_group(self):\n        return self.is_first_in_group('pipe')\n\n    def is_last_in_pipeline_group(self):\n        return self.is_last_in_group('pipe')\n\n    def is_first_in_data_group(self):\n        return self.is_first_in_group('data')\n\n    def is_last_in_data_group(self):\n        return self.is_last_in_group('data')\n\n    def is_first_in_model_group(self):\n        return self.is_first_in_group('model')\n\n    def is_last_in_model_group(self):\n        return self.is_last_in_group('model')\n\n    def get_prev_global_rank(self, mode = 'pipe'):\n        local_rank = self.get_group_rank(mode)\n        world_size = self.get_group_size(mode)\n        ranks_in_group = self.get_ranks_in_group(mode)\n\n        return ranks_in_group[(local_rank - 1) % world_size]\n\n    def get_next_global_rank(self, mode = 'pipe'):\n        local_rank = self.get_group_rank(mode)\n        world_size = self.get_group_size(mode)\n        ranks_in_group = self.get_ranks_in_group(mode)\n\n        return ranks_in_group[(local_rank + 1) % world_size]\n\n    def is_mode_inited(self, mode):\n        return mode in self._groups and self.get_group_size(mode)>1\n\n    def all_dp_ranks(self):\n        return self._ranks_all['data']\n\n    def all_ranks(self, mode):\n        if not self._is_inited(mode):\n            assert False, f\"{mode} is not initialized!\"\n\n        return self._ranks_all[mode]\n\n    def is_first_group(self, mode):\n        # process group of 'type' may have several groups,\n        # sometime we only need process in the 'first' group to dp sth\n\n        if not self._is_inited(mode):\n            assert False, f\"{mode} is not initialized!\"\n\n        group_ranks = self._ranks_in_group[mode]\n        if group_ranks == self._ranks_all[mode][0]:\n            return True\n        else:\n            return  False", "\n\ntorch_parallel_context = ProcessTopology()\n\ndef is_using_pp():\n    return torch_parallel_context.is_mode_inited('pipe')\n\ndef test_comm():\n    import torch\n    tmp = torch.rand([100,1024]).cuda()\n    torch.cuda.synchronize()\n    dist.all_reduce(tmp, group=None)\n    for mode in ['data', 'tensor', 'pipe', 'model', 'moe_dp', 'moe_ep']:\n        if torch_parallel_context.is_mode_inited(mode):\n            dist.all_reduce(tmp, group=torch_parallel_context.get_group(mode))\n            torch.cuda.synchronize()\n        print('passed:', mode)\n\n    len_dl_tensor = torch.tensor([0], dtype=torch.long).cuda()\n    if torch_parallel_context.is_first_in_group('model'):\n        len_dl = 10\n        len_dl_tensor = torch.tensor([len_dl], dtype=torch.long).cuda()\n\n    dist.broadcast(len_dl_tensor,0)\n\n    if torch_parallel_context.is_mode_inited('model'):\n        dist.broadcast(len_dl_tensor, torch_parallel_context.get_ranks_in_group('model')[0], torch_parallel_context.get_group('model'))\n        torch.cuda.synchronize()\n\n    if torch_parallel_context.is_mode_inited('tensor'):\n        outs = [torch.rand_like(tmp) for _ in range(torch_parallel_context.get_group_size('tensor'))]\n        dist.all_gather(outs, tmp, group=torch_parallel_context.get_group('tensor'))\n        torch.cuda.synchronize()\n\n    if torch_parallel_context.is_mode_inited('pipe'):\n        if torch_parallel_context.is_first_in_pipeline_group():\n            dist.send(tmp, torch_parallel_context.get_next_global_rank('pipe'))\n        if torch_parallel_context.is_last_in_pipeline_group():\n            dist.recv(tmp, torch_parallel_context.get_prev_global_rank('pipe'))\n        torch.cuda.synchronize()\n    dist.barrier()\n    print(\"Finished test_comm --- \")", ""]}
{"filename": "torchdistpackage/dist/model_parallel_ckpt.py", "chunked_list": ["# this file include functions for saving/loading checkpoints for Model Parallelism\nfrom torchdistpackage import tpc\n\ndef get_mp_ckpt_suffix():\n    \"\"\"\n        get the suffix of state_dict filename, in format: f\"_tp_{tp_rank}_pp_{pp_rank}.pth\"\n        example:\n            \"_tp_1_pp_2.pth\", mean this partition is Rank 1 in TensorParallel and Rank 2 in PipelineParallel\n    \"\"\"\n    fname = \"\"\n\n    if is_mode_inited('tensor'):\n        tp_rank = tpc.get_group_rank(\"tensor\")\n        fname += f\"_tp_{tp_rank}\"\n    if is_mode_inited('pipe'):\n        pp_rank = tpc.get_group_rank(\"pipe\")\n        fname += f\"_pp_{pp_rank}\"\n\n    fname += f\".pth\"\n\n    return fname", ""]}
{"filename": "explore/perf/test_timm.py", "chunked_list": ["import os\nimport time\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom timm.models import create_model\nfrom timm.models.vision_transformer import VisionTransformer\nfrom torch.cuda.amp import GradScaler, autocast\n\nfrom functorch.compile import memory_efficient_fusion", "\nfrom functorch.compile import memory_efficient_fusion\nfrom functorch.compile import min_cut_rematerialization_partition\nfrom functorch.compile import ts_compile\nfrom functorch.compile import aot_function, \\\n    make_boxed_func, ts_compile\nfrom functorch.compile import ts_compile, aot_module\n\ndef train_aot(model,input, amp_enable=False, warmup=False):\n    # aot_compiled_mod = aot_module(model,\n    #                               fw_compiler=ts_compile,\n    #                               bw_compiler=ts_compile)\n    aot_compiled_mod=memory_efficient_fusion(model)\n    for i in range(0, 3):\n        with autocast(enabled=amp_enable):\n            out=aot_compiled_mod(input)\n        out.sum().backward()\n    torch.cuda.synchronize()\n    time_start=time.time()\n    for i in range(0, 10):\n        with autocast(enabled=amp_enable):\n            out=aot_compiled_mod(input)\n        out.sum().backward()\n\n    torch.cuda.synchronize()\n    if not warmup:\n        print(f'time used: {time.time()-time_start}')\n    return time.time()-time_start", "def train_aot(model,input, amp_enable=False, warmup=False):\n    # aot_compiled_mod = aot_module(model,\n    #                               fw_compiler=ts_compile,\n    #                               bw_compiler=ts_compile)\n    aot_compiled_mod=memory_efficient_fusion(model)\n    for i in range(0, 3):\n        with autocast(enabled=amp_enable):\n            out=aot_compiled_mod(input)\n        out.sum().backward()\n    torch.cuda.synchronize()\n    time_start=time.time()\n    for i in range(0, 10):\n        with autocast(enabled=amp_enable):\n            out=aot_compiled_mod(input)\n        out.sum().backward()\n\n    torch.cuda.synchronize()\n    if not warmup:\n        print(f'time used: {time.time()-time_start}')\n    return time.time()-time_start", "\n\ndef train(model, input, amp_enable=False, warmup=False):\n    for i in range(0, 3):\n        with autocast(enabled=amp_enable):\n            out=model(input)\n        out.sum().backward()\n    torch.cuda.synchronize()\n    time_start=time.time()\n    for i in range(0, 10):\n        with autocast(enabled=amp_enable):\n            out=model(input)\n        out.sum().backward()\n\n    torch.cuda.synchronize()\n    if not warmup:\n        print(f'time used: {time.time()-time_start}')\n    return time.time()-time_start", "\n\ndef train_g(model, input, amp_enable=False, warmup=False):\n    # Placeholders used for capture\n    static_input = torch.randn_like(input, device='cuda')\n    g = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g):\n        with autocast(enabled=amp_enable):\n            out=model(static_input)\n        out.sum().backward()\n\n    static_input.copy_(input)\n    torch.cuda.synchronize()\n    time_start=time.time()\n    for i in range(0, 5):\n        g.replay()\n\n    torch.cuda.synchronize()\n    if not warmup:\n        print(f'time used: {time.time()-time_start}')", "\ndef train_g_2(model, input, amp_enable=False, warmup=False):\n    torch.cuda.synchronize()\n    model_g = torch.cuda.make_graphed_callables(model, (input,))\n    time_start=time.time()\n    for i in range(0, 5):\n        with autocast(enabled=amp_enable):\n            out=model_g(input)\n        out.sum().backward()\n\n    torch.cuda.synchronize()\n    print(f'time used: {time.time()-time_start}')", "\ndef run_train_funcs(train, model, input):\n    # warmup, ignore\n    train(model, input, warmup=True)\n\n    # torch.backends.cuda.matmul.allow_tf32=False\n    # torch.backends.cudnn.allow_tf32=False\n    os.environ['NVIDIA_TF32_OVERRIDE']='0'\n    print('----train with fp32----')\n    train(model, input)\n    print('----train with autocast----')\n    train(model, input, amp_enable=True)\n    print('----train with tf32----')\n    os.environ['NVIDIA_TF32_OVERRIDE']='1'\n    torch.backends.cuda.matmul.allow_tf32=True\n    torch.backends.cudnn.allow_tf32=True\n    train(model, input)", "\nprint(torch.backends.cuda.matmul.allow_tf32)\nprint(torch.backends.cudnn.allow_tf32)\n\ndef enable_tf32():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n\ndef disable_tf32():\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = True", "def disable_tf32():\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = True\n\n\nenable_tf32()\nmodel=create_model('vit_large_patch16_224', #vit_large_patch16_384\n                   pretrained=False,\n                   num_classes=None,\n                   drop_rate=0,", "                   num_classes=None,\n                   drop_rate=0,\n                   drop_path_rate=0.3)\nmodel.cuda().train()\n\ndef gen_inp(batch=64):\n    return torch.rand([batch, 3, 224, 224])\n\n# train(model, gen_inp().cuda())\n", "# train(model, gen_inp().cuda())\n\n# train(model, gen_inp().cuda())\n\ntrain_aot(model, gen_inp().cuda())\n\nprint(\"max mem\", torch.cuda.max_memory_allocated()/(1e9))"]}
{"filename": "explore/fx/fx_simple.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom torch import fx\nfrom torch.fx import symbolic_trace\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.fc1 = nn.Linear(10, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, input):\n        out = self.fc1(input)\n        return self.fc2(out)", "\nmodule = MyModule()\n# \u7b26\u53f7\u8ffd\u8e2a\u8fd9\u4e2a\u6a21\u5757\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# symbolic_traced.graph.print_tabular()\n\n# # \u4e2d\u95f4\u8868\u793a\n# # High-level intermediate representation (IR) - Graph representation", "# # \u4e2d\u95f4\u8868\u793a\n# # High-level intermediate representation (IR) - Graph representation\n# print(symbolic_traced.graph)\n\n# # \u751f\u6210\u4ee3\u7801\n# # Code generation - valid Python code\n# print(symbolic_traced.code)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        print(node.op, node.name, node.target)\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n        elif node.op == 'call_module':\n            import pdb;pdb.set_trace()\n            print(\"module\")\n            # if node.starget =\n        elif node.op == 'output':\n            import pdb;pdb.set_trace()\n            print('output')\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)", "def transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        print(node.op, node.name, node.target)\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n        elif node.op == 'call_module':\n            import pdb;pdb.set_trace()\n            print(\"module\")\n            # if node.starget =\n        elif node.op == 'output':\n            import pdb;pdb.set_trace()\n            print('output')\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)", "\ntransform(module, torch.fx.Tracer)"]}
{"filename": "explore/fx/torch_fx_profile.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\n# modified from:\n# https://github.com/jamesr66a/tutorials/blob/f8f67243b50c55939cc6153bbfd6158bba5b780d/intermediate_source/fx_profiling_tutorial.py\n\n\n\"\"\"\n(beta) Building a Simple CPU Performance Profiler with FX\n*******************************************************\n**Author**: `James Reed <https://github.com/jamesr66a>`_", "*******************************************************\n**Author**: `James Reed <https://github.com/jamesr66a>`_\n\nIn this tutorial, we are going to use FX to do the following:\n\n1) Capture PyTorch Python code in a way that we can inspect and gather\n   statistics about the structure and execution of the code\n2) Build out a small class that will serve as a simple performance \"profiler\",\n   collecting runtime statistics about each part of the model from actual\n   runs.", "   collecting runtime statistics about each part of the model from actual\n   runs.\n\n\"\"\"\n\n######################################################################\n# For this tutorial, we are going to use the torchvision ResNet18 model\n# for demonstration purposes.\n\nimport torch", "\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = models.resnet18().cuda()\nrn18.train()\n\n######################################################################\n# Now that we have our model, we want to inspect deeper into its", "######################################################################\n# Now that we have our model, we want to inspect deeper into its\n# performance. That is, for the following invocation, which parts\n# of the model are taking the longest?\ninput = torch.randn(5, 3, 224, 224).cuda()\noutput = rn18(input)\n\n######################################################################\n# A common way of answering that question is to go through the program\n# source, add code that collects timestamps at various points in the", "# A common way of answering that question is to go through the program\n# source, add code that collects timestamps at various points in the\n# program, and compare the difference between those timestamps to see\n# how long the regions between the timestamps take.\n#\n# That technique is certainly applicable to PyTorch code, however it\n# would be nicer if we didn't have to copy over model code and edit it,\n# especially code we haven't written (like this torchvision model).\n# Instead, we are going to use FX to automate this \"instrumentation\"\n# process without needing to modify any source.", "# Instead, we are going to use FX to automate this \"instrumentation\"\n# process without needing to modify any source.\n\n######################################################################\n# First, let's get some imports out of the way (we will be using all\n# of these later in the code).\n\nimport statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import Interpreter", "from typing import Any, Dict, List\nfrom torch.fx import Interpreter\n\n######################################################################\n# .. note::\n#     ``tabulate`` is an external library that is not a dependency of PyTorch.\n#     We will be using it to more easily visualize performance data. Please\n#     make sure you've installed it from your favorite Python package source.\n\n######################################################################", "\n######################################################################\n# Capturing the Model with Symbolic Tracing\n# -----------------------------------------\n# Next, we are going to use FX's symbolic tracing mechanism to capture\n# the definition of our model in a data structure we can manipulate\n# and examine.\n\ntraced_rn18 = torch.fx.symbolic_trace(rn18)\nprint(traced_rn18.graph)", "traced_rn18 = torch.fx.symbolic_trace(rn18)\nprint(traced_rn18.graph)\n\n######################################################################\n# This gives us a Graph representation of the ResNet18 model. A Graph\n# consists of a series of Nodes connected to each other. Each Node\n# represents a call-site in the Python code (whether to a function,\n# a module, or a method) and the edges (represented as ``args`` and ``kwargs``\n# on each node) represent the values passed between these call-sites. More\n# information about the Graph representation and the rest of FX's APIs ca", "# on each node) represent the values passed between these call-sites. More\n# information about the Graph representation and the rest of FX's APIs ca\n# be found at the FX documentation https://pytorch.org/docs/master/fx.html.\n\n\n######################################################################\n# Creating a Profiling Interpreter\n# --------------------------------\n# Next, we are going to create a class that inherits from ``torch.fx.Interpreter``.\n# Though the ``GraphModule`` that ``symbolic_trace`` produces compiles Python code", "# Next, we are going to create a class that inherits from ``torch.fx.Interpreter``.\n# Though the ``GraphModule`` that ``symbolic_trace`` produces compiles Python code\n# that is run when you call a ``GraphModule``, an alternative way to run a\n# ``GraphModule`` is by executing each ``Node`` in the ``Graph`` one by one. That is\n# the functionality that ``Interpreter`` provides: It interprets the graph node-\n# by-node.\n#\n# By inheriting from ``Interpreter``, we can override various functionality and\n# install the profiling behavior we want. The goal is to have an object to which\n# we can pass a model, invoke the model 1 or more times, then get statistics about", "# install the profiling behavior we want. The goal is to have an object to which\n# we can pass a model, invoke the model 1 or more times, then get statistics about\n# how long the model and each part of the model took during those runs.\n#\n# Let's define our ``ProfilingInterpreter`` class:\n\nclass ProfilingInterpreter(Interpreter):\n    def __init__(self, gm):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        # gm = torch.fx.symbolic_trace(mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entrypoint for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -> Any:\n        # Record the time we started running the model\n        torch.cuda.synchronize()\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        torch.cuda.synchronize()\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : torch.fx.Node) -> Any:\n        # Record the time we started running the op\n        torch.cuda.synchronize()\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        torch.cuda.synchronize()\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n\n        n.__setattr__('time_cost', t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -> str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)", "\n######################################################################\n# .. note::\n#       We use Python's ``time.time`` function to pull wall clock\n#       timestamps and compare them. This is not the most accurate\n#       way to measure performance, and will only give us a first-\n#       order approximation. We use this simple technique only for the\n#       purpose of demonstration in this tutorial.\n\n######################################################################", "\n######################################################################\n# Investigating the Performance of ResNet18\n# -----------------------------------------\n# We can now use ``ProfilingInterpreter`` to inspect the performance\n# characteristics of our ResNet18 model;\n\ninterp = ProfilingInterpreter(traced_rn18)\ninterp.run(input)\nprint(interp.summary(True))", "interp.run(input)\nprint(interp.summary(True))\n\n######################################################################\n# There are two things we should call out here:\n#\n# * MaxPool2d takes up the most time. This is a known issue:\n#   https://github.com/pytorch/pytorch/issues/51393\n# * BatchNorm2d also takes up significant time. We can continue this\n#   line of thinking and optimize this in the Conv-BN Fusion with FX", "# * BatchNorm2d also takes up significant time. We can continue this\n#   line of thinking and optimize this in the Conv-BN Fusion with FX\n#   tutorial TODO: link\n#\n#\n# Conclusion\n# ----------\n# As we can see, using FX we can easily capture PyTorch programs (even\n# ones we don't have the source code for!) in a machine-interpretable\n# format and use that for analysis, such as the performance analysis", "# ones we don't have the source code for!) in a machine-interpretable\n# format and use that for analysis, such as the performance analysis\n# we've done here. FX opens up an exiciting world of possibilities for\n# working with PyTorch programs.\n#\n# Finally, since FX is still in beta, we would be happy to hear any\n# feedback you have about using it. Please feel free to use the\n# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker\n# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n# you might have.", "# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n# you might have.\n\n# import pdb;pdb.set_trace()\nfor node in traced_rn18.graph.nodes:\n    # import pdb;pdb.set_trace()\n    print(node.op, node.name, node.target, node.time_cost)"]}
{"filename": "explore/fx/colo_fx.py", "chunked_list": ["import torch\nfrom torch.fx import symbolic_trace\nimport colossalai\n\nfrom colossalai.fx.passes.meta_info_prop import MetaInfoProp\n\nBATCH_SIZE = 2\nDIM_IN = 4\nDIM_HIDDEN = 16\nDIM_OUT = 16", "DIM_HIDDEN = 16\nDIM_OUT = 16\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(DIM_IN, DIM_HIDDEN),\n    torch.nn.Linear(DIM_HIDDEN, DIM_OUT),\n    )\ninput_sample = torch.rand(BATCH_SIZE, DIM_IN)\ngm = symbolic_trace(model)\ninterp = MetaInfoProp(gm)\ninterp.run(input_sample)", "interp = MetaInfoProp(gm)\ninterp.run(input_sample)\nprint(interp.summary(unit='kb'))    # don't panic if some statistics are 0.00 MB\n"]}
{"filename": "explore/fx/fx_graph_split.py", "chunked_list": ["import copy\n\nimport torch\nimport torch.nn as nn\nfrom torch import fx\nfrom torch.fx import symbolic_trace\nimport statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import Interpreter\n", "from torch.fx import Interpreter\n\n\nimport torchvision\nfrom torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n\nimport torchvision.models as models\n\nclass ProfilingInterpreter(Interpreter):\n    def __init__(self, gm):\n        super().__init__(gm)\n\n        self.total_runtime_sec : List[float] = []\n        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n\n\n    def run(self, *args) -> Any:\n        # Record the time we started running the model\n        torch.cuda.synchronize()\n        t_start = time.perf_counter()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        torch.cuda.synchronize()\n        t_end = time.perf_counter()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        self.module.__setattr__('time_cost', t_end - t_start)\n\n        return return_val\n\n    def run_node(self, n : torch.fx.Node) -> Any:\n        # Record the time we started running the op\n        torch.cuda.synchronize()\n        t_start = time.perf_counter()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        torch.cuda.synchronize()\n        t_end = time.perf_counter()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n\n        n.__setattr__('time_cost', t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -> str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)", "class ProfilingInterpreter(Interpreter):\n    def __init__(self, gm):\n        super().__init__(gm)\n\n        self.total_runtime_sec : List[float] = []\n        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n\n\n    def run(self, *args) -> Any:\n        # Record the time we started running the model\n        torch.cuda.synchronize()\n        t_start = time.perf_counter()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        torch.cuda.synchronize()\n        t_end = time.perf_counter()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        self.module.__setattr__('time_cost', t_end - t_start)\n\n        return return_val\n\n    def run_node(self, n : torch.fx.Node) -> Any:\n        # Record the time we started running the op\n        torch.cuda.synchronize()\n        t_start = time.perf_counter()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        torch.cuda.synchronize()\n        t_end = time.perf_counter()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n\n        n.__setattr__('time_cost', t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -> str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)", "\n\nrn18 = models.resnet18().cuda()\nrn18.train()\n\ninput = torch.randn(32, 3, 224, 224).cuda()\noutput = rn18(input)\n\n\ntraced_rn18 = torch.fx.symbolic_trace(rn18)", "\ntraced_rn18 = torch.fx.symbolic_trace(rn18)\n# print(traced_rn18.graph)\n\n\ninterp = ProfilingInterpreter(traced_rn18)\ntraced_out = interp.run(input)\ntraced_out = interp.run(input)\ntraced_out = interp.run(input)\nassert torch.allclose(output, traced_out)", "traced_out = interp.run(input)\nassert torch.allclose(output, traced_out)\n# print(interp.summary(True))\n# import pdb;pdb.set_trace()\ntotal_time_cost = traced_rn18.time_cost\naccum_time =0\n\n# \u8fd9\u91cc\u5207\u5206\u7684\u903b\u8f91\u5b58\u5728\u95ee\u9898\uff1a\u65e0\u6cd5\u5904\u7406\u50cfresidual\u7684\u7ed3\u6784\uff0c\u53ea\u80fd\u9002\u7528\u6240\u6709\u7684\u90e8\u5206\u90fd\u662f\u5355\u8f93\u5165\u5355\u8f93\u51fa\n\ndef split_gm_into2(gm, time_map, time_accum):\n    accum_time =0\n    first_gm = copy.deepcopy(gm)\n    split_node_name = None\n    new_output_name = None\n    for node in first_gm.graph.nodes:\n        accum_time += time_map.get(node.name,0)\n        if accum_time>=time_accum and not split_node_name:\n            # jump nodes which has multiple inputs\n            if len(node.args)>1:\n                continue\n            else:\n                new_output=first_gm.graph.output(node)\n                split_node_name = node.name\n                new_output_name = new_output.name\n\n        if node.op=='output' and node.name != new_output_name:\n            first_gm.graph.erase_node(node)\n    first_gm.graph.eliminate_dead_code()\n    first_gm.recompile()\n\n    found = False\n    sencond_gm = copy.deepcopy(gm)\n    not_needed_nodes = []\n    for node in sencond_gm.graph.nodes:\n        if split_node_name != node.name and not found:\n            # sencond_gm.graph.erase_node(node)\n            not_needed_nodes.append(node)\n        elif split_node_name == node.name:\n            found = True\n            new_input = sencond_gm.graph.placeholder(split_node_name)\n            node.replace_all_uses_with(new_input)\n            sencond_gm.graph.erase_node(node)\n            break\n    sencond_gm.graph.eliminate_dead_code()\n\n    sencond_gm.recompile()\n    return first_gm,sencond_gm", "\ndef split_gm_into2(gm, time_map, time_accum):\n    accum_time =0\n    first_gm = copy.deepcopy(gm)\n    split_node_name = None\n    new_output_name = None\n    for node in first_gm.graph.nodes:\n        accum_time += time_map.get(node.name,0)\n        if accum_time>=time_accum and not split_node_name:\n            # jump nodes which has multiple inputs\n            if len(node.args)>1:\n                continue\n            else:\n                new_output=first_gm.graph.output(node)\n                split_node_name = node.name\n                new_output_name = new_output.name\n\n        if node.op=='output' and node.name != new_output_name:\n            first_gm.graph.erase_node(node)\n    first_gm.graph.eliminate_dead_code()\n    first_gm.recompile()\n\n    found = False\n    sencond_gm = copy.deepcopy(gm)\n    not_needed_nodes = []\n    for node in sencond_gm.graph.nodes:\n        if split_node_name != node.name and not found:\n            # sencond_gm.graph.erase_node(node)\n            not_needed_nodes.append(node)\n        elif split_node_name == node.name:\n            found = True\n            new_input = sencond_gm.graph.placeholder(split_node_name)\n            node.replace_all_uses_with(new_input)\n            sencond_gm.graph.erase_node(node)\n            break\n    sencond_gm.graph.eliminate_dead_code()\n\n    sencond_gm.recompile()\n    return first_gm,sencond_gm", "\ndef get_splited_model(gm, partition_id:int, num_partitions:int):\n    \"\"\"\n        partition_id: from 1, e.g. if num_partitions=3, partition_id={1,2,3}\n    \"\"\"\n    total_time_cost = gm.time_cost\n    time_map = {}\n    for node in gm.graph.nodes:\n        # import pdb;pdb.set_trace()\n        time_map[node.name] = node.time_cost\n    gms = split_gm_into2(gm, time_map, total_time_cost/num_partitions)\n    return gms[partition_id]", "\n\ngm=get_splited_model(traced_rn18, 1, 2)\n\nprint(gm.graph)"]}
{"filename": "explore/model_parallel/mlp.py", "chunked_list": ["import torch\nfrom torch import nn as nn\nfrom torch.nn.parameter import Parameter\n\nfrom tp_utils import get_tp_group, set_tp_group, TpLinear, RowParallelLinear, ColParallelLinear, \\\n    gather_from_sequence_parallel_region\n\n\nclass Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\n        From timm, but modified to run with tensor parallel and sequence parallel.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            tp_group = None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = bias\n\n        set_tp_group(tp_group)\n        self.fc1 = TpLinear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = TpLinear(hidden_features, out_features, bias=bias)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x", "class Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\n        From timm, but modified to run with tensor parallel and sequence parallel.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            tp_group = None,\n            bias=True,\n            drop=0.,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = bias\n\n        set_tp_group(tp_group)\n        self.fc1 = TpLinear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = TpLinear(hidden_features, out_features, bias=bias)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x", "\nclass TpMlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n\n        From timm, but modified to run with tensor parallel and sequence parallel.\n    \"\"\"\n    def __init__(\n            self,\n            in_features,\n            hidden_features=None,\n            out_features=None,\n            act_layer=nn.GELU,\n            tp_group = None,\n            bias=True,\n            drop=0.,\n            sequence_parallel=True\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = bias\n        self.sequence_parallel = sequence_parallel\n\n        set_tp_group(tp_group)\n        self.fc1 = ColParallelLinear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = RowParallelLinear(hidden_features, out_features, bias=bias, sequence_parallel=sequence_parallel)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        if self.sequence_parallel:\n            # assume input tensor is sequence parallel\n            x = gather_from_sequence_parallel_region(x)\n\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x"]}
{"filename": "explore/moe/ds_fmoe_main.py", "chunked_list": ["# Use this version of deepspeed:\n# https://github.com/KimmiShi/DeepSpeed/tree/fmoe_v0.9.0\n\n# Use this version of fastmoe:\n# https://github.com/KimmiShi/fastmoe\n\nfrom timm.models import create_model\nimport deepspeed\n\n\ndef main():\n    model = create_model(...)\n    args.ep = 4\n\n\n    # replace vit.mlp with MoE layer\n    from deepspeed.utils import groups\n    from deepspeed.comm.comm import init_distributed\n    init_distributed(dist_backend='nccl')\n    groups._create_expert_and_data_parallel(args.ep)\n    for block in model.encoder.blocks:\n        original_mlp = block.mlp\n        feat_in = original_mlp.fc1.weight.shape[1]\n        block.mlp = deepspeed.moe.layer.MoE(hidden_size=feat_in, expert=block.mlp, num_experts=args.ep,\n                                            ep_size=args.ep, use_fmoe=True)\n\n    def create_moe_param_groups(model):\n        from deepspeed.moe.utils import split_params_into_different_moe_groups_for_optimizer\n        parameters = {'params': [p for p in model.parameters()], 'name': 'parameters'}\n        return split_params_into_different_moe_groups_for_optimizer(parameters)\n    parameters = create_moe_param_groups(model)\n    optimizer = create_optimizer_v2(parameters, **optimizer_kwargs(cfg=args))\n    model, optimizer, _, _ = deepspeed.initialize(args=args,\n                                                    model=model,\n                                                    optimizer=optimizer)", "\n\ndef main():\n    model = create_model(...)\n    args.ep = 4\n\n\n    # replace vit.mlp with MoE layer\n    from deepspeed.utils import groups\n    from deepspeed.comm.comm import init_distributed\n    init_distributed(dist_backend='nccl')\n    groups._create_expert_and_data_parallel(args.ep)\n    for block in model.encoder.blocks:\n        original_mlp = block.mlp\n        feat_in = original_mlp.fc1.weight.shape[1]\n        block.mlp = deepspeed.moe.layer.MoE(hidden_size=feat_in, expert=block.mlp, num_experts=args.ep,\n                                            ep_size=args.ep, use_fmoe=True)\n\n    def create_moe_param_groups(model):\n        from deepspeed.moe.utils import split_params_into_different_moe_groups_for_optimizer\n        parameters = {'params': [p for p in model.parameters()], 'name': 'parameters'}\n        return split_params_into_different_moe_groups_for_optimizer(parameters)\n    parameters = create_moe_param_groups(model)\n    optimizer = create_optimizer_v2(parameters, **optimizer_kwargs(cfg=args))\n    model, optimizer, _, _ = deepspeed.initialize(args=args,\n                                                    model=model,\n                                                    optimizer=optimizer)", ""]}
{"filename": "examples/test_ddp.py", "chunked_list": ["import copy\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nimport timm\n\nfrom torchdistpackage import setup_distributed_slurm, NaiveDDP, fix_rand\n", "from torchdistpackage import setup_distributed_slurm, NaiveDDP, fix_rand\n\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.fc1 = nn.Linear(10, 10)\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, input):\n        out = self.fc1(input)\n        return self.fc2(out)", "\n\nsetup_distributed_slurm()\nrank = dist.get_rank()\n\n\ndef test_ddp(model, input_shape):\n    model = model.cuda()\n    model2 = copy.deepcopy(model)\n\n    my_ddp_model = NaiveDDP(model, sync=False, gradient_as_bucket_view=True)\n    torch_ddp_model = DDP(model2, broadcast_buffers=False)\n\n    optim = torch.optim.Adam(my_ddp_model.parameters(), lr=0.00015)\n    optim2 = torch.optim.Adam(torch_ddp_model.parameters(), lr=0.00015)\n\n    for i in range(10):\n        # make sure initial param is equal\n        for p1, p2 in zip(my_ddp_model.parameters(), torch_ddp_model.parameters()):\n            if not torch.allclose(p1, p2):\n                import pdb;pdb.set_trace()\n                assert False, \"model param not equal\"\n\n        x = torch.rand(input_shape).cuda()  # + rank \uff08should make sure inputs are the same)\n\n        optim.zero_grad()\n        optim2.zero_grad()\n\n        out = my_ddp_model(x)\n        out.sum().backward()\n        if hasattr(my_ddp_model, \"reduce_gradients\"):\n            my_ddp_model.reduce_gradients()\n\n        out2 = torch_ddp_model(x)\n        out2.sum().backward()\n\n        assert torch.allclose(out2, out)\n\n        # compare grads\n        for p1, p2 in zip(my_ddp_model.parameters(), torch_ddp_model.parameters()):\n            if (p1.grad - p2.grad).sum() > 1e-6:\n                if not torch.allclose(p1.grad, p2.grad):\n                    import pdb\n\n                    pdb.set_trace()\n                    assert False\n\n        optim.step()\n        optim2.step()\n        if rank == 0:\n            print(\"passed round -- \", i)", "\n\ndef test_simple_module():\n    model = MyModule().cuda()\n    test_ddp(model, [3, 10])\n\n\ndef test_timm_resnet():\n    model = timm.create_model(\"resnet50\", pretrained=False).cuda()\n    test_ddp(model, [4, 3, 224, 224])", "\n\ndef test_timm_vit():\n    model = timm.create_model(\"vit_large_patch16_224_in21k\", pretrained=False).cuda()\n    test_ddp(model, [4, 3, 224, 224])\n\n\nfix_rand()\n\ntest_simple_module()", "\ntest_simple_module()\ntest_timm_resnet()\n# test_timm_vit()\n"]}
{"filename": "examples/model_parallel/test_attn.py", "chunked_list": ["import torch\nimport torch.distributed as dist\n\nfrom torchdistpackage.parallel import Attention,TpAttention\n\nfrom torchdistpackage import fix_rand, setup_distributed_slurm\n\nsetup_distributed_slurm()\nfix_rand()\n\ndef test_attn(nh=8, in_dim=1024, drop=0., seq_len=128):\n    attn = Attention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n    opt = torch.optim.AdamW(attn.parameters())\n\n\n    tp_attn = TpAttention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n    opt_tp = torch.optim.AdamW(tp_attn.parameters())\n\n    tp_attn.qkv.init_weight_from_full_attn(attn.qkv.weight)\n    tp_attn.proj.init_weight_from_full(attn.proj.weight)\n\n\n    for _ in range(2):\n        inp = torch.rand((32, seq_len, in_dim)).cuda()\n\n        opt_tp.zero_grad()\n        opt.zero_grad()\n\n        out = attn(inp)\n        tp_out = tp_attn(inp)\n\n        assert torch.allclose(out, tp_out)\n        print(\"fwd passed\")\n\n        out.mean().backward()\n        tp_out.mean().backward()\n\n        grad_out_buffer_2 = [torch.empty_like(tp_attn.proj.linear.weight.grad) for _ in range(dist.get_world_size())]\n        dist.all_gather(grad_out_buffer_2, tp_attn.proj.linear.weight.grad)\n        fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\n        if not torch.allclose(attn.proj.weight.grad, fc2_grad_full):\n            import pdb;pdb.set_trace()\n        print(\"bwd passed\")\n\n        opt.step()\n        opt_tp.step()", "fix_rand()\n\ndef test_attn(nh=8, in_dim=1024, drop=0., seq_len=128):\n    attn = Attention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n    opt = torch.optim.AdamW(attn.parameters())\n\n\n    tp_attn = TpAttention(in_dim, num_heads=nh, attn_drop=drop, proj_drop=drop).cuda()\n    opt_tp = torch.optim.AdamW(tp_attn.parameters())\n\n    tp_attn.qkv.init_weight_from_full_attn(attn.qkv.weight)\n    tp_attn.proj.init_weight_from_full(attn.proj.weight)\n\n\n    for _ in range(2):\n        inp = torch.rand((32, seq_len, in_dim)).cuda()\n\n        opt_tp.zero_grad()\n        opt.zero_grad()\n\n        out = attn(inp)\n        tp_out = tp_attn(inp)\n\n        assert torch.allclose(out, tp_out)\n        print(\"fwd passed\")\n\n        out.mean().backward()\n        tp_out.mean().backward()\n\n        grad_out_buffer_2 = [torch.empty_like(tp_attn.proj.linear.weight.grad) for _ in range(dist.get_world_size())]\n        dist.all_gather(grad_out_buffer_2, tp_attn.proj.linear.weight.grad)\n        fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\n        if not torch.allclose(attn.proj.weight.grad, fc2_grad_full):\n            import pdb;pdb.set_trace()\n        print(\"bwd passed\")\n\n        opt.step()\n        opt_tp.step()", "\n\n\ntest_attn(nh=8, in_dim=1024, seq_len=128)"]}
{"filename": "examples/model_parallel/test_tpmlp.py", "chunked_list": ["import torch\nimport torch.distributed as dist\n\nfrom torchdistpackage import setup_distributed_slurm\nfrom torchdistpackage import fix_rand\n\nfrom torchdistpackage.parallel import TpMlp, Mlp\n\n\n\ndef test_mlp(dim1=1024, in_feat=2048, outfeat=2048, hiddenfeat=8192):\n    input = torch.rand((dim1, in_feat)).cuda()\n    dist.broadcast(input, 0)\n\n    mlp = Mlp(in_feat, hiddenfeat, outfeat).cuda()\n    tp_mlp = TpMlp(in_feat, hiddenfeat, outfeat).cuda()\n\n    tp_mlp.fc2.init_weight_from_full(mlp.fc2.weight)\n    tp_mlp.fc1.init_weight_from_full(mlp.fc1.weight)\n\n    mlp_out = mlp(input)\n\n    tp_mlp_out = tp_mlp(input)\n\n    assert torch.allclose(mlp_out, tp_mlp_out)\n    print(\"fwd passed\")\n\n    mlp_out.mean().backward()\n    tp_mlp_out.mean().backward()\n\n    grad_out_buffer_2 = [torch.empty_like(tp_mlp.fc2.linear.weight.grad) for _ in range(dist.get_world_size())]\n    dist.all_gather(grad_out_buffer_2, tp_mlp.fc2.linear.weight.grad)\n    fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\n    assert torch.allclose(mlp.fc2.weight.grad, fc2_grad_full)\n\n    grad_out_buffer_1 = [torch.empty_like(tp_mlp.fc1.linear.weight.grad) for _ in range(dist.get_world_size())]\n    dist.all_gather(grad_out_buffer_1, tp_mlp.fc1.linear.weight.grad)\n    fc1_grad_full = torch.cat(grad_out_buffer_1, dim=1)\n    assert torch.allclose(mlp.fc1.weight.grad, fc1_grad_full, rtol=1e-04, atol=1e-04)\n    print(\"bwd passed\")", "\n\ndef test_mlp(dim1=1024, in_feat=2048, outfeat=2048, hiddenfeat=8192):\n    input = torch.rand((dim1, in_feat)).cuda()\n    dist.broadcast(input, 0)\n\n    mlp = Mlp(in_feat, hiddenfeat, outfeat).cuda()\n    tp_mlp = TpMlp(in_feat, hiddenfeat, outfeat).cuda()\n\n    tp_mlp.fc2.init_weight_from_full(mlp.fc2.weight)\n    tp_mlp.fc1.init_weight_from_full(mlp.fc1.weight)\n\n    mlp_out = mlp(input)\n\n    tp_mlp_out = tp_mlp(input)\n\n    assert torch.allclose(mlp_out, tp_mlp_out)\n    print(\"fwd passed\")\n\n    mlp_out.mean().backward()\n    tp_mlp_out.mean().backward()\n\n    grad_out_buffer_2 = [torch.empty_like(tp_mlp.fc2.linear.weight.grad) for _ in range(dist.get_world_size())]\n    dist.all_gather(grad_out_buffer_2, tp_mlp.fc2.linear.weight.grad)\n    fc2_grad_full = torch.cat(grad_out_buffer_2, dim=0)\n\n    assert torch.allclose(mlp.fc2.weight.grad, fc2_grad_full)\n\n    grad_out_buffer_1 = [torch.empty_like(tp_mlp.fc1.linear.weight.grad) for _ in range(dist.get_world_size())]\n    dist.all_gather(grad_out_buffer_1, tp_mlp.fc1.linear.weight.grad)\n    fc1_grad_full = torch.cat(grad_out_buffer_1, dim=1)\n    assert torch.allclose(mlp.fc1.weight.grad, fc1_grad_full, rtol=1e-04, atol=1e-04)\n    print(\"bwd passed\")", "\n\n\nif __name__=='__main__':\n    fix_rand()\n    setup_distributed_slurm()\n    test_mlp()\n"]}
{"filename": "examples/model_parallel/test_pipeline.py", "chunked_list": ["import os\nimport torch\nimport torch.nn as nn\n\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n", "from torch.utils.data.distributed import DistributedSampler\n\nfrom torchdistpackage import tpc, setup_distributed_slurm, test_comm\nfrom torchdistpackage.parallel import partition_uniform, forward_backward\n\n\nclass DummyClsDataset(Dataset):\n    def __init__(self, shape, num_samples=1000):\n        self.num_samples = num_samples\n        self.shape = shape\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        del idx\n\n        img = torch.randn(self.shape)\n        label = torch.randint(0, 10, (1,)).squeeze()\n\n        return img, label", "\n\ndef get_dataloaders(\n    batch_size, batch_shape, train_samples, test_samples\n) -> Tuple[DataLoader, DataLoader]:\n    \"\"\" get dataloaders \"\"\"\n    train_data = DummyClsDataset(batch_shape, train_samples)\n    sampler = DistributedSampler(train_data, shuffle=True)\n\n    train_dataloader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        num_workers=0,\n        pin_memory=True,\n        sampler=sampler,\n    )\n\n    test_dataloader = DataLoader(\n        DummyClsDataset(batch_shape, test_samples), batch_size=batch_size, shuffle=True\n    )\n\n    return train_dataloader, test_dataloader", "\n\ndef build_seq_model():\n    model = nn.Sequential(\n        nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10), nn.ReLU(), nn.ReLU()\n    )\n    return model\n\n\n# Define some config", "\n# Define some config\nBATCH_SIZE = 512\nNUM_EPOCHS = 2\nNUM_CHUNKS = 2\nNUM_MICRO_BATCHES = 4\n\n\nsetup_distributed_slurm()\nworld_size = int(os.environ[\"SLURM_NTASKS\"])", "setup_distributed_slurm()\nworld_size = int(os.environ[\"SLURM_NTASKS\"])\npp_size = 2\ndist_config = [(\"pipe\", pp_size), (\"data\", world_size / (pp_size))]\ntpc.setup_process_groups(dist_config)\n\nmodel = build_seq_model().cuda()\nmodel = nn.Sequential(*partition_uniform(model)).cuda()\n\n", "\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# build dataloader\nroot = os.environ.get(\"DATA\", \"./data\")\ntrain_dataloader, test_dataloader = get_dataloaders(BATCH_SIZE, [10], 4000, 100)\ntest_dataloader = None\n", "test_dataloader = None\n\n\ndef pp_fwd_fn(ins):\n    input_img = ins\n    img_feat = model(input_img)\n\n    if tpc.is_last_in_pipeline_group():\n        loss = img_feat.mean()\n        return loss\n\n    return img_feat", "\n\ndef local_forward_backward(inp, model):\n    inputs = []\n    if tpc.is_first_in_pipeline_group():\n        inputs.append(inp)\n\n    forward_backward(\n        optimizer,\n        pp_fwd_fn,\n        None,\n        inputs,\n        num_microbatches=NUM_MICRO_BATCHES,\n        forward_only=False,\n        dtype=torch.float32,\n        scatter_gather_tensors=False,\n    )", "\n\nfor epoch in range(NUM_EPOCHS):\n\n    for img, label in train_dataloader:\n        img = img.cuda()\n        label = label.cuda()\n\n        local_forward_backward(img, model)\n        optimizer.step()\n        print(\"-------------\")", ""]}
{"filename": "examples/model_parallel/test_transformer.py", "chunked_list": ["\nimport torch\nimport torch.distributed as dist\n\nfrom torchdistpackage.parallel.tensor_parallel.transformer import Transformer\n\nfrom torchdistpackage import fix_rand, setup_distributed_slurm\n\nsetup_distributed_slurm()\nfix_rand()", "setup_distributed_slurm()\nfix_rand()\n\n\ndef test_model(dim, depth, nh=2, dtype=torch.float):\n    model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=False, sequence_parallel=False).cuda().to(dtype)\n    # tp_model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=True, sequence_parallel=False).cuda().to(dtype)\n    tp_model = Transformer(dim, depth=depth, num_heads=nh, tensor_parallel=True, sequence_parallel=True).cuda().to(dtype)\n\n\n    opt = torch.optim.AdamW(model.parameters())\n    tp_opt = torch.optim.AdamW(tp_model.parameters())\n\n    for ind in range(len(model.blocks)):\n        tp_model.blocks[ind].init_from_full(model.blocks[ind])\n        # sp_model.blocks[ind].init_from_full(model.blocks[ind])\n\n    for _ in range(10):\n        inp = torch.rand((32, 1024, dim)).cuda().to(dtype)\n\n        opt.zero_grad()\n\n        out = model(inp)\n        tp_out = tp_model(inp)\n        assert torch.allclose(out, tp_out, rtol=1e-1, atol=1e-02)\n\n        import pdb;pdb.set_trace()\n        # TODO: fix this mis alignment\n        assert torch.allclose(out, tp_out, rtol=1e-2, atol=1e-02)\n        assert torch.allclose(out, tp_out, rtol=1e-04, atol=1e-04)\n        assert torch.allclose(out, tp_out, rtol=1e-05, atol=1e-05)\n        out.mean().backward()\n        tp_out.mean().backward()\n\n        opt.step()\n        tp_opt.step()", "\ntest_model(1024, 8)"]}
