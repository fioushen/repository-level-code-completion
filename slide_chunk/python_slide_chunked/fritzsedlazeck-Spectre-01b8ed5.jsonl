{"filename": "spectreCNVPopulation.py", "chunked_list": ["import gzip\nimport os\nimport logging as logger\nimport traceback\nimport json\nfrom util.outputWriter import VCFOutput\nfrom analysis.cnv_candidate import CNVCandidate\n\n\nclass SpectrePopulation(object):\n    def __init__(self, sample_id, output_dir, genome_info):\n        self.logger = logger\n        self.sample_id = sample_id\n        self.output_dir = output_dir\n        self.final_candidates = {}  # Holds all CNVs calls that have been analyzed and filtered by Spectre\n        self.raw_candidates = {}  # Holds all raw CNVs calls before any filtering was applied by Spectre\n        self.cnv_call_list = {}  # Holds resulting cNV calls.\n        self.genome_info = genome_info\n        self.as_dev = False  # TODO get outside propagation\n\n    def merge_genome_info(self, new_genome_info: dict) -> None:\n        \"\"\"\n        Merging genome information incase different genomic information is stored in the .spc files.\n         This ensures the correct metadata information in the resulting population output VCF.\n        :param new_genome_info: dict of genome information\n        :return: None\n        \"\"\"\n        if not self.genome_info:\n            self.genome_info = new_genome_info.copy()\n        else:\n            if self.genome_info.keys() != new_genome_info.keys():\n                logger.warning(\"Genomics information of provided samples are not matching!\")\n            for key1, values1 in self.genome_info.items():\n                for key2, values2 in new_genome_info.items():\n                    if key1 == key2:\n                        for value2 in values2:\n                            if value2 not in values1:\n                                values1.append(value2)\n\n    def load_files(self, files: list) -> None:\n        \"\"\"\n        Determination of loading process for the candidate generation.\n        :param files: list of paths to the .spc\n        :return: None\n        \"\"\"\n\n        for file in files:\n            try:\n                if os.path.exists(file):\n                    if str(file).__contains__(\".spc\") or str(file).__contains__(\".spc.gz\"):\n                        self.load_candidates_from_intermediate_file_spc(file)\n                else:\n                    raise\n            except:\n                self.logger.error(f\"File does not exist! Provided : {file}\")\n\n    def load_candidates_from_intermediate_file_spc(self, file) -> None:\n        \"\"\"\n        Load candidates from a json format file and converting the content to candidates.\n        :param file: path to pickled file\n        :return: None\n        \"\"\"\n        spc_dict = dict()\n        try:\n            if '.gz' in file:\n                with gzip.open(file, \"rt\", encoding=\"UTF-8\") as input_file:\n                    spc_dict = json.load(input_file)\n            else:\n                with open(file, \"rt\") as input_file:\n                    spc_dict = json.load(input_file)\n            # Convert dictionary to candidates\n            filename = os.path.basename(file).split(\".spc\")[0]\n            self.convert_spc_to_candidate_list(filename, spc_dict)\n        except:\n            self.logger.error(traceback.print_exc())\n            self.logger.error(f\"Check if file meets the JSON standard. Error in file {file}\")\n        pass\n\n    def convert_dict_to_candidate_list(self,filename, candidates_dict):\n        result = dict([(chrom, []) for chrom in candidates_dict.keys()])\n        for chrom, candidates in candidates_dict.items():\n            new_candidate = CNVCandidate()\n            for candidate in candidates:\n                for candidate_key, candidate_value in candidate.items():\n                    if candidate_key in new_candidate.__dict__.keys():\n                        new_candidate.__setattr__(candidate_key, candidate_value)\n                        new_candidate.__setattr__('sample_origin', filename)  # update source to filename\n                        pass\n                new_candidate.reinitialize_candidate_values()\n                result[chrom].append(new_candidate)\n        return result\n\n    def convert_spc_to_candidate_list(self, filename, candidate_dict: dict):\n        if \"spectre\" not in candidate_dict[\"metadata\"][\"source\"]:\n            self.logger.warning(\"Provided .spc file does not originate from Spectre.\")\n            self.logger.warning(\"Trying to convert the provided file\")\n\n        # Get genome information\n        if \"genome_info\" in candidate_dict.keys():\n            self.merge_genome_info(candidate_dict['genome_info'])\n\n        # Get final/refined cnvs\n        if 'refined_cnvs' in candidate_dict.keys():\n            # Init candidate dictionary\n            if filename not in self.final_candidates.keys():\n                self.final_candidates[filename] = dict()\n            self.final_candidates[filename] = self.convert_dict_to_candidate_list(filename, candidate_dict['refined_cnvs'])\n\n        # Get raw cnvs\n        if 'raw_cnvs' in candidate_dict.keys():\n            if filename not in self.raw_candidates.keys():\n                self.raw_candidates[filename] = dict()\n            self.raw_candidates[filename] = self.convert_dict_to_candidate_list(filename, candidate_dict['raw_cnvs'])\n        pass\n\n    def cnv_call_population(self) -> None:\n        \"\"\"\n        Starts CNV population calling\n        :return: None\n        \"\"\"\n        self.logger.info(\"Starting population CNV calls\")\n        self.call_cnv()\n\n    @staticmethod\n    def candidate_overlapping(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates are overlapping\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if candidates are overlapping.\n        \"\"\"\n        return cnv1.start <= cnv2.start <= cnv1.end or cnv1.start <= cnv2.end <= cnv1.end\n\n    @staticmethod\n    def candidate_same_cn(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates have the same copy number status\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if the copy number status matches.\n        \"\"\"\n        return cnv1.cn_status == cnv2.cn_status\n\n    @staticmethod\n    def candidate_same_cn_type(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates have the same copy number type\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if the copy number type matches.\n        \"\"\"\n        return cnv1.type == cnv2.type\n\n    def call_cnv_final_candidates(self) -> None:\n        \"\"\"\n        Creating a structure which holds all overlapping CNVs of the final CNV candidates.\n        :return: None\n        \"\"\"\n        # everything against everything\n        for samples_key1, samples_values1 in self.final_candidates.items():\n            for samples_key2, samples_values2 in self.final_candidates.items():\n                for sample_chrom1, sample_values1 in samples_values1.items():\n                    for sample_chrom2, sample_values2 in samples_values2.items():\n                        # check for same chromosome\n                        if sample_chrom1 == sample_chrom2:\n                            # all individual samples against each other\n                            for sample1 in sample_values1:\n                                for sample2 in sample_values2:\n                                    is_overlapping = self.candidate_overlapping(sample1, sample2)\n                                    is_same_cn = self.candidate_same_cn(sample1, sample2)\n                                    is_same_cn_type = self.candidate_same_cn_type(sample1, sample2)\n                                    if is_overlapping and is_same_cn and is_same_cn_type:\n                                        # check if chromosome exists in cnv_call_list\n                                        if sample_chrom1 not in self.cnv_call_list.keys():\n                                            self.cnv_call_list[sample_chrom1] = []  # create list for\n                                        # check if sample1 is in the list\n                                        if sample1 not in self.cnv_call_list[sample_chrom1]:\n                                            self.cnv_call_list[sample_chrom1].append(sample1)\n\n                                        # check if all keys are available in sampel1\n                                        if sample1.support_cnv_calls.keys() != self.final_candidates.keys():\n                                            for key in self.final_candidates.keys():\n                                                if key not in sample1.support_cnv_calls.keys():\n                                                    sample1.support_cnv_calls[key] = set()\n                                            sample1.support_cnv_calls = dict(sorted(sample1.support_cnv_calls.items()))\n\n                                        # add sample2 to the support cnvs in sample1\n                                        sample1.support_cnv_calls[samples_key2].add(sample2)\n\n    def cnv_lookup_in_raw_candidates(self) -> None:\n        \"\"\"\n        Check if any of the final CNVs are supported by any non-refined CNVs. If a CNV is found it will be\n        added to the candidate.\n        :return: None\n        \"\"\"\n        missed_cnt = 0\n        for variant_key, variants in self.cnv_call_list.items():\n            for variant in variants:\n                for raw_sample_key, value in self.raw_candidates.items():\n                    for chrom_key, candidates in value.items():\n                        for candidate in candidates:\n                            if variant.sample_origin != candidate.sample_origin:\n                                # qualification checks\n                                is_overlapping = self.candidate_overlapping(variant, candidate)\n                                is_same_cn = self.candidate_same_cn(variant, candidate)\n                                is_same_cn_type = self.candidate_same_cn_type(variant, candidate)\n                                if is_overlapping and is_same_cn and is_same_cn_type:\n                                    variant.support_cnv_calls[candidate.sample_origin].add(candidate)\n                                    missed_cnt += 1\n\n    def call_cnv(self) -> None:\n        \"\"\"\n        Starts CNV calling with CNVs from multiple samples.\n        :return: None\n        \"\"\"\n        self.logger.info(f\"Starting population mode with samples: {', '.join(list(self.final_candidates.keys()))}\")\n        # generating union table of final overlaps\n        self.call_cnv_final_candidates()\n        # look up if all missing fields are covered by any raw cnv call\n        self.cnv_lookup_in_raw_candidates()\n        # Writing results to disk\n        output_file = f\"{self.output_dir}/population_mode_{self.sample_id}.vcf\"\n        self.logger.info(f\"Writing population VCF @: {output_file}\")\n        vcf_output = VCFOutput(output_file=output_file, genome_info=self.genome_info)\n        vcf_output.make_vcf(\n            chromosome_list=self.cnv_call_list.keys(), cnv_candidate_list=self.cnv_call_list,\n            sample_id=self.final_candidates.keys(), population_sample_ids=list(self.final_candidates.keys()))", "\nclass SpectrePopulation(object):\n    def __init__(self, sample_id, output_dir, genome_info):\n        self.logger = logger\n        self.sample_id = sample_id\n        self.output_dir = output_dir\n        self.final_candidates = {}  # Holds all CNVs calls that have been analyzed and filtered by Spectre\n        self.raw_candidates = {}  # Holds all raw CNVs calls before any filtering was applied by Spectre\n        self.cnv_call_list = {}  # Holds resulting cNV calls.\n        self.genome_info = genome_info\n        self.as_dev = False  # TODO get outside propagation\n\n    def merge_genome_info(self, new_genome_info: dict) -> None:\n        \"\"\"\n        Merging genome information incase different genomic information is stored in the .spc files.\n         This ensures the correct metadata information in the resulting population output VCF.\n        :param new_genome_info: dict of genome information\n        :return: None\n        \"\"\"\n        if not self.genome_info:\n            self.genome_info = new_genome_info.copy()\n        else:\n            if self.genome_info.keys() != new_genome_info.keys():\n                logger.warning(\"Genomics information of provided samples are not matching!\")\n            for key1, values1 in self.genome_info.items():\n                for key2, values2 in new_genome_info.items():\n                    if key1 == key2:\n                        for value2 in values2:\n                            if value2 not in values1:\n                                values1.append(value2)\n\n    def load_files(self, files: list) -> None:\n        \"\"\"\n        Determination of loading process for the candidate generation.\n        :param files: list of paths to the .spc\n        :return: None\n        \"\"\"\n\n        for file in files:\n            try:\n                if os.path.exists(file):\n                    if str(file).__contains__(\".spc\") or str(file).__contains__(\".spc.gz\"):\n                        self.load_candidates_from_intermediate_file_spc(file)\n                else:\n                    raise\n            except:\n                self.logger.error(f\"File does not exist! Provided : {file}\")\n\n    def load_candidates_from_intermediate_file_spc(self, file) -> None:\n        \"\"\"\n        Load candidates from a json format file and converting the content to candidates.\n        :param file: path to pickled file\n        :return: None\n        \"\"\"\n        spc_dict = dict()\n        try:\n            if '.gz' in file:\n                with gzip.open(file, \"rt\", encoding=\"UTF-8\") as input_file:\n                    spc_dict = json.load(input_file)\n            else:\n                with open(file, \"rt\") as input_file:\n                    spc_dict = json.load(input_file)\n            # Convert dictionary to candidates\n            filename = os.path.basename(file).split(\".spc\")[0]\n            self.convert_spc_to_candidate_list(filename, spc_dict)\n        except:\n            self.logger.error(traceback.print_exc())\n            self.logger.error(f\"Check if file meets the JSON standard. Error in file {file}\")\n        pass\n\n    def convert_dict_to_candidate_list(self,filename, candidates_dict):\n        result = dict([(chrom, []) for chrom in candidates_dict.keys()])\n        for chrom, candidates in candidates_dict.items():\n            new_candidate = CNVCandidate()\n            for candidate in candidates:\n                for candidate_key, candidate_value in candidate.items():\n                    if candidate_key in new_candidate.__dict__.keys():\n                        new_candidate.__setattr__(candidate_key, candidate_value)\n                        new_candidate.__setattr__('sample_origin', filename)  # update source to filename\n                        pass\n                new_candidate.reinitialize_candidate_values()\n                result[chrom].append(new_candidate)\n        return result\n\n    def convert_spc_to_candidate_list(self, filename, candidate_dict: dict):\n        if \"spectre\" not in candidate_dict[\"metadata\"][\"source\"]:\n            self.logger.warning(\"Provided .spc file does not originate from Spectre.\")\n            self.logger.warning(\"Trying to convert the provided file\")\n\n        # Get genome information\n        if \"genome_info\" in candidate_dict.keys():\n            self.merge_genome_info(candidate_dict['genome_info'])\n\n        # Get final/refined cnvs\n        if 'refined_cnvs' in candidate_dict.keys():\n            # Init candidate dictionary\n            if filename not in self.final_candidates.keys():\n                self.final_candidates[filename] = dict()\n            self.final_candidates[filename] = self.convert_dict_to_candidate_list(filename, candidate_dict['refined_cnvs'])\n\n        # Get raw cnvs\n        if 'raw_cnvs' in candidate_dict.keys():\n            if filename not in self.raw_candidates.keys():\n                self.raw_candidates[filename] = dict()\n            self.raw_candidates[filename] = self.convert_dict_to_candidate_list(filename, candidate_dict['raw_cnvs'])\n        pass\n\n    def cnv_call_population(self) -> None:\n        \"\"\"\n        Starts CNV population calling\n        :return: None\n        \"\"\"\n        self.logger.info(\"Starting population CNV calls\")\n        self.call_cnv()\n\n    @staticmethod\n    def candidate_overlapping(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates are overlapping\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if candidates are overlapping.\n        \"\"\"\n        return cnv1.start <= cnv2.start <= cnv1.end or cnv1.start <= cnv2.end <= cnv1.end\n\n    @staticmethod\n    def candidate_same_cn(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates have the same copy number status\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if the copy number status matches.\n        \"\"\"\n        return cnv1.cn_status == cnv2.cn_status\n\n    @staticmethod\n    def candidate_same_cn_type(cnv1: CNVCandidate, cnv2: CNVCandidate) -> bool:\n        \"\"\"\n        Checking if two CNV candidates have the same copy number type\n        :param cnv1: CNV candidate 1\n        :param cnv2: CNV candidate 2\n        :return: True if the copy number type matches.\n        \"\"\"\n        return cnv1.type == cnv2.type\n\n    def call_cnv_final_candidates(self) -> None:\n        \"\"\"\n        Creating a structure which holds all overlapping CNVs of the final CNV candidates.\n        :return: None\n        \"\"\"\n        # everything against everything\n        for samples_key1, samples_values1 in self.final_candidates.items():\n            for samples_key2, samples_values2 in self.final_candidates.items():\n                for sample_chrom1, sample_values1 in samples_values1.items():\n                    for sample_chrom2, sample_values2 in samples_values2.items():\n                        # check for same chromosome\n                        if sample_chrom1 == sample_chrom2:\n                            # all individual samples against each other\n                            for sample1 in sample_values1:\n                                for sample2 in sample_values2:\n                                    is_overlapping = self.candidate_overlapping(sample1, sample2)\n                                    is_same_cn = self.candidate_same_cn(sample1, sample2)\n                                    is_same_cn_type = self.candidate_same_cn_type(sample1, sample2)\n                                    if is_overlapping and is_same_cn and is_same_cn_type:\n                                        # check if chromosome exists in cnv_call_list\n                                        if sample_chrom1 not in self.cnv_call_list.keys():\n                                            self.cnv_call_list[sample_chrom1] = []  # create list for\n                                        # check if sample1 is in the list\n                                        if sample1 not in self.cnv_call_list[sample_chrom1]:\n                                            self.cnv_call_list[sample_chrom1].append(sample1)\n\n                                        # check if all keys are available in sampel1\n                                        if sample1.support_cnv_calls.keys() != self.final_candidates.keys():\n                                            for key in self.final_candidates.keys():\n                                                if key not in sample1.support_cnv_calls.keys():\n                                                    sample1.support_cnv_calls[key] = set()\n                                            sample1.support_cnv_calls = dict(sorted(sample1.support_cnv_calls.items()))\n\n                                        # add sample2 to the support cnvs in sample1\n                                        sample1.support_cnv_calls[samples_key2].add(sample2)\n\n    def cnv_lookup_in_raw_candidates(self) -> None:\n        \"\"\"\n        Check if any of the final CNVs are supported by any non-refined CNVs. If a CNV is found it will be\n        added to the candidate.\n        :return: None\n        \"\"\"\n        missed_cnt = 0\n        for variant_key, variants in self.cnv_call_list.items():\n            for variant in variants:\n                for raw_sample_key, value in self.raw_candidates.items():\n                    for chrom_key, candidates in value.items():\n                        for candidate in candidates:\n                            if variant.sample_origin != candidate.sample_origin:\n                                # qualification checks\n                                is_overlapping = self.candidate_overlapping(variant, candidate)\n                                is_same_cn = self.candidate_same_cn(variant, candidate)\n                                is_same_cn_type = self.candidate_same_cn_type(variant, candidate)\n                                if is_overlapping and is_same_cn and is_same_cn_type:\n                                    variant.support_cnv_calls[candidate.sample_origin].add(candidate)\n                                    missed_cnt += 1\n\n    def call_cnv(self) -> None:\n        \"\"\"\n        Starts CNV calling with CNVs from multiple samples.\n        :return: None\n        \"\"\"\n        self.logger.info(f\"Starting population mode with samples: {', '.join(list(self.final_candidates.keys()))}\")\n        # generating union table of final overlaps\n        self.call_cnv_final_candidates()\n        # look up if all missing fields are covered by any raw cnv call\n        self.cnv_lookup_in_raw_candidates()\n        # Writing results to disk\n        output_file = f\"{self.output_dir}/population_mode_{self.sample_id}.vcf\"\n        self.logger.info(f\"Writing population VCF @: {output_file}\")\n        vcf_output = VCFOutput(output_file=output_file, genome_info=self.genome_info)\n        vcf_output.make_vcf(\n            chromosome_list=self.cnv_call_list.keys(), cnv_candidate_list=self.cnv_call_list,\n            sample_id=self.final_candidates.keys(), population_sample_ids=list(self.final_candidates.keys()))", ""]}
{"filename": "spectreCNV.py", "chunked_list": ["import os\nimport logging as logger\nimport util.mosdepthReader\nfrom analysis.analysis import CNVAnalysis\n\n\nclass SpectreCNV:\n\n    def __init__(self, coverage_dir, bin_size, out_dir, metadata_file_fasta, genome_info, sample_id=\"\",\n                 snv_file_vcf=\"\", only_chr_list=\"\", ploidy=2, min_cnv_len=1000000, as_dev=False, dev_params=None,\n                 debug_dir=\"\"):\n        self.as_dev = as_dev\n        # logger\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.genome_info = genome_info\n        self.metadata_reference = metadata_file_fasta\n        self.sample_id = sample_id if sample_id != \"\" else \"sample-\"  # TODO: random string\n        self.snv_file_vcf = snv_file_vcf\n        self.only_chr_list = only_chr_list\n        self.min_cnv_len = min_cnv_len\n        self.ploidy = ploidy\n        self.snv_file_bed_af = \"\"\n        self.mosdepth_data = None\n        self.out_bed = \"out.bed\"\n        self.out_vcf = \"out.vcf\"\n        self.bin_size = bin_size\n        self.out_dir = out_dir\n        self.__get_config(coverage_dir)  # fill 'self.operation_dict' working variable\n        # population\n        self.population = {}\n        # self.__get_population_config(population)\n        # dev\n        self.dev_params = dev_params  # all params from spectre_args obj (SpectreCallParam)\n        self.debug_dir = debug_dir\n        # self.cnv_analysis = None  # TODO init by using CNVAnalysis\n\n        self.cnv_analysis = CNVAnalysis(coverage_file=self.mosdepth_data.coverage_file,\n                                        coverage_mosdepth_data=self.mosdepth_data.mosdepth_summary_data,\n                                        output_directory=self.out_dir, outbed=self.out_bed, outvcf=self.out_vcf,\n                                        bin_size=self.bin_size, genome_info=self.genome_info, sample_id=self.sample_id,\n                                        metadata_ref=self.metadata_reference, snv_file=self.snv_file_vcf,\n                                        only_chr_list=self.only_chr_list, ploidy=self.ploidy, min_cnv_len=min_cnv_len,\n                                        as_dev=self.as_dev, dev_params=self.dev_params, debug_dir=self.debug_dir)\n\n    def coverage_dir_files(self, coverage_dir):\n        coverage_dir = os.path.abspath(os.path.expanduser(coverage_dir))\n        coverage_file = \"\"\n        coverage_summary_file = \"\"\n\n        for each_dir in os.listdir(coverage_dir):\n            if \"mosdepth.summary.txt\" in each_dir:\n                coverage_summary_file = f'{coverage_dir}/{each_dir}'\n            elif \".regions.bed.gz\" in each_dir and (\"csi\" not in each_dir and \"tbi\" not in each_dir):\n                coverage_file = f'{coverage_dir}/{each_dir}'\n            else:\n                pass\n        if coverage_file != \"\" and coverage_summary_file != \"\":\n            return [coverage_file, coverage_summary_file]\n        else:\n            self.logger.error(f'coverage file or summary not found, directory {coverage_dir} has the following files:\\n'\n                              f'  {os.listdir(coverage_dir)}')\n        return [\"\", \"\"]\n\n    def __get_config(self, coverage_dir):\n        self.logger.info(f'Spectre calculating for: {str(coverage_dir)} and bin size: {self.bin_size}')\n        # input coverage\n        [coverage_file, cov_summary_file] = self.coverage_dir_files(coverage_dir)\n        self.mosdepth_data = util.mosdepthReader.MosdepthReader(coverage_file, cov_summary_file)\n        self.mosdepth_data.summary_data()\n        # output bed dir/file\n        self.out_bed = os.path.join(os.path.join(self.out_dir, f'{self.sample_id}_cnv.bed'))\n        # output VCF dir/file\n        self.out_vcf = os.path.join(os.path.join(self.out_dir, f'{self.sample_id}_cnv.vcf'))\n\n    def __get_config_dict(self, coverage_dir) -> dict:\n        # get basic mosdepth coverage for the provided population sample\n        result = {}\n        self.logger.info(f\"Spectre calculating population sample for: {str(coverage_dir)} and bin size {self.bin_size}\")\n        [coverage_file, cov_summary_file] = self.coverage_dir_files(coverage_dir)\n        result[\"mosdepth_data\"] = util.mosdepthReader.MosdepthReader(coverage_file, cov_summary_file)\n        result[\"mosdepth_data\"].summary_data()  # calculate summary\n        return result\n\n    def __get_population_config(self, population_coverage_dirs):\n        # get config for the provided population samples\n        for population_coverage_dir in population_coverage_dirs:\n            population_sample = os.path.join(population_coverage_dir).split(\"/\")[-2]\n            self.logger.info(f\"{population_sample}\\t{population_coverage_dir}\")\n            self.population[population_sample] = self.__get_config_dict(population_coverage_dir)\n\n    def cnv_call(self):\n        # Data normalization\n        self.logger.info(\"Data normalization and outlier removal (right tail)\")\n        self.cnv_analysis.data_normalization()\n\n        # Coverage analysis\n        self.logger.info(f\"CNV calling - Coverage for sample: {self.sample_id}\")\n        # get raw CNV for original sample\n        self.cnv_analysis.call_cnv_coverage(write_csv=self.as_dev)\n        self.cnv_analysis.get_cnv_metrics()\n        # self.cnv_analysis.write_intermediate_candidates(\"raw\")\n        # refine cnvs\n        self.cnv_analysis.refine_cnv_calls(self.as_dev)  # set to self.as_dev\n\n        # SNV analysis\n        if self.snv_file_vcf != \"\":\n            self.logger.info(\"CNV candidates by SNV\")\n            snv_file_basename_no_dot = \"_\".join(os.path.basename(self.snv_file_vcf).split('.'))\n            self.snv_file_bed_af = f'{self.out_dir}/{snv_file_basename_no_dot}.bed'\n            self.cnv_analysis.convert_vcf_to_tabular(self.snv_file_bed_af)\n            self.cnv_analysis.call_cnv_af_region()\n\n        # CNV metrics\n        # self.logger.warning(\"Disabled CNV metrics\")\n        self.logger.info(\"Calculate CNV metrics\")\n        self.cnv_analysis.get_cnv_metrics(refined_cnvs=True)\n\n        # Make output files\n        self.logger.info(\"Final candidates are written to spc file\")\n        self.cnv_analysis.write_intermediate_candidates()\n        self.logger.info(\"Results are writen to bed file\")\n        self.cnv_analysis.cnv_result_bed()\n        self.logger.info(\"Results are writen to VCF file\")\n        self.cnv_analysis.cnv_result_vcf()\n        self.logger.info(\"Result plot in progress\")\n        self.cnv_analysis.cnv_plot()\n\n        # End\n        self.logger.info(f\"Output dir: {self.out_dir}\")\n        self.logger.info(f\"Done with sample {self.sample_id}\")", "        # sys.exit(0)\n"]}
{"filename": "spectre.py", "chunked_list": ["#!/usr/bin/env python3\nimport argparse\nimport sys\nimport spectreCNV\nimport spectreCNVPopulation\nimport os\nimport pysam\nimport logging as logger\nfrom util.metadata.metadataCollector import FastaRef\nfrom multiprocessing import Pool", "from util.metadata.metadataCollector import FastaRef\nfrom multiprocessing import Pool\n\n\nclass SpectreCallParam(object):\n    def __init__(self):\n        self.bin_size = 1  # in kb\n        self.coverage_dir = \"\"\n        self.sample_id = \"\"\n        self.out_dir = \"\"\n        self.reference = \"\"\n        self.metadata = \"\"\n        self.snv = \"\"\n        self.n_size = 5\n        self.save_only = False  # this one is not updated as we need the return to occur\n        self.black_list = \"\"\n        self.only_chr_list = \"\"\n        self.ploidy = 2\n        self.min_cnv_len = 1000000\n        self.call_from_console = False\n        # dev/debug + hidden params\n        self.as_dev = False\n        self.max_std_outlier_rm = 5\n        self.mosdepth_cov_genome_chr_diff = 0.10  # 10%\n        self.lower_2n_threshold = 1.5\n        self.upper_2n_threshold = 2.5\n        self.cov_diff_threshold = 0.80\n        self.dist_proportion = 0.25\n        self.candidate_final_threshold = 100000  # 100kb\n        self.population_mosdepth = []\n        self.threads = 1\n        self.run_population_mode = False\n\n    def set_params_from_args(self, user_args):\n        self.bin_size = user_args.bin_size\n        self.coverage_dir = user_args.coverage_dir\n        self.sample_id = user_args.sample_id\n        self.out_dir = user_args.output_dir\n        self.reference = user_args.reference\n        self.metadata = user_args.metadata\n        self.snv = user_args.snv_file\n        self.black_list = user_args.black_list_file\n        self.only_chr_list = user_args.only_chr_list\n        self.ploidy = user_args.ploidy\n        self.min_cnv_len = user_args.min_cnv_len\n        self.n_size = user_args.n_size\n        self.threads = user_args.threads\n        self.run_population_mode = user_args.run_population_mode\n\n        # dev/debug + hidden params\n        self.as_dev = user_args.as_dev\n        self.max_std_outlier_rm = user_args.max_std_outlier_rm  # 5\n        self.mosdepth_cov_genome_chr_diff = user_args.mosdepth_cov_genome_chr_diff  # 0.10  # 10%\n        self.lower_2n_threshold = user_args.lower_2n_threshold  # 1.5\n        self.upper_2n_threshold = user_args.upper_2n_threshold  # 2.5\n        self.cov_diff_threshold = user_args.cov_diff_threshold  # 0.80\n        self.dist_proportion = user_args.dist_proportion  # 0.25\n        self.candidate_final_threshold = user_args.candidate_final_threshold  # 100000  # 100kb", "\n\n\nclass SpectreMetadataParam(object):\n    def __init__(self):\n        self.reference = \"\"\n        self.bin_size = 1\n        self.metadata = \"\"\n        self.out_dir = \"\"\n        self.n_size = 5\n        self.save_only = False\n        self.as_dev = False\n        self.black_list = \"\"\n        self.call_from_console = False\n\n    def set_params_from_args(self, user_args, metadata_from_console=False):\n        self.reference = user_args.reference\n        self.bin_size = user_args.bin_size\n        self.metadata = user_args.metadata\n        self.out_dir = user_args.output_dir\n        self.n_size = user_args.n_size\n        self.save_only = user_args.save_only\n        self.black_list = user_args.black_list_file\n        self.as_dev = user_args.as_dev\n        self.call_from_console = metadata_from_console", "\n\nclass SpectrePopulationMode(object):\n    def __init__(self):\n        self.candidates = []\n        self.sample_id = \"\"\n        self.out_dir = \"\"\n        self.reference = \"\"\n        self.as_dev = False\n\n    def set_params_from_args(self, user_args):\n        self.candidates = user_args.population_candidates\n        self.sample_id = user_args.sample_id\n        self.out_dir = user_args.output_dir\n        self.reference = user_args.reference\n        self.as_dev = user_args.as_dev", "\n\ndef outside_spectre_worker(si: dict):\n    worker = spectreCNV.SpectreCNV(coverage_dir=si[\"coverage_dir\"], bin_size=si[\"bin_size\"],\n                                   out_dir=si[\"out_dir\"], metadata_file_fasta=si[\"metadata_file_fasta\"],\n                                   genome_info=si[\"genome_info\"], sample_id=si[\"sample_id\"],\n                                   snv_file_vcf=si[\"snv_file_vcf\"], only_chr_list=si[\"only_chr_list\"],\n                                   ploidy=si[\"ploidy_arg\"],min_cnv_len=si[\"min_cnv_len\"], as_dev=si[\"as_dev\"],\n                                   dev_params=si[\"dev_params\"], debug_dir=si[\"debug_dir\"])\n    worker.cnv_call()\n    return worker.cnv_analysis.intermediate_candidates_file_location", "\n\nclass Spectre:\n\n    def __init__(self, as_dev=False):\n        # version\n        self.version = \"0.1-alpha\"\n        self.logger = logger\n        self.debug_dir = \"\"\n        # for spectre cnv caller\n        self.spectre_args = SpectreCallParam()\n        self.sample_dir_list = []\n        # for metadata/removeNs\n        self.metadata_args = SpectreMetadataParam()\n        # metadata from reference genome (Ns)\n        self.__mdr = dict()\n        # metadata from reference genome for VCF\n        self.genome = dict()\n        # for benchmark\n        self.benchmark = {\n            \"1\": 1\n        }\n        # for population mode\n        self.population_args = SpectrePopulationMode()\n\n    def display_version(self):\n        self.logger.info(f'Spectre version: {self.version}')\n\n    @staticmethod\n    def make_genome_info(genome_info_pysam):\n        genome_info = {\"chromosomes\": genome_info_pysam.references,\n                       \"chr_lengths\": genome_info_pysam.lengths,\n                       \"chr_lengths_by_name\": {}}\n        for chr_name, chr_len in zip(genome_info_pysam.references, genome_info_pysam.lengths):\n            genome_info[\"chr_lengths_by_name\"][chr_name] = chr_len\n        return genome_info\n\n    def meta_data_extraction(self):\n        # ----------- Metadata extraction from ref file  -----------\n        # self.metadata_args.as_dev  # Note: this is not used\n        reference_fas = self.metadata_args.reference\n        bin_size = self.metadata_args.bin_size\n        output_dir = os.path.abspath(os.path.expanduser(self.metadata_args.out_dir))\n        threshold = self.metadata_args.n_size\n        # if \"call_from_console\" then the meta_data_report serves as output only, otherwise as both\n        meta_data_report = self.metadata_args.metadata if not self.metadata_args.call_from_console \\\n            else f'{output_dir}/{self.metadata_args.metadata}'\n        default_metadata_name = f'{output_dir}/metadata.mdr'  # default\n        save_only = self.metadata_args.save_only\n        fasta_metadata = FastaRef()\n        blacklist_data_bed = self.metadata_args.black_list  # bed format\n        self.logger.info(\"Extraction of metadata is activated\")\n\n        # metadata parameter given?\n        if meta_data_report != \"\":\n            meta_data_report = os.path.abspath(os.path.expanduser(meta_data_report))\n        else:\n            self.logger.info(\"Looking for default metadata.mdr\")\n            meta_data_report = os.path.abspath(os.path.expanduser(default_metadata_name))\n        # metadata file exists\n        if not os.path.exists(meta_data_report):\n            self.logger.info(f'Extracting metadata from {reference_fas}')\n            metadata_result = fasta_metadata.get_n_regions(filepath=reference_fas, report_output_dir=output_dir,\n                                                           out_file_name=meta_data_report, threshold=threshold,\n                                                           bin_size=bin_size, save_only=save_only)\n        else:\n            self.logger.info(f'Extracting metadata from {meta_data_report}')\n            metadata_result = fasta_metadata.extract_n_regions_from_report(meta_data_report)\n        if blacklist_data_bed != \"\":\n            self.logger.debug(\"Using blacklist\")\n            blacklist_results = fasta_metadata.extract_blacklisted_regions(blacklist_data_bed)\n            metadata_result = fasta_metadata.merge_metadata(metadata_result, blacklist_results)\n        # return metadata object (dict) after writing to file?\n        if save_only:\n            pass\n        else:\n            self.logger.debug(\"returned meta Object\")\n            return metadata_result\n\n    def __set_genome_info(self, reference):\n        pysam_genome = pysam.FastaFile(reference)\n        self.genome = self.make_genome_info(pysam_genome)\n        pysam_genome.close()\n\n    def spectre_exe(self):\n        # Parameters\n        self.display_version()\n        self.logger.info(\"Spectre enabled\")\n        bin_size = self.spectre_args.bin_size\n        coverage_dirs = [os.path.abspath(os.path.expanduser(directory)) for directory in self.spectre_args.coverage_dir]\n        sample_ids = self.spectre_args.sample_id\n        output_dir = os.path.abspath(os.path.expanduser(self.spectre_args.out_dir))\n        reference = os.path.abspath(os.path.expanduser(self.spectre_args.reference))\n        metadata = self.spectre_args.metadata\n        snv_file = self.spectre_args.snv\n        black_list_bed = self.spectre_args.black_list\n        only_chr_list = self.spectre_args.only_chr_list\n        ploidy_arg = self.spectre_args.ploidy\n        min_cnv_len = self.spectre_args.min_cnv_len\n        threads = self.spectre_args.threads\n        as_dev = self.spectre_args.as_dev\n        run_population_mode = self.spectre_args.run_population_mode\n        # get the metadata\n        self.metadata_args.reference = reference\n        self.metadata_args.bin_size = bin_size\n        self.metadata_args.metadata = metadata\n        self.metadata_args.black_list = black_list_bed\n        self.metadata_args.out_dir = output_dir\n        self.metadata_args.n_size = self.spectre_args.n_size\n        self.metadata_args.save_only = self.spectre_args.save_only\n        self.__mdr = self.meta_data_extraction()\n        self.__set_genome_info(reference)  # genome information\n\n        # Setting up directories\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        if as_dev:\n            self.debug_dir = f\"{output_dir}/debug\"\n            if not os.path.exists(self.debug_dir):\n                os.makedirs(self.debug_dir)\n        else:\n            self.debug_dir = output_dir\n\n        # Preparing spectre instructions for multiprocess\n        spectre_instructions = []\n        for sample_id, coverage_dir in zip(sample_ids, coverage_dirs):\n            instructions = {\"coverage_dir\": coverage_dir, \"bin_size\": bin_size,\n                            \"metadata_file_fasta\": self.__mdr.copy(),\n                            \"out_dir\": output_dir, \"genome_info\": self.genome.copy(),\n                            \"sample_id\": sample_id, \"snv_file_vcf\": snv_file, \"only_chr_list\": only_chr_list,\n                            \"ploidy_arg\": ploidy_arg, \"as_dev\": as_dev, \"dev_params\": self.spectre_args,\n                            \"debug_dir\": self.debug_dir, \"min_cnv_len\":min_cnv_len}\n            spectre_instructions.append(instructions.copy())\n\n        # Distribute Samples over cores/threads\n        with Pool(processes=threads) as pool:\n            results = pool.map(outside_spectre_worker, tuple(spectre_instructions))\n        intermediate_file_paths = [x for x in results]\n\n        if run_population_mode and len(intermediate_file_paths) > 1:\n            self.population_exe(\"population_file\", intermediate_file_paths, output_dir, reference, as_dev)\n\n        self.logger.info(\"Spectre finished\")\n        sys.exit(0)\n\n    def population_exe(self, population_sample_name=\"\", population_intermediate_files=None, outputdir=\"\",reference=\"\", as_dev=False):\n        self.logger.info(\"Starting Spectre population mode\")\n        # Adjusting parameters\n        sample_ids = self.population_args.sample_id if population_sample_name == \"\" else population_sample_name\n        population_paths = self.population_args.candidates if not population_intermediate_files else population_intermediate_files\n        output_dir = os.path.abspath(os.path.expanduser(self.population_args.out_dir)) if outputdir == \"\" else outputdir\n        as_dev = self.population_args.as_dev if not as_dev else as_dev\n        reference = self.population_args.reference if not reference else reference\n\n        # Required to load the genome information, if any other file format than .spc is provided\n        if not any(\".spc\" in s for s in population_paths):\n            if not self.genome:\n                self.__set_genome_info(reference)\n\n        self.logger.info(f\"Population mode: Loaded samples {population_paths}\")\n        # Directory setup\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        if as_dev:\n            debug_dir = f\"{output_dir}/debug\"\n            if not os.path.exists(debug_dir):\n                os.makedirs(debug_dir)\n\n        # Population mode setup\n        __spectre_population_worker = spectreCNVPopulation.SpectrePopulation(sample_id=sample_ids,\n                                                                             output_dir=output_dir,\n                                                                             genome_info=self.genome)\n        __spectre_population_worker.load_files(population_paths)  # Loading files for population mode\n        __spectre_population_worker.cnv_call_population()  # Starting the population CNV calculations", "\n\n# Arguments\ndef get_arguments():\n    spectre_help = \"\"\"\n    vcf_utils <command> [<args>]\n    Spectre:\n        CNVCaller:\n            Required\n                --bin-size     Bin/Window size (same as Mosdepth)\n                --coverage     Coverage directory from Mosdepth output. Expects the following files:\n                                   <prefix>.mosdepth.summary.txt\n                                   <prefix>.regions.bed.gz\n                                   <prefix>.regions.bed.gz.csi\n                               Can be one or more directories. Example:\n                                    --coverage /path/dir1/ /path/dir2/\n                --sample-id    Sample name/ID. Can be one or more ID. Example:\n                                    --sample-id id1 id2\n                --output-dir   Output directory\n                --reference    Reference sequence used for mapping (for N removal)\n            Optional, if missing it will be created\n                --metadata     Metadata file for Ns removal\n            Optional\n                --blacklist    Blacklist in bed format for sites that will be ignored (Default = \"\")\n                --only-chr     Comma separated list of chromosomes to use\n                --ploidy       Set the ploidy for the analysis, useful for sex chromosomes (Default = 2)\n                --snv          VCF file containing the SNV for the same sample CNV want to be called\n                --n-size       Length of consecutive Ns (Default = 5)\n                --min_cnv_len  Minimum length of CNV (Default 1mb)\n                --population   Runs the population mode on all provided samples\n                --threads      Amount of threads (This will boost performance if multiple samples are provided)\n\n                \n        removeNs:\n            Required\n                --reference    Reference genome used for mapping\n                --output-dir   Output dir\n                --output-file  Output file for results\n                --bin-size     Bin/Window size (same as Mosdepth)\n            Optional\n                --blacklist    Blacklist in bed format for sites that will be ignored (Default = \"\")\n                --n-size       Length of consecutive Ns (Default = 5)\n                --save-only    Will only save the metadata file and not show the results in screen (Default = False)\n                \n        population:\n            Required\n                --candidates   At least 2 candidate files (.spc or .vcf) which should be taken into consideration for the population mode.\n                --sample-id    Name of the output file\n                --output-dir   Output directory\n            Optional\n                --reference    Reference sequence (Required if VCF files are used!)\n        Version:\n            version    Shows current version/build\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Spectre CNV caller\",\n        usage=spectre_help\n    )\n    subparsers = parser.add_subparsers(help=spectre_help, dest=\"command\")\n\n    # ############################################################################################ #\n    # Version\n    version_help = \"Gives the version number\"\n    subparser_version = subparsers.add_parser(\"version\", help=version_help)\n    subparser_version.add_argument('-0', '--0', action='store_true', required=False, dest='_0', default=False, help='')\n\n    # ############################################################################################ #\n    # CNV caller\n    cnv_caller_help = \"...\"\n    subparser_cnv_caller = subparsers.add_parser(\"CNVCaller\", help=cnv_caller_help)\n    # Required\n    subparser_cnv_caller.add_argument('-b', '--bin-size', type=int, required=True, dest='bin_size', default=500,\n                                      help='..., default = 1kb')\n    subparser_cnv_caller.add_argument('-c', '--coverage', type=str, required=True, dest='coverage_dir', default=\"\",\n                                      help='..., default = None', nargs='+')\n    subparser_cnv_caller.add_argument('-s', '--sample-id', type=str, required=True, dest='sample_id', default=\"\",\n                                      help='..., default = None', nargs='+')\n    subparser_cnv_caller.add_argument('-d', '--output-dir', type=str, required=True, dest='output_dir', default=\".\",\n                                      help='..., default = None')\n    subparser_cnv_caller.add_argument('-r', '--reference', type=str, required=True, dest='reference', default=\"\",\n                                      help='..., default = None')\n    # Optional, if missing will be created\n    subparser_cnv_caller.add_argument('-m', '--metadata', type=str, required=False, dest='metadata', default=\"\",\n                                      help='..., default = None')\n    # Optional\n    subparser_cnv_caller.add_argument('-v', '--snv', type=str, required=False, dest='snv_file', default=\"\",\n                                      help='...')\n    subparser_cnv_caller.add_argument('-l', '--blacklist', type=str, required=False, dest='black_list_file',\n                                      default=\"\",\n                                      help='...')\n    subparser_cnv_caller.add_argument('-o', '--only-chr', type=str, required=False, dest='only_chr_list', default=\"\",\n                                      help='...')\n    subparser_cnv_caller.add_argument('-p', '--ploidy', type=int, required=False, dest='ploidy', default=2,\n                                      help='..., default = 2')\n    subparser_cnv_caller.add_argument('-n', '--n-size', type=int, required=False, dest='n_size', default=5,\n                                      help='..., default = 5')\n    subparser_cnv_caller.add_argument('-mcl', '--min-cnv-len', type=int, required=False, dest='min_cnv_len', default=1000000,\n                                      help='..., default = 1000000')\n    subparser_cnv_caller.add_argument('-t', '--threads', type=int, required=False, dest='threads', default=1,\n                                      help='..., default = 1')\n    subparser_cnv_caller.add_argument('-i', '--population', action='store_true', required=False,\n                                      dest='run_population_mode', default=False, help='...s, default = False')\n\n    # Dev\n    subparser_cnv_caller.add_argument('-0', '--dev', action='store_true', required=False, dest='as_dev', default=False,\n                                      help='dev, default = False')\n    subparser_cnv_caller.add_argument('-01', '--dev-max-std-outlier-rm', type=int, required=False,\n                                      dest='max_std_outlier_rm', default=5, help='..., default = 5')\n    subparser_cnv_caller.add_argument('-02', '--mosdepth-cov-genome-chr-diff', type=float, required=False,\n                                      dest='mosdepth_cov_genome_chr_diff', default=0.10, help='..., default = 0.10')\n    subparser_cnv_caller.add_argument('-03', '--lower-2n-threshold', type=float, required=False,\n                                      dest='lower_2n_threshold', default=1.5, help='..., default = 2.5')\n    subparser_cnv_caller.add_argument('-04', '--upper-2n-threshold', type=float, required=False,\n                                      dest='upper_2n_threshold', default=2.5, help='..., default = 2.5')\n    subparser_cnv_caller.add_argument('-05', '--cov-diff-threshold', type=float, required=False,\n                                      dest='cov_diff_threshold', default=0.80, help='..., default = 0.80')\n    subparser_cnv_caller.add_argument('-06', '--dist-proportion', type=float, required=False, dest='dist_proportion',\n                                      default=0.25, help='..., default = 0.25')\n    subparser_cnv_caller.add_argument('-07', '--candidate-final-threshold', type=int, required=False,\n                                      dest='candidate_final_threshold', default=100000,\n                                      help='..., default = 100,000')  # 100kb\n\n    # ############################################################################################ #\n    # Metadata to remove Ns\n    metadata_help = \"...\"\n    subparser_metadata = subparsers.add_parser(\"removeNs\", help=metadata_help)\n    # Required\n    subparser_metadata.add_argument('-r', '--reference', type=str, required=True, dest='reference', default=\"\",\n                                    help='..., default = None')\n    subparser_metadata.add_argument('-d', '--output-dir', type=str, required=True, dest='output_dir', default=\"\",\n                                    help='..., default = None')\n    subparser_metadata.add_argument('-f', '--output-file', type=str, required=True, dest='metadata', default=\"\",\n                                    help='..., default = None')\n    subparser_metadata.add_argument('-b', '--bin-size', type=int, required=True, dest='bin_size', default=\"\",\n                                    help='..., default = None')\n    # Optional\n    subparser_metadata.add_argument('-n', '--n-size', type=int, required=False, dest='n_size', default=5,\n                                    help='..., default = 5')\n    subparser_metadata.add_argument('-s', '--save-only', action='store_true', required=False, dest='save_only',\n                                    default=False, help='save_only, default = False')\n    subparser_metadata.add_argument('-l', '--blacklist', type=str, required=False, dest='black_list_file', default=\"\",\n                                    help='...')\n    # Dev\n    subparser_metadata.add_argument('-0', '--dev', action='store_true', required=False, dest='as_dev', default=False,\n                                    help='dev, default = False')\n\n    # ############################################################################################ #\n    # Population mode\n    cnv_population_help = \"...\"\n    subparser_cnv_population = subparsers.add_parser(\"population\", help=cnv_population_help)\n    # Required\n    subparser_cnv_population.add_argument('-c', '--candidates', type=str, required=True, dest='population_candidates',\n                                          default=\"\", help='..., default = None', nargs='+')\n    subparser_cnv_population.add_argument('-s', '--sample-id', type=str, required=True, dest='sample_id', default=\"\",\n                                          help='..., default = None')\n    subparser_cnv_population.add_argument('-d', '--output-dir', type=str, required=True, dest='output_dir', default=\".\",\n                                          help='..., default = None')\n    subparser_cnv_population.add_argument('-0', '--dev', action='store_true', required=False, dest='as_dev',\n                                          default=False,\n                                          help='dev, default = False')\n    # Optional\n    subparser_cnv_population.add_argument('-r', '--reference', type=str, required=False, dest='reference', default=\"\",\n                                          help='..., default = None')\n\n    # ############################################################################################ #\n    args = parser.parse_args()\n    return args, spectre_help", "\n\ndef main():\n    spectre_args, spectre_help = get_arguments()\n    command = spectre_args.command\n    try:\n        run_as_dev = spectre_args.as_dev\n    except AttributeError:\n        run_as_dev = False\n    # logger init\n    logger.basicConfig(\n        format='spectre::%(process)d:%(levelname)s> %(message)s', level=logger.DEBUG, force=True) if run_as_dev else \\\n        logger.basicConfig(format='spectre::%(levelname)s> %(message)s', level=logger.INFO, force=True)\n    logger.debug(f' Debug output is enabled') if run_as_dev else None\n    spectre_run = Spectre(run_as_dev)\n    min_bin_size = 500\n    if command == \"CNVCaller\":\n        logger.error(\"Bin size too small\") if spectre_args.bin_size < min_bin_size else \"\"\n        # ARGS:  bin_size, coverage_file_bed, sample_id, output_dir, reference=\"\", metadata=\"\", ...\n        spectre_run.spectre_args.set_params_from_args(spectre_args)\n        spectre_run.spectre_exe()\n    elif command == \"removeNs\":\n        logger.error(\"Bin size too small\") if spectre_args.bin_size < min_bin_size else \"\"\n        # ARGS:  reference_fas, output_dir, meta_data_report, bin_size=500, threshold=5\n        metadata_call_from_console = True\n        spectre_run.metadata_args.set_params_from_args(spectre_args, metadata_call_from_console)\n        spectre_run.meta_data_extraction()\n    elif command == \"population\":\n        logger.error(\"at least two candidates are required\") if len(spectre_args.population_candidates) < 2 else \"\"\n        spectre_run.population_args.set_params_from_args(spectre_args)\n        spectre_run.population_exe()\n    elif command == \"version\":\n        spectre_run.display_version()\n    else:\n        logger.error(spectre_help)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "plots/plot.py", "chunked_list": ["import logging as logger\nimport matplotlib.pyplot as plot_engine\nfrom matplotlib import gridspec\nimport numpy as np\n\n\nclass CoveragePlot:\n    def __init__(self, as_dev=False):\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # the plot\n        self.figure = plot_engine.figure(figsize=(8, 4))\n        gs = gridspec.GridSpec(1, 1)\n        self.main_plot = plot_engine.subplot(gs[0])        # colors\n        self.coverage_color = \"#67a9cf\"\n        # legends\n        self.main = \"\"\n        self.x_axis = \"\"\n        self.y_axis = \"\"\n        self.file_prefix = \"test\"\n        self.output_directory = \"./\"\n\n    def plot_coverage(self, current_chromosome=\"\", coverage=None):\n        self.logger.debug(\"plotting coverage\")\n        # main plot\n        self.main_plot.plot(np.array(coverage[\"pos\"]), np.array(coverage[\"cov\"]), color=self.coverage_color,\n                            linewidth='0.5')\n        # save and close\n        self.figure.savefig(f'{self.output_directory}/{self.file_prefix}-{current_chromosome}.png', dpi=300)\n        self.logger.info(f'Plot saved: {self.file_prefix}-{current_chromosome}.png')\n        self.figure.clf()", "\n\nclass CNVPlot:\n    def __init__(self, as_dev=False):\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # the plot\n        self.figure = plot_engine.figure(figsize=(8, 4))\n        gs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])\n        self.main_plot = plot_engine.subplot(gs[0])\n        self.candidates_plot = plot_engine.subplot(gs[1])\n        self.candidates_plot.axes.get_yaxis().set_visible(False)\n        # colors\n        self.coverage_color = \"#67a9cf\"\n        self.cnv_color = {\"DUP\": \"#d73027\", \"DEL\": \"#1a9850\"}\n        # legends\n        self.main = \"\"\n        self.x_axis = \"\"\n        self.y_axis = \"\"\n        self.axis_ylim = {\"bottom\": 0, \"top\": 6}  # not showing over 6x coverage, min can not be lower than 0\n        self.file_prefix = \"test\"\n        self.output_directory = \"./\"\n\n    def plot_coverage_cnv(self, current_chromosome=\"\", stats=None, coverage=None, cnv_cand_list=None, bounds=None):\n        if stats is None or coverage is None:\n            self.logger.error(\"bot parameters are needed\")\n        self.logger.debug(\"plotting coverage + CNV\")\n        # main plot\n        self.main_plot.plot(np.array(coverage[\"pos\"]), np.array(coverage[\"cov\"]), color=self.coverage_color,\n                            linewidth='0.5')\n        self.main_plot.axes.set_ylim(bottom=self.axis_ylim[\"bottom\"], top=self.axis_ylim[\"top\"])\n        start = coverage[\"pos\"][0]\n        end = coverage[\"pos\"][-1]\n        self.candidates_plot.plot(np.array([start, end]), np.array([0, 0]), linewidth='0', color=\"#ffffff\")\n        # add CNV candidates\n        self.logger.info(f\"CNVs in chromosome: {current_chromosome}\")\n        if cnv_cand_list is not None:\n            for cnv in cnv_cand_list:\n                start = cnv.start\n                end = cnv.end\n                cnv_color = self.cnv_color[cnv.type]\n                self.candidates_plot.plot(np.array([start, end]), np.array([0, 0]), linewidth='5', color=cnv_color)\n        # save and close\n        self.main_plot.plot(np.array([1, stats.chromosome_len]), np.array([stats.median, stats.median]),\n                            linewidth='1', color=\"#000000\")\n        if bounds is not None:\n            [upperb, lowerb] = bounds if len(bounds) == 2 else [np.NaN, np.NaN]\n            self.main_plot.plot(np.array([1, stats.chromosome_len]), np.array([lowerb, lowerb]),\n                                linewidth='1', color=\"#dd3497\")\n            self.main_plot.plot(np.array([1, stats.chromosome_len]), np.array([upperb, upperb]),\n                                linewidth='1', color=\"#dd3497\")\n        self.figure.suptitle(f'{self.file_prefix} chromosome: {current_chromosome}')\n        self.figure.savefig(f'{self.output_directory}/plot-coverage-{self.file_prefix}-{current_chromosome}.png', dpi=300)\n        self.logger.info(f'Plot saved: plot-coverage-{self.file_prefix}-{current_chromosome}.png')\n        self.figure.clf()", ""]}
{"filename": "plots/__init__.py", "chunked_list": [""]}
{"filename": "util/__init__.py", "chunked_list": [""]}
{"filename": "util/vcf_parser.py", "chunked_list": ["import pandas as pd\nimport re\nimport logging as logger\nimport os\nimport pysam\nimport gzip\nfrom analysis.cnv_candidate import CNVCandidate\n\n\nclass VCFSNVParser(object):\n    def __init__(self, min_chromosome_len=1e6, as_dev=False):\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.min_chromosome_len = min_chromosome_len\n\n    @staticmethod\n    def __get_absolute_path(input_path):\n        return os.path.abspath(os.path.expanduser(input_path))\n\n    def __create_tabix_file(self, input_path):\n        self.logger.info(f\"No index file found - Creating new one at {input_path}.tbi\")\n        pysam.tabix_index(input_path, preset=\"bed\", force=True)\n\n    def vcf_pysam(self, path):\n        # setup\n        vcf_path = self.__get_absolute_path(path)\n        vcf_tabix_path = vcf_path + \".tbi\"\n\n        # checking for index file\n        if not os.path.isfile(vcf_tabix_path):\n            self.__create_tabix_file(vcf_path)\n\n        vcf_file = pysam.VariantFile(vcf_path)  # loading vcf file\n        records = vcf_file.fetch()\n        for record in records:\n            x = record.samples.values()\n            print()\n\n        for uid, x in enumerate(vcf_file.fetch()):\n            print(x.format.keys())\n            af = x.format.values()[4].number\n\n    def vcf_to_dataframe(self, path):\n        self.logger.debug(\"Converting vcf to dataframe\")\n        vcf_path = self.__get_absolute_path(path)\n\n        # pre allocation\n        vcf_entries = list()\n        vcf_file = open(vcf_path, \"r\") if \"gz\" not in vcf_path else gzip.open(vcf_path, \"rt\")\n        chrom_, start_, quality_, filter_, info_, format_, sample_ = \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n        # how AF is described by tool:\n        #   clair3   -> AF in FORMAT/SAMPLE\n        #   longshot -> AF not present, AC contains the number of reads for REF,ALT: AF=ALT/(REF+ALT)\n        #   p-m-deep -> AF not present, VAF in FORMAT/SAMPLE\n        #  if in info --- af = float(list(filter(lambda x: \"VAF\" in x, info_.split(\";\")))[0].split(\"=\")[1])\n        use_chromosomes = []\n        for line in vcf_file:\n            line = line.rstrip(\"\\n\")\n            # skip header\n            if line.startswith(\"#\"):\n                if line.__contains__(\"contig\"):\n                    # example:  ##contig=<ID=chr1_KI270706v1_random,length=175055>\n                    try:\n                        chr_id_len = re.search('.*<ID=(.*),length=(.*)>', line)\n                        [chr_id, chr_len] = [chr_id_len.group(1), int(chr_id_len.group(2))]\n                        if chr_len >= self.min_chromosome_len:\n                            use_chromosomes.append(chr_id)\n                    except AttributeError:\n                        self.logger.debug(line)\n            else:\n                [chrom_, start_, _, _, _, quality_, filter_, info_, format_, sample_] = line.split(\"\\t\")\n                if chrom_ in use_chromosomes:\n                    # searching in INFO for AF\n                    if format_.__contains__(\"AF\"):\n                        # clair3 or pepper-margin-deepvariant\n                        try:\n                            # clair3\n                            format_.split(\":\").index(\"AF\")\n                            af_index = format_.split(\":\").index(\"AF\")\n                            af = float(sample_.split(\":\")[af_index])\n                            # self.logger.debug(\"like clair3\")\n                        except ValueError:\n                            if format_.__contains__(\"VAF\"):\n                                # pepper-margin-deepvariant\n                                af_index = format_.split(\":\").index(\"VAF\")\n                                af = float(sample_.split(\":\")[af_index])\n                                # self.logger.debug(\"like pepper-margin-deepvariant\")\n                            else:\n                                pass\n                    elif info_.__contains__(\"AC\"):\n                        # longshot\n                        ac = list(filter(lambda x: \"AC\" in x, info_.split(\";\")))[0].split(\"=\")[1]\n                        [ref_count, alt_count] = ac.split(\",\")\n                        af = float(alt_count) / (float(ref_count) + float(alt_count))\n                        # self.logger.debug(\"like longshot\")\n                    else:\n                        # TODO use: DR/DV for calculating\n                        af = \"NA\"\n                    vcf_entries.append([chrom_, int(start_), None, af])\n        vcf_file.close()\n        return pd.DataFrame(data=vcf_entries, columns=[\"chrom_\", \"start_\", \"end_\", \"af_\"])\n\n    def get_mosdepth_chromosome_borders(self, mosdepth_file: str = \"\"):\n        in_path = self.__get_absolute_path(mosdepth_file)\n        file = pd.read_csv(in_path, sep=\"\\t\", header=None, names=[\"chrom_\", \"start_\", \"end_\", \"af_\"])\n        return file\n\n    def dataframe_to_tabular_file(self, df_snv: pd.DataFrame = None, mosdepth_input: str = \"\", out_path: str = \"\"):\n        self.logger.debug(\"Writing dataframe to tabular\")\n        df_mosdepth = self.get_mosdepth_chromosome_borders(mosdepth_input)\n        df_mosdepth_grouped = df_mosdepth.groupby(\"chrom_\")\n        df_snps_grouped = df_snv.groupby(\"chrom_\")\n\n        df_final = pd.DataFrame()\n\n        for mosdepth_chrom_key in df_mosdepth_grouped.indices.keys():\n            if mosdepth_chrom_key in df_snps_grouped.indices.keys():\n                df_mosdepth_chrom = df_mosdepth.loc[df_mosdepth_grouped.indices[mosdepth_chrom_key]]\n                bins = list(df_mosdepth_chrom.start_)\n                labels = list(df_mosdepth_chrom.start_)[:-1]  # must be len(bins)-1\n\n                df_snps_chrom = df_snv.iloc[df_snps_grouped.indices[mosdepth_chrom_key]]\n                df_snps_chrom[\"startbin_\"] = pd.cut(x=df_snps_chrom.start_, bins=bins, labels=labels,\n                                                    include_lowest=False)\n                df_snps_chrom[\"startbin_af_\"] = df_snps_chrom.groupby(\"startbin_\")[\"af_\"].transform('mean')\n                df_snps_chrom.sort_values(by=[\"startbin_\"], inplace=True)\n                df_snps_chrom.drop_duplicates(\"startbin_\", inplace=True)\n\n                df_snps_chrom.drop([\"start_\", \"end_\", \"af_\"], axis=1, inplace=True)  # clean\n\n                df_merged = pd.merge(df_mosdepth_chrom, df_snps_chrom,\n                                     left_on=[\"chrom_\", \"start_\"],\n                                     right_on=[\"chrom_\", \"startbin_\"],\n                                     how=\"left\")\n\n                df_merged[\"startbin_\"] = df_merged[\"start_\"] + 1\n                df_merged[\"startend_\"] = df_merged[\"end_\"]\n                df_merged[\"startbin_af_\"].fillna(value=0, inplace=True)\n\n                df_final = pd.concat([df_final, df_merged[[\"chrom_\", \"startbin_\", \"startend_\", \"startbin_af_\"]]],\n                                     ignore_index=True)\n                # df_final = pd.concat([df_final,df_snps_chrom],ignore_index=True)\n\n            pass\n\n        df_final.to_csv(out_path, sep=\"\\t\", index=False, header=None)\n        return df_final", "\nclass VCFSNVParser(object):\n    def __init__(self, min_chromosome_len=1e6, as_dev=False):\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.min_chromosome_len = min_chromosome_len\n\n    @staticmethod\n    def __get_absolute_path(input_path):\n        return os.path.abspath(os.path.expanduser(input_path))\n\n    def __create_tabix_file(self, input_path):\n        self.logger.info(f\"No index file found - Creating new one at {input_path}.tbi\")\n        pysam.tabix_index(input_path, preset=\"bed\", force=True)\n\n    def vcf_pysam(self, path):\n        # setup\n        vcf_path = self.__get_absolute_path(path)\n        vcf_tabix_path = vcf_path + \".tbi\"\n\n        # checking for index file\n        if not os.path.isfile(vcf_tabix_path):\n            self.__create_tabix_file(vcf_path)\n\n        vcf_file = pysam.VariantFile(vcf_path)  # loading vcf file\n        records = vcf_file.fetch()\n        for record in records:\n            x = record.samples.values()\n            print()\n\n        for uid, x in enumerate(vcf_file.fetch()):\n            print(x.format.keys())\n            af = x.format.values()[4].number\n\n    def vcf_to_dataframe(self, path):\n        self.logger.debug(\"Converting vcf to dataframe\")\n        vcf_path = self.__get_absolute_path(path)\n\n        # pre allocation\n        vcf_entries = list()\n        vcf_file = open(vcf_path, \"r\") if \"gz\" not in vcf_path else gzip.open(vcf_path, \"rt\")\n        chrom_, start_, quality_, filter_, info_, format_, sample_ = \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n        # how AF is described by tool:\n        #   clair3   -> AF in FORMAT/SAMPLE\n        #   longshot -> AF not present, AC contains the number of reads for REF,ALT: AF=ALT/(REF+ALT)\n        #   p-m-deep -> AF not present, VAF in FORMAT/SAMPLE\n        #  if in info --- af = float(list(filter(lambda x: \"VAF\" in x, info_.split(\";\")))[0].split(\"=\")[1])\n        use_chromosomes = []\n        for line in vcf_file:\n            line = line.rstrip(\"\\n\")\n            # skip header\n            if line.startswith(\"#\"):\n                if line.__contains__(\"contig\"):\n                    # example:  ##contig=<ID=chr1_KI270706v1_random,length=175055>\n                    try:\n                        chr_id_len = re.search('.*<ID=(.*),length=(.*)>', line)\n                        [chr_id, chr_len] = [chr_id_len.group(1), int(chr_id_len.group(2))]\n                        if chr_len >= self.min_chromosome_len:\n                            use_chromosomes.append(chr_id)\n                    except AttributeError:\n                        self.logger.debug(line)\n            else:\n                [chrom_, start_, _, _, _, quality_, filter_, info_, format_, sample_] = line.split(\"\\t\")\n                if chrom_ in use_chromosomes:\n                    # searching in INFO for AF\n                    if format_.__contains__(\"AF\"):\n                        # clair3 or pepper-margin-deepvariant\n                        try:\n                            # clair3\n                            format_.split(\":\").index(\"AF\")\n                            af_index = format_.split(\":\").index(\"AF\")\n                            af = float(sample_.split(\":\")[af_index])\n                            # self.logger.debug(\"like clair3\")\n                        except ValueError:\n                            if format_.__contains__(\"VAF\"):\n                                # pepper-margin-deepvariant\n                                af_index = format_.split(\":\").index(\"VAF\")\n                                af = float(sample_.split(\":\")[af_index])\n                                # self.logger.debug(\"like pepper-margin-deepvariant\")\n                            else:\n                                pass\n                    elif info_.__contains__(\"AC\"):\n                        # longshot\n                        ac = list(filter(lambda x: \"AC\" in x, info_.split(\";\")))[0].split(\"=\")[1]\n                        [ref_count, alt_count] = ac.split(\",\")\n                        af = float(alt_count) / (float(ref_count) + float(alt_count))\n                        # self.logger.debug(\"like longshot\")\n                    else:\n                        # TODO use: DR/DV for calculating\n                        af = \"NA\"\n                    vcf_entries.append([chrom_, int(start_), None, af])\n        vcf_file.close()\n        return pd.DataFrame(data=vcf_entries, columns=[\"chrom_\", \"start_\", \"end_\", \"af_\"])\n\n    def get_mosdepth_chromosome_borders(self, mosdepth_file: str = \"\"):\n        in_path = self.__get_absolute_path(mosdepth_file)\n        file = pd.read_csv(in_path, sep=\"\\t\", header=None, names=[\"chrom_\", \"start_\", \"end_\", \"af_\"])\n        return file\n\n    def dataframe_to_tabular_file(self, df_snv: pd.DataFrame = None, mosdepth_input: str = \"\", out_path: str = \"\"):\n        self.logger.debug(\"Writing dataframe to tabular\")\n        df_mosdepth = self.get_mosdepth_chromosome_borders(mosdepth_input)\n        df_mosdepth_grouped = df_mosdepth.groupby(\"chrom_\")\n        df_snps_grouped = df_snv.groupby(\"chrom_\")\n\n        df_final = pd.DataFrame()\n\n        for mosdepth_chrom_key in df_mosdepth_grouped.indices.keys():\n            if mosdepth_chrom_key in df_snps_grouped.indices.keys():\n                df_mosdepth_chrom = df_mosdepth.loc[df_mosdepth_grouped.indices[mosdepth_chrom_key]]\n                bins = list(df_mosdepth_chrom.start_)\n                labels = list(df_mosdepth_chrom.start_)[:-1]  # must be len(bins)-1\n\n                df_snps_chrom = df_snv.iloc[df_snps_grouped.indices[mosdepth_chrom_key]]\n                df_snps_chrom[\"startbin_\"] = pd.cut(x=df_snps_chrom.start_, bins=bins, labels=labels,\n                                                    include_lowest=False)\n                df_snps_chrom[\"startbin_af_\"] = df_snps_chrom.groupby(\"startbin_\")[\"af_\"].transform('mean')\n                df_snps_chrom.sort_values(by=[\"startbin_\"], inplace=True)\n                df_snps_chrom.drop_duplicates(\"startbin_\", inplace=True)\n\n                df_snps_chrom.drop([\"start_\", \"end_\", \"af_\"], axis=1, inplace=True)  # clean\n\n                df_merged = pd.merge(df_mosdepth_chrom, df_snps_chrom,\n                                     left_on=[\"chrom_\", \"start_\"],\n                                     right_on=[\"chrom_\", \"startbin_\"],\n                                     how=\"left\")\n\n                df_merged[\"startbin_\"] = df_merged[\"start_\"] + 1\n                df_merged[\"startend_\"] = df_merged[\"end_\"]\n                df_merged[\"startbin_af_\"].fillna(value=0, inplace=True)\n\n                df_final = pd.concat([df_final, df_merged[[\"chrom_\", \"startbin_\", \"startend_\", \"startbin_af_\"]]],\n                                     ignore_index=True)\n                # df_final = pd.concat([df_final,df_snps_chrom],ignore_index=True)\n\n            pass\n\n        df_final.to_csv(out_path, sep=\"\\t\", index=False, header=None)\n        return df_final", "\n\nclass VCFtoCandidate(object):\n    def __init__(self):\n        pass\n\n    def vcf_to_candidates(self,vcf_path):\n        df = self.vcf_ot_dataframe(vcf_path)\n        cnv_candidate_list = self.dataframe_to_candidates(df)\n        return cnv_candidate_list\n\n    def vcf_ot_dataframe(self, vcf_path: str = ''):\n        # Loading VCF file\n        vcf_file = open(vcf_path, \"r\") if \"gz\" not in vcf_path else gzip.open(vcf_path, \"rt\")\n        lines = [line.strip() for line in vcf_file.readlines()]\n        vcf_file.close()\n        # Creating dataframe\n        df = pd.DataFrame(lines, columns=['input'])\n        df = df.input.str.split('\\t', expand=True)\n        df = df[~df.iloc[:, 0].str.contains('##')]  # header definitions starting with ##\n        df.iloc[0, 0] = df.iloc[0, 0][1:]  # first line is header line of vcf entries removing the # in the first col\n        df.columns = df.iloc[0]\n        df = df[1:]  # removing first row which is used for the column names\n        return df\n\n    def dataframe_to_candidates(self, df: pd.DataFrame):\n        cnv_candidate_list = {}\n        for cnv in df.itertuples():\n            # Parse basics\n            candidate = CNVCandidate()\n            candidate.chromosome = cnv.CHROM\n            candidate.start = int(cnv.POS)\n            candidate.id = cnv.ID\n            info_dict = {i.split('=')[0]: i.split('=')[1] for i in cnv.INFO.split(';')}\n            candidate.end = int(info_dict['END'])\n            candidate.cn_status = int(info_dict['CN'])\n            candidate.type = info_dict['SVTYPE']\n            candidate.size = int(info_dict['SVLEN'])\n\n            population_mode = len(df.columns[9:]) < 2  # TRUE more than 2 samples are present in the VCF\n            # Form\n            format_set = [i for i in cnv.FORMAT.split(':')]\n            for sample_id, sample in zip(list(df.columns[9:]), list(cnv[9 + 1:])):  # +1 due to the index column\n                sample_gq = 0\n                if not population_mode:  # without special flags it is not possible\n                    candidate.sample_origin = sample_id\n                sample_cells = sample.split(':')\n                if \"GQ\" in format_set:\n                    gq_idx = format_set.index('GQ')  # get gene quality score\n                    sample_gq = int(sample_cells[gq_idx])\n                    candidate.statistics['z-score'] = {}\n                    candidate.statistics['z-score']['sample_score'] = sample_gq\n                if \"GT\" in format_set:\n                    gt_idx = format_set.index('GT')\n                    candidate.gt = sample_cells[gt_idx]\n\n                # Could be left black as only details for the known variant are known and loaded from VCF\n                if \"ID\" in format_set:\n                    id_idx = format_set.index('ID')\n                    support_samples = sample_cells[id_idx].split(',')\n                    # Get all supporting cnvs from a given sample\n                    for support_sample_id in support_samples:\n                        # add only not NULL supports\n                        if support_sample_id != 'NULL':\n                            if sample_id not in candidate.support_cnv_calls.keys():\n                                candidate.support_cnv_calls[sample_id] = set()\n                            support_candidate = CNVCandidate(sample_id)\n                            support_candidate.id = support_sample_id\n                            support_candidate.statistics['z-score'] = {}\n                            support_candidate.statistics['z-score']['sample_score'] = sample_gq\n                            candidate.support_cnv_calls[sample_id].add(support_candidate)\n\n            if cnv.CHROM not in cnv_candidate_list.keys():\n                cnv_candidate_list[cnv.CHROM] = []\n            cnv_candidate_list[cnv.CHROM].append(candidate)\n        return cnv_candidate_list", ""]}
{"filename": "util/OSUtil.py", "chunked_list": ["import os\nimport gzip\n\n\nclass OSUtil:\n    @classmethod\n    def get_absolute_path(cls, relative_user_path) -> str:\n        \"\"\"\n        Extends a relative path to an absolute user path\n        :param relative_user_path: relative path\n        :return: absolute user path\n        \"\"\"\n        return os.path.abspath(os.path.expanduser(relative_user_path))\n\n    @classmethod\n    def get_lines_by_chromosome(cls, filepath):\n        filepath = cls.get_absolute_path(filepath)\n        lines_by_chromosome = {}\n        file_handler_read = gzip.open(filepath, \"rt\") if \"gz\" in filepath else open(filepath, \"r\")\n        current_chromosome = \"\"\n        for line in file_handler_read:\n            chromosome = line.rstrip(\"\\n\").split(\"\\t\")[0]\n            if current_chromosome == \"\":\n                current_chromosome = chromosome\n                lines_by_chromosome[chromosome] = 1\n            elif current_chromosome != chromosome:\n                current_chromosome = chromosome\n                lines_by_chromosome[chromosome] = 1\n            else:\n                lines_by_chromosome[chromosome] += 1\n        file_handler_read.close()\n        return lines_by_chromosome\n\n    @classmethod\n    def get_lines_of_file(cls, filepath):\n        filepath = cls.get_absolute_path(filepath)\n        file_handler_read = open(filepath, \"r\") if \"gz\" not in filepath else gzip.open(filepath, \"rt\")\n        cnt = 0\n        for line in file_handler_read:\n            cnt += 1\n\n        file_handler_read.close()\n        return cnt"]}
{"filename": "util/outputWriter.py", "chunked_list": ["import gzip\nimport string\nimport random\nimport numpy as np\nimport json\nfrom collections import Counter\n\n\nclass BedOutput(object):\n    def __init__(self, output_file):\n        self.output_bed = output_file\n\n    def make_bed(self, chromosome_list, cnv_candidate_list):\n        file_handler = open(self.output_bed, \"w\")\n        for each_chromosome in chromosome_list:\n            for each_candidate in cnv_candidate_list[each_chromosome]:\n                result_list = [each_candidate.chromosome, str(each_candidate.start), str(each_candidate.end),\n                               each_candidate.type, str(each_candidate.size),\n                               str(round(each_candidate.median_cov_norm, 2))]\n\n                bed_line = \"\\t\".join(result_list)\n                file_handler.write(f'{bed_line}\\n')\n        file_handler.close()", "class BedOutput(object):\n    def __init__(self, output_file):\n        self.output_bed = output_file\n\n    def make_bed(self, chromosome_list, cnv_candidate_list):\n        file_handler = open(self.output_bed, \"w\")\n        for each_chromosome in chromosome_list:\n            for each_candidate in cnv_candidate_list[each_chromosome]:\n                result_list = [each_candidate.chromosome, str(each_candidate.start), str(each_candidate.end),\n                               each_candidate.type, str(each_candidate.size),\n                               str(round(each_candidate.median_cov_norm, 2))]\n\n                bed_line = \"\\t\".join(result_list)\n                file_handler.write(f'{bed_line}\\n')\n        file_handler.close()", "\n\nclass VCFLine(object):\n    def __init__(self):\n        self.CHROM = \"\"\n        self.POS = 0\n        self.ID = \".\"\n        self.REF = \"N\"\n        self.ALT = \".\"  # DEL/DUP\n        self.QUAL = \".\"\n        self.FILTER = \".\"\n        self.INFO = \".\"\n        self.FORMAT = \"GT:HO:GQ\"\n        self.format_data = []\n        self.sample_format_data = {}\n        self.supp_vec = {}\n\n    def format_vcf_line(self):\n        sample_format_list = []\n        if len(self.format_data) > 0:\n            sample_format_list = [\":\".join(self.format_data)]\n\n        for key, value in self.sample_format_data.items():\n            sample_format_list.append(\":\".join([str(s) for s in value]))\n\n        return \"\\t\".join([self.CHROM, str(self.POS), self.ID, self.REF, self.ALT, self.QUAL, self.FILTER,\n                          self.INFO, self.FORMAT] + sample_format_list)", "\n\nclass VCFOutput(object):\n    def __init__(self, output_file, genome_info):\n        self.supp_vec = {}\n        self.output_vcf = output_file\n        self.population_sample_ids = []\n        self.genome_info = genome_info\n        # example {'chromosomes': ['chr6'], 'chr_lengths': [170805979], 'chr_lengths_by_name': {'chr6': 170805979}}\n\n    @staticmethod\n    def id_generator(size=8, chars=string.ascii_uppercase + string.digits):\n        return ''.join(random.choice(chars) for _ in range(size))\n\n    def make_vcf_contigs(self):\n        vcf_contigs = []\n        for contig_name in self.genome_info[\"chromosomes\"]:\n            contig_length = self.genome_info[\"chr_lengths_by_name\"][contig_name]\n            vcf_contigs.append(f'##contig=<ID={contig_name},length={contig_length}>')\n        return \"\\n\".join(vcf_contigs)\n\n    def make_vcf_header(self):\n        population_mode = False\n        if self.population_sample_ids:\n            if len(self.population_sample_ids) > 1:\n                population_mode = True\n\n        contigs = self.make_vcf_contigs()\n        vcf_header = ['##fileformat=VCFv4.4', '##FILTER=<ID=PASS,Description=\"All filters passed\">',\n                      '##source=Spectre', contigs,\n                      '##ALT=<ID=DEL,Description=\"Deletion\">',\n                      '##ALT=<ID=DUP,Description=\"Duplications\">',\n                      '##FILTER=<ID=UNRESOLVED,Description=\"An insertion that is longer than the '\n                      'read and thus we cannot predict the full size.\">'\n                      ]\n        # Add Info section\n        vcf_header += ['##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the structural variant\">',\n                       '##INFO=<ID=SVLEN,Number=1,Type=Integer,Description=\"Length of the SV\">',\n                       '##INFO=<ID=SVTYPE,Number=1,Type=String,Description=\"Type of copy number variant\">',\n                       '##INFO=<ID=CN,Number=1,Type=Integer,Description=\"Estimated copy number status\">', ]\n        if population_mode:\n            vcf_header += ['##INFO=<ID=SUPP_VEC,Number=1,Type=String,Description=\"Support vector\">']\n\n        # Add Format section\n        vcf_header += ['##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">',\n                       '##FORMAT=<ID=HO,Number=2,Type=Float,Description=\"Homozygosity proportion\">',\n                       '##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype quality\">',\n                       '##FORMAT=<ID=ID,Number=1,Type=String,Description=\"Population ID of supporting CNV calls\">']\n\n        if self.population_sample_ids:\n            if population_mode:\n                s = f\"##Spectre_population_samples={','.join(self.population_sample_ids)}\"\n            else:\n                s = f\"##Spectre_sample={','.join(self.population_sample_ids)}\"\n\n            vcf_header.append(s)\n        return \"\\n\".join(vcf_header)\n\n    @staticmethod\n    def make_vcf_sample_header(sample_id: list):\n        return \"\\t\".join([\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"QUAL\", \"FILTER\", \"INFO\", \"FORMAT\"] + sample_id)\n\n    @staticmethod\n    def set_svtype(candidate_type):\n        sv_types = {\"DEL\": \"<DEL>\", \"DUP\": \"<DUP>\"}\n        if candidate_type in sv_types:\n            return sv_types[candidate_type]\n        else:\n            return \".\"\n\n    def vcf_result(self, chromosome_list, cnv_candidate_list):\n        vcf_lines = []\n        for each_chromosome in chromosome_list:\n            for each_candidate in cnv_candidate_list[each_chromosome]:\n                vcf_line = VCFLine()\n                vcf_line.CHROM = each_candidate.chromosome\n                vcf_line.POS = each_candidate.start\n\n                vcf_line.ID = each_candidate.id\n                vcf_line.ALT = self.set_svtype(each_candidate.type)\n                vcf_line.INFO = f\"END={each_candidate.end};SVLEN={each_candidate.size};SVTYPE={each_candidate.type};\" \\\n                                f\"CN={each_candidate.cn_status}\"\n                # checking if any other CNV through merging supported the given CNV\n                vcf_line.supp_vec = self.supp_vec.copy()\n                if not each_candidate.support_cnv_calls:\n\n                    vcf_line.format_data = [each_candidate.gt, f'{round(each_candidate.het_score, 2)}',\n                                            f\"{int(each_candidate.statistics['z-score']['sample_score'])}\"]\n                else:\n                    vcf_line.format_data = []\n                    vcf_line.FORMAT += \":ID\"  # ADD ID tag in format as everything that is following are IDs\n                    vcf_line.supp_vec = dict(\n                        [(str(sample_key), 0) for sample_key in each_candidate.support_cnv_calls.keys()])\n                    for sample_key, sample_value in each_candidate.support_cnv_calls.items():\n                        if sample_value:\n                            ids = []\n                            scores = []\n                            gts = []\n                            # sample_id =\"\"\n                            for candidate in list(sample_value):\n                                ids.append(candidate.id)\n                                scores.append(candidate.statistics['z-score']['sample_score'])\n                                gts.append(candidate.gt)\n                            gt = Counter(gts).most_common(1)[0][0]\n                            score = np.mean(scores)\n                            vcf_line.sample_format_data[sample_key] = [gt, \"0.0\", int(score), \",\".join(ids)]\n                            vcf_line.supp_vec[sample_key] = 1\n                        else:\n                            vcf_line.sample_format_data[sample_key] = [\"./.\", \"0.0\", 0, \"NULL\"]\n                    # add support vector only if population mode is active\n                    vcf_line.INFO += \";SUPP_VEC=\" + \"\".join([str(s) for s in vcf_line.supp_vec.values()])\n                vcf_lines.append(vcf_line.format_vcf_line())\n        return \"\\n\".join(vcf_lines)\n\n    def make_vcf(self, chromosome_list, cnv_candidate_list, sample_id, population_sample_ids=None):\n        # converting population sample ids from set to list\n        if not population_sample_ids:\n            population_sample_ids = [sample_id]\n        self.population_sample_ids = list(population_sample_ids)\n        self.supp_vec = dict([(i, 0) for i in self.population_sample_ids])\n\n        file_handler = open(self.output_vcf, \"w\") if \"gz\" not in self.output_vcf else gzip.open(self.output_vcf, \"wt\")\n        vcf_header = self.make_vcf_header()\n        vcf_sample_header = self.make_vcf_sample_header(self.population_sample_ids)\n        vcf_lines = self.vcf_result(chromosome_list, cnv_candidate_list)\n        file_handler.write(f'{vcf_header}\\n')\n        file_handler.write(f'{vcf_sample_header}\\n')\n        file_handler.write(f'{vcf_lines}\\n')\n        file_handler.close()", "\n\nclass IntermediateFile(object):\n    def __init__(self, output_dir: str):\n        self.output_dir = output_dir\n\n    @staticmethod\n    def convert_genome_info_to_dictionary(genome_info: dict):\n        tmp_genome_inf = dict([(key, []) for key in genome_info.keys()])\n        return \"\"\n        # for info in genome_info;\n\n    @staticmethod\n    def convert_candidates_to_dictionary(candidates: dict):\n        tmp_candidates_dict = dict([(key, []) for key in candidates.keys()])\n        for key, candidates in candidates.items():\n            tmp_candidates = []\n            tmp_candidates_dict[key] = tmp_candidates\n            for candidate in candidates:\n                tmp_dict = dict(candidate.__dict__)\n                tmp_dict = {k: v for k, v in tmp_dict.items() if v}\n                for key, value in tmp_dict.items():\n                    if isinstance(value, set):\n                        tmp_dict[key] = list(value)\n                tmp_dict.pop('logger')\n                tmp_candidates.append(tmp_dict)\n        return tmp_candidates_dict\n\n    def write_intermediate_file(self, output_object, filename: str) -> str:\n        output_path = f\"{self.output_dir}/{filename}.spc.gz\"\n        with gzip.open(output_path, 'wt', encoding='UTF-8') as out_file:\n            json.dump(output_object, out_file, indent=\"\\t\")\n        return output_path", ""]}
{"filename": "util/mosdepthReader.py", "chunked_list": ["import gzip\nimport numpy as np\n\n\nclass MosdepthSummary(object):\n    def __init__(self):\n        self.chromosomes = []\n        self.chr_mean_coverage = {}\n        self.genome_mean_coverage = None\n        self.genome_bases = None\n\n    def add_chromosome(self, new_chr):\n        self.chromosomes.append(new_chr)\n\n    def add_coverage(self, new_chr, new_cov):\n        self.chr_mean_coverage[new_chr] = float(new_cov)\n\n    def add_genome_summary(self, genome_mean_coverage, genome_bases):\n        self.genome_mean_coverage = float(genome_mean_coverage)\n        self.genome_bases = int(genome_bases)\n\n    def update_genome_mean(self):\n        self.genome_mean_coverage = np.mean([value for index, value in self.chr_mean_coverage.items()])", "\n\nclass MosdepthReader(object):\n    def __init__(self, coverage_file, summary_file):\n        self.coverage_file = coverage_file\n        self.summary_file = summary_file\n        self.mosdepth_summary_data = MosdepthSummary()\n        self.min_chr_lengh = 1e6  # 1mb to remove alts\n\n    def summary_data(self):\n        found_total_flag = False\n        summary_file_handler = open(self.summary_file, \"r\") if \"gz\" not in self.summary_file \\\n            else gzip.open(self.summary_file, \"rt\")\n        for line in summary_file_handler:\n            # skip header\n            if \"chrom\" not in line:\n                if \"_region\" not in line:\n                    # chrom  length  bases  mean  min  max\n                    [chromosome, chr_length, bases, mean_cov, _, _] = line.rstrip(\"\\n\").split(\"\\t\")\n                    if float(chr_length) > self.min_chr_lengh:\n                        if chromosome == \"total\":\n                            self.mosdepth_summary_data.add_genome_summary(mean_cov, bases)\n                            found_total_flag = True\n                        else:\n                            self.mosdepth_summary_data.add_chromosome(chromosome)\n                            self.mosdepth_summary_data.add_coverage(chromosome, mean_cov)\n        if not found_total_flag:\n            self.mosdepth_summary_data.update_genome_mean()\n        summary_file_handler.close()", ""]}
{"filename": "util/dataAnalyzer.py", "chunked_list": ["import numpy as np\n\n\nclass NormaldataAnalyser:\n\n    @classmethod\n    def get_candidate_statistics(cls, normalized_candidates) -> tuple:\n        \"\"\"\n        :param normalized_candidates:\n        :return:\n        \"\"\"\n        min_val = np.nanmin(normalized_candidates)\n        max_val = np.nanmax(normalized_candidates)\n        avg = np.nanmean(normalized_candidates)\n        std = np.nanstd(normalized_candidates)\n        med = np.nanmedian(normalized_candidates)\n        return avg, std, min_val, max_val, med\n\n    @classmethod\n    def normalize_candidates(cls, candidates: np.array(list), use_value: float) -> np.array(list):\n        \"\"\"\n        Normalizes an i\n        :param candidates:\n        :param use_value: can be median or mean\n        :return:\n        \"\"\"\n        # return candidates/use_value\n        return candidates/use_value  # the median will be CN2, we derive the rest from them\n\n\n\n    @classmethod\n    def get_slope(cls, n:int, normalized_candidates: np.array(list)) -> np.array(list):\n        x = np.array(range(0, n))\n        y = np.array(list)\n        slope_array = np.zeros(normalized_candidates.size)\n        l = 0\n        slope = 0\n        for i in range(0,normalized_candidates.size,1):\n            if float(normalized_candidates[i]) != 0:\n                l = i - n\n                if i > n:\n                    y = normalized_candidates[l:i].astype(np.float)\n                    slope = cls.get_slope_from_values(x,y)\n                else:\n                    slope = 0\n                slope_array[i] = float(slope)\n        return slope_array\n\n\n    @classmethod\n    def get_slope_from_values(cls, x, y) -> float:\n        \"\"\"\n        Calculates the slope of the regression of the given x and y values.\n        :param x: Numpy array with walues in the X-axis\n        :param y: Numpy array with values in the Y-axis\n        :return: float of the slope value.\n        \"\"\"\n        return ((x*y).mean(axis=0) - x.mean()*y.mean(axis=0)) / ((x**2).mean() - (x.mean())**2)", ""]}
{"filename": "util/cnv_id.py", "chunked_list": ["import random\nimport string\n\n\nclass CNV_ID(object):\n\n    def id_generator(self, size=8, chars=string.ascii_uppercase + string.digits):\n        \"\"\"\n        Generates a random ID\n        :param size: size of the ID\n        :param chars: vocabulary (default only upper case chars)\n        :return: random generated id with given size\n        \"\"\"\n        return ''.join(random.choice(chars) for _ in range(size))\n\n    @classmethod\n    def n_id_generator(cls, n=1, existing_ids=None, size=8, chars=string.ascii_uppercase + string.digits):\n        \"\"\"\n        Generates n amounts of unique ids with given size\n        :param n: amount of new unique ids\n        :param existing_ids: list of already existing ids\n        :param size:  sie of the ID\n        :param chars: vocabulary (default only upper case chars)\n        :return: list of random generated id with given size\n        \"\"\"\n        new_ids = []\n\n        if existing_ids is None:\n            existing_ids = []\n\n        for i in range(0, n):\n            while True:\n                id = cls.id_generator(n, size, chars)\n                if id not in existing_ids:\n                    new_ids.append(id)\n                    existing_ids.append(id)\n                    break\n\n        return new_ids", "\n\n"]}
{"filename": "util/metadata/metadataCollector.py", "chunked_list": ["import mmap\nimport os\nimport gzip\nimport logging as logger\n\n\nclass FastaRef:\n\n    def __init__(self, as_dev=False):\n        # logger\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # ... TODO\n        self.__nList = list()\n        self.__nFrom = 0\n        self.__nTo = 0\n        self.__gCnt = 0\n        self.__cCnt = 0\n        self.__aCnt = 0\n        self.__tCnt = 0\n        # report\n        self.static_report = \"\"\n\n    def get_n_regions(self, filepath, report_output_dir, out_file_name, threshold=5, bin_size=500,\n                      save_only=False) -> dict:\n        ref_dict = dict()\n        i = 0\n        j = 0\n        g_cnt = 0\n        c_cnt = 0\n        a_cnt = 0\n        t_cnt = 0\n        chromosome_cnt = 0\n        chromosome = \"\"\n        # assure abs paths\n        path = os.path.abspath(os.path.expanduser(filepath))\n        report_output_dir = os.path.abspath(os.path.expanduser(report_output_dir))\n        file_handler_fasta = open(path, \"r\") if \"gz\" not in path else gzip.open(path, \"rt\")\n        for line in file_handler_fasta:\n            term = line.rstrip(\"\\n\")  # cut away the newline from line (\\n)\n            # check if line is not a comment or header\n            if not (term[:1] == '>' or term[:1] == ';'):\n                for t in term:\n                    c = t.upper()\n                    if c == \"N\":\n                        j += 1\n                    else:\n                        if (j - i) >= threshold:\n                            start = i + 1\n                            end = j - 1\n                            # check if values are not on mod binSize = 0 position\n                            if (start % bin_size) != 0:\n                                start = start - (start % bin_size)\n                            if (end % bin_size) != 0:\n                                end = end + (bin_size - (end % bin_size))\n                            ref_dict[str(chromosome)].append((start, end))\n                        i = j\n                        j += 1\n                        # grab bp for gc statistic\n                        if c == \"G\":\n                            g_cnt += 1\n                        elif c == \"C\":\n                            c_cnt += 1\n                        elif c == \"A\":\n                            a_cnt += 1\n                        elif c == \"T\":\n                            t_cnt += 1\n                    chromosome_cnt += 1\n            else:\n                # allocate space in dict for current chromosome\n                if chromosome != \"\":\n                    # pre-print before new chrom is select\n                    self.logger.info(f'length: {str(chromosome_cnt)}\\t{ref_dict[str(chromosome)]}')\n                chromosome = str(term[1:]).split(\" \")[0]\n                ref_dict[str(chromosome)] = list()\n                self.logger.info(f'Reading {term}')\n                i = 0\n                j = 0\n                chromosome_cnt = 0\n\n        self.__nList = ref_dict\n        self.__nTo = j\n        self.__cCnt = c_cnt\n        self.__gCnt = g_cnt\n        self.__aCnt = a_cnt\n        self.__tCnt = t_cnt\n        self.__get_statistic_report(report_output_dir, out_file_name)\n        if not save_only:\n            return ref_dict\n        return {}\n\n    def __get_statistic_report(self, output_dir, filename):\n        # static_report is a long string of text\n        self.static_report = \"#####\\tStatistic report\\n\"\n        # gc coverage\n        self.logger.info(\"Calculating bp statistics\")\n        self.static_report += \"#####\\tBasepair statistic\\n\"\n        self.static_report += \"BPDEF\\tA\\tT\\tC\\tG\\tGC%\\tTotal bp\\n\"\n        self.static_report += \"BPSTA\\t\" + str(self.__aCnt) + \"\\t\" + str(self.__tCnt) + \"\\t\" + str(self.__cCnt) + \\\n                              \"\\t\" + str(self.__gCnt) + \"\\t\" + str((self.__gCnt + self.__cCnt) / self.__nTo * 100) + \\\n                              \"\\t\" + str(self.__nTo) + \"\\n\"\n\n        # positions of n in sequence\n        self.logger.info(\"Calculating N positions\")\n        self.static_report += \"#####\\tN-Sequence positions in given reference file\\n\"\n        self.static_report += \"NSDEF\\tFrom\\tTo\\n\"\n        for chromosome in self.__nList:\n            for start, end in self.__nList[chromosome]:\n                self.static_report += \"NSPOS\\t\" + str(chromosome) + \"\\t\" + str(start) + \"\\t\" + str(end) + \"\\n\"\n\n        # write statistic report to file\n        self.logger.info(\"Writing report\")\n        self.__write_report(output_dir, filename)\n\n    @classmethod\n    def extract_n_regions_from_report(cls, input_file: str) -> dict:\n        \"\"\"\n        Loads all N regions that previously have been extracted from the reference genome and stored in a .mdr file\n        :param input_file: /path/to/<file_name>.mdr\n        :return: dict with all N regions according to the chromosome\n        \"\"\"\n        result = dict()\n        # assure abs paths\n        path = os.path.abspath(os.path.expanduser(input_file))\n        with open(path, \"r\") as fp:\n            # map file into memory\n            mm = mmap.mmap(fp.fileno(), 0, prot=mmap.PROT_READ)\n            for line in iter(mm.readline, b\"\"):\n                # decode line and cut away last char from line (\\n)\n                term = line.decode(\"utf-8\")[:-1]\n                if term[:5].__eq__(\"NSPOS\"):\n                    cells = term.strip().split(\"\\t\")\n                    if str(cells[1]) not in result:  # [1] = chromosome name\n                        result[str(cells[1])] = []\n                    result[str(cells[1])].append((cells[2], cells[3]))  # [2] start, [3] end\n        return result\n\n    @classmethod\n    def extract_blacklisted_regions(cls, input_file: str) -> dict:\n        result = dict()\n        path = os.path.abspath(os.path.expanduser(input_file))\n        with open(path, \"r\") as fp:\n            mm = mmap.mmap(fp.fileno(), 0, prot=mmap.PROT_READ)\n            for line in iter(mm.readline, b\"\"):\n                # decode line and cut away last char from line (\\n)\n                term = line.decode(\"utf-8\")[:-1]\n                chrom, start, end = term.split(\"\\t\")[:3]\n                if str(chrom) not in result:\n                    result[str(chrom)] = []\n                result[str(chrom)].append((start,end))\n        return result\n\n    @classmethod\n    def merge_metadata(cls, m1:dict, m2:dict)->dict:\n        \"\"\"\n        Merges two dictionaries with the structure str:list\n        :param m1: dict m1 with structure str:list\n        :param m2: dict m2 with structure str:list\n        :return: merged dict with structure str:list\n        \"\"\"\n        result = m1.copy()\n        for key2 in m2.keys():\n            # check if key in results\n            if key2 in result:\n                # check if values are in it\n                for value2 in m2[key2]:\n                    # add if value2 not in result\n                    if value2 not in result[key2]:\n                        result[key2].append(value2)\n            else:\n                # if not in results -> add whole d2[key2] into it\n                result[key2] = m2[key2]\n        return result\n\n    def __write_report(self, output_dir, filename):\n        self.logger.info(output_dir)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        with open(os.path.join(output_dir, filename), \"w\") as file:\n            file.writelines(self.static_report)", ""]}
{"filename": "util/metadata/__init__.py", "chunked_list": [""]}
{"filename": "analysis/call_cnv_coverage.py", "chunked_list": ["import numpy as np\nfrom analysis.cnv_candidate import CNVCandidate\nimport pandas as pd\nimport logging as logger\n\n\nclass CNVCall(object):\n    def __init__(self, as_dev=False):\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n\n    def cnv_coverage(self, chromosome_coverage_data, bin_size, chromosome_name, sample_origin=\"\",\n                     lower_bound: float = None,\n                     upper_bound: float = None):\n        # get chromosome_coverage_data (\"self.genome_analysis\"), for each chromosome, \"cov_data:\n        # * .position: list size n\n        #   .coverage_raw = np.NaN\n        #   .positions = np.NaN\n        #   .coverage_log2 = np.NaN         # log2(x)\n        #   .normalized_cov = np.NaN        # x/median\n        # * .normalized_cov_ploidy = np.NaN   # x/median * 2 -> for diploid\n        candidate_list = []\n        # local\n        min_run = 10\n        run = 0\n        cnv_pos_cand_list = []\n        cnv_cov_cand_list = []\n        run_current_pos = 0\n        run_start = 0\n        cnv_type = \"\"\n        current_cnv_type = \"\"\n        # lower_bound, upper_bound = 1.5, 2.5  # values outside the CN2\n        if not isinstance(chromosome_coverage_data.normalized_cov_ploidy, np.ndarray) or \\\n                not isinstance(chromosome_coverage_data.positions, np.ndarray):\n            self.logger.debug(type(chromosome_coverage_data.normalized_cov_ploidy))\n            self.logger.debug(type(chromosome_coverage_data.positions))\n            self.logger.error(f'No data is available, check that the mosdepth.region.bed.gz file is not empty '\n                              f'or that the selected chromosome (--only-chr <CHR>) exists')\n\n        for (cov, pos) in zip(chromosome_coverage_data.normalized_cov_ploidy, chromosome_coverage_data.positions):\n            # start/continue run\n            if not np.isnan(cov):\n                # Determine CNV Type\n                if cov > upper_bound or cov < lower_bound:\n                    cnv_type = \"DUP\" if cov > upper_bound else \"DEL\"\n                    # start run\n                    if run_start == 0:\n                        current_cnv_type = cnv_type\n                        run_start = pos\n                    else:\n                        # continue run if not in the chromosome end\n                        if pos - run_current_pos < bin_size + 1 and current_cnv_type == cnv_type:\n                            run += 1\n                            cnv_pos_cand_list.append(pos)\n                            cnv_cov_cand_list.append(cov)\n                        # break run end of chromosome\n                        else:\n                            if len(cnv_pos_cand_list) > 1:\n                                cnv_candidates = CNVCandidate(sample_origin, self.as_dev)\n                                cnv_candidates.push_candidates(chromosome_name, cnv_pos_cand_list, cnv_cov_cand_list,\n                                                               current_cnv_type)\n\n                                if len(cnv_pos_cand_list) >= min_run:\n                                    candidate_list.append(cnv_candidates)\n                            # restart run\n                            run = 1\n                            cnv_pos_cand_list = [pos]\n                            cnv_cov_cand_list = [cov]\n                            run_start = pos\n                            current_cnv_type = cnv_type\n                # break run\n                else:\n                    if len(cnv_pos_cand_list) > 1:\n                        cnv_candidates = CNVCandidate(sample_origin, self.as_dev)\n                        cnv_candidates.push_candidates(chromosome_name, cnv_pos_cand_list, cnv_cov_cand_list,\n                                                       current_cnv_type)\n\n                        if len(cnv_pos_cand_list) >= min_run:\n                            candidate_list.append(cnv_candidates)\n                    # restart run\n                    run = 1\n                    cnv_pos_cand_list = []\n                    cnv_cov_cand_list = []\n                    run_start = pos\n                    current_cnv_type = \"\"\n                run_current_pos = pos\n\n        return candidate_list", ""]}
{"filename": "analysis/analysis.py", "chunked_list": ["import os\nimport gzip\nimport pysam\nimport numpy as np\nimport pandas as pd\nimport logging as logger\nimport util.outputWriter\nimport util.vcf_parser as vcf\nfrom util.OSUtil import OSUtil as OSUt\nfrom util.dataAnalyzer import NormaldataAnalyser as NorAn", "from util.OSUtil import OSUtil as OSUt\nfrom util.dataAnalyzer import NormaldataAnalyser as NorAn\nfrom util.cnv_id import CNV_ID\nfrom plots.plot import CNVPlot\nfrom plots.plot import CoveragePlot\nfrom analysis.coverage_stats import CoverageStatistics\nfrom analysis.coverage_stats import CoverageData\nfrom analysis.call_cnv_coverage import CNVCall\nfrom analysis.call_cnv_AF import CNVCall as CNVAnalysisSNP\nfrom analysis.cnv_metrics import CNVMetrics", "from analysis.call_cnv_AF import CNVCall as CNVAnalysisSNP\nfrom analysis.cnv_metrics import CNVMetrics\n\n\nclass CNVAnalysis(object):\n    def __init__(self, coverage_file, coverage_mosdepth_data, bin_size, output_directory, outbed, outvcf, genome_info,\n                 sample_id, metadata_ref, snv_file, only_chr_list=\"\", ploidy=2,min_cnv_len=1000000, as_dev=False,\n                 dev_params=None, debug_dir=\"\"):\n        # input\n        self.coverage_file = coverage_file\n        self.mosdepth_data = coverage_mosdepth_data  # has chr_mean_coverage, genome_mean_coverage, genome_bases\n        self.output_directory = output_directory\n        self.output_bed = outbed\n        self.output_vcf = outvcf\n        self.sample_id = sample_id\n        self.snv_file = snv_file\n        self.population_sample_ids = set()  # hold the filename of the population samples\n        # bin size !important\n        self.bin_size = int(bin_size)\n        self.genome_info = genome_info\n        self.metadata = metadata_ref\n        # whitelist, only this chr\n        self.only_chr_list = str(only_chr_list).split(\",\") if only_chr_list != \"\" else genome_info[\"chromosomes\"]\n        self.ploidy = ploidy\n        # logger\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # work variables\n        self.positions = np.zeros(0)\n        self.coverage = np.zeros(0)\n        self.coverage_log2 = np.zeros(0)\n        # results by chromosome\n        self.genome_analysis = {}  # use this for CNV call NOTE: here is the coverage data under \"cov_data\"\n        self.cnv_calls_list = {}  # use this for CNV call\n        self.raw_cnv_calls_list = {}  # use this for storing raw CNV call\n        self.intermediate_candidates_file_location = \"\"  # holds output path of serialized cnv object\n        self.cnv_merged = {}  # use this for CNV call\n        self.existing_cnv_ids = []  # holds all already used cnv IDs\n        # snv data\n        self.snv_af_bed = \"\"\n        self.snv_af_df = None\n        # dev/debug + hidden params\n        self.as_dev = as_dev\n        self.debug_dir = debug_dir\n        # cnv metrics\n        self.cnv_metrics = None\n\n        # TODO\n        self.min_chr_length = 1e6\n        self.max_std_outlier_rm = 5\n        self.mosdepth_cov_genome_chr_diff = 0.10  # 10%\n        self.lower_2n_threshold = 0.0  # are overwritten after data_normalization was called\n        self.upper_2n_threshold = 0.0  # are overwritten after data_normalization was called\n        self.cov_diff_threshold = 0.80\n        self.dist_proportion = 0.25\n        self.dist_min_overwrite = 10000  # 10kb\n        self.candidate_final_threshold = min_cnv_len #100000  # 100kb\n\n    # Data normalization\n    def data_normalization(self):\n        \"\"\"\n        Normalize single chromosome bins\n        \"\"\"\n        self.logger.debug(self.coverage_file)\n        file_size_lines = OSUt.get_lines_by_chromosome(self.coverage_file)\n        if len(file_size_lines) == 0:\n            self.logger.error(\"Empty file\")\n\n        coverage_full_path = os.path.abspath(os.path.expanduser(self.coverage_file))\n        coverage_file_handler = gzip.open(coverage_full_path, \"rt\") if \"gz\" in self.coverage_file \\\n            else open(coverage_full_path, \"r\")\n\n        list_index = 0\n        previous_chromosome = \"\"\n        for line in coverage_file_handler:\n            [chromosome, start, _, coverage] = line.rstrip(\"\\n\").split(\"\\t\")\n            if chromosome in self.only_chr_list:\n                start = int(start)\n                coverage = float(coverage)\n                if previous_chromosome == \"\":\n                    # init\n                    self.positions = np.zeros(file_size_lines[chromosome])\n                    self.coverage = np.zeros(file_size_lines[chromosome])\n                    self.coverage_log2 = np.zeros(file_size_lines[chromosome])\n                    list_index = 0\n                    previous_chromosome = chromosome\n                    # first elem\n                    self.positions[list_index] = start\n                    self.coverage[list_index] = coverage\n                    self.coverage_log2[list_index] = np.log2(coverage) if coverage != 0 else np.NaN  # use np.nanFUN\n                    list_index += 1\n                elif previous_chromosome != chromosome:\n                    # analysis here\n                    self.logger.debug(previous_chromosome)\n                    self.__remove_n_region_by_chromosome(previous_chromosome)\n                    cov_stats, norm_stats, cov_data = self.__normalization_and_statistics(previous_chromosome)\n                    self.genome_analysis[previous_chromosome] = {\"cov_data\": cov_data,\n                                                                 \"statistics\": cov_stats,\n                                                                 \"norm_statistics\": norm_stats}\n                    # init new chromosome\n                    self.positions = np.zeros(file_size_lines[chromosome])\n                    self.coverage = np.zeros(file_size_lines[chromosome])\n                    self.coverage_log2 = np.zeros(file_size_lines[chromosome])\n                    list_index = 0\n                    previous_chromosome = chromosome\n                    # first elem\n                    self.positions[list_index] = start\n                    self.coverage[list_index] = coverage\n                    self.coverage_log2[list_index] = np.log2(coverage) if coverage != 0 else np.NaN  # use np.nanFUN\n                    list_index += 1\n                else:\n                    self.positions[list_index] = start\n                    self.coverage[list_index] = coverage\n                    self.coverage_log2[list_index] = np.log2(coverage) if coverage != 0 else np.NaN  # use np.nanFUN\n                    list_index += 1\n        coverage_file_handler.close()\n\n        # compute leftover chromosome\n        self.__remove_n_region_by_chromosome(previous_chromosome)\n        cov_stats, norm_stats, cov_data = self.__normalization_and_statistics(previous_chromosome)\n        self.genome_analysis[previous_chromosome] = {\"cov_data\": cov_data,\n                                                     \"statistics\": cov_stats,\n                                                     \"norm_statistics\": norm_stats}\n\n        # setup, calculating border for deletion and duplications\n        self.cnv_metrics = CNVMetrics(genome_analysis=self.genome_analysis,\n                                      cnv_calls=self.cnv_calls_list,\n                                      exclusion_zones=self.metadata,\n                                      hashname=self.sample_id,\n                                      ploidy=self.ploidy,\n                                      output_dir=self.output_directory,\n                                      as_dev=self.as_dev, debug_dir=self.debug_dir)\n        # self.cnv_metrics.get_del_dup_borders()\n        self.lower_2n_threshold = self.cnv_metrics.del_border\n        self.upper_2n_threshold = self.cnv_metrics.dup_border\n\n        # clean up\n        self.positions = np.zeros(0)\n        self.coverage = np.zeros(0)\n        self.coverage_log2 = np.zeros(0)\n\n    def __normalization_and_statistics(self, chromosome) -> tuple:\n        self.logger.info(f'Number positions to be tested on chromosome {chromosome}: {self.coverage.size}')\n        # clean up\n        cov_stats = CoverageStatistics()\n        norm_stats = CoverageStatistics()\n        cov_data = CoverageData()\n        cov_data.normalized_cov = np.array(0)\n        if self.coverage.size > 0:\n            # calculate avg, std, median and remove outliers (right tail)\n            # use genome-wide average\n            # NOTE: select average coverage from genome-wide or chromosome depending on how different they are\n            genome_avg_cov = self.mosdepth_data.genome_mean_coverage\n            chromosome_avg_coverage = np.nanmean(self.coverage)\n            diff_genome_chr_coverage = abs(\n                genome_avg_cov - chromosome_avg_coverage) < genome_avg_cov * self.mosdepth_cov_genome_chr_diff\n            use_this_avg_cov = genome_avg_cov if diff_genome_chr_coverage else chromosome_avg_coverage\n            [avg, std] = [float(use_this_avg_cov), np.nanstd(self.coverage)]\n            for idx in range(0, self.coverage.size, 1):\n                cov = self.coverage[idx]\n                if cov > avg + (self.max_std_outlier_rm * std):\n                    self.coverage[idx] = np.NaN\n\n            # re-calculate avg, std, median without outliers (right tail)\n            chromosome_avg_coverage = np.nanmean(self.coverage)\n            use_this_avg_cov = genome_avg_cov  # if diff_genome_chr_coverage else chromosome_avg_coverage\n            [avg, std, med] = [float(use_this_avg_cov), np.nanstd(self.coverage), np.nanmedian(self.coverage)]\n            self.logger.debug([f'avg: {genome_avg_cov}|{chromosome_avg_coverage}, std: {std}, med: {med}'])\n\n            if chromosome in self.genome_info[\"chr_lengths_by_name\"]:\n                # data\n                cov_data.positions = self.positions\n                cov_data.coverage_raw = self.coverage\n                cov_data.coverage_log2 = self.coverage_log2\n                # stats\n                cov_stats.chromosome_len = self.genome_info[\"chr_lengths_by_name\"][chromosome]\n                cov_stats.chromosome_name = chromosome\n                cov_stats.average = avg\n                cov_stats.std_dev = std\n                cov_stats.median = med\n                cov_stats.min = np.nanmin(self.coverage)\n                cov_stats.max = np.nanmax(self.coverage)\n\n                # normalization, based on diploid organisms\n                normalize_by = med  # med:median | avg:average\n                normalized_candidates = NorAn.normalize_candidates(self.coverage, normalize_by)\n                normalized_candidates_ploidy = normalized_candidates * self.ploidy\n                if len(normalized_candidates) < 1:\n                    normalized_candidates_ploidy = normalized_candidates\n\n                # statistics for normalized bin\n                avg, std, min_val, max_val, med = NorAn.get_candidate_statistics(normalized_candidates)\n                norm_stats.chromosome_len = self.genome_info[\"chr_lengths_by_name\"][chromosome]\n                norm_stats.chromosome_name = chromosome\n                norm_stats.median = med\n                norm_stats.average = avg\n                norm_stats.std_dev = std\n                norm_stats.min = min_val\n                norm_stats.max = max_val\n\n                # norm data\n                cov_data.normalized_cov = normalized_candidates\n                cov_data.normalized_cov_ploidy = normalized_candidates_ploidy\n            else:\n                self.logger.warning(f\"Chromosome:{chromosome} not found in gene info!\")\n        return cov_stats, norm_stats, cov_data\n\n    def __remove_n_region_by_chromosome(self, chrom) -> None:\n        \"\"\"\n        Compare CNV candidates with \"N\" regions from the reference genome. If the candidate starts or ends inside a\n        \"N\" region, the candidate will is removed by assigning a np.nan value.\n        :param chrom: current chromosome\n        :return: None\n        \"\"\"\n        # we use two variables which contain all the current chromosome info\n        # self.positions\n        # self.coverage\n        if chrom in self.metadata:\n            for meta_start, meta_end in self.metadata[chrom]:\n                meta_start = int(meta_start)\n                meta_end = int(meta_end)\n                overlap_n = np.logical_and(self.positions >= meta_start, self.positions <= meta_end)\n                self.coverage[overlap_n] = np.nan\n\n    # *** MAIN ***\n    def call_cnv_coverage(self, cnv_id_list=None, write_csv=False):\n        if cnv_id_list is None:\n            cnv_id_list = []\n        # unique CNV IDs\n        for each_chromosome in self.genome_analysis.keys():\n            # global\n            # Section 1: Generating raw CNV candidates\n            self.logger.info(f\"Calculating CNVs for {self.sample_id} @ chromosome {each_chromosome}\")\n            cnv_caller = CNVCall(self.as_dev)\n            candidates_cnv_list = cnv_caller.cnv_coverage(self.genome_analysis[each_chromosome][\"cov_data\"],\n                                                          self.bin_size, each_chromosome, self.sample_id,\n                                                          self.cnv_metrics.del_border, self.cnv_metrics.dup_border)\n\n            # Generating CNV IDs\n            new_ids = CNV_ID.n_id_generator(len(candidates_cnv_list), cnv_id_list)\n            self.existing_cnv_ids = self.existing_cnv_ids + new_ids\n            for candidate, id in zip(candidates_cnv_list, new_ids):\n                candidate.set_id(id)\n\n            # save candidates for each chromosome\n            self.cnv_calls_list[each_chromosome] = candidates_cnv_list\n        self.raw_cnv_calls_list = self.cnv_calls_list.copy()\n\n    def refine_cnv_calls(self, write_csv=False):\n        self.logger.info(\"refining cnv calls\")\n        for each_chromosome in self.genome_analysis.keys():\n            candidates_cnv_list = self.cnv_calls_list[each_chromosome]\n            if write_csv:\n                self.dev_write_csv(each_chromosome)\n\n            self.logger.debug(f'total cnv candidates in {each_chromosome}: {len(candidates_cnv_list)} before merge')\n            final_cnv_candidates = self.merge_candidates(candidates_cnv_list, each_chromosome)\n            cnv_call_list = self.scaffold_candidates(final_cnv_candidates, each_chromosome) \\\n                if len(final_cnv_candidates) >= 2 else final_cnv_candidates\n            self.cnv_calls_list[each_chromosome] = cnv_call_list\n        pass\n\n    # Candidate CNV merge by distance and CNV type\n    def merge_candidates(self, candidates_cnv_list, chromosome_name):\n        if len(candidates_cnv_list) > 1:\n            dev_candidates_string = \"\"\n            merge_rounds = 1\n            self.logger.debug(f'Current merge cycle {merge_rounds}')\n            [n_merges, merged_candidates, _] = self.cnv_candidate_merge(candidates_cnv_list)\n            while n_merges > 0:\n                merge_rounds += 1\n                self.logger.debug(f'Current merge cycle {merge_rounds}')\n                # self.logger.info(f'n candidates in {chromosome_name}: {len(merged_candidates)}')\n                [n_merges, merged_candidates, dev_candidates_string] = self.cnv_candidate_merge(merged_candidates)\n            # self.logger.debug(dev_candidates_string)\n            self.logger.debug(f'Total merge rounds: {merge_rounds}')\n            candidates_cnv_list = self.clean_merged_candidates(merged_candidates)\n            # self.logger.debug(f'n candidates {chromosome_name}: {len(candidates_cnv_list)}')\n        return candidates_cnv_list\n\n    def cnv_candidate_merge(self, cnv_candidates):\n        def dev_candidates_merge(header=False):  # dev\n            if header:\n                return f'chr\\tleft\\tright\\tleft_size\\tright_size\\tcov_left\\tcov_right\\tdist_OK\\ttype_OK\\tCN_OK'\n            else:\n                return (f'{cnv_candidates[idx].chromosome}\\t{end0}\\t{start1}\\t'\n                        f'{cnv_cand.size}\\t{cnv_candidates[idx + 1].size}\\t'\n                        f'{cnv_candidates[idx].median_cov_norm}\\t{cnv_candidates[idx + 1].median_cov_norm}\\t'\n                        f'{(start1 - end0) < max_merge_distance}\\t{same_type}\\t{same_cnv_status}')\n\n        n_candidates = len(cnv_candidates)\n        # distance between end of i to start of i+1\n        # first on is given\n        idx = 0\n        n_merges = 0\n        merges_candidates = []\n        cnv_cand = cnv_candidates[idx]\n        cov_diff_threshold = self.cov_diff_threshold  # selected by trial and error in simulations\n        # dist => distance\n        dist_proportion = self.dist_proportion  # selected by trial and error, max distance proportion for merge\n        dist_min_overwrite = self.dist_min_overwrite\n        dev_candidates_string = [dev_candidates_merge(header=True)]\n        while idx < n_candidates - 1:\n            end0 = cnv_candidates[idx].end\n            start1 = cnv_candidates[idx + 1].start\n            same_type = cnv_candidates[idx].type == cnv_candidates[idx + 1].type\n            # copy number status difference\n            same_cnv_status = abs(\n                cnv_candidates[idx].median_cov_norm - cnv_candidates[idx + 1].median_cov_norm) <= cov_diff_threshold\n            # distance between merging windows is a % of the average length of the windows to be merged\n            max_merge_distance = max(np.mean([cnv_cand.size, cnv_candidates[idx + 1].size]) * dist_proportion,\n                                     dist_min_overwrite)\n            dev_candidates_string.append(dev_candidates_merge())\n            # Evaluate if the new span is covered by any blacklisted region\n            # Check if span_over_blacklisted_region = true if no overlap exists\n            span_over_blacklisted_region = any(\n                any(end0 <= int(val) <= start1 for val in tup) for tup in self.metadata[cnv_cand.chromosome])\n\n            if (start1 - end0) < max_merge_distance and same_type and same_cnv_status and \\\n                    not span_over_blacklisted_region:\n                # merge and extend\n                n_merges += 1\n                cnv_cand.add_candidates(\n                    cnv_candidates[idx + 1].pos,\n                    cnv_candidates[idx + 1].cov,\n                    cnv_candidates[idx + 1].id,\n                    cnv_candidates[idx + 1].merged_sample_references\n                )\n            else:\n                # save and show\n                cnv_cand.median_coverage_candidates_merged()\n                merges_candidates.append(cnv_cand)\n                # cnv_cand.show_candidates()  # DEV\n                # init\n                cnv_cand = cnv_candidates[idx + 1]\n            idx += 1\n        # one last time for the remaining ones\n        merges_candidates.append(cnv_cand)\n        # cnv_cand.show_candidates()  # DEV\n        return [n_merges, merges_candidates, \"\\n\".join(dev_candidates_string)]\n\n    def clean_merged_candidates(self, merged_candidates):\n        # current threshold is 100kb\n        clean_merged = []\n        for each_cand in merged_candidates:\n            if each_cand.size >= self.candidate_final_threshold:\n                clean_merged.append(each_cand)\n        return clean_merged\n\n    # Fill in the gaps after merge\n    def scaffold_candidates(self, final_candidates_list, chromosome_name):\n        cnv_list = []\n        n_scaffolds = 0\n        cov_data_chr = self.genome_analysis[chromosome_name][\"cov_data\"]\n        _index = 0\n        _cand = final_candidates_list[_index]\n        while _index < len(final_candidates_list) - 1:\n            _cand_next = final_candidates_list[_index + 1]\n            if _cand.type == _cand_next.type:\n                gap_start = np.where(cov_data_chr.positions == _cand.end)\n                gap_end = np.where(cov_data_chr.positions == _cand_next.start)\n                gap_data = cov_data_chr.normalized_cov_ploidy[gap_start[0][0]:gap_end[0][0]]\n                scaf_cov = np.nanmedian(gap_data)\n\n                # Restricting cnv scaffolding merge over 100 following N regions (NaN values)\n                scaf_cov_nan_overflow = np.count_nonzero(np.isnan(np.array(gap_data))) > 0\n                if abs(\n                        scaf_cov - np.nanmedian(list(_cand.cov) + list(_cand_next.cov))\n                ) <= self.cov_diff_threshold and not scaf_cov_nan_overflow:\n                    _cand.add_candidates(_cand_next.pos, _cand_next.cov, _cand_next.id,\n                                         _cand_next.merged_sample_references)\n                    n_scaffolds += 1\n                else:\n                    _cand.median_coverage_candidates_merged()\n                    cnv_list.append(_cand)\n                    _cand = _cand_next\n            else:\n                _cand.median_coverage_candidates_merged()\n                cnv_list.append(_cand)\n                # init with next\n                _cand = _cand_next\n            _index += 1\n        # one final\n        _cand.median_coverage_candidates_merged()\n        cnv_list.append(_cand)\n        return cnv_list\n\n    # Output results\n    def cnv_result_bed(self, method=\"\"):\n        if method == \"\":\n            output_bed = self.output_bed\n        else:\n            output_bed = os.path.join(os.path.join(self.output_directory, f'{method}{self.sample_id}.bed'))\n\n        bed_output = util.outputWriter.BedOutput(output_bed)\n        bed_output.make_bed(self.genome_analysis.keys(), self.cnv_calls_list)\n\n    def cnv_result_vcf(self, method=\"\"):\n        output_vcf = os.path.join(os.path.join(self.output_directory, f'{method}{self.sample_id}.vcf'))\n        vcf_output = util.outputWriter.VCFOutput(output_vcf, self.genome_info)\n        vcf_output.make_vcf(self.genome_analysis.keys(), self.cnv_calls_list, self.sample_id)\n\n    # Plots\n    def coverage_plot(self):\n        for each_chromosome in self.genome_analysis.keys():\n            plot_cnv = CoveragePlot()\n            plot_cnv.file_prefix = f'plot1-coverage-{self.sample_id}'\n            plot_cnv.output_directory = self.output_directory\n            chr_cov_data = self.genome_analysis[each_chromosome][\"cov_data\"]\n            plot_cnv.plot_coverage(each_chromosome, {\"pos\": chr_cov_data.positions, \"cov\": chr_cov_data.raw_coverage})\n\n    def cnv_plot(self, methode=\"\"):\n        for each_chromosome in self.genome_analysis.keys():\n            chr_cov_data = self.genome_analysis[each_chromosome][\"cov_data\"]\n            cov_stats = self.genome_analysis[each_chromosome][\"norm_statistics\"]\n            lower_bound = self.lower_2n_threshold\n            upper_bound = self.upper_2n_threshold\n            cov_stats.median = cov_stats.median * self.ploidy  # for diploid\n            new_plot_device = CNVPlot()\n            new_plot_device.output_directory = self.output_directory\n            new_plot_device.file_prefix = methode + self.sample_id\n            new_plot_device.plot_coverage_cnv(each_chromosome, cov_stats,\n                                              {\"pos\": chr_cov_data.positions,\n                                               \"cov\": chr_cov_data.normalized_cov_ploidy},\n                                              self.cnv_calls_list[each_chromosome], [lower_bound, upper_bound])\n\n    def convert_vcf_to_tabular(self, snv_af_bed_output):\n        self.logger.info(\"Parsing VCF to BED file\")\n        vcf_parser = vcf.VCFSNVParser(self.min_chr_length, self.as_dev)\n        self.logger.debug(\"Parsing: VCF -> DataFrame\")\n        vcf_df = vcf_parser.vcf_to_dataframe(self.snv_file)\n        self.logger.debug(\"Parsing: DataFrame -> BED\")\n        self.snv_af_df = vcf_parser.dataframe_to_tabular_file(vcf_df, self.coverage_file, snv_af_bed_output)\n        self.snv_af_bed = snv_af_bed_output\n\n    def call_cnv_af_region(self):\n        # self.cnv_calls_list[each_chromosome]\n        cnv_by_af = CNVAnalysisSNP(genome_info=self.genome_info,\n                                   output_directory=self.output_directory,\n                                   sample_id=self.sample_id,\n                                   as_dev=self.as_dev)\n        self.logger.info(\"Calculating CNV events based on SNV data\")\n        self.cnv_calls_list = cnv_by_af.af_cnv_call_region(self.cnv_calls_list, self.snv_af_bed)\n        cnv_by_af.call_cnv_af(self.snv_af_df, write_csv=True)\n\n    def get_cnv_metrics(self, refined_cnvs: bool = False):\n        \"\"\"\n        Calculates for every CNV call a metrics like the p value with statistical tests\n        :return:\n        \"\"\"\n        self.cnv_metrics.evaluate_cnvs(self.cnv_calls_list, refined_cnvs)\n\n    def write_intermediate_candidates(self, candidate_type: str = \"\") -> str:\n        \"\"\"\n        Writing intermediate object files out\n        :param candidate_type:\n        :return:\n        \"\"\"\n\n        intermediate_output_writer = util.outputWriter.IntermediateFile(self.output_directory)\n        #genome_info = intermediate_output_writer.convert_candidates_to_dictionary(self.genome_info)\n        cnv_calls_list_dict = intermediate_output_writer.convert_candidates_to_dictionary(self.cnv_calls_list)\n        raw_cnv_calls_list_dict = intermediate_output_writer.convert_candidates_to_dictionary(self.raw_cnv_calls_list)\n\n        analysis_dict = {\n                \"metadata\": {\"source\":\"spectre\",\"spectre_version\": \"0.1\"},\n                \"genome_info\": self.genome_info,\n                \"raw_cnvs\": raw_cnv_calls_list_dict,\n                \"refined_cnvs\": cnv_calls_list_dict,\n                \"analysis_metrics\": {\n                    \"min_chr_length\": self.min_chr_length,\n                    \"max_std_outlier_rm\": self.max_std_outlier_rm,\n                    \"mosdepth_cov_genome_chr_diff\": self.mosdepth_cov_genome_chr_diff,\n                    \"lower_2n_threshold\": self.lower_2n_threshold,\n                    \"upper_2n_threshold\": self.upper_2n_threshold,\n                    \"cov_diff_threshold\": self.cov_diff_threshold,\n                    \"dist_proportion\": self.dist_proportion,\n                    \"dist_min_overwrite\": self.dist_min_overwrite,\n                    \"candidate_final_threshold\": self.candidate_final_threshold,\n                    \"genome_mean\": np.mean(self.cnv_metrics.df_coverage_candidate_no_excl_zone_random_samples['coverage']),\n                    \"genome_sd\": np.std(self.cnv_metrics.df_coverage_candidate_no_excl_zone_random_samples['coverage']),\n                    \"genome_var\": np.var(self.cnv_metrics.df_coverage_candidate_no_excl_zone_random_samples['coverage'])\n                }\n            }\n        output_path = intermediate_output_writer.write_intermediate_file(analysis_dict, f\"{self.sample_id}\")\n        self.intermediate_candidates_file_location = output_path\n        return output_path\n\n    # ############################################\n    # dev\n    def dev_write_csv(self, each_chromosome):\n        csv_results = pd.DataFrame(\n            data={\"position\": self.genome_analysis[each_chromosome][\"cov_data\"].positions,\n                  \"mosdepth_cov\": self.genome_analysis[each_chromosome][\"cov_data\"].coverage_raw,\n                  \"norm_cov\": self.genome_analysis[each_chromosome][\"cov_data\"].normalized_cov,\n                  \"ploidy_cov\": self.genome_analysis[each_chromosome][\"cov_data\"].normalized_cov_ploidy\n                  }\n        )\n        csv_results[\"chr\"] = each_chromosome\n        output_file = f\"{self.debug_dir}/cnv_{self.sample_id}_byCoverage_chr{each_chromosome}.csv\"\n        self.logger.debug(f\"Writing coverage to {output_file}\")\n        csv_results.to_csv(output_file, index=False)", ""]}
{"filename": "analysis/__init__.py", "chunked_list": [""]}
{"filename": "analysis/cnv_metrics.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport hashlib\nimport random\nfrom scipy.stats import ks_2samp, norm\nimport logging as logger\n\n\nclass CNVMetrics(object):\n    def __init__(self, genome_analysis, exclusion_zones, cnv_calls=None,\n                 hashname: str = \"mock_dataname_of_coverage.csv\",ploidy:float=2.0,\n                 output_dir: str = \"\", as_dev: bool = False, debug_dir=\"\"):\n        self.genome_analysis = genome_analysis\n        self.cnv_calls = cnv_calls\n        self.hashname = hashname\n        self.ploidy = ploidy\n        self.output_dir = output_dir\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # conversion\n        self.df_mosdepth_coverage = self.__convert_genome_analysis_to_coverage_dataframe(genome_analysis)\n        self.blacklist_exclusions = self.__convert_blacklist_exclusion_zone(exclusion_zones)\n        # if cnv_calls:  # check if cnv_calls is emtpy\n        #    self.cnv_exclusion = self.__convert_cnv_calls_to_exclusions(cnv_calls)\n        #    self.blacklist_cnv_exclusion = self.__merge_dict_with_lists_as_value(self.blacklist_exclusions,\n        #                                                                         self.cnv_exclusion)\n        self.__prepare_cnv_evaluation()\n\n        # calculate borders\n        self.del_border, self.dup_border = self.calculate_del_dup_borders(1.0, 1.0, self.ploidy)\n        self.debug_dir = debug_dir\n\n    def __convert_genome_analysis_to_coverage_dataframe(self, genome_analysis):\n        \"\"\"\n        Convert the standard Spectre genome_analysis into a pandas dataframe, using the minimal required fields\n        to evaluate the CNV calls.\n        :param genome_analysis: standard Spectre genome_analysis dictionary\n        :return: pandas dataframe holding at least column values [\"chr\", \"position\", \"coverage\"]\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Convert Genome analysis to coverage dataframe\")\n        list_modsepth_coverage = [[key, genome_analysis[key][\"cov_data\"].positions,\n                                   genome_analysis[key][\"cov_data\"].normalized_cov_ploidy] for key in\n                                  genome_analysis.keys()]\n        df_mosdepth_coverage = pd.DataFrame(list_modsepth_coverage)\n        df_mosdepth_coverage.columns = [\"chr\", \"position\", \"coverage\"]\n        df_mosdepth_coverage = df_mosdepth_coverage.explode([\"position\", \"coverage\"])\n        df_mosdepth_coverage.reset_index(inplace=True)\n        return df_mosdepth_coverage\n\n    def __convert_cnv_calls_to_exclusions(self, cnv_call_list: dict):\n        \"\"\"\n        Converts the Spectre into a specific form of dictionary, which will be used to perform the statistical\n        analysis. Furthermore, the cnv calls play a role in randomly sampling coverage values which will be used\n        in the statistical analysis.\n        :param cnv_call_list: standard Spectre cnv_call_list\n        :return: Dictionary with a list of Dictionaries entries\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Converting cnv calls to exclusion zone\")\n        result = {}\n        for key in cnv_call_list.keys():\n            if key not in result.keys():\n                result[key] = []\n            for candidate in cnv_call_list[key]:\n                result[key].append({\"start\": candidate.start, \"end\": candidate.end, \"info\": \"cnv\"})\n        return result\n\n    def __convert_blacklist_exclusion_zone(self, blacklist: dict):\n        \"\"\"\n        Converts the given blacklist which contains a dict with lists of tupels into a dict with lists\n        and items as dicts.\n        :param blacklist: standard Spectre blacklist\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Converting blacklist to exclusion zone\")\n        result = {}\n        for key in blacklist.keys():\n            if key not in result.keys():\n                result[key] = []\n            # convert tupel into dictionary\n            for start, end in blacklist[key]:\n                result[key].append({\"start\": start, \"end\": end, \"info\": \"blacklist\"})\n        return result\n\n    def __merge_dict_with_lists_as_value(self, dict_1: dict, dict_2: dict) -> dict:\n        \"\"\"\n        Merges two dictionaries which hold a list as value and creating a completely new dictionary\n        in which the lists are appended to the other one.\n        :param dict_1: primary dictionary. (values will be first in the merged list)\n        :param dict_2: secondary dictionary. (values will be appended to the dict_1 values)\n        :return: merged and newly created dictionary\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Merging exclusion zones\")\n        dict_result = dict_1.copy()\n        for key in dict_2:\n            if key not in dict_1.keys():\n                dict_result[key] = dict_2[key].copy()\n            else:\n                dict_result[key] = [*dict_result[key], *dict_2[key].copy()]\n        return dict_result\n\n    def get_exclusion_zones_indices_in_coverage_data(self, df_mosdepth_coverage, dict_excl_zone):\n        \"\"\"\n        Gets all mosdepth coverage dataframe indices, which are within the exclusion zone.\n        :param df_mosdepth_coverage: dataframe with mosdepth coverage\n        :param dict_excl_zone: list of indices\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Getting exclusion zone indices in mosdepth coverage data\")\n        excl_indices = []\n        for excl_key in dict_excl_zone.keys():\n            df_chr = df_mosdepth_coverage.loc[df_mosdepth_coverage.chr == excl_key]\n            for excl_zone in dict_excl_zone[excl_key]:\n                in_excl_zone = df_chr.loc[\n                    df_chr.position.between((int(excl_zone[\"start\"]) - 1), int(excl_zone[\"end\"]))].index  # [\"index\"]\n\n                if len(in_excl_zone) > 0:\n                    excl_indices = excl_indices + list(in_excl_zone)\n        return excl_indices\n\n    def random_sample_selection(self, df_source, amount: float = 0.1):\n        \"\"\"\n        Selecting random samples outside any exclusion zone (blacklist, cnv, ...)\n        :param df_source:\n        :param amount:\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Generating random coverage sample indices\")\n        result_hash_of_filename = hashlib.md5(self.hashname.encode()).hexdigest()  # used for the random samples\n        random.seed(result_hash_of_filename)\n\n        fraction = amount\n        last_index = len(df_source.index) - 1  # len(df.index)\n        # last_index = df_source[\"excl_zone\"].size - 1\n        cov_window_amount = int(last_index * fraction)\n\n        samples_indices = random.sample(range(0, last_index), cov_window_amount)\n        sorted_samples_indices = sorted(samples_indices)\n        return sorted_samples_indices\n\n    def get_ks_test(self, cnv_coverage, df_random_sample_coverage):\n        \"\"\"\n        Calculating the ks-Score for given cnv coverage values and comparing the mean and sd to the distribution of\n        randomly samples coverage datasets\n        :param cnv_coverage: list of coverage values\n        :param df_random_sample_coverage: dataframe holding randomly selected coverage data\n        :return: dictionary with: cnv_call_mean, score (z-score), pvalue, ...\n        \"\"\"\n        # self.logger.debug(\"CNV-Metrics: Performing the KS-test\")\n        result = {}\n\n        cnv_coverage = np.array(cnv_coverage)\n        cnv_coverage = cnv_coverage[~np.isnan(cnv_coverage)]\n        random_sample_coverage = df_random_sample_coverage[\"coverage\"]\n        random_sample_coverage = random_sample_coverage.dropna()\n        statistic, pvalue = ks_2samp(cnv_coverage, random_sample_coverage)\n\n        result[\"cnv_call_mean\"] = round(np.nanmean(cnv_coverage), 3)\n        result[\"pvalue\"] = pvalue\n        result[\"sample_score\"] = abs(pvalue * 255)\n        result[\"score\"] = np.nan\n        result[\"statistics\"] = statistic\n        result[\"test_type\"] = \"ks-score\"\n        return result\n\n    def get_z_score(self, cnv_coverage, df_random_sample_coverage):\n        \"\"\"\n        Calculating the Z-Score for given cnv coverage values and comparing the mean and sd to the distribution of\n        randomly samples coverage datasets\n        :param cnv_coverage: list of coverage values\n        :param df_random_sample_coverage: dataframe holding randomly selected coverage data\n        :return: dictionary with: cnv_call_mean, score (z-score), pvalue, ...\n        \"\"\"\n        # self.logger.debug(\"CNV-Metrics: Performing the Z-Test\")\n        result = {}\n        cnv_coverage = np.array(cnv_coverage)\n        cnv_coverage = cnv_coverage[~np.isnan(cnv_coverage)]  # exclude NaN values\n        cnv_coverage = cnv_coverage[~np.isinf(cnv_coverage)]  # exclude -inf/inf values\n\n        random_sample_coverage = df_random_sample_coverage[\"coverage\"]\n        random_sample_coverage.replace([np.inf, -np.inf], np.nan, inplace=True)  # replace -inf/inf values with nan\n        random_sample_coverage = random_sample_coverage.dropna()  # exclude all nan values\n\n        mean = np.mean(cnv_coverage)\n        sd = np.std(cnv_coverage)\n\n        # Z score\n        z = (np.mean(random_sample_coverage) - mean) / (sd / np.sqrt(len(random_sample_coverage)))\n        # pvalue\n        z_capped = min(z, 10) if z > 1 else max(-10, z)\n        # pvalue = 2 * (1 - norm.cdf(abs(z_capped)))  # Two-tailed test\n        pvalue = norm.sf(abs(z_capped)) * 2\n        # pvalue = 2 * (1 - norm.cdf(abs(z)))  # Two-tailed test\n        gq = -np.log10(pvalue) * 10\n        gq_capped = min(60, int(gq))\n\n        result[\"cnv_call_mean\"] = np.nanmean(cnv_coverage)\n        result[\"score\"] = z_capped\n        result[\"pvalue\"] = pvalue\n        result[\"sample_score\"] = gq_capped\n        result[\"statistics\"] = None\n        result[\"test_type\"] = \"z-score\"\n        return result\n\n    def __prepare_cnv_evaluation(self) -> None:\n        \"\"\"\n        Preparing all the necessary variables, to performe the CNV evaluation on the cnv candidates\n        :return: None\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Preparing all parameters for the cnv evaluation\")\n        df_coverage_candidate = self.df_mosdepth_coverage.copy()\n        df_coverage_candidate[\"blacklist\"] = False  # holds only the values if row is in blacklist\n        # df_coverage_candidate[\"cnv\"] = False  # holds only values that are in cnv_events\n        # df_coverage_candidate[\"excl_zone\"] = False  # holds the combination of blacklist and cnv\n        df_coverage_candidate.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_coverage_candidate_no_Nans = df_coverage_candidate.dropna(subset=[\"coverage\"])  # contains no nan values\n\n        # get exclusion indices\n        excl_zone_blacklist_indices = self.get_exclusion_zones_indices_in_coverage_data(df_coverage_candidate_no_Nans,\n                                                                                        self.blacklist_exclusions)\n\n        # Add exclusion indices\n        # if len(excl_zone_blacklist_indices) > 0:\n        # df_coverage_candidate_no_Nans.loc[excl_zone_blacklist_indices, \"blacklist\"] = True\n        # df_coverage_candidate_no_Nans.loc[df_coverage_candidate_no_Nans.index.isin(excl_zone_blacklist_indices), \"blacklist\"] = True\n        df_coverage_candidate_no_Nans.iloc[\n            excl_zone_blacklist_indices, df_coverage_candidate_no_Nans.columns.get_loc(\"blacklist\")] = True\n\n        # load curated data\n        df_coverage_candidate_no_blacklist = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.blacklist == False]\n\n        # get random samples\n        no_blacklist_indices = self.random_sample_selection(df_coverage_candidate_no_blacklist, 0.1)\n        self.df_coverage_candidate_no_blacklist_random_samples = df_coverage_candidate_no_blacklist.iloc[\n            no_blacklist_indices]\n\n        # add for later use\n        if self.cnv_calls:\n            self.__recalculate_exlustion_zone(self.cnv_calls)\n        else:\n            self.df_coverage_candidate_no_excl_zone_random_samples = self.df_coverage_candidate_no_blacklist_random_samples.copy()\n\n    def __recalculate_exlustion_zone(self, cnv_calls) -> None:\n        \"\"\"\n        Recalculates the random sample coverage, based on new cnv_calls. This ensures that no coverage sample from\n        the blacklist or an existing CNV call will be used to give the CNV calls a score.\n        :param cnv_calls:\n        :return: None\n        \"\"\"\n        # setup\n        self.cnv_exclusion = self.__convert_cnv_calls_to_exclusions(cnv_calls)\n        self.blacklist_cnv_exclusion = self.__merge_dict_with_lists_as_value(self.blacklist_exclusions,\n                                                                             self.cnv_exclusion)\n\n        df_coverage_candidate = self.df_mosdepth_coverage.copy()\n        # df_coverage_candidate[\"blacklist\"] = False  # holds only the values if row is in blacklist\n        # df_coverage_candidate[\"cnv\"] = False  # holds only values that are in cnv_events\n        df_coverage_candidate[\"excl_zone\"] = False  # holds the combination of blacklist and cnv\n        df_coverage_candidate.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_coverage_candidate_no_Nans = df_coverage_candidate.dropna(subset=[\"coverage\"])  # contains no nan values\n\n        # get inclustio\n        # excl_zone_cnv_indices = self.get_exclusion_zones_indices_in_coverage_data(\n        #    df_coverage_candidate_no_Nans,self.cnv_exclusion)\n        excl_zone_backlist_cnv_indices = self.get_exclusion_zones_indices_in_coverage_data(\n            df_coverage_candidate_no_Nans, self.blacklist_cnv_exclusion)\n\n        # Add exclusion indices\n        # df_coverage_candidate_no_Nans.loc[excl_zone_cnv_indices, [\"cnv\"]] = True\n\n        if len(excl_zone_backlist_cnv_indices) > 0:\n            df_coverage_candidate_no_Nans.loc[excl_zone_backlist_cnv_indices, \"excl_zone\"] = True\n\n        # df_coverage_candidate_no_cnv = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.cnv == False]\n        df_coverage_candidate_no_excl_zone = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.excl_zone == False]\n\n        # Get random samples for samples with the same random\n        # no_cnv_indices = self.random_sample_selection(df_coverage_candidate_no_cnv, 0.1)\n        # self.df_coverage_candidate_no_cnv_random_samples = df_coverage_candidate_no_cnv.iloc[no_cnv_indices]\n\n        no_excl_zone_indices = self.random_sample_selection(df_coverage_candidate_no_excl_zone, 0.1)\n        self.df_coverage_candidate_no_excl_zone_random_samples = df_coverage_candidate_no_excl_zone.iloc[\n            no_excl_zone_indices]\n        pass\n\n    @staticmethod\n    def get_lower_border(mean: float, sd: float, sd_multiplier: float, epsilon: float = 0.01,\n                         epsilon_multiplier: float = 0):\n        return mean - (sd * sd_multiplier + (epsilon * epsilon_multiplier))\n\n    @staticmethod\n    def get_upper_border(mean: float, sd: float, sd_multiplier: float, epsilon: float = 0.01,\n                         epsilon_multiplier: float = 0):\n        return mean + (sd * sd_multiplier + (epsilon * epsilon_multiplier))\n\n    def calculate_del_dup_borders(self, del_border_multiplier: float = 1.0, dup_border_multiplier: float = 1.0,\n                                  ploidy: float = 2.0) -> tuple:\n        \"\"\"\n        Calculate dynamic borders for deletions (DEL) and duplications (DUP), based on the randomly selected coverage\n        samples.\n        :param del_border_multiplier: Multiplier for the tightness of the lower (deletion) border\n        :param dup_border_multiplier: Multiplier for the tightness of the upper (duplication) border\n        :return: Tupel with deletion border and duplication border (DEL,DUP)\n        \"\"\"\n        mean = float(np.mean(self.df_coverage_candidate_no_excl_zone_random_samples['coverage']))\n        sd = float(np.std(self.df_coverage_candidate_no_excl_zone_random_samples['coverage']))\n        var = np.var(self.df_coverage_candidate_no_excl_zone_random_samples['coverage'])\n\n        # Rough border estimation\n        lower_border = mean - (sd * del_border_multiplier)\n        upper_border = mean + (sd * dup_border_multiplier)\n\n        # Find optimal border\n        ploidy_offset_deletion_border = ploidy - 1\n        offset_duplication_border = ploidy + 1\n        epsilon = 0.01\n        step_cnt = 1\n        max_steps = 1000\n        found_new_deletion_border = False\n        found_new_duplication_border = False\n        ratio = 0.4\n\n        while step_cnt < max_steps:\n            # Calculate DEL border\n            if not found_new_deletion_border:\n                del_border_to_ploidy_offset = abs(lower_border - ploidy_offset_deletion_border) * (1 - ratio)\n                del_border_to_ploidy = abs(lower_border - ploidy) * ratio\n                if del_border_to_ploidy_offset > del_border_to_ploidy:\n                    lower_border = self.get_lower_border(mean, sd, del_border_multiplier, epsilon, step_cnt)\n                else:\n                    found_new_deletion_border = True\n\n            # Calculate DUP border\n            if not found_new_duplication_border:\n                dup_border_to_ploidy_offset = abs(upper_border - offset_duplication_border) * (1 - ratio)\n                dup_border_to_ploidy = abs(upper_border - ploidy) * ratio\n                if dup_border_to_ploidy_offset > dup_border_to_ploidy:\n                    upper_border = self.get_upper_border(mean, sd, dup_border_multiplier, epsilon, step_cnt)\n                else:\n                    found_new_duplication_border = True\n\n            step_cnt += 1\n\n        return lower_border, upper_border\n\n    def evaluate_cnvs(self, cnv_calls=None, refined_cnvs: bool = False) -> dict:\n        \"\"\"\n        Evaluating all submitted CNV calls, by applying statistical tests. Optionally, plotting the location of the CNVs\n        on the global coverage samples per chr.\n        :return: Modified CNVCandidates\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Evaluating all CNVs\")\n\n        if cnv_calls is not None:\n            self.logger.debug(\"CNV-Metrics: Recalculating\")\n            self.__recalculate_exlustion_zone(cnv_calls)\n\n        self.logger.info(f\"CNV-Metrics: DEL border:{self.del_border} \\t DUP border:{self.dup_border}\")\n        border_del = self.del_border\n        border_dup = self.dup_border\n\n        for cnv_chr in self.cnv_calls.keys():\n            if self.as_dev:\n                self.logger.debug(f\"CNV-Metrics: Creating a new plot at chromosome {cnv_chr}\")\n                fig, ax = plt.subplots()\n                x, bins, p = ax.hist(self.df_coverage_candidate_no_excl_zone_random_samples['coverage'], bins=100,\n                                     range=[0.0, 5.0])\n                width = p.patches[0].get_width()\n\n                ax.axvline(border_del, ymax=1, color=\"red\", linestyle=\":\")\n                ax.axvline(border_dup, ymax=1, color=\"green\", linestyle=\":\")\n                ax.axhline()\n                ax.text(1.2 - 0.65, max(x), f\"DEL: {round(self.del_border)}\")\n                ax.text(2.7 - 0.65, max(x), f\"DUP: {round(self.dup_border)}\")\n                ax.set_title(f\"Mean coverage position of CNVs at {cnv_chr} based on random global coverage samples\")\n                ax.set_xlabel('Coverage')\n                ax.set_ylabel('Count')\n\n            for cnv in self.cnv_calls[cnv_chr]:\n                # Z-Test 2 - DIY\n                if refined_cnvs:\n                    cnv_metrics_Z = self.get_z_score(cnv.cov, self.df_coverage_candidate_no_excl_zone_random_samples)\n                else:\n                    cnv_metrics_Z = {}\n                    cnv_metrics_Z[\"statistics\"] = None\n                    cnv_metrics_Z[\"score\"] = 0\n                    cnv_metrics_Z[\"pvalue\"] = 0\n                    cnv_metrics_Z[\"sample_score\"] = 0\n                    cnv_metrics_Z[\"cnv_call_mean\"] = np.nanmean(cnv.cov)\n\n                cnv.statistics[\"z-score\"] = {\"statistics\": cnv_metrics_Z[\"statistics\"],\n                                             \"score\": cnv_metrics_Z[\"score\"],\n                                             \"pvalue\": cnv_metrics_Z[\"pvalue\"],\n                                             \"sample_score\": cnv_metrics_Z[\"sample_score\"]}\n\n                if self.as_dev:\n                    if not np.isnan(cnv_metrics_Z[\"cnv_call_mean\"]):\n\n                        # pre check --> avoide div by 0\n                        if cnv_metrics_Z[\"cnv_call_mean\"] == 0:\n                            x_index = 0\n                        else:\n                            if not np.isinf(cnv_metrics_Z[\"cnv_call_mean\"]):\n                                x_index = int(cnv_metrics_Z[\"cnv_call_mean\"] / width)\n                            else:\n                                x_index = int(x[-1])\n                        # check if out of bounds\n                        if x_index >= len(x):\n                            cnv_y = x[-1]\n                        else:\n                            cnv_y = int(x[x_index])\n                        cnv_y = int(cnv_y)\n                        annotation_endpoint_offset = 5 * (100 - x_index) * (\n                                1 - (cnv_y / 150)) + (max(x) / 10)  # np.random.randint(100,300)\n\n                        # Add\n                        ax.annotate(f\"{cnv.id}\", xy=(cnv_metrics_Z[\"cnv_call_mean\"], cnv_y),\n                                    xytext=(cnv_metrics_Z[\"cnv_call_mean\"], cnv_y + annotation_endpoint_offset),\n                                    rotation=60,\n                                    fontsize=12, arrowprops=dict(facecolor='green', shrink=0.05))\n            if self.as_dev:\n                plot_path = f\"{self.debug_dir}/cnv-calls-{self.hashname}-at_chr-{cnv_chr}.png\"\n                self.logger.debug(f\"CNV-Metrics:{plot_path}\")\n                fig.savefig(plot_path)\n        return self.cnv_calls", "\nclass CNVMetrics(object):\n    def __init__(self, genome_analysis, exclusion_zones, cnv_calls=None,\n                 hashname: str = \"mock_dataname_of_coverage.csv\",ploidy:float=2.0,\n                 output_dir: str = \"\", as_dev: bool = False, debug_dir=\"\"):\n        self.genome_analysis = genome_analysis\n        self.cnv_calls = cnv_calls\n        self.hashname = hashname\n        self.ploidy = ploidy\n        self.output_dir = output_dir\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        # conversion\n        self.df_mosdepth_coverage = self.__convert_genome_analysis_to_coverage_dataframe(genome_analysis)\n        self.blacklist_exclusions = self.__convert_blacklist_exclusion_zone(exclusion_zones)\n        # if cnv_calls:  # check if cnv_calls is emtpy\n        #    self.cnv_exclusion = self.__convert_cnv_calls_to_exclusions(cnv_calls)\n        #    self.blacklist_cnv_exclusion = self.__merge_dict_with_lists_as_value(self.blacklist_exclusions,\n        #                                                                         self.cnv_exclusion)\n        self.__prepare_cnv_evaluation()\n\n        # calculate borders\n        self.del_border, self.dup_border = self.calculate_del_dup_borders(1.0, 1.0, self.ploidy)\n        self.debug_dir = debug_dir\n\n    def __convert_genome_analysis_to_coverage_dataframe(self, genome_analysis):\n        \"\"\"\n        Convert the standard Spectre genome_analysis into a pandas dataframe, using the minimal required fields\n        to evaluate the CNV calls.\n        :param genome_analysis: standard Spectre genome_analysis dictionary\n        :return: pandas dataframe holding at least column values [\"chr\", \"position\", \"coverage\"]\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Convert Genome analysis to coverage dataframe\")\n        list_modsepth_coverage = [[key, genome_analysis[key][\"cov_data\"].positions,\n                                   genome_analysis[key][\"cov_data\"].normalized_cov_ploidy] for key in\n                                  genome_analysis.keys()]\n        df_mosdepth_coverage = pd.DataFrame(list_modsepth_coverage)\n        df_mosdepth_coverage.columns = [\"chr\", \"position\", \"coverage\"]\n        df_mosdepth_coverage = df_mosdepth_coverage.explode([\"position\", \"coverage\"])\n        df_mosdepth_coverage.reset_index(inplace=True)\n        return df_mosdepth_coverage\n\n    def __convert_cnv_calls_to_exclusions(self, cnv_call_list: dict):\n        \"\"\"\n        Converts the Spectre into a specific form of dictionary, which will be used to perform the statistical\n        analysis. Furthermore, the cnv calls play a role in randomly sampling coverage values which will be used\n        in the statistical analysis.\n        :param cnv_call_list: standard Spectre cnv_call_list\n        :return: Dictionary with a list of Dictionaries entries\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Converting cnv calls to exclusion zone\")\n        result = {}\n        for key in cnv_call_list.keys():\n            if key not in result.keys():\n                result[key] = []\n            for candidate in cnv_call_list[key]:\n                result[key].append({\"start\": candidate.start, \"end\": candidate.end, \"info\": \"cnv\"})\n        return result\n\n    def __convert_blacklist_exclusion_zone(self, blacklist: dict):\n        \"\"\"\n        Converts the given blacklist which contains a dict with lists of tupels into a dict with lists\n        and items as dicts.\n        :param blacklist: standard Spectre blacklist\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Converting blacklist to exclusion zone\")\n        result = {}\n        for key in blacklist.keys():\n            if key not in result.keys():\n                result[key] = []\n            # convert tupel into dictionary\n            for start, end in blacklist[key]:\n                result[key].append({\"start\": start, \"end\": end, \"info\": \"blacklist\"})\n        return result\n\n    def __merge_dict_with_lists_as_value(self, dict_1: dict, dict_2: dict) -> dict:\n        \"\"\"\n        Merges two dictionaries which hold a list as value and creating a completely new dictionary\n        in which the lists are appended to the other one.\n        :param dict_1: primary dictionary. (values will be first in the merged list)\n        :param dict_2: secondary dictionary. (values will be appended to the dict_1 values)\n        :return: merged and newly created dictionary\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Merging exclusion zones\")\n        dict_result = dict_1.copy()\n        for key in dict_2:\n            if key not in dict_1.keys():\n                dict_result[key] = dict_2[key].copy()\n            else:\n                dict_result[key] = [*dict_result[key], *dict_2[key].copy()]\n        return dict_result\n\n    def get_exclusion_zones_indices_in_coverage_data(self, df_mosdepth_coverage, dict_excl_zone):\n        \"\"\"\n        Gets all mosdepth coverage dataframe indices, which are within the exclusion zone.\n        :param df_mosdepth_coverage: dataframe with mosdepth coverage\n        :param dict_excl_zone: list of indices\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Getting exclusion zone indices in mosdepth coverage data\")\n        excl_indices = []\n        for excl_key in dict_excl_zone.keys():\n            df_chr = df_mosdepth_coverage.loc[df_mosdepth_coverage.chr == excl_key]\n            for excl_zone in dict_excl_zone[excl_key]:\n                in_excl_zone = df_chr.loc[\n                    df_chr.position.between((int(excl_zone[\"start\"]) - 1), int(excl_zone[\"end\"]))].index  # [\"index\"]\n\n                if len(in_excl_zone) > 0:\n                    excl_indices = excl_indices + list(in_excl_zone)\n        return excl_indices\n\n    def random_sample_selection(self, df_source, amount: float = 0.1):\n        \"\"\"\n        Selecting random samples outside any exclusion zone (blacklist, cnv, ...)\n        :param df_source:\n        :param amount:\n        :return:\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Generating random coverage sample indices\")\n        result_hash_of_filename = hashlib.md5(self.hashname.encode()).hexdigest()  # used for the random samples\n        random.seed(result_hash_of_filename)\n\n        fraction = amount\n        last_index = len(df_source.index) - 1  # len(df.index)\n        # last_index = df_source[\"excl_zone\"].size - 1\n        cov_window_amount = int(last_index * fraction)\n\n        samples_indices = random.sample(range(0, last_index), cov_window_amount)\n        sorted_samples_indices = sorted(samples_indices)\n        return sorted_samples_indices\n\n    def get_ks_test(self, cnv_coverage, df_random_sample_coverage):\n        \"\"\"\n        Calculating the ks-Score for given cnv coverage values and comparing the mean and sd to the distribution of\n        randomly samples coverage datasets\n        :param cnv_coverage: list of coverage values\n        :param df_random_sample_coverage: dataframe holding randomly selected coverage data\n        :return: dictionary with: cnv_call_mean, score (z-score), pvalue, ...\n        \"\"\"\n        # self.logger.debug(\"CNV-Metrics: Performing the KS-test\")\n        result = {}\n\n        cnv_coverage = np.array(cnv_coverage)\n        cnv_coverage = cnv_coverage[~np.isnan(cnv_coverage)]\n        random_sample_coverage = df_random_sample_coverage[\"coverage\"]\n        random_sample_coverage = random_sample_coverage.dropna()\n        statistic, pvalue = ks_2samp(cnv_coverage, random_sample_coverage)\n\n        result[\"cnv_call_mean\"] = round(np.nanmean(cnv_coverage), 3)\n        result[\"pvalue\"] = pvalue\n        result[\"sample_score\"] = abs(pvalue * 255)\n        result[\"score\"] = np.nan\n        result[\"statistics\"] = statistic\n        result[\"test_type\"] = \"ks-score\"\n        return result\n\n    def get_z_score(self, cnv_coverage, df_random_sample_coverage):\n        \"\"\"\n        Calculating the Z-Score for given cnv coverage values and comparing the mean and sd to the distribution of\n        randomly samples coverage datasets\n        :param cnv_coverage: list of coverage values\n        :param df_random_sample_coverage: dataframe holding randomly selected coverage data\n        :return: dictionary with: cnv_call_mean, score (z-score), pvalue, ...\n        \"\"\"\n        # self.logger.debug(\"CNV-Metrics: Performing the Z-Test\")\n        result = {}\n        cnv_coverage = np.array(cnv_coverage)\n        cnv_coverage = cnv_coverage[~np.isnan(cnv_coverage)]  # exclude NaN values\n        cnv_coverage = cnv_coverage[~np.isinf(cnv_coverage)]  # exclude -inf/inf values\n\n        random_sample_coverage = df_random_sample_coverage[\"coverage\"]\n        random_sample_coverage.replace([np.inf, -np.inf], np.nan, inplace=True)  # replace -inf/inf values with nan\n        random_sample_coverage = random_sample_coverage.dropna()  # exclude all nan values\n\n        mean = np.mean(cnv_coverage)\n        sd = np.std(cnv_coverage)\n\n        # Z score\n        z = (np.mean(random_sample_coverage) - mean) / (sd / np.sqrt(len(random_sample_coverage)))\n        # pvalue\n        z_capped = min(z, 10) if z > 1 else max(-10, z)\n        # pvalue = 2 * (1 - norm.cdf(abs(z_capped)))  # Two-tailed test\n        pvalue = norm.sf(abs(z_capped)) * 2\n        # pvalue = 2 * (1 - norm.cdf(abs(z)))  # Two-tailed test\n        gq = -np.log10(pvalue) * 10\n        gq_capped = min(60, int(gq))\n\n        result[\"cnv_call_mean\"] = np.nanmean(cnv_coverage)\n        result[\"score\"] = z_capped\n        result[\"pvalue\"] = pvalue\n        result[\"sample_score\"] = gq_capped\n        result[\"statistics\"] = None\n        result[\"test_type\"] = \"z-score\"\n        return result\n\n    def __prepare_cnv_evaluation(self) -> None:\n        \"\"\"\n        Preparing all the necessary variables, to performe the CNV evaluation on the cnv candidates\n        :return: None\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Preparing all parameters for the cnv evaluation\")\n        df_coverage_candidate = self.df_mosdepth_coverage.copy()\n        df_coverage_candidate[\"blacklist\"] = False  # holds only the values if row is in blacklist\n        # df_coverage_candidate[\"cnv\"] = False  # holds only values that are in cnv_events\n        # df_coverage_candidate[\"excl_zone\"] = False  # holds the combination of blacklist and cnv\n        df_coverage_candidate.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_coverage_candidate_no_Nans = df_coverage_candidate.dropna(subset=[\"coverage\"])  # contains no nan values\n\n        # get exclusion indices\n        excl_zone_blacklist_indices = self.get_exclusion_zones_indices_in_coverage_data(df_coverage_candidate_no_Nans,\n                                                                                        self.blacklist_exclusions)\n\n        # Add exclusion indices\n        # if len(excl_zone_blacklist_indices) > 0:\n        # df_coverage_candidate_no_Nans.loc[excl_zone_blacklist_indices, \"blacklist\"] = True\n        # df_coverage_candidate_no_Nans.loc[df_coverage_candidate_no_Nans.index.isin(excl_zone_blacklist_indices), \"blacklist\"] = True\n        df_coverage_candidate_no_Nans.iloc[\n            excl_zone_blacklist_indices, df_coverage_candidate_no_Nans.columns.get_loc(\"blacklist\")] = True\n\n        # load curated data\n        df_coverage_candidate_no_blacklist = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.blacklist == False]\n\n        # get random samples\n        no_blacklist_indices = self.random_sample_selection(df_coverage_candidate_no_blacklist, 0.1)\n        self.df_coverage_candidate_no_blacklist_random_samples = df_coverage_candidate_no_blacklist.iloc[\n            no_blacklist_indices]\n\n        # add for later use\n        if self.cnv_calls:\n            self.__recalculate_exlustion_zone(self.cnv_calls)\n        else:\n            self.df_coverage_candidate_no_excl_zone_random_samples = self.df_coverage_candidate_no_blacklist_random_samples.copy()\n\n    def __recalculate_exlustion_zone(self, cnv_calls) -> None:\n        \"\"\"\n        Recalculates the random sample coverage, based on new cnv_calls. This ensures that no coverage sample from\n        the blacklist or an existing CNV call will be used to give the CNV calls a score.\n        :param cnv_calls:\n        :return: None\n        \"\"\"\n        # setup\n        self.cnv_exclusion = self.__convert_cnv_calls_to_exclusions(cnv_calls)\n        self.blacklist_cnv_exclusion = self.__merge_dict_with_lists_as_value(self.blacklist_exclusions,\n                                                                             self.cnv_exclusion)\n\n        df_coverage_candidate = self.df_mosdepth_coverage.copy()\n        # df_coverage_candidate[\"blacklist\"] = False  # holds only the values if row is in blacklist\n        # df_coverage_candidate[\"cnv\"] = False  # holds only values that are in cnv_events\n        df_coverage_candidate[\"excl_zone\"] = False  # holds the combination of blacklist and cnv\n        df_coverage_candidate.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_coverage_candidate_no_Nans = df_coverage_candidate.dropna(subset=[\"coverage\"])  # contains no nan values\n\n        # get inclustio\n        # excl_zone_cnv_indices = self.get_exclusion_zones_indices_in_coverage_data(\n        #    df_coverage_candidate_no_Nans,self.cnv_exclusion)\n        excl_zone_backlist_cnv_indices = self.get_exclusion_zones_indices_in_coverage_data(\n            df_coverage_candidate_no_Nans, self.blacklist_cnv_exclusion)\n\n        # Add exclusion indices\n        # df_coverage_candidate_no_Nans.loc[excl_zone_cnv_indices, [\"cnv\"]] = True\n\n        if len(excl_zone_backlist_cnv_indices) > 0:\n            df_coverage_candidate_no_Nans.loc[excl_zone_backlist_cnv_indices, \"excl_zone\"] = True\n\n        # df_coverage_candidate_no_cnv = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.cnv == False]\n        df_coverage_candidate_no_excl_zone = df_coverage_candidate_no_Nans.loc[df_coverage_candidate.excl_zone == False]\n\n        # Get random samples for samples with the same random\n        # no_cnv_indices = self.random_sample_selection(df_coverage_candidate_no_cnv, 0.1)\n        # self.df_coverage_candidate_no_cnv_random_samples = df_coverage_candidate_no_cnv.iloc[no_cnv_indices]\n\n        no_excl_zone_indices = self.random_sample_selection(df_coverage_candidate_no_excl_zone, 0.1)\n        self.df_coverage_candidate_no_excl_zone_random_samples = df_coverage_candidate_no_excl_zone.iloc[\n            no_excl_zone_indices]\n        pass\n\n    @staticmethod\n    def get_lower_border(mean: float, sd: float, sd_multiplier: float, epsilon: float = 0.01,\n                         epsilon_multiplier: float = 0):\n        return mean - (sd * sd_multiplier + (epsilon * epsilon_multiplier))\n\n    @staticmethod\n    def get_upper_border(mean: float, sd: float, sd_multiplier: float, epsilon: float = 0.01,\n                         epsilon_multiplier: float = 0):\n        return mean + (sd * sd_multiplier + (epsilon * epsilon_multiplier))\n\n    def calculate_del_dup_borders(self, del_border_multiplier: float = 1.0, dup_border_multiplier: float = 1.0,\n                                  ploidy: float = 2.0) -> tuple:\n        \"\"\"\n        Calculate dynamic borders for deletions (DEL) and duplications (DUP), based on the randomly selected coverage\n        samples.\n        :param del_border_multiplier: Multiplier for the tightness of the lower (deletion) border\n        :param dup_border_multiplier: Multiplier for the tightness of the upper (duplication) border\n        :return: Tupel with deletion border and duplication border (DEL,DUP)\n        \"\"\"\n        mean = float(np.mean(self.df_coverage_candidate_no_excl_zone_random_samples['coverage']))\n        sd = float(np.std(self.df_coverage_candidate_no_excl_zone_random_samples['coverage']))\n        var = np.var(self.df_coverage_candidate_no_excl_zone_random_samples['coverage'])\n\n        # Rough border estimation\n        lower_border = mean - (sd * del_border_multiplier)\n        upper_border = mean + (sd * dup_border_multiplier)\n\n        # Find optimal border\n        ploidy_offset_deletion_border = ploidy - 1\n        offset_duplication_border = ploidy + 1\n        epsilon = 0.01\n        step_cnt = 1\n        max_steps = 1000\n        found_new_deletion_border = False\n        found_new_duplication_border = False\n        ratio = 0.4\n\n        while step_cnt < max_steps:\n            # Calculate DEL border\n            if not found_new_deletion_border:\n                del_border_to_ploidy_offset = abs(lower_border - ploidy_offset_deletion_border) * (1 - ratio)\n                del_border_to_ploidy = abs(lower_border - ploidy) * ratio\n                if del_border_to_ploidy_offset > del_border_to_ploidy:\n                    lower_border = self.get_lower_border(mean, sd, del_border_multiplier, epsilon, step_cnt)\n                else:\n                    found_new_deletion_border = True\n\n            # Calculate DUP border\n            if not found_new_duplication_border:\n                dup_border_to_ploidy_offset = abs(upper_border - offset_duplication_border) * (1 - ratio)\n                dup_border_to_ploidy = abs(upper_border - ploidy) * ratio\n                if dup_border_to_ploidy_offset > dup_border_to_ploidy:\n                    upper_border = self.get_upper_border(mean, sd, dup_border_multiplier, epsilon, step_cnt)\n                else:\n                    found_new_duplication_border = True\n\n            step_cnt += 1\n\n        return lower_border, upper_border\n\n    def evaluate_cnvs(self, cnv_calls=None, refined_cnvs: bool = False) -> dict:\n        \"\"\"\n        Evaluating all submitted CNV calls, by applying statistical tests. Optionally, plotting the location of the CNVs\n        on the global coverage samples per chr.\n        :return: Modified CNVCandidates\n        \"\"\"\n        self.logger.debug(\"CNV-Metrics: Evaluating all CNVs\")\n\n        if cnv_calls is not None:\n            self.logger.debug(\"CNV-Metrics: Recalculating\")\n            self.__recalculate_exlustion_zone(cnv_calls)\n\n        self.logger.info(f\"CNV-Metrics: DEL border:{self.del_border} \\t DUP border:{self.dup_border}\")\n        border_del = self.del_border\n        border_dup = self.dup_border\n\n        for cnv_chr in self.cnv_calls.keys():\n            if self.as_dev:\n                self.logger.debug(f\"CNV-Metrics: Creating a new plot at chromosome {cnv_chr}\")\n                fig, ax = plt.subplots()\n                x, bins, p = ax.hist(self.df_coverage_candidate_no_excl_zone_random_samples['coverage'], bins=100,\n                                     range=[0.0, 5.0])\n                width = p.patches[0].get_width()\n\n                ax.axvline(border_del, ymax=1, color=\"red\", linestyle=\":\")\n                ax.axvline(border_dup, ymax=1, color=\"green\", linestyle=\":\")\n                ax.axhline()\n                ax.text(1.2 - 0.65, max(x), f\"DEL: {round(self.del_border)}\")\n                ax.text(2.7 - 0.65, max(x), f\"DUP: {round(self.dup_border)}\")\n                ax.set_title(f\"Mean coverage position of CNVs at {cnv_chr} based on random global coverage samples\")\n                ax.set_xlabel('Coverage')\n                ax.set_ylabel('Count')\n\n            for cnv in self.cnv_calls[cnv_chr]:\n                # Z-Test 2 - DIY\n                if refined_cnvs:\n                    cnv_metrics_Z = self.get_z_score(cnv.cov, self.df_coverage_candidate_no_excl_zone_random_samples)\n                else:\n                    cnv_metrics_Z = {}\n                    cnv_metrics_Z[\"statistics\"] = None\n                    cnv_metrics_Z[\"score\"] = 0\n                    cnv_metrics_Z[\"pvalue\"] = 0\n                    cnv_metrics_Z[\"sample_score\"] = 0\n                    cnv_metrics_Z[\"cnv_call_mean\"] = np.nanmean(cnv.cov)\n\n                cnv.statistics[\"z-score\"] = {\"statistics\": cnv_metrics_Z[\"statistics\"],\n                                             \"score\": cnv_metrics_Z[\"score\"],\n                                             \"pvalue\": cnv_metrics_Z[\"pvalue\"],\n                                             \"sample_score\": cnv_metrics_Z[\"sample_score\"]}\n\n                if self.as_dev:\n                    if not np.isnan(cnv_metrics_Z[\"cnv_call_mean\"]):\n\n                        # pre check --> avoide div by 0\n                        if cnv_metrics_Z[\"cnv_call_mean\"] == 0:\n                            x_index = 0\n                        else:\n                            if not np.isinf(cnv_metrics_Z[\"cnv_call_mean\"]):\n                                x_index = int(cnv_metrics_Z[\"cnv_call_mean\"] / width)\n                            else:\n                                x_index = int(x[-1])\n                        # check if out of bounds\n                        if x_index >= len(x):\n                            cnv_y = x[-1]\n                        else:\n                            cnv_y = int(x[x_index])\n                        cnv_y = int(cnv_y)\n                        annotation_endpoint_offset = 5 * (100 - x_index) * (\n                                1 - (cnv_y / 150)) + (max(x) / 10)  # np.random.randint(100,300)\n\n                        # Add\n                        ax.annotate(f\"{cnv.id}\", xy=(cnv_metrics_Z[\"cnv_call_mean\"], cnv_y),\n                                    xytext=(cnv_metrics_Z[\"cnv_call_mean\"], cnv_y + annotation_endpoint_offset),\n                                    rotation=60,\n                                    fontsize=12, arrowprops=dict(facecolor='green', shrink=0.05))\n            if self.as_dev:\n                plot_path = f\"{self.debug_dir}/cnv-calls-{self.hashname}-at_chr-{cnv_chr}.png\"\n                self.logger.debug(f\"CNV-Metrics:{plot_path}\")\n                fig.savefig(plot_path)\n        return self.cnv_calls", ""]}
{"filename": "analysis/call_cnv_AF.py", "chunked_list": ["import pandas as pd\nimport numpy as np\nimport pysam\nimport logging as logger\n\n\nclass CNVCall(object):\n    def __init__(self, genome_info:dict, output_directory=\"\", sample_id=\"\", as_dev:bool = False):\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.output_directory = output_directory\n        self.sample_id = sample_id\n        self.genome_info = genome_info\n        # snv bed\n        self.snv_af_bed_bgzip = None\n        # candidates coverage\n        self.cnv_candidates_by_coverage = None\n        # AF to CN state concordance\n        # difference between the \"expected AF\" and \"computed AF\" for a given CN state\n        self.af_diff_buffer_default = 0.1  # will be min(default, 20% min ratio); e.g. if 4:1, then min(0.5, 1/5 * 0.2)\n        self.default_proportion_min_af = 0.3  # 30%\n        # all AF are MAF meaning if AF > 0.5, then AF = 1 - AF\n        self.cn_state_to_af = {\n            0: 0,                # complete DEL then we should not have data, init = 0\n            1: 0,                # 0 or 1, as we use MAF, then only 0\n            3: 1/3,              # 2:1 ratio, as we use MAF then 1/3\n            4: [1/4, 2/4],       # 3:1 or 2:2 ratio, then 1/4 or 2/4\n            5: [1/5, 2/5],       # 4:1 or 3:2 ratio, then 1/5 or 2/5\n            6: [1/6, 2/6, 3/6],  # 5:1 or 4:2 or 3:3 ratio, then 1/6 or 2/6 or 3/6\n        }\n\n    # CN states and CNV calls\n    def get_CNV_type_from_af(self,af,slack):\n        maf = af if af < 0.5 else 1 - af  # minor allele frequency\n        upper_boundaries = 0.7\n        lower_boundaries = 0.3\n        if maf == 0.0 or maf == 1.0:\n            return \"DEL\"\n        elif upper_boundaries - slack < maf < upper_boundaries + slack or lower_boundaries - slack < maf < lower_boundaries + slack:\n            # if maf is beteen 0.7+/-slack (upper_boundaries) or 0.3+/-slack DUP\n            return \"DUP\"\n        else:\n            return \"\"\n\n    def check_af_duplication(self, cn, maf, slack) -> bool:\n        \"\"\"\n        Determines if the combination of copy number and minor allele frequency corresponds to a duplication.\n        :param cn: copy number\n        :param maf: minor allele frequency\n        :param slack: +/- off set from target allele frequency\n        :return: true = Duplication\n        \"\"\"\n        dr = cn % 2\n        # check for whole chromosome duplication\n        if dr == 0:  # duplication residual\n            if 0.5 - slack < maf < 0.5 + slack:  # one copy got duplicated\n                return True\n            else:\n                return False\n\n        # calculate upper and lower boundaries\n        lower_target_af = (dr / cn)  # CN = 3 -> lower_target_af = 1/3\n        upper_target_af = ((cn - dr) / cn)  # CN = 3 -> upper_target_af = 2/3\n\n        if lower_target_af - slack < maf < lower_target_af + slack or upper_target_af - slack < maf < upper_target_af + slack:\n            return True\n        return np.nan()\n\n    def af_cnv_call_region(self, cnv_candidates, snv_af_bed):\n        cnv_calls = {}\n        self.cnv_candidates_by_coverage = cnv_candidates\n        self.snv_af_bed_bgzip = f'{snv_af_bed}.gz'\n        pysam.tabix_compress(filename_in=snv_af_bed, filename_out=self.snv_af_bed_bgzip, force=True)\n        pysam.tabix_index(filename=self.snv_af_bed_bgzip, force=True, preset=\"bed\")\n        tabix_file = pysam.TabixFile(self.snv_af_bed_bgzip)\n        for each_chromosome in cnv_candidates.keys():\n            self.logger.debug(each_chromosome)\n            cnv_calls[each_chromosome] = []\n            for each_candidate in cnv_candidates[each_chromosome]:\n                af_list = []\n                for af_overlap_candidate in tabix_file.fetch(each_chromosome, each_candidate.start, each_candidate.end):\n                    [_, _, _, af_bin] = af_overlap_candidate.split(\"\\t\")\n                    af_list.append(float(af_bin))\n                af = np.mean(np.array(af_list))\n                if self.af_cn_state_concordance(af, each_candidate.cn_status):\n                    cnv_calls[each_chromosome].append(each_candidate)\n                else:\n                    self.logger.debug(f'FP,{each_candidate.chromosome},{each_candidate.start},{each_candidate.end},'\n                                    f'{each_candidate.cn_status},{each_candidate.type},{af}')\n        return cnv_calls\n\n    def af_cn_state_concordance(self, af, cn_state):\n        def af_cn_concordance(_af, _e_af, _af_diff_threshold):\n            self.logger.debug([abs(_af - _e_af), _af_diff_threshold])\n            if abs(_af - _e_af) <= _af_diff_threshold:\n                return 1\n            else:\n                return 0\n\n        score = 0\n        if int(cn_state) > 6:\n            # cn_state = \"6\"\n            score += 1\n        else:\n            expected_af = self.cn_state_to_af[cn_state]\n            if isinstance(expected_af, list):\n                for each_expected_af in expected_af:\n                    af_diff_threshold = each_expected_af * self.default_proportion_min_af\n                    score += af_cn_concordance(af, each_expected_af, af_diff_threshold)\n            else:\n                af_diff_threshold = expected_af*self.default_proportion_min_af if cn_state > 1 \\\n                    else self.af_diff_buffer_default\n                score += af_cn_concordance(af, expected_af, af_diff_threshold)\n            # if score == 0 then it is a false positive (FP) call, else we cannot decide, adn we do not decide\n        self.logger.debug(f'score: {score}')\n        return score != 0\n\n    ### DEV\n    def call_cnv_af(self, snv_af_df:pd.DataFrame,write_csv=False):\n        self.snv_af_df = snv_af_df\n        self.snv_af_df.columns = [\"chrom\",\"start\",\"stop\",\"af\"]\n        self.snv_af_df[\"cnvtype\"] = self.snv_af_df.apply(lambda row: self.get_CNV_type_from_af(row[\"af\"], 0.1), axis=1)\n        if write_csv:\n            self.logger.debug(\"writing CSV\"+ self.output_directory+\"/snv_af_df.csv\")\n            self.snv_af_df.to_csv(self.output_directory+\"/snv_af_df.csv\",sep=\"\\t\",index_label=False)\n\n        for each_chromosome in self.genome_info[\"chromosomes\"]:\n            print(each_chromosome)\n        pass", ""]}
{"filename": "analysis/cnv_candidate.py", "chunked_list": ["import numpy as np\nimport logging as logger\n\n\nclass CNVCandidate(object):\n    def __init__(self, sample_origin=\"\", as_dev=False):\n        self.chromosome = \"\"\n        self.start = 0\n        self.end = 0\n        self.size = 0\n        self.pos = []\n        self.cov = []\n        self.type = \"\"\n        self.cn_status = np.nan\n        self.gt = \"./.\"\n        self.median_cov_norm = np.nan\n        self.size_kb = np.nan\n        self.het_score = float()  # makes sense from 0-1\n        self.as_dev = as_dev\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.statistics = {}\n        self.support_cnv_calls = {}\n        self.id = \"\"\n        self.sample_origin = sample_origin  # e.g. hg002\n        self.merged_sample_references = set()\n        self.median_cov_norm = 0.0\n\n    def push_candidates(self, chromosome, cnv_cand_list, cnv_cov_cand_list, cnv_type):\n        self.chromosome = chromosome\n        self.pos = cnv_cand_list\n        self.start = int(self.pos[0])\n        self.end = int(self.pos[-1])\n        self.size = self.end - self.start\n        self.size_kb = int((self.end - self.start) / 1000)\n        self.cov = cnv_cov_cand_list\n        [self.cn_status, self.median_cov_norm] = self.set_copy_number_status(cnv_cov_cand_list)\n        self.set_gt()\n        self.type = cnv_type\n\n    def add_candidates(self, cnv_cand_pos, cnv_cand_cov, cnv_id, cnv_merged_ids):\n        self.pos = list(self.pos) + list(cnv_cand_pos)\n        self.start = int(self.pos[0])\n        self.end = int(self.pos[-1])\n        self.size = self.end - self.start\n        self.size_kb = int((self.end - self.start) / 1000)\n        self.cov = list(self.cov) + list(cnv_cand_cov)\n        self.merged_sample_references.add(cnv_id)\n        self.merged_sample_references |= cnv_merged_ids  # \"|\" joins sets\n\n    def set_gt(self):\n        if self.cn_status == 0:\n            self.gt = \"1/1\"\n        elif self.cn_status == 1 or self.cn_status == 3:\n            self.gt = \"0/1\"\n        elif self.cn_status == 2:\n            self.gt = \"0/0\"\n        else:\n            self.gt = \"./.\"\n\n    def median_coverage_candidates_merged(self):\n        if len(self.cov) > 0:\n            self.median_cov_norm = np.nanmedian(self.cov)\n        else:\n            self.median_cov_norm = 0.0\n\n    def set_id(self, id: str):\n        self.id = f\"Spectre.{self.type}.{id}\"\n\n    def reinitialize_candidate_values(self) -> None:\n        \"\"\"\n        Used after loading data from .spc file\n        :return: None\n        \"\"\"\n        self.cn_status = self.set_copy_number_status(self.cov)[0]  # set copy number status\n        self.median_coverage_candidates_merged()  # median of normalized coverage\n        self.merged_sample_references = set(self.merged_sample_references)  # convert list to set\n\n    @staticmethod\n    def set_copy_number_status(candidate_coverage):\n        median_candidates_coverage = np.nanmedian(candidate_coverage)\n        if median_candidates_coverage == np.inf or median_candidates_coverage == np.nan:\n            return [2, 2.0]  # we are working with diploid data\n            # one for regular signal ... inf can not be a valid CN state\n        # copy number state is given by integer type conversion of the float value median coverage, e.g. 1.51-> 1\n        return [int(round(median_candidates_coverage, 0)), median_candidates_coverage]\n\n    # functions required to use this object as a key in a dictionary\n    def __hash__(self):\n        return hash(self.id)\n\n    def __eq__(self, other):\n        return hash(self.id) == hash(other.id)", ""]}
{"filename": "analysis/coverage_stats.py", "chunked_list": ["import numpy as np\nimport logging as logger\n\n\nclass CoverageStatistics(object):\n    # should have \"get\" and \"set\" func but just use them directly\n    def __init__(self, as_dev=False):\n        # {\"average\": np.NaN, \"std_dev\": np.NaN, \"min\": np.NaN, \"max\": np.NaN}\n        # Stats\n        logger.basicConfig(level=logger.DEBUG) if as_dev else logger.basicConfig(level=logger.INFO)\n        self.logger = logger\n        self.chromosome_len = 0\n        self.chromosome_name = \"\"\n        self.average = np.NaN\n        self.std_dev = np.NaN\n        self.min = np.NaN\n        self.max = np.NaN\n        self.median = np.NaN\n\n    def print(self):\n        print_me = f'Statistics of chromosome {self.chromosome_name}' \\\n                   f'  chromosome length: {self.chromosome_len}\\n' \\\n                   f'  average coverage: {self.average}\\n' \\\n                   f'  median coverage: {self.median}\\n' \\\n                   f'  standard deviation: {np.round(self.std_dev, 3)}\\n' \\\n                   f'  min, max coverage: {np.round(self.min, 3)}, {np.round(self.max, 3)}\\n'\n        logger.info(print_me)", "\n\nclass CoverageData(object):\n    # should have \"get\" and \"set\" func but just use them directly\n    def __init__(self):\n        # raw data\n        self.coverage_raw = np.NaN\n        self.positions = np.NaN\n        self.coverage_log2 = np.NaN         # log2(x)\n        self.normalized_cov = np.NaN        # x/median\n        self.normalized_cov_ploidy = np.NaN   # x/median * 2 -> for diploid", ""]}
