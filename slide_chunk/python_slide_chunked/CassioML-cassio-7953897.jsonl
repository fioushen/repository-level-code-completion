{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\nimport pathlib\n\nhere = pathlib.Path(__file__).parent.resolve()\n\nsetup(\n    name='cassio',\n    version='0.1.0',\n    author='Stefano Lottini',\n    author_email='stefano.lottini@datastax.com',", "    author='Stefano Lottini',\n    author_email='stefano.lottini@datastax.com',\n    package_dir={\"\": \"src\"},\n    packages=find_packages(where='src'),\n    # entry_points={\n    #     \"console_scripts\": [\n    #         # will we ever have a command-line cassio script?\n    #         \"cassio=cassio:main\",\n    #     ],\n    # },", "    #     ],\n    # },\n    url='https://github.com/hemidactylus/cassio',\n    license='LICENSE.txt',\n    description='A framework-agnostic Python library to seamlessly integrate Apache Cassandra(R) with ML/LLM/genAI workloads.',\n    long_description=(here / \"README.md\").read_text(encoding=\"utf-8\"),\n    long_description_content_type=\"text/markdown\",\n    install_requires=[\n        \"numpy>=1.0\",\n        \"cassandra-driver>=3.28.0\",", "        \"numpy>=1.0\",\n        \"cassandra-driver>=3.28.0\",\n    ],\n    python_requires=\">=3.8\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        #\n        \"Programming Language :: Python :: 3\",", "        #\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",", "        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    keywords=\"cassandra, ai, llm\",\n)\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"\nfixtures for testing\n\"\"\"\n\nimport os\n\nimport pytest\n\nfrom cassandra.cluster import Cluster  # type: ignore\nfrom cassandra.auth import PlainTextAuthProvider  # type: ignore", "from cassandra.cluster import Cluster  # type: ignore\nfrom cassandra.auth import PlainTextAuthProvider  # type: ignore\n\nfrom cassio.table.cql import MockDBSession\n\n\n# DB session (as per settings detected in env vars)\ndbSession = None\n\n\ndef createDBSessionSingleton():\n    global dbSession\n    if dbSession is None:\n        mode = os.environ[\"TEST_DB_MODE\"]\n        # the proper DB session is created as required\n        if mode == \"ASTRA_DB\":\n            ASTRA_DB_SECURE_BUNDLE_PATH = os.environ[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n            ASTRA_DB_CLIENT_ID = \"token\"\n            ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n            cluster = Cluster(\n                cloud={\n                    \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n                },\n                auth_provider=PlainTextAuthProvider(\n                    ASTRA_DB_CLIENT_ID,\n                    ASTRA_DB_APPLICATION_TOKEN,\n                ),\n            )\n            dbSession = cluster.connect()\n        elif mode == \"LOCAL_CASSANDRA\":\n            CASSANDRA_USERNAME = os.environ.get(\"CASSANDRA_USERNAME\")\n            CASSANDRA_PASSWORD = os.environ.get(\"CASSANDRA_PASSWORD\")\n            if CASSANDRA_USERNAME and CASSANDRA_PASSWORD:\n                auth_provider = PlainTextAuthProvider(\n                    CASSANDRA_USERNAME,\n                    CASSANDRA_PASSWORD,\n                )\n            else:\n                auth_provider = None\n            CASSANDRA_CONTACT_POINTS = os.environ.get(\"CASSANDRA_CONTACT_POINTS\")\n            if CASSANDRA_CONTACT_POINTS:\n                contact_points = [\n                    cp.strip() for cp in CASSANDRA_CONTACT_POINTS.split(\",\")\n                ]\n            else:\n                contact_points = None\n            #\n            cluster = Cluster(\n                contact_points,\n                auth_provider=auth_provider,\n            )\n            localSession = cluster.connect()\n            return localSession\n        else:\n            raise NotImplementedError\n    return dbSession", "\n\ndef createDBSessionSingleton():\n    global dbSession\n    if dbSession is None:\n        mode = os.environ[\"TEST_DB_MODE\"]\n        # the proper DB session is created as required\n        if mode == \"ASTRA_DB\":\n            ASTRA_DB_SECURE_BUNDLE_PATH = os.environ[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n            ASTRA_DB_CLIENT_ID = \"token\"\n            ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n            cluster = Cluster(\n                cloud={\n                    \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n                },\n                auth_provider=PlainTextAuthProvider(\n                    ASTRA_DB_CLIENT_ID,\n                    ASTRA_DB_APPLICATION_TOKEN,\n                ),\n            )\n            dbSession = cluster.connect()\n        elif mode == \"LOCAL_CASSANDRA\":\n            CASSANDRA_USERNAME = os.environ.get(\"CASSANDRA_USERNAME\")\n            CASSANDRA_PASSWORD = os.environ.get(\"CASSANDRA_PASSWORD\")\n            if CASSANDRA_USERNAME and CASSANDRA_PASSWORD:\n                auth_provider = PlainTextAuthProvider(\n                    CASSANDRA_USERNAME,\n                    CASSANDRA_PASSWORD,\n                )\n            else:\n                auth_provider = None\n            CASSANDRA_CONTACT_POINTS = os.environ.get(\"CASSANDRA_CONTACT_POINTS\")\n            if CASSANDRA_CONTACT_POINTS:\n                contact_points = [\n                    cp.strip() for cp in CASSANDRA_CONTACT_POINTS.split(\",\")\n                ]\n            else:\n                contact_points = None\n            #\n            cluster = Cluster(\n                contact_points,\n                auth_provider=auth_provider,\n            )\n            localSession = cluster.connect()\n            return localSession\n        else:\n            raise NotImplementedError\n    return dbSession", "\n\ndef getDBKeyspace():\n    mode = os.environ[\"TEST_DB_MODE\"]\n    if mode == \"ASTRA_DB\":\n        ASTRA_DB_KEYSPACE = os.environ[\"ASTRA_DB_KEYSPACE\"]\n        return ASTRA_DB_KEYSPACE\n    elif mode == \"LOCAL_CASSANDRA\":\n        CASSANDRA_KEYSPACE = os.environ[\"CASSANDRA_KEYSPACE\"]\n        return CASSANDRA_KEYSPACE", "\n\n# Fixtures\n\n\n@pytest.fixture(scope=\"session\")\ndef db_session():\n    return createDBSessionSingleton()\n\n", "\n\n@pytest.fixture(scope=\"session\")\ndef db_keyspace():\n    return getDBKeyspace()\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_db_session():\n    return MockDBSession()", "def mock_db_session():\n    return MockDBSession()\n"]}
{"filename": "tests/integration/test_vector_table.py", "chunked_list": ["\"\"\"\nVector search tests\n\"\"\"\n\nimport time\nimport pytest\n\nfrom cassio.vector import VectorTable\n\n", "\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestVectorTable:\n    \"\"\"\n    DB-backed tests for VectorTable\n    \"\"\"\n\n    def test_put_and_get(self, db_session, db_keyspace):\n        vtable_name1 = \"vector_table_1\"\n        v_emb_dim_1 = 3\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name1};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name1,\n            embedding_dimension=v_emb_dim_1,\n            primary_key_type=\"TEXT\",\n        )\n        v_table.put(\n            \"document\",\n            [1, 2, 3],\n            \"doc_id\",\n            {\"a\": \"value_1\"},\n            None,\n        )\n        assert v_table.get(\"doc_id\") == {\n            \"document_id\": \"doc_id\",\n            \"metadata\": {\"a\": \"value_1\"},\n            \"document\": \"document\",\n            \"embedding_vector\": [1, 2, 3],\n        }\n\n    def test_put_and_search(self, db_session, db_keyspace):\n        vtable_name_2 = \"vector_table_2\"\n        v_emb_dim_2 = 3\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_2};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name_2,\n            embedding_dimension=v_emb_dim_2,\n            primary_key_type=\"TEXT\",\n        )\n        v_table.put(\n            \"document\",\n            [5, 5, 10],\n            \"doc_id1\",\n            {\"a\": 1},\n            None,\n        )\n        v_table.put(\n            \"document\",\n            [\n                10,\n                5,\n                5,\n            ],\n            \"doc_id2\",\n            {\"a\": 2},\n            None,\n        )\n        v_table.put(\n            \"document\",\n            [5, 10, 5],\n            \"doc_id3\",\n            {\"a\": 3},\n            None,\n        )\n        matches = v_table.search(\n            [6, 10, 6],\n            1,\n            \"cos\",\n            0.5,\n        )\n        assert len(matches) == 1\n        assert matches[0][\"document_id\"] == \"doc_id3\"\n\n    def test_put_and_search_async(self, db_session, db_keyspace):\n        vtable_name_2a = \"vector_table_2async\"\n        v_emb_dim_2a = 3\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_2a};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name_2a,\n            embedding_dimension=v_emb_dim_2a,\n            primary_key_type=\"TEXT\",\n        )\n        futures = [\n            v_table.put_async(\n                \"document\",\n                [5, 5, 10],\n                \"doc_id1\",\n                {\"a\": 1},\n                None,\n            ),\n            v_table.put_async(\n                \"document\",\n                [\n                    10,\n                    5,\n                    5,\n                ],\n                \"doc_id2\",\n                {\"a\": 2},\n                None,\n            ),\n            v_table.put_async(\n                \"document\",\n                [5, 10, 5],\n                \"doc_id3\",\n                {\"a\": 3},\n                None,\n            ),\n        ]\n        for f in futures:\n            _ = f.result()\n        matches = v_table.search(\n            [6, 10, 6],\n            1,\n            \"cos\",\n            0.5,\n        )\n        assert len(matches) == 1\n        assert matches[0][\"document_id\"] == \"doc_id3\"\n\n    def test_put_intpk_and_get(self, db_session, db_keyspace):\n        vtable_name_3 = \"vector_table_3\"\n        v_emb_dim_3 = 6\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_3};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name_3,\n            embedding_dimension=v_emb_dim_3,\n            primary_key_type=\"INT\",\n        )\n        v_table.put(\n            \"document_int\",\n            [0.1] * v_emb_dim_3,\n            9999,\n            {\"a\": \"value_1\"},\n            None,\n        )\n        match = v_table.get(9999)\n        assert match[\"document\"] == \"document_int\"\n        assert match[\"metadata\"] == {\"a\": \"value_1\"}\n\n        match_no = v_table.get(123)\n        assert match_no is None\n\n    def test_null_json(self, db_session, db_keyspace):\n        vtable_name4 = \"vector_table_4\"\n        v_emb_dim_4 = 3\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name4};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name4,\n            embedding_dimension=v_emb_dim_4,\n            primary_key_type=\"TEXT\",\n        )\n        v_table.put(\n            \"document\",\n            [1, 2, 3],\n            \"doc_id\",\n            None,\n            None,\n        )\n        assert v_table.get(\"doc_id\") == {\n            \"document_id\": \"doc_id\",\n            \"metadata\": {},\n            \"document\": \"document\",\n            \"embedding_vector\": [1, 2, 3],\n        }\n\n    def test_nullsearch_results(self, db_session, db_keyspace):\n        vtable_name5 = \"vector_table_5\"\n        v_emb_dim_5 = 5\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name5};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name5,\n            embedding_dimension=v_emb_dim_5,\n            primary_key_type=\"INT\",\n        )\n        v_table.put(\"boasting\", [2, 2, 2, 2, 2], 123)\n        assert v_table.search([1, 0, 0, 0, 0], 10, \"cos\", 1.01) == []\n        # cannot use zero-vectors with cosine similarity:\n        with pytest.raises(ValueError):\n            _ = v_table.search([0, 0, 0, 0, 0], 10, \"cos\", 1.01)\n        v_table.clear()\n\n    def test_ttl(self, db_session, db_keyspace):\n        vtable_name6 = \"vector_table_6\"\n        v_emb_dim_6 = 2\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name6};\")\n        v_table = VectorTable(\n            db_session,\n            db_keyspace,\n            table=vtable_name6,\n            embedding_dimension=v_emb_dim_6,\n            primary_key_type=\"TEXT\",\n        )\n        #\n        v_table.put(\"this is short lived\", [1, 0], \"short_lived\", ttl_seconds=2)\n        v_table.put(\"this is long lived\", [0, 1], \"long_lived\", ttl_seconds=5)\n        time.sleep(0.2)\n        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 2\n        time.sleep(2.5)\n        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 1\n        time.sleep(3.0)\n        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 0", "\n\nif __name__ == \"__main__\":\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n\n    TestVectorTable().test_null_json(s, k)\n", ""]}
{"filename": "tests/integration/test_tableclasses_clusteredmetadatavectorcassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - ClusteredMetadataVectorCassandraTable\n\"\"\"\nimport math\nimport pytest\n\nfrom cassandra import InvalidRequest  # type: ignore\n\nfrom cassio.table.tables import (\n    ClusteredMetadataVectorCassandraTable,", "from cassio.table.tables import (\n    ClusteredMetadataVectorCassandraTable,\n)\n\n\nN = 16\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestClusteredMetadataVectorCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"c_m_v_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        # \"INT\" here means: partition_id is a number (for fun)\n        t = ClusteredMetadataVectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=[\"INT\", \"TEXT\"],\n            partition_id=0,\n        )\n\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n            t.put(\n                row_id=f\"theta_{n_theta}\",\n                body_blob=f\"theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n                metadata={\n                    group: True,\n                    \"n_theta_mod_2\": n_theta % 2,\n                    \"group\": group,\n                },\n            )\n        # fill another partition (999 = \"the other one\")\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n            t.put(\n                row_id=f\"Z_theta_{n_theta}\",\n                body_blob=f\"Z_theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n                partition_id=999,\n                metadata={\n                    group: True,\n                    \"n_theta_mod_2\": n_theta % 2,\n                    \"group\": group,\n                },\n            )\n\n        # retrieval\n        theta_1 = t.get(row_id=\"theta_1\")\n        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n        assert theta_1[\"partition_id\"] == 0\n\n        # retrieval with metadata filtering\n        theta_1b = t.get(row_id=\"theta_1\", metadata={\"odd\": True})\n        assert theta_1b == theta_1\n        theta_1n = t.get(row_id=\"theta_1\", metadata={\"even\": True})\n        assert theta_1n is None\n\n        # ANN\n        # a vector halfway between 0 and 1 inserted above\n        query_theta = 1 * math.pi * 2 / (2 * N)\n        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n        ann_results1 = list(t.ann_search(ref_vector, n=4))\n        assert {r[\"row_id\"] for r in ann_results1[:2]} == {\"theta_1\", \"theta_0\"}\n        assert {r[\"row_id\"] for r in ann_results1[2:4]} == {\"theta_2\", \"theta_15\"}\n        # ANN with metadata filtering\n        ann_results_md1 = list(t.ann_search(ref_vector, n=4, metadata={\"odd\": True}))\n        assert {r[\"row_id\"] for r in ann_results_md1[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md1[2:4]} == {\"theta_3\", \"theta_13\"}\n        # and in another way...\n        ann_results_md2 = list(t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\"}))\n        assert {r[\"row_id\"] for r in ann_results_md2[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md2[2:4]} == {\"theta_3\", \"theta_13\"}\n        # with two conditions ...\n        ann_results_md3 = list(\n            t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\", \"odd\": True})\n        )\n        assert {r[\"row_id\"] for r in ann_results_md3[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md3[2:4]} == {\"theta_3\", \"theta_13\"}\n\n        # retrieval on 999\n        ztheta_1 = t.get(row_id=\"Z_theta_1\", partition_id=999)\n        assert abs(ztheta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(ztheta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n        assert ztheta_1[\"partition_id\"] == 999\n\n        # retrieval with metadata filtering on 999\n        ztheta_1b = t.get(row_id=\"Z_theta_1\", metadata={\"odd\": True}, partition_id=999)\n        assert ztheta_1b == ztheta_1\n        ztheta_1n = t.get(row_id=\"Z_theta_1\", metadata={\"even\": True}, partition_id=999)\n        assert ztheta_1n is None\n        # \"theta_1\" is not an ID on 999:\n        ztheta_1n2 = t.get(row_id=\"theta_1\", metadata={\"odd\": True}, partition_id=999)\n        assert ztheta_1n2 is None\n\n        # ANN on 999\n        # a vector halfway between 0 and 1 inserted above\n        zquery_theta = 1 * math.pi * 2 / (2 * N)\n        zref_vector = [math.cos(zquery_theta), math.sin(zquery_theta)]\n        zann_results1 = list(t.ann_search(zref_vector, n=4, partition_id=999))\n        assert {r[\"row_id\"] for r in zann_results1[:2]} == {\"Z_theta_1\", \"Z_theta_0\"}\n        assert {r[\"row_id\"] for r in zann_results1[2:4]} == {\"Z_theta_2\", \"Z_theta_15\"}\n        # ANN with metadata filtering\n        zann_results_md1 = list(\n            t.ann_search(zref_vector, n=4, metadata={\"odd\": True}, partition_id=999)\n        )\n        assert {r[\"row_id\"] for r in zann_results_md1[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md1[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n        # and in another way...\n        zann_results_md2 = list(\n            t.ann_search(zref_vector, n=4, metadata={\"group\": \"odd\"}, partition_id=999)\n        )\n        assert {r[\"row_id\"] for r in zann_results_md2[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md2[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n        # with two conditions ...\n        zann_results_md3 = list(\n            t.ann_search(\n                zref_vector,\n                n=4,\n                metadata={\"group\": \"odd\", \"odd\": True},\n                partition_id=999,\n            )\n        )\n        assert {r[\"row_id\"] for r in zann_results_md3[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md3[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n\n        # cross-partition ANN search test\n        t_xpart = ClusteredMetadataVectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=[\"INT\", \"TEXT\"],\n            skip_provisioning=True,\n        )\n        # a vector at 1/4 step from the \"_0\" for both partitions\n        xp_query_theta = 1 * math.pi * 2 / (4 * N)\n        xp_vector = [math.cos(xp_query_theta), math.sin(xp_query_theta)]\n        xpart_results = list(\n            t_xpart.ann_search(\n                xp_vector,\n                n=2,\n            )\n        )\n        assert {r[\"row_id\"] for r in xpart_results} == {\"theta_0\", \"Z_theta_0\"}\n        # \"cross partition GET\" (i.e. partition_id not specified).\n        # Outside of ANN this should throw an error\n        with pytest.raises(InvalidRequest):\n            _ = t.get(row_id=\"not_enough_info\", partition_id=None)\n        with pytest.raises(InvalidRequest):\n            _ = t_xpart.get(row_id=\"not_enough_info\")\n\n        t.clear()", "@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestClusteredMetadataVectorCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"c_m_v_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        # \"INT\" here means: partition_id is a number (for fun)\n        t = ClusteredMetadataVectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=[\"INT\", \"TEXT\"],\n            partition_id=0,\n        )\n\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n            t.put(\n                row_id=f\"theta_{n_theta}\",\n                body_blob=f\"theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n                metadata={\n                    group: True,\n                    \"n_theta_mod_2\": n_theta % 2,\n                    \"group\": group,\n                },\n            )\n        # fill another partition (999 = \"the other one\")\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n            t.put(\n                row_id=f\"Z_theta_{n_theta}\",\n                body_blob=f\"Z_theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n                partition_id=999,\n                metadata={\n                    group: True,\n                    \"n_theta_mod_2\": n_theta % 2,\n                    \"group\": group,\n                },\n            )\n\n        # retrieval\n        theta_1 = t.get(row_id=\"theta_1\")\n        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n        assert theta_1[\"partition_id\"] == 0\n\n        # retrieval with metadata filtering\n        theta_1b = t.get(row_id=\"theta_1\", metadata={\"odd\": True})\n        assert theta_1b == theta_1\n        theta_1n = t.get(row_id=\"theta_1\", metadata={\"even\": True})\n        assert theta_1n is None\n\n        # ANN\n        # a vector halfway between 0 and 1 inserted above\n        query_theta = 1 * math.pi * 2 / (2 * N)\n        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n        ann_results1 = list(t.ann_search(ref_vector, n=4))\n        assert {r[\"row_id\"] for r in ann_results1[:2]} == {\"theta_1\", \"theta_0\"}\n        assert {r[\"row_id\"] for r in ann_results1[2:4]} == {\"theta_2\", \"theta_15\"}\n        # ANN with metadata filtering\n        ann_results_md1 = list(t.ann_search(ref_vector, n=4, metadata={\"odd\": True}))\n        assert {r[\"row_id\"] for r in ann_results_md1[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md1[2:4]} == {\"theta_3\", \"theta_13\"}\n        # and in another way...\n        ann_results_md2 = list(t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\"}))\n        assert {r[\"row_id\"] for r in ann_results_md2[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md2[2:4]} == {\"theta_3\", \"theta_13\"}\n        # with two conditions ...\n        ann_results_md3 = list(\n            t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\", \"odd\": True})\n        )\n        assert {r[\"row_id\"] for r in ann_results_md3[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md3[2:4]} == {\"theta_3\", \"theta_13\"}\n\n        # retrieval on 999\n        ztheta_1 = t.get(row_id=\"Z_theta_1\", partition_id=999)\n        assert abs(ztheta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(ztheta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n        assert ztheta_1[\"partition_id\"] == 999\n\n        # retrieval with metadata filtering on 999\n        ztheta_1b = t.get(row_id=\"Z_theta_1\", metadata={\"odd\": True}, partition_id=999)\n        assert ztheta_1b == ztheta_1\n        ztheta_1n = t.get(row_id=\"Z_theta_1\", metadata={\"even\": True}, partition_id=999)\n        assert ztheta_1n is None\n        # \"theta_1\" is not an ID on 999:\n        ztheta_1n2 = t.get(row_id=\"theta_1\", metadata={\"odd\": True}, partition_id=999)\n        assert ztheta_1n2 is None\n\n        # ANN on 999\n        # a vector halfway between 0 and 1 inserted above\n        zquery_theta = 1 * math.pi * 2 / (2 * N)\n        zref_vector = [math.cos(zquery_theta), math.sin(zquery_theta)]\n        zann_results1 = list(t.ann_search(zref_vector, n=4, partition_id=999))\n        assert {r[\"row_id\"] for r in zann_results1[:2]} == {\"Z_theta_1\", \"Z_theta_0\"}\n        assert {r[\"row_id\"] for r in zann_results1[2:4]} == {\"Z_theta_2\", \"Z_theta_15\"}\n        # ANN with metadata filtering\n        zann_results_md1 = list(\n            t.ann_search(zref_vector, n=4, metadata={\"odd\": True}, partition_id=999)\n        )\n        assert {r[\"row_id\"] for r in zann_results_md1[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md1[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n        # and in another way...\n        zann_results_md2 = list(\n            t.ann_search(zref_vector, n=4, metadata={\"group\": \"odd\"}, partition_id=999)\n        )\n        assert {r[\"row_id\"] for r in zann_results_md2[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md2[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n        # with two conditions ...\n        zann_results_md3 = list(\n            t.ann_search(\n                zref_vector,\n                n=4,\n                metadata={\"group\": \"odd\", \"odd\": True},\n                partition_id=999,\n            )\n        )\n        assert {r[\"row_id\"] for r in zann_results_md3[:2]} == {\n            \"Z_theta_1\",\n            \"Z_theta_15\",\n        }\n        assert {r[\"row_id\"] for r in zann_results_md3[2:4]} == {\n            \"Z_theta_3\",\n            \"Z_theta_13\",\n        }\n\n        # cross-partition ANN search test\n        t_xpart = ClusteredMetadataVectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=[\"INT\", \"TEXT\"],\n            skip_provisioning=True,\n        )\n        # a vector at 1/4 step from the \"_0\" for both partitions\n        xp_query_theta = 1 * math.pi * 2 / (4 * N)\n        xp_vector = [math.cos(xp_query_theta), math.sin(xp_query_theta)]\n        xpart_results = list(\n            t_xpart.ann_search(\n                xp_vector,\n                n=2,\n            )\n        )\n        assert {r[\"row_id\"] for r in xpart_results} == {\"theta_0\", \"Z_theta_0\"}\n        # \"cross partition GET\" (i.e. partition_id not specified).\n        # Outside of ANN this should throw an error\n        with pytest.raises(InvalidRequest):\n            _ = t.get(row_id=\"not_enough_info\", partition_id=None)\n        with pytest.raises(InvalidRequest):\n            _ = t_xpart.get(row_id=\"not_enough_info\")\n\n        t.clear()", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m \\\n    #   tests.integration.test_tableclasses_clusteredmetadatavectorcassandratable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestClusteredMetadataVectorCassandraTable().test_crud(s, k)", ""]}
{"filename": "tests/integration/test_tableclasses_metadatavectorcassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - MetadataVectorCassandraTable\n\"\"\"\nimport math\nimport pytest\n\nfrom cassio.table.tables import (\n    MetadataVectorCassandraTable,\n)\n", ")\n\n\nN = 16\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestMetadataVectorCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"m_v_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = MetadataVectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=\"TEXT\",\n        )\n\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n            t.put(\n                row_id=f\"theta_{n_theta}\",\n                body_blob=f\"theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n                metadata={\n                    group: True,\n                    \"n_theta_mod_2\": n_theta % 2,\n                    \"group\": group,\n                },\n            )\n\n        # retrieval\n        theta_1 = t.get(row_id=\"theta_1\")\n        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\n        # retrieval with metadata filtering\n        theta_1b = t.get(row_id=\"theta_1\", metadata={\"odd\": True})\n        assert theta_1b == theta_1\n        theta_1n = t.get(row_id=\"theta_1\", metadata={\"even\": True})\n        assert theta_1n is None\n\n        # ANN\n        # a vector halfway between 0 and 1 inserted above\n        query_theta = 1 * math.pi * 2 / (2 * N)\n        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n        ann_results1 = list(t.ann_search(ref_vector, n=4))\n        assert {r[\"row_id\"] for r in ann_results1[:2]} == {\"theta_1\", \"theta_0\"}\n        assert {r[\"row_id\"] for r in ann_results1[2:4]} == {\"theta_2\", \"theta_15\"}\n        # ANN with metadata filtering\n        ann_results_md1 = list(t.ann_search(ref_vector, n=4, metadata={\"odd\": True}))\n        assert {r[\"row_id\"] for r in ann_results_md1[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md1[2:4]} == {\"theta_3\", \"theta_13\"}\n        # and in another way...\n        ann_results_md2 = list(t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\"}))\n        assert {r[\"row_id\"] for r in ann_results_md2[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md2[2:4]} == {\"theta_3\", \"theta_13\"}\n        # with two conditions ...\n        ann_results_md3 = list(\n            t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\", \"odd\": True})\n        )\n        assert {r[\"row_id\"] for r in ann_results_md3[:2]} == {\"theta_1\", \"theta_15\"}\n        assert {r[\"row_id\"] for r in ann_results_md3[2:4]} == {\"theta_3\", \"theta_13\"}\n\n        # metric search testing\n        # a vector between 15 and 1 inserted above, but closer to the 1\n        query_theta_mt = 1 * math.pi * 2 / (2 * N + 1)\n        ref_vector_mt = [math.cos(query_theta_mt), math.sin(query_theta_mt)]\n        ann_results_mt = list(\n            t.metric_ann_search(\n                ref_vector_mt,\n                n=2,\n                metric=\"cos\",\n                metric_threshold=0.8,\n                metadata={\"group\": \"odd\"},\n            )\n        )\n        assert [r[\"row_id\"] for r in ann_results_mt] == [\"theta_1\", \"theta_15\"]\n        assert ann_results_mt[0][\"distance\"] > ann_results_mt[1][\"distance\"]\n        # a max distance makes this call return just two results despite the n=3:\n        ann_results_mt_l2 = list(\n            t.metric_ann_search(\n                ref_vector_mt,\n                n=3,\n                metric=\"l2\",\n                metric_threshold=0.6,\n                metadata={\"group\": \"odd\"},\n            )\n        )\n        assert [r[\"row_id\"] for r in ann_results_mt_l2] == [\"theta_1\", \"theta_15\"]\n        assert ann_results_mt_l2[0][\"distance\"] < ann_results_mt_l2[1][\"distance\"]\n        t.clear()", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n    #   tests.integration.test_tableclasses_metadatavectorcassandratable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestMetadataVectorCassandraTable().test_crud(s, k)", ""]}
{"filename": "tests/integration/test_tableclasses_plaincassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - PlainCassandraTable\n\"\"\"\n\nimport pytest\n\nfrom cassio.table.tables import (\n    PlainCassandraTable,\n)\n", ")\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestPlainCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = PlainCassandraTable(\n            db_session, db_keyspace, table_name, primary_key_type=\"TEXT\"\n        )\n        t.put(row_id=\"empty_row\")\n        gotten1 = t.get(row_id=\"empty_row\")\n        assert gotten1 == {\"row_id\": \"empty_row\", \"body_blob\": None}\n        t.put(row_id=\"full_row\", body_blob=\"body_blob\")\n        gotten2 = t.get(row_id=\"full_row\")\n        assert gotten2 == {\"row_id\": \"full_row\", \"body_blob\": \"body_blob\"}\n        t.delete(row_id=\"full_row\")\n        gotten2n = t.get(row_id=\"full_row\")\n        assert gotten2n is None\n        t.clear()\n        gotten1n = t.get(row_id=\"empty_row\")\n        assert gotten1n is None", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n    #   tests.integration.test_tableclasses_plaincassandratable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestPlainCassandraTable().test_crud(s, k)", ""]}
{"filename": "tests/integration/test_tableclasses_clusteredcassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - ClusteredCassandraTable\n\"\"\"\n\nimport pytest\n\nfrom cassio.table.tables import (\n    ClusteredCassandraTable,\n)\n", ")\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestClusteredCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"c_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = ClusteredCassandraTable(\n            db_session, db_keyspace, table_name, partition_id=\"my_part\"\n        )\n        t.put(row_id=\"reg_row\", body_blob=\"reg_blob\")\n        gotten1 = t.get(row_id=\"reg_row\")\n        assert gotten1 == {\n            \"row_id\": \"reg_row\",\n            \"partition_id\": \"my_part\",\n            \"body_blob\": \"reg_blob\",\n        }\n        t.put(row_id=\"irr_row\", partition_id=\"other_p\", body_blob=\"irr_blob\")\n        gotten2n = t.get(row_id=\"irr_row\")\n        assert gotten2n is None\n        gotten2 = t.get(row_id=\"irr_row\", partition_id=\"other_p\")\n        assert gotten2 == {\n            \"row_id\": \"irr_row\",\n            \"partition_id\": \"other_p\",\n            \"body_blob\": \"irr_blob\",\n        }\n        #\n        t.delete(row_id=\"reg_row\")\n        assert t.get(row_id=\"reg_row\") is None\n        t.delete(row_id=\"irr_row\")\n        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is not None\n        t.delete(row_id=\"irr_row\", partition_id=\"other_p\")\n        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is None\n        #\n        t.put(row_id=\"nr1\")\n        t.put(row_id=\"nr2\", partition_id=\"another_p\")\n        assert t.get(row_id=\"nr1\") is not None\n        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n        t.delete_partition()\n        assert t.get(row_id=\"nr1\") is None\n        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n        t.clear()\n\n    def test_partition_ordering(self, db_session, db_keyspace):\n        table_name_asc = \"c_ct_asc\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_asc};\")\n        table_name_desc = \"c_ct_desc\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_desc};\")\n        t_asc = ClusteredCassandraTable(\n            db_session, db_keyspace, table_name_asc, partition_id=\"my_part\"\n        )\n        t_desc = ClusteredCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name_desc,\n            partition_id=\"my_part\",\n            ordering_in_partition=\"desc\",\n        )\n        #\n        t_asc.put(row_id=\"row1\", body_blob=\"blob1\")\n        t_asc.put(row_id=\"row2\", body_blob=\"blob1\")\n        t_asc.put(row_id=\"row3\", body_blob=\"blob1\")\n        part_rows = t_asc.get_partition(n=2)\n        assert [gotten[\"row_id\"] for gotten in part_rows] == [\"row1\", \"row2\"]\n        assert len(list(t_asc.get_partition())) == 3\n        assert len(list(t_asc.get_partition(n=10))) == 3\n        #\n        t_desc.put(row_id=\"row1\", body_blob=\"blob1\")\n        t_desc.put(row_id=\"row2\", body_blob=\"blob1\")\n        t_desc.put(row_id=\"row3\", body_blob=\"blob1\")\n        part_rows = t_desc.get_partition(n=2)\n        assert [gotten[\"row_id\"] for gotten in part_rows] == [\"row3\", \"row2\"]\n        assert len(list(t_desc.get_partition())) == 3\n        assert len(list(t_desc.get_partition(n=10))) == 3\n        #\n        t_asc.clear()\n        t_desc.clear()\n\n    def test_crud_async(self, db_session, db_keyspace):\n        table_name = \"c_ct_asy\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = ClusteredCassandraTable(\n            db_session, db_keyspace, table_name, partition_id=\"my_part\"\n        )\n        rf1 = t.put_async(row_id=\"reg_row\", body_blob=\"reg_blob\")\n        _ = rf1.result()\n        gotten1 = t.get(row_id=\"reg_row\")\n        assert gotten1 == {\n            \"row_id\": \"reg_row\",\n            \"partition_id\": \"my_part\",\n            \"body_blob\": \"reg_blob\",\n        }\n        rf2 = t.put_async(\n            row_id=\"irr_row\", partition_id=\"other_p\", body_blob=\"irr_blob\"\n        )\n        _ = rf2.result()\n        gotten2n = t.get(row_id=\"irr_row\")\n        assert gotten2n is None\n        gotten2 = t.get(row_id=\"irr_row\", partition_id=\"other_p\")\n        assert gotten2 == {\n            \"row_id\": \"irr_row\",\n            \"partition_id\": \"other_p\",\n            \"body_blob\": \"irr_blob\",\n        }\n        #\n        rf3 = t.delete_async(row_id=\"reg_row\")\n        _ = rf3.result()\n        assert t.get(row_id=\"reg_row\") is None\n        rf4 = t.delete_async(row_id=\"irr_row\")\n        _ = rf4.result()\n        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is not None\n        rf5 = t.delete_async(row_id=\"irr_row\", partition_id=\"other_p\")\n        _ = rf5.result()\n        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is None\n        #\n        rf6 = t.put_async(row_id=\"nr1\")\n        _ = rf6.result()\n        rf7 = t.put_async(row_id=\"nr2\", partition_id=\"another_p\")\n        _ = rf7.result()\n        assert t.get(row_id=\"nr1\") is not None\n        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n        rf8 = t.delete_partition_async()\n        _ = rf8.result()\n        assert t.get(row_id=\"nr1\") is None\n        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n        rf9 = t.clear_async()\n        _ = rf9.result()\n        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is None", "\n\nif __name__ == \"__main__\":\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestClusteredCassandraTable().test_crud(s, k)\n", ""]}
{"filename": "tests/integration/test_tableclasses_vectorcassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - VectorCassandraTable\n\"\"\"\nimport math\nimport pytest\n\nfrom cassio.table.tables import (\n    VectorCassandraTable,\n)\n", ")\n\n\nN = 8\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestVectorCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"v_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = VectorCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            vector_dimension=2,\n            primary_key_type=\"TEXT\",\n        )\n\n        for n_theta in range(N):\n            theta = n_theta * math.pi * 2 / N\n            t.put(\n                row_id=f\"theta_{n_theta}\",\n                body_blob=f\"theta = {theta:.4f}\",\n                vector=[math.cos(theta), math.sin(theta)],\n            )\n\n        # retrieval\n        theta_1 = t.get(row_id=\"theta_1\")\n        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\n        # ANN\n        # a vector halfway between 0 and 1 inserted above\n        query_theta = 1 * math.pi * 2 / (2 * N)\n        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n        ann_results = list(t.ann_search(ref_vector, n=4))\n        assert {r[\"row_id\"] for r in ann_results[:2]} == {\"theta_1\", \"theta_0\"}\n        assert {r[\"row_id\"] for r in ann_results[2:4]} == {\"theta_2\", \"theta_7\"}\n\n        t.clear()", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n    #   tests.integration.test_tableclasses_vectorcassandratable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestVectorCassandraTable().test_crud(s, k)", ""]}
{"filename": "tests/integration/test_tableclasses_metadatacassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - MetadataCassandraTable\n\"\"\"\n\nimport pytest\n\nfrom cassio.table.tables import (\n    MetadataCassandraTable,\n)\n", ")\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestMetadataCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"m_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = MetadataCassandraTable(\n            db_session, db_keyspace, table_name, primary_key_type=\"TEXT\"\n        )\n        t.put(row_id=\"row1\", body_blob=\"bb1\")\n        gotten1 = t.get(row_id=\"row1\")\n        assert gotten1 == {\"row_id\": \"row1\", \"body_blob\": \"bb1\", \"metadata\": {}}\n        gotten1_s = list(t.find_entries(row_id=\"row1\", n=1))[0]\n        assert gotten1_s == {\"row_id\": \"row1\", \"body_blob\": \"bb1\", \"metadata\": {}}\n        t.put(row_id=\"row2\", metadata={})\n        gotten2 = t.get(row_id=\"row2\")\n        assert gotten2 == {\"row_id\": \"row2\", \"body_blob\": None, \"metadata\": {}}\n        md3 = {\"a\": 1, \"b\": \"Bee\", \"c\": True}\n        md3_string = {\"a\": \"1.0\", \"b\": \"Bee\", \"c\": \"true\"}\n        t.put(row_id=\"row3\", metadata=md3)\n        gotten3 = t.get(row_id=\"row3\")\n        assert gotten3 == {\"row_id\": \"row3\", \"body_blob\": None, \"metadata\": md3_string}\n        md4 = {\"c1\": True, \"c2\": True, \"c3\": True}\n        md4_string = {\"c1\": \"true\", \"c2\": \"true\", \"c3\": \"true\"}\n        t.put(row_id=\"row4\", metadata=md4)\n        gotten4 = t.get(row_id=\"row4\")\n        assert gotten4 == {\"row_id\": \"row4\", \"body_blob\": None, \"metadata\": md4_string}\n        # metadata searches:\n        md_gotten3a = t.get(metadata={\"a\": 1})\n        assert md_gotten3a == gotten3\n        md_gotten3b = t.get(metadata={\"b\": \"Bee\", \"c\": True})\n        assert md_gotten3b == gotten3\n        md_gotten4a = t.get(metadata={\"c1\": True, \"c3\": True})\n        assert md_gotten4a == gotten4\n        md_gotten4b = t.get(row_id=\"row4\", metadata={\"c1\": True, \"c3\": True})\n        assert md_gotten4b == gotten4\n        # 'search' proper\n        t.put(row_id=\"twin_a\", metadata={\"twin\": True, \"index\": 0})\n        t.put(row_id=\"twin_b\", metadata={\"twin\": True, \"index\": 1})\n        md_twins_gotten = sorted(\n            t.find_entries(metadata={\"twin\": True}, n=3),\n            key=lambda res: int(float(res[\"metadata\"][\"index\"])),\n        )\n        expected = [\n            {\n                \"metadata\": {\"twin\": \"true\", \"index\": \"0.0\"},\n                \"row_id\": \"twin_a\",\n                \"body_blob\": None,\n            },\n            {\n                \"metadata\": {\"twin\": \"true\", \"index\": \"1.0\"},\n                \"row_id\": \"twin_b\",\n                \"body_blob\": None,\n            },\n        ]\n        assert md_twins_gotten == expected\n        assert list(t.find_entries(row_id=\"fake\", n=10)) == []\n        #\n        t.clear()\n\n    def test_md_routing(self, db_session, db_keyspace):\n        test_md = {\"mds\": \"string\", \"mdn\": 255, \"mdb\": True}\n        test_md_string = {\"mds\": \"string\", \"mdn\": \"255.0\", \"mdb\": \"true\"}\n        #\n        table_name_all = \"m_ct_rtall\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_all};\")\n        t_all = MetadataCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name_all,\n            primary_key_type=\"TEXT\",\n            metadata_indexing=\"all\",\n        )\n        t_all.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md)\n        gotten_all = list(t_all.find_entries(metadata={\"mds\": \"string\"}, n=1))[0]\n        assert gotten_all[\"metadata\"] == test_md_string\n        t_all.clear()\n        #\n        table_name_none = \"m_ct_rtnone\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_none};\")\n        t_none = MetadataCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name_none,\n            primary_key_type=\"TEXT\",\n            metadata_indexing=\"none\",\n        )\n        t_none.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md)\n        with pytest.raises(ValueError):\n            # querying on non-indexed metadata fields:\n            t_none.find_entries(metadata={\"mds\": \"string\"}, n=1)\n        gotten_none = t_none.get(row_id=\"row1\")\n        assert gotten_none[\"metadata\"] == test_md_string\n        t_none.clear()\n        #\n        test_md_allowdeny = {\n            \"mdas\": \"MDAS\",\n            \"mdds\": \"MDDS\",\n            \"mdan\": 255,\n            \"mddn\": 127,\n            \"mdab\": True,\n            \"mddb\": True,\n        }\n        test_md_allowdeny_string = {\n            \"mdas\": \"MDAS\",\n            \"mdds\": \"MDDS\",\n            \"mdan\": \"255.0\",\n            \"mddn\": \"127.0\",\n            \"mdab\": \"true\",\n            \"mddb\": \"true\",\n        }\n        #\n        table_name_allow = \"m_ct_rtallow\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_allow};\")\n        t_allow = MetadataCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name_allow,\n            primary_key_type=\"TEXT\",\n            metadata_indexing=(\"allow\", {\"mdas\", \"mdan\", \"mdab\"}),\n        )\n        t_allow.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md_allowdeny)\n        with pytest.raises(ValueError):\n            t_allow.find_entries(metadata={\"mdds\": \"MDDS\"}, n=1)\n        gotten_allow = list(t_allow.find_entries(metadata={\"mdas\": \"MDAS\"}, n=1))[0]\n        assert gotten_allow[\"metadata\"] == test_md_allowdeny_string\n        t_allow.clear()\n        #\n        table_name_deny = \"m_ct_rtdeny\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_deny};\")\n        t_deny = MetadataCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name_deny,\n            primary_key_type=\"TEXT\",\n            metadata_indexing=(\"deny\", {\"mdds\", \"mddn\", \"mddb\"}),\n        )\n        t_deny.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md_allowdeny)\n        with pytest.raises(ValueError):\n            t_deny.find_entries(metadata={\"mdds\": \"MDDS\"}, n=1)\n        gotten_deny = list(t_deny.find_entries(metadata={\"mdas\": \"MDAS\"}, n=1))[0]\n        assert gotten_deny[\"metadata\"] == test_md_allowdeny_string\n        t_deny.clear()", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n    #   tests.integration.test_tableclasses_MetadataCassandraTable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestMetadataCassandraTable().test_crud(s, k)\n    TestMetadataCassandraTable().test_md_routing(s, k)", ""]}
{"filename": "tests/integration/test_tableclasses_elasticcassandratable.py", "chunked_list": ["\"\"\"\nTable classes integration test - ElasticCassandraTable\n\"\"\"\n\nimport pytest\n\nfrom cassio.table.tables import (\n    ElasticCassandraTable,\n)\n", ")\n\n\n@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\nclass TestElasticCassandraTable:\n    def test_crud(self, db_session, db_keyspace):\n        table_name = \"e_ct\"\n        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n        #\n        t = ElasticCassandraTable(\n            db_session,\n            db_keyspace,\n            table_name,\n            keys=[\"k1\", \"k2\"],\n            primary_key_type=[\"INT\", \"TEXT\"],\n        )\n        t.put(k1=1, k2=\"one\", body_blob=\"bb_1\")\n        gotten1 = t.get(k1=1, k2=\"one\")\n        assert gotten1 == {\"k1\": 1, \"k2\": \"one\", \"body_blob\": \"bb_1\"}\n        t.clear()", "\n\nif __name__ == \"__main__\":\n    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n    #   tests.integration.test_tableclasses_elasticcassandratable\n    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\n    s = createDBSessionSingleton()\n    k = getDBKeyspace()\n    TestElasticCassandraTable().test_crud(s, k)", ""]}
{"filename": "tests/unit/test_metadata_policy_normalization.py", "chunked_list": ["\"\"\"\nNormalization of metadata policy specification options\n\"\"\"\n\nfrom cassio.table.table_types import (\n    MetadataIndexingMode,\n)\n\nfrom cassio.table.mixins import MetadataMixin\n", "from cassio.table.mixins import MetadataMixin\n\n\nclass TestNormalizeMetadataPolicy:\n    def test_normalize_metadata_policy(self):\n        #\n        mdp1 = MetadataMixin._normalize_metadata_indexing_policy(\"all\")\n        assert mdp1 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, set())\n        #\n        mdp2 = MetadataMixin._normalize_metadata_indexing_policy(\"none\")\n        assert mdp2 == (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, set())\n        #\n        mdp3 = MetadataMixin._normalize_metadata_indexing_policy(\n            (\"default_to_Unsearchable\", [\"x\", \"y\"]),\n        )\n        assert mdp3 == (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, {\"x\", \"y\"})\n        #\n        mdp4 = MetadataMixin._normalize_metadata_indexing_policy(\n            (\"DenyList\", [\"z\"]),\n        )\n        assert mdp4 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, {\"z\"})\n        # s\n        mdp5 = MetadataMixin._normalize_metadata_indexing_policy(\n            (\"deny_LIST\", \"singlefield\")\n        )\n        assert mdp5 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, {\"singlefield\"})", ""]}
{"filename": "tests/unit/test_tableclasses_cql_generation.py", "chunked_list": ["\"\"\"\nCQL for mixin-based table classes tests\n\"\"\"\n\nfrom cassio.table.tables import (\n    VectorCassandraTable,\n    ClusteredElasticMetadataVectorCassandraTable,\n)\n\n\nclass TestTableClassesCQLGeneration:\n    def test_vector_cassandra_table(self, mock_db_session):\n        vt = VectorCassandraTable(\n            mock_db_session, \"k\", \"tn\", vector_dimension=765, primary_key_type=\"UUID\"\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"CREATE TABLE IF NOT EXISTS k.tn (  row_id UUID,   body_blob TEXT,   vector VECTOR<FLOAT,765>, PRIMARY KEY ( ( row_id )   )) ;\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n            ]\n        )\n\n        vt.delete(row_id=\"ROWID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE row_id = ?;\",\n                    (\"ROWID\",),\n                ),\n            ]\n        )\n\n        vt.get(row_id=\"ROWID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE row_id = ? ;\",\n                    (\"ROWID\",),\n                ),\n            ]\n        )\n\n        vt.put(row_id=\"ROWID\", body_blob=\"BODYBLOB\", vector=\"VECTOR\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, row_id) VALUES (?, ?, ?)  ;\",\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        \"ROWID\",\n                    ),\n                ),\n            ]\n        )\n\n        vt.ann_search([10, 11], 2)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn ORDER BY vector ANN OF ?  LIMIT ?;\",\n                    (\n                        [10, 11],\n                        2,\n                    ),\n                ),\n            ]\n        )\n\n        vt.clear()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"TRUNCATE TABLE k.tn;\",\n                    tuple(),\n                ),\n            ]\n        )\n\n    def test_clustered_elastic_metadata_vector_cassandra_table(self, mock_db_session):\n        cemvt = ClusteredElasticMetadataVectorCassandraTable(\n            mock_db_session,\n            \"k\",\n            \"tn\",\n            keys=[\"a\", \"b\"],\n            vector_dimension=765,\n            primary_key_type=[\"PUUID\", \"AT\", \"BT\"],\n            ttl_seconds=123,\n            partition_id=\"PRE-PART-ID\",\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"CREATE TABLE IF NOT EXISTS k.tn (  partition_id PUUID,   key_desc TEXT,   key_vals TEXT,   body_blob TEXT,   vector VECTOR<FLOAT,765>, attributes_blob TEXT,  metadata_s MAP<TEXT,TEXT>, PRIMARY KEY ( ( partition_id ) , key_desc, key_vals )) WITH CLUSTERING ORDER BY (key_desc ASC, key_vals ASC);\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS eidx_metadata_s_tn ON k.tn (ENTRIES(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n            ]\n        )\n\n        cemvt.delete(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n                ),\n            ]\n        )\n\n        cemvt.get(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n                ),\n            ]\n        )\n\n        cemvt.delete_partition(partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE partition_id = ?;\",\n                    (\"PARTITIONID\",),\n                ),\n            ]\n        )\n\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        md1 = {\"num1\": 123, \"num2\": 456, \"str1\": \"STR1\", \"tru1\": True}\n        md2 = {\"tru1\": True, \"tru2\": True}\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n            metadata=md1,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        {\n                            \"str1\": \"STR1\",\n                            \"num1\": \"123.0\",\n                            \"num2\": \"456.0\",\n                            \"tru1\": \"true\",\n                        },\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n            metadata=md2,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.put(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\", metadata=md2)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"PARTITIONID\", n=10)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? LIMIT ?;\",\n                    (\"PARTITIONID\", 10),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n                    (\"PARTITIONID\",),\n                ),\n            ]\n        )\n\n        cemvt.get_partition()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n                    (\"PRE-PART-ID\",),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\", partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\", [10, 11], 2),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PRE-PART-ID\", [10, 11], 2),\n                ),\n            ]\n        )\n\n        search_md = {\"mdks\": \"mdv\", \"mdkn\": 123, \"mdke\": True}\n        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.ann_search(\n            [100, 101], 9, a=\"MDA\", b=\"MDB\", partition_id=\"MDPART\", metadata=search_md\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"123.0\",\n                        \"mdv\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"MDPART\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"123.0\",\n                        \"mdv\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"PRE-PART-ID\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", \"PRE-PART-ID\"),\n                ),\n            ]\n        )\n\n        search_md_part = {\"mdke\": True, \"mdke2\": True}\n        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.ann_search(\n            [100, 101],\n            9,\n            a=\"MDA\",\n            b=\"MDB\",\n            partition_id=\"MDPART\",\n            metadata=search_md_part,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"true\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"MDPART\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"true\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"PRE-PART-ID\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", \"PRE-PART-ID\"),\n                ),\n            ]\n        )\n\n        cemvt.clear()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"TRUNCATE TABLE k.tn;\",\n                    tuple(),\n                ),\n            ]\n        )", "\n\nclass TestTableClassesCQLGeneration:\n    def test_vector_cassandra_table(self, mock_db_session):\n        vt = VectorCassandraTable(\n            mock_db_session, \"k\", \"tn\", vector_dimension=765, primary_key_type=\"UUID\"\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"CREATE TABLE IF NOT EXISTS k.tn (  row_id UUID,   body_blob TEXT,   vector VECTOR<FLOAT,765>, PRIMARY KEY ( ( row_id )   )) ;\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n            ]\n        )\n\n        vt.delete(row_id=\"ROWID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE row_id = ?;\",\n                    (\"ROWID\",),\n                ),\n            ]\n        )\n\n        vt.get(row_id=\"ROWID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE row_id = ? ;\",\n                    (\"ROWID\",),\n                ),\n            ]\n        )\n\n        vt.put(row_id=\"ROWID\", body_blob=\"BODYBLOB\", vector=\"VECTOR\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, row_id) VALUES (?, ?, ?)  ;\",\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        \"ROWID\",\n                    ),\n                ),\n            ]\n        )\n\n        vt.ann_search([10, 11], 2)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn ORDER BY vector ANN OF ?  LIMIT ?;\",\n                    (\n                        [10, 11],\n                        2,\n                    ),\n                ),\n            ]\n        )\n\n        vt.clear()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"TRUNCATE TABLE k.tn;\",\n                    tuple(),\n                ),\n            ]\n        )\n\n    def test_clustered_elastic_metadata_vector_cassandra_table(self, mock_db_session):\n        cemvt = ClusteredElasticMetadataVectorCassandraTable(\n            mock_db_session,\n            \"k\",\n            \"tn\",\n            keys=[\"a\", \"b\"],\n            vector_dimension=765,\n            primary_key_type=[\"PUUID\", \"AT\", \"BT\"],\n            ttl_seconds=123,\n            partition_id=\"PRE-PART-ID\",\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"CREATE TABLE IF NOT EXISTS k.tn (  partition_id PUUID,   key_desc TEXT,   key_vals TEXT,   body_blob TEXT,   vector VECTOR<FLOAT,765>, attributes_blob TEXT,  metadata_s MAP<TEXT,TEXT>, PRIMARY KEY ( ( partition_id ) , key_desc, key_vals )) WITH CLUSTERING ORDER BY (key_desc ASC, key_vals ASC);\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n                (\n                    \"CREATE CUSTOM INDEX IF NOT EXISTS eidx_metadata_s_tn ON k.tn (ENTRIES(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n                    tuple(),\n                ),\n            ]\n        )\n\n        cemvt.delete(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n                ),\n            ]\n        )\n\n        cemvt.get(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n                ),\n            ]\n        )\n\n        cemvt.delete_partition(partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"DELETE FROM k.tn WHERE partition_id = ?;\",\n                    (\"PARTITIONID\",),\n                ),\n            ]\n        )\n\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        md1 = {\"num1\": 123, \"num2\": 456, \"str1\": \"STR1\", \"tru1\": True}\n        md2 = {\"tru1\": True, \"tru2\": True}\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n            metadata=md1,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        {\n                            \"str1\": \"STR1\",\n                            \"num1\": \"123.0\",\n                            \"num2\": \"456.0\",\n                            \"tru1\": \"true\",\n                        },\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.put(\n            partition_id=\"PARTITIONID\",\n            a=\"A\",\n            b=\"B\",\n            body_blob=\"BODYBLOB\",\n            vector=\"VECTOR\",\n            metadata=md2,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        \"BODYBLOB\",\n                        \"VECTOR\",\n                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.put(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\", metadata=md2)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"INSERT INTO k.tn (metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n                    (\n                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n                        '[\"a\",\"b\"]',\n                        '[\"A\",\"B\"]',\n                        \"PARTITIONID\",\n                        123,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"PARTITIONID\", n=10)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? LIMIT ?;\",\n                    (\"PARTITIONID\", 10),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n                    (\"PARTITIONID\",),\n                ),\n            ]\n        )\n\n        cemvt.get_partition()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n                    (\"PRE-PART-ID\",),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\", partition_id=\"PARTITIONID\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\", [10, 11], 2),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\")\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PRE-PART-ID\", [10, 11], 2),\n                ),\n            ]\n        )\n\n        search_md = {\"mdks\": \"mdv\", \"mdkn\": 123, \"mdke\": True}\n        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.ann_search(\n            [100, 101], 9, a=\"MDA\", b=\"MDB\", partition_id=\"MDPART\", metadata=search_md\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"123.0\",\n                        \"mdv\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"MDPART\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"123.0\",\n                        \"mdv\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"PRE-PART-ID\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(metadata=search_md)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"123.0\", \"mdv\", \"PRE-PART-ID\"),\n                ),\n            ]\n        )\n\n        search_md_part = {\"mdke\": True, \"mdke2\": True}\n        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.ann_search(\n            [100, 101],\n            9,\n            a=\"MDA\",\n            b=\"MDB\",\n            partition_id=\"MDPART\",\n            metadata=search_md_part,\n        )\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"true\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"MDPART\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n                    (\n                        \"true\",\n                        \"true\",\n                        '[\"a\",\"b\"]',\n                        '[\"MDA\",\"MDB\"]',\n                        \"PRE-PART-ID\",\n                        [100, 101],\n                        9,\n                    ),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", \"MDPART\"),\n                ),\n            ]\n        )\n\n        cemvt.get_partition(metadata=search_md_part)\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n                    (\"true\", \"true\", \"PRE-PART-ID\"),\n                ),\n            ]\n        )\n\n        cemvt.clear()\n        mock_db_session.assert_last_equal(\n            [\n                (\n                    \"TRUNCATE TABLE k.tn;\",\n                    tuple(),\n                ),\n            ]\n        )", ""]}
{"filename": "tests/unit/test_metadata_string_coercion.py", "chunked_list": ["\"\"\"\nStringification of everything in the simple metadata handling\n\"\"\"\n\nfrom cassio.table.mixins import MetadataMixin\n\n\nclass TestNormalizeMetadataPolicy:\n    def test_normalize_metadata_policy(self):\n        md_mixin = MetadataMixin(\"s\", \"k\", \"t\", skip_provisioning=True)\n\n        stringified = md_mixin._split_metadata_fields(\n            {\n                \"integer\": 1,\n                \"float\": 2.0,\n                \"boolean\": True,\n                \"null\": None,\n                \"string\": \"letter E\",\n                \"something\": RuntimeError(\"You cannot do this!\"),\n            }\n        )\n\n        expected = {\n            \"integer\": \"1.0\",\n            \"float\": \"2.0\",\n            \"boolean\": \"true\",\n            \"null\": \"null\",\n            \"string\": \"letter E\",\n            \"something\": str(RuntimeError(\"You cannot do this!\")),\n        }\n\n        assert stringified == {\"metadata_s\": expected}", ""]}
{"filename": "tests/unit/test_imports.py", "chunked_list": ["\"\"\"\nJust importing everything to smoke-test python3.8+ syntax issues, etc.\nTODO: make this more robust in making sure all code is imported.\n\"\"\"\n\n\nclass TestImports:\n    def test_import_db_extractor(self):\n        from cassio.db_extractor import CassandraExtractor  # type: ignore  # noqa: F401\n\n    def test_import_history(self):\n        from cassio.history import StoredBlobHistory  # type: ignore  # noqa: F401\n\n    def test_import_keyvalue(self):\n        from cassio.keyvalue import KVCache  # type: ignore  # noqa: F401\n\n    def test_import_table(self):\n        from cassio.table.tables import PlainCassandraTable  # noqa: F401\n\n    def test_import_utils(self):\n        from cassio.utils import distance_metrics  # noqa: F401\n\n    def test_import_vector(self):\n        from cassio.vector import VectorTable  # noqa: F401", ""]}
{"filename": "src/cassio/__init__.py", "chunked_list": ["import cassio.vector\nimport cassio.keyvalue\nimport cassio.db_extractor\nimport cassio.history\n"]}
{"filename": "src/cassio/db_extractor/__init__.py", "chunked_list": ["from cassio.db_extractor.cassandra_extractor import CassandraExtractor\n"]}
{"filename": "src/cassio/db_extractor/cassandra_extractor.py", "chunked_list": ["\"\"\"\nAn extractor able to resolve single-row lookups from Cassandra tables in\na keyspace, with a fair amount of metadata inspection.\n\"\"\"\n\nfrom functools import reduce\nfrom typing import List\n\nfrom cassandra.query import SimpleStatement\n", "from cassandra.query import SimpleStatement\n\n\nRETRIEVE_ONE_ROW_CQL_TEMPLATE = 'SELECT * FROM {keyspace}.{table} WHERE {whereClause} LIMIT 1'\n\n\ndef _table_primary_key_columns(session, keyspace, table) -> List[str]:\n    table = session.cluster.metadata.keyspaces[keyspace].tables[table]\n    return [\n        col.name for col in table.partition_key\n    ] + [\n        col.name for col in table.clustering_key\n    ]", "\n\nclass CassandraExtractor:\n\n    def __init__(self, session, keyspace, field_mapper, literal_nones):\n        self.session = session\n        self.keyspace = keyspace\n        self.field_mapper = field_mapper\n        self.literal_nones = literal_nones  # TODO: handle much better\n        # derived fields\n        self.tables_needed = {fmv[0] for fmv in field_mapper.values()}\n        self.primary_key_map = {\n            table: _table_primary_key_columns(self.session, self.keyspace, table)\n            for table in self.tables_needed\n        }\n        # all primary-key values needed across tables\n        self.requiredParameters = list(reduce(lambda accum, nw: accum | set(nw), self.primary_key_map.values(), set()))\n\n        # TODOs:\n        #   move this getter creation someplace else\n        #   query a table only once (grouping required variables by source table,\n        #   selecting only those unless function passed)\n        def _getter(**kwargs):\n            def _retrieve_field(_table2, _key_columns, _column_or_extractor, _key_value_map):\n                selector = SimpleStatement(RETRIEVE_ONE_ROW_CQL_TEMPLATE.format(\n                    keyspace=keyspace,\n                    table=_table2,\n                    whereClause=' AND '.join(\n                        f'{kc} = %s'\n                        for kc in _key_columns\n                    ),\n                ))\n                values = tuple([\n                    _key_value_map[kc]\n                    for kc in _key_columns\n                ])\n                row = session.execute(selector, values).one()\n                if row:\n                    if callable(_column_or_extractor):\n                        return _column_or_extractor(row)\n                    else:\n                        return getattr(row, _column_or_extractor)\n                else:\n                    if literal_nones:\n                        return None\n                    else:\n                        raise ValueError('No data found for %s from %s.%s' % (\n                            str(_column_or_extractor),\n                            keyspace,\n                            _table2,\n                        ))\n\n            return {\n                field: _retrieve_field(table, self.primary_key_map[table], columnOrExtractor, kwargs)\n                for field, (table, columnOrExtractor) in field_mapper.items()\n            }\n\n        self.getter = _getter\n\n    def __call__(self, **kwargs):\n        return self.getter(**kwargs)", ""]}
{"filename": "src/cassio/table/mixins.py", "chunked_list": ["from operator import itemgetter\nimport json\n\nfrom typing import (\n    Any,\n    List,\n    Dict,\n    Iterable,\n    Optional,\n    Set,", "    Optional,\n    Set,\n    Tuple,\n    Union,\n)\n\nfrom cassandra.cluster import ResponseFuture  # type: ignore\n\nfrom cassio.utils.vector.distance_metrics import distance_metrics\n", "from cassio.utils.vector.distance_metrics import distance_metrics\n\nfrom cassio.table.cql import (\n    CQLOpType,\n    DELETE_CQL_TEMPLATE,\n    SELECT_CQL_TEMPLATE,\n    CREATE_INDEX_CQL_TEMPLATE,\n    # CREATE_KEYS_INDEX_CQL_TEMPLATE,\n    CREATE_ENTRIES_INDEX_CQL_TEMPLATE,\n    SELECT_ANN_CQL_TEMPLATE,", "    CREATE_ENTRIES_INDEX_CQL_TEMPLATE,\n    SELECT_ANN_CQL_TEMPLATE,\n)\nfrom cassio.table.table_types import (\n    ColumnSpecType,\n    RowType,\n    RowWithDistanceType,\n    normalize_type_desc,\n    rearrange_pk_type,\n    MetadataIndexingMode,", "    rearrange_pk_type,\n    MetadataIndexingMode,\n    MetadataIndexingPolicy,\n    is_metadata_field_indexed,\n)\nfrom cassio.table.base_table import BaseTable\n\n\nclass BaseTableMixin(BaseTable):\n    \"\"\"All other mixins should inherit from this one.\"\"\"\n\n    pass", "class BaseTableMixin(BaseTable):\n    \"\"\"All other mixins should inherit from this one.\"\"\"\n\n    pass\n\n\nclass ClusteredMixin(BaseTableMixin):\n    def __init__(\n        self,\n        *pargs: Any,\n        partition_id_type: Union[str, List[str]] = [\"TEXT\"],\n        partition_id: Optional[Any] = None,\n        ordering_in_partition: str = \"ASC\",\n        **kwargs: Any,\n    ) -> None:\n        self.partition_id_type = normalize_type_desc(partition_id_type)\n        self.partition_id = partition_id\n        self.ordering_in_partition = ordering_in_partition.upper()\n        super().__init__(*pargs, **kwargs)\n\n    def _schema_pk(self) -> List[ColumnSpecType]:\n        assert len(self.partition_id_type) == 1\n        return [\n            (\"partition_id\", self.partition_id_type[0]),\n        ]\n\n    def _schema_cc(self) -> List[ColumnSpecType]:\n        return self._schema_row_id()\n\n    def _extract_where_clause_blocks(\n        self, args_dict: Any\n    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n        \"\"\"\n        If a null partition_id arrives to WHERE construction, it is silently\n        discarded from the set of conditions to create.\n        This enables e.g. ANN vector search across partitions of a clustered table.\n\n        It is the database's responsibility to raise an error if unacceptable.\n        \"\"\"\n        if \"partition_id\" in args_dict and args_dict[\"partition_id\"] is None:\n            cleaned_args_dict = {\n                k: v for k, v in args_dict.items() if k != \"partition_id\"\n            }\n        else:\n            cleaned_args_dict = args_dict\n        #\n        return super()._extract_where_clause_blocks(cleaned_args_dict)\n\n    def _delete_partition(\n        self, is_async: bool, partition_id: Optional[str] = None\n    ) -> None:\n        _partition_id = self.partition_id if partition_id is None else partition_id\n        #\n        where_clause = \"WHERE \" + \"partition_id = %s\"\n        delete_cql_vals = (_partition_id,)\n        delete_cql = DELETE_CQL_TEMPLATE.format(\n            where_clause=where_clause,\n        )\n        if is_async:\n            return self.execute_cql_async(\n                delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE\n            )\n        else:\n            self.execute_cql(delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE)\n            return\n\n    def delete_partition(self, partition_id: Optional[str] = None) -> None:\n        self._delete_partition(is_async=False, partition_id=partition_id)\n        return None\n\n    def delete_partition_async(\n        self, partition_id: Optional[str] = None\n    ) -> ResponseFuture:\n        return self._delete_partition(is_async=True, partition_id=partition_id)\n\n    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n        # if partition id provided in call, takes precedence over instance value\n        arg_pid = args_dict.get(\"partition_id\")\n        instance_pid = self.partition_id\n        _partition_id = instance_pid if arg_pid is None else arg_pid\n        new_args_dict = {\n            **{\"partition_id\": _partition_id},\n            **args_dict,\n        }\n        return super()._normalize_kwargs(new_args_dict)\n\n    def get_partition(\n        self, partition_id: Optional[str] = None, n: Optional[int] = None, **kwargs: Any\n    ) -> Iterable[RowType]:\n        _partition_id = self.partition_id if partition_id is None else partition_id\n        get_p_cql_vals: Tuple[Any, ...] = tuple()\n        #\n        # TODO: work on a columns: Optional[List[str]] = None\n        # (but with nuanced handling of the column-magic we have here)\n        columns = None\n        if columns is None:\n            columns_desc = \"*\"\n        else:\n            # TODO: handle translations here?\n            # columns_desc = \", \".join(columns)\n            raise NotImplementedError(\"Column selection is not implemented.\")\n        # WHERE can admit other sources (e.g. medata if the corresponding mixin)\n        # so we escalate to standard WHERE-creation route and reinject the partition\n        n_kwargs = self._normalize_kwargs(\n            {\n                **{\"partition_id\": _partition_id},\n                **kwargs,\n            }\n        )\n        (args_dict, wc_blocks, wc_vals) = self._extract_where_clause_blocks(n_kwargs)\n        # check for exhaustion:\n        assert args_dict == {}\n        where_clause = \"WHERE \" + \" AND \".join(wc_blocks)\n        where_cql_vals = list(wc_vals)\n        #\n        if n is None:\n            limit_clause = \"\"\n            limit_cql_vals = []\n        else:\n            limit_clause = \"LIMIT %s\"\n            limit_cql_vals = [n]\n        #\n        select_cql = SELECT_CQL_TEMPLATE.format(\n            columns_desc=columns_desc,\n            where_clause=where_clause,\n            limit_clause=limit_clause,\n        )\n        get_p_cql_vals = tuple(where_cql_vals + limit_cql_vals)\n        return (\n            self._normalize_row(raw_row)\n            for raw_row in self.execute_cql(\n                select_cql,\n                args=get_p_cql_vals,\n                op_type=CQLOpType.READ,\n            )\n        )\n\n    def get_partition_async(\n        self, partition_id: Optional[str] = None, n: Optional[int] = None, **kwargs: Any\n    ) -> ResponseFuture:\n        raise NotImplementedError(\"Asynchronous reads are not supported.\")", "\n\nclass MetadataMixin(BaseTableMixin):\n    def __init__(\n        self,\n        *pargs: Any,\n        metadata_indexing: Union[Tuple[str, Iterable[str]], str] = \"all\",\n        **kwargs: Any,\n    ) -> None:\n        self.metadata_indexing_policy = self._normalize_metadata_indexing_policy(\n            metadata_indexing\n        )\n        super().__init__(*pargs, **kwargs)\n\n    @staticmethod\n    def _normalize_metadata_indexing_policy(\n        metadata_indexing: Union[Tuple[str, Iterable[str]], str]\n    ) -> MetadataIndexingPolicy:\n        mode: MetadataIndexingMode\n        fields: Set[str]\n        # metadata indexing policy normalization:\n        if isinstance(metadata_indexing, str):\n            if metadata_indexing.lower() == \"all\":\n                mode, fields = (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, set())\n            elif metadata_indexing.lower() == \"none\":\n                mode, fields = (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, set())\n            else:\n                raise ValueError(\n                    f\"Unsupported metadata_indexing value '{metadata_indexing}'\"\n                )\n        else:\n            assert len(metadata_indexing) == 2\n            # it's a 2-tuple (mode, fields) still to normalize\n            _mode, _field_spec = metadata_indexing\n            fields = {_field_spec} if isinstance(_field_spec, str) else set(_field_spec)\n            if _mode.lower() in {\n                \"default_to_unsearchable\",\n                \"allowlist\",\n                \"allow\",\n                \"allow_list\",\n            }:\n                mode = MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE\n            elif _mode.lower() in {\n                \"default_to_searchable\",\n                \"denylist\",\n                \"deny\",\n                \"deny_list\",\n            }:\n                mode = MetadataIndexingMode.DEFAULT_TO_SEARCHABLE\n            else:\n                raise ValueError(\n                    f\"Unsupported metadata indexing mode specification '{_mode}'\"\n                )\n        return (mode, fields)\n\n    def _schema_da(self) -> List[ColumnSpecType]:\n        return super()._schema_da() + [\n            (\"attributes_blob\", \"TEXT\"),\n            (\"metadata_s\", \"MAP<TEXT,TEXT>\"),\n        ]\n\n    def db_setup(self) -> None:\n        # Currently: an 'entries' index on the metadata_s column\n        super().db_setup()\n        #\n        entries_index_columns = [\"metadata_s\"]\n        for entries_index_column in entries_index_columns:\n            index_name = f\"eidx_{entries_index_column}\"\n            index_column = f\"{entries_index_column}\"\n            create_index_cql = CREATE_ENTRIES_INDEX_CQL_TEMPLATE.format(\n                index_name=index_name,\n                index_column=index_column,\n            )\n            self.execute_cql(create_index_cql, op_type=CQLOpType.SCHEMA)\n        #\n        return\n\n    @staticmethod\n    def _serialize_md_dict(md_dict: Dict[str, Any]) -> str:\n        return json.dumps(md_dict, separators=(\",\", \":\"), sort_keys=True)\n\n    @staticmethod\n    def _deserialize_md_dict(md_string: str) -> Dict[str, Any]:\n        return json.loads(md_string)\n\n    @staticmethod\n    def _coerce_string(value: Any) -> str:\n        if isinstance(value, str):\n            return value\n        elif isinstance(value, bool):\n            # bool MUST come before int in this chain of ifs!\n            return json.dumps(value)\n        elif isinstance(value, int):\n            # we don't want to store '1' and '1.0' differently\n            # for the sake of metadata-filtered retrieval:\n            return json.dumps(float(value))\n        elif isinstance(value, float):\n            return json.dumps(value)\n        elif value is None:\n            return json.dumps(value)\n        else:\n            # when all else fails ...\n            return str(value)\n\n    def _split_metadata_fields(self, md_dict: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Split the *indexed* part of the metadata in separate parts,\n        one per Cassandra column.\n\n        Currently: everything gets cast to a string and goes to a single table\n        column. This means:\n            - strings are fine\n            - floats and integers v: they are cast to str(v)\n            - booleans: 'true'/'false' (JSON style)\n            - None => 'null' (JSON style)\n            - anything else v => str(v), no questions asked\n\n        Caveat: one gets strings back when reading metadata\n        \"\"\"\n\n        # TODO: more care about types here\n        stringy_part = {k: self._coerce_string(v) for k, v in md_dict.items()}\n        return {\n            \"metadata_s\": stringy_part,\n        }\n\n    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n        md_columns_defaults: Dict[str, Any] = {\n            \"metadata_s\": {},\n        }\n        pre_normalized = super()._normalize_row(raw_row)\n        #\n        row_rest = {\n            k: v\n            for k, v in pre_normalized.items()\n            if k not in md_columns_defaults\n            if k != \"attributes_blob\"\n        }\n        mergee_md_fields = {\n            k: v for k, v in pre_normalized.items() if k in md_columns_defaults\n        }\n        normalized_mergee_md_fields = {\n            k: v if v is not None else md_columns_defaults[k]\n            for k, v in mergee_md_fields.items()\n        }\n        r_md_from_s = {\n            k: v for k, v in normalized_mergee_md_fields[\"metadata_s\"].items()\n        }\n        #\n        raw_attr_blob = pre_normalized.get(\"attributes_blob\")\n        if raw_attr_blob is not None:\n            r_attrs = self._deserialize_md_dict(raw_attr_blob)\n        else:\n            r_attrs = {}\n        #\n        row_metadata = {\n            \"metadata\": {\n                **r_attrs,\n                **r_md_from_s,\n            },\n        }\n        #\n        normalized = {\n            **row_metadata,\n            **row_rest,\n        }\n        return normalized\n\n    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n        _metadata_input_dict = args_dict.get(\"metadata\", {})\n        # separate indexed and non-indexed (=attributes) as per indexing policy\n        metadata_indexed_dict = {\n            k: v\n            for k, v in _metadata_input_dict.items()\n            if is_metadata_field_indexed(k, self.metadata_indexing_policy)\n        }\n        attributes_dict = {\n            k: self._coerce_string(v)\n            for k, v in _metadata_input_dict.items()\n            if not is_metadata_field_indexed(k, self.metadata_indexing_policy)\n        }\n        #\n        if attributes_dict != {}:\n            attributes_fields = {\n                \"attributes_blob\": self._serialize_md_dict(attributes_dict)\n            }\n        else:\n            attributes_fields = {}\n        #\n        new_metadata_fields = {\n            k: v\n            for k, v in self._split_metadata_fields(metadata_indexed_dict).items()\n            if v != {} and v != set()\n        }\n        #\n        new_args_dict = {\n            **{k: v for k, v in args_dict.items() if k != \"metadata\"},\n            **attributes_fields,\n            **new_metadata_fields,\n        }\n        return super()._normalize_kwargs(new_args_dict)\n\n    def _extract_where_clause_blocks(\n        self, args_dict: Any\n    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n        # This always happens after a corresponding _normalize_kwargs,\n        # so the metadata, if present, appears as split-fields.\n        assert \"metadata\" not in args_dict\n        if \"attributes_blob\" in args_dict:\n            raise ValueError(\"Non-indexed metadata fields cannot be used in queries.\")\n        md_keys = {\"metadata_s\"}\n        new_args_dict = {k: v for k, v in args_dict.items() if k not in md_keys}\n        # Here the \"metadata\" entry is made into specific where clauses\n        split_metadata = {k: v for k, v in args_dict.items() if k in md_keys}\n        these_wc_blocks: List[str] = []\n        these_wc_vals_list: List[Any] = []\n        # WHERE creation:\n        for k, v in sorted(split_metadata.get(\"metadata_s\", {}).items()):\n            these_wc_blocks.append(f\"metadata_s['{k}'] = %s\")\n            these_wc_vals_list.append(v)\n        # no new kwargs keys are created, all goes to WHERE\n        this_args_dict: Dict[str, Any] = {}\n        these_wc_vals = tuple(these_wc_vals_list)\n        # ready to defer to superclass(es), then collate-and-return\n        (s_args_dict, s_wc_blocks, s_wc_vals) = super()._extract_where_clause_blocks(\n            new_args_dict\n        )\n        return (\n            {**this_args_dict, **s_args_dict},\n            these_wc_blocks + s_wc_blocks,\n            tuple(list(these_wc_vals) + list(s_wc_vals)),\n        )\n\n    def find_entries(self, n: int, **kwargs: Any) -> Iterable[RowType]:\n        columns_desc, where_clause, get_cql_vals = self._parse_select_core_params(\n            **kwargs\n        )\n        limit_clause = \"LIMIT %s\"\n        limit_cql_vals = [n]\n        select_vals = tuple(list(get_cql_vals) + limit_cql_vals)\n        #\n        select_cql = SELECT_CQL_TEMPLATE.format(\n            columns_desc=columns_desc,\n            where_clause=where_clause,\n            limit_clause=limit_clause,\n        )\n        result_set = self.execute_cql(\n            select_cql, args=select_vals, op_type=CQLOpType.READ\n        )\n        return (self._normalize_row(result) for result in result_set)\n\n    def find_entries_async(self, n: int, **kwargs: Any) -> ResponseFuture:\n        raise NotImplementedError(\"Asynchronous reads are not supported.\")", "\n\nclass VectorMixin(BaseTableMixin):\n    def __init__(self, *pargs: Any, vector_dimension: int, **kwargs: Any) -> None:\n        self.vector_dimension = vector_dimension\n        super().__init__(*pargs, **kwargs)\n\n    def _schema_da(self) -> List[ColumnSpecType]:\n        return super()._schema_da() + [\n            (\"vector\", f\"VECTOR<FLOAT,{self.vector_dimension}>\")\n        ]\n\n    def db_setup(self) -> None:\n        super().db_setup()\n        # index on the vector column:\n        index_name = \"idx_vector\"\n        index_column = \"vector\"\n        create_index_cql = CREATE_INDEX_CQL_TEMPLATE.format(\n            index_name=index_name,\n            index_column=index_column,\n        )\n        self.execute_cql(create_index_cql, op_type=CQLOpType.SCHEMA)\n        return\n\n    def ann_search(\n        self, vector: List[float], n: int, **kwargs: Any\n    ) -> Iterable[RowType]:\n        n_kwargs = self._normalize_kwargs(kwargs)\n        # TODO: work on a columns: Optional[List[str]] = None\n        # (but with nuanced handling of the column-magic we have here)\n        columns = None\n        if columns is None:\n            columns_desc = \"*\"\n        else:\n            # TODO: handle translations here?\n            # columns_desc = \", \".join(columns)\n            raise NotImplementedError(\"Column selection is not implemented.\")\n        #\n        if all(x == 0 for x in vector):\n            # TODO: lift/relax this constraint when non-cosine metrics are there.\n            raise ValueError(\"Cannot use identically-zero vectors in cos/ANN search.\")\n        #\n        vector_column = \"vector\"\n        vector_cql_vals = [vector]\n        #\n        (\n            rest_kwargs,\n            where_clause_blocks,\n            where_cql_vals,\n        ) = self._extract_where_clause_blocks(n_kwargs)\n        assert rest_kwargs == {}\n        if where_clause_blocks == []:\n            where_clause = \"\"\n        else:\n            where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n        #\n        limit_clause = \"LIMIT %s\"\n        limit_cql_vals = [n]\n        #\n        select_ann_cql = SELECT_ANN_CQL_TEMPLATE.format(\n            columns_desc=columns_desc,\n            vector_column=vector_column,\n            where_clause=where_clause,\n            limit_clause=limit_clause,\n        )\n        #\n        select_ann_cql_vals = tuple(\n            list(where_cql_vals) + vector_cql_vals + limit_cql_vals\n        )\n        result_set = self.execute_cql(\n            select_ann_cql, args=select_ann_cql_vals, op_type=CQLOpType.READ\n        )\n        return (self._normalize_row(result) for result in result_set)\n\n    def ann_search_async(\n        self, vector: List[float], n: int, **kwargs: Any\n    ) -> ResponseFuture:\n        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\n    def metric_ann_search(\n        self,\n        vector: List[float],\n        n: int,\n        metric: str,\n        metric_threshold: Optional[float] = None,\n        **kwargs: Any,\n    ) -> Iterable[RowWithDistanceType]:\n        rows = list(self.ann_search(vector, n, **kwargs))\n        if rows == []:\n            return []\n        else:\n            # sort, cut, validate and prepare for returning\n            # evaluate metric\n            distance_function, distance_reversed = distance_metrics[metric]\n            row_vectors = [row[\"vector\"] for row in rows]\n            # enrich with their metric score\n            rows_with_metric = list(\n                zip(\n                    distance_function(row_vectors, vector),\n                    rows,\n                )\n            )\n            # sort rows by metric score. First handle metric/threshold\n            if metric_threshold is not None:\n                if distance_reversed:\n\n                    def _thresholder(mtx, thr):\n                        return mtx >= thr\n\n                else:\n\n                    def _thresholder(mtx, thr):\n                        return mtx <= thr\n\n            else:\n                # no hits are discarded\n                def _thresholder(mtx, thr):\n                    return True\n\n            #\n            sorted_passing_rows = sorted(\n                (\n                    pair\n                    for pair in rows_with_metric\n                    if _thresholder(pair[0], metric_threshold)\n                ),\n                key=itemgetter(0),\n                reverse=distance_reversed,\n            )\n            # return a list of hits with their distance (as JSON)\n            enriched_hits = (\n                {\n                    **hit,\n                    **{\"distance\": distance},\n                }\n                for distance, hit in sorted_passing_rows\n            )\n            return enriched_hits\n\n    def metric_ann_search_async(\n        self, vector: List[float], n: int, **kwargs: Any\n    ) -> ResponseFuture:\n        raise NotImplementedError(\"Asynchronous reads are not supported.\")", "\n\nclass ElasticKeyMixin(BaseTableMixin):\n    def __init__(self, *pargs: Any, keys: List[str], **kwargs: Any) -> None:\n        if \"row_id_type\" in kwargs:\n            raise ValueError(\"'row_id_type' not allowed for elastic tables.\")\n        self.keys = keys\n        self.key_desc = self._serialize_key_list(self.keys)\n        row_id_type = [\"TEXT\", \"TEXT\"]\n        new_kwargs = {\n            **{\"row_id_type\": row_id_type},\n            **kwargs,\n        }\n        super().__init__(*pargs, **new_kwargs)\n\n    @staticmethod\n    def _serialize_key_list(key_vals: List[Any]) -> str:\n        return json.dumps(key_vals, separators=(\",\", \":\"), sort_keys=True)\n\n    @staticmethod\n    def _deserialize_key_list(keys_str: str) -> List[Any]:\n        return json.loads(keys_str)\n\n    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n        key_fields = {\"key_desc\", \"key_vals\"}\n        pre_normalized = super()._normalize_row(raw_row)\n        row_key = {k: v for k, v in pre_normalized.items() if k in key_fields}\n        row_rest = {k: v for k, v in pre_normalized.items() if k not in key_fields}\n        if row_key == {}:\n            key_dict = {}\n        else:\n            # unpack the keys\n            assert len(row_key) == 2\n            assert self._deserialize_key_list(row_key[\"key_desc\"]) == self.keys\n            key_dict = {\n                k: v\n                for k, v in zip(\n                    self.keys,\n                    self._deserialize_key_list(row_key[\"key_vals\"]),\n                )\n            }\n        return {\n            **key_dict,\n            **row_rest,\n        }\n\n    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n        # transform provided \"keys\" into the elastic-representation two-val form\n        key_args = {k: v for k, v in args_dict.items() if k in self.keys}\n        # the \"key\" is passed all-or-nothing:\n        assert set(key_args.keys()) == set(self.keys) or key_args == {}\n        if key_args != {}:\n            key_vals = self._serialize_key_list(\n                [key_args[key_col] for key_col in self.keys]\n            )\n            #\n            key_args_dict = {\n                \"key_vals\": key_vals,\n                \"key_desc\": self.key_desc,\n            }\n            other_args_dict = {k: v for k, v in args_dict.items() if k not in self.keys}\n            new_args_dict = {\n                **key_args_dict,\n                **other_args_dict,\n            }\n        else:\n            new_args_dict = args_dict\n        return super()._normalize_kwargs(new_args_dict)\n\n    @staticmethod\n    def _schema_row_id() -> List[ColumnSpecType]:\n        return [\n            (\"key_desc\", \"TEXT\"),\n            (\"key_vals\", \"TEXT\"),\n        ]", "\n\nclass TypeNormalizerMixin(BaseTableMixin):\n\n    clustered: bool = False\n    elastic: bool = False\n\n    def __init__(self, *pargs: Any, **kwargs: Any) -> None:\n        if \"primary_key_type\" in kwargs:\n            pk_arg = kwargs[\"primary_key_type\"]\n            num_elastic_keys = len(kwargs[\"keys\"]) if self.elastic else None\n            col_type_map = rearrange_pk_type(pk_arg, self.clustered, num_elastic_keys)\n            new_kwargs = {\n                **col_type_map,\n                **{k: v for k, v in kwargs.items() if k != \"primary_key_type\"},\n            }\n        else:\n            new_kwargs = kwargs\n        super().__init__(*pargs, **new_kwargs)", ""]}
{"filename": "src/cassio/table/__init__.py", "chunked_list": ["from cassio.table.tables import PlainCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredMetadataCassandraTable  # noqa: F401\nfrom cassio.table.tables import MetadataCassandraTable  # noqa: F401\nfrom cassio.table.tables import VectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredVectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredMetadataVectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import MetadataVectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import ElasticCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredElasticCassandraTable  # noqa: F401", "from cassio.table.tables import ElasticCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredElasticCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredElasticMetadataCassandraTable  # noqa: F401\nfrom cassio.table.tables import ElasticMetadataCassandraTable  # noqa: F401\nfrom cassio.table.tables import ElasticVectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import ClusteredElasticVectorCassandraTable  # noqa: F401\nfrom cassio.table.tables import (\n    ClusteredElasticMetadataVectorCassandraTable,  # noqa: F401\n)\nfrom cassio.table.tables import ElasticMetadataVectorCassandraTable  # noqa: F401", ")\nfrom cassio.table.tables import ElasticMetadataVectorCassandraTable  # noqa: F401\n"]}
{"filename": "src/cassio/table/tables.py", "chunked_list": ["from cassio.table.base_table import BaseTable\nfrom cassio.table.mixins import (\n    ClusteredMixin,\n    MetadataMixin,\n    VectorMixin,\n    ElasticKeyMixin,\n    #\n    TypeNormalizerMixin,\n)\n", ")\n\n\nclass PlainCassandraTable(TypeNormalizerMixin, BaseTable):\n    pass\n\n\nclass ClusteredCassandraTable(TypeNormalizerMixin, ClusteredMixin, BaseTable):\n    clustered = True\n    pass", "\n\nclass ClusteredMetadataCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ClusteredMixin, BaseTable\n):\n    clustered = True\n    pass\n\n\nclass MetadataCassandraTable(TypeNormalizerMixin, MetadataMixin, BaseTable):\n    pass", "\nclass MetadataCassandraTable(TypeNormalizerMixin, MetadataMixin, BaseTable):\n    pass\n\n\nclass VectorCassandraTable(TypeNormalizerMixin, VectorMixin, BaseTable):\n    pass\n\n\nclass ClusteredVectorCassandraTable(\n    TypeNormalizerMixin, VectorMixin, ClusteredMixin, BaseTable\n):\n    clustered = True\n    pass", "\nclass ClusteredVectorCassandraTable(\n    TypeNormalizerMixin, VectorMixin, ClusteredMixin, BaseTable\n):\n    clustered = True\n    pass\n\n\nclass ClusteredMetadataVectorCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ClusteredMixin, VectorMixin, BaseTable\n):\n    clustered = True\n    pass", "class ClusteredMetadataVectorCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ClusteredMixin, VectorMixin, BaseTable\n):\n    clustered = True\n    pass\n\n\nclass MetadataVectorCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, VectorMixin, BaseTable\n):\n    pass", "\n\nclass ElasticCassandraTable(TypeNormalizerMixin, ElasticKeyMixin, BaseTable):\n    elastic = True\n    pass\n\n\nclass ClusteredElasticCassandraTable(\n    TypeNormalizerMixin, ClusteredMixin, ElasticKeyMixin, BaseTable\n):\n    clustered = True\n    elastic = True\n    pass", "\n\nclass ClusteredElasticMetadataCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ElasticKeyMixin, ClusteredMixin, BaseTable\n):\n    clustered = True\n    elastic = True\n    pass\n\n\nclass ElasticMetadataCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ElasticKeyMixin, BaseTable\n):\n    elastic = True\n    pass", "\n\nclass ElasticMetadataCassandraTable(\n    TypeNormalizerMixin, MetadataMixin, ElasticKeyMixin, BaseTable\n):\n    elastic = True\n    pass\n\n\nclass ElasticVectorCassandraTable(\n    TypeNormalizerMixin, VectorMixin, ElasticKeyMixin, BaseTable\n):\n    elastic = True\n    pass", "\nclass ElasticVectorCassandraTable(\n    TypeNormalizerMixin, VectorMixin, ElasticKeyMixin, BaseTable\n):\n    elastic = True\n    pass\n\n\nclass ClusteredElasticVectorCassandraTable(\n    TypeNormalizerMixin, ClusteredMixin, ElasticKeyMixin, VectorMixin, BaseTable\n):\n    clustered = True\n    elastic = True\n    pass", "class ClusteredElasticVectorCassandraTable(\n    TypeNormalizerMixin, ClusteredMixin, ElasticKeyMixin, VectorMixin, BaseTable\n):\n    clustered = True\n    elastic = True\n    pass\n\n\nclass ClusteredElasticMetadataVectorCassandraTable(\n    TypeNormalizerMixin,\n    MetadataMixin,\n    ElasticKeyMixin,\n    ClusteredMixin,\n    VectorMixin,\n    BaseTable,\n):\n    clustered = True\n    elastic = True\n    pass", "class ClusteredElasticMetadataVectorCassandraTable(\n    TypeNormalizerMixin,\n    MetadataMixin,\n    ElasticKeyMixin,\n    ClusteredMixin,\n    VectorMixin,\n    BaseTable,\n):\n    clustered = True\n    elastic = True\n    pass", "\n\nclass ElasticMetadataVectorCassandraTable(\n    MetadataMixin, ElasticKeyMixin, VectorMixin, BaseTable\n):\n    elastic = True\n    pass\n"]}
{"filename": "src/cassio/table/cql.py", "chunked_list": ["from typing import Union\nfrom enum import Enum\n\nfrom cassandra.query import SimpleStatement, PreparedStatement  # type: ignore\n\n\nclass CQLOpType(Enum):\n    SCHEMA = 1\n    WRITE = 2\n    READ = 3", "\n\nCREATE_TABLE_CQL_TEMPLATE = \"\"\"CREATE TABLE IF NOT EXISTS {{table_fqname}} ({columns_spec} PRIMARY KEY {primkey_spec}) {clustering_spec};\"\"\"  # noqa: E501\n\nTRUNCATE_TABLE_CQL_TEMPLATE = \"\"\"TRUNCATE TABLE {{table_fqname}};\"\"\"\n\nDELETE_CQL_TEMPLATE = \"\"\"DELETE FROM {{table_fqname}} {where_clause};\"\"\"\n\nSELECT_CQL_TEMPLATE = (\n    \"\"\"SELECT {columns_desc} FROM {{table_fqname}} {where_clause} {limit_clause};\"\"\"", "SELECT_CQL_TEMPLATE = (\n    \"\"\"SELECT {columns_desc} FROM {{table_fqname}} {where_clause} {limit_clause};\"\"\"\n)\n\nINSERT_ROW_CQL_TEMPLATE = \"\"\"INSERT INTO {{table_fqname}} ({columns_desc}) VALUES ({value_placeholders}) {ttl_spec} ;\"\"\"  # noqa: E501\n\nCREATE_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} ({index_column}) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\nCREATE_KEYS_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} (KEYS({index_column})) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n", "CREATE_KEYS_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} (KEYS({index_column})) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\nCREATE_ENTRIES_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} (ENTRIES({index_column})) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\nSELECT_ANN_CQL_TEMPLATE = \"\"\"SELECT {columns_desc} FROM {{table_fqname}} {where_clause} ORDER BY {vector_column} ANN OF %s {limit_clause};\"\"\"  # noqa: E501\n\nCQLStatementType = Union[str, SimpleStatement, PreparedStatement]\n\n# Mock DB session\nclass MockDBSession:\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        self.statements = []\n\n    @staticmethod\n    def getStatementBody(statement: CQLStatementType) -> str:\n        if isinstance(statement, str):\n            _statement = statement\n        elif isinstance(statement, SimpleStatement):\n            _statement = statement.query_string\n        elif isinstance(statement, PreparedStatement):\n            _statement = statement.query_string\n        else:\n            raise ValueError()\n        return _statement\n\n    @staticmethod\n    def normalizeCQLStatement(statement: CQLStatementType) -> str:\n        _statement = MockDBSession.getStatementBody(statement)\n        _s = (\n            _statement.replace(\";\", \" \")\n            .replace(\"%s\", \" %s \")\n            .replace(\"?\", \" ? \")\n            .replace(\"=\", \" = \")\n            .replace(\")\", \" ) \")\n            .replace(\"(\", \" ( \")\n            .replace(\"\\n\", \" \")\n        )\n        return \" \".join(\n            tok.lower() for tok in (_t.strip() for _t in _s.split(\" \") if _t.strip())\n        )\n\n    @staticmethod\n    def prepare(statement):\n        # A very unusable 'prepared statement' just for tracing/debugging:\n        return PreparedStatement(None, 0, 0, statement, \"keyspace\", None, None, None)\n\n    def execute(self, statement, arguments=tuple()):\n        if self.verbose:\n            #\n            st_body = self.getStatementBody(statement)\n            if isinstance(statement, str):\n                st_type = \"STR\"\n                placeholder_count = st_body.count(\"%s\")\n                assert \"?\" not in st_body\n            elif isinstance(statement, SimpleStatement):\n                st_type = \"SIM\"\n                placeholder_count = st_body.count(\"%s\")\n                assert \"?\" not in st_body\n            elif isinstance(statement, PreparedStatement):\n                st_type = \"PRE\"\n                placeholder_count = st_body.count(\"?\")\n                assert \"%s\" not in st_body\n            #\n            assert placeholder_count == len(arguments)\n            #\n            print(f\"CQL_EXECUTE [{st_type}]:\")\n            print(f\"    {st_body}\")\n            if arguments:\n                print(f\"    {str(arguments)}\")\n        self.statements.append((statement, arguments))\n        return []\n\n    def last_raw(self, n):\n        if n <= 0:\n            return []\n        else:\n            return self.statements[-n:]\n\n    def last(self, n):\n        return [\n            (\n                self.normalizeCQLStatement(stmt),\n                data,\n            )\n            for stmt, data in self.last_raw(n)\n        ]\n\n    def assert_last_equal(self, expected_statements):\n        # used for testing\n        last_executed = self.last(len(expected_statements))\n        assert len(last_executed) == len(expected_statements)\n        for s_exe, s_expe in zip(last_executed, expected_statements):\n            assert s_exe[1] == s_expe[1], f\"EXE#{str(s_exe[1])}# != EXPE#{s_expe[1]}#\"\n            exe_cql = self.normalizeCQLStatement(s_exe[0])\n            expe_cql = self.normalizeCQLStatement(s_expe[0])\n            assert exe_cql == expe_cql, f\"EXE#{exe_cql}# != EXPE#{expe_cql}#\"", "# Mock DB session\nclass MockDBSession:\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        self.statements = []\n\n    @staticmethod\n    def getStatementBody(statement: CQLStatementType) -> str:\n        if isinstance(statement, str):\n            _statement = statement\n        elif isinstance(statement, SimpleStatement):\n            _statement = statement.query_string\n        elif isinstance(statement, PreparedStatement):\n            _statement = statement.query_string\n        else:\n            raise ValueError()\n        return _statement\n\n    @staticmethod\n    def normalizeCQLStatement(statement: CQLStatementType) -> str:\n        _statement = MockDBSession.getStatementBody(statement)\n        _s = (\n            _statement.replace(\";\", \" \")\n            .replace(\"%s\", \" %s \")\n            .replace(\"?\", \" ? \")\n            .replace(\"=\", \" = \")\n            .replace(\")\", \" ) \")\n            .replace(\"(\", \" ( \")\n            .replace(\"\\n\", \" \")\n        )\n        return \" \".join(\n            tok.lower() for tok in (_t.strip() for _t in _s.split(\" \") if _t.strip())\n        )\n\n    @staticmethod\n    def prepare(statement):\n        # A very unusable 'prepared statement' just for tracing/debugging:\n        return PreparedStatement(None, 0, 0, statement, \"keyspace\", None, None, None)\n\n    def execute(self, statement, arguments=tuple()):\n        if self.verbose:\n            #\n            st_body = self.getStatementBody(statement)\n            if isinstance(statement, str):\n                st_type = \"STR\"\n                placeholder_count = st_body.count(\"%s\")\n                assert \"?\" not in st_body\n            elif isinstance(statement, SimpleStatement):\n                st_type = \"SIM\"\n                placeholder_count = st_body.count(\"%s\")\n                assert \"?\" not in st_body\n            elif isinstance(statement, PreparedStatement):\n                st_type = \"PRE\"\n                placeholder_count = st_body.count(\"?\")\n                assert \"%s\" not in st_body\n            #\n            assert placeholder_count == len(arguments)\n            #\n            print(f\"CQL_EXECUTE [{st_type}]:\")\n            print(f\"    {st_body}\")\n            if arguments:\n                print(f\"    {str(arguments)}\")\n        self.statements.append((statement, arguments))\n        return []\n\n    def last_raw(self, n):\n        if n <= 0:\n            return []\n        else:\n            return self.statements[-n:]\n\n    def last(self, n):\n        return [\n            (\n                self.normalizeCQLStatement(stmt),\n                data,\n            )\n            for stmt, data in self.last_raw(n)\n        ]\n\n    def assert_last_equal(self, expected_statements):\n        # used for testing\n        last_executed = self.last(len(expected_statements))\n        assert len(last_executed) == len(expected_statements)\n        for s_exe, s_expe in zip(last_executed, expected_statements):\n            assert s_exe[1] == s_expe[1], f\"EXE#{str(s_exe[1])}# != EXPE#{s_expe[1]}#\"\n            exe_cql = self.normalizeCQLStatement(s_exe[0])\n            expe_cql = self.normalizeCQLStatement(s_expe[0])\n            assert exe_cql == expe_cql, f\"EXE#{exe_cql}# != EXPE#{expe_cql}#\"", ""]}
{"filename": "src/cassio/table/table_types.py", "chunked_list": ["from enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\nColumnSpecType = Tuple[str, str]\nRowType = Dict[str, Any]\nRowWithDistanceType = Dict[str, Any]\nSessionType = Any\n\n\nclass MetadataIndexingMode(Enum):\n    DEFAULT_TO_UNSEARCHABLE = 1\n    DEFAULT_TO_SEARCHABLE = 2", "\nclass MetadataIndexingMode(Enum):\n    DEFAULT_TO_UNSEARCHABLE = 1\n    DEFAULT_TO_SEARCHABLE = 2\n\n\nMetadataIndexingPolicy = Tuple[MetadataIndexingMode, Set[str]]\n\n\ndef is_metadata_field_indexed(field_name: str, policy: MetadataIndexingPolicy) -> bool:\n    p_mode, p_fields = policy\n    if p_mode == MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE:\n        return field_name in p_fields\n    elif p_mode == MetadataIndexingMode.DEFAULT_TO_SEARCHABLE:\n        return field_name not in p_fields\n    else:\n        raise ValueError(f\"Unexpected metadata indexing mode {p_mode}\")", "\ndef is_metadata_field_indexed(field_name: str, policy: MetadataIndexingPolicy) -> bool:\n    p_mode, p_fields = policy\n    if p_mode == MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE:\n        return field_name in p_fields\n    elif p_mode == MetadataIndexingMode.DEFAULT_TO_SEARCHABLE:\n        return field_name not in p_fields\n    else:\n        raise ValueError(f\"Unexpected metadata indexing mode {p_mode}\")\n", "\n\ndef normalize_type_desc(type_desc: Union[str, List[str]]) -> List[str]:\n    if isinstance(type_desc, str):\n        return [type_desc]\n    else:\n        return type_desc\n\n\ndef rearrange_pk_type(\n    pk_type: Union[str, List[str]],\n    clustered: bool = False,\n    num_elastic_keys: Optional[int] = None,\n) -> Dict[str, List[str]]:\n    \"\"\"A compatibility layer with the 'primary_key_type' specifier on init.\"\"\"\n    _pk_type = normalize_type_desc(pk_type)\n    if clustered:\n        pk_type, rest_type = _pk_type[0:1], _pk_type[1:]\n        if num_elastic_keys:\n            assert len(rest_type) == num_elastic_keys\n            return {\n                \"partition_id_type\": pk_type,\n            }\n        else:\n            return {\n                \"row_id_type\": rest_type,\n                \"partition_id_type\": pk_type,\n            }\n    else:\n        if num_elastic_keys:\n            assert len(_pk_type) == num_elastic_keys\n            return {}\n        else:\n            return {\n                \"row_id_type\": _pk_type,\n            }", "\ndef rearrange_pk_type(\n    pk_type: Union[str, List[str]],\n    clustered: bool = False,\n    num_elastic_keys: Optional[int] = None,\n) -> Dict[str, List[str]]:\n    \"\"\"A compatibility layer with the 'primary_key_type' specifier on init.\"\"\"\n    _pk_type = normalize_type_desc(pk_type)\n    if clustered:\n        pk_type, rest_type = _pk_type[0:1], _pk_type[1:]\n        if num_elastic_keys:\n            assert len(rest_type) == num_elastic_keys\n            return {\n                \"partition_id_type\": pk_type,\n            }\n        else:\n            return {\n                \"row_id_type\": rest_type,\n                \"partition_id_type\": pk_type,\n            }\n    else:\n        if num_elastic_keys:\n            assert len(_pk_type) == num_elastic_keys\n            return {}\n        else:\n            return {\n                \"row_id_type\": _pk_type,\n            }", ""]}
{"filename": "src/cassio/table/base_table.py", "chunked_list": ["from typing import Any, List, Dict, Iterable, Optional, Set, Tuple, Union\n\nfrom cassandra.query import SimpleStatement, PreparedStatement  # type: ignore\nfrom cassandra.cluster import ResultSet  # type: ignore\nfrom cassandra.cluster import ResponseFuture  # type: ignore\n\nfrom cassio.table.table_types import (\n    ColumnSpecType,\n    RowType,\n    SessionType,", "    RowType,\n    SessionType,\n    normalize_type_desc,\n)\nfrom cassio.table.cql import (\n    CQLOpType,\n    CREATE_TABLE_CQL_TEMPLATE,\n    TRUNCATE_TABLE_CQL_TEMPLATE,\n    DELETE_CQL_TEMPLATE,\n    SELECT_CQL_TEMPLATE,", "    DELETE_CQL_TEMPLATE,\n    SELECT_CQL_TEMPLATE,\n    INSERT_ROW_CQL_TEMPLATE,\n)\n\n\nclass BaseTable:\n\n    ordering_in_partition: Optional[str] = None\n\n    def __init__(\n        self,\n        session: SessionType,\n        keyspace: str,\n        table: str,\n        ttl_seconds: Optional[int] = None,\n        row_id_type: Union[str, List[str]] = [\"TEXT\"],\n        skip_provisioning=False,\n    ) -> None:\n        self.session = session\n        self.keyspace = keyspace\n        self.table = table\n        self.ttl_seconds = ttl_seconds\n        self.row_id_type = normalize_type_desc(row_id_type)\n        self.skip_provisioning = skip_provisioning\n        self._prepared_statements: Dict[str, PreparedStatement] = {}\n        self.db_setup()\n\n    def _schema_row_id(self) -> List[ColumnSpecType]:\n        assert len(self.row_id_type) == 1\n        return [\n            (\"row_id\", self.row_id_type[0]),\n        ]\n\n    def _schema_pk(self) -> List[ColumnSpecType]:\n        return self._schema_row_id()\n\n    def _schema_cc(self) -> List[ColumnSpecType]:\n        return []\n\n    def _schema_da(self) -> List[ColumnSpecType]:\n        return [\n            (\"body_blob\", \"TEXT\"),\n        ]\n\n    def _schema(self) -> Dict[str, List[ColumnSpecType]]:\n        return {\n            \"pk\": self._schema_pk(),\n            \"cc\": self._schema_cc(),\n            \"da\": self._schema_da(),\n        }\n\n    def _schema_primary_key(self) -> List[ColumnSpecType]:\n        return self._schema_pk() + self._schema_cc()\n\n    def _schema_collist(self) -> List[ColumnSpecType]:\n        full_list = self._schema_da() + self._schema_cc() + self._schema_pk()\n        return full_list\n\n    def _schema_colnameset(self) -> Set[str]:\n        full_list = self._schema_collist()\n        full_set = set(col for col, _ in full_list)\n        assert len(full_list) == len(full_set)\n        return full_set\n\n    def _desc_table(self) -> str:\n        columns = self._schema()\n        col_str = (\n            \"[(\"\n            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"pk\"])\n            + \") \"\n            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"cc\"])\n            + \"] \"\n            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"da\"])\n        )\n        return col_str\n\n    def _extract_where_clause_blocks(\n        self, args_dict: Any\n    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n        # Removes some of the passed kwargs and returns the remaining,\n        # plus the pieces for a WHERE\n        _allowed_colspecs = self._schema_collist()\n        passed_columns = sorted(\n            [col for col, _ in _allowed_colspecs if col in args_dict]\n        )\n        residual_args = {k: v for k, v in args_dict.items() if k not in passed_columns}\n        where_clause_blocks = [f\"{col} = %s\" for col in passed_columns]\n        where_clause_vals = tuple([args_dict[col] for col in passed_columns])\n        return (\n            residual_args,\n            where_clause_blocks,\n            where_clause_vals,\n        )\n\n    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n        return args_dict\n\n    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n        if isinstance(raw_row, dict):\n            dict_row = raw_row\n        else:\n            dict_row = raw_row._asdict()\n        #\n        return dict_row\n\n    def _delete(self, is_async: bool, **kwargs: Any) -> Union[None, ResponseFuture]:\n        n_kwargs = self._normalize_kwargs(kwargs)\n        (\n            rest_kwargs,\n            where_clause_blocks,\n            delete_cql_vals,\n        ) = self._extract_where_clause_blocks(n_kwargs)\n        assert rest_kwargs == {}\n        where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n        delete_cql = DELETE_CQL_TEMPLATE.format(\n            where_clause=where_clause,\n        )\n        if is_async:\n            return self.execute_cql_async(\n                delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE\n            )\n        else:\n            self.execute_cql(delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE)\n            return None\n\n    def delete(self, **kwargs: Any) -> None:\n        self._delete(is_async=False, **kwargs)\n        return None\n\n    def delete_async(self, **kwargs: Any) -> ResponseFuture:\n        return self._delete(is_async=True, **kwargs)\n\n    def _clear(self, is_async: bool) -> Union[None, ResponseFuture]:\n        truncate_table_cql = TRUNCATE_TABLE_CQL_TEMPLATE.format()\n        if is_async:\n            return self.execute_cql_async(\n                truncate_table_cql, args=tuple(), op_type=CQLOpType.WRITE\n            )\n        else:\n            self.execute_cql(truncate_table_cql, args=tuple(), op_type=CQLOpType.WRITE)\n            return None\n\n    def clear(self) -> None:\n        self._clear(is_async=False)\n        return None\n\n    def clear_async(self) -> ResponseFuture:\n        return self._clear(is_async=True)\n\n    def _parse_select_core_params(\n        self, **kwargs: Any\n    ) -> Tuple[str, str, Tuple[Any, ...]]:\n        n_kwargs = self._normalize_kwargs(kwargs)\n        # TODO: work on a columns: Optional[List[str]] = None\n        # (but with nuanced handling of the column-magic we have here)\n        columns = None\n        if columns is None:\n            columns_desc = \"*\"\n        else:\n            # TODO: handle translations here?\n            # columns_desc = \", \".join(columns)\n            raise NotImplementedError(\"Column selection is not implemented.\")\n        #\n        (\n            rest_kwargs,\n            where_clause_blocks,\n            select_cql_vals,\n        ) = self._extract_where_clause_blocks(n_kwargs)\n        assert rest_kwargs == {}\n        where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n        return columns_desc, where_clause, select_cql_vals\n\n    def get(self, **kwargs: Any) -> Optional[RowType]:\n        columns_desc, where_clause, get_cql_vals = self._parse_select_core_params(\n            **kwargs\n        )\n        limit_clause = \"\"\n        limit_cql_vals: List[Any] = []\n        select_vals = tuple(list(get_cql_vals) + limit_cql_vals)\n        #\n        select_cql = SELECT_CQL_TEMPLATE.format(\n            columns_desc=columns_desc,\n            where_clause=where_clause,\n            limit_clause=limit_clause,\n        )\n        # dancing around the result set (to comply with type checking):\n        result_set = self.execute_cql(\n            select_cql, args=select_vals, op_type=CQLOpType.READ\n        )\n        if isinstance(result_set, ResultSet):\n            result = result_set.one()\n        else:\n            result = None\n        #\n        if result is None:\n            return result\n        else:\n            return self._normalize_row(result)\n\n    def get_async(self, **kwargs) -> ResponseFuture:\n        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\n    def _put(self, is_async: bool, **kwargs: Any) -> Union[None, ResponseFuture]:\n        n_kwargs = self._normalize_kwargs(kwargs)\n        primary_key = self._schema_primary_key()\n        assert set(col for col, _ in primary_key) - set(n_kwargs.keys()) == set()\n        columns = [col for col, _ in self._schema_collist() if col in n_kwargs]\n        columns_desc = \", \".join(columns)\n        insert_cql_vals = [n_kwargs[col] for col in columns]\n        value_placeholders = \", \".join(\"%s\" for _ in columns)\n        #\n        ttl_seconds = (\n            n_kwargs[\"ttl_seconds\"] if \"ttl_seconds\" in n_kwargs else self.ttl_seconds\n        )\n        if ttl_seconds is not None:\n            ttl_spec = \"USING TTL %s\"\n            ttl_vals = [ttl_seconds]\n        else:\n            ttl_spec = \"\"\n            ttl_vals = []\n        #\n        insert_cql_args = tuple(insert_cql_vals + ttl_vals)\n        insert_cql = INSERT_ROW_CQL_TEMPLATE.format(\n            columns_desc=columns_desc,\n            value_placeholders=value_placeholders,\n            ttl_spec=ttl_spec,\n        )\n        #\n        if is_async:\n            return self.execute_cql_async(\n                insert_cql, args=insert_cql_args, op_type=CQLOpType.WRITE\n            )\n        else:\n            self.execute_cql(insert_cql, args=insert_cql_args, op_type=CQLOpType.WRITE)\n            return None\n\n    def put(self, **kwargs: Any) -> None:\n        self._put(is_async=False, **kwargs)\n        return None\n\n    def put_async(self, **kwargs: Any) -> ResponseFuture:\n        return self._put(is_async=True, **kwargs)\n\n    def db_setup(self) -> None:\n        _schema = self._schema()\n        column_specs = [\n            f\"{col_spec[0]} {col_spec[1]}\"\n            for _schema_grp in [\"pk\", \"cc\", \"da\"]\n            for col_spec in _schema[_schema_grp]\n        ]\n        pk_spec = \", \".join(col for col, _ in _schema[\"pk\"])\n        cc_spec = \", \".join(col for col, _ in _schema[\"cc\"])\n        primkey_spec = f\"( ( {pk_spec} ) {',' if _schema['cc'] else ''} {cc_spec} )\"\n        if _schema[\"cc\"]:\n            clu_core = \", \".join(\n                f\"{col} {self.ordering_in_partition}\" for col, _ in _schema[\"cc\"]\n            )\n            clustering_spec = f\"WITH CLUSTERING ORDER BY ({clu_core})\"\n        else:\n            clustering_spec = \"\"\n        #\n        create_table_cql = CREATE_TABLE_CQL_TEMPLATE.format(\n            columns_spec=\" \".join(f\"  {cs},\" for cs in column_specs),\n            primkey_spec=primkey_spec,\n            clustering_spec=clustering_spec,\n        )\n        self.execute_cql(create_table_cql, op_type=CQLOpType.SCHEMA)\n\n    def _finalize_cql_semitemplate(self, cql_semitemplate: str) -> str:\n        table_fqname = f\"{self.keyspace}.{self.table}\"\n        table_name = self.table\n        final_cql = cql_semitemplate.format(\n            table_fqname=table_fqname, table_name=table_name\n        )\n        return final_cql\n\n    def _obtain_prepared_statement(self, final_cql) -> PreparedStatement:\n        # TODO: improve this placeholder handling\n        _preparable_cql = final_cql.replace(\"%s\", \"?\")\n        # handle the cache of prepared statements\n        if _preparable_cql not in self._prepared_statements:\n            self._prepared_statements[_preparable_cql] = self.session.prepare(\n                _preparable_cql\n            )\n        return self._prepared_statements[_preparable_cql]\n\n    def execute_cql(\n        self,\n        cql_semitemplate: str,\n        op_type: CQLOpType,\n        args: Tuple[Any, ...] = tuple(),\n    ) -> Iterable[RowType]:\n        final_cql = self._finalize_cql_semitemplate(cql_semitemplate)\n        #\n        if op_type == CQLOpType.SCHEMA and self.skip_provisioning:\n            # these operations are not executed for this instance:\n            return []\n        else:\n            if op_type == CQLOpType.SCHEMA:\n                # schema operations are not to be 'prepared'\n                statement = SimpleStatement(final_cql)\n            else:\n                statement = self._obtain_prepared_statement(final_cql)\n            #\n            return self.session.execute(statement, args)\n\n    def execute_cql_async(\n        self,\n        cql_semitemplate: str,\n        op_type: CQLOpType,\n        args: Tuple[Any, ...] = tuple(),\n    ) -> ResponseFuture:\n        final_cql = self._finalize_cql_semitemplate(cql_semitemplate)\n        #\n        if op_type == CQLOpType.SCHEMA:\n            raise RuntimeError(\"Schema operations cannot be asynchronous\")\n        else:\n            statement = self._obtain_prepared_statement(final_cql)\n            #\n            return self.session.execute_async(statement, args)", ""]}
{"filename": "src/cassio/utils/__init__.py", "chunked_list": ["from cassio.utils.vector import distance_metrics  # noqa: F401\n"]}
{"filename": "src/cassio/utils/vector/distance_metrics.py", "chunked_list": ["from typing import List, Dict, Tuple, Callable\n\nimport numpy as np\n\n\n# distance definitions. These all work batched in the first argument.\ndef distance_dot_product(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    \"\"\"\n    Given a list [emb_i] and a reference rEmb vector,\n    return a list [distance_i] where each distance is\n        distance_i = distance(emb_i, rEmb)\n    At the moment only the dot product is supported\n    (which for unitary vectors is the cosine difference).\n\n    Not particularly optimized.\n    \"\"\"\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(\n        np.dot(\n            v1s,\n            v2.T,\n        )\n    )", "\n\ndef distance_cos_difference(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(\n        np.dot(\n            v1s,\n            v2.T,\n        )\n        / (np.linalg.norm(v1s, axis=1) * np.linalg.norm(v2))\n    )", "\n\ndef distance_l1(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(np.linalg.norm(v1s - v2, axis=1, ord=1))\n\n\ndef distance_l2(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(np.linalg.norm(v1s - v2, axis=1, ord=2))", "\n\ndef distance_l2(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(np.linalg.norm(v1s - v2, axis=1, ord=2))\n\n\ndef distance_max(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(np.linalg.norm(v1s - v2, axis=1, ord=np.inf))", "\n\ndef distance_max(\n    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n) -> List[float]:\n    v1s = np.array(embedding_vectors, dtype=float)\n    v2 = np.array(reference_embedding_vector, dtype=float)\n    return list(np.linalg.norm(v1s - v2, axis=1, ord=np.inf))\n\n", "\n\n# The tuple is:\n#   (\n#       function,\n#       sorting 'reverse' argument, nearest-to-farthest\n#   )\n# (i.e. True means that:\n#     - in that metric higher is closer and that\n#     - cutoff should be metric > threshold)", "#     - in that metric higher is closer and that\n#     - cutoff should be metric > threshold)\ndistance_metrics: Dict[str, Tuple[Callable, bool]] = {\n    \"cos\": (\n        distance_cos_difference,\n        True,\n    ),\n    \"dot\": (\n        distance_dot_product,\n        True,", "        distance_dot_product,\n        True,\n    ),\n    \"l1\": (\n        distance_l1,\n        False,\n    ),\n    \"l2\": (\n        distance_l2,\n        False,", "        distance_l2,\n        False,\n    ),\n    \"max\": (\n        distance_max,\n        False,\n    ),\n}\n", ""]}
{"filename": "src/cassio/utils/vector/__init__.py", "chunked_list": ["from cassio.utils.vector.distance_metrics import distance_metrics  # noqa: F401\n"]}
{"filename": "src/cassio/keyvalue/k_v_cache.py", "chunked_list": ["\"\"\"\nhandling of key-value storage on a Cassandra table.\nOne row per partition, serializes a multiple partition key into a string\n\"\"\"\n\nfrom warnings import warn\nfrom typing import Any, Dict, List, Optional\n\nfrom cassandra.cluster import Session  # type: ignore\n", "from cassandra.cluster import Session  # type: ignore\n\nfrom cassio.table.tables import ElasticCassandraTable\n\n\nclass KVCache:\n    \"\"\"\n    This class is a rewriting of the KVCache created for use in LangChain\n    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\n    It mostly provides a translation layer between parameters and key names,\n    using a clustered table class internally.\n\n    Additional kwargs, for use in this new table class, are passed as they are\n    in order to enable their usage already before adapting the LangChain\n    integration code.\n    \"\"\"\n\n    DEPRECATION_MESSAGE = (\n        \"Class `KVCache` is a legacy construct and \"\n        \"will be deprecated in future versions of CassIO.\"\n    )\n\n    def __init__(self, session: Session, keyspace: str, table: str, keys: List[Any]):\n        #\n        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n        # for LangChain this is what we expect - no other uses are planned:\n        assert all(isinstance(k, str) for k in keys)\n        p_k_type = [\"TEXT\"] * len(keys)\n        #\n        self.table = ElasticCassandraTable(\n            session,\n            keyspace,\n            table,\n            keys=keys,\n            primary_key_type=p_k_type,\n        )\n\n    def clear(self) -> None:\n        self.table.clear()\n        return None\n\n    def put(\n        self,\n        key_dict: Dict[str, str],\n        cache_value: str,\n        ttl_seconds: Optional[int] = None,\n    ) -> None:\n        self.table.put(body_blob=cache_value, ttl_seconds=ttl_seconds, **key_dict)\n        return None\n\n    def get(self, key_dict) -> Optional[str]:\n        entry = self.table.get(**key_dict)\n        if entry is None:\n            return None\n        else:\n            return entry[\"body_blob\"]\n\n    def delete(self, key_dict) -> None:\n        \"\"\"Will not complain if the row does not exist.\"\"\"\n        self.table.delete(**key_dict)\n        return None", ""]}
{"filename": "src/cassio/keyvalue/__init__.py", "chunked_list": ["from cassio.keyvalue.k_v_cache import KVCache  # noqa: F401\n"]}
{"filename": "src/cassio/vector/vector_table.py", "chunked_list": ["\"\"\"\nCompatibility layer for legacy VectorTable (used by LangChain integration\n(as of August 2023).\n\nNote: This is to be replaced by direct usage of the table-class-hierarchy classes.\n\"\"\"\n\nfrom warnings import warn\nfrom typing import List, Dict, Any, Optional\n", "from typing import List, Dict, Any, Optional\n\nfrom cassandra.cluster import ResponseFuture  # type: ignore\n\nfrom cassio.table.table_types import RowType\nfrom cassio.table.tables import (\n    MetadataVectorCassandraTable,\n)\n\nnew_columns_to_legacy = {", "\nnew_columns_to_legacy = {\n    \"row_id\": \"document_id\",\n    \"body_blob\": \"document\",\n    \"vector\": \"embedding_vector\",\n}\nlegacy_columns_to_new = {v: k for k, v in new_columns_to_legacy.items()}\n\n\nclass VectorTable:\n    \"\"\"\n    This class is a rewriting of the VectorTable created for use in LangChain\n    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\n    It mostly provides a translation layer between parameters and key names,\n    using a metadata+vector table class internally.\n\n    Additional kwargs, for use in this new table class, are passed as they are\n    in order to enable their usage already before adapting the LangChain\n    integration code.\n    \"\"\"\n\n    DEPRECATION_MESSAGE = (\n        \"Class `VectorTable` is a legacy construct and \"\n        \"will be deprecated in future versions of CassIO.\"\n    )\n\n    def __init__(self, *pargs: Any, **kwargs: Dict[str, Any]):\n        #\n        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n        #\n        if \"embedding_dimension\" in kwargs:\n            vector_dimension = kwargs[\"embedding_dimension\"]\n            new_kwargs = {\n                **{\n                    k: v\n                    for k, v in kwargs.items()\n                    if k != \"embedding_dimension\"\n                    # let's get rid of the infamous 'auto_id' here:\n                    if k != \"auto_id\"\n                },\n                **{\"vector_dimension\": vector_dimension},\n            }\n        else:\n            new_kwargs = kwargs\n        # this legacy VectorTable will have everything indexed for search:\n        md_kwargs = {\n            **{\"metadata_indexing\": \"all\"},\n            **new_kwargs,\n        }\n        #\n        self.table = MetadataVectorCassandraTable(*pargs, **md_kwargs)\n\n    def search(\n        self,\n        embedding_vector: List[float],\n        top_k: int,\n        metric: str = \"cos\",\n        metric_threshold: Optional[float] = None,\n        **kwargs: Any,\n    ) -> List[RowType]:\n        # get rows by ANN\n        enriched_hits = self.table.metric_ann_search(\n            vector=embedding_vector,\n            n=top_k,\n            metric=metric,\n            metric_threshold=metric_threshold,\n            **kwargs,\n        )\n        #\n        return [self._make_dict_legacy(rich_hit) for rich_hit in enriched_hits]\n\n    def put(\n        self,\n        document: str,\n        embedding_vector: List[float],\n        document_id: Any,\n        metadata: Dict[str, Any] = {},\n        ttl_seconds: Optional[int] = None,\n        **kwargs: Any,\n    ) -> None:\n        self.table.put(\n            row_id=document_id,\n            body_blob=document,\n            vector=embedding_vector,\n            metadata=metadata or {},\n            ttl_seconds=ttl_seconds,\n            **kwargs,\n        )\n\n    def put_async(\n        self,\n        document: str,\n        embedding_vector: List[float],\n        document_id: Any,\n        metadata: Dict[str, Any],\n        ttl_seconds: int,\n        **kwargs: Any,\n    ) -> ResponseFuture:\n        return self.table.put_async(\n            row_id=document_id,\n            body_blob=document,\n            vector=embedding_vector,\n            metadata=metadata or {},\n            ttl_seconds=ttl_seconds,\n            **kwargs,\n        )\n\n    def get(self, document_id: Any, **kwargs: Any) -> Optional[RowType]:\n        row_or_none = self.table.get(row_id=document_id, **kwargs)\n        if row_or_none:\n            return self._make_dict_legacy(row_or_none)\n        else:\n            return row_or_none\n\n    def delete(self, document_id: Any, **kwargs: Any) -> None:\n        self.table.delete(row_id=document_id, **kwargs)\n        return None\n\n    def clear(self) -> None:\n        self.table.clear()\n        return None\n\n    @staticmethod\n    def _make_dict_legacy(new_dict: RowType) -> RowType:\n        return {new_columns_to_legacy.get(k, k): v for k, v in new_dict.items()}", "\nclass VectorTable:\n    \"\"\"\n    This class is a rewriting of the VectorTable created for use in LangChain\n    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\n    It mostly provides a translation layer between parameters and key names,\n    using a metadata+vector table class internally.\n\n    Additional kwargs, for use in this new table class, are passed as they are\n    in order to enable their usage already before adapting the LangChain\n    integration code.\n    \"\"\"\n\n    DEPRECATION_MESSAGE = (\n        \"Class `VectorTable` is a legacy construct and \"\n        \"will be deprecated in future versions of CassIO.\"\n    )\n\n    def __init__(self, *pargs: Any, **kwargs: Dict[str, Any]):\n        #\n        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n        #\n        if \"embedding_dimension\" in kwargs:\n            vector_dimension = kwargs[\"embedding_dimension\"]\n            new_kwargs = {\n                **{\n                    k: v\n                    for k, v in kwargs.items()\n                    if k != \"embedding_dimension\"\n                    # let's get rid of the infamous 'auto_id' here:\n                    if k != \"auto_id\"\n                },\n                **{\"vector_dimension\": vector_dimension},\n            }\n        else:\n            new_kwargs = kwargs\n        # this legacy VectorTable will have everything indexed for search:\n        md_kwargs = {\n            **{\"metadata_indexing\": \"all\"},\n            **new_kwargs,\n        }\n        #\n        self.table = MetadataVectorCassandraTable(*pargs, **md_kwargs)\n\n    def search(\n        self,\n        embedding_vector: List[float],\n        top_k: int,\n        metric: str = \"cos\",\n        metric_threshold: Optional[float] = None,\n        **kwargs: Any,\n    ) -> List[RowType]:\n        # get rows by ANN\n        enriched_hits = self.table.metric_ann_search(\n            vector=embedding_vector,\n            n=top_k,\n            metric=metric,\n            metric_threshold=metric_threshold,\n            **kwargs,\n        )\n        #\n        return [self._make_dict_legacy(rich_hit) for rich_hit in enriched_hits]\n\n    def put(\n        self,\n        document: str,\n        embedding_vector: List[float],\n        document_id: Any,\n        metadata: Dict[str, Any] = {},\n        ttl_seconds: Optional[int] = None,\n        **kwargs: Any,\n    ) -> None:\n        self.table.put(\n            row_id=document_id,\n            body_blob=document,\n            vector=embedding_vector,\n            metadata=metadata or {},\n            ttl_seconds=ttl_seconds,\n            **kwargs,\n        )\n\n    def put_async(\n        self,\n        document: str,\n        embedding_vector: List[float],\n        document_id: Any,\n        metadata: Dict[str, Any],\n        ttl_seconds: int,\n        **kwargs: Any,\n    ) -> ResponseFuture:\n        return self.table.put_async(\n            row_id=document_id,\n            body_blob=document,\n            vector=embedding_vector,\n            metadata=metadata or {},\n            ttl_seconds=ttl_seconds,\n            **kwargs,\n        )\n\n    def get(self, document_id: Any, **kwargs: Any) -> Optional[RowType]:\n        row_or_none = self.table.get(row_id=document_id, **kwargs)\n        if row_or_none:\n            return self._make_dict_legacy(row_or_none)\n        else:\n            return row_or_none\n\n    def delete(self, document_id: Any, **kwargs: Any) -> None:\n        self.table.delete(row_id=document_id, **kwargs)\n        return None\n\n    def clear(self) -> None:\n        self.table.clear()\n        return None\n\n    @staticmethod\n    def _make_dict_legacy(new_dict: RowType) -> RowType:\n        return {new_columns_to_legacy.get(k, k): v for k, v in new_dict.items()}", ""]}
{"filename": "src/cassio/vector/__init__.py", "chunked_list": ["from cassio.vector.vector_table import VectorTable  # noqa: F401\n"]}
{"filename": "src/cassio/history/stored_blob_history.py", "chunked_list": ["\"\"\"\nmanagement of \"history\" of stored blobs, grouped\nby some 'session id'. Overwrites are not supported by design.\n\"\"\"\n\nimport uuid\nfrom warnings import warn\nfrom typing import Any, Dict, Iterable, Optional\n\nfrom cassandra.cluster import Session  # type: ignore", "\nfrom cassandra.cluster import Session  # type: ignore\n\nfrom cassio.table.tables import ClusteredCassandraTable\n\n\nclass StoredBlobHistory:\n    \"\"\"\n    This class is a rewriting of the StoredBlobHistory created for use in LangChain\n    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\n    It mostly provides a translation layer between parameters and key names,\n    using a clustered table class internally.\n\n    Additional kwargs, for use in this new table class, are passed as they are\n    in order to enable their usage already before adapting the LangChain\n    integration code.\n    \"\"\"\n\n    DEPRECATION_MESSAGE = (\n        \"Class `StoredBlobHistory` is a legacy construct and \"\n        \"will be deprecated in future versions of CassIO.\"\n    )\n\n    def __init__(\n        self, session: Session, keyspace: str, table_name: str, **kwargs: Dict[str, Any]\n    ):\n        #\n        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n        # specifications are added such as the type of the row_id\n        full_kwargs = {\n            **kwargs,\n            **{\n                \"primary_key_type\": [\"TEXT\", \"TIMEUUID\"],\n                # latest entries are returned first\n                \"ordering_in_partition\": \"DESC\",\n            },\n        }\n        self.table = ClusteredCassandraTable(\n            session=session,\n            keyspace=keyspace,\n            table=table_name,\n            **full_kwargs,\n        )\n\n    def store(self, session_id: str, blob: str, ttl_seconds: Optional[int]):\n        this_row_id = uuid.uuid1()\n        self.table.put(\n            partition_id=session_id,\n            row_id=this_row_id,\n            body_blob=blob,\n            ttl_seconds=ttl_seconds,\n        )\n\n    def retrieve(\n        self, session_id: str, max_count: Optional[int] = None\n    ) -> Iterable[str]:\n        # The latest are returned, in chronological order\n        return [\n            row[\"body_blob\"]\n            for row in self.table.get_partition(\n                partition_id=session_id,\n                n=max_count,\n            )\n        ][::-1]\n\n    def clear_session_id(self, session_id: str) -> None:\n        self.table.delete_partition(session_id)\n        return None", ""]}
{"filename": "src/cassio/history/__init__.py", "chunked_list": ["from cassio.history.stored_blob_history import StoredBlobHistory  # noqa: F401\n"]}
