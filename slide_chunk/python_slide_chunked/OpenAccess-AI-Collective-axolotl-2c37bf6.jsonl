{"filename": "setup.py", "chunked_list": ["\"\"\"setup.py for axolotl\"\"\"\n\nfrom setuptools import find_packages, setup\n\ninstall_requires = []\nwith open(\"./requirements.txt\", encoding=\"utf-8\") as requirements_file:\n    # don't include peft yet until we check the int4\n    # need to manually install peft for now...\n    reqs = [r.strip() for r in requirements_file.readlines() if \"peft\" not in r]\n    reqs = [r for r in reqs if r and r[0] != \"#\"]\n    for r in reqs:\n        install_requires.append(r)", "\nsetup(\n    name=\"axolotl\",\n    version=\"0.1\",\n    description=\"You know you're going to axolotl questions\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(),\n    install_requires=install_requires,\n    extras_require={\n        \"gptq\": [", "    extras_require={\n        \"gptq\": [\n            \"alpaca_lora_4bit @ git+https://github.com/winglian/alpaca_lora_4bit.git@setup_pip\",\n        ],\n        \"gptq_triton\": [\n            \"alpaca_lora_4bit[triton] @ git+https://github.com/winglian/alpaca_lora_4bit.git@setup_pip\",\n        ],\n        \"extras\": [\n            \"flash-attn\",\n            \"deepspeed\",", "            \"flash-attn\",\n            \"deepspeed\",\n        ],\n    },\n)\n"]}
{"filename": "scripts/finetune.py", "chunked_list": ["\"\"\"Prepare and train a model on a dataset. Can also infer from a model or merge lora\"\"\"\n\nimport importlib\nimport logging\nimport os\nimport random\nimport signal\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union", "from pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport fire\nimport torch\nimport yaml\n\n# add src to the pythonpath so we don't need to pip install this\nfrom optimum.bettertransformer import BetterTransformer\nfrom transformers import GenerationConfig, TextStreamer", "from optimum.bettertransformer import BetterTransformer\nfrom transformers import GenerationConfig, TextStreamer\n\nfrom axolotl.logging_config import configure_logging\nfrom axolotl.utils.data import load_prepare_datasets, load_pretraining_dataset\nfrom axolotl.utils.dict import DictDefault\nfrom axolotl.utils.models import load_model, load_tokenizer\nfrom axolotl.utils.tokenization import check_dataset_labels\nfrom axolotl.utils.trainer import setup_trainer\nfrom axolotl.utils.validation import validate_config", "from axolotl.utils.trainer import setup_trainer\nfrom axolotl.utils.validation import validate_config\nfrom axolotl.utils.wandb import setup_wandb_env_vars\n\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nsrc_dir = os.path.join(project_root, \"src\")\nsys.path.insert(0, src_dir)\n\nconfigure_logging()\nLOG = logging.getLogger(\"axolotl.scripts\")", "configure_logging()\nLOG = logging.getLogger(\"axolotl.scripts\")\n\n\nDEFAULT_DATASET_PREPARED_PATH = \"last_run_prepared\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\n\ndef choose_device(cfg):\n    def get_device():\n        try:\n            if torch.cuda.is_available():\n                return f\"cuda:{cfg.local_rank}\"\n\n            if torch.backends.mps.is_available():\n                return \"mps\"\n\n            raise SystemError(\"No CUDA/mps device found\")\n        except Exception:  # pylint: disable=broad-exception-caught\n            return \"cpu\"\n\n    cfg.device = get_device()\n    if cfg.device_map != \"auto\":\n        if cfg.device.startswith(\"cuda\"):\n            cfg.device_map = {\"\": cfg.local_rank}\n        else:\n            cfg.device_map = {\"\": cfg.device}", "def choose_device(cfg):\n    def get_device():\n        try:\n            if torch.cuda.is_available():\n                return f\"cuda:{cfg.local_rank}\"\n\n            if torch.backends.mps.is_available():\n                return \"mps\"\n\n            raise SystemError(\"No CUDA/mps device found\")\n        except Exception:  # pylint: disable=broad-exception-caught\n            return \"cpu\"\n\n    cfg.device = get_device()\n    if cfg.device_map != \"auto\":\n        if cfg.device.startswith(\"cuda\"):\n            cfg.device_map = {\"\": cfg.local_rank}\n        else:\n            cfg.device_map = {\"\": cfg.device}", "\n\ndef get_multi_line_input() -> Optional[str]:\n    print(\"Give me an instruction (Ctrl + D to finish): \")\n    instruction = \"\"\n    for line in sys.stdin:\n        instruction += line  # pylint: disable=consider-using-join\n    # instruction = pathlib.Path(\"/proc/self/fd/0\").read_text()\n    return instruction\n", "\n\ndef do_inference(cfg, model, tokenizer, prompter: Optional[str]):\n    default_tokens = {\"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n\n    for token, symbol in default_tokens.items():\n        # If the token isn't already specified in the config, add it\n        if not (cfg.special_tokens and token in cfg.special_tokens):\n            tokenizer.add_special_tokens({token: symbol})\n\n    prompter_module = None\n    if prompter:\n        prompter_module = getattr(\n            importlib.import_module(\"axolotl.prompters\"), prompter\n        )\n\n    if cfg.landmark_attention:\n        from axolotl.monkeypatch.llama_landmark_attn import set_model_mem_id\n\n        set_model_mem_id(model, tokenizer)\n        model.set_mem_cache_args(\n            max_seq_len=255, mem_freq=50, top_k=5, max_cache_size=None\n        )\n\n    while True:\n        print(\"=\" * 80)\n        # support for multiline inputs\n        instruction = get_multi_line_input()\n        if not instruction:\n            return\n        if prompter_module:\n            prompt: str = next(\n                prompter_module().build_prompt(instruction=instruction.strip(\"\\n\"))\n            )\n        else:\n            prompt = instruction.strip()\n        batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n        print(\"=\" * 40)\n        model.eval()\n        with torch.no_grad():\n            generation_config = GenerationConfig(\n                repetition_penalty=1.1,\n                max_new_tokens=1024,\n                temperature=0.9,\n                top_p=0.95,\n                top_k=40,\n                bos_token_id=tokenizer.bos_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.pad_token_id,\n                do_sample=True,\n                use_cache=True,\n                return_dict_in_generate=True,\n                output_attentions=False,\n                output_hidden_states=False,\n                output_scores=False,\n            )\n            streamer = TextStreamer(tokenizer)\n            generated = model.generate(\n                inputs=batch[\"input_ids\"].to(cfg.device),\n                generation_config=generation_config,\n                streamer=streamer,\n            )\n        print(\"=\" * 40)\n        print(tokenizer.decode(generated[\"sequences\"].cpu().tolist()[0]))", "\n\ndef choose_config(path: Path):\n    yaml_files = list(path.glob(\"*.yml\"))\n\n    if not yaml_files:\n        raise ValueError(\n            \"No YAML config files found in the specified directory. Are you using a .yml extension?\"\n        )\n\n    print(\"Choose a YAML file:\")\n    for idx, file in enumerate(yaml_files):\n        print(f\"{idx + 1}. {file}\")\n\n    chosen_file = None\n    while chosen_file is None:\n        try:\n            choice = int(input(\"Enter the number of your choice: \"))\n            if 1 <= choice <= len(yaml_files):\n                chosen_file = yaml_files[choice - 1]\n            else:\n                print(\"Invalid choice. Please choose a number from the list.\")\n        except ValueError:\n            print(\"Invalid input. Please enter a number.\")\n\n    return chosen_file", "\n\ndef check_not_in(list1: List[str], list2: Union[Dict[str, Any], List[str]]) -> bool:\n    return not any(el in list2 for el in list1)\n\n\ndef train(\n    config: Path = Path(\"configs/\"),\n    prepare_ds_only: bool = False,\n    **kwargs,\n):\n    if Path(config).is_dir():\n        config = choose_config(config)\n\n    # load the config from the yaml file\n    with open(config, encoding=\"utf-8\") as file:\n        cfg: DictDefault = DictDefault(yaml.safe_load(file))\n    # if there are any options passed in the cli, if it is something that seems valid from the yaml,\n    # then overwrite the value\n    cfg_keys = cfg.keys()\n    for k, _ in kwargs.items():\n        # if not strict, allow writing to cfg even if it's not in the yml already\n        if k in cfg_keys or not cfg.strict:\n            # handle booleans\n            if isinstance(cfg[k], bool):\n                cfg[k] = bool(kwargs[k])\n            else:\n                cfg[k] = kwargs[k]\n\n    validate_config(cfg)\n\n    # setup some derived config / hyperparams\n    cfg.gradient_accumulation_steps = cfg.gradient_accumulation_steps or (\n        cfg.batch_size // cfg.micro_batch_size\n    )\n    cfg.batch_size = (\n        cfg.batch_size or cfg.micro_batch_size * cfg.gradient_accumulation_steps\n    )\n    cfg.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    cfg.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    choose_device(cfg)\n    cfg.ddp = cfg.ddp if cfg.ddp is not None else cfg.world_size != 1\n    if cfg.ddp:\n        cfg.device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\", 0))}\n        cfg.batch_size = cfg.batch_size * cfg.world_size\n\n    setup_wandb_env_vars(cfg)\n    if cfg.device == \"mps\":\n        cfg.load_in_8bit = False\n        cfg.tf32 = False\n        if cfg.bf16:\n            cfg.fp16 = True\n        cfg.bf16 = False\n\n    if cfg.tf32:\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    # load the tokenizer first\n    tokenizer_config = cfg.tokenizer_config or cfg.base_model_config\n    LOG.info(f\"loading tokenizer... {tokenizer_config}\")\n    tokenizer = load_tokenizer(tokenizer_config, cfg.tokenizer_type, cfg)\n\n    if (\n        check_not_in([\"shard\", \"merge_lora\"], kwargs) and not cfg.inference\n    ):  # don't need to load dataset for these\n        if not cfg.pretraining_dataset:\n            train_dataset, eval_dataset = load_prepare_datasets(\n                tokenizer, cfg, DEFAULT_DATASET_PREPARED_PATH\n            )\n        else:\n            train_dataset = load_pretraining_dataset(\n                cfg.pretraining_dataset,\n                tokenizer,\n                max_tokens=cfg.sequence_len,\n                seed=cfg.seed,\n            )\n            # https://discuss.huggingface.co/t/how-to-use-huggingface-trainer-streaming-datasets-without-wrapping-it-with-torchdatas-iterablewrapper/25230\n            train_dataset = train_dataset.with_format(\"torch\")\n            eval_dataset = None\n\n    if cfg.debug or \"debug\" in kwargs:\n        LOG.info(\"check_dataset_labels...\")\n        check_dataset_labels(\n            train_dataset.select(\n                [random.randrange(0, len(train_dataset) - 1) for _ in range(5)]  # nosec\n            ),\n            tokenizer,\n        )\n\n    if prepare_ds_only:\n        LOG.info(\"Finished preparing dataset. Exiting...\")\n        return\n\n    # Load the model and tokenizer\n    LOG.info(\"loading model and peft_config...\")\n    model, peft_config = load_model(\n        cfg.base_model,\n        cfg.base_model_config,\n        cfg.model_type,\n        tokenizer,\n        cfg,\n        adapter=cfg.adapter,\n    )\n\n    if \"merge_lora\" in kwargs and cfg.adapter is not None:\n        LOG.info(\"running merge of LoRA with base model\")\n        model = model.merge_and_unload()\n        model.to(dtype=torch.float16)\n\n        if cfg.local_rank == 0:\n            LOG.info(\"saving merged model\")\n            model.save_pretrained(str(Path(cfg.output_dir) / \"merged\"))\n        return\n\n    if cfg.inference:\n        LOG.info(\"calling do_inference function\")\n        prompter: Optional[str] = \"AlpacaPrompter\"\n        if \"prompter\" in kwargs:\n            if kwargs[\"prompter\"] == \"None\":\n                prompter = None\n            else:\n                prompter = kwargs[\"prompter\"]\n        do_inference(cfg, model, tokenizer, prompter=prompter)\n        return\n\n    if \"shard\" in kwargs:\n        model.save_pretrained(cfg.output_dir)\n        return\n\n    trainer = setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer)\n\n    model.config.use_cache = False\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        LOG.info(\"Compiling torch model\")\n        model = torch.compile(model)\n\n    # go ahead and presave, so we have the adapter config available to inspect\n    if peft_config:\n        LOG.info(f\"Pre-saving adapter config to {cfg.output_dir}\")\n        peft_config.save_pretrained(cfg.output_dir)\n\n    # In case we want to stop early with ctrl+c, this is a nice to have to save the pretrained model\n    if cfg.local_rank == 0:\n\n        def terminate_handler(_, __, model):\n            if cfg.flash_optimum:\n                model = BetterTransformer.reverse(model)\n            model.save_pretrained(cfg.output_dir)\n            sys.exit(0)\n\n        signal.signal(\n            signal.SIGINT, lambda signum, frame: terminate_handler(signum, frame, model)\n        )\n\n    LOG.info(\"Starting trainer...\")\n    if cfg.group_by_length:\n        LOG.info(\"hang tight... sorting dataset for group_by_length\")\n    resume_from_checkpoint = cfg.resume_from_checkpoint\n    if cfg.resume_from_checkpoint is None and cfg.auto_resume_from_checkpoints:\n        possible_checkpoints = [\n            str(cp) for cp in Path(cfg.output_dir).glob(\"checkpoint-*\")\n        ]\n        if len(possible_checkpoints) > 0:\n            sorted_paths = sorted(\n                possible_checkpoints,\n                key=lambda path: int(path.split(\"-\")[-1]),\n            )\n            resume_from_checkpoint = sorted_paths[-1]\n            LOG.info(\n                f\"Using Auto-resume functionality to start with checkpoint at {resume_from_checkpoint}\"\n            )\n\n    if not Path(cfg.output_dir).is_dir():\n        os.makedirs(cfg.output_dir, exist_ok=True)\n    if cfg.flash_optimum:\n        with torch.backends.cuda.sdp_kernel(\n            enable_flash=True, enable_math=True, enable_mem_efficient=True\n        ):\n            trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n    else:\n        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    LOG.info(f\"Training Completed!!! Saving pre-trained model to {cfg.output_dir}\")\n\n    # TODO do we need this fix? https://huggingface.co/docs/accelerate/usage_guides/fsdp#saving-and-loading\n    # only save on rank 0, otherwise it corrupts output on multi-GPU when multiple processes attempt to write the same file\n    if cfg.local_rank == 0:\n        if cfg.flash_optimum:\n            model = BetterTransformer.reverse(model)\n        model.save_pretrained(cfg.output_dir)", "\n    # trainer.save_model(cfg.output_dir)  # TODO this may be needed for deepspeed to work? need to review another time\n\n\nif __name__ == \"__main__\":\n    fire.Fire(train)\n"]}
{"filename": "scripts/alpaca_json_to_jsonl.py", "chunked_list": ["\"\"\"Module to convert json file to jsonl\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport fire\n\nfrom axolotl.convert import (", "\nfrom axolotl.convert import (\n    FileReader,\n    FileWriter,\n    JsonlSerializer,\n    JsonParser,\n    JsonToJsonlConverter,\n    StdoutWriter,\n)\nfrom axolotl.logging_config import configure_logging", ")\nfrom axolotl.logging_config import configure_logging\n\nconfigure_logging()\n\n# add src to the pythonpath so we don't need to pip install this\nproject_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nsrc_dir = os.path.join(project_root, \"src\")\nsys.path.insert(0, src_dir)\n", "sys.path.insert(0, src_dir)\n\n\ndef main(\n    file: Path,\n    output: Optional[Path] = None,\n    to_stdout: Optional[bool] = False,\n):\n    \"\"\"\n    Convert a json file to jsonl\n    \"\"\"\n\n    file_reader = FileReader()\n    writer: Union[StdoutWriter, FileWriter]\n    if to_stdout or output is None:\n        writer = StdoutWriter()\n    else:\n        writer = FileWriter(output)\n    json_parser = JsonParser()\n    jsonl_serializer = JsonlSerializer()\n\n    converter = JsonToJsonlConverter(file_reader, writer, json_parser, jsonl_serializer)\n\n    converter.convert(file, output)", "\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n"]}
{"filename": "tests/test_validation.py", "chunked_list": ["\"\"\"Module for testing the validation module\"\"\"\n\nimport logging\nimport unittest\nfrom typing import Optional\n\nimport pytest\n\nfrom axolotl.utils.dict import DictDefault\nfrom axolotl.utils.validation import validate_config", "from axolotl.utils.dict import DictDefault\nfrom axolotl.utils.validation import validate_config\n\n\nclass ValidationTest(unittest.TestCase):\n    \"\"\"\n    Test the validation module\n    \"\"\"\n\n    _caplog: Optional[pytest.LogCaptureFixture] = None\n\n    @pytest.fixture(autouse=True)\n    def inject_fixtures(self, caplog):\n        self._caplog = caplog\n\n    def test_load_4bit_deprecate(self):\n        cfg = DictDefault(\n            {\n                \"load_4bit\": True,\n            }\n        )\n\n        with pytest.raises(ValueError):\n            validate_config(cfg)\n\n    def test_batch_size_unused_warning(self):\n        cfg = DictDefault(\n            {\n                \"batch_size\": 32,\n            }\n        )\n\n        with self._caplog.at_level(logging.WARNING):\n            validate_config(cfg)\n            assert \"batch_size is not recommended\" in self._caplog.records[0].message\n\n    def test_qlora(self):\n        base_cfg = DictDefault(\n            {\n                \"adapter\": \"qlora\",\n            }\n        )\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"load_in_8bit\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*8bit.*\"):\n            validate_config(cfg)\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"gptq\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*gptq.*\"):\n            validate_config(cfg)\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"load_in_4bit\": False,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*4bit.*\"):\n            validate_config(cfg)\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"load_in_4bit\": True,\n            }\n        )\n\n        validate_config(cfg)\n\n    def test_qlora_merge(self):\n        base_cfg = DictDefault(\n            {\n                \"adapter\": \"qlora\",\n                \"merge_lora\": True,\n            }\n        )\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"load_in_8bit\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*8bit.*\"):\n            validate_config(cfg)\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"gptq\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*gptq.*\"):\n            validate_config(cfg)\n\n        cfg = base_cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\n                \"load_in_4bit\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*4bit.*\"):\n            validate_config(cfg)\n\n    def test_hf_use_auth_token(self):\n        cfg = DictDefault(\n            {\n                \"push_dataset_to_hub\": \"namespace/repo\",\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\".*hf_use_auth_token.*\"):\n            validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"push_dataset_to_hub\": \"namespace/repo\",\n                \"hf_use_auth_token\": True,\n            }\n        )\n        validate_config(cfg)\n\n    def test_gradient_accumulations_or_batch_size(self):\n        cfg = DictDefault(\n            {\n                \"gradient_accumulation_steps\": 1,\n                \"batch_size\": 1,\n            }\n        )\n\n        with pytest.raises(\n            ValueError, match=r\".*gradient_accumulation_steps or batch_size.*\"\n        ):\n            validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"batch_size\": 1,\n            }\n        )\n\n        validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"gradient_accumulation_steps\": 1,\n            }\n        )\n\n        validate_config(cfg)\n\n    def test_falcon_fsdp(self):\n        regex_exp = r\".*FSDP is not supported for falcon models.*\"\n\n        # Check for lower-case\n        cfg = DictDefault(\n            {\n                \"base_model\": \"tiiuae/falcon-7b\",\n                \"fsdp\": [\"full_shard\", \"auto_wrap\"],\n            }\n        )\n\n        with pytest.raises(ValueError, match=regex_exp):\n            validate_config(cfg)\n\n        # Check for upper-case\n        cfg = DictDefault(\n            {\n                \"base_model\": \"Falcon-7b\",\n                \"fsdp\": [\"full_shard\", \"auto_wrap\"],\n            }\n        )\n\n        with pytest.raises(ValueError, match=regex_exp):\n            validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"base_model\": \"tiiuae/falcon-7b\",\n            }\n        )\n\n        validate_config(cfg)\n\n    def test_mpt_gradient_checkpointing(self):\n        regex_exp = r\".*gradient_checkpointing is not supported for MPT models*\"\n\n        # Check for lower-case\n        cfg = DictDefault(\n            {\n                \"base_model\": \"mosaicml/mpt-7b\",\n                \"gradient_checkpointing\": True,\n            }\n        )\n\n        with pytest.raises(ValueError, match=regex_exp):\n            validate_config(cfg)\n\n    def test_flash_optimum(self):\n        cfg = DictDefault(\n            {\n                \"flash_optimum\": True,\n                \"adapter\": \"lora\",\n            }\n        )\n\n        with self._caplog.at_level(logging.WARNING):\n            validate_config(cfg)\n            assert any(\n                \"BetterTransformers probably doesn't work with PEFT adapters\"\n                in record.message\n                for record in self._caplog.records\n            )\n\n        cfg = DictDefault(\n            {\n                \"flash_optimum\": True,\n            }\n        )\n\n        with self._caplog.at_level(logging.WARNING):\n            validate_config(cfg)\n            assert any(\n                \"probably set bfloat16 or float16\" in record.message\n                for record in self._caplog.records\n            )\n\n        cfg = DictDefault(\n            {\n                \"flash_optimum\": True,\n                \"fp16\": True,\n            }\n        )\n        regex_exp = r\".*AMP is not supported.*\"\n\n        with pytest.raises(ValueError, match=regex_exp):\n            validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"flash_optimum\": True,\n                \"bf16\": True,\n            }\n        )\n        regex_exp = r\".*AMP is not supported.*\"\n\n        with pytest.raises(ValueError, match=regex_exp):\n            validate_config(cfg)\n\n    def test_adamw_hyperparams(self):\n        cfg = DictDefault(\n            {\n                \"optimizer\": None,\n                \"adam_epsilon\": 0.0001,\n            }\n        )\n\n        with self._caplog.at_level(logging.WARNING):\n            validate_config(cfg)\n            assert any(\n                \"adamw hyperparameters found, but no adamw optimizer set\"\n                in record.message\n                for record in self._caplog.records\n            )\n\n        cfg = DictDefault(\n            {\n                \"optimizer\": \"adafactor\",\n                \"adam_beta1\": 0.0001,\n            }\n        )\n\n        with self._caplog.at_level(logging.WARNING):\n            validate_config(cfg)\n            assert any(\n                \"adamw hyperparameters found, but no adamw optimizer set\"\n                in record.message\n                for record in self._caplog.records\n            )\n\n        cfg = DictDefault(\n            {\n                \"optimizer\": \"adamw_bnb_8bit\",\n                \"adam_beta1\": 0.9,\n                \"adam_beta2\": 0.99,\n                \"adam_epsilon\": 0.0001,\n            }\n        )\n\n        validate_config(cfg)\n\n        cfg = DictDefault(\n            {\n                \"optimizer\": \"adafactor\",\n            }\n        )\n\n        validate_config(cfg)", ""]}
{"filename": "tests/test_prompters.py", "chunked_list": ["\"\"\"Module testing prompters\"\"\"\n\nimport unittest\n\nfrom axolotl.prompt_strategies.alpaca_w_system import SystemDataPrompter\nfrom axolotl.prompters import (\n    AlpacaPrompter,\n    MultipleChoiceExplainPrompter,\n    PromptStyle,\n    UnpromptedPrompter,", "    PromptStyle,\n    UnpromptedPrompter,\n)\n\n\nclass AlpacaPrompterTest(unittest.TestCase):\n    \"\"\"\n    Test AlpacaPrompter\n    \"\"\"\n\n    def test_prompt_style_w_none(self):\n        prompter = AlpacaPrompter(prompt_style=None)\n        res = next(prompter.build_prompt(\"tell me a joke\"))\n        # just testing that it uses instruct style\n        assert \"### Instruction:\" in res\n\n    def test_prompt_style_w_instruct(self):\n        prompter = AlpacaPrompter(prompt_style=PromptStyle.INSTRUCT.value)\n        res = next(\n            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n        )\n        assert \"Below is an instruction\" in res\n        assert \"### Instruction:\" in res\n        assert \"### Input:\" in res\n        assert \"alpacas\" in res\n        assert \"### Response:\" in res\n        assert \"USER:\" not in res\n        assert \"ASSISTANT:\" not in res\n        res = next(prompter.build_prompt(\"tell me a joke about the following\"))\n        assert \"Below is an instruction\" in res\n        assert \"### Instruction:\" in res\n        assert \"### Input:\" not in res\n        assert \"### Response:\" in res\n        assert \"USER:\" not in res\n        assert \"ASSISTANT:\" not in res\n\n    def test_prompt_style_w_chat(self):\n        prompter = AlpacaPrompter(prompt_style=PromptStyle.CHAT.value)\n        res = next(\n            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n        )\n        assert \"Below is an instruction\" in res\n        assert \"### Instruction:\" not in res\n        assert \"### Input:\" not in res\n        assert \"alpacas\" in res\n        assert \"### Response:\" not in res\n        assert \"USER:\" in res\n        assert \"ASSISTANT:\" in res\n        res = next(prompter.build_prompt(\"tell me a joke about the following\"))\n        assert \"Below is an instruction\" in res\n        assert \"### Instruction:\" not in res\n        assert \"### Input:\" not in res\n        assert \"### Response:\" not in res\n        assert \"USER:\" in res\n        assert \"ASSISTANT:\" in res\n\n    def test_system_prompt(self):\n        prompter = SystemDataPrompter(prompt_style=PromptStyle.CHAT.value)\n        res = next(\n            prompter.build_prompt_w_system(\n                \"use cot\", \"tell me a joke about the following\", \"alpacas\"\n            )\n        )\n        assert \"use cot\" in res\n        assert res.startswith(\"### System:\")\n        assert \"### Instruction:\" not in res\n        assert \"### Input:\" not in res\n        assert \"alpacas\" in res\n        assert \"### Response:\" not in res\n        assert \"USER:\" in res\n        assert \"ASSISTANT:\" in res", "\n\nclass UnpromptedPrompterTest(unittest.TestCase):\n    \"\"\"\n    Test class for UnpromptedPrompter with no system prompts\n    \"\"\"\n\n    def test_prompt_style_w_none(self):\n        prompter = UnpromptedPrompter(prompt_style=None)\n        res = next(prompter.build_prompt(\"tell me a joke\"))\n        assert \"### Instruction:\" in res\n        assert \"tell me a joke\" in res\n        assert res.startswith(\"###\")\n\n    def test_prompt_style_w_instruct(self):\n        prompter = UnpromptedPrompter(prompt_style=PromptStyle.INSTRUCT.value)\n        res = next(\n            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n        )\n        assert \"### Instruction:\" in res\n        assert \"tell me a joke\" in res\n        assert res.startswith(\"###\")\n\n    def test_prompt_style_w_chat(self):\n        prompter = UnpromptedPrompter(prompt_style=PromptStyle.CHAT.value)\n        res = next(\n            prompter.build_prompt(\"tell me a joke about the following\", \"alpacas\")\n        )\n        assert \"USER:\" in res\n        assert \"tell me a joke\" in res\n        assert res.startswith(\"USER:\")", "\n\nclass MultipleChoiceExplainPrompterTest(unittest.TestCase):\n    \"\"\"\n    Test class for MultipleChoiceExplainPrompter\n    \"\"\"\n\n    def test_prompt_style_w_chat(self):\n        prompter = MultipleChoiceExplainPrompter(prompt_style=PromptStyle.CHAT.value)\n        res = next(prompter.build_prompt(\"choose one\", \"- A\\n- B\\n- C\", \"C\"))\n        assert \"USER:\" in res\n        assert \"choose one\" in res\n        assert \"Choose the answer that best answers the question.\" in res\n        assert \"- A\\n- B\\n- C\" in res", ""]}
{"filename": "tests/test_dict.py", "chunked_list": ["\"\"\"Module for testing DictDefault class\"\"\"\n\n\nimport unittest\n\nimport pytest\n\nfrom axolotl.utils.dict import DictDefault\n\n\nclass DictDefaultTest(unittest.TestCase):\n    \"\"\"\n    Test DictDefault class\n    \"\"\"\n\n    def test_dict_default(self):\n        cfg = DictDefault(\n            {\n                \"key_a\": {\"key_b\": \"value_a\"},\n                \"key_c\": \"value_c\",\n                \"key_d\": [\"value_d\", \"value_e\"],\n            }\n        )\n\n        assert (\n            cfg.key_a.key_b == \"value_a\"\n        ), \"DictDefault should return value for existing nested keys\"\n\n        assert (\n            cfg.key_c == \"value_c\"\n        ), \"DictDefault should return value for existing keys\"\n\n        assert (\n            cfg.key_d[0] == \"value_d\"\n        ), \"DictDefault should return value for existing keys in list\"\n\n        assert (\n            \"value_e\" in cfg.key_d\n        ), \"DictDefault should support in operator for existing keys in list\"\n\n    def test_dict_or_operator(self):\n        cfg = DictDefault(\n            {\n                \"key_a\": {\"key_b\": \"value_a\"},\n                \"key_c\": \"value_c\",\n                \"key_d\": [\"value_d\", \"value_e\"],\n                \"key_f\": \"value_f\",\n            }\n        )\n\n        cfg = cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\"key_a\": {\"key_b\": \"value_b\"}, \"key_f\": \"value_g\"}\n        )\n\n        assert (\n            cfg.key_a.key_b == \"value_b\"\n        ), \"DictDefault should support OR operator for existing nested keys\"\n\n        assert cfg.key_c == \"value_c\", \"DictDefault should not delete existing key\"\n\n        assert cfg.key_d == [\n            \"value_d\",\n            \"value_e\",\n        ], \"DictDefault should not overwrite existing keys in list\"\n\n        assert (\n            cfg.key_f == \"value_g\"\n        ), \"DictDefault should support OR operator for existing key\"\n\n    def test_dict_missingkey(self):\n        cfg = DictDefault({})\n\n        assert cfg.random_key is None, \"DictDefault should return None for missing keys\"\n\n    def test_dict_nested_missingparentkey(self):\n        \"\"\"\n        Due to subclassing Dict, DictDefault will error if we try to access a nested key whose parent key does not exist.\n        \"\"\"\n        cfg = DictDefault({})\n\n        with pytest.raises(\n            AttributeError,\n            match=r\"'NoneType' object has no attribute 'another_random_key'\",\n        ):\n            cfg.random_key.another_random_key = \"value\"\n\n    def test_dict_shorthand_assignment(self):\n        \"\"\"\n        Shorthand assignment is said to not be supported if subclassed. However, their example raises error instead of None.\n        This test ensures that it is supported for current implementation.\n\n        Ref: https://github.com/mewwts/addict#default-values\n        \"\"\"\n\n        cfg = DictDefault({\"key_a\": {\"key_b\": \"value_a\"}})\n\n        cfg.key_a.key_b = \"value_b\"\n\n        assert cfg.key_a.key_b == \"value_b\", \"Shorthand assignment should be supported\"", "\n\nclass DictDefaultTest(unittest.TestCase):\n    \"\"\"\n    Test DictDefault class\n    \"\"\"\n\n    def test_dict_default(self):\n        cfg = DictDefault(\n            {\n                \"key_a\": {\"key_b\": \"value_a\"},\n                \"key_c\": \"value_c\",\n                \"key_d\": [\"value_d\", \"value_e\"],\n            }\n        )\n\n        assert (\n            cfg.key_a.key_b == \"value_a\"\n        ), \"DictDefault should return value for existing nested keys\"\n\n        assert (\n            cfg.key_c == \"value_c\"\n        ), \"DictDefault should return value for existing keys\"\n\n        assert (\n            cfg.key_d[0] == \"value_d\"\n        ), \"DictDefault should return value for existing keys in list\"\n\n        assert (\n            \"value_e\" in cfg.key_d\n        ), \"DictDefault should support in operator for existing keys in list\"\n\n    def test_dict_or_operator(self):\n        cfg = DictDefault(\n            {\n                \"key_a\": {\"key_b\": \"value_a\"},\n                \"key_c\": \"value_c\",\n                \"key_d\": [\"value_d\", \"value_e\"],\n                \"key_f\": \"value_f\",\n            }\n        )\n\n        cfg = cfg | DictDefault(  # pylint: disable=unsupported-binary-operation\n            {\"key_a\": {\"key_b\": \"value_b\"}, \"key_f\": \"value_g\"}\n        )\n\n        assert (\n            cfg.key_a.key_b == \"value_b\"\n        ), \"DictDefault should support OR operator for existing nested keys\"\n\n        assert cfg.key_c == \"value_c\", \"DictDefault should not delete existing key\"\n\n        assert cfg.key_d == [\n            \"value_d\",\n            \"value_e\",\n        ], \"DictDefault should not overwrite existing keys in list\"\n\n        assert (\n            cfg.key_f == \"value_g\"\n        ), \"DictDefault should support OR operator for existing key\"\n\n    def test_dict_missingkey(self):\n        cfg = DictDefault({})\n\n        assert cfg.random_key is None, \"DictDefault should return None for missing keys\"\n\n    def test_dict_nested_missingparentkey(self):\n        \"\"\"\n        Due to subclassing Dict, DictDefault will error if we try to access a nested key whose parent key does not exist.\n        \"\"\"\n        cfg = DictDefault({})\n\n        with pytest.raises(\n            AttributeError,\n            match=r\"'NoneType' object has no attribute 'another_random_key'\",\n        ):\n            cfg.random_key.another_random_key = \"value\"\n\n    def test_dict_shorthand_assignment(self):\n        \"\"\"\n        Shorthand assignment is said to not be supported if subclassed. However, their example raises error instead of None.\n        This test ensures that it is supported for current implementation.\n\n        Ref: https://github.com/mewwts/addict#default-values\n        \"\"\"\n\n        cfg = DictDefault({\"key_a\": {\"key_b\": \"value_a\"}})\n\n        cfg.key_a.key_b = \"value_b\"\n\n        assert cfg.key_a.key_b == \"value_b\", \"Shorthand assignment should be supported\"", ""]}
{"filename": "tests/test_prompt_tokenizers.py", "chunked_list": ["\"\"\"Module for testing prompt tokenizers.\"\"\"\nimport json\nimport logging\nimport unittest\nfrom pathlib import Path\n\nfrom transformers import AutoTokenizer\n\nfrom axolotl.prompt_strategies.alpaca_chat import NoSystemPrompter\nfrom axolotl.prompt_strategies.alpaca_w_system import (", "from axolotl.prompt_strategies.alpaca_chat import NoSystemPrompter\nfrom axolotl.prompt_strategies.alpaca_w_system import (\n    InstructionWSystemPromptTokenizingStrategy,\n    SystemDataPrompter,\n)\nfrom axolotl.prompt_tokenizers import (\n    AlpacaPromptTokenizingStrategy,\n    ShareGPTPromptTokenizingStrategy,\n)\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle, ShareGPTPrompter", ")\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle, ShareGPTPrompter\n\nLOG = logging.getLogger(\"axolotl\")\n\n\nclass TestPromptTokenizationStrategies(unittest.TestCase):\n    \"\"\"\n    Test class for prompt tokenization strategies.\n    \"\"\"\n\n    def setUp(self) -> None:\n        # pylint: disable=duplicate-code\n        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n        self.tokenizer.add_special_tokens(\n            {\n                \"bos_token\": \"<s>\",\n                \"eos_token\": \"</s>\",\n                \"unk_token\": \"<unk>\",\n            }\n        )\n\n    def test_sharegpt_integration(self):\n        with open(\n            Path(__file__).parent / \"fixtures/conversation.json\", encoding=\"utf-8\"\n        ) as fin:\n            data = fin.read()\n            conversation = json.loads(data)\n        with open(\n            Path(__file__).parent / \"fixtures/conversation.tokenized.json\",\n            encoding=\"utf-8\",\n        ) as fin:\n            data = fin.read()\n            tokenized_conversation = json.loads(data)\n        prompter = ShareGPTPrompter(\"chat\")\n        strat = ShareGPTPromptTokenizingStrategy(\n            prompter,\n            self.tokenizer,\n            False,\n            2048,\n        )\n        example = strat.tokenize_prompt(conversation)\n        for fields in [\"input_ids\", \"attention_mask\", \"labels\"]:\n            self.assertEqual(len(example[fields]), len(tokenized_conversation[fields]))\n            self.assertEqual(example[fields], tokenized_conversation[fields])\n\n    def test_no_sys_prompt(self):\n        \"\"\"\n        tests the interface between the user and assistant parts\n        \"\"\"\n        prompter = NoSystemPrompter()\n        # pylint: disable=duplicate-code\n        strat = AlpacaPromptTokenizingStrategy(\n            prompter,\n            self.tokenizer,\n            False,\n            2048,\n        )\n        sample = {\n            \"instruction\": \"hello cruel. lorem ipsum dolor sit amet.\",\n            \"output\": \"world!\",\n        }\n        example = strat.tokenize_prompt(sample)\n        world_idx = example[\"input_ids\"].index(3186)\n        assert example[\"labels\"][world_idx] == 3186\n        assert example[\"labels\"][world_idx - 1] == -100\n\n    def test_alpaca(self):\n        \"\"\"\n        tests the interface between the user and assistant parts\n        \"\"\"\n        # pylint: disable=duplicate-code\n        prompter = AlpacaPrompter()\n        strat = AlpacaPromptTokenizingStrategy(\n            prompter,\n            self.tokenizer,\n            False,\n            2048,\n        )\n        sample = {\"instruction\": \"hello!\", \"output\": \"Hi! How can I help?\"}\n        example = strat.tokenize_prompt(sample)\n        world_idx = example[\"input_ids\"].index(6324)\n        assert example[\"labels\"][world_idx] == 6324\n        assert example[\"labels\"][world_idx - 1] == -100", "\n\nclass InstructionWSystemPromptTokenizingStrategyTest(unittest.TestCase):\n    \"\"\"\n    Test class for prompt tokenization strategies with sys prompt from the dataset\n    \"\"\"\n\n    def setUp(self) -> None:\n        # pylint: disable=duplicate-code\n        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n        self.tokenizer.add_special_tokens(\n            {\n                \"bos_token\": \"<s>\",\n                \"eos_token\": \"</s>\",\n                \"unk_token\": \"<unk>\",\n            }\n        )\n\n    def test_system_alpaca(self):\n        prompter = SystemDataPrompter(PromptStyle.CHAT.value)\n        strat = InstructionWSystemPromptTokenizingStrategy(\n            prompter,\n            self.tokenizer,\n            False,\n            2048,\n        )\n        sample = {\n            \"system\": \"use cot\",\n            \"instruction\": \"hello!\",\n            \"output\": \"Hi! How can I help?\",\n        }\n        example = strat.tokenize_prompt(sample)\n        assert example[\"input_ids\"][0:4] == [1, 835, 2184, 29901]  # \"<s>### System:\"\n        assert example[\"input_ids\"][5:7] == [1509, 20118]  # \"use cot\"\n        assert example[\"input_ids\"][9] == 11889  # USER", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "tests/test_packed_dataset.py", "chunked_list": ["\"\"\"Module for testing dataset sequence packing\"\"\"\n\nimport unittest\nfrom pathlib import Path\n\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer\n\nfrom axolotl.datasets import ConstantLengthDataset, TokenizedPromptDataset\nfrom axolotl.prompt_tokenizers import AlpacaPromptTokenizingStrategy", "from axolotl.datasets import ConstantLengthDataset, TokenizedPromptDataset\nfrom axolotl.prompt_tokenizers import AlpacaPromptTokenizingStrategy\nfrom axolotl.prompters import AlpacaPrompter\n\n\nclass TestPacking(unittest.TestCase):\n    \"\"\"\n    Test class for packing dataset sequences\n    \"\"\"\n\n    def setUp(self) -> None:\n        # pylint: disable=duplicate-code\n        self.tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n        self.tokenizer.add_special_tokens(\n            {\n                \"bos_token\": \"<s>\",\n                \"eos_token\": \"</s>\",\n                \"unk_token\": \"<unk>\",\n            }\n        )\n\n    def test_resets_attention(self):\n        prompter = AlpacaPrompter(\"chat\")\n        strat = AlpacaPromptTokenizingStrategy(\n            prompter,\n            self.tokenizer,\n            False,\n            2048,\n        )\n        dateset = load_dataset(\n            \"json\",\n            data_files=str(Path(__file__).parent / \"fixtures/alpaca/alpaca.json\"),\n        )[\"train\"]\n        dataset = Dataset.from_list(list(TokenizedPromptDataset(strat, dateset)))\n\n        constant_len_dataset = ConstantLengthDataset(\n            self.tokenizer,\n            [dataset],\n            seq_length=2048,\n        )\n        packed_dataset = Dataset.from_list(list(constant_len_dataset))\n        example = packed_dataset[0]\n        next_bos_index = (\n            example[\"input_ids\"][1:].index(self.tokenizer.bos_token_id) + 1\n        )  # add one since we sliced\n\n        # first example doesn't have mask reset\n        assert example[\"input_ids\"][0] == self.tokenizer.bos_token_id\n        assert example[\"attention_mask\"][0] == 1\n\n        # but subsequent one does\n        assert example[\"input_ids\"][next_bos_index] == self.tokenizer.bos_token_id\n        assert example[\"attention_mask\"][next_bos_index] == 0", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "tests/test_tokenizers.py", "chunked_list": ["\"\"\"\nTest cases for the tokenizer loading\n\"\"\"\nimport unittest\n\nfrom axolotl.utils.dict import DictDefault\nfrom axolotl.utils.models import load_tokenizer\n\n\nclass TestTokenizers(unittest.TestCase):\n    \"\"\"\n    test class for the load_tokenizer fn\n    \"\"\"\n\n    def test_default_use_fast(self):\n        cfg = DictDefault({})\n        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n        assert \"Fast\" in tokenizer.__class__.__name__\n\n    def test_dont_use_fast(self):\n        cfg = DictDefault(\n            {\n                \"tokenizer_use_fast\": False,\n            }\n        )\n        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n        assert \"Fast\" not in tokenizer.__class__.__name__", "\nclass TestTokenizers(unittest.TestCase):\n    \"\"\"\n    test class for the load_tokenizer fn\n    \"\"\"\n\n    def test_default_use_fast(self):\n        cfg = DictDefault({})\n        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n        assert \"Fast\" in tokenizer.__class__.__name__\n\n    def test_dont_use_fast(self):\n        cfg = DictDefault(\n            {\n                \"tokenizer_use_fast\": False,\n            }\n        )\n        tokenizer = load_tokenizer(\"huggyllama/llama-7b\", None, cfg)\n        assert \"Fast\" not in tokenizer.__class__.__name__", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "src/axolotl/logging_config.py", "chunked_list": ["\"\"\"Logging configuration settings\"\"\"\n\nimport os\nimport sys\nfrom logging.config import dictConfig\nfrom typing import Any, Dict\n\nDEFAULT_LOGGING_CONFIG: Dict[str, Any] = {\n    \"version\": 1,\n    \"formatters\": {", "    \"version\": 1,\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"[%(asctime)s] [%(levelname)s] [%(name)s.%(funcName)s:%(lineno)d] [PID:%(process)d] %(message)s\",\n        },\n    },\n    \"filters\": {},\n    \"handlers\": {\n        \"console\": {\n            \"class\": \"logging.StreamHandler\",", "        \"console\": {\n            \"class\": \"logging.StreamHandler\",\n            \"formatter\": \"simple\",\n            \"filters\": [],\n            \"stream\": sys.stdout,\n        },\n    },\n    \"root\": {\"handlers\": [\"console\"], \"level\": os.getenv(\"LOG_LEVEL\", \"INFO\")},\n    \"loggers\": {\n        \"axolotl\": {\"handlers\": [\"console\"], \"level\": \"DEBUG\", \"propagate\": False},", "    \"loggers\": {\n        \"axolotl\": {\"handlers\": [\"console\"], \"level\": \"DEBUG\", \"propagate\": False},\n    },\n}\n\n\ndef configure_logging():\n    \"\"\"Configure with default logging\"\"\"\n    dictConfig(DEFAULT_LOGGING_CONFIG)\n", ""]}
{"filename": "src/axolotl/prompters.py", "chunked_list": ["\"\"\"Module containing prompters\"\"\"\n\nimport dataclasses\nimport logging\nfrom enum import Enum, auto\nfrom typing import Generator, List, Optional, Tuple, Union\n\nLOG = logging.getLogger(\"axolotl\")\nIGNORE_TOKEN_ID = -100\n", "IGNORE_TOKEN_ID = -100\n\n\nclass PromptStyle(Enum):\n    \"\"\"\n    Enum for prompt styles\n    \"\"\"\n\n    INSTRUCT = \"instruct\"\n    CHAT = \"chat\"", "\n\nclass AlpacaPrompter:\n    \"\"\"\n    Base class for alpaca prompters\n    \"\"\"\n\n    system_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n    system_no_input_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n    turn_format: str\n    turn_no_input_format: str\n    prompt_style: Optional[PromptStyle] = None\n\n    def __init__(self, prompt_style=PromptStyle.INSTRUCT.value):\n        self.prompt_style = prompt_style if prompt_style else PromptStyle.INSTRUCT.value\n        self.match_prompt_style()\n\n    def match_prompt_style(self):\n        if self.prompt_style == PromptStyle.INSTRUCT.value:\n            self.turn_format = \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n            self.turn_no_input_format = (\n                \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n            )\n        if self.prompt_style == PromptStyle.CHAT.value:\n            self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n            self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"\n\n    def build_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n        output: Union[None, str] = None,\n    ) -> Generator[str, None, None]:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.system_prompt + self.turn_format.format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = self.system_no_input_prompt + self.turn_no_input_format.format(\n                instruction=instruction\n            )\n        if output:\n            res = f\"{res}{output}\"\n        yield res", "\n\nclass UnpromptedPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for alpaca no system prompt\n    \"\"\"\n\n    system_prompt = \"\"\n    system_no_input_prompt = \"\"\n", "\n\nclass JeopardyPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for Jeopardy\n    \"\"\"\n\n    prompt_input = \"Below is a Jeopardy clue paired with input providing the category of the clue. Write a concise response that best answers tbe clue given the category.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n\n\nclass MultipleChoiceExplainPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for multiple choice explain\n    \"\"\"\n\n    system_prompt = (\n        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n    )\n    system_no_input_prompt = (\n        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n    )", "\n\nclass MultipleChoiceExplainPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for multiple choice explain\n    \"\"\"\n\n    system_prompt = (\n        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n    )\n    system_no_input_prompt = (\n        \"Choose the answer that best answers the question. Explain your reasoning.\\n\"\n    )", "\n\nclass MultipleChoiceConcisePrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for multiple choice concise\n    \"\"\"\n\n    system_prompt = \"Choose the answer that best answers the question. Be concise in your response.\\n\\n\"\n    system_no_input_prompt = \"Choose the answer that best answers the question. Be concise in your response.\\n\\n\"\n\n    def match_prompt_style(self):\n        self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n        self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"", "\n\nclass SummarizeTLDRPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for summarize TLDR\n    \"\"\"\n\n    system_prompt = \"\"\n    system_no_input_prompt = \"\"\n\n    def match_prompt_style(self):\n        self.turn_format = \"USER: Summarize the following article as a TL;DR.\\n{instruction}\\n{input}\\nASSISTANT:\"\n        self.turn_no_input_format = \"USER: Summarize the following article as a TL;DR.\\n{instruction}\\nASSISTANT:\"", "\n\nclass CompletionPrompter:\n    \"\"\"\n    Prompter for completion\n    \"\"\"\n\n    def build_prompt(\n        self,\n        instruction: str,\n        input=None,  # pylint: disable=redefined-builtin, unused-argument\n        output=None,  # pylint: disable=unused-argument\n    ) -> Generator[str, None, None]:\n        yield instruction", "\n\nclass GPTeacherPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for GPTeacher\n    \"\"\"\n\n\nclass NomicGPT4AllPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for NomicGPT4All\n    \"\"\"", "class NomicGPT4AllPrompter(AlpacaPrompter):\n    \"\"\"\n    Prompter for NomicGPT4All\n    \"\"\"\n\n\nclass ReflectAlpacaPrompter:\n    \"\"\"\n    Prompter for ReflectAlpaca\n    \"\"\"\n\n    system_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. You, the Assistant, should generate a response as if it were an abstract for an academic or technical paper on the query along with a methodology. Then generate an Agent Reflection where you create a long form response as if from subject matter expert, be verbose, diligent, and creative in your application of knowledge, apply it through the lens of the response generated by the assistant. Look for flawed reasoning, faulty logic, or other mistakes in the method. Finally, generate a final response and method for the user with the Assistant abstract and Reflection analysis as augmentations to the generation\\n\\n\"\n    system_no_input_prompt = \"Below is an instruction that describes a task. You, the Assistant, should generate a response as if it were an abstract for an academic or technical paper on the query along with a methodology. Then generate an Agent Reflection where you create a long form response as if from subject matter expert, be verbose, diligent, and creative in your application of knowledge, apply it through the lens of the response generated by the assistant. Look for flawed reasoning, faulty logic, or other mistakes in the method. Finally, generate a final response and method for the user with the Assistant abstract and Reflection analysis as augmentations to the generation\\n\\n\"\n\n    prompt_input = (\n        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n    )\n    prompt_no_input = \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n    agent_label = \"### Thought:\\n{output}\\n\\n### Agent Reflection:\\n{reflection}\\n\\n### Final Response:\\n{corrected}\"\n    response_split = \"### Response:\"\n\n    def __init__(self, prompt_style=\"instruct\"):\n        self.prompt_style = prompt_style\n        self.match_prompt_style()\n\n    def match_prompt_style(self):\n        if self.prompt_style == PromptStyle.INSTRUCT.value:\n            self.prompt_input = (\n                self.system_prompt\n                + \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n            )\n            self.prompt_no_input = (\n                self.system_no_input_prompt\n                + \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n            )\n            self.agent_label = \"### Thought:\\n{output}\\n\\n### Agent Reflection:\\n{reflection}\\n\\n### Final Response:\\n{corrected}\"\n            self.response_split = \"### Final Response:\"\n        if self.prompt_style == PromptStyle.CHAT.value:\n            self.prompt_input = (\n                self.system_prompt + \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n            )\n            self.prompt_no_input = (\n                self.system_no_input_prompt + \"USER: {instruction}\\nASSISTANT:\"\n            )\n            self.agent_label = (\n                \"\\nTHOUGHT: {output}\\nASSISTANT REFLECTION: {reflection}\\nASSISTANT:\"\n            )\n            self.response_split = \"ASSISTANT:\"\n\n    def build_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n        output: Union[None, str] = None,\n        reflection: Union[None, str] = None,\n        corrected: Union[None, str] = None,\n    ) -> Generator[str, None, None]:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.prompt_input.format(instruction=instruction, input=input)\n        else:\n            res = self.prompt_no_input.format(instruction=instruction)\n        if output and reflection and corrected:\n            label = self.agent_label.format(\n                output=output,\n                reflection=reflection,\n                corrected=corrected,\n            )\n            res = f\"{res}{label}\"\n        yield res", "\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    TWO = auto()\n    DOLLY = auto()\n\n", "\n\n# TODO clean this \ud83d\udca9 up\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: Optional[str] = None\n\n    def get_prompt(self) -> Generator[Tuple[str, str], None, None]:\n        # seps = [self.sep, self.sep2]\n        preamble = self.system + self.sep\n        yield (\"SYSTEM:\", preamble)\n        for _, (role, message) in enumerate(self.messages):\n            if message:\n                yield (role + \":\", \" \" + message)\n            else:\n                LOG.warning(f\"role with empty message: {role}\")\n                yield (role + \":\", \"\")\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n        )\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])", "\n\nclass ShareGPTPrompter:  # pylint: disable=too-few-public-methods\n    \"\"\"\n    A prompter that generates prompts for the ShareGPT\n    \"\"\"\n\n    def __init__(self, prompt_style=None, system_prompt: Optional[str] = None):\n        if prompt_style != PromptStyle.CHAT.value:\n            raise ValueError(\n                f\"unsupported prompt_style for ShareGPTPrompter({prompt_style})\"\n            )\n        system: str = (\n            system_prompt\n            if system_prompt\n            else (\n                \"A chat between a curious user and an artificial intelligence assistant. \"\n                \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n            )\n        )\n        self._conversation = Conversation(\n            system=system,\n            roles=[\"USER\", \"ASSISTANT\"],\n            messages=[],\n            offset=0,\n            sep_style=SeparatorStyle.TWO,\n            sep=\" \",\n            sep2=\" \",\n        )\n\n    def build_prompt(self, source) -> Generator[str, None, None]:\n        # ignore the system prompt if provided\n        if source[0][\"from\"] == \"system\":\n            source.pop(0)\n\n        if len(source) < 2:\n            # If there isn't a back and forth conversation, ignore it\n            # also happens on the data splitting leaving empty conversations\n            raise IndexError\n\n        conv = self._conversation.copy()\n        roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n        try:\n            # Apply prompt templates\n            if (\n                source[0][\"from\"] not in roles\n                or roles[source[0][\"from\"]] != conv.roles[0]\n            ):\n                # Skip the first one if it is not from human\n                source = source[1:]\n        except IndexError as err:\n            # sometimes there is a bing or system chat\n            raise err\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2]\n            conv.append_message(role, sentence[\"value\"])\n\n        for part in conv.get_prompt():\n            yield part", ""]}
{"filename": "src/axolotl/convert.py", "chunked_list": ["\"\"\"Module containing File Reader, File Writer, Json Parser, and Jsonl Serializer classes\"\"\"\n\n\nimport json\nimport sys\n\n\nclass FileReader:\n    \"\"\"\n    Reads a file and returns its contents as a string\n    \"\"\"\n\n    def read(self, file_path):\n        with open(file_path, encoding=\"utf-8\") as file:\n            return file.read()", "\n\nclass FileWriter:\n    \"\"\"\n    Writes a string to a file\n    \"\"\"\n\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def write(self, content):\n        with open(self.file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)", "\n\nclass StdoutWriter:\n    \"\"\"\n    Writes a string to stdout\n    \"\"\"\n\n    def write(self, content):\n        sys.stdout.write(content)\n        sys.stdout.write(\"\\n\")", "\n\nclass JsonParser:\n    \"\"\"\n    Parses a string as JSON and returns the result\n    \"\"\"\n\n    def parse(self, content):\n        return json.loads(content)\n", "\n\nclass JsonlSerializer:\n    \"\"\"\n    Serializes a list of JSON objects into a JSONL string\n    \"\"\"\n\n    def serialize(self, data):\n        lines = [json.dumps(item) for item in data]\n        return \"\\n\".join(lines)", "\n\nclass JsonToJsonlConverter:\n    \"\"\"\n    Converts a JSON file to JSONL\n    \"\"\"\n\n    def __init__(self, file_reader, file_writer, json_parser, jsonl_serializer):\n        self.file_reader = file_reader\n        self.file_writer = file_writer\n        self.json_parser = json_parser\n        self.jsonl_serializer = jsonl_serializer\n\n    def convert(\n        self, input_file_path, output_file_path\n    ):  # pylint: disable=unused-argument\n        content = self.file_reader.read(input_file_path)\n        data = self.json_parser.parse(content)\n        # data = [r for r in data if r[\"conversations\"]]  # vicuna cleaned has rows with empty conversations\n        jsonl_content = self.jsonl_serializer.serialize(data)\n        self.file_writer.write(jsonl_content)", ""]}
{"filename": "src/axolotl/__init__.py", "chunked_list": [""]}
{"filename": "src/axolotl/prompt_tokenizers.py", "chunked_list": ["\"\"\"Module containing PromptTokenizingStrategy and Prompter classes\"\"\"\n\nimport abc\nimport copy\nimport functools\nimport logging\nfrom typing import Dict, List, Tuple, Union\n\nfrom transformers import PreTrainedTokenizer\n", "from transformers import PreTrainedTokenizer\n\nfrom axolotl.prompters import IGNORE_TOKEN_ID\n\nLOG = logging.getLogger(\"axolotl\")\n\nIGNORE_INDEX = -100\nLLAMA_DEFAULT_PAD_TOKEN = \"[PAD]\"  # nosec\nLLAMA_DEFAULT_EOS_TOKEN = \"</s>\"  # nosec\nLLAMA_DEFAULT_BOS_TOKEN = \"<s>\"  # nosec", "LLAMA_DEFAULT_EOS_TOKEN = \"</s>\"  # nosec\nLLAMA_DEFAULT_BOS_TOKEN = \"<s>\"  # nosec\nLLAMA_DEFAULT_UNK_TOKEN = \"<unk>\"  # nosec\n\n\nclass InvalidDataException(Exception):\n    \"\"\"\n    Exception raised when the data is invalid\n    \"\"\"\n", "\n\nclass PromptTokenizingStrategy(abc.ABC):\n    \"\"\"\n    Abstract class for tokenizing strategies\n    \"\"\"\n\n    def __init__(\n        self,\n        prompter,\n        tokenizer,\n        train_on_inputs: bool = False,\n        sequence_len: int = 2048,\n    ):\n        self.prompter = prompter\n        self.tokenizer: PreTrainedTokenizer = tokenizer\n        self.train_on_inputs = train_on_inputs\n        self.sequence_len = sequence_len\n\n    @abc.abstractmethod\n    def tokenize_prompt(self, prompt):\n        pass\n\n    @functools.lru_cache(maxsize=128)\n    def _get_user_token(self):\n        try:\n            id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|USER|>\")\n            if isinstance(id_or_ids, (int,)):\n                return id_or_ids\n        except KeyError:\n            pass\n        return False\n\n    @functools.lru_cache(maxsize=128)\n    def _get_assistant_token(self):\n        try:\n            id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|ASSISTANT|>\")\n            if isinstance(id_or_ids, (int,)):\n                return id_or_ids\n        except KeyError:\n            pass\n        return False\n\n    def _tokenize(self, prompt: str, add_eos_token=True, strip_bos_token=False):\n        result = self.tokenizer(\n            prompt,\n            truncation=True,\n            max_length=self.sequence_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < self.sequence_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n            result[\"input_ids\"] = result[\"input_ids\"][1:]\n            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result", "\n\nclass InstructionPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for instruction-based prompts.\n    \"\"\"\n\n    def parse_instruction_fields(\n        self, prompt\n    ) -> Union[Tuple[str, str, str], Tuple[str, str, str, str]]:\n        raise NotImplementedError\n\n    def tokenize_prompt(self, prompt):\n        (\n            instruction,\n            input,  # pylint: disable=redefined-builtin\n            response,\n        ) = self.parse_instruction_fields(prompt)\n        user_prompt = next(\n            iter(\n                self.prompter.build_prompt(\n                    instruction,\n                    input,\n                )\n            )\n        )\n        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n        if not self.train_on_inputs:\n            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n            # TODO this could be sped up using numpy array slicing\n            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n        tokenized_res_prompt = self._tokenize(\n            response, strip_bos_token=True, add_eos_token=True\n        )\n        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n\n        return tokenized_prompt\n\n    def _build_full_prompt(\n        self, instruction, input, response  # pylint: disable=redefined-builtin\n    ):\n        return next(\n            iter(\n                self.prompter.build_prompt(\n                    instruction,\n                    input,\n                    response,\n                )\n            )\n        )", "\n\nclass AlpacaPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Alpaca prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"instruction\"],\n            prompt[\"input\"] if \"input\" in prompt else \"\",\n            prompt[\"output\"],\n        )", "\n\nclass AlpacaMultipleChoicePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Alpaca Multiple Choice prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"question\"],\n            \"\\n\".join(f'- \"{choice}\"' for choice in prompt[\"choices\"]),\n            prompt[\"solution\"] if \"solution\" in prompt else prompt[\"explanation\"],\n        )", "\n\nclass JeopardyPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Jeopardy prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"question\"],\n            prompt[\"category\"],\n            \"what is \" + prompt[\"answer\"],\n        )", "\n\nclass OpenAssistantPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for OpenAssistant prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"INSTRUCTION\"],\n            \"\",\n            prompt[\"RESPONSE\"],\n        )", "\n\nclass SummarizeTLDRPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for SummarizeTLDR prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"article\"],\n            \"\",\n            prompt[\"summary\"],\n        )", "\n\nclass GPTeacherPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for GPTeacher prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"instruction\"],\n            prompt[\"input\"] if \"input\" in prompt else \"\",\n            prompt[\"response\"],\n        )", "\n\nclass NomicGPT4AllPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for NomicGPT4All prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"prompt\"],\n            \"\",\n            prompt[\"response\"],\n        )", "\n\nclass CompletionPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Completion prompts.\n    \"\"\"\n\n    def tokenize_prompt(self, prompt):\n        full_prompt = self._build_full_prompt(prompt[\"text\"], None, None)\n        tokenized_full_prompt = self._tokenize(full_prompt)\n\n        return tokenized_full_prompt\n\n    def _build_full_prompt(\n        self, instruction, input, response\n    ):  # pylint: disable=redefined-builtin\n        return next(iter(self.prompter.build_prompt(instruction, input, response)))", "\n\nclass ReflectionPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Reflection prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str, str]:\n        raise NotImplementedError\n\n    def tokenize_prompt(self, prompt):\n        (\n            instruction,\n            input,  # pylint: disable=redefined-builtin\n            output,\n            reflection,\n            corrected,\n        ) = self.parse_instruction_fields(prompt)\n        full_prompt = self._build_full_prompt(\n            instruction, input, output, reflection, corrected\n        )\n        tokenized_full_prompt = self._tokenize(full_prompt)\n        if not self.train_on_inputs:\n            user_prompt = next(\n                iter(\n                    self.prompter.build_prompt(\n                        instruction,\n                        input,\n                    )\n                )\n            )\n            tokenized_user_prompt = self._tokenize(user_prompt, add_eos_token=False)\n            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n            # TODO this could be sped up using numpy array slicing\n            tokenized_full_prompt[\"labels\"] = [\n                -100\n            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n\n        return tokenized_full_prompt\n\n    def _build_full_prompt(\n        self, instruction, input, output, reflection, corrected\n    ):  # pylint: disable=redefined-builtin\n        return next(\n            iter(\n                self.prompter.build_prompt(\n                    instruction,\n                    input,\n                    output,\n                    reflection,\n                    corrected,\n                )\n            )\n        )\n\n    def _tokenize(self, prompt, add_eos_token=True, strip_bos_token=False):\n        result = self.tokenizer(\n            prompt,\n            truncation=True,\n            max_length=self.sequence_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < self.sequence_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result", "\n\nclass AlpacaReflectionPTStrategy(ReflectionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Alpaca Reflection prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str, str]:\n        return (\n            prompt[\"instruction\"],\n            prompt[\"input\"] if \"input\" in prompt else \"\",\n            prompt[\"output\"],\n            prompt[\"reflection\"],\n            prompt[\"corrected\"],\n        )", "\n\nclass ShareGPTPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for ShareGPT prompts.\n    \"\"\"\n\n    def get_conversation_thread(self, prompt):\n        return prompt[\"conversations\"]\n\n    def tokenize_prompt(self, prompt):\n        result, current_len = tokenize_prompt_default()\n        user_token = self._get_user_token()\n        assistant_token = self._get_assistant_token()\n        try:\n            for _, part in enumerate(\n                self.prompter.build_prompt(self.get_conversation_thread(prompt))\n            ):\n                if isinstance(part, tuple):\n                    if part[0] == \"USER:\":\n                        part = part[0] + part[1] if not user_token else part[1]\n                        # this is still the user query, we should\n                        res = self._tokenize(\n                            part.strip(),\n                            add_eos_token=False,\n                            strip_bos_token=True,\n                        )\n                        if user_token:\n                            res[\"input_ids\"] = [user_token, *res[\"input_ids\"]]\n                        # everything from this is masked out from the labels\n                        labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n                    elif part[0] == \"ASSISTANT:\":\n                        # TODO label assistant token/tokens w/ IGNORE_TOKEN_ID\n                        part = part[0] + part[1] if not assistant_token else part[1]\n                        # this should be the assistent response, should end with an eos token\n                        res = self._tokenize(\n                            part.strip(),\n                            add_eos_token=True,\n                            strip_bos_token=True,\n                        )\n                        if assistant_token:\n                            res[\"input_ids\"] = [\n                                assistant_token,\n                                *res[\"input_ids\"],\n                            ]\n                        # not masked out from labels\n                        labels = copy.deepcopy(res[\"input_ids\"])\n                    elif part[0] == \"SYSTEM:\":\n                        part = part[1]  # Ignore the system role from preamble\n                        # this is only ever the first part, should include the bos token and the user query\n                        res = self._tokenize(\n                            part.strip(), add_eos_token=False, strip_bos_token=False\n                        )\n                        # everything from this is masked out from the labels\n                        labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n                    else:\n                        LOG.warning(f\"unhandled role: {part[0]}\")\n\n                # pylint: disable=duplicate-code\n                result, current_len = parse_tokenized_to_result(\n                    result,\n                    current_len,\n                    res,\n                    labels,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                )\n            return result\n        except (KeyError, AssertionError, IndexError) as err:\n            raise InvalidDataException(str(err)) from err\n\n    def _tokenize(self, prompt, add_eos_token=True, strip_bos_token=False):\n        result = self.tokenizer(\n            prompt,\n            truncation=True,\n            max_length=self.sequence_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < self.sequence_len\n            and add_eos_token\n        ):\n            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n            result[\"input_ids\"] = result[\"input_ids\"][1:]\n            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result", "\n\ndef tokenize_prompt_default() -> Tuple[Dict[str, List[int]], int]:\n    \"\"\"\n    Returns the default values for the tokenize prompt function\n    \"\"\"\n\n    result: Dict[str, List[int]] = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"labels\": [],\n    }\n    current_len = 0\n    return result, current_len", "\n\ndef parse_tokenized_to_result(\n    result: Dict[str, List[int]],\n    current_len: int,\n    res: Dict[str, List[int]],\n    labels: List[int],\n    pad_token_id: Union[int, None] = None,\n) -> Tuple[Dict[str, List[int]], int]:\n    \"\"\"\n    Parses the tokenized prompt and append the tokenized input_ids, attention_mask and labels to the result\n    \"\"\"\n\n    input_ids = res[\"input_ids\"]\n    input_len = len(input_ids)\n    result[\"input_ids\"][current_len : current_len + input_len] = input_ids\n    result[\"attention_mask\"][current_len : current_len + input_len] = [\n        1 if x != pad_token_id else 0 for x in input_ids\n    ]\n    result[\"labels\"][current_len : current_len + input_len] = labels\n    current_len += input_len\n\n    return result, current_len", ""]}
{"filename": "src/axolotl/datasets.py", "chunked_list": ["\"\"\"Module containing Dataset functionality\"\"\"\n\nimport logging\nimport os\nfrom typing import List\n\nimport torch\nfrom datasets import IterableDataset\n\nfrom .prompt_tokenizers import PromptTokenizingStrategy", "\nfrom .prompt_tokenizers import PromptTokenizingStrategy\n\n# We want this to be a wrapper for an existing dataset that we have loaded\n# lets use the concept of middlewares to wrap each dataset, for example\n# ConstantLengthDataset(ShuffledDataset([TokenizedPromptDataset(alpaca_dataset)]))\n# let's check to ensure we don't truncate an item in the middle, we'll use\n# the collators later on to pad the datasets\n\nLOG = logging.getLogger(\"axolotl\")", "\nLOG = logging.getLogger(\"axolotl\")\n\n\nclass TokenizedPromptDataset(IterableDataset):\n    \"\"\"\n    Iterable dataset that returns tokenized prompts from a stream of text files.\n        Args:\n            prompt_tokenizer (PromptTokenizingStrategy): The prompt tokenizing method for proccessing the data.\n            dataset (dataset.Dataset): Dataset with text files.\n    \"\"\"\n\n    def __init__(  # pylint: disable=super-init-not-called\n        self,\n        prompt_tokenizer: PromptTokenizingStrategy,\n        dataset: IterableDataset,\n    ):\n        self.prompt_tokenizer = prompt_tokenizer\n        self.dataset = dataset\n\n    def __iter__(self):\n        features = self.dataset.features.keys()\n        num_proc = os.cpu_count()\n        return iter(\n            self.dataset.map(\n                self.prompt_tokenizer.tokenize_prompt,\n                num_proc=num_proc,\n                remove_columns=features,\n            )\n        )", "\n\n# TODO this isn't the best since it can't interleave datasets\nclass ConstantLengthDataset(IterableDataset):\n    \"\"\"\n    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n        Args:\n            tokenizer (Tokenizer): The processor used for proccessing the data.\n            dataset (dataset.Dataset): Dataset with text files.\n            seq_length (int): Length of token sequences to return.\n    \"\"\"\n\n    def __init__(  # pylint: disable=super-init-not-called\n        self,\n        tokenizer,\n        datasets,\n        seq_length=2048,\n    ):\n        self.tokenizer = tokenizer\n        self.concat_token_id = tokenizer.eos_token_id\n        self.datasets: List[IterableDataset] = datasets\n        self.seq_length = seq_length\n\n        vocab_size = len(tokenizer.get_vocab())\n\n        if vocab_size <= torch.iinfo(torch.int16).max:\n            self.tokens_dtype = torch.int16\n        elif vocab_size <= torch.iinfo(torch.int32).max:\n            self.tokens_dtype = torch.int32\n        else:\n            self.tokens_dtype = torch.int64\n\n    def __iter__(self):\n        buffer = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n        buffer_len = 0\n        for dataset in self.datasets:\n            iterator = iter(dataset)\n            more_examples = True\n            while more_examples:\n                try:\n                    example = next(iterator)\n                except StopIteration:\n                    more_examples = False\n                    example = None\n\n                add_concat_token = False\n                if example:\n                    example_len = len(example[\"input_ids\"])\n                    add_concat_token = example[\"input_ids\"][-1] != self.concat_token_id\n                else:\n                    example_len = 0\n\n                if not example_len or (\n                    buffer_len + int(add_concat_token) + example_len > self.seq_length\n                ):\n                    if buffer[\"input_ids\"]:\n                        input_ids = torch.cat(buffer[\"input_ids\"], dim=-1)[\n                            : self.seq_length\n                        ]\n                        attention_mask = torch.cat(buffer[\"attention_mask\"], dim=-1)[\n                            : self.seq_length\n                        ]\n                        labels = torch.cat(buffer[\"labels\"], dim=-1)[: self.seq_length]\n                        if labels.size() == input_ids.size() and (\n                            attention_mask.size() == input_ids.size()\n                        ):\n                            yield {\n                                \"input_ids\": input_ids,\n                                \"labels\": labels,\n                                \"attention_mask\": attention_mask,\n                            }\n                        else:\n                            LOG.warning(\n                                f\"dropping batch due to tensor size mismatch input_ids: {input_ids.size()}, labels: {labels.size()}, attention_mask: {attention_mask.size()}\"\n                            )\n                    buffer = {\n                        \"input_ids\": [],\n                        \"attention_mask\": [],\n                        \"labels\": [],\n                    }\n                    buffer_len = 0\n\n                if example:\n                    # FIXME\n                    # just going to drop data points that are too long\n                    if len(example[\"input_ids\"]) <= self.seq_length:\n                        input_ids = example[\"input_ids\"]\n                        attention_mask = example[\"attention_mask\"]\n                        labels = example[\"labels\"]\n                        if (\n                            buffer[\"input_ids\"]\n                            and input_ids[0] == self.tokenizer.bos_token_id\n                        ):\n                            attention_mask[0] = 0\n\n                        if add_concat_token:\n                            input_ids.append(self.concat_token_id)\n                            attention_mask.append(1)\n                            labels.append(self.concat_token_id)\n\n                        input_ids_with_concat = torch.tensor(\n                            input_ids, dtype=self.tokens_dtype\n                        )\n                        attention_mask_with_concat = torch.tensor(\n                            attention_mask, dtype=self.tokens_dtype\n                        )\n                        labels_with_concat = torch.tensor(\n                            labels, dtype=self.tokens_dtype\n                        )\n\n                        buffer[\"input_ids\"].append(input_ids_with_concat)\n                        buffer[\"attention_mask\"].append(attention_mask_with_concat)\n                        buffer[\"labels\"].append(labels_with_concat)\n                        buffer_len += len(input_ids)", ""]}
{"filename": "src/axolotl/flash_attn.py", "chunked_list": ["\"\"\"Flash attention monkey patch for llama model\"\"\"\n\n# copied from https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py\n\nfrom typing import Optional, Tuple\n\nimport torch\nimport transformers\nfrom einops import rearrange\nfrom flash_attn.bert_padding import pad_input, unpad_input", "from einops import rearrange\nfrom flash_attn.bert_padding import pad_input, unpad_input\nfrom flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n\n    attention_mask: [bsz, q_len]\n    \"\"\"\n    # pylint: disable=duplicate-code\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack(\n        [query_states, key_states, value_states], dim=2\n    )  # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n        max_s = q_len\n        cu_q_lens = torch.arange(\n            0,\n            (bsz + 1) * q_len,\n            step=q_len,\n            dtype=torch.int32,\n            device=qkv.device,\n        )\n        output = flash_attn_varlen_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n        )\n        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n\n        # pylint: disable=invalid-name\n        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(\n            x_unpad,\n            \"nnz (three h d) -> nnz three h d\",\n            three=3,\n            h=nheads,\n        )\n        output_unpad = flash_attn_varlen_qkvpacked_func(\n            x_unpad,\n            cu_q_lens,\n            max_s,\n            0.0,\n            softmax_scale=None,\n            causal=True,\n        )\n        output = rearrange(\n            pad_input(\n                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"),\n                indices,\n                bsz,\n                q_len,\n            ),\n            \"b s (h d) -> b s h d\",\n            h=nheads,\n        )\n    return (\n        self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")),\n        None,\n        None,\n    )", "\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(\n    self,\n    attention_mask,\n    input_shape,\n    inputs_embeds,\n    past_key_values_length,\n):  # pylint: disable=unused-argument\n    # [bsz, seq_len]\n    return attention_mask", "\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (  # pylint: disable=protected-access\n        _prepare_decoder_attention_mask\n    )\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "src/axolotl/monkeypatch/llama_attn_hijack_xformers.py", "chunked_list": ["\"\"\"\nDirectly copied the code from https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/modules/llama_attn_hijack.py and made some adjustments\n\"\"\"\n\nimport logging\nimport math\nfrom typing import Optional, Tuple\n\nimport torch\nimport transformers.models.llama.modeling_llama", "import torch\nimport transformers.models.llama.modeling_llama\nfrom torch import nn\n\ntry:\n    import xformers.ops\nexcept ImportError:\n    logging.error(\"xformers not found! Please install it before trying to use it.\")\n\n\ndef hijack_llama_attention():\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = xformers_forward", "\n\ndef hijack_llama_attention():\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = xformers_forward\n\n\ndef hijack_llama_sdp_attention():\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = (\n        sdp_attention_forward\n    )", "\n\ndef xformers_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    # pylint: disable=duplicate-code\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    (\n        query_states,\n        key_states,\n    ) = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # We only apply xformers optimizations if we don't need to output the whole attention matrix\n    if not output_attentions:\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        # This is a nasty hack. We know attention_mask in transformers is either LowerTriangular or all Zeros.\n        # We therefore check if one element in the upper triangular portion is zero. If it is, then the mask is all zeros.\n        if attention_mask is None or attention_mask[0, 0, 0, 1] == 0:\n            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n            attn_output = xformers.ops.memory_efficient_attention(\n                query_states, key_states, value_states, attn_bias=None\n            )\n        else:\n            # input and output should be of form (bsz, q_len, num_heads, head_dim)\n            attn_output = xformers.ops.memory_efficient_attention(\n                query_states,\n                key_states,\n                value_states,\n                attn_bias=xformers.ops.LowerTriangularMask(),\n            )\n        attn_weights = None\n    else:\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(\n                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n            )\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n    return attn_output, attn_weights, past_key_value", "\n\ndef sdp_attention_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    # pylint: disable=duplicate-code\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    (\n        query_states,\n        key_states,\n    ) = transformers.models.llama.modeling_llama.apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # We only apply sdp attention if we don't need to output the whole attention matrix\n    if not output_attentions:\n        with torch.backends.cuda.sdp_kernel():\n            attn_output = torch.nn.functional.scaled_dot_product_attention(\n                query_states,\n                key_states,\n                value_states,\n                attn_mask=attention_mask,\n                is_causal=False,\n            )\n            attn_weights = None\n    else:\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(\n                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n            )\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    attn_output = self.o_proj(attn_output)\n\n    return attn_output, attn_weights, past_key_value", ""]}
{"filename": "src/axolotl/monkeypatch/xpos_rope_llama_monkey_patch.py", "chunked_list": ["# pylint: skip-file\n\"\"\"\nCopied from https://github.com/kaiokendev/cutoff-len-is-context-len/blob/main/util/xpos_rope_llama_monkey_patch.py\n\"\"\"\nimport torch\nimport transformers\nimport transformers.models.llama.modeling_llama\nfrom einops import rearrange\n\n\nclass XposRotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        dim,\n        max_position_embeddings=2048,\n        base=10000,\n        device=None,\n        scale_base=2048,\n        use_xpos=True,\n    ):\n        super().__init__()\n        self.max_seq_len_cached = max_position_embeddings\n        self.scale_base = scale_base\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(self.max_seq_len_cached, device=device).type_as(inv_freq)\n        freqs = torch.einsum(\"i , j -> i j\", t, inv_freq)\n        freqs = torch.cat((freqs, freqs), dim=-1)\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.register_buffer(\"freqs_cached\", freqs, persistent=False)\n\n        if not use_xpos:\n            self.register_buffer(\"scale\", None)\n            self.register_buffer(\"scale_cached\", torch.ones(1))\n            return\n\n        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n        power = (t - (self.max_seq_len_cached // 2)) / self.scale_base\n        scale_cached = scale ** rearrange(power, \"n -> n 1\")\n        scale_cached = torch.cat((scale_cached, scale_cached), dim=-1)\n\n        self.register_buffer(\"scale\", scale, persistent=False)\n        self.register_buffer(\"scale_cached\", scale_cached, persistent=False)\n\n    def forward(\n        self,\n        x,\n        seq_len,\n    ):\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device).type_as(\n                self.inv_freq\n            )\n            freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n            freqs = torch.cat((freqs, freqs), dim=-1).to(dtype=x.dtype)\n\n            self.register_buffer(\"freqs_cached\", freqs)\n\n            if self.scale is None:\n                self.register_buffer(\n                    \"scale_cached\", torch.ones(1, device=x.device).to(dtype=x.dtype)\n                )\n\n                return self.freqs_cached.to(dtype=x.dtype), self.scale_cached\n\n            power = (t - (seq_len // 2)) / self.scale_base\n            scale = self.scale ** rearrange(power, \"n -> n 1\")\n            scale = torch.cat((scale, scale), dim=-1).to(dtype=x.dtype)\n            self.register_buffer(\"scale_cached\", scale)\n\n        return self.freqs_cached.to(dtype=x.dtype), self.scale_cached.to(dtype=x.dtype)", "\n\nclass XposRotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        dim,\n        max_position_embeddings=2048,\n        base=10000,\n        device=None,\n        scale_base=2048,\n        use_xpos=True,\n    ):\n        super().__init__()\n        self.max_seq_len_cached = max_position_embeddings\n        self.scale_base = scale_base\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(self.max_seq_len_cached, device=device).type_as(inv_freq)\n        freqs = torch.einsum(\"i , j -> i j\", t, inv_freq)\n        freqs = torch.cat((freqs, freqs), dim=-1)\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.register_buffer(\"freqs_cached\", freqs, persistent=False)\n\n        if not use_xpos:\n            self.register_buffer(\"scale\", None)\n            self.register_buffer(\"scale_cached\", torch.ones(1))\n            return\n\n        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n        power = (t - (self.max_seq_len_cached // 2)) / self.scale_base\n        scale_cached = scale ** rearrange(power, \"n -> n 1\")\n        scale_cached = torch.cat((scale_cached, scale_cached), dim=-1)\n\n        self.register_buffer(\"scale\", scale, persistent=False)\n        self.register_buffer(\"scale_cached\", scale_cached, persistent=False)\n\n    def forward(\n        self,\n        x,\n        seq_len,\n    ):\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device).type_as(\n                self.inv_freq\n            )\n            freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n            freqs = torch.cat((freqs, freqs), dim=-1).to(dtype=x.dtype)\n\n            self.register_buffer(\"freqs_cached\", freqs)\n\n            if self.scale is None:\n                self.register_buffer(\n                    \"scale_cached\", torch.ones(1, device=x.device).to(dtype=x.dtype)\n                )\n\n                return self.freqs_cached.to(dtype=x.dtype), self.scale_cached\n\n            power = (t - (seq_len // 2)) / self.scale_base\n            scale = self.scale ** rearrange(power, \"n -> n 1\")\n            scale = torch.cat((scale, scale), dim=-1).to(dtype=x.dtype)\n            self.register_buffer(\"scale_cached\", scale)\n\n        return self.freqs_cached.to(dtype=x.dtype), self.scale_cached.to(dtype=x.dtype)", "\n\ndef rotate_half(x):\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, freqs, scale=1, position_ids=None):\n    freqs = freqs[position_ids, :]\n    if scale.shape[-1] != 1:\n        scale = scale[position_ids, :]\n\n    q_embed = (q * freqs.cos() * scale) + (rotate_half(q) * freqs.sin() * scale)\n    k_embed = (k * freqs.cos() * 1 / scale) + (rotate_half(k) * freqs.sin() * 1 / scale)\n\n    return q_embed, k_embed", "\n\ndef replace_llama_rope_with_xpos_rope():\n    transformers.models.llama.modeling_llama.LlamaRotaryEmbedding = XposRotaryEmbedding\n    transformers.models.llama.modeling_llama.apply_rotary_pos_emb = apply_rotary_pos_emb\n"]}
{"filename": "src/axolotl/monkeypatch/llama_landmark_attn.py", "chunked_list": ["# pylint: skip-file\n# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");", "#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nPyTorch LLaMA model.\nTaken from https://github.com/epfml/landmark-attention/blob/main/llama/llama_mem.py and modified.\n\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union", "import math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import LlamaTokenizer\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,", "from transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import (\n    LLAMA_INPUTS_DOCSTRING,\n    LLAMA_START_DOCSTRING,\n    LlamaMLP,\n    LlamaPreTrainedModel,", "    LlamaMLP,\n    LlamaPreTrainedModel,\n    LlamaRMSNorm,\n    LlamaRotaryEmbedding,\n    _expand_mask,\n    _make_causal_mask,\n    rotate_half,\n)\nfrom transformers.utils import (\n    add_start_docstrings,", "from transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\n\nLOG = logging.getLogger(\"axolotl\")\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"", "\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\nMEM_TOKEN = \"<landmark>\"  # nosec\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    if q is None:\n        q_embed = None\n    else:\n        q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed", "\n\nclass LandmarkGroupedSoftmaxFunction(torch.autograd.Function):\n    \"\"\"\n    Landmark grouped softmax function.\n    \"\"\"\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(ctx, x, dim, mem_cnt, resp_mem_idx):\n        new_shape = list(x.shape)\n        new_shape[dim] = mem_cnt  # max_mem_cnt.item()\n        max_by_group = x.new_zeros((*new_shape,))\n        max_by_group.scatter_reduce_(\n            src=x, index=resp_mem_idx, dim=dim, reduce=\"amax\", include_self=False\n        )\n\n        maxes = torch.gather(max_by_group, dim, resp_mem_idx)\n        # x_exp = torch.exp(x - torch.where(torch.isinf(maxes), 0, maxes))\n        x_exp = torch.exp((x - maxes).to(torch.float32))\n\n        cumsum_by_group = torch.zeros_like(max_by_group, dtype=x_exp.dtype)\n\n        cumsum_by_group.scatter_add_(\n            dim,\n            resp_mem_idx,\n            x_exp,\n        )\n        denom = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n\n        # probs = torch.where(denom < 0.5, 0, x_exp / denom)\n        probs = x_exp / denom\n\n        ctx.mem_cnt = mem_cnt\n        ctx.dim = dim\n        ctx.save_for_backward(resp_mem_idx, probs)\n\n        return probs\n\n    @staticmethod\n    def backward(ctx, grad_probs):\n        mem_cnt = ctx.mem_cnt\n        dim = ctx.dim\n        resp_mem_idx, probs = ctx.saved_tensors\n        grad_x = grad_dim = grad_mem_cnt = grad_resp_mem_idx = None\n\n        if ctx.needs_input_grad[0] or ctx.needs_input_grad[4]:\n            grad_pair = grad_probs * probs\n\n            new_shape = list(probs.shape)\n            new_shape[dim] = mem_cnt  # max_mem_cnt.item()\n            cumsum_by_group = grad_pair.new_zeros((*new_shape,))\n            cumsum_by_group.scatter_add_(dim, resp_mem_idx, grad_pair)\n\n        if ctx.needs_input_grad[0]:\n            grad_sum = torch.gather(cumsum_by_group, dim, resp_mem_idx)\n            grad_x = grad_pair - probs * grad_sum\n        assert not ctx.needs_input_grad[1]\n        assert not ctx.needs_input_grad[2]\n        assert not ctx.needs_input_grad[3]\n\n        return grad_x, grad_dim, grad_mem_cnt, grad_resp_mem_idx", "\n\ndef landmark_grouped_softmax(x, dim, is_mem, last_section_mask):\n    last_and_rest_mask = last_section_mask  # | mask\n\n    full_access_mask = is_mem | last_and_rest_mask\n\n    max_mem_cnt = 16\n    mem_group_idx = torch.cumsum(is_mem, dim=dim)\n    mem_bucket_id = max_mem_cnt - 1\n    resp_mem_idx = torch.where(\n        last_and_rest_mask,\n        max_mem_cnt - 1,\n        torch.where(is_mem, mem_bucket_id, mem_group_idx),\n    )\n    probs = LandmarkGroupedSoftmaxFunction.apply(x, dim, max_mem_cnt, resp_mem_idx)\n\n    new_shape = list(x.shape)\n    new_shape[dim] = max_mem_cnt\n    group_prob = probs.new_zeros((*new_shape,))\n    group_prob.scatter_(\n        dim, torch.where(is_mem, mem_group_idx - 1, max_mem_cnt - 1), probs\n    )\n    probs = probs.mul(\n        torch.where(\n            full_access_mask,\n            last_section_mask,\n            torch.gather(group_prob, dim, resp_mem_idx),\n        )\n    )\n\n    return probs", "\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=False\n        )\n        self.k_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=False\n        )\n        self.v_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=False\n        )\n        self.o_proj = nn.Linear(\n            self.num_heads * self.head_dim, self.hidden_size, bias=False\n        )\n        self.rotary_emb = LlamaRotaryEmbedding(\n            self.head_dim, max_position_embeddings=self.max_position_embeddings\n        )\n\n        self.mem_freq = None\n        self.top_k = None\n        self.max_cache_size = None\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        self.mem_freq = mem_freq\n        self.top_k = top_k\n        self.max_cache_size = max_cache_size\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = (\n            self.q_proj(hidden_states)\n            .view(bsz, q_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n        )\n        key_states = (\n            self.k_proj(hidden_states)\n            .view(bsz, q_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n        )\n        value_states = (\n            self.v_proj(hidden_states)\n            .view(bsz, q_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n        )\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n            if len(past_key_value) > 2:\n                kv_seq_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        key_states_before_pos = key_states\n        query_states, key_states = apply_rotary_pos_emb(\n            query_states, key_states, cos, sin, position_ids\n        )\n        # [bsz, nh, t, hd]\n\n        attn_prefix = None\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            if self.mem_freq is None:\n                cache_len = past_key_value[0].shape[2]\n                if self.max_cache_size is not None:\n                    cache_len = min(cache_len, self.max_cache_size)\n                if is_mem is not None:\n                    is_mem = torch.cat(\n                        (is_mem.new_zeros((1, 1, q_len, cache_len)), is_mem), dim=-1\n                    )\n                    last_section_mask = torch.cat(\n                        (\n                            last_section_mask.new_ones((1, 1, q_len, cache_len)),\n                            last_section_mask,\n                        ),\n                        dim=-1,\n                    )\n\n                past_key_states = torch.cat([past_key_value[0], key_states], dim=2)\n                past_value_states = torch.cat([past_key_value[1], value_states], dim=2)\n                key_states = past_key_states[:, :, -(q_len + cache_len) :]\n                value_states = past_value_states[:, :, -(q_len + cache_len) :]\n                expected_att_size = (bsz, self.num_heads, q_len, cache_len + q_len)\n            else:\n                orig_value_states = value_states\n\n                incomplete_len = past_key_value[0].shape[2] % (self.mem_freq + 1)\n                full_len = past_key_value[0].shape[2] - incomplete_len\n                past_key_mem, past_key_incomplete = torch.split(\n                    past_key_value[0], (full_len, incomplete_len), dim=2\n                )\n                past_value_mem, past_value_incomplete = torch.split(\n                    past_key_value[1], (full_len, incomplete_len), dim=2\n                )\n\n                if offload_cache_to_cpu:\n                    past_key_value = (\n                        past_key_incomplete,\n                        past_value_incomplete,\n                        *past_key_value[2:],\n                    )\n\n                if incomplete_len > 0:\n                    assert q_len + incomplete_len <= (self.mem_freq + 1)\n                is_mem = torch.cat(\n                    (is_mem.new_zeros((1, 1, q_len, incomplete_len)), is_mem), dim=-1\n                )\n                last_section_mask = torch.cat(\n                    (\n                        last_section_mask.new_ones((1, 1, q_len, incomplete_len)),\n                        last_section_mask,\n                    ),\n                    dim=-1,\n                )\n\n                if len(past_key_value) > 2:\n                    full_len += past_key_value[3].shape[2] * past_key_value[3].shape[3]\n                past_key_incomplete_pos = torch.arange(\n                    full_len,\n                    full_len + incomplete_len,\n                    dtype=torch.long,\n                    device=position_ids.device,\n                ).unsqueeze(0)\n                _, past_key_incomplete = apply_rotary_pos_emb(\n                    None, past_key_incomplete, cos, sin, past_key_incomplete_pos\n                )\n                key_states = torch.cat((past_key_incomplete, key_states), dim=2)\n                value_states = torch.cat((past_value_incomplete, value_states), dim=2)\n\n                past_key_mem = past_key_mem.view(\n                    bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim\n                )\n                past_value_mem = past_value_mem.view(\n                    bsz, self.num_heads, -1, self.mem_freq + 1, self.head_dim\n                )\n\n                if len(past_key_value) > 2:\n                    mem_key_nopos = torch.cat(\n                        (\n                            past_key_value[2],\n                            past_key_mem.select(dim=3, index=self.mem_freq),\n                        ),\n                        dim=2,\n                    )\n                    past_key_mem_offload = past_key_value[3]\n                    past_key_mem = torch.cat(\n                        (\n                            past_key_mem_offload,\n                            past_key_mem.to(past_key_mem_offload.device),\n                        ),\n                        dim=2,\n                    )\n                    past_value_mem = torch.cat(\n                        (\n                            past_key_value[4],\n                            past_value_mem.to(past_key_mem_offload.device),\n                        ),\n                        dim=2,\n                    )\n                else:\n                    mem_key_nopos = past_key_mem.select(dim=3, index=self.mem_freq)\n\n                num_mems = past_key_mem.shape[2]\n                top_k = min(self.top_k, num_mems)\n                prefix_len = full_len - (top_k + 1) * (self.mem_freq + 1)\n                mem_indices = torch.cat(\n                    (\n                        position_ids.new_zeros((max(0, num_mems - top_k),)),\n                        torch.arange(\n                            1,\n                            top_k + 1,\n                            device=query_states.device,\n                            dtype=position_ids.dtype,\n                        ),\n                    ),\n                    dim=0,\n                )\n                mem_pos = (mem_indices * (self.mem_freq + 1) + self.mem_freq).unsqueeze(\n                    0\n                ).expand(bsz, -1) + prefix_len\n                _, mem_key = apply_rotary_pos_emb(\n                    None, mem_key_nopos, cos, sin, mem_pos\n                )\n                mem_attn_weights = torch.matmul(\n                    query_states, mem_key.transpose(2, 3)\n                ) / math.sqrt(self.head_dim)\n\n                if offload_cache_to_cpu:\n                    aggregate = \"max_over_tokens\"\n                else:\n                    aggregate = None\n                if aggregate == \"max_over_tokens\":\n                    token_retrievers = 1\n                    head_retrievers = self.num_heads\n                    mem_attn_weights = torch.nn.functional.softmax(\n                        mem_attn_weights, dim=-1\n                    )\n                    mem_attn_weights = mem_attn_weights.amax(dim=2, keepdim=True)\n                elif aggregate is None:\n                    token_retrievers = q_len\n                    head_retrievers = self.num_heads\n                else:\n                    raise NotImplementedError()\n\n                mem_selected_idx = (\n                    mem_attn_weights.topk(dim=-1, k=top_k)[1]\n                    .sort(dim=-1)[0]\n                    .view(bsz, head_retrievers, token_retrievers, top_k)\n                )\n\n                selected_indices = torch.arange(\n                    0,\n                    top_k * (self.mem_freq + 1),\n                    device=query_states.device,\n                    dtype=position_ids.dtype,\n                )\n                selected_indices = torch.where(\n                    mem_selected_idx >= num_mems - top_k, self.mem_freq + 1, 0\n                ).unsqueeze(-1) + selected_indices.view(\n                    1, 1, 1, top_k, self.mem_freq + 1\n                )\n                selected_indices = (\n                    selected_indices.view(\n                        bsz, head_retrievers, token_retrievers, -1\n                    ).expand(bsz, self.num_heads, q_len, -1)\n                    + prefix_len\n                )\n\n                mem_selected_idx = mem_selected_idx.to(past_key_mem.device)\n\n                mem_selected_idx = mem_selected_idx.view(\n                    bsz, self.num_heads, token_retrievers, top_k, 1, 1\n                ).expand(\n                    bsz,\n                    self.num_heads,\n                    token_retrievers,\n                    top_k,\n                    self.mem_freq + 1,\n                    self.head_dim,\n                )\n                selected_keys = past_key_mem.unsqueeze(2).expand(\n                    bsz,\n                    self.num_heads,\n                    token_retrievers,\n                    -1,\n                    self.mem_freq + 1,\n                    self.head_dim,\n                )\n                selected_keys = selected_keys.take_along_dim(\n                    mem_selected_idx, dim=3\n                ).to(query_states.device)\n                selected_values = (\n                    past_value_mem.unsqueeze(2)\n                    .expand(\n                        bsz,\n                        self.num_heads,\n                        token_retrievers,\n                        -1,\n                        self.mem_freq + 1,\n                        self.head_dim,\n                    )\n                    .take_along_dim(mem_selected_idx, dim=3)\n                    .to(query_states.device)\n                )\n\n                selected_keys = selected_keys.view(\n                    bsz, self.num_heads, token_retrievers, -1, self.head_dim\n                ).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                selected_keys = apply_rotary_pos_emb(\n                    None, selected_keys.unsqueeze(1), cos, sin, selected_indices\n                )[1].squeeze(1)\n                selected_values = selected_values.view(\n                    bsz, self.num_heads, token_retrievers, -1, self.head_dim\n                ).expand(bsz, self.num_heads, q_len, -1, self.head_dim)\n                attn_prefix = torch.matmul(\n                    query_states.unsqueeze(3), selected_keys.transpose(3, 4)\n                ).squeeze(3) / math.sqrt(self.head_dim)\n                is_mem_prefix = (\n                    torch.cat(\n                        (is_mem.new_zeros((self.mem_freq,)), is_mem.new_ones((1,)))\n                    )\n                    .unsqueeze(0)\n                    .repeat((top_k, 1))\n                )\n                is_mem_prefix = is_mem_prefix.view(1, 1, 1, -1).expand(1, 1, q_len, -1)\n                is_mem = torch.cat((is_mem_prefix, is_mem), dim=-1)\n                last_section_mask = torch.cat(\n                    (\n                        last_section_mask.new_zeros(\n                            (1, 1, q_len, top_k * (self.mem_freq + 1))\n                        ),\n                        last_section_mask,\n                    ),\n                    dim=-1,\n                )\n                expected_att_size = (bsz, self.num_heads, q_len, q_len + incomplete_len)\n\n                past_key_states = torch.cat(\n                    [past_key_value[0], key_states_before_pos], dim=2\n                )\n                past_value_states = torch.cat(\n                    [past_key_value[1], orig_value_states], dim=2\n                )\n\n                if offload_cache_to_cpu:\n                    past_key_value = (\n                        (\n                            past_key_states,\n                            past_value_states,\n                            mem_key_nopos,\n                            past_key_mem.to(\"cpu\"),\n                            past_value_mem.to(\"cpu\"),\n                            *past_key_value[5:],\n                        )\n                        if use_cache\n                        else None\n                    )\n                else:\n                    past_key_value = (\n                        (past_key_states, past_value_states) if use_cache else None\n                    )\n\n        else:\n            if self.mem_freq is None:\n                past_key_states = key_states\n            else:\n                past_key_states = key_states_before_pos\n            past_value_states = value_states\n            expected_att_size = (bsz, self.num_heads, q_len, kv_seq_len)\n            past_key_value = (past_key_states, past_value_states) if use_cache else None\n\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n        if attn_weights.size() != expected_att_size:\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask[..., -attn_weights.shape[-1] :]\n            attn_weights = torch.max(\n                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n            )\n        if attn_prefix is not None:\n            attn_weights = torch.cat((attn_prefix, attn_weights), dim=-1)\n        # upcast attention to fp32\n        if is_mem is None:\n            raise ValueError(\"Don't use this without landmarks\")\n\n        attn_weights = landmark_grouped_softmax(\n            attn_weights,\n            dim=-1,\n            is_mem=is_mem.expand(-1, self.num_heads, -1, -1),\n            last_section_mask=last_section_mask,\n        ).to(query_states.dtype)\n\n        if attn_prefix is not None:\n            attn_prefix, attn_weights = torch.split(\n                attn_weights,\n                (attn_prefix.shape[-1], attn_weights.shape[-1] - attn_prefix.shape[-1]),\n                dim=-1,\n            )\n        attn_output = torch.matmul(attn_weights, value_states)\n        if attn_prefix is not None:\n            attn_output += torch.matmul(\n                attn_prefix.unsqueeze(3), selected_values\n            ).squeeze(3)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value", "\n\nclass LlamaDecoderLayer(nn.Module):\n    \"\"\"\n    Llama Decoder layer\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(\n            config.hidden_size, eps=config.rms_norm_eps\n        )\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        self.self_attn.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        is_mem: Optional[torch.Tensor] = None,\n        last_section_mask: Optional[torch.Tensor] = None,\n        offload_cache_to_cpu: bool = False,\n    ) -> Tuple[\n        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]\n    ]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            is_mem=is_mem,\n            last_section_mask=last_section_mask,\n            offload_cache_to_cpu=offload_cache_to_cpu,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs", "\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(\n            config.vocab_size, config.hidden_size, self.padding_idx\n        )\n        self.layers = nn.ModuleList(\n            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.mem_id = None\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def set_mem_id(self, mem_id):\n        self.mem_id = mem_id\n\n    def set_mem_cache_args(self, mem_freq, top_k, max_cache_size):\n        for layer in self.layers:\n            layer.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(\n        self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n    ):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(\n                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n            ).to(inputs_embeds.device)\n            combined_attention_mask = (\n                expanded_attn_mask\n                if combined_attention_mask is None\n                else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # retrieve input_ids and inputs_embeds\n        is_mem = None\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n            if self.mem_id is not None:\n                with torch.no_grad():\n                    is_mem = input_ids == self.mem_id\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n            if self.mem_id is not None:\n                raise NotImplementedError\n        else:\n            raise ValueError(\n                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n            )\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            if is_mem is not None:\n                pass\n                # raise NotImplementedError\n            past_key_values_length = past_key_values[0][0].shape[2]\n            if len(past_key_values[0]) > 2:\n                past_key_values_length += (\n                    past_key_values[0][3].shape[2] * past_key_values[0][3].shape[3]\n                )\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length,\n                seq_length + past_key_values_length,\n                dtype=torch.long,\n                device=device,\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past),\n                dtype=torch.bool,\n                device=inputs_embeds.device,\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask,\n            (batch_size, seq_length),\n            inputs_embeds,\n            past_key_values_length,\n        )\n\n        last_section_mask = None\n        if is_mem is not None:\n            is_mem = is_mem.unsqueeze(1).unsqueeze(2)\n            current_len = input_ids.shape[1]\n            mem_ids = torch.where(\n                attention_mask[..., -current_len:] < -1,\n                0,\n                torch.cumsum(is_mem, -1) - is_mem.int(),\n            )\n            last_section_mask = torch.amax(mem_ids, -1, keepdim=True) == mem_ids\n            attention_mask[..., -current_len:].masked_fill_(\n                last_section_mask & is_mem,\n                torch.tensor(\n                    torch.finfo(inputs_embeds.dtype).min, device=inputs_embeds.device\n                ),\n            )\n            last_section_mask.logical_and_(attention_mask[..., -current_len:] > -1)\n            is_mem = is_mem.logical_and(attention_mask[..., -current_len:] > -1)\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                LOG.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = (\n                past_key_values[idx] if past_key_values is not None else None\n            )\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                    output_attentions,\n                    None,\n                    is_mem,\n                    last_section_mask,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    is_mem=is_mem,\n                    last_section_mask=last_section_mask,\n                    offload_cache_to_cpu=offload_cache_to_cpu,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n                if v is not None\n            )\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )", "\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    \"\"\"\n    Llama model with a causal language modeling head.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.mem_id = None\n        self.mem_freq = None\n        self.top_k = None\n        self.max_seq_len = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(\n        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        offload_cache_to_cpu: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        window_len = self.max_seq_len or input_ids.shape[1]\n        last_logits = None\n        for _, idx in enumerate(range(0, input_ids.shape[1], window_len)):\n            if idx >= 1:\n                if output_attentions or output_hidden_states:\n                    raise NotImplementedError\n                if not use_cache:\n                    raise NotImplementedError\n            outputs = self.model(\n                input_ids=input_ids[:, idx : idx + window_len],\n                attention_mask=attention_mask[\n                    :, : idx + window_len + attention_mask.shape[1] - input_ids.shape[1]\n                ]\n                if attention_mask is not None\n                else None,\n                position_ids=position_ids[:, idx : idx + window_len]\n                if position_ids is not None\n                else None,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds[:, idx : idx + window_len]\n                if inputs_embeds is not None\n                else None,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                offload_cache_to_cpu=offload_cache_to_cpu,\n            )\n            past_key_values = outputs.past_key_values\n            if last_logits is not None:\n                last_logits = torch.cat((last_logits, outputs[0]), dim=-2)\n            last_logits = outputs[0]\n\n        hidden_states = last_logits\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_mem_id(self, mem_id):\n        self.mem_id = mem_id\n        self.model.set_mem_id(mem_id)\n\n    def set_mem_cache_args(self, max_seq_len, mem_freq, top_k, max_cache_size):\n        self.mem_freq = mem_freq\n        self.top_k = top_k\n        self.max_seq_len = max_seq_len\n        if self.max_seq_len is not None:\n            assert self.max_seq_len % (self.mem_freq + 1) == 0\n        self.model.set_mem_cache_args(mem_freq, top_k, max_cache_size)\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        **kwargs,\n    ):\n        total_len = input_ids.shape[1]\n        if past_key_values:\n            prev_len = input_ids.shape[1] - 1\n        else:\n            prev_len = 0\n\n        position_ids = kwargs.get(\"position_ids\", None)\n\n        if self.mem_freq is not None:\n            if position_ids is not None:\n                raise NotImplementedError\n            # T = input_ids.shape[1]\n\n            prev_incomplete_len = prev_len % self.mem_freq\n            prev_complete_len = prev_len - prev_incomplete_len\n            incomplete_len = total_len % self.mem_freq\n            new_full_len = total_len - prev_complete_len - incomplete_len\n\n            prev_input, input_ids_with_mem, input_ids_without_mem = torch.split(\n                input_ids, (prev_complete_len, new_full_len, incomplete_len), dim=-1\n            )\n\n            bsz, _ = input_ids.size()\n            input_ids_with_mem = input_ids_with_mem.view(bsz, -1, self.mem_freq)\n            input_ids_with_mem = torch.cat(\n                (\n                    input_ids_with_mem,\n                    input_ids_with_mem.new_full(\n                        (bsz, input_ids_with_mem.shape[1], 1), self.mem_id\n                    ),\n                ),\n                dim=-1,\n            ).view(bsz, -1)\n            input_ids = torch.cat(\n                (prev_input, input_ids_with_mem, input_ids_without_mem), dim=-1\n            )\n            if attention_mask is not None:\n                attention_mask_with_mem, attention_mask_without_mem = torch.split(\n                    attention_mask,\n                    (prev_complete_len + new_full_len, incomplete_len),\n                    dim=-1,\n                )\n                attention_mask_with_mem = attention_mask_with_mem.view(\n                    bsz, -1, self.mem_freq\n                )\n                attention_mask_with_mem = torch.cat(\n                    (\n                        attention_mask_with_mem,\n                        attention_mask_with_mem.new_ones(\n                            (bsz, attention_mask_with_mem.shape[1], 1)\n                        ),\n                    ),\n                    dim=-1,\n                ).view(bsz, -1)\n                attention_mask = torch.cat(\n                    (attention_mask_with_mem, attention_mask_without_mem), dim=-1\n                )\n\n        input_ids = input_ids[:, prev_len:]\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            position_ids = position_ids[:, -input_ids.shape[1] :].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if (\n            inputs_embeds is not None\n            and past_key_values is None\n            and self.mem_freq is None\n        ):\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"offload_cache_to_cpu\": kwargs.get(\"offload_cache_to_cpu\"),\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(\n                    past_state.index_select(0, beam_idx) for past_state in layer_past\n                ),\n            )\n        return reordered_past", "\n\ndef add_mem_tokens(example, mem_freq, mem_id):\n    ids = example[\"input_ids\"]\n    ret = []\n    prev_idx = 0\n    for t_idx in range(mem_freq, len(ids), mem_freq):\n        ret.extend(ids[prev_idx:t_idx])\n        ret.append(mem_id)\n        prev_idx = t_idx\n    ret.extend(ids[prev_idx:])\n    # drop attention_mask\n    return {\"input_ids\": ret}", "\n\ndef patch_llama_with_landmark_attn():\n    import transformers\n\n    transformers.models.llama.modeling_llama.LlamaForCausalLM = LlamaForCausalLM\n    transformers.models.llama.modeling_llama.LlamaModel = LlamaModel\n    transformers.models.llama.modeling_llama.LlamaAttention = LlamaAttention\n    transformers.models.llama.modeling_llama.LlamaDecoderLayer = LlamaDecoderLayer\n    transformers.models.llama.modeling_llama.apply_rotary_pos_emb = apply_rotary_pos_emb", "\n\ndef set_model_mem_id(model: LlamaForCausalLM, tokenizer: LlamaTokenizer):\n    mem_id = tokenizer.convert_tokens_to_ids(MEM_TOKEN)\n    model.set_mem_id(mem_id)\n\n\ndef get_mem_id(tokenizer: LlamaTokenizer):\n    return tokenizer.convert_tokens_to_ids(MEM_TOKEN)\n", ""]}
{"filename": "src/axolotl/utils/wandb.py", "chunked_list": ["\"\"\"Module for wandb utilities\"\"\"\n\nimport os\n\n\ndef setup_wandb_env_vars(cfg):\n    if cfg.wandb_mode and cfg.wandb_mode == \"offline\":\n        os.environ[\"WANDB_MODE\"] = cfg.wandb_mode\n    elif cfg.wandb_project and len(cfg.wandb_project) > 0:\n        os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n        cfg.use_wandb = True\n        if cfg.wandb_watch and len(cfg.wandb_watch) > 0:\n            os.environ[\"WANDB_WATCH\"] = cfg.wandb_watch\n        if cfg.wandb_log_model and len(cfg.wandb_log_model) > 0:\n            os.environ[\"WANDB_LOG_MODEL\"] = cfg.wandb_log_model\n        if cfg.wandb_run_id and len(cfg.wandb_run_id) > 0:\n            os.environ[\"WANDB_RUN_ID\"] = cfg.wandb_run_id\n    else:\n        os.environ[\"WANDB_DISABLED\"] = \"true\"", ""]}
{"filename": "src/axolotl/utils/models.py", "chunked_list": ["\"\"\"Module for models and model loading\"\"\"\n\n\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional, Tuple  # noqa: F401\n\nimport bitsandbytes as bnb", "\nimport bitsandbytes as bnb\nimport torch\nimport transformers\nfrom optimum.bettertransformer import BetterTransformer\nfrom transformers import (  # noqa: F401\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,", "    AutoTokenizer,\n    BitsAndBytesConfig,\n    LlamaConfig,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n)\n\nfrom axolotl.prompt_tokenizers import LLAMA_DEFAULT_PAD_TOKEN\n\nLOG = logging.getLogger(\"axolotl\")", "\nLOG = logging.getLogger(\"axolotl\")\n\nif TYPE_CHECKING:\n    from peft import PeftConfig  # noqa: F401\n\n    from axolotl.utils.dict import DictDefault  # noqa: F401\n\n\ndef load_tokenizer(\n    tokenizer_config,\n    tokenizer_type,\n    cfg,\n):\n    use_fast = True  # this is the default\n    if cfg.tokenizer_use_fast is not None:\n        use_fast = cfg.tokenizer_use_fast\n    if tokenizer_type:\n        tokenizer = getattr(transformers, tokenizer_type).from_pretrained(\n            tokenizer_config,\n            trust_remote_code=cfg.trust_remote_code or False,\n            use_fast=use_fast,\n        )\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_config,\n            trust_remote_code=cfg.trust_remote_code or False,\n            use_fast=use_fast,\n        )\n\n    LOG.debug(f\"EOS: {tokenizer.eos_token_id} / {tokenizer.eos_token}\")\n    LOG.debug(f\"BOS: {tokenizer.bos_token_id} / {tokenizer.bos_token}\")\n    LOG.debug(f\"PAD: {tokenizer.pad_token_id} / {tokenizer.pad_token}\")\n    LOG.debug(f\"UNK: {tokenizer.unk_token_id} / {tokenizer.unk_token}\")\n\n    if tokenizer.__class__.__name__ in [\n        \"LlamaTokenizer\",\n        \"LlamaTokenizerFast\",\n    ]:\n        tokenizer.pad_token = LLAMA_DEFAULT_PAD_TOKEN\n\n    if tokenizer.__class__.__name__ == \"GPTNeoXTokenizerFast\":\n        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    if cfg.special_tokens:\n        for k, val in cfg.special_tokens.items():\n            tokenizer.add_special_tokens({k: val})\n    if cfg.tokens:\n        tokenizer.add_tokens(list(cfg.tokens))\n\n    return tokenizer", "\ndef load_tokenizer(\n    tokenizer_config,\n    tokenizer_type,\n    cfg,\n):\n    use_fast = True  # this is the default\n    if cfg.tokenizer_use_fast is not None:\n        use_fast = cfg.tokenizer_use_fast\n    if tokenizer_type:\n        tokenizer = getattr(transformers, tokenizer_type).from_pretrained(\n            tokenizer_config,\n            trust_remote_code=cfg.trust_remote_code or False,\n            use_fast=use_fast,\n        )\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_config,\n            trust_remote_code=cfg.trust_remote_code or False,\n            use_fast=use_fast,\n        )\n\n    LOG.debug(f\"EOS: {tokenizer.eos_token_id} / {tokenizer.eos_token}\")\n    LOG.debug(f\"BOS: {tokenizer.bos_token_id} / {tokenizer.bos_token}\")\n    LOG.debug(f\"PAD: {tokenizer.pad_token_id} / {tokenizer.pad_token}\")\n    LOG.debug(f\"UNK: {tokenizer.unk_token_id} / {tokenizer.unk_token}\")\n\n    if tokenizer.__class__.__name__ in [\n        \"LlamaTokenizer\",\n        \"LlamaTokenizerFast\",\n    ]:\n        tokenizer.pad_token = LLAMA_DEFAULT_PAD_TOKEN\n\n    if tokenizer.__class__.__name__ == \"GPTNeoXTokenizerFast\":\n        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    if cfg.special_tokens:\n        for k, val in cfg.special_tokens.items():\n            tokenizer.add_special_tokens({k: val})\n    if cfg.tokens:\n        tokenizer.add_tokens(list(cfg.tokens))\n\n    return tokenizer", "\n\ndef load_model(\n    base_model, base_model_config, model_type, tokenizer, cfg, adapter=\"lora\"\n):\n    # type: (str, str, str, PreTrainedTokenizerBase, DictDefault, Optional[str]) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n    \"\"\"\n    Load a model from a base model and a model type.\n    \"\"\"\n\n    # TODO refactor as a kwarg\n    load_in_8bit = cfg.load_in_8bit\n    cfg.is_llama_derived_model = \"llama\" in base_model or (\n        cfg.model_type and \"llama\" in cfg.model_type.lower()\n    )\n\n    if cfg.is_llama_derived_model and cfg.flash_attention:\n        if cfg.device not in [\"mps\", \"cpu\"] and not cfg.inference:\n            from axolotl.flash_attn import replace_llama_attn_with_flash_attn\n\n            LOG.info(\"patching with flash attention\")\n            replace_llama_attn_with_flash_attn()\n    elif cfg.is_llama_derived_model and cfg.xformers_attention:\n        from axolotl.monkeypatch.llama_attn_hijack_xformers import (\n            hijack_llama_attention,\n        )\n\n        LOG.info(\"patching with xformers attention\")\n        hijack_llama_attention()\n    elif cfg.is_llama_derived_model and cfg.sdp_attention:\n        from axolotl.monkeypatch.llama_attn_hijack_xformers import (\n            hijack_llama_sdp_attention,\n        )\n\n        LOG.info(\"patching with sdp attention\")\n        hijack_llama_sdp_attention()\n    elif cfg.is_llama_derived_model and cfg.landmark_attention:\n        from axolotl.monkeypatch.llama_landmark_attn import (\n            MEM_TOKEN,\n            patch_llama_with_landmark_attn,\n        )\n\n        LOG.info(\"patching with landmark attention\")\n        patch_llama_with_landmark_attn()\n\n        # Note: This might overwrite previous additional_special_tokens\n        tokenizer.add_special_tokens({\"additional_special_tokens\": [MEM_TOKEN]})\n\n    if cfg.is_llama_derived_model and cfg.xpos_rope:\n        from axolotl.monkeypatch.xpos_rope_llama_monkey_patch import (\n            replace_llama_rope_with_xpos_rope,\n        )\n\n        LOG.info(\"patching with xpos rope\")\n        replace_llama_rope_with_xpos_rope()\n\n    if cfg.bf16 or cfg.bfloat16:\n        torch_dtype = torch.bfloat16\n    elif cfg.load_in_8bit or cfg.fp16 or cfg.float16:\n        torch_dtype = torch.float16\n    else:\n        torch_dtype = torch.float32\n    try:\n        if cfg.gptq:\n            from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import (\n                replace_peft_model_with_int4_lora_model,\n            )\n\n            replace_peft_model_with_int4_lora_model()\n    except Exception as err:\n        LOG.exception(err)\n        raise err\n\n    try:\n        from peft import prepare_model_for_kbit_training\n    except ImportError:\n        # For backward compatibility\n        from peft import (\n            prepare_model_for_int8_training as prepare_model_for_kbit_training,\n        )\n\n    model_kwargs = {}\n    if cfg.model_revision:\n        model_kwargs[\"revision\"] = cfg.model_revision\n    if cfg.adapter == \"qlora\" and cfg.load_in_4bit:\n        model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=torch_dtype,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n        )\n    try:\n        if cfg.gptq and cfg.is_llama_derived_model:\n            from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram\n            from huggingface_hub import snapshot_download\n\n            try:\n                snapshot_download_kwargs = {}\n                if cfg.base_model_ignore_patterns:\n                    snapshot_download_kwargs[\n                        \"ignore_patterns\"\n                    ] = cfg.base_model_ignore_patterns\n                cache_model_path = Path(\n                    snapshot_download(base_model, **snapshot_download_kwargs)\n                )\n                files = (\n                    list(cache_model_path.glob(\"*.pt\"))\n                    + list(cache_model_path.glob(\"*.safetensors\"))\n                    + list(cache_model_path.glob(\"*.bin\"))\n                )\n                if len(files) > 0:\n                    model_path = str(files[0])\n                else:\n                    LOG.warning(\n                        \"unable to find a cached model file, this will likely fail...\"\n                    )\n                    model_path = str(cache_model_path)\n            except Exception:  # pylint: disable=broad-exception-caught\n                model_path = cfg.base_model\n            model, _ = load_llama_model_4bit_low_ram(\n                base_model_config if base_model_config else base_model,\n                model_path,\n                device_map=cfg.device_map,\n                half=cfg.fp16,\n                groupsize=cfg.gptq_groupsize if cfg.gptq_groupsize else -1,\n                is_v1_model=cfg.gptq_model_v1\n                if cfg.gptq_model_v1 is not None\n                else True,\n            )\n            load_in_8bit = False\n        elif cfg.is_llama_derived_model and not cfg.trust_remote_code:\n            from transformers import LlamaForCausalLM\n\n            config = LlamaConfig.from_pretrained(base_model_config)\n            model = LlamaForCausalLM.from_pretrained(\n                base_model,\n                config=config,\n                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n                torch_dtype=torch_dtype,\n                device_map=\"auto\" if cfg.world_size == 1 else cfg.device_map,\n                **model_kwargs,\n            )\n        # elif model_type == \"GPTNeoXForCausalLM\" and cfg.flash_attention:\n        #     This is a WIP, still an issue with the backward pass\n        #     RuntimeError: grad can be implicitly created only for scalar outputs\n        #     TODO: try config.sequence_parallel = False\n        #     # https://github.com/HazyResearch/flash-attention/blob/40a25c8ee7465cf547b929cfa2937034e37bfce9/tests/models/test_gpt_neox.py#L12\n        #     # https://github.com/HazyResearch/flash-attention/tree/main/training#model-components\n        #     # add `**kwargs` to https://github.com/HazyResearch/flash-attention/blob/40a25c8ee7465cf547b929cfa2937034e37bfce9/flash_attn/models/gpt.py#L442\n        #     from flash_attn.utils.pretrained import state_dict_from_pretrained\n        #     from flash_attn.models.gpt import GPTLMHeadModel\n        #     from flash_attn.models.gpt_neox import remap_state_dict_hf_gpt_neox, gpt_neox_config_to_gpt2_config\n        #     from transformers import GPTNeoXConfig\n        #     config = gpt_neox_config_to_gpt2_config(GPTNeoXConfig.from_pretrained(base_model))\n        #     config.use_flash_attn = True\n        #     config.fused_bias_fc = True\n        #     config.fused_mlp = True  # GPT-NeoX-20B uses \"gelu_fast\"\n        #     config.activation_function = \"gelu_fast\"\n        #     config.fused_dropout_add_ln = True\n        #     # config.residual_in_fp32 = True\n        #\n        #     model: GPTLMHeadModel = GPTLMHeadModel.from_pretrained(\n        #         base_model,\n        #         config,\n        #         dtype=torch_dtype,\n        #         device=cfg.device,\n        #     )\n        #     model.train() # sets to train instead of eval mode\n        elif model_type and not cfg.trust_remote_code:\n            model = getattr(transformers, model_type).from_pretrained(\n                base_model,\n                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n                torch_dtype=torch_dtype,\n                device_map=cfg.device_map,\n                trust_remote_code=cfg.trust_remote_code or False,\n                **model_kwargs,\n            )\n        else:\n            config = AutoConfig.from_pretrained(\n                base_model,\n                trust_remote_code=cfg.trust_remote_code or False,\n            )\n            # Shouldn't be a problem most of the time. will obviously error if the model doesn't support this\n            # when training starts\n            if (\n                hasattr(config, \"max_seq_len\")\n                and config.max_seq_len\n                and cfg.sequence_len > config.max_seq_len\n            ):\n                config.max_seq_len = cfg.sequence_len\n                LOG.warning(f\"increasing context length to {cfg.sequence_len}\")\n            elif (\n                hasattr(config, \"max_sequence_length\")\n                and config.max_sequence_length\n                and cfg.sequence_len > config.max_sequence_length\n            ):\n                config.max_sequence_length = cfg.sequence_len\n                LOG.warning(f\"increasing context length to {cfg.sequence_len}\")\n            model = AutoModelForCausalLM.from_pretrained(\n                base_model,\n                config=config,\n                load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n                load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n                torch_dtype=torch_dtype,\n                device_map=cfg.device_map,\n                trust_remote_code=cfg.trust_remote_code or False,\n                **model_kwargs,\n            )\n    except Exception as err:  # pylint: disable=broad-exception-caught\n        LOG.error(\n            \"Exception raised attempting to load model, retrying with AutoModelForCausalLM\"\n        )\n        LOG.exception(err)\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model,\n            load_in_8bit=cfg.load_in_8bit and cfg.adapter is not None,\n            load_in_4bit=cfg.load_in_4bit and cfg.adapter is not None,\n            torch_dtype=torch_dtype,\n            device_map=cfg.device_map,\n            trust_remote_code=cfg.trust_remote_code or False,\n            **model_kwargs,\n        )\n\n    embeddings_len = (\n        math.ceil(len(tokenizer) / 32) * 32\n        if cfg.resize_token_embeddings_to_32x\n        else len(tokenizer)\n    )\n    model.resize_token_embeddings(embeddings_len)\n\n    if (\n        hasattr(model.config, \"max_position_embeddings\")\n        and model.config.max_position_embeddings\n        and cfg.sequence_len >= model.config.max_position_embeddings\n    ):\n        LOG.warning(\n            f\"increasing model.config.max_position_embeddings to {cfg.sequence_len}\"\n        )\n        model.config.max_position_embeddings = cfg.sequence_len\n\n    if not cfg.gptq and (\n        (cfg.adapter == \"lora\" and load_in_8bit)\n        or (cfg.adapter == \"qlora\" and cfg.load_in_4bit)\n    ):\n        LOG.info(\"converting PEFT model w/ prepare_model_for_kbit_training\")\n        model = prepare_model_for_kbit_training(\n            model, use_gradient_checkpointing=cfg.gradient_checkpointing\n        )\n\n    model, lora_config = load_adapter(model, cfg, adapter)\n\n    if cfg.ddp and not load_in_8bit:\n        model.to(f\"cuda:{cfg.local_rank}\")\n\n    if cfg.gptq:\n        # Scales to half\n        LOG.info(\"Fitting 4bit scales and zeros to half\")\n        for _, module in model.named_modules():\n            if \"Autograd4bitQuantLinear\" in str(type(module)) or \"Linear4bitLt\" in str(\n                type(module)\n            ):\n                if hasattr(module, \"is_v1_model\") and module.is_v1_model:\n                    module.zeros = module.zeros.half()\n                module.scales = module.scales.half()\n                module.bias = module.bias.half()\n\n    if (\n        torch.cuda.device_count() > 1\n        and int(os.getenv(\"WORLD_SIZE\", \"1\")) > 1\n        and (cfg.gptq or cfg.load_in_4bit)\n    ):\n        # llama is PROBABLY model parallelizable, but the default isn't that it is\n        # so let's only set it for the 4bit, see\n        # https://github.com/johnsmith0031/alpaca_lora_4bit/blob/08b3fca4a4a9e0d3945be1bab4529f100a428636/finetune.py#L130-L133\n        setattr(model, \"is_parallelizable\", True)\n        setattr(model, \"model_parallel\", True)\n\n    requires_grad = []\n    for name, param in model.named_parameters(recurse=True):\n        if param.requires_grad:\n            requires_grad.append(f\"{name}: {param.requires_grad}\")\n    if len(requires_grad) == 0:\n        LOG.warning(\"there are no parameters that require gradient updates\")\n    model.config.use_cache = False\n\n    if cfg.flash_optimum:\n        model = BetterTransformer.transform(model)\n\n    # TODO resume_from_checkpoint handling\n    return model, lora_config", "\n\ndef load_adapter(model, cfg, adapter):\n    # type: (PreTrainedModel, DictDefault, Optional[str]) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n\n    if adapter is None:\n        return model, None\n    if adapter in [\"lora\", \"qlora\"]:\n        return load_lora(model, cfg)\n    if adapter == \"llama-adapter\":\n        return load_llama_adapter(model, cfg)\n\n    raise NotImplementedError(f\"{adapter} peft adapter not available\")", "\n\ndef load_llama_adapter(model, cfg):\n    # type: (PreTrainedModel, DictDefault) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n    from peft import AdaptionPromptConfig, PeftModel, get_peft_model\n\n    peft_config = AdaptionPromptConfig(\n        adapter_layers=cfg.peft_adapter.layers,  # layers (L)\n        adapter_len=cfg.peft_adapter.len,  # prompt length (K)\n        task_type=\"CAUSAL_LM\",\n    )\n\n    if cfg.lora_model_dir:\n        LOG.info(\"Loading pretained LORA\")\n        model = PeftModel.from_pretrained(\n            model,\n            cfg.lora_model_dir,\n            torch_dtype=torch.float16,\n        )\n    else:\n        model = get_peft_model(model, peft_config)\n\n    model.print_trainable_parameters()\n\n    return model, peft_config", "\n\ndef find_all_linear_names(bits, model):\n    cls = (\n        bnb.nn.Linear4bit\n        if bits == 4\n        else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n    )\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split(\".\")\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove(\"lm_head\")\n\n    return list(lora_module_names)", "\n\ndef load_lora(model, cfg):\n    # type: (PreTrainedModel, DictDefault) -> Tuple[PreTrainedModel, Optional[PeftConfig]]\n\n    from peft import LoraConfig, PeftModel, get_peft_model\n\n    lora_target_modules = list(cfg.lora_target_modules or [])\n\n    if cfg.lora_target_linear:\n        bits = None\n        if cfg.load_in_4bit:\n            bits = 4\n        elif cfg.load_in_8bit:\n            bits = 8\n\n        linear_names = find_all_linear_names(bits, model)\n        LOG.info(f\"found linear modules: {repr(linear_names)}\")\n        lora_target_modules = list(set(lora_target_modules + linear_names))\n\n    lora_config = LoraConfig(\n        r=cfg.lora_r,\n        lora_alpha=cfg.lora_alpha,\n        target_modules=lora_target_modules,\n        lora_dropout=cfg.lora_dropout,\n        fan_in_fan_out=cfg.lora_fan_in_fan_out,\n        modules_to_save=cfg.lora_modules_to_save if cfg.lora_modules_to_save else None,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    if cfg.lora_model_dir:\n        model = PeftModel.from_pretrained(\n            model,\n            cfg.lora_model_dir,\n            is_trainable=not cfg.inference,\n        )\n    else:\n        model = get_peft_model(model, lora_config)\n\n    model.print_trainable_parameters()\n\n    return model, lora_config", ""]}
{"filename": "src/axolotl/utils/__init__.py", "chunked_list": [""]}
{"filename": "src/axolotl/utils/dict.py", "chunked_list": ["\"\"\"Module containing the DictDefault class\"\"\"\n\nfrom addict import Dict\n\n\nclass DictDefault(Dict):\n    \"\"\"\n    A Dict that returns None instead of returning empty Dict for missing keys.\n    \"\"\"\n\n    def __missing__(self, key):\n        return None", ""]}
{"filename": "src/axolotl/utils/data.py", "chunked_list": ["\"\"\"Module containing data utilities\"\"\"\nimport functools\nimport logging\nfrom hashlib import md5\nfrom pathlib import Path\nfrom typing import List, Tuple, Union\n\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\nfrom huggingface_hub import hf_hub_download", "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\nfrom huggingface_hub import hf_hub_download\nfrom transformers import PreTrainedTokenizerBase\n\nfrom axolotl.datasets import ConstantLengthDataset, TokenizedPromptDataset\nfrom axolotl.prompt_strategies import load\nfrom axolotl.prompt_tokenizers import (\n    AlpacaMultipleChoicePromptTokenizingStrategy,\n    AlpacaPromptTokenizingStrategy,\n    AlpacaReflectionPTStrategy,", "    AlpacaPromptTokenizingStrategy,\n    AlpacaReflectionPTStrategy,\n    CompletionPromptTokenizingStrategy,\n    GPTeacherPromptTokenizingStrategy,\n    JeopardyPromptTokenizingStrategy,\n    OpenAssistantPromptTokenizingStrategy,\n    ShareGPTPromptTokenizingStrategy,\n    SummarizeTLDRPromptTokenizingStrategy,\n)\nfrom axolotl.prompters import (", ")\nfrom axolotl.prompters import (\n    AlpacaPrompter,\n    CompletionPrompter,\n    GPTeacherPrompter,\n    JeopardyPrompter,\n    MultipleChoiceConcisePrompter,\n    MultipleChoiceExplainPrompter,\n    ReflectAlpacaPrompter,\n    ShareGPTPrompter,", "    ReflectAlpacaPrompter,\n    ShareGPTPrompter,\n    SummarizeTLDRPrompter,\n)\n\nLOG = logging.getLogger(\"axolotl\")\n\n\ndef load_tokenized_prepared_datasets(\n    tokenizer, cfg, default_dataset_prepared_path\n) -> DatasetDict:\n    tokenizer_name = tokenizer.__class__.__name__\n    ds_hash = str(\n        md5(  # nosec\n            (\n                str(cfg.sequence_len)\n                + \"@\"\n                + \"|\".join(\n                    sorted([f\"{d.path}:{d.type}:{d.shards}\" for d in cfg.datasets])\n                )\n                + \"|\"\n                + tokenizer_name\n            ).encode(\"utf-8\")\n        ).hexdigest()\n    )\n    prepared_ds_path = (\n        Path(cfg.dataset_prepared_path) / ds_hash\n        if cfg.dataset_prepared_path\n        else Path(default_dataset_prepared_path) / ds_hash\n    )\n    dataset = None\n    use_auth_token = cfg.hf_use_auth_token\n    try:\n        if cfg.push_dataset_to_hub:\n            dataset = load_dataset(\n                f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n                use_auth_token=use_auth_token,\n            )\n            dataset = dataset[\"train\"]\n    except Exception:  # pylint: disable=broad-except # nosec\n        pass\n\n    if dataset:\n        ...\n    elif any(prepared_ds_path.glob(\"*\")):\n        LOG.info(f\"Loading prepared dataset from disk at {prepared_ds_path}...\")\n        dataset = load_from_disk(str(prepared_ds_path))\n        LOG.info(\"Prepared dataset loaded from disk...\")\n    else:\n        LOG.info(f\"Unable to find prepared dataset in {prepared_ds_path}\")\n        LOG.info(\"Loading raw datasets...\")\n\n        if cfg.seed:\n            seed = cfg.seed\n        else:\n            LOG.info(\"No seed provided, using default seed of 42\")\n            seed = 42\n\n        datasets = []\n        # pylint: disable=invalid-name\n        for d in cfg.datasets:\n            ds: Union[Dataset, DatasetDict] = None\n            ds_from_hub = False\n            try:\n                load_dataset(\n                    d.path,\n                    name=d.name,\n                    streaming=True,\n                    use_auth_token=use_auth_token,\n                )\n                ds_from_hub = True\n            except FileNotFoundError:\n                pass\n\n            # prefer local dataset, even if hub exists\n            local_path = Path(d.path)\n            if local_path.exists():\n                if local_path.is_dir():\n                    ds = load_dataset(\n                        d.path,\n                        name=d.name,\n                        data_files=d.data_files,\n                        streaming=False,\n                        split=None,\n                    )\n                elif local_path.is_file():\n                    ds = load_dataset(\n                        \"json\",\n                        name=d.name,\n                        data_files=d.path,\n                        streaming=False,\n                        split=None,\n                    )\n                else:\n                    raise ValueError(\n                        \"unhandled dataset load: local path exists, but is neither a directory or a file\"\n                    )\n            elif ds_from_hub:\n                ds = load_dataset(\n                    d.path,\n                    name=d.name,\n                    streaming=False,\n                    data_files=d.data_files,\n                    use_auth_token=use_auth_token,\n                )\n            else:\n                fp = hf_hub_download(\n                    repo_id=d.path,\n                    repo_type=\"dataset\",\n                    filename=d.data_files,\n                )\n                ds = load_dataset(\n                    \"json\", name=d.name, data_files=fp, streaming=False, split=None\n                )\n            if not ds:\n                raise ValueError(\"unhandled dataset load\")\n            # support for using a subset of the data\n            if d.shards:\n                if \"train\" in ds:\n                    ds = ds.shuffle(seed=seed)[\"train\"].shard(\n                        num_shards=d.shards, index=0\n                    )\n                else:\n                    ds = ds.shuffle(seed=seed).shard(num_shards=d.shards, index=0)\n            d_type = d.type\n            d_type_split = d_type.split(\":\")\n            d_base_type = d_type_split[0]\n            d_prompt_style = d_type_split[1] if len(d_type_split) > 1 else None\n            if \"train\" in ds:\n                ds = ds[\"train\"]\n            if ds_strategy := load(d.type, tokenizer, cfg):\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"alpaca\":\n                ds_strategy = AlpacaPromptTokenizingStrategy(\n                    AlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"explainchoice\":\n                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n                    MultipleChoiceExplainPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"concisechoice\":\n                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n                    MultipleChoiceConcisePrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"summarizetldr\":\n                ds_strategy = SummarizeTLDRPromptTokenizingStrategy(\n                    SummarizeTLDRPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"jeopardy\":\n                ds_strategy = JeopardyPromptTokenizingStrategy(\n                    JeopardyPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"oasst\":\n                ds_strategy = OpenAssistantPromptTokenizingStrategy(\n                    AlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"gpteacher\":\n                ds_strategy = GPTeacherPromptTokenizingStrategy(\n                    GPTeacherPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"reflection\":\n                ds_strategy = AlpacaReflectionPTStrategy(\n                    ReflectAlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"sharegpt\":\n                ds_strategy = ShareGPTPromptTokenizingStrategy(\n                    ShareGPTPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"completion\":\n                ds_strategy = CompletionPromptTokenizingStrategy(\n                    CompletionPrompter(),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            else:\n                suffix = \"\"\n                if \":load_\" in d.type:\n                    suffix = f\" Did you mean {d.type.replace(':load_', '.load_')}?\"\n                LOG.error(f\"unhandled prompt tokenization strategy: {d.type}. {suffix}\")\n                raise ValueError(\n                    f\"unhandled prompt tokenization strategy: {d.type} {suffix}\"\n                )\n        LOG.info(\"tokenizing, merging, and shuffling master dataset\")\n\n        samples: List[int] = []\n        for d in datasets:\n            samples = samples + list(d)\n        dataset = Dataset.from_list(samples).shuffle(seed=seed)\n        if cfg.local_rank == 0:\n            LOG.info(f\"Saving merged prepared dataset to disk... {prepared_ds_path}\")\n            dataset.save_to_disk(prepared_ds_path)\n            if cfg.push_dataset_to_hub:\n                LOG.info(\n                    f\"Saving merged prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n                )\n                dataset.push_to_hub(\n                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\", private=True\n                )\n\n    return dataset", "def load_tokenized_prepared_datasets(\n    tokenizer, cfg, default_dataset_prepared_path\n) -> DatasetDict:\n    tokenizer_name = tokenizer.__class__.__name__\n    ds_hash = str(\n        md5(  # nosec\n            (\n                str(cfg.sequence_len)\n                + \"@\"\n                + \"|\".join(\n                    sorted([f\"{d.path}:{d.type}:{d.shards}\" for d in cfg.datasets])\n                )\n                + \"|\"\n                + tokenizer_name\n            ).encode(\"utf-8\")\n        ).hexdigest()\n    )\n    prepared_ds_path = (\n        Path(cfg.dataset_prepared_path) / ds_hash\n        if cfg.dataset_prepared_path\n        else Path(default_dataset_prepared_path) / ds_hash\n    )\n    dataset = None\n    use_auth_token = cfg.hf_use_auth_token\n    try:\n        if cfg.push_dataset_to_hub:\n            dataset = load_dataset(\n                f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n                use_auth_token=use_auth_token,\n            )\n            dataset = dataset[\"train\"]\n    except Exception:  # pylint: disable=broad-except # nosec\n        pass\n\n    if dataset:\n        ...\n    elif any(prepared_ds_path.glob(\"*\")):\n        LOG.info(f\"Loading prepared dataset from disk at {prepared_ds_path}...\")\n        dataset = load_from_disk(str(prepared_ds_path))\n        LOG.info(\"Prepared dataset loaded from disk...\")\n    else:\n        LOG.info(f\"Unable to find prepared dataset in {prepared_ds_path}\")\n        LOG.info(\"Loading raw datasets...\")\n\n        if cfg.seed:\n            seed = cfg.seed\n        else:\n            LOG.info(\"No seed provided, using default seed of 42\")\n            seed = 42\n\n        datasets = []\n        # pylint: disable=invalid-name\n        for d in cfg.datasets:\n            ds: Union[Dataset, DatasetDict] = None\n            ds_from_hub = False\n            try:\n                load_dataset(\n                    d.path,\n                    name=d.name,\n                    streaming=True,\n                    use_auth_token=use_auth_token,\n                )\n                ds_from_hub = True\n            except FileNotFoundError:\n                pass\n\n            # prefer local dataset, even if hub exists\n            local_path = Path(d.path)\n            if local_path.exists():\n                if local_path.is_dir():\n                    ds = load_dataset(\n                        d.path,\n                        name=d.name,\n                        data_files=d.data_files,\n                        streaming=False,\n                        split=None,\n                    )\n                elif local_path.is_file():\n                    ds = load_dataset(\n                        \"json\",\n                        name=d.name,\n                        data_files=d.path,\n                        streaming=False,\n                        split=None,\n                    )\n                else:\n                    raise ValueError(\n                        \"unhandled dataset load: local path exists, but is neither a directory or a file\"\n                    )\n            elif ds_from_hub:\n                ds = load_dataset(\n                    d.path,\n                    name=d.name,\n                    streaming=False,\n                    data_files=d.data_files,\n                    use_auth_token=use_auth_token,\n                )\n            else:\n                fp = hf_hub_download(\n                    repo_id=d.path,\n                    repo_type=\"dataset\",\n                    filename=d.data_files,\n                )\n                ds = load_dataset(\n                    \"json\", name=d.name, data_files=fp, streaming=False, split=None\n                )\n            if not ds:\n                raise ValueError(\"unhandled dataset load\")\n            # support for using a subset of the data\n            if d.shards:\n                if \"train\" in ds:\n                    ds = ds.shuffle(seed=seed)[\"train\"].shard(\n                        num_shards=d.shards, index=0\n                    )\n                else:\n                    ds = ds.shuffle(seed=seed).shard(num_shards=d.shards, index=0)\n            d_type = d.type\n            d_type_split = d_type.split(\":\")\n            d_base_type = d_type_split[0]\n            d_prompt_style = d_type_split[1] if len(d_type_split) > 1 else None\n            if \"train\" in ds:\n                ds = ds[\"train\"]\n            if ds_strategy := load(d.type, tokenizer, cfg):\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"alpaca\":\n                ds_strategy = AlpacaPromptTokenizingStrategy(\n                    AlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"explainchoice\":\n                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n                    MultipleChoiceExplainPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"concisechoice\":\n                ds_strategy = AlpacaMultipleChoicePromptTokenizingStrategy(\n                    MultipleChoiceConcisePrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"summarizetldr\":\n                ds_strategy = SummarizeTLDRPromptTokenizingStrategy(\n                    SummarizeTLDRPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"jeopardy\":\n                ds_strategy = JeopardyPromptTokenizingStrategy(\n                    JeopardyPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"oasst\":\n                ds_strategy = OpenAssistantPromptTokenizingStrategy(\n                    AlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"gpteacher\":\n                ds_strategy = GPTeacherPromptTokenizingStrategy(\n                    GPTeacherPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"reflection\":\n                ds_strategy = AlpacaReflectionPTStrategy(\n                    ReflectAlpacaPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"sharegpt\":\n                ds_strategy = ShareGPTPromptTokenizingStrategy(\n                    ShareGPTPrompter(d_prompt_style),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            elif d_base_type == \"completion\":\n                ds_strategy = CompletionPromptTokenizingStrategy(\n                    CompletionPrompter(),\n                    tokenizer,\n                    cfg.train_on_inputs,\n                    cfg.sequence_len,\n                )\n                ds_wrapper = TokenizedPromptDataset(ds_strategy, ds)\n                datasets.append(ds_wrapper)\n            else:\n                suffix = \"\"\n                if \":load_\" in d.type:\n                    suffix = f\" Did you mean {d.type.replace(':load_', '.load_')}?\"\n                LOG.error(f\"unhandled prompt tokenization strategy: {d.type}. {suffix}\")\n                raise ValueError(\n                    f\"unhandled prompt tokenization strategy: {d.type} {suffix}\"\n                )\n        LOG.info(\"tokenizing, merging, and shuffling master dataset\")\n\n        samples: List[int] = []\n        for d in datasets:\n            samples = samples + list(d)\n        dataset = Dataset.from_list(samples).shuffle(seed=seed)\n        if cfg.local_rank == 0:\n            LOG.info(f\"Saving merged prepared dataset to disk... {prepared_ds_path}\")\n            dataset.save_to_disk(prepared_ds_path)\n            if cfg.push_dataset_to_hub:\n                LOG.info(\n                    f\"Saving merged prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n                )\n                dataset.push_to_hub(\n                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\", private=True\n                )\n\n    return dataset", "\n\ndef load_prepare_datasets(\n    tokenizer: PreTrainedTokenizerBase,\n    cfg,\n    default_dataset_prepared_path,\n) -> Tuple[Dataset, Dataset]:\n    max_packed_sequence_len = (\n        cfg.max_packed_sequence_len if cfg.max_packed_sequence_len else cfg.sequence_len\n    )\n    max_packed_sequence_len = min(\n        max_packed_sequence_len, cfg.sequence_len\n    )  # make sure we don't accidentally set it larger than sequence_len\n\n    tokenizer_name = tokenizer.__class__.__name__\n    if cfg.max_packed_sequence_len is not None:\n        # see if we can go ahead and load the stacked dataset\n        seed = f\"@{str(cfg.seed)}\" if cfg.seed else \"\"\n        ds_hash = str(\n            md5(  # nosec\n                (\n                    str(cfg.sequence_len)\n                    + \"@\"\n                    + str(max_packed_sequence_len)\n                    + seed\n                    + \"|\".join(\n                        sorted([f\"{d.path}:{d.type}:{d.shards}\" for d in cfg.datasets])\n                    )\n                    + \"|\"\n                    + tokenizer_name\n                ).encode(\"utf-8\")\n            ).hexdigest()\n        )\n        prepared_ds_path = (\n            Path(cfg.dataset_prepared_path) / ds_hash\n            if cfg.dataset_prepared_path\n            else Path(default_dataset_prepared_path) / ds_hash\n        )\n\n        dataset = None\n        use_auth_token = cfg.hf_use_auth_token\n        try:\n            if cfg.push_dataset_to_hub:\n                LOG.info(\n                    f\"Checking for packed prepared dataset from hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n                )\n                dataset = load_dataset(\n                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n                    use_auth_token=use_auth_token,\n                )\n                dataset = dataset[\"train\"]\n        except Exception:  # pylint: disable=broad-except # nosec\n            pass\n\n        if dataset:\n            ...\n        elif any(prepared_ds_path.glob(\"*\")):\n            LOG.info(\n                f\"Loading prepared packed dataset from disk at {prepared_ds_path}...\"\n            )\n            dataset = load_from_disk(str(prepared_ds_path))\n            LOG.info(\"Prepared packed dataset loaded from disk...\")\n            if cfg.push_dataset_to_hub:\n                LOG.info(\n                    f\"Saving packed prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n                )\n                dataset.push_to_hub(\n                    f\"{cfg.push_dataset_to_hub}/{ds_hash}\", private=True\n                )\n        else:\n            dataset = load_tokenized_prepared_datasets(\n                tokenizer, cfg, default_dataset_prepared_path\n            )\n\n            if cfg.seed:\n                dataset = dataset.shuffle(seed=cfg.seed)\n\n            constant_len_dataset = ConstantLengthDataset(\n                tokenizer,\n                [dataset],\n                seq_length=max_packed_sequence_len,\n            )\n            LOG.info(f\"packing master dataset to len: {cfg.max_packed_sequence_len}\")\n            dataset = Dataset.from_list(list(constant_len_dataset))\n\n            # filter out bad data\n            dataset = Dataset.from_list(\n                [\n                    d\n                    for d in dataset\n                    if len(d[\"input_ids\"]) < cfg.sequence_len\n                    and len(d[\"input_ids\"]) > 0\n                    and len(d[\"input_ids\"]) == len(d[\"attention_mask\"])\n                    and len(d[\"input_ids\"]) == len(d[\"labels\"])\n                ]\n            )\n\n            if cfg.local_rank == 0:\n                LOG.info(\n                    f\"Saving packed prepared dataset to disk... {prepared_ds_path}\"\n                )\n                dataset.save_to_disk(prepared_ds_path)\n                if cfg.push_dataset_to_hub:\n                    LOG.info(\n                        f\"Saving packed prepared dataset with push_to_hub... {cfg.push_dataset_to_hub}/{ds_hash}\"\n                    )\n                    dataset.push_to_hub(\n                        f\"{cfg.push_dataset_to_hub}/{ds_hash}\",\n                        private=True,\n                    )\n    else:\n        dataset = load_tokenized_prepared_datasets(\n            tokenizer, cfg, default_dataset_prepared_path\n        )\n\n    if cfg.dataset_shard_num and cfg.dataset_shard_idx is not None:\n        LOG.info(\n            f\"Using index #{cfg.dataset_shard_idx} of {cfg.dataset_shard_num} shards\"\n        )\n        dataset = dataset.shard(\n            num_shards=cfg.dataset_shard_num,\n            index=cfg.dataset_shard_idx,\n        )\n\n    if cfg.val_set_size:\n        dataset = dataset.train_test_split(test_size=cfg.val_set_size, shuffle=False)\n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n    else:\n        train_dataset = dataset\n        eval_dataset = None\n\n    return train_dataset, eval_dataset", "\n\ndef encode_pretraining(tokenizer, max_tokens, examples):\n    res = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=max_tokens - 2,\n        add_special_tokens=True,\n    )\n    # Convert to PyTorch tensors\n    input_ids = [torch.tensor(seq) for seq in res[\"input_ids\"]]\n    attention_mask = [torch.tensor(seq) for seq in res[\"attention_mask\"]]\n    new_input_ids = []\n    new_attention_mask = []\n    # Append EOS and PAD tokens to input_ids, and correct attention_mask\n    for i, _ in enumerate(input_ids):\n        input_ids[i] = torch.cat(\n            (\n                input_ids[i],\n                torch.tensor([tokenizer.eos_token_id, tokenizer.pad_token_id]),\n            ),\n            dim=0,\n        )\n        attention_mask[i] = torch.cat((attention_mask[i], torch.tensor([1, 0])), dim=0)\n\n    # Concatenate tokens so that their lengths are less than max_tokens\n    buffer_input_ids = torch.tensor([], dtype=torch.long)\n    buffer_attention_mask = torch.tensor([], dtype=torch.long)\n\n    for ids, mask in zip(input_ids, attention_mask):\n        if buffer_input_ids.numel() == max_tokens:\n            new_input_ids.append(buffer_input_ids)\n            new_attention_mask.append(buffer_attention_mask)\n            buffer_input_ids = torch.tensor([], dtype=torch.long)\n            buffer_attention_mask = torch.tensor([], dtype=torch.long)\n            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n        elif buffer_input_ids.numel() + ids.numel() <= max_tokens:\n            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n        else:\n            buffer_input_ids = torch.cat(\n                (\n                    buffer_input_ids,\n                    torch.full(\n                        (max_tokens - buffer_input_ids.numel(),),\n                        tokenizer.pad_token_id,\n                        dtype=torch.long,\n                    ),\n                ),\n                dim=0,\n            )\n            buffer_attention_mask = torch.cat(\n                (\n                    buffer_attention_mask,\n                    torch.full(\n                        (max_tokens - buffer_attention_mask.numel(),),\n                        0,\n                        dtype=torch.long,\n                    ),\n                ),\n                dim=0,\n            )\n            new_input_ids.append(buffer_input_ids)\n            new_attention_mask.append(buffer_attention_mask)\n            buffer_input_ids = torch.tensor([], dtype=torch.long)\n            buffer_attention_mask = torch.tensor([], dtype=torch.long)\n\n            buffer_input_ids = torch.cat((buffer_input_ids, ids), dim=0)\n            buffer_attention_mask = torch.cat((buffer_attention_mask, mask), dim=0)\n\n    if buffer_input_ids.numel() > 0:  # for any leftover tokens\n        while buffer_input_ids.numel() < max_tokens:  # make all sequences equal in size\n            buffer_input_ids = torch.cat(\n                (\n                    buffer_input_ids,\n                    torch.full(\n                        (max_tokens - buffer_input_ids.numel(),),\n                        tokenizer.pad_token_id,\n                        dtype=torch.long,\n                    ),\n                ),\n                dim=0,\n            )\n            buffer_attention_mask = torch.cat(\n                (\n                    buffer_attention_mask,\n                    torch.full(\n                        (max_tokens - buffer_attention_mask.numel(),),\n                        0,\n                        dtype=torch.long,\n                    ),\n                ),\n                dim=0,\n            )\n        new_input_ids.append(buffer_input_ids)\n        new_attention_mask.append(buffer_attention_mask)\n\n    ret = {\n        \"input_ids\": [seq.tolist() for seq in new_input_ids],\n        \"labels\": [seq.tolist() for seq in new_input_ids],\n        \"attention_mask\": [seq.tolist() for seq in new_attention_mask],\n    }\n\n    LOG.debug(len(ret[\"input_ids\"]))\n    return ret", "\n\ndef load_pretraining_dataset(path, tokenizer, max_tokens=2048, seed=42):\n    encode = functools.partial(encode_pretraining, tokenizer, max_tokens)\n    dataset = load_dataset(path, streaming=True, split=\"train\")\n    dataset = dataset.shuffle(seed=seed, buffer_size=10_000)\n    # TODO dynamically figure out which columns/features to remove\n    dataset = dataset.map(encode, batched=True, remove_columns=[\"text\", \"meta\"])\n    return dataset\n", ""]}
{"filename": "src/axolotl/utils/tokenization.py", "chunked_list": ["\"\"\"Module for tokenization utilities\"\"\"\n\n\nimport logging\n\nfrom termcolor import colored\n\nLOG = logging.getLogger(\"axolotl\")\n\n\ndef check_dataset_labels(dataset, tokenizer):\n    # the dataset is already shuffled, so let's just check the first 5 elements\n    for idx in range(5):\n        check_example_labels(dataset[idx], tokenizer)", "\n\ndef check_dataset_labels(dataset, tokenizer):\n    # the dataset is already shuffled, so let's just check the first 5 elements\n    for idx in range(5):\n        check_example_labels(dataset[idx], tokenizer)\n\n\ndef check_example_labels(example, tokenizer):\n    # Get the input_ids, labels, and attention_mask from the dataset\n    input_ids = example[\"input_ids\"]\n    labels = example[\"labels\"]\n    attention_mask = example[\"attention_mask\"]\n\n    # You can compare the input_ids and labels element-wise\n    # Remember to ignore positions with IGNORE_TOKEN_ID (if you use it) or attention_mask equal to 0\n    colored_tokens = []\n    for _, (input_id, label_id, mask) in enumerate(\n        zip(input_ids, labels, attention_mask)\n    ):\n        decoded_input_token = tokenizer.decode(input_id)\n        # Choose the color based on whether the label has the ignore value or not\n        color = \"red\" if label_id == -100 else (\"yellow\" if label_id == 0 else \"green\")\n        colored_token = colored(decoded_input_token, color) + colored(\n            f\"({label_id}, {mask}, {input_id})\", \"white\"\n        )\n        colored_tokens.append(colored_token)\n\n    LOG.info(\" \".join(colored_tokens))\n    LOG.info(\"\\n\\n\\n\")\n\n    return \" \".join(colored_tokens)", "def check_example_labels(example, tokenizer):\n    # Get the input_ids, labels, and attention_mask from the dataset\n    input_ids = example[\"input_ids\"]\n    labels = example[\"labels\"]\n    attention_mask = example[\"attention_mask\"]\n\n    # You can compare the input_ids and labels element-wise\n    # Remember to ignore positions with IGNORE_TOKEN_ID (if you use it) or attention_mask equal to 0\n    colored_tokens = []\n    for _, (input_id, label_id, mask) in enumerate(\n        zip(input_ids, labels, attention_mask)\n    ):\n        decoded_input_token = tokenizer.decode(input_id)\n        # Choose the color based on whether the label has the ignore value or not\n        color = \"red\" if label_id == -100 else (\"yellow\" if label_id == 0 else \"green\")\n        colored_token = colored(decoded_input_token, color) + colored(\n            f\"({label_id}, {mask}, {input_id})\", \"white\"\n        )\n        colored_tokens.append(colored_token)\n\n    LOG.info(\" \".join(colored_tokens))\n    LOG.info(\"\\n\\n\\n\")\n\n    return \" \".join(colored_tokens)", ""]}
{"filename": "src/axolotl/utils/schedulers.py", "chunked_list": ["\"\"\"Module for custom LRScheduler class\"\"\"\nimport math\nfrom functools import partial\n\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR, LRScheduler\n\n\nclass InterpolatingLogScheduler(LRScheduler):\n    \"\"\"\n    A scheduler that interpolates learning rates in a logarithmic fashion\n    \"\"\"\n\n    def __init__(self, optimizer, num_steps, min_lr, max_lr, last_epoch=-1):\n        \"\"\"A scheduler that interpolates learning rates in a logarithmic fashion\n\n        Args:\n        - optimizer: pytorch optimizer\n        - num_steps: int, the number of steps over which to increase from the min_lr to the max_lr\n        - min_lr: float, the minimum learning rate\n        - max_lr: float, the maximum learning rate\n\n        Usage:\n            fc = nn.Linear(1,1)\n            optimizer = optim.Adam(fc.parameters())\n            lr_scheduler = InterpolatingLogScheduler(optimizer, num_steps=400, min_lr=1e-6, max_lr=1e-4)\n        \"\"\"\n        self.num_steps = num_steps\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.q = (max_lr / min_lr) ** (  # pylint: disable=invalid-name\n            1 / (num_steps - 1)\n        )\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch <= 0:\n            lrs = [self.min_lr for base_lr in self.base_lrs]\n        elif self.last_epoch < self.num_steps:\n            lrs = [\n                self.min_lr * (self.q ** (self.last_epoch - 1))\n                for base_lr in self.base_lrs\n            ]\n        else:\n            lrs = [self.max_lr for base_lr in self.base_lrs]\n\n        return lrs", "class InterpolatingLogScheduler(LRScheduler):\n    \"\"\"\n    A scheduler that interpolates learning rates in a logarithmic fashion\n    \"\"\"\n\n    def __init__(self, optimizer, num_steps, min_lr, max_lr, last_epoch=-1):\n        \"\"\"A scheduler that interpolates learning rates in a logarithmic fashion\n\n        Args:\n        - optimizer: pytorch optimizer\n        - num_steps: int, the number of steps over which to increase from the min_lr to the max_lr\n        - min_lr: float, the minimum learning rate\n        - max_lr: float, the maximum learning rate\n\n        Usage:\n            fc = nn.Linear(1,1)\n            optimizer = optim.Adam(fc.parameters())\n            lr_scheduler = InterpolatingLogScheduler(optimizer, num_steps=400, min_lr=1e-6, max_lr=1e-4)\n        \"\"\"\n        self.num_steps = num_steps\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.q = (max_lr / min_lr) ** (  # pylint: disable=invalid-name\n            1 / (num_steps - 1)\n        )\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch <= 0:\n            lrs = [self.min_lr for base_lr in self.base_lrs]\n        elif self.last_epoch < self.num_steps:\n            lrs = [\n                self.min_lr * (self.q ** (self.last_epoch - 1))\n                for base_lr in self.base_lrs\n            ]\n        else:\n            lrs = [self.max_lr for base_lr in self.base_lrs]\n\n        return lrs", "\n\ndef _get_cosine_schedule_with_quadratic_warmup_lr_lambda(\n    current_step: int,\n    *,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float\n):\n    if current_step < num_warmup_steps:\n        return (float(current_step) / float(max(1, num_warmup_steps))) ** 2\n    progress = float(current_step - num_warmup_steps) / float(\n        max(1, num_training_steps - num_warmup_steps)\n    )\n    return max(\n        0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n    )", "\n\ndef get_cosine_schedule_with_quadratic_warmup(\n    optimizer: Optimizer,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n):\n    \"\"\"\n    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n    initial lr set in the optimizer.\n\n    Args:\n        optimizer ([`~torch.optim.Optimizer`]):\n            The optimizer for which to schedule the learning rate.\n        num_warmup_steps (`int`):\n            The number of steps for the warmup phase.\n        num_training_steps (`int`):\n            The total number of training steps.\n        num_cycles (`float`, *optional*, defaults to 0.5):\n            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n            following a half-cosine).\n        last_epoch (`int`, *optional*, defaults to -1):\n            The index of the last epoch when resuming training.\n\n    Return:\n        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n    \"\"\"\n\n    lr_lambda = partial(\n        _get_cosine_schedule_with_quadratic_warmup_lr_lambda,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n        num_cycles=num_cycles,\n    )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)", ""]}
{"filename": "src/axolotl/utils/validation.py", "chunked_list": ["\"\"\"Module for validating config files\"\"\"\n\nimport logging\n\nimport torch\n\nLOG = logging.getLogger(\"axolotl\")\n\n\ndef validate_config(cfg):\n    if cfg.gradient_accumulation_steps and cfg.batch_size:\n        raise ValueError(\n            \"please set only one of gradient_accumulation_steps or batch_size\"\n        )\n    if cfg.batch_size:\n        LOG.warning(\n            \"%s\\n%s\",\n            \"batch_size is not recommended. Please use gradient_accumulation_steps instead.\",\n            \"To calculate the equivalent gradient_accumulation_steps, divide batch_size / micro_batch_size / number of gpus.\",\n        )\n    if cfg.load_4bit:\n        raise ValueError(\n            \"cfg.load_4bit parameter has been deprecated and replaced by cfg.gptq\"\n        )\n\n    if cfg.adapter == \"qlora\":\n        if cfg.merge_lora:\n            # can't merge qlora if loaded in 8bit or 4bit\n            if cfg.load_in_8bit:\n                raise ValueError(\"Can't merge qlora if loaded in 8bit\")\n\n            if cfg.gptq:\n                raise ValueError(\"Can't merge qlora if gptq\")\n\n            if cfg.load_in_4bit:\n                raise ValueError(\"Can't merge qlora if loaded in 4bit\")\n\n        else:\n            if cfg.load_in_8bit:\n                raise ValueError(\"Can't load qlora in 8bit\")\n\n            if cfg.gptq:\n                raise ValueError(\"Can't load qlora if gptq\")\n\n            if not cfg.load_in_4bit:\n                raise ValueError(\"Require cfg.load_in_4bit to be True for qlora\")\n\n    if not cfg.load_in_8bit and cfg.adapter == \"lora\":\n        LOG.warning(\"We recommend setting `load_in_8bit: true` for LORA finetuning\")\n\n    if cfg.trust_remote_code:\n        LOG.warning(\n            \"`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\"\n        )\n\n    if cfg.push_dataset_to_hub and cfg.hf_use_auth_token is not True:\n        raise ValueError(\n            \"Require cfg.hf_use_auth_token to be True for push_dataset_to_hub\"\n        )\n\n    if (cfg.base_model and \"falcon\" in cfg.base_model.lower()) and cfg.fsdp:\n        raise ValueError(\"FSDP is not supported for falcon models\")\n\n    if (\n        cfg.base_model and \"mpt\" in cfg.base_model.lower()\n    ) and cfg.gradient_checkpointing:\n        raise ValueError(\"gradient_checkpointing is not supported for MPT models\")\n\n    if cfg.flash_optimum is True:\n        if cfg.adapter:\n            LOG.warning(\"BetterTransformers probably doesn't work with PEFT adapters\")\n        if cfg.fp16 or cfg.bf16:\n            raise ValueError(\"AMP is not supported with BetterTransformer\")\n        if cfg.float16 is not True and cfg.bloat16 is not True:\n            LOG.warning(\n                \"You should probably set bfloat16 or float16 to true to \"\n                \"load the model in float16 for BetterTransformers\"\n            )\n        if int(torch.__version__.split(\".\")[0]) < 2:\n            LOG.warning(\"torch>=2.0.0 required\")\n            raise ValueError(\n                f\"flash_optimum for BetterTransformers may not be used with {torch.__version__}\"\n            )\n\n    if cfg.pretraining_dataset and cfg.group_by_length:\n        LOG.warning(\n            \"You probably want to disable group_by_length as it will force a streamed dataset to download completely.\"\n        )\n\n    if any([cfg.adam_beta1, cfg.adam_beta2, cfg.adam_epsilon]) and (\n        not cfg.optimizer or \"adamw\" not in cfg.optimizer\n    ):\n        LOG.warning(\"adamw hyperparameters found, but no adamw optimizer set\")\n\n    if cfg.push_to_hub_model_id:\n        raise ValueError(\n            \"push_to_hub_model_id is deprecated. Please use hub_model_id instead.\"\n        )", "\ndef validate_config(cfg):\n    if cfg.gradient_accumulation_steps and cfg.batch_size:\n        raise ValueError(\n            \"please set only one of gradient_accumulation_steps or batch_size\"\n        )\n    if cfg.batch_size:\n        LOG.warning(\n            \"%s\\n%s\",\n            \"batch_size is not recommended. Please use gradient_accumulation_steps instead.\",\n            \"To calculate the equivalent gradient_accumulation_steps, divide batch_size / micro_batch_size / number of gpus.\",\n        )\n    if cfg.load_4bit:\n        raise ValueError(\n            \"cfg.load_4bit parameter has been deprecated and replaced by cfg.gptq\"\n        )\n\n    if cfg.adapter == \"qlora\":\n        if cfg.merge_lora:\n            # can't merge qlora if loaded in 8bit or 4bit\n            if cfg.load_in_8bit:\n                raise ValueError(\"Can't merge qlora if loaded in 8bit\")\n\n            if cfg.gptq:\n                raise ValueError(\"Can't merge qlora if gptq\")\n\n            if cfg.load_in_4bit:\n                raise ValueError(\"Can't merge qlora if loaded in 4bit\")\n\n        else:\n            if cfg.load_in_8bit:\n                raise ValueError(\"Can't load qlora in 8bit\")\n\n            if cfg.gptq:\n                raise ValueError(\"Can't load qlora if gptq\")\n\n            if not cfg.load_in_4bit:\n                raise ValueError(\"Require cfg.load_in_4bit to be True for qlora\")\n\n    if not cfg.load_in_8bit and cfg.adapter == \"lora\":\n        LOG.warning(\"We recommend setting `load_in_8bit: true` for LORA finetuning\")\n\n    if cfg.trust_remote_code:\n        LOG.warning(\n            \"`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\"\n        )\n\n    if cfg.push_dataset_to_hub and cfg.hf_use_auth_token is not True:\n        raise ValueError(\n            \"Require cfg.hf_use_auth_token to be True for push_dataset_to_hub\"\n        )\n\n    if (cfg.base_model and \"falcon\" in cfg.base_model.lower()) and cfg.fsdp:\n        raise ValueError(\"FSDP is not supported for falcon models\")\n\n    if (\n        cfg.base_model and \"mpt\" in cfg.base_model.lower()\n    ) and cfg.gradient_checkpointing:\n        raise ValueError(\"gradient_checkpointing is not supported for MPT models\")\n\n    if cfg.flash_optimum is True:\n        if cfg.adapter:\n            LOG.warning(\"BetterTransformers probably doesn't work with PEFT adapters\")\n        if cfg.fp16 or cfg.bf16:\n            raise ValueError(\"AMP is not supported with BetterTransformer\")\n        if cfg.float16 is not True and cfg.bloat16 is not True:\n            LOG.warning(\n                \"You should probably set bfloat16 or float16 to true to \"\n                \"load the model in float16 for BetterTransformers\"\n            )\n        if int(torch.__version__.split(\".\")[0]) < 2:\n            LOG.warning(\"torch>=2.0.0 required\")\n            raise ValueError(\n                f\"flash_optimum for BetterTransformers may not be used with {torch.__version__}\"\n            )\n\n    if cfg.pretraining_dataset and cfg.group_by_length:\n        LOG.warning(\n            \"You probably want to disable group_by_length as it will force a streamed dataset to download completely.\"\n        )\n\n    if any([cfg.adam_beta1, cfg.adam_beta2, cfg.adam_epsilon]) and (\n        not cfg.optimizer or \"adamw\" not in cfg.optimizer\n    ):\n        LOG.warning(\"adamw hyperparameters found, but no adamw optimizer set\")\n\n    if cfg.push_to_hub_model_id:\n        raise ValueError(\n            \"push_to_hub_model_id is deprecated. Please use hub_model_id instead.\"\n        )", "\n    # TODO\n    # MPT 7b\n    # https://github.com/facebookresearch/bitsandbytes/issues/25\n    # no 8bit adaAmw w bf16\n\n    # GPT-NeoX\n    # evals broken when extending context len\n    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 162, in forward                        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/optimum/bettertransformer/models/attention.py\", line 74, in gpt2_wrapped_scaled_dot_product", "    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\", line 162, in forward                        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n    # File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/optimum/bettertransformer/models/attention.py\", line 74, in gpt2_wrapped_scaled_dot_product\n    # attention_mask = causal_mask + attention_mask\n    # RuntimeError: The size of tensor a (2048) must match the size of tensor b (8132) at non-singleton dimension 3\n"]}
{"filename": "src/axolotl/utils/callbacks.py", "chunked_list": ["\"\"\"Callbacks for Trainer class\"\"\"\n\nimport os\n\nfrom optimum.bettertransformer import BetterTransformer\nfrom transformers import (\n    TrainerCallback,\n    TrainerControl,\n    TrainerState,\n    TrainingArguments,", "    TrainerState,\n    TrainingArguments,\n)\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, IntervalStrategy\n\n\nclass SavePeftModelCallback(TrainerCallback):  # pylint: disable=too-few-public-methods\n    \"\"\"Callback to save the PEFT adapter\"\"\"\n\n    def on_save(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        checkpoint_folder = os.path.join(\n            args.output_dir,\n            f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\",\n        )\n\n        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n        kwargs[\"model\"].save_pretrained(peft_model_path)\n\n        return control", "\n\nclass SaveBetterTransformerModelCallback(\n    TrainerCallback\n):  # pylint: disable=too-few-public-methods\n    \"\"\"Callback to save the BetterTransformer wrapped model\"\"\"\n\n    def on_step_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ):\n        # Save\n        if (\n            args.save_strategy == IntervalStrategy.STEPS\n            and args.save_steps > 0\n            and state.global_step % args.save_steps == 0\n        ):\n            control.should_save = True\n\n        if control.should_save:\n            checkpoint_folder = os.path.join(\n                args.output_dir,\n                f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\",\n            )\n\n            model = BetterTransformer.reverse(kwargs[\"model\"])\n            model.save_pretrained(checkpoint_folder)\n            # FIXME - need to cleanup old checkpoints\n\n            # since we're saving here, we don't need the trainer loop to attempt to save too b/c\n            # the trainer will raise an exception since it can't save a BetterTransformer wrapped model\n            control.should_save = False\n        return control", ""]}
{"filename": "src/axolotl/utils/trainer.py", "chunked_list": ["\"\"\"Module containing the Trainer class and related functions\"\"\"\n\nimport importlib\nimport logging\nimport math\nimport os\nimport sys\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional", "from pathlib import Path\nfrom typing import Optional\n\nimport bitsandbytes as bnb\nimport torch.cuda\nimport transformers\nfrom torch import nn\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom transformers import EarlyStoppingCallback, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import get_parameter_names", "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import get_parameter_names\n\nfrom axolotl.utils.callbacks import (\n    SaveBetterTransformerModelCallback,\n    SavePeftModelCallback,\n)\nfrom axolotl.utils.schedulers import (\n    InterpolatingLogScheduler,\n    get_cosine_schedule_with_quadratic_warmup,", "    InterpolatingLogScheduler,\n    get_cosine_schedule_with_quadratic_warmup,\n)\n\nLOG = logging.getLogger(\"axolotl\")\n\n\n@dataclass\nclass AxolotlTrainingArguments(TrainingArguments):\n    \"\"\"\n    Extend the base TrainingArguments for axolotl helpers\n    \"\"\"\n\n    lr_quadratic_warmup: bool = field(\n        default=False,\n        metadata={\"help\": \"Use quadratic warmup for cosine scheduling.\"},\n    )", "class AxolotlTrainingArguments(TrainingArguments):\n    \"\"\"\n    Extend the base TrainingArguments for axolotl helpers\n    \"\"\"\n\n    lr_quadratic_warmup: bool = field(\n        default=False,\n        metadata={\"help\": \"Use quadratic warmup for cosine scheduling.\"},\n    )\n", "\n\nclass AxolotlTrainer(Trainer):\n    \"\"\"\n    Extend the base Trainer for axolotl helpers\n    \"\"\"\n\n    args = None  # type: AxolotlTrainingArguments\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: torch.optim.Optimizer = None\n    ):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n        passed as an argument.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n            optimizer (torch.optim.Optimizer): The training optimizer\n        \"\"\"\n\n        # fmt: off\n        if self.lr_scheduler is None:  # type: ignore  # pylint: disable=access-member-before-definition\n            # fmt: on\n            if (\n                self.args.lr_scheduler_type == \"cosine\"\n                and self.args.lr_quadratic_warmup is True\n            ):\n                self.lr_scheduler = get_cosine_schedule_with_quadratic_warmup(  # pylint: disable=attribute-defined-outside-init\n                    optimizer,\n                    num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                    num_training_steps=num_training_steps,\n                )\n            else:\n                return super().create_scheduler(num_training_steps, optimizer)\n        return self.lr_scheduler", "\n\nclass OneCycleLRSchedulerTrainer(AxolotlTrainer):\n    \"\"\"\n    Trainer subclass that uses the OneCycleLR scheduler\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lr_scheduler = None\n\n    def create_scheduler(\n        self,\n        num_training_steps: int,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n    ):\n        optimizer = self.optimizer if optimizer is None else optimizer\n        num_warmup_steps = self.args.get_warmup_steps(num_training_steps)\n        pct_start = num_warmup_steps / num_training_steps\n\n        self.lr_scheduler = OneCycleLR(\n            optimizer,\n            max_lr=self.args.learning_rate,\n            total_steps=num_training_steps,\n            pct_start=pct_start,\n            div_factor=6,\n        )\n\n        return self.lr_scheduler", "\n\ndef setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer):\n    total_num_steps = int(\n        math.ceil(len(train_dataset) * cfg.num_epochs / cfg.batch_size)\n    )\n    warmup_steps = (\n        cfg.warmup_steps\n        if cfg.warmup_steps is not None\n        else min(int(0.03 * total_num_steps), 100)\n    )\n    logging_steps = (\n        cfg.logging_steps\n        if cfg.logging_steps is not None\n        else max(min(int(0.005 * total_num_steps), 10), 1)\n    )\n\n    training_arguments_kwargs = {}\n    if cfg.bf16 == \"full\":\n        training_arguments_kwargs[\"bf16_full_eval\"] = True\n    else:\n        training_arguments_kwargs[\"bf16\"] = cfg.bf16\n    training_arguments_kwargs[\"fp16\"] = (cfg.fp16 and not cfg.bf16) or False\n    training_arguments_kwargs[\"tf32\"] = cfg.tf32\n    training_arguments_kwargs[\"warmup_steps\"] = warmup_steps\n    training_arguments_kwargs[\"logging_steps\"] = logging_steps\n\n    if cfg.seed:\n        training_arguments_kwargs[\"seed\"] = cfg.seed\n\n    if cfg.gradient_checkpointing:\n        if cfg.gptq:\n            from alpaca_lora_4bit.gradient_checkpointing import (\n                apply_gradient_checkpointing,\n            )\n\n            gradient_checkpointing_ratio = (\n                cfg.gradient_checkpointing_ratio\n                if cfg.gradient_checkpointing_ratio\n                else 1.0\n            )\n            apply_gradient_checkpointing(\n                model, checkpoint_ratio=gradient_checkpointing_ratio\n            )\n        else:\n            training_arguments_kwargs[\n                \"gradient_checkpointing\"\n            ] = cfg.gradient_checkpointing\n    if cfg.fsdp:\n        training_arguments_kwargs[\"fsdp\"] = cfg.fsdp\n        if cfg.fsdp_config:\n            training_arguments_kwargs[\"fsdp_config\"] = dict(cfg.fsdp_config)\n\n    if cfg.lr_quadratic_warmup is not None:\n        training_arguments_kwargs[\"lr_quadratic_warmup\"] = cfg.lr_quadratic_warmup\n\n    # deepspeed\n    if (\n        os.environ.get(\"ACCELERATE_USE_DEEPSPEED\") == \"true\"\n        and torch.cuda.device_count() > 1\n    ):\n        if cfg.deepspeed:\n            training_arguments_kwargs[\"deepspeed\"] = cfg.deepspeed\n        else:\n            # make a guess here\n            # TODO search Path(\"./\") for one\n            training_arguments_kwargs[\"deepspeed\"] = \"./ds_config.json\"\n\n    if cfg.adam_beta1:\n        training_arguments_kwargs[\"adam_beta1\"] = cfg.adam_beta1\n    if cfg.adam_beta2:\n        training_arguments_kwargs[\"adam_beta2\"] = cfg.adam_beta2\n    if cfg.adam_epsilon:\n        training_arguments_kwargs[\"adam_epsilon\"] = cfg.adam_epsilon\n    if cfg.max_grad_norm:\n        training_arguments_kwargs[\"max_grad_norm\"] = cfg.max_grad_norm\n\n    if cfg.hub_model_id:\n        training_arguments_kwargs[\"hub_model_id\"] = cfg.hub_model_id\n        training_arguments_kwargs[\"push_to_hub\"] = True\n        training_arguments_kwargs[\"hub_private_repo\"] = True\n\n    if cfg.save_safetensors:\n        training_arguments_kwargs[\"save_safetensors\"] = cfg.save_safetensors\n\n    training_args = AxolotlTrainingArguments(  # pylint: disable=unexpected-keyword-arg\n        per_device_train_batch_size=cfg.micro_batch_size,\n        per_device_eval_batch_size=cfg.eval_batch_size\n        if cfg.eval_batch_size is not None\n        else cfg.micro_batch_size,\n        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n        eval_accumulation_steps=cfg.gradient_accumulation_steps,\n        num_train_epochs=cfg.num_epochs,\n        learning_rate=cfg.learning_rate,\n        evaluation_strategy=\"steps\" if cfg.val_set_size > 0 else \"no\",\n        save_strategy=\"steps\" if cfg.save_steps else \"epoch\",\n        eval_steps=cfg.eval_steps if cfg.val_set_size > 0 else None,\n        save_steps=cfg.save_steps,\n        output_dir=cfg.output_dir,\n        save_total_limit=3,\n        load_best_model_at_end=(\n            cfg.load_best_model_at_end is not False\n            and cfg.val_set_size > 0\n            and cfg.save_steps\n            and cfg.save_steps % cfg.eval_steps == 0\n            and cfg.load_in_8bit is not True\n        )\n        or False,\n        ddp_find_unused_parameters=False if cfg.ddp else None,\n        group_by_length=cfg.group_by_length,\n        report_to=\"wandb\" if cfg.use_wandb else None,\n        run_name=cfg.wandb_run_id if cfg.use_wandb else None,\n        optim=cfg.optimizer if cfg.optimizer else \"adamw_hf\",\n        lr_scheduler_type=cfg.lr_scheduler\n        if cfg.lr_scheduler and cfg.lr_scheduler not in (\"one_cycle\", \"log_sweep\")\n        else \"cosine\",\n        weight_decay=cfg.weight_decay if cfg.weight_decay is not None else 0.0,\n        **training_arguments_kwargs,\n    )\n\n    trainer_kwargs = {}\n\n    if cfg.optimizer == \"adamw_anyprecision\":\n        if Path(cfg.torchdistx_path).exists():\n            sys.path.append(cfg.torchdistx_path)\n            importlib.import_module(\"torchdistx\")\n    if (\n        cfg.optimizer == \"adamw_bnb_8bit\"\n        and not cfg.gptq\n        and \"deepspeed\" not in training_arguments_kwargs\n        and not cfg.fsdp\n    ):\n        decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if (n in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": training_args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in model.named_parameters()\n                    if (n not in decay_parameters and p.requires_grad)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n\n        optimizer = bnb.optim.Adam8bit(\n            optimizer_grouped_parameters,\n            betas=(training_args.adam_beta1, training_args.adam_beta2),\n            eps=training_args.adam_epsilon,\n            lr=training_args.learning_rate,\n        )\n\n        if cfg.lr_scheduler == \"one_cycle\":\n            lr_scheduler_kwargs = (\n                cfg.lr_scheduler_kwargs if cfg.lr_scheduler_kwargs else {}\n            )\n            lr_scheduler = OneCycleLR(\n                optimizer,\n                cfg.learning_rate,\n                total_steps=total_num_steps,\n                epochs=cfg.num_epochs,\n                div_factor=cfg.lr_div_factor if cfg.lr_div_factor else 6,\n                **lr_scheduler_kwargs,\n            )\n        elif cfg.lr_scheduler == \"log_sweep\":\n            lr_scheduler = InterpolatingLogScheduler(\n                optimizer,\n                cfg.warmup_steps,\n                cfg.log_sweep_min_lr if cfg.log_sweep_min_lr else 1e-10,\n                cfg.log_sweep_max_lr if cfg.log_sweep_max_lr else 10,\n            )\n        else:\n            lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n                optimizer,\n                training_args.warmup_steps,\n                total_num_steps,\n            )\n        trainer_kwargs[\"optimizers\"] = (optimizer, lr_scheduler)\n\n    callbacks = []\n    # TODO on_save callback to sync checkpoints to GCP/AWS in background\n    if cfg.early_stopping_patience:\n        early_stop_cb = EarlyStoppingCallback(\n            cfg.early_stopping_patience,\n        )\n        callbacks.append(early_stop_cb)\n\n    if cfg.local_rank == 0 and cfg.adapter in [\n        \"lora\",\n        \"qlora\",\n    ]:  # only save in rank 0\n        callbacks.append(SavePeftModelCallback)\n\n    if hasattr(model, \"use_bettertransformer\") and model.use_bettertransformer is True:\n        callbacks.append(SaveBetterTransformerModelCallback)\n\n    data_collator_kwargs = {\n        \"padding\": True,\n    }\n    if cfg.collator_pad_to_longest:\n        data_collator_kwargs[\"padding\"] = \"longest\"\n    else:\n        data_collator_kwargs[\"pad_to_multiple_of\"] = 8\n\n    if cfg.is_llama_derived_model and cfg.landmark_attention:\n        from functools import partial\n\n        from axolotl.monkeypatch.llama_landmark_attn import (\n            add_mem_tokens,\n            get_mem_id,\n            set_model_mem_id,\n        )\n\n        set_model_mem_id(model, tokenizer)\n\n        LOG.info(\"Adding landmark attention tokens to dataset\")\n\n        for dataset in [train_dataset, eval_dataset]:\n            dataset = dataset.map(\n                partial(add_mem_tokens, mem_freq=50, mem_id=get_mem_id(tokenizer)),\n                batched=False,\n                num_proc=32,\n            )\n\n    trainer_cls = (\n        OneCycleLRSchedulerTrainer\n        if cfg.lr_scheduler == \"one_cycle\" and (cfg.fsdp or cfg.adapter == \"qlora\")\n        else AxolotlTrainer\n    )\n    trainer = trainer_cls(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        args=training_args,\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer,\n            return_tensors=\"pt\",\n            **data_collator_kwargs,\n        ),\n        callbacks=callbacks,\n        **trainer_kwargs,\n    )\n\n    return trainer", ""]}
{"filename": "src/axolotl/prompt_strategies/creative_acr.py", "chunked_list": ["\"\"\"Module loading the CreativePromptTokenizingStrategy and similar classes\"\"\"\n\nfrom typing import Generator, Tuple, Union\n\nimport yaml\n\nfrom axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy\n\n\nclass CreativeAnsweringPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Creative Answering\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        question = prompt[\"instruction\"]\n        answer = prompt[\n            \"revision\"\n        ]  # don't use prompt[answer], that's data we don't want in the dataset\n        return (\n            question,\n            \"\",\n            answer,\n        )", "\nclass CreativeAnsweringPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Creative Answering\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        question = prompt[\"instruction\"]\n        answer = prompt[\n            \"revision\"\n        ]  # don't use prompt[answer], that's data we don't want in the dataset\n        return (\n            question,\n            \"\",\n            answer,\n        )", "\n\nclass CreativeCritiquePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Creative Critique\n    \"\"\"\n\n    user_prompt = \"\"\"Given the following Question and Response, critique the Response on a scale of 1-10. You should critique the answer in the following criteria:\nrefusal: whether the Response refuses to answer the Question. Responses containing language similar to \"As an AI language model, I am not capable of ...\" is a form of refusal and should have a low refusal score. 10 is no refusal, 0 is refuses to answer the question.\nprescriptive bias: whether the Response attempts to prescribe or dictate certain actions or behaviors of the user. 10 no prescriptive bias present, 0 means there is prescriptive bias.\ncreativity: score how creative the Response is. 10 is most creative, 0 lacks creativity\ncomprehensiveness: score how comprehensive and in-depth the Response is. 10 is most comprehensive and in-depth, 0 lacks coverage and depth\nThen for each criteria, give a detailed explanation of why the response doesn't meet the criteria (don't reference the score), what it did right, and how it could be improved for that criteria.\n\nOutput your answer in YAML format like so:\nscores:\n  refusal: <0-10>\n  prescriptive_bias: <0-10>\n  creativity: <0-10>\n  comprehensiveness: <0-10>\ncritiques:\n  refusal:\n    explanation: ...\n    improvements: ...\n  prescriptive_bias:\n    explanation: ...\n    improvements: ...\n  creativity:\n    explanation: ...\n    improvements: ...\n  comprehensiveness:\n    explanation: ...\n    improvements: ...\n\nQuestion: {question}\nAnswer: {answer}\n\"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        scores = yaml.dump(\n            prompt[\"scores\"],\n            default_flow_style=False,\n            Dumper=yaml.Dumper,\n        )\n        critiques = yaml.dump(\n            prompt[\"critiques\"],\n            default_flow_style=False,\n            Dumper=yaml.Dumper,\n        )\n        evaluation = scores + critiques\n        question = prompt[\"instruction\"]\n        answer = prompt[\"answer\"]\n        return (\n            self.user_prompt.format(question=question, answer=answer),\n            \"\",\n            evaluation,\n        )", "\n\nclass CreativeRevisePromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Creative Revise\n    \"\"\"\n\n    user_prompt = \"\"\"Definitions:\nrefusal: whether the Response refuses to answer the Question. Responses containing language similar to \"As an AI language model, I am not capable of ...\" is a form of refusal and should have a low refusal score. 10 is no refusal, 0 is refuses to answer the question.\nprescriptive bias: whether the Response attempts to prescribe or dictate certain actions or behaviors of the user. 10 no prescriptive bias present, 0 means their is prescriptive bias.\ncreativity: score how creative the Response is. 10 is most creative, 0 lacks creativity\ncomprehensiveness: score how comprehensive and in-depth the Response is. 10 is most comprehensive and in-depth, 0 lacks coverage and depth\n\nGiven the following Question, Response, and Evaluation, revise the Response based on the Evaluation and recommendations for improvements. Reply only with the revised response.\n\nQuestion: {question}\nAnswer: {answer}\nEvaluation:\n{evaluation}\n\"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        scores = yaml.dump(\n            prompt[\"scores\"],\n            default_flow_style=False,\n            Dumper=yaml.Dumper,\n        )\n        critiques = yaml.dump(\n            prompt[\"critiques\"],\n            default_flow_style=False,\n            Dumper=yaml.Dumper,\n        )\n        evaluation = scores + critiques\n        question = prompt[\"instruction\"]\n        answer = prompt[\"answer\"]\n        return (\n            self.user_prompt.format(\n                question=question, answer=answer, evaluation=evaluation\n            ),\n            \"\",\n            prompt[\"revision\"],\n        )", "\n\nclass CreativePrompterBase:\n    \"\"\"\n    Base class for Creative Prompters\n    \"\"\"\n\n    system_prompt = \"\"\n    prompt_input = \"{system_prompt}\\nUSER: {instruction}\\nASSISTANT:\"\n\n    def build_prompt(\n        self,\n        instruction: str,\n        input: Union[  # pylint: disable=redefined-builtin, unused-argument\n            None, str\n        ] = None,\n        output: Union[None, str] = None,\n    ) -> Generator[str, None, None]:\n        if self.system_prompt:\n            res = f\"{self.system_prompt}\\nUSER: {instruction}\\nASSISTANT:\"\n        else:\n            res = f\"USER: {instruction}\\nASSISTANT:\"\n        if output:\n            res = f\"{res}{output}\"\n        yield res", "\n\nclass CreativeAnswerPrompter(CreativePrompterBase):\n    \"\"\"\n    Prompter for Creative Answering\n    \"\"\"\n\n    system_prompt = \"Answer the following question in a comprehensive, in-depth, and creative way. Additionally your response should be relevant, accurate, and free of any ambiguity.\"\n\n\nclass CreativeCritiquePrompter(CreativePrompterBase):\n    \"\"\"\n    Prompter for Creative Critique\n    \"\"\"\n\n    system_prompt = \"\"", "\n\nclass CreativeCritiquePrompter(CreativePrompterBase):\n    \"\"\"\n    Prompter for Creative Critique\n    \"\"\"\n\n    system_prompt = \"\"\n\n\nclass CreativeRevisePrompter(CreativePrompterBase):\n    \"\"\"\n    Prompter for Creative Revise\n    \"\"\"\n\n    system_prompt = \"\"", "\n\nclass CreativeRevisePrompter(CreativePrompterBase):\n    \"\"\"\n    Prompter for Creative Revise\n    \"\"\"\n\n    system_prompt = \"\"\n\n\ndef load_answer(tokenizer, cfg):\n    return CreativeAnsweringPromptTokenizingStrategy(\n        CreativeAnswerPrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\ndef load_answer(tokenizer, cfg):\n    return CreativeAnsweringPromptTokenizingStrategy(\n        CreativeAnswerPrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_critique(tokenizer, cfg):\n    return CreativeCritiquePromptTokenizingStrategy(\n        CreativeCritiquePrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_revise(tokenizer, cfg):\n    return CreativeRevisePromptTokenizingStrategy(\n        CreativeRevisePrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", ""]}
{"filename": "src/axolotl/prompt_strategies/pygmalion.py", "chunked_list": ["\"\"\"Module containing the PygmalionPromptTokenizingStrategy and PygmalionPrompter class\"\"\"\n\nimport copy\nimport logging\nfrom collections import defaultdict\nfrom typing import Generator, List, Tuple\n\nfrom axolotl.prompt_tokenizers import (\n    PromptTokenizingStrategy,\n    parse_tokenized_to_result,", "    PromptTokenizingStrategy,\n    parse_tokenized_to_result,\n    tokenize_prompt_default,\n)\n\nLOG = logging.getLogger(\"axolotl\")\n\nIGNORE_TOKEN_ID = -100\n\n\nclass PygmalionPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Pygmalion.\n    \"\"\"\n\n    bot_prefix_token_ids: List[int] = []\n\n    def __init__(self, prompter, tokenizer, *args, **kwargs):\n        super().__init__(prompter, tokenizer, *args, **kwargs)\n        res = self._tokenize(\"<|model|>\", add_eos_token=False, strip_bos_token=True)\n        self.bot_prefix_token_ids = res[\"input_ids\"]\n\n    def tokenize_prompt(self, prompt):\n        result, current_len = tokenize_prompt_default()\n        for _, part in enumerate(self.prompter.build_prompt(prompt[\"conversations\"])):\n            role, message = part\n            if role == \"system\":\n                prefix = \"<|system|>\"\n                # this should include a bos token, no eos token, strip trailing \"\\n<START>\"\n                if message.endswith(\"\\n<START>\"):\n                    message = message[:-8]\n                res = self._tokenize(\n                    prefix + \"Persona: \" + message.strip(),\n                    add_eos_token=False,\n                    strip_bos_token=False,\n                )\n                # everything from this is masked out from the labels\n                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n            elif role == \"human\":\n                prefix = \"<|user|>\"\n                res = self._tokenize(\n                    prefix + \" \" + message.strip(),\n                    add_eos_token=False,\n                    strip_bos_token=True,\n                )\n                # everything from this is masked out from the labels\n                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n            elif role == \"bot\":\n                prefix = \"<|model|>\"\n                res = self._tokenize(\n                    prefix + \" \" + message.strip(),\n                    add_eos_token=True,\n                    strip_bos_token=True,\n                )\n                # mask out the prefix token, rest is not masked out from labels\n                # make sure we create the labels first, otherwise we get incorrect lengths\n                labels = [IGNORE_TOKEN_ID] * len(self.bot_prefix_token_ids) + [\n                    *copy.deepcopy(res[\"input_ids\"])\n                ][len(self.bot_prefix_token_ids) :]\n            else:\n                LOG.warning(f\"unknown role in conversation: {role}\")\n                res = defaultdict(lambda: [])\n\n            # pylint: disable=duplicate-code\n            result, current_len = parse_tokenized_to_result(\n                result,\n                current_len,\n                res,\n                labels,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        return result", "\n\nclass PygmalionPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for Pygmalion.\n    \"\"\"\n\n    bot_prefix_token_ids: List[int] = []\n\n    def __init__(self, prompter, tokenizer, *args, **kwargs):\n        super().__init__(prompter, tokenizer, *args, **kwargs)\n        res = self._tokenize(\"<|model|>\", add_eos_token=False, strip_bos_token=True)\n        self.bot_prefix_token_ids = res[\"input_ids\"]\n\n    def tokenize_prompt(self, prompt):\n        result, current_len = tokenize_prompt_default()\n        for _, part in enumerate(self.prompter.build_prompt(prompt[\"conversations\"])):\n            role, message = part\n            if role == \"system\":\n                prefix = \"<|system|>\"\n                # this should include a bos token, no eos token, strip trailing \"\\n<START>\"\n                if message.endswith(\"\\n<START>\"):\n                    message = message[:-8]\n                res = self._tokenize(\n                    prefix + \"Persona: \" + message.strip(),\n                    add_eos_token=False,\n                    strip_bos_token=False,\n                )\n                # everything from this is masked out from the labels\n                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n            elif role == \"human\":\n                prefix = \"<|user|>\"\n                res = self._tokenize(\n                    prefix + \" \" + message.strip(),\n                    add_eos_token=False,\n                    strip_bos_token=True,\n                )\n                # everything from this is masked out from the labels\n                labels = [IGNORE_TOKEN_ID] * len(res[\"input_ids\"])\n            elif role == \"bot\":\n                prefix = \"<|model|>\"\n                res = self._tokenize(\n                    prefix + \" \" + message.strip(),\n                    add_eos_token=True,\n                    strip_bos_token=True,\n                )\n                # mask out the prefix token, rest is not masked out from labels\n                # make sure we create the labels first, otherwise we get incorrect lengths\n                labels = [IGNORE_TOKEN_ID] * len(self.bot_prefix_token_ids) + [\n                    *copy.deepcopy(res[\"input_ids\"])\n                ][len(self.bot_prefix_token_ids) :]\n            else:\n                LOG.warning(f\"unknown role in conversation: {role}\")\n                res = defaultdict(lambda: [])\n\n            # pylint: disable=duplicate-code\n            result, current_len = parse_tokenized_to_result(\n                result,\n                current_len,\n                res,\n                labels,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        return result", "\n\nclass PygmalionPrompter:\n    \"\"\"\n    Prompter for Pygmalion.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def build_prompt(\n        self, source, *args, **kwargs  # pylint: disable=unused-argument\n    ) -> Generator[Tuple[str, str], None, None]:\n        for msg in source:\n            yield msg[\"role\"], msg[\"value\"]", "\n\ndef load(tokenizer, cfg):\n    return PygmalionPromptTokenizingStrategy(\n        PygmalionPrompter(), tokenizer, cfg.train_on_inputs, cfg.sequence_len\n    )\n"]}
{"filename": "src/axolotl/prompt_strategies/__init__.py", "chunked_list": ["\"\"\"Module to load prompt strategies.\"\"\"\n\nimport importlib\n\n\ndef load(strategy, tokenizer, cfg):\n    try:\n        load_fn = \"load\"\n        if strategy.split(\".\")[-1].startswith(\"load_\"):\n            load_fn = strategy.split(\".\")[-1]\n            strategy = \".\".join(strategy.split(\".\")[:-1])\n        mod = importlib.import_module(f\".{strategy}\", \"axolotl.prompt_strategies\")\n        func = getattr(mod, load_fn)\n        return func(tokenizer, cfg)\n    except Exception:  # pylint: disable=broad-exception-caught\n        return None", ""]}
{"filename": "src/axolotl/prompt_strategies/alpaca_w_system.py", "chunked_list": ["\"\"\"\nPrompt strategies loader for alpaca instruction datasets with system prompts\n\"\"\"\nfrom typing import Generator, Tuple, Union\n\nfrom axolotl.prompt_tokenizers import PromptTokenizingStrategy\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle\n\n\nclass InstructionWSystemPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for instruction-based prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str]:\n        return (\n            prompt[\"instruction\"],\n            prompt[\"input\"] if \"input\" in prompt else \"\",\n            prompt[\"output\"],\n            prompt[\"system\"],\n        )\n\n    def tokenize_prompt(self, prompt):\n        # pylint: disable=duplicate-code\n        (\n            instruction,\n            input,  # pylint: disable=redefined-builtin\n            response,\n            system,\n        ) = self.parse_instruction_fields(prompt)\n        user_prompt = next(\n            iter(\n                self.prompter.build_prompt_w_system(\n                    system,\n                    instruction,\n                    input,\n                )\n            )\n        )\n        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n        if not self.train_on_inputs:\n            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n            # TODO this could be sped up using numpy array slicing\n            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n        tokenized_res_prompt = self._tokenize(\n            response, strip_bos_token=True, add_eos_token=True\n        )\n        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n\n        return tokenized_prompt", "\nclass InstructionWSystemPromptTokenizingStrategy(PromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for instruction-based prompts.\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str]:\n        return (\n            prompt[\"instruction\"],\n            prompt[\"input\"] if \"input\" in prompt else \"\",\n            prompt[\"output\"],\n            prompt[\"system\"],\n        )\n\n    def tokenize_prompt(self, prompt):\n        # pylint: disable=duplicate-code\n        (\n            instruction,\n            input,  # pylint: disable=redefined-builtin\n            response,\n            system,\n        ) = self.parse_instruction_fields(prompt)\n        user_prompt = next(\n            iter(\n                self.prompter.build_prompt_w_system(\n                    system,\n                    instruction,\n                    input,\n                )\n            )\n        )\n        tokenized_prompt = self._tokenize(user_prompt, add_eos_token=False)\n        if not self.train_on_inputs:\n            user_prompt_len = len(tokenized_prompt[\"input_ids\"])\n            # TODO this could be sped up using numpy array slicing\n            tokenized_prompt[\"labels\"] = [-100] * user_prompt_len\n        tokenized_res_prompt = self._tokenize(\n            response, strip_bos_token=True, add_eos_token=True\n        )\n        tokenized_prompt[\"input_ids\"] += tokenized_res_prompt[\"input_ids\"]\n        tokenized_prompt[\"attention_mask\"] += tokenized_res_prompt[\"attention_mask\"]\n        tokenized_prompt[\"labels\"] += tokenized_res_prompt[\"input_ids\"]\n\n        return tokenized_prompt", "\n\nclass SystemDataPrompter(AlpacaPrompter):\n    \"\"\"\n    Alpaca Style Prompter that uses system prompts from the dataset\n    \"\"\"\n\n    def build_prompt_w_system(\n        self,\n        system: str,\n        instruction: str,\n        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n        output: Union[None, str] = None,\n    ) -> Generator[str, None, None]:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        formatted_sys_prompt = f\"### System:\\n{system}\\n\\n\" if system else \"\"\n        if input:\n            res = formatted_sys_prompt + self.turn_format.format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = formatted_sys_prompt + self.turn_no_input_format.format(\n                instruction=instruction\n            )\n        if output:\n            res = f\"{res}{output}\"\n        yield res", "\n\nclass OpenOrcaSystemDataPrompter(SystemDataPrompter):\n    \"\"\"\n    Alpaca Style Prompter that uses system prompts from the dataset, with OpenOrca prompts\n    \"\"\"\n\n    def match_prompt_style(self):\n        if self.prompt_style == PromptStyle.INSTRUCT.value:\n            self.turn_format = \"### User:\\n{instruction}\\n\\n### Additional Context:\\n{input}\\n\\n### Assistant:\\n\"\n            self.turn_no_input_format = \"### User:\\n{instruction}\\n\\n### Assistant:\\n\"\n        if self.prompt_style == PromptStyle.CHAT.value:\n            self.turn_format = \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n            self.turn_no_input_format = \"USER: {instruction}\\nASSISTANT:\"", "\n\nclass OpenOrcaPromptTokenizingStrategy(InstructionWSystemPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for OpenOrca datasets\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str, str]:\n        return (\n            prompt[\"question\"],\n            \"\",\n            prompt[\"response\"],\n            prompt[\"system_prompt\"],\n        )", "\n\ndef load(tokenizer, cfg):\n    return load_chat(tokenizer, cfg)\n\n\ndef load_instruct(tokenizer, cfg):\n    return InstructionWSystemPromptTokenizingStrategy(\n        SystemDataPrompter(PromptStyle.INSTRUCT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\ndef load_chat(tokenizer, cfg):\n    return InstructionWSystemPromptTokenizingStrategy(\n        SystemDataPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_open_orca(tokenizer, cfg):\n    return OpenOrcaPromptTokenizingStrategy(\n        OpenOrcaSystemDataPrompter(PromptStyle.INSTRUCT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", ""]}
{"filename": "src/axolotl/prompt_strategies/alpaca_chat.py", "chunked_list": ["\"\"\"Module containing the AlpacaQAPromptTokenizingStrategy class\"\"\"\n\nfrom typing import Tuple\n\nfrom axolotl.prompt_tokenizers import (\n    AlpacaPromptTokenizingStrategy,\n    InstructionPromptTokenizingStrategy,\n)\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle, UnpromptedPrompter\n", "from axolotl.prompters import AlpacaPrompter, PromptStyle, UnpromptedPrompter\n\n\ndef load(tokenizer, cfg):\n    return AlpacaPromptTokenizingStrategy(\n        AlpacaPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\nclass AlpacaConcisePrompter(AlpacaPrompter):\n    \"\"\"\n    Alpaca Prompter extending the system prompt to ask for concise chat-instruct answers\n    \"\"\"\n\n    system_prompt = \"Below is an instruction from a USER that describes a task, paired with an input that provides further context. The ASSISTANT writes a response that concisely and appropriately completes the request.\\n\\n\"\n    system_no_input_prompt = \"Below is an instruction from a USER that describes a task. The ASSISTANT writes a response that appropriately and concisely completes the request.\\n\\n\"\n", "\n\nclass AlpacaChatPrompter(AlpacaPrompter):\n    \"\"\"\n    Alpaca Chat Prompter extending the system prompt to for chat-instruct answers\n    \"\"\"\n\n    system_prompt = \"Below is an instruction from a USER that describes a task, paired with an input that provides further context. The ASSISTANT writes a response that concisely and appropriately completes the request.\\n\\n\"\n    system_no_input_prompt = \"Below is an instruction from a USER that describes a task. The ASSISTANT writes a response that appropriately and concisely completes the request.\\n\\n\"\n\n    def __init__(self):  # pylint: disable=super-init-not-called\n        self.prompt_style = PromptStyle.CHAT.value\n        self.match_prompt_style()", "\n\nclass NoSystemPrompter(AlpacaPrompter):\n    \"\"\"\n    Null Prompter with no system prompts\n    \"\"\"\n\n    system_prompt = \"\"\n    system_no_input_prompt = \"\"\n    turn_format = \"{instruction} {input} \"\n    turn_no_input_format = \"{instruction} \"\n\n    def __init__(self):  # pylint: disable=super-init-not-called\n        pass", "\n\nclass AlpacaQAPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for AlpacaQA\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"question\"],\n            \"\",\n            prompt[\"answer\"],\n        )", "\n\nclass CamelAIPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenizing strategy for CamelAI datasets\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"message_1\"],\n            \"\",\n            prompt[\"message_2\"],\n        )", "\n\ndef load_concise(tokenizer, cfg):\n    return AlpacaPromptTokenizingStrategy(\n        AlpacaConcisePrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_qa(tokenizer, cfg):\n    return AlpacaQAPromptTokenizingStrategy(\n        AlpacaChatPrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_camel_ai(tokenizer, cfg):\n    return CamelAIPromptTokenizingStrategy(\n        AlpacaChatPrompter(),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_no_prompt(tokenizer, cfg):\n    return AlpacaPromptTokenizingStrategy(\n        UnpromptedPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", ""]}
{"filename": "src/axolotl/prompt_strategies/alpaca_instruct.py", "chunked_list": ["\"\"\"Module loading the AlpacaInstructPromptTokenizingStrategy class\"\"\"\n\nfrom axolotl.prompt_tokenizers import AlpacaPromptTokenizingStrategy\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle, UnpromptedPrompter\n\n\ndef load(tokenizer, cfg):\n    return AlpacaPromptTokenizingStrategy(\n        AlpacaPrompter(PromptStyle.INSTRUCT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\ndef load_no_prompt(tokenizer, cfg):\n    return AlpacaPromptTokenizingStrategy(\n        UnpromptedPrompter(PromptStyle.INSTRUCT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", ""]}
{"filename": "src/axolotl/prompt_strategies/sharegpt_jokes.py", "chunked_list": ["\"\"\"Module for Jokes prompts using sharegpt style \"\"\"\nfrom axolotl.prompt_tokenizers import ShareGPTPromptTokenizingStrategy\nfrom axolotl.prompters import PromptStyle, ShareGPTPrompter\n\n\ndef load(tokenizer, cfg):\n    return SimpleJokesShareGPTPromptTokenizingStrategy(\n        ShareGPTPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\nclass SimpleJokesShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n    \"\"\"\n    Tokenization strategy for asking bot to tell a joke and then explain why its funny\n    \"\"\"\n\n    # title, text, explanation\n    def get_conversation_thread(self, prompt):\n        title = \"\" if not prompt[\"title\"] else prompt[\"title\"] + \" \"\n        return [\n            {\"from\": \"human\", \"value\": \"Tell me a joke.\"},\n            {\"from\": \"gpt\", \"value\": title + prompt[\"text\"]},\n            {\"from\": \"human\", \"value\": \"Why is that joke funny?\"},\n            {\"from\": \"gpt\", \"value\": prompt[\"explanation\"]},\n        ]", ""]}
{"filename": "src/axolotl/prompt_strategies/sharegpt_simple.py", "chunked_list": ["\"\"\"Module containing the SimpleShareGPTPromptTokenizingStrategy class\"\"\"\n\nfrom axolotl.prompt_tokenizers import ShareGPTPromptTokenizingStrategy\nfrom axolotl.prompters import PromptStyle, ShareGPTPrompter\n\n\ndef load(tokenizer, cfg):\n    return SimpleShareGPTPromptTokenizingStrategy(\n        ShareGPTPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\n\ndef load_role(tokenizer, cfg):\n    return SimpleRoleShareGPTPromptTokenizingStrategy(\n        ShareGPTPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\ndef load_guanaco(tokenizer, cfg):\n    return GuanacoShareGPTPromptTokenizingStrategy(\n        ShareGPTPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n", "\n\nclass SimpleShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n    \"\"\"\n    basic sharegpt strategy to grab conversations from the sample row\n    \"\"\"\n\n    def get_conversation_thread(self, prompt):\n        return prompt[\"conversations\"]\n", "\n\nclass SimpleRoleShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n    \"\"\"\n    basic sharegpt strategy to grab conversations from the sample row, but uses role instead of from\n    \"\"\"\n\n    def get_conversation_thread(self, prompt):\n        conversations = prompt[\"conversations\"]\n        # remap role: prompter/assistant, text: ... => from: human/gpt, value: ...\n        turns = [{\"from\": t[\"role\"], \"value\": t[\"value\"]} for t in conversations]\n        return turns", "\n\nclass GuanacoShareGPTPromptTokenizingStrategy(ShareGPTPromptTokenizingStrategy):\n    \"\"\"\n    sharegpt strategy that remaps oasst data to sharegpt format\n    \"\"\"\n\n    def get_conversation_thread(self, prompt):\n        conversations = prompt[\"conversations\"]\n        # remap role: prompter/assistant, text: ... => from: human/gpt, value: ...\n        role_map = {\"prompter\": \"human\", \"assistant\": \"gpt\"}\n        turns = [\n            {\"from\": role_map[t[\"role\"]], \"value\": t[\"text\"]} for t in conversations\n        ]\n        return turns", ""]}
{"filename": "src/axolotl/prompt_strategies/context_qa.py", "chunked_list": ["\"\"\"Module containing the classes for Context QA Prompt Tokenization Strategies\"\"\"\nfrom typing import Tuple\n\nfrom axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle\n\n\n# article, unanswerable_question, question, answer\ndef load_404(tokenizer, cfg):\n    return AlpacaMissingInfoContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "def load_404(tokenizer, cfg):\n    return AlpacaMissingInfoContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n\n\ndef load(tokenizer, cfg):\n    return AlpacaContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )", "\ndef load(tokenizer, cfg):\n    return AlpacaContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n\n\nclass AlpacaContextPrompter(AlpacaPrompter):\n    \"\"\"\n    Customized system prompted for concise QA\n    \"\"\"\n\n    system_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )\n    system_no_input_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )", "\n\nclass AlpacaContextPrompter(AlpacaPrompter):\n    \"\"\"\n    Customized system prompted for concise QA\n    \"\"\"\n\n    system_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )\n    system_no_input_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )", "\n\nclass AlpacaContextPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenization Strategy to combine in-context article with a question and answer\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"article\"] + \"\\n===\\n\" + prompt[\"question\"],\n            \"\",\n            prompt[\"answer\"],\n        )", "\n\nclass AlpacaMissingInfoContextPromptTokenizingStrategy(\n    InstructionPromptTokenizingStrategy\n):\n    \"\"\"\n    Tokenization Strategy to combine in-context article with a question that can't be answered\n    from the context and a default response to that effect\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"article\"] + \"\\n===\\n\" + prompt[\"unanswerable_question\"],\n            \"\",\n            \"The context provided does not contain any information about your inquiry. \"\n            \"Therefore, I'm unable to answer your question based on the given context.\",\n        )", ""]}
