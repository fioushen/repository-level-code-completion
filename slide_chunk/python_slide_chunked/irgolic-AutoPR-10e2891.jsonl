{"filename": "autopr/main.py", "chunked_list": ["from typing import Optional, Any, Type\n\nfrom git.repo import Repo\nfrom pydantic import BaseSettings\n\nfrom .models.events import EventUnion, IssueLabelEvent\nfrom .repos.completions_repo import get_completions_repo\nfrom .services.action_service import ActionService\nfrom .services.agent_service import AgentService\nfrom .services.chain_service import ChainService", "from .services.agent_service import AgentService\nfrom .services.chain_service import ChainService\nfrom .services.commit_service import CommitService\nfrom .services.diff_service import GitApplyService\nfrom .services.publish_service import PublishService\nfrom .services.rail_service import RailService\n\nimport structlog\n\n\nclass Settings(BaseSettings):\n    agent_id: str = 'plan_and_code'\n    agent_config: Optional[dict[str, Any]] = None\n\n    base_branch: str = 'main'\n    target_branch_name_template: str = 'autopr/{issue_number}'\n    overwrite_existing: bool = False\n    loading_gif_url: str = \"https://media0.giphy.com/media/l3nWhI38IWDofyDrW/giphy.gif\"\n    model: str = \"gpt-4\"\n    temperature: float = 0.8\n    rail_temperature: float = 0.4\n    context_limit: int = 8192\n    min_tokens: int = 1000\n    max_tokens: int = 2000\n    num_reasks: int = 2", "\n\nclass Settings(BaseSettings):\n    agent_id: str = 'plan_and_code'\n    agent_config: Optional[dict[str, Any]] = None\n\n    base_branch: str = 'main'\n    target_branch_name_template: str = 'autopr/{issue_number}'\n    overwrite_existing: bool = False\n    loading_gif_url: str = \"https://media0.giphy.com/media/l3nWhI38IWDofyDrW/giphy.gif\"\n    model: str = \"gpt-4\"\n    temperature: float = 0.8\n    rail_temperature: float = 0.4\n    context_limit: int = 8192\n    min_tokens: int = 1000\n    max_tokens: int = 2000\n    num_reasks: int = 2", "\n\nclass MainService:\n    settings_class: Type[Settings] = Settings\n    publish_service_class: Type[PublishService] = PublishService\n\n    def __init__(self):\n        self.log = structlog.get_logger()\n\n        self.settings = settings = self.settings_class.parse_obj({})  # pyright workaround\n        self.event = self.get_event()\n        self.repo_path = self.get_repo_path()\n        self.repo = Repo(self.repo_path)\n        self.branch_name = self.get_branch_name()\n        self.base_branch_name = self.get_base_branch_name()\n        self.publish_service = self.get_publish_service()\n\n        # Create commit service\n        commit_service = CommitService(\n            repo=self.repo,\n            repo_path=self.repo_path,\n            branch_name=self.branch_name,\n            base_branch_name=self.base_branch_name,\n        )\n        commit_service.ensure_branch_exists()\n\n        # Create completions repo\n        completions_repo = get_completions_repo(\n            publish_service=self.publish_service,\n            model=settings.model,\n            context_limit=settings.context_limit,\n            min_tokens=settings.min_tokens,\n            max_tokens=settings.max_tokens,\n            temperature=settings.temperature,\n        )\n\n        # Create rail and chain service\n        rail_service = RailService(\n            completions_repo=completions_repo,\n            min_tokens=settings.min_tokens,\n            context_limit=settings.context_limit,\n            num_reasks=settings.num_reasks,\n            temperature=settings.rail_temperature,\n            publish_service=self.publish_service,\n        )\n        chain_service = ChainService(\n            completions_repo=completions_repo,\n            publish_service=self.publish_service,\n            context_limit=settings.context_limit,\n            min_tokens=settings.min_tokens,\n        )\n\n        # Create diff service\n        diff_service = GitApplyService(repo=self.repo)\n\n        # Create action service and agent service\n        action_service = ActionService(\n            repo=self.repo,\n            completions_repo=completions_repo,\n            rail_service=rail_service,\n            publish_service=self.publish_service,\n            chain_service=chain_service,\n        )\n        self.agent_service = AgentService(\n            repo=self.repo,\n            publish_service=self.publish_service,\n            rail_service=rail_service,\n            chain_service=chain_service,\n            diff_service=diff_service,\n            commit_service=commit_service,\n            action_service=action_service,\n        )\n\n    def run(self):\n        # Generate the PR\n        self.agent_service.run_agent(self.settings.agent_id, self.settings.agent_config, self.event)\n\n    def get_repo_path(self):\n        raise NotImplementedError\n\n    def get_event(self) -> EventUnion:\n        raise NotImplementedError\n\n    def get_publish_service(self, **additional_kwargs):\n        # Get repo owner and name from remote URL\n        remote_url = self.repo.remotes.origin.url\n        owner, repo_name = remote_url.removesuffix(\".git\").split('/')[-2:]\n\n        if isinstance(self.event, IssueLabelEvent):\n            issue = self.event.issue\n            pull_request_number = None\n        else:\n            issue = None\n            pull_request_number = self.event.pull_request.number\n\n        return self.publish_service_class(\n            owner=owner,\n            repo_name=repo_name,\n            head_branch=self.branch_name,\n            base_branch=self.base_branch_name,\n            issue=issue,\n            pull_request_number=pull_request_number,\n            loading_gif_url=self.settings.loading_gif_url,\n            overwrite_existing=self.settings.overwrite_existing,\n            **additional_kwargs,\n        )\n\n    def get_branch_name(self):\n        if isinstance(self.event, IssueLabelEvent):\n            branch_name = self.settings.target_branch_name_template.format(issue_number=self.event.issue.number)\n            if not self.settings.overwrite_existing:\n                # check if branch exists, if so, append a number to the end\n                remote = self.repo.remote()\n                references = remote.fetch()\n                i = 2\n                while f'{remote.name}/{branch_name}' in [ref.name for ref in references]:\n                    branch_name = self.settings.target_branch_name_template.format(\n                        issue_number=self.event.issue.number) + f'-{i}'\n                    i += 1\n            return branch_name\n        else:\n            return self.event.pull_request.head_branch\n\n    def get_base_branch_name(self):\n        if isinstance(self.event, IssueLabelEvent):\n            return self.settings.base_branch\n        else:\n            return self.event.pull_request.base_branch", ""]}
{"filename": "autopr/validators.py", "chunked_list": ["import os\nimport re\nfrom typing import Union, Any, Dict, List, Optional\n\nfrom git import GitCommandError, Tree, Blob\n\nfrom autopr.services.diff_service import DiffService\nfrom guardrails import register_validator, Validator\nfrom guardrails.validators import EventDetail, Filter\nfrom git.repo import Repo", "from guardrails.validators import EventDetail, Filter\nfrom git.repo import Repo\n\nimport structlog\nlog = structlog.get_logger()\n\n\n@register_validator(name=\"filepath\", data_type=\"string\")\nclass FilePath(Validator):\n    \"\"\"Validate value is a valid file path.\n    - Name for `format` attribute: `filepath`\n    - Supported data types: `string`\n    \"\"\"\n    def validate(self, key: str, value: Any, schema: Dict) -> Union[Dict, List]:\n        log.debug(\"Validating filepath...\", key=key, value=value)\n\n        # Check if filepath is a string\n        if not isinstance(value, str):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is not a string.\",\n                None,\n            )\n\n        # Is it normalized?\n        if value != os.path.normpath(value):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is not normalized.\",\n                None,\n            )\n\n        # Check that it's not a directory\n        # The directory does not need to exist, so we can't use os.path.isdir\n        if value.endswith(os.sep):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is a directory.\",\n                None,\n            )\n\n        return schema\n\n    def fix(self, error: EventDetail) -> Dict:\n        value = error.value\n\n        # Check if filepath is a string\n        if not isinstance(value, str):\n            error.schema[error.key] = Filter()\n            return error.schema\n\n        # Fix paths like \\\\dir\\file.txt to /dir/file.txt\n        value = os.path.normpath(value)\n        error.schema[error.key] = value\n\n        # Check that it's not a directory\n        try:\n            self.validate(error.key, value, error.schema)\n        except EventDetail:\n            error.schema[error.key] = Filter()\n            return error.schema\n\n        return error.schema", "class FilePath(Validator):\n    \"\"\"Validate value is a valid file path.\n    - Name for `format` attribute: `filepath`\n    - Supported data types: `string`\n    \"\"\"\n    def validate(self, key: str, value: Any, schema: Dict) -> Union[Dict, List]:\n        log.debug(\"Validating filepath...\", key=key, value=value)\n\n        # Check if filepath is a string\n        if not isinstance(value, str):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is not a string.\",\n                None,\n            )\n\n        # Is it normalized?\n        if value != os.path.normpath(value):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is not normalized.\",\n                None,\n            )\n\n        # Check that it's not a directory\n        # The directory does not need to exist, so we can't use os.path.isdir\n        if value.endswith(os.sep):\n            raise EventDetail(\n                key,\n                value,\n                schema,\n                f\"File path '{value}' is a directory.\",\n                None,\n            )\n\n        return schema\n\n    def fix(self, error: EventDetail) -> Dict:\n        value = error.value\n\n        # Check if filepath is a string\n        if not isinstance(value, str):\n            error.schema[error.key] = Filter()\n            return error.schema\n\n        # Fix paths like \\\\dir\\file.txt to /dir/file.txt\n        value = os.path.normpath(value)\n        error.schema[error.key] = value\n\n        # Check that it's not a directory\n        try:\n            self.validate(error.key, value, error.schema)\n        except EventDetail:\n            error.schema[error.key] = Filter()\n            return error.schema\n\n        return error.schema", ""]}
{"filename": "autopr/__init__.py", "chunked_list": [""]}
{"filename": "autopr/gh_actions_entrypoint.py", "chunked_list": ["import json\nimport os\nfrom typing import Any\n\nfrom autopr.main import Settings, MainService\n\nimport yaml\n\nfrom autopr.log_config import configure_logging\nfrom autopr.services.event_service import GitHubEventService", "from autopr.log_config import configure_logging\nfrom autopr.services.event_service import GitHubEventService\nfrom autopr.services.publish_service import GitHubPublishService\n\nconfigure_logging()\n\nimport structlog\nlog = structlog.get_logger()\n\n\nclass GitHubActionSettings(Settings):\n    class Config:\n        env_prefix = 'INPUT_'\n\n        @classmethod\n        def parse_env_var(cls, field_name: str, raw_val: str) -> Any:\n            if field_name.endswith('agent_config'):\n                return yaml.safe_load(raw_val)\n            return cls.json_loads(raw_val)  # type: ignore", "\n\nclass GitHubActionSettings(Settings):\n    class Config:\n        env_prefix = 'INPUT_'\n\n        @classmethod\n        def parse_env_var(cls, field_name: str, raw_val: str) -> Any:\n            if field_name.endswith('agent_config'):\n                return yaml.safe_load(raw_val)\n            return cls.json_loads(raw_val)  # type: ignore", "\n\nclass GithubMainService(MainService):\n    settings_class = GitHubActionSettings\n    publish_service_class = GitHubPublishService\n\n    def get_repo_path(self):\n        return os.environ['GITHUB_WORKSPACE']\n\n    def get_event(self):\n        github_token = os.environ['INPUT_GITHUB_TOKEN']\n        event_name = os.environ['GITHUB_EVENT_NAME']\n        event_path = os.environ['GITHUB_EVENT_PATH']\n\n        # Load event\n        with open(event_path, 'r') as f:\n            event_json = json.load(f)\n\n        print(json.dumps(event_json, indent=2))\n\n        # Extract event\n        event_service = GitHubEventService(\n            github_token=github_token,\n        )\n        return event_service.parse_event(event_name, event_json)\n\n    def get_publish_service(self, **additional_kwargs):\n        github_token = os.environ['INPUT_GITHUB_TOKEN']\n        run_id = os.environ['GITHUB_RUN_ID']\n\n        return super().get_publish_service(\n            token=github_token,\n            run_id=run_id,\n            **additional_kwargs,\n        )", "\n\nif __name__ == '__main__':\n    log.info(\"Starting gh_actions_entrypoint.py\")\n\n    main_service = GithubMainService()\n    main_service.run()\n"]}
{"filename": "autopr/log_config.py", "chunked_list": ["def configure_logging(pretty=True):\n    import logging\n    import structlog\n\n    logging.basicConfig(\n        level=logging.INFO\n    )\n\n    if pretty:\n        processors = [\n            structlog.dev.ConsoleRenderer(colors=True),\n        ]\n    else:\n        processors = []\n\n    structlog.configure(\n        processors=processors,\n        cache_logger_on_first_use=True,\n    )", ""]}
{"filename": "autopr/agents/base.py", "chunked_list": ["import traceback\nfrom typing import ClassVar, Collection, Type\n\nfrom git.repo import Repo\n\nfrom autopr.models.events import EventUnion\nfrom autopr.services.action_service import ActionService\nfrom autopr.services.chain_service import ChainService\nfrom autopr.services.commit_service import CommitService\nfrom autopr.services.diff_service import DiffService", "from autopr.services.commit_service import CommitService\nfrom autopr.services.diff_service import DiffService\nfrom autopr.services.publish_service import PublishService\nfrom autopr.services.rail_service import RailService\n\nimport structlog\n\n\nclass Agent:\n    \"\"\"\n    Base class for agents.\n    An agent is responsible for orchestrating the entire process of generating a pull request by invoking actions.\n    \"\"\"\n\n    #: The ID of the agent, used to identify it in the settings.\n    id: ClassVar[str]\n\n    def __init__(\n        self,\n        rail_service: RailService,\n        chain_service: ChainService,\n        diff_service: DiffService,\n        commit_service: CommitService,\n        publish_service: PublishService,\n        action_service: ActionService,\n        repo: Repo,\n        **kwargs,\n    ):\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n        self.diff_service = diff_service\n        self.commit_service = commit_service\n        self.publish_service = publish_service\n        self.action_service = action_service\n        self.repo = repo\n\n        self.log = structlog.get_logger(service=\"brain\",\n                                        id=self.id)\n        if kwargs:\n            self.log.warning(\"Agent did not use additional options\", kwargs=kwargs)\n\n    def handle_event(\n        self,\n        event: EventUnion,\n    ) -> None:\n        \"\"\"\n        Override this method to implement your own logic of the agent.\n        This method should orchestrate the entire process of generating a pull request.\n        \"\"\"\n        raise NotImplementedError", "class Agent:\n    \"\"\"\n    Base class for agents.\n    An agent is responsible for orchestrating the entire process of generating a pull request by invoking actions.\n    \"\"\"\n\n    #: The ID of the agent, used to identify it in the settings.\n    id: ClassVar[str]\n\n    def __init__(\n        self,\n        rail_service: RailService,\n        chain_service: ChainService,\n        diff_service: DiffService,\n        commit_service: CommitService,\n        publish_service: PublishService,\n        action_service: ActionService,\n        repo: Repo,\n        **kwargs,\n    ):\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n        self.diff_service = diff_service\n        self.commit_service = commit_service\n        self.publish_service = publish_service\n        self.action_service = action_service\n        self.repo = repo\n\n        self.log = structlog.get_logger(service=\"brain\",\n                                        id=self.id)\n        if kwargs:\n            self.log.warning(\"Agent did not use additional options\", kwargs=kwargs)\n\n    def handle_event(\n        self,\n        event: EventUnion,\n    ) -> None:\n        \"\"\"\n        Override this method to implement your own logic of the agent.\n        This method should orchestrate the entire process of generating a pull request.\n        \"\"\"\n        raise NotImplementedError", "\n\ndef get_all_agents(parent=Agent) -> Collection[Type[Agent]]:\n    # import to initialize Action subclasses\n    import autopr.agents\n\n    descendants = set()\n    for subclass in parent.__subclasses__():\n        descendants.add(subclass)\n        # get subclasses of subclasses\n        descendants.update(get_all_agents(subclass))\n    return descendants", ""]}
{"filename": "autopr/agents/__init__.py", "chunked_list": ["from os.path import dirname, basename, isfile, join\nimport glob\n\n# Import all modules in this directory\n\nfile_modules = glob.glob(join(dirname(__file__), \"*.py\"))\nfile_basenames = [basename(f)[:-3] for f in file_modules if isfile(f) and not f.endswith('__init__.py')]\ndirectory_module_inits = glob.glob(join(dirname(__file__), \"*\", \"__init__.py\"))\ndirectory_modules = [dirname(f) for f in directory_module_inits]\ndirectory_module_basenames = [basename(f) for f in directory_modules]", "directory_modules = [dirname(f) for f in directory_module_inits]\ndirectory_module_basenames = [basename(f) for f in directory_modules]\n__all__ = file_basenames + directory_module_basenames\n\nfrom . import *\n"]}
{"filename": "autopr/agents/plan_and_code.py", "chunked_list": ["from typing import Collection\n\nimport structlog\n\nfrom autopr.actions.base import ContextDict\nfrom autopr.actions.utils.commit import PullRequestDescription, CommitPlan, PullRequestAmendment\nfrom autopr.agents.base import Agent\nfrom autopr.models.events import EventUnion, PullRequestCommentEvent, IssueLabelEvent\n\nlog = structlog.get_logger()", "\nlog = structlog.get_logger()\n\n\nclass PlanAndCode(Agent):\n    \"\"\"\n    A simple agent that:\n    - plans commits from issues or pull request comments,\n    - opens and responds to pull requests,\n    - writes commits to the pull request.\n    \"\"\"\n\n    #: The ID of the agent, used to identify it in the settings\n    id = \"plan_and_code\"\n\n    def __init__(\n        self,\n        *args,\n        inspection_actions: Collection[str] = (\n            \"look_at_files\",\n        ),\n        planning_actions: Collection[str] = (\n            \"plan_pull_request\",\n            \"request_more_information\"\n        ),\n        response_actions: Collection[str] = (\n            \"plan_commits\",\n            \"request_more_information\",\n        ),\n        codegen_actions: Collection[str] = (\n            'new_file',\n            'edit_file',\n        ),\n        max_codegen_iterations: int = 5,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.inspection_actions = inspection_actions\n        self.planning_actions = planning_actions\n        self.response_actions = response_actions\n        self.codegen_actions = codegen_actions\n        self.max_codegen_iterations = max_codegen_iterations\n\n    def write_commit(\n        self,\n        commit_plan: CommitPlan,\n        context: ContextDict,\n        context_headings: dict[str, str]\n    ) -> ContextDict:\n        self.publish_service.start_section(f\"\ud83d\udd28 Writing commit {commit_plan.commit_message}\")\n\n        # Set the current commit in the context\n        context['current_commit'] = commit_plan\n\n        # Clear action_history in the context for each commit\n        context['action_history'] = []\n\n        # Generate the changes\n        context = self.action_service.run_actions_iteratively(\n            self.codegen_actions,\n            context,\n            context_headings={\n                'current_commit': 'Commit we are currently generating',\n                'action_history': 'Actions that have been run so far',\n                **context_headings,\n            },\n            max_iterations=self.max_codegen_iterations,\n            include_finished=True,\n        )\n\n        # Show the diff in the progress report\n        diff = self.diff_service.get_diff()\n        if diff:\n            self.publish_service.publish_code_block(\n                heading=\"Diff\",\n                code=diff,\n                language=\"diff\",\n            )\n            self.publish_service.end_section(f\"\u2705 Committed {commit_plan.commit_message}\")\n        else:\n            self.publish_service.end_section(f\"\u26a0\ufe0f Empty commit {commit_plan.commit_message}\")\n\n        # Commit and push the changes\n        self.commit_service.commit(commit_plan.commit_message, push=True)\n\n        return context\n\n    def respond_to_pr_comment(\n        self,\n        event: PullRequestCommentEvent,\n    ):\n        # Checkout the head branch\n        head_branch = event.pull_request.head_branch\n        base_branch = event.pull_request.base_branch\n        self.repo.heads[head_branch].checkout()\n\n        # Get list of commits on the branch\n        commits = [\n            commit.message\n            for commit in self.repo.iter_commits(f\"origin/{base_branch}..{head_branch}\")\n        ]\n\n        # Initialize the context\n        context = ContextDict(\n            commits_in_pull_request=commits,\n            request=event.new_comment,\n        )\n\n        # Run the inspection actions\n        context = self.action_service.run_actions_iteratively(\n            self.inspection_actions,\n            context,\n            max_iterations=1,\n        )\n\n        # Run the response actions\n        context = self.action_service.run_actions_iteratively(\n            self.response_actions,\n            context,\n            max_iterations=1,\n        )\n\n        # Get the pull request description from the context\n        if 'pull_request_amendment' not in context:\n            # Stop the agent if the action did not return a pull request description\n            return\n        pull_request_amendment = context['pull_request_amendment']\n        if not isinstance(pull_request_amendment, PullRequestAmendment):\n            raise ValueError(f\"Action returned a pull request amendment that is not a PullRequestDescription object\")\n\n        if pull_request_amendment.comment:\n            # Comment on the pull request\n            self.publish_service.publish_comment(pull_request_amendment.comment)\n\n        if pull_request_amendment.commits:\n            # Stop the agent if the action did not return a pull request description\n            commits = pull_request_amendment.commits\n            if not all(isinstance(commit, CommitPlan)\n                       for commit in commits):\n                raise ValueError(f\"Action returned commits that are not CommitPlan objects\")\n\n            for current_commit in commits:\n                context = self.write_commit(\n                    current_commit,\n                    context,\n                    context_headings={\n                        'pull_request': \"Pull request that we are adding commits to\",\n                        'request': \"Request that we are responding to\",\n                    }\n                )\n                self.log.info(f\"Committed {current_commit.commit_message}\")\n\n    def create_pull_request(\n        self,\n        event: IssueLabelEvent,\n    ) -> None:\n        # Create new branch\n        self.commit_service.overwrite_new_branch()\n\n        issue = event.issue\n\n        # Initialize the context\n        context = ContextDict(\n            issue=issue,\n        )\n\n        # Run the inspection actions\n        context = self.action_service.run_actions_iteratively(\n            self.inspection_actions,\n            context,\n            max_iterations=1,\n        )\n\n        # Generate the pull request plan (commit messages and relevant filepaths)\n        context = self.action_service.run_actions_iteratively(\n            self.planning_actions,\n            context,\n            max_iterations=1,\n        )\n\n        # Get the pull request description from the context\n        if 'pull_request_description' not in context:\n            # Stop the agent if the action did not return a pull request description\n            return\n        pr_desc = context['pull_request_description']\n        if not isinstance(pr_desc, PullRequestDescription):\n            raise TypeError(f\"Actions returned a pull request description of type \"\n                            f\"{type(pr_desc)} instead of PullRequestDescription\")\n\n        # Publish the description\n        self.publish_service.set_title(pr_desc.title)\n        self.publish_service.publish_comment(pr_desc.body)\n\n        for current_commit in pr_desc.commits:\n            context = self.write_commit(\n                current_commit,\n                context,\n                context_headings={\n                    'pull_request_description': 'Plan for the pull request',\n                },\n            )\n\n    def handle_event(\n        self,\n        event: EventUnion,\n    ) -> None:\n        if isinstance(event, IssueLabelEvent):\n            self.create_pull_request(event)\n        elif isinstance(event, PullRequestCommentEvent):\n            self.respond_to_pr_comment(event)\n        else:\n            raise NotImplementedError(f\"Event type {type(event)} not supported\")", ""]}
{"filename": "autopr/repos/completions_repo.py", "chunked_list": ["from typing import Optional\n\nimport openai\nimport openai.error\nimport structlog\nfrom tenacity import retry, retry_if_exception_type, wait_random_exponential, stop_after_attempt\n\nfrom autopr.services.publish_service import PublishService\nfrom autopr.utils import tokenizer\n", "from autopr.utils import tokenizer\n\n\nclass CompletionsRepo:\n    \"\"\"\n    Repository that handles running completions through a language model.\n    \"\"\"\n\n    #: A list of models that this repo implements. Set this in the subclass.\n    models: list[str]\n\n    def __init__(\n        self,\n        publish_service: PublishService,\n        model: str,\n        max_tokens: int = 2000,\n        min_tokens: int = 1000,\n        context_limit: int = 8192,\n        temperature: float = 0.8,\n    ):\n        self.publish_service = publish_service\n        self.model = model\n        self.max_tokens = max_tokens\n        self.min_tokens = min_tokens\n        self.context_limit = context_limit\n        self.temperature = temperature\n\n        self.tokenizer = tokenizer.get_tokenizer()\n        self.log = structlog.get_logger(repo=self.__class__.__name__)\n\n    def complete(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        examples: Optional[list[tuple[str, str]]] = None,\n        temperature: Optional[float] = None,\n    ) -> str:\n        log = self.log.bind(\n            model=self.model,\n            prompt=prompt,\n        )\n        if examples is None:\n            examples = []\n        if system_prompt is None:\n            system_prompt = \"You are a helpful assistant.\"\n        if temperature is None:\n            temperature = self.temperature\n\n        length = len(self.tokenizer.encode(prompt))\n        max_tokens = min(self.max_tokens, self.context_limit - length)\n\n        self.log.info(\n            \"Running completion\",\n            prompt=prompt,\n        )\n        try:\n            result = self._complete(\n                system_prompt=system_prompt,\n                examples=examples,\n                prompt=prompt,\n                max_tokens=max_tokens,\n                temperature=temperature,\n            )\n        except openai.error.InvalidRequestError as e:\n            if \"`gpt-4` does not exist\" not in str(e):\n                raise e\n\n            # Warn that the user doesn't have access to gpt-4\n            while len(self.publish_service.sections_stack) > 1:\n                self.publish_service.end_section()\n            self.publish_service.publish_update(\n                \"\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f Your OpenAI API key does not have access to the `gpt-4` model. \"\n                \"Please note that ChatGPT Plus does not give you access to the `gpt-4` API; \" \n                \"you need to sign up on [the GPT-4 API waitlist](https://openai.com/waitlist/gpt-4-api). \"\n            )\n            raise e\n\n        log.info(\n            \"Completed\",\n            result=result,\n        )\n        return result\n\n    def _complete(\n        self,\n        system_prompt: str,\n        examples: list[tuple[str, str]],\n        prompt: str,\n        max_tokens: int,\n        temperature: float,\n    ) -> str:\n        \"\"\"\n        Subclass this method to implement the language model call.\n        \"\"\"\n        raise NotImplementedError", "\n\nopenai_retry_if_union = (\n    retry_if_exception_type(openai.error.Timeout)\n    | retry_if_exception_type(openai.error.APIError)\n    | retry_if_exception_type(openai.error.APIConnectionError)\n    | retry_if_exception_type(openai.error.RateLimitError)\n    | retry_if_exception_type(openai.error.ServiceUnavailableError)\n)\n", ")\n\n\nclass OpenAIChatCompletionsRepo(CompletionsRepo):\n    models = [\n        'gpt-4',\n        'gpt-3.5-turbo',\n    ]\n\n    @retry(\n        retry=openai_retry_if_union,\n        wait=wait_random_exponential(min=1, max=240),\n        stop=stop_after_attempt(8)\n    )\n    def _complete(\n        self,\n        prompt: str,\n        system_prompt: str,\n        examples: list[tuple[str, str]],\n        max_tokens: int,\n        temperature: float,\n    ) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n        ]\n        for example in examples:\n            messages.append({\"role\": \"user\", \"content\": example[0]})\n            messages.append({\"role\": \"assistant\", \"content\": example[1]})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n\n        openai_response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        if openai_response is None or not isinstance(openai_response, dict):\n            self.log.error(\n                \"OpenAI chat completion returned invalid response\",\n                openai_response=openai_response,\n            )\n            return \"\"\n        self.log.info(\n            \"Ran OpenAI chat completion\",\n            openai_response=openai_response,\n        )\n        return openai_response[\"choices\"][0][\"message\"][\"content\"]", "\n\nclass OpenAICompletionsRepo(CompletionsRepo):\n    models = [\n        'text-davinci-003',\n    ]\n\n    @retry(\n        retry=openai_retry_if_union,\n        wait=wait_random_exponential(min=1, max=240),\n        stop=stop_after_attempt(8)\n    )\n    def _complete(\n        self,\n        prompt: str,\n        system_prompt: str,\n        examples: list[tuple[str, str]],\n        max_tokens: int,\n        temperature: float,\n    ) -> str:\n        prompt = system_prompt\n        for example in examples:\n            prompt += f\"\\n\\n{example[0]}\\n{example[1]}\"\n        prompt += f\"\\n\\n{prompt}\"\n\n        openai_response = openai.Completion.create(\n            model=self.model,\n            prompt=prompt,\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        self.log.info(\n            \"Ran OpenAI completion\",\n            openai_response=openai_response,\n        )\n        if openai_response is None or not isinstance(openai_response, dict):\n            self.log.error(\n                \"OpenAI chat completion returned invalid response\",\n                openai_response=openai_response,\n            )\n            return \"\"\n        return openai_response[\"choices\"][0][\"text\"]", "\n\ndef get_completions_repo(\n    publish_service: PublishService,\n    model: str = \"gpt-4\",\n    max_tokens: int = 2000,\n    min_tokens: int = 1000,\n    context_limit: int = 8192,\n    temperature: float = 0.8,\n):\n    repo_implementations = CompletionsRepo.__subclasses__()\n    for repo_implementation in repo_implementations:\n        if model in repo_implementation.models:\n            return repo_implementation(\n                publish_service=publish_service,\n                model=model,\n                max_tokens=max_tokens,\n                min_tokens=min_tokens,\n                context_limit=context_limit,\n                temperature=temperature,\n            )\n    raise ValueError(f\"Model {model} not implemented\")", ""]}
{"filename": "autopr/repos/__init__.py", "chunked_list": [""]}
{"filename": "autopr/actions/new_file.py", "chunked_list": ["import os\nfrom typing import Optional, Any\n\nfrom autopr.actions.base import Action, ContextDict\nfrom autopr.actions.utils.file import GeneratedHunkOutputParser, ContextFile, GeneratedFileHunk, make_file_context, \\\n    add_element_to_context_list\n\nfrom autopr.actions.utils.commit import CommitPlan\nfrom autopr.models.prompt_chains import PromptChain\n", "from autopr.models.prompt_chains import PromptChain\n\n\nclass NewFileChain(PromptChain):\n    output_parser = GeneratedHunkOutputParser()\n    prompt_template = f\"\"\"Hey, we've got a new file to create.\n\n{{context}}\n\nThis is the codebase subset we decided to look at:\n```\n{{context_hunks}}\n```\n\nThis is the plan for the file we're creating:\n```\n{{plan}}\n```\n\nPlease send me the contents of the file.\n\n{{format_instructions}}\"\"\"\n\n    context: ContextDict\n    context_hunks: list[ContextFile]\n    plan: str", "\n\nclass NewFile(Action):\n    id = \"new_file\"\n    description = \"Make a new file.\"\n\n    class Arguments(Action.Arguments):\n        filepath: str\n        description: str\n\n        output_spec = f\"\"\"\n<string\n    name=\"filepath\"\n    description=\"Path to the newly created file.\"\n    required=\"true\"\n/>\n<string\n    name=\"description\"\n    description=\"Description of the contents of the new file.\"\n    required=\"true\"\n/>\n\"\"\"\n\n    def run(\n        self,\n        args: Arguments,\n        context: ContextDict,\n    ) -> ContextDict:\n        self.publish_service.update_section(title=f\"\ud83d\udcc4 Creating new file: {args.filepath}\")\n\n        # Check if file exists\n        repo_path = self.repo.working_tree_dir\n        assert repo_path is not None\n        filepath = os.path.join(repo_path, args.filepath)\n        if os.path.exists(filepath):\n            self.publish_service.update_section(title=f\"\u274c Failed to create new file: {args.filepath} \"\n                                                      f\"(file already exists)\")\n            return add_element_to_context_list(\n                context,\n                \"action_history\",\n                f\"Failed to create new file: {args.filepath} (file already exists)\"\n            )\n        # Check if filename is a directory (doesnt exist yet)\n        if os.path.basename(filepath) == \"\":\n            self.publish_service.update_section(title=f\"\u274c Failed to create new file: {args.filepath} \"\n                                                      f\"(filename is a directory)\")\n            return add_element_to_context_list(\n                context,\n                \"action_history\",\n                f\"Failed to create new file: {args.filepath} (filename is a directory)\"\n            )\n\n        # Get the other relevant hunks\n        if \"current_commit\" not in context:\n            self.log.error(\"Context does not specify current_commit\")\n            context_hunks = []\n        elif not isinstance(current_commit := context[\"current_commit\"], CommitPlan):\n            self.log.error(\"Context current_commit is not a CommitPlan\")\n            context_hunks = []\n        else:\n            # TODO remove the hunk that's being edited, but leave surrounding context\n            #  or rather, build better context by iteratively asking about it\n            context_hunks = make_file_context(self.repo, current_commit)\n\n        # Run new file langchain\n        new_file_chain = NewFileChain(\n            context=context,\n            context_hunks=context_hunks,\n            plan=args.description,\n        )\n        new_file_hunk: Optional[GeneratedFileHunk] = self.chain_service.run_chain(new_file_chain)\n        if new_file_hunk is None:\n            self.publish_service.update_section(title=f\"\u274c Failed to create new file: {args.filepath}\")\n            return add_element_to_context_list(\n                context,\n                \"action_history\",\n                f\"Failed to create new file: {args.filepath}\"\n            )\n\n        # Create dirs\n        dirpath = os.path.dirname(filepath)\n        os.makedirs(dirpath, exist_ok=True)\n\n        # Write file\n        path = os.path.join(repo_path, filepath)\n        with open(path, \"w\") as f:\n            f.write(new_file_hunk.contents)\n\n        self.publish_service.update_section(title=f\"\ud83d\udcc4 Created new file: {args.filepath}\")\n        if outcome := new_file_hunk.outcome:\n            outcome = \" with outcome: \" + outcome\n        return add_element_to_context_list(\n            context,\n            \"action_history\",\n            f\"Created {args.filepath}{outcome}\"\n        )", ""]}
{"filename": "autopr/actions/base.py", "chunked_list": ["from typing import ClassVar, List, Type, Collection, Any, Optional\n\nimport pydantic\nimport structlog\nfrom git.repo import Repo\nfrom pydantic import Extra\n\nfrom autopr.models.rail_objects import RailObject\nfrom autopr.services.chain_service import ChainService\nfrom autopr.services.publish_service import PublishService", "from autopr.services.chain_service import ChainService\nfrom autopr.services.publish_service import PublishService\nfrom autopr.services.rail_service import RailService\n\n\nclass ContextDict(dict[str, Any]):\n    \"\"\"\n    A dictionary of context variables passed between actions.\n    Overrides `__str__` to format the context in a prompt-friendly way.\n    \"\"\"\n\n    def select_keys(self, keys: Collection[str]) -> \"ContextDict\":\n        \"\"\"\n        Select a subset of keys from the context.\n        \"\"\"\n        for key in keys:\n            if key not in self:\n                raise KeyError(f\"Key {key} not found in context\")\n        return ContextDict({key: self[key] for key in keys})\n\n    @staticmethod\n    def key_to_heading(key: str) -> str:\n        \"\"\"\n        Convert a context key to a heading.\n        \"\"\"\n        return key.replace(\"_\", \" \").title()\n\n    def as_string(\n        self,\n        variable_headings: Optional[dict[str, str]] = None,\n        enclosure_mark: str = \"+-+\",\n    ):\n        \"\"\"\n        Format the context as a string.\n\n        Parameters\n        ----------\n\n        variable_headings\n            A dictionary mapping context keys to headings.\n            If not provided, the keys will be used as headings.\n        enclosure_mark\n            The string to use to enclose each variable.\n        \"\"\"\n        if variable_headings is None:\n            variable_headings = {}\n\n        # Add any missing keys to the variable headings\n        for key in self:\n            if key not in variable_headings:\n                variable_headings[key] = self.key_to_heading(key)\n\n        context_string = \"Given context variables enclosed by \" + enclosure_mark + \":\"\n        for key, value in self.items():\n            # Format the value as a string\n            if isinstance(value, list):\n                valstr = \"\\n\".join(str(item) for item in value)\n            else:\n                valstr = str(value)\n\n            # Add the variable to the context string\n            context_string += f\"\"\"\\n\\n{variable_headings[key]}:\n{enclosure_mark}\n{valstr}\n{enclosure_mark}\"\"\"\n\n        return context_string\n\n    def __str__(self):\n        return self.as_string()", "\n\nclass Action:\n    \"\"\"\n    Base class for all actions.\n\n    Actions are the basic unit of work in the autonomous agent.\n    They are responsible for performing a single task, affecting the environment in some way,\n    and returning a string describing the result of the action.\n    \"\"\"\n\n    # The ID of the action, used to identify it in the AutoPR configuration. `finished` is a reserved ID. Required.\n    id: ClassVar[str]\n\n    # The description of the action, used to describe it to the LLM upon action selection.\n    description: ClassVar[str] = \"\"\n\n    class Arguments(RailObject):\n        \"\"\"\n        Arguments for the action.\n        If the action takes arguments that should be queried by the LLM (e.g., filename),\n        subclass and override this class in your Action subclass,\n        defining the arguments as pydantic fields.\n\n        Example:\n            ```\n            class Arguments(Action.Arguments):\n                filepaths: list[str]\n\n                output_spec = \"<list name='filepaths'><string/></list>\"\n            ```\n        \"\"\"\n\n        # `output_spec` describes the structure of the arguments for guardrails,\n        # to be mapped into the model subclass.\n        # Soon to be deprecated, as guardrails adopts pydantic Field parameters.\n        output_spec: ClassVar[str] = \"\"\n\n    def run(\n        self,\n        args: Arguments,\n        context: ContextDict,\n    ) -> ContextDict:\n        \"\"\"\n        Run the action.\n        The return value should be a string describing the action's result.\n\n        Parameters\n        ----------\n        args : Arguments\n            The arguments provided by the LLM, as per the overridden `Arguments` class.\n        context : ContextDict\n            The context for the action,\n            containing information about e.g., the issue, the current commit, and other actions' results.\n\n        Returns\n        -------\n        ContextDict\n            The updated context, adding any new information to the context.\n        \"\"\"\n        raise NotImplementedError\n\n    def __init__(\n        self,\n        repo: Repo,\n        rail_service: RailService,\n        chain_service: ChainService,\n        publish_service: PublishService,\n        **kwargs,\n    ):\n        # Use repo for git operations\n        self.repo = repo\n\n        # Use rail and chain services to run LLM calls\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n\n        # Use publish service for in-progress updates\n        self.publish_service = publish_service\n\n        # Use log for more detailed debug messages\n        self.log = structlog.get_logger(service=\"action\",\n                                        id=self.id)\n\n        # Define kwargs to pass configuration to actions via `action_config`\n        if kwargs:\n            self.log.warning(\"Action received unexpected kwargs that it will ignore\",\n                             kwargs=kwargs)", "\n\ndef get_all_actions(parent=Action) -> Collection[Type[Action]]:\n    # import to initialize Action subclasses\n    import autopr.actions\n\n    descendants = set()\n    for subclass in parent.__subclasses__():\n        descendants.add(subclass)\n        # get subclasses of subclasses\n        descendants.update(get_all_actions(subclass))\n    return descendants", ""]}
{"filename": "autopr/actions/look_at_files.py", "chunked_list": ["from typing import Union, Optional\n\nimport pydantic\nfrom git.repo import Repo\n\nfrom autopr.actions.base import Action, ContextDict\nfrom autopr.actions.utils.commit import PullRequestDescription\nfrom autopr.models.artifacts import Issue\nfrom autopr.models.prompt_rails import PromptRail\nfrom autopr.models.rail_objects import RailObject", "from autopr.models.prompt_rails import PromptRail\nfrom autopr.models.rail_objects import RailObject\nfrom autopr.utils.repo import repo_to_file_descriptors, trim_chunk, filter_seen_chunks, FileDescriptor\n\n\nclass InitialFileSelectResponse(RailObject):\n    output_spec = \"\"\"<list name=\"filepaths\">\n    <string\n        description=\"Files in this repository that we should look at.\"\n    />\n</list>\"\"\"\n\n    filepaths: list[str]\n\n    @classmethod\n    def get_rail_spec(cls):\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{cls.output_spec}\n</output>\n<instructions>\nYou are a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@json_suffix_prompt_examples\n</instructions>\n<prompt>\nGiven the following document surrounded by `+++++`, answer the following questions. \nIf the answer doesn't exist in the document, enter `null`.\n\n+++++\n{{{{raw_document}}}}\n+++++\n\nExtract information from this document and return a JSON that follows the correct schema.\nIf looking at files would be a waste of time, please submit an empty list.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"", "\n\nclass InitialFileSelect(PromptRail):\n    # Select files given issue and files in repo\n    prompt_template = f\"\"\"Hey, somebody just opened an issue in my repo, could you help me write a pull request?\n\n{{context}}\n\nThe list of files in the repo is:\n```{{filepaths_with_token_lengths}}```\n\nShould we take a look at any files? If so, pick only a few files (max {{token_limit}} tokens). \nRespond with a very short rationale, and a list of files.\nIf looking at files would be a waste of time with regard to the issue, respond with an empty list.\"\"\"\n\n    output_type = InitialFileSelectResponse\n    # extra_params = {\n    #     'temperature': 0,\n    # }\n\n    context: ContextDict\n    file_descriptors: list[FileDescriptor]\n    token_limit: int\n\n    def get_string_params(self) -> dict[str, str]:\n        return {\n            'context': str(self.context),\n            'filepaths_with_token_lengths': '\\n'.join([\n                file_descriptor.filepaths_with_token_lengths_to_str()\n                for file_descriptor in self.file_descriptors\n            ]),\n            'token_limit': str(self.token_limit),\n        }", "\n\nclass LookAtFilesResponse(RailObject):\n    output_spec = \"\"\"<string \n    name=\"notes\" \n    description=\"Notes relevant to solving the issue, that we will use to plan our code commits.\" \n    length=\"1 1000\"\n    on-fail=\"noop\" \n/>\n<list name=\"filepaths_we_should_look_at\">\n    <string\n        description=\"The paths to files we should look at next in the repo. Drop any files that are a waste of time with regard to the issue.\"\n    />\n</list>\"\"\"\n\n    filepaths_we_should_look_at: Optional[list[str]] = None\n    notes: str\n\n    @classmethod\n    def get_rail_spec(cls):\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{cls.output_spec}\n</output>\n<instructions>\nYou are a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@json_suffix_prompt_examples\n</instructions>\n<prompt>\nGiven the following document surrounded by `+++++`, answer the following questions. \nIf the answer doesn't exist in the document, enter `null`.\n\n+++++\n{{{{raw_document}}}}\n+++++\n\nExtract information from this document and return a JSON that follows the correct schema.\nIf looking at files would be a waste of time, please submit an empty list.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"", "\n\nclass LookAtFiles(PromptRail):\n    # Select files given issue, unseen files in repo, and notes\n    prompt_template = f\"\"\"Hey, somebody just submitted an issue, could you own it, and write a pull request?\n\n{{context}}\n\nWe've decided to look at these files:\n```{{codebase}}```\n\nThe list of files in the repo that we haven't taken a look at yet:\n```{{filepaths_with_token_lengths}}```\n\nTake some notes that will help us plan our code commits, in an effort to close the issue. \nAlso, should we take a look at any other files? If so, pick only a few files (max {{token_limit}} tokens).\nRespond with some very brief notes, and a list of files to continue looking at. \nIf looking at files would be a waste of time with regard to the issue, respond with an empty list.\"\"\"\n\n    output_type = LookAtFilesResponse\n    # extra_params = {\n    #     'temperature': 0.2,\n    # }\n\n    context: ContextDict\n    selected_file_contents: list[FileDescriptor]\n    prospective_file_descriptors: list[FileDescriptor]\n    token_limit: int\n    _filtered_prospective_file_descriptors: list[FileDescriptor] = pydantic.PrivateAttr(default_factory=list)\n\n    def get_string_params(self) -> dict[str, str]:\n        self._filtered_prospective_file_descriptors = filter_seen_chunks(\n            self.selected_file_contents, self.prospective_file_descriptors\n        )\n\n        return {\n            'context': str(self.context),\n            'codebase': '\\n'.join([\n                file_descriptor.filenames_and_contents_to_str()\n                for file_descriptor in self.selected_file_contents\n            ]),\n            'filepaths_with_token_lengths': '\\n'.join([\n                file_descriptor.filepaths_with_token_lengths_to_str()\n                for file_descriptor in self._filtered_prospective_file_descriptors\n            ]),\n            'token_limit': str(self.token_limit),\n        }\n\n    def trim_params(self) -> bool:\n        if trim_chunk(self.selected_file_contents):\n            return True\n        return super().trim_params()", "\n\nclass ContinueLookingAtFiles(PromptRail):\n    # Continue selecting files and generating fp_notes given issue, unseen files in repo, and notes\n    prompt_template = f\"\"\"Hey, somebody just submitted an issue, could you own it, and write a pull request?\n\n{{context}}\n\nSome notes we've taken while looking at files so far:\n```{{notes}}```\n\nWe've decided to look at these files:\n```{{codebase}}```\n\nThe list of files in the repo that we haven't taken a look at yet:\n```{{filepaths_with_token_lengths}}```\n\nTake some notes that will help us plan commits and write code to fix the issue. \nAlso, let me know if we should take a look at any other files \u2013 our budget is {{token_limit}} tokens.\"\"\"\n\n    output_type = LookAtFilesResponse\n    # extra_params = {\n    #     'temperature': 0.2,\n    # }\n\n    context: ContextDict\n    notes: str\n    selected_file_contents: list[FileDescriptor]\n    prospective_file_descriptors: list[FileDescriptor]\n    token_limit: int\n    _filtered_prospective_file_descriptors: list[FileDescriptor] = pydantic.PrivateAttr(default_factory=list)\n\n    def get_string_params(self) -> dict[str, str]:\n        self._filtered_prospective_file_descriptors = filter_seen_chunks(\n            self.selected_file_contents, self.prospective_file_descriptors\n        )\n\n        return {\n            'context': str(self.context),\n            'notes': self.notes,\n            'codebase': '\\n'.join([\n                file_descriptor.filenames_and_contents_to_str()\n                for file_descriptor in self.selected_file_contents\n            ]),\n            'filepaths_with_token_lengths': '\\n'.join([\n                file_descriptor.filepaths_with_token_lengths_to_str()\n                for file_descriptor in self._filtered_prospective_file_descriptors\n            ]),\n            'token_limit': str(self.token_limit),\n        }\n\n    def trim_params(self) -> bool:\n        if trim_chunk(self.selected_file_contents):\n            return True\n        return super().trim_params()", "\n\nclass InspectFiles(Action):\n    \"\"\"\n    Iteratively select files to look at, taking notes while looking at them.\n\n    File selection is performed by giving the agent a list of filenames, and asking it to select a subset of them.\n\n    Parameters\n    ----------\n    file_context_token_limit: int\n        The maximum size taken up by the file context (concatenated file chunks) in the prompt.\n\n    file_chunk_size: int\n        The maximum token size of each chunk that a file is split into.\n    \"\"\"\n\n    id = 'look_at_files'\n\n    def __init__(\n        self,\n        *args,\n        file_context_token_limit: int = 5000,\n        file_chunk_size: int = 500,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.file_context_token_limit = file_context_token_limit\n        self.file_chunk_size = file_chunk_size\n\n    def get_initial_filepaths(\n        self,\n        files: list[FileDescriptor],\n        context: ContextDict,\n    ) -> list[str]:\n        self.log.debug('Getting filepaths to look at...')\n\n        response = self.rail_service.run_prompt_rail(\n            InitialFileSelect(\n                context=context,\n                file_descriptors=files,\n                token_limit=self.file_context_token_limit\n            )\n        )\n        if response is None or not isinstance(response, InitialFileSelectResponse):\n            real_filepaths = []\n        else:\n            real_filepaths = [fp for fp in response.filepaths if fp is not None]\n            if len(response.filepaths) != len(real_filepaths):\n                self.log.debug(f'Got hallucinated filepaths: {set(response.filepaths) - set(real_filepaths)}')\n            if real_filepaths:\n                self.log.debug(f'Got filepaths:')\n                for filepath in real_filepaths:\n                    self.log.debug(f' -  {filepath}')\n\n        return real_filepaths\n\n    def write_notes_about_files(\n        self,\n        files: list[FileDescriptor],\n        context: ContextDict,\n        filepaths: list[str]\n    ) -> str:\n        self.log.debug('Looking at files...')\n\n        file_contents = [\n            f.copy(deep=True) for f in files\n            if f.path in filepaths\n        ]\n        rail = LookAtFiles(\n            context=context,\n            selected_file_contents=file_contents,\n            prospective_file_descriptors=[f.copy(deep=True) for f in files],\n            token_limit=self.file_context_token_limit,\n        )\n        response = self.rail_service.run_prompt_rail(rail)\n        if response is None or not isinstance(response, LookAtFilesResponse):\n            raise ValueError('Error looking at files')\n        filepaths = response.filepaths_we_should_look_at or []\n        notes = response.notes\n\n        viewed_filepaths_up_to_chunk: dict[str, int] = {}\n        reasks = self.rail_service.num_reasks\n        while filepaths and reasks > 0:\n            reasks -= 1\n\n            # See if all requested files have already been viewed\n            for fp in rail.selected_file_contents:\n                viewed_filepaths_up_to_chunk[fp.path] = fp.end_chunk\n            file_contents = []\n            for f in files:\n                if f.path not in filepaths:\n                    continue\n                if f.path in viewed_filepaths_up_to_chunk:\n                    chunk_num = viewed_filepaths_up_to_chunk[f.path]\n                    if chunk_num == f.end_chunk:\n                        continue\n                    new_f = f.copy(deep=True)\n                    new_f.start_chunk = chunk_num\n                else:\n                    new_f = f.copy(deep=True)\n                file_contents.append(new_f)\n\n            if not file_contents:\n                break\n\n            self.log.debug(f'Looking at more files... ({reasks} reasks left)')\n            for fp in filepaths:\n                self.log.debug(f' - {fp}')\n            rail = ContinueLookingAtFiles(\n                context=context,\n                notes=notes,\n                selected_file_contents=file_contents,\n                prospective_file_descriptors=rail._filtered_prospective_file_descriptors,\n                token_limit=self.file_context_token_limit,\n            )\n            response = self.rail_service.run_prompt_rail(rail)\n            if response is None or not isinstance(response, LookAtFilesResponse):\n                filepaths = []\n            else:\n                filepaths = response.filepaths_we_should_look_at or []\n                notes += f'\\n{response.notes}'\n\n        return notes\n\n    def run(\n        self,\n        args: Action.Arguments,\n        context: ContextDict,\n    ) -> ContextDict:\n        self.publish_service.update_section(\"\ud83d\udcd6 Looking at files\")\n\n        # Get files\n        files = repo_to_file_descriptors(self.repo, self.file_context_token_limit, self.file_chunk_size)\n\n        # Get the filepaths to look at\n        filepaths = self.get_initial_filepaths(files, context)\n\n        if filepaths:\n            # Look at the files\n            notes = self.write_notes_about_files(files, context, filepaths)\n        else:\n            notes = \"The repository's contents were irrelevant, only create new files to address the issue.\"\n\n        context['notes'] = notes\n\n        self.publish_service.update_section(\"\ud83d\udcd6 Looked at files\")\n        return context", ""]}
{"filename": "autopr/actions/plan_commits.py", "chunked_list": ["from autopr.actions.base import Action, ContextDict\nfrom autopr.actions.utils.commit import CommitPlan, PullRequestAmendment\nfrom autopr.models.artifacts import Issue, PullRequest\n\n\nclass PlanCommits(Action):\n    id = \"plan_commits\"\n    description = \"Plan commits to add to the pull request.\"\n\n    class Arguments(Action.Arguments):\n        pull_request_amendment: PullRequestAmendment\n\n        output_spec = f\"\"\"<object\n    name='pull_request_amendment'\n>\n{PullRequestAmendment.output_spec}\n</object>\"\"\"\n\n    def run(self, arguments: Arguments, context: ContextDict) -> ContextDict:\n        self.publish_service.update_section(\"\ud83d\udcdd Planning commits\")\n        # Add the commits to the pull request\n        context['pull_request_amendment'] = arguments.pull_request_amendment\n\n        # If there is a comment, add it to the issue\n        if arguments.pull_request_amendment.comment:\n            self.publish_service.publish_comment(arguments.pull_request_amendment.comment)\n\n        self.publish_service.update_section(\"\ud83d\udcdd Planned commits\")\n        # Return the context\n        return context", ""]}
{"filename": "autopr/actions/edit_file.py", "chunked_list": ["import os\nimport re\nfrom typing import Optional\n\nfrom autopr.actions.base import Action, ContextDict\nfrom autopr.actions.new_file import NewFile\nfrom autopr.actions.utils.commit import CommitPlan\nfrom autopr.actions.utils.file import add_element_to_context_list, GeneratedHunkOutputParser, ContextFile, \\\n    ContextCodeHunk, make_file_context, GeneratedFileHunk\nfrom autopr.models.prompt_chains import PromptChain", "    ContextCodeHunk, make_file_context, GeneratedFileHunk\nfrom autopr.models.prompt_chains import PromptChain\n\n\nclass RewriteCodeHunkChain(PromptChain):\n    output_parser = GeneratedHunkOutputParser()\n    prompt_template = f\"\"\"Hey, we've got a new code hunk to diff.\n\n{{context}}\n\nThis is the codebase subset we decided to look at:\n```\n{{context_hunks}}\n```\n\nThis is the hunk we're rewriting:\n```\n{{hunk_contents}}\n```\n\nThis is the plan for how we want to rewrite the hunk:\n```\n{{plan}}\n```\n\nPlease rewrite the hunk to match the plan, but do not include any lines prefixed with | in the result.\n\nRULES:\n- ONLY rewrite the lines prefixed with *, \n- submit only the lines without the * prefix,\n- do not preserve the relative leading indentation of the lines (start the hunk's indentation at 0).\n\n{{format_instructions}}\"\"\"\n\n    context: ContextDict\n    context_hunks: list[ContextFile]\n    hunk_contents: ContextCodeHunk\n    plan: str", "\n\nclass EditFile(Action):\n    id = \"edit_file\"\n    description = \"Rewrite a code hunk in a file.\"\n\n    class Arguments(Action.Arguments):\n        filepath: str\n        description: str\n        start_line: Optional[int] = None\n        end_line: Optional[int] = None\n\n        output_spec = f\"\"\"\n<string\n    name=\"filepath\"\n    description=\"Path to the file to be edited.\"\n    required=\"true\"\n/>\n<string\n    name=\"description\"\n    description=\"Description of the changes to be made to the file.\"\n    required=\"true\"\n/>\n<integer\n    name=\"start_line\"\n    description=\"The line number of the first line of the hunk to be edited.\"\n    format=\"positive\"\n    required=\"false\"\n    on-fail=\"noop\"\n/>\n<integer\n    name=\"end_line\"\n    description=\"The line number of the last line of the hunk to be edited. Keep the hunk as short as possible while fulfilling the description.\"\n    format=\"positive\"\n    required=\"false\"\n    on-fail=\"noop\"\n/>\n\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        num_context_lines: int = 3,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n        # num_context_lines is the number of lines of context to show around the hunk being edited\n        self.num_context_lines = num_context_lines\n\n    @staticmethod\n    def _split_into_lines(text: str) -> list[str]:\n        lines = text.splitlines()\n        # If text ends with a newline, we want to keep that as a line\n        if text.rstrip() != text:\n            lines.append(\"\")\n        return lines\n\n    def run(\n        self,\n        args: Arguments,\n        context: ContextDict,\n    ) -> ContextDict:\n        # Check if file exists\n        repo_path = self.repo.working_tree_dir\n        assert repo_path\n        filepath = os.path.join(repo_path, args.filepath)\n        if not os.path.exists(filepath):\n            create_file_action = NewFile(\n                repo=self.repo,\n                rail_service=self.rail_service,\n                chain_service=self.chain_service,\n                publish_service=self.publish_service,\n            )\n            create_args = NewFile.Arguments(\n                filepath=args.filepath,\n                description=args.description,\n            )\n            self.log.warning(f\"File {filepath} does not exist, creating it instead.\")\n            return create_file_action.run(create_args, context)\n\n        self.publish_service.update_section(title=f\"\u270d\ufe0f Editing file: {args.filepath}\")\n\n        # Grab file contents\n        with open(filepath, \"r\") as f:\n            lines = self._split_into_lines(f.read())\n\n        # Get relevant hunk\n        start_line, end_line = args.start_line, args.end_line\n        if not lines:\n            code_hunk = ContextCodeHunk(\n                code_hunk=[],\n            )\n            indent = 0\n        elif start_line is None or end_line is None:\n            line_nums = list(range(1, len(lines) + 1))\n            code_hunk = ContextCodeHunk(\n                code_hunk=list(zip(line_nums, lines)),\n                highlight_line_numbers=line_nums,\n            )\n            indent = 0\n        else:\n            # Limit line numbers\n            start_line, end_line = min(max(start_line, 1), len(lines)), min(max(end_line, 1), len(lines))\n            end_line = max(start_line, end_line)\n\n            code_hunk_lines: list[tuple[int, str]] = []\n            context_start_line = max(1, start_line - self.num_context_lines)\n            context_end_line = min(len(lines), end_line + self.num_context_lines)\n            for line_num in range(context_start_line, context_end_line + 1):\n                code_hunk_lines.append((line_num, lines[line_num - 1]))\n            highlight_line_nums = list(range(start_line, end_line + 1))\n            code_hunk = ContextCodeHunk(\n                code_hunk=code_hunk_lines,\n                highlight_line_numbers=highlight_line_nums,\n            )\n\n            # Find the indentation common to all highlighted lines in the hunk\n            highlighted_lines = [\n                lines[line_num - 1]\n                for line_num in highlight_line_nums\n            ]\n            lines_for_indent = [\n                len(line) - len(line.lstrip())\n                for line in highlighted_lines\n                if line.strip()\n            ]\n            if not lines_for_indent:\n                indent = 0\n            else:\n                indent = min(lines_for_indent)\n\n        # Get the other relevant hunks\n        if \"current_commit\" not in context:\n            self.log.error(\"Context does not specify current_commit\")\n            context_hunks = []\n        elif not isinstance(current_commit := context[\"current_commit\"], CommitPlan):\n            self.log.error(\"Context current_commit is not a CommitPlan\")\n            context_hunks = []\n        else:\n            # TODO remove the hunk that's being edited, but leave surrounding context\n            #  or rather, build better context by iteratively asking about it\n            context_hunks = make_file_context(self.repo, current_commit)\n\n        # Run edit file langchain\n        edit_file_chain = RewriteCodeHunkChain(\n            context=context,\n            context_hunks=context_hunks,\n            hunk_contents=code_hunk,\n            plan=args.description,\n        )\n        edit_file_hunk: Optional[GeneratedFileHunk] = self.chain_service.run_chain(edit_file_chain)\n        if edit_file_hunk is None:\n            self.publish_service.update_section(title=f\"\u274c Failed to edit file: {args.filepath}\")\n            return add_element_to_context_list(\n                context,\n                \"action_history\",\n                f\"Failed to edit file {args.filepath}\",\n            )\n\n        new_lines = edit_file_hunk.contents\n        # if all lines start with \"<int> | \" or \"<int> * \"\n        if all(re.match(r\"^\\s*\\d+\\s*[|*]\\s*\", line)\n               for line in new_lines.splitlines()):\n            # Remove the prefix from all lines\n            new_lines = \"\\n\".join(\n                re.sub(r\"^\\s*\\d+\\s*[|*]\\s*\", \"\", line)\n                for line in new_lines.splitlines()\n            )\n\n        # Add indentation to new lines\n        indented_lines = []\n        for line in self._split_into_lines(new_lines):\n            if not line.strip():\n                indented_lines.append(\"\")\n                continue\n            indented_lines.append(\" \" * indent + line)\n\n        # Replace lines in file\n        if start_line is not None and end_line is not None:\n            lines = lines[:start_line - 1] + indented_lines + lines[end_line:]\n        else:\n            lines = indented_lines\n\n        # Write file\n        path = os.path.join(repo_path, filepath)\n        with open(path, \"w\") as f:\n            f.write(\"\\n\".join(lines))\n\n        self.publish_service.update_section(title=f\"\u270d\ufe0f Edited file: {args.filepath}\")\n        return add_element_to_context_list(\n            context,\n            \"action_history\",\n            f\"Edited file, with outcome: {edit_file_hunk.outcome}\"\n        )", ""]}
{"filename": "autopr/actions/__init__.py", "chunked_list": ["from os.path import dirname, basename, isfile, join\nimport glob\n\n# Import all modules in this directory\n\nfile_modules = glob.glob(join(dirname(__file__), \"*.py\"))\nfile_basenames = [basename(f)[:-3] for f in file_modules if isfile(f) and not f.endswith('__init__.py')]\ndirectory_module_inits = glob.glob(join(dirname(__file__), \"*\", \"__init__.py\"))\ndirectory_modules = [dirname(f) for f in directory_module_inits]\ndirectory_module_basenames = [basename(f) for f in directory_modules]", "directory_modules = [dirname(f) for f in directory_module_inits]\ndirectory_module_basenames = [basename(f) for f in directory_modules]\n__all__ = file_basenames + directory_module_basenames\n\nfrom . import *\n"]}
{"filename": "autopr/actions/request_more_info.py", "chunked_list": ["from autopr.actions.base import Action, ContextDict\nfrom autopr.models.artifacts import Issue\n\n\nclass RequestMoreInfo(Action):\n    id = \"request_more_information\"\n    description = \"Request more information from the user.\"\n\n    class Arguments(Action.Arguments):\n        message: str\n\n        output_spec = \"<string name='message'/>\"\n\n    def run(self, arguments: Arguments, context: ContextDict) -> ContextDict:\n        # Get the issue from the context\n        if 'issue' in context:\n            issue = context['issue']\n            if not isinstance(issue, Issue):\n                self.log.error(f\"Expected issue to be of type Issue, got {type(issue)}\")\n                raise TypeError(f\"Expected issue to be of type Issue, got {type(issue)}\")\n            issue_number = issue.number\n        else:\n            issue_number = None\n\n        # Get the message from the arguments\n        message = arguments.message\n\n        # Add a comment to the issue\n        success = self.publish_service.publish_comment(message, issue_number)\n        if not success:\n            self.log.error(f\"Failed to comment on issue\")\n            raise RuntimeError(f\"Failed to comment on issue\")\n\n        # Return the context\n        return context", ""]}
{"filename": "autopr/actions/plan_pr.py", "chunked_list": ["from autopr.actions.base import Action, ContextDict\nfrom autopr.actions.utils.commit import PullRequestDescription\nfrom autopr.models.artifacts import Issue\nfrom autopr.models.prompt_rails import PromptRail\n\n\nclass ProposePullRequestRail(PromptRail):\n    # Generate proposed list of commit messages, given notes and issue\n    prompt_template = f\"\"\"Hey somebody just submitted an issue, could you own it, write some commits, and a pull request?\n\nThese are notes we took while looking at the repo:\n```{{notes_taken_while_looking_at_files}}```\n\nThis is the issue that was opened:\n```{{issue}}```\n\nWhen you're done, send me the pull request title, body, and a list of commits, each coupled with which files we should be looking at to write the commit's code.\nEnsure you specify the files relevant to the commit, especially if the commit is a refactor.\nFolders are created automatically; do not make them in their own commit.\"\"\"\n\n    output_type = PullRequestDescription\n    # extra_params = {\n    #     'temperature': 0.1,\n    # }\n\n    notes_taken_while_looking_at_files: str\n    issue: Issue", "\n\nclass PlanPullRequest(Action):\n    id = \"plan_pull_request\"\n    description = \"Propose a pull request to the user.\"\n\n    def propose_pull_request(self, issue: Issue, notes: str) -> PullRequestDescription:\n        self.log.debug('Getting commit messages...')\n        pr_desc = self.rail_service.run_prompt_rail(\n            ProposePullRequestRail(\n                issue=issue,\n                notes_taken_while_looking_at_files=notes,\n            )\n        )\n        if pr_desc is None or not isinstance(pr_desc, PullRequestDescription):\n            raise ValueError('Error proposing pull request')\n        return pr_desc\n\n    def run(\n        self,\n        args: Action.Arguments,\n        context: ContextDict,\n    ) -> ContextDict:\n        self.publish_service.update_section('\ud83d\udcdd Planning pull request')\n        # Get issue\n        if 'issue' not in context:\n            raise ValueError('No `issue` key in context')\n        issue = context['issue']\n        if not isinstance(issue, Issue):\n            raise ValueError(f'Context `issue` is type {type(issue)}, not Issue')\n\n        # Get notes\n        if 'notes' not in context:\n            raise ValueError('No `notes` key in context')\n        notes = context['notes']\n        if not isinstance(notes, str):\n            raise ValueError(f'Context `notes` is type {type(notes)}, not str')\n\n        # Write pull request description\n        pr_desc = self.propose_pull_request(issue, notes)\n\n        # Save the pull request description to the context\n        context['pull_request_description'] = pr_desc\n\n        self.publish_service.update_section('\ud83d\udcdd Planned pull request')\n        return context", ""]}
{"filename": "autopr/actions/utils/__init__.py", "chunked_list": [""]}
{"filename": "autopr/actions/utils/file.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\nfrom typing import Optional, Any\n\nfrom git.repo import Repo\n\nfrom autopr.actions.base import ContextDict\nfrom autopr.actions.utils.commit import CommitPlan\nfrom autopr.models.prompt_chains import PromptChain", "from autopr.actions.utils.commit import CommitPlan\nfrom autopr.models.prompt_chains import PromptChain\n\nimport pydantic\n\nfrom langchain.schema import BaseOutputParser\n\nimport structlog\nlog = structlog.get_logger()\n", "log = structlog.get_logger()\n\n\n###\n# File hunk generation\n###\n\n\nclass GeneratedFileHunk(pydantic.BaseModel):\n    \"\"\"\n    A generated hunk of code, followed by the outcome of the generation.\n    TODO explore better ways of reflecting on the output of the generation than `outcome`.\n    \"\"\"\n    contents: str\n    outcome: str", "class GeneratedFileHunk(pydantic.BaseModel):\n    \"\"\"\n    A generated hunk of code, followed by the outcome of the generation.\n    TODO explore better ways of reflecting on the output of the generation than `outcome`.\n    \"\"\"\n    contents: str\n    outcome: str\n\n\nclass GeneratedHunkOutputParser(BaseOutputParser):\n    \"\"\"\n    An output parser for the generated hunk, in the format of:\n        ```\n        <string>\n        ```\n        {\n            \"outcome\": <string>\n        }\n    \"\"\"\n    def parse(self, output: str) -> Optional[GeneratedFileHunk]:\n        output_lines = output.split(\"\\n\")\n\n        try:\n            # Filter through the output until the first ``` is found\n            while not output_lines[0].startswith(\"```\"):\n                output_lines.pop(0)\n            output_lines.pop(0)\n\n            # Find the last ``` line\n            reversed_lines = output_lines[::-1]\n            while not reversed_lines[0].startswith(\"```\"):\n                reversed_lines.pop(0)\n            reversed_lines.pop(0)\n            lines = reversed_lines[::-1]\n\n            code = \"\\n\".join(lines)\n\n            # Retrieve the JSON\n            json_lines = output_lines[len(lines) + 1:]\n            try:\n                outcome = json.loads(\"\\n\".join(json_lines))[\"outcome\"]\n            except json.JSONDecodeError:\n                outcome = \"\"\n        except:\n            # TODO reask to fix the output\n            return None\n        return GeneratedFileHunk(\n            contents=code,\n            outcome=outcome,\n        )\n\n    def get_format_instructions(self) -> str:\n        return \"\"\"RESPONSE FORMAT INSTRUCTIONS\n----------------------------\n\nWhen responding to me, please use the following format. Make sure you return both the code enclosed in backticks and the JSON immediately after.\n\n```\n<string>\n```\n{\n    \"outcome\": string  # A description of the outcome of the attempt to rewrite the file hunk according to the problem statement.\n}\n\"\"\"", "\nclass GeneratedHunkOutputParser(BaseOutputParser):\n    \"\"\"\n    An output parser for the generated hunk, in the format of:\n        ```\n        <string>\n        ```\n        {\n            \"outcome\": <string>\n        }\n    \"\"\"\n    def parse(self, output: str) -> Optional[GeneratedFileHunk]:\n        output_lines = output.split(\"\\n\")\n\n        try:\n            # Filter through the output until the first ``` is found\n            while not output_lines[0].startswith(\"```\"):\n                output_lines.pop(0)\n            output_lines.pop(0)\n\n            # Find the last ``` line\n            reversed_lines = output_lines[::-1]\n            while not reversed_lines[0].startswith(\"```\"):\n                reversed_lines.pop(0)\n            reversed_lines.pop(0)\n            lines = reversed_lines[::-1]\n\n            code = \"\\n\".join(lines)\n\n            # Retrieve the JSON\n            json_lines = output_lines[len(lines) + 1:]\n            try:\n                outcome = json.loads(\"\\n\".join(json_lines))[\"outcome\"]\n            except json.JSONDecodeError:\n                outcome = \"\"\n        except:\n            # TODO reask to fix the output\n            return None\n        return GeneratedFileHunk(\n            contents=code,\n            outcome=outcome,\n        )\n\n    def get_format_instructions(self) -> str:\n        return \"\"\"RESPONSE FORMAT INSTRUCTIONS\n----------------------------\n\nWhen responding to me, please use the following format. Make sure you return both the code enclosed in backticks and the JSON immediately after.\n\n```\n<string>\n```\n{\n    \"outcome\": string  # A description of the outcome of the attempt to rewrite the file hunk according to the problem statement.\n}\n\"\"\"", "\n\n###\n# File context creation\n###\n\n\nclass ContextCodeHunk(pydantic.BaseModel):\n    \"\"\"\n    A hunk of code that is part of the context for code generation.\n    \"\"\"\n    highlight_line_numbers: list[int] = pydantic.Field(default_factory=list)\n    code_hunk: list[tuple[int, str]]\n\n    def __str__(self):\n        if not self.code_hunk:\n            return ''\n        max_line_num_width = len(str(self.code_hunk[-1][0]))\n        lines = []\n        for num, line_content in self.code_hunk:\n            num_width = len(str(num))\n            line = ' ' * (max_line_num_width - num_width) + str(num)\n            if num in self.highlight_line_numbers:\n                line += ' * '\n            else:\n                line += ' | '\n            line += line_content\n            lines.append(line)\n        return '\\n'.join(lines)", "\n\nclass ContextFile(pydantic.BaseModel):\n    \"\"\"\n    A file that is part of the context for code generation.\n    \"\"\"\n    filepath: str\n    code_hunks: list[ContextCodeHunk]\n\n    def __str__(self):\n        code_hunks_s = '\\n\\n'.join(\n            [str(code_hunk) for code_hunk in self.code_hunks]\n        )\n        return f\">>> File: {self.filepath}\\n\\n\" + code_hunks_s", "\n\ndef split_into_lines(text: str) -> list[str]:\n    lines = text.splitlines()\n    # If text ends with a newline, we want to keep that as a line\n    if text.rstrip() != text:\n        lines.append(\"\")\n    return lines\n\n\ndef get_lines(\n    repo: Repo,\n    filepath: str,\n    start_line: Optional[int] = None,\n    end_line: Optional[int] = None,\n) -> Optional[list[tuple[int, str]]]:\n    working_dir = repo.working_tree_dir\n    assert working_dir is not None\n    path = os.path.join(working_dir, filepath)\n    if not os.path.exists(path):\n        log.error(f\"File {filepath} not in repo\")\n        return None\n    if not os.path.isfile(path):\n        log.error(f\"{filepath} is not a file\")\n        return None\n\n    with open(path, 'r') as f:\n        lines = split_into_lines(f.read())\n    code_hunk: list[tuple[int, str]] = []\n\n    # Get and limit line numbers\n    start_line = start_line or 1\n    start_line = min(max(start_line, 1), len(lines))\n    end_line = end_line or len(lines)\n    end_line = min(max(end_line, 1), len(lines))\n    end_line = max(start_line, end_line)\n\n    for line_num in range(start_line, end_line + 1):\n        code_hunk.append((line_num, lines[line_num - 1]))\n    return code_hunk", "\n\ndef get_lines(\n    repo: Repo,\n    filepath: str,\n    start_line: Optional[int] = None,\n    end_line: Optional[int] = None,\n) -> Optional[list[tuple[int, str]]]:\n    working_dir = repo.working_tree_dir\n    assert working_dir is not None\n    path = os.path.join(working_dir, filepath)\n    if not os.path.exists(path):\n        log.error(f\"File {filepath} not in repo\")\n        return None\n    if not os.path.isfile(path):\n        log.error(f\"{filepath} is not a file\")\n        return None\n\n    with open(path, 'r') as f:\n        lines = split_into_lines(f.read())\n    code_hunk: list[tuple[int, str]] = []\n\n    # Get and limit line numbers\n    start_line = start_line or 1\n    start_line = min(max(start_line, 1), len(lines))\n    end_line = end_line or len(lines)\n    end_line = min(max(end_line, 1), len(lines))\n    end_line = max(start_line, end_line)\n\n    for line_num in range(start_line, end_line + 1):\n        code_hunk.append((line_num, lines[line_num - 1]))\n    return code_hunk", "\n\ndef make_file_context(\n    repo: Repo,\n    commit: CommitPlan,\n) -> list[ContextFile]:\n    hunks_by_filepath = defaultdict(list)\n    for hunk in commit.relevant_file_hunks:\n        fp = hunk.filepath\n        hunks_by_filepath[fp].append(hunk)\n\n    context = []\n    for fp, hunks in hunks_by_filepath.items():\n        code_hunks = []\n        for hunk in hunks:\n            lines = get_lines(\n                repo=repo,\n                filepath=fp,\n                start_line=hunk.start_line,\n                end_line=hunk.end_line,\n            )\n            if lines is None:\n                continue\n            code_hunks.append(\n                ContextCodeHunk(\n                    code_hunk=lines,\n                )\n            )\n\n        if code_hunks:\n            context.append(\n                ContextFile(\n                    filepath=fp,\n                    code_hunks=code_hunks,\n                )\n            )\n\n    return context", "\n\ndef add_element_to_context_list(\n    context: ContextDict,\n    key: str,\n    element: Any,\n) -> ContextDict:\n    if key not in context:\n        context[key] = []\n    context[key].append(element)\n    return context", ""]}
{"filename": "autopr/actions/utils/commit.py", "chunked_list": ["from typing import Optional\n\nimport pydantic\n\nfrom autopr.models.rail_objects import RailObject\n\n\nclass FileReference(RailObject):\n    output_spec = \"\"\"<string\n    name=\"filepath\"\n    description=\"The path to the file we are looking at.\"\n    format=\"filepath\"\n    on-fail=\"fix\"\n/>\n<integer\n    name=\"start_line\"\n    description=\"The line number of the first line of the hunk.\"\n    format=\"positive\"\n    required=\"false\"\n    on-fail=\"noop\"\n/>\n<integer\n    name=\"end_line\"\n    description=\"The line number of the last line of the hunk.\"\n    format=\"positive\"\n    required=\"false\"\n    on-fail=\"noop\"\n/>\"\"\"\n\n    filepath: str\n    start_line: Optional[int] = None\n    end_line: Optional[int] = None\n\n    def __str__(self):\n        s = self.filepath\n        if self.start_line is not None:\n            s += f\":L{self.start_line}\"\n            if self.end_line is not None:\n                s += f\"-L{self.end_line}\"\n        return s", "\n\nclass CommitPlan(RailObject):\n    output_spec = f\"\"\"<string\n    name=\"commit_message\"\n    description=\"The commit message, concisely describing the changes made.\"\n    length=\"1 100\"\n    on-fail=\"noop\"\n/>\n<list\n    name=\"relevant_file_hunks\"\n    description=\"The files we should be looking at while writing this commit. Include files that whose contents will be called by the code in this commit, and files that will be changed by this commit.\"\n>\n<object>\n{FileReference.output_spec}\n</object>\n</list>\n<string\n    name=\"commit_changes_description\"\n    description=\"A description of the changes made in this commit, in the form of a list of bullet points.\"\n    required=\"true\"\n    length=\"1 1000\"\n/>\"\"\"\n\n    commit_message: str\n    relevant_file_hunks: list[FileReference] = pydantic.Field(default_factory=list)\n    commit_changes_description: str = \"\"\n\n    def __str__(self):\n        return self.commit_message + '\\n\\n' + self.commit_changes_description", "\n\nclass PullRequestDescription(RailObject):\n    output_spec = f\"\"\"<string \n    name=\"title\" \n    description=\"The title of the pull request.\"\n/>\n<string \n    name=\"body\" \n    description=\"The body of the pull request.\"\n/>\n<list \n    name=\"commits\"\n    on-fail=\"reask\"\n    description=\"The commits that will be made in this pull request. Commits must change the code in the repository, and must not be empty.\"\n>\n<object>\n{CommitPlan.output_spec}\n</object>\n</list>\"\"\"\n\n    title: str\n    body: str\n    commits: list[CommitPlan]\n\n    def __str__(self):\n        pr_text_description = f\"Title: {self.title}\\n\\n{self.body}\\n\\n\"\n        for i, commit_plan in enumerate(self.commits):\n            prefix = f\" {' ' * len(str(i + 1))}  \"\n            changes_prefix = f\"\\n{prefix}  \"\n            pr_text_description += (\n                f\"{str(i + 1)}. Commit: {commit_plan.commit_message}\\n\"\n                f\"{prefix}Files: \"\n                f\"{', '.join([str(fh) for fh in commit_plan.relevant_file_hunks])}\\n\"\n                f\"{prefix}Changes:\"\n                f\"{changes_prefix}{changes_prefix.join(commit_plan.commit_changes_description.splitlines())}\\n\"\n            )\n        return pr_text_description", "\n\nclass PullRequestAmendment(RailObject):\n    output_spec = f\"\"\"<string\n    name=\"comment\"\n    description=\"A comment to add to the pull request.\"\n    required=\"false\"\n    on-fail=\"noop\"\n/>\n<list\n    name=\"commits\"\n    required=\"false\"\n    on-fail=\"reask\"\n    description=\"Additional commits to add to the pull request. Commits must change the code in the repository, and must not be empty.\"\n>\n<object>\n{CommitPlan.output_spec}\n</object>\n</list>\"\"\"\n\n    comment: Optional[str] = None\n    commits: list[CommitPlan] = pydantic.Field(default_factory=list)", ""]}
{"filename": "autopr/utils/tokenizer.py", "chunked_list": ["from typing import Optional\n\nimport transformers\n\n# FIXME use tiktoken instead\n\n_cached_tokenizer = None\n\n\ndef get_tokenizer():\n    global _cached_tokenizer\n\n    if _cached_tokenizer is None:\n        _cached_tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n    return _cached_tokenizer", "\ndef get_tokenizer():\n    global _cached_tokenizer\n\n    if _cached_tokenizer is None:\n        _cached_tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n    return _cached_tokenizer\n"]}
{"filename": "autopr/utils/repo.py", "chunked_list": ["from typing import Optional\n\nfrom git import Blob\nfrom git.repo import Repo\nimport pydantic\n\nimport structlog\nimport os\n\nfrom pathspec import PathSpec", "\nfrom pathspec import PathSpec\nfrom pathspec.patterns.gitwildmatch import GitWildMatchPattern\nfrom autopr.utils.tokenizer import get_tokenizer\n\nlog = structlog.get_logger()\n\n\nclass FileDescriptor(pydantic.BaseModel):\n    path: str\n    token_length: int\n    chunks: list[list[tuple[int, str]]]  # list of (line number, line content) pairs\n    start_chunk: int = 0\n    end_chunk: int = -1  # this will be overwritten by the root validator\n\n    @pydantic.root_validator(pre=True)\n    def validate_end_chunk(cls, values):\n        if 'end_chunk' not in values:\n            values['end_chunk'] = len(values['chunks'])\n        return values\n\n    def filepaths_with_token_lengths_to_str(self) -> str:\n        # TODO give info on what chunks we've already seen\n        return f'{self.path} ({str(self.token_length)} tokens)'\n        # chunks_left = self.end_chunk - self.start_chunk\n        # return f'{self.path} ({str(self.token_length)} tokens) ({str(chunks_left)} chunks left)'\n\n    def filenames_and_contents_to_str(self) -> str:\n        contents = ''\n        if self.start_chunk > 0:\n            contents += f'... #  (omitting {self.start_chunk} chunks)\\n'\n        # TODO make the line numbers right-aligned with padded spaces,\n        #  so that the line numbers don't change the start of the line\n        contents += '\\n'.join([\n            f'{str(line_number)} {line_content}'\n            for chunk in self.chunks[self.start_chunk:self.end_chunk]\n            for line_number, line_content in chunk\n        ])\n        if self.end_chunk < len(self.chunks):\n            contents += f'\\n... #  (omitting {len(self.chunks) - self.end_chunk} chunks)'\n        return f'>>> Path: {self.path}:\\n\\n{contents}'", "class FileDescriptor(pydantic.BaseModel):\n    path: str\n    token_length: int\n    chunks: list[list[tuple[int, str]]]  # list of (line number, line content) pairs\n    start_chunk: int = 0\n    end_chunk: int = -1  # this will be overwritten by the root validator\n\n    @pydantic.root_validator(pre=True)\n    def validate_end_chunk(cls, values):\n        if 'end_chunk' not in values:\n            values['end_chunk'] = len(values['chunks'])\n        return values\n\n    def filepaths_with_token_lengths_to_str(self) -> str:\n        # TODO give info on what chunks we've already seen\n        return f'{self.path} ({str(self.token_length)} tokens)'\n        # chunks_left = self.end_chunk - self.start_chunk\n        # return f'{self.path} ({str(self.token_length)} tokens) ({str(chunks_left)} chunks left)'\n\n    def filenames_and_contents_to_str(self) -> str:\n        contents = ''\n        if self.start_chunk > 0:\n            contents += f'... #  (omitting {self.start_chunk} chunks)\\n'\n        # TODO make the line numbers right-aligned with padded spaces,\n        #  so that the line numbers don't change the start of the line\n        contents += '\\n'.join([\n            f'{str(line_number)} {line_content}'\n            for chunk in self.chunks[self.start_chunk:self.end_chunk]\n            for line_number, line_content in chunk\n        ])\n        if self.end_chunk < len(self.chunks):\n            contents += f'\\n... #  (omitting {len(self.chunks) - self.end_chunk} chunks)'\n        return f'>>> Path: {self.path}:\\n\\n{contents}'", "\n\ndef trim_chunk(file_desc_with_chunk_start_end: list[FileDescriptor]) -> bool:\n    if file_desc_with_chunk_start_end:\n        # Find file with most chunks\n        longest_num = 0\n        longest_i = 0\n        for i, desc in enumerate(file_desc_with_chunk_start_end):\n            num_chunks = desc.end_chunk - desc.start_chunk\n            if num_chunks > longest_num:\n                longest_num = num_chunks\n                longest_i = i\n\n        desc = file_desc_with_chunk_start_end[longest_i]\n\n        # If we've already looked at the whole file, remove it from the list\n        if desc.start_chunk == desc.end_chunk - 1:\n            del file_desc_with_chunk_start_end[longest_i]\n            return True\n\n        # Otherwise, shave a chunk off the end\n        desc.end_chunk -= 1\n        file_desc_with_chunk_start_end[longest_i] = desc\n        return True\n    return False", "\n\ndef filter_seen_chunks(seen_fds: list[FileDescriptor], prospective_fds: list[FileDescriptor]) -> list[FileDescriptor]:\n    fds_copy = [f.copy(deep=True) for f in prospective_fds]\n    omit_prospective_fd_indices = []\n    for selected_fd in seen_fds:\n        # If it's in prospective_file_descriptors, update its start_chunk\n        for prospective_fd in fds_copy:\n            if prospective_fd.path == selected_fd.path:\n                # If we've already looked at the whole file, remove it from the list\n                if prospective_fd.end_chunk == selected_fd.end_chunk:\n                    omit_prospective_fd_indices.append(fds_copy.index(prospective_fd))\n                else:\n                    prospective_fd.start_chunk = selected_fd.end_chunk\n                break\n    for i in sorted(omit_prospective_fd_indices, reverse=True):\n        del fds_copy[i]\n    return fds_copy", "\n\n_file_descriptor_cache: dict[tuple[bytes, int, int], list[FileDescriptor]] = {}\n\n\ndef repo_to_file_descriptors(repo: Repo, context_window: int, file_chunk_size: int) -> list[FileDescriptor]:\n    repo_tree = repo.head.commit.tree\n\n    key = (repo_tree.binsha, context_window, file_chunk_size)\n    ignore_patterns = parse_gptignore(repo)\n\n    if key in _file_descriptor_cache:\n        return [fd.copy(deep=True) for fd in _file_descriptor_cache[key]]\n\n    file_descriptor_list = []\n    for blob in repo_tree.traverse():\n        if not isinstance(blob, Blob):\n            continue\n\n        if blob.type == 'tree':\n            continue\n\n        if is_path_ignored(blob.path, ignore_patterns):\n            continue\n\n        try:\n            content = blob.data_stream.read().decode()\n        except UnicodeDecodeError:\n            log.debug(f\"Error decoding file: {blob.path}\")\n            continue\n\n        tokenizer = get_tokenizer()\n\n        tokens = tokenizer.encode(content)\n        # Split into chunks up to the last newline\n        chunks: list[list[tuple[int, str]]] = []\n        line_buffer = []\n        for i, line in enumerate(content.splitlines()):\n            line_buffer.append((i, line))\n            # FIXME speed this up\n            token_length = len(tokenizer.encode(\n                '\\n'.join([l[1] for l in line_buffer])\n            ))\n            if token_length >= file_chunk_size:\n                chunks.append(line_buffer)\n                line_buffer = []\n        if line_buffer:\n            chunks.append(line_buffer)\n\n        token_length = len(tokens)\n        file_descriptor_list.append(FileDescriptor(\n            path=blob.path,\n            token_length=token_length,\n            chunks=chunks,\n        ))\n\n    _file_descriptor_cache[key] = file_descriptor_list\n    return file_descriptor_list", "\n\ndef is_path_ignored(path: str, ignore_patterns: list[str]) -> bool:\n    # Ensure we're working with a relative path\n    relative_path = os.path.relpath(path)\n    pathspec = PathSpec.from_lines(GitWildMatchPattern, ignore_patterns)\n    return pathspec.match_file(relative_path)\n\n\ndef parse_gptignore(repo: Repo, gptignore_file: str = \".gptignore\") -> list[str]:\n    if gptignore_file not in repo.head.commit.tree:\n        return []\n\n    gptignore_blob = repo.head.commit.tree / gptignore_file\n    gptignore_content = gptignore_blob.data_stream.read().decode()\n\n    ignore_patterns = []\n    for line in gptignore_content.splitlines():\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            ignore_patterns.append(line)\n\n    return ignore_patterns", "\ndef parse_gptignore(repo: Repo, gptignore_file: str = \".gptignore\") -> list[str]:\n    if gptignore_file not in repo.head.commit.tree:\n        return []\n\n    gptignore_blob = repo.head.commit.tree / gptignore_file\n    gptignore_content = gptignore_blob.data_stream.read().decode()\n\n    ignore_patterns = []\n    for line in gptignore_content.splitlines():\n        line = line.strip()\n        if line and not line.startswith(\"#\"):\n            ignore_patterns.append(line)\n\n    return ignore_patterns", "\n"]}
{"filename": "autopr/tests/test_rail_specs.py", "chunked_list": ["from unittest.mock import MagicMock\n\nimport pytest\nimport guardrails as gr\nfrom autopr.actions.base import Action, ContextDict, get_all_actions\n\nfrom autopr.models.rail_objects import RailObject\n\n# Make sure to import, so all rail objects initialize\nimport autopr.actions", "# Make sure to import, so all rail objects initialize\nimport autopr.actions\nfrom autopr.services.action_service import ActionService\n\n\n@pytest.mark.parametrize(\n    \"rail_type\",\n    RailObject.__subclasses__()\n)\ndef test_guardrails_spec_validity(rail_type):\n    \"\"\"Test that all guardrails specs are valid.\"\"\"\n    rail_spec = rail_type.get_rail_spec()\n    print(rail_spec)\n    gr.Guard.from_rail_string(rail_spec)", ")\ndef test_guardrails_spec_validity(rail_type):\n    \"\"\"Test that all guardrails specs are valid.\"\"\"\n    rail_spec = rail_type.get_rail_spec()\n    print(rail_spec)\n    gr.Guard.from_rail_string(rail_spec)\n\n\nclass A(Action):\n    id = \"a\"\n    description = \"i am a\"\n\n    class Arguments(Action.Arguments):\n        a: str\n        b: str\n\n        output_spec = \"<string name='a'/><string name='b'/>\"\n\n    def run(self, arguments: Arguments, context: ContextDict) -> ContextDict:\n        return context", "class A(Action):\n    id = \"a\"\n    description = \"i am a\"\n\n    class Arguments(Action.Arguments):\n        a: str\n        b: str\n\n        output_spec = \"<string name='a'/><string name='b'/>\"\n\n    def run(self, arguments: Arguments, context: ContextDict) -> ContextDict:\n        return context", "\n\nclass B(Action):\n    id = \"b\"\n\n    def run(self, arguments: Action.Arguments, context: ContextDict) -> ContextDict:\n        return context\n\n\naction_service = ActionService(", "\naction_service = ActionService(\n    repo=MagicMock(),\n    completions_repo=MagicMock(),\n    publish_service=MagicMock(),\n    rail_service=MagicMock(),\n    chain_service=MagicMock(),\n)\n\n", "\n\nall_actions = get_all_actions()\nall_action_ids = [a.id for a in all_actions]\n\n\n@pytest.mark.parametrize(\n    \"rail_spec\",\n    [\n        action_service._write_action_selection_rail_spec(['a'], include_finished=True),", "    [\n        action_service._write_action_selection_rail_spec(['a'], include_finished=True),\n        action_service._write_action_selection_rail_spec(['b'], include_finished=False),\n        action_service._write_action_selection_rail_spec(['a', 'b'], include_finished=True),\n        action_service._write_action_selection_rail_spec(['a', 'b'], include_finished=False),\n        action_service._write_action_args_query_rail_spec(A.Arguments),\n    ]\n)\ndef test_mock_action_service_spec_validity(rail_spec):\n    \"\"\"Test that all action service specs are valid.\"\"\"\n    print(rail_spec)\n    gr.Guard.from_rail_string(rail_spec)", "def test_mock_action_service_spec_validity(rail_spec):\n    \"\"\"Test that all action service specs are valid.\"\"\"\n    print(rail_spec)\n    gr.Guard.from_rail_string(rail_spec)\n\n\n@pytest.mark.parametrize(\n    \"rail_spec\",\n    [\n        action_service._write_action_selection_rail_spec(all_action_ids, include_finished=True),", "    [\n        action_service._write_action_selection_rail_spec(all_action_ids, include_finished=True),\n    ] + [\n        action_service._write_action_selection_rail_spec([a_id], include_finished=True)\n        for a_id in all_action_ids\n    ]\n)\ndef test_real_action_service_spec_validity(rail_spec):\n    print(rail_spec)\n    gr.Guard.from_rail_string(rail_spec)"]}
{"filename": "autopr/tests/__init__.py", "chunked_list": [""]}
{"filename": "autopr/tests/test_publish_service.py", "chunked_list": ["from unittest.mock import patch, Mock\n\nfrom autopr.services.publish_service import GitHubPublishService\n\n\n@patch('requests.get')\n@patch('requests.post')\n@patch('requests.patch')\ndef test_github_publish_service(mock_patch, mock_post, mock_get):\n    # Mock responses for each request call\n    mock_get.return_value = Mock(status_code=200, json=lambda: [{'number': 1, 'node_id': 'node1'}])\n    mock_post.return_value = Mock(status_code=201, json=lambda: {'number': 2, 'id': 'comment1'})\n    mock_patch.return_value = Mock(status_code=200, json=lambda: {})\n\n    service = GitHubPublishService(\n        token='my_token',\n        run_id='123',\n        owner='user',\n        repo_name='repo',\n        head_branch='branch1',\n        base_branch='branch2',\n        issue=None,\n        pull_request_number=None,\n        loading_gif_url=\"https://media.giphy.com/media/3oEjI6SIIHBdRxXI40/giphy.gif\",\n        overwrite_existing=False,\n    )\n\n    # Test _find_existing_pr\n    pr = service._find_existing_pr()\n    assert pr is not None\n    assert pr['number'] == 1\n    assert pr['node_id'] == 'node1'\n\n    # Test _create_pr\n    pr = service._create_pr('title', ['body1', 'body2'], True)\n    assert pr is not None\n    assert pr['number'] == 2\n    assert service._comment_ids == [service.PRBodySentinel, 'comment1']\n\n    # Test _update_pr_body\n    service._update_pr_body(1, 'new body')\n    mock_patch.assert_called_with(\n        'https://api.github.com/repos/user/repo/pulls/1',\n        headers=service._get_headers(),\n        json={'body': 'new body'}\n    )\n\n    # Test _update_pr_comment\n    service._update_pr_comment('comment1', 'new comment')\n    mock_patch.assert_called_with(\n        'https://api.github.com/repos/user/repo/issues/comments/comment1',\n        headers=service._get_headers(),\n        json={'body': 'new comment'}\n    )\n\n    # Test _publish_comment\n    comment_id = service._publish_comment('new comment', 1)\n    assert comment_id == 'comment1'", "def test_github_publish_service(mock_patch, mock_post, mock_get):\n    # Mock responses for each request call\n    mock_get.return_value = Mock(status_code=200, json=lambda: [{'number': 1, 'node_id': 'node1'}])\n    mock_post.return_value = Mock(status_code=201, json=lambda: {'number': 2, 'id': 'comment1'})\n    mock_patch.return_value = Mock(status_code=200, json=lambda: {})\n\n    service = GitHubPublishService(\n        token='my_token',\n        run_id='123',\n        owner='user',\n        repo_name='repo',\n        head_branch='branch1',\n        base_branch='branch2',\n        issue=None,\n        pull_request_number=None,\n        loading_gif_url=\"https://media.giphy.com/media/3oEjI6SIIHBdRxXI40/giphy.gif\",\n        overwrite_existing=False,\n    )\n\n    # Test _find_existing_pr\n    pr = service._find_existing_pr()\n    assert pr is not None\n    assert pr['number'] == 1\n    assert pr['node_id'] == 'node1'\n\n    # Test _create_pr\n    pr = service._create_pr('title', ['body1', 'body2'], True)\n    assert pr is not None\n    assert pr['number'] == 2\n    assert service._comment_ids == [service.PRBodySentinel, 'comment1']\n\n    # Test _update_pr_body\n    service._update_pr_body(1, 'new body')\n    mock_patch.assert_called_with(\n        'https://api.github.com/repos/user/repo/pulls/1',\n        headers=service._get_headers(),\n        json={'body': 'new body'}\n    )\n\n    # Test _update_pr_comment\n    service._update_pr_comment('comment1', 'new comment')\n    mock_patch.assert_called_with(\n        'https://api.github.com/repos/user/repo/issues/comments/comment1',\n        headers=service._get_headers(),\n        json={'body': 'new comment'}\n    )\n\n    # Test _publish_comment\n    comment_id = service._publish_comment('new comment', 1)\n    assert comment_id == 'comment1'", ""]}
{"filename": "autopr/models/rail_objects.py", "chunked_list": ["import json\nfrom typing import List, ClassVar, Optional, Union\nfrom typing_extensions import TypeAlias\n\nimport pydantic\n\nfrom autopr.models.artifacts import DiffStr\n\n\nclass RailObject(pydantic.BaseModel):\n    \"\"\"\n    A RailObject is a pydantic model that represents the output of a guardrails call.\n    See PromptRail and RailService, and [Guardrails docs](https://docs.guardrails.io/) for more information.\n\n    To define your own RailObject:\n    - write an XML string compatible with the guardrails XML spec in the `output_spec` class variable\n    - define your parameters as pydantic instance attributes\n    \"\"\"\n\n    output_spec: ClassVar[str]\n\n    @classmethod\n    def get_rail_spec(cls):\n        \"\"\"\n        Get the XML spec template used to define the guardrails output.\n        Should include a `{{raw_document}}` in the prompt section,\n        which will be replaced by the input prompt/two-step LLM output.\n        \"\"\"\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{cls.output_spec}\n</output>\n<instructions>\nYou are a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@json_suffix_prompt_examples\n</instructions>\n<prompt>\nGiven the following document surrounded by `+++++`, answer the following questions. \nIf the answer doesn't exist in the document, enter `null`.\n\n+++++\n{{{{raw_document}}}}\n+++++\n\nExtract information from this document and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"", "\nclass RailObject(pydantic.BaseModel):\n    \"\"\"\n    A RailObject is a pydantic model that represents the output of a guardrails call.\n    See PromptRail and RailService, and [Guardrails docs](https://docs.guardrails.io/) for more information.\n\n    To define your own RailObject:\n    - write an XML string compatible with the guardrails XML spec in the `output_spec` class variable\n    - define your parameters as pydantic instance attributes\n    \"\"\"\n\n    output_spec: ClassVar[str]\n\n    @classmethod\n    def get_rail_spec(cls):\n        \"\"\"\n        Get the XML spec template used to define the guardrails output.\n        Should include a `{{raw_document}}` in the prompt section,\n        which will be replaced by the input prompt/two-step LLM output.\n        \"\"\"\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{cls.output_spec}\n</output>\n<instructions>\nYou are a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@json_suffix_prompt_examples\n</instructions>\n<prompt>\nGiven the following document surrounded by `+++++`, answer the following questions. \nIf the answer doesn't exist in the document, enter `null`.\n\n+++++\n{{{{raw_document}}}}\n+++++\n\nExtract information from this document and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"", ""]}
{"filename": "autopr/models/artifacts.py", "chunked_list": ["from typing import Literal, Optional\n\nimport pydantic\nfrom typing_extensions import TypeAlias\n\n\nclass Message(pydantic.BaseModel):\n    body: str = \"\"\n    author: str\n\n    def __str__(self):\n        return f\"{self.author}: {self.body}\\n\\n\"", "\n\nclass Thread(pydantic.BaseModel):\n    messages: list[Message]\n\n    def __str__(self):\n        return \"\\n\".join(str(message) for message in self.messages)\n\n\nclass Issue(Thread):\n    number: int\n    title: str\n    author: str\n\n    def __str__(self):\n        return f\"#{self.number} {self.title}\\n\\n\" + super().__str__()", "\nclass Issue(Thread):\n    number: int\n    title: str\n    author: str\n\n    def __str__(self):\n        return f\"#{self.number} {self.title}\\n\\n\" + super().__str__()\n\n\nclass PullRequest(Issue):\n    base_branch: str\n    head_branch: str\n    # code_review: list[CodeComment]\n\n    def __str__(self):\n        return f\"#{self.number} {self.title}\\n\\n\" + \"\\n\".join(\n            str(message) for message in self.messages\n        )", "\n\nclass PullRequest(Issue):\n    base_branch: str\n    head_branch: str\n    # code_review: list[CodeComment]\n\n    def __str__(self):\n        return f\"#{self.number} {self.title}\\n\\n\" + \"\\n\".join(\n            str(message) for message in self.messages\n        )", "        # ) + \"\\n\\n\" + \"\\n\".join(\n        #     str(thread) for thread in self.code_review\n        # )\n\n\n# class CodeComment(Thread):\n#     commit_sha: str\n#     filepath: str\n#     status: Literal[\"APPROVE\", \"REQUEST_CHANGES\", \"COMMENT\"]\n#", "#     status: Literal[\"APPROVE\", \"REQUEST_CHANGES\", \"COMMENT\"]\n#\n#     start_line_number: int\n#     end_line_number: Optional[int] = None\n#\n#     def __str__(self):\n#         return f\"{self.commit_sha}\\n\" \\\n#                f\"{self.filepath}:L{self.start_line_number}\" + f\"{f'-L{self.end_line_number}' if self.end_line_number else ''}\\n\" \\\n#                f\"{self.status}\\n\\n\" + \"\\n\".join(str(message) for message in self.messages)\n", "#                f\"{self.status}\\n\\n\" + \"\\n\".join(str(message) for message in self.messages)\n\n\nDiffStr: TypeAlias = str\n"]}
{"filename": "autopr/models/events.py", "chunked_list": ["from typing import Literal, Union\n\nimport pydantic\n\nfrom autopr.models.artifacts import Issue, Message, PullRequest  # , CodeComment\n\n\nclass Event(pydantic.BaseModel):\n    \"\"\"\n    Events trigger AutoPR to run in different ways.\n    \"\"\"\n    event_type: str", "\n\nclass IssueLabelEvent(Event):\n    \"\"\"\n    Event triggered when a label is added to an issue.\n    \"\"\"\n    event_type: Literal['issue_label'] = 'issue_label'\n\n    issue: Issue\n    label: str", "\n\nclass PullRequestCommentEvent(Event):\n    \"\"\"\n    Event triggered when a comment is added to a pull request.\n    \"\"\"\n    event_type: Literal['pull_request_comment'] = 'pull_request_comment'\n\n    pull_request: PullRequest\n    new_comment: Message", "\n\n# class CodeReviewEvent(Event):\n#     \"\"\"\n#     Event triggered when a comment is added to a code review.\n#     \"\"\"\n#     event_type: Literal['code_review'] = 'code_review'\n#\n#     pull_request: PullRequest\n#     new_code_comments: list[CodeComment]", "#     pull_request: PullRequest\n#     new_code_comments: list[CodeComment]\n#     new_comment: Message\n\n\nEventUnion = Union[IssueLabelEvent, PullRequestCommentEvent]  # | CodeReviewEventa\n"]}
{"filename": "autopr/models/prompt_base.py", "chunked_list": ["from typing import ClassVar\n\nimport pydantic\nimport structlog\n\nfrom autopr.utils.tokenizer import get_tokenizer\n\nlog = structlog.get_logger()\n\n\nclass PromptBase(pydantic.BaseModel):\n    \"\"\"\n    Base class for all prompt specifications.\n\n    Prompt parameters should be specified as pydantic instance attributes.\n    They will be automatically filled into the `prompt_template` string,\n    wherever they are referenced as {param}.\n    \"\"\"\n\n    #: The prompt template to use for the LLM call (reference string parameters as {param}).\n    prompt_template: ClassVar[str] = ''\n\n    # TODO implement extra_params in rail_service and chain_service\n    #: Extra parameters to pass to the guardrails LLM call.\n    # extra_params: ClassVar[dict[str, Any]] = {}\n\n    def get_prompt_message(self) -> str:\n        \"\"\"\n        Get the prompt message that is sent the LLM call.\n        \"\"\"\n        spec = self.prompt_template\n        prompt_params = self.get_string_params()\n        return spec.format(**prompt_params)\n\n    def get_string_params(self) -> dict[str, str]:\n        \"\"\"\n        Get the parameters of the prompt as a dictionary of strings.\n        Override this method to specify your own parameters.\n        \"\"\"\n        prompt_params = {}\n        for key, value in self:\n            if isinstance(value, list):\n                prompt_params[key] = '\\n\\n'.join(\n                    [str(item) for item in value]\n                )\n            else:\n                prompt_params[key] = str(value)\n        return prompt_params\n\n    def calculate_prompt_token_length(self) -> int:\n        \"\"\"\n        Calculate the number of tokens in the prompt message.\n        \"\"\"\n        tokenizer = get_tokenizer()\n        prompt_message = self.get_prompt_message()\n        return len(tokenizer.encode(prompt_message))\n\n    def ensure_token_length(self, max_length: int) -> bool:\n        \"\"\"\n        Ensure that the prompt message is no longer than `max_length` tokens.\n        \"\"\"\n        # Make sure there are at least `min_tokens` tokens left\n        while max_length < self.calculate_prompt_token_length():\n            # Iteratively trim the params\n            if not self.trim_params():\n                rail_name = self.__class__.__name__\n                log.debug(f'Could not trim params on rail {rail_name}: {self.get_string_params()}')\n                return False\n        return True\n\n    def trim_params(self) -> bool:\n        \"\"\"\n        Override this method to trim the parameters of the prompt.\n        This is called when the prompt is too long. By default, this method\n        removes the last element of the first list it finds.\n\n        TODO give this method better heuristics for trimming, so it doesn't just\n         get called over and over again.\n        \"\"\"\n\n        log.warning(\"Naively trimming params\", rail=self)\n        prompt_params = dict(self)\n        # If there are any lists, remove the last element of the first one you find\n        for key, value in prompt_params.items():\n            if isinstance(value, list) and len(value) > 0:\n                setattr(self, key, value[:-1])\n                return True\n        return False", "\n\nclass PromptBase(pydantic.BaseModel):\n    \"\"\"\n    Base class for all prompt specifications.\n\n    Prompt parameters should be specified as pydantic instance attributes.\n    They will be automatically filled into the `prompt_template` string,\n    wherever they are referenced as {param}.\n    \"\"\"\n\n    #: The prompt template to use for the LLM call (reference string parameters as {param}).\n    prompt_template: ClassVar[str] = ''\n\n    # TODO implement extra_params in rail_service and chain_service\n    #: Extra parameters to pass to the guardrails LLM call.\n    # extra_params: ClassVar[dict[str, Any]] = {}\n\n    def get_prompt_message(self) -> str:\n        \"\"\"\n        Get the prompt message that is sent the LLM call.\n        \"\"\"\n        spec = self.prompt_template\n        prompt_params = self.get_string_params()\n        return spec.format(**prompt_params)\n\n    def get_string_params(self) -> dict[str, str]:\n        \"\"\"\n        Get the parameters of the prompt as a dictionary of strings.\n        Override this method to specify your own parameters.\n        \"\"\"\n        prompt_params = {}\n        for key, value in self:\n            if isinstance(value, list):\n                prompt_params[key] = '\\n\\n'.join(\n                    [str(item) for item in value]\n                )\n            else:\n                prompt_params[key] = str(value)\n        return prompt_params\n\n    def calculate_prompt_token_length(self) -> int:\n        \"\"\"\n        Calculate the number of tokens in the prompt message.\n        \"\"\"\n        tokenizer = get_tokenizer()\n        prompt_message = self.get_prompt_message()\n        return len(tokenizer.encode(prompt_message))\n\n    def ensure_token_length(self, max_length: int) -> bool:\n        \"\"\"\n        Ensure that the prompt message is no longer than `max_length` tokens.\n        \"\"\"\n        # Make sure there are at least `min_tokens` tokens left\n        while max_length < self.calculate_prompt_token_length():\n            # Iteratively trim the params\n            if not self.trim_params():\n                rail_name = self.__class__.__name__\n                log.debug(f'Could not trim params on rail {rail_name}: {self.get_string_params()}')\n                return False\n        return True\n\n    def trim_params(self) -> bool:\n        \"\"\"\n        Override this method to trim the parameters of the prompt.\n        This is called when the prompt is too long. By default, this method\n        removes the last element of the first list it finds.\n\n        TODO give this method better heuristics for trimming, so it doesn't just\n         get called over and over again.\n        \"\"\"\n\n        log.warning(\"Naively trimming params\", rail=self)\n        prompt_params = dict(self)\n        # If there are any lists, remove the last element of the first one you find\n        for key, value in prompt_params.items():\n            if isinstance(value, list) and len(value) > 0:\n                setattr(self, key, value[:-1])\n                return True\n        return False", ""]}
{"filename": "autopr/models/__init__.py", "chunked_list": [""]}
{"filename": "autopr/models/prompt_chains.py", "chunked_list": ["from typing import ClassVar, Any, Type, Optional\n\nfrom autopr.models.prompt_base import PromptBase\nfrom langchain.schema import BaseOutputParser\n\nimport structlog\n\nlog = structlog.get_logger()\n\n\nclass PromptChain(PromptBase):\n    \"\"\"\n    A prompt chain is a pydantic model used to specify a prompt for a langchain call.\n    See ChainService and [Langchain docs](https://docs.langchain.ai/) for more information.\n\n    To define your own prompt chain:\n    - write a prompt template in the `prompt_template` class variable, referencing parameters as {param}\n    - define your parameters as pydantic instance attributes\n    - optionally define an output parser as the `output_parser` class variable\n\n\n    \"\"\"\n\n    #: The output parser to run the response through.\n    output_parser: ClassVar[Optional[BaseOutputParser]] = None\n\n    def get_prompt_message(self) -> str:\n        \"\"\"\n        Get the prompt message that is sent the LLM call.\n        Ordinarily the format instructions are passed as a partial variable,\n        so this method is overridden to include them for the trimming calculation.\n        \"\"\"\n        spec = self.prompt_template\n        prompt_params = self.get_string_params()\n        if self.output_parser is not None:\n            prompt_params['format_instructions'] = self.output_parser.get_format_instructions()\n        return spec.format(**prompt_params)", "\n\nclass PromptChain(PromptBase):\n    \"\"\"\n    A prompt chain is a pydantic model used to specify a prompt for a langchain call.\n    See ChainService and [Langchain docs](https://docs.langchain.ai/) for more information.\n\n    To define your own prompt chain:\n    - write a prompt template in the `prompt_template` class variable, referencing parameters as {param}\n    - define your parameters as pydantic instance attributes\n    - optionally define an output parser as the `output_parser` class variable\n\n\n    \"\"\"\n\n    #: The output parser to run the response through.\n    output_parser: ClassVar[Optional[BaseOutputParser]] = None\n\n    def get_prompt_message(self) -> str:\n        \"\"\"\n        Get the prompt message that is sent the LLM call.\n        Ordinarily the format instructions are passed as a partial variable,\n        so this method is overridden to include them for the trimming calculation.\n        \"\"\"\n        spec = self.prompt_template\n        prompt_params = self.get_string_params()\n        if self.output_parser is not None:\n            prompt_params['format_instructions'] = self.output_parser.get_format_instructions()\n        return spec.format(**prompt_params)", ""]}
{"filename": "autopr/models/prompt_rails.py", "chunked_list": ["import typing\nfrom typing import ClassVar, Any\nimport pydantic\n\nfrom autopr.models.prompt_base import PromptBase\nfrom autopr.models.rail_objects import RailObject\n\nimport structlog\n\nlog = structlog.get_logger()", "\nlog = structlog.get_logger()\n\n\nclass PromptRail(PromptBase):\n    \"\"\"\n    A prompt rail is a pydantic model used to specify a prompt for a guardrails LLM call.\n    See RailObject, RailService, and [Guardrails docs](https://docs.guardrails.io/) for more information.\n\n    To define your own prompt rail:\n    - declare your output type by subclassing RailObject, and referencing it in the `output_type` class variable\n    - write a prompt in the `prompt_template` class variable, referencing parameters as {param}\n    - define your parameters as pydantic instance attributes\n    \"\"\"\n\n    #: Whether to invoke the guardrails LLM call on the output of an ordinary LLM call, or just by itself.\n    two_step: ClassVar[bool] = True\n\n    #: The RailObject type to parse the LLM response into.\n    output_type: ClassVar[typing.Type[RailObject]]", ""]}
{"filename": "autopr/services/agent_service.py", "chunked_list": ["from typing import Optional, Any\n\nimport structlog\nfrom git.repo import Repo\n\nfrom autopr.agents.base import Agent, get_all_agents\nfrom autopr.models.events import EventUnion\nfrom autopr.services.action_service import ActionService\nfrom autopr.services.chain_service import ChainService\nfrom autopr.services.commit_service import CommitService", "from autopr.services.chain_service import ChainService\nfrom autopr.services.commit_service import CommitService\nfrom autopr.services.diff_service import DiffService\nfrom autopr.services.publish_service import PublishService\nfrom autopr.services.rail_service import RailService\n\n\nclass AgentService:\n    def __init__(\n        self,\n        rail_service: RailService,\n        chain_service: ChainService,\n        diff_service: DiffService,\n        commit_service: CommitService,\n        publish_service: PublishService,\n        action_service: ActionService,\n        repo: Repo,\n    ):\n        self.repo = repo\n        self.publish_service = publish_service\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n        self.diff_service = diff_service\n        self.commit_service = commit_service\n        self.action_service = action_service\n\n        # Load all agents in the `autopr/agents` directory\n        self.agents: dict[str, type[Agent]] = {\n            agent.id: agent\n            for agent in get_all_agents()\n        }\n\n        self.log = structlog.get_logger(service=\"agent_service\")\n\n    def run_agent(\n        self,\n        agent_id: str,\n        agent_config: Optional[dict[str, Any]],\n        event: EventUnion,\n    ):\n        # Get the agent\n        agent_type = self.agents[agent_id]\n        agent = agent_type(\n            repo=self.repo,\n            rail_service=self.rail_service,\n            chain_service=self.chain_service,\n            diff_service=self.diff_service,\n            commit_service=self.commit_service,\n            publish_service=self.publish_service,\n            action_service=self.action_service,\n            **(agent_config or {}),\n        )\n\n        # Publish an empty pull request\n        self.publish_service.update()\n\n        # Publish a warning if using gpt-3.5-turbo\n        if self.rail_service.completions_repo.model == \"gpt-3.5-turbo\":\n            self.publish_service.publish_update(\n                \"\u26a0\ufe0f\u26a0\ufe0f\u26a0\ufe0f Warning: Using `gpt-3.5-turbo` completion model. \"\n                \"AutoPR is currently not optimized for this model. \"\n                \"See https://github.com/irgolic/AutoPR/issues/65 for more details. \"\n                \"In the mean time, if you have access to the `gpt-4` API, please use that instead. \"\n                \"Please note that ChatGPT Plus does not give you access to the `gpt-4` API; \"\n                \"you need to sign up on [the GPT-4 API waitlist](https://openai.com/waitlist/gpt-4-api). \"\n            )\n\n        self.log.info(\"Generating changes\", event_=event)\n        try:\n            agent.handle_event(event)\n        except Exception as e:\n            self.log.exception(\"Agent failed\", event_=event, exc_info=e)\n            self.publish_service.finalize(success=False)\n            raise e\n\n        self.log.info(\"Generated changes\", event_=event)\n\n        # Finalize the pull request (put progress updates in a collapsible)\n        self.publish_service.finalize(success=True)", ""]}
{"filename": "autopr/services/event_service.py", "chunked_list": ["from typing import Any\n\nimport requests\nimport structlog\n\nfrom autopr.models.artifacts import Issue, Message, PullRequest\nfrom autopr.models.events import IssueLabelEvent, EventUnion, PullRequestCommentEvent\n\n\nclass EventService:\n    \"\"\"\n    Service for parsing events that trigger AutoPR into one of the `EventUnion` types.\n\n    To support other platforms (Gitlab/Bitbucket/Gitea), subclass this and override `parse_event`.\n    See irgolic/AutoPR#46 for more details.\n    \"\"\"\n\n    def parse_event(self, event_name: str, event: dict[str, Any]) -> EventUnion:\n        raise NotImplementedError", "\nclass EventService:\n    \"\"\"\n    Service for parsing events that trigger AutoPR into one of the `EventUnion` types.\n\n    To support other platforms (Gitlab/Bitbucket/Gitea), subclass this and override `parse_event`.\n    See irgolic/AutoPR#46 for more details.\n    \"\"\"\n\n    def parse_event(self, event_name: str, event: dict[str, Any]) -> EventUnion:\n        raise NotImplementedError", "\n\nclass GitHubEventService(EventService):\n    \"\"\"\n    Service for parsing GitHub events into one of the `EventUnion` types.\n\n    Currently only supports `IssueLabelEvent`, which is triggered when a label is added to an issue.\n\n    See https://docs.github.com/en/webhooks-and-events/events/issue-event-types\n    \"\"\"\n\n    def __init__(\n        self,\n        github_token: str,\n    ):\n        self.github_token = github_token\n        self.log = structlog.get_logger()\n\n    def get_headers(self):\n        return {\n            'Accept': 'application/vnd.github.v3+json',\n            'Authorization': f'Bearer {self.github_token}'\n        }\n\n    def _to_issue_label_event(self, event: dict[str, Any]) -> IssueLabelEvent:\n        \"\"\"\n        See https://docs.github.com/en/webhooks-and-events/events/issue-event-types#labeled\n        \"\"\"\n        # Get issue comments\n        url = event['issue']['comments_url']\n        assert url.startswith('https://api.github.com/repos/'), \"Unexpected comments_url\"\n        self.log.info(\"Getting issue comments\", url=url)\n        headers = self.get_headers()\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        comments_json = response.json()\n        self.log.info(\"Got issue comments\", comments=comments_json)\n\n        # Get body\n        comments_list = []\n        body_message = Message(\n            body=event['issue']['body'] or \"\",\n            author=event['issue']['user']['login'],\n        )\n        comments_list.append(body_message)\n\n        # Get comments\n        for comment_json in comments_json:\n            comment = Message(\n                body=comment_json['body'] or \"\",\n                author=comment_json['user']['login'],\n            )\n            comments_list.append(comment)\n\n        # Create issue\n        issue = Issue(\n            number=event['issue']['number'],\n            title=event['issue']['title'],\n            author=event['issue']['user']['login'],\n            messages=comments_list,\n        )\n\n        return IssueLabelEvent(\n            issue=issue,\n            label=event['label']['name'],\n        )\n\n    def _to_pull_request_comment_event(self, event: dict[str, Any]) -> PullRequestCommentEvent:\n        \"\"\"\n        See https://docs.github.com/en/webhooks-and-events/webhooks/webhook-events-and-payloads#issue_comment\n        \"\"\"\n        headers = self.get_headers()\n\n        # Get issue comments\n        url = event['issue']['comments_url']\n        assert url.startswith('https://api.github.com/repos/'), \"Unexpected comments_url\"\n        self.log.info(\"Getting issue comments\", url=url)\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        comments_json = response.json()\n        self.log.info(\"Got issue comments\", comments=comments_json)\n\n        # Get body\n        comments_list = []\n        body_message = Message(\n            body=event['issue']['body'] or \"\",\n            author=event['issue']['user']['login'],\n        )\n        comments_list.append(body_message)\n\n        # Get comments\n        for comment_json in comments_json:\n            comment = Message(\n                body=comment_json['body'] or \"\",\n                author=comment_json['user']['login'],\n            )\n            comments_list.append(comment)\n\n        # Get pull request\n        url = event['issue']['pull_request']['url']\n        assert url.startswith('https://api.github.com/repos/'), \"Unexpected pull_request url\"\n        self.log.info(\"Getting pull request\", url=url)\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        pull_request_json = response.json()\n        self.log.info(\"Got pull request\", pull_request=pull_request_json)\n\n        # Get branch names\n        head_branch = pull_request_json['head']['ref']\n        base_branch = pull_request_json['base']['ref']\n\n        # Create pull request\n        pull_request = PullRequest(\n            number=event['issue']['number'],\n            title=event['issue']['title'],\n            author=event['issue']['user']['login'],\n            messages=comments_list,\n            head_branch=head_branch,\n            base_branch=base_branch,\n        )\n\n        return PullRequestCommentEvent(\n            pull_request=pull_request,\n            new_comment=Message(\n                body=event['comment']['body'] or \"\",\n                author=event['comment']['user']['login'],\n            ),\n        )\n\n\n    def parse_event(self, event_name: str, event_dict: dict[str, Any]):\n        if event_name == 'issues':\n            return self._to_issue_label_event(event_dict)\n        elif event_name == 'issue_comment':\n            return self._to_pull_request_comment_event(event_dict)\n        raise ValueError(f\"Unsupported event name: {event_name}\")", ""]}
{"filename": "autopr/services/action_service.py", "chunked_list": ["import traceback\nfrom typing import Optional, Collection, Type\n\nimport pydantic\nimport structlog\nfrom git.repo import Repo\n\nfrom autopr.actions.base import get_all_actions, Action\n\nfrom autopr.actions.base import ContextDict", "\nfrom autopr.actions.base import ContextDict\nfrom autopr.models.rail_objects import RailObject\nfrom autopr.repos.completions_repo import CompletionsRepo\nfrom autopr.services.chain_service import ChainService\nfrom autopr.services.publish_service import PublishService\nfrom autopr.services.rail_service import RailService\n\n\nclass ActionService:\n    class Finished(Action):\n        id = \"finished\"\n\n        class Arguments(Action.Arguments):\n            reason: str\n\n            output_spec = \"\"\"<string\n                name=\"reason\"\n                required=\"true\"\n            />\"\"\"\n\n        def run(self, arguments: Action.Arguments, context: ContextDict) -> ContextDict:\n            return context\n\n    def __init__(\n        self,\n        repo: Repo,\n        completions_repo: CompletionsRepo,\n        publish_service: PublishService,\n        rail_service: RailService,\n        chain_service: ChainService,\n        num_reasks: int = 3\n    ):\n        self.repo = repo\n        self.completions_repo = completions_repo\n        self.publish_service = publish_service\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n        self.num_reasks = num_reasks\n\n        # Load all actions in the `autopr/actions` directory\n        self.actions: dict[str, type[Action]] = {\n            action.id: action\n            for action in get_all_actions()\n        }\n\n        self.log = structlog.get_logger(service=\"action_service\")\n\n    def _write_action_selection_rail_spec(\n        self,\n        action_ids: Collection[str],\n        include_finished: bool = False,\n    ) -> str:\n        # Add finished action\n        if include_finished and \"finished\" not in action_ids:\n            action_ids = [*action_ids, \"finished\"]\n        # Write the \"choice\" output spec\n        output_spec = f\"\"\"<choice\n            name=\"action\"\n            on-fail-choice=\"reask\"\n        >\"\"\"\n        for action_id in action_ids:\n            action = self.actions[action_id]\n            output_spec += f\"\"\"<case\n                name=\"{action.id}\"\n                {f'description=\"{action.description}\"' if action.description else \"\"}\n            >\n            <object\n                name=\"{action.id}\"\n            >\"\"\"\n            if action.Arguments.output_spec:\n                output_spec += action.Arguments.output_spec\n            else:\n                if action.Arguments is not Action.Arguments:\n                    # Guardrails RFC (GRAIL-001) plans to generate output specs from pydantic models automatically,\n                    # but for now, we need to manually write them.\n                    raise ValueError(\n                        f\"{action.__name__}.Arguments ({action_id}) is missing an output spec\"\n                    )\n                output_spec += \"\"\"<string \n                    name=\"reason\"\n                />\"\"\"\n            output_spec += f\"\"\"\n            </object>\n            \"\"\"\n            output_spec += f\"\"\"</case>\"\"\"\n        output_spec += f\"\"\"</choice>\"\"\"\n\n        # Wrap it in a rail spec\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{output_spec}\n</output>\n<instructions>\nYou are AutoPR, an autonomous pull request creator and a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@autopr_json_suffix_prompt_examples\n</instructions>\n<prompt>\n{{{{context}}}}\n\nYou are about to make a decision on what to do next, and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"\n\n    @staticmethod\n    def _write_action_args_query_rail_spec(\n        arguments: Type[Action.Arguments],\n    ) -> str:\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{arguments.output_spec}\n</output>\n<instructions>\nYou are AutoPR, an autonomous pull request creator and a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@autopr_json_suffix_prompt_examples\n</instructions>\n<prompt>\n{{{{context}}}}\n\nYou are about to make a decision on what to do next, and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"\n\n    def instantiate_action(\n        self,\n        action_type: Type[Action],\n    ):\n        return action_type(\n            repo=self.repo,\n            rail_service=self.rail_service,\n            chain_service=self.chain_service,\n            publish_service=self.publish_service,\n        )\n\n    def run_action(\n        self,\n        action_id: str,\n        context: ContextDict,\n    ) -> ContextDict:\n        # Get the action\n        action_type = self.actions[action_id]\n\n        section_title = f\"\ud83d\ude80 Running {action_id}\"\n        self.publish_service.start_section(section_title)\n\n        # If the action defines arguments, ask the LLM to fill them in\n        if action_type.Arguments is not Action.Arguments:\n            # Ask the LLM to fill in the arguments\n            arguments = self.ask_for_action_arguments(\n                action_type=action_type,\n                context=context,\n            )\n            if arguments is None:\n                self.log.error(\"Guardrails failed to specify action arguments\")\n                return context\n        else:\n            arguments = Action.Arguments()\n\n        # Instantiate the action\n        action = self.instantiate_action(action_type)\n\n        # Run the action\n        try:\n            results = action.run(arguments, context)\n        except Exception:\n            self.log.exception(f\"Failed to run action {action_id}\")\n            self.publish_service.publish_code_block(\n                heading=\"Error\",\n                code=traceback.format_exc(),\n                language=\"python\",  # FIXME\n                                    #  does nice syntax highlighting for tracebacks, but should be made configurable\n            )\n            self.publish_service.end_section(f\"\u274c Failed {action_id}\")\n            raise\n\n        if self.publish_service.sections_stack[-1].title == section_title:\n            self.publish_service.end_section(f\"\u2705 Finished {action_id}\")\n        self.publish_service.end_section()\n\n        return results\n\n    def run_actions_iteratively(\n        self,\n        action_ids: Collection[str],\n        context: ContextDict,\n        context_headings: Optional[dict[str, str]] = None,\n        max_iterations: int = 5,\n        include_finished: bool = False,\n    ) -> ContextDict:\n        for _ in range(max_iterations):\n            if len(action_ids) == 1 and not include_finished:\n                action_id = next(iter(action_ids))\n                context = self.run_action(action_id, context)\n                continue\n\n            self.publish_service.start_section(\"\u2753 Choosing next action\")\n            # Pick an action\n            pick = self.pick_action(\n                action_ids=action_ids,\n                context=context,\n                include_finished=include_finished,\n                context_headings=context_headings,\n            )\n            if pick is None or pick[0].id == \"finished\":\n                self.publish_service.end_section(\"\ud83c\udfc1 Finished\")\n                break\n            action_type, args = pick\n\n            # Instantiate the action\n            action = self.instantiate_action(action_type)\n\n            self.publish_service.update_section(f\"\ud83d\ude80 Running {action.id}\")\n\n            # Run the action\n            context = action.run(args, context)\n\n            self.publish_service.end_section()\n\n        return context\n\n    def ask_for_action_arguments(\n        self,\n        action_type: Type[Action],\n        context: ContextDict,\n    ) -> Optional[Action.Arguments]:\n        if action_type.Arguments is Action.Arguments:\n            # No arguments to fill in\n            return Action.Arguments()\n\n        # Generate the arguments query spec\n        rail_spec = self._write_action_args_query_rail_spec(\n            arguments=action_type.Arguments,\n        )\n\n        # Run the rail\n        dict_o = self.rail_service.run_rail_string(\n            rail_spec,\n            prompt_params={\n                \"context\": context.as_string(),\n            },\n            heading=\"action arguments\",\n        )\n        if dict_o is None:\n            self.log.error(\"Guardrails failed to specify action arguments\")\n            return None\n\n        # Parse the arguments\n        try:\n            args = action_type.Arguments.parse_obj(dict_o)\n        except pydantic.ValidationError as e:\n            self.log.error(\"Guardrails failed to parse action arguments\", error=e)\n            return None\n        return args\n\n    def pick_action(\n        self,\n        action_ids: Collection[str],\n        context: ContextDict,\n        include_finished: bool = False,\n        context_headings: Optional[dict[str, str]] = None,\n    ) -> Optional[tuple[type[Action], Action.Arguments]]:\n        \"\"\"\n        Pick an action to run next.\n\n        Returns a tuple of the action type and the arguments to instantiate it with.\n\n        \"\"\"\n        # Generate the action-select rail spec\n        rail_spec = self._write_action_selection_rail_spec(\n            action_ids=action_ids,\n            include_finished=include_finished,\n        )\n        self.log.debug(\"Wrote action-selection rail spec:\\n%s\", rail_spec=rail_spec)\n\n        # Instantiate the rail\n        dict_o = self.rail_service.run_rail_string(\n            rail_spec,\n            prompt_params={\n                \"context\": context.as_string(\n                    variable_headings=context_headings,\n                ),\n            },\n            heading=\"action choice\",\n        )\n        if dict_o is None:\n            self.log.error(\"Guardrails failed to choose an action\")\n            return None\n\n        # Get the action\n        action_id = dict_o[\"action\"]\n        if action_id == \"finished\":\n            # Done!\n            return None\n\n        action_args_dict = dict_o.get(action_id, {})\n        action = self.actions[action_id]\n        args = action.Arguments.parse_obj(action_args_dict)\n        return action, args", "\nclass ActionService:\n    class Finished(Action):\n        id = \"finished\"\n\n        class Arguments(Action.Arguments):\n            reason: str\n\n            output_spec = \"\"\"<string\n                name=\"reason\"\n                required=\"true\"\n            />\"\"\"\n\n        def run(self, arguments: Action.Arguments, context: ContextDict) -> ContextDict:\n            return context\n\n    def __init__(\n        self,\n        repo: Repo,\n        completions_repo: CompletionsRepo,\n        publish_service: PublishService,\n        rail_service: RailService,\n        chain_service: ChainService,\n        num_reasks: int = 3\n    ):\n        self.repo = repo\n        self.completions_repo = completions_repo\n        self.publish_service = publish_service\n        self.rail_service = rail_service\n        self.chain_service = chain_service\n        self.num_reasks = num_reasks\n\n        # Load all actions in the `autopr/actions` directory\n        self.actions: dict[str, type[Action]] = {\n            action.id: action\n            for action in get_all_actions()\n        }\n\n        self.log = structlog.get_logger(service=\"action_service\")\n\n    def _write_action_selection_rail_spec(\n        self,\n        action_ids: Collection[str],\n        include_finished: bool = False,\n    ) -> str:\n        # Add finished action\n        if include_finished and \"finished\" not in action_ids:\n            action_ids = [*action_ids, \"finished\"]\n        # Write the \"choice\" output spec\n        output_spec = f\"\"\"<choice\n            name=\"action\"\n            on-fail-choice=\"reask\"\n        >\"\"\"\n        for action_id in action_ids:\n            action = self.actions[action_id]\n            output_spec += f\"\"\"<case\n                name=\"{action.id}\"\n                {f'description=\"{action.description}\"' if action.description else \"\"}\n            >\n            <object\n                name=\"{action.id}\"\n            >\"\"\"\n            if action.Arguments.output_spec:\n                output_spec += action.Arguments.output_spec\n            else:\n                if action.Arguments is not Action.Arguments:\n                    # Guardrails RFC (GRAIL-001) plans to generate output specs from pydantic models automatically,\n                    # but for now, we need to manually write them.\n                    raise ValueError(\n                        f\"{action.__name__}.Arguments ({action_id}) is missing an output spec\"\n                    )\n                output_spec += \"\"\"<string \n                    name=\"reason\"\n                />\"\"\"\n            output_spec += f\"\"\"\n            </object>\n            \"\"\"\n            output_spec += f\"\"\"</case>\"\"\"\n        output_spec += f\"\"\"</choice>\"\"\"\n\n        # Wrap it in a rail spec\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{output_spec}\n</output>\n<instructions>\nYou are AutoPR, an autonomous pull request creator and a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@autopr_json_suffix_prompt_examples\n</instructions>\n<prompt>\n{{{{context}}}}\n\nYou are about to make a decision on what to do next, and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"\n\n    @staticmethod\n    def _write_action_args_query_rail_spec(\n        arguments: Type[Action.Arguments],\n    ) -> str:\n        return f\"\"\"\n<rail version=\"0.1\">\n<output>\n{arguments.output_spec}\n</output>\n<instructions>\nYou are AutoPR, an autonomous pull request creator and a helpful assistant only capable of communicating with valid JSON, and no other text.\n\n@autopr_json_suffix_prompt_examples\n</instructions>\n<prompt>\n{{{{context}}}}\n\nYou are about to make a decision on what to do next, and return a JSON that follows the correct schema.\n\n@xml_prefix_prompt\n\n{{output_schema}}\n</prompt>\n</rail>\n\"\"\"\n\n    def instantiate_action(\n        self,\n        action_type: Type[Action],\n    ):\n        return action_type(\n            repo=self.repo,\n            rail_service=self.rail_service,\n            chain_service=self.chain_service,\n            publish_service=self.publish_service,\n        )\n\n    def run_action(\n        self,\n        action_id: str,\n        context: ContextDict,\n    ) -> ContextDict:\n        # Get the action\n        action_type = self.actions[action_id]\n\n        section_title = f\"\ud83d\ude80 Running {action_id}\"\n        self.publish_service.start_section(section_title)\n\n        # If the action defines arguments, ask the LLM to fill them in\n        if action_type.Arguments is not Action.Arguments:\n            # Ask the LLM to fill in the arguments\n            arguments = self.ask_for_action_arguments(\n                action_type=action_type,\n                context=context,\n            )\n            if arguments is None:\n                self.log.error(\"Guardrails failed to specify action arguments\")\n                return context\n        else:\n            arguments = Action.Arguments()\n\n        # Instantiate the action\n        action = self.instantiate_action(action_type)\n\n        # Run the action\n        try:\n            results = action.run(arguments, context)\n        except Exception:\n            self.log.exception(f\"Failed to run action {action_id}\")\n            self.publish_service.publish_code_block(\n                heading=\"Error\",\n                code=traceback.format_exc(),\n                language=\"python\",  # FIXME\n                                    #  does nice syntax highlighting for tracebacks, but should be made configurable\n            )\n            self.publish_service.end_section(f\"\u274c Failed {action_id}\")\n            raise\n\n        if self.publish_service.sections_stack[-1].title == section_title:\n            self.publish_service.end_section(f\"\u2705 Finished {action_id}\")\n        self.publish_service.end_section()\n\n        return results\n\n    def run_actions_iteratively(\n        self,\n        action_ids: Collection[str],\n        context: ContextDict,\n        context_headings: Optional[dict[str, str]] = None,\n        max_iterations: int = 5,\n        include_finished: bool = False,\n    ) -> ContextDict:\n        for _ in range(max_iterations):\n            if len(action_ids) == 1 and not include_finished:\n                action_id = next(iter(action_ids))\n                context = self.run_action(action_id, context)\n                continue\n\n            self.publish_service.start_section(\"\u2753 Choosing next action\")\n            # Pick an action\n            pick = self.pick_action(\n                action_ids=action_ids,\n                context=context,\n                include_finished=include_finished,\n                context_headings=context_headings,\n            )\n            if pick is None or pick[0].id == \"finished\":\n                self.publish_service.end_section(\"\ud83c\udfc1 Finished\")\n                break\n            action_type, args = pick\n\n            # Instantiate the action\n            action = self.instantiate_action(action_type)\n\n            self.publish_service.update_section(f\"\ud83d\ude80 Running {action.id}\")\n\n            # Run the action\n            context = action.run(args, context)\n\n            self.publish_service.end_section()\n\n        return context\n\n    def ask_for_action_arguments(\n        self,\n        action_type: Type[Action],\n        context: ContextDict,\n    ) -> Optional[Action.Arguments]:\n        if action_type.Arguments is Action.Arguments:\n            # No arguments to fill in\n            return Action.Arguments()\n\n        # Generate the arguments query spec\n        rail_spec = self._write_action_args_query_rail_spec(\n            arguments=action_type.Arguments,\n        )\n\n        # Run the rail\n        dict_o = self.rail_service.run_rail_string(\n            rail_spec,\n            prompt_params={\n                \"context\": context.as_string(),\n            },\n            heading=\"action arguments\",\n        )\n        if dict_o is None:\n            self.log.error(\"Guardrails failed to specify action arguments\")\n            return None\n\n        # Parse the arguments\n        try:\n            args = action_type.Arguments.parse_obj(dict_o)\n        except pydantic.ValidationError as e:\n            self.log.error(\"Guardrails failed to parse action arguments\", error=e)\n            return None\n        return args\n\n    def pick_action(\n        self,\n        action_ids: Collection[str],\n        context: ContextDict,\n        include_finished: bool = False,\n        context_headings: Optional[dict[str, str]] = None,\n    ) -> Optional[tuple[type[Action], Action.Arguments]]:\n        \"\"\"\n        Pick an action to run next.\n\n        Returns a tuple of the action type and the arguments to instantiate it with.\n\n        \"\"\"\n        # Generate the action-select rail spec\n        rail_spec = self._write_action_selection_rail_spec(\n            action_ids=action_ids,\n            include_finished=include_finished,\n        )\n        self.log.debug(\"Wrote action-selection rail spec:\\n%s\", rail_spec=rail_spec)\n\n        # Instantiate the rail\n        dict_o = self.rail_service.run_rail_string(\n            rail_spec,\n            prompt_params={\n                \"context\": context.as_string(\n                    variable_headings=context_headings,\n                ),\n            },\n            heading=\"action choice\",\n        )\n        if dict_o is None:\n            self.log.error(\"Guardrails failed to choose an action\")\n            return None\n\n        # Get the action\n        action_id = dict_o[\"action\"]\n        if action_id == \"finished\":\n            # Done!\n            return None\n\n        action_args_dict = dict_o.get(action_id, {})\n        action = self.actions[action_id]\n        args = action.Arguments.parse_obj(action_args_dict)\n        return action, args", ""]}
{"filename": "autopr/services/__init__.py", "chunked_list": [""]}
{"filename": "autopr/services/publish_service.py", "chunked_list": ["import json\nimport sys\nimport traceback\nfrom typing import Optional, Union, Any, Type\n\nimport pydantic\nimport requests\n\nfrom autopr.models.artifacts import Issue\n", "from autopr.models.artifacts import Issue\n\nimport structlog\n\n\nclass CodeBlock(pydantic.BaseModel):\n    \"\"\"\n    A block of text to be shown as a code block in the pull request description.\n    \"\"\"\n    heading: str\n    code: str\n    language: str = \"xml\"\n    default_open: bool = False\n\n    def __str__(self):\n        return f\"\"\"<details{\" open\" if self.default_open else \"\"}>\n<summary>{self.heading}</summary>\n\n~~~{self.language}\n{self.code}\n~~~\n\n</details>\"\"\"", "\n\nclass UpdateSection(pydantic.BaseModel):\n    \"\"\"\n    A section of the pull request description, used to keep state while publishing updates.\n    \"\"\"\n    level: int\n    title: str\n    updates: list[Union[str, CodeBlock, 'UpdateSection']] = pydantic.Field(default_factory=list)\n", "\n\nclass PublishService:\n    \"\"\"\n    Service for publishing updates to the pull request description.\n\n    To control update sections, call:\n    - `start_section` to start a new section\n    - `end_section` to end the current section (optionally with results and a new title)\n    - `update_section` to update the current section title\n\n    To publish updates to the current section, call:\n    - `publish_update` to publish a simple textual update\n    - `publish_code_block` to publish text in a triple-backtick-style code block\n    \"\"\"\n\n    def __init__(\n        self,\n        owner: str,\n        repo_name: str,\n        head_branch: str,\n        base_branch: str,\n        issue: Optional[Issue] = None,\n        pull_request_number: Optional[int] = None,\n        loading_gif_url: str = \"https://media.giphy.com/media/3oEjI6SIIHBdRxXI40/giphy.gif\",\n        overwrite_existing: bool = False,\n    ):\n        self.owner = owner\n        self.repo_name = repo_name\n        self.head_branch = head_branch\n        self.base_branch = base_branch\n        self.issue = issue\n        self.pr_number = pull_request_number\n        self.loading_gif_url = loading_gif_url\n        self.overwrite_existing = overwrite_existing\n\n        # GitHub comment length limit is ~262144, not 65536 as stated in the docs\n        self.max_comment_length = 260000\n\n        if issue is not None:\n            self.title: str = f\"Fix #{issue.number}: {issue.title}\"\n        else:\n            self.title: str = \"AutoPR\"\n        self.root_section = UpdateSection(\n            level=0,\n            title=\"root\",\n        )\n        self.sections_stack: list[UpdateSection] = [self.root_section]\n\n        self.log = structlog.get_logger(service=\"publish\")\n\n        self._last_code_block: Optional[CodeBlock] = None\n\n        self.error_report_template = \"\"\"\n## Traceback\n\n```\n{error}\n```\n\"\"\"\n        self.new_error_report_link_template = \"https://github.com/irgolic/AutoPR/issues/new?\" \\\n                                              \"title={title}&\" \\\n                                              \"labels=bug&\" \\\n                                              \"body={body}\"\n\n    def set_title(self, title: str):\n        \"\"\"\n        Set the pull request title and body.\n        A description heading will be added to the body.\n\n        Parameters\n        ----------\n        title: str\n            The title of the pull request\n        body: str\n            The body of the pull request\n        \"\"\"\n        if self.pr_number is None:\n            self.update()\n            if self.pr_number is None:\n                raise RuntimeError(\"Error creating pull request\")\n        else:\n            self._set_title(title)\n\n    def publish_update(\n        self,\n        text: str,\n        section_title: Optional[str] = None,\n    ):\n        \"\"\"\n        Publish a simple text update to the current section.\n\n        Parameters\n        ----------\n        text: str\n            The text to publish\n        section_title: str, optional\n            The title that the parent section should be updated to\n        \"\"\"\n        self.sections_stack[-1].updates.append(text)\n        if section_title:\n            if self.sections_stack is self.root_section:\n                raise ValueError(\"Cannot set section title on root section\")\n            self.sections_stack[-1].title = section_title\n        self.log.debug(\"Publishing update\", text=text)\n        self.update()\n\n    def publish_code_block(\n        self,\n        heading: str,\n        code: str,\n        default_open: bool = False,\n        language: str = \"xml\",\n        section_title: Optional[str] = None,\n    ):\n        \"\"\"\n        Publish a code block as a collapsible child to the current section.\n\n        Parameters\n        ----------\n        heading: str\n            The title of the collapsible\n        code: str\n            The contents of the collapsible\n        default_open: bool, optional\n            Whether the collapsible should be open by default\n        language: str, optional\n            The language of the code (defaults to python)\n        section_title: str, optional\n            The title that the parent section should be updated to\n        \"\"\"\n        block = CodeBlock(\n            heading=heading,\n            code=code,\n            language=language,\n            default_open=default_open,\n        )\n        self._last_code_block = block\n        self.sections_stack[-1].updates.append(block)\n        if section_title:\n            if self.sections_stack is self.root_section:\n                raise ValueError(\"Cannot set section title on root section\")\n            self.sections_stack[-1].title = section_title\n        self.update()\n\n    def start_section(\n        self,\n        title: str,\n    ):\n        \"\"\"\n        Start a new section.\n\n        Parameters\n        ----------\n        title: str\n            The title of the new section\n        \"\"\"\n        self.log.debug(\"Starting section\", title=title)\n        new_section = UpdateSection(\n            level=len(self.sections_stack),\n            title=title,\n        )\n        self.sections_stack[-1].updates.append(new_section)  # Add the new section as a child\n        self.sections_stack.append(new_section)\n        self.update()\n\n    def update_section(self, title: str):\n        \"\"\"\n        Update the title of the current section.\n\n        Parameters\n        ----------\n        title: str\n            The new title of the current section\n        \"\"\"\n        if len(self.sections_stack) == 1:\n            raise ValueError(\"Cannot set section title on root section\")\n        self.log.debug(\"Updating section\", title=title)\n        self.sections_stack[-1].title = title\n        self.update()\n\n    def end_section(\n        self,\n        title: Optional[str] = None,\n    ):\n        \"\"\"\n        End the current section.\n\n        Parameters\n        ----------\n        title: str, optional\n            The title that section should be updated to\n        result: str, optional\n            The result of the section\n        \"\"\"\n        if len(self.sections_stack) == 1:\n            raise ValueError(\"Cannot end root section\")\n        self.log.debug(\"Ending section\", title=title)\n        if title:\n            self.sections_stack[-1].title = title\n        self.sections_stack.pop()\n\n        self.update()\n\n    def _contains_last_code_block(self, parent: UpdateSection) -> bool:\n        for section in reversed(parent.updates):\n            if isinstance(section, CodeBlock):\n                return section is self._last_code_block\n            elif isinstance(section, UpdateSection):\n                return self._contains_last_code_block(section)\n        return False\n\n    def _build_progress_update(self, section: UpdateSection, open_default: bool = False) -> str:\n        progress = \"\"\n        # Get list of steps\n        updates = []\n        for update in section.updates:\n            if isinstance(update, UpdateSection):\n                # Recursively build updates\n                updates += [self._build_progress_update(\n                    update,\n                    open_default=(\n                        self._contains_last_code_block(update) or update is section.updates[-1]\n                    ),\n                )]\n                continue\n            if isinstance(update, CodeBlock):\n                # If is the last code block\n                if self._last_code_block is None or update is self._last_code_block or update is section.updates[-1]:\n                    # Clone the block and set default_open to True\n                    update = update.copy()\n                    update.default_open = True\n                updates += [str(update)]\n                continue\n            updates += [update]\n\n        # Prefix updates with quotation\n        updates = '\\n\\n'.join(updates)\n        updates = '\\n'.join([f\"> {line}\" for line in updates.splitlines()])\n\n        # Leave the last section open if we're not finalizing (i.e. if we're still running or errored)\n        progress += f\"\"\"<details{' open' if open_default else ''}>\n<summary>{section.title}</summary>\n\n{updates}\n</details>\"\"\"\n\n        return progress\n\n    def _build_bodies(self, success: Optional[bool] = None) -> list[str]:\n        \"\"\"\n        Builds the body of the pull request, splitting it into multiple bodies if necessary.\n        Assumes that the top-level section groups are each small enough to fit within `max_comment_length`.\n        \"\"\"\n        bodies = []\n\n        body = \"\"\n        if self.issue is not None:\n            # Add Fixes magic word\n            body += f\"Fixes #{self.issue.number}\\n\\n\"\n\n        # Build status\n        body += f\"## Status\\n\\n\"\n        if success is None:\n            body += \"This pull request is being autonomously generated by [AutoPR](https://github.com/irgolic/AutoPR).\"\n        elif not success:\n            body += f\"This pull request was being autonomously generated by \" \\\n                    f\"[AutoPR](https://github.com/irgolic/AutoPR), but it encountered an error.\"\n            if sys.exc_info()[0] is not None:\n                body += f\"\\n\\nError:\\n\\n```\\n{traceback.format_exc()}\\n```\"\n            body += f'\\n\\nPlease <a href=\"{self._build_issue_template_link()}\">open an issue</a> to report this.'\n        elif success:\n            body += f\"This pull request was autonomously generated by [AutoPR](https://github.com/irgolic/AutoPR).\\n\\n\" \\\n                    f\"If there's a problem with this pull request, please \" \\\n                    f\"[open an issue]({self._build_issue_template_link()}).\"\n\n        for section in self.root_section.updates:\n            if isinstance(section, UpdateSection):\n                progress_update = self._build_progress_update(\n                    section,\n                    open_default=(\n                        not success and\n                        (section is self.root_section.updates[-1] or self._contains_last_code_block(section))\n                    ),\n                )\n            else:\n                progress_update = str(section)\n            if len(body) + len('\\n\\n' + progress_update) > self.max_comment_length:\n                bodies += [body]\n                body = f\"## Status (continued)\\n\\n{progress_update}\"\n            else:\n                body += f\"\\n\\n{progress_update}\"\n\n        if success is None:\n            body += f\"\\n\\n\" \\\n                    f'<img src=\"{self.loading_gif_url}\"' \\\n                    f' width=\"200\" height=\"200\"/>'\n        bodies += [body]\n        # self.log.debug(\"Built bodies\", bodies=bodies)\n        return bodies\n\n    def _build_issue_template_link(self, **kwargs):\n        if sys.exc_info()[0] is not None:\n            error = traceback.format_exc()\n        else:\n            error = \"No traceback\"\n        kwargs['error'] = error\n\n        body = self.error_report_template.format(**kwargs)\n        if sys.exc_info()[0] is not None:\n            title = traceback.format_exception_only(sys.exc_info()[0], sys.exc_info()[1])[0].strip()\n        elif self.issue is not None:\n            title = f'Error fixing \"{self.issue.title}\"'\n        else:\n            title = \"Error running AutoPR\"\n\n        issue_link = self.new_error_report_link_template.format(\n            body=body,\n            title=title,\n        )\n        # Map characters to their URL-encoded equivalents\n        encoded_url = issue_link.replace(' ', '%20').replace('\\n', '%0A').replace('\"', '%22').replace(\"#\", \"%23\")\n        return encoded_url\n\n    def update(self):\n        \"\"\"\n        Update the PR body with the current progress.\n        \"\"\"\n        bodies = self._build_bodies()\n        self._publish_progress(bodies)\n\n    def finalize(self, success: bool):\n        \"\"\"\n        Finalize the PR, either successfully or unsuccessfully.\n        Will render the final PR description without the loading gif.\n\n        Parameters\n        ----------\n        success: bool\n            Whether the PR was successful or not\n        \"\"\"\n        bodies = self._build_bodies(success=success)\n        self._publish_progress(bodies, success=success)\n\n    def publish_comment(self, text: str, issue_number: Optional[int] = None) -> Optional[str]:\n        if issue_number is None:\n            if self.pr_number is None:\n                self.update()\n                if self.pr_number is None:\n                    raise RuntimeError(\"Error creating pull request\")\n            issue_number = self.pr_number\n        return self._publish_comment(text, issue_number)\n\n    def _publish_comment(self, text: str, issue_number: int) -> Optional[str]:\n        \"\"\"\n        Publish a comment to the issue (pull requests are also issues).\n\n        Parameters\n        ----------\n\n        text: str\n            The text to comment\n        issue_number: Optional[int]\n            The issue number to comment on. If None, should comment on the PR.\n        \"\"\"\n        raise NotImplementedError\n\n    def _set_title(self, title: str):\n        \"\"\"\n        Set the title of the pull request.\n\n        Parameters\n        ----------\n\n        title: str\n            The title to set\n        \"\"\"\n        raise NotImplementedError\n\n    def _publish_progress(\n        self,\n        bodies: list[str],\n        success: bool = False,\n    ):\n        \"\"\"\n        Publish the PR to the provider.\n\n        Parameters\n        ----------\n        title: str\n            The title of the PR\n        bodies: list[str]\n            The bodies of the PR (split into multiple according to `max_comment_length`)\n        success: bool\n            Whether generation was successful or not\n        \"\"\"\n        raise NotImplementedError", "\n\nclass GitHubPublishService(PublishService):\n    \"\"\"\n    Publishes the PR to GitHub.\n\n    Sets it as draft while it's being updated, and removes the draft status when it's finalized.\n    Adds a shield linking to the action logs, a \"Fixes #{issue_number}\" link.\n\n    \"\"\"\n\n    class PRBodySentinel:\n        pass\n\n    def __init__(\n        self,\n        token: str,\n        run_id: str,\n        owner: str,\n        repo_name: str,\n        head_branch: str,\n        base_branch: str,\n        issue: Optional[Issue] = None,\n        pull_request_number: Optional[int] = None,\n        loading_gif_url: str = \"https://media.giphy.com/media/3oEjI6SIIHBdRxXI40/giphy.gif\",\n        overwrite_existing: bool = False,\n    ):\n        super().__init__(\n            owner=owner,\n            repo_name=repo_name,\n            head_branch=head_branch,\n            base_branch=base_branch,\n            issue=issue,\n            pull_request_number=pull_request_number,\n            loading_gif_url=loading_gif_url,\n            overwrite_existing=overwrite_existing,\n        )\n        self.token = token\n        self.run_id = run_id\n\n        self.pr_node_id: Optional[str] = None\n\n        self._drafts_supported = True\n\n        # list of comment IDs, incl. PRBodySentinel to denote the body of the PR\n        self._comment_ids: list[Union[str, Type[GitHubPublishService.PRBodySentinel]]] = []\n\n        self.max_char_length = 65536\n\n        self.error_report_template = \"\"\"\n{shield}\n\nAutoPR encountered an error.  \nIssue: {issue_link}  \nPull Request: {pr_link}\n\n\"\"\" + self.error_report_template\n\n    def _log_failed_request(\n        self,\n        reason: str,\n        response: requests.Response,\n        request_url: str,\n        request_headers: Optional[dict[str, Any]] = None,\n        request_params: Optional[dict[str, Any]] = None,\n        request_body: Optional[dict[str, Any]] = None,\n    ):\n        try:\n            text = response.json()\n        except json.JSONDecodeError:\n            text = response.text\n\n        self.log.error(\n            reason,\n            request_url=request_url,\n            request_headers=request_headers,\n            request_params=request_params,\n            request_body=request_body,\n            response_text=text,\n            response_code=response.status_code,\n            response_headers=response.headers,\n        )\n\n    def _get_headers(self):\n        return {\n            'Authorization': f'Bearer {self.token}',\n            'Accept': 'application/vnd.github+json',\n            'X-GitHub-Api-Version': '2022-11-28',\n        }\n\n    def _get_shield(self, success: Optional[bool] = None):\n        action_url = f'https://github.com/{self.owner}/{self.repo_name}/actions/runs/{self.run_id}'\n        if success is None:\n            shield = f\"[![AutoPR Running](https://img.shields.io/badge/AutoPR-running-yellow)]({action_url})\"\n        elif success:\n            shield = f\"[![AutoPR Success](https://img.shields.io/badge/AutoPR-success-brightgreen)]({action_url})\"\n        else:\n            shield = f\"[![AutoPR Failure](https://img.shields.io/badge/AutoPR-failure-red)]({action_url})\"\n        return shield\n\n    def _build_issue_template_link(self, **kwargs):\n        shield = self._get_shield(success=False)\n        kwargs['shield'] = shield\n        if self.issue is not None:\n            kwargs['issue_link'] = f\"https://github.com/{self.owner}/{self.repo_name}/issues/{self.issue.number}\"\n        else:\n            kwargs['issue_link'] = \"None\"\n        if self.pr_number is not None:\n            kwargs['pr_link'] = f\"https://github.com/{self.owner}/{self.repo_name}/pull/{self.pr_number}\"\n        else:\n            kwargs['pr_link'] = \"None\"\n        return super()._build_issue_template_link(**kwargs)\n\n    def _build_bodies(self, success: Optional[bool] = None):\n        bodies = super()._build_bodies(success=success)\n\n        # Make shield\n        shield = self._get_shield(success=success)\n        bodies[0] = shield + '\\n\\n' + bodies[0]\n        return bodies\n\n    def _set_title(self, title: str):\n        self._update_pr_title(self.pr_number, title)\n\n    def _publish_progress(self, bodies: list[str], success: bool = False):\n        # If overwrite existing, find the PR number\n        if not self.pr_number and self.overwrite_existing:\n            pr = self._find_existing_pr()\n            if pr is not None:\n                self.pr_number = pr['number']\n                self.pr_node_id = pr['node_id']\n\n        # If PR does not exist yet, create it\n        if not self.pr_number:\n            pr = self._create_pr(self.title, bodies, success)\n            if pr is None:\n                raise RuntimeError(\"Failed to create PR\")\n            self.pr_number = pr['number']\n            self.pr_node_id = pr['node_id']\n            return\n\n        # Update the comments\n        for i, body in enumerate(bodies):\n            if i >= len(self._comment_ids):\n                comment_id = self.publish_comment(body, self.pr_number)\n                if comment_id is None:\n                    raise RuntimeError(\"Failed to publish progress comment\")\n                self._comment_ids.append(comment_id)\n                continue\n            comment_id = self._comment_ids[i]\n            if comment_id is self.PRBodySentinel:\n                self._update_pr_body(self.pr_number, body)\n            else:\n                self._update_pr_comment(str(comment_id), body)\n\n        # Update draft status\n        if self._drafts_supported:\n            if self.pr_node_id is None:\n                self.pr_node_id = self._get_pull_request_node_id(self.pr_number)\n            self._set_pr_draft_status(self.pr_node_id, not success)\n\n    def _find_existing_pr(self) -> Optional[dict[str, Any]]:\n        \"\"\"\n        Returns the PR dict of the first open pull request with the same head and base branches\n        \"\"\"\n\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/pulls'\n        headers = self._get_headers()\n        params = {'state': 'open', 'head': f'{self.owner}:{self.head_branch}', 'base': self.base_branch}\n        response = requests.get(url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            prs = response.json()\n            if prs:\n                return prs[0]  # Return the first pull request found\n\n        self._log_failed_request(\n            'Failed to get pull requests',\n            request_url=url,\n            request_headers=headers,\n            request_params=params,\n            response=response,\n        )\n        return None\n\n    def _create_pr(self, title: str, bodies: list[str], success: bool) -> dict[str, Any]:\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/pulls'\n        headers = self._get_headers()\n        data = {\n            'head': self.head_branch,\n            'base': self.base_branch,\n            'title': title,\n            'body': bodies[0],\n        }\n        if self._drafts_supported:\n            data['draft'] = \"true\" if not success else \"false\"\n        response = requests.post(url, json=data, headers=headers)\n\n        if response.status_code != 201:\n            # if draft pull request is not supported\n            if self._is_draft_error(response.text):\n                del data['draft']\n                response = requests.post(url, json=data, headers=headers)\n                if response.status_code != 201:\n                    self._log_failed_request(\n                        'Failed to create pull request',\n                        request_url=url,\n                        request_headers=headers,\n                        request_body=data,\n                        response=response,\n                    )\n\n                    raise RuntimeError('Failed to create pull request')\n            else:\n                self._log_failed_request(\n                    'Failed to create pull request',\n                    request_url=url,\n                    request_headers=headers,\n                    request_body=data,\n                    response=response,\n                )\n                raise RuntimeError('Failed to create pull request')\n\n        self.log.debug('Pull request created successfully',\n                       headers=response.headers)\n        pr = response.json()\n        pr_number = pr['number']\n\n        self._comment_ids = [self.PRBodySentinel]\n\n        # Add additional bodies as comments\n        for body in bodies[1:]:\n            id_ = self.publish_comment(body, pr_number)\n            if id_ is None:\n                raise RuntimeError(\"Failed to publish progress comment\")\n            self._comment_ids.append(id_)\n\n        return pr\n\n    def _patch_pr(self, pr_number: int, data: dict[str, Any]):\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/pulls/{pr_number}'\n        headers = self._get_headers()\n        response = requests.patch(url, json=data, headers=headers)\n\n        if response.status_code == 200:\n            self.log.debug('Pull request updated successfully')\n            return\n\n        self._log_failed_request(\n            'Failed to update pull request',\n            request_url=url,\n            request_headers=headers,\n            request_body=data,\n            response=response,\n        )\n\n    def _is_draft_error(self, response_text: str):\n        response_obj = json.loads(response_text)\n        is_draft_error = 'message' in response_obj and \\\n            'draft pull requests are not supported' in response_obj['message'].lower()\n        if is_draft_error:\n            self.log.warning(\"Pull request drafts error on this repo\")\n            self._drafts_supported = False\n        return is_draft_error\n\n    def _get_pull_request_node_id(self, pr_number: str) -> str:\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/pulls/{pr_number}'\n        headers = self._get_headers()\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            return response.json()['node_id']\n\n        self._log_failed_request(\n            'Failed to get pull request node id',\n            request_url=url,\n            request_headers=headers,\n            response=response,\n        )\n\n        raise RuntimeError('Failed to get pull request node id')\n\n    def _set_pr_draft_status(self, pr_node_id: str, is_draft: bool):\n        # sadly this is only supported by graphQL\n        if is_draft:\n            graphql_query = '''\n                mutation ConvertPullRequestToDraft($pullRequestId: ID!) {\n                  convertPullRequestToDraft(input: { pullRequestId: $pullRequestId }) {\n                    clientMutationId\n                  }\n                }\n            '''\n        else:\n            graphql_query = '''\n                mutation MarkPullRequestReadyForReview($pullRequestId: ID!) {\n                  markPullRequestReadyForReview(input: { pullRequestId: $pullRequestId }) {\n                    clientMutationId\n                  }\n                }\n            '''\n        headers = self._get_headers() | {\n            'Content-Type': 'application/json'\n        }\n\n        # Undraft the pull request\n        data = {'pullRequestId': pr_node_id}\n        url = 'https://api.github.com/graphql'\n        body = {'query': graphql_query, 'variables': data}\n        response = requests.post(\n            url,\n            headers=headers,\n            json=body,\n        )\n\n        if response.status_code == 200:\n            self.log.debug('Pull request draft status updated successfully')\n            return\n\n        self._log_failed_request(\n            'Failed to update pull request draft status',\n            request_url=url,\n            request_headers=headers,\n            request_body=body,\n            response=response,\n        )\n\n        self._drafts_supported = False\n\n    def _update_pr_body(self, pr_number: int, body: str):\n        self._patch_pr(pr_number, {'body': body})\n\n    def _update_pr_title(self, pr_number: int, title: str):\n        self._patch_pr(pr_number, {'title': title})\n\n    def _update_pr_comment(self, comment_id: str, body: str):\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/issues/comments/{comment_id}'\n        headers = self._get_headers()\n        response = requests.patch(url, json={'body': body}, headers=headers)\n\n        if response.status_code == 200:\n            self.log.debug('Comment updated successfully')\n            return\n\n        self._log_failed_request(\n            'Failed to update comment',\n            request_url=url,\n            request_headers=headers,\n            request_body={'body': body},\n            response=response,\n        )\n\n    def _publish_comment(self, text: str, issue_number: int) -> Optional[str]:\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo_name}/issues/{issue_number}/comments'\n        headers = self._get_headers()\n        data = {\n            'body': text,\n        }\n        response = requests.post(url, json=data, headers=headers)\n\n        if response.status_code == 201:\n            self.log.debug('Commented on issue successfully')\n            return response.json()['id']\n\n        self._log_failed_request(\n            'Failed to comment on issue',\n            request_url=url,\n            request_headers=headers,\n            request_body=data,\n            response=response,\n        )\n        return None", "\n\nclass DummyPublishService(PublishService):\n    def __init__(self):\n        super().__init__(\n            owner='',\n            repo_name='',\n            head_branch='',\n            base_branch='',\n        )\n\n    def _publish_progress(self, body: str, success: bool = False):\n        pass\n\n    def _set_title(self, title: str):\n        pass\n\n    def _publish_comment(self, text: str, issue_number: int) -> Optional[str]:\n        pass", ""]}
{"filename": "autopr/services/diff_service.py", "chunked_list": ["import tempfile\nfrom typing import Optional\n\nimport structlog\nfrom git.repo import Repo\n\nfrom autopr.models.artifacts import DiffStr\n\nlog = structlog.get_logger()\n", "log = structlog.get_logger()\n\n\nclass DiffService:\n    \"\"\"\n    Service for getting and applying diffs.\n\n    Diffs are represented as `DiffStr` (a type alias for `str`).\n    \"\"\"\n\n    def __init__(\n        self,\n        repo: Repo,\n    ):\n        self.repo = repo\n\n    def apply_diff(self, diff: DiffStr, check: bool = False) -> None:\n        raise NotImplementedError()\n\n    def get_diff(self, filepaths: Optional[list[str]] = None) -> DiffStr:\n        if not filepaths:\n            # Add all files in repo\n            self.repo.git.execute([\"git\", \"add\", \"-A\"])\n        else:\n            # Add specific files\n            self.repo.git.execute([\"git\", \"add\", *filepaths])\n        # Get diff\n        diff = self.repo.git.execute([\"git\", \"diff\", \"--staged\"])\n        # Reset staged files\n        self.repo.git.execute([\"git\", \"reset\", \"HEAD\"])\n        return DiffStr(diff)", "\n\nclass GitApplyService(DiffService):\n    def apply_diff(self, diff: DiffStr, check: bool = False) -> None:\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(diff.encode())\n            f.flush()\n            log.debug('Applying diff...')\n            self.repo.git.execute([\"git\",\n                                   \"apply\",\n                                   \"--allow-empty\",\n                                   f.name])", "\n\nclass PatchService(DiffService):\n    def apply_diff(self, diff: DiffStr, check: bool = False) -> None:\n        with tempfile.NamedTemporaryFile(suffix=\".diff\") as f:\n            f.write(diff.encode())\n            f.flush()\n            log.debug('Applying diff...')\n            commands = [\n                \"patch\",\n                \"--no-backup-if-mismatch\",\n                \"--ignore-whitespace\",\n                \"-p0\",\n                \"--force\",\n                \"-i\",\n                f.name\n            ]\n            if check:\n                commands += [\"--dry-run\"]\n            self.repo.git.execute(commands)", ""]}
{"filename": "autopr/services/chain_service.py", "chunked_list": ["import logging\nfrom typing import Any, Union, Optional, Callable\n\nimport openai.error\nimport pydantic\nimport structlog\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n\nfrom autopr.services.publish_service import PublishService\nfrom langchain.llms.base import BaseLLM", "from autopr.services.publish_service import PublishService\nfrom langchain.llms.base import BaseLLM\n\nfrom langchain.chat_models.base import BaseChatModel\n\nfrom langchain.schema import BaseOutputParser, PromptValue\n\nfrom autopr.models.prompt_chains import PromptChain\nfrom autopr.repos.completions_repo import CompletionsRepo\nfrom langchain import PromptTemplate, OpenAI", "from autopr.repos.completions_repo import CompletionsRepo\nfrom langchain import PromptTemplate, OpenAI\nfrom langchain.chat_models import ChatOpenAI as LangChainChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n\n\nclass ChatOpenAI(LangChainChatOpenAI):\n    request_timeout = 240\n\n    def _create_retry_decorator(self) -> Callable[[Any], Any]:\n        # override langchain's retry decorator to wait up to 240 seconds instead of 10\n        min_seconds = 1\n        max_seconds = 240\n        return retry(\n            reraise=True,\n            stop=stop_after_attempt(self.max_retries),\n            wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\n            retry=(\n                retry_if_exception_type(openai.error.Timeout)\n                | retry_if_exception_type(openai.error.APIError)\n                | retry_if_exception_type(openai.error.APIConnectionError)\n                | retry_if_exception_type(openai.error.RateLimitError)\n                | retry_if_exception_type(openai.error.ServiceUnavailableError)\n            ),\n        )", "\n\nclass ChainService:\n    \"\"\"\n    Service that handles running langchain completions according to a PromptChain subclass.\n\n    This service is responsible for:\n    - compiling the prompt according to `PromptChain.prompt_template` and `PromptChain.get_string_params()`\n    - running the prompt through langchain\n    - parsing the output according to `PromptChain.output_parser`\n    - Keeping `publish_service` informed of what's going on\n    \"\"\"\n\n    def __init__(\n        self,\n        completions_repo: CompletionsRepo,\n        publish_service: PublishService,\n        context_limit: int = 8192,\n        min_tokens: int = 2000,\n    ):\n        self.completions_repo = completions_repo\n        self.publish_service = publish_service\n        self.context_limit = context_limit\n        self.min_tokens = min_tokens\n\n        # TODO find a better way to integrate completions repo with langchain\n        #   can we make a BaseLanguageModel that takes a completions repo?\n        #   or should we replace completions repo with BaseLanguageModel?\n        self.model: Union[BaseChatModel, BaseLLM]\n        if completions_repo.model in [\n            \"gpt-4\",\n            \"gpt-3.5-turbo\"\n        ]:\n            self.model = ChatOpenAI(\n                model_name=completions_repo.model,\n                temperature=completions_repo.temperature,\n                max_tokens=completions_repo.max_tokens,\n            )  # type: ignore\n        elif completions_repo.model == \"text-davinci-003\":\n            self.model = OpenAI(\n                model_name=completions_repo.model,\n                temperature=completions_repo.temperature,\n                max_tokens=completions_repo.max_tokens,\n            )  # type: ignore\n        else:\n            raise ValueError(f\"Unsupported model {completions_repo.model}\")\n\n        self.log = structlog.get_logger().bind(\n            model=completions_repo.model,\n            service=\"ChainService\",\n        )\n\n    def _get_model_template(\n        self,\n        chain: PromptChain,\n        parser: Optional[BaseOutputParser],\n    ) -> PromptValue:\n        variables = dict(chain.get_string_params())\n        variable_names = list(variables.keys())\n        partial_variables = {}\n        if parser is not None:\n            partial_variables[\"format_instructions\"] = parser.get_format_instructions()\n\n        if isinstance(self.model, BaseChatModel):\n            template = ChatPromptTemplate(\n                messages=[\n                    HumanMessagePromptTemplate.from_template(chain.prompt_template)\n                ],\n                input_variables=variable_names,\n                partial_variables=partial_variables,\n            )\n        else:\n            template = PromptTemplate(\n                template=chain.prompt_template,\n                input_variables=variable_names,\n                partial_variables=partial_variables,\n            )\n        return template.format_prompt(**variables)\n\n    def _run_model(self, template: PromptValue) -> Any:\n        if isinstance(self.model, BaseChatModel):\n            return self.model(template.to_messages()).content\n        else:\n            return self.model(template.to_string())\n\n    def run_chain(self, chain: PromptChain) -> Any:\n        self.publish_service.start_section(f\"\u26d3 Running {chain.__class__.__name__} chain\")\n        # Make sure the prompt is not too long\n        max_length = self.context_limit - self.min_tokens\n        success = chain.ensure_token_length(max_length)\n        if not success:\n            return None\n\n        if chain.output_parser:\n            parser = chain.output_parser\n        else:\n            parser = None\n        prompt_value = self._get_model_template(chain, parser)\n        str_prompt = prompt_value.to_string()\n\n        self.publish_service.publish_code_block(\n            heading=\"Prompt\",\n            code=str_prompt,\n        )\n        self.log.info(\"Running chain\", prompt=str_prompt)\n\n        raw_output = self._run_model(prompt_value)\n\n        self.publish_service.publish_code_block(\n            heading=\"Output\",\n            code=raw_output,\n        )\n        self.log.info(\"Got result\", raw_output=raw_output)\n\n        if parser is not None:\n            output = parser.parse(raw_output)\n            self.log.info(\"Parsed output\", result=output)\n            if output is None:\n                self.publish_service.end_section(f\"\u274c Chain {chain.__class__.__name__} failed to parse result\")\n                return None\n            else:\n                self.publish_service.publish_code_block(\n                    heading=\"Parsed output\",\n                    code=output.json(indent=2) if isinstance(output, pydantic.BaseModel) else str(output),\n                )\n        else:\n            output = raw_output\n        self.publish_service.end_section(f\"\u26d3 {chain.__class__.__name__} completed\")\n        return output", ""]}
{"filename": "autopr/services/rail_service.py", "chunked_list": ["import pkg_resources\nimport lxml.etree as ET\n\nimport json\nimport traceback\nfrom typing import Callable, Any, Optional, TypeVar, Type\n\nimport pydantic\n\nimport guardrails as gr", "\nimport guardrails as gr\nfrom autopr.models.rail_objects import RailObject\nfrom autopr.models.prompt_rails import PromptRail\n\nimport structlog\n\nfrom autopr.repos.completions_repo import CompletionsRepo\nfrom autopr.services.publish_service import PublishService\nfrom guardrails.utils.constants import constants", "from autopr.services.publish_service import PublishService\nfrom guardrails.utils.constants import constants\n\nlog = structlog.get_logger()\n\nRailObjectSubclass = TypeVar('RailObjectSubclass', bound=RailObject)\nBaseModelSubclass = TypeVar('BaseModelSubclass', bound=pydantic.BaseModel)\n\n\nclass RailService:\n    \"\"\"\n    Service for invoking guardrails according to PromptRail and RailObject subclasses.\n    See PromptRail, RailObject, and [Guardrails docs](https://shreyar.github.io/guardrails/rail/) for more information.\n\n    To make a guardrails call:\n    - define a RailObject subclass\n    - define a PromptRail subclass\n    - instantiate the PromptRail\n    - call `rail_service.run_prompt_rail(rail)` with the instantiated PromptRail\n\n    For example:\n\n        class Colors(RailObject):\n            output_spec = '<list name=\"colors\"><string/></list>'\n\n            colors: list[str]\n\n        class MyPromptRail(PromptRail):\n            output_type = Colors\n            prompt_template = \"What colors is {something}?\"\n\n            something: str\n\n        rail = MyPromptRail(something=\"a zebra\")\n        colors = rail_service.run_prompt_rail(rail)\n\n        print(colors)  # colors=['black', 'white']\n\n    This service is responsible for:\n    - Compiling prompts according to `PromptRail.prompt_template`, `RailObject.output_spec`,\n      and `RailObject.get_rail_spec()`\n    - Invoking a guardrail LLM calls,\n      optionally after an ordinary LLM call if `PromptRail.two_step` is True\n    - Parsing the guardrail LLM response into a RailObject (pydantic) instance\n    - Publishing the RailObject instance to the publish service\n    - Keeping `publish_service` informed of what's going on\n\n\n    Parameters\n    ----------\n    min_tokens: int\n        Minimum number of tokens to leave in the context window to allow for response\n    context_limit: int\n        Context window token size limit\n    num_reasks: int\n        Number of times to re-ask the guardrail if it fails\n    temperature: float\n        Temperature to use for guardrails calls\n    raw_system_prompt: str\n        System prompt to use for ordinary LLM calls (if `PromptRail.two_step` is True)\n    \"\"\"\n\n    _constants_imported = False\n\n    def __init__(\n        self,\n        completions_repo: CompletionsRepo,\n        publish_service: PublishService,\n        min_tokens: int = 1000,\n        context_limit: int = 8192,\n        num_reasks: int = 2,\n        temperature: float = 0.8,\n        raw_system_prompt: str = 'You are a software developer and git nerd, a helpful planning and coding assistant.',\n    ):\n        self.completions_repo = completions_repo\n        self.publish_service = publish_service\n        self.min_tokens = min_tokens\n        self.context_limit = context_limit\n        self.num_reasks = num_reasks\n        self.temperature = temperature\n        self.raw_system_prompt = raw_system_prompt\n\n        self._import_constants()\n\n    def _import_constants(self):\n        if self._constants_imported:\n            return\n\n        constants_file = pkg_resources.resource_filename('autopr', 'constants.xml')\n\n        with open(constants_file, \"r\") as f:\n            xml = f.read()\n\n        parser = ET.XMLParser(encoding=\"utf-8\")\n        parsed_constants = ET.fromstring(xml, parser=parser)\n\n        for child in parsed_constants:\n            if isinstance(child, ET._Comment):\n                continue\n            if isinstance(child, str):\n                continue\n\n            constant_name = child.tag\n            constant_value = child.text\n            constants[constant_name] = constant_value\n\n        self._constants_imported = True\n\n    def run_rail_string(\n        self,\n        rail_spec: str,\n        prompt_params: dict[str, Any],\n        heading: str = \"\",\n    ) -> Optional[dict[str, Any]]:\n        \"\"\"\n        Run a guardrails call with the given rail spec and prompt parameters.\n        \"\"\"\n        title_heading = heading[0].upper() + heading[1:]\n        self.publish_service.start_section(f\"\ud83d\udee4 Running {heading} rail\")\n\n        instructions = self.get_rail_instructions(rail_spec, prompt_params)\n        if instructions.strip():\n            self.publish_service.publish_code_block(\n                heading='Instructions',\n                code=instructions,\n                language='xml',  # xml for nice guardrails highlighting\n            )\n\n        str_prompt = self.get_rail_message(rail_spec, prompt_params)\n        self.publish_service.publish_code_block(\n            heading='Prompt',\n            code=str_prompt,\n            language='xml',  # xml for nice guardrails highlighting\n        )\n\n        def completion_func(prompt: str, instructions: str):\n            return self.completions_repo.complete(\n                prompt=prompt,\n                system_prompt=instructions,\n                temperature=self.temperature,\n            )\n\n        try:\n            pr_guard = gr.Guard.from_rail_string(\n                rail_spec,  # make sure to import custom validators before this\n                num_reasks=self.num_reasks,\n            )\n\n            log.debug(\n                'Running rail',\n                rail_spec=rail_spec,\n                prompt_params=prompt_params,\n            )\n            # Invoke guardrails\n            raw_o, dict_o = pr_guard(\n                completion_func,\n                prompt_params=prompt_params\n            )\n        except Exception:\n            log.exception('Error running rail',\n                          prompt=str_prompt)\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {title_heading} derailed (guardrails error)\")\n            return None\n\n        log.debug('Ran rail',\n                  raw_output=raw_o,\n                  dict_output=dict_o)\n        self.publish_service.publish_code_block(\n            heading='Raw output',\n            code=raw_o,\n            language='json',\n        )\n\n        if dict_o is None:\n            log.warning(f'Got None from rail',\n                        rail_spec=rail_spec,\n                        prompt_params=prompt_params)\n            self.publish_service.end_section(f\"\ud83d\udca5 {title_heading} derailed (guardrails returned None)\")\n            return None\n\n        self.publish_service.publish_code_block(\n            heading='Parsed output',\n            code=json.dumps(dict_o, indent=2),\n            language='json',\n        )\n        self.publish_service.end_section(f\"\ud83d\udee4 Ran {heading} rail\")\n\n        return dict_o\n\n    def run_rail_model(\n        self,\n        model: Type[BaseModelSubclass],\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> Optional[BaseModelSubclass]:\n        \"\"\"\n        Run a guardrails call with a pydantic model to parse the response into.\n        \"\"\"\n        self.publish_service.start_section(f\"\ud83d\udee4 Running {model.__name__} on rail\")\n\n        def completion_func(prompt: str, instructions: str):\n            return self.completions_repo.complete(\n                prompt=prompt,\n                system_prompt=instructions,\n                temperature=self.temperature,\n            )\n\n        pr_guard = gr.Guard.from_rail_string(\n            rail_spec,  # make sure to import custom validators before this\n            num_reasks=self.num_reasks,\n        )\n\n        prompt = self.get_rail_message(rail_spec, prompt_params)\n        log.debug('Running rail',\n                  rail_model=model.__name__,\n                  rail_message=prompt)\n        self.publish_service.publish_code_block(\n            heading='Prompt',\n            code=prompt,\n            language='xml',  # xml for nice guardrails highlighting\n        )\n\n        # Invoke guardrails\n        try:\n            raw_o, dict_o = pr_guard(\n                completion_func,\n                prompt_params=prompt_params,\n            )\n        except Exception:\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n                default_open=True,\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (guardrails error)\")\n            log.exception(f'Guardrails threw an exception',\n                          rail_model=model.__name__,\n                          rail_message=prompt)\n            return None\n\n        log.debug('Ran rail',\n                  rail_model=model.__name__,\n                  raw_output=raw_o,\n                  dict_output=dict_o)\n\n        self.publish_service.publish_code_block(\n            heading='Raw output',\n            code=raw_o,\n            language='json',\n        )\n\n        if dict_o is None:\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (guardrails returned None)\")\n            log.warning(f'Got None from rail',\n                        rail_model=model.__name__,\n                        raw_output=raw_o)\n            return None\n\n        self.publish_service.publish_code_block(\n            heading='Parsed output',\n            code=json.dumps(dict_o, indent=2),\n            language='json',\n        )\n\n        # Parse the output into a pydantic object\n        try:\n            parsed_obj = model.parse_obj(dict_o)\n            self.publish_service.publish_code_block(\n                heading='Validated output',\n                code=parsed_obj.json(indent=2),\n                language='json',\n            )\n            self.publish_service.end_section(f\"\ud83d\udee4 Ran {model.__name__} on rail\")\n            return parsed_obj\n        except pydantic.ValidationError:\n            log.warning(f'Got invalid output from rail',\n                        rail_object=model.__name__,\n                        raw_output=raw_o,\n                        dict_output=dict_o)\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n                default_open=True,\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (validation error)\")\n            return None\n\n    def run_rail_object(\n        self,\n        rail_object: Type[RailObjectSubclass],\n        raw_document: str\n    ) -> Optional[RailObjectSubclass]:\n        \"\"\"\n        Transforms the `raw_document` into a pydantic instance described by `rail_object`.\n        \"\"\"\n        rail_spec = rail_object.get_rail_spec()\n        return self.run_rail_model(\n            model=rail_object,\n            rail_spec=rail_spec,\n            prompt_params={\n                'raw_document': raw_document,\n            },\n        )\n\n    def run_prompt_rail(\n        self,\n        rail: PromptRail\n    ) -> Optional[RailObject]:\n        \"\"\"\n        Runs a PromptRail, asking the LLM a question and parsing the response into `PromptRail.output_type`.\n\n        :param rail:\n        :return:\n        \"\"\"\n        # Make sure the prompt is not too long\n        max_length = self.context_limit - self.min_tokens\n        success = rail.ensure_token_length(max_length)\n        if not success:\n            return None\n\n        # Run the rail\n        prompt = rail.get_prompt_message()\n        if rail.two_step:\n            initial_prompt = prompt\n            self.publish_service.start_section(f\"\ud83d\udcac Asking for {rail.__class__.__name__}\")\n\n            self.publish_service.publish_code_block(\n                heading=\"Prompt\",\n                code=prompt,\n                language=\"\",\n            )\n            prompt = self.completions_repo.complete(\n                prompt=initial_prompt,\n                system_prompt=self.raw_system_prompt,\n            )\n            self.publish_service.publish_code_block(\n                heading=\"Response\",\n                code=prompt,\n                language=\"\",\n            )\n            self.publish_service.end_section(f\"\ud83d\udcac Asked for {rail.__class__.__name__}\")\n        return self.run_rail_object(rail.output_type, prompt)\n\n    @staticmethod\n    def get_rail_instructions(\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> str:\n        pr_guard = gr.Guard.from_rail_string(rail_spec)\n        return str(pr_guard.instructions.format(**prompt_params))\n\n    @staticmethod\n    def get_rail_message(\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> str:\n        pr_guard = gr.Guard.from_rail_string(rail_spec)\n        return str(pr_guard.prompt.format(**prompt_params))", "\nclass RailService:\n    \"\"\"\n    Service for invoking guardrails according to PromptRail and RailObject subclasses.\n    See PromptRail, RailObject, and [Guardrails docs](https://shreyar.github.io/guardrails/rail/) for more information.\n\n    To make a guardrails call:\n    - define a RailObject subclass\n    - define a PromptRail subclass\n    - instantiate the PromptRail\n    - call `rail_service.run_prompt_rail(rail)` with the instantiated PromptRail\n\n    For example:\n\n        class Colors(RailObject):\n            output_spec = '<list name=\"colors\"><string/></list>'\n\n            colors: list[str]\n\n        class MyPromptRail(PromptRail):\n            output_type = Colors\n            prompt_template = \"What colors is {something}?\"\n\n            something: str\n\n        rail = MyPromptRail(something=\"a zebra\")\n        colors = rail_service.run_prompt_rail(rail)\n\n        print(colors)  # colors=['black', 'white']\n\n    This service is responsible for:\n    - Compiling prompts according to `PromptRail.prompt_template`, `RailObject.output_spec`,\n      and `RailObject.get_rail_spec()`\n    - Invoking a guardrail LLM calls,\n      optionally after an ordinary LLM call if `PromptRail.two_step` is True\n    - Parsing the guardrail LLM response into a RailObject (pydantic) instance\n    - Publishing the RailObject instance to the publish service\n    - Keeping `publish_service` informed of what's going on\n\n\n    Parameters\n    ----------\n    min_tokens: int\n        Minimum number of tokens to leave in the context window to allow for response\n    context_limit: int\n        Context window token size limit\n    num_reasks: int\n        Number of times to re-ask the guardrail if it fails\n    temperature: float\n        Temperature to use for guardrails calls\n    raw_system_prompt: str\n        System prompt to use for ordinary LLM calls (if `PromptRail.two_step` is True)\n    \"\"\"\n\n    _constants_imported = False\n\n    def __init__(\n        self,\n        completions_repo: CompletionsRepo,\n        publish_service: PublishService,\n        min_tokens: int = 1000,\n        context_limit: int = 8192,\n        num_reasks: int = 2,\n        temperature: float = 0.8,\n        raw_system_prompt: str = 'You are a software developer and git nerd, a helpful planning and coding assistant.',\n    ):\n        self.completions_repo = completions_repo\n        self.publish_service = publish_service\n        self.min_tokens = min_tokens\n        self.context_limit = context_limit\n        self.num_reasks = num_reasks\n        self.temperature = temperature\n        self.raw_system_prompt = raw_system_prompt\n\n        self._import_constants()\n\n    def _import_constants(self):\n        if self._constants_imported:\n            return\n\n        constants_file = pkg_resources.resource_filename('autopr', 'constants.xml')\n\n        with open(constants_file, \"r\") as f:\n            xml = f.read()\n\n        parser = ET.XMLParser(encoding=\"utf-8\")\n        parsed_constants = ET.fromstring(xml, parser=parser)\n\n        for child in parsed_constants:\n            if isinstance(child, ET._Comment):\n                continue\n            if isinstance(child, str):\n                continue\n\n            constant_name = child.tag\n            constant_value = child.text\n            constants[constant_name] = constant_value\n\n        self._constants_imported = True\n\n    def run_rail_string(\n        self,\n        rail_spec: str,\n        prompt_params: dict[str, Any],\n        heading: str = \"\",\n    ) -> Optional[dict[str, Any]]:\n        \"\"\"\n        Run a guardrails call with the given rail spec and prompt parameters.\n        \"\"\"\n        title_heading = heading[0].upper() + heading[1:]\n        self.publish_service.start_section(f\"\ud83d\udee4 Running {heading} rail\")\n\n        instructions = self.get_rail_instructions(rail_spec, prompt_params)\n        if instructions.strip():\n            self.publish_service.publish_code_block(\n                heading='Instructions',\n                code=instructions,\n                language='xml',  # xml for nice guardrails highlighting\n            )\n\n        str_prompt = self.get_rail_message(rail_spec, prompt_params)\n        self.publish_service.publish_code_block(\n            heading='Prompt',\n            code=str_prompt,\n            language='xml',  # xml for nice guardrails highlighting\n        )\n\n        def completion_func(prompt: str, instructions: str):\n            return self.completions_repo.complete(\n                prompt=prompt,\n                system_prompt=instructions,\n                temperature=self.temperature,\n            )\n\n        try:\n            pr_guard = gr.Guard.from_rail_string(\n                rail_spec,  # make sure to import custom validators before this\n                num_reasks=self.num_reasks,\n            )\n\n            log.debug(\n                'Running rail',\n                rail_spec=rail_spec,\n                prompt_params=prompt_params,\n            )\n            # Invoke guardrails\n            raw_o, dict_o = pr_guard(\n                completion_func,\n                prompt_params=prompt_params\n            )\n        except Exception:\n            log.exception('Error running rail',\n                          prompt=str_prompt)\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {title_heading} derailed (guardrails error)\")\n            return None\n\n        log.debug('Ran rail',\n                  raw_output=raw_o,\n                  dict_output=dict_o)\n        self.publish_service.publish_code_block(\n            heading='Raw output',\n            code=raw_o,\n            language='json',\n        )\n\n        if dict_o is None:\n            log.warning(f'Got None from rail',\n                        rail_spec=rail_spec,\n                        prompt_params=prompt_params)\n            self.publish_service.end_section(f\"\ud83d\udca5 {title_heading} derailed (guardrails returned None)\")\n            return None\n\n        self.publish_service.publish_code_block(\n            heading='Parsed output',\n            code=json.dumps(dict_o, indent=2),\n            language='json',\n        )\n        self.publish_service.end_section(f\"\ud83d\udee4 Ran {heading} rail\")\n\n        return dict_o\n\n    def run_rail_model(\n        self,\n        model: Type[BaseModelSubclass],\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> Optional[BaseModelSubclass]:\n        \"\"\"\n        Run a guardrails call with a pydantic model to parse the response into.\n        \"\"\"\n        self.publish_service.start_section(f\"\ud83d\udee4 Running {model.__name__} on rail\")\n\n        def completion_func(prompt: str, instructions: str):\n            return self.completions_repo.complete(\n                prompt=prompt,\n                system_prompt=instructions,\n                temperature=self.temperature,\n            )\n\n        pr_guard = gr.Guard.from_rail_string(\n            rail_spec,  # make sure to import custom validators before this\n            num_reasks=self.num_reasks,\n        )\n\n        prompt = self.get_rail_message(rail_spec, prompt_params)\n        log.debug('Running rail',\n                  rail_model=model.__name__,\n                  rail_message=prompt)\n        self.publish_service.publish_code_block(\n            heading='Prompt',\n            code=prompt,\n            language='xml',  # xml for nice guardrails highlighting\n        )\n\n        # Invoke guardrails\n        try:\n            raw_o, dict_o = pr_guard(\n                completion_func,\n                prompt_params=prompt_params,\n            )\n        except Exception:\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n                default_open=True,\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (guardrails error)\")\n            log.exception(f'Guardrails threw an exception',\n                          rail_model=model.__name__,\n                          rail_message=prompt)\n            return None\n\n        log.debug('Ran rail',\n                  rail_model=model.__name__,\n                  raw_output=raw_o,\n                  dict_output=dict_o)\n\n        self.publish_service.publish_code_block(\n            heading='Raw output',\n            code=raw_o,\n            language='json',\n        )\n\n        if dict_o is None:\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (guardrails returned None)\")\n            log.warning(f'Got None from rail',\n                        rail_model=model.__name__,\n                        raw_output=raw_o)\n            return None\n\n        self.publish_service.publish_code_block(\n            heading='Parsed output',\n            code=json.dumps(dict_o, indent=2),\n            language='json',\n        )\n\n        # Parse the output into a pydantic object\n        try:\n            parsed_obj = model.parse_obj(dict_o)\n            self.publish_service.publish_code_block(\n                heading='Validated output',\n                code=parsed_obj.json(indent=2),\n                language='json',\n            )\n            self.publish_service.end_section(f\"\ud83d\udee4 Ran {model.__name__} on rail\")\n            return parsed_obj\n        except pydantic.ValidationError:\n            log.warning(f'Got invalid output from rail',\n                        rail_object=model.__name__,\n                        raw_output=raw_o,\n                        dict_output=dict_o)\n            self.publish_service.publish_code_block(\n                heading='Error',\n                code=traceback.format_exc(),\n                language='python',\n                default_open=True,\n            )\n            self.publish_service.end_section(f\"\ud83d\udca5 {model.__name__} derailed (validation error)\")\n            return None\n\n    def run_rail_object(\n        self,\n        rail_object: Type[RailObjectSubclass],\n        raw_document: str\n    ) -> Optional[RailObjectSubclass]:\n        \"\"\"\n        Transforms the `raw_document` into a pydantic instance described by `rail_object`.\n        \"\"\"\n        rail_spec = rail_object.get_rail_spec()\n        return self.run_rail_model(\n            model=rail_object,\n            rail_spec=rail_spec,\n            prompt_params={\n                'raw_document': raw_document,\n            },\n        )\n\n    def run_prompt_rail(\n        self,\n        rail: PromptRail\n    ) -> Optional[RailObject]:\n        \"\"\"\n        Runs a PromptRail, asking the LLM a question and parsing the response into `PromptRail.output_type`.\n\n        :param rail:\n        :return:\n        \"\"\"\n        # Make sure the prompt is not too long\n        max_length = self.context_limit - self.min_tokens\n        success = rail.ensure_token_length(max_length)\n        if not success:\n            return None\n\n        # Run the rail\n        prompt = rail.get_prompt_message()\n        if rail.two_step:\n            initial_prompt = prompt\n            self.publish_service.start_section(f\"\ud83d\udcac Asking for {rail.__class__.__name__}\")\n\n            self.publish_service.publish_code_block(\n                heading=\"Prompt\",\n                code=prompt,\n                language=\"\",\n            )\n            prompt = self.completions_repo.complete(\n                prompt=initial_prompt,\n                system_prompt=self.raw_system_prompt,\n            )\n            self.publish_service.publish_code_block(\n                heading=\"Response\",\n                code=prompt,\n                language=\"\",\n            )\n            self.publish_service.end_section(f\"\ud83d\udcac Asked for {rail.__class__.__name__}\")\n        return self.run_rail_object(rail.output_type, prompt)\n\n    @staticmethod\n    def get_rail_instructions(\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> str:\n        pr_guard = gr.Guard.from_rail_string(rail_spec)\n        return str(pr_guard.instructions.format(**prompt_params))\n\n    @staticmethod\n    def get_rail_message(\n        rail_spec: str,\n        prompt_params: dict[str, Any]\n    ) -> str:\n        pr_guard = gr.Guard.from_rail_string(rail_spec)\n        return str(pr_guard.prompt.format(**prompt_params))", ""]}
{"filename": "autopr/services/commit_service.py", "chunked_list": ["import os\n\nfrom git.repo import Repo\n\nimport structlog\n\n\nclass CommitService:\n    \"\"\"\n    Service for creating branches, committing changes, and calling `git push` on the repository.\n\n    Ensures there is always a commit on the branch.\n    \"\"\"\n\n    def __init__(\n        self,\n        repo: Repo,\n        repo_path: str,\n        branch_name: str,\n        base_branch_name: str,\n    ):\n        self.repo = repo\n        self.repo_path = repo_path\n        self.branch_name = branch_name\n        self.base_branch_name = base_branch_name\n\n        self._empty_commit_message = \"[placeholder]\"\n\n        self.log = structlog.get_logger(service=\"commit\")\n\n    def overwrite_new_branch(self):\n        # Checkout and pull base branch\n        self.log.debug(f'Checking out {self.base_branch_name}...')\n        self.repo.heads[self.base_branch_name].checkout()\n        self.log.debug('Pulling latest changes...')\n        self.repo.remotes.origin.pull()\n\n        # If branch already exists, delete it\n        if self.branch_name in self.repo.heads:\n            self.log.debug(f'Deleting existing branch {self.branch_name}...')\n            self.repo.delete_head(self.branch_name, force=True)\n\n        # Create new branch with create_new_ref\n        self.log.debug(f'Creating new branch {self.branch_name}...')\n        self.repo.create_head(self.branch_name, self.base_branch_name)\n\n        # Checkout new branch\n        self.repo.heads[self.branch_name].checkout()\n\n        # Create empty commit\n        self.commit(self._empty_commit_message)\n\n    def ensure_branch_exists(self):\n        # Fetch\n        self.log.debug('Fetching...')\n        self.repo.remotes.origin.fetch()\n        remote = self.repo.remote()\n        references = remote.fetch()\n\n        # If branch already exists, checkout and pull\n        if f'{remote.name}/{self.branch_name}' in [ref.name for ref in references]:\n            # Check if branch exists locally\n            if self.branch_name in [ref.name for ref in self.repo.heads]:\n                self.log.debug(f'Checking out {self.branch_name}...')\n                self.repo.heads[self.branch_name].checkout()\n                self.log.debug('Pulling latest changes...')\n                self.repo.remotes.origin.pull()\n            else:\n                # If not, create a local branch that tracks the remote branch\n                self.log.debug(f'Checking out -b {self.branch_name}...')\n                self.repo.create_head(self.branch_name, f'{remote.name}/{self.branch_name}').checkout()\n        else:\n            self.log.debug(f'Branch {self.branch_name} does not exist, creating...')\n            self.overwrite_new_branch()\n\n    def commit(self, commit_message: str, push: bool = True) -> None:\n        # Remove empty commit if exists\n        if commit_message != self._empty_commit_message and \\\n                self.repo.head.commit.message.rstrip() == self._empty_commit_message:\n            self.log.debug('Removing empty commit...')\n            self.repo.git.execute([\"git\", \"reset\", \"HEAD^\"])\n\n        # Remove guardrails log if exists (so it's not committed later)\n        if 'guardrails.log' in self.repo.untracked_files:\n            self.log.debug('Removing guardrails.log...')\n            os.remove(\n                os.path.join(self.repo_path, 'guardrails.log')\n            )\n\n        # Add and commit all\n        self.repo.git.execute([\"git\", \"add\", \".\"])\n        self.repo.git.execute([\"git\", \"commit\", \"--allow-empty\", \"-m\", commit_message])\n\n        # Get the commit's diff for log\n        diff = self.repo.git.execute([\"git\", \"diff\", \"HEAD^\", \"HEAD\"])\n        self.log.info(\"Committed changes\", commit_message=commit_message, diff=diff)\n\n        # Push branch to remote\n        if push:\n            self.log.debug(f'Pushing branch {self.branch_name} to remote...')\n            self.repo.git.execute([\"git\", \"push\", \"-f\", \"origin\", self.branch_name])", ""]}
