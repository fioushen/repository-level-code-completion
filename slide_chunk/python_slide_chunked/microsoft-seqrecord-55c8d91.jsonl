{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n    name=\"seqrecord\",\n    version=\"0.1.8\",\n    description=\"various bugs fix\",\n    author_email=\"shuhang0chen@gmail.com\",\n    maintainer_email=\"shuhang0chen@gmail.com\",\n    packages=find_packages(),\n    install_requires=[\"numpy\", \"torch\", \"torchdata\", \"aiofiles\"],", "    packages=find_packages(),\n    install_requires=[\"numpy\", \"torch\", \"torchdata\", \"aiofiles\"],\n)\n"]}
{"filename": "seqrecord/__init__.py", "chunked_list": ["from .weather.seqrecord import WSeqRecord\n"]}
{"filename": "seqrecord/utils.py", "chunked_list": ["from collections import OrderedDict\nimport io\nimport os\nfrom typing import Callable, List, Tuple\n\nPRODUCER_SLEEP_INTERVAL = 0.0001  # Interval between buffer fullfilment checks\nCONSUMER_SLEEP_INTERVAL = (\n    0.0001  # Interval between checking items availablitity in buffer\n)\n", ")\n\n\ndef distribute_loads(works: int, num_processes: int) -> List[Tuple[int, int]]:\n    \"\"\"Given the overall works and number of processes, allocate evenly the loads that each process should take.\n\n    Args:\n        works (int): amount of over all work\n        num_processes (int): number of processes available\n\n    Returns:\n        List[Tuple[int, int]]: indices of work each process is responsible for\n    \"\"\"\n    assert (\n        works >= num_processes\n    ), \"The amount of works is less than number of processes.\"\n    ans = []\n    start = 0\n    works_remain = works\n    loads_per_process = round(works / num_processes)\n    for i in range(num_processes):\n        if works_remain % (num_processes - i) == 0:\n            loads_per_process = works_remain // (num_processes - i)\n        else:\n            loads_per_process = round(works_remain / (num_processes - i))\n        end = min(start + loads_per_process, works)\n        ans.append((start, end))\n        works_remain -= end - start\n        start = end\n    return ans", "\n\nclass LRUCache:\n    \"\"\"LRU cache using OrderedDict\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    def __init__(self, capacity: int, monitoring: bool = False) -> None:\n        self.capacity = capacity\n        self.values = OrderedDict()  # Dict[int, io.BufferedReader] = {}\n        self.monitoring = monitoring\n        if self.monitoring:\n            self.hits = 0\n            self.misses = 0\n\n    def put(self, key: int, value):\n        \"\"\"add (key, value) to cache following lru policy using ordered dict\n\n        Args:\n            ele (_type_): _description_\n\n        Returns:\n            evicted element in case there is post-processing needed.\n        \"\"\"\n        evicted = None\n        if key not in self.values:\n            if len(self.values) == self.capacity:\n                # popitem() returns key, value pair\n                _, evicted = self.values.popitem(last=False)\n        else:\n            self.values.pop(key)\n        self.values[key] = value\n        return evicted\n\n    def get(self, key: int):\n        \"\"\"retrieve element from cache\n\n        Args:\n            key (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        res = None\n        if key in self.values:\n            self.values[key] = self.values.pop(key)\n            res = self.values[key]\n        if self.monitoring:\n            self.hits = self.hits + (res is not None)\n            self.misses = self.misses + (res is None)\n        return res\n\n    def pop(self, key: int):\n        return self.values.pop(key)\n\n    def keys(self):\n        return [key for key in self.values.keys()]\n\n    def stats(self) -> str:\n\n        if self.monitoring:\n            return f\"Cache hitting rate is {self.hits / (self.hits + self.misses)} with overall number of reads {self.hits + self.misses}\"\n        else:\n            return \"Monitoring was not enabled for cache, hence no stats available\"", "\n\nclass FileManager:\n    \"\"\"manager opening and closing of file descriptors, with a LRU Cache for opened files (remote files, connection).\"\"\"\n\n    # todo: add stats for monitoring cache miss and hits\n\n    def __init__(\n        self,\n        cache_capacity: int,\n        monitoring: bool = False,\n    ) -> None:\n        self.cache = LRUCache(cache_capacity, monitoring)\n\n    def open_file(self, file_idx: int, file_path: str) -> io.BufferedReader:\n        f = self.cache.get(file_idx)\n        if f is None:\n            # todo: make this a variable or function or something fixed\n            file_path = file_path\n            f = open(file_path, \"rb\")\n            evicted = self.cache.put(key=file_idx, value=f)\n            if evicted is not None:\n                evicted.close()\n        return f\n\n    def close_all_files(self) -> None:\n        \"\"\"Close all open file descriptors in cache\"\"\"\n        for key in self.cache.keys():\n            self.cache.pop(key).close()\n        return", "\n\nclass WriterBuffer:\n    def __init__(self) -> None:\n        self.buffer = io.BytesIO()\n\n    def write(self, buffer) -> None:\n        self.buffer.write(buffer)\n\n    def is_empty(self) -> bool:\n        return self.buffer.tell() == 0\n\n    def getbuffer(self) -> memoryview:\n        return self.buffer.getbuffer()\n\n    def getvalue(self):\n        return self.buffer.getvalue()\n\n    def clear(self) -> None:\n        self.buffer.flush()\n        self.buffer.seek(0)\n\n    def close(self) -> None:\n        self.buffer.close()", "\n\nclass TimeTracker:\n    def __init__(self) -> None:\n        self.time: float = 0.0\n        self.nbytes: int = 0\n\n    def add(self, time: float, nbytes: int) -> None:\n        self.time += time\n        self.nbytes += nbytes\n\n    def summarize(self) -> str:\n        return f\"Took {self.time} to read {self.nbytes} bytes, with average rate {self.nbytes / self.time} bytes/s\"", ""]}
{"filename": "seqrecord/robot/datapipes.py", "chunked_list": ["\"\"\"Iterative datapipes built from seqrecord\"\"\"\n\n\nfrom typing import Callable, Dict, List, Optional\n\nimport numpy as np\nimport torch\nimport torchdata.datapipes as dp\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom .seqrecord import SeqRecord\n\n\n@dp.functional_datapipe(\"video_datapipe\")\nclass VideoDatapipeFromSeqRecord(dp.iter.IterDataPipe):\n    \"\"\"A torch datapiple class that iteratively read video(episode) segment from record files.\"\"\"\n\n    def __init__(\n        self,\n        record: SeqRecord,\n        segment_len: int,\n        features_rename: Dict[str, str],\n        shuffle_recordfiles: bool = False,\n    ) -> None:\n        super().__init__()\n        self.segmentproto = record.get_proto4segment(\n            segment_len, [feature for feature in features_rename]\n        )\n        self.record = record\n        self.features_rename = features_rename\n        self.shuffle_recordfiles = shuffle_recordfiles\n\n    def __iter__(self):\n        for segment in self.record.read_segments(\n            self.segmentproto, shuffle_recordfiles=self.shuffle_recordfiles\n        ):\n            res = {}\n            for feature in self.features_rename:\n                res[self.features_rename[feature]] = segment[feature]\n            yield res", "\n\n@dp.functional_datapipe(\"item_datapipe\")\nclass ItemDatapipeFromSeqRecord(dp.iter.IterDataPipe):\n    \"\"\"A torch datapiple class that iteratively read item (frame) from record files.\"\"\"\n\n    def __init__(\n        self,\n        record: SeqRecord,\n        features_rename: Dict[str, str],\n        shuffle_recordfiles: bool = False,\n    ) -> None:\n        super().__init__()\n        self.record = record\n        self.features_rename = features_rename\n        self.shuffle_recordfiles = shuffle_recordfiles\n\n    def __iter__(self):\n        res = {}\n        for item in self.record.read_items(\n            features=[feature for feature in self.features_rename],\n            shuffle_recordfiles=self.shuffle_recordfiles,\n        ):\n            res = {}\n            for feature in self.features_rename:\n                res[self.features_rename[feature]] = item[feature]\n            yield res", "\n\ndef collate_fn(batch: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n    collated_batch: Dict[str, torch.Tensor] = {}\n    for feature in batch[0]:\n        collated_batch[feature] = torch.from_numpy(\n            np.stack([batch[i][feature] for i in range(len(batch))], axis=0)\n        )\n    return collated_batch\n", "\n\ndef list2array(data_list: Dict[str, List[np.ndarray]]) -> Dict[str, np.ndarray]:\n    \"\"\"transform data from list of np.array to a single numpy array. Only needed for video datapipes.\n    Args:\n        data_np (Dict[str, List[np.ndarray]]): _description_\n    Returns:\n        Dict[str, np.ndarray]: _description_\n    \"\"\"\n    data_array: Dict[str, np.ndarray] = {}\n    for feature in data_list:\n        data_array[feature] = np.stack(data_list[feature], axis=0)\n    return data_array", "\n\ndef build_datapipes(\n    datapipe: dp.iter.IterDataPipe,\n    shuffle_buffer_size: Optional[int],\n    batch_size: int,\n    mappings: List[Callable],\n) -> dp.iter.IterDataPipe:\n    \"\"\"Iteratively apply operations to datapipe: shuffle, sharding, map, batch, collator\n\n    Args:\n        datapipe (dp.datapipe.IterDataPipe): entry datapipe\n        shuffle_buffer_size (Optional[int]): buffer size for pseudo-shuffle\n        batch_size (int):\n        mappings (List[Callable]): a list of transforms applied to datapipe, between sharding and batch\n\n    Returns:\n        dp.datapipe.IterDataPipe: transformed datapipe ready to be sent to dataloader\n    \"\"\"\n    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    if shuffle_buffer_size is not None:\n        datapipe = datapipe.shuffle(buffer_size=shuffle_buffer_size)\n    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n    datapipe = datapipe.sharding_filter()\n    for i, mapping in enumerate(mappings):\n        datapipe = datapipe.map(fn=mapping)\n    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n    # your samples will be batched more than once. You should choose one or the other.\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    datapipe = datapipe.batch(batch_size=batch_size, drop_last=True)\n    datapipe = datapipe.collate(collate_fn=collate_fn)\n    return datapipe", ""]}
{"filename": "seqrecord/robot/__init__.py", "chunked_list": [""]}
{"filename": "seqrecord/robot/seqrecord.py", "chunked_list": ["\"\"\"A package for decoding and encoding each item in data file.\"\"\"\n\nimport collections\nimport io\nimport os\nimport pickle\nfrom typing import (\n    Any,\n    BinaryIO,\n    Dict,", "    BinaryIO,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union,\n    Deque,", "    Union,\n    Deque,\n)\nimport numpy as np\nimport yaml\nfrom seqrecord.utils import WriterBuffer\nimport copy\nfrom collections import deque\nimport time\nfrom seqrecord.utils import PRODUCER_SLEEP_INTERVAL, CONSUMER_SLEEP_INTERVAL", "import time\nfrom seqrecord.utils import PRODUCER_SLEEP_INTERVAL, CONSUMER_SLEEP_INTERVAL\nimport threading\n\nMAX_RECORDFILE_SIZE = 1e9  # 1e8, 100 mb, maximum size of a single record file\n\nRSR = TypeVar(\"RSR\", bound=\"RSeqRecord\")\n\n\ndef recordfileidx2path(recorddir, recordfile_idx: int) -> str:\n    return os.path.join(recorddir, f\"records_{recordfile_idx}.bin\")", "\ndef recordfileidx2path(recorddir, recordfile_idx: int) -> str:\n    return os.path.join(recorddir, f\"records_{recordfile_idx}.bin\")\n\n\n# todo: add metadata for each episode\n\n\nclass RSeqRecord:\n    \"\"\"A serialization protocal that stores sequences of episodes into record files, while provides metadata of each episode/frame\n    to read segments from dataset.\"\"\"\n\n    def __init__(\n        self,\n        recorddir: str,\n    ) -> None:\n        # folder that stores data in a separate directory (subfolder)\n        self.recorddir: str = recorddir\n        os.makedirs(self.recorddir, exist_ok=True)\n\n        self.features_written = None\n        self.num_bytes: int = 0  # number of bytes written into current record file\n        self.file_idx: int = 0  # number of record file created for dataset\n        # track the idx endpoints for each record file, [[start_idx, end_idx]], both are inclusive\n        self.idx_range_of_files: List[Tuple[int, int]] = []\n        # file object for current record file\n        self.file_desc: Optional[BinaryIO] = None\n        # serialization proto info of each data item\n        self.metadata: Dict[int, dict] = {}\n        # index of current data item to be processed\n        self.frame_idx: int = 0\n\n        # a cache dict that stores protocal info for each (segment_len, sub features)\n        self.metadata_segment_cache: Dict[str, dict] = {}\n        self.write_buffer = WriterBuffer()\n\n    def get_recordfiles(self) -> List[str]:\n        return [self.recordfile_idx_to_path(i) for i in range(self.file_idx)]\n\n    def write_item(\n        self,\n        frame: Dict[str, np.ndarray],\n        is_seq_start: bool,\n    ) -> None:\n        \"\"\"write one item data dict(feature->np.ndarray) into bytes and write encoded bytes into\n        current record files.\n\n        Args:\n            item (Dict[str, np.ndarray]): feature to data (np.ndarray)\n            is_seq_start (bool): denote if the item is the beginning of a sequence\n        \"\"\"\n        if self.features_written is None:\n            self.features_written = [key for key in frame]\n        if is_seq_start:\n            self.seq_start()\n\n        # get file name and start position for item\n        self.metadata[self.frame_idx] = {\n            \"frame_idx\": self.frame_idx,\n            \"file_idx\": self.file_idx,\n            \"bytes_offset\": self.num_bytes,\n            \"is_seq_start\": is_seq_start,\n        }\n        num_bytes_in_frame = 0\n        for feature, data in frame.items():\n            self.metadata[self.frame_idx][feature] = {\n                \"is_none\": (\n                    data.dtype == np.dtype(\"O\") and data == None\n                ),  # this feature is essentially missing, and\n                \"dtype\": data.dtype,\n                \"shape\": data.shape,\n                \"bytes_offset\": num_bytes_in_frame,\n                \"nbytes\": data.nbytes,\n            }\n            self.write_buffer.write(data.tobytes())\n            num_bytes_in_frame += data.nbytes\n\n        self.num_bytes += num_bytes_in_frame\n        self.frame_idx += 1\n\n        return\n\n    def seq_start(self) -> None:\n        \"\"\"Notify the record that a new sequence is being written, let the record decide if we need\n        a new record file to write into.\n\n        Two cases we need to open new file:\n            1. we currently do not have record file to write into\n            2. current file size is big enough (larger than MAX_RECORDFILE_SIZE)\n        \"\"\"\n        if self.num_bytes > MAX_RECORDFILE_SIZE:\n            # current record file big enough\n            self.num_bytes = 0\n            self.file_idx += 1\n            self.idx_range_of_files[-1].append(self.frame_idx - 1)\n            self.idx_range_of_files.append([self.frame_idx])\n            self.file_desc.write(self.write_buffer.getbuffer())\n            self.file_desc.flush()\n            self.file_desc.close()\n            self.write_buffer.clear()\n            self.file_desc = open(\n                recordfileidx2path(self.recorddir, self.file_idx),\n                mode=\"wb\",\n            )\n        elif self.file_desc == None:\n            # no opened record file to write into\n            self.idx_range_of_files.append([self.frame_idx])\n            self.file_desc = open(\n                recordfileidx2path(self.recorddir, self.file_idx), mode=\"wb\"\n            )\n\n    def read_frame(\n        self,\n        file_desc: Union[io.BufferedReader, BinaryIO],\n        metadata_frame: Dict[str, Union[int, dict]],\n        features: List[str],\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n        decoded dictionary(feature->data(np.ndarray)) of the item.\n\n        Args:\n            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n\n        Returns:\n            Dict[str, np.ndarray]: data\n        \"\"\"\n        frame = {}\n        frame_offset = metadata_frame[\"bytes_offset\"]\n        for feature in features:\n            frame[feature] = np.memmap(\n                file_desc,\n                dtype=metadata_frame[feature][\"dtype\"],\n                mode=\"r\",\n                offset=frame_offset + metadata_frame[feature][\"bytes_offset\"],\n                shape=metadata_frame[feature][\"shape\"],\n            )\n        # * do we need to close the memmap?\n        return frame\n\n    def read_frame_frombuffer(\n        self,\n        file_desc: Union[io.BufferedReader, BinaryIO],\n        metadata_frame: Dict[str, Union[int, dict]],\n        features: List[str],\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n        decoded dictionary(feature->data(np.ndarray)) of the item, where decoding is done by\n        np.frombuffer()\n\n        Args:\n            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n\n        Returns:\n            Dict[str, np.ndarray]: data\n        \"\"\"\n        frame = {}\n        file_desc.seek(metadata_frame[\"bytes_offset\"])\n        for feature in features:\n            bytes = file_desc.read(metadata_frame[feature][\"nbytes\"])\n            array1d = np.frombuffer(\n                bytes,\n                dtype=metadata_frame[feature][\"dtype\"],\n            )\n            frame[feature] = array1d.reshape(metadata_frame[feature][\"shape\"])\n        return frame\n\n    def read_frames(\n        self, features: List[str]\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        \"\"\"Given that the dataset has been recored, decode the record sequentially, each time\n        returning a dict that contains the data item.\n\n        Args:\n            features [List[str]]: a list of features requested to read from item\n            shuffle_recordfile: bool: if we shuffle at the record file level when reading items in the record\n        Yields:\n            Generator[Dict[str, np.ndarray], None, None]: data item [feature->data]. All data items are being returned sequentially\n        \"\"\"\n        recordfiles = list(range(self.file_idx))\n        for i in recordfiles:\n            recordfile_path = recordfileidx2path(self.recorddir, i)\n            endpoints = self.idx_range_of_files[i]\n            with open(recordfile_path, mode=\"rb\") as f:\n                for idx in range(endpoints[0], endpoints[1] + 1):\n                    item = self.read_frame(f, self.metadata[idx], features)\n                    yield {feature: item[feature] for feature in features}\n\n    def get_metadata4segment(\n        self, segment_len: int, sub_features: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate a protocal for reading segments from records. Each data item of segment should\n        contain all features in sub_features.\n\n        Note:\n        Only call this function when record has scanned all data in dataset, and record has valid attributes: rootdir, recordfile_idx\n\n        Args:\n            segment_len (int): length of segment we are reading, 1< segment_len < sequence length\n            sub_features: (Optional[List[str]]): features (modalities data) we need for each data item in segment to contain. If it is None,\n            then we read all features.\n\n        Returns:\n            Dict[str, Any]: protocal needed for reading segments from data\n        \"\"\"\n\n        def has_sub_features(itemproto: Dict[str, Any]) -> bool:\n            return all(not itemproto[feature][\"is_none\"] for feature in sub_features)\n\n        def update_segmentproto(item_idx: int, is_segment_start: bool) -> None:\n            if is_segment_start:\n                head4segment.append(item_idx)\n            recordfile_idx = self.metadata[item_idx][\"file_idx\"]\n            file2segment_items[recordfile_idx].append((is_segment_start, item_idx))\n            return\n\n        if sub_features is None:\n            sub_features = list(self.features_written)\n        else:\n            assert all(\n                feature in self.features_written for feature in sub_features\n            ), \"Unknow features requested\"\n        cache_key = str(segment_len) + \"#\" + \"#\".join(sorted(sub_features))\n        if cache_key in self.metadata_segment_cache:\n            return self.metadata_segment_cache[cache_key]\n        head4segment: List[int] = []\n        file2segment_items: dict[int, List[Tuple[bool, int]]] = collections.defaultdict(\n            list\n        )\n        q = collections.deque()\n        q_has_seg_tail = False  # indicates if the elements currently in queue are tail of some segment\n        for idx in range(self.frame_idx):\n            itemproto = self.metadata[idx]\n            if (not has_sub_features(itemproto)) or (itemproto[\"is_seq_start\"]):\n                # new seq start\n                while q:\n                    if q_has_seg_tail:\n                        update_segmentproto(q.popleft(), is_segment_start=False)\n                    else:\n                        q.popleft()\n                q_has_seg_tail = False\n                if has_sub_features(itemproto):\n                    # a valid start of sequence\n                    q.append(idx)\n            else:\n                q.append(idx)\n                if len(q) == segment_len:\n                    # claim: elements in the queue must be from the same sequence\n                    update_segmentproto(q.popleft(), is_segment_start=True)\n                    q_has_seg_tail = True\n\n        if q and q_has_seg_tail:\n            # front element in queue is need as last element of some segment\n            update_segmentproto(q.popleft(), is_segment_start=False)\n\n        # 1. new seq (including broken) added before queue pops out\n        #       the remaining elements in queue are completely useless\n        # 2. new seq (including broken) added after queue has popped out\n        #       the remaining elements are not start of segment but are tails of some segment\n        self.metadata_segment_cache[cache_key] = {\n            \"segment_len\": segment_len,\n            \"features\": sub_features,\n            \"head4segment\": head4segment,\n            \"file2segment_items\": file2segment_items,\n        }\n        return self.metadata_segment_cache[cache_key]\n\n    def read_segments(self, segment_proto: dict):\n        \"\"\"Iterate through the whole records and return segments sequential.\n\n        Yields:\n            segment_proto: info on in given segment_len and features\n        \"\"\"\n        segment_len = segment_proto[\"segment_len\"]\n        recordfile_ids = list(segment_proto[\"file2segment_items\"].keys())\n        for recordfile_idx in recordfile_ids:\n            item_list = segment_proto[\"file2segment_items\"][recordfile_idx]\n            recordfile_path = recordfileidx2path(self.recorddir, recordfile_idx)\n            q = collections.deque()\n            with open(recordfile_path, mode=\"rb\") as f:\n                for is_segment_start, item_idx in item_list:\n                    q.append(\n                        (\n                            is_segment_start,\n                            self.read_frame(\n                                f, self.metadata[item_idx], segment_proto[\"features\"]\n                            ),\n                        )\n                    )\n                    while not q[0][0]:\n                        q.popleft()\n                    if len(q) == segment_len:\n                        yield self.collate_items(q)\n                        q.popleft()\n\n    def read_one_segment(\n        self,\n        segment_len: int,\n        head_idx: int,\n    ) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Read a segment (of lenght segment_len) starting from the item index being head_idx.\n\n        Args:\n            segment_len (int): length of segment we need to generate\n            head_idx (int): item_idx of the head of the segment to be read.\n\n        Returns:\n            Dict[str, np.ndarray]: segment data\n        \"\"\"\n        recordfile_path = recordfileidx2path(\n            self.recorddir, self.metadata[head_idx][\"recordfile_idx\"]\n        )\n        q = []\n        with open(recordfile_path, mode=\"rb\") as f:\n            for idx in range(head_idx, head_idx + segment_len):\n                q.append(\n                    (\n                        idx == head_idx,\n                        self.read_frame_frombuffer(f, self.metadata[idx]),\n                    )\n                )\n        return self.collate_items(q)\n\n    def collate_items(\n        self, q: Sequence[Tuple[bool, dict]]\n    ) -> Dict[str, List[np.ndarray]]:\n        segment = {}\n        features = q[0][1].keys()\n        for feature in features:\n            segment[feature] = [item[feature] for _, item in q]\n        return segment\n\n    def close_recordfile(self):\n        \"\"\"Close opened file descriptor!\n\n        This needs to be called when finishes scanning over the dataset.\n        \"\"\"\n        self.idx_range_of_files[-1].append(self.frame_idx - 1)\n        self.file_desc.write(self.write_buffer.getbuffer())\n        self.write_buffer.close()\n        self.write_buffer = None\n        self.file_desc.flush()\n        self.file_desc.close()\n        self.file_idx += 1\n        self.file_desc = None\n\n    def dump(self) -> None:\n        \"\"\"save attributes of instance of record into a file.\n\n        Note:\n        saving attribute dict instead of pickled class: pickling class and loading it is a mess because of\n        path issues.\n        \"\"\"\n        dic = copy.deepcopy(self.__dict__)\n        with open(os.path.join(self.recorddir, \"record.dict\"), mode=\"wb\") as f:\n            pickle.dump(dic, file=f)\n\n        # save some attributes of the seqrecord to yaml for human inspection\n        dic[\"metadata_segment_cache\"] = None\n        for key, val in dic[\"metadata\"].items():\n            for feature in dic[\"features_written\"]:\n                val[feature][\"dtype\"] = val[feature][\"dtype\"].str\n                val[feature][\"shape\"] = list(val[feature][\"shape\"])\n        with open(os.path.join(self.recorddir, \"record_dict.yaml\"), mode=\"w\") as f:\n            f.write(\"# Configs for human inspection only!\\n\")\n            f.write(yaml.dump(dic))\n\n    @classmethod\n    def load_record_from_dict(cls, recorddir: str) -> RSR:\n        \"\"\"return an instance of sequence record from file that stores attributes of record as a\n        dict (stored at path).\n\n        Args:\n            path (str): path to the file that stores dict of attributes of seqrecord\n\n        Returns:\n            SR: an instance of record\n        \"\"\"\n\n        file_path = os.path.join(recorddir, \"record.dict\")\n        with open(file_path, mode=\"rb\") as f:\n            obj_dict = pickle.load(f)\n        obj = cls(\n            recorddir=recorddir,\n        )\n        obj_dict.pop(\"recorddir\", None)\n        for key, value in obj_dict.items():\n            setattr(obj, key, value)\n        return obj", "class RSeqRecord:\n    \"\"\"A serialization protocal that stores sequences of episodes into record files, while provides metadata of each episode/frame\n    to read segments from dataset.\"\"\"\n\n    def __init__(\n        self,\n        recorddir: str,\n    ) -> None:\n        # folder that stores data in a separate directory (subfolder)\n        self.recorddir: str = recorddir\n        os.makedirs(self.recorddir, exist_ok=True)\n\n        self.features_written = None\n        self.num_bytes: int = 0  # number of bytes written into current record file\n        self.file_idx: int = 0  # number of record file created for dataset\n        # track the idx endpoints for each record file, [[start_idx, end_idx]], both are inclusive\n        self.idx_range_of_files: List[Tuple[int, int]] = []\n        # file object for current record file\n        self.file_desc: Optional[BinaryIO] = None\n        # serialization proto info of each data item\n        self.metadata: Dict[int, dict] = {}\n        # index of current data item to be processed\n        self.frame_idx: int = 0\n\n        # a cache dict that stores protocal info for each (segment_len, sub features)\n        self.metadata_segment_cache: Dict[str, dict] = {}\n        self.write_buffer = WriterBuffer()\n\n    def get_recordfiles(self) -> List[str]:\n        return [self.recordfile_idx_to_path(i) for i in range(self.file_idx)]\n\n    def write_item(\n        self,\n        frame: Dict[str, np.ndarray],\n        is_seq_start: bool,\n    ) -> None:\n        \"\"\"write one item data dict(feature->np.ndarray) into bytes and write encoded bytes into\n        current record files.\n\n        Args:\n            item (Dict[str, np.ndarray]): feature to data (np.ndarray)\n            is_seq_start (bool): denote if the item is the beginning of a sequence\n        \"\"\"\n        if self.features_written is None:\n            self.features_written = [key for key in frame]\n        if is_seq_start:\n            self.seq_start()\n\n        # get file name and start position for item\n        self.metadata[self.frame_idx] = {\n            \"frame_idx\": self.frame_idx,\n            \"file_idx\": self.file_idx,\n            \"bytes_offset\": self.num_bytes,\n            \"is_seq_start\": is_seq_start,\n        }\n        num_bytes_in_frame = 0\n        for feature, data in frame.items():\n            self.metadata[self.frame_idx][feature] = {\n                \"is_none\": (\n                    data.dtype == np.dtype(\"O\") and data == None\n                ),  # this feature is essentially missing, and\n                \"dtype\": data.dtype,\n                \"shape\": data.shape,\n                \"bytes_offset\": num_bytes_in_frame,\n                \"nbytes\": data.nbytes,\n            }\n            self.write_buffer.write(data.tobytes())\n            num_bytes_in_frame += data.nbytes\n\n        self.num_bytes += num_bytes_in_frame\n        self.frame_idx += 1\n\n        return\n\n    def seq_start(self) -> None:\n        \"\"\"Notify the record that a new sequence is being written, let the record decide if we need\n        a new record file to write into.\n\n        Two cases we need to open new file:\n            1. we currently do not have record file to write into\n            2. current file size is big enough (larger than MAX_RECORDFILE_SIZE)\n        \"\"\"\n        if self.num_bytes > MAX_RECORDFILE_SIZE:\n            # current record file big enough\n            self.num_bytes = 0\n            self.file_idx += 1\n            self.idx_range_of_files[-1].append(self.frame_idx - 1)\n            self.idx_range_of_files.append([self.frame_idx])\n            self.file_desc.write(self.write_buffer.getbuffer())\n            self.file_desc.flush()\n            self.file_desc.close()\n            self.write_buffer.clear()\n            self.file_desc = open(\n                recordfileidx2path(self.recorddir, self.file_idx),\n                mode=\"wb\",\n            )\n        elif self.file_desc == None:\n            # no opened record file to write into\n            self.idx_range_of_files.append([self.frame_idx])\n            self.file_desc = open(\n                recordfileidx2path(self.recorddir, self.file_idx), mode=\"wb\"\n            )\n\n    def read_frame(\n        self,\n        file_desc: Union[io.BufferedReader, BinaryIO],\n        metadata_frame: Dict[str, Union[int, dict]],\n        features: List[str],\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n        decoded dictionary(feature->data(np.ndarray)) of the item.\n\n        Args:\n            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n\n        Returns:\n            Dict[str, np.ndarray]: data\n        \"\"\"\n        frame = {}\n        frame_offset = metadata_frame[\"bytes_offset\"]\n        for feature in features:\n            frame[feature] = np.memmap(\n                file_desc,\n                dtype=metadata_frame[feature][\"dtype\"],\n                mode=\"r\",\n                offset=frame_offset + metadata_frame[feature][\"bytes_offset\"],\n                shape=metadata_frame[feature][\"shape\"],\n            )\n        # * do we need to close the memmap?\n        return frame\n\n    def read_frame_frombuffer(\n        self,\n        file_desc: Union[io.BufferedReader, BinaryIO],\n        metadata_frame: Dict[str, Union[int, dict]],\n        features: List[str],\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"Given record file descriptor and serialization proto of a single data item, return the\n        decoded dictionary(feature->data(np.ndarray)) of the item, where decoding is done by\n        np.frombuffer()\n\n        Args:\n            recordfile_desc (io.BufferedReader): python file object of the record file (required by numpy)\n            itemproto (Dict[str, Any]): dict that contains protocal info of a specific data item\n\n        Returns:\n            Dict[str, np.ndarray]: data\n        \"\"\"\n        frame = {}\n        file_desc.seek(metadata_frame[\"bytes_offset\"])\n        for feature in features:\n            bytes = file_desc.read(metadata_frame[feature][\"nbytes\"])\n            array1d = np.frombuffer(\n                bytes,\n                dtype=metadata_frame[feature][\"dtype\"],\n            )\n            frame[feature] = array1d.reshape(metadata_frame[feature][\"shape\"])\n        return frame\n\n    def read_frames(\n        self, features: List[str]\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        \"\"\"Given that the dataset has been recored, decode the record sequentially, each time\n        returning a dict that contains the data item.\n\n        Args:\n            features [List[str]]: a list of features requested to read from item\n            shuffle_recordfile: bool: if we shuffle at the record file level when reading items in the record\n        Yields:\n            Generator[Dict[str, np.ndarray], None, None]: data item [feature->data]. All data items are being returned sequentially\n        \"\"\"\n        recordfiles = list(range(self.file_idx))\n        for i in recordfiles:\n            recordfile_path = recordfileidx2path(self.recorddir, i)\n            endpoints = self.idx_range_of_files[i]\n            with open(recordfile_path, mode=\"rb\") as f:\n                for idx in range(endpoints[0], endpoints[1] + 1):\n                    item = self.read_frame(f, self.metadata[idx], features)\n                    yield {feature: item[feature] for feature in features}\n\n    def get_metadata4segment(\n        self, segment_len: int, sub_features: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Generate a protocal for reading segments from records. Each data item of segment should\n        contain all features in sub_features.\n\n        Note:\n        Only call this function when record has scanned all data in dataset, and record has valid attributes: rootdir, recordfile_idx\n\n        Args:\n            segment_len (int): length of segment we are reading, 1< segment_len < sequence length\n            sub_features: (Optional[List[str]]): features (modalities data) we need for each data item in segment to contain. If it is None,\n            then we read all features.\n\n        Returns:\n            Dict[str, Any]: protocal needed for reading segments from data\n        \"\"\"\n\n        def has_sub_features(itemproto: Dict[str, Any]) -> bool:\n            return all(not itemproto[feature][\"is_none\"] for feature in sub_features)\n\n        def update_segmentproto(item_idx: int, is_segment_start: bool) -> None:\n            if is_segment_start:\n                head4segment.append(item_idx)\n            recordfile_idx = self.metadata[item_idx][\"file_idx\"]\n            file2segment_items[recordfile_idx].append((is_segment_start, item_idx))\n            return\n\n        if sub_features is None:\n            sub_features = list(self.features_written)\n        else:\n            assert all(\n                feature in self.features_written for feature in sub_features\n            ), \"Unknow features requested\"\n        cache_key = str(segment_len) + \"#\" + \"#\".join(sorted(sub_features))\n        if cache_key in self.metadata_segment_cache:\n            return self.metadata_segment_cache[cache_key]\n        head4segment: List[int] = []\n        file2segment_items: dict[int, List[Tuple[bool, int]]] = collections.defaultdict(\n            list\n        )\n        q = collections.deque()\n        q_has_seg_tail = False  # indicates if the elements currently in queue are tail of some segment\n        for idx in range(self.frame_idx):\n            itemproto = self.metadata[idx]\n            if (not has_sub_features(itemproto)) or (itemproto[\"is_seq_start\"]):\n                # new seq start\n                while q:\n                    if q_has_seg_tail:\n                        update_segmentproto(q.popleft(), is_segment_start=False)\n                    else:\n                        q.popleft()\n                q_has_seg_tail = False\n                if has_sub_features(itemproto):\n                    # a valid start of sequence\n                    q.append(idx)\n            else:\n                q.append(idx)\n                if len(q) == segment_len:\n                    # claim: elements in the queue must be from the same sequence\n                    update_segmentproto(q.popleft(), is_segment_start=True)\n                    q_has_seg_tail = True\n\n        if q and q_has_seg_tail:\n            # front element in queue is need as last element of some segment\n            update_segmentproto(q.popleft(), is_segment_start=False)\n\n        # 1. new seq (including broken) added before queue pops out\n        #       the remaining elements in queue are completely useless\n        # 2. new seq (including broken) added after queue has popped out\n        #       the remaining elements are not start of segment but are tails of some segment\n        self.metadata_segment_cache[cache_key] = {\n            \"segment_len\": segment_len,\n            \"features\": sub_features,\n            \"head4segment\": head4segment,\n            \"file2segment_items\": file2segment_items,\n        }\n        return self.metadata_segment_cache[cache_key]\n\n    def read_segments(self, segment_proto: dict):\n        \"\"\"Iterate through the whole records and return segments sequential.\n\n        Yields:\n            segment_proto: info on in given segment_len and features\n        \"\"\"\n        segment_len = segment_proto[\"segment_len\"]\n        recordfile_ids = list(segment_proto[\"file2segment_items\"].keys())\n        for recordfile_idx in recordfile_ids:\n            item_list = segment_proto[\"file2segment_items\"][recordfile_idx]\n            recordfile_path = recordfileidx2path(self.recorddir, recordfile_idx)\n            q = collections.deque()\n            with open(recordfile_path, mode=\"rb\") as f:\n                for is_segment_start, item_idx in item_list:\n                    q.append(\n                        (\n                            is_segment_start,\n                            self.read_frame(\n                                f, self.metadata[item_idx], segment_proto[\"features\"]\n                            ),\n                        )\n                    )\n                    while not q[0][0]:\n                        q.popleft()\n                    if len(q) == segment_len:\n                        yield self.collate_items(q)\n                        q.popleft()\n\n    def read_one_segment(\n        self,\n        segment_len: int,\n        head_idx: int,\n    ) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Read a segment (of lenght segment_len) starting from the item index being head_idx.\n\n        Args:\n            segment_len (int): length of segment we need to generate\n            head_idx (int): item_idx of the head of the segment to be read.\n\n        Returns:\n            Dict[str, np.ndarray]: segment data\n        \"\"\"\n        recordfile_path = recordfileidx2path(\n            self.recorddir, self.metadata[head_idx][\"recordfile_idx\"]\n        )\n        q = []\n        with open(recordfile_path, mode=\"rb\") as f:\n            for idx in range(head_idx, head_idx + segment_len):\n                q.append(\n                    (\n                        idx == head_idx,\n                        self.read_frame_frombuffer(f, self.metadata[idx]),\n                    )\n                )\n        return self.collate_items(q)\n\n    def collate_items(\n        self, q: Sequence[Tuple[bool, dict]]\n    ) -> Dict[str, List[np.ndarray]]:\n        segment = {}\n        features = q[0][1].keys()\n        for feature in features:\n            segment[feature] = [item[feature] for _, item in q]\n        return segment\n\n    def close_recordfile(self):\n        \"\"\"Close opened file descriptor!\n\n        This needs to be called when finishes scanning over the dataset.\n        \"\"\"\n        self.idx_range_of_files[-1].append(self.frame_idx - 1)\n        self.file_desc.write(self.write_buffer.getbuffer())\n        self.write_buffer.close()\n        self.write_buffer = None\n        self.file_desc.flush()\n        self.file_desc.close()\n        self.file_idx += 1\n        self.file_desc = None\n\n    def dump(self) -> None:\n        \"\"\"save attributes of instance of record into a file.\n\n        Note:\n        saving attribute dict instead of pickled class: pickling class and loading it is a mess because of\n        path issues.\n        \"\"\"\n        dic = copy.deepcopy(self.__dict__)\n        with open(os.path.join(self.recorddir, \"record.dict\"), mode=\"wb\") as f:\n            pickle.dump(dic, file=f)\n\n        # save some attributes of the seqrecord to yaml for human inspection\n        dic[\"metadata_segment_cache\"] = None\n        for key, val in dic[\"metadata\"].items():\n            for feature in dic[\"features_written\"]:\n                val[feature][\"dtype\"] = val[feature][\"dtype\"].str\n                val[feature][\"shape\"] = list(val[feature][\"shape\"])\n        with open(os.path.join(self.recorddir, \"record_dict.yaml\"), mode=\"w\") as f:\n            f.write(\"# Configs for human inspection only!\\n\")\n            f.write(yaml.dump(dic))\n\n    @classmethod\n    def load_record_from_dict(cls, recorddir: str) -> RSR:\n        \"\"\"return an instance of sequence record from file that stores attributes of record as a\n        dict (stored at path).\n\n        Args:\n            path (str): path to the file that stores dict of attributes of seqrecord\n\n        Returns:\n            SR: an instance of record\n        \"\"\"\n\n        file_path = os.path.join(recorddir, \"record.dict\")\n        with open(file_path, mode=\"rb\") as f:\n            obj_dict = pickle.load(f)\n        obj = cls(\n            recorddir=recorddir,\n        )\n        obj_dict.pop(\"recorddir\", None)\n        for key, value in obj_dict.items():\n            setattr(obj, key, value)\n        return obj", ""]}
{"filename": "seqrecord/robot/tests/test_seqrecord.py", "chunked_list": ["\"\"\"Unit test for seqrecord's functionalities\"\"\"\nimport unittest\nfrom typing import Dict, List\n\nimport numpy as np\nimport numpy.testing as nptest\nfrom seqrecord.robot.seqrecord import RSeqRecord\n\n\ndef concate_list(file2segment_item: Dict[str, list]):\n    res = []\n    for key in sorted(file2segment_item):\n        res = res + file2segment_item[key]\n    return res", "\ndef concate_list(file2segment_item: Dict[str, list]):\n    res = []\n    for key in sorted(file2segment_item):\n        res = res + file2segment_item[key]\n    return res\n\n\nclass Test_RSeqRecord(unittest.TestCase):\n    def test_encode_decode(self):\n        \"\"\"Testing encode and decode of items, no segment involved.\"\"\"\n        record, dataset, features = build_simple_dataset()\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % 4 == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n        record.dump()\n        # decode dataset\n        for i, item in enumerate(record.read_frames(features=features)):\n            for feature in features:\n                nptest.assert_equal(\n                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n                )\n        loaded_record = RSeqRecord.load_record_from_dict(\"./output/seqrecord_test/\")\n\n    def test_idx4segment(self):\n        \"\"\"Having the record written (and various attributes setup), generate an index protocal for\n        specific segment len.\"\"\"\n        record, dataset, features = build_simple_dataset()\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        # segment len =2, sequence len =4, full features\n        seg_len = 2\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        self.assertEqual(len(items), 10)\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, list(range(10)))\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 1, 2, 4, 5, 6, 8]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len =4, sequence len =4, full features\n        seg_len = 4\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 8)\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 4]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len =3, sequence len =4, full feature\n        seg_len = 3\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        self.assertEqual(len(ids), 8)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 1, 4, 5]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n    def test_idx4segment_brokenfeatures(self):\n        \"\"\"Test the idx4segment with some features from dataset missing.\"\"\"\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([3, 4])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 6)\n        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(\n            segment_len=seg_len, sub_features=[\"A100\"]\n        )\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(items), 6)\n        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n\n                seg_len = 3\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in [\"A100\"]:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        metadata_segment = record.get_metadata4segment(\n            segment_len=seg_len, sub_features=[\"i5\", \"s1\"]\n        )\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 8)\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 1, 4, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in [\"i5\", \"s1\"]:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([3, 6])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 3)\n        self.assertListEqual(ids, [0, 1, 2])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([2, 6])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 0)", "class Test_RSeqRecord(unittest.TestCase):\n    def test_encode_decode(self):\n        \"\"\"Testing encode and decode of items, no segment involved.\"\"\"\n        record, dataset, features = build_simple_dataset()\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % 4 == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n        record.dump()\n        # decode dataset\n        for i, item in enumerate(record.read_frames(features=features)):\n            for feature in features:\n                nptest.assert_equal(\n                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n                )\n        loaded_record = RSeqRecord.load_record_from_dict(\"./output/seqrecord_test/\")\n\n    def test_idx4segment(self):\n        \"\"\"Having the record written (and various attributes setup), generate an index protocal for\n        specific segment len.\"\"\"\n        record, dataset, features = build_simple_dataset()\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        # segment len =2, sequence len =4, full features\n        seg_len = 2\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        self.assertEqual(len(items), 10)\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, list(range(10)))\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 1, 2, 4, 5, 6, 8]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len =4, sequence len =4, full features\n        seg_len = 4\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 8)\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 4]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len =3, sequence len =4, full feature\n        seg_len = 3\n        idx4segment = record.get_metadata4segment(segment_len=seg_len)\n        self.assertEqual(len(ids), 8)\n        items = concate_list(idx4segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if i in [0, 1, 4, 5]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = idx4segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(idx4segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n    def test_idx4segment_brokenfeatures(self):\n        \"\"\"Test the idx4segment with some features from dataset missing.\"\"\"\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([3, 4])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 6)\n        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n        for i, (is_segment_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_segment_start)\n            else:\n                self.assertFalse(is_segment_start)\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(\n            segment_len=seg_len, sub_features=[\"A100\"]\n        )\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(items), 6)\n        self.assertListEqual(ids, [0, 1, 2, 5, 6, 7])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n\n                seg_len = 3\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in [\"A100\"]:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        metadata_segment = record.get_metadata4segment(\n            segment_len=seg_len, sub_features=[\"i5\", \"s1\"]\n        )\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 8)\n        self.assertListEqual(ids, [0, 1, 2, 3, 4, 5, 6, 7])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 1, 4, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in [\"i5\", \"s1\"]:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([3, 6])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 3)\n        self.assertListEqual(ids, [0, 1, 2])\n        for i, (is_seg_start, item) in enumerate(items):\n            if item in [0, 5]:\n                self.assertTrue(is_seg_start)\n            else:\n                self.assertFalse(is_seg_start)\n        heads = metadata_segment[\"head4segment\"]\n        for i, segment in enumerate(record.read_segments(metadata_segment)):\n            for j in range(seg_len):\n                for feature in features:\n                    nptest.assert_equal(\n                        dataset[heads[i] + j][feature],\n                        segment[feature][j],\n                        err_msg=\"\",\n                        verbose=True,\n                    )\n\n        # segment len = 3, sequence len =4, break two features\n        record, dataset, features = build_broken_dataset([2, 6])\n        seq_len = 4\n        # encode dataset\n        for i, item in enumerate(dataset):\n            if i % seq_len == 0:\n                # mock start of a sequence\n                record.write_item(item, True)\n            else:\n                record.write_item(item, False)\n        record.close_recordfile()\n\n        seg_len = 3\n        metadata_segment = record.get_metadata4segment(segment_len=seg_len)\n        items = concate_list(metadata_segment[\"file2segment_items\"])\n        ids = [item_idx for _, item_idx in items]\n        self.assertEqual(len(ids), 0)", "\n\ndef build_simple_dataset():\n    \"\"\"Generate a fake dataset to test methods of Record.\n    Returns:\n        _type_: _description_\n    \"\"\"\n    rootdir = \"./output/seqrecord_test/\"\n    seq_len = 10\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    dataset = [{} for _ in range(seq_len)]\n    for i in range(seq_len):\n        for feature in features:\n            shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n            dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.ones(shape=shape)\n            )\n    record = RSeqRecord(rootdir)\n    return record, dataset, features", "\n\ndef build_broken_dataset(feature_is_none_list: List[int]):\n    \"\"\"Generate a fake dataset to test methods of SeqRecord where some features does not exist.\n    Params:\n        feature_is_none_list (List[str]): indices of data-frame that have missing features\n    Returns:\n        None\n    \"\"\"\n    rootdir = \"./output/seqrecord_test/\"\n    seq_len = 10\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    dataset = [{} for _ in range(seq_len)]\n    for i in range(seq_len):\n        for feature in features:\n            shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n            dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n            if feature != \"A100\" or (i not in feature_is_none_list):\n                dataset[i][feature] = (\n                    np.random.rand(shape[0], shape[1])\n                    if dtype == \"float32\"\n                    else np.ones(shape=shape)\n                )\n            else:\n                dataset[i][feature] = np.array(None)\n    record = RSeqRecord(rootdir)\n    return record, dataset, features", "\n\ndef build_seq_dataset():\n    \"\"\"Generate an aritificial dataset to test methods of SeqRecord, w\"\"\"\n    rootdir = \"./output/seqrecord_test/\"\n    seq_len = 10\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    dataset = [{} for _ in range(seq_len)]\n    for feature in features:\n        shape = (np.random.randint(1, 5), np.random.randint(2, 7))\n        dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n        for i in range(seq_len):\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.ones(shape=shape)\n            )\n    record = RSeqRecord(rootdir)\n    return record, dataset, features", "\n\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    unittest.main()\n"]}
{"filename": "seqrecord/robot/example/dataset_datapipes.py", "chunked_list": ["from seqrecorder.seqrecord import SeqRecord\nfrom seqrecorder.datapipes import VideoDatapipeFromSeqRecord\n\nimport numpy as np\nfrom typing import Callable, Dict, List, Optional\nimport torch.utils.data.datapipes as dp\nimport torch\n\n\ndef list2array(data_list: Dict[str, List[np.ndarray]]) -> Dict[str, np.ndarray]:\n    \"\"\"transform data from list of np.array to a single numpy array.\n    Args:\n        data_np (Dict[str, List[np.ndarray]]): _description_\n    Returns:\n        Dict[str, np.ndarray]: _description_\n    \"\"\"\n    data_array: Dict[str, np.ndarray] = {}\n    for feature in data_list:\n        data_array[feature] = np.stack(data_list[feature], axis=0)\n    return data_array", "\ndef list2array(data_list: Dict[str, List[np.ndarray]]) -> Dict[str, np.ndarray]:\n    \"\"\"transform data from list of np.array to a single numpy array.\n    Args:\n        data_np (Dict[str, List[np.ndarray]]): _description_\n    Returns:\n        Dict[str, np.ndarray]: _description_\n    \"\"\"\n    data_array: Dict[str, np.ndarray] = {}\n    for feature in data_list:\n        data_array[feature] = np.stack(data_list[feature], axis=0)\n    return data_array", "\n\ndef collate_fn(batch: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n    collated_batch: Dict[str, torch.Tensor] = {}\n    for feature in batch[0]:\n        collated_batch[feature] = torch.from_numpy(\n            np.stack([batch[i][feature] for i in range(len(batch))], axis=0)\n        )\n    return collated_batch\n", "\n\ndef build_iter_datapipe(\n    recorddir, segment_len, features, transform: Optional[Callable]\n):\n    record = SeqRecord.load_record_from_dict(recorddir)\n    datapipe = VideoDatapipeFromSeqRecord(record, segment_len, features)\n    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    datapipe = dp.iter.Shuffler(datapipe, buffer_size=1000)\n    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n    datapipe = dp.iter.ShardingFilter(datapipe)\n    datapipe = dp.iter.Mapper(datapipe, fn=list2array)\n    if transform is not None:\n        datapipe = dp.iter.Mapper(datapipe, fn=transform)\n    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n    # your samples will be batched more than once. You should choose one or the other.\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    datapipe = dp.iter.Batcher(datapipe, batch_size=2, drop_last=True)\n    datapipe = dp.iter.Collator(datapipe, collate_fn=collate_fn)\n    return datapipe", "\n\nif __name__ == \"__main__\":\n\n    recorddir = \"./output/recorddataset/\"\n    segment_len = 3\n    features = [\"image_left\", \"image_right\"]\n\n    datapipe = build_iter_datapipe(recorddir, segment_len, features, None)\n\n    for seq in datapipe:\n        print(seq.keys())", ""]}
{"filename": "seqrecord/robot/example/record_dataset.py", "chunked_list": ["\"\"\"Create a dummy dataset and transform it into SeqRecord format.\n\"\"\"\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom dataio.seqrecord import SeqRecord\n\n\ndef encode_dummy_dataset(\n    recorddir: str,\n) -> None:\n    \"\"\"create and transform an artificial dataset to SeqRecord format.\n\n    Args:\n        recorddir (str): directory where the seqrecord files will be saved\n    \"\"\"\n    # attributes of dataset\n    num_seq = 10\n    seq_len = 7\n    features = {\"image_left\": \"RGBImage\", \"image_right\": \"RGBImage\"}\n    record = SeqRecord(\n        recorddir=recorddir,\n        features=features,\n        pretransform_module_path=\"dataio.example.dataset_transform\",\n    )\n\n    for _ in tqdm(range(num_seq)):\n        for j in range(seq_len):\n            item = {\n                \"image_left\": np.random.rand(224, 224, 3),\n                \"image_right\": np.random.rand(224, 224, 3),\n            }\n            record.write_item(item, (j == 0))\n    record.close_recordfile()\n    record.dump()", "def encode_dummy_dataset(\n    recorddir: str,\n) -> None:\n    \"\"\"create and transform an artificial dataset to SeqRecord format.\n\n    Args:\n        recorddir (str): directory where the seqrecord files will be saved\n    \"\"\"\n    # attributes of dataset\n    num_seq = 10\n    seq_len = 7\n    features = {\"image_left\": \"RGBImage\", \"image_right\": \"RGBImage\"}\n    record = SeqRecord(\n        recorddir=recorddir,\n        features=features,\n        pretransform_module_path=\"dataio.example.dataset_transform\",\n    )\n\n    for _ in tqdm(range(num_seq)):\n        for j in range(seq_len):\n            item = {\n                \"image_left\": np.random.rand(224, 224, 3),\n                \"image_right\": np.random.rand(224, 224, 3),\n            }\n            record.write_item(item, (j == 0))\n    record.close_recordfile()\n    record.dump()", "\n\nif __name__ == \"__main__\":\n    recorddir = \"./output/recorddataset/\"\n    encode_dummy_dataset(recorddir)\n"]}
{"filename": "seqrecord/robot/example/dataset_transform.py", "chunked_list": ["\"\"\"Implement modality transform utilities for habitat dataset.\"\"\"\n\nfrom dataclasses import dataclass\nimport sys\nfrom typing import Any, Dict\n\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n", "from torchvision import transforms\n\n\ndef instantiate_modal_class(args: Dict[str, Any]) -> Any:\n    \"\"\"Instantiates a class [defined in this module] with the given args that contains class name\n    and init args.\n\n    Args:\n        init: Dict of the form {\"modal\":...,\"$attributes\":...}.\n    Returns:\n        The instantiated class object.\n    \"\"\"\n    args_class = getattr(\n        sys.modules[__name__], args[\"modal\"]\n    )  # get class object from this module by its class name\n    return args_class(**args[\"kwargs\"])", "\n\nclass InputTransform:\n    def __init__(self, modal_config: Dict[str, Any]) -> None:\n        self.inputs = {}\n        self.feature_map = {}\n        for key, modal in modal_config.items():\n            modal[\"kwargs\"][\"feature\"] = key\n            if modal[\"kwargs\"].get(\"feature_in_batch\", None) is not None:\n                # this feature is not being sent to model\n                self.feature_map[key] = modal[\"kwargs\"][\"feature_in_batch\"]\n            self.inputs[key] = instantiate_modal_class(modal)\n        return\n\n    def pre_transform(self, x: Dict[str, np.ndarray]):\n        \"\"\"apply pre-transform on frame data.\n\n        Args:\n            x (Dict[str, np.ndarray]): each value of the dict is one single frame data\n        \"\"\"\n        res = {}\n        for key, value in x.items():\n            # static methods can be called from instances:\n            # https://docs.python.org/3/library/functions.html#staticmethod\n            res[key] = self.inputs[key].pre_transform(value)\n        return res\n\n    def transform(\n        self, transform_type: str, x: Dict[str, np.ndarray]\n    ) -> Dict[str, torch.Tensor]:\n\n        res = {}\n        for _, input in self.inputs.items():\n            key = input.feature\n            if key not in self.feature_map:\n                # this input is not being sent to model in batch\n                continue\n\n            res[self.feature_map[key]] = self.inputs[key].transform(\n                transform_type, x[key]\n            )\n        return res", "\n\nclass RGBImage:\n    \"\"\"\n\n    Args:\n        ModalityTransform (): RGB images\n    \"\"\"\n\n    MEAN = [0.485, 0.456, 0.406]\n    STD = [0.229, 0.224, 0.225]\n\n    def __init__(self, feature: str, img_dim: int, feature_in_batch: str):\n        self.feature: str = feature\n        self.feature_in_batch: str = feature_in_batch\n        # transforms.Compose is not supported by torch jit script.\n        # see: https://github.com/pytorch/vision/blob/main/torchvision/transforms/transforms.py\n        # center crop and resize: If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\n        # normalize: essential code is tensor.sub_(mean).div_(std), so it should be fine with 'sequence' (batched) data.\n        # transforms.Compose is not supported by torch jit script.\n        self.train_trans = torch.nn.Sequential(\n            transforms.RandomResizedCrop(img_dim, scale=(0.8, 1.0)),\n            transforms.Normalize(mean=RGBImage.MEAN, std=RGBImage.STD),\n        )\n\n    @staticmethod\n    def pre_transform(x: np.ndarray) -> np.ndarray:\n        \"\"\"Transform numpy array of image (size:[h,w,c], dtype:int8) to array (size:[c, h, w],\n        dtype:float32)\n\n        Args:\n            x (np.ndarray): (size:[h,w,c], dtype:int8)\n\n        Returns:\n            np.ndarray: image (size:[c, h, w], dtype:float32)\n        \"\"\"\n        x = x.transpose(2, 0, 1)  # h, w, c -> c, h, w\n        return x\n\n    def transform(self, transform_type: str, x: np.ndarray) -> torch.Tensor:\n        \"\"\"Apply transform to batch or a single frame of images represented by torch.Tensor, not\n        pil image!\n\n        Args:\n            transform_type (str): type of transform {train|val}\n            x (np.ndarray): batch of sequence of rgb image represented by np.ndarry with shape  h, w, c\n\n        Returns:\n            np.ndarray: with shape [num_units, c, unit_len, h, w]\n        \"\"\"\n        pass", ""]}
{"filename": "seqrecord/weather/datapipes.py", "chunked_list": ["\"\"\"Iterative datapipes toread weather dataset in seqrecord format\"\"\"\n\nfrom typing import Callable, Dict, Iterator, List, Optional, Tuple\nfrom time import perf_counter\nimport numpy as np\nimport torch\nimport torchdata.datapipes as datapipe\nfrom torchdata.datapipes.iter import IterableWrapper, IterDataPipe\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom .seqrecord import WSeqRecord\n\n# todo: let collate_fn be a parameter of build_wdatapipe!\n\n\n@datapipe.functional_datapipe(\"gen_framepair\")\nclass FramePairsFromWSeqRecord(datapipe.iter.IterDataPipe):\n    \"\"\"A torch datapiple class that iteratively read frame pairs from weather dataset (encoded by WSeqRecord).\"\"\"\n\n    def __init__(\n        self,\n        source_dp: datapipe.iter.IterDataPipe,\n    ) -> None:\n        super().__init__()\n        self.source_dp = source_dp\n\n    def __iter__(self):\n        yield from WSeqRecord.iterate_framepairs_from_files(\n            self.source_dp,\n        )", "class FramePairsFromWSeqRecord(datapipe.iter.IterDataPipe):\n    \"\"\"A torch datapiple class that iteratively read frame pairs from weather dataset (encoded by WSeqRecord).\"\"\"\n\n    def __init__(\n        self,\n        source_dp: datapipe.iter.IterDataPipe,\n    ) -> None:\n        super().__init__()\n        self.source_dp = source_dp\n\n    def __iter__(self):\n        yield from WSeqRecord.iterate_framepairs_from_files(\n            self.source_dp,\n        )", "\n\n@datapipe.functional_datapipe(\"gen_fileidx\")\nclass FileidxFromWSeqRecord(datapipe.iter.IterDataPipe):\n    \"\"\"A torch datapiple class that iteratively read fileidx from weather dataset (encoded by WSeqRecord).\"\"\"\n\n    def __init__(\n        self,\n        record_dp: datapipe.iter.IterDataPipe,\n    ) -> None:\n        super().__init__()\n        self.record_dp = record_dp\n\n    def __iter__(self):\n        for record in self.record_dp:\n            for fileidx in range(record.num_files):\n                yield record, fileidx", "\n\ndef build_wdatapipe(\n    records: List[WSeqRecord],\n    file_shuffle_buffer_size: Optional[int],\n    data_shuffle_buffer_size: Optional[int],\n    batch_size: int,\n    mappings: List[Callable],\n    collate_fn: Callable,\n) -> datapipe.iter.IterDataPipe:\n    \"\"\"Iteratively apply operations to datapipe: shuffle, sharding, map, batch, collator\n\n    Args:\n        datapipe (datapipe.datapipe.IterDataPipe): entry datapipe\n        shuffle_buffer_size (Optional[int]): buffer size for pseudo-shuffle\n        batch_size (int):\n        mappings (List[Callable]): a list of transforms applied to datapipe, between sharding and batch\n\n    Returns:\n        datapipe.datapipe.IterDataPipe: transformed datapipe ready to be sent to dataloader\n    \"\"\"\n    dp = IterableWrapper(records).gen_fileidx()\n    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    if file_shuffle_buffer_size is not None:\n        dp = dp.shuffle(buffer_size=file_shuffle_buffer_size)\n    # sharding: Place ShardingFilter (datapipe.sharding_filter) as early as possible in the pipeline,\n    # especially before expensive operations such as decoding, in order to avoid repeating these expensive operations across worker/distributed processes.\n    # output will be a sequence of file_idx(s) distributed to different workers (\n    dp = dp.sharding_filter()\n    dp = dp.gen_framepair()\n    # dp = dp.shuffle(buffer_size=data_shuffle_buffer_size)\n    for i, mapping in enumerate(mappings):\n        dp = dp.map(fn=mapping)\n    # Note that if you choose to use Batcher while setting batch_size > 1 for DataLoader,\n    # your samples will be batched more than once. You should choose one or the other.\n    # https://pytorch.org/data/main/tutorial.html#working-with-dataloader\n    dp = dp.batch(batch_size=batch_size, drop_last=True)\n    dp = dp.collate(collate_fn=collate_fn)\n    return dp", ""]}
{"filename": "seqrecord/weather/__init__.py", "chunked_list": [""]}
{"filename": "seqrecord/weather/constants.py", "chunked_list": ["NAME_TO_VAR = {\n    \"2m_temperature\": \"t2m\",\n    \"10m_u_component_of_wind\": \"u10\",\n    \"10m_v_component_of_wind\": \"v10\",\n    \"mean_sea_level_pressure\": \"msl\",\n    \"surface_pressure\": \"sp\",\n    \"toa_incident_solar_radiation\": \"tisr\",\n    \"total_precipitation\": \"tp\",\n    \"land_sea_mask\": \"lsm\",\n    \"orography\": \"orography\",", "    \"land_sea_mask\": \"lsm\",\n    \"orography\": \"orography\",\n    \"lattitude\": \"lat2d\",\n    \"geopotential\": \"z\",\n    \"u_component_of_wind\": \"u\",\n    \"v_component_of_wind\": \"v\",\n    \"temperature\": \"t\",\n    \"relative_humidity\": \"r\",\n    \"specific_humidity\": \"q\",\n    \"2m_dewpoint_temperature\": \"d2m\",", "    \"specific_humidity\": \"q\",\n    \"2m_dewpoint_temperature\": \"d2m\",\n    \"total_cloud_cover\": \"tcc\",\n    \"total_column_water_vapour\": \"tcwv\",\n    \"sea_surface_temperature\": \"sst\",\n    \"skin_temperature\": \"skt\",\n}\n\nVAR_TO_NAME = {v: k for k, v in NAME_TO_VAR.items()}\nVAR_TO_NAME.update({k: k for k in NAME_TO_VAR.keys()})", "VAR_TO_NAME = {v: k for k, v in NAME_TO_VAR.items()}\nVAR_TO_NAME.update({k: k for k in NAME_TO_VAR.keys()})\n\nSINGLE_LEVEL_VARS = [\n    \"2m_temperature\",\n    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",\n    \"mean_sea_level_pressure\",\n    \"surface_pressure\",\n    \"toa_incident_solar_radiation\",", "    \"surface_pressure\",\n    \"toa_incident_solar_radiation\",\n    \"total_precipitation\",\n    \"land_sea_mask\",\n    \"orography\",\n    \"lattitude\",\n]\nPRESSURE_LEVEL_VARS = [\n    \"geopotential\",\n    \"u_component_of_wind\",", "    \"geopotential\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n    \"temperature\",\n    \"relative_humidity\",\n    \"specific_humidity\",\n]\n\nALL_LEVELS = [\n    1,", "ALL_LEVELS = [\n    1,\n    2,\n    3,\n    5,\n    7,\n    10,\n    20,\n    30,\n    50,", "    30,\n    50,\n    70,\n    100,\n    125,\n    150,\n    175,\n    200,\n    225,\n    250,", "    225,\n    250,\n    300,\n    350,\n    400,\n    450,\n    500,\n    550,\n    600,\n    650,", "    600,\n    650,\n    700,\n    750,\n    775,\n    800,\n    825,\n    850,\n    875,\n    900,", "    875,\n    900,\n    925,\n    950,\n    975,\n    1000,\n]\n\nNAME_LEVEL_TO_VAR_LEVEL = {}\n\nfor var in SINGLE_LEVEL_VARS:\n    NAME_LEVEL_TO_VAR_LEVEL[var] = NAME_TO_VAR[var]", "NAME_LEVEL_TO_VAR_LEVEL = {}\n\nfor var in SINGLE_LEVEL_VARS:\n    NAME_LEVEL_TO_VAR_LEVEL[var] = NAME_TO_VAR[var]\n\nfor var in PRESSURE_LEVEL_VARS:\n    for l in ALL_LEVELS:\n        NAME_LEVEL_TO_VAR_LEVEL[var + \"_\" + str(l)] = NAME_TO_VAR[var] + \"_\" + str(l)\n\nVAR_LEVEL_TO_NAME_LEVEL = {v: k for k, v in NAME_LEVEL_TO_VAR_LEVEL.items()}", "\nVAR_LEVEL_TO_NAME_LEVEL = {v: k for k, v in NAME_LEVEL_TO_VAR_LEVEL.items()}\n"]}
{"filename": "seqrecord/weather/seqrecord.py", "chunked_list": ["\"\"\"A package for encoding and decoding weather dataset.\"\"\"\n\nimport asyncio\nimport copy\nimport io\nimport os\nimport pickle\nimport random\nimport threading\nimport time", "import threading\nimport time\nfrom collections import deque\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    Any,\n    BinaryIO,\n    Deque,\n    Dict,", "    Deque,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Tuple,\n    TypeVar,\n    Union,\n    Iterator,\n)", "    Iterator,\n)\n\nimport aiofiles\nimport numpy as np\nimport yaml\nimport subprocess\nimport shutil\n\nfrom seqrecord.utils import (", "\nfrom seqrecord.utils import (\n    CONSUMER_SLEEP_INTERVAL,\n    PRODUCER_SLEEP_INTERVAL,\n    FileManager,\n    LRUCache,\n    TimeTracker,\n    WriterBuffer,\n)\n", ")\n\nMAX_RECORDFILE_SIZE = 2e9  # 1e9  # 1e8, 100 mb, maximum size of a single record file\nFILE_CACHE_SIZE = 10  # 10, maximum number of record files to keep in local disk\nWSR = TypeVar(\"WSR\", bound=\"WSeqRecord\")\n\n# todo: add property stuff for attributes not to be changed by user\n# todo: MISSING, some transform work, subsample etc. Need to make sure data is same as produced by existing dataset\n\n\nclass _PrefetchData:\n    def __init__(self, source_data_generator, buffer_size: int):\n        self.run_prefetcher = True\n        # python deque is thread safe for appends and pops from opposite sides.\n        # ref: https://stackoverflow.com/questions/8554153/is-this-deque-thread-safe-in-python\n        self.prefetch_buffer: Deque = deque()\n        self.buffer_size: int = buffer_size\n        self.source_data_generator = source_data_generator", "\n\nclass _PrefetchData:\n    def __init__(self, source_data_generator, buffer_size: int):\n        self.run_prefetcher = True\n        # python deque is thread safe for appends and pops from opposite sides.\n        # ref: https://stackoverflow.com/questions/8554153/is-this-deque-thread-safe-in-python\n        self.prefetch_buffer: Deque = deque()\n        self.buffer_size: int = buffer_size\n        self.source_data_generator = source_data_generator", "\n\nclass WSeqRecord:\n    \"\"\"A serialization protocal that stores a single continuous long sequence of weather data into record files, while provides metadata of each frame to enable efficient random access of frames.\"\"\"\n\n    def __init__(self, recorddir: str, local_cache_dir: Optional[str] = None) -> None:\n        \"\"\"_summary_\n\n        Args:\n            recorddir (str): directory record files is placed\n        \"\"\"\n        # folder that stores data in a separate directory (subfolder)\n        self.recorddir: str = recorddir\n        os.makedirs(self.recorddir, exist_ok=True)\n        # in case we use realtive path '~'\n        self.local_cache_dir = local_cache_dir\n        if local_cache_dir is not None:\n            self.local_cache_dir = os.path.abspath(os.path.expanduser(local_cache_dir))\n            if (\n                os.path.exists(self.local_cache_dir)\n                and len(os.listdir(self.local_cache_dir)) > 0\n            ):\n                print(\"Warning: local cache dir is not empty. Clearing it now.\")\n                shutil.rmtree(self.local_cache_dir, ignore_errors=True)\n            os.makedirs(self.local_cache_dir, exist_ok=True)\n        self.features_written = None\n\n    @staticmethod\n    def subfolder_name(rank: int, world_size: int) -> str:\n        return f\"{rank}\"\n\n    @staticmethod\n    def fileidx2name(file_idx: int) -> str:\n        return f\"record_{file_idx}.bin\"\n\n    def fileidx2path(self, file_idx: int, local_cache_dir: Optional[str] = None):\n        \"\"\"Turn absolute file idx into relative path of the corresponding record file.\n        Write to self.local_dir and move.\n\n        Args:\n            file_idx (int): _description_\n            local_cache_dir (str, optional): The directory to cache the record file. Defaults to None.\n        Returns:\n            _type_: _description_\n\n        Yields:\n            _type_: _description_\n        \"\"\"\n        dir = self.recorddir if local_cache_dir is None else local_cache_dir\n        rank_id = self.meta_file[file_idx].get(\"rank_id\", -1)\n        if rank_id == -1:\n            # there is no rank hierarachy\n            return os.path.join(dir, self.fileidx2name(file_idx))\n        else:\n            return os.path.join(\n                dir,\n                self.subfolder_name(rank_id, self.num_ranks),\n                f\"record_{self.meta_file[file_idx]['rel_file_idx']}.bin\",\n            )\n\n    def recordfile_generator(self, frame_generator: Iterator):\n        \"\"\"Ignore the complexity of rank/world size from multi-processing. This method only\n        focus on file/frame.\n\n        Args:\n            frame_generator (callable): _description_\n\n        Yields:\n            _type_: _description_\n        \"\"\"\n        try:\n            write_buffer = WriterBuffer()\n            num_bytes = 0\n            self.meta_file = {}\n            self.meta_frame = {}\n            frame_idx = 0\n            file_idx = 0\n            for frame in frame_generator:\n                if self.features_written is None:\n                    self.features_written = [key for key in frame]\n                if num_bytes == 0:\n                    # new file\n                    self.meta_file[file_idx] = {\n                        \"frame_idx_start\": frame_idx,\n                        \"relative_path\": self.fileidx2name(file_idx),\n                    }\n                    # relative path to the record file does not contain directory of the corresponding seqrecord\n                self.meta_frame[frame_idx] = {\n                    \"file_idx\": file_idx,\n                    \"bytes_offset\": num_bytes,\n                }\n                num_bytes_in_frame = 0\n                for feature, data in frame.items():\n                    self.meta_frame[frame_idx][feature] = {\n                        \"is_none\": (\n                            data.dtype == np.dtype(\"O\") and data == None\n                        ),  # this feature is essentially missing, and\n                        \"dtype\": data.dtype,\n                        \"shape\": data.shape,\n                        \"bytes_offset\": num_bytes_in_frame,\n                        \"nbytes\": data.nbytes,\n                    }\n                    write_buffer.write(data.tobytes())\n                    num_bytes_in_frame += data.nbytes\n\n                self.meta_frame[frame_idx][\"nbytes\"] = num_bytes_in_frame\n                frame_idx += 1\n                num_bytes += num_bytes_in_frame\n                if num_bytes > MAX_RECORDFILE_SIZE:\n                    # current file is big enough\n                    num_bytes = 0\n                    self.meta_file[file_idx][\"frame_idx_end\"] = frame_idx\n                    write_buffer.clear()\n                    yield (\n                        self.fileidx2path(\n                            file_idx, local_cache_dir=self.local_cache_dir\n                        ),\n                        write_buffer.getvalue(),\n                    )\n                    file_idx += 1\n\n            if (\n                file_idx in self.meta_file\n                and self.meta_file[file_idx].get(\"frame_idx_end\", None) is None\n            ):\n                # there is content left in the write_buffer\n                self.meta_file[file_idx][\"frame_idx_end\"] = frame_idx\n                yield (\n                    self.fileidx2path(file_idx, self.local_cache_dir),\n                    write_buffer.getvalue(),\n                )\n                file_idx += 1\n        finally:\n            write_buffer.close()\n            self.num_files = file_idx\n            self.num_frames = frame_idx\n\n    def put_frame(self, frame_generator: callable, prefetch_buffer_size: int = 5):\n        # should be only adding frames here\n        # two threads this function keep writing and send them to buffer\n        # a separate thread writes the buffer to files as long as the buffer is non-empty\n        try:\n            prefetch_data = _PrefetchData(\n                self.recordfile_generator(frame_generator=frame_generator),\n                prefetch_buffer_size,\n            )\n            thread = threading.Thread(\n                target=WSeqRecord.prefetch_thread_worker,\n                args=(prefetch_data,),\n                daemon=True,\n            )\n            thread.start()\n            file_cache = []\n            subprocesses = deque()\n            while prefetch_data.run_prefetcher:\n                if len(prefetch_data.prefetch_buffer) > 0:\n                    (\n                        file_path,\n                        content,\n                    ) = prefetch_data.prefetch_buffer.popleft()\n                    with open(file_path, \"wb\") as f:\n                        f.write(content)\n                    file_cache.append(file_path)\n                    if (\n                        self.local_cache_dir is not None\n                        and len(file_cache) > FILE_CACHE_SIZE\n                    ):\n                        # move record files to recorddir in the background\n                        subprocesses.append(\n                            subprocess.Popen(\n                                [\n                                    \"mv\",\n                                ]\n                                + file_cache\n                                + [f\"{self.recorddir}/\"]\n                            )\n                        )\n                        file_cache = []\n                else:\n                    # TODO: Calculate sleep interval based on previous availability speed\n                    time.sleep(CONSUMER_SLEEP_INTERVAL)\n                    # todo: check if poll() is used properly\n                    if len(subprocesses) > 0 and subprocesses[0].poll() is not None:\n                        subprocesses.popleft()\n\n        finally:\n            prefetch_data.run_prefetcher = False\n            if thread is not None:\n                thread.join()\n                thread = None\n            if self.local_cache_dir is not None and len(file_cache) > 0:\n                subprocesses.append(\n                    subprocess.Popen(\n                        [\n                            \"mv\",\n                        ]\n                        + file_cache\n                        + [f\"{self.recorddir}/\"]\n                    )\n                )\n                file_cache = []\n            for p in subprocesses:\n                p.wait()\n\n    def read_frame(\n        self,\n        file_desc: Union[io.BufferedReader, BinaryIO],\n        metadata_frame: Dict[str, Union[int, dict]],\n        features: List[str],\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"Given record file descriptor and serialization proto of a single frame, return the\n        decoded dictionary(feature->data(np.ndarray)) of the item.\n\n        Args:\n            file_desc (io.BufferedReader): python file object of the record file (required by numpy)\n            metadata_frame (Dict[str, Any]): dict that contains meta info of a specific frame\n            features (List[str]):  features requested for frame\n        Returns:\n            Dict[str, np.ndarray]: data\n        \"\"\"\n        frame = {}\n        frame_offset = metadata_frame[\"bytes_offset\"]\n        for feature in features:\n            frame[feature] = np.memmap(\n                file_desc,\n                dtype=metadata_frame[feature][\"dtype\"],\n                mode=\"r\",\n                offset=frame_offset + metadata_frame[feature][\"bytes_offset\"],\n                shape=metadata_frame[feature][\"shape\"],\n            )\n        return frame\n\n    def iterate_frames(\n        self, features: List[str]\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        \"\"\"Iterate sequentially over frames in the dataset\n\n        Args:\n            features (List[str]): a list of feature names requested from frames\n\n        Returns:\n            _type_: _description_\n\n        Yields:\n            Generator[Dict[str, np.ndarray], None, None]: generates one-frame data\n        \"\"\"\n        for file_idx in range(self.num_files):\n            file_desc = open(\n                os.path.join(self.recorddir, self.meta_file[file_idx][\"relative_path\"]),\n                mode=\"rb\",\n            )\n            for idx in range(\n                self.meta_file[file_idx][\"frame_idx_start\"],\n                self.meta_file[file_idx][\"frame_idx_end\"],\n            ):\n                frame = self.read_frame(file_desc, self.meta_frame[idx], features)\n\n                yield {feature: frame[feature] for feature in features}\n            file_desc.close()\n\n    # todo: test effect of caching on real data\n    def iterate_framepairs(\n        self,\n        input_features: List[str],\n        target_features: List[str],\n        max_pred_steps: int,\n        filedesc_cache_cap: int = 10,\n        frame_cache_cap: int = 20,\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        \"\"\"Iterate frames over the whole dataset\n\n        # todo: to think about, if we don't shuffle files, then cache based on frame idx is convenient and effective.\n        Args:\n            input_features [List[str]]: a list of features requested for input\n            target_features [List[str]]: a list of features requested for target\n            max_pred_steps [int]: maximum number of leap steps for predictive frame\n        Yields:\n            Generator[Dict[str, np.ndarray], None, None]: data item [feature->data]. All data items are being returned sequentially\n        \"\"\"\n        file_manager = FileManager(\n            cache_capacity=filedesc_cache_cap,\n        )\n        # given that, input and target features do not overlap, we only cache target frame\n        # LRU might not be suitable, evicting based on idx seems better\n        frame_cache = LRUCache(frame_cache_cap)\n\n        for fileidx4input in range(self.num_files):\n            filedesc4input = file_manager.open_file(\n                file_idx=fileidx4input,\n                file_path=os.path.join(\n                    self.recorddir, self.meta_file[fileidx4input][\"relative_path\"]\n                ),\n            )\n            endpoints = (\n                self.meta_file[fileidx4input][\"frame_idx_start\"],\n                self.meta_file[fileidx4input][\"frame_idx_end\"],\n            )\n            # no target frame to predict for the last frame\n            for frameidx4input in range(\n                endpoints[0],\n                min(endpoints[1], self.num_frames - 1),  # self.num_frames\n            ):\n                input_frame = self.read_frame(\n                    filedesc4input, self.meta_frame[frameidx4input], input_features\n                )\n                # get the target frame for prediction, both start, stop inclusive\n                lookahead_steps = min(\n                    random.randint(1, max_pred_steps),\n                    self.num_frames - 1 - frameidx4input,\n                )\n                frameidx4target = frameidx4input + lookahead_steps\n                target_frame = frame_cache.get(frameidx4target)\n                if target_frame is None:\n                    fileidx4target = self.meta_frame[frameidx4target][\"file_idx\"]\n                    filedesc4target = file_manager.open_file(\n                        file_idx=fileidx4target,\n                        file_path=os.path.join(\n                            self.recorddir,\n                            self.meta_file[fileidx4target][\"relative_path\"],\n                        ),\n                    )\n                    target_frame = self.read_frame(\n                        filedesc4target,\n                        self.meta_frame[frameidx4target],\n                        target_features,\n                    )\n                # colllate input and target frames so that input and target frame are np.ndarray\n                input_frame = np.vstack(\n                    [input_frame[feature] for feature in input_features]\n                )\n                target_frame = np.vstack(\n                    [target_frame[feature] for feature in target_features]\n                )\n                yield {\n                    \"input\": input_frame,\n                    \"target\": target_frame,\n                    \"lookahead_steps\": np.asarray(lookahead_steps),\n                    \"input_features\": input_features,\n                    \"target_features\": target_features,\n                }\n        file_manager.close_all_files()\n\n    def async_iterate_framepairs(\n        self,\n        input_features: List[str],\n        target_features: List[str],\n        max_pred_steps: int,\n        filedesc_cache_cap: int = 10,\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        \"\"\"Asyncly read two frames from (possibly) two files.\n\n        Notes:\n            No frame cache\n\n        Returns:\n            _type_: _description_\n\n        Yields:\n            _type_: _description_\n        \"\"\"\n        # setup a single event loop for async read\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n        # file_desc_cache is only used for target frame, since we are iterating for the input frame,\n        file_desc_cache = LRUCache(capacity=filedesc_cache_cap)\n        # given that, input and target features do not overlap, we only cache target frame\n        # LRU might not be suitable, evicting based on idx seems better\n\n        # read two frames using asyn io\n        # file cache should only be used for future frames, since base file desc is used continuously\n        try:\n            for fileidx4input in range(self.num_files):\n                filedesc4input = None\n                endpoints = (\n                    self.meta_file[fileidx4input][\"frame_idx_start\"],\n                    self.meta_file[fileidx4input][\"frame_idx_end\"],\n                )\n\n                # no target frame to predict for the last frame\n                for frameidx4input in range(\n                    endpoints[0], min(endpoints[1], self.num_frames - 1)\n                ):\n                    lookahead_steps = min(\n                        random.randint(1, max_pred_steps),\n                        self.num_frames - 1 - frameidx4input,\n                    )\n                    frameidx4target = frameidx4input + lookahead_steps\n                    fileidx4target = self.meta_frame[frameidx4target][\"file_idx\"]\n\n                    filedesc4target = file_desc_cache.get(fileidx4target)\n                    if filedesc4input is None and filedesc4target is None:\n                        # both files need to be opened\n                        file_descs = loop.run_until_complete(\n                            asyncio.gather(\n                                async_open_file(\n                                    fileidx4input,\n                                    None,\n                                    os.path.join(\n                                        self.recorddir,\n                                        self.meta_file[fileidx4input][\"relative_path\"],\n                                    ),\n                                ),\n                                async_open_file(\n                                    fileidx4target,\n                                    file_desc_cache,\n                                    os.path.join(\n                                        self.recorddir,\n                                        self.meta_file[fileidx4target][\"relative_path\"],\n                                    ),\n                                ),\n                            )\n                        )\n                        # order of return values are preserved. Ref: https://stackoverflow.com/questions/54668701/asyncio-gather-scheduling-order-guarantee\n                        filedesc4input, filedesc4target = file_descs[0], file_descs[1]\n                    elif filedesc4input is None:\n                        # only need files for input frame\n                        file_descs = loop.run_until_complete(\n                            async_open_file(\n                                fileidx4input,\n                                None,\n                                os.path.join(\n                                    self.recorddir,\n                                    self.meta_file[fileidx4input][\"relative_path\"],\n                                ),\n                            )\n                        )\n                        filedesc4input = file_descs\n                    elif filedesc4target is None:\n                        # only need files for target frame\n                        file_descs = loop.run_until_complete(\n                            async_open_file(\n                                fileidx4target,\n                                file_desc_cache,\n                                os.path.join(\n                                    self.recorddir,\n                                    self.meta_file[fileidx4target][\"relative_path\"],\n                                ),\n                            )\n                        )\n                        filedesc4target = file_descs\n\n                    frame_pairs = loop.run_until_complete(\n                        asyncio.gather(\n                            async_read_frame(\n                                filedesc4input,\n                                self.meta_frame[frameidx4input],\n                                input_features,\n                            ),\n                            async_read_frame(\n                                filedesc4target,\n                                self.meta_frame[frameidx4target],\n                                target_features,\n                            ),\n                        )\n                    )\n                    yield {\n                        \"input\": frame_pairs[0],\n                        \"target\": frame_pairs[1],\n                        \"lookahead_steps\": np.asarray(lookahead_steps),\n                        \"input_features\": input_features,\n                        \"target_features\": target_features,\n                    }\n                # close file descriptor for\n\n                if filedesc4input is not None:\n                    loop.run_until_complete(close_aiofile(filedesc4input))\n        finally:\n            # close open files\n            loop.run_until_complete(close_files_in_cache(file_desc_cache))\n\n            # wrap up async works\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.close()\n\n    @staticmethod\n    def prefetch_thread_worker(prefetch_data):\n        # Lazily import to prevent circular import\n        # shc: not sure what this is for?\n        from torchdata.dataloader2 import communication\n\n        itr = iter(prefetch_data.source_data_generator)\n        stop_iteration = False\n        while prefetch_data.run_prefetcher:\n            if (\n                len(prefetch_data.prefetch_buffer) < prefetch_data.buffer_size\n                and not stop_iteration\n            ):\n                try:\n                    item = next(itr)\n                    prefetch_data.prefetch_buffer.append(item)\n                except StopIteration:\n                    stop_iteration = True\n                # shc: probably not necessary for now\n                except communication.iter.InvalidStateResetRequired:\n                    stop_iteration = True\n                except communication.iter.TerminateRequired:\n                    prefetch_data.run_prefetcher = False\n            elif stop_iteration and len(prefetch_data.prefetch_buffer) == 0:\n                prefetch_data.run_prefetcher = False\n            else:  # Buffer is full, waiting for main thread to consume items\n                # TODO: Calculate sleep interval based on previous consumption speed\n                time.sleep(PRODUCER_SLEEP_INTERVAL)\n\n    def fetch_framepairs(\n        self,\n        input_features: List[str],\n        target_features: List[str],\n        max_pred_steps: int,\n        prefetch_buffer_size: int = 10,\n    ):\n\n        if prefetch_buffer_size < 1:\n            yield from self.iterate_framepairs(\n                input_features, target_features, max_pred_steps\n            )\n        else:\n            # ref: https://github.com/pytorch/data/blob/main/torchdata/datapipes/iter/util/prefetcher.py\n            # preftech using a separate thread\n            try:\n                prefetch_data = _PrefetchData(\n                    self.iterate_framepairs(\n                        input_features, target_features, max_pred_steps\n                    ),\n                    prefetch_buffer_size,\n                )\n                thread = threading.Thread(\n                    target=WSeqRecord.prefetch_thread_worker,\n                    args=(prefetch_data,),\n                    daemon=True,\n                )\n                thread.start()\n                while prefetch_data.run_prefetcher:\n                    if len(prefetch_data.prefetch_buffer) > 0:\n                        yield prefetch_data.prefetch_buffer.popleft()\n                    else:\n                        # TODO: Calculate sleep interval based on previous availability speed\n                        time.sleep(CONSUMER_SLEEP_INTERVAL)\n            finally:\n                prefetch_data.run_prefetcher = False\n                if thread is not None:\n                    thread.join()\n                    thread = None\n\n    @staticmethod\n    def iterate_framepairs_from_files(\n        fileidx_generator: Iterator[int],\n        filedesc_cache_cap: int = 10,\n        frame_cache_cap: int = 20,\n    ) -> Generator[Dict[str, np.ndarray], None, None]:\n        file_manager = FileManager(\n            cache_capacity=filedesc_cache_cap,\n        )\n        frame_cache = LRUCache(frame_cache_cap)\n\n        for record, fileidx4input in fileidx_generator:\n            filedesc4input = file_manager.open_file(\n                file_idx=fileidx4input,\n                file_path=os.path.join(\n                    record.recorddir, record.meta_file[fileidx4input][\"relative_path\"]\n                ),\n            )\n            endpoints = (\n                record.meta_file[fileidx4input][\"frame_idx_start\"],\n                record.meta_file[fileidx4input][\"frame_idx_end\"],\n            )\n            # no target frame to predict for the last frame\n            for frameidx4input in range(\n                endpoints[0],\n                min(endpoints[1], record.num_frames - 1),  # self.num_frames\n            ):\n                input_frame = record.read_frame(\n                    filedesc4input,\n                    record.meta_frame[frameidx4input],\n                    record.framereader_args[\"input_features\"],\n                )\n                # get the target frame for prediction, both start, stop inclusive\n                lookahead_steps = min(\n                    random.randint(1, record.framereader_args[\"max_pred_steps\"]),\n                    record.num_frames - 1 - frameidx4input,\n                )\n                frameidx4target = frameidx4input + lookahead_steps\n                target_frame = frame_cache.get(frameidx4target)\n                if target_frame is None:\n                    fileidx4target = record.meta_frame[frameidx4target][\"file_idx\"]\n                    filedesc4target = file_manager.open_file(\n                        fileidx4target,\n                        file_path=os.path.join(\n                            record.recorddir,\n                            record.meta_file[fileidx4target][\"relative_path\"],\n                        ),\n                    )\n                    target_frame = record.read_frame(\n                        filedesc4target,\n                        record.meta_frame[frameidx4target],\n                        record.framereader_args[\"target_features\"],\n                    )\n                # colllate input and target frames so that input and target frame are np.ndarray\n                # each feature is a two-dimensional np.ndarray\n                # output is channelxheightxwidth\n                input_frame = np.stack(\n                    [\n                        input_frame[feature]\n                        for feature in record.framereader_args[\"input_features\"]\n                    ],\n                    axis=0,\n                )\n                target_frame = np.stack(\n                    [\n                        target_frame[feature]\n                        for feature in record.framereader_args[\"target_features\"]\n                    ],\n                    axis=0,\n                )\n                # print(self.timer.summarize())\n                yield {\n                    \"input\": input_frame,\n                    \"target\": target_frame,\n                    \"lead_times\": np.asarray(\n                        lookahead_steps * record.framereader_args[\"hours_per_step\"],\n                        dtype=input_frame.dtype,\n                    ),\n                    \"meta_data\": record.framereader_args,\n                }\n        file_manager.close_all_files()\n\n    @staticmethod\n    def iterate_frames_from_file(\n        fileidx_generator: Iterator[int],\n        filedesc_cache_cap: int = 10,\n        frame_cache_cap: int = 20,\n    ):\n        \"\"\"read input/target frames from record files, where input consists of multiple consecutive frames and target frame is one future frame.\n\n        Notes:\n            # ! $lead_steps is with respect to the last frame frame in $input_frames\n            # ! Assume input target features are the same\n        Args:\n            fileidx_generator (Iterator[int]): _description_\n            filedesc_cache_cap (int, optional): _description_. Defaults to 10.\n            frame_cache_cap (int, optional): _description_. Defaults to 20.\n\n        Yields:\n            _type_: _description_\n        \"\"\"\n        file_manager = FileManager(\n            cache_capacity=filedesc_cache_cap,\n        )\n        frame_cache = LRUCache(frame_cache_cap)\n        inputframes_queue = deque()\n        for record, fileidx4input in fileidx_generator:\n            filedesc4input = file_manager.open_file(\n                file_idx=fileidx4input,\n                file_path=os.path.join(\n                    record.recorddir, record.meta_file[fileidx4input][\"relative_path\"]\n                ),\n            )\n            endpoints = (\n                record.meta_file[fileidx4input][\"frame_idx_start\"],\n                min(\n                    record.meta_file[fileidx4input][\"frame_idx_end\"],\n                    record.num_frames - 1,\n                    # no target frame left to predict for the last frame\n                ),\n            )\n            # fileidx from generator may not be consecutive\n            if len(inputframes_queue) > 0:\n                print(\"testing concatnation \", inputframes_queue[-1][0], endpoints[0])\n            if (\n                len(inputframes_queue) == 0\n                or inputframes_queue[-1][0] + 1 != endpoints[0]\n            ):\n                # consective files, the input frames queue from last file can be used\n                # do we need to worry about memory leak issues here, I guess not?\n                print(\"not concated!\")\n                inputframes_queue = deque()\n            print(f\"in the {fileidx4input}th file, {endpoints[0]} to {endpoints[1]}!\")\n            for frameidx4input in range(endpoints[0], endpoints[1]):\n                input_frame = frame_cache.get(frameidx4input)\n                if input_frame is None:\n                    input_frame = record.read_frame(\n                        filedesc4input,\n                        record.meta_frame[frameidx4input],\n                        record.framereader_args[\"input_features\"],\n                    )\n                    # no need to cache input frames since it is not likely to be reused\n                inputframes_queue.append((frameidx4input, input_frame))\n                if (\n                    len(inputframes_queue)\n                    >= record.framereader_args[\"num_frames_in_input\"]\n                ):\n                    # ready to eject input/target pairs\n                    max_pred_steps = min(\n                        record.framereader_args[\"max_pred_steps\"],\n                        record.num_frames - 1 - frameidx4input,\n                    )\n                    # both sides of randint are inclusive\n                    lookahead_steps = random.randint(1, max_pred_steps)\n                    frameidx4target = frameidx4input + lookahead_steps\n                    target_frame = frame_cache.get(frameidx4target)\n                    if target_frame is None:\n                        fileidx4target = record.meta_frame[frameidx4target][\"file_idx\"]\n                        filedesc4target = file_manager.open_file(\n                            fileidx4target,\n                            file_path=os.path.join(\n                                record.recorddir,\n                                record.meta_file[fileidx4target][\"relative_path\"],\n                            ),\n                        )\n                        target_frame = record.read_frame(\n                            filedesc4target,\n                            record.meta_frame[frameidx4target],\n                            record.framereader_args[\"target_features\"],\n                        )\n                    # colllate input and target frames so that input and target frame are np.ndarray\n                    # each feature is a two-dimensional np.ndarray\n                    # input_frames is Lis[cxhxw] with length num_frames_in_input\n                    input_frames = [\n                        (\n                            idx,\n                            np.stack(\n                                [\n                                    frame[feature]\n                                    for feature in record.framereader_args[\n                                        \"input_features\"\n                                    ]\n                                ],\n                                axis=0,\n                            ),\n                        )\n                        for idx, frame in inputframes_queue\n                    ]\n\n                    # target_frame is cxhxw\n                    target_frame = np.stack(\n                        [\n                            target_frame[feature]\n                            for feature in record.framereader_args[\"target_features\"]\n                        ],\n                        axis=0,\n                    )\n\n                    yield {\n                        \"input\": input_frames,  # List[np.ndarray with shape cxhxw]\n                        \"target\": target_frame,  # np.ndarray with shape cxhxw\n                        \"lead_time\": np.asarray(\n                            lookahead_steps * record.framereader_args[\"hours_per_step\"],\n                            dtype=input_frames[0][1].dtype,\n                        ),\n                        \"meta_data\": record.framereader_args,\n                    }\n                    inputframes_queue.popleft()\n        file_manager.close_all_files()\n\n    def dump_record(self, rank: Optional[int] = None) -> None:\n        \"\"\"save attributes of instance of record into a pickled file and yaml file for visual inspection.\n\n        Note:\n        saving attribute dict instead of pickled class: pickling class and loading it is a mess because of\n        path issues.\n        \"\"\"\n        file_name = f\"record_{rank}\" if rank is not None else \"record_all\"\n        dic = copy.deepcopy(self.__dict__)\n        # do not want to pickle a python module\n        with open(os.path.join(self.recorddir, f\"{file_name}.dict\"), mode=\"wb\") as f:\n            pickle.dump(dic, file=f)\n\n        # transform some features to make them readable in yaml\n        for _, val in dic[\"meta_frame\"].items():\n            for feature in dic[\"features_written\"]:\n                val[feature][\"dtype\"] = val[feature][\"dtype\"].str\n                val[feature][\"shape\"] = list(val[feature][\"shape\"])\n        with open(os.path.join(self.recorddir, f\"{file_name}.yaml\"), mode=\"w\") as f:\n            f.write(\"# Configs for human inspection only!\\n\")\n            f.write(yaml.dump(dic))\n\n    @classmethod\n    def load_record(cls, recorddir: str, rank: Optional[int] = None) -> WSR:\n        \"\"\"return an instance of sequence record from file that stores attributes of record as a\n        dict (stored at path).\n\n        Args:\n            path (str): path to the file that stores dict of attributes of seqrecord\n\n        Returns:\n            WSR: an instance of record\n        \"\"\"\n\n        file_path = os.path.join(\n            recorddir, \"record_all.dict\" if rank is None else f\"record_{rank}.dict\"\n        )\n        with open(file_path, mode=\"rb\") as f:\n            obj_dict = pickle.load(f)\n        obj = cls(\n            recorddir=recorddir,\n        )\n        obj_dict.pop(\"recorddir\", None)\n        for key, value in obj_dict.items():\n            setattr(obj, key, value)\n        return obj\n\n    @classmethod\n    def gather_subseqrecords(\n        cls,\n        recorddir: str,\n        world_size: int,\n        rank2folder: Optional[Dict[int, str]] = None,\n    ) -> WSR:\n        # make everything hierarchical to make it consistent\n        if rank2folder is None:\n            rank2folder = {\n                i: cls.subfolder_name(i, world_size) for i in range(world_size)\n            }\n        sub_records = []\n        for i in range(world_size):\n            sub_records.append(\n                cls.load_record(os.path.join(recorddir, rank2folder[i]), rank=i)\n            )\n\n        # combine meta data\n        features_written = sub_records[0].features_written\n\n        # meta data on each rank collected data\n        meta_rank = {}\n        meta_file = {}\n        meta_frame = {}\n        abs_file_idx = 0\n        abs_frame_idx = 0\n        for i in range(world_size):\n            meta_rank[i] = {\n                \"file_idx_start\": abs_file_idx,\n                \"file_idx_end\": abs_file_idx + sub_records[i].num_files,\n                \"frame_idx_start\": abs_frame_idx,\n            }\n            for j in range(sub_records[i].num_files):\n                meta_file[abs_file_idx] = {\n                    \"relative_path\": os.path.join(\n                        rank2folder[i],\n                        sub_records[i].meta_file[j][\"relative_path\"],\n                    ),\n                    \"frame_idx_start\": abs_frame_idx,\n                }\n                for k in range(\n                    sub_records[i].meta_file[j][\"frame_idx_start\"],\n                    sub_records[i].meta_file[j][\"frame_idx_end\"],\n                ):\n                    meta_frame[abs_frame_idx] = sub_records[i].meta_frame[k]\n                    meta_frame[abs_frame_idx][\"rel_frame_idx\"] = k\n                    meta_frame[abs_frame_idx][\"file_idx\"] = abs_file_idx\n                    abs_frame_idx += 1\n                meta_file[abs_file_idx][\"frame_idx_end\"] = abs_frame_idx\n                abs_file_idx += 1\n            meta_rank[i][\"frame_idx_end\"] = abs_frame_idx\n\n        record = cls(recorddir)\n        record.meta_file = meta_file\n        record.meta_frame = meta_frame\n        record.meta_rank = meta_rank\n        record.features_written = features_written\n\n        record.num_ranks = world_size\n        record.num_files = abs_file_idx\n        record.num_frames = abs_frame_idx\n        return record\n\n    def set_framereader_args(self, args: Dict[str, Any]) -> None:\n        self.framereader_args = args", "\n\nasync def async_open_file(\n    file_idx: int, file_desc_cache: Optional[LRUCache], file_path: str\n):\n    file_desc = await aiofiles.open(file_path, \"rb\")\n    if file_desc_cache is not None:\n        evicted = file_desc_cache.put(file_idx, file_desc)\n        if evicted is not None:\n            await evicted.close()", "    return file_desc\n\n\nasync def close_files_in_cache(file_desc_cache: LRUCache) -> None:\n    for key in file_desc_cache.keys():\n        await file_desc_cache.pop(key).close()\n    return None\n\n\nasync def close_aiofile(file_desc: io.BufferedReader) -> None:", "\nasync def close_aiofile(file_desc: io.BufferedReader) -> None:\n    await file_desc.close()\n    return\n\n\n# notes: have to use file manager (instead of an lru function since we need to close files)\n#        how to verify we are doing async?\n# other approaches to be compared with:\n#           1. read the whole frame and extract feature data (since reading small pieces of data multiple times is probably slow)", "# other approaches to be compared with:\n#           1. read the whole frame and extract feature data (since reading small pieces of data multiple times is probably slow)\n#           2. no async at all\nasync def async_read_frame(\n    file_desc: io.BufferedReader, metadata_frame: dict, features: List[str]\n) -> np.ndarray:\n    \"\"\"Given frame metadata and file object that contain frame data, read features from the frame data\n    according to features\n\n    Args:", "\n    Args:\n        file_desc (io.BufferedReader): file object that contains the frame data (file object returned by aiofiles) is a subtype of BufferedReader\n        metadata_frame (dict): _description_\n        features (List[str]): _description_\n\n    Returns:\n        np.ndarray: _description_\n    \"\"\"\n    await file_desc.seek(metadata_frame[\"bytes_offset\"])", "    \"\"\"\n    await file_desc.seek(metadata_frame[\"bytes_offset\"])\n    data_bytes = await file_desc.read(metadata_frame[\"nbytes\"])\n    frame = {}\n    # read the whole chunk or we read each file separately\n    for feature in features:\n        # b = file_desc.read(metadata[feature][\"nbytes\"])\n        # array1d = np.frombuffer(\n        #     bytes,\n        #     dtype=metadata[feature][\"dtype\"],\n        # )\n        # frame[feature] = array1d\n        # `await` halts `async_read_frame` and gives control back\n        start = metadata_frame[feature][\"bytes_offset\"]\n        end = start + metadata_frame[feature][\"nbytes\"]\n        frame[feature] = np.frombuffer(\n            data_bytes[start:end],\n            dtype=metadata_frame[feature][\"dtype\"],\n        ).reshape(metadata_frame[feature][\"shape\"])", "\n    frame_array = np.vstack([frame[feature] for feature in features])\n    return frame_array\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord_mp.py", "chunked_list": ["\"\"\"test parallel write, gather meta information, and read frame pairs\"\"\"\n\nimport unittest\nfrom typing import Dict, List\nimport random\nimport os\nimport numpy as np\nimport numpy.testing as nptest\nimport seqrecord.weather.seqrecord as wseqrecord\nfrom seqrecord.weather.seqrecord import WSeqRecord", "import seqrecord.weather.seqrecord as wseqrecord\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.weather.format.weathernp2seq_mp import distribute_loads\nfrom itertools import islice\nfrom multiprocessing import Process\n\n\nclass TestWSeqRecord(unittest.TestCase):\n    def test_read_frame(self):\n\n        print(\"Test read frame\")\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)\n        for i, item in enumerate(record.iterate_frames(features=features)):\n            for feature in features:\n                nptest.assert_equal(\n                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n                )\n\n    def test_read_frame_pair(self):\n\n        print(\"Test read frame pairs\")\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.iterate_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_fetch_frame_pair(self):\n\n        print(\"Test fetch frame pairs\")\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.fetch_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_async_iterate_frame_pair(self):\n\n        print(\"Test async iterate frame pairs\")\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.async_iterate_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_iterate_frame_pair_from_files(self):\n\n        print(\"Test iterate frame pairs from files\")\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        read_args = {\n            \"input_features\": features[:random_partition],\n            \"target_features\": features[random_partition:],\n            \"max_pred_steps\": max_pred_steps,\n        }\n        record.add_read_args(read_args)\n        fileidx_generator = ((record, i) for i in range(record.num_files))\n        # read frame pairs\n        for i, item in enumerate(\n            record.iterate_framepairs_from_files(fileidx_generator)\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_local_cache_writer(self):\n        print(\"Test iterate frame pairs from files\")\n\n        wseqrecord.MAX_RECORDFILE_SIZE = 1e8\n        dataset, rootdir, features = build_weather_dataset()\n\n        record = parallel_write(dataset, rootdir, num_processes=3)", "\n\ndef build_weather_dataset():\n    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n    rootdir = \"./output/wseqrecord_test/\"\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    time_horizon = 10000\n    dataset = [{} for _ in range(time_horizon)]\n    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n    for feature in features:\n        for i in range(time_horizon):\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.random.randint(low=0, high=255, size=shape)\n            )\n    features = [feature for feature in features]\n    return dataset, rootdir, features", "\n\ndef write_frame(rank, world_size, rootdir, frame_gen, load_amound):\n    sub_wsrecord = WSeqRecord(\n        os.path.join(rootdir, WSeqRecord.subfolder_name(rank, world_size)),\n        local_cache_dir=os.path.join(\"./output/cache/\", str(rank)),\n    )\n    sub_wsrecord.put_frame(frame_gen)\n    sub_wsrecord.dump_record(rank)\n", "\n\ndef parallel_write(dataset, rootdir, num_processes):\n\n    dividens = distribute_loads(len(dataset), num_processes)\n    processes = []\n    for i in range(num_processes):\n        sub_dataset_generator = islice(iter(dataset), dividens[i][0], dividens[i][1])\n        p = Process(\n            target=write_frame,\n            args=(\n                i,\n                num_processes,\n                rootdir,\n                sub_dataset_generator,\n                dividens[i][1] - dividens[i][0],\n            ),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(num_processes):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_processes)\n    record.dump_record()\n    return record", "\n\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord_dist.py", "chunked_list": ["\"\"\"test multiple (two) levels of parallel write, gather meta information\"\"\"\n\nimport unittest\nfrom typing import Dict, List\nimport random\nimport os\nimport numpy as np\nimport numpy.testing as nptest\nimport seqrecord.weather.seqrecord as wseqrecord\nfrom seqrecord.weather.seqrecord import WSeqRecord", "import seqrecord.weather.seqrecord as wseqrecord\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.weather.format.weathernp2seq_mp import distribute_loads\nfrom itertools import islice\nfrom multiprocessing import Process\n\n\nclass TestWSeqRecord(unittest.TestCase):\n    def test_read_frame(self):\n        print(\"Test read frame\")\n        num_nodes, num_processes = 2, 3\n        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n        for i, item in enumerate(record.iterate_frames(features=features)):\n            for feature in features:\n                nptest.assert_equal(\n                    item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n                )\n\n    def test_read_frame_pair(self):\n\n        print(\"Test read frame pairs\")\n        num_nodes, num_processes = 3, 4\n        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.iterate_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_fetch_frame_pair(self):\n\n        print(\"Test fetch frame pairs\")\n        num_nodes, num_processes = 2, 4\n        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.fetch_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_async_iterate_frame_pair(self):\n\n        print(\"Test async iterate frame pairs\")\n        num_nodes, num_processes = 3, 5\n        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n\n        input_features = features[:random_partition]\n        output_features = features[random_partition:]\n        # read frame pairs\n        for i, item in enumerate(\n            record.async_iterate_framepairs(\n                input_features, output_features, max_pred_steps=max_pred_steps\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )\n\n    def test_iterate_frame_pair_from_files(self):\n\n        print(\"Test iterate frame pairs from files\")\n        num_nodes, num_processes = 4, 7\n        record, dataset, rootdir, features = distributed_write(num_nodes, num_processes)\n\n        max_pred_steps = 100\n        random_partition = random.randint(1, len(features) - 2)\n        read_args = {\n            \"input_features\": features[:random_partition],\n            \"target_features\": features[random_partition:],\n            \"max_pred_steps\": max_pred_steps,\n        }\n        record.add_read_args(read_args)\n\n        fileidx_generator = ((record, i) for i in range(record.num_files))\n        # read frame pairs\n        for i, item in enumerate(\n            record.iterate_framepairs_from_files(\n                fileidx_generator,\n            )\n        ):\n            x, y, lookahead_steps = (\n                item[\"input\"],\n                item[\"target\"],\n                item[\"lookahead_steps\"],\n            )\n            x_target = np.vstack([dataset[i][feature] for feature in input_features])\n            y_target = np.vstack(\n                [dataset[i + lookahead_steps][feature] for feature in output_features]\n            )\n            nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n            nptest.assert_equal(\n                y,\n                y_target,\n                err_msg=\"\",\n                verbose=True,\n            )", "\n\ndef build_weather_dataset():\n    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n    rootdir = \"./output/wseqrecord_test/\"\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    time_horizon = 10000\n    dataset = [{} for _ in range(time_horizon)]\n    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n    for feature in features:\n        for i in range(time_horizon):\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.random.randint(low=0, high=255, size=shape)\n            )\n    features = [feature for feature in features]\n    return dataset, rootdir, features", "\n\ndef write_frame(rank, world_size, rootdir, frame_gen, load_amound):\n    sub_wsrecord = WSeqRecord(\n        os.path.join(rootdir, WSeqRecord.subfolder_name(rank, world_size))\n    )\n    sub_wsrecord.put_frame(frame_gen)\n    sub_wsrecord.dump_record(rank)\n\n\ndef parallel_write(dataset, node_rank, rootdir, num_processes):\n\n    dividens = distribute_loads(len(dataset), num_processes)\n    processes = []\n    for i in range(num_processes):\n        sub_dataset_generator = islice(iter(dataset), dividens[i][0], dividens[i][1])\n        p = Process(\n            target=write_frame,\n            args=(\n                i,\n                num_processes,\n                rootdir,\n                sub_dataset_generator,\n                dividens[i][1] - dividens[i][0],\n            ),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(num_processes):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_processes)\n    record.dump(node_rank)\n    return record", "\n\ndef parallel_write(dataset, node_rank, rootdir, num_processes):\n\n    dividens = distribute_loads(len(dataset), num_processes)\n    processes = []\n    for i in range(num_processes):\n        sub_dataset_generator = islice(iter(dataset), dividens[i][0], dividens[i][1])\n        p = Process(\n            target=write_frame,\n            args=(\n                i,\n                num_processes,\n                rootdir,\n                sub_dataset_generator,\n                dividens[i][1] - dividens[i][0],\n            ),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(num_processes):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_processes)\n    record.dump(node_rank)\n    return record", "\n\ndef distributed_write(num_nodes, num_processes):\n\n    complete_dataset = []\n    for i in range(num_nodes):\n        dataset, rootdir, features = build_weather_dataset()\n        parallel_write(dataset, i, os.path.join(rootdir, str(i)), num_processes)\n        complete_dataset = complete_dataset + dataset\n\n    record = WSeqRecord.gather_subseqrecords(rootdir, world_size=num_nodes)\n    record.dump()\n    return record, complete_dataset, rootdir, features", "\n\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_wseqrecord.py", "chunked_list": ["\"\"\"Unit test for Wseqrecord's correctness and speed\"\"\"\nimport unittest\nfrom typing import Dict, List\nimport random\nimport numpy as np\nimport numpy.testing as nptest\nimport seqrecord.weather.seqrecord as wseqrecord\nfrom seqrecord.weather.seqrecord import WSeqRecord\n\n\ndef test_iter_frames(file_size: int):\n    dataset, record, features = build_dataset()\n\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n    record.put_frame(frame_generator=iter(dataset))\n    record.dump_record()\n\n    # loaded_record = WSeqRecord.load_record_from_dict(\"./output/wseqrecord_test/\")\n    # decode dataset\n    for i, item in enumerate(record.iterate_frames(features=features)):\n        for feature in features:\n            nptest.assert_equal(\n                item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n            )", "\n\ndef test_iter_frames(file_size: int):\n    dataset, record, features = build_dataset()\n\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n    record.put_frame(frame_generator=iter(dataset))\n    record.dump_record()\n\n    # loaded_record = WSeqRecord.load_record_from_dict(\"./output/wseqrecord_test/\")\n    # decode dataset\n    for i, item in enumerate(record.iterate_frames(features=features)):\n        for feature in features:\n            nptest.assert_equal(\n                item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n            )", "\n\ndef test_iter_frame_pairs(file_size: int, max_pred_steps: int):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset()\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n    record.put_frame(frame_generator=iter(dataset))\n    record.dump_record()\n\n    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n    random_partition = random.randint(1, len(features) - 2)\n    input_features = features[:random_partition]\n    output_features = features[random_partition:]\n    # read frame pairs\n    for i, item in enumerate(\n        loaded_record.iterate_framepairs(\n            input_features, output_features, max_pred_steps=max_pred_steps\n        )\n    ):\n        x, y, lookahead_steps = (\n            item[\"input\"],\n            item[\"target\"],\n            item[\"lookahead_steps\"],\n        )\n        lookahead_steps_stats[lookahead_steps - 1] += 1\n        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n        y_target = np.vstack(\n            [dataset[i + lookahead_steps][feature] for feature in output_features]\n        )\n        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n        nptest.assert_equal(\n            y,\n            y_target,\n            err_msg=\"\",\n            verbose=True,\n        )\n    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]", "\n\ndef test_fetch_frame_pairs(file_size: int, max_pred_steps: int):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset()\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n\n    record.put_frame(frame_generator=iter(dataset))\n    record.dump_record()\n\n    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n    random_partition = random.randint(1, len(features) - 2)\n    input_features = features[:random_partition]\n    output_features = features[random_partition:]\n    # read frame pairs\n    for i, item in enumerate(\n        loaded_record.fetch_framepairs(\n            input_features,\n            output_features,\n            max_pred_steps=max_pred_steps,\n            prefetch_buffer_size=10,\n        )\n    ):\n        x, y, lookahead_steps = (\n            item[\"input\"],\n            item[\"target\"],\n            item[\"lookahead_steps\"],\n        )\n        lookahead_steps_stats[lookahead_steps - 1] += 1\n        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n        y_target = np.vstack(\n            [dataset[i + lookahead_steps][feature] for feature in output_features]\n        )\n        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n        nptest.assert_equal(\n            y,\n            y_target,\n            err_msg=\"\",\n            verbose=True,\n        )\n    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]", "\n\ndef test_async_read_frame_pairs(file_size: int, max_pred_steps: int):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset()\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n    record.put_frame(frame_generator=iter(dataset))\n    record.dump_record()\n\n    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n    random_partition = random.randint(1, len(features) - 2)\n    input_features = features[:random_partition]\n    output_features = features[random_partition:]\n    # read frame pairs\n    for i, item in enumerate(\n        loaded_record.async_iterate_framepairs(\n            input_features, output_features, max_pred_steps=max_pred_steps\n        )\n    ):\n        x, y, lookahead_steps = (\n            item[\"input\"],\n            item[\"target\"],\n            item[\"lookahead_steps\"],\n        )\n        lookahead_steps_stats[lookahead_steps - 1] += 1\n        x_target = np.vstack([dataset[i][feature] for feature in input_features])\n        y_target = np.vstack(\n            [dataset[i + lookahead_steps][feature] for feature in output_features]\n        )\n        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n        nptest.assert_equal(\n            y,\n            y_target,\n            err_msg=\"\",\n            verbose=True,\n        )\n    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]", "\n\ndef test_threaded_write_frame(file_size: int, max_pred_steps: int):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset()\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n    record.put_frame(iter(dataset), 5)\n    record.dump_record()\n\n    # loaded_record = WSeqRecord.load_record_from_dict(\"./output/wseqrecord_test/\")\n    # decode dataset\n    for i, item in enumerate(record.iterate_frames(features=features)):\n        for feature in features:\n            nptest.assert_equal(\n                item[feature], dataset[i][feature], err_msg=\"\", verbose=True\n            )", "\n\ndef test_read_framepairs_from_file(file_size: int, max_pred_steps: int):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset()\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n\n    record.put_frame(iter(dataset), 5)\n    record.dump_record()\n\n    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n    random_partition = random.randint(1, len(features) - 2)\n    read_args = {\n        \"input_features\": features[:random_partition],\n        \"target_features\": features[random_partition:],\n        \"max_pred_steps\": max_pred_steps,\n    }\n    loaded_record.add_read_args(read_args)\n    # read frame pairs\n    for i, item in enumerate(\n        WSeqRecord.iterate_framepairs_from_files(\n            ((loaded_record, i) for i in range(loaded_record.num_files))\n        )\n    ):\n        x, y, lookahead_steps = (\n            item[\"input\"],\n            item[\"target\"],\n            item[\"lookahead_steps\"],\n        )\n        lookahead_steps_stats[lookahead_steps - 1] += 1\n        x_target = np.vstack(\n            [dataset[i][feature] for feature in read_args[\"input_features\"]]\n        )\n        y_target = np.vstack(\n            [\n                dataset[i + lookahead_steps][feature]\n                for feature in read_args[\"target_features\"]\n            ]\n        )\n        nptest.assert_equal(x, x_target, err_msg=\"\", verbose=True)\n        nptest.assert_equal(\n            y,\n            y_target,\n            err_msg=\"\",\n            verbose=True,\n        )\n    return [d / sum(lookahead_steps_stats) for d in lookahead_steps_stats]", "\n\ndef test_read_frames_from_file(\n    num_frames_in_input: int,\n    max_pred_steps: int,\n    num_frames: int,\n    file_size: int,\n):\n    assert max_pred_steps > 1, \"maximum prediction steps need to be greater than 1\"\n    dataset, record, features = build_weather_dataset(num_frames)\n    lookahead_steps_stats = [0 for _ in range(max_pred_steps)]\n    wseqrecord.MAX_RECORDFILE_SIZE = file_size\n    # encode dataset\n\n    record.put_frame(iter(dataset), 5)\n    record.dump_record()\n\n    loaded_record = WSeqRecord.load_record(\"./output/wseqrecord_test/\")\n\n    read_args = {\n        \"input_features\": features,\n        \"target_features\": features,\n        \"max_pred_steps\": max_pred_steps,\n        \"hours_per_step\": 1,\n        \"num_frames_in_input\": num_frames_in_input,\n    }\n    loaded_record.set_framereader_args(read_args)\n    # read frame pairs\n    num_input_target_pairs = 0\n    for i, item in enumerate(\n        WSeqRecord.iterate_frames_from_file(\n            ((loaded_record, i) for i in range(loaded_record.num_files))\n        )\n    ):\n        print(f\"testing the {i}-th input-target pair\")\n        num_input_target_pairs += 1\n        x, y, lookahead_steps = (\n            item[\"input\"],\n            item[\"target\"],\n            item[\"lead_time\"],\n        )\n        x_target = [\n            np.stack(\n                [dataset[i + j][feature] for feature in read_args[\"input_features\"]],\n                axis=0,\n            )\n            for j in range(num_frames_in_input)\n        ]\n\n        for j in range(num_frames_in_input):\n            frame_idx, frame = x[j]\n            print(\n                f\"inside {i}-th input_frames, testing the {j}-th (rel) and {frame_idx}-th (abs) frame\"\n            )\n            nptest.assert_equal(\n                frame,\n                x_target[j],\n                err_msg=\"\",\n                verbose=True,\n            )\n        print(\n            f\"with {num_frames_in_input=} and {num_frames}, {lookahead_steps=} and target frame idx is {i + num_frames_in_input+ lookahead_steps-1}\"\n        )\n        y_target = np.stack(\n            [\n                dataset[i + num_frames_in_input + lookahead_steps - 1][feature]\n                for feature in read_args[\"target_features\"]\n            ]\n        )\n        nptest.assert_equal(\n            y,\n            y_target,\n            err_msg=\"\",\n            verbose=True,\n        )\n    print(\"num_input_target_pairs: \", num_input_target_pairs)\n    return num_input_target_pairs", "\n\nclass TestWSeqRecord(unittest.TestCase):\n    def test_encode_decode(self):\n        \"\"\"Testing encode and decode of frames.\"\"\"\n        print(\"testing reading frame\")\n        test_iter_frames(1e4)\n        test_iter_frames(1e6)\n        test_iter_frames(1e8)\n        test_iter_frames(1e10)\n\n    def test_read_frame_pairs(self):\n        \"\"\"Testing iteratively read frame pairs\"\"\"\n        # todo: fix feature bug and max_steps bugs\n        print(\"testing reading frame pairs\")\n        lookahead_stats = test_iter_frame_pairs(1e6, max_pred_steps=10)\n        lookahead_stats = test_iter_frame_pairs(1e8, max_pred_steps=5)\n        lookahead_stats = test_iter_frame_pairs(1e10, max_pred_steps=13)\n\n        lookahead_stats = test_iter_frame_pairs(1e6, max_pred_steps=100)\n        lookahead_stats = test_iter_frame_pairs(1e8, max_pred_steps=1000)\n        lookahead_stats = test_iter_frame_pairs(1e10, max_pred_steps=10000)\n\n    def test_fetch_frame_pairs(self):\n        \"\"\"Tesing reading frame pairs with prefetching\"\"\"\n        print(\"testing fetching frame pairs\")\n        test_fetch_frame_pairs(1e6, max_pred_steps=10)\n        test_fetch_frame_pairs(1e8, max_pred_steps=5)\n        test_fetch_frame_pairs(1e10, max_pred_steps=13)\n\n        test_fetch_frame_pairs(1e6, max_pred_steps=100)\n        test_fetch_frame_pairs(1e8, max_pred_steps=1000)\n        test_fetch_frame_pairs(1e10, max_pred_steps=10000)\n\n    def test_async_read_frame_pairs(self):\n        \"\"\"Testing reading frame pairs using asyncio\"\"\"\n\n        print(\"testing async read frame pairs\")\n        test_async_read_frame_pairs(1e6, max_pred_steps=10)\n        test_async_read_frame_pairs(1e8, max_pred_steps=5)\n        test_async_read_frame_pairs(1e10, max_pred_steps=13)\n        test_async_read_frame_pairs(1e6, max_pred_steps=100)\n        test_async_read_frame_pairs(1e8, max_pred_steps=1000)\n        test_async_read_frame_pairs(1e10, max_pred_steps=10000)\n\n    def test_single_async(self):\n\n        print(\"Single test\")\n        test_async_read_frame_pairs(1e8, max_pred_steps=1000)\n\n    def test_threaded_write(self):\n\n        test_threaded_write_frame(1e6, max_pred_steps=100)\n\n    def test_read_framepairs_from_file(self):\n        print(\"Testing reading frame files\")\n\n        test_read_framepairs_from_file(1e6, max_pred_steps=100)\n\n    def test_read_frames_from_file(self):\n        print(\"Testing reading frames from file\")\n        num_frames_in_input = 5\n        max_pred_steps = 10\n        num_frames = 15\n        num_inputtarget_pairs = test_read_frames_from_file(\n            num_frames_in_input, max_pred_steps, num_frames, 1e6\n        )\n        self.assertEqual(num_inputtarget_pairs, num_frames - num_frames_in_input)", "\n        # todo: test with inconsecutive frames\n\n\ndef build_dataset():\n    \"\"\"Generate an aritificial dataset to test methods of SeqRecord, w\"\"\"\n    rootdir = \"./output/wseqrecord_test/\"\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    time_horizon = 1000\n    dataset = [{} for _ in range(time_horizon)]\n    for feature in features:\n        shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n        dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n        for i in range(time_horizon):\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.random.randint(low=0, high=255, size=shape)\n            )\n    record = WSeqRecord(rootdir)\n    features = [feature for feature in features]\n    return dataset, record, features", "\n\ndef build_weather_dataset(time_horizon: int = 1000):\n    \"\"\"Generate an aritificial weather dataset so that each feature has the same size\"\"\"\n    rootdir = \"./output/wseqrecord_test/\"\n    features = {\"s1\": None, \"i5\": None, \"i7\": None, \"v100\": None, \"A100\": None}\n    dataset = [{} for _ in range(time_horizon)]\n    shape = (np.random.randint(1, 100), np.random.randint(1, 100))\n    dtype = np.random.choice([\"float32\", \"int\", \"bool\"])\n    for feature in features:\n        for i in range(time_horizon):\n            dataset[i][feature] = (\n                np.random.rand(shape[0], shape[1])\n                if dtype == \"float32\"\n                else np.random.randint(low=0, high=255, size=shape)\n            )\n    record = WSeqRecord(rootdir)\n    features = [feature for feature in features]\n    return dataset, record, features", "\n\nif __name__ == \"__main__\":\n    np.random.seed(0)\n    unittest.main()\n"]}
{"filename": "seqrecord/weather/test/test_utils.py", "chunked_list": ["from seqrecord.utils import distribute_loads\nimport unittest\n\n\nclass TestUtils(unittest.TestCase):\n    def test_distribute_loads(self):\n        works = 10\n        num_processes = 2\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n\n        num_processes = 3\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n\n        num_processes = 4\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n\n        works = 11\n        num_processes = 2\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n\n        works = 19\n        num_processes = 2\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n        works = 11\n        num_processes = 2\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n        works = 19\n        num_processes = 10\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n        works = 11\n        num_processes = 5\n        print(\n            f\"{works=}, {num_processes=}, the result is: {distribute_loads(works, num_processes)}\"\n        )\n\n        works = 44\n        num_processes = 10\n        res = distribute_loads(works, num_processes)\n        print(\n            f\"{works=}, {num_processes=}, the result is: {res}, with maximum load {max(r[1]-r[0] for r in res)}\"\n        )\n\n        works = 44\n        num_processes = 21\n        res = distribute_loads(works, num_processes)\n        print(\n            f\"{works=}, {num_processes=}, the result is: {res}, with maximum load {max(r[1]-r[0] for r in res)}\"\n        )", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_debug.py", "chunked_list": ["import os\nimport sys\nimport numpy as np\nimport xarray as xr\nimport psutil\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.utils import distribute_loads\nimport os\nfrom typing import List, Tuple, Iterator\nimport re", "from typing import List, Tuple, Iterator\nimport re\nimport numpy as np\nfrom tqdm import tqdm\nfrom multiprocessing import Process\nimport click\nfrom itertools import islice\nfrom seqrecord.weather.format.constants import NAME_TO_VAR\n\nsingle_vars = [", "\nsingle_vars = [\n    \"2m_temperature\",\n    # \"10m_u_component_of_wind\",\n    # \"10m_v_component_of_wind\",\n    # \"mean_sea_level_pressure\",\n    # # \"surface_pressure\",\n    # # \"2m_dewpoint_temperature\",\n    # # \"total_precipitation\",\n    # \"total_cloud_cover\",", "    # # \"total_precipitation\",\n    # \"total_cloud_cover\",\n    # \"total_column_water_vapour\",\n]\n\n\nHOURS_PER_MONTH = 744\nHOURS_PER_SHARD = 384\n\nFRAME_BUFFER_SIZE = 50", "\nFRAME_BUFFER_SIZE = 50\n\n\ndef frame_generator(rank: int, path, year, month_gen: Iterator):\n    def get_num_frames_in_month(m):\n        var = single_vars[0]\n        var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n        ds = xr.open_dataset(var_path)\n\n        return ds[NAME_TO_VAR[var]].shape[0]\n\n    for m in month_gen:\n        num_frames_in_month = get_num_frames_in_month(m)\n        print(f\"{num_frames_in_month=}\")\n        for frame_idx in range(0, num_frames_in_month, FRAME_BUFFER_SIZE):\n            np_vars = {}\n            end_frame_idx = min(frame_idx + FRAME_BUFFER_SIZE, num_frames_in_month)\n            for var in single_vars:\n                var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n                ds = xr.open_dataset(var_path)\n                code = NAME_TO_VAR[var]\n                assert len(ds[code].shape) == 3\n\n                np_vars[var] = ds[code][frame_idx:end_frame_idx].to_numpy()\n\n                del ds\n                # assert np_vars[var].shape[0] == HOURS_PER_MONTH\n            print(f\"/n RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\")\n            print(\n                f\"memory of the np array: {sum(arr.nbytes for _, arr in np_vars.items())}\"\n            )\n            for frame_idx in range(0, end_frame_idx - frame_idx):\n                frame = {}\n                for key in np_vars:\n                    frame[key] = np_vars[key][frame_idx]\n                print(\n                    f\"memory of the frame np array: {sum(arr.nbytes for _, arr in frame.items())} /n\"\n                )\n                yield frame\n                break", "\n\ndef grib2np(rank, world_size, config, year, month_gen):\n    \"\"\"\n    Convert grib files to numpy arrays and save them to disk.\n    \"\"\"\n    sub_wseqrecord = WSeqRecord(\n        os.path.join(\n            config[\"wseqrecord_dir\"],\n            WSeqRecord.subfolder_name(rank, world_size),\n        )\n    )\n\n    sub_wseqrecord.put_frame(\n        frame_generator(rank, config[\"grib_dataset_dir\"], year, month_gen)\n    )\n    sub_wseqrecord.dump_record(rank=rank)", "\n\n# notes: # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n@click.command()\n@click.option(\n    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n)\n@click.option(\"--year\", type=int, required=True)\n@click.option(\"--num-processes\", type=int, default=6)\ndef main(dataset_mount_dir: str, year: int, num_processes: int):\n    print(f\"configs {dataset_mount_dir=}, {year=}, {num_processes=}.\\n\")\n    year = 1979\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    DEFAULT_CONFIG = {\n        \"num_processes\": num_processes,\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/test/{year}\",\n        ),\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    month_generator = range(1, 13)\n    dividens = distribute_loads(12, config[\"num_processes\"])\n    processes = []\n    for i in range(config[\"num_processes\"]):\n        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n        p = Process(\n            target=grib2np,\n            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(config[\"num_processes\"]):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    print(\"Combining meta-seqrecord\")\n    record = WSeqRecord.gather_subseqrecords(\n        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    )\n\n    record.dump()", "@click.option(\"--num-processes\", type=int, default=6)\ndef main(dataset_mount_dir: str, year: int, num_processes: int):\n    print(f\"configs {dataset_mount_dir=}, {year=}, {num_processes=}.\\n\")\n    year = 1979\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    DEFAULT_CONFIG = {\n        \"num_processes\": num_processes,\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/test/{year}\",\n        ),\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    month_generator = range(1, 13)\n    dividens = distribute_loads(12, config[\"num_processes\"])\n    processes = []\n    for i in range(config[\"num_processes\"]):\n        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n        p = Process(\n            target=grib2np,\n            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(config[\"num_processes\"]):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    print(\"Combining meta-seqrecord\")\n    record = WSeqRecord.gather_subseqrecords(\n        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    )\n\n    record.dump()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "seqrecord/weather/format/weathernp2seq.py", "chunked_list": ["\"\"\"transform existing weather dataset to sequence weather format.\n\"\"\"\nfrom seqrecord.weather.seqrecord import WSeqRecord\nimport os\nfrom typing import List\nimport re\nimport numpy as np\nfrom tqdm import tqdm\n\n# test correctness of saved seqrecord format", "\n# test correctness of saved seqrecord format\n\n# dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\nDEFAULT_CONFIG = {\n    \"dataset_mount_dir\": \"/datadrive/weatherdatastorage2/datasets\",  # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    \"wsrecord_dir\": \"CMIP6/MPI-ESM/wseqrecord/test/1.40625deg_equally_np_all_levels/train/\",\n    \"wdataset_dir\": \"CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\",\n    \"wdataset_split\": \"train\",\n}", "    \"wdataset_split\": \"train\",\n}\n\n\ndef sort_weatherfiles(files: List[os.DirEntry]) -> List[os.DirEntry]:\n    \"\"\"Return sorted files in dir, make sure files are sorted incrementally in time.\n    # todo: check with Jayesh this sorting is correct\n    Example file names: 195001010600-195501010000_33.npz from CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\n\n    Args:\n        files (List[os.DirEntry]): each element in list is a file name\n    \"\"\"\n\n    def str2nums(direntry):\n        nums = re.split(\"-|_\", direntry.name.split(\".\")[0])\n        nums = tuple(map(int, nums))\n        return nums\n\n    return sorted(files, key=str2nums)", "\n\ndef main(config: dict) -> None:\n\n    wsrecord = WSeqRecord(\n        os.path.join(config[\"dataset_mount_dir\"], config[\"wsrecord_dir\"])\n    )\n\n    files_dirs = os.scandir(\n        os.path.join(config[\"dataset_mount_dir\"], config[\"wdataset_dir\"])\n    )\n    files = list(filter(lambda direntry: direntry.is_file(), files_dirs))\n    files = sort_weatherfiles(files)\n\n    def frame_generator(files):\n        i = 0\n        for file_path in tqdm(files):\n            if i >= 50:\n                break\n            data = np.load(file_path)\n            num_frames = data[data.files[0]].shape[0]\n            for rel_frame_idx in range(num_frames):\n                frame = {}\n                for key in data.files:\n                    frame[key] = data[key][rel_frame_idx]\n                yield frame\n            i += 1\n\n    wsrecord.put_frame(frame_generator(files), 5)\n    wsrecord.dump()", "\n\nif __name__ == \"__main__\":\n\n    config = DEFAULT_CONFIG\n    main(config)\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_mp.py", "chunked_list": ["import os\nimport math\nimport numpy as np\nimport xarray as xr\nimport psutil\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.utils import distribute_loads\nimport os\nfrom typing import List, Tuple, Iterator\nimport click", "from typing import List, Tuple, Iterator\nimport click\nimport numpy as np\nfrom tqdm import tqdm\nfrom multiprocessing import Process\nfrom itertools import islice\nfrom seqrecord.weather.constants import NAME_TO_VAR\n\nsingle_vars = [\n    \"2m_temperature\",", "single_vars = [\n    \"2m_temperature\",\n    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",\n    \"mean_sea_level_pressure\",\n    \"surface_pressure\",\n    \"2m_dewpoint_temperature\",\n    # \"total_precipitation\",\n    \"skin_temperature\",\n    \"sea_surface_temperature\",", "    \"skin_temperature\",\n    \"sea_surface_temperature\",\n    \"total_cloud_cover\",\n    \"total_column_water_vapour\",\n]\n\npressure_vars = [\n    \"geopotential\",\n    \"specific_humidity\",\n    \"temperature\",", "    \"specific_humidity\",\n    \"temperature\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n]\n\nLEVELS = [\n    1,\n    2,\n    3,", "    2,\n    3,\n    5,\n    7,\n    10,\n    20,\n    30,\n    50,\n    70,\n    100,", "    70,\n    100,\n    125,\n    150,\n    175,\n    200,\n    225,\n    250,\n    300,\n    350,", "    300,\n    350,\n    400,\n    450,\n    500,\n    550,\n    600,\n    650,\n    700,\n    750,", "    700,\n    750,\n    775,\n    800,\n    825,\n    850,\n    875,\n    900,\n    925,\n    950,", "    925,\n    950,\n    975,\n    1000,\n]\n#\nHOURS_PER_MONTH = 744\nHOURS_PER_SHARD = 384\n\n\ndef frame_generator(rank: int, path, year, month_gen: Iterator):\n\n    for m in month_gen:\n        ds_files = {}\n        num_frames_in_month = -1\n        for var in single_vars:\n            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n            code = NAME_TO_VAR[var]\n            ds_files[var] = xr.open_dataset(var_path)[code]\n            if num_frames_in_month == -1:\n                num_frames_in_month = ds_files[var].shape[0]\n        for var in pressure_vars:\n            for level in LEVELS:\n                var_path = os.path.join(\n                    path,\n                    f\"{year}\",\n                    f\"{m:02d}\",\n                    f\"{var}_{level}.grib\",\n                )\n                code = NAME_TO_VAR[var]\n                ds_files[f\"{var}_{level}\"] = xr.open_dataset(var_path)[code]\n\n        pbar = (\n            tqdm(\n                range(num_frames_in_month),\n                desc=f\"formating frames in month {m}\",\n                total=num_frames_in_month,\n            )\n            if rank == 0\n            else range(num_frames_in_month)\n        )\n        if rank == 0:\n            print(\n                f\"/n When rank 0 opens one month's files, the RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\"\n            )\n        for frame_idx in pbar:\n            np_vars = {}\n            for key, item in ds_files.items():\n                np_vars[key] = item[frame_idx].to_numpy()\n            yield np_vars", "\n\ndef frame_generator(rank: int, path, year, month_gen: Iterator):\n\n    for m in month_gen:\n        ds_files = {}\n        num_frames_in_month = -1\n        for var in single_vars:\n            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n            code = NAME_TO_VAR[var]\n            ds_files[var] = xr.open_dataset(var_path)[code]\n            if num_frames_in_month == -1:\n                num_frames_in_month = ds_files[var].shape[0]\n        for var in pressure_vars:\n            for level in LEVELS:\n                var_path = os.path.join(\n                    path,\n                    f\"{year}\",\n                    f\"{m:02d}\",\n                    f\"{var}_{level}.grib\",\n                )\n                code = NAME_TO_VAR[var]\n                ds_files[f\"{var}_{level}\"] = xr.open_dataset(var_path)[code]\n\n        pbar = (\n            tqdm(\n                range(num_frames_in_month),\n                desc=f\"formating frames in month {m}\",\n                total=num_frames_in_month,\n            )\n            if rank == 0\n            else range(num_frames_in_month)\n        )\n        if rank == 0:\n            print(\n                f\"/n When rank 0 opens one month's files, the RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\"\n            )\n        for frame_idx in pbar:\n            np_vars = {}\n            for key, item in ds_files.items():\n                np_vars[key] = item[frame_idx].to_numpy()\n            yield np_vars", "\n\ndef grib2np(rank, world_size, config, year, month_gen):\n    \"\"\"\n    Convert grib files to numpy arrays and save them to disk.\n    \"\"\"\n    sub_wseqrecord = WSeqRecord(\n        recorddir=os.path.join(\n            config[\"wseqrecord_dir\"],\n            WSeqRecord.subfolder_name(rank, world_size),\n        ),\n        local_cache_dir=os.path.join(\n            config[\"local_cache_dir\"],\n            WSeqRecord.subfolder_name(rank, world_size),\n        ),\n    )\n\n    sub_wseqrecord.put_frame(\n        frame_generator(rank, config[\"grib_dataset_dir\"], year, month_gen)\n    )\n    print(\"rank\", rank, \" finished, dummping metadata!\")\n    sub_wseqrecord.dump_record(rank=rank)", "\n\n@click.command()\n@click.option(\n    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n)\n@click.option(\"--local-cache-dir\", type=str, default=\"~/record_cache\")\n@click.option(\"--year\", type=int, required=True)\n@click.option(\"--num-processes\", type=int, default=12)\ndef main(dataset_mount_dir: str, local_cache_dir: str, year: int, num_processes: int):\n    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    DEFAULT_CONFIG = {\n        \"num_processes\": num_processes,\n        \"local_cache_dir\": local_cache_dir,\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/test/{year}\",\n        ),\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    month_generator = range(1, 13)\n    dividens = distribute_loads(12, config[\"num_processes\"])\n    processes = []\n    for i in range(config[\"num_processes\"]):\n        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n        p = Process(\n            target=grib2np,\n            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(config[\"num_processes\"]):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(\n        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    )\n\n    record.dump_record()", "@click.option(\"--num-processes\", type=int, default=12)\ndef main(dataset_mount_dir: str, local_cache_dir: str, year: int, num_processes: int):\n    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    DEFAULT_CONFIG = {\n        \"num_processes\": num_processes,\n        \"local_cache_dir\": local_cache_dir,\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/test/{year}\",\n        ),\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    month_generator = range(1, 13)\n    dividens = distribute_loads(12, config[\"num_processes\"])\n    processes = []\n    for i in range(config[\"num_processes\"]):\n        sub_month_generator = islice(month_generator, dividens[i][0], dividens[i][1])\n        p = Process(\n            target=grib2np,\n            args=(i, config[\"num_processes\"], config, year, sub_month_generator),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(config[\"num_processes\"]):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(\n        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    )\n\n    record.dump_record()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "seqrecord/weather/format/post_dist.py", "chunked_list": ["import os\nfrom seqrecord.weather.seqrecord import WSeqRecord\n\n\ndef main():\n    years = range(1979, 2016)\n    recorddir = \"/datadrive/azure_storage/weathereastus/era5seqrecord/aml_dist\"\n    for year in years:\n        print(f\"Gathering {year}'s data\")\n        sub_dir = os.path.join(recorddir, str(year))\n        WSeqRecord.gather_subseqrecords(sub_dir, 12).dump(rank=year - 1979)\n\n    print(\"Gathering all years' data\")\n    WSeqRecord.gather_subseqrecords(\n        recorddir,\n        len(years),\n        rank2folder={i: str(year) for i, year in enumerate(years)},\n    ).dump()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord.py", "chunked_list": ["import os\nimport math\nimport numpy as np\nimport xarray as xr\nimport psutil\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.utils import distribute_loads\nimport os\nfrom typing import List, Tuple, Iterator\nimport click", "from typing import List, Tuple, Iterator\nimport click\nimport numpy as np\nfrom tqdm import tqdm\nfrom seqrecord.weather.constants import NAME_TO_VAR\n\nsingle_vars = [\n    \"2m_temperature\",\n    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",", "    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",\n    \"mean_sea_level_pressure\",\n    \"surface_pressure\",\n    \"2m_dewpoint_temperature\",\n    # \"total_precipitation\",\n    \"skin_temperature\",\n    \"sea_surface_temperature\",\n    \"total_cloud_cover\",\n    \"total_column_water_vapour\",", "    \"total_cloud_cover\",\n    \"total_column_water_vapour\",\n]\n\npressure_vars = [\n    \"geopotential\",\n    \"specific_humidity\",\n    \"temperature\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",", "    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n]\n\nLEVELS = [\n    1,\n    2,\n    3,\n    5,\n    7,", "    5,\n    7,\n    10,\n    20,\n    30,\n    50,\n    70,\n    100,\n    125,\n    150,", "    125,\n    150,\n    175,\n    200,\n    225,\n    250,\n    300,\n    350,\n    400,\n    450,", "    400,\n    450,\n    500,\n    550,\n    600,\n    650,\n    700,\n    750,\n    775,\n    800,", "    775,\n    800,\n    825,\n    850,\n    875,\n    900,\n    925,\n    950,\n    975,\n    1000,", "    975,\n    1000,\n]\n#\nHOURS_PER_MONTH = 744\nHOURS_PER_SHARD = 384\n\n\ndef frame_generator(path, year):\n    def get_num_frames_in_month(m: int) -> int:\n        \"\"\"Read a single variable of one-month data and return the number of frames in that month\n\n        Assume:\n            within a month, every variable has the same number of frames\n\n        Args:\n            m (int): _description_\n\n        Returns:\n            int: _description_\n        \"\"\"\n        var = single_vars[0]\n        var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n        ds = xr.open_dataset(var_path)\n\n        return ds[NAME_TO_VAR[var]].shape[0]\n\n    for m in tqdm(range(1, 13)):\n        num_frames_in_month = get_num_frames_in_month(m)\n        np_vars = {}\n        for var in single_vars:\n            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n            ds = xr.open_dataset(var_path)\n            code = NAME_TO_VAR[var]\n            assert len(ds[code].shape) == 3\n\n            np_vars[var] = ds[code].to_numpy()\n\n            del ds\n            # assert np_vars[var].shape[0] == HOURS_PER_MONTH\n\n        for var in pressure_vars:\n            for level in LEVELS:\n                var_path = os.path.join(\n                    path,\n                    f\"{year}\",\n                    f\"{m:02d}\",\n                    f\"{var}_{level}.grib\",\n                )\n                ds = xr.open_dataset(var_path)\n                code = NAME_TO_VAR[var]\n                np_vars[f\"{var}_{level}\"] = ds[code].to_numpy()\n                del ds\n                # assert np_vars[f\"{var}_{level}\"].shape[0] == HOURS_PER_MONTH\n\n        for i in range(num_frames_in_month):\n            frame = {}\n            for key in np_vars:\n                frame[key] = np_vars[key][i]\n            yield frame", "def frame_generator(path, year):\n    def get_num_frames_in_month(m: int) -> int:\n        \"\"\"Read a single variable of one-month data and return the number of frames in that month\n\n        Assume:\n            within a month, every variable has the same number of frames\n\n        Args:\n            m (int): _description_\n\n        Returns:\n            int: _description_\n        \"\"\"\n        var = single_vars[0]\n        var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n        ds = xr.open_dataset(var_path)\n\n        return ds[NAME_TO_VAR[var]].shape[0]\n\n    for m in tqdm(range(1, 13)):\n        num_frames_in_month = get_num_frames_in_month(m)\n        np_vars = {}\n        for var in single_vars:\n            var_path = os.path.join(path, f\"{year}\", f\"{m:02d}\", f\"{var}_0.grib\")\n            ds = xr.open_dataset(var_path)\n            code = NAME_TO_VAR[var]\n            assert len(ds[code].shape) == 3\n\n            np_vars[var] = ds[code].to_numpy()\n\n            del ds\n            # assert np_vars[var].shape[0] == HOURS_PER_MONTH\n\n        for var in pressure_vars:\n            for level in LEVELS:\n                var_path = os.path.join(\n                    path,\n                    f\"{year}\",\n                    f\"{m:02d}\",\n                    f\"{var}_{level}.grib\",\n                )\n                ds = xr.open_dataset(var_path)\n                code = NAME_TO_VAR[var]\n                np_vars[f\"{var}_{level}\"] = ds[code].to_numpy()\n                del ds\n                # assert np_vars[f\"{var}_{level}\"].shape[0] == HOURS_PER_MONTH\n\n        for i in range(num_frames_in_month):\n            frame = {}\n            for key in np_vars:\n                frame[key] = np_vars[key][i]\n            yield frame", "\n\ndef grib2np(record, config, year):\n    \"\"\"\n    Convert grib files to numpy arrays and save them to disk.\n    \"\"\"\n    record.put_frame(frame_generator(config[\"grib_dataset_dir\"], year))\n\n\n@click.command()", "\n@click.command()\n@click.option(\n    \"--dataset-mount-dir\", type=str, default=\"/datadrive/azure_storage/weathereastus\"\n)\n@click.option(\"--year\", type=int, required=True)\ndef main(dataset_mount_dir: str, year: int):\n    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    DEFAULT_CONFIG = {\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/aml/{year}\",\n        ),\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    record = WSeqRecord(config[\"wseqrecord_dir\"])\n    grib2np(record, config, year)\n    print(\"Dumping\")\n    record.dump_record()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "seqrecord/weather/format/grib2seqrecord_dist.py", "chunked_list": ["\"\"\"Running distributed (over nodes) transformation of gribdata to seqrecord.\n\n    No join operation is performed.\n\nReturns:\n    _type_: _description_\n\nYields:\n    _type_: _description_\n\"\"\"", "    _type_: _description_\n\"\"\"\nimport sys\nimport os\nimport xarray as xr\nimport psutil\nfrom seqrecord.weather.seqrecord import WSeqRecord\nfrom seqrecord.utils import distribute_loads\nimport os\nfrom typing import List, Tuple, Iterator", "import os\nfrom typing import List, Tuple, Iterator\nfrom tqdm import tqdm\nimport torch.distributed as dist\nfrom itertools import islice\nfrom seqrecord.weather.constants import NAME_TO_VAR\n\nNUM_PROCESS_PER_NODE = 12  # make sure each node can handle processing 12 processes in the same time (months)\n\nsingle_vars = [", "\nsingle_vars = [\n    \"2m_temperature\",\n    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",\n    \"mean_sea_level_pressure\",\n    \"surface_pressure\",\n    \"2m_dewpoint_temperature\",\n    # \"total_precipitation\",\n    \"skin_temperature\",", "    # \"total_precipitation\",\n    \"skin_temperature\",\n    \"sea_surface_temperature\",\n    \"total_cloud_cover\",\n    \"total_column_water_vapour\",\n]\n\npressure_vars = [\n    \"geopotential\",\n    \"specific_humidity\",", "    \"geopotential\",\n    \"specific_humidity\",\n    \"temperature\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n]\n\nLEVELS = [\n    1,\n    2,", "    1,\n    2,\n    3,\n    5,\n    7,\n    10,\n    20,\n    30,\n    50,\n    70,", "    50,\n    70,\n    100,\n    125,\n    150,\n    175,\n    200,\n    225,\n    250,\n    300,", "    250,\n    300,\n    350,\n    400,\n    450,\n    500,\n    550,\n    600,\n    650,\n    700,", "    650,\n    700,\n    750,\n    775,\n    800,\n    825,\n    850,\n    875,\n    900,\n    925,", "    900,\n    925,\n    950,\n    975,\n    1000,\n]\n#\nHOURS_PER_MONTH = 744\nHOURS_PER_SHARD = 384\n", "HOURS_PER_SHARD = 384\n\n\ndef frame_generator(year, local_rank, path):\n    \"\"\"Process one month's data of $year.\n\n    Args:\n        year (_type_): _description_\n        local_rank (_type_): _description_\n        path (_type_): _description_\n\n    Yields:\n        _type_: _description_\n    \"\"\"\n    month = local_rank + 1  # local_rank : [0-12), month: [1, 13)\n    ds_files = {}\n    num_frames_in_month = -1\n    for var in single_vars:\n        var_path = os.path.join(path, f\"{year}\", f\"{month:02d}\", f\"{var}_0.grib\")\n        code = NAME_TO_VAR[var]\n        ds_files[var] = xr.open_dataset(var_path)[code]\n        if num_frames_in_month == -1:\n            num_frames_in_month = ds_files[var].shape[0]\n    for var in pressure_vars:\n        for level in LEVELS:\n            var_path = os.path.join(\n                path,\n                f\"{year}\",\n                f\"{month:02d}\",\n                f\"{var}_{level}.grib\",\n            )\n            code = NAME_TO_VAR[var]\n            ds_files[f\"{var}_{var}\"] = xr.open_dataset(var_path)[code]\n\n    pbar = (\n        tqdm(\n            range(num_frames_in_month),\n            desc=f\"formating frames in month {month}\",\n            total=num_frames_in_month,\n        )\n        if local_rank == 0\n        else range(num_frames_in_month)\n    )\n    if local_rank == 0:\n        print(\n            f\"/n When rank 0 opens one month's files, the RAM usage (GB): {psutil.virtual_memory()[3]/1000000000}\"\n        )\n    for frame_idx in pbar:\n        np_vars = {}\n        for key, item in ds_files.items():\n            np_vars[key] = item[frame_idx].to_numpy()\n        yield np_vars", "\n\ndef grib2np(year, local_rank, config):\n    \"\"\"\n    Convert grib files to numpy arrays and save them to disk.\n    \"\"\"\n    sub_wseqrecord = WSeqRecord(\n        os.path.join(\n            config[\"wseqrecord_dir\"],\n            f\"{year}\",\n            WSeqRecord.subfolder_name(local_rank, NUM_PROCESS_PER_NODE),\n        ),\n        local_cache_dir=os.path.join(\n            config[\"local_cache_dir\"],\n            f\"{year}\",\n            WSeqRecord.subfolder_name(local_rank, NUM_PROCESS_PER_NODE),\n        ),\n    )\n\n    sub_wseqrecord.put_frame(\n        frame_generator(year, local_rank, config[\"grib_dataset_dir\"])\n    )\n    sub_wseqrecord.dump_record(rank=local_rank)", "\n\n# def initialize():\n#     dist.init_process_group(backend=\"gloo\")\n\n\ndef main(num_nodes: int):\n    \"\"\"Each node formats several years' data (one year at a time). Within each node, spawns 12 processes where\n    each process takes care of one month's data.\n\n    Args:\n        dataset_mount_dir (str): _description_\n        num_nodes (int): _description_\n    \"\"\"\n    # initialize()\n    # dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\n    # \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\n    dataset_mount_dir = \"/mnt\"\n    DEFAULT_CONFIG = {\n        \"wseqrecord_dir\": os.path.join(\n            dataset_mount_dir,\n            f\"era5seqrecord/aml_dist/\",\n        ),\n        \"local_cache_dir\": \"~/record_cache\",\n        \"grib_dataset_dir\": os.path.join(dataset_mount_dir, \"era5\"),\n    }\n\n    config = DEFAULT_CONFIG\n\n    source_generator = range(1979, 2016)  # full-dataset: 1979-2023\n    dividens = distribute_loads(len(source_generator), num_nodes)\n    # each node processes one year's data. torchrun will spawn 12 processes within each node, and each process (identified by local_rank) processes one month's data.\n    # ref: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu\n    node_rank = int(os.environ[\"NODE_RANK\"])\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n\n    sub_datasource_generator = islice(\n        source_generator, dividens[node_rank][0], dividens[node_rank][1]\n    )\n    for year in sub_datasource_generator:\n        print(\n            f\"Transforming {year}'s data on the {node_rank}-th node with local rank {local_rank}\"\n        )\n        grib2np(\n            year,\n            local_rank,  # local_rank: range(0, 12) corresponds month range(1, 13)\n            config,\n        )\n    print(\n        f\"Transforming {year}'s data on the {node_rank}-th node with local rank {local_rank} is done.\"\n    )", "    # # combine sub-seqrecord\n    # record = WSeqRecord.gather_subseqrecords(\n    #     config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    # )\n\n    # record.dump()\n\n\nif __name__ == \"__main__\":\n    num_nodes = int(sys.argv[1])\n    main(num_nodes)", "if __name__ == \"__main__\":\n    num_nodes = int(sys.argv[1])\n    main(num_nodes)\n"]}
{"filename": "seqrecord/weather/format/validation.py", "chunked_list": ["from seqrecord import WSeqRecord, build_wdatapipe\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchdata.dataloader2 import (\n    DataLoader2,\n    DistributedReadingService,\n    MultiProcessingReadingService,\n    SequentialReadingService,\n)", "    SequentialReadingService,\n)\nfrom time import perf_counter\nimport click\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\nimport sys\n\n", "\n\nMASTER_ADDR = \"127.0.0.1\"\nWORLD_SIZE = 2\nimport socket\n\n\ndef _get_open_port():\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((\"\", 0))\n    port = s.getsockname()[1]\n    s.close()\n    return str(port)", "\n\nos.environ[\"MASTER_ADDR\"] = MASTER_ADDR\nos.environ[\"MASTER_PORT\"] = _get_open_port()\n\n\n# constants\nVAR = [\n    \"2m_temperature\",\n    \"10m_u_component_of_wind\",", "    \"2m_temperature\",\n    \"10m_u_component_of_wind\",\n    \"10m_v_component_of_wind\",\n    \"mean_sea_level_pressure\",\n    \"total_cloud_cover\",\n    \"total_column_water_vapour\",\n    \"geopotential_1\",\n    \"geopotential_2\",\n    \"geopotential_3\",\n    \"geopotential_5\",", "    \"geopotential_3\",\n    \"geopotential_5\",\n    \"geopotential_7\",\n    \"geopotential_10\",\n    \"geopotential_20\",\n    \"geopotential_30\",\n    \"geopotential_70\",\n    \"geopotential_125\",\n    \"geopotential_175\",\n    \"geopotential_225\",", "    \"geopotential_175\",\n    \"geopotential_225\",\n    \"geopotential_350\",\n    \"geopotential_450\",\n    \"geopotential_550\",\n    \"geopotential_650\",\n    \"geopotential_750\",\n    \"geopotential_775\",\n    \"geopotential_800\",\n    \"geopotential_825\",", "    \"geopotential_800\",\n    \"geopotential_825\",\n    \"geopotential_875\",\n    \"geopotential_900\",\n    \"geopotential_950\",\n    \"geopotential_975\",\n    \"specific_humidity_1\",\n    \"specific_humidity_2\",\n    \"specific_humidity_3\",\n    \"specific_humidity_5\",", "    \"specific_humidity_3\",\n    \"specific_humidity_5\",\n    \"specific_humidity_7\",\n    \"specific_humidity_10\",\n    \"specific_humidity_20\",\n    \"specific_humidity_30\",\n    \"specific_humidity_70\",\n    \"specific_humidity_125\",\n    \"specific_humidity_175\",\n    \"specific_humidity_225\",", "    \"specific_humidity_175\",\n    \"specific_humidity_225\",\n    \"specific_humidity_350\",\n    \"specific_humidity_450\",\n    \"specific_humidity_550\",\n    \"specific_humidity_650\",\n    \"specific_humidity_750\",\n    \"specific_humidity_775\",\n    \"specific_humidity_800\",\n    \"specific_humidity_825\",", "    \"specific_humidity_800\",\n    \"specific_humidity_825\",\n    \"specific_humidity_875\",\n    \"specific_humidity_900\",\n    \"specific_humidity_950\",\n    \"specific_humidity_975\",\n    \"temperature_1\",\n    \"temperature_2\",\n    \"temperature_3\",\n    \"temperature_5\",", "    \"temperature_3\",\n    \"temperature_5\",\n    \"temperature_7\",\n    \"temperature_10\",\n    \"temperature_20\",\n    \"temperature_30\",\n    \"temperature_70\",\n    \"temperature_125\",\n    \"temperature_175\",\n    \"temperature_225\",", "    \"temperature_175\",\n    \"temperature_225\",\n    \"temperature_350\",\n    \"temperature_450\",\n    \"temperature_550\",\n    \"temperature_650\",\n    \"temperature_750\",\n    \"temperature_775\",\n    \"temperature_800\",\n    \"temperature_825\",", "    \"temperature_800\",\n    \"temperature_825\",\n    \"temperature_875\",\n    \"temperature_900\",\n    \"temperature_950\",\n    \"temperature_975\",\n    \"u_component_of_wind_1\",\n    \"u_component_of_wind_2\",\n    \"u_component_of_wind_3\",\n    \"u_component_of_wind_5\",", "    \"u_component_of_wind_3\",\n    \"u_component_of_wind_5\",\n    \"u_component_of_wind_7\",\n    \"u_component_of_wind_10\",\n    \"u_component_of_wind_20\",\n    \"u_component_of_wind_30\",\n    \"u_component_of_wind_70\",\n    \"u_component_of_wind_125\",\n    \"u_component_of_wind_175\",\n    \"u_component_of_wind_225\",", "    \"u_component_of_wind_175\",\n    \"u_component_of_wind_225\",\n    \"u_component_of_wind_350\",\n    \"u_component_of_wind_450\",\n    \"u_component_of_wind_550\",\n    \"u_component_of_wind_650\",\n    \"u_component_of_wind_750\",\n    \"u_component_of_wind_775\",\n    \"u_component_of_wind_800\",\n    \"u_component_of_wind_825\",", "    \"u_component_of_wind_800\",\n    \"u_component_of_wind_825\",\n    \"u_component_of_wind_875\",\n    \"u_component_of_wind_900\",\n    \"u_component_of_wind_950\",\n    \"u_component_of_wind_975\",\n    \"v_component_of_wind_1\",\n    \"v_component_of_wind_2\",\n    \"v_component_of_wind_3\",\n    \"v_component_of_wind_5\",", "    \"v_component_of_wind_3\",\n    \"v_component_of_wind_5\",\n    \"v_component_of_wind_7\",\n    \"v_component_of_wind_10\",\n    \"v_component_of_wind_20\",\n    \"v_component_of_wind_30\",\n    \"v_component_of_wind_70\",\n    \"v_component_of_wind_125\",\n    \"v_component_of_wind_175\",\n    \"v_component_of_wind_225\",", "    \"v_component_of_wind_175\",\n    \"v_component_of_wind_225\",\n    \"v_component_of_wind_350\",\n    \"v_component_of_wind_450\",\n    \"v_component_of_wind_550\",\n    \"v_component_of_wind_650\",\n    \"v_component_of_wind_750\",\n    \"v_component_of_wind_775\",\n    \"v_component_of_wind_800\",\n    \"v_component_of_wind_825\",", "    \"v_component_of_wind_800\",\n    \"v_component_of_wind_825\",\n    \"v_component_of_wind_875\",\n    \"v_component_of_wind_900\",\n    \"v_component_of_wind_950\",\n    \"v_component_of_wind_975\",\n]\nVAR4TEST = [\n    \"2m_temperature\",\n    \"geopotential_1\",", "    \"2m_temperature\",\n    \"geopotential_1\",\n]\n\n\ndef identity(x):\n    return x\n\n\ndef mp_loader(dp):\n    rs = MultiProcessingReadingService(num_workers=4)\n    dl = DataLoader2(dp, reading_service=rs)\n    print(\"MP reading serivce\")\n    num_frames = 0\n    for i, batch in tqdm(enumerate(dl)):\n        # batch_size is 1\n        num_frames += 1\n        item = batch[0]\n        print(\"\\n\")\n        for key, val in item.items():\n            if isinstance(val, dict):\n                print(key, val.keys())\n            elif isinstance(val, torch.Tensor):\n                print(key, val.size())\n            elif isinstance(val, np.ndarray):\n                print(key, val.shape)\n            else:\n                print(key, val)\n    print(f\"num_frames: {num_frames}\")\n    dl.shutdown()", "\ndef mp_loader(dp):\n    rs = MultiProcessingReadingService(num_workers=4)\n    dl = DataLoader2(dp, reading_service=rs)\n    print(\"MP reading serivce\")\n    num_frames = 0\n    for i, batch in tqdm(enumerate(dl)):\n        # batch_size is 1\n        num_frames += 1\n        item = batch[0]\n        print(\"\\n\")\n        for key, val in item.items():\n            if isinstance(val, dict):\n                print(key, val.keys())\n            elif isinstance(val, torch.Tensor):\n                print(key, val.size())\n            elif isinstance(val, np.ndarray):\n                print(key, val.shape)\n            else:\n                print(key, val)\n    print(f\"num_frames: {num_frames}\")\n    dl.shutdown()", "\n\ndef dist_loader(rank, world_size, dp, q):\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    rs = DistributedReadingService()\n    dl = DataLoader2(dp, reading_service=rs)\n    cnt = 0\n    for d in tqdm(dl, desc=f\"loading on rank {rank}\"):\n        cnt += 1\n        # Mimic distributed training step\n        dist.barrier()\n    q.put(cnt)\n    dl.shutdown()", "\n\ndef mp_dist_training(rank, world_size, dp, q):\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    mp_rs = MultiProcessingReadingService(num_workers=3)\n    dist_rs = DistributedReadingService()\n    rs = SequentialReadingService(dist_rs, mp_rs)\n    dl = DataLoader2(dp, reading_service=rs)\n    cnt = 0\n    for d in tqdm(dl, desc=f\"loading on rank {rank} with mp reading service\"):\n        cnt += 1\n        # Mimic distributed training step\n        dist.barrier()\n    q.put(cnt)\n    dl.shutdown()", "\n\ndef dist_run(loader, dp):\n    ctx = mp.get_context(\"fork\")  # Notebook doesn't work well with spawn\n    pqs = []\n    for rank in range(WORLD_SIZE):\n        q = ctx.Queue()\n        p = ctx.Process(target=loader, args=(rank, WORLD_SIZE, dp, q))\n        pqs.append((p, q))\n        p.start()\n\n    for rank in range(WORLD_SIZE):\n        cnt = pqs[rank][1].get()\n        print(f\"DataLoader2 on rank {rank} received {cnt} data\")\n        pqs[rank][0].join()", "\n\n@click.command()\n@click.option(\n    \"--container-dir\", default=\"/datadrive/azure_storage/weathereastus/era5seqrecord\"\n)\n@click.option(\n    \"--reading-service\", required=True, type=click.Choice([\"mp\", \"dist\", \"mpdist\"])\n)\n@click.option(\"--testing\", required=True, type=bool)\ndef main(container_dir: str, reading_service: str, testing: bool = True):\n    recorddir = f\"{container_dir}/local/1980\"\n    record = WSeqRecord.load_record(recorddir=recorddir)\n    var_list = VAR4TEST if testing else VAR\n    record.set_framereader_args(\n        {\n            \"input_features\": var_list,\n            \"target_features\": var_list,\n            \"hours_per_step\": 1,\n            \"max_pred_steps\": 10,\n        }\n    )\n\n    dp = build_wdatapipe(\n        [record], None, None, batch_size=1, mappings=[], collate_fn=identity\n    )\n    print(f\"Testing: {testing}\")\n    if reading_service == \"mp\":\n        print(\"Testing mp reading with 4 workers:\")\n        mp_loader(dp)\n    elif reading_service == \"dist\":\n        print(f\"Testing dist reading with {WORLD_SIZE} nodes:\")\n        dist_run(dist_loader, dp)\n    else:\n        print(f\"Testing dist mp reading with {WORLD_SIZE} nodes and 3 workers:\")\n        dist_run(mp_dist_training, dp)", ")\n@click.option(\"--testing\", required=True, type=bool)\ndef main(container_dir: str, reading_service: str, testing: bool = True):\n    recorddir = f\"{container_dir}/local/1980\"\n    record = WSeqRecord.load_record(recorddir=recorddir)\n    var_list = VAR4TEST if testing else VAR\n    record.set_framereader_args(\n        {\n            \"input_features\": var_list,\n            \"target_features\": var_list,\n            \"hours_per_step\": 1,\n            \"max_pred_steps\": 10,\n        }\n    )\n\n    dp = build_wdatapipe(\n        [record], None, None, batch_size=1, mappings=[], collate_fn=identity\n    )\n    print(f\"Testing: {testing}\")\n    if reading_service == \"mp\":\n        print(\"Testing mp reading with 4 workers:\")\n        mp_loader(dp)\n    elif reading_service == \"dist\":\n        print(f\"Testing dist reading with {WORLD_SIZE} nodes:\")\n        dist_run(dist_loader, dp)\n    else:\n        print(f\"Testing dist mp reading with {WORLD_SIZE} nodes and 3 workers:\")\n        dist_run(mp_dist_training, dp)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "seqrecord/weather/format/weathernp2seq_mp.py", "chunked_list": ["\"\"\"transform existing weather dataset to sequence weather format.\n\nnotes:\n    - assume the entire dataset is stored in the single storage account.\n\"\"\"\nfrom seqrecord.weather.seqrecord import WSeqRecord\nimport os\nfrom typing import List, Tuple, Iterator\nimport re\nimport numpy as np", "import re\nimport numpy as np\nfrom tqdm import tqdm\nfrom multiprocessing import Process\nfrom itertools import islice\n\n# test correctness of saved seqrecord format\n\n# dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\ndataset_mount_dir = \"/datadrive/weatherdatastorage2/datasets\"", "# dataset: 5.625deg_equally_np_all_levels,  1.40625deg_equally_np_all_levels(ultimate goal)\ndataset_mount_dir = \"/datadrive/weatherdatastorage2/datasets\"\n# \"/datadrive/weatherdatastorage2/datasets\",  # \"/mnt/data\",\nDEFAULT_CONFIG = {\n    \"num_processes\": 4,\n    \"wseqrecord_dir\": os.path.join(\n        dataset_mount_dir,\n        \"CMIP6/MPI-ESM/wseqrecord/dev/5.625deg_equally_np_all_levels/train/\",\n    ),\n    \"wdataset_dir\": os.path.join(", "    ),\n    \"wdataset_dir\": os.path.join(\n        dataset_mount_dir, \"CMIP6/MPI-ESM/5.625deg_equally_np_all_levels/train/\"\n    ),\n}\n\n\ndef sort_weatherfiles(files: List[os.DirEntry]) -> List[os.DirEntry]:\n    \"\"\"Return sorted files in dir, make sure files are sorted incrementally in time.\n    # todo: check with Jayesh this sorting is correct\n    Example file names: 195001010600-195501010000_33.npz from CMIP6/MPI-ESM/1.40625deg_equally_np_all_levels/train/\n\n    Args:\n        files (List[os.DirEntry]): each element in list is a file name\n    \"\"\"\n\n    def str2nums(direntry):\n        nums = re.split(\"-|_\", direntry.name.split(\".\")[0])\n        nums = tuple(map(int, nums))\n        return nums\n\n    return sorted(files, key=str2nums)", "\n\ndef weatherdata2seqrecord(\n    rank: int,\n    world_size: int,\n    config: dict,\n    sub_weatherfile_generator: Iterator,\n    sub_loads: int,\n) -> None:\n\n    sub_wsrecord = WSeqRecord(\n        os.path.join(\n            config[\"wseqrecord_dir\"], WSeqRecord.subfolder_name(rank, world_size)\n        )\n    )\n\n    def frame_generator(files: Iterator):\n        i = 0\n        pbar = (\n            tqdm(files, desc=\"Formatting progress on rank 0:\", total=sub_loads)\n            if rank == 0\n            else files\n        )\n        for file_path in pbar:\n            data = np.load(file_path)\n            num_frames = data[data.files[0]].shape[0]\n            for rel_frame_idx in range(num_frames):\n                frame = {}\n                for key in data.files:\n                    frame[key] = data[key][rel_frame_idx]\n                yield frame\n            i += 1\n\n    sub_wsrecord.put_frame(frame_generator(sub_weatherfile_generator), 5)\n    sub_wsrecord.dump(rank)", "\n\ndef distribute_loads(works: int, num_processes: int) -> List[Tuple[int, int]]:\n    \"\"\"Given the overall works and number of processes, allocate evenly the loads that each process should take.\n\n    Args:\n        works (int): amount of over all work\n        num_processes (int): number of processes available\n\n    Returns:\n        List[Tuple[int, int]]: indices of work each process is responsible for\n    \"\"\"\n    assert (\n        works >= num_processes\n    ), \"The amount of works is less than number of processes.\"\n    ans = []\n    start = 0\n    loads_per_process = round(works / num_processes)\n    for i in range(num_processes):\n        end = start + loads_per_process if i < num_processes - 1 else works\n        ans.append((start, end))\n        start = end\n    return ans", "\n\nif __name__ == \"__main__\":\n\n    config = DEFAULT_CONFIG\n\n    # gather existing weather dataset files\n    files_dirs = os.scandir(os.path.join(config[\"wdataset_dir\"]))\n    files = list(filter(lambda direntry: direntry.is_file(), files_dirs))\n    # make sure files are ordered by time incrementally\n    # different datasets require different sortting methods\n    files = sort_weatherfiles(files)[:4]\n    weatherfile_generator = iter(files)\n    dividens = distribute_loads(len(files), config[\"num_processes\"])\n    processes = []\n    for i in range(config[\"num_processes\"]):\n        sub_weatherfile_generator = islice(\n            weatherfile_generator, dividens[i][0], dividens[i][1]\n        )\n        p = Process(\n            target=weatherdata2seqrecord,\n            args=(\n                i,\n                config[\"num_processes\"],\n                config,\n                sub_weatherfile_generator,\n                dividens[i][1] - dividens[i][0],\n            ),\n        )\n        processes.append(p)\n        p.start()\n    # all process complete successfully\n    for i in range(config[\"num_processes\"]):\n        processes[i].join()\n\n    # combine sub-seqrecord\n    record = WSeqRecord.gather_subseqrecords(\n        config[\"wseqrecord_dir\"], world_size=config[\"num_processes\"]\n    )", ""]}
{"filename": "seqrecord/weather/datapipe/datapipe.py", "chunked_list": ["import os\nfrom torchdata.dataloader2 import (\n    DataLoader2,\n    MultiProcessingReadingService,\n    DistributedReadingService,\n    SequentialReadingService,\n)\nimport torchdata\nfrom torchdata.datapipes import functional_datapipe\nfrom torchdata.datapipes.iter import IterableWrapper, IterDataPipe", "from torchdata.datapipes import functional_datapipe\nfrom torchdata.datapipes.iter import IterableWrapper, IterDataPipe\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport socket\nimport torch\n\nfrom torch.utils.data.datapipes.iter.sharding import SHARDING_PRIORITIES\n\nMASTER_ADDR = \"127.0.0.1\"", "\nMASTER_ADDR = \"127.0.0.1\"\nWORLD_SIZE = 2\n\n\n@functional_datapipe(\"square_dp\")\nclass SqaureDP(IterDataPipe):\n    def __init__(self, source_datapipe) -> None:\n        self.source_datapipe = source_datapipe\n        self.acc = []\n\n    def __iter__(self):\n        for num in self.source_datapipe:\n            print(f\"getting number {num} and square it to get {num*num}\")\n            self.acc.append(num)\n            yield num * num\n        print(f\"\\n {self.acc} \\n\")", "\n\ndef _get_open_port():\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((\"\", 0))\n    port = s.getsockname()[1]\n    s.close()\n    return str(port)\n\n", "\n\nos.environ[\"MASTER_ADDR\"] = MASTER_ADDR\nos.environ[\"MASTER_PORT\"] = _get_open_port()\n\n\ndef dist_dl(rank, world_size):\n    # using the old apis of torch data to shard\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    dp = IterableWrapper([i for i in range(10)]).sharding_filter().square_dp()\n    mp_rs = MultiProcessingReadingService(num_workers=3)\n    dist_rs = DistributedReadingService()\n    rs = SequentialReadingService(dist_rs, mp_rs)\n    dl = DataLoader2(dp, reading_service=rs)\n    for d in dl:\n        d\n    dl.shutdown()", "\n\ndef mp_dl():\n    dp = (\n        IterableWrapper([i for i in range(10)])\n        .sharding_filter(sharding_group_filter=SHARDING_PRIORITIES.DEFAULT)\n        .sharding_filter(sharding_group_filter=SHARDING_PRIORITIES.MULTIPROCESSING)\n        .square_dp()\n    )\n    mp_rs = MultiProcessingReadingService(num_workers=3)\n    dist_rs = DistributedReadingService()\n    rs = SequentialReadingService(dist_rs, mp_rs)\n    dl = DataLoader2(dp, reading_service=rs)\n    for i, x in enumerate(dl):\n        # print(x)\n        x\n    dl.shutdown()", "\n\nif __name__ == \"__main__\":\n\n    ctx = mp.get_context(\"fork\")  # Notebook doesn't work well with spawn\n\nfor rank in range(WORLD_SIZE):\n    p = ctx.Process(target=dist_dl, args=(rank, WORLD_SIZE))\n    p.start()\n", ""]}
