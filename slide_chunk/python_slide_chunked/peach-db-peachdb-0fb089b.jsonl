{"filename": "setup.py", "chunked_list": ["from pathlib import Path\n\nfrom setuptools import find_packages, setup  # type: ignore\n\nrequirements = []\n\n# Read requirements from files\nwith open(\"requirements.txt\", \"r\") as f:\n    requirements.extend(f.read().splitlines())\n", "\n# read the contents README\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / \"README.md\").read_text()\n\nsetup(\n    name=\"peachdb\",\n    version=\"0.1.0\",\n    packages=find_packages(),\n    install_requires=requirements,", "    packages=find_packages(),\n    install_requires=requirements,\n    dependency_links=[\"https://download.pytorch.org/whl/cu113\"],\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache License 2.0\",\n    project_urls={\n        \"Source on GitHub\": \"https://github.com/peach-db/peachdb\",\n        \"Documentation\": \"https://github.com/peach-db/peachdb/blob/master/README.md\",\n    },", "        \"Documentation\": \"https://github.com/peach-db/peachdb/blob/master/README.md\",\n    },\n    python_requires=\">=3.10\",\n)\n"]}
{"filename": "deploy_api.py", "chunked_list": ["import os\nimport shelve\nimport tempfile\nimport traceback\nfrom uuid import uuid4\n\nimport openai\nimport pandas as pd\nimport uvicorn\nfrom fastapi import FastAPI, WebSocket", "import uvicorn\nfrom fastapi import FastAPI, WebSocket\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\nfrom rich import print\n\nfrom peachdb import EmptyNamespace, PeachDB\nfrom peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot", "from peachdb import EmptyNamespace, PeachDB\nfrom peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot\nfrom peachdb.constants import SHELVE_DB\n\napp = FastAPI()\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],", "    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nEMBEDDING_GENERATOR = \"openai_ada\"\nEMBEDDING_BACKEND = \"exact_cpu\"\n\n\ndef _validate_data(texts, ids, metadatas_list):\n    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n    assert all(isinstance(m, dict) for m in metadatas_list), \"All metadata must be dicts\"\n    assert len(set([str(x.keys()) for x in metadatas_list])) == 1, \"All metadata must have the same keys\"", "\ndef _validate_data(texts, ids, metadatas_list):\n    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n    assert all(isinstance(m, dict) for m in metadatas_list), \"All metadata must be dicts\"\n    assert len(set([str(x.keys()) for x in metadatas_list])) == 1, \"All metadata must have the same keys\"\n\n\ndef _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace):\n    assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n    assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\n    if namespace is not None:\n        assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"", "def _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace):\n    assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n    assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\n    if namespace is not None:\n        assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"\n\n\ndef _process_input_data(request_json: dict) -> pd.DataFrame:\n    input_data = request_json\n    namespace = request_json.get(\"namespace\", None)\n\n    # TODO: make metadata optional?\n\n    # tuples of (id: str, text: list[float], metadata: dict[str, str]])\n    data = input_data[\"data\"]\n\n    assert len(set([len(d) for d in data])) == 1, \"All data must have the same length\"\n    if len(data[0]) == 3:\n        # we got (ids, texts, metadata)\n        pass\n    elif len(data[0]) == 2:\n        # we got (ids, texts)\n        data = [(d[0], d[1], {}) for d in data]\n    else:\n        raise ValueError(\"Data must be of the form (ids, texts) or (ids, texts, metadata)\")\n\n    ids = [str(d[0]) for d in data]\n    texts = [d[1] for d in data]\n    metadatas_list: list = [d[2] for d in data]\n\n    _validate_data(texts, ids, metadatas_list)\n\n    # convert metadata from input format to one we can create a dataframe from.\n    metadatas_dict: dict[str, list] = {\n        key: [metadata[key] for metadata in metadatas_list] for key in metadatas_list[0].keys()\n    }\n\n    _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace)\n    if namespace is None:\n        metadatas_dict[\"namespace\"] = [None] * len(ids)\n    else:\n        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\n    df = pd.DataFrame(\n        data={\n            \"ids\": ids,\n            \"texts\": texts,\n            **metadatas_dict,\n        }\n    )\n\n    return df", "def _process_input_data(request_json: dict) -> pd.DataFrame:\n    input_data = request_json\n    namespace = request_json.get(\"namespace\", None)\n\n    # TODO: make metadata optional?\n\n    # tuples of (id: str, text: list[float], metadata: dict[str, str]])\n    data = input_data[\"data\"]\n\n    assert len(set([len(d) for d in data])) == 1, \"All data must have the same length\"\n    if len(data[0]) == 3:\n        # we got (ids, texts, metadata)\n        pass\n    elif len(data[0]) == 2:\n        # we got (ids, texts)\n        data = [(d[0], d[1], {}) for d in data]\n    else:\n        raise ValueError(\"Data must be of the form (ids, texts) or (ids, texts, metadata)\")\n\n    ids = [str(d[0]) for d in data]\n    texts = [d[1] for d in data]\n    metadatas_list: list = [d[2] for d in data]\n\n    _validate_data(texts, ids, metadatas_list)\n\n    # convert metadata from input format to one we can create a dataframe from.\n    metadatas_dict: dict[str, list] = {\n        key: [metadata[key] for metadata in metadatas_list] for key in metadatas_list[0].keys()\n    }\n\n    _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace)\n    if namespace is None:\n        metadatas_dict[\"namespace\"] = [None] * len(ids)\n    else:\n        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\n    df = pd.DataFrame(\n        data={\n            \"ids\": ids,\n            \"texts\": texts,\n            **metadatas_dict,\n        }\n    )\n\n    return df", "\n\n@app.post(\"/upsert-text\")\nasync def upsert_handler(request: Request):\n    \"\"\"\n    Takes texts as input rather than vectors (unlike Pinecone).\n    \"\"\"\n    input_data = await request.json()\n\n    project_name = input_data.get(\"project_name\", None)\n    if project_name is None:\n        return Response(content=\"Project name must be specified.\", status_code=400)", "\n    project_name = input_data.get(\"project_name\", None)\n    if project_name is None:\n        return Response(content=\"Project name must be specified.\", status_code=400)\n\n    peach_db = PeachDB(\n        project_name=project_name,\n        embedding_generator=EMBEDDING_GENERATOR,\n        embedding_backend=EMBEDDING_BACKEND,\n    )", "        embedding_backend=EMBEDDING_BACKEND,\n    )\n    new_data_df = _process_input_data(input_data)\n    namespace = input_data.get(\"namespace\", None)\n\n    with shelve.open(SHELVE_DB) as shelve_db:\n        project_info = shelve_db.get(project_name, None)\n        assert project_info is not None, \"Project not found\"\n        assert not project_info[\"lock\"], \"Some other process is currently reading/writing to this project\"\n        # TODO: replace with a lock we can await.\n        project_info[\"lock\"] = True\n        shelve_db[project_name] = project_info", "\n    if os.path.exists(project_info[\"exp_compound_csv_path\"]):\n        data_df = pd.read_csv(project_info[\"exp_compound_csv_path\"])\n\n        if namespace is None:\n            data_df_namespace = data_df[data_df[\"namespace\"].isnull()]\n        else:\n            data_df_namespace = data_df[data_df[\"namespace\"] == namespace]\n\n        # Check for intersection between the \"ids\" column of data_df and new_data_df\n        # TODO: add support on backend for string ids.\n        if len(set(data_df_namespace[\"ids\"].apply(str)).intersection(set(new_data_df[\"ids\"].apply(str)))) != 0:\n            with shelve.open(SHELVE_DB) as shelve_db:\n                project_info = shelve_db.get(project_name, None)\n                project_info[\"lock\"] = False\n                shelve_db[project_name] = project_info\n\n            return Response(\n                content=\"New data contains IDs that already exist in the database for this namespace. This is not allowed.\",\n                status_code=400,\n            )", "\n    # We use unique csv_name to avoid conflicts in the stored data.\n    with tempfile.NamedTemporaryFile(suffix=f\"{uuid4()}.csv\") as tmp:\n        # TODO: what happens if ids' conflict?\n        new_data_df.to_csv(tmp.name, index=False)  # TODO: check it won't cause an override.\n\n        peach_db.upsert_text(\n            csv_path=tmp.name,\n            column_to_embed=\"texts\",\n            id_column_name=\"ids\",\n            # TODO: below is manually set, this might cause issues!\n            embeddings_output_s3_bucket_uri=\"s3://metavoice-vector-db/deployed_solution/\",\n            namespace=namespace,\n        )", "\n    if os.path.exists(project_info[\"exp_compound_csv_path\"]):\n        # Update the data_df with the new data, and save to disk.\n        data_df = pd.concat([data_df, new_data_df], ignore_index=True)\n        data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n    else:\n        new_data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n\n    # release lock\n    with shelve.open(SHELVE_DB) as shelve_db:\n        project_info = shelve_db.get(project_name, None)\n        project_info[\"lock\"] = False\n        shelve_db[project_name] = project_info", "    # release lock\n    with shelve.open(SHELVE_DB) as shelve_db:\n        project_info = shelve_db.get(project_name, None)\n        project_info[\"lock\"] = False\n        shelve_db[project_name] = project_info\n\n\n@app.get(\"/query\")\nasync def query_embeddings_handler(request: Request):\n    data = await request.json()", "async def query_embeddings_handler(request: Request):\n    data = await request.json()\n\n    project_name = data.get(\"project_name\", None)\n    if project_name is None:\n        return Response(content=\"Project name must be specified.\", status_code=400)\n\n    peach_db = PeachDB(\n        project_name=project_name,\n        embedding_generator=EMBEDDING_GENERATOR,", "        project_name=project_name,\n        embedding_generator=EMBEDDING_GENERATOR,\n        embedding_backend=EMBEDDING_BACKEND,\n    )\n\n    text = data[\"text\"]\n    top_k = int(data.get(\"top_k\", 5))\n    namespace = data.get(\"namespace\", None)\n\n    try:\n        ids, distances, metadata = peach_db.query(query_input=text, modality=\"text\", namespace=namespace, top_k=top_k)\n    except EmptyNamespace:\n        return Response(content=\"Empty namespace.\", status_code=400)", "\n    try:\n        ids, distances, metadata = peach_db.query(query_input=text, modality=\"text\", namespace=namespace, top_k=top_k)\n    except EmptyNamespace:\n        return Response(content=\"Empty namespace.\", status_code=400)\n\n    result = []\n    # TODO: we're aligning distances, ids, and metadata from different sources which could cause bugs.\n    # Fix this.\n    for id, dist in zip(ids, distances):\n        values = metadata[metadata[\"ids\"] == id].values[0]\n        columns = list(metadata.columns)\n        columns = [\n            c for c in columns if c != \"namespace\"\n        ]  # Causes an error with JSON encoding when \"namespace\" is None and ends up as NaN here.\n        result_dict = {columns[i]: values[i] for i in range(len(columns))}\n        result_dict[\"distance\"] = dist\n        result.append(result_dict)", "    # Fix this.\n    for id, dist in zip(ids, distances):\n        values = metadata[metadata[\"ids\"] == id].values[0]\n        columns = list(metadata.columns)\n        columns = [\n            c for c in columns if c != \"namespace\"\n        ]  # Causes an error with JSON encoding when \"namespace\" is None and ends up as NaN here.\n        result_dict = {columns[i]: values[i] for i in range(len(columns))}\n        result_dict[\"distance\"] = dist\n        result.append(result_dict)", "\n    return {\"result\": result}\n\n\n@app.post(\"/create-bot\")\nasync def create_bot_handler(request: Request):\n    try:\n        request_json = await request.json()\n        try:\n            bot = QABot(\n                bot_id=request_json[\"bot_id\"],\n                system_prompt=request_json[\"system_prompt\"],\n                llm_model_name=request_json[\"llm_model_name\"] if \"llm_model_name\" in request_json else \"gpt-3.5-turbo\",\n                embedding_model=request_json[\"embedding_model_name\"]\n                if \"embedding_model_name\" in request_json\n                else \"openai_ada\",\n            )\n        except BadBotInputError as e:\n            return Response(content=str(e), status_code=400)\n\n        try:\n            bot.add_data(documents=request_json[\"documents\"])\n            return Response(content=\"Bot created successfully.\", status_code=200)\n        except openai.error.RateLimitError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n        except openai.error.AuthenticationError:\n            return Response(content=\"There's been an authentication error. Please contact the team.\", status_code=400)\n        except openai.error.ServiceUnavailableError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n    except Exception as e:\n        traceback.print_exc()\n        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)", "\n\n@app.post(\"/create-conversation\")\nasync def create_conversation_handler(request: Request):\n    try:\n        request_json = await request.json()\n\n        if \"bot_id\" not in request_json:\n            return Response(content=\"bot_id must be specified.\", status_code=400)\n\n        if \"query\" not in request_json:\n            return Response(content=\"query must be specified.\", status_code=400)\n\n        bot_id = request_json[\"bot_id\"]\n        query = request_json[\"query\"]\n\n        bot = QABot(bot_id=bot_id)\n        try:\n            for cid, response in bot.create_conversation_with_query(query=query):\n                break\n        except openai.error.RateLimitError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n        except openai.error.ServiceUnavailableError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n\n        return {\n            \"conversation_id\": cid,\n            \"response\": response,\n        }\n    except Exception as e:\n        traceback.print_exc()\n        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)", "\n\n@app.websocket_route(\"/ws-create-conversation\")\nasync def ws_create_conversation_handler(websocket: WebSocket):\n    try:\n        await websocket.accept()\n        request_json = await websocket.receive_json()\n\n        if \"bot_id\" not in request_json:\n            await websocket.close(reason=\"bot_id must be specified.\", code=1003)\n\n        if \"query\" not in request_json:\n            await websocket.close(reason=\"query must be specified.\", code=1003)\n\n        bot_id = request_json[\"bot_id\"]\n        query = request_json[\"query\"]\n\n        try:\n            bot = QABot(bot_id=bot_id)\n            for cid, response in bot.create_conversation_with_query(query=query, stream=True):\n                await websocket.send_json(\n                    {\n                        \"conversation_id\": cid,\n                        \"response\": response,\n                    }\n                )\n        except openai.error.RateLimitError:\n            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n        except BadBotInputError as e:\n            await websocket.close(reason=str(e), code=1003)\n        # TODO: add below error message to add endpoints... Can we handle all these exceptions together somehow?\n        except openai.error.ServiceUnavailableError:\n            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n    except Exception as e:\n        traceback.print_exc()\n        await websocket.close(reason=\"An unknown error occured. Please contact the team.\", code=1003)", "\n\n@app.post(\"/continue-conversation\")\nasync def continue_conversation_handler(request: Request):\n    try:\n        request_json = await request.json()\n\n        if \"bot_id\" not in request_json:\n            return Response(content=\"bot_id must be specified.\", status_code=400)\n        if \"conversation_id\" not in request_json:\n            return Response(content=\"conversation_id must be specified.\", status_code=400)\n        if \"query\" not in request_json:\n            return Response(content=\"query must be specified.\", status_code=400)\n\n        bot_id = request_json[\"bot_id\"]\n        conversation_id = request_json[\"conversation_id\"]\n        query = request_json[\"query\"]\n\n        bot = QABot(bot_id=bot_id)\n        try:\n            for response in bot.continue_conversation_with_query(conversation_id=conversation_id, query=query):\n                break\n        except ConversationNotFoundError:\n            return Response(content=\"Conversation not found. Please check `conversation_id`\", status_code=400)\n        except openai.error.RateLimitError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n        except openai.error.ServiceUnavailableError:\n            return Response(\n                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n            )\n\n        return {\n            \"response\": response,\n        }\n    except Exception as e:\n        traceback.print_exc()\n        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)", "\n\n@app.websocket_route(\"/ws-continue-conversation\")\nasync def ws_continue_conversation_handler(websocket: WebSocket):\n    try:\n        await websocket.accept()\n        request_json = await websocket.receive_json()\n\n        if \"bot_id\" not in request_json:\n            await websocket.close(reason=\"bot_id must be specified.\", code=1003)\n        if \"conversation_id\" not in request_json:\n            await websocket.close(reason=\"conversation_id must be specified.\", code=1003)\n        if \"query\" not in request_json:\n            await websocket.close(reason=\"query must be specified.\", code=1003)\n\n        bot_id = request_json[\"bot_id\"]\n        conversation_id = request_json[\"conversation_id\"]\n        query = request_json[\"query\"]\n\n        bot = QABot(bot_id=bot_id)\n        try:\n            for response in bot.continue_conversation_with_query(\n                conversation_id=conversation_id, query=query, stream=True\n            ):\n                await websocket.send_json(\n                    {\n                        \"response\": response,\n                    }\n                )\n        except ConversationNotFoundError:\n            await websocket.close(reason=\"Conversation not found. Please check `conversation_id`\", code=1003)\n        except openai.error.RateLimitError:\n            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n        # TODO: add below error message to add endpoints... Can we handle all these exceptions together somehow?\n        except openai.error.ServiceUnavailableError:\n            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n\n    except Exception as e:\n        traceback.print_exc()\n        await websocket.close(reason=\"An unknown error occured. Please contact the team.\", code=1003)", "\n\nif __name__ == \"__main__\":\n    port = 8000\n    uvicorn.run(\"deploy_api:app\", host=\"0.0.0.0\", port=port)  # , reload=True)\n"]}
{"filename": "run_bot.py", "chunked_list": ["import shelve\n\nfrom peachdb.bots.qa import QABot\nfrom peachdb.constants import CONVERSATIONS_DB\n\nbot = QABot(\n    bot_id=\"my_bot_12\",\n    embedding_model=\"openai_ada\",\n    llm_model_name=\"gpt-3.5-turbo\",\n    system_prompt=\"Please answer questions about the 1880 Greenback Party National Convention.\",", "    llm_model_name=\"gpt-3.5-turbo\",\n    system_prompt=\"Please answer questions about the 1880 Greenback Party National Convention.\",\n)\n\nbot.add_data(\n    documents=[\n        \"The 1880 Greenback Party National Convention convened in Chicago from June 9 to June 11 to select presidential and vice presidential nominees and write a party platform for the Greenback Party in the United States presidential election of 1880. Delegates chose James B. Weaver of Iowa for President and Barzillai J. Chambers of Texas for Vice President.\",\n        'The Greenback Party was a newcomer to the political scene in 1880 having arisen, mostly in the nation\\'s West and South, as a response to the economic depression that followed the Panic of 1873. During the Civil War, Congress had authorized \"greenbacks\", a form of money redeemable in government bonds, rather than in then-traditional gold. After the war, many Democrats and Republicans in the East sought to return to the gold standard, and the government began to withdraw greenbacks from circulation. The reduction of the money supply, combined with the economic depression, made life harder for debtors, farmers, and industrial laborers; the Greenback Party hoped to draw support from these groups.',\n    ]\n)", "    ]\n)\n\ncid, answer = bot.create_conversation_with_query(\"Where did the convention convene?\")\n\nanswer_2 = bot.continue_conversation_with_query(cid, \"Who was the vice presidential nominee?\")\n\nanswer_3 = bot.continue_conversation_with_query(cid, \"What was The Greenback Party?\")\n", ""]}
{"filename": "peachdb_grpc/api_server.py", "chunked_list": ["\"\"\"\nModule docstring:\nThis module is a streaming gRPC server for the bot service.\nIt serves to handle requests related to bot creation, conversation creation and continuation.\n\"\"\"\n\nimport asyncio\nimport functools\nimport traceback\nfrom typing import AsyncIterable, Iterator", "import traceback\nfrom typing import AsyncIterable, Iterator\n\nimport api_pb2 as api_pb2  # type: ignore\nimport api_pb2_grpc as api_pb2_grpc  # type: ignore\nimport grpc  # type: ignore\nimport openai\n\nfrom peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot, UnexpectedGPTRoleResponse\n", "from peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot, UnexpectedGPTRoleResponse\n\n\ndef grpc_error_handler_async_fn(fn):\n    \"\"\"\n    A decorator to handle any unhandled errors and map them to appropriate gRPC status codes.\n    :param fn: The function to be decorated.\n    :return: Decorated function.\n    \"\"\"\n\n    @functools.wraps(fn)\n    async def wrapper(self, request, context):\n        try:\n            return await fn(self, request, context)\n        except BadBotInputError as e:\n            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, str(e))\n        except openai.error.RateLimitError:\n            await context.abort(\n                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n            )\n        except openai.error.AuthenticationError:\n            await context.abort(\n                grpc.StatusCode.UNAUTHENTICATED, \"There's been an authentication error. Please contact the team.\"\n            )\n        except openai.error.ServiceUnavailableError:\n            await context.abort(\n                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n            )\n        except UnexpectedGPTRoleResponse:\n            await context.abort(grpc.StatusCode.INTERNAL, \"GPT-3 responded with a role that was not expected.\")\n        except ConversationNotFoundError:\n            await context.abort(\n                grpc.StatusCode.INVALID_ARGUMENT, \"Conversation not found. Please check `conversation_id`\"\n            )\n        except Exception as e:\n            traceback.print_exc()\n            await context.abort(grpc.StatusCode.UNKNOWN, \"An unknown error occurred. Please contact the team.\")\n\n    return wrapper", "\n\ndef grpc_error_handler_async_gen(fn):\n    \"\"\"\n    A decorator to handle any unhandled errors and map them to appropriate gRPC status codes.\n    :param fn: The function to be decorated.\n    :return: Decorated function.\n    \"\"\"\n\n    @functools.wraps(fn)\n    async def wrapper(self, request, context):\n        try:\n            async for response in fn(self, request, context):\n                yield response\n        except BadBotInputError as e:\n            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, str(e))\n        except openai.error.RateLimitError:\n            await context.abort(\n                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n            )\n        except openai.error.ServiceUnavailableError:\n            await context.abort(\n                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n            )\n        except openai.error.AuthenticationError:\n            await context.abort(\n                grpc.StatusCode.UNAUTHENTICATED, \"There's been an authentication error. Please contact the team.\"\n            )\n        except UnexpectedGPTRoleResponse:\n            await context.abort(grpc.StatusCode.INTERNAL, \"GPT-3 responded with a role that was not expected.\")\n        except ConversationNotFoundError:\n            await context.abort(\n                grpc.StatusCode.INVALID_ARGUMENT, \"Conversation not found. Please check `conversation_id`\"\n            )\n        except Exception as e:\n            traceback.print_exc()\n            await context.abort(grpc.StatusCode.UNKNOWN, \"An unknown error occurred. Please contact the team.\")\n\n    return wrapper", "\n\nclass BotServiceServicer(api_pb2_grpc.BotServiceServicer):\n    @grpc_error_handler_async_fn\n    async def CreateBot(self, request: api_pb2.CreateBotRequest, context) -> api_pb2.CreateBotResponse:\n        \"\"\"\n        RPC method to create a new bot instance.\n        :param request: Request instance for CreateBot.\n        :param context: Context instance for CreateBot.\n        :return: CreateBotResponse instance.\n        \"\"\"\n        bot = QABot(\n            bot_id=request.bot_id,\n            system_prompt=request.system_prompt,\n            llm_model_name=request.llm_model_name if request.HasField(\"llm_model_name\") else \"gpt-3.5-turbo\",\n            embedding_model=request.embedding_model_name if request.HasField(\"embedding_model_name\") else \"openai_ada\",\n        )\n        bot.add_data(documents=list(request.documents))\n        return api_pb2.CreateBotResponse(status=\"Bot created successfully.\")\n\n    @grpc_error_handler_async_gen\n    async def CreateConversation(\n        self, request: api_pb2.CreateConversationRequest, context\n    ) -> AsyncIterable[api_pb2.CreateConversationResponse]:\n        \"\"\"\n        RPC method to create a new conversation for a bot.\n        :param request: Request instance for CreateConversation.\n        :param context: Context instance for CreateConversation.\n        :return: CreateConversationResponse instance stream.\n        \"\"\"\n        await self._check(request, \"bot_id\", context)\n        await self._check(request, \"query\", context)\n\n        bot_id = request.bot_id\n        query = request.query\n\n        bot = QABot(bot_id=bot_id)\n        generator = bot.create_conversation_with_query(query=query, stream=True)\n        assert isinstance(generator, Iterator)\n\n        for cid, response in generator:\n            yield api_pb2.CreateConversationResponse(conversation_id=cid, response=response)\n\n    @grpc_error_handler_async_gen\n    async def ContinueConversation(\n        self, request: api_pb2.ContinueConversationRequest, context\n    ) -> AsyncIterable[api_pb2.ContinueConversationResponse]:\n        \"\"\"\n        RPC method to continue a conversation for a bot.\n        :param request: Request instance for ContinueConversation.\n        :param context: Context instance for ContinueConversation.\n        :return: ContinueConversationResponse instance stream.\n        \"\"\"\n        await self._check(request, \"bot_id\", context)\n        await self._check(request, \"conversation_id\", context)\n        await self._check(request, \"query\", context)\n\n        bot_id = request.bot_id\n        conversation_id = request.conversation_id\n        query = request.query\n\n        bot = QABot(bot_id=bot_id)\n        response_gen = bot.continue_conversation_with_query(conversation_id=conversation_id, query=query, stream=True)\n        for response in response_gen:\n            yield api_pb2.ContinueConversationResponse(response=response)\n\n    async def _check(self, request, field, context):\n        if not getattr(request, field):\n            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, f\"{field} must be specified.\")", "\n\nasync def serve() -> None:\n    \"\"\"\n    Start a gRPC server.\n    :return: None\n    \"\"\"\n    server = grpc.aio.server()\n    api_pb2_grpc.add_BotServiceServicer_to_server(BotServiceServicer(), server)\n    server.add_insecure_port(\"[::]:50051\")", "    api_pb2_grpc.add_BotServiceServicer_to_server(BotServiceServicer(), server)\n    server.add_insecure_port(\"[::]:50051\")\n    await server.start()\n    await server.wait_for_termination()\n\n\nif __name__ == \"__main__\":\n    asyncio.get_event_loop().run_until_complete(serve())\n", ""]}
{"filename": "peachdb_grpc/api_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: api.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n\n# @@protoc_insertion_point(imports)", "\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n    b'\\n\\tapi.proto\\x12\\x07peachdb\"\\xb8\\x01\\n\\x10\\x43reateBotRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\rsystem_prompt\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tdocuments\\x18\\x03 \\x03(\\t\\x12\\x1b\\n\\x0ellm_model_name\\x18\\x04 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12!\\n\\x14\\x65mbedding_model_name\\x18\\x05 \\x01(\\tH\\x01\\x88\\x01\\x01\\x42\\x11\\n\\x0f_llm_model_nameB\\x17\\n\\x15_embedding_model_name\"#\\n\\x11\\x43reateBotResponse\\x12\\x0e\\n\\x06status\\x18\\x01 \\x01(\\t\":\\n\\x19\\x43reateConversationRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05query\\x18\\x02 \\x01(\\t\"G\\n\\x1a\\x43reateConversationResponse\\x12\\x17\\n\\x0f\\x63onversation_id\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08response\\x18\\x02 \\x01(\\t\"U\\n\\x1b\\x43ontinueConversationRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0f\\x63onversation_id\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05query\\x18\\x03 \\x01(\\t\"0\\n\\x1c\\x43ontinueConversationResponse\\x12\\x10\\n\\x08response\\x18\\x01 \\x01(\\t2\\x9e\\x02\\n\\nBotService\\x12\\x44\\n\\tCreateBot\\x12\\x19.peachdb.CreateBotRequest\\x1a\\x1a.peachdb.CreateBotResponse\"\\x00\\x12\\x61\\n\\x12\\x43reateConversation\\x12\".peachdb.CreateConversationRequest\\x1a#.peachdb.CreateConversationResponse\"\\x00\\x30\\x01\\x12g\\n\\x14\\x43ontinueConversation\\x12$.peachdb.ContinueConversationRequest\\x1a%.peachdb.ContinueConversationResponse\"\\x00\\x30\\x01\\x62\\x06proto3'\n)\n", ")\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"api_pb2\", globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n    DESCRIPTOR._options = None\n    _CREATEBOTREQUEST._serialized_start = 23\n    _CREATEBOTREQUEST._serialized_end = 207\n    _CREATEBOTRESPONSE._serialized_start = 209\n    _CREATEBOTRESPONSE._serialized_end = 244\n    _CREATECONVERSATIONREQUEST._serialized_start = 246\n    _CREATECONVERSATIONREQUEST._serialized_end = 304\n    _CREATECONVERSATIONRESPONSE._serialized_start = 306\n    _CREATECONVERSATIONRESPONSE._serialized_end = 377\n    _CONTINUECONVERSATIONREQUEST._serialized_start = 379\n    _CONTINUECONVERSATIONREQUEST._serialized_end = 464\n    _CONTINUECONVERSATIONRESPONSE._serialized_start = 466\n    _CONTINUECONVERSATIONRESPONSE._serialized_end = 514\n    _BOTSERVICE._serialized_start = 517\n    _BOTSERVICE._serialized_end = 803", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "peachdb_grpc/__init__.py", "chunked_list": [""]}
{"filename": "peachdb_grpc/api_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport api_pb2 as api__pb2  # type: ignore\nimport grpc  # type: ignore\n\n\nclass BotServiceStub(object):\n    \"\"\"The service definition\"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.CreateBot = channel.unary_unary(\n            \"/peachdb.BotService/CreateBot\",\n            request_serializer=api__pb2.CreateBotRequest.SerializeToString,\n            response_deserializer=api__pb2.CreateBotResponse.FromString,\n        )\n        self.CreateConversation = channel.unary_stream(\n            \"/peachdb.BotService/CreateConversation\",\n            request_serializer=api__pb2.CreateConversationRequest.SerializeToString,\n            response_deserializer=api__pb2.CreateConversationResponse.FromString,\n        )\n        self.ContinueConversation = channel.unary_stream(\n            \"/peachdb.BotService/ContinueConversation\",\n            request_serializer=api__pb2.ContinueConversationRequest.SerializeToString,\n            response_deserializer=api__pb2.ContinueConversationResponse.FromString,\n        )", "\n\nclass BotServiceServicer(object):\n    \"\"\"The service definition\"\"\"\n\n    def CreateBot(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")\n\n    def CreateConversation(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")\n\n    def ContinueConversation(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")", "\n\ndef add_BotServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \"CreateBot\": grpc.unary_unary_rpc_method_handler(\n            servicer.CreateBot,\n            request_deserializer=api__pb2.CreateBotRequest.FromString,\n            response_serializer=api__pb2.CreateBotResponse.SerializeToString,\n        ),\n        \"CreateConversation\": grpc.unary_stream_rpc_method_handler(\n            servicer.CreateConversation,\n            request_deserializer=api__pb2.CreateConversationRequest.FromString,\n            response_serializer=api__pb2.CreateConversationResponse.SerializeToString,\n        ),\n        \"ContinueConversation\": grpc.unary_stream_rpc_method_handler(\n            servicer.ContinueConversation,\n            request_deserializer=api__pb2.ContinueConversationRequest.FromString,\n            response_serializer=api__pb2.ContinueConversationResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\"peachdb.BotService\", rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))", "\n\n# This class is part of an EXPERIMENTAL API.\nclass BotService(object):\n    \"\"\"The service definition\"\"\"\n\n    @staticmethod\n    def CreateBot(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_unary(\n            request,\n            target,\n            \"/peachdb.BotService/CreateBot\",\n            api__pb2.CreateBotRequest.SerializeToString,\n            api__pb2.CreateBotResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n        )\n\n    @staticmethod\n    def CreateConversation(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_stream(\n            request,\n            target,\n            \"/peachdb.BotService/CreateConversation\",\n            api__pb2.CreateConversationRequest.SerializeToString,\n            api__pb2.CreateConversationResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n        )\n\n    @staticmethod\n    def ContinueConversation(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_stream(\n            request,\n            target,\n            \"/peachdb.BotService/ContinueConversation\",\n            api__pb2.ContinueConversationRequest.SerializeToString,\n            api__pb2.ContinueConversationResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n        )", ""]}
{"filename": "peachdb_grpc/test_api_client.py", "chunked_list": ["import asyncio\n\nimport api_pb2  # type: ignore\nimport api_pb2_grpc  # type: ignore\nimport grpc  # type: ignore\n\n\ndef create_bot(stub: api_pb2_grpc.BotServiceStub):\n    repsonse = stub.CreateBot(\n        api_pb2.CreateBotRequest(\n            bot_id=\"grpc_test_4\", documents=[\"Hello\", \"World\"], system_prompt=\"Answer questions about this document.\"\n        )\n    )\n\n    return repsonse", "\n\nasync def create_conversation(stub: api_pb2_grpc.BotServiceStub):\n    responses = stub.CreateConversation(\n        api_pb2.CreateConversationRequest(bot_id=\"grpc_test_4\", query=\"What is this document about?\")\n    )\n\n    async for response in responses:\n        print(\"Create conversation response:\")\n        print(response)", "        print(\"Create conversation response:\")\n        print(response)\n\n\nasync def continue_conversation(stub: api_pb2_grpc.BotServiceStub):\n    responses = stub.ContinueConversation(\n        api_pb2.ContinueConversationRequest(\n            bot_id=\"grpc_test_4\",\n            conversation_id=\"335700eb-0b4b-4094-a317-f03efe71e09e\",\n            query=\"What is this document about?\",", "            conversation_id=\"335700eb-0b4b-4094-a317-f03efe71e09e\",\n            query=\"What is this document about?\",\n        )\n    )\n\n    async for response in responses:\n        # TODO: make streamable given openAI.\n        print(\"Continue conversation response:\")\n        print(response)\n", "        print(response)\n\n\nasync def main() -> None:\n    async with grpc.aio.insecure_channel(\"localhost:50051\") as channel:\n        # async with grpc.aio.insecure_channel(\"1.tcp.ngrok.io:24448\") as channel:\n        stub = api_pb2_grpc.BotServiceStub(channel)\n\n        # print(await create_bot(stub))\n        # await create_conversation(stub)", "        # print(await create_bot(stub))\n        # await create_conversation(stub)\n        await continue_conversation(stub)\n\n        # await asyncio.gather(\n        #     create_conversation(stub),\n        #     continue_conversation(stub)\n        # )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"]}
{"filename": "peachdb/validators.py", "chunked_list": ["import os\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\n\nfrom peachdb.embedder.utils import S3File, is_s3_uri\n\n\ndef validate_embedding_generator(engine: str):\n    supported_engines = [\"sentence_transformer_L12\", \"imagebind\", \"openai_ada\"]\n    assert (\n        engine in supported_engines\n    ), f\"Unsupported embedding generator. Currently supported engines are: {', '.join(supported_engines)}\"", "\ndef validate_embedding_generator(engine: str):\n    supported_engines = [\"sentence_transformer_L12\", \"imagebind\", \"openai_ada\"]\n    assert (\n        engine in supported_engines\n    ), f\"Unsupported embedding generator. Currently supported engines are: {', '.join(supported_engines)}\"\n\n\ndef validate_distance_metric(metric: str):\n    supported_metrics = [\"l2\", \"cosine\"]\n    assert (\n        metric in supported_metrics\n    ), f\"Unsupported distance metric. The metric should be one of the following: {', '.join(supported_metrics)}\"", "def validate_distance_metric(metric: str):\n    supported_metrics = [\"l2\", \"cosine\"]\n    assert (\n        metric in supported_metrics\n    ), f\"Unsupported distance metric. The metric should be one of the following: {', '.join(supported_metrics)}\"\n\n\ndef validate_embedding_backend(backend: str):\n    supported_backends = [\"exact_cpu\", \"exact_gpu\", \"approx\"]\n    assert (\n        backend in supported_backends\n    ), f\"Unsupported embedding backend. The backend should be one of the following: {', '.join(supported_backends)}\"", "\n\ndef validate_csv_path(csv_path: str):\n    assert csv_path, \"csv_path parameter is missing. Please provide a valid local file path or an S3 URI.\"\n\n    # TODO: in case of S3 URI, check if the URI exists\n    if not is_s3_uri(csv_path):\n        assert os.path.exists(\n            csv_path\n        ), f\"The provided csv_path '{csv_path}' does not exist. Please ensure that the path is either a valid local file path or an S3 URI (e.g., s3://path/to/csv).\"", "\n\ndef validate_columns(column_to_embed: str, id_column_name: str, csv_path: str):\n    assert column_to_embed\n    assert id_column_name\n\n    def _check(data: pd.DataFrame):\n        assert column_to_embed in data.columns, f\"column_to_embed parameter is missing in {data.columns}\"\n        assert id_column_name in data.columns, f\"id_column_name parameter is missing in {data.columns}\"\n\n        try:\n            ids = np.array(data[id_column_name].values.tolist()).astype(\"int64\")\n        except:\n            print(\"[red]Only INTEGER datatype is supported for id column right now[/]\")\n\n    if is_s3_uri(csv_path):\n        with S3File(csv_path) as downloaded_csv:\n            data = pd.read_csv(downloaded_csv)\n            _check(data)\n    else:\n        data = pd.read_csv(csv_path)\n        _check(data)", ""]}
{"filename": "peachdb/__init__.py", "chunked_list": ["\"\"\"\nPeachDB Library\n\"\"\"\nimport abc\nimport datetime\nimport os\nimport shelve\nimport shutil\nfrom typing import List, Optional, Tuple\n", "from typing import List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport uvicorn\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.requests import Request\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok  # type: ignore", "from pydantic import BaseModel\nfrom pyngrok import ngrok  # type: ignore\nfrom rich import print\nfrom rich.prompt import Prompt\n\nimport peachdb.utils\nfrom peachdb.backends import get_backend\nfrom peachdb.backends.backend_base import BackendBase\nfrom peachdb.backends.numpy_backend import NumpyBackend\nfrom peachdb.constants import BLOB_STORE, SHELVE_DB", "from peachdb.backends.numpy_backend import NumpyBackend\nfrom peachdb.constants import BLOB_STORE, SHELVE_DB\nfrom peachdb.embedder import EmbeddingProcessor\nfrom peachdb.embedder.utils import Modality, is_s3_uri\nfrom peachdb.validators import (\n    validate_columns,\n    validate_csv_path,\n    validate_distance_metric,\n    validate_embedding_backend,\n    validate_embedding_generator,", "    validate_embedding_backend,\n    validate_embedding_generator,\n)\n\n\nclass QueryResponse(BaseModel):\n    ids: list\n    distances: list\n    metadata: List[dict]\n", "\n\nclass _Base(abc.ABC):\n    @abc.abstractmethod\n    def query(\n        self,\n        query_input: str,\n        modality: Modality,\n        namespace: Optional[str],\n        store_modality: Optional[Modality] = None,\n        top_k: int = 5,\n    ):\n        pass", "\n\nclass EmptyNamespace(ValueError):\n    pass\n\n\nclass PeachDB(_Base):\n    def __init__(\n        self,\n        project_name,\n        embedding_generator: str = \"sentence_transformer_L12\",\n        distance_metric: str = \"cosine\",\n        embedding_backend: str = \"exact_cpu\",\n    ):\n        validate_embedding_generator(embedding_generator)\n        validate_distance_metric(distance_metric)\n        validate_embedding_backend(embedding_backend)\n        super().__init__()\n        self._project_name = project_name\n        self._embedding_generator = embedding_generator\n        self._distance_metric = distance_metric\n        self._embedding_backend = embedding_backend\n\n        with shelve.open(SHELVE_DB) as shelve_db:\n            if self._project_name in shelve_db.keys():\n                assert set(shelve_db[self._project_name].keys()) == set(\n                    [\n                        \"embedding_generator\",\n                        \"exp_compound_csv_path\",\n                        \"query_logs\",\n                        \"upsertion_logs\",\n                        \"distance_metric\",\n                        \"embedding_backend\",\n                        \"lock\",\n                        \"init_logs\",\n                    ]\n                ), \"The project name already exists but the data is corrupted. Please delete the project and try again.\"\n\n                project_info = shelve_db[self._project_name]\n                project_info[\"init_logs\"].append({\"time\": datetime.datetime.now()})\n                shelve_db[project_name] = project_info\n            else:\n                shelve_db[project_name] = {\n                    \"embedding_generator\": embedding_generator,\n                    \"exp_compound_csv_path\": os.path.join(BLOB_STORE, project_name, \"exp_compound.csv\"),\n                    \"query_logs\": [],\n                    \"upsertion_logs\": [],\n                    \"distance_metric\": distance_metric,\n                    \"embedding_backend\": embedding_backend,\n                    \"lock\": False,\n                    \"init_logs\": [{\"time\": datetime.datetime.now()}],\n                }\n                print(f\"[u]PeachDB has been created for project: [bold green]{project_name}[/bold green][/u]\")\n\n        self._db: Optional[BackendBase] = None\n\n    def deploy(self):\n        if self._db is None:\n            self._get_db_backend()\n\n        app = FastAPI()\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n        @app.get(\"/query\", response_model=QueryResponse)\n        async def query_handler(query_input: str, modality: str | Modality, top_k: int = 5):\n            if isinstance(modality, str):\n                modality = Modality(modality)\n            ids, distances, metadata = self.query(query_input, modality=modality, top_k=top_k)\n            return {\n                \"ids\": ids.tolist(),\n                \"distances\": distances.tolist(),\n                \"metadata\": metadata.to_dict(orient=\"records\"),\n            }\n\n        port = 8000\n        url = ngrok.connect(port)\n        print(f\"[green]Public URL: {url}[/green]\")\n        try:\n            uvicorn.run(app, host=\"0.0.0.0\", port=port)\n        except KeyboardInterrupt:\n            self._db.cleanup()\n\n    def query(\n        self,\n        query_input: str,\n        modality: str | Modality,\n        namespace: Optional[str] = None,\n        store_modality: Optional[Modality] = None,\n        top_k: int = 5,\n    ) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n        assert query_input and modality\n        with shelve.open(SHELVE_DB) as shelve_db:\n            project_info = shelve_db[self._project_name]\n            assert not project_info[\"lock\"], \"Please wait for the upsertion to finish before querying.\"\n\n        # TODO: add query logs.\n        if isinstance(modality, str):\n            modality = Modality(modality)\n\n        self._db = self._get_db_backend(namespace)\n        # was originally doing below, but now we just instantiate a new backend which loads everything into memory.\n        # # check insertion logs for any new upsertion, and download locally\n        # self._db.download_data_for_new_upsertions(project_info[\"upsertion_logs\"])\n\n        assert isinstance(self._db, NumpyBackend), \"Only NumpyBackend is supported for now.\"\n\n        ids, distances = self._db.process_query(query=query_input, top_k=top_k, modality=modality)\n        metadata = self._db.fetch_metadata(ids, namespace=namespace)\n\n        return ids, distances, metadata\n\n    # TODO: handle store_modality\n    def _get_db_backend(self, namespace: Optional[str] = None, store_modality: Optional[Modality] = None):\n        with shelve.open(SHELVE_DB) as shelve_db:\n            project_info = shelve_db[self._project_name]\n            metadata_path = project_info[\"exp_compound_csv_path\"]\n\n            upsertions_namespace = [x for x in project_info[\"upsertion_logs\"] if x[\"namespace\"] == namespace]\n\n            if len(upsertions_namespace) < 1:\n                raise EmptyNamespace(\"No embeddings in this namespace! Please upsert data before running your query\")\n\n            upsertion_embedding_dirs = [x[\"embeddings_dir\"] for x in upsertions_namespace]\n            assert (\n                len(set(upsertion_embedding_dirs)) == 1\n            ), \"All upsertions in a namespace must have the same embeddings_dir\"\n\n            last_upsertion = upsertions_namespace[-1]\n\n        embeddings_dir = last_upsertion[\"embeddings_dir\"]\n        id_column_name = last_upsertion[\"id_column_name\"]\n\n        # TODO: fix if we have multiple modalities stored. (#multi-modality)\n        store_modality = store_modality if store_modality is not None else Modality(last_upsertion[\"modality\"])\n\n        return get_backend(\n            embedding_generator=self._embedding_generator,\n            embedding_backend=self._embedding_backend,\n            distance_metric=self._distance_metric,\n            embeddings_dir=embeddings_dir,\n            metadata_path=metadata_path,\n            id_column_name=id_column_name,\n            modality=store_modality,\n        )\n\n    def _upsert(\n        self,\n        csv_path: str,\n        column_to_embed: str,\n        id_column_name: str,\n        modality: Modality,\n        namespace: Optional[str] = None,\n        embeddings_output_s3_bucket_uri: Optional[str] = None,\n        max_rows: Optional[int] = None,\n    ):\n        validate_csv_path(csv_path)\n        validate_columns(column_to_embed, id_column_name, csv_path)\n\n        if is_s3_uri(csv_path):\n            assert (\n                embeddings_output_s3_bucket_uri\n            ), \"Please provide `embeddings_output_s3_bucket_uri` for output embeddings when the `csv_path` is an S3 URI.\"\n\n            assert is_s3_uri(\n                embeddings_output_s3_bucket_uri\n            ), f\"The provided output_bucket_s3_uri {embeddings_output_s3_bucket_uri} is not a S3 URI\"\n\n        shelve_db = shelve.open(SHELVE_DB)\n\n        processor = EmbeddingProcessor(\n            csv_path=csv_path,\n            column_to_embed=column_to_embed,\n            id_column_name=id_column_name,\n            max_rows=max_rows,\n            embedding_model_name=self._embedding_generator,\n            project_name=self._project_name,\n            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n            modality=modality,\n            namespace=namespace,\n        )\n\n        processor.process()\n\n        _save = shelve_db[self._project_name]\n        _save[\"upsertion_logs\"].append(\n            {\n                \"embeddings_dir\": processor.embeddings_output_dir,\n                \"csv_path\": csv_path,\n                \"column_to_embed\": column_to_embed,\n                \"id_column_name\": id_column_name,\n                \"max_rows\": max_rows,\n                \"embeddings_output_s3_bucket_uri\": embeddings_output_s3_bucket_uri,\n                \"modality\": modality.value,\n                \"namespace\": namespace,\n            }\n        )\n        shelve_db[self._project_name] = _save\n        shelve_db.close()\n\n        peachdb.utils.sync_cache_dir_s3()\n\n    def upsert_text(\n        self,\n        csv_path: str,\n        column_to_embed: str,\n        id_column_name: str,\n        namespace: Optional[str] = None,\n        embeddings_output_s3_bucket_uri: Optional[str] = None,\n        max_rows: Optional[int] = None,\n    ):\n        self._upsert(\n            csv_path=csv_path,\n            column_to_embed=column_to_embed,\n            id_column_name=id_column_name,\n            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n            modality=Modality.TEXT,\n            namespace=namespace,\n            max_rows=max_rows,\n        )\n\n    def upsert_audio(\n        self,\n        csv_path: str,\n        column_to_embed: str,\n        id_column_name: str,\n        embeddings_output_s3_bucket_uri: Optional[str] = None,\n        max_rows: Optional[int] = None,\n    ):\n        self._upsert(\n            csv_path=csv_path,\n            column_to_embed=column_to_embed,\n            id_column_name=id_column_name,\n            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n            max_rows=max_rows,\n            modality=Modality.AUDIO,\n        )\n\n    def upsert_image(\n        self,\n        csv_path: str,\n        column_to_embed: str,\n        id_column_name: str,\n        embeddings_output_s3_bucket_uri: Optional[str] = None,\n        max_rows: Optional[int] = None,\n    ):\n        self._upsert(\n            csv_path=csv_path,\n            column_to_embed=column_to_embed,\n            id_column_name=id_column_name,\n            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n            max_rows=max_rows,\n            modality=Modality.IMAGE,\n        )\n\n    @staticmethod\n    def delete(project_name: str):\n        db = shelve.open(SHELVE_DB)\n        if project_name not in db.keys():\n            print(f\"Project: {project_name} does not exist.\")\n            return\n\n        answer = Prompt.ask(f\"[bold red]Would you like to delete the project: {project_name}? (y/N)[/]\")\n        delete_project = answer.lower() == \"y\"\n        if delete_project:\n            del db[project_name]\n            project_dir = os.path.join(BLOB_STORE, project_name)\n            if os.path.exists(project_dir):\n                shutil.rmtree(project_dir)\n\n        print(f\"[green]Successfully deleted project: {project_name}[/]\")\n\n    @staticmethod\n    def _ensure_unique_project_name(project_name: str):\n        with shelve.open(SHELVE_DB) as db:\n            assert (\n                project_name not in db.keys()\n            ), f\"The project name '{project_name}' already exists. Please choose a unique name.\"", ""]}
{"filename": "peachdb/utils.py", "chunked_list": ["import subprocess\n\nfrom peachdb.constants import _DISK_CACHE_DIR\n\n\ndef sync_cache_dir_s3():\n    subprocess.Popen([\"aws\", \"s3\", \"sync\", _DISK_CACHE_DIR, \"s3://metavoice-vector-db/peachdb-cache/\"])\n"]}
{"filename": "peachdb/constants.py", "chunked_list": ["import os\n\n_USER_DIR = os.path.expanduser(\"~/\")\n_DISK_CACHE_DIR = os.path.join(_USER_DIR, \".peachdb\")\nos.makedirs(_DISK_CACHE_DIR, exist_ok=True)\n\nSHELVE_DB = f\"{_DISK_CACHE_DIR}/db\"\nBLOB_STORE = f\"{_DISK_CACHE_DIR}/blobs\"\nCACHED_REQUIREMENTS_TXT = f\"{_DISK_CACHE_DIR}/requirements.txt\"\nBOTS_DB = f\"{_DISK_CACHE_DIR}/bots.db\"", "CACHED_REQUIREMENTS_TXT = f\"{_DISK_CACHE_DIR}/requirements.txt\"\nBOTS_DB = f\"{_DISK_CACHE_DIR}/bots.db\"\nCONVERSATIONS_DB = f\"{_DISK_CACHE_DIR}/conversations.db\"\n\nGIT_REQUIREMENTS_TXT = \"https://raw.githubusercontent.com/peach-db/peachdb/master/requirements.txt\"\n"]}
{"filename": "peachdb/bots/qa.py", "chunked_list": ["import dotenv\n\ndotenv.load_dotenv()\n\nimport shelve\nimport tempfile\nfrom typing import Iterator, Optional, Union\nfrom uuid import uuid4\n\nimport openai", "\nimport openai\nimport pandas as pd\n\nfrom peachdb import PeachDB\nfrom peachdb.constants import BOTS_DB, CONVERSATIONS_DB, SHELVE_DB\n\n\nclass ConversationNotFoundError(ValueError):\n    pass", "class ConversationNotFoundError(ValueError):\n    pass\n\n\nclass UnexpectedGPTRoleResponse(ValueError):\n    pass\n\n\ndef _validate_embedding_model(embedding_model: str):\n    assert embedding_model in [\"openai_ada\"]", "def _validate_embedding_model(embedding_model: str):\n    assert embedding_model in [\"openai_ada\"]\n\n\ndef _validate_llm_model(llm_model):\n    assert llm_model in [\"gpt-3.5-turbo\", \"gpt-4\"]\n\n\ndef _process_input_data(namespace, ids: list[str], texts: list[str], metadatas_dict) -> pd.DataFrame:\n    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n    if metadatas_dict is not None:\n        assert all(isinstance(m, dict) for m in metadatas_dict), \"All metadata must be dicts\"\n        assert len(set([str(x.keys()) for x in metadatas_dict])) == 1, \"All metadata must have the same keys\"\n\n        # convert metadata from input format to one we can create a dataframe from.\n        metadatas_dict = {key: [metadata[key] for metadata in metadatas_dict] for key in metadatas_dict[0].keys()}\n\n        assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n        assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\n        if namespace is not None:\n            assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"\n    else:\n        metadatas_dict = {}\n\n    if namespace is None:\n        metadatas_dict[\"namespace\"] = [None] * len(ids)\n    else:\n        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\n    df = pd.DataFrame(\n        data={\n            \"ids\": ids,\n            \"texts\": texts,\n            **metadatas_dict,\n        }\n    )\n\n    return df", "def _process_input_data(namespace, ids: list[str], texts: list[str], metadatas_dict) -> pd.DataFrame:\n    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n    if metadatas_dict is not None:\n        assert all(isinstance(m, dict) for m in metadatas_dict), \"All metadata must be dicts\"\n        assert len(set([str(x.keys()) for x in metadatas_dict])) == 1, \"All metadata must have the same keys\"\n\n        # convert metadata from input format to one we can create a dataframe from.\n        metadatas_dict = {key: [metadata[key] for metadata in metadatas_dict] for key in metadatas_dict[0].keys()}\n\n        assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n        assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\n        if namespace is not None:\n            assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"\n    else:\n        metadatas_dict = {}\n\n    if namespace is None:\n        metadatas_dict[\"namespace\"] = [None] * len(ids)\n    else:\n        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\n    df = pd.DataFrame(\n        data={\n            \"ids\": ids,\n            \"texts\": texts,\n            **metadatas_dict,\n        }\n    )\n\n    return df", "\n\ndef _peachdb_upsert_wrapper(peach_db_instance, peach_db_project_name: str, namespace, ids, texts, metadatas_dict):\n    new_data_df = _process_input_data(namespace, ids, texts, metadatas_dict)\n\n    with tempfile.NamedTemporaryFile(suffix=f\"{uuid4()}.csv\") as tmp:\n        new_data_df.to_csv(tmp.name, index=False)  # TODO: check it won't cause an override.\n\n        peach_db_instance.upsert_text(\n            csv_path=tmp.name,\n            column_to_embed=\"texts\",\n            id_column_name=\"ids\",\n            # TODO: below is manually set, this might cause issues!\n            embeddings_output_s3_bucket_uri=\"s3://metavoice-vector-db/deployed_solution/\",\n            namespace=namespace,\n        )\n\n    with shelve.open(SHELVE_DB) as db:\n        project_info = db[peach_db_project_name]\n        new_data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n\n    return True", "\n\nclass BadBotInputError(ValueError):\n    pass\n\n\nclass QABot:\n    def __init__(\n        self,\n        bot_id: str,\n        embedding_model: Optional[str] = None,\n        llm_model_name: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n    ):\n        with shelve.open(BOTS_DB) as db:\n            if bot_id in db:\n                if system_prompt is not None:\n                    raise BadBotInputError(\n                        \"System prompt cannot be changed for existing bot. Maybe you want to create a new bot?\"\n                    )\n                if embedding_model is not None:\n                    raise BadBotInputError(\n                        \"Embedding model cannot be changed for existing bot. Maybe you want to create a new bot?\"\n                    )\n                if llm_model_name is not None:\n                    raise BadBotInputError(\n                        \"LLM model cannot be changed for existing bot. Maybe you want to create a new bot?\"\n                    )\n                self._peachdb_project_id = db[bot_id][\"peachdb_project_id\"]\n                self._embedding_model = db[bot_id][\"embedding_model\"]\n                self._llm_model_name = db[bot_id][\"llm_model_name\"]\n                self._system_prompt = db[bot_id][\"system_prompt\"]\n            else:\n                if system_prompt is None:\n                    raise BadBotInputError(\"System prompt must be specified for new bot.\")\n                if embedding_model is None:\n                    raise BadBotInputError(\"Embedding model must be specified for new bot.\")\n                if llm_model_name is None:\n                    raise BadBotInputError(\"LLM model must be specified for new bot.\")\n\n                self._peachdb_project_id = f\"{uuid4()}_{bot_id}\"\n                self._embedding_model = embedding_model\n                self._llm_model_name = llm_model_name\n                self._system_prompt = system_prompt\n\n                db[bot_id] = {\n                    \"peachdb_project_id\": self._peachdb_project_id,\n                    \"embedding_model\": self._embedding_model,\n                    \"llm_model_name\": self._llm_model_name,\n                    \"system_prompt\": self._system_prompt,\n                }\n\n        _validate_embedding_model(self._embedding_model)\n        _validate_llm_model(self._llm_model_name)\n\n        self.peach_db = PeachDB(\n            project_name=self._peachdb_project_id,\n            embedding_generator=self._embedding_model,\n        )\n\n        if self._llm_model_name in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n            self._llm_model = lambda messages, stream: openai.ChatCompletion.create(\n                messages=[{\"role\": \"system\", \"content\": self._system_prompt}] + messages,\n                model=self._llm_model_name,\n                stream=stream,\n            )\n        else:\n            raise ValueError(f\"Unknown/Unsupported LLM model: {self._llm_model_name}\")\n\n    def add_data(self, documents: list[str]):\n        _peachdb_upsert_wrapper(\n            peach_db_instance=self.peach_db,\n            peach_db_project_name=self._peachdb_project_id,\n            namespace=None,\n            ids=[str(i) for i in range(len(documents))],\n            texts=documents,\n            metadatas_dict=None,\n        )\n\n    def _llm_response(\n        self, conversation_id: str, messages: list[dict[str, str]], stream: bool = False\n    ) -> Iterator[tuple[str, str]]:\n        \"\"\"\n        Responds to the given messages with the LLM model. Additionally, it appends to the shelve db the current conversation (After response has been returned from GPT).\n        \"\"\"\n        response = self._llm_model(messages=messages, stream=stream)\n\n        if stream:\n            response_str = \"\"\n\n            for resp in response:\n                delta = resp.choices[0].delta\n\n                if \"role\" in delta:\n                    if delta.role != \"assistant\":\n                        raise UnexpectedGPTRoleResponse(f\"Expected assistant response, got {delta.role} response.\")\n\n                if \"content\" in delta:\n                    response_str += delta[\"content\"]\n                    yield conversation_id, delta[\"content\"]\n\n                # keep updating shelve with current conversation.\n                with shelve.open(CONVERSATIONS_DB) as db:\n                    db[conversation_id] = messages + [{\"role\": \"assistant\", \"content\": response_str}]\n        else:\n            response_message = response[\"choices\"][0][\"message\"]\n            if response_message.role != \"assistant\":\n                raise UnexpectedGPTRoleResponse(f\"Expected assistant response, got {response_message.role} response.\")\n\n            with shelve.open(CONVERSATIONS_DB) as db:\n                db[conversation_id] = messages + [response_message]\n\n            yield conversation_id, response_message[\"content\"]\n\n    def _create_unique_conversation_id(self) -> str:\n        # get conversation id not in shelve.\n        id = str(uuid4())\n        with shelve.open(CONVERSATIONS_DB) as db:\n            while id in db:\n                id = str(uuid4())\n\n        return id\n\n    def create_conversation_with_query(\n        self, query: str, top_k: int = 3, stream: bool = False\n    ) -> Iterator[tuple[str, str]]:\n        _, _, context_metadata = self.peach_db.query(query, top_k=top_k, modality=\"text\")\n        assert \"texts\" in context_metadata\n\n        contextual_query = \"Use the below snippets to answer the subsequent questions. If the answer can't be found, write \\\"I don't know.\\\"\"\n        for text in context_metadata[\"texts\"]:\n            contextual_query += f\"\\n\\nSnippet:\\n{text}\"\n        contextual_query += f\"\\n\\nQuestion:{query}\"\n\n        # add context to query\n        messages = [\n            {\"role\": \"user\", \"content\": contextual_query},\n        ]\n\n        conversation_id = self._create_unique_conversation_id()\n\n        if stream:\n            for x in self._llm_response(conversation_id, messages, stream=True):\n                yield x\n        else:\n            for x in self._llm_response(conversation_id, messages, stream=False):\n                yield x\n\n    def continue_conversation_with_query(\n        self, conversation_id: str, query: str, top_k: int = 3, stream: bool = False\n    ) -> Iterator[str]:\n        with shelve.open(CONVERSATIONS_DB) as db:\n            if conversation_id not in db:\n                raise ConversationNotFoundError(\"Conversation ID not found.\")\n\n            messages = db[conversation_id]\n\n        messages.append({\"role\": \"user\", \"content\": query})\n\n        if stream:\n            for _, response in self._llm_response(conversation_id, messages, stream=True):\n                yield response\n        else:\n            for _, response in self._llm_response(conversation_id, messages, stream=False):\n                yield response", ""]}
{"filename": "peachdb/embedder/openai_ada.py", "chunked_list": ["from dotenv import load_dotenv\n\nload_dotenv()\n\nimport os\n\nimport openai\n\nif os.environ.get(\"OPENAI_API_KEY\") == None:\n    raise ValueError(\"OPENAI_API_KEY environment variable not set\")", "if os.environ.get(\"OPENAI_API_KEY\") == None:\n    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n\n\nclass OpenAIAdaEmbedder:\n    def __init__(self):\n        self.embedder = openai.Embedding\n\n    def calculate_embeddings(self, text_inputs: list) -> list:\n        response = self.embedder.create(input=text_inputs, model=\"text-embedding-ada-002\")\n        embeddings = [data[\"embedding\"] for data in response[\"data\"]]\n\n        return embeddings", "\n\nif __name__ == \"__main__\":\n    embedder = OpenAIAdaEmbedder()\n    import numpy as np\n\n    print(np.max(embedder.calculate_embeddings([\"hello\"] * 5), axis=1))\n    import time\n\n    time.sleep(10)\n    print(np.max(embedder.calculate_embeddings([\"hello\"] * 5), axis=1))", ""]}
{"filename": "peachdb/embedder/__init__.py", "chunked_list": ["import os\nimport shutil\nfrom collections import namedtuple\nfrom typing import List, Optional, Type\n\nimport duckdb\nimport modal\nimport pandas as pd\nimport pyarrow as pa  # type: ignore\nimport pyarrow.parquet as pq  # type: ignore", "import pyarrow as pa  # type: ignore\nimport pyarrow.parquet as pq  # type: ignore\nfrom rich import print\n\nimport peachdb.embedder.containers.base\nfrom peachdb.constants import BLOB_STORE\nfrom peachdb.embedder.utils import Modality, S3File, is_s3_uri\n\nChunk = namedtuple(\"Chunk\", [\"texts_or_paths\", \"ids\"])\n", "Chunk = namedtuple(\"Chunk\", [\"texts_or_paths\", \"ids\"])\n\n\n# TODO: split into two separate classes LocalEmbeddingsProcessor & S3EmbeddingsProcessor\nclass EmbeddingProcessor:\n    def __init__(\n        self,\n        csv_path: str,\n        column_to_embed: str,\n        id_column_name: str,\n        embedding_model_name: str,\n        project_name: str,\n        modality: Modality,\n        namespace: Optional[str] = None,\n        embeddings_output_s3_bucket_uri: Optional[str] = None,\n        max_rows: Optional[int] = None,\n    ):\n        self._csv_path = csv_path\n        self._column_to_embed = column_to_embed\n        self._id_column_name = id_column_name\n        self._embedding_model_name = embedding_model_name\n        self._max_rows = max_rows\n        self._project_name = project_name\n        self._embeddings_output_s3_bucket_uri = (\n            embeddings_output_s3_bucket_uri.strip(\"/\") + \"/\" if embeddings_output_s3_bucket_uri else None\n        )\n        self._namespace = namespace\n        self._modality = modality\n\n        if self._embedding_model_name == \"sentence_transformer_L12\":\n            from peachdb.embedder.containers.sentence_transformer import SentenceTransformerEmbedder, sbert_stub\n\n            self._embedding_model: Optional[\n                Type[peachdb.embedder.containers.base.EmbeddingModelBase]\n            ] = SentenceTransformerEmbedder\n            self._embedding_model_stub: Optional[modal.Stub] = sbert_stub\n            self._embedding_model_chunk_size = 10000\n        elif self._embedding_model_name == \"imagebind\":\n            from peachdb.embedder.containers.multimodal_imagebind import ImageBindEmbdedder, imagebind_stub\n\n            self._embedding_model = ImageBindEmbdedder\n            self._embedding_model_stub = imagebind_stub\n            self._embedding_model_chunk_size = 1000\n        elif self._embedding_model_name == \"openai_ada\":\n            from peachdb.embedder.openai_ada import OpenAIAdaEmbedder\n\n            self._embedding_model = None\n            self._embedding_model_stub = None\n            # TODO: use this\n            self._embedding_model_chunk_size = 1000\n            self._openai_ada_embedder = OpenAIAdaEmbedder()\n\n        else:\n            raise ValueError(f\"Invalid embedding model name: {self._embedding_model_name}\")\n\n    @property\n    def embeddings_output_dir(self):\n        if is_s3_uri(self._csv_path):\n            return f\"{self._embeddings_output_s3_bucket_uri}{self._project_name}/embeddings/{self._namespace}\"\n\n        dir = f\"{BLOB_STORE}/{self._project_name}/embeddings/{self._namespace}\"\n        os.makedirs(dir, exist_ok=True)\n        return dir\n\n    def process(self):\n        dataset = self._download_and_read_dataset(self._csv_path)\n\n        print(\"[bold]Chunking data into batches...[/bold]\")\n        chunked_data = self._chunk_data(dataset)\n\n        print(\"[bold]Running embedding model on each chunk in parallel...[/bold]\")\n        self._run_model(chunked_data)\n\n    def _download_and_read_dataset(self, csv_path: str) -> str:\n        \"\"\"Supports a local/s3 reference to the csv formatted dataset\"\"\"\n        if not is_s3_uri(self._csv_path):\n            print(\"[bold]Loading data into memory...[/bold]\")\n            # local ref has been provided. make a copy within .peachdb for persistence\n            project_blob_dir = f\"{BLOB_STORE}/{self._project_name}\"\n            os.makedirs(project_blob_dir, exist_ok=True)\n\n            fname = self._csv_path.split(\"/\")[-1]\n            dataset_path = f\"{project_blob_dir}/{self._namespace}_{fname}\"\n            shutil.copy2(self._csv_path, dataset_path)\n            return self._read_dataset(dataset_path)\n        else:\n            print(\"[bold]Downloading data from S3...[/bold]\")\n            with S3File(self._csv_path) as downloaded_dataset:\n                print(\"[bold]Loading data into memory...[/bold]\")\n                return self._read_dataset(downloaded_dataset)\n\n    def _read_dataset(self, dataset_path: str):\n        data = duckdb.read_csv(dataset_path, header=True)\n        sql_query = f\"SELECT {self._column_to_embed}, {self._id_column_name} FROM data\"\n        if self._max_rows:\n            sql_query += f\" LIMIT {self._max_rows}\"\n        return duckdb.sql(sql_query).fetchall()  # NOTE: takes 2 mins 10 seconds for a large dataset\n\n    def _chunk_data(self, fetched_data) -> List[Chunk]:\n        chunk_size = self._embedding_model_chunk_size\n        chunked_data = [fetched_data[i : i + chunk_size] for i in range(0, len(fetched_data), chunk_size)]\n        print(f\"[bold]...{len(fetched_data)} rows were split into {len(chunked_data)} chunks[/bold]\")\n\n        inputs = []\n\n        for chunk in chunked_data:\n            texts_or_paths = []\n            ids = []\n            for text_or_path, id in chunk:\n                texts_or_paths.append(text_or_path)\n                ids.append(str(id))\n            inputs += [Chunk(texts_or_paths, ids)]\n\n        return inputs\n\n    def _run_model(self, chunks: List[Chunk]):\n        if self._embedding_model_name == \"openai_ada\":\n            assert not is_s3_uri(self._csv_path)\n            assert self._modality == Modality.TEXT\n\n            for idx, chunk in enumerate(chunks):\n                embeddings_dict = {}\n                embeddings_dict[\"ids\"] = chunk.ids\n                embeddings_dict[\"text_embeddings\"] = self._openai_ada_embedder.calculate_embeddings(\n                    chunk.texts_or_paths\n                )\n                df = pd.DataFrame(embeddings_dict)\n                result = pa.Table.from_pandas(df)\n\n                fname = self._csv_path.split(\"/\")[-1].split(\".\")[0]\n                pq.write_table(\n                    result, f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\"\n                )\n\n        else:\n            assert self._embedding_model_stub is not None and self._embedding_model is not None\n\n            with self._embedding_model_stub.run():\n                st = self._embedding_model()\n\n                # NOTE: a unique fname here is being used to avoid conflicts in the stored data!\n                # Be careful about changing this logic.\n                fname = self._csv_path.split(\"/\")[-1].split(\".\")[0]\n                input_tuples = [\n                    # expected: (ids, output_path, texts, audio_paths, image_paths, show_progress)\n                    (\n                        chunk.ids,\n                        f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\",\n                        # TODO: enable support of using multiple modalities at the same time here (#multi-modality)\n                        chunk.texts_or_paths if self._modality == Modality.TEXT else None,\n                        chunk.texts_or_paths if self._modality == Modality.AUDIO else None,\n                        chunk.texts_or_paths if self._modality == Modality.IMAGE else None,\n                        True,\n                    )\n                    for idx, chunk in enumerate(chunks)\n                ]\n                results = list(st.calculate_embeddings.starmap(input_tuples))  # type: ignore\n\n                if not is_s3_uri(self._csv_path):\n                    for idx, result in enumerate(results):\n                        pq.write_table(\n                            result, f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\"\n                        )", ""]}
{"filename": "peachdb/embedder/utils.py", "chunked_list": ["import abc\nimport concurrent.futures\nimport datetime\nimport logging\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom enum import Enum\nfrom functools import cache, wraps", "from enum import Enum\nfrom functools import cache, wraps\nfrom types import SimpleNamespace\nfrom typing import List, Optional, Type, Union\n\nimport tqdm  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n\nclass Modality(Enum):\n    TEXT = \"text\"\n    AUDIO = \"audio\"\n    IMAGE = \"image\"", "\n\nclass Modality(Enum):\n    TEXT = \"text\"\n    AUDIO = \"audio\"\n    IMAGE = \"image\"\n\n\ndef handle_s3_download_error(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            message = f\"Failed to download {args[0]} from S3. Error: {str(e)}\"\n            logger.exception(message)\n            raise FileNotFoundError(message)\n\n    return wrapper", "def handle_s3_download_error(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            message = f\"Failed to download {args[0]} from S3. Error: {str(e)}\"\n            logger.exception(message)\n            raise FileNotFoundError(message)\n\n    return wrapper", "\n\n@cache\ndef _verify_aws_cli_installed() -> None:\n    try:\n        subprocess.check_output([\"aws\", \"--version\"], stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError:\n        raise EnvironmentError(\"AWS CLI not installed. Please install it and try again.\")\n\n\nclass S3Entity(metaclass=abc.ABCMeta):\n    \"\"\"Abstract base class for S3 Entities\"\"\"\n\n    def __init__(self, s3_path: str):\n        _verify_aws_cli_installed()\n        self.s3_path = s3_path\n        self.temp_resource: Optional[Union[tempfile._TemporaryFileWrapper, tempfile.TemporaryDirectory]] = None\n\n    @abc.abstractmethod\n    def download(self):\n        pass\n\n    @abc.abstractmethod\n    def cleanup(self):\n        pass\n\n    def __enter__(self) -> str:\n        self.download()\n\n        assert self.temp_resource is not None\n        return self.temp_resource.name\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n    @handle_s3_download_error\n    def _download_from_s3(self, command: str) -> None:\n        assert self.temp_resource is not None\n\n        with subprocess.Popen(\n            [\"aws\", \"s3\", command, self.s3_path, self.temp_resource.name],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            bufsize=1,\n            universal_newlines=True,\n        ) as download_process:\n            assert download_process.stdout is not None\n            for line in download_process.stdout:\n                print(\"\\r\" + line.strip().ljust(120), end=\"\", flush=True)\n            print()", "\n\nclass S3Entity(metaclass=abc.ABCMeta):\n    \"\"\"Abstract base class for S3 Entities\"\"\"\n\n    def __init__(self, s3_path: str):\n        _verify_aws_cli_installed()\n        self.s3_path = s3_path\n        self.temp_resource: Optional[Union[tempfile._TemporaryFileWrapper, tempfile.TemporaryDirectory]] = None\n\n    @abc.abstractmethod\n    def download(self):\n        pass\n\n    @abc.abstractmethod\n    def cleanup(self):\n        pass\n\n    def __enter__(self) -> str:\n        self.download()\n\n        assert self.temp_resource is not None\n        return self.temp_resource.name\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n    @handle_s3_download_error\n    def _download_from_s3(self, command: str) -> None:\n        assert self.temp_resource is not None\n\n        with subprocess.Popen(\n            [\"aws\", \"s3\", command, self.s3_path, self.temp_resource.name],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            bufsize=1,\n            universal_newlines=True,\n        ) as download_process:\n            assert download_process.stdout is not None\n            for line in download_process.stdout:\n                print(\"\\r\" + line.strip().ljust(120), end=\"\", flush=True)\n            print()", "\n\nclass S3Files(S3Entity):\n    def __init__(self, s3_paths: list[str]):\n        _verify_aws_cli_installed()\n        self.s3_paths = s3_paths\n        self.temp_resources: Optional[List[tempfile._TemporaryFileWrapper]] = None\n\n    @handle_s3_download_error\n    def copy_file(self, path, resource_name) -> None:\n        command = [\"aws\", \"s3\", \"cp\", path, resource_name.name]\n\n        with subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            bufsize=1,\n            universal_newlines=True,\n        ) as download_process:\n            assert download_process.stdout is not None\n            for line in download_process.stdout:\n                pass\n\n    def _download_from_s3(self) -> None:\n        assert self.temp_resources is not None\n\n        print(\"CPU count: \", os.cpu_count())\n\n        futures = []\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.s3_paths)) as executor:\n            for path, resource in tqdm.tqdm(\n                zip(self.s3_paths, self.temp_resources), total=len(self.s3_paths), desc=\"Scheduling download threads\"\n            ):\n                futures.append(executor.submit(self.copy_file, path, resource))\n\n            for future in tqdm.tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(futures),\n                desc=\"Waiting for threads to finish\",\n            ):\n                future.result()\n\n    def cleanup(self):\n        [x.cleanup() for x in self.temp_resources]\n\n    def download(self) -> list[str]:\n        self.temp_resources = [tempfile.NamedTemporaryFile(delete=True) for _ in tqdm.tqdm(self.s3_paths)]\n        self._download_from_s3()\n        return [x.name for x in self.temp_resources]", "\n\nclass S3File(S3Entity):\n    \"\"\"Represents an S3 File\"\"\"\n\n    def download(self) -> str:\n        self.temp_resource = tempfile.NamedTemporaryFile(delete=True)\n        self._download_from_s3(\"cp\")\n        return self.temp_resource.name\n\n    def cleanup(self):\n        self.temp_resource.close()", "\n\nclass S3Folder(S3Entity):\n    \"\"\"Represents an S3 Folder\"\"\"\n\n    def download(self) -> str:\n        self.temp_resource = tempfile.TemporaryDirectory()\n        self._download_from_s3(\"sync\")\n        return self.temp_resource.name\n\n    def cleanup(self):\n        self.temp_resource.cleanup()", "\n\ndef create_unique_id() -> str:\n    now = datetime.datetime.now()\n    now_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n    return now_str + \"_\" + str(uuid.uuid4())\n\n\ndef create_unique_parquet_name() -> str:\n    return f\"{create_unique_id()}.parquet\"", "\ndef create_unique_parquet_name() -> str:\n    return f\"{create_unique_id()}.parquet\"\n\n\ndef is_s3_uri(path: str) -> bool:\n    return \"s3://\" in path\n"]}
{"filename": "peachdb/embedder/containers/sentence_transformer.py", "chunked_list": ["from typing import Optional, Union\n\nimport modal\nimport pyarrow as pa  # type: ignore\n\nfrom peachdb.embedder.containers.base import EmbeddingModelBase, base_container_image, modal_compute_spec_decorator\nfrom peachdb.embedder.models.sentence_transformer import SentenceTransformerModel\n\nSENTENCE_TRANSFORMER_BATCH_SIZE = 64\n", "SENTENCE_TRANSFORMER_BATCH_SIZE = 64\n\nsbert_stub = modal.Stub(\"SBERT\")\n\n\ndef download_model():\n    SentenceTransformerModel.download_model()\n\n\nimage = base_container_image.run_function(download_model)", "\nimage = base_container_image.run_function(download_model)\n\n\n@modal_compute_spec_decorator(stub=sbert_stub, image=image)\nclass SentenceTransformerEmbedder(EmbeddingModelBase):\n    def __enter__(self):\n        self.model = SentenceTransformerModel()\n\n    def _calculate_text_embeddings(self, texts, show_progress_bar: bool):\n        return self.model.encode_texts(\n            texts,\n            batch_size=SENTENCE_TRANSFORMER_BATCH_SIZE,\n            show_progress_bar=show_progress_bar,\n        )\n\n    # TODO: are defining these functions necessary?\n    def _calculate_audio_embeddings(self, audio_paths, show_progress_bar: bool):\n        raise NotImplementedError\n\n    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool):\n        raise NotImplementedError\n\n    @property\n    def _can_take_text_input(cls):\n        return True\n\n    @property\n    def _can_take_audio_input(cls):\n        return False\n\n    @property\n    def _can_take_image_input(cls):\n        # return True - once implemented!\n        return False\n\n    # We need to rewrite this function in all the inherited class so we can use the @modal method decorator.\n    # TODO: check if above statement is true / if we can factor this out.\n    @modal.method()\n    def calculate_embeddings(  # type: ignore\n        self,\n        ids: list,\n        output_path: str,\n        texts: Optional[list] = None,\n        audio_paths: Optional[list] = None,\n        image_paths: Optional[list] = None,\n        show_progress_bar: bool = False,\n    ) -> Union[None, pa.Table]:\n        return super().calculate_embeddings(\n            ids=ids,\n            output_path=output_path,\n            texts=texts,\n            audio_paths=audio_paths,\n            image_paths=image_paths,\n            show_progress_bar=show_progress_bar,\n        )", "\n\n# We have a function here instead of putting it into __main__ so that `modal shell` works\n@sbert_stub.function(image=image)\ndef test(s3_bucket_path: str):\n    st = SentenceTransformerEmbedder()\n    embeddings = st.calculate_embeddings.call(\n        texts=[\"hello\", \"world\"],\n        ids=[1, 2],\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\nif __name__ == \"__main__\":\n    # Run as \"python sentence_transformer.py s3://<bucket_name>/test_mainfn/\"\n    import sys\n\n    with sbert_stub.run():\n        test.call(sys.argv[1])\n", ""]}
{"filename": "peachdb/embedder/containers/multimodal_imagebind.py", "chunked_list": ["from typing import Optional, Union\n\nimport modal\nimport numpy as np\nimport pyarrow as pa  # type: ignore\n\nfrom peachdb.embedder.containers.base import EmbeddingModelBase, base_container_image, modal_compute_spec_decorator\nfrom peachdb.embedder.models.multimodal_imagebind import ImageBindModel\nfrom peachdb.embedder.utils import S3File, is_s3_uri\n", "from peachdb.embedder.utils import S3File, is_s3_uri\n\nIMAGEBIND_BATCH_SIZE = 1024\n\nimagebind_stub = modal.Stub(\"ImageBind\")\n\n\ndef download_model():\n    ImageBindModel.download_model()\n", "\n\nimage = base_container_image.run_function(download_model)\n\n\n@modal_compute_spec_decorator(stub=imagebind_stub, image=image)\nclass ImageBindEmbdedder(EmbeddingModelBase):\n    def __enter__(self):\n        self.model = ImageBindModel()\n\n    def _calculate_text_embeddings(self, texts, show_progress_bar: bool) -> np.ndarray:\n        return self.model.encode_texts(texts, IMAGEBIND_BATCH_SIZE, show_progress_bar)\n\n    def _calculate_audio_embeddings(self, audio_paths, show_progress_bar: bool) -> np.ndarray:\n        # TODO: add handling of different batch sizes to EmbeddingProcessor. (#multi-modality)\n        return self.model.encode_audio(audio_paths, IMAGEBIND_BATCH_SIZE // 8, show_progress_bar)\n\n    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool) -> np.ndarray:\n        # TODO: add handling of different batch sizes to EmbeddingProcessor. (#multi-modality)\n        return self.model.encode_image(image_paths, IMAGEBIND_BATCH_SIZE // 8, show_progress_bar)\n\n    @property\n    def _can_take_text_input(cls) -> bool:\n        return True\n\n    @property\n    def _can_take_audio_input(cls) -> bool:\n        return True\n\n    @property\n    def _can_take_image_input(cls) -> bool:\n        return True\n\n    # We need to rewrite this function in all the inherited class so we can use the @modal method decorator.\n    # TODO: check if above statement is true / if we can factor this out.\n    @modal.method()\n    def calculate_embeddings(  # type: ignore\n        self,\n        ids: list,\n        output_path: str,\n        texts: Optional[list] = None,\n        audio_paths: Optional[list] = None,\n        image_paths: Optional[list] = None,\n        show_progress_bar: bool = False,\n    ) -> Union[None, pa.Table]:\n        return super().calculate_embeddings(\n            ids=ids,\n            output_path=output_path,\n            texts=texts,\n            audio_paths=audio_paths,\n            image_paths=image_paths,\n            show_progress_bar=show_progress_bar,\n        )", "\n\n### Test functions ###\n# We have a function here instead of putting it into __main__ so that `modal shell` works\n@imagebind_stub.function(image=image)\ndef test_texts(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    embeddings = ib.calculate_embeddings.call(\n        texts=[\"hello\", \"world\"],\n        ids=[1, 2],\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\n@imagebind_stub.function(image=image)\ndef test_audio(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    embeddings = ib.calculate_embeddings.call(\n        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 2,\n        ids=list(range(2)),\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\n@imagebind_stub.function(image=image)\ndef test_texts_audio(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    embeddings = ib.calculate_embeddings.call(\n        texts=[\"hello\", \"world\"],\n        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 2,\n        ids=list(range(2)),\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\n@imagebind_stub.function(image=image)\ndef test_texts_audio_batched(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    embeddings = ib.calculate_embeddings.call(\n        texts=[\"hello\"] * 1025,\n        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 1025,\n        ids=list(range(1025)),\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\n@imagebind_stub.function(image=image)\ndef test_audio_batched(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    embeddings = ib.calculate_embeddings.call(\n        # texts=[\"hello\"] * 1025,\n        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 10000,\n        ids=list(range(1025)),\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\n@imagebind_stub.function(image=image)\ndef test_image(s3_bucket_path: str):\n    ib = ImageBindEmbdedder()\n    NUM_IMAGES = 1025\n    embeddings = ib.calculate_embeddings.call(\n        image_paths=[\"s3://metavoice-vector-db/bird_image.jpg\"] * NUM_IMAGES,\n        ids=list(range(NUM_IMAGES)),\n        output_path=s3_bucket_path,\n        show_progress_bar=True,\n    )", "\n\nif __name__ == \"__main__\":\n    # Run as \"python multimodal_imagebind.py s3://<bucket_name>/test_mainfn/\"\n    import sys\n\n    with imagebind_stub.run():\n        # test_texts.call(sys.argv[1])\n        # test_audio.call(sys.argv[1])\n        # test_texts_audio.call(sys.argv[1])\n        # test_texts_audio_batched.call(sys.argv[1])\n        # test_audio_batched.call(sys.argv[1])\n        test_image.call(sys.argv[1])", ""]}
{"filename": "peachdb/embedder/containers/base.py", "chunked_list": ["import os\nimport subprocess\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport boto3  # type: ignore\nimport modal\nimport numpy as np\nimport pandas as pd", "import numpy as np\nimport pandas as pd\nimport pyarrow as pa  # type: ignore\nimport pyarrow.parquet as pq  # type: ignore\nimport requests\n\nfrom peachdb.constants import CACHED_REQUIREMENTS_TXT, GIT_REQUIREMENTS_TXT\nfrom peachdb.embedder.utils import S3File, S3Files, is_s3_uri\n\n# Logic to get a requirements.txt file for the base image when package is on PyPI.", "\n# Logic to get a requirements.txt file for the base image when package is on PyPI.\ndev_requirements_path = Path(__file__).parents[3] / \"requirements.txt\"\nif os.path.exists(dev_requirements_path):\n    requirements_path: Union[Path, str] = dev_requirements_path\nelse:\n    response = requests.get(GIT_REQUIREMENTS_TXT)\n\n    response.raise_for_status()\n\n    with open(CACHED_REQUIREMENTS_TXT, \"w\") as f:\n        f.write(response.text)\n\n    requirements_path = CACHED_REQUIREMENTS_TXT", "\n# Grab AWS credentials from ~/.aws/credentials using boto3.\n# This code is written this way as it ends up getting executed inside the container creation process as well,\n# and so ends up with empty credentials. Doing it this way means empty credentials get set in the container creation process,\n# but the actual model serving process will have the correct credentials.\n# TODO: fix scope for bad error handling here. really want to error if these don't exist locally!\n# But we don't have stub here to run `is_inside`.\nsecrets = []\n_aws_boto_session = boto3.Session()\nif _aws_boto_session != None:\n    _aws_credentials = _aws_boto_session.get_credentials()\n    if _aws_credentials != None:\n        secrets = [\n            modal.Secret.from_dict(\n                {\"AWS_ACCESS_KEY_ID\": _aws_credentials.access_key, \"AWS_SECRET_ACCESS_KEY\": _aws_credentials.secret_key}\n            ),\n        ]", "_aws_boto_session = boto3.Session()\nif _aws_boto_session != None:\n    _aws_credentials = _aws_boto_session.get_credentials()\n    if _aws_credentials != None:\n        secrets = [\n            modal.Secret.from_dict(\n                {\"AWS_ACCESS_KEY_ID\": _aws_credentials.access_key, \"AWS_SECRET_ACCESS_KEY\": _aws_credentials.secret_key}\n            ),\n        ]\n", "\n# Requirements for the base image of models we want to serve.\n# We don't add the requirements.txt here as that contains requirements across ALL our models.\nbase_container_image = (\n    modal.Image.debian_slim()\n    .apt_install(\"curl\", \"zip\", \"git\")\n    .run_commands(\n        \"curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\",\n        \"unzip awscliv2.zip\",\n        \"./aws/install\",", "        \"unzip awscliv2.zip\",\n        \"./aws/install\",\n        \"rm -rf awscliv2.zip aws\",\n    )\n    .pip_install_from_requirements(str(requirements_path))\n    # Container creation flow from inside a pypi package doesn't pick up it's own files.\n    .pip_install(\"git+https://github.com/peach-db/peachdb\")\n)\n\nmodal_compute_spec_decorator = lambda stub, image: stub.cls(", "\nmodal_compute_spec_decorator = lambda stub, image: stub.cls(\n    image=image,\n    gpu=\"T4\",\n    timeout=400,\n    secrets=secrets,\n    concurrency_limit=500,\n)\n\n\nclass EmbeddingModelBase(ABC):\n    @abstractmethod\n    def _calculate_text_embeddings(self, texts: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def _calculate_audio_embeddings(self, audio_paths: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @property\n    @abstractmethod\n    def _can_take_text_input(cls) -> bool:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _can_take_audio_input(cls) -> bool:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _can_take_image_input(cls) -> bool:\n        raise NotImplementedError\n\n    def _check_s3_credentials(self):\n        try:\n            subprocess.check_output([\"aws\", \"s3\", \"ls\", \"s3://\"], stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError:\n            raise EnvironmentError(\n                \"AWS CLI not configured. Please set credentials locally using `aws configure` and try again.\"\n            )\n\n    def calculate_embeddings(\n        self,\n        ids: list,\n        output_path: str,\n        texts: Optional[list] = None,\n        audio_paths: Optional[list] = None,\n        image_paths: Optional[list] = None,\n        show_progress_bar: bool = False,\n    ) -> Union[None, pa.Table]:\n        assert (\n            texts is not None or audio_paths is not None or image_paths is not None\n        ), \"Must provide at least one input.\"\n\n        if (\n            is_s3_uri(output_path)\n            # TODO: refactor\n            or (any([is_s3_uri(audio_path) for audio_path in audio_paths]) if audio_paths is not None else False)\n            or (any([is_s3_uri(image_path) for image_path in image_paths]) if image_paths is not None else False)\n        ):\n            self._check_s3_credentials()\n\n        embeddings_dict = {}\n        embeddings_dict[\"ids\"] = ids\n\n        if texts is not None:\n            print(\"Processing text input...\")\n            if self._can_take_text_input:\n                text_embeddings = self._calculate_text_embeddings(texts, show_progress_bar)\n                # TODO: update wherever this variable is used upstream\n                embeddings_dict[\"text_embeddings\"] = text_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take text input.\")\n\n        # TODO: refactor below two if statements.\n        # TODO: think about if we want to error if a modality is not supported by a model, or just\n        # error, and then let things continue.\n        if audio_paths is not None:\n            print(\"Processing audio input...\")\n            if self._can_take_audio_input:\n                assert (\n                    len(set([is_s3_uri(x) for x in audio_paths])) == 1\n                ), \"All audio paths must be either local or S3 paths.\"\n\n                is_s3 = all([is_s3_uri(x) for x in audio_paths])\n\n                if is_s3:\n                    print(\"Downloading audio files from S3... Creating handlers...\")\n                    audio_files_handler = S3Files(audio_paths)\n\n                    print(\"Handlers created, downloading...\")\n                    local_audio_paths = audio_files_handler.download()\n                else:\n                    local_audio_paths = audio_paths\n\n                audio_embeddings = self._calculate_audio_embeddings(local_audio_paths, show_progress_bar)\n                embeddings_dict[\"audio_embeddings\"] = audio_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take audio input.\")\n\n        if image_paths is not None:\n            if self._can_take_image_input:\n                assert (\n                    len(set([is_s3_uri(x) for x in image_paths])) == 1\n                ), \"All image paths must be either local or S3 paths.\"\n\n                is_s3 = all([is_s3_uri(x) for x in image_paths])\n\n                if is_s3:\n                    print(\"Downloading audio files from S3... Creating handlers...\")\n                    image_file_handlers = S3Files(image_paths)\n\n                    print(\"Handlers created, downloading...\")\n                    local_image_paths = image_file_handlers.download()\n                else:\n                    local_image_paths = image_paths\n\n                image_embeddings = self._calculate_image_embeddings(local_image_paths, show_progress_bar)\n                embeddings_dict[\"image_embeddings\"] = image_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take image input.\")\n\n        tmp_output_path = \"/root/embeddings.parquet\"\n\n        df = pd.DataFrame(embeddings_dict)\n        table = pa.Table.from_pandas(df)\n        pq.write_table(table, tmp_output_path)\n\n        if is_s3_uri(output_path):\n            os.system(f\"aws s3 cp {tmp_output_path} {output_path}\")\n            return None\n\n        return table", "\n\nclass EmbeddingModelBase(ABC):\n    @abstractmethod\n    def _calculate_text_embeddings(self, texts: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def _calculate_audio_embeddings(self, audio_paths: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool) -> np.ndarray:\n        pass\n\n    @property\n    @abstractmethod\n    def _can_take_text_input(cls) -> bool:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _can_take_audio_input(cls) -> bool:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _can_take_image_input(cls) -> bool:\n        raise NotImplementedError\n\n    def _check_s3_credentials(self):\n        try:\n            subprocess.check_output([\"aws\", \"s3\", \"ls\", \"s3://\"], stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError:\n            raise EnvironmentError(\n                \"AWS CLI not configured. Please set credentials locally using `aws configure` and try again.\"\n            )\n\n    def calculate_embeddings(\n        self,\n        ids: list,\n        output_path: str,\n        texts: Optional[list] = None,\n        audio_paths: Optional[list] = None,\n        image_paths: Optional[list] = None,\n        show_progress_bar: bool = False,\n    ) -> Union[None, pa.Table]:\n        assert (\n            texts is not None or audio_paths is not None or image_paths is not None\n        ), \"Must provide at least one input.\"\n\n        if (\n            is_s3_uri(output_path)\n            # TODO: refactor\n            or (any([is_s3_uri(audio_path) for audio_path in audio_paths]) if audio_paths is not None else False)\n            or (any([is_s3_uri(image_path) for image_path in image_paths]) if image_paths is not None else False)\n        ):\n            self._check_s3_credentials()\n\n        embeddings_dict = {}\n        embeddings_dict[\"ids\"] = ids\n\n        if texts is not None:\n            print(\"Processing text input...\")\n            if self._can_take_text_input:\n                text_embeddings = self._calculate_text_embeddings(texts, show_progress_bar)\n                # TODO: update wherever this variable is used upstream\n                embeddings_dict[\"text_embeddings\"] = text_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take text input.\")\n\n        # TODO: refactor below two if statements.\n        # TODO: think about if we want to error if a modality is not supported by a model, or just\n        # error, and then let things continue.\n        if audio_paths is not None:\n            print(\"Processing audio input...\")\n            if self._can_take_audio_input:\n                assert (\n                    len(set([is_s3_uri(x) for x in audio_paths])) == 1\n                ), \"All audio paths must be either local or S3 paths.\"\n\n                is_s3 = all([is_s3_uri(x) for x in audio_paths])\n\n                if is_s3:\n                    print(\"Downloading audio files from S3... Creating handlers...\")\n                    audio_files_handler = S3Files(audio_paths)\n\n                    print(\"Handlers created, downloading...\")\n                    local_audio_paths = audio_files_handler.download()\n                else:\n                    local_audio_paths = audio_paths\n\n                audio_embeddings = self._calculate_audio_embeddings(local_audio_paths, show_progress_bar)\n                embeddings_dict[\"audio_embeddings\"] = audio_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take audio input.\")\n\n        if image_paths is not None:\n            if self._can_take_image_input:\n                assert (\n                    len(set([is_s3_uri(x) for x in image_paths])) == 1\n                ), \"All image paths must be either local or S3 paths.\"\n\n                is_s3 = all([is_s3_uri(x) for x in image_paths])\n\n                if is_s3:\n                    print(\"Downloading audio files from S3... Creating handlers...\")\n                    image_file_handlers = S3Files(image_paths)\n\n                    print(\"Handlers created, downloading...\")\n                    local_image_paths = image_file_handlers.download()\n                else:\n                    local_image_paths = image_paths\n\n                image_embeddings = self._calculate_image_embeddings(local_image_paths, show_progress_bar)\n                embeddings_dict[\"image_embeddings\"] = image_embeddings.tolist()\n            else:\n                raise Exception(\"This model cannot take image input.\")\n\n        tmp_output_path = \"/root/embeddings.parquet\"\n\n        df = pd.DataFrame(embeddings_dict)\n        table = pa.Table.from_pandas(df)\n        pq.write_table(table, tmp_output_path)\n\n        if is_s3_uri(output_path):\n            os.system(f\"aws s3 cp {tmp_output_path} {output_path}\")\n            return None\n\n        return table", ""]}
{"filename": "peachdb/embedder/containers/__init__.py", "chunked_list": [""]}
{"filename": "peachdb/embedder/models/sentence_transformer.py", "chunked_list": ["import torch\nfrom sentence_transformers import SentenceTransformer  # type: ignore\n\nfrom peachdb.embedder.models.base import BaseModel\n\nMODEL_NAME = \"all-MiniLM-L12-v2\"\n\n\nclass SentenceTransformerModel(BaseModel):\n    def __init__(self) -> None:\n        self.model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def encode_texts(self, texts, batch_size, show_progress_bar):\n        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=show_progress_bar)\n\n    def encode_audio(self, local_paths, batch_size, show_progress_bar):\n        raise NotImplementedError\n\n    def encode_image(self, local_paths, batch_size, show_progress_bar):\n        raise NotImplementedError\n\n    @staticmethod\n    def download_model():\n        SentenceTransformer(MODEL_NAME, device=\"cpu\")", "class SentenceTransformerModel(BaseModel):\n    def __init__(self) -> None:\n        self.model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def encode_texts(self, texts, batch_size, show_progress_bar):\n        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=show_progress_bar)\n\n    def encode_audio(self, local_paths, batch_size, show_progress_bar):\n        raise NotImplementedError\n\n    def encode_image(self, local_paths, batch_size, show_progress_bar):\n        raise NotImplementedError\n\n    @staticmethod\n    def download_model():\n        SentenceTransformer(MODEL_NAME, device=\"cpu\")", ""]}
{"filename": "peachdb/embedder/models/multimodal_imagebind.py", "chunked_list": ["import numpy as np\nimport torch\nimport tqdm  # type: ignore\nfrom imagebind.data import (  # type: ignore\n    load_and_transform_audio_data,\n    load_and_transform_text,\n    load_and_transform_vision_data,\n)\nfrom imagebind.models import imagebind_model  # type: ignore\nfrom imagebind.models.imagebind_model import ModalityType  # type: ignore", "from imagebind.models import imagebind_model  # type: ignore\nfrom imagebind.models.imagebind_model import ModalityType  # type: ignore\n\nfrom peachdb.embedder.models.base import BaseModel\n\n\nclass ImageBindModel(BaseModel):\n    def __init__(self) -> None:\n        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # TODO: can we factor to base model?\n\n        self.model = imagebind_model.imagebind_huge(pretrained=True).eval().to(self.device)\n\n    # TODO: we want all the encodings in one function so that we can get the embeddings in one go!\n    # Otherwise we are wasting compute. Refactor given this.\n    # HAVE A SINGLE ENCODE FUNCTION!\n\n    def encode_texts(self, texts, batch_size, show_progress_bar) -> np.ndarray:\n        embeddings = []\n        for start_index in tqdm.tqdm(range(0, len(texts), batch_size), desc=\"Batches\", disable=not show_progress_bar):\n            texts_batch = texts[start_index : start_index + batch_size]\n            inputs_batch = {ModalityType.TEXT: load_and_transform_text(texts_batch, self.device)}\n\n            with torch.no_grad():\n                embeddings.append(self.model(inputs_batch)[ModalityType.TEXT].cpu().numpy())\n\n        return np.concatenate(embeddings, axis=0)\n\n    def encode_audio(self, local_paths, batch_size, show_progress_bar) -> np.ndarray:\n        embeddings = []\n        for start_index in tqdm.tqdm(\n            range(0, len(local_paths), batch_size), desc=\"Batches\", disable=not show_progress_bar\n        ):\n            batch_local_paths = local_paths[start_index : start_index + batch_size]\n            batched_inputs = {ModalityType.AUDIO: load_and_transform_audio_data(batch_local_paths, self.device)}\n\n            with torch.no_grad():\n                embeddings.append(self.model(batched_inputs)[ModalityType.AUDIO].cpu().numpy())\n\n        return np.concatenate(embeddings, axis=0)\n\n    def encode_image(self, local_paths, batch_size, show_progress_bar) -> np.ndarray:\n        embeddings = []\n        for start_index in tqdm.tqdm(\n            range(0, len(local_paths), batch_size), desc=\"Batches\", disable=not show_progress_bar\n        ):\n            batch_local_paths = local_paths[start_index : start_index + batch_size]\n            batched_inputs = {ModalityType.VISION: load_and_transform_vision_data(batch_local_paths, self.device)}\n\n            with torch.no_grad():\n                embeddings.append(self.model(batched_inputs)[ModalityType.VISION].cpu().numpy())\n\n        return np.concatenate(embeddings, axis=0)\n\n    @staticmethod\n    def download_model():\n        imagebind_model.imagebind_huge(pretrained=True)", ""]}
{"filename": "peachdb/embedder/models/base.py", "chunked_list": ["import abc\n\n\nclass BaseModel(abc.ABC):\n    @abc.abstractmethod\n    def __init__(self) -> None:\n        pass\n\n    @abc.abstractmethod\n    def encode_texts(self, texts, batch_size, show_progress_bar):\n        pass\n\n    @abc.abstractmethod\n    def encode_audio(self, local_paths, batch_size, show_progress_bar):\n        pass\n\n    @abc.abstractmethod\n    def encode_image(self, local_paths, batch_size, show_progress_bar):\n        pass\n\n    @staticmethod\n    @abc.abstractmethod\n    def download_model():\n        pass", ""]}
{"filename": "peachdb/embedder/models/__init__.py", "chunked_list": [""]}
{"filename": "peachdb/backends/backend_base.py", "chunked_list": ["import abc\nimport dataclasses\nimport os\nfrom typing import Optional\n\nimport duckdb\nimport numpy as np\nimport pandas as pd\nfrom rich import print\n", "from rich import print\n\nimport peachdb.embedder.models.base\nfrom peachdb.embedder.models.multimodal_imagebind import ImageBindModel\nfrom peachdb.embedder.models.sentence_transformer import SentenceTransformerModel\nfrom peachdb.embedder.openai_ada import OpenAIAdaEmbedder\nfrom peachdb.embedder.utils import Modality, S3File, S3Folder, is_s3_uri\n\n\n@dataclasses.dataclass\nclass BackendConfig:\n    embedding_generator: str\n    distance_metric: str\n    embeddings_dir: str\n    metadata_path: str\n    id_column_name: str\n    modality: Modality", "\n@dataclasses.dataclass\nclass BackendConfig:\n    embedding_generator: str\n    distance_metric: str\n    embeddings_dir: str\n    metadata_path: str\n    id_column_name: str\n    modality: Modality\n", "\n\nclass BackendBase(abc.ABC):\n    def __init__(\n        self,\n        backend_config: BackendConfig,\n    ):\n        # TODO: refactor below to clean up\n        embeddings_dir = backend_config.embeddings_dir\n        metadata_path = backend_config.metadata_path\n        embedding_generator = backend_config.embedding_generator\n        distance_metric = backend_config.distance_metric\n        id_column_name = backend_config.id_column_name\n        modality = backend_config.modality\n        self._distance_metric = distance_metric\n        self._id_column_name = id_column_name\n        self._metadata_filepath = self._get_metadata_filepath(metadata_path)\n        self._modality = modality\n        self._embedding_generator = embedding_generator\n\n        self._embeddings, self._ids = self._get_embeddings(embeddings_dir)\n        if len(set(self._ids)) != len(self._ids):\n            raise ValueError(\"Duplicate ids found in the embeddings file.\")\n\n        if self._embedding_generator == \"sentence_transformer_L12\":\n            self._encoder: peachdb.embedder.models.base.BaseModel = SentenceTransformerModel()\n        elif self._embedding_generator == \"imagebind\":\n            self._encoder = ImageBindModel()\n        elif self._embedding_generator == \"openai_ada\":\n            self._openai_encoder = OpenAIAdaEmbedder()\n        else:\n            raise ValueError(f\"Unknown embedding generator: {embedding_generator}\")\n\n    @abc.abstractmethod\n    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n        pass\n\n    def process_query(self, query: str, modality: Modality, top_k: int = 5) -> tuple:\n        print(\"Embedding query...\")\n        if self._embedding_generator == \"openai_ada\":\n            assert modality == Modality.TEXT\n            query_embedding = np.asarray(self._openai_encoder.calculate_embeddings([query])[0:1])\n            return self._process_query(query_embedding, top_k)\n        else:\n            if modality == Modality.TEXT:\n                query_embedding = self._encoder.encode_texts(texts=[query], batch_size=1, show_progress_bar=True)\n            elif modality == Modality.AUDIO:\n                query_embedding = self._encoder.encode_audio(local_paths=[query], batch_size=1, show_progress_bar=True)\n            elif modality == Modality.IMAGE:\n                query_embedding = self._encoder.encode_image(local_paths=[query], batch_size=1, show_progress_bar=True)\n            else:\n                raise ValueError(f\"Unknown modality: {modality}\")\n\n            return self._process_query(query_embedding, top_k)\n\n    def fetch_metadata(self, ids, namespace: Optional[str]) -> pd.DataFrame:\n        print(\"Fetching metadata...\")\n\n        # NOTE: this is a hack, as we keep updating the metadata.\n        data = duckdb.read_csv(self._metadata_filepath, header=True)\n        id_str = \" OR \".join([f\"{self._id_column_name} = '{id}'\" for id in ids])\n        if namespace is None:\n            metadata = duckdb.sql(f\"SELECT * FROM data WHERE {id_str}\").df()\n        else:\n            metadata = duckdb.sql(f\"SELECT * FROM data WHERE ({id_str}) AND (namespace = '{namespace}')\").df()\n\n        return metadata\n\n    def _get_embeddings(self, embeddings_dir: str):\n        if not is_s3_uri(embeddings_dir):\n            return self._load_embeddings(embeddings_dir)\n\n        print(\"[bold]Downloading calculated embeddings...[/bold]\")\n        with S3Folder(embeddings_dir) as tmp_local_embeddings_dir:\n            return self._load_embeddings(tmp_local_embeddings_dir)\n\n    def _load_embeddings(self, embeddings_dir: str) -> tuple:\n        \"\"\"Loads and preprocesses the embeddings from a parquet file.\"\"\"\n        assert os.path.exists(embeddings_dir)\n\n        print(\"[bold]Loading embeddings from parquet file...[/bold]\")\n        df = pd.read_parquet(embeddings_dir, \"pyarrow\")\n\n        print(\"[bold]Converting embeddings to numpy array...[/bold]\")\n        if self._modality == Modality.TEXT:\n            # TODO: these keys name are used in embedder.containers.base, so we should refactor\n            embeddings = np.array(df[\"text_embeddings\"].values.tolist()).astype(\"float32\")\n        elif self._modality == Modality.AUDIO:\n            embeddings = np.array(df[\"audio_embeddings\"].values.tolist()).astype(\"float32\")\n        elif self._modality == Modality.IMAGE:\n            embeddings = np.array(df[\"image_embeddings\"].values.tolist()).astype(\"float32\")\n        else:\n            raise ValueError(f\"Unknown modality: {self._modality}\")\n        ids = np.asarray(df[\"ids\"].apply(str).values.tolist())\n        return embeddings, ids\n\n    def _get_metadata_filepath(self, metadata_path: str) -> str:\n        if not is_s3_uri(metadata_path):\n            return metadata_path\n\n        print(\"[bold]Downloading metadata file...[/bold]\")\n        self._metadata_fileref = S3File(metadata_path)\n        return self._metadata_fileref.download()\n\n    def cleanup(self):\n        if is_s3_uri(self._metadata_path):\n            self._metadata_fileref.cleanup()\n\n        if is_s3_uri(self._embeddings_dir):\n            self._embeddings_dir.cleanup()", ""]}
{"filename": "peachdb/backends/torch_backend.py", "chunked_list": ["import numpy as np\nimport torch\n\nfrom peachdb.backends.backend_base import BackendBase, BackendConfig\nfrom peachdb.embedder.utils import Modality\n\n\ndef _check_dims(query_embed: torch.Tensor, embeds: torch.Tensor):\n    if query_embed.dim() == 1:\n        query_embed = query_embed.unsqueeze(0)\n    elif query_embed.dim() == 2:\n        if query_embed.size(0) != 1:\n            raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n    else:\n        raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n\n    if embeds.dim() != 2:\n        raise ValueError(\"embeds should be a 2-D matrix\")\n\n    return query_embed, embeds", "\n\ndef l2(query_embed: torch.Tensor, embeds: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Calculate l2 distance between a query embedding and a set of embeddings.\n    \"\"\"\n    query_embed, embeds = _check_dims(query_embed, embeds)\n\n    return torch.norm(query_embed - embeds, dim=1)\n", "\n\ndef cosine(query_embed: torch.Tensor, embeds: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Can be used to compute cosine \"distance\" between any number of query embeddings\n    and a set of embeddings.\n    result[i, j] = 1 - torch.dot(query_embed[i], embeds[j])\n    \"\"\"\n    query_embed, embeds = _check_dims(query_embed, embeds)\n\n    return (\n        1\n        - torch.mm(query_embed, embeds.t())\n        / (torch.norm(query_embed, dim=1).unsqueeze(1) * torch.norm(embeds, dim=1).unsqueeze(0))\n    )[0]", "\n\nclass TorchBackend(BackendBase):\n    def __init__(\n        self,\n        backend_config: BackendConfig,\n    ):\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA is required to use TorchDB\")\n\n        super().__init__(\n            backend_config=backend_config,\n        )\n        self.device = torch.device(\"cuda\")\n        self._embeddings = torch.from_numpy(self._embeddings).to(self.device)  # Ensure the tensor is on the GPU\n\n    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n        query_embedding = torch.from_numpy(query_embedding).to(self.device)\n\n        print(\"Calculating distances...\")\n        distances = (\n            l2(query_embedding, self._embeddings)\n            if self._distance_metric == \"l2\"\n            else cosine(query_embedding, self._embeddings)\n        )\n\n        print(\"Getting top results...\")\n        results = torch.argsort(distances)[:top_k].cpu().numpy()\n        return self._ids[results], distances[results].cpu().numpy()", "\n\nif __name__ == \"__main__\":\n    import scipy.spatial.distance as scipy_distance  # type: ignore\n    from sentence_transformers.util import cos_sim as st_cos_sim  # type: ignore\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for dim in [3, 384, 1536]:\n        a = torch.rand(dim, device=device)\n        b = torch.rand(3, dim, device=device)\n\n        # cosine\n        cosine_result = cosine(a, b)\n        for i in range(b.shape[0]):\n            np.testing.assert_allclose(\n                scipy_distance.cosine(a.cpu().numpy(), b[i].cpu().numpy()),\n                cosine_result[i].cpu().numpy(),\n                rtol=1e-4,\n            )\n            np.testing.assert_allclose(\n                1 - st_cos_sim(a.cpu().numpy(), b[i].cpu().numpy()).numpy(),\n                cosine_result[i].cpu().numpy(),\n                rtol=1e-4,\n            )\n\n        # l2\n        l2_result = l2(a, b)\n        for i in range(b.shape[0]):\n            np.testing.assert_allclose(\n                scipy_distance.euclidean(a.cpu().numpy(), b[i].cpu().numpy()),\n                l2_result[i].cpu().numpy(),\n                rtol=1e-4,\n            )", ""]}
{"filename": "peachdb/backends/__init__.py", "chunked_list": ["from typing import Dict, Union\n\nfrom peachdb.backends.backend_base import BackendBase, BackendConfig\nfrom peachdb.backends.hnsw_backend import HNSWBackend\nfrom peachdb.backends.numpy_backend import NumpyBackend\nfrom peachdb.backends.torch_backend import TorchBackend\nfrom peachdb.embedder.utils import Modality\n\n\ndef get_backend(\n    embedding_generator: str,\n    distance_metric: str,\n    embedding_backend: str,\n    embeddings_dir: str,\n    metadata_path: str,\n    id_column_name: str,\n    modality: Modality,\n) -> BackendBase:\n    backend_config = BackendConfig(\n        embeddings_dir=embeddings_dir,\n        metadata_path=metadata_path,\n        embedding_generator=embedding_generator,\n        distance_metric=distance_metric,\n        id_column_name=id_column_name,\n        modality=modality,\n    )\n\n    if embedding_backend == \"exact_cpu\":\n        return NumpyBackend(backend_config)\n    elif embedding_backend == \"exact_gpu\":\n        return TorchBackend(backend_config)\n    elif embedding_backend == \"approx\":\n        return HNSWBackend(backend_config)\n    else:\n        raise ValueError(f\"Unknown value for embedding_backend, provided: {embedding_backend}\")", "\ndef get_backend(\n    embedding_generator: str,\n    distance_metric: str,\n    embedding_backend: str,\n    embeddings_dir: str,\n    metadata_path: str,\n    id_column_name: str,\n    modality: Modality,\n) -> BackendBase:\n    backend_config = BackendConfig(\n        embeddings_dir=embeddings_dir,\n        metadata_path=metadata_path,\n        embedding_generator=embedding_generator,\n        distance_metric=distance_metric,\n        id_column_name=id_column_name,\n        modality=modality,\n    )\n\n    if embedding_backend == \"exact_cpu\":\n        return NumpyBackend(backend_config)\n    elif embedding_backend == \"exact_gpu\":\n        return TorchBackend(backend_config)\n    elif embedding_backend == \"approx\":\n        return HNSWBackend(backend_config)\n    else:\n        raise ValueError(f\"Unknown value for embedding_backend, provided: {embedding_backend}\")", ""]}
{"filename": "peachdb/backends/numpy_backend.py", "chunked_list": ["import os\nfrom typing import Tuple\n\nimport duckdb\nimport numpy as np\nimport pandas as pd\nfrom rich import print\n\nfrom peachdb.backends.backend_base import BackendBase\n", "from peachdb.backends.backend_base import BackendBase\n\n\ndef _check_dims(query_embed: np.ndarray, embeds: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    if query_embed.ndim == 1:\n        query_embed = query_embed[np.newaxis, :]\n    elif query_embed.ndim == 2:\n        if query_embed.shape[0] != 1:\n            raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n    else:\n        raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n\n    if embeds.ndim != 2:\n        raise ValueError(\"embeds should be a 2-D matrix\")\n\n    return query_embed, embeds", "\n\ndef l2(query_embed: np.ndarray, embeds: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate l2 distance between a query embedding and a set of embeddings.\n    \"\"\"\n    query_embed, embeds = _check_dims(query_embed, embeds)\n\n    return np.linalg.norm(query_embed - embeds, axis=1)\n", "\n\ndef cosine(query_embed: np.ndarray, embeds: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Can be used to compute cosine \"distance\" between any number of query embeddings\n    and a set of embeddings.\n    result[i, j] = 1 - np.dot(query_embed[i], embeds[j])\n    \"\"\"\n    query_embed, embeds = _check_dims(query_embed, embeds)\n\n    return (1 - (query_embed @ embeds.T) / (np.linalg.norm(query_embed, axis=1) * np.linalg.norm(embeds, axis=1)))[0]", "\n\nclass NumpyBackend(BackendBase):\n    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n        print(\"Calculating distances...\")\n        distances = (\n            l2(query_embedding, self._embeddings)\n            if self._distance_metric == \"l2\"\n            else cosine(query_embedding, self._embeddings)\n        )\n\n        print(\"Getting top results...\")\n        results = np.argsort(distances)[:top_k]\n        return self._ids[results], distances[results]\n\n    def download_data_for_new_upsertions(self, upsertion_logs: list):\n        # TODO: add test that we're not loading any extra data!\n        # TODO: this is likely going to be quite slow?\n        self._embeddings, self._ids = self._get_embeddings(upsertion_logs[-1][\"embeddings_dir\"])", "\n\nif __name__ == \"__main__\":\n    import scipy.spatial.distance as scipy_distance  # type: ignore\n    from sentence_transformers.util import cos_sim as st_cos_sim  # type: ignore\n\n    for dim in [3, 384, 1536]:\n        a = np.random.rand(dim)\n        b = np.random.rand(3, dim)\n\n        # cosine\n        cosine_result = cosine(a, b)\n        for i in range(b.shape[0]):\n            np.testing.assert_allclose(scipy_distance.cosine(a, b[i]), cosine_result[i])\n            np.testing.assert_allclose(1 - st_cos_sim(a, b[i]).numpy(), cosine_result[i])\n\n        # l2\n        l2_result = l2(a, b)\n        for i in range(b.shape[0]):\n            np.testing.assert_allclose(scipy_distance.euclidean(a, b[i]), l2_result[i])", ""]}
{"filename": "peachdb/backends/hnsw_backend.py", "chunked_list": ["from typing import Tuple\n\nimport hnswlib  # type: ignore\nimport numpy as np\nfrom rich import print\n\nfrom peachdb.backends.backend_base import BackendBase, BackendConfig\nfrom peachdb.embedder.utils import Modality\n\n\nclass HNSWBackend(BackendBase):\n    def __init__(\n        self,\n        backend_config: BackendConfig,\n    ):\n        super().__init__(\n            backend_config=backend_config,\n        )\n        if self._embeddings.ndim != 2:\n            raise ValueError(\"embeddings should be a 2-D matrix\")\n\n        self._dim = self._embeddings.shape[1]\n        # create hnsw index.\n        self._hnsw_index = hnswlib.Index(space=self._distance_metric, dim=self._dim)\n\n        self._max_elements = self._embeddings.shape[0]\n        # initialise index.\n        # TODO: fix to support multiple upserts. (#multiple-upserts)\n        self._hnsw_index.init_index(\n            max_elements=self._max_elements,\n            ef_construction=min(200, self._embeddings.shape[0]),  # default param\n            M=16,  # default param\n            random_seed=100,\n        )\n\n        # add data points to index.\n        print(\"[bold]Adding data points to index...[/bold]\")\n        self._hnsw_index.add_items(self._embeddings, self._ids)\n\n        # set hnsw ef param\n        self._hnsw_index.set_ef(max(200, self._embeddings.shape[0]))\n\n    def _process_query(self, query_embedding, top_k: int = 5):\n        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n        if query_embedding.ndim != 1 and not (query_embedding.ndim == 2 and query_embedding.shape[0] == 1):\n            raise ValueError(\"query_embedding should be a vector or a matrix with one row\")\n\n        print(\"Getting top results...\")\n        labels, distances = self._hnsw_index.knn_query(query_embedding, k=top_k)\n        return labels[0], distances[0]", "\n\nclass HNSWBackend(BackendBase):\n    def __init__(\n        self,\n        backend_config: BackendConfig,\n    ):\n        super().__init__(\n            backend_config=backend_config,\n        )\n        if self._embeddings.ndim != 2:\n            raise ValueError(\"embeddings should be a 2-D matrix\")\n\n        self._dim = self._embeddings.shape[1]\n        # create hnsw index.\n        self._hnsw_index = hnswlib.Index(space=self._distance_metric, dim=self._dim)\n\n        self._max_elements = self._embeddings.shape[0]\n        # initialise index.\n        # TODO: fix to support multiple upserts. (#multiple-upserts)\n        self._hnsw_index.init_index(\n            max_elements=self._max_elements,\n            ef_construction=min(200, self._embeddings.shape[0]),  # default param\n            M=16,  # default param\n            random_seed=100,\n        )\n\n        # add data points to index.\n        print(\"[bold]Adding data points to index...[/bold]\")\n        self._hnsw_index.add_items(self._embeddings, self._ids)\n\n        # set hnsw ef param\n        self._hnsw_index.set_ef(max(200, self._embeddings.shape[0]))\n\n    def _process_query(self, query_embedding, top_k: int = 5):\n        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n        if query_embedding.ndim != 1 and not (query_embedding.ndim == 2 and query_embedding.shape[0] == 1):\n            raise ValueError(\"query_embedding should be a vector or a matrix with one row\")\n\n        print(\"Getting top results...\")\n        labels, distances = self._hnsw_index.knn_query(query_embedding, k=top_k)\n        return labels[0], distances[0]", ""]}
