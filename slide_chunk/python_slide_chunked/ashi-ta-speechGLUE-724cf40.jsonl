{"filename": "hub.py", "chunked_list": ["# Copyleft (c), Speech Lab, NTU, Taiwan\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# This code only add three upstream models based on the following code:\n# https://github.com/s3prl/s3prl/blob/v0.4.10/s3prl/hub.py\n\nfrom s3prl.downstream.timit_phone.hubconf import timit_posteriorgram\nfrom s3prl.upstream.apc.hubconf import *\nfrom s3prl.upstream.ast.hubconf import *\nfrom s3prl.upstream.audio_albert.hubconf import *\nfrom s3prl.upstream.baseline.hubconf import *", "from s3prl.upstream.audio_albert.hubconf import *\nfrom s3prl.upstream.baseline.hubconf import *\nfrom s3prl.upstream.byol_a.hubconf import *\nfrom s3prl.upstream.byol_s.hubconf import *\nfrom s3prl.upstream.cpc.hubconf import *\nfrom s3prl.upstream.data2vec.hubconf import *\nfrom s3prl.upstream.decoar2.hubconf import *\nfrom s3prl.upstream.decoar.hubconf import *\nfrom s3prl.upstream.decoar_layers.hubconf import *\nfrom s3prl.upstream.distiller.hubconf import *", "from s3prl.upstream.decoar_layers.hubconf import *\nfrom s3prl.upstream.distiller.hubconf import *\nfrom s3prl.upstream.example.hubconf import *\nfrom s3prl.upstream.hf_hubert.hubconf import *\nfrom s3prl.upstream.hf_wav2vec2.hubconf import *\nfrom s3prl.upstream.hubert.hubconf import *\nfrom s3prl.upstream.lighthubert.hubconf import *\nfrom s3prl.upstream.log_stft.hubconf import *\nfrom s3prl.upstream.mae_ast.hubconf import *\nfrom s3prl.upstream.mockingjay.hubconf import *", "from s3prl.upstream.mae_ast.hubconf import *\nfrom s3prl.upstream.mockingjay.hubconf import *\nfrom s3prl.upstream.mos_prediction.hubconf import *\nfrom s3prl.upstream.npc.hubconf import *\nfrom s3prl.upstream.pase.hubconf import *\nfrom s3prl.upstream.passt.hubconf import *\nfrom s3prl.upstream.roberta.hubconf import *\nfrom s3prl.upstream.ssast.hubconf import *\nfrom s3prl.upstream.tera.hubconf import *\nfrom s3prl.upstream.unispeech_sat.hubconf import *", "from s3prl.upstream.tera.hubconf import *\nfrom s3prl.upstream.unispeech_sat.hubconf import *\nfrom s3prl.upstream.vggish.hubconf import *\nfrom s3prl.upstream.vq_apc.hubconf import *\nfrom s3prl.upstream.vq_wav2vec.hubconf import *\nfrom s3prl.upstream.wav2vec2.hubconf import *\nfrom s3prl.upstream.wav2vec.hubconf import *\nfrom s3prl.upstream.wavlm.hubconf import *\nfrom s3prl.upstream.hf_nlp_ssl.hubconf import *\nfrom s3prl.upstream.hf_speechssl_no_pretrained_weights.hubconf import *", "from s3prl.upstream.hf_nlp_ssl.hubconf import *\nfrom s3prl.upstream.hf_speechssl_no_pretrained_weights.hubconf import *\nfrom s3prl.upstream.embedding.hubconf import *\n\n\ndef options(only_registered_ckpt: bool = False):\n    all_options = []\n    for name, value in globals().items():\n        torch_hubconf_policy = not name.startswith(\"_\") and callable(value)\n        if torch_hubconf_policy and name != \"options\":\n            if only_registered_ckpt and (\n                name.endswith(\"_local\")\n                or name.endswith(\"_url\")\n                or name.endswith(\"_gdriveid\")\n                or name.endswith(\"_custom\")\n            ):\n                continue\n            all_options.append(name)\n\n    return all_options", ""]}
{"filename": "downstream/glue/model.py", "chunked_list": ["import torch.nn as nn\n\n\nclass SequenceClassifierWithCLSPooling(nn.Module):\n    def __init__(self, input_dim, output_dim, pooling_dim=None, dropout=0.1, **kwargs):\n        super(SequenceClassifierWithCLSPooling, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.pooling_dim = pooling_dim\n        if pooling_dim is None:\n            pooling_dim = input_dim\n        else:\n            self.pooling_linear = nn.Linear(input_dim, pooling_dim)\n            self.pooling_activation = nn.Tanh()\n        self.classifier = nn.Linear(pooling_dim, output_dim)\n\n    def forward(self, features, features_len=None):\n        # features: BxTxF\n        # The first token output corresponding to [CLS] token for the BERT model\n        cls_tensor = features[:, 0, :]\n        if self.pooling_dim is not None:\n            cls_tensor = self.pooling_linear(cls_tensor)\n            cls_tensor = self.pooling_activation(cls_tensor)\n        cls_tensor = self.dropout(cls_tensor)\n        logits = self.classifier(cls_tensor)\n        return logits, None", ""]}
{"filename": "downstream/glue/expert.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\n# This code follows the downstream interface of S3PRL\n# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\nfrom pathlib import Path\n", "from pathlib import Path\n\nimport evaluate\nimport torch\nimport torch.nn as nn\nfrom torch.distributed import is_initialized\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom ..model import *", "\nfrom ..model import *\nfrom .dataset import GLUEDataset\nfrom .model import *\n\ntask_to_metrics = {\n    \"cola\": (\"matthews_correlation\", None),\n    \"mnli\": (\"accuracy\", None),\n    \"mrpc\": (\"accuracy\", \"f1\"),\n    \"qnli\": (\"accuracy\", None),", "    \"mrpc\": (\"accuracy\", \"f1\"),\n    \"qnli\": (\"accuracy\", None),\n    \"qqp\": (\"accuracy\", \"f1\"),\n    \"rte\": (\"accuracy\", None),\n    \"sst2\": (\"accuracy\", None),\n    \"stsb\": (\"pearson\", \"spearmanr\"),\n    \"wnli\": (\"accuracy\", None),\n}\n\n\nclass DownstreamExpert(nn.Module):\n    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n        super(DownstreamExpert, self).__init__()\n        self.upstream_dim = upstream_dim\n        self.datarc = downstream_expert[\"datarc\"]\n        self.modelrc = downstream_expert[\"modelrc\"]\n        self.expdir = expdir\n        self.upstream_ckpt = kwargs[\"upstream_ckpt\"]\n\n        # define a task\n        self.glue_task = self.datarc[\"glue_task\"]\n        self.is_regression = self.glue_task == \"stsb\"\n        if not self.is_regression:\n            self.objective = nn.CrossEntropyLoss()\n            self.num_class = GLUEDataset(\n                \"train\", upstream_ckpt=self.upstream_ckpt, **self.datarc\n            ).num_class\n        else:\n            self.objective = nn.MSELoss()\n            self.num_class = 1\n            print(f\"{self.glue_task} will be executed as a regression task\")\n\n        model_cls = eval(self.modelrc[\"select\"])\n        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n        projector_dim = self.modelrc.get(\"projector_dim\", None)\n        if projector_dim is not None:\n            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n            model_input_dim = projector_dim\n        else:\n            self.projector = None\n            model_input_dim = upstream_dim\n        self.model = model_cls(\n            input_dim=model_input_dim,\n            output_dim=self.num_class,\n            **model_conf,\n        )\n\n        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n        if self.normalize:\n            print(\"Use Tanh normalization\")\n            self.norm_act_fn = nn.Tanh()\n\n        self.dropout = self.modelrc.get(\"dropout\", None)\n        if self.dropout is not None:\n            print(\"Use dropout after projection\")\n            self.dropout = nn.Dropout(self.dropout)\n\n        self.metric = evaluate.load(\"glue\", self.glue_task)\n        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.glue_task]\n        self.expdir = expdir\n        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\n    def _get_train_dataloader(self, dataset):\n        sampler = DistributedSampler(dataset) if is_initialized() else None\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"train_batch_size\"],\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _get_eval_dataloader(self, dataset):\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"eval_batch_size\"],\n            shuffle=False,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    # Interface\n    def get_dataloader(self, split):\n        if not hasattr(self, f\"{split}_dataset\"):\n            setattr(\n                self,\n                f\"{split}_dataset\",\n                GLUEDataset(split, upstream_ckpt=self.upstream_ckpt, **self.datarc),\n            )\n\n        if split == \"train\":\n            return self._get_train_dataloader(self.train_dataset)\n        else:\n            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\n    # Interface\n    def forward(self, mode, features, labels, filenames, records, **kwargs):\n        device = features[0].device\n        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n            device=device\n        )\n\n        features = pad_sequence(features, batch_first=True)\n        if self.projector is not None:\n            features = self.projector(features)\n        if self.normalize:\n            features = self.norm_act_fn(features)\n        if self.dropout is not None:\n            features = self.dropout(features)\n        predicted, _ = self.model(features, features_len)\n\n        if not self.is_regression:\n            labels = torch.LongTensor(labels).to(features.device).view(-1)\n            predicted = predicted.view(-1, self.num_class)\n            predicted_id_value = torch.argmax(predicted, dim=-1)\n        else:\n            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n            predicted = predicted.squeeze()\n            predicted_id_value = predicted\n\n        loss = self.objective(predicted, labels)\n\n        records[\"loss\"].append(loss.item())\n        records[\"filename\"] += filenames\n        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n        records[\"truth\"] += labels.cpu().flatten().tolist()\n\n        return loss\n\n    def dump_prediction(self, outpath, filename, pred, label, step=0):\n        with open(outpath, \"w\") as file:\n            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n            file.writelines(line)\n\n    # interface\n    def log_records(self, mode, records, logger, global_step, **kwargs):\n        dev_update1 = False\n        dev_update2 = False\n        save_names = []\n\n        # loss related\n        values = records[\"loss\"]\n        loss_average = torch.FloatTensor(values).mean().item()\n        logger.add_scalar(\n            f\"glue-{self.glue_task}/{mode}-loss\",\n            loss_average,\n            global_step=global_step,\n        )\n\n        # score related\n        results = self.metric.compute(\n            predictions=records[\"predict\"], references=records[\"truth\"]\n        )\n        print(f\"{mode}: {results}\")\n        for k in [self.metric_keys1, self.metric_keys2]:\n            if k is None:\n                continue\n            result = results[k]\n            logger.add_scalar(\n                f\"glue-{self.glue_task}/{mode}-{k}\",\n                result,\n                global_step=global_step,\n            )\n            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n                if mode == \"dev\":\n                    if result > self.best_metric1_score and k == self.metric_keys1:\n                        dev_update1 = True\n                        self.best_metric1_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n                    if result > self.best_metric2_score and k == self.metric_keys2:\n                        dev_update2 = True\n                        self.best_metric2_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n        if mode == \"test\":\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n            )\n        elif mode == \"dev\" and dev_update1:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n        elif mode == \"dev\" and dev_update2:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n\n        return save_names", "\n\nclass DownstreamExpert(nn.Module):\n    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n        super(DownstreamExpert, self).__init__()\n        self.upstream_dim = upstream_dim\n        self.datarc = downstream_expert[\"datarc\"]\n        self.modelrc = downstream_expert[\"modelrc\"]\n        self.expdir = expdir\n        self.upstream_ckpt = kwargs[\"upstream_ckpt\"]\n\n        # define a task\n        self.glue_task = self.datarc[\"glue_task\"]\n        self.is_regression = self.glue_task == \"stsb\"\n        if not self.is_regression:\n            self.objective = nn.CrossEntropyLoss()\n            self.num_class = GLUEDataset(\n                \"train\", upstream_ckpt=self.upstream_ckpt, **self.datarc\n            ).num_class\n        else:\n            self.objective = nn.MSELoss()\n            self.num_class = 1\n            print(f\"{self.glue_task} will be executed as a regression task\")\n\n        model_cls = eval(self.modelrc[\"select\"])\n        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n        projector_dim = self.modelrc.get(\"projector_dim\", None)\n        if projector_dim is not None:\n            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n            model_input_dim = projector_dim\n        else:\n            self.projector = None\n            model_input_dim = upstream_dim\n        self.model = model_cls(\n            input_dim=model_input_dim,\n            output_dim=self.num_class,\n            **model_conf,\n        )\n\n        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n        if self.normalize:\n            print(\"Use Tanh normalization\")\n            self.norm_act_fn = nn.Tanh()\n\n        self.dropout = self.modelrc.get(\"dropout\", None)\n        if self.dropout is not None:\n            print(\"Use dropout after projection\")\n            self.dropout = nn.Dropout(self.dropout)\n\n        self.metric = evaluate.load(\"glue\", self.glue_task)\n        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.glue_task]\n        self.expdir = expdir\n        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\n    def _get_train_dataloader(self, dataset):\n        sampler = DistributedSampler(dataset) if is_initialized() else None\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"train_batch_size\"],\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _get_eval_dataloader(self, dataset):\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"eval_batch_size\"],\n            shuffle=False,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    # Interface\n    def get_dataloader(self, split):\n        if not hasattr(self, f\"{split}_dataset\"):\n            setattr(\n                self,\n                f\"{split}_dataset\",\n                GLUEDataset(split, upstream_ckpt=self.upstream_ckpt, **self.datarc),\n            )\n\n        if split == \"train\":\n            return self._get_train_dataloader(self.train_dataset)\n        else:\n            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\n    # Interface\n    def forward(self, mode, features, labels, filenames, records, **kwargs):\n        device = features[0].device\n        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n            device=device\n        )\n\n        features = pad_sequence(features, batch_first=True)\n        if self.projector is not None:\n            features = self.projector(features)\n        if self.normalize:\n            features = self.norm_act_fn(features)\n        if self.dropout is not None:\n            features = self.dropout(features)\n        predicted, _ = self.model(features, features_len)\n\n        if not self.is_regression:\n            labels = torch.LongTensor(labels).to(features.device).view(-1)\n            predicted = predicted.view(-1, self.num_class)\n            predicted_id_value = torch.argmax(predicted, dim=-1)\n        else:\n            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n            predicted = predicted.squeeze()\n            predicted_id_value = predicted\n\n        loss = self.objective(predicted, labels)\n\n        records[\"loss\"].append(loss.item())\n        records[\"filename\"] += filenames\n        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n        records[\"truth\"] += labels.cpu().flatten().tolist()\n\n        return loss\n\n    def dump_prediction(self, outpath, filename, pred, label, step=0):\n        with open(outpath, \"w\") as file:\n            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n            file.writelines(line)\n\n    # interface\n    def log_records(self, mode, records, logger, global_step, **kwargs):\n        dev_update1 = False\n        dev_update2 = False\n        save_names = []\n\n        # loss related\n        values = records[\"loss\"]\n        loss_average = torch.FloatTensor(values).mean().item()\n        logger.add_scalar(\n            f\"glue-{self.glue_task}/{mode}-loss\",\n            loss_average,\n            global_step=global_step,\n        )\n\n        # score related\n        results = self.metric.compute(\n            predictions=records[\"predict\"], references=records[\"truth\"]\n        )\n        print(f\"{mode}: {results}\")\n        for k in [self.metric_keys1, self.metric_keys2]:\n            if k is None:\n                continue\n            result = results[k]\n            logger.add_scalar(\n                f\"glue-{self.glue_task}/{mode}-{k}\",\n                result,\n                global_step=global_step,\n            )\n            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n                if mode == \"dev\":\n                    if result > self.best_metric1_score and k == self.metric_keys1:\n                        dev_update1 = True\n                        self.best_metric1_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n                    if result > self.best_metric2_score and k == self.metric_keys2:\n                        dev_update2 = True\n                        self.best_metric2_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n        if mode == \"test\":\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n            )\n        elif mode == \"dev\" and dev_update1:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n        elif mode == \"dev\" and dev_update2:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n\n        return save_names", ""]}
{"filename": "downstream/glue/dataset.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\n# This code follows the downstream interface of S3PRL\n# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\nimport os\n", "import os\n\nimport numpy as np\nimport pandas as pd\nfrom espnet2.bin.tts_inference import Text2Speech\nfrom torch.utils.data.dataset import Dataset\nfrom transformers import AutoTokenizer\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),", "task_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),", "    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\nclass GLUEDataset(Dataset):\n    def __init__(self, split, glue_task, glue_root, upstream_ckpt, **kwargs):\n        super(GLUEDataset, self).__init__()\n\n        self.glue_task = glue_task\n        self.glue_root = glue_root\n        self.split_sets = kwargs[split]\n        self.glue_dir = os.path.join(glue_root, glue_task)\n        self.upstream_ckpt = upstream_ckpt\n\n        assert os.path.isdir(\n            self.glue_dir\n        ), \"Please first run `python downstream/glue_asr/data_prep.py -h` to get TTS version text file.\"\n\n        table_list = []\n        for item in self.split_sets:\n            file_path = os.path.join(self.glue_dir, item, \"data.csv\")\n            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n            table_list.append(pd.read_csv(file_path))\n\n        self.sentence1_key, self.sentence2_key = task_to_keys[self.glue_task]\n        self.df_dataset = pd.concat(table_list)\n        assert len(self.df_dataset) != 0, f\"0 data found for {split}\"\n        self.num_class = len(set(list(self.df_dataset[\"label\"])))\n\n        # tokenizer\n        self.use_phoneme = kwargs.get(\"use_phoneme\", False)\n        if self.use_phoneme:\n            print(\"Use phoneme tokenizer\")\n            # ckpt is the ESPnet TTS model name such as \"kan-bayashi/ljspeech_vits\"\n            text2speech = Text2Speech.from_pretrained(self.upstream_ckpt, device=\"cuda\")\n            self.proc_fn = text2speech.preprocess_fn\n            self.text_name = self.proc_fn.text_name\n        else:\n            use_fast_tokenizer = kwargs.get(\"use_fast_tokenizer\", True)\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.upstream_ckpt,\n                cache_dir=\"data\",\n                use_fast=use_fast_tokenizer,\n            )\n            self.max_seq_length = self.tokenizer.model_max_length\n            # whether to distinguish the first and second sentence\n            self.use_segment_emb = kwargs.get(\"use_segment_emb\", True)\n            # whether to use only text token embedding (if True, not use [CLS], [SEP]...)\n            self.use_only_text_token = kwargs.get(\"use_only_text_token\", False)\n\n    def _x_name(self, index):\n        return self.glue_task + \"_\" + str(index)\n\n    def _load_text(self, index):\n        texts = (\n            (str(self.df_dataset[self.sentence1_key][index]),)\n            if self.sentence2_key is None\n            else (\n                str(self.df_dataset[self.sentence1_key][index]),\n                str(self.df_dataset[self.sentence2_key][index]),\n            )\n        )\n        return texts\n\n    def _load_bpe_token(self, args):\n        if self.use_only_text_token:\n            token_id_seq = self.tokenizer.tokenize(\n                *args, max_length=self.max_seq_length, truncation=True\n            )\n            token_id_seq = np.array(\n                self.tokenizer.convert_tokens_to_ids(token_id_seq)\n            ).reshape(1, 1, -1)\n        else:\n            if self.use_segment_emb:\n                token_id_seq = self.tokenizer(\n                    *args,\n                    max_length=self.max_seq_length,\n                    truncation=True,\n                    return_tensors=\"np\",\n                )\n                # not use attention_mask in order to pad later\n                # keys: ['input_ids', 'token_type_ids', 'attention_mask']\n                if set(token_id_seq.keys()) != set(\n                    [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n                ):\n                    raise ValueError(\n                        f\"Invalid tokenize output keys: {token_id_seq.keys()}\"\n                    )\n                token_id_seq = np.array(\n                    [\n                        token_id_seq[k]\n                        for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n                    ]\n                )  # KxBxT (K=3, B=1)\n            else:\n                token_id_seq = self.tokenizer(\n                    *args,\n                    max_length=self.max_seq_length,\n                    truncation=True,\n                    return_tensors=\"np\",\n                )[\n                    \"input_ids\"\n                ]  # BxT\n                token_id_seq = np.expand_dims(token_id_seq, 0)  # KxBxT (K=1, B=1)\n\n        if token_id_seq.shape[1] != 1:\n            raise ValueError(f\"Invalid batch size ({token_id_seq.shape[1]})\")\n        return token_id_seq[:, 0, :].transpose(1, 0)  # KxT -> TxK\n\n    def _load_phoneme_token(self, args):\n        if self.sentence2_key is None:\n            token_id_seq = self.proc_fn._text_process({self.text_name: args[0]})[\"text\"]\n        else:\n            token_id_seq = np.concatenate(\n                [\n                    self.proc_fn._text_process({self.text_name: args[0]})[\"text\"],\n                    np.array([-1]),\n                    self.proc_fn._text_process({self.text_name: args[1]})[\"text\"],\n                ]\n            )\n        # adding two value for [PAD] and [SEP] token (define [PAD] and [SEP] token as 0 and 1)\n        token_id_seq += 2\n        return token_id_seq  # T\n\n    def __len__(self):\n        return len(self.df_dataset)\n\n    def __getitem__(self, index):\n        label = self.df_dataset[\"label\"][index]\n        filename = self._x_name(index)\n        texts = self._load_text(index)\n        if self.use_phoneme:\n            token_seq = self._load_phoneme_token(texts)\n        else:\n            token_seq = self._load_bpe_token(texts)\n        return token_seq, label, filename\n\n    def collate_fn(self, samples):\n        return zip(*samples)", ""]}
{"filename": "downstream/glue/__init__.py", "chunked_list": [""]}
{"filename": "downstream/speechglue/mk_white_noise.py", "chunked_list": ["import os\nimport sys\n\nimport numpy as np\nimport soundfile\n\nSAMPLE_RATE = 16000\n\nnp.random.seed(seed=0)\n\nif len(sys.argv) == 1:\n    duration_msec = 50\n    use_50ms = True\nelse:\n    duration_msec = int(sys.argv[1])\n    use_50ms = False", "np.random.seed(seed=0)\n\nif len(sys.argv) == 1:\n    duration_msec = 50\n    use_50ms = True\nelse:\n    duration_msec = int(sys.argv[1])\n    use_50ms = False\n\nduration = int(SAMPLE_RATE * duration_msec / 1000)", "\nduration = int(SAMPLE_RATE * duration_msec / 1000)\nsep_sig = np.random.randn(duration)\n# prevent from a saturation\nsep_sig = sep_sig / np.max(np.abs(sep_sig)) * 0.99\n\nif use_50ms:\n    out_path = os.path.join(\"dump\", \"white_noise.wav\")\nelse:\n    out_path = os.path.join(\"dump\", f\"white_noise_{duration_msec}ms.wav\")", "soundfile.write(out_path, sep_sig, SAMPLE_RATE, \"PCM_16\")\n"]}
{"filename": "downstream/speechglue/model.py", "chunked_list": ["import torch.nn as nn\n\n# import the various pooling layers from:\n# https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/model.py\nfrom ..model import *\n\n\nclass SequenceClassifierWithDropout(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        pooling=\"MeanPooling\",\n        activation=\"ReLU\",\n        dropout=0.1,\n        pooling_dim=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.pooling_dim = pooling_dim\n        if pooling_dim is None:\n            pooling_dim = input_dim\n        else:\n            self.pooling_linear = nn.Linear(input_dim, pooling_dim)\n            self.pooling_activation = nn.Tanh()\n        self.pooler = eval(pooling)(input_dim=input_dim, activation=activation)\n        self.classifier = nn.Linear(pooling_dim, output_dim)\n\n    def forward(self, hidden_state, features_len=None):\n\n        pooled_tensor, features_len = self.pooler(hidden_state, features_len)\n        if self.pooling_dim is not None:\n            pooled_tensor = self.pooling_linear(pooled_tensor)\n            pooled_tensor = self.pooling_activation(pooled_tensor)\n        pooled_tensor = self.dropout(pooled_tensor)\n        logit = self.classifier(pooled_tensor)\n        return logit, None", ""]}
{"filename": "downstream/speechglue/expert.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\n# This code follows the downstream interface of S3PRL\n# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\nfrom pathlib import Path\n", "from pathlib import Path\n\nimport evaluate\nimport torch\nimport torch.nn as nn\nfrom torch.distributed import is_initialized\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom ..model import *", "\nfrom ..model import *\nfrom .dataset import SpeechGLUEDataset\nfrom .model import *\n\ntask_to_metrics = {\n    \"cola\": (\"matthews_correlation\", None),\n    \"mnli\": (\"accuracy\", None),\n    \"mrpc\": (\"accuracy\", \"f1\"),\n    \"qnli\": (\"accuracy\", None),", "    \"mrpc\": (\"accuracy\", \"f1\"),\n    \"qnli\": (\"accuracy\", None),\n    \"qqp\": (\"accuracy\", \"f1\"),\n    \"rte\": (\"accuracy\", None),\n    \"sst2\": (\"accuracy\", None),\n    \"stsb\": (\"pearson\", \"spearmanr\"),\n    \"wnli\": (\"accuracy\", None),\n}\n\n\nclass DownstreamExpert(nn.Module):\n    \"\"\"\n    Used to handle downstream-specific operations\n    eg. downstream forward, metric computation, contents to log\n    \"\"\"\n\n    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n        super(DownstreamExpert, self).__init__()\n        self.upstream_dim = upstream_dim\n        self.datarc = downstream_expert[\"datarc\"]\n        self.modelrc = downstream_expert[\"modelrc\"]\n        self.expdir = expdir\n\n        # define a task\n        self.speechglue_task = self.datarc[\"speechglue_task\"]\n        self.is_regression = self.speechglue_task == \"stsb\"\n        if not self.is_regression:\n            self.objective = nn.CrossEntropyLoss()\n            self.num_class = SpeechGLUEDataset(\"train\", **self.datarc).num_class\n        else:\n            self.objective = nn.MSELoss()\n            self.num_class = 1\n            print(f\"{self.speechglue_task} will be executed as a regression task\")\n\n        model_cls = eval(self.modelrc[\"select\"])\n        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n        projector_dim = self.modelrc.get(\"projector_dim\", None)\n        if projector_dim is not None:\n            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n            model_input_dim = projector_dim\n        else:\n            self.projector = None\n            model_input_dim = upstream_dim\n\n        self.model = model_cls(\n            input_dim=model_input_dim,\n            output_dim=self.num_class,\n            **model_conf,\n        )\n\n        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n        if self.normalize:\n            print(\"Use Tanh normalization\")\n            self.norm_act_fn = nn.Tanh()\n\n        self.dropout = self.modelrc.get(\"dropout\", None)\n        if self.dropout is not None:\n            print(\"Use dropout after projection\")\n            self.dropout = nn.Dropout(self.dropout)\n\n        self.late_concat = self.datarc.get(\"late_concat\", False)\n\n        self.metric = evaluate.load(\"glue\", self.speechglue_task)\n        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.speechglue_task]\n        self.expdir = expdir\n        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\n    def _get_train_dataloader(self, dataset):\n        sampler = DistributedSampler(dataset) if is_initialized() else None\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"train_batch_size\"],\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _get_eval_dataloader(self, dataset):\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"eval_batch_size\"],\n            shuffle=False,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    # Interface\n    def get_dataloader(self, split):\n        if not hasattr(self, f\"{split}_dataset\"):\n            setattr(self, f\"{split}_dataset\", SpeechGLUEDataset(split, **self.datarc))\n\n        if split == \"train\":\n            return self._get_train_dataloader(self.train_dataset)\n        else:\n            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\n    # Interface\n    def forward(self, mode, features, labels, filenames, records, **kwargs):\n        # add zeros corresponding to [SEP] token\n        if self.late_concat:\n            features = self.separate_and_concate_data(features)\n        device = features[0].device\n        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n            device=device\n        )\n\n        features = pad_sequence(features, batch_first=True)\n        if self.projector is not None:\n            features = self.projector(features)\n        if self.normalize:\n            features = self.norm_act_fn(features)\n        if self.dropout is not None:\n            features = self.dropout(features)\n        predicted, _ = self.model(features, features_len)\n\n        if not self.is_regression:\n            labels = torch.LongTensor(labels).to(features.device).view(-1)\n            predicted = predicted.view(-1, self.num_class)\n            predicted_id_value = torch.argmax(predicted, dim=-1)\n        else:\n            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n            predicted = predicted.squeeze()\n            predicted_id_value = predicted\n\n        loss = self.objective(predicted, labels)\n\n        # records[\"acc\"] += (predicted_id_value == labels).view(-1).cpu().float().tolist()\n        records[\"loss\"].append(loss.item())\n        records[\"filename\"] += filenames\n        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n        records[\"truth\"] += labels.cpu().flatten().tolist()\n\n        return loss\n\n    def separate_and_concate_data(self, feats):\n        # feats is list of tensors(TxF)\n        assert len(feats) % 2 == 0\n        total_num = len(feats) // 2\n        sep_vec = torch.zeros(\n            [1, feats[0].shape[1]], dtype=feats[0].dtype, device=feats[0].device\n        )\n        return [\n            torch.cat((feats[i], sep_vec, feats[i + total_num]), dim=0)\n            for i in range(total_num)\n        ]\n\n    def dump_prediction(self, outpath, filename, pred, label, step=0):\n        with open(outpath, \"w\") as file:\n            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n            file.writelines(line)\n\n    # interface\n    def log_records(self, mode, records, logger, global_step, **kwargs):\n        dev_update1 = False\n        dev_update2 = False\n        save_names = []\n\n        # loss related\n        values = records[\"loss\"]\n        loss_average = torch.FloatTensor(values).mean().item()\n        logger.add_scalar(\n            f\"speechglue-{self.speechglue_task}/{mode}-loss\",\n            loss_average,\n            global_step=global_step,\n        )\n\n        # score related\n        results = self.metric.compute(\n            predictions=records[\"predict\"], references=records[\"truth\"]\n        )\n        print(f\"{mode}: {results}\")\n        for k in [self.metric_keys1, self.metric_keys2]:\n            if k is None:\n                continue\n            result = results[k]\n            logger.add_scalar(\n                f\"speechglue-{self.speechglue_task}/{mode}-{k}\",\n                result,\n                global_step=global_step,\n            )\n            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n                if mode == \"dev\":\n                    if result > self.best_metric1_score and k == self.metric_keys1:\n                        dev_update1 = True\n                        self.best_metric1_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n                    if result > self.best_metric2_score and k == self.metric_keys2:\n                        dev_update2 = True\n                        self.best_metric2_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n        if mode == \"test\":\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n            )\n        elif mode == \"dev\" and dev_update1:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n        elif mode == \"dev\" and dev_update2:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n\n        return save_names", "\n\nclass DownstreamExpert(nn.Module):\n    \"\"\"\n    Used to handle downstream-specific operations\n    eg. downstream forward, metric computation, contents to log\n    \"\"\"\n\n    def __init__(self, upstream_dim, downstream_expert, expdir, **kwargs):\n        super(DownstreamExpert, self).__init__()\n        self.upstream_dim = upstream_dim\n        self.datarc = downstream_expert[\"datarc\"]\n        self.modelrc = downstream_expert[\"modelrc\"]\n        self.expdir = expdir\n\n        # define a task\n        self.speechglue_task = self.datarc[\"speechglue_task\"]\n        self.is_regression = self.speechglue_task == \"stsb\"\n        if not self.is_regression:\n            self.objective = nn.CrossEntropyLoss()\n            self.num_class = SpeechGLUEDataset(\"train\", **self.datarc).num_class\n        else:\n            self.objective = nn.MSELoss()\n            self.num_class = 1\n            print(f\"{self.speechglue_task} will be executed as a regression task\")\n\n        model_cls = eval(self.modelrc[\"select\"])\n        model_conf = self.modelrc.get(self.modelrc[\"select\"], {})\n        projector_dim = self.modelrc.get(\"projector_dim\", None)\n        if projector_dim is not None:\n            self.projector = nn.Linear(upstream_dim, self.modelrc[\"projector_dim\"])\n            model_input_dim = projector_dim\n        else:\n            self.projector = None\n            model_input_dim = upstream_dim\n\n        self.model = model_cls(\n            input_dim=model_input_dim,\n            output_dim=self.num_class,\n            **model_conf,\n        )\n\n        self.normalize = self.modelrc.get(\"tanh_normalization\", False)\n        if self.normalize:\n            print(\"Use Tanh normalization\")\n            self.norm_act_fn = nn.Tanh()\n\n        self.dropout = self.modelrc.get(\"dropout\", None)\n        if self.dropout is not None:\n            print(\"Use dropout after projection\")\n            self.dropout = nn.Dropout(self.dropout)\n\n        self.late_concat = self.datarc.get(\"late_concat\", False)\n\n        self.metric = evaluate.load(\"glue\", self.speechglue_task)\n        self.metric_keys1, self.metric_keys2 = task_to_metrics[self.speechglue_task]\n        self.expdir = expdir\n        self.register_buffer(\"best_metric1_score\", torch.zeros(1))\n        self.register_buffer(\"best_metric2_score\", torch.zeros(1))\n\n    def _get_train_dataloader(self, dataset):\n        sampler = DistributedSampler(dataset) if is_initialized() else None\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"train_batch_size\"],\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _get_eval_dataloader(self, dataset):\n        return DataLoader(\n            dataset,\n            batch_size=self.datarc[\"eval_batch_size\"],\n            shuffle=False,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    # Interface\n    def get_dataloader(self, split):\n        if not hasattr(self, f\"{split}_dataset\"):\n            setattr(self, f\"{split}_dataset\", SpeechGLUEDataset(split, **self.datarc))\n\n        if split == \"train\":\n            return self._get_train_dataloader(self.train_dataset)\n        else:\n            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\n    # Interface\n    def forward(self, mode, features, labels, filenames, records, **kwargs):\n        # add zeros corresponding to [SEP] token\n        if self.late_concat:\n            features = self.separate_and_concate_data(features)\n        device = features[0].device\n        features_len = torch.IntTensor([len(feat) for feat in features]).to(\n            device=device\n        )\n\n        features = pad_sequence(features, batch_first=True)\n        if self.projector is not None:\n            features = self.projector(features)\n        if self.normalize:\n            features = self.norm_act_fn(features)\n        if self.dropout is not None:\n            features = self.dropout(features)\n        predicted, _ = self.model(features, features_len)\n\n        if not self.is_regression:\n            labels = torch.LongTensor(labels).to(features.device).view(-1)\n            predicted = predicted.view(-1, self.num_class)\n            predicted_id_value = torch.argmax(predicted, dim=-1)\n        else:\n            labels = torch.FloatTensor(labels).to(features.device).squeeze()\n            predicted = predicted.squeeze()\n            predicted_id_value = predicted\n\n        loss = self.objective(predicted, labels)\n\n        # records[\"acc\"] += (predicted_id_value == labels).view(-1).cpu().float().tolist()\n        records[\"loss\"].append(loss.item())\n        records[\"filename\"] += filenames\n        records[\"predict\"] += predicted_id_value.cpu().flatten().tolist()\n        records[\"truth\"] += labels.cpu().flatten().tolist()\n\n        return loss\n\n    def separate_and_concate_data(self, feats):\n        # feats is list of tensors(TxF)\n        assert len(feats) % 2 == 0\n        total_num = len(feats) // 2\n        sep_vec = torch.zeros(\n            [1, feats[0].shape[1]], dtype=feats[0].dtype, device=feats[0].device\n        )\n        return [\n            torch.cat((feats[i], sep_vec, feats[i + total_num]), dim=0)\n            for i in range(total_num)\n        ]\n\n    def dump_prediction(self, outpath, filename, pred, label, step=0):\n        with open(outpath, \"w\") as file:\n            line = [f\"{step},{f},{p},{l}\\n\" for f, p, l in zip(filename, pred, label)]\n            file.writelines(line)\n\n    # interface\n    def log_records(self, mode, records, logger, global_step, **kwargs):\n        dev_update1 = False\n        dev_update2 = False\n        save_names = []\n\n        # loss related\n        values = records[\"loss\"]\n        loss_average = torch.FloatTensor(values).mean().item()\n        logger.add_scalar(\n            f\"speechglue-{self.speechglue_task}/{mode}-loss\",\n            loss_average,\n            global_step=global_step,\n        )\n\n        # score related\n        results = self.metric.compute(\n            predictions=records[\"predict\"], references=records[\"truth\"]\n        )\n        print(f\"{mode}: {results}\")\n        for k in [self.metric_keys1, self.metric_keys2]:\n            if k is None:\n                continue\n            result = results[k]\n            logger.add_scalar(\n                f\"speechglue-{self.speechglue_task}/{mode}-{k}\",\n                result,\n                global_step=global_step,\n            )\n            with open(Path(self.expdir) / \"train.csv\", \"a\") as f:\n                f.write(f\"{mode},{global_step},{loss_average},{k},{result}\\n\")\n                if mode == \"dev\":\n                    if result > self.best_metric1_score and k == self.metric_keys1:\n                        dev_update1 = True\n                        self.best_metric1_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n                    if result > self.best_metric2_score and k == self.metric_keys2:\n                        dev_update2 = True\n                        self.best_metric2_score = torch.ones(1) * result\n                        f.write(\n                            f\"{mode},{global_step},{loss_average},{k}_update,{result}\\n\"\n                        )\n                        save_names.append(f\"{mode}-{k}-best.ckpt\")\n\n        if mode == \"test\":\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n            )\n        elif mode == \"dev\" and dev_update1:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys1}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n        elif mode == \"dev\" and dev_update2:\n            self.dump_prediction(\n                Path(self.expdir) / f\"dump_{mode}_{self.metric_keys2}_best.csv\",\n                records[\"filename\"],\n                records[\"predict\"],\n                records[\"truth\"],\n                global_step,\n            )\n\n        return save_names", ""]}
{"filename": "downstream/speechglue/dataset.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\n# This code follows the downstream interface of S3PRL\n# https://github.com/s3prl/s3prl/tree/main/s3prl/downstream\n\nimport os\n", "import os\n\nimport pandas as pd\nimport torch\nimport torchaudio\nfrom torch.utils.data.dataset import Dataset\n\nSAMPLE_RATE = 16000\nSEP_DURATION = int(0.05 * 16000)  # 50 ms\n", "SEP_DURATION = int(0.05 * 16000)  # 50 ms\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),", "    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\nclass SpeechGLUEDataset(Dataset):\n    def __init__(self, split, speechglue_task, speechglue_root, **kwargs):\n        super(SpeechGLUEDataset, self).__init__()\n\n        self.speechglue_task = speechglue_task\n        self.speechglue_root = speechglue_root\n        self.sample_rate = SAMPLE_RATE\n        self.split_sets = kwargs[split]\n        self.speechglue_dir = os.path.join(speechglue_root, speechglue_task)\n        # use a fixed random signal\n        sep_sig_length = kwargs.get(\"sep_sig_length\", 50)\n        if sep_sig_length == 50:\n            self.sep_sig_path = os.path.join(\n                \"downstream\", \"speechglue\", \"white_noise.wav\"\n            )\n        else:\n            print(f\"Use {sep_sig_length}ms SEP signal\")\n            self.sep_sig_path = os.path.join(\n                \"dump\", f\"white_noise_{sep_sig_length}ms.wav\"\n            )\n        self.late_concat = kwargs.get(\"late_concat\", False)\n\n        assert os.path.isdir(\n            self.speechglue_dir\n        ), \"Please first run `python downstream/speechglue_asr/data_prep.py -h` to get TTS file.\"\n\n        table_list = []\n        for item in self.split_sets:\n            file_path = os.path.join(self.speechglue_dir, item, \"data.csv\")\n            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n            table_list.append(pd.read_csv(file_path))\n\n        self.sentence1_key, self.sentence2_key = task_to_keys[self.speechglue_task]\n        self.df_dataset = pd.concat(table_list)\n        assert len(self.df_dataset) != 0, f\"0 data found for {split}\"\n        self.num_class = len(set(list(self.df_dataset[\"label\"])))\n\n        if not self.late_concat:\n            self.sep_sig, sr = torchaudio.load(self.sep_sig_path)\n\n    def _x_name(self, index):\n        return self.speechglue_task + \"_\" + str(index)\n\n    def _load_wav(self, index):\n        wav1, sr = torchaudio.load(self.df_dataset[\"file_\" + self.sentence1_key][index])\n        assert (\n            sr == self.sample_rate\n        ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n        if self.sentence2_key is None:\n            return wav1.view(-1)\n        else:\n            wav2, sr = torchaudio.load(\n                self.df_dataset[\"file_\" + self.sentence2_key][index]\n            )\n            assert (\n                sr == self.sample_rate\n            ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n            if not self.late_concat:\n                sep_sig = self.sep_sig.to(device=wav1.device, dtype=wav1.dtype)\n                return torch.cat((wav1.view(-1), sep_sig.view(-1), wav2.view(-1)))\n            else:\n                return wav1.view(-1), wav2.view(-1)\n\n    def __len__(self):\n        return len(self.df_dataset)\n\n    def __getitem__(self, index):\n        label = self.df_dataset[\"label\"][index]\n        filename = self._x_name(index)\n        if not self.late_concat:\n            wav = self._load_wav(index).numpy()\n            return wav, label, filename\n        else:\n            wav1, wav2 = self._load_wav(index)\n            return wav1.numpy(), wav2.numpy(), label, filename\n\n    def collate_fn(self, samples):\n        if not self.late_concat:\n            return zip(*samples)\n        else:\n            wavs1, wavs2, labels, filenames = zip(*samples)\n            all_wavs = wavs1 + wavs2\n            return all_wavs, labels, filenames", ""]}
{"filename": "downstream/speechglue/__init__.py", "chunked_list": [""]}
{"filename": "downstream/speechglue/data_prep.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\nimport argparse\nimport logging\nimport math\nimport os\nimport re", "import os\nimport re\nimport unicodedata\nfrom builtins import str as unicode\n\nimport numpy as np\nimport soundfile\nimport tacotron_cleaner.cleaners\nfrom datasets import load_dataset\nfrom espnet2.bin.tts_inference import Text2Speech", "from datasets import load_dataset\nfrom espnet2.bin.tts_inference import Text2Speech\nfrom inflect import NumOutOfRangeError\nfrom librosa import resample\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),", "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Data preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\"--verbose\", \"-V\", default=1, type=int, help=\"Verbose option\")\n    parser.add_argument(\n        \"--data-dir\",\n        type=str,\n        default=\"data\",\n        help=\"Path to storing the original GLUE dataset downloaded from huggingface.co\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cuda\", \"cpu\"],\n        help=\"Pytorch device\",\n    )\n    parser.add_argument(\n        \"--num-workers\",\n        type=int,\n        default=1,\n        help=\"Number of workers for map() of TTS\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task for synthesizing\",\n    )\n    parser.add_argument(\n        \"--max-tts-sample\",\n        type=int,\n        default=None,\n        help=\"Number of limited examples for synthesizing (for testing purposes only)\",\n    )\n    parser.add_argument(\n        \"--tts-model\",\n        type=str,\n        default=\"kan-bayashi/ljspeech_vits\",\n        help=\"Name of ESPnet TTS model (listed in https://github.com/espnet/espnet_model_zoo)\",\n    )\n    return parser", "\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Data preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\"--verbose\", \"-V\", default=1, type=int, help=\"Verbose option\")\n    parser.add_argument(\n        \"--data-dir\",\n        type=str,\n        default=\"data\",\n        help=\"Path to storing the original GLUE dataset downloaded from huggingface.co\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cuda\", \"cpu\"],\n        help=\"Pytorch device\",\n    )\n    parser.add_argument(\n        \"--num-workers\",\n        type=int,\n        default=1,\n        help=\"Number of workers for map() of TTS\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task for synthesizing\",\n    )\n    parser.add_argument(\n        \"--max-tts-sample\",\n        type=int,\n        default=None,\n        help=\"Number of limited examples for synthesizing (for testing purposes only)\",\n    )\n    parser.add_argument(\n        \"--tts-model\",\n        type=str,\n        default=\"kan-bayashi/ljspeech_vits\",\n        help=\"Name of ESPnet TTS model (listed in https://github.com/espnet/espnet_model_zoo)\",\n    )\n    return parser", "\n\ndef text_normalization(text, idx=0):\n    # espnet-TTS uses the preprocessing sequence of text-cleaner & g2p\n    # e.g. kan-bayashi/ljspeech_vits configure with tacotron cleaner & g2p_en_no_space\n    # therefore, this code also uses same text-cleaner & text-processing in a g2p\n    # https://github.com/espnet/espnet_tts_frontend/blob/master/tacotron_cleaner/cleaners.py\n    # https://github.com/Kyubyong/g2p/blob/master/g2p_en/g2p.py\n\n    def _space_normalization(text_with_space):\n        # text normalization related with a space\n        t_list = text_with_space.split()\n        norm_list = []\n        i = 0\n        while i < len(t_list):\n            if i < len(t_list) - 1:\n                # merge two words with an apostrophe (e.g., \"can' t\" -> \"can't\")\n                if t_list[i + 1][0] == \"'\":\n                    norm_list.append(t_list[i] + t_list[i + 1])\n                    i += 1\n                # add space after comma (e.g., \",2000\" -> \", 2000\")\n                elif t_list[i + 1][0] == \",\":\n                    if t_list[i + 1] == \",\":\n                        norm_list.extend([t_list[i] + \",\"])\n                    else:\n                        norm_list.extend([t_list[i] + \",\", t_list[i + 1][1:].strip()])\n                    i += 1\n                # add space after period (e.g., \".2000\" -> \". 2000\")\n                elif t_list[i + 1][0] == \".\":\n                    if t_list[i + 1] == \".\":\n                        norm_list.extend([t_list[i] + \".\"])\n                    else:\n                        norm_list.extend([t_list[i] + \".\", t_list[i + 1][1:].strip()])\n                    i += 1\n                else:\n                    norm_list.append(t_list[i])\n            else:\n                norm_list.append(t_list[i])\n            i += 1\n        return \" \".join(norm_list)\n\n    norm_text = _space_normalization(\n        text.replace(\". . .\", \".\").replace(\"...\", \".\").replace(\"$,\", \"$\")\n    )\n    try:\n        norm_text = tacotron_cleaner.cleaners.custom_english_cleaners(norm_text)\n        # from https://github.com/Kyubyong/g2p/blob/master/g2p_en/g2p.py#L148\n        # but normalize_numbers() has already been applied in the custom_english_cleaners()\n        norm_text = unicode(norm_text)\n        norm_text = \"\".join(\n            char\n            for char in unicodedata.normalize(\"NFD\", norm_text)\n            if unicodedata.category(char) != \"Mn\"\n        )\n        norm_text = norm_text.lower()\n        norm_text = re.sub(\"[^ a-z'.,?!\\-]\", \"\", norm_text)\n        norm_text = norm_text.replace(\"i.e.\", \"that is\")\n        norm_text = norm_text.replace(\"e.g.\", \"for example\")\n        # space-related normalization again after removing some punctuations\n        norm_text = _space_normalization(norm_text)\n    except (RuntimeError, NumOutOfRangeError) as e:\n        # Some sentences can't be tokenized to vocode.\n        # E.g., contain only symbols such as \"(\" and \")\"\n        # E.g., contain a number out of range (i.e., NumOutOfRangeError of https://github.com/jaraco/inflect)\n        logging.warning(\n            f\"{e}\\n\"\n            + \"Invalid sentence may be inputted and this column will be deleted.\"\n            + f\" (sentence: {text}, idx: {idx})\"\n        )\n        norm_text = None\n\n    if norm_text == \"\":\n        norm_text = None\n        logging.warning(\n            \"Invalid sentence may be inputted and this column will be deleted.\"\n            + f\" (sentence: {text}, idx: {idx})\"\n        )\n    return norm_text", "\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # Setup logging\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n        logging.warning(\"Skip DEBUG/INFO messages\")\n\n    # check args\n    if args.num_workers > 1 and args.device == \"cuda\":\n        logging.warning(\"only single GPU decoding is supported\")\n        args.num_workers = 1\n\n    # instantiate the text-to-speech model\n    text2speech = Text2Speech.from_pretrained(args.tts_model, device=args.device)\n\n    if args.glue_task == \"all\":\n        task_names = task_to_keys.keys()\n    else:\n        task_names = [args.glue_task]\n\n    for task_name in task_names:\n        logging.info(\"[\" + task_name + \"] Start dataset preparation\")\n        # set a data split\n        valid_column = \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n        eval_column = \"test_matched\" if task_name == \"mnli\" else \"test\"\n        extend_column = (\n            [\"validation_mismatched\", \"test_mismatched\"] if task_name == \"mnli\" else []\n        )\n        data_splits = [\"train\", valid_column, eval_column] + extend_column\n        logging.info(\"[\" + task_name + \"] Splits of dataset = \" + str(data_splits))\n\n        sentence1_key, sentence2_key = task_to_keys[task_name]\n\n        # TTS function applied by a map()\n        def tts(examples):\n            # first sentence\n            dirname = str(math.floor(examples[\"idx\"] / 10000) * 10000)\n            outdir_base = os.path.abspath(\n                os.path.join(args.dump_dir, task_name, data_split)\n            )\n            wav_name = str(examples[\"idx\"]) + \".wav\"\n            out_path1 = os.path.join(outdir_base, sentence1_key, dirname, wav_name)\n            text1 = text_normalization(examples[sentence1_key], examples[\"idx\"])\n            if text1 is None:\n                out_path1 = None\n                length1 = None\n            else:\n                speech = text2speech(text1)[\"wav\"]\n                speech = resample(\n                    speech.cpu().numpy(),\n                    orig_sr=text2speech.fs,\n                    target_sr=16000,\n                    res_type=\"kaiser_best\",\n                )\n                length1 = speech.shape[0]\n                soundfile.write(out_path1, speech, 16000, \"PCM_16\")\n\n            if sentence2_key is None:\n                return {\n                    sentence1_key: text1,\n                    \"file_\" + sentence1_key: out_path1,\n                    \"length_\" + sentence1_key: length1,\n                }\n\n            # second sentence\n            out_path2 = os.path.join(outdir_base, sentence2_key, dirname, wav_name)\n            text2 = text_normalization(examples[sentence2_key], examples[\"idx\"])\n            if text2 is None:\n                out_path1 = None  # for filtering\n                out_path2 = None\n                length1 = None\n                length2 = None\n            else:\n                speech = text2speech(text2)[\"wav\"]\n                speech = resample(\n                    speech.cpu().numpy(),\n                    orig_sr=text2speech.fs,\n                    target_sr=16000,\n                    res_type=\"kaiser_best\",\n                )\n                length2 = speech.shape[0]\n                soundfile.write(out_path2, speech, 16000, \"PCM_16\")\n            return {\n                sentence1_key: text1,\n                sentence2_key: text2,\n                \"file_\" + sentence1_key: out_path1,\n                \"file_\" + sentence2_key: out_path2,\n                \"length_\" + sentence1_key: length1,\n                \"length_\" + sentence2_key: length2,\n            }\n\n        # initialize a dataset and generate synthesized speech data\n        logging.info(\"[\" + task_name + \"] Generating TTS data\")\n        for data_split in data_splits:\n            # take a dataset from HuggingFace's GLUE\n            raw_datasets = load_dataset(\n                \"glue\",\n                task_name,\n                split=data_split,\n                cache_dir=args.data_dir,\n                use_auth_token=None,\n            )\n            num_utt = len(raw_datasets)\n            logging.info(\n                f\"The number of rows of the original data of {data_split} split: {num_utt}\"\n            )\n\n            # make output directories\n            dirnames = np.arange(0, math.floor(num_utt / 10000) * 10000 + 1, 10000)\n            for dirname in dirnames:\n                os.makedirs(\n                    os.path.join(\n                        args.dump_dir,\n                        task_name,\n                        data_split,\n                        sentence1_key,\n                        str(dirname),\n                    ),\n                    exist_ok=True,\n                )\n                if sentence2_key is not None:\n                    os.makedirs(\n                        os.path.join(\n                            args.dump_dir,\n                            task_name,\n                            data_split,\n                            sentence2_key,\n                            str(dirname),\n                        ),\n                        exist_ok=True,\n                    )\n\n            # limit the number of examples for testing\n            if args.max_tts_sample is not None:\n                raw_datasets = raw_datasets.select(range(args.max_tts_sample))\n            # run a text-to-speech\n            tts_datasets = raw_datasets.map(\n                tts,\n                num_proc=args.num_workers,\n                desc=\"Running TTS on the \"\n                + data_split\n                + \" set of \"\n                + task_name\n                + \" dataset\",\n            )\n            # filter rows that could not TTS\n            tts_datasets = tts_datasets.filter(\n                lambda example: example[\"file_\" + sentence1_key] is not None\n            )\n            logging.info(\n                f\"The number of rows of the synthesized data of {data_split} split: {len(tts_datasets)}\\n\"\n                + \"-----------------------\"\n            )\n            # save the audio files with CSV format\n            tts_datasets.to_csv(\n                os.path.join(args.dump_dir, task_name, data_split, \"data.csv\"),\n                index=False,\n            )\n        logging.info(\"[\" + task_name + \"] Successfully finished dataset preparation\")\n    logging.info(\"All dataset preparation finished successfully\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "downstream/speechglue_asr/select_sample.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\nimport argparse\nimport os\nimport random\n\nimport pandas as pd", "\nimport pandas as pd\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),", "    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Data preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task\",\n    )\n    parser.add_argument(\n        \"--split\",\n        type=str,\n        default=\"train\",\n        choices=[\"train\", \"validation\", \"test\"],\n        help=\"Split of dataset\",\n    )\n    parser.add_argument(\n        \"--max-hours\",\n        type=int,\n        default=None,\n        help=\"Upper limit of time in hours\",\n    )\n    parser.add_argument(\n        \"--no-use-predefined-sampling\",\n        action=\"store_true\",\n        help=\"No predefined sampling\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=1,\n        help=\"Random seed value\",\n    )\n    return parser", "def get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Data preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task\",\n    )\n    parser.add_argument(\n        \"--split\",\n        type=str,\n        default=\"train\",\n        choices=[\"train\", \"validation\", \"test\"],\n        help=\"Split of dataset\",\n    )\n    parser.add_argument(\n        \"--max-hours\",\n        type=int,\n        default=None,\n        help=\"Upper limit of time in hours\",\n    )\n    parser.add_argument(\n        \"--no-use-predefined-sampling\",\n        action=\"store_true\",\n        help=\"No predefined sampling\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=1,\n        help=\"Random seed value\",\n    )\n    return parser", "\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    random.seed(args.seed)\n    max_samples = int(args.max_hours * 60 * 60 * 16000)\n\n    if args.glue_task == \"all\":\n        task_names = task_to_keys.keys()\n    else:\n        task_names = [args.glue_task]\n\n    for task_name in task_names:\n        # set a data split\n        if args.split == \"train\":\n            data_splits = [\"train\"]\n        elif args.split == \"validation\":\n            data_splits = (\n                [\"validation_matched\", \"validation_mismatched\"]\n                if task_name == \"mnli\"\n                else [\"validation\"]\n            )\n        elif args.split == \"test\":\n            data_splits = (\n                [\"test_matched\", \"test_mismatched\"] if task_name == \"mnli\" else [\"test\"]\n            )\n        else:\n            raise ValueError(\n                \"args.split must be one of the ['train', 'validation', 'test']\"\n            )\n\n        for data_split in data_splits:\n            file_path = os.path.join(args.dump_dir, task_name, data_split, \"data.csv\")\n            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n            table = pd.read_csv(file_path)\n\n            sentence1_key, sentence2_key = task_to_keys[task_name]\n            file_paths = table[\"file_\" + sentence1_key].tolist()\n            labels = table[sentence1_key].tolist()\n            lengths = table[\"length_\" + sentence1_key].tolist()\n            if sentence2_key is not None:\n                file_paths.extend(table[\"file_\" + sentence2_key].tolist())\n                labels.extend(table[sentence2_key].tolist())\n                lengths.extend(table[\"length_\" + sentence2_key].tolist())\n\n            if sum(lengths) < max_samples:\n                current_hours = round(sum(lengths) / 16000 / 60 / 60, 1)\n                print(\n                    f\"The {data_split} set of {task_name} task ({current_hours}) is already less than {args.max_hours} hours.\"\n                )\n                continue\n\n            select_outdir = os.path.join(\n                \"downstream\", \"speechglue_asr\", \"selected_uttids\"\n            )\n            select_filename = task_name + \"_\" + data_split + \".list\"\n            if args.no_use_predefined_sampling:\n                uttids = list(range(len(file_paths)))\n                random.shuffle(uttids)\n            else:\n                with open(os.path.join(select_outdir, select_filename), \"r\") as f:\n                    uttids = [int(i) for i in f.read().splitlines()]\n\n            num_sample = 0\n            file_paths_lim = []\n            labels_lim = []\n            lengths_lim = []\n            uttids_lim = []\n            for uttid in uttids:\n                num_sample += lengths[uttid]\n                if num_sample < max_samples:\n                    file_paths_lim.append(file_paths[uttid])\n                    labels_lim.append(labels[uttid])\n                    lengths_lim.append(lengths[uttid])\n                    uttids_lim.append(uttid)\n                else:\n                    break\n\n            df_dataset = pd.DataFrame(\n                data={\n                    \"file_path\": file_paths_lim,\n                    \"length\": lengths_lim,\n                    \"label\": labels_lim,\n                },\n                columns=[\"file_path\", \"length\", \"label\"],\n            )\n            df_outdir = os.path.join(\n                args.dump_dir, task_name, data_split + \"-\" + str(args.max_hours)\n            )\n            os.makedirs(df_outdir, exist_ok=True)\n            df_dataset.to_csv(os.path.join(df_outdir, \"data.csv\"), index=False)\n\n            if args.no_use_predefined_sampling:\n                os.makedirs(select_outdir, exist_ok=True)\n                with open(os.path.join(select_outdir, select_filename), \"w\") as f:\n                    f.writelines([str(l) + os.linesep for l in uttids_lim])", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "downstream/speechglue_asr/expert.py", "chunked_list": ["# Copyright (c) Facebook, Inc. All Rights Reserved\n\n# Copyleft (c), Speech Lab, NTU, Taiwan\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# This code only changes lines 82 and 90 of the following code (with some code formatting):\n# https://github.com/s3prl/s3prl/blob/v0.4.10/s3prl/downstream/asr/expert.py\n# L82: in order to load a dictionary of each task on SpeechGLUE dataset\n# L350: in order to use 'dev' set (not 'dev-clean' set of LibriSpeech)\n\nimport math  # noqa", "\nimport math  # noqa\nimport os\nfrom argparse import Namespace\nfrom pathlib import Path\n\nimport editdistance\nimport torch\nimport torch.nn as nn\nfrom torch.distributed import is_initialized", "import torch.nn as nn\nfrom torch.distributed import is_initialized\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom ..model import *\nfrom .dataset import SequenceDataset\nfrom .dictionary import Dictionary\nfrom .model import *\n", "from .model import *\n\n\ndef token_to_word(text):\n    # Hard coding but it is only used here for now.\n    # Assumption that units are characters. Doesn't handle BPE.\n    # Inter-character separator is \" \" and inter-word separator is \"|\".\n    return text.replace(\" \", \"\").replace(\"|\", \" \").strip()\n\n\ndef get_decoder(decoder_args_dict, dictionary):\n    decoder_args = Namespace(**decoder_args_dict)\n\n    if decoder_args.decoder_type == \"kenlm\":\n        from .w2l_decoder import W2lKenLMDecoder\n\n        decoder_args.beam_size_token = len(dictionary)\n        if isinstance(decoder_args.unk_weight, str):\n            decoder_args.unk_weight = eval(decoder_args.unk_weight)\n        return W2lKenLMDecoder(decoder_args, dictionary)\n\n    return None", "\n\ndef get_decoder(decoder_args_dict, dictionary):\n    decoder_args = Namespace(**decoder_args_dict)\n\n    if decoder_args.decoder_type == \"kenlm\":\n        from .w2l_decoder import W2lKenLMDecoder\n\n        decoder_args.beam_size_token = len(dictionary)\n        if isinstance(decoder_args.unk_weight, str):\n            decoder_args.unk_weight = eval(decoder_args.unk_weight)\n        return W2lKenLMDecoder(decoder_args, dictionary)\n\n    return None", "\n\nclass DownstreamExpert(nn.Module):\n    \"\"\"\n    Used to handle downstream-specific operations\n    eg. downstream forward, metric computation, contents to log\n    \"\"\"\n\n    def __init__(\n        self, upstream_dim, upstream_rate, downstream_expert, expdir, **kwargs\n    ):\n        \"\"\"\n        Args:\n            upstream_dim: int\n                Different upstream will give different representation dimension\n                You might want to first project them to the same dimension\n\n            upstream_rate: int\n                160: for upstream with 10 ms per frame\n                320: for upstream with 20 ms per frame\n\n            downstream_expert: dict\n                The 'downstream_expert' field specified in your downstream config file\n                eg. downstream/example/config.yaml\n\n            expdir: string\n                The expdir from command-line argument, you should save all results into\n                this directory, like some logging files.\n\n            **kwargs: dict\n                All the arguments specified by the argparser in run_downstream.py\n                and all the other fields in config.yaml, in case you need it.\n\n                Note1. Feel free to add new argument for __init__ as long as it is\n                a command-line argument or a config field. You can check the constructor\n                code in downstream/runner.py\n        \"\"\"\n\n        super(DownstreamExpert, self).__init__()\n        self.upstream_dim = upstream_dim\n        self.upstream_rate = upstream_rate\n        self.datarc = downstream_expert[\"datarc\"]\n        self.modelrc = downstream_expert[\"modelrc\"]\n        self.expdir = expdir\n\n        self.dictionary = Dictionary.load(\n            os.path.join(\n                self.datarc[\"speechglue_root\"],\n                self.datarc[\"speechglue_task\"],\n                \"char.dict\",\n            )\n        )\n\n        self.projector = nn.Linear(upstream_dim, self.modelrc[\"project_dim\"])\n        model_cls = eval(self.modelrc[\"select\"])\n        model_conf = self.modelrc[self.modelrc[\"select\"]]\n        self.model = model_cls(\n            self.modelrc[\"project_dim\"],\n            len(self.dictionary.symbols),\n            upstream_rate,\n            **model_conf,\n        )\n        self.blank = self.dictionary.bos()\n        self.objective = nn.CTCLoss(\n            blank=self.blank, zero_infinity=self.datarc[\"zero_infinity\"]\n        )\n        decoder_args = self.datarc.get(\"decoder_args\")\n        self.decoder = get_decoder(decoder_args, self.dictionary)\n        self.register_buffer(\"best_score\", torch.ones(1) * 100)\n\n    # Interface\n    def get_dataloader(self, split):\n        \"\"\"\n        Args:\n            split: string\n                The name of the dataloader, can be train/dev/test-clean/test-other for asr\n\n        Return:\n            a torch.utils.data.DataLoader returning each batch in the format of:\n\n            [wav1, wav2, ...], your_other_contents1, your_other_contents2, ...\n\n            where wav1, wav2 ... are in variable length\n            each wav is torch.FloatTensor in cpu with:\n                1. dim() == 1\n                2. sample_rate == 16000\n                3. directly loaded by torchaudio\n        \"\"\"\n        if not hasattr(self, f\"{split}_dataset\"):\n            batch_size = (\n                self.datarc[\"batch_size\"]\n                if split == \"train\"\n                else self.datarc[\"eval_batch_size\"]\n            )\n            setattr(\n                self,\n                f\"{split}_dataset\",\n                SequenceDataset(split, batch_size, self.dictionary, **self.datarc),\n            )\n\n        if split == \"train\":\n            return self._get_train_dataloader(self.train_dataset)\n        else:\n            return self._get_eval_dataloader(getattr(self, f\"{split}_dataset\"))\n\n    def _get_train_dataloader(self, dataset):\n        sampler = DistributedSampler(dataset) if is_initialized() else None\n        return DataLoader(\n            dataset,\n            batch_size=1,\n            shuffle=(sampler is None),\n            sampler=sampler,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _get_eval_dataloader(self, dataset):\n        return DataLoader(\n            dataset,\n            batch_size=1,\n            shuffle=False,\n            num_workers=self.datarc[\"num_workers\"],\n            collate_fn=dataset.collate_fn,\n        )\n\n    def _compute_metrics(\n        self, pred_tokens_all, pred_words_all, target_tokens_all, target_words_all\n    ):\n        \"\"\"Computes WER and UER given the prediction and true transcriptions\"\"\"\n        unit_error_sum = 0.0\n        word_error_sum = 0.0\n        unit_length_sum = 0\n        word_length_sum = 0\n\n        for pred_tokens, pred_words, target_tokens, target_words in zip(\n            pred_tokens_all, pred_words_all, target_tokens_all, target_words_all\n        ):\n\n            pred_tokens = pred_tokens.split()\n            target_tokens = target_tokens.split()\n            unit_error_sum += editdistance.eval(pred_tokens, target_tokens)\n            unit_length_sum += len(target_tokens)\n\n            word_error_sum += editdistance.eval(pred_words, target_words)\n            word_length_sum += len(target_words)\n\n        uer, wer = 100.0, 100.0\n        if unit_length_sum > 0:\n            uer = 100.0 * unit_error_sum / unit_length_sum\n        if word_length_sum > 0:\n            wer = 100.0 * word_error_sum / word_length_sum\n\n        return uer, wer\n\n    def _decode(self, log_probs, input_lens):\n        \"\"\"Decoder that take log probabilities as input and outputs decoded seq\"\"\"\n        pred_tokens_batch = []\n        pred_words_batch = []\n\n        for log_prob, in_len in zip(log_probs, input_lens):\n            log_prob = log_prob[:in_len].unsqueeze(0)\n            decoded = None\n            if self.decoder is not None and not self.training:\n                decoded = self.decoder.decode(log_prob)\n                if len(decoded) >= 1:\n                    decoded = decoded[0]\n                    decoded = None if len(decoded) < 1 else decoded[0]\n\n            pred_token_ids = log_prob.argmax(dim=-1).unique_consecutive()\n            pred_token_ids = pred_token_ids[pred_token_ids != self.blank].tolist()\n            pred_tokens = self.dictionary.string(pred_token_ids)\n\n            if decoded is not None and \"words\" in decoded:\n                pred_words = decoded[\"words\"]\n            else:\n                pred_words = token_to_word(pred_tokens).split()\n\n            pred_tokens_batch.append(pred_tokens)\n            pred_words_batch.append(pred_words)\n\n        return pred_tokens_batch, pred_words_batch\n\n    def _get_log_probs(self, features):\n        device = features[0].device\n        features_len = torch.IntTensor([len(feat) for feat in features])\n        features = pad_sequence(features, batch_first=True).to(device=device)\n        features = self.projector(features)\n        logits, log_probs_len = self.model(features, features_len)\n        log_probs = nn.functional.log_softmax(logits, dim=-1)\n        return log_probs, log_probs_len\n\n    def inference(self, features, filenames):\n        log_probs, log_probs_len = self._get_log_probs(features)\n        _, pred_words_batch = self._decode(\n            log_probs.float().contiguous().cpu(), log_probs_len\n        )\n        hyps = [\" \".join(hyp) for hyp in pred_words_batch]\n\n        if filenames != []:\n            with open(Path(self.expdir) / \"inference.ark\", \"w\") as file:\n                for hyp, filename in zip(hyps, filenames):\n                    file.write(f\"{filename} {hyp}\\n\")\n\n        return hyps\n\n    # Interface\n    def forward(self, split, features, labels, filenames, records, **kwargs):\n        \"\"\"\n        Args:\n            split: string\n                The name of the dataloader, can be train/dev/test-clean/test-other for asr\n\n            features:\n                list of unpadded features [feat1, feat2, ...]\n                each feat is in torch.FloatTensor and already\n                put in the device assigned by command-line args\n\n            your_other_contents1, ... :\n                in the order defined by your dataloader (dataset + collate_fn)\n                these are all in cpu, and you can move them to the same device\n                as features\n\n            records:\n                defaultdict(list), by appending contents into records,\n                these contents can be averaged and logged on Tensorboard\n                later by self.log_records (also customized by you)\n\n                Note1. downstream/runner.py will call self.log_records\n                    1. every `log_step` during training\n                    2. once after evalute the whole dev/test dataloader\n\n                Note2. `log_step` is defined in your downstream config\n                eg. downstream/example/config.yaml\n\n        Return:\n            loss:\n                the loss to be optimized, should not be detached\n                a single scalar in torch.FloatTensor\n        \"\"\"\n        log_probs, log_probs_len = self._get_log_probs(features)\n        device = features[0].device\n        labels = [torch.IntTensor(l) for l in labels]\n        labels_len = torch.IntTensor([len(label) for label in labels]).to(device=device)\n        labels = pad_sequence(\n            labels,\n            batch_first=True,\n            padding_value=self.dictionary.pad(),\n        ).to(device=device)\n\n        loss = self.objective(\n            log_probs.transpose(0, 1),  # (N, T, C) -> (T, N, C)\n            labels,\n            log_probs_len,\n            labels_len,\n        )\n        records[\"loss\"].append(loss.item())\n\n        target_tokens_batch = []\n        target_words_batch = []\n        for label in labels:\n            label_idx = (label != self.dictionary.pad()) & (\n                label != self.dictionary.eos()\n            )\n            target_token_ids = label[label_idx].tolist()\n            target_tokens = self.dictionary.string(target_token_ids)\n            target_words = token_to_word(target_tokens).split()\n\n            target_tokens_batch.append(target_tokens)\n            target_words_batch.append(target_words)\n\n        with torch.no_grad():\n            pred_tokens_batch, pred_words_batch = self._decode(\n                log_probs.float().contiguous().cpu(), log_probs_len\n            )\n\n        records[\"target_tokens\"] += target_tokens_batch\n        records[\"target_words\"] += target_words_batch\n        records[\"pred_tokens\"] += pred_tokens_batch\n        records[\"pred_words\"] += pred_words_batch\n        records[\"filenames\"] += filenames\n\n        return loss\n\n    # interface\n    def log_records(\n        self, split, records, logger, global_step, batch_ids, total_batch_num, **kwargs\n    ):\n        \"\"\"\n        Args:\n            split: string\n                'train':\n                    records and batchids contain contents for `log_step` batches\n                    `log_step` is defined in your downstream config\n                    eg. downstream/example/config.yaml\n\n                'dev' or 'test-clean' or 'test-other' :\n                    records and batchids contain contents for the entire evaluation dataset\n\n            records:\n                defaultdict(list), contents already prepared by self.forward\n\n            logger:\n                Tensorboard SummaryWriter\n                please use f'{your_task_name}/{split}-{key}' as key name to log your contents,\n                preventing conflict with the logging of other tasks\n\n            global_step:\n                The global_step when training, which is helpful for Tensorboard logging\n\n            batch_ids:\n                The batches contained in records when enumerating over the dataloader\n\n            total_batch_num:\n                The total amount of batches in the dataloader\n\n        Return:\n            a list of string\n                Each string is a filename we wish to use to save the current model\n                according to the evaluation result, like the best.ckpt on the dev set\n                You can return nothing or an empty list when no need to save the checkpoint\n        \"\"\"\n        loss = torch.FloatTensor(records[\"loss\"]).mean().item()\n        print(f\"{split} loss: {loss}\")\n\n        uer, wer = self._compute_metrics(\n            records[\"pred_tokens\"],\n            records[\"pred_words\"],\n            records[\"target_tokens\"],\n            records[\"target_words\"],\n        )\n\n        logger.add_scalar(f\"asr/{split}-loss\", loss, global_step=global_step)\n        logger.add_scalar(f\"asr/{split}-uer\", uer, global_step=global_step)\n        logger.add_scalar(f\"asr/{split}-wer\", wer, global_step=global_step)\n        print(f\"{split} uer: {uer}\")\n        print(f\"{split} wer: {wer}\")\n\n        save_names = []\n        if split == \"dev\" and wer < self.best_score:\n            self.best_score = torch.ones(1) * wer\n            save_names.append(f\"{split}-best.ckpt\")\n\n        if \"test\" in split or \"dev\" in split:\n            lm = \"noLM\" if self.decoder is None else \"LM\"\n            hyp_ark = open(os.path.join(self.expdir, f\"{split}-{lm}-hyp.ark\"), \"w\")\n            ref_ark = open(os.path.join(self.expdir, f\"{split}-{lm}-ref.ark\"), \"w\")\n            for filename, hyp, ref in zip(\n                records[\"filenames\"], records[\"pred_words\"], records[\"target_words\"]\n            ):\n                hyp = \" \".join(hyp)\n                ref = \" \".join(ref)\n                hyp_ark.write(f\"{filename} {hyp}\\n\")\n                ref_ark.write(f\"{filename} {ref}\\n\")\n            hyp_ark.close()\n            ref_ark.close()\n\n        return save_names", ""]}
{"filename": "downstream/speechglue_asr/dataset.py", "chunked_list": ["# Copyleft (c), Speech Lab, NTU, Taiwan\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# This code changes to load speechGLUE data based on the following code (and some code formatting).\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\n# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n", "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n\nimport os\nimport re\n\nimport pandas as pd\nimport torchaudio\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom .dictionary import Dictionary\n\nSAMPLE_RATE = 16000\nHALF_BATCHSIZE_TIME = 2000\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),", "    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}", "    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\n####################\n# Sequence Dataset #\n####################\nclass SequenceDataset(Dataset):\n    def __init__(\n        self, split, bucket_size, dictionary, speechglue_task, speechglue_root, **kwargs\n    ):\n        super(SequenceDataset, self).__init__()\n\n        self.dictionary = dictionary\n        self.speechglue_task = speechglue_task\n        self.speechglue_root = speechglue_root\n        self.sample_rate = SAMPLE_RATE\n        self.split_sets = kwargs[split]\n        self.speechglue_dir = os.path.join(speechglue_root, speechglue_task)\n\n        # Read table for bucketing\n        assert os.path.isdir(\n            self.speechglue_dir\n        ), \"Please first run `python downstream/speechglue_asr/data_prep.py -h` to get TTS file.\"\n\n        # Wavs\n        table_list = []\n        for item in self.split_sets:\n            file_path = os.path.join(self.speechglue_dir, item, \"data.csv\")\n            assert os.path.isfile(file_path), f\"{file_path} is not found.\"\n            table_list.append(pd.read_csv(file_path))\n\n        table_list = pd.concat(table_list)\n\n        dataset_columns = [\"file_path\", \"length\", \"label\"]\n        # the case of a dataset with a limited amount of samples in advance\n        if set(table_list.columns) == set(dataset_columns):\n            df_dataset = table_list\n        else:\n            sentence1_key, sentence2_key = task_to_keys[self.speechglue_task]\n            file_paths = table_list[\"file_\" + sentence1_key].tolist()\n            labels = table_list[sentence1_key].tolist()\n            lengths = table_list[\"length_\" + sentence1_key].tolist()\n            if sentence2_key is not None:\n                file_paths.extend(table_list[\"file_\" + sentence2_key].tolist())\n                labels.extend(table_list[sentence2_key].tolist())\n                lengths.extend(table_list[\"length_\" + sentence2_key].tolist())\n            df_dataset = pd.DataFrame(\n                data={\"file_path\": file_paths, \"length\": lengths, \"label\": labels},\n                columns=dataset_columns,\n            )\n\n        df_dataset = df_dataset.sort_values(by=[\"length\"], ascending=False)\n\n        X = df_dataset[\"file_path\"].tolist()\n        X_lens = df_dataset[\"length\"].tolist()\n        Y = self._load_transcript(df_dataset[\"label\"].tolist())\n        Y = [\n            self.dictionary.encode_line(y, line_tokenizer=lambda x: x.split()).long()\n            for y in Y\n        ]\n        assert len(X) != 0, f\"0 data found for {split}\"\n\n        # Use bucketing to allow different batch sizes at run time\n        self.X = []\n        self.Y = []\n        batch_x, batch_len, batch_y = [], [], []\n\n        for x, x_len, y in tqdm(\n            zip(X, X_lens, Y),\n            total=len(X),\n            desc=f\"ASR dataset {split}\",\n            dynamic_ncols=True,\n        ):\n            batch_x.append(x)\n            batch_len.append(x_len)\n            batch_y.append(y)\n\n            # Fill in batch_x until batch is full\n            if len(batch_x) == bucket_size:\n                # Half the batch size if seq too long\n                if (bucket_size >= 2) and (max(batch_len) > HALF_BATCHSIZE_TIME):\n                    self.X.append(batch_x[: bucket_size // 2])\n                    self.X.append(batch_x[bucket_size // 2 :])\n                    self.Y.append(batch_y[: bucket_size // 2])\n                    self.Y.append(batch_y[bucket_size // 2 :])\n                else:\n                    self.X.append(batch_x)\n                    self.Y.append(batch_y)\n                batch_x, batch_len, batch_y = [], [], []\n\n        # Gather the last batch\n        if len(batch_x) > 1:\n            self.X.append(batch_x)\n            self.Y.append(batch_y)\n\n    def _parse_x_name(self, x):\n        return \"-\".join(x.split(\"/\")[-4:]).split(\".\")[0]\n\n    def _load_wav(self, wav_path):\n        wav, sr = torchaudio.load(wav_path)\n        assert (\n            sr == self.sample_rate\n        ), f\"Sample rate mismatch: real {sr}, config {self.sample_rate}\"\n        return wav.view(-1)\n\n    def _load_transcript(self, x_list):\n        def process_trans(transcript):\n            transcript = re.sub(\"[.,?!]\", \"\", transcript).replace(\" \", \"|\")\n            # word to char\n            return \" \".join(list(transcript)) + \" |\"\n\n        return [process_trans(x) for x in x_list]\n\n    def _build_dictionary(\n        self, transcripts, workers=1, threshold=-1, nwords=-1, padding_factor=8\n    ):\n        d = Dictionary()\n        transcript_list = list(transcripts.values())\n        Dictionary.add_transcripts_to_dictionary(transcript_list, d, workers)\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        # Load acoustic feature and pad\n        wav_batch = [self._load_wav(x_file).numpy() for x_file in self.X[index]]\n        label_batch = [y.numpy() for y in self.Y[index]]\n        filename_batch = [self._parse_x_name(x_file) for x_file in self.X[index]]\n        return (\n            wav_batch,\n            label_batch,\n            filename_batch,\n        )  # bucketing, return ((wavs, labels))\n\n    def collate_fn(self, items):\n        assert len(items) == 1\n        return (\n            items[0][0],\n            items[0][1],\n            items[0][2],\n        )  # hack bucketing, return (wavs, labels, filenames)", ""]}
{"filename": "downstream/speechglue_asr/mk_char_dict.py", "chunked_list": ["# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# we utilize the GLUE tasks listed in the below code\n# https://github.com/huggingface/transformers/blob/7378726df60b9cf399aacfe372fea629c1c4c7d3/examples/pytorch/text-classification/run_glue.py\n\nimport argparse\nimport os\nimport re\n\nimport pandas as pd", "\nimport pandas as pd\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),", "    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Dictionary preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task\",\n    )\n\n    return parser", "def get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=\"Dictionary preparation for SpeechGLUE\",\n    )\n    parser.add_argument(\n        \"--dump-dir\",\n        type=str,\n        default=\"dump\",\n        help=\"Path to storing the SpeechGLUE dataset\",\n    )\n    parser.add_argument(\n        \"--glue-task\",\n        type=str,\n        default=\"all\",\n        choices=[\"all\"] + list(task_to_keys.keys()),\n        help=\"Name of the GLUE task\",\n    )\n\n    return parser", "\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    if args.glue_task == \"all\":\n        task_names = task_to_keys.keys()\n    else:\n        task_names = [args.glue_task]\n\n    for task_name in task_names:\n        sentence1_key, sentence2_key = task_to_keys[task_name]\n        csv_path = os.path.join(args.dump_dir, task_name, \"train\", \"data.csv\")\n        # some sentences include only \"null\"\n        # therefore, keep_default_na is added to interpret as is\n        csv = pd.read_csv(csv_path, keep_default_na=False)\n        sentences = list(csv[sentence1_key])\n        if sentence2_key is not None:\n            sentences.extend(list(csv[sentence2_key]))\n        sentences = \"|\".join(sentences)\n        sentences = re.sub(\"[.,?!]\", \"\", sentences).replace(\" \", \"|\") + \"|\"\n        char_counts = {c: sentences.count(c) for c in set(sentences)}\n        outdic = os.path.join(args.dump_dir, task_name, \"char.dict\")\n\n        with open(outdic, \"w\") as f:\n            for x in sorted(\n                char_counts.items(), key=lambda char: char[1], reverse=True\n            ):\n                f.write(x[0] + \" \" + str(x[1]) + \"\\n\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "downstream/speechglue_asr/__init__.py", "chunked_list": [""]}
{"filename": "upstream/hf_nlp_ssl/expert.py", "chunked_list": ["import logging\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import AutoModel, AutoTokenizer\n\nSAMPLE_RATE = 16000\nHF_INPUT_KEYS = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass UpstreamExpert(torch.nn.Module):\n    def __init__(self, ckpt, **kwds):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(ckpt)\n        tokenizer = AutoTokenizer.from_pretrained(\n            ckpt,\n            cache_dir=\"data\",\n        )\n        self.pad_values = [tokenizer.pad_token_id, tokenizer.pad_token_type_id, 0]\n        self.vocab_size = tokenizer.vocab_size\n\n    def pad_token(self, tokens):\n        # https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py\n        device = tokens[0].device\n        key_size = tokens[0].shape[1]\n        output_dict = {}\n        for key_id in range(key_size):\n            key_name = HF_INPUT_KEYS[key_id]\n            padded_token = pad_sequence(\n                [token[:, key_id] for token in tokens],\n                batch_first=True,\n                padding_value=self.pad_values[key_id],\n            )\n            output_dict[key_name] = padded_token.to(dtype=torch.int64, device=device)\n        return output_dict\n\n    def get_downsample_rates(self, key: str = None) -> int:\n        return 1\n\n    def forward(self, tokens):\n        # tokens: List of FloatTensor(TxK)\n        # when Featurizer instantiation, tokens is List of FloatTensor(T)\n        # https://github.com/s3prl/s3prl/blob/main/s3prl/upstream/interfaces.py\n        if tokens[0].dim() == 1 and tokens[0].shape[0] == SAMPLE_RATE:\n            print(\"Featurizer instantiation related forward\")\n            tokens[0] = torch.randint(\n                0, self.vocab_size, (20, 1), device=tokens[0].device\n            )\n        input_dict = self.pad_token(tokens)\n        output_values = self.model(\n            **input_dict, output_hidden_states=True\n        )  # Tuple of BxTxF\n        return {\"hidden_states\": output_values.hidden_states}", ""]}
{"filename": "upstream/hf_nlp_ssl/__init__.py", "chunked_list": [""]}
{"filename": "upstream/hf_nlp_ssl/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\n\ndef hf_nlp_ssl(ckpt, *args, **kwargs):\n    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/expert.py", "chunked_list": ["import logging\n\nimport torch\nfrom transformers import AutoConfig, AutoFeatureExtractor, AutoModel\n\nSAMPLE_RATE = 16000\nEXAMPLE_SEC = 5\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\nclass UpstreamExpert(torch.nn.Module):\n    def __init__(self, ckpt, **kwds):\n        super().__init__()\n        config = AutoConfig.from_pretrained(ckpt)\n        self.extracter = AutoFeatureExtractor.from_pretrained(ckpt)\n        self.model = AutoModel.from_config(config)\n\n    def get_downsample_rates(self, key: str = None) -> int:\n        return 320\n\n    def forward(self, wavs):\n        device = wavs[0].device\n        wavs = [wav.detach().cpu().numpy() for wav in wavs]\n        input_values = self.extracter(\n            wavs,\n            return_tensors=\"pt\",\n            padding=True,\n            return_attention_mask=True,\n            sampling_rate=SAMPLE_RATE,\n        ).to(device)\n        output_values = self.model(**input_values, output_hidden_states=True)\n\n        return {\"hidden_states\": output_values.hidden_states}", ""]}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/__init__.py", "chunked_list": [""]}
{"filename": "upstream/hf_speechssl_no_pretrained_weights/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\n\ndef hf_speechssl_no_pretrained_weights(ckpt, *args, **kwargs):\n    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
{"filename": "upstream/embedding/expert.py", "chunked_list": ["import logging\n\nimport torch\nimport torch.nn as nn\nimport yaml\nfrom espnet2.bin.tts_inference import Text2Speech\nfrom torch.nn.utils.rnn import pad_sequence\n\nSAMPLE_RATE = 16000\n", "SAMPLE_RATE = 16000\n\nlogger = logging.getLogger(__name__)\n\n\nclass UpstreamExpert(torch.nn.Module):\n    def __init__(self, ckpt, model_config=None, **kwds):\n        super().__init__()\n        # ckpt is the ESPnet TTS model name such as \"kan-bayashi/ljspeech_vits\"\n        text2speech = Text2Speech.from_pretrained(ckpt, device=\"cuda\")\n        # adding two value for [PAD] and [SEP] token (define [PAD] and [SEP] token as 0 and 1)\n        self.vocab_size = (\n            len(text2speech.preprocess_fn.token_id_converter.token_list) + 2\n        )\n        print(f\"Phoneme vocabulary size is {self.vocab_size}\")\n\n        if model_config is not None:\n            print(\n                \"[UpstreamExpert] - Using upstream expert config file from:\",\n                model_config,\n            )\n            with open(model_config, \"r\") as file:\n                options = yaml.load(file, Loader=yaml.FullLoader)\n        else:\n            print(\"[UpstreamExpert] - Using the default upstream expert config\")\n            options = {\n                \"embedding_size\": 256,\n            }\n        self.model = nn.Embedding(\n            self.vocab_size, options[\"embedding_size\"], padding_idx=0\n        )\n        print(f\"Embedding size is {options['embedding_size']}\")\n\n    def get_downsample_rates(self, key: str = None) -> int:\n        return 1\n\n    def forward(self, tokens):\n        # tokens: List of FloatTensor(T)\n        # when Featurizer instantiation, tokens is List of FloatTensor(T)\n        # https://github.com/s3prl/s3prl/blob/main/s3prl/upstream/interfaces.py\n        if tokens[0].dim() == 1 and tokens[0].shape[0] == SAMPLE_RATE:\n            print(\"Featurizer instantiation related forward\")\n            tokens[0] = torch.randint(\n                0, self.vocab_size, (20, 1), device=tokens[0].device\n            )\n        padded_token = pad_sequence(tokens, batch_first=True, padding_value=0).to(\n            dtype=torch.int64\n        )  # BxT\n        output_values = self.model(padded_token)  # BxTxF\n        return {\"hidden_states\": output_values}", ""]}
{"filename": "upstream/embedding/__init__.py", "chunked_list": [""]}
{"filename": "upstream/embedding/hubconf.py", "chunked_list": ["from .expert import UpstreamExpert as _UpstreamExpert\n\n\ndef embedding(ckpt, *args, **kwargs):\n    return _UpstreamExpert(ckpt, *args, **kwargs)\n"]}
