{"filename": "__init__.py", "chunked_list": [""]}
{"filename": "utils/__init__.py", "chunked_list": [""]}
{"filename": "utils/dir_utils.py", "chunked_list": ["import argparse\nimport datetime\nimport json\nimport os\nimport random\nimport subprocess\nimport sys\nimport munch\nimport yaml\nimport numpy as np", "import yaml\nimport numpy as np\nimport torch\n\ndef mkdirs(*paths):\n    \"\"\"Makes a list of directories.\n\n    \"\"\"\n    for path in paths:\n        if not os.path.exists(path):\n            os.makedirs(path)", "\ndef set_dir_from_config(config):\n    \"\"\"Creates a output folder for experiment (and save config files).\n\n    Naming format: {root (e.g. results)}/{tag (exp id)}/{seed}_{timestamp}_{git commit id}\n\n    \"\"\"\n    # Make run folder (of a seed run for an experiment)\n    seed = str(config.seed) if config.seed is not None else \"-\"\n    timestamp = str(datetime.datetime.now().strftime(\"%b-%d-%H-%M-%S\"))\n    try:\n        commit_id = subprocess.check_output(\n            [\"git\", \"describe\", \"--tags\", \"--always\"]\n        ).decode(\"utf-8\").strip()\n        commit_id = str(commit_id)\n    except:\n        commit_id = \"-\"\n    run_dir = \"seed{}_{}_{}\".format(seed, timestamp, commit_id)\n    # Make output folder.\n    config.output_dir = os.path.join(config.output_dir, config.tag, run_dir)\n    mkdirs(config.output_dir)\n    # Save config.\n    with open(os.path.join(config.output_dir, 'config.yaml'), \"w\") as file:\n        yaml.dump(munch.unmunchify(config), file, default_flow_style=False)\n    # Save command.\n    with open(os.path.join(config.output_dir, 'cmd.txt'), 'a') as file:\n        file.write(\" \".join(sys.argv) + \"\\n\")", ""]}
{"filename": "utils/plotting_utils.py", "chunked_list": ["import matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cmx\nfrom mpl_toolkits.mplot3d import Axes3D\ndef scatter3d(x,y,z, cs, colorsMap='jet'):\n    cm = plt.get_cmap(colorsMap)\n    cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))\n    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    ax.scatter(x, y, z, c=scalarMap.to_rgba(cs))\n    scalarMap.set_array(cs)\n    fig.colorbar(scalarMap)\n    plt.show()", "\ndef plot_trained_gp(targets, means, preds, fig_count=0, show=False):\n    lower, upper = preds.confidence_region()\n    fig_count += 1\n    plt.figure(fig_count)\n    plt.fill_between(list(range(lower.shape[0])), lower.detach().numpy(), upper.detach().numpy(), alpha=0.5, label='95%')\n    plt.plot(means.squeeze(), 'r', label='GP Mean')\n    plt.plot(targets.squeeze(), '*k', label='Targets')\n    plt.legend()\n    plt.title('Fitted GP')\n    plt.xlabel('Time (s)')\n    plt.ylabel('v')\n    if show:\n        plt.show()\n\n    return fig_count", ""]}
{"filename": "utils/math_utils.py", "chunked_list": ["import casadi as cs\nimport numpy as np\nimport scipy\n\ndef get_cost_weight_matrix(weights,\n                           dim\n                           ):\n    \"\"\"Gets weight matrix from input args.\n\n    \"\"\"\n    if len(weights) == dim:\n        W = np.diag(weights)\n    elif len(weights) == 1:\n        W = np.diag(weights * dim)\n    else:\n        raise Exception(\"Wrong dimension for cost weights.\")\n    return W", "\ndef csQuadCost(y1, y2, Q):\n    \"\"\"\n    Creates a quadratic cost term || y1 - y2 ||_Q^2 and adds it to cost for use in CasAdi optimization.\n    Args:\n        y1: n x 1 array\n        y2: n x 1 array\n        Q: n x n gain matrix\n        cost: Casadi running cost\n    \"\"\"\n\n    cost = (y1 - y2).T @ Q @ (y1 - y2)\n    return cost", "\ndef rk_discrete(f, n, m, dt):\n    \"\"\"Runge Kutta discretization for the function.\n\n    Args:\n        f (casadi function): Function to discretize.\n        n (int): state dimensions.\n        m (int): input dimension.\n        dt (float): discretization time.\n\n    Return:\n        x_next (casadi function?):\n    \"\"\"\n    X = cs.SX.sym('X', n)\n    U = cs.SX.sym('U', m)\n    # Runge-Kutta 4 integration\n    k1 = f(X,         U)\n    k2 = f(X+dt/2*k1, U)\n    k3 = f(X+dt/2*k2, U)\n    k4 = f(X+dt*k3,   U)\n    x_next = X + dt/6*(k1+2*k2+2*k3+k4)\n    rk_dyn = cs.Function('rk_f', [X, U], [x_next], ['x0', 'p'], ['xf'])\n\n    return rk_dyn", "\ndef euler_discrete(f, n, m, dt):\n    \"\"\"Euler discretization for the function.\n\n    Args:\n        f (casadi function): Function to discretize.\n        n (int): state dimensions.\n        m (int): input dimension.\n        dt (float): discretization time.\n\n    Return:\n        x_next (casadi function?):\n    \"\"\"\n    X = cs.SX.sym('X', n)\n    U = cs.SX.sym('U', m)\n    x_next = X + f(X, U)*dt\n    euler_dyn = cs.Function('euler_f', [X, U], [x_next], ['x0', 'p'], ['xf'])\n\n    return  euler_dyn", "\ndef discretize_linear_system(A,\n                             B,\n                             dt,\n                             exact=False\n                             ):\n    \"\"\"Discretize a linear system.\n\n    dx/dt = A x + B u\n    --> xd[k+1] = Ad xd[k] + Bd ud[k] where xd[k] = x(k*dt)\n\n    Args:\n        A: np.array, system transition matrix.\n        B: np.array, input matrix.\n        dt: scalar, step time interval.\n        exact: bool, if to use exact discretization.\n\n    Returns:\n        Discretized matrices Ad, Bd.\n\n    \"\"\"\n    state_dim, input_dim = A.shape[1], B.shape[1]\n    if exact:\n        M = np.zeros((state_dim + input_dim, state_dim + input_dim))\n        M[:state_dim, :state_dim] = A\n        M[:state_dim, state_dim:] = B\n        Md = scipy.linalg.expm(M * dt)\n        Ad = Md[:state_dim, :state_dim]\n        Bd = Md[:state_dim, state_dim:]\n    else:\n        I = np.eye(state_dim)\n        Ad = I + A * dt\n        Bd = B * dt\n    return Ad, Bd", ""]}
{"filename": "symbolics/mgf_derivative.py", "chunked_list": ["from sympy import *\ninit_printing(use_unicode=True)\nw1, w2, w3, s, mu, v, sigma = symbols('w1 w2 w3 s mu v sigma')\ngamma1, gamma2, gamma3, gamma4, gamma5, u = symbols('gamma1 gamma2 gamma3 gamma4 gamma5 u')\n\nd = w2*v**2 + w1*v - w3\na = -2*v*w2 - w1\nA = w2\nc = sqrt(sigma)*(a/2 + A*mu)\nlam = sigma", "c = sqrt(sigma)*(a/2 + A*mu)\nlam = sigma\n\nM = exp(s*(d + a*mu + A*mu**2) + s**2*c**2/(1-2*s*lam))*(1-2*s*lam)**(-1/2)\n\ndmds = diff(M,s)\nprint(\"Mean:\")\npprint(dmds.subs(s,0))\nprint()\nd2mds2 = diff(dmds,s)", "print()\nd2mds2 = diff(dmds,s)\nprint(\"Var:\")\npprint(d2mds2.subs(s,0))\n\nprint('With gammas:')\nmu_val = gamma1 + gamma2 * u\nsigma_val = sqrt(gamma3 + gamma4*u + gamma5*u**2)\ndmds = diff(M,s)\nprint(\"Mean:\")", "dmds = diff(M,s)\nprint(\"Mean:\")\npprint(dmds.subs({s: 0, mu: mu_val, sigma: sigma_val}))\nprint()\nd2mds2 = diff(dmds,s)\nprint(\"Var:\")\npprint(d2mds2.subs({s: 0, mu: mu_val, sigma: sigma_val}))\n"]}
{"filename": "symbolics/__init__.py", "chunked_list": [""]}
{"filename": "symbolics/step_traj_approx.py", "chunked_list": ["from sympy import *\nimport numpy as np\nfrom quad_1D.quad_1d import Quad1D\nimport matplotlib.pyplot as plt\ninit_printing(use_unicode=True)\nt, delta_t, omega, amp = symbols('t delta_t omega amp')\nz_0 = amp*(0.5 + 0.5*tanh((t-delta_t)*omega))\nz_1 = diff(z_0, t)\nz_2 = diff(z_1, t)\nv_ref = diff(z_2,t)", "z_2 = diff(z_1, t)\nv_ref = diff(z_2,t)\n\npprint('z_0:')\npprint(z_0)\npprint('z_1:')\npprint(z_1)\npprint('z_2:')\npprint(z_2)\npprint('vref:')", "pprint(z_2)\npprint('vref:')\npprint(v_ref)\n\ndt = 0.01\nT_prior = 7 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0.0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type='increasing_freq')\nAmp = 0.2", "quad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type='increasing_freq')\nAmp = 0.2\nomega = 20.0\nt = np.arange(0, 5, dt)\n\nz_ref, v_ref = quad_prior.reference_generator(t, Amp, omega, ref_type='step')\n"]}
{"filename": "learning/gpmpc_gp_utils.py", "chunked_list": ["\"\"\"Utility functions for Gaussian Processes.\n\n\"\"\"\nimport os.path\nimport numpy as np\nimport gpytorch\nimport torch\nimport matplotlib.pyplot as plt\nimport casadi as cs\nimport shelve", "import casadi as cs\nimport shelve\nfrom copy import deepcopy\nfrom math import ceil\nfrom munch import munchify\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import qmc", "from sklearn.model_selection import train_test_split\nfrom scipy.stats import qmc\n\nfrom utils.dir_utils import mkdirs\n\ntorch.manual_seed(0)\n\n\ndef covSEard(x, z, ell, sf2):\n    \"\"\"GP squared exponential kernel.\n\n    This function is based on the 2018 GP-MPC library by Helge-Andr\u00e9 Lang\u00e5ker\n\n    Args:\n        x (np.array or casadi.MX/SX): First vector.\n        z (np.array or casadi.MX/SX): Second vector.\n        ell (np.array or casadi.MX/SX): Length scales.\n        sf2 (float or casadi.MX/SX): output scale parameter.\n\n    Returns:\n        SE kernel (casadi.MX/SX): SE kernel.\n\n    \"\"\"\n    dist = cs.sum1((x - z) ** 2 / ell ** 2)\n    return sf2 * cs.SX.exp(-.5 * dist)", "def covSEard(x, z, ell, sf2):\n    \"\"\"GP squared exponential kernel.\n\n    This function is based on the 2018 GP-MPC library by Helge-Andr\u00e9 Lang\u00e5ker\n\n    Args:\n        x (np.array or casadi.MX/SX): First vector.\n        z (np.array or casadi.MX/SX): Second vector.\n        ell (np.array or casadi.MX/SX): Length scales.\n        sf2 (float or casadi.MX/SX): output scale parameter.\n\n    Returns:\n        SE kernel (casadi.MX/SX): SE kernel.\n\n    \"\"\"\n    dist = cs.sum1((x - z) ** 2 / ell ** 2)\n    return sf2 * cs.SX.exp(-.5 * dist)", "\n\nclass ZeroMeanIndependentMultitaskGPModel(gpytorch.models.ExactGP):\n    \"\"\"Multidimensional Gaussian Process model with zero mean function.\n\n    Or constant mean and radial basis function kernel (SE).\n\n    \"\"\"\n\n    def __init__(self, train_x, train_y, likelihood, nx):\n        \"\"\"Initialize a multidimensional Gaussian Process model with zero mean function.\n\n        Args:\n            train_x (torch.Tensor): input training data (input_dim X N samples).\n            train_y (torch.Tensor): output training data (output dim x N samples).\n            likelihood (gpytorch.likelihood): Likelihood function (gpytorch.likelihoods.MultitaskGaussianLikelihood).\n            nx (int): dimension of the target output (output dim)\n\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.n = nx\n        # For Zero mean function.\n        self.mean_module = gpytorch.means.ZeroMean(batch_shape=torch.Size([self.n]))\n        # For constant mean function.\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([self.n]),\n                                                                                    ard_num_dims=train_x.shape[1]),\n                                                         batch_shape=torch.Size([self.n]),\n                                                         ard_num_dims=train_x.shape[1])\n\n    def forward(self, x):\n        \"\"\"\n\n        \"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(gpytorch.distributions.MultivariateNormal(\n            mean_x,\n            covar_x))", "\n\nclass ZeroMeanIndependentGPModel(gpytorch.models.ExactGP):\n    \"\"\"Single dimensional output Gaussian Process model with zero mean function.\n\n    Or constant mean and radial basis function kernel (SE).\n\n    \"\"\"\n\n    def __init__(self, train_x, train_y, likelihood):\n        \"\"\"Initialize a single dimensional Gaussian Process model with zero mean function.\n\n        Args:\n            train_x (torch.Tensor): input training data (input_dim X N samples).\n            train_y (torch.Tensor): output training data (output dim x N samples).\n            likelihood (gpytorch.likelihood): Likelihood function (gpytorch.likelihoods.GaussianLikelihood).\n\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        # For Zero mean function.\n        self.mean_module = gpytorch.means.ZeroMean()\n        # For constant mean function.\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]),\n                                                         ard_num_dims=train_x.shape[1])\n\n    def forward(self, x):\n        \"\"\"\n\n        \"\"\"\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)", "\n\nclass GaussianProcessCollection:\n    \"\"\"Collection of GaussianProcesses for multioutput GPs.\n\n    \"\"\"\n\n    def __init__(self, model_type, likelihood, target_dim, input_mask=None, target_mask=None, normalize=False):\n        \"\"\"Creates a single GaussianProcess for each output dimension.\n\n        Args:\n            model_type (gpytorch model class): Model class for the GP (ZeroMeanIndependentGPModel).\n            likelihood (gpytorch.likelihood): likelihood function.\n            target_dim (int): Dimension of the output (how many GPs to make).\n            input_mask (list): Input dimensions to keep. If None, use all input dimensions.\n            target_mask (list): Target dimensions to keep. If None, use all target dimensions.\n            normalize (bool): If True, scale all data between -1 and 1.\n\n        \"\"\"\n        self.gp_list = []\n        self.model_type = model_type\n        self.likelihood = likelihood\n        self.optimizer = None\n        self.model = None\n        self.NORMALIZE = normalize\n        self.input_mask = input_mask\n        self.target_mask = target_mask\n        for i in range(target_dim):\n            self.gp_list.append(GaussianProcess(model_type,\n                                                deepcopy(likelihood),\n                                                input_mask=input_mask,\n                                                normalize=normalize))\n\n    def _init_properties(self, train_inputs, train_targets):\n        \"\"\"Initialize useful properties.\n\n        Args:\n            train_inputs, train_targets (torch.tensors): Input and target training data.\n\n        \"\"\"\n        target_dimension = train_targets.shape[1]\n        self.input_dimension = train_inputs.shape[1]\n        self.output_dimension = target_dimension\n        self.n_training_samples = train_inputs.shape[0]\n\n    def init_with_hyperparam(self, train_inputs, train_targets, path_to_statedicts):\n        \"\"\"Load hyperparameters from a state_dict.\n\n        Args:\n            train_inputs, train_targets (torch.tensors): Input and target training data.\n            path_to_statedicts (str): Path to where the state dicts are saved.\n\n        \"\"\"\n        self._init_properties(train_inputs, train_targets)\n        target_dimension = train_targets.shape[1]\n        gp_K_plus_noise_list = []\n        gp_K_plus_noise_inv_list = []\n        for gp_ind, gp in enumerate(self.gp_list):\n            path = os.path.join(path_to_statedicts, 'best_model_%s.pth' % self.target_mask[gp_ind])\n            print(\"#########################################\")\n            print(\"#       Loading GP dimension %s         #\" % self.target_mask[gp_ind])\n            print(\"#########################################\")\n            print('Path: %s' % path)\n            gp.init_with_hyperparam(train_inputs, train_targets[:, self.target_mask[gp_ind]], path)\n            gp_K_plus_noise_list.append(gp.model.K_plus_noise.detach())\n            gp_K_plus_noise_inv_list.append(gp.model.K_plus_noise_inv.detach())\n            print('Loaded!')\n        gp_K_plus_noise = torch.stack(gp_K_plus_noise_list)\n        gp_K_plus_noise_inv = torch.stack(gp_K_plus_noise_inv_list)\n        self.K_plus_noise = gp_K_plus_noise\n        self.K_plus_noise_inv = gp_K_plus_noise_inv\n        self.casadi_predict = self.make_casadi_predict_func()\n\n    def get_hyperparameters(self, as_numpy=False):\n        \"\"\"Get the outputscale and lengthscale from the kernel matrices of the GPs.\n\n        \"\"\"\n        lengthscale_list = []\n        output_scale_list = []\n        noise_list = []\n        for gp in self.gp_list:\n            lengthscale_list.append(gp.model.covar_module.base_kernel.lengthscale.detach())\n            output_scale_list.append(gp.model.covar_module.outputscale.detach())\n            noise_list.append(gp.model.likelihood.noise.detach())\n        lengthscale = torch.cat(lengthscale_list)\n        outputscale = torch.Tensor(output_scale_list)\n        noise = torch.Tensor(noise_list)\n        if as_numpy:\n            return lengthscale.numpy(), outputscale.numpy(), noise.numpy(), self.K_plus_noise.detach().numpy()\n        else:\n            return lengthscale, outputscale, noise, self.K_plus_noise\n\n    def train(self,\n              train_x_raw,\n              train_y_raw,\n              test_x_raw,\n              test_y_raw,\n              n_train=[500],\n              learning_rate=[0.01],\n              gpu=False,\n              dir='results'):\n        \"\"\"Train the GP using Train_x and Train_y.\n\n        Args:\n            train_x: Torch tensor (N samples [rows] by input dim [cols])\n            train_y: Torch tensor (N samples [rows] by target dim [cols])\n\n        \"\"\"\n        self._init_properties(train_x_raw, train_y_raw)\n        self.model_paths = []\n        mkdirs(dir)\n        gp_K_plus_noise_inv_list = []\n        gp_K_plus_noise_list = []\n        for gp_ind, gp in enumerate(self.gp_list):\n            lr = learning_rate[self.target_mask[gp_ind]]\n            n_t = n_train[self.target_mask[gp_ind]]\n            print(\"#########################################\")\n            print(\"#      Training GP dimension %s         #\" % self.target_mask[gp_ind])\n            print(\"#########################################\")\n            print(\"Train iterations: %s\" % n_t)\n            print(\"Learning Rate:: %s\" % lr)\n            gp.train(train_x_raw,\n                     train_y_raw[:, self.target_mask[gp_ind]],\n                     test_x_raw,\n                     test_y_raw[:, self.target_mask[gp_ind]],\n                     n_train=n_t,\n                     learning_rate=lr,\n                     gpu=gpu,\n                     fname=os.path.join(dir, 'best_model_%s.pth' % self.target_mask[gp_ind]))\n            self.model_paths.append(dir)\n            gp_K_plus_noise_list.append(gp.model.K_plus_noise)\n            gp_K_plus_noise_inv_list.append(gp.model.K_plus_noise_inv)\n        gp_K_plus_noise = torch.stack(gp_K_plus_noise_list)\n        gp_K_plus_noise_inv = torch.stack(gp_K_plus_noise_inv_list)\n        self.K_plus_noise = gp_K_plus_noise\n        self.K_plus_noise_inv = gp_K_plus_noise_inv\n        self.casadi_predict = self.make_casadi_predict_func()\n\n    def predict(self, x, requires_grad=False, return_pred=True):\n        \"\"\"\n\n        Args:\n            x : torch.Tensor (N_samples x input DIM).\n\n        Return\n            Predictions\n                mean : torch.tensor (nx X N_samples).\n                lower : torch.tensor (nx X N_samples).\n                upper : torch.tensor (nx X N_samples).\n\n        \"\"\"\n        means_list = []\n        cov_list = []\n        pred_list = []\n        for gp in self.gp_list:\n            if return_pred:\n                mean, cov, pred = gp.predict(x, requires_grad=requires_grad, return_pred=return_pred)\n                pred_list.append(pred)\n            else:\n                mean, cov = gp.predict(x, requires_grad=requires_grad, return_pred=return_pred)\n            means_list.append(mean)\n            cov_list.append(cov)\n        means = torch.tensor(means_list)\n        cov = torch.diag(torch.cat(cov_list).squeeze())\n        if return_pred:\n            return means, cov, pred_list\n        else:\n            return means, cov\n\n    def make_casadi_predict_func(self):\n        \"\"\"\n        Assume train_inputs and train_tergets are already\n        \"\"\"\n\n        means_list = []\n        Nz = len(self.input_mask)\n        Ny = len(self.target_mask)\n        z = cs.SX.sym('z1', Nz)\n        y = cs.SX.zeros(Ny)\n        for gp_ind, gp in enumerate(self.gp_list):\n            y[gp_ind] = gp.casadi_predict(z=z)['mean']\n        casadi_predict = cs.Function('pred', [z], [y], ['z'], ['mean'])\n        return casadi_predict\n\n    def prediction_jacobian(self, query):\n        \"\"\"Return Jacobian.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def plot_trained_gp(self, inputs, targets, fig_count=0):\n        \"\"\"Plot the trained GP given the input and target data.\n\n        \"\"\"\n        for gp_ind, gp in enumerate(self.gp_list):\n            fig_count = gp.plot_trained_gp(inputs,\n                                           targets[:, self.target_mask[gp_ind], None],\n                                           self.target_mask[gp_ind],\n                                           fig_count=fig_count)\n            fig_count += 1\n\n    def _kernel_list(self, x1, x2=None):\n        \"\"\"Evaluate the kernel given vectors x1 and x2.\n\n        Args:\n            x1 (torch.Tensor): First vector.\n            x2 (torch.Tensor): Second vector.\n\n        Returns:\n            list of LazyTensor Kernels.\n\n        \"\"\"\n        if x2 is None:\n            x2 = x1\n        # todo: Make normalization at the GPCollection level?\n        # if self.NORMALIZE:\n        #    x1 = torch.from_numpy(self.gp_list[0].scaler.transform(x1.numpy()))\n        #    x2 = torch.from_numpy(self.gp_list[0].scaler.transform(x2.numpy()))\n        k_list = []\n        for gp in self.gp_list:\n            k_list.append(gp.model.covar_module(x1, x2))\n        return k_list\n\n    def kernel(self, x1, x2=None):\n        \"\"\"Evaluate the kernel given vectors x1 and x2.\n\n        Args:\n            x1 (torch.Tensor): First vector.\n            x2 (torch.Tensor): Second vector.\n\n        Returns:\n            Torch tensor of the non-lazy kernel matrices.\n\n        \"\"\"\n        k_list = self._kernel_list(x1, x2)\n        non_lazy_tensors = [k.evaluate() for k in k_list]\n        return torch.stack(non_lazy_tensors)\n\n    def kernel_inv(self, x1, x2=None):\n        \"\"\"Evaluate the inverse kernel given vectors x1 and x2.\n\n        Only works for square kernel.\n\n        Args:\n            x1 (torch.Tensor): First vector.\n            x2 (torch.Tensor): Second vector.\n\n        Returns:\n            Torch tensor of the non-lazy inverse kernel matrices.\n\n        \"\"\"\n        if x2 is None:\n            x2 = x1\n        assert x1.shape == x2.shape, ValueError(\"x1 and x2 need to have the same shape.\")\n        k_list = self._kernel_list(x1, x2)\n        num_of_points = x1.shape[0]\n        # Efficient inversion is performed VIA inv_matmul on the laze tensor with Identity.\n        non_lazy_tensors = [k.inv_matmul(torch.eye(num_of_points).double()) for k in k_list]\n        return torch.stack(non_lazy_tensors)", "\n\nclass GaussianProcess:\n    \"\"\"Gaussian Process decorator for gpytorch.\n\n    \"\"\"\n\n    def __init__(self, model_type, likelihood, input_mask=None, target_mask=None, normalize=False):\n        \"\"\"Initialize Gaussian Process.\n\n        Args:\n            model_type (gpytorch model class): Model class for the GP (ZeroMeanIndependentMultitaskGPModel).\n            likelihood (gpytorch.likelihood): likelihood function.\n            normalize (bool): If True, scale all data between -1 and 1. (prototype and not fully operational).\n\n        \"\"\"\n        self.model_type = model_type\n        self.likelihood = likelihood\n        self.optimizer = None\n        self.model = None\n        self.NORMALIZE = normalize\n        self.input_mask = input_mask\n        self.target_mask = target_mask\n\n    def _init_model(self, train_inputs, train_targets):\n        \"\"\"Init GP model from train inputs and train_targets.\n\n        \"\"\"\n        if train_targets.ndim > 1:\n            target_dimension = train_targets.shape[1]\n        else:\n            target_dimension = 1\n\n        if self.NORMALIZE:\n            # Define normalization scaler.\n            self.scaler = preprocessing.StandardScaler().fit(train_inputs.numpy())\n            train_inputs = torch.from_numpy(self.scaler.transform(train_inputs.numpy()))\n\n        if self.model is None:\n            self.model = self.model_type(train_inputs, train_targets, self.likelihood)\n        # Extract dimensions for external use.\n        self.input_dimension = train_inputs.shape[1]\n        self.output_dimension = target_dimension\n        self.n_training_samples = train_inputs.shape[0]\n\n    def _compute_GP_covariances(self, train_x):\n        \"\"\"Compute K(X,X) + sigma*I and its inverse.\n\n        \"\"\"\n        # Pre-compute inverse covariance plus noise to speed-up computation.\n        K_lazy = self.model.covar_module(train_x.double())\n        K_lazy_plus_noise = K_lazy.add_diag(self.model.likelihood.noise)\n        n_samples = train_x.shape[0]\n        self.model.K_plus_noise = K_lazy_plus_noise.matmul(torch.eye(n_samples).double())\n        self.model.K_plus_noise_inv = K_lazy_plus_noise.inv_matmul(torch.eye(n_samples).double())  # self.model.K_plus_noise_inv_2 = torch.inverse(self.model.K_plus_noise) # Equivalent to above but slower.\n\n    def init_with_hyperparam(self, train_inputs, train_targets, path_to_statedict):\n        \"\"\"Load hyperparameters from a state_dict.\n\n        \"\"\"\n        if self.input_mask is not None:\n            train_inputs = train_inputs[:, self.input_mask]\n        if self.target_mask is not None:\n            train_targets = train_targets[:, self.target_mask]\n        device = torch.device('cpu')\n        state_dict = torch.load(path_to_statedict, map_location=device)\n        self._init_model(train_inputs, train_targets)\n        if self.NORMALIZE:\n            train_inputs = torch.from_numpy(self.scaler.transform(train_inputs.numpy()))\n        self.model.load_state_dict(state_dict)\n        self.model.double()  # needed otherwise loads state_dict as float32\n        self._compute_GP_covariances(train_inputs)\n        self.casadi_predict = self.make_casadi_prediction_func(train_inputs, train_targets)\n\n    def train(self,\n              train_input_data,\n              train_target_data,\n              test_input_data,\n              test_target_data,\n              n_train=500,\n              learning_rate=0.01,\n              gpu=False,\n              fname='best_model.pth', ):\n        \"\"\"Train the GP using Train_x and Train_y.\n\n        Args:\n            train_x: Torch tensor (N samples [rows] by input dim [cols])\n            train_y: Torch tensor (N samples [rows] by target dim [cols])\n\n        \"\"\"\n        train_x_raw = train_input_data\n        train_y_raw = train_target_data\n        test_x_raw = test_input_data\n        test_y_raw = test_target_data\n        if self.input_mask is not None:\n            train_x_raw = train_x_raw[:, self.input_mask]\n            test_x_raw = test_x_raw[:, self.input_mask]\n        if self.target_mask is not None:\n            train_y_raw = train_y_raw[:, self.target_mask]\n            test_y_raw = test_y_raw[:, self.target_mask]\n        self._init_model(train_x_raw, train_y_raw)\n        if self.NORMALIZE:\n            train_x = torch.from_numpy(self.scaler.transform(train_x_raw))\n            test_x = torch.from_numpy(self.scaler.transform(test_x_raw))\n            train_y = train_y_raw\n            test_y = test_y_raw\n        else:\n            train_x = train_x_raw\n            train_y = train_y_raw\n            test_x = test_x_raw\n            test_y = test_y_raw\n        if gpu:\n            train_x = train_x.cuda()\n            train_y = train_y.cuda()\n            test_x = test_x.cuda()\n            test_y = test_y.cuda()\n            self.model = self.model.cuda()\n            self.likelihood = self.likelihood.cuda()\n        self.model.double()\n        self.likelihood.double()\n        self.model.train()\n        self.likelihood.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        last_loss = 99999999\n        best_loss = 99999999\n        loss = torch.tensor(0)\n        i = 0\n        while i < n_train and torch.abs(loss - last_loss) > 1e-2:\n            with torch.no_grad():\n                self.model.eval()\n                self.likelihood.eval()\n                test_output = self.model(test_x)\n                test_loss = -mll(test_output, test_y)\n            self.model.train()\n            self.likelihood.train()\n            self.optimizer.zero_grad()\n            output = self.model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            if i % 100 == 0:\n                print('Iter %d/%d - MLL trian Loss: %.3f, Posterior Test Loss: %0.3f' % (\n                    i + 1, n_train, loss.item(), test_loss.item()))\n\n            self.optimizer.step()\n            # if test_loss < best_loss:\n            #    best_loss = test_loss\n            #    state_dict = self.model.state_dict()\n            #    torch.save(state_dict, fname)\n            #    best_epoch = i\n            if loss < best_loss:\n                best_loss = loss\n                state_dict = self.model.state_dict()\n                torch.save(state_dict, fname)\n                best_epoch = i\n\n            i += 1\n        print(\"Training Complete\")\n        print(\"Lowest epoch: %s\" % best_epoch)\n        print(\"Lowest Loss: %s\" % best_loss)\n        self.model = self.model.cpu()\n        self.likelihood = self.likelihood.cpu()\n        train_x = train_x.cpu()\n        train_y = train_y.cpu()\n        self.model.load_state_dict(torch.load(fname))\n        self._compute_GP_covariances(train_x)\n        self.casadi_predict = self.make_casadi_prediction_func(train_x, train_y)\n\n    def predict(self, x, requires_grad=False, return_pred=True):\n        \"\"\"\n\n        Args:\n            x : torch.Tensor (N_samples x input DIM).\n\n        Returns:\n            Predictions\n                mean : torch.tensor (nx X N_samples).\n                lower : torch.tensor (nx X N_samples).\n                upper : torch.tensor (nx X N_samples).\n\n        \"\"\"\n        self.model.eval()\n        self.likelihood.eval()\n        if type(x) is np.ndarray:\n            x = torch.from_numpy(x).double()\n        if self.input_mask is not None:\n            x = x[:, self.input_mask]\n        if self.NORMALIZE:\n            x = torch.from_numpy(self.scaler.transform(x))\n        if requires_grad:\n            predictions = self.likelihood(self.model(x))\n            mean = predictions.mean\n            cov = predictions.covariance_matrix\n        else:\n            with torch.no_grad(), gpytorch.settings.fast_pred_var(state=True), gpytorch.settings.fast_pred_samples(state=True):\n                predictions = self.likelihood(self.model(x))\n                mean = predictions.mean\n                cov = predictions.covariance_matrix\n        if return_pred:\n            return mean, cov, predictions\n        else:\n            return mean, cov\n\n    def prediction_jacobian(self, query):\n        mean_der, cov_der = torch.autograd.functional.jacobian(lambda x: self.predict(x,\n                                                                                      requires_grad=True,\n                                                                                      return_pred=False),\n                                                               query.double())\n        return mean_der.detach().squeeze()\n\n    def make_casadi_prediction_func(self, train_inputs, train_targets):\n        \"\"\"\n        Assumes train_inputs and train_targets are already masked.\n        \"\"\"\n        train_inputs = train_inputs.numpy()\n        train_targets = train_targets.numpy()\n        lengthscale = self.model.covar_module.base_kernel.lengthscale.detach().numpy()\n        output_scale = self.model.covar_module.outputscale.detach().numpy()\n        Nx = len(self.input_mask)\n        z = cs.SX.sym('z', Nx)\n        K_z_ztrain = cs.Function('k_z_ztrain',\n                                 [z],\n                                 [covSEard(z, train_inputs.T, lengthscale.T, output_scale)],\n                                 ['z'],\n                                 ['K'])\n        predict = cs.Function('pred',\n                              [z],\n                              [K_z_ztrain(z=z)['K'] @ self.model.K_plus_noise_inv.detach().numpy() @ train_targets],\n                              ['z'],\n                              ['mean'])\n        return predict\n\n    def plot_trained_gp(self, inputs, targets, output_label, fig_count=0):\n        if self.target_mask is not None:\n            targets = targets[:, self.target_mask]\n        means, covs, preds = self.predict(inputs)\n        t = np.arange(inputs.shape[0])\n        lower, upper = preds.confidence_region()\n        for i in range(self.output_dimension):\n            fig_count += 1\n            plt.figure(fig_count)\n            if lower.ndim > 1:\n                plt.fill_between(t, lower[:, i].detach().numpy(), upper[:, i].detach().numpy(), alpha=0.5, label='95%')\n                plt.plot(t, means[:, i], 'r', label='GP Mean')\n                plt.plot(t, targets[:, i], '*k', label='Data')\n            else:\n                plt.fill_between(t, lower.detach().numpy(), upper.detach().numpy(), alpha=0.5, label='95%')\n                plt.plot(t, means, 'r', label='GP Mean')\n                plt.plot(t, targets, '*k', label='Data')\n            plt.legend()\n            plt.title('Fitted GP x%s' % output_label)\n            plt.xlabel('Time (s)')\n            plt.ylabel('v')\n            plt.show()\n        return fig_count", "\n\ndef kmeans_centriods(n_cent, data, rand_state=0):\n    \"\"\"kmeans clustering. Useful for finding reasonable inducing points.\n\n    Args:\n        n_cent (int): Number of centriods.\n        data (np.array): Data to find the centroids of n_samples X n_features.\n\n    Return:\n        centriods (np.array): Array of centriods (n_cent X n_features).\n\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_cent, random_state=rand_state).fit(data)\n    return kmeans.cluster_centers_", "\n\nclass DataHandler:\n    @classmethod\n    def load(cls, load_dir):\n        shelf_name = os.path.join(load_dir, 'data_handler.out')\n        init_dict = {}\n        with shelve.open(shelf_name) as myshelf:\n            for key in myshelf:\n                #setattr(cls, key, myshelf['key'])\n                init_dict[key] = myshelf[key]\n            myshelf.close()\n        prior_model_name = os.path.join(load_dir, 'prior_model.casadi')\n        init_dict['prior_model'] = cs.Function.load(prior_model_name)\n        new_class = cls(**init_dict)\n        return new_class\n\n    def __init__(self,\n                 x_data: np.array=None,\n                 u_data: np.array=None,\n                 prior_model: cs.Function=None,\n                 save_dir: str=None,\n                 train_test_ratio: float=0.8,\n                 noise: dict=None,\n                 normalize_inputs=False,\n                 normalize_outputs=False,\n                 num_samples=None,\n                 seed=42):\n\n        self.x_data = x_data\n        self.u_data = u_data\n        self.prior_model = prior_model\n        self.train_test_ratio = train_test_ratio\n        self.num_samples = num_samples\n        self.seed = seed\n        self.noise = noise\n        self.normalize_inputs = normalize_inputs\n        self.normalize_outputs = normalize_outputs\n        self.save_dir = save_dir\n        self.save_dict = deepcopy(self.__dict__)\n        self.data = None\n        self._initialize()\n\n    def _initialize(self):\n        rand_generator = np.random.default_rng(self.seed)\n\n        if isinstance(self.x_data, list):\n            inputs = []\n            targets = []\n            x_seq = []\n            u_seq = []\n            x_next_seq = []\n            for i in range(len(self.x_data)):\n                inputs_i, targets_i, x_seq_i, u_seq_i, x_next_seq_i = trajectory_to_training_data(self.x_data[i],\n                                                                                                  self.u_data[i],\n                                                                                                  self.prior_model)\n                inputs.append(inputs_i)\n                targets.append(targets_i)\n                x_seq.append(x_seq_i)\n                u_seq.append(u_seq_i)\n                x_next_seq.append(x_next_seq_i)\n            inputs = np.vstack(inputs)\n            targets = np.vstack(targets)\n        else:\n            inputs, targets, x_seq, u_seq, x_next_seq = trajectory_to_training_data(self.x_data,\n                                                                                    self.u_data,\n                                                                                    self.prior_model)\n            x_seq = [x_seq]\n            u_seq = [u_seq]\n            x_next_seq = [x_next_seq]\n\n        if self.num_samples is not None and self.num_samples < inputs.shape[0]:\n            interval = int(np.ceil(inputs.shape[0]/self.num_samples))\n        else:\n            interval = 1\n        train_inputs, train_targets, test_inputs, test_targets = make_train_and_test_sets(inputs[::interval,:],\n                                                                                          targets[::interval,:],\n                                                                                          ratio=self.train_test_ratio,\n                                                                                          rand_generator=rand_generator)\n        if self.normalize_inputs or self.normalize_outputs:\n            raise NotImplementedError(\"Normalization is still TBC.\")\n\n        if self.noise is not None:\n            train_targets = add_noise(train_targets, self.noise, rand_generator)\n            test_targets = add_noise(test_targets, self.noise, rand_generator)\n\n        self.data = {'train_inputs': train_inputs,\n                     'train_targets': train_targets,\n                     'test_inputs': test_inputs,\n                     'test_targets': test_targets,\n                     'x_seq': x_seq,\n                     'u_seq': u_seq,\n                     'x_next_seq': x_next_seq}\n        self.data = munchify(self.data)\n\n    def save(self, save_dir=None):\n        if save_dir is None:\n            save_dir = self.save_dir\n        full_dir = os.path.join(save_dir, 'data_handler')\n        mkdirs(full_dir)\n        fname = os.path.join(full_dir, 'data_handler.out')\n        with shelve.open(fname) as myshelf:\n            for kw, val in self.save_dict.items():\n                if not('prior_model' == kw):\n                    myshelf[kw] = val\n            myshelf.close()\n        cs_fname = os.path.join(full_dir, 'prior_model.casadi')\n        self.prior_model.save(cs_fname)\n\n    def select_subsamples_with_kmeans(self, n_sub, seed):\n        centroids = kmeans_centriods(n_sub, self.data.train_inputs, rand_state=seed)\n\n        contiguous_masked_inputs = np.ascontiguousarray(self.data.train_inputs) # required for version sklearn later than 1.0.2\n        inds, dist_mat = pairwise_distances_argmin_min(centroids, contiguous_masked_inputs)\n        input_data = self.data.train_inputs[inds]\n        target_data = self.data.train_targets[inds]\n        return input_data, target_data", "\n\ndef trajectory_to_training_data(x_data, u_data, prior_model):\n        x_seq, u_seq, x_next_seq = gather_training_samples(x_data,\n                                                           u_data)\n\n        inputs, targets = make_inputs_and_targets(x_seq, u_seq, x_next_seq, prior_model)\n\n        return inputs, targets, x_seq, u_seq, x_next_seq\n\ndef gather_training_samples(x_data: np.array,\n                            u_data: np.array):\n    \"\"\"\n    Preprocesses the data into states, next_states, and inputs, and processthem accordingly.\n\n    Args:\n        x_data: n_data+1 X dim(x) state data\n        u_data: n_data X 1 input data (for single input only for now)\n        num_samples: integer for the desired number of samples to take. If None, take them all\n        rand_generator: random number generator for repeatability.\n\n    Returns:\n        x_seq: num_samples X dim(x) for the initial states\n        u_seq: num_samples X dim(u) for the inputs at x_seq\n        x_next_seq: num_samples X dim(x) for the next states.\n    \"\"\"\n    n = u_data.shape[0]\n    rand_inds_int = np.arange(0, n)\n    next_inds_int = rand_inds_int + 1\n    x_seq = x_data[rand_inds_int, :]\n    u_seq = u_data[rand_inds_int, :]\n    x_next_seq = x_data[next_inds_int, :]\n\n    return x_seq, u_seq, x_next_seq", "\ndef gather_training_samples(x_data: np.array,\n                            u_data: np.array):\n    \"\"\"\n    Preprocesses the data into states, next_states, and inputs, and processthem accordingly.\n\n    Args:\n        x_data: n_data+1 X dim(x) state data\n        u_data: n_data X 1 input data (for single input only for now)\n        num_samples: integer for the desired number of samples to take. If None, take them all\n        rand_generator: random number generator for repeatability.\n\n    Returns:\n        x_seq: num_samples X dim(x) for the initial states\n        u_seq: num_samples X dim(u) for the inputs at x_seq\n        x_next_seq: num_samples X dim(x) for the next states.\n    \"\"\"\n    n = u_data.shape[0]\n    rand_inds_int = np.arange(0, n)\n    next_inds_int = rand_inds_int + 1\n    x_seq = x_data[rand_inds_int, :]\n    u_seq = u_data[rand_inds_int, :]\n    x_next_seq = x_data[next_inds_int, :]\n\n    return x_seq, u_seq, x_next_seq", "\ndef make_inputs_and_targets(x_seq,\n                            u_seq,\n                            x_next_seq,\n                            prior_model):\n    \"\"\"Converts trajectory data for GP trianing.\n    Assumes equilibrium is all zeros.\n\n    Args:\n        x_seq (list): state sequence of np.array (nx,).\n        u_seq (list): action sequence of np.array (nu,).\n        x_next_seq (list): next state sequence of np.array (nx,).\n\n    Returns:\n        np.array: inputs for GP training, (N, nx+nu).\n        np.array: targets for GP training, (N, nx).\n\n    \"\"\"\n    # Get the predicted dynamics. This is a linear prior, thus we need to account for the fact that\n    # it is linearized about an eq using self.X_GOAL and self.U_GOAL.\n    x_pred_seq = prior_model(x0=x_seq.T , p=u_seq.T)['xf'].toarray()\n    targets = (x_next_seq.T - x_pred_seq).transpose()  # (N, nx).\n    inputs = np.hstack([x_seq, u_seq])  # (N, nx+nu).\n    return inputs, targets", "\ndef make_train_and_test_sets(inputs, targets, ratio, rand_generator):\n    train_idx, test_idx = train_test_split(\n        list(range(inputs.shape[0])),\n        train_size=ratio,\n        random_state=np.random.RandomState(rand_generator.bit_generator)\n    )\n    train_inputs = inputs[train_idx, :]\n    train_targets = targets[train_idx, :]\n    test_inputs = inputs[test_idx, :]\n    test_targets = targets[test_idx, :]\n    return train_inputs, train_targets, test_inputs, test_targets", "\ndef add_noise(x_array, noise, rand_generator):\n    \"\"\"\n    Add noise to x_array.\n    Args:\n        x_array: (np.array) N x nx array\n        noise: (dict) Dictionary with mean and std for each dim\n        rand_generator: numper random generator\n\n    Returns:\n        noisy_x_array\n    \"\"\"\n    noisy_x_array = x_array + rand_generator.normal(loc=noise['mean'], scale=noise['std'], size=x_array.shape)\n    return noisy_x_array", "\ndef combine_prior_and_gp(prior_model, gp_predict, input_mask, target_mask,):\n\n    x = cs.SX.sym('x', 3)\n    u = cs.SX.sym('u', 1)\n    z = cs.vertcat(x, u)\n    z = z[input_mask,:]\n    Bd = make_bd(3, target_mask)\n\n    x_next = prior_model(x0=x, p=u)['xf'] + Bd @ gp_predict(z=z)['mean']\n\n    combined_dyn = cs.Function('dyn_func',\n                               [x, u],\n                               [x_next],\n                               ['x0', 'p'],\n                               ['xf'])\n    return combined_dyn", "\ndef make_bd(n_sys, tar_mask):\n    Bd = np.zeros((n_sys, len(tar_mask)))\n    for input_ind, output_ind in enumerate(tar_mask):\n        Bd[output_ind, input_ind] = 1\n\n    return Bd\n\ndef get_LHS_samples(lower_bounds=None, upper_bounds=None, num_samples=None, seed=42):\n    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed=seed)\n    samples = sampler.random(n=num_samples)\n    scaled_samples = qmc.scale(samples, lower_bounds, upper_bounds)\n\n    return scaled_samples", "def get_LHS_samples(lower_bounds=None, upper_bounds=None, num_samples=None, seed=42):\n    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed=seed)\n    samples = sampler.random(n=num_samples)\n    scaled_samples = qmc.scale(samples, lower_bounds, upper_bounds)\n\n    return scaled_samples\n\ndef get_MVN_samples(means, cov, num_samples, seed=42):\n    sampler = qmc.MultivariateNormalQMC(mean=means, cov=cov)\n    samples = sampler.random(num_samples)\n\n    return samples", "\ndef get_next_real_states(x, u, quad):\n    x_next = quad.cs_true_flat_dyn_from_x_and_u(x=x, u=u)['x_next'].toarray()\n    return x_next\n\ndef generate_samples_into_sequences(sampler, sampler_args, quad):\n    # LHS params\n    xu_seqs = sampler(**sampler_args)\n    xs = xu_seqs[:, :3].T\n    us = xu_seqs[None, :,-1]\n    x_nexts = get_next_real_states(xs, us, quad)\n    x_data = []\n    u_data = []\n    for i in range(xs.shape[1]):\n        if not(any(np.isnan(x_nexts[:,i]))):\n            x_data.append(np.vstack((xs[:,i], x_nexts[:,i])))\n            u_data.append(us[:,i,None])\n    return x_data, u_data", "\ndef kmeans_centriods(n_cent, data, rand_state=0):\n    \"\"\"kmeans clustering. Useful for finding reasonable inducing points.\n\n    Args:\n        n_cent (int): Number of centriods.\n        data (np.array): Data to find the centroids of n_samples X n_features.\n\n    Return:\n        centriods (np.array): Array of centriods (n_cent X n_features).\n\n    \"\"\"\n    kmeans = KMeans(n_clusters=n_cent, random_state=rand_state).fit(data)\n    return kmeans.cluster_centers_", ""]}
{"filename": "learning/__init__.py", "chunked_list": [""]}
{"filename": "learning/gp_utils.py", "chunked_list": ["import numpy as np\nimport gpytorch\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nfrom gpytorch.constraints import Positive\n\ndef weighted_distance(x1 : torch.Tensor, x2 : torch.Tensor, L : torch.Tensor) -> torch.Tensor:\n    \"\"\"Computes (x1-x2)^T L (x1-x2)\n    Args:\n        x1 (torch.tensor) : N x n tensor (N is number of samples and n is dimension of vector)\n        x2 (torch.tensor) : M x n tensor\n        L (torch.tensor) : nxn wieght matrix\n\n    Returns:\n        weighted_distances (torch.tensor) : N x M tensor of all the prossible products\n\n    \"\"\"\n    N = x1.shape[0]\n    M = x2.shape[0]\n    n = L.shape[0]\n    # subtract all vectors in x2 from all vectors in x1 (results in NxMxn matrix)\n    diff = x1.unsqueeze(1) - x2\n    diff_T = diff.reshape(N,M,n,1)\n    diff = diff.reshape(N,M,1,n)\n    L = L.reshape(1,1,n,n)\n    L = L.repeat(N,M,1,1)\n    weighted_distances = torch.matmul(diff, torch.matmul(L,diff_T))\n    return weighted_distances.squeeze()", "\ndef squared_exponential(x1,x2,L,var):\n    return var * torch.exp(-0.5 * weighted_distance(x1, x2, L))\n\nclass AffineKernel(gpytorch.kernels.Kernel):\n    is_stationary = False\n    def __init__(self, input_dim,\n                 length_prior=None,\n                 length_constraint=None,\n                 variance_prior=None,\n                 variance_constraint=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.input_dim = input_dim\n        self.register_parameter(\n            name='raw_length', parameter=torch.nn.Parameter(torch.zeros(2*(self.input_dim-1)))\n        )\n        self.register_parameter(\n            name='raw_variance', parameter=torch.nn.Parameter(torch.zeros(2))\n        )\n        # set the parameter constraint to be positive, when nothing is specified\n        if length_constraint is None:\n            length_constraint = Positive()\n        if variance_constraint is None:\n            variance_constraint = Positive()\n        # register the constraints\n        self.register_constraint(\"raw_length\", length_constraint)\n        self.register_constraint(\"raw_variance\", variance_constraint)\n        # set the parameter prior, see\n        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n        if length_prior is not None:\n            self.register_prior(\n                \"length_prior\",\n                length_prior,\n                lambda m: m.length,\n                lambda m, v: m._set_length(v),\n            )\n        if variance_prior is not None:\n            self.register_prior(\n                \"variance_prior\",\n                variance_prior,\n                lambda m: m.variance,\n                lambda m, v: m._set_variance(v),\n            )\n\n    # now set up the 'actual' paramter\n    @property\n    def length(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_length_constraint.transform(self.raw_length)\n\n    @length.setter\n    def length(self, value):\n        return self._set_length(value)\n\n    def _set_length(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_length)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_length=self.raw_length_constraint.inverse_transform(value))\n\n\n    @property\n    def variance(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_variance_constraint.transform(self.raw_variance)\n\n    @variance.setter\n    def variance(self, value):\n        return self._set_variance(value)\n\n    def _set_variance(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_variance)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_variance=self.raw_variance_constraint.inverse_transform(value))\n\n    def kappa_alpha(self,x1, x2):\n        L_alpha = torch.diag(1 / (self.length[0:self.input_dim-1] ** 2))\n        var_alpha = self.variance[0]\n        return squared_exponential(x1, x2, L_alpha, var_alpha)\n\n    def kappa_beta(self, x1, x2):\n        L_beta = torch.diag(1 / (self.length[self.input_dim-1:] ** 2))\n        var_beta = self.variance[1]\n        return squared_exponential(x1, x2, L_beta, var_beta)\n\n    def kappa(self,x1, x2):\n        z1 = x1[:, 0:self.input_dim - 1]\n        u1 = x1[:, -1, None]\n        z2 = x2[:, 0:self.input_dim - 1]\n        u2 = x2[:, -1, None]\n        u_mat = u1.unsqueeze(1) * u2\n        kappa = self.kappa_alpha(z1, z2) + self.kappa_beta(z1, z2) * u_mat.squeeze()\n        if kappa.dim() < 2:\n            return kappa.unsqueeze(0)\n        else:\n            return kappa\n    # this is the kernel function\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        if last_dim_is_batch:\n            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n        kern = self.kappa(x1, x2)\n        if diag:\n            return kern.diag()\n        else:\n            return kern", "\nclass AffineGP(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.input_dim = train_x.shape[1]\n        #self.output_dim = train_y.shape[1]\n        self.output_dim = 1\n        self.n = 1     #output dimension\n        #self.mean_module = gpytorch.means.ConstantMean()\n        self.mean_module = None\n        self.covar_module = AffineKernel(self.input_dim)\n        self.K_plus_noise_inv = None\n\n    def forward(self, x):\n        mean_x = self.mean_module(x) # is this needed for ZeroMean?\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n    def compute_gammas(self, query):\n        return NotImplementedError\n\n    def mean_and_cov_from_gammas(self,query):\n        gamma_1, gamma_2, gamma_3, gamma_4, gamma_5 = self.compute_gammas(query)\n        u = query[:, None, 1]\n        means_from_gamma = gamma_1 + gamma_2.mul(u)\n        covs_from_gamma = gamma_3 + gamma_4.mul(u) + gamma_5.mul(u ** 2) + self.likelihood.noise.detach()\n        upper_from_gamma = means_from_gamma + covs_from_gamma.sqrt() * 2\n        lower_from_gamma = means_from_gamma - covs_from_gamma.sqrt() * 2\n        return means_from_gamma, covs_from_gamma, upper_from_gamma, lower_from_gamma", "\nclass ZeroMeanAffineGP(AffineGP):\n    def __init__(self, train_x, train_y, likelihood):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n\n    def compute_gammas(self, query):\n        # Parse inputs\n        with torch.no_grad():\n            n_train_samples = self.train_targets.shape[0]\n            n_query_samples = query.shape[0]\n            zq = query[:,0:self.input_dim-1]\n            uq = query[:,-1,None]\n            z_train = self.train_inputs[0][:,0:self.input_dim-1]\n            u_train = self.train_inputs[0][:,-1,None].tile(n_query_samples)\n            # Precompute useful matrics\n            k_a = self.covar_module.kappa_alpha(zq, z_train)\n            if k_a.dim() == 1:\n                k_a = k_a.unsqueeze(0)\n            k_b = self.covar_module.kappa_beta(zq, z_train).mul(u_train.T)\n            if k_b.dim() == 1:\n                k_b = k_b.unsqueeze(0)\n            Psi = self.train_targets.reshape((n_train_samples,1))\n            # compute gammas (Note: inv_matmul(R, L) = L * inv(K) * R\n            gamma_1 = k_a @ self.K_plus_noise_inv @ Psi\n            gamma_2 = k_b @ self.K_plus_noise_inv @ Psi\n            gamma_3 = torch.diag(self.covar_module.kappa_alpha(zq,zq) - k_a @ self.K_plus_noise_inv @ k_a.T)\n            gamma_4 = torch.diag(-( k_b @ self.K_plus_noise_inv @ k_a.T + k_a @ self.K_plus_noise_inv @ k_b.T))\n            gamma_5 = torch.diag(self.covar_module.kappa_beta(zq,zq) - k_b @ self.K_plus_noise_inv @ k_b.T)\n        return gamma_1, gamma_2, gamma_3.unsqueeze(1), gamma_4.unsqueeze(1), gamma_5.unsqueeze(1)", "\nclass ConstantMeanAffineGP(AffineGP):\n    def __init__(self, train_x, train_y, likelihood, mean_prior=None):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n            mean_prior (NOT SURE) : prior on the constant mean\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n\n    def compute_gammas(self, query):\n        # Parse inputs\n        with torch.no_grad():\n            n_train_samples = self.train_targets.shape[0]\n            n_query_samples = query.shape[0]\n            zq = query[:,0:self.input_dim-1]\n            uq = query[:,-1,None]\n            z_train = self.train_inputs[0][:,0:self.input_dim-1]\n            u_train = self.train_inputs[0][:,-1,None].tile(n_query_samples)\n            # Precompute useful matrics\n            k_a = self.covar_module.kappa_alpha(zq, z_train)\n            if k_a.dim() == 1:\n                k_a = k_a.unsqueeze(0)\n            k_b = self.covar_module.kappa_beta(zq, z_train).mul(u_train.T)\n            if k_b.dim() == 1:\n                k_b = k_b.unsqueeze(0)\n            Psi = self.train_targets.reshape((n_train_samples,1))\n            # compute gammas (Note: inv_matmul(R, L) = L * inv(K) * R\n            gamma_1 = k_a @ self.K_plus_noise_inv @ (Psi - u_train*self.mean_module.constant)\n            gamma_2 = self.mean_module.constant + k_b @ self.K_plus_noise_inv @ (Psi - u_train*self.mean_module.constant)\n            gamma_3 = torch.diag(self.covar_module.kappa_alpha(zq,zq) - k_a @ self.K_plus_noise_inv @ k_a.T)\n            gamma_4 = torch.diag(-( k_b @ self.K_plus_noise_inv @ k_a.T + k_a @ self.K_plus_noise_inv @ k_b.T))\n            gamma_5 = torch.diag(self.covar_module.kappa_beta(zq,zq) - k_b @ self.K_plus_noise_inv @ k_b.T)\n        return gamma_1, gamma_2, gamma_3.unsqueeze(1), gamma_4.unsqueeze(1), gamma_5.unsqueeze(1)", "\nclass GaussianProcess():\n    def __init__(self, model_type, likelihood, n, save_dir):\n        \"\"\"\n        Gaussian Process decorator for gpytorch\n        Args:\n            model_type (gpytorch model class): Model class for the GP (ZeroMeanIndependentMultitaskGPModel)\n            likelihood (gpytorch.likelihood): likelihood function\n            n (int): Dimension of input state space\n\n        \"\"\"\n        self.model_type = model_type\n        self.likelihood = likelihood\n        self.m = n\n        self.optimizer = None\n        self.model = None\n        self.save_dir = save_dir\n\n\n    def init_with_hyperparam(self,\n                            path_to_model,\n                             train_inputs=None,\n                             train_targets=None\n                             ):\n        device = torch.device('cpu')\n        fname_sd = os.path.join(path_to_model, 'model.pth')\n        state_dict = torch.load(fname_sd, map_location=device)\n        if train_targets is None or train_inputs is None:\n            fname_data = os.path.join(path_to_model, 'train_data.pt')\n            data = torch.load(fname_data)\n            if train_inputs is None:\n                train_inputs = data['inputs']\n            if train_targets is None:\n                train_targets = data['targets']\n        if self.model is None:\n            self.model = self.model_type(train_inputs,\n                                         train_targets,\n                                         self.likelihood)\n\n        self.model.load_state_dict(state_dict)\n        self.model.double() # needed\n        self._compute_GP_covariances(train_inputs)\n\n    def _compute_GP_covariances(self,\n                                train_x\n                                ):\n        \"\"\"Compute K(X,X) + sigma*I and its inverse.\n\n        \"\"\"\n        # Pre-compute inverse covariance plus noise to speed-up computation.\n        K_lazy = self.model.covar_module(train_x.double())\n        K_lazy_plus_noise = K_lazy.add_diag(self.model.likelihood.noise)\n        n_samples = train_x.shape[0]\n        self.model.K_plus_noise_inv = K_lazy_plus_noise.inv_matmul(torch.eye(n_samples).double())\n\n    def train(self, train_x, train_y, n_train=150, learning_rate=0.01, gpu=True):\n        \"\"\"\n        Train the GP using Train_x and Train_y\n        train_x: Torch tensor (dim input x N samples)\n        train_y: Torch tensor (nx x N samples)\n\n        \"\"\"\n        self.n = train_x.shape[1]\n        self.m = 1\n        self.output_dim = 1\n        self.input_dim = train_x.shape[1]\n        if self.model is None:\n            self.model = self.model_type(train_x, train_y, self.likelihood)\n        else:\n            train_x = torch.reshape(train_x, self.model.train_inputs[0].shape)\n            train_x = torch.cat([train_x, self.model.train_inputs[0]])\n            train_y = torch.cat([train_y, self.model.train_targets])\n            self.model.set_train_data(train_x, train_y, False)\n        if gpu:\n            train_x = train_x.cuda()\n            train_y = train_y.cuda()\n            self.model.cuda()\n            self.likelihood.cuda()\n\n        self.model.double()\n        self.likelihood.double()\n        self.model.train()\n        self.likelihood.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        for i in range(n_train):\n            self.optimizer.zero_grad()\n            output = self.model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            print('Iter %d/%d - Loss: %.3f' % (i + 1, n_train, loss.item()))\n            self.optimizer.step()\n\n        # compute inverse covariance plus noise for faster computation later\n        self.model = self.model.cpu()\n        self.likelihood = self.likelihood.cpu()\n        train_x = train_x.cpu()\n        train_y = train_y.cpu()\n        state_dict = self.model.state_dict()\n        fname = os.path.join(self.save_dir, 'model.pth')\n        torch.save(state_dict, fname)\n        data = {'inputs': train_x, 'targets': train_y}\n        torch.save(data, os.path.join(self.save_dir, 'train_data.pt'))\n        self._compute_GP_covariances(train_x)\n\n    def predict(self, x, requires_grad=False, return_pred=True):\n        \"\"\"\n        x : torch.Tensor (input dim X N_samples)\n        Return\n            Predicitons\n            mean : torch.tensor (nx X N_samples)\n            lower : torch.tensor (nx X N_samples)\n            upper : torch.tensor (nx X N_samples)\n        \"\"\"\n        #x = torch.from_numpy(x).double()\n        self.model.eval()\n        self.likelihood.eval()\n        #with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        if type(x) is np.ndarray:\n            x = torch.from_numpy(x).double()\n\n        if requires_grad:\n            predictions = self.likelihood(self.model(x))\n            mean = predictions.mean\n            cov = predictions.covariance_matrix\n        else:\n            with torch.no_grad():\n                predictions = self.likelihood(self.model(x))\n                mean = predictions.mean\n                cov = predictions.covariance_matrix\n        if return_pred:\n            return mean, cov, predictions\n        else:\n            return mean, cov\n\n    def prediction_jacobian(self, query):\n        gammas = self.model.compute_gammas(query)\n        mean_der = gammas[1]\n        cov_der = gammas[4]\n        #mean_der, _ = torch.autograd.functional.jacobian(\n        #                        lambda x: self.predict(x, requires_grad=True, return_pred=False),\n        #                        query.double())\n        #k_query_query = torch.autograd.functional.hessian(\n        #                               lambda x: self.model.covar_module.kappa(x,x), query.double()\n        #)\n        #k_v_v = k_query_query.squeeze()[-1,-1]\n        #k_a_prime = torch.autograd.functional.jacobian(\n        #        lambda x: self.model.covar_module.kappa(x, self.model.train_inputs[0]), query.double()\n        #)\n        #k_a = k_a_prime.squeeze()[:,-1,None]\n        #cov_der = k_v_v - k_a.T @ self.model.K_plus_noise_inv @ k_a #+ self.model.likelihood.noise\n\n        #k_v_v = self.model.covar_module.kappa_beta(query[:,None,0:3], query[:,None,0:3])\n        #u_train = self.model.train_inputs[0][:, -1, None]\n        #k_b = self.model.covar_module.kappa_beta(query[:,None,0:3], self.model.train_inputs[0][:,0:3]).mul(u_train.T)\n        #if k_b.dim() == 1:\n        #    k_b = k_b.unsqueeze(0)\n        ##k_b = self.model.covar_module.kappa_beta(query[:,None,0:3],self.model.train_inputs[0][:,0:3]).unsqueeze(0)\n        #cov_der = k_v_v - k_b @ self.model.K_plus_noise_inv @ k_b.T  #+ self.model.likelihood.noise\n        #cov_der = k_v_v\n\n        return mean_der.detach(), cov_der.detach()\n\n    def plot_trained_gp(self, t, fig_count=0):\n        means, covs, preds = self.predict(self.model.train_inputs[0])\n        lower, upper = preds.confidence_region()\n        fig_count += 1\n        plt.figure(fig_count)\n        plt.fill_between(t, lower.detach().numpy(), upper.detach().numpy(), alpha=0.5, label='95%')\n        plt.plot(t, means, 'r', label='GP Mean')\n        plt.plot(t, self.model.train_targets, '*k', label='Data')\n        plt.legend()\n        plt.title('Fitted GP')\n        plt.xlabel('Time (s)')\n        plt.ylabel('v')\n        plt.show()\n\n        return fig_count", "\ndef affine_kernel(z1, z2, params_a, params_b):\n    variance_a = params_a[0]\n    length_scales_a = params_a[1:]\n    variance_b = params_b[1]\n    length_scales_b = params_b[1:]\n\n    x1 = z1[:,0:-1]\n    x2 = z2[:,0:-1]\n    k_a = se_kernel(x1, x2, variance_a, length_scales_a)\n    k_b = se_kernel_u(z1, z2, variance_b, length_scales_b)\n\n\n    k = k_a + k_b\n    return k", "\ndef se_kernel_u(z1, z2, variance, length_scales):\n    \"\"\"\n    x1 = Nsamples x input\n    x2 = Nsamples x inputs\n    length_scales : size of input\n    \"\"\"\n    N1, n = z1.shape\n    N2, n = z2.shape\n    x1 = z1[:,0:-1]\n    x2 = z2[:,0:-1]\n    u1 = z1[:,-1]\n    u2 = z2[:,-1]\n    L_inv = np.diag(1/length_scales**2)\n    val = np.zeros((N1,N2))\n    for i in range(N1):\n        for j in range(N2):\n            val[i,j] = u1[i]*u2[j]*variance*np.exp(-0.5*(x1[np.newaxis,i,:].T-x2[np.newaxis,j,:].T).T @ L_inv @ (x1[np.newaxis,i,:].T - x2[np.newaxis,j,:].T))\n\n    val = val\n\n    return val", "\ndef se_kernel(x1, x2, variance, length_scales):\n    \"\"\"\n    x1 = Nsamples x input\n    x2 = Nsamples x inputs\n    length_scales : size of input\n    \"\"\"\n    N1, n = x1.shape\n    N2, n = x2.shape\n    L_inv = np.diag(1/length_scales**2)/2.0\n    val = np.zeros((N1,N2))\n    for i in range(N1):\n        for j in range(N2):\n            val[i,j] = variance*np.exp(-0.5*(x1[np.newaxis,i,:].T-x2[np.newaxis,j,:].T).T @ L_inv @ (x1[np.newaxis,i,:].T - x2[np.newaxis,j,:].T))\n    val = val\n\n    return val", "\n\n"]}
{"filename": "testing/testing_fmpc.py", "chunked_list": ["import seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.fmpc import FMPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import SOCPProblem\n\n# Model Parameters\ndt = 0.02 # Discretization of simulation\nT = 5.0 # Simulation time", "dt = 0.02 # Discretization of simulation\nT = 5.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'increasing_sine'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)", "# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.10 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\n# Controller Parameters\nhorizon = 50", "# Controller Parameters\nhorizon = 50\nq_mpc = [20.0, 15.0, 5.0]\n#q_mpc = [10.0,  5.0, 5.0]\n#q_mpc = [1.0,  1.0, 1.0]\nr_mpc = [0.1]\nsolver = 'ipopt'\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,", "            horizon=horizon,\n            dt=dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver)\nfmpc.reset()\n\n#fmpc = FMPC(quad=quad,\n#            horizon=horizon,\n#            dt=dt,", "#            horizon=horizon,\n#            dt=dt,\n#            q_mpc=q_mpc,\n#            r_mpc=r_mpc,\n#            solver=solver)\n#fmpc.reset()\n\n# Reference\nAmp = 0.2\nomega = 0.9", "Amp = 0.2\nomega = 0.9\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\n# Robust lqr smoothness term near error of zero\neps = 0.0001\n\n# FMPC Prob", "\n# FMPC Prob\n#fmpc_prob = SOCPProblem('SOCP', quad_prior, beta, input_bound=None, state_bound=None, ctrl=fmpc)\n\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt", "params['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    fmpc, # FB ctrl", "    reference_generator, # reference\n    fmpc, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=True\n)\n\n#x_from_z = quad.cs_x_from_z(z=fmpc_data_i['z'].T)['x'].toarray()\n#", "#x_from_z = quad.cs_x_from_z(z=fmpc_data_i['z'].T)['x'].toarray()\n#\n#x_int = np.zeros_like(fmpc_data_i['z'].T)\n#x_int[:,0] = x_from_z[:,0]\n#us = fmpc_data_i['u'][:-1]\n#N = us.shape[0]\n#for i in range(N):\n#    x_int[:, i+1] = quad.cs_nonlin_dyn_discrete(x0=x_int[:,i],p=us[i])['xf'].toarray().squeeze()\n#plt.figure()\n#plt.plot(x_int[0,:], label='From Nonliner dyn')", "#plt.figure()\n#plt.plot(x_int[0,:], label='From Nonliner dyn')\n#plt.plot(fmpc_data_i['z'][:,0], label='From Z')\n#plt.title('Comparing different integration methods')\n#plt.legend()\n#plt.show()\n"]}
{"filename": "testing/testing_gpmpc.py", "chunked_list": ["import munch\nimport seaborn as sns\nimport numpy as np\nimport gpytorch\nimport torch\nsns.set(style=\"whitegrid\")\nfrom copy import deepcopy\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom quad_1D.expr_utils import feedback_loop", "from controllers.mpc import MPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import LQR\nfrom learning.gpmpc_gp_utils import DataHandler, GaussianProcessCollection, ZeroMeanIndependentGPModel, combine_prior_and_gp\nfrom utils.dir_utils import set_dir_from_config\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'gpmpc_testing'}\nconfig = munch.munchify(config)", "           'tag': 'gpmpc_testing'}\nconfig = munch.munchify(config)\nset_dir_from_config(config)\n\n\n# Model Parameters\ndt = 0.05 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper", "N = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'increasing_sine'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\n", "\n\nT_prior = 7 # Thrust\ntau_prior = 0.10 # Time constant\ngamma_prior = 0.0 # Drag\nquad_prior = Quad1D(thrust=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\nrun_gpmpc = True\n# GP params\nsigmas = 0.0001", "# GP params\nsigmas = 0.0001\nnoise = {'mean': [0.0, 0.0, 0.0],\n         'std': [sigmas, sigmas, sigmas]}\nnum_samples = 1000\nn_train = [2000, 2000, 2000]\nlr = [0.05, 0.05, 0.05]\n#noise = None\n\n", "\n\n# Controller Parameters\nhorizon = 50\nq_mpc = [10.0, 0.1, 0.1]\nr_mpc = [0.1]\nsolver = 'ipopt'\nmpc = MPC(quad=quad_prior,\n          horizon=horizon,\n          dt=dt,", "          horizon=horizon,\n          dt=dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver)\nmpc.reset()\n\nprior_lqr_controller = LQR('Prior LQR', quad_prior, deepcopy(mpc.Q), deepcopy(mpc.R))\n\n# Reference", "\n# Reference\nAmp = 0.2\nomega = 0.9\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\n# Robust lqr smoothness term near error of zero\neps = 0.0001", "# Robust lqr smoothness term near error of zero\neps = 0.0001\n# simulation parameters\nx_data = []\nu_data = []\nomega_list = [0.3, 0.5, 0.7, 0.9]\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m", "params['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nfor omega in omega_list:\n    params['omega'] = omega\n    fmpc_data_i, fig_count = feedback_loop(\n        params, # paramters\n        None, # GP model\n        quad.true_flat_dynamics, # flat dynamics to step with\n        reference_generator, # reference\n        #prior_lqr_controller, # FB ctrl\n        mpc, # FB ctrl\n        secondary_controllers=None, # No comparison\n        online_learning=False,\n        fig_count=0,\n        plot=False\n    )\n\n    x_data.append(quad.cs_x_from_z(z=fmpc_data_i['z'].T)['x'].toarray().T)\n    u_data.append(fmpc_data_i['u'])\n    mpc.reset()", "prior_model = deepcopy(quad_prior.cs_lin_dyn)\nsave_dir = config.output_dir\n\ndh = DataHandler(x_data=x_data,\n                 u_data=u_data,\n                 prior_model=prior_model,\n                 save_dir=save_dir,\n                 noise=noise,\n                 num_samples=num_samples)\n", "                 num_samples=num_samples)\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood(\n                    constraint=gpytorch.constraints.GreaterThan(1e-6),\n).double()\ninput_mask = [1,2,3]\ntarget_mask = [1,2]\ngp = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                               likelihood,\n                               len(target_mask),", "                               likelihood,\n                               len(target_mask),\n                               input_mask=input_mask,\n                               target_mask=target_mask,\n                                                     )\n\ngp.train(torch.from_numpy(dh.data.train_inputs),\n         torch.from_numpy(dh.data.train_targets),\n         torch.from_numpy(dh.data.test_inputs),\n         torch.from_numpy(dh.data.test_targets),", "         torch.from_numpy(dh.data.test_inputs),\n         torch.from_numpy(dh.data.test_targets),\n         n_train=n_train,\n         learning_rate=lr,\n         gpu=True,\n         dir=config.output_dir)\n\n# Load a GP with fewer kernel points\ngp_small = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                               likelihood,", "gp_small = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                               likelihood,\n                               len(target_mask),\n                               input_mask=input_mask,\n                               target_mask=target_mask,\n                                                     )\nN_gp_small = 200\ninterval = int(np.ceil(N/N_gp_small))\ngp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh.data.train_inputs[::interval,:]),\n                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),", "gp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh.data.train_inputs[::interval,:]),\n                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),\n                              path_to_statedicts=config.output_dir)\n\ngp_precict = gp_small.make_casadi_predict_func()\ndyn_func = combine_prior_and_gp(prior_model, gp_precict, input_mask, target_mask)\n\nx_next_pred = dyn_func(x0=dh.data.x_seq[0].T, p=dh.data.u_seq[0].T)['xf'].toarray()\nx_next_pred_pr = prior_model(x0=dh.data.x_seq[0].T, p=dh.data.u_seq[0].T)['xf'].toarray()\n", "x_next_pred_pr = prior_model(x0=dh.data.x_seq[0].T, p=dh.data.u_seq[0].T)['xf'].toarray()\n\npred_RMSE = np.sum((x_next_pred - dh.data.x_next_seq[0].T)**2)\nprior_RMSE = np.sum((x_next_pred_pr - dh.data.x_next_seq[0].T)**2)\nprint(f'GP RMSE: {pred_RMSE}')\nprint(f'Prior RMSE: {prior_RMSE}')\n\n#test traj\nparams['omega'] = 0.\n", "params['omega'] = 0.\n\n\nmpc_prior_data, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    #prior_lqr_controller, # FB ctrl\n    mpc, # FB ctrl", "    #prior_lqr_controller, # FB ctrl\n    mpc, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=True\n)\n\n\nif run_gpmpc:\n    gpmpc = MPC(quad=quad_prior,\n                horizon=horizon,\n                dt=dt,\n                q_mpc=q_mpc,\n                r_mpc=r_mpc,\n                solver=solver,\n                dynamics=dyn_func)\n    gpmpc.reset()\n    gpmpc_data_i, fig_count = feedback_loop(\n        params, # paramters\n        None, # GP model\n        quad.true_flat_dynamics, # flat dynamics to step with\n        reference_generator, # reference\n        #prior_lqr_controller, # FB ctrl\n        gpmpc, # FB ctrl\n        secondary_controllers=None, # No comparison\n        online_learning=False,\n        fig_count=0,\n        plot=True\n    )", "\nif run_gpmpc:\n    gpmpc = MPC(quad=quad_prior,\n                horizon=horizon,\n                dt=dt,\n                q_mpc=q_mpc,\n                r_mpc=r_mpc,\n                solver=solver,\n                dynamics=dyn_func)\n    gpmpc.reset()\n    gpmpc_data_i, fig_count = feedback_loop(\n        params, # paramters\n        None, # GP model\n        quad.true_flat_dynamics, # flat dynamics to step with\n        reference_generator, # reference\n        #prior_lqr_controller, # FB ctrl\n        gpmpc, # FB ctrl\n        secondary_controllers=None, # No comparison\n        online_learning=False,\n        fig_count=0,\n        plot=True\n    )", ""]}
{"filename": "testing/testing_mpc.py", "chunked_list": ["import seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom quad_1D.expr_utils import feedback_loop\n\n# Model Parameters\ndt = 0.05 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step", "T = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'increasing_sine'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n", "quad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\n\nT_prior = 10 # Thrust\ntau_prior = 0.2 # Time constant\ngamma_prior = 3 # Drag\nquad_prior = Quad1D(thrust=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\n# Controller Parameters\nhorizon = 20", "# Controller Parameters\nhorizon = 20\nq_mpc = [10.0, 1.0, 1.0]\nr_mpc = [0.1]\nsolver = 'ipopt'\nmpc = MPC(#quad=quad_prior,\n          quad=quad,\n          horizon=horizon,\n          dt=dt,\n          q_mpc=q_mpc,", "          dt=dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver)\nmpc.reset()\n\n\n# Reference\nAmp = 0.2\nomega = 0.9", "Amp = 0.2\nomega = 0.9\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\n# Robust lqr smoothness term near error of zero\neps = 0.0001\n# simulation parameters\nparams = {}", "# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(\n    params, # paramters", "fmpc_data_i, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    mpc, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=True", "    fig_count=0,\n    plot=True\n)\n\n#x_from_z = quad.cs_x_fom_z(z=fmpc_data_i['z'].T)['x'].toarray()\n#\n#x_int = np.zeros_like(fmpc_data_i['z'].T)\n#x_int[:,0] = x_from_z[:,0]\n#us = fmpc_data_i['u'][:-1]\n#N = us.shape[0]", "#us = fmpc_data_i['u'][:-1]\n#N = us.shape[0]\n#for i in range(N):\n#    x_int[:, i+1] = quad.cs_nonlin_dyn_discrete(x0=x_int[:,i],p=us[i])['xf'].toarray().squeeze()\n#plt.figure()\n#plt.plot(x_int[0,:], label='From Nonliner dyn')\n#plt.plot(fmpc_data_i['z'][:,0], label='From Z')\n#plt.title('Comparing different integration methods')\n#plt.legend()\n#plt.show()", "#plt.legend()\n#plt.show()\n"]}
{"filename": "testing/testing_dlqr.py", "chunked_list": ["import seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.dlqr import DLQR\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import SOCPProblem\n\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time", "dt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'step'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)", "# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\n\n# Controller Parameters", "\n# Controller Parameters\nhorizon = 100\nq_lqr = [10.0, 0.0, 0.0]\nr_lqr = [0.1]\n#dlqr = DLQR(quad=quad_prior,\n#            horizon=horizon,\n#            dt=dt,\n#            q_lqr=q_lqr,\n#            r_lqr=r_lqr)", "#            q_lqr=q_lqr,\n#            r_lqr=r_lqr)\n\ndlqr = DLQR(quad=quad,\n            horizon=horizon,\n            dt=dt,\n            q_lqr=q_lqr,\n            r_lqr=r_lqr)\n\n# Reference", "\n# Reference\nAmp = 0.1\nomega = 5.0\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\n\n# simulation parameters", "\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(", "params['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    dlqr, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,", "    online_learning=False,\n    fig_count=0,\n    plot=True\n)\n\n"]}
{"filename": "testing/plotting_stab_con_dist.py", "chunked_list": ["import seaborn as sns\nimport numpy as np\nsns.set(style=\"whitegrid\")\nimport matplotlib.pyplot as plt\n\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.fmpc import FMPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import SOCPProblem\n", "from quad_1D.controllers import SOCPProblem\n\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag", "tau = 0.2 # Time constant\ngamma = 3 # Drag\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n", "quad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n\n# Controller Parameters\nhorizon = 100\nq_mpc = [20.0, 15.0, 5.0]\nr_mpc = [0.1]\nsolver = 'qrqp'\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,", "            horizon=horizon,\n            dt=dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver)\nfmpc.reset()\n\n# Controller Parameters\nhorizon = 100\nq_mpc = [20.0, 15.0, 5.0]", "horizon = 100\nq_mpc = [20.0, 15.0, 5.0]\nr_mpc = [0.1]\nsolver = 'ipopt'\n#upper_bounds = {'z0': 0.25}\nupper_bounds = None\n#lower_bounds = {'z0': -10}\nlower_bounds = None\ncon_tol = 0.0\nh = np.array([[1.0, 0.0, 0.0]]).T", "con_tol = 0.0\nh = np.array([[1.0, 0.0, 0.0]]).T\nbcon = 0.25\nphi_p = 3.0\nstate_bounds = {'h': h,\n                'b': bcon,\n                'phi_p': phi_p}\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,", "            horizon=horizon,\n            dt=dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver,\n            upper_bounds=upper_bounds,\n            lower_bounds=lower_bounds,\n            con_tol=0.0)\nfmpc.reset()\n# Reference", "fmpc.reset()\n# Reference\nAmp = 0.5\nomega = 1.0\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\n\n# simulation parameters", "\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(", "params['omega'] = omega\nfmpc_data_i, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    fmpc, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,", "    online_learning=False,\n    fig_count=0,\n    plot=True\n)\nz =  fmpc_data_i['z'][:-1,:]\ne = fmpc_data_i['z'][:-1,:] - fmpc_data_i['z_ref'][:-1,:]\npsi = fmpc_data_i['v'] - fmpc_data_i['v_des']\n\nu_max = 45.0/180.0*np.pi\n", "u_max = 45.0/180.0*np.pi\n\nNw1 = (fmpc.Ad - fmpc.Bd @ fmpc.K).T @ fmpc.P @ fmpc.Bd\nterm_1 = -e @ Nw1 * psi\nw2 = fmpc.Bd.T @ fmpc.P @ fmpc.Bd\nterm_2 = w2 * psi**2\n\nv_pmax = quad_prior.cs_v_from_u(z=z.T, u=u_max)['v'].toarray() - fmpc_data_i['v_des'].T\nv_nmax = quad_prior.cs_v_from_u(z=z.T, u=-u_max)['v'].toarray() - fmpc_data_i['v_des'].T\nterm_2_max = w2*np.maximum(np.abs(v_pmax) , np.abs(v_nmax)).squeeze()**2", "v_nmax = quad_prior.cs_v_from_u(z=z.T, u=-u_max)['v'].toarray() - fmpc_data_i['v_des'].T\nterm_2_max = w2*np.maximum(np.abs(v_pmax) , np.abs(v_nmax)).squeeze()**2\nall_term = term_1 + term_2\n\nplt.figure()\nplt.plot(np.abs(term_1), label=\"|| Term 1 ||\")\nplt.plot(np.abs(term_2), label=\"|| Term 2 ||\")\nplt.plot(np.abs(term_2_max).T, label=\"|| Term 2 max||\")\nplt.legend()\nplt.show()", "plt.legend()\nplt.show()\n"]}
{"filename": "testing/testing_socp_dlqr.py", "chunked_list": ["import seaborn as sns\nimport numpy as np\nimport shelve\nsns.set(style=\"whitegrid\")\nimport gpytorch\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.dlqr import DLQR\nfrom quad_1D.expr_utils import feedback_loop\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n#from quad_1D.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp", "from controllers.discrete_socp_filter import DiscreteSOCPFilter\n#from quad_1D.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\n\n\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper", "N = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant", "T_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n\n# Controller Parameters\nq_lqr = [20.0, 15.0, 5.0]\nr_lqr = [0.1]\ndlqr = DLQR(quad=quad_prior,\n            dt=dt,", "dlqr = DLQR(quad=quad_prior,\n            dt=dt,\n            q_lqr=q_lqr,\n            r_lqr=r_lqr)\n\n#dlqr = DLQR(quad=quad,\n#            horizon=horizon,\n#            dt=dt,\n#            q_lqr=q_lqr,\n#            r_lqr=r_lqr)", "#            q_lqr=q_lqr,\n#            r_lqr=r_lqr)\n\n# Reference\nAmp = 0.2\nomega = 0.8\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0", "delta = 0.05\nbeta = 2.0\nd_weight=10000.0 # lower weight makes less chattering.\ninput_bound = 45.0*np.pi/180.0\n\noutput_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/affine_gp/saved/seed42_Mar-01-17-52-44_9bf1cd2'\n\ngp_type = ZeroMeanAffineGP\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood, 1, output_dir)", "likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood, 1, output_dir)\ngp_inv.init_with_hyperparam(output_dir)\n\n# SOCP Prob\nh = np.array([[1.0, 0.0, 0.0]]).T\nbcon = 0.25\nphi_p = 3.0\nstate_bounds = {'h': h,\n                'b': bcon,", "state_bounds = {'h': h,\n                'b': bcon,\n                'phi_p': phi_p}\ndlqr_prob = DiscreteSOCPFilter('SOCP', quad_prior, beta, d_weight=d_weight, input_bound=input_bound, state_bound=state_bounds, ctrl=dlqr)\n\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m", "params['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nsocp_data, fig_count = feedback_loop(\n    params, # paramters\n    gp_inv, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference", "    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    dlqr_prob, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=True\n)\n\n", "\n"]}
{"filename": "testing/testing_exp_mpc.py", "chunked_list": ["import munch\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom experiments.experiments import Experiment\n\n# Model Parameters\ndt = 0.02 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step", "T = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'step'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n", "quad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.10 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\nhorizon = 50\nq_mpc = [10.0, 1.0, 1.0]\nr_mpc = [0.1]", "q_mpc = [10.0, 1.0, 1.0]\nr_mpc = [0.1]\nsolver = 'ipopt'\nmpc = MPC(quad=quad_prior,\n          horizon=horizon,\n          dt=dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver)\nmpc.reset()", "          solver=solver)\nmpc.reset()\n\n# Controller Parameters\nhorizon = 50\nq_fmpc = [20.0, 15.0, 5.0]\n#q_fmpc = [10.0,  0.0, 0.0]\n#q_mpc = [1.0,  1.0, 1.0]\nr_fmpc = [0.1]\nsolver = 'ipopt'", "r_fmpc = [0.1]\nsolver = 'ipopt'\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,\n            q_mpc=q_fmpc,\n            r_mpc=r_fmpc,\n            solver=solver)\nfmpc.reset()\n", "fmpc.reset()\n\nreference_generator = quad.reference_generator\n\nAmp = 0.2\nomega = 0.9\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'mpc_testing'}\nconfig = munch.munchify(config)", "           'tag': 'mpc_testing'}\nconfig = munch.munchify(config)\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nexp = Experiment('mpc', quad, [mpc, fmpc], reference_generator, params, config)", "params['omega'] = omega\nexp = Experiment('mpc', quad, [mpc, fmpc], reference_generator, params, config)\nexp.run_experiment()\nexp.plot_tracking()\n"]}
{"filename": "testing/testing_gps.py", "chunked_list": ["import torch\nimport gpytorch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom learning.gp_utils import GaussianProcess, ConstantMeanAffineGP\n\nN = 200\nx_max = 2\nx_min = 0", "x_max = 2\nx_min = 0\nx_delta = x_max*0.1\n# Make training data\ntrain_z = torch.linspace(x_min, x_max, N).double()\ntrain_u = torch.linspace(x_min, x_max,N).double()\ntrain_y = torch.sin(train_z * (2 * np.pi)) + train_u + torch.randn(train_z.size()) * np.sqrt(0.04)\ntrain_x = torch.stack((train_z, train_u)).T\n# Define Model and train\n#gp_type = ZeroMeanAffineGP", "# Define Model and train\n#gp_type = ZeroMeanAffineGP\ngp_type = ConstantMeanAffineGP\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp = GaussianProcess(gp_type, likelihood, 1)\ngp.train(train_x, train_y)\ntest_x = torch.linspace(x_min-x_delta, x_max+x_delta,int(N/2)).double()\ntest_x = torch.stack((test_x,test_x)).T\n# Plot\nmeans, covs, predictions = gp.predict(test_x)", "# Plot\nmeans, covs, predictions = gp.predict(test_x)\nlower, upper = predictions.confidence_region()\n# Compute mean and covariance using gammas\n#means_from_gamma = torch.zeros((N,1))\n#covs_from_gamma = torch.zeros((N,1))\n#cov_2_from_gamma = torch.zeros((N,1))\nx = test_x[:,None,0]\nu = test_x[:,None,1]\n#gamma_1, gamma_2, gamma_3, gamma_4, gamma_5, cov_2 = gp.model.compute_gammas(test_x)", "u = test_x[:,None,1]\n#gamma_1, gamma_2, gamma_3, gamma_4, gamma_5, cov_2 = gp.model.compute_gammas(test_x)\n#means_from_gamma = gamma_1 + gamma_2.mul(u)\n#covs_from_gamma = gamma_3 + gamma_4.mul(u) + gamma_5.mul(u**2) + gp.likelihood.noise.detach()\n#cov_2_from_gamma = cov_2\n#upper_from_gamma = means_from_gamma + covs_from_gamma.sqrt()*2\n#lower_from_gamma = means_from_gamma - covs_from_gamma.sqrt()*2\n#upper_from_gamma = means_from_gamma + cov_2_from_gamma.sqrt()*2\n#lower_from_gamma = means_from_gamma - cov_2_from_gamma.sqrt()*2\nmeans_from_gamma, cov_from_gamma, upper_from_gamma, lower_from_gamma = gp.model.mean_and_cov_from_gammas(test_x)", "#lower_from_gamma = means_from_gamma - cov_2_from_gamma.sqrt()*2\nmeans_from_gamma, cov_from_gamma, upper_from_gamma, lower_from_gamma = gp.model.mean_and_cov_from_gammas(test_x)\n# plot gamma computed means\nplt.fill_between(test_x.numpy()[:,0], lower.numpy(), upper.numpy(), alpha=0.5, label='95%')\nplt.fill_between(test_x.numpy()[:,0], lower_from_gamma[:,0].numpy(), upper_from_gamma[:,0].numpy(), alpha=0.5, label='95% from gammas')\nplt.plot(test_x.numpy()[:,0],means,'r', label='GP Mean')\nplt.plot(train_x.numpy()[:,0],train_y,'*k', label='Data')\nplt.plot(test_x.numpy()[:,0],means_from_gamma.detach().numpy()[:,0],'m', label='gamma mean')\nplt.legend()\nplt.show()", "plt.legend()\nplt.show()\ngg = gp.predict(test_x[5,None,:])\n"]}
{"filename": "testing/testing_gp_jacobians.py", "chunked_list": ["import numpy as np\nimport torch\nimport gpytorch\nimport matplotlib.pyplot as plt\nfrom quad_1D.quad_1d import Quad1D\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import LQR\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\n\n# Model Parameters", "\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 5.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nT = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\n# Define 2d quadrotor and reference traj", "gamma = 3 # Drag\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=T, tau=tau, gamma=gamma, dt=dt)\nreference_generator = quad.reference_generator\n# Prior model\nT_prior = 10 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n# Input bounds", "quad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n# Input bounds\ntheta_lim = 35/180*np.pi\n# LQR matrices\nQ = np.diag([10.0, 10.0, 10.0])\nR = 0.1 * np.eye(quad.m)\n# Reference\nAmp = 0.4\nomega = 1.0\n# Probabilistic guarantee of 1-delta", "omega = 1.0\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nprob_theshold = np.sqrt(1.0-delta)\nbeta = 2.0\n# Robust lqr smoothness term near error of zero\neps = 0.0001\n# Compute the LQR Gain matrix and ARE soln for the quad\nquad.lqr_gain_and_ARE_soln(Q, R)\nquad_prior.lqr_gain_and_ARE_soln(Q, R)", "quad.lqr_gain_and_ARE_soln(Q, R)\nquad_prior.lqr_gain_and_ARE_soln(Q, R)\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = 3\nparams['m'] = 1\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega", "params['Amp'] = Amp\nparams['omega'] = omega\n# Gather Training Data for GP\nprint(\"Generating Training Data...\")\nproir_lqr_controller = LQR('Prior LQR', quad_prior, Q, R)\ntrue_lqr_controller = LQR('True LQR', quad, Q, R)\nprior_model_fb_data, fig_count = feedback_loop(\n                                                params, # paramters\n                                                None, # GP model\n                                                quad.true_flat_dynamics, # flat dynamics to step with", "                                                None, # GP model\n                                                quad.true_flat_dynamics, # flat dynamics to step with\n                                                reference_generator, # reference\n                                                proir_lqr_controller, # FB ctrl\n                                                [true_lqr_controller] # comparison\n                                               )\n## Train the nonlinear inverse GP (z,u) -> v\ninterval = 1\nz_train = prior_model_fb_data['z']\nu_train = prior_model_fb_data['u']", "z_train = prior_model_fb_data['z']\nu_train = prior_model_fb_data['u']\nv_measured_train = prior_model_fb_data['v']\ntrain_input_inv = np.hstack((z_train[::interval,:], u_train[::interval]))\ntrain_targets_inv = (v_measured_train[::interval] # use v_prior_log, u_prior_log, and z_prior_log\n                 - quad_prior.v_from_u(u_train[::interval].T, z_train[::interval,:].T).T\n                 + np.random.normal(0, 1.0, size=(int(N/interval),1)) )\n# Train the gp\nprint(\"Training the GP (u,z) -> v..\")\ngp_type = ZeroMeanAffineGP", "print(\"Training the GP (u,z) -> v..\")\ngp_type = ZeroMeanAffineGP\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood, 1)\ntrain_x_inv = torch.from_numpy(train_input_inv).double()\ntrain_y_inv = torch.from_numpy(train_targets_inv).squeeze().double()\ngp_inv.train(train_x_inv, train_y_inv, n_train=50)\nfig_count = gp_inv.plot_trained_gp(prior_model_fb_data['t'][::interval,0], fig_count=fig_count)\n# Train the nonlinear term GP (z,v) -> u\ntrain_input_nl = np.hstack((z_train[::interval,:], v_measured_train[::interval]))", "# Train the nonlinear term GP (z,v) -> u\ntrain_input_nl = np.hstack((z_train[::interval,:], v_measured_train[::interval]))\n# output data = v_cmd (v_des) - v_measured) + nois to match 2021 Paper\nv_cmd = prior_model_fb_data['v_des']\ntrain_targets_nl = (v_cmd[::interval]   #\n                  - v_measured_train[::interval] #quad_prior.u_from_v(v_measured_train[::interval].T, z_train[::interval,:].T).T\n                  + np.random.normal(0, 2.0, size=(int(N/interval),1)) )\n# Train the gp\nprint(\"Training the GP (v,z) -> u..\")\ngp_nl = GaussianProcess(gp_type, likelihood, 1)", "print(\"Training the GP (v,z) -> u..\")\ngp_nl = GaussianProcess(gp_type, likelihood, 1)\ntrain_x_nl = torch.from_numpy(train_input_nl).double()\ntrain_y_nl = torch.from_numpy(train_targets_nl).squeeze().double()\ngp_nl.train(train_x_nl, train_y_nl, n_train=50)\nfig_count = gp_nl.plot_trained_gp(prior_model_fb_data['t'][::interval,0], fig_count=fig_count)\nnoise = 2.0\nvariance = 20.0\ngpy_nl = train_gp(noise, variance, 4, train_input_nl, train_targets_nl)\nmeans_gpy, covs_gpy = gpy_nl.predict(train_input_nl)", "gpy_nl = train_gp(noise, variance, 4, train_input_nl, train_targets_nl)\nmeans_gpy, covs_gpy = gpy_nl.predict(train_input_nl)\nlower = means_gpy - np.sqrt(covs_gpy)*2\nupper = means_gpy + np.sqrt(covs_gpy)*2\nfig_count += 1\nplt.figure(fig_count)\nplt.fill_between(prior_model_fb_data['t'][::interval,0], lower[:,0], upper[:,0])\nplt.plot(prior_model_fb_data['t'][::interval,0], means_gpy[:,0], 'r')\nplt.plot(prior_model_fb_data['t'][::interval,0], train_targets_nl[:,0], 'k*')\nplt.title('GPy GP')", "plt.plot(prior_model_fb_data['t'][::interval,0], train_targets_nl[:,0], 'k*')\nplt.title('GPy GP')\nplt.show()\n"]}
{"filename": "testing/testing_LHS_train.py", "chunked_list": ["import seaborn as sns\nsns.set(style=\"whitegrid\")\nimport numpy as np\nimport torch\nimport gpytorch\nimport munch\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n", "\n\nfrom quad_1D.quad_1d import Quad1D\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\nfrom utils.plotting_utils import scatter3d, plot_trained_gp\nfrom utils.dir_utils import set_dir_from_config\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'affine_gp'}", "           'output_dir': './results/',\n           'tag': 'affine_gp'}\nconfig = munch.munchify(config)\nset_dir_from_config(config)\n\nseed = 42\n\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time", "dt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\n# Define 2d quadrotor and reference traj\nquad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt)\n", "quad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(thrust=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n\n# Reference\nAmp = 0.2\ninputs = []", "Amp = 0.2\ninputs = []\ntargets = []\nomegalist = [0.3, 0.5, 0.7, 0.9]\nsig = 0.0001\nfor omega in omegalist:\n    t = np.arange(0,T, dt)\n    #z_ref, v_real = quad.reference_generator(t, Amp, omega, ref_type='increasing_freq')\n    z_ref, v_real = quad.reference_generator(t, Amp, omega)\n    u_ref = quad.cs_u_from_v(z=z_ref, v=v_real)['u'].toarray()\n    v_hat = quad_prior.cs_v_from_u(z=z_ref, u=u_ref)['v'].toarray()\n    u_ref_prior = quad_prior.cs_u_from_v(z=z_ref, v=v_hat)['u'].toarray()\n\n    noise = np.random.normal(0, sig, size=v_hat.shape)\n    v_hat_noisy = v_real + noise\n    inputs.append(torch.from_numpy(np.vstack((z_ref, u_ref))).double().T)\n    targets.append(torch.from_numpy(v_real).double().T)", "inputs = torch.vstack(inputs)\ntargets = torch.vstack(targets)\n\n# Sampling data points\nN = 1000\nd = 4\nn_train=500\nlr=0.1\ninterval = int(np.ceil(inputs.shape[0]/N))\ninputs = inputs[::interval, :]", "interval = int(np.ceil(inputs.shape[0]/N))\ninputs = inputs[::interval, :]\ntargets = targets[::interval, :]\n\ntrain_in, test_in, train_tar, test_tar  = train_test_split(inputs, targets, test_size=0.2, random_state=seed)\n\ngp_type = ZeroMeanAffineGP\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood, 1, config.output_dir)\ngp_inv.train(train_in, train_tar.squeeze(), n_train=n_train, learning_rate=lr)", "gp_inv = GaussianProcess(gp_type, likelihood, 1, config.output_dir)\ngp_inv.train(train_in, train_tar.squeeze(), n_train=n_train, learning_rate=lr)\n\nmeans, covs, preds = gp_inv.predict(test_in)\nerrors = means - test_tar.squeeze()\nabs_errors = torch.abs(errors)\nrel_errors = abs_errors/torch.abs(test_tar.squeeze())\n\nscatter3d(test_in[:,0], test_in[:,1], test_in[:,2], errors)\n", "scatter3d(test_in[:,0], test_in[:,1], test_in[:,2], errors)\n\n# Reference\nAmp = 0.2\nomega = 0.6\nt = np.arange(0,10, dt)\nz_test, v_test_real = quad.reference_generator(t, Amp, omega)\nu_test = quad.cs_u_from_v(z=z_test, v=v_test_real)['u'].toarray()\nref_gp_ins = torch.from_numpy(np.vstack((z_test, u_test))).T\ndelv_pred, u_cov, preds = gp_inv.predict(ref_gp_ins)", "ref_gp_ins = torch.from_numpy(np.vstack((z_test, u_test))).T\ndelv_pred, u_cov, preds = gp_inv.predict(ref_gp_ins)\nv_test_prior = quad_prior.cs_v_from_u(z=z_test, u=u_test)['v'].toarray()\n#v_pred = delv_pred.T + v_test_prior\nv_pred = delv_pred.T\n\nfigcount = plot_trained_gp(v_test_real, v_pred, preds, fig_count=1, show=True)\n\nlikelihood2 = gpytorch.likelihoods.GaussianLikelihood()\ngp2 = GaussianProcess(gp_type, likelihood2, 1, config.output_dir)", "likelihood2 = gpytorch.likelihoods.GaussianLikelihood()\ngp2 = GaussianProcess(gp_type, likelihood2, 1, config.output_dir)\ngp2.init_with_hyperparam(config.output_dir)\n\ndelv_pred2, u_cov2, preds2 = gp2.predict(ref_gp_ins)\nv_pred2 = delv_pred2.T\nplot_trained_gp(v_test_real, v_pred2, preds2, fig_count=figcount, show=True)\n"]}
{"filename": "testing/testing.py", "chunked_list": ["import seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.fmpc import FMPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import SOCPProblem\nimport control\n\n# Model Parameters\ndt = 0.02 # Discretization of simulation", "# Model Parameters\ndt = 0.02 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\n# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt)", "# Define 2d quadrotor and reference traj\nquad = Quad1D(T=Thrust, tau=tau, gamma=gamma, dt=dt)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(T=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt)\n\n# Controller Parameters\nhorizon = 50", "# Controller Parameters\nhorizon = 50\nq_mpc = [20.0, 15.0, 5.0]\nr_mpc = [0.1]\nsolver = 'qrqp'\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,", "            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver)\nfmpc.reset()\n\nQ = fmpc.Q\nR = fmpc.R\nAd = quad.Ad\nBd = quad.Bd\n", "Bd = quad.Bd\n\nK, P, E = control.dlqr(Ad, Bd, Q, R)\n"]}
{"filename": "testing/testing_socp_fmpc.py", "chunked_list": ["import seaborn as sns\nimport numpy as np\nfrom copy import deepcopy\nimport shelve\nsns.set(style=\"whitegrid\")\nimport gpytorch\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.fmpc import FMPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter", "from quad_1D.expr_utils import feedback_loop\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n#from quad_1D.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess, train_gp\n\n\n# Model Parameters\ndt = 0.01 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step", "T = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type='increasing_sine'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n", "quad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\nT_prior = 7.0 # Thrust\ntau_prior = 0.15 # Time constant\ngamma_prior = 0 # Drag\nquad_prior = Quad1D(thrust=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\n# Controller Parameters\nhorizon = 100\nq_mpc = [20.0, 15.0, 5.0]", "horizon = 100\nq_mpc = [20.0, 15.0, 5.0]\n#q_mpc = [50.0, 0.1, 0.1]\nr_mpc = [0.1]\nsolver = 'ipopt'\n#upper_bounds = {'z0': 0.25}\nupper_bounds = None\n#lower_bounds = {'z0': -10}\nlower_bounds = None\ncon_tol = 0.0", "lower_bounds = None\ncon_tol = 0.0\nh = np.array([[1.0, 0.0, 0.0]]).T\nbcon = 0.25\nphi_p = 3.0\nstate_bounds = {'h': h,\n                'b': bcon,\n                'phi_p': phi_p}\nfmpc = FMPC(quad=quad_prior,\n            horizon=horizon,", "fmpc = FMPC(quad=quad_prior,\n            horizon=horizon,\n            dt=dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver,\n            upper_bounds=upper_bounds,\n            lower_bounds=lower_bounds,\n            con_tol=0.0)\nfmpc.reset()", "            con_tol=0.0)\nfmpc.reset()\n\n\n#dlqr = DLQR(quad=quad,\n#            horizon=horizon,\n#            dt=dt,\n#            q_lqr=q_lqr,\n#            r_lqr=r_lqr)\n", "#            r_lqr=r_lqr)\n\n# Reference\nAmp = 0.2\nomega = 0.6\nreference_generator = quad.reference_generator\n# Probabilistic guarantee of 1-delta\ndelta = 0.05\nbeta = 2.0\nd_weight=0.0 # lower weight makes less chattering.", "beta = 2.0\nd_weight=0.0 # lower weight makes less chattering.\ninput_bound = 45.0*np.pi/180.0\n\noutput_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/affine_gp/saved/seed42_Mar-01-17-52-44_9bf1cd2'\n\ngp_type = ZeroMeanAffineGP\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood, 1, output_dir)\ngp_inv.init_with_hyperparam(output_dir)", "gp_inv = GaussianProcess(gp_type, likelihood, 1, output_dir)\ngp_inv.init_with_hyperparam(output_dir)\n\n# SOCP Prob\nfmpc_prob = DiscreteSOCPFilter('SOCP',\n                               quad_prior,\n                               beta,\n                               d_weight=d_weight,\n                               input_bound=input_bound,\n                               state_bound=None,", "                               input_bound=input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# simulation parameters\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m", "params['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nsocp_data, fig_count = feedback_loop(\n    params, # paramters\n    None, #gp_inv, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference", "    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    fmpc_prob, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=True\n)\n\n", "\n"]}
{"filename": "testing/testing_gpmpc_LHS.py", "chunked_list": ["import munch\nimport seaborn as sns\nimport numpy as np\nimport gpytorch\nimport torch\nsns.set(style=\"whitegrid\")\nfrom copy import deepcopy\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom quad_1D.expr_utils import feedback_loop", "from controllers.mpc import MPC\nfrom quad_1D.expr_utils import feedback_loop\nfrom quad_1D.controllers import LQR\nfrom learning.gpmpc_gp_utils import DataHandler, GaussianProcessCollection, ZeroMeanIndependentGPModel, combine_prior_and_gp, generate_samples_into_sequences, get_LHS_samples, get_MVN_samples\nfrom utils.dir_utils import set_dir_from_config\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'gpmpc_testing'}\nconfig = munch.munchify(config)", "           'tag': 'gpmpc_testing'}\nconfig = munch.munchify(config)\nset_dir_from_config(config)\n\n\n# Model Parameters\ndt = 0.02 # Discretization of simulation\nT = 10.0 # Simulation time\nN = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper", "N = int(T/dt) # Number of time step\n# Taken from LCSS 2021 paper\nThrust = 10 # Thrust\ntau = 0.2 # Time constant\ngamma = 3 # Drag\nref_type = 'increasing_sine'\n# Define 2d quadrotor and reference traj\nquad = Quad1D(thrust=Thrust, tau=tau, gamma=gamma, dt=dt, ref_type=ref_type)\n\n", "\n\nT_prior = 20 # Thrust\ntau_prior = 0.05 # Time constant\ngamma_prior = 0.0 # Drag\nquad_prior = Quad1D(thrust=T_prior, tau=tau_prior, gamma=gamma_prior, dt=dt, ref_type=ref_type)\n\nrun_gpmpc = True\n# GP params\nsigmas = 0.0001", "# GP params\nsigmas = 0.0001\nnoise = {'mean': [0.0, 0.0, 0.0],\n         'std': [sigmas, sigmas, sigmas]}\nnum_samples = 5000\nn_train = [2000, 2000, 2000]\nlr = [0.05, 0.05, 0.05]\n#noise = None\n\n# 0.05 Hz using LHS", "\n# 0.05 Hz using LHS\n#gp_load_path = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/gpmpc_testing/seed42_Mar-09-15-46-39_fe35b85'\n# 0.02 Hz using LHS\n#gp_load_path = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/gpmpc_testing/seed42_Mar-09-17-16-35_fe35b85'\ngp_load_path = None\nseed = 42\n# LHS params\nlb = [-0.01, -2.0, -1.5, -0.6 ]\nub = [0.01, 2.0, 1.5, 0.6]", "lb = [-0.01, -2.0, -1.5, -0.6 ]\nub = [0.01, 2.0, 1.5, 0.6]\nLHS_sampler_args = {'lower_bounds': lb, 'upper_bounds': ub, 'num_samples': num_samples, 'seed': seed}\n\nx_data, u_data = generate_samples_into_sequences(get_LHS_samples, LHS_sampler_args, quad)\n\n#means = [0, 0, 0, 0]\n#cov = np.diag([0.1, 0.2, 0.2, 0.2])\n#MVN_sampler_args = {'means': means, 'cov': cov, 'num_samples': num_samples, 'seed': seed }\n#x_data, u_data = generate_samples_into_sequences(get_MVN_samples, MVN_sampler_args, quad)", "#MVN_sampler_args = {'means': means, 'cov': cov, 'num_samples': num_samples, 'seed': seed }\n#x_data, u_data = generate_samples_into_sequences(get_MVN_samples, MVN_sampler_args, quad)\n\n# Controller Parameters\nhorizon = 50\nq_mpc = [10.0, 0.1, 0.1]\nr_mpc = [0.1]\nsolver = 'ipopt'\nmpc = MPC(quad=quad_prior,\n          horizon=horizon,", "mpc = MPC(quad=quad_prior,\n          horizon=horizon,\n          dt=dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver)\nmpc.reset()\n\n\ninput_mask = [1,2,3]", "\ninput_mask = [1,2,3]\ntarget_mask = [1,2]\n\nprior_model = deepcopy(quad_prior.cs_lin_dyn)\nsave_dir = config.output_dir\nlikelihood = gpytorch.likelihoods.GaussianLikelihood(\n    constraint=gpytorch.constraints.GreaterThan(1e-6),\n).double()\n", ").double()\n\ndh = DataHandler(x_data=x_data,\n                 u_data=u_data,\n                 prior_model=prior_model,\n                 save_dir=save_dir,\n                 noise=noise,\n                 num_samples=num_samples)\nif gp_load_path is None:\n\n    gp = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                   likelihood,\n                                   len(target_mask),\n                                   input_mask=input_mask,\n                                   target_mask=target_mask,\n                                                         )\n\n    gp.train(torch.from_numpy(dh.data.train_inputs),\n             torch.from_numpy(dh.data.train_targets),\n             torch.from_numpy(dh.data.test_inputs),\n             torch.from_numpy(dh.data.test_targets),\n             n_train=n_train,\n             learning_rate=lr,\n             gpu=True,\n             dir=config.output_dir)", "if gp_load_path is None:\n\n    gp = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                   likelihood,\n                                   len(target_mask),\n                                   input_mask=input_mask,\n                                   target_mask=target_mask,\n                                                         )\n\n    gp.train(torch.from_numpy(dh.data.train_inputs),\n             torch.from_numpy(dh.data.train_targets),\n             torch.from_numpy(dh.data.test_inputs),\n             torch.from_numpy(dh.data.test_targets),\n             n_train=n_train,\n             learning_rate=lr,\n             gpu=True,\n             dir=config.output_dir)", "\n\n\n#test traj\nAmp = 0.2\nomega = 0.5\nparams = {}\nparams['N'] = N\nparams['n'] = quad.n\nparams['m'] = quad.m", "params['n'] = quad.n\nparams['m'] = quad.m\nparams['dt'] = dt\nparams['Amp'] = Amp\nparams['omega'] = omega\nreference_generator = quad.reference_generator\nmpc_prior_data, fig_count = feedback_loop(\n    params, # paramters\n    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with", "    None, # GP model\n    quad.true_flat_dynamics, # flat dynamics to step with\n    reference_generator, # reference\n    #prior_lqr_controller, # FB ctrl\n    mpc, # FB ctrl\n    secondary_controllers=None, # No comparison\n    online_learning=False,\n    fig_count=0,\n    plot=False\n)", "    plot=False\n)\n\n\n# Load a GP with fewer kernel points\ngp_small = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                     likelihood,\n                                     len(target_mask),\n                                     input_mask=input_mask,\n                                     target_mask=target_mask,", "                                     input_mask=input_mask,\n                                     target_mask=target_mask,\n                                     )\nN_gp_small = 200\n#interval = int(np.ceil(mpc_prior_data['z'].shape[0]*0.8/N_gp_small))\n#dh_small = DataHandler(x_data=mpc_prior_data['z'],\n#                       u_data=mpc_prior_data['u'],\n#                       prior_model=prior_model,\n#                       save_dir=save_dir,\n#                       noise=noise,", "#                       save_dir=save_dir,\n#                       noise=noise,\n#                       num_samples=mpc_prior_data['z'].shape[0])\n#in_data_small, tar_data_small = dh_small.select_subsamples_with_kmeans(N_gp_small, seed)\n\n#gp_small.init_with_hyperparam(train_inputs=torch.from_numpy(in_data_small),\n#                              train_targets=torch.from_numpy(tar_data_small),\n#                              path_to_statedicts=gp_load_path)\n\n#gp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh_small.data.train_inputs[::interval,:]),", "\n#gp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh_small.data.train_inputs[::interval,:]),\n#                              train_targets=torch.from_numpy(dh_small.data.train_targets[::interval,:]),\n#                              path_to_statedicts=gp_load_path)\ninterval = int(np.ceil(dh.data.train_inputs.shape[0]/N_gp_small))\ngp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh.data.train_inputs[::interval,:]),\n                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),\n                              path_to_statedicts=config.output_dir)\n                              #path_to_statedicts=gp_load_path)\ngp_precict = gp_small.make_casadi_predict_func()", "                              #path_to_statedicts=gp_load_path)\ngp_precict = gp_small.make_casadi_predict_func()\ndyn_func = combine_prior_and_gp(prior_model, gp_precict, input_mask, target_mask)\n\n#x_next_pred = dyn_func(x0=dh.data.x_seq[0].T, p=dh.data.u_seq[0].T)['xf'].toarray()\n#x_next_pred_pr = prior_model(x0=dh.data.x_seq[0].T, p=dh.data.u_seq[0].T)['xf'].toarray()\n#\n#pred_RMSE = np.sum((x_next_pred - dh.data.x_next_seq[0].T)**2)\n#prior_RMSE = np.sum((x_next_pred_pr - dh.data.x_next_seq[0].T)**2)\n#print(f'GP RMSE: {pred_RMSE}')", "#prior_RMSE = np.sum((x_next_pred_pr - dh.data.x_next_seq[0].T)**2)\n#print(f'GP RMSE: {pred_RMSE}')\n#print(f'Prior RMSE: {prior_RMSE}')\n\n\nif run_gpmpc:\n    gpmpc = MPC(quad=quad_prior,\n                name='GPMPC',\n                horizon=horizon,\n                dt=dt,\n                q_mpc=q_mpc,\n                r_mpc=r_mpc,\n                solver=solver,\n                dynamics=dyn_func)\n    gpmpc.reset()\n    gpmpc_data_i, fig_count = feedback_loop(\n        params, # paramters\n        None, # GP model\n        quad.true_flat_dynamics, # flat dynamics to step with\n        reference_generator, # reference\n        #prior_lqr_controller, # FB ctrl\n        gpmpc, # FB ctrl\n        secondary_controllers=None, # No comparison\n        online_learning=False,\n        fig_count=0,\n        plot=False\n    )", "import matplotlib.pyplot as plt\nfig, ax = plt.subplots(3)\nfor i in range(3):\n    ax[i].plot(mpc_prior_data['z_ref'][:,i], label='ref')\n    ax[i].plot(gpmpc_data_i['z'][:,i], label='GPMPC')\n    ax[i].plot(mpc_prior_data['z'][:,i], label='MPC')\nplt.legend()\nplt.show()\n", ""]}
{"filename": "controllers/mpc.py", "chunked_list": ["\"\"\"Model Predictive Control.\n\n\"\"\"\nimport numpy as np\nimport casadi as cs\n\nfrom sys import platform\nfrom copy import deepcopy\n\n#from safe_control_gym.controllers.base_controller import BaseController", "\n#from safe_control_gym.controllers.base_controller import BaseController\n#from safe_control_gym.controllers.mpc.mpc_utils import get_cost_weight_matrix, compute_discrete_lqr_gain_from_cont_linear_system, rk_discrete, compute_state_rmse, reset_constraints\n#from safe_control_gym.envs.benchmark_env import Task\n#from safe_control_gym.envs.constraints import GENERAL_CONSTRAINTS, create_constraint_list\n\nfrom copy import deepcopy\nfrom utils.math_utils import get_cost_weight_matrix, csQuadCost, rk_discrete\n\nclass MPC:\n    \"\"\"MPC with full nonlinear model.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            quad,\n            name='MPC',\n            horizon: int = 10,\n            dt: float = 0.1,\n            q_mpc: list = [1],\n            r_mpc: list = [1],\n            solver: str = 'ipopt',\n            constraints: dict = None,\n            dynamics: cs.Function = None,\n            reference_generator = None,\n            upper_bounds: dict = None,\n            lower_bounds: dict = None,\n            input_bound: float = None,\n            con_tol: float = 0.0,\n            ):\n        \"\"\"Creates task and controller.\n\n        Args:\n            env_func (Callable): function to instantiate task/environment.\n            horizon (int): mpc planning horizon.\n            q_mpc (list): diagonals of state cost weight.\n            r_mpc (list): diagonals of input/action cost weight.\n            warmstart (bool): if to initialize from previous iteration.\n            soft_constraints (bool): Formulate the constraints as soft constraints.\n            terminate_run_on_done (bool): Terminate the run when the environment returns done or not.\n            constraint_tol (float): Tolerance to add the the constraint as sometimes solvers are not exact.\n            output_dir (str): output directory to write logs and results.\n            additional_constraints (list): List of additional constraints\n            use_gpu (bool): False (use cpu) True (use cuda).\n            seed (int): random seed.\n\n        \"\"\"\n        # setup env\n        self.quad = quad\n        self.solver = solver\n        self.dt = dt\n        self.T = horizon\n        self.nx = self.quad.n\n        self.nu = self.quad.m\n        self.nz = self.quad.n\n        self.name = name\n\n        self.constraints = {}\n        self.input_bound = input_bound\n        # Handle constraints\n        if upper_bounds is not None and lower_bounds is not None:\n            assert len(upper_bounds.keys()) == len(upper_bounds), 'Provide upper and lower bounds together.'\n            for state in self.quad.state_names:\n                if state in upper_bounds.keys():\n                    self.constraints[state] = {}\n                    self.constraints[state]['upper_bound'] = upper_bounds[state]\n                    self.constraints[state]['lower_bound'] = lower_bounds[state]\n            self.con_tol = con_tol\n\n        # Setup controller parameters\n        self.Q = get_cost_weight_matrix(q_mpc, self.nx)\n        self.R = get_cost_weight_matrix(r_mpc, self.nu)\n\n        # compute mappings and flat dynamics\n        if dynamics is None:\n            #self.dyn_func = quad.cs_nonlin_dyn_discrete\n            self.dyn_func = quad.cs_lin_dyn\n        else:\n            self.dyn_func = dynamics\n\n        if reference_generator is None:\n            self.reference_generator = quad.real_reference_generator\n        else:\n            self.reference_generator = reference_generator\n\n        self.v_hist = None\n\n        # setup optimizer\n        self.x_prev = None\n        self.u_prev = None\n        self.setup_optimizer()\n\n    def setup_optimizer(self):\n        nz, nu = self.nz, self.nu\n        T = self.T\n        # Define optimizer and variables.\n        opti = cs.Opti()\n        x_var = opti.variable(nz, T+1)\n        u_var = opti.variable(nu, T)\n        x_0 = opti.parameter(nz, 1)\n        x_ref = opti.parameter(self.nz, T+1)\n\n        # Dynamics constraints\n        opti.subject_to(x_var[:,0] == x_0)\n        for i in range(T):\n            next_state = self.dyn_func(x0=x_var[:,i], p=u_var[:,i])['xf']\n            opti.subject_to(x_var[:, i+1] == next_state)\n\n        cost = 0\n        for i in range(1,T+1):\n            #cost += csQuadCost(x_var[[0,4],i], x_ref[[0,4],i], self.Q)\n            cost += csQuadCost(x_var[:, i], x_ref[:, i], self.Q)\n        for i in range(T):\n            cost += csQuadCost(u_var[:,i], np.zeros((nu,1)), self.R)\n\n        if not(self.constraints == {}):\n            for state in self.constraints.keys():\n                for i in range(0,T+1):\n                    opti.subject_to(opti.bounded(self.constraints[state]['lower_bound'] + self.con_tol,\n                                                 x_var[self.quad.state_indices[state], i],\n                                                 self.constraints[state]['upper_bound'] - self.con_tol))\n        if self.input_bound is not None:\n            for i in range(0,T):\n                opti.subject_to(opti.bounded(-self.input_bound, u_var[:,i], self.input_bound))\n\n        opti.minimize(cost)\n        opts = {\"expand\": True}\n        if platform == \"linux\":\n            opts.update({\"print_time\": 1})\n            opti.solver(self.solver, opts)\n        elif platform == \"darwin\":\n            opts.update({\"ipopt.max_iter\": 100})\n            opti.solver('ipopt', opts)\n        else:\n            print(\"[ERROR]: CasADi solver tested on Linux and OSX only.\")\n            exit()\n        #opts.update({\"qpsol_options\": {\"tol\": 1e-4}})\n        opti.solver(self.solver, opts)\n\n        self.opti_dict = {\n            \"opti\": opti,\n            \"x_var\": x_var,\n            \"u_var\": u_var,\n            \"x_0\": x_0,\n            \"x_ref\": x_ref,\n            \"cost\": cost\n        }\n\n    def compute_feedback_input(self,\n                               gp,\n                               z,\n                               x_ref,\n                               v_ref,\n                               x_init=None,\n                               t=None,\n                               params=None,\n                               **kwargs):\n\n        x_0 = self.quad.cs_x_from_z(z=z)['x'].toarray()\n        if t is None:\n            raise ValueError(\"MPC needs the current sim time to genereate the reference.\")\n        if params is None:\n            raise ValueError(\"MPC needs Ampa and Omega to genereate the reference.\")\n        x, u, return_status = self.select_flat_input(x_0, t, params)\n        z_opt = self.quad.cs_z_from_x(x=x)['z']\n        v_des = self.quad.cs_v_from_u(z=z_opt, u=u)['v'].toarray()\n\n        return u, v_des, return_status, 0\n\n    def select_flat_input(self, obs, t, params):\n        amp = params[\"Amp\"]\n        omega = params[\"omega\"]\n        #x_obs = obs[:,None]\n        x_obs = obs\n        if self.x_prev is not None:\n            x_error = self.x_prev[:, 1] - np.squeeze(x_obs)\n            self.results_dict['x_error'].append(deepcopy(x_error))\n        self.results_dict['flat_states'].append(deepcopy(x_obs))\n        opti_dict = self.opti_dict\n        opti = opti_dict[\"opti\"]\n        x_var = opti_dict[\"x_var\"]\n        u_var = opti_dict[\"u_var\"]\n        x_0 = opti_dict[\"x_0\"]\n        x_ref = opti_dict[\"x_ref\"]\n        cost = opti_dict[\"cost\"]\n        # Assign the initial state.\n        opti.set_value(x_0, x_obs)\n        # Assign reference trajectory within horizon.\n        t_ref = np.linspace(t, t+self.dt*self.T, num=self.T+1)\n        goal_states = self.reference_generator(t_ref, amp, omega)[0]\n        self.results_dict['goal_states'].append(goal_states)\n        opti.set_value(x_ref, goal_states)\n        if self.x_prev is not None and self.u_prev is not None:\n            x_guess = np.hstack((x_obs, self.x_prev[:,1:]))\n            u_guess = np.hstack((self.u_prev[:,1:], self.u_prev[:,-1,None]))\n            opti.set_initial(x_var, x_guess)\n            opti.set_initial(u_var, u_guess)\n        # Solve the optimization problem.\n        try:\n            sol = opti.solve()\n            x_val, u_val = np.atleast_2d(sol.value(x_var)), np.atleast_2d(sol.value(u_var))\n            self.x_prev = x_val\n            self.u_prev = u_val\n            self.results_dict['horizon_states'].append(deepcopy(self.x_prev))\n            self.results_dict['horizon_inputs'].append(deepcopy(self.u_prev))\n            return_status = True\n        except RuntimeError as e:\n            print(e)\n            return_status = False #opti.return_status()\n            print(\"[Warn]: %s\" % return_status)\n            #raise ValueError()\n\n        return x_val[:,0], u_val[:,0], return_status\n\n\n    def setup_results_dict(self):\n        \"\"\"\n\n        \"\"\"\n        self.results_dict = { 'obs': [],\n                              'reward': [],\n                              'done': [],\n                              'info': [],\n                              'action': [],\n                              'horizon_inputs': [],\n                              'horizon_states': [],\n                              'goal_states': [],\n                              'flat_states': [],\n                              'x_error': [],\n                              'augmented_states': [],\n                              'frames': [],\n                              'state_mse': [],\n                              'common_cost': [],\n                              'state': [],\n                              'state_error': [],\n                              't_wall': []\n                              }\n\n    def reset(self, dynamics=None):\n        self.x_prev = None\n        self.u_prev = None\n\n        # Reset Data Storage.\n        self.setup_results_dict()\n\n        # compute mappings and flat dynamics\n        if dynamics is None:\n            if self.dyn_func is None:\n                self.dyn_func = self.quad.cs_nonlin_dyn_discrete\n        else:\n            self.dyn_func = dynamics\n\n\n        # setup optimizer\n        self.setup_optimizer()", "\nclass MPC:\n    \"\"\"MPC with full nonlinear model.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            quad,\n            name='MPC',\n            horizon: int = 10,\n            dt: float = 0.1,\n            q_mpc: list = [1],\n            r_mpc: list = [1],\n            solver: str = 'ipopt',\n            constraints: dict = None,\n            dynamics: cs.Function = None,\n            reference_generator = None,\n            upper_bounds: dict = None,\n            lower_bounds: dict = None,\n            input_bound: float = None,\n            con_tol: float = 0.0,\n            ):\n        \"\"\"Creates task and controller.\n\n        Args:\n            env_func (Callable): function to instantiate task/environment.\n            horizon (int): mpc planning horizon.\n            q_mpc (list): diagonals of state cost weight.\n            r_mpc (list): diagonals of input/action cost weight.\n            warmstart (bool): if to initialize from previous iteration.\n            soft_constraints (bool): Formulate the constraints as soft constraints.\n            terminate_run_on_done (bool): Terminate the run when the environment returns done or not.\n            constraint_tol (float): Tolerance to add the the constraint as sometimes solvers are not exact.\n            output_dir (str): output directory to write logs and results.\n            additional_constraints (list): List of additional constraints\n            use_gpu (bool): False (use cpu) True (use cuda).\n            seed (int): random seed.\n\n        \"\"\"\n        # setup env\n        self.quad = quad\n        self.solver = solver\n        self.dt = dt\n        self.T = horizon\n        self.nx = self.quad.n\n        self.nu = self.quad.m\n        self.nz = self.quad.n\n        self.name = name\n\n        self.constraints = {}\n        self.input_bound = input_bound\n        # Handle constraints\n        if upper_bounds is not None and lower_bounds is not None:\n            assert len(upper_bounds.keys()) == len(upper_bounds), 'Provide upper and lower bounds together.'\n            for state in self.quad.state_names:\n                if state in upper_bounds.keys():\n                    self.constraints[state] = {}\n                    self.constraints[state]['upper_bound'] = upper_bounds[state]\n                    self.constraints[state]['lower_bound'] = lower_bounds[state]\n            self.con_tol = con_tol\n\n        # Setup controller parameters\n        self.Q = get_cost_weight_matrix(q_mpc, self.nx)\n        self.R = get_cost_weight_matrix(r_mpc, self.nu)\n\n        # compute mappings and flat dynamics\n        if dynamics is None:\n            #self.dyn_func = quad.cs_nonlin_dyn_discrete\n            self.dyn_func = quad.cs_lin_dyn\n        else:\n            self.dyn_func = dynamics\n\n        if reference_generator is None:\n            self.reference_generator = quad.real_reference_generator\n        else:\n            self.reference_generator = reference_generator\n\n        self.v_hist = None\n\n        # setup optimizer\n        self.x_prev = None\n        self.u_prev = None\n        self.setup_optimizer()\n\n    def setup_optimizer(self):\n        nz, nu = self.nz, self.nu\n        T = self.T\n        # Define optimizer and variables.\n        opti = cs.Opti()\n        x_var = opti.variable(nz, T+1)\n        u_var = opti.variable(nu, T)\n        x_0 = opti.parameter(nz, 1)\n        x_ref = opti.parameter(self.nz, T+1)\n\n        # Dynamics constraints\n        opti.subject_to(x_var[:,0] == x_0)\n        for i in range(T):\n            next_state = self.dyn_func(x0=x_var[:,i], p=u_var[:,i])['xf']\n            opti.subject_to(x_var[:, i+1] == next_state)\n\n        cost = 0\n        for i in range(1,T+1):\n            #cost += csQuadCost(x_var[[0,4],i], x_ref[[0,4],i], self.Q)\n            cost += csQuadCost(x_var[:, i], x_ref[:, i], self.Q)\n        for i in range(T):\n            cost += csQuadCost(u_var[:,i], np.zeros((nu,1)), self.R)\n\n        if not(self.constraints == {}):\n            for state in self.constraints.keys():\n                for i in range(0,T+1):\n                    opti.subject_to(opti.bounded(self.constraints[state]['lower_bound'] + self.con_tol,\n                                                 x_var[self.quad.state_indices[state], i],\n                                                 self.constraints[state]['upper_bound'] - self.con_tol))\n        if self.input_bound is not None:\n            for i in range(0,T):\n                opti.subject_to(opti.bounded(-self.input_bound, u_var[:,i], self.input_bound))\n\n        opti.minimize(cost)\n        opts = {\"expand\": True}\n        if platform == \"linux\":\n            opts.update({\"print_time\": 1})\n            opti.solver(self.solver, opts)\n        elif platform == \"darwin\":\n            opts.update({\"ipopt.max_iter\": 100})\n            opti.solver('ipopt', opts)\n        else:\n            print(\"[ERROR]: CasADi solver tested on Linux and OSX only.\")\n            exit()\n        #opts.update({\"qpsol_options\": {\"tol\": 1e-4}})\n        opti.solver(self.solver, opts)\n\n        self.opti_dict = {\n            \"opti\": opti,\n            \"x_var\": x_var,\n            \"u_var\": u_var,\n            \"x_0\": x_0,\n            \"x_ref\": x_ref,\n            \"cost\": cost\n        }\n\n    def compute_feedback_input(self,\n                               gp,\n                               z,\n                               x_ref,\n                               v_ref,\n                               x_init=None,\n                               t=None,\n                               params=None,\n                               **kwargs):\n\n        x_0 = self.quad.cs_x_from_z(z=z)['x'].toarray()\n        if t is None:\n            raise ValueError(\"MPC needs the current sim time to genereate the reference.\")\n        if params is None:\n            raise ValueError(\"MPC needs Ampa and Omega to genereate the reference.\")\n        x, u, return_status = self.select_flat_input(x_0, t, params)\n        z_opt = self.quad.cs_z_from_x(x=x)['z']\n        v_des = self.quad.cs_v_from_u(z=z_opt, u=u)['v'].toarray()\n\n        return u, v_des, return_status, 0\n\n    def select_flat_input(self, obs, t, params):\n        amp = params[\"Amp\"]\n        omega = params[\"omega\"]\n        #x_obs = obs[:,None]\n        x_obs = obs\n        if self.x_prev is not None:\n            x_error = self.x_prev[:, 1] - np.squeeze(x_obs)\n            self.results_dict['x_error'].append(deepcopy(x_error))\n        self.results_dict['flat_states'].append(deepcopy(x_obs))\n        opti_dict = self.opti_dict\n        opti = opti_dict[\"opti\"]\n        x_var = opti_dict[\"x_var\"]\n        u_var = opti_dict[\"u_var\"]\n        x_0 = opti_dict[\"x_0\"]\n        x_ref = opti_dict[\"x_ref\"]\n        cost = opti_dict[\"cost\"]\n        # Assign the initial state.\n        opti.set_value(x_0, x_obs)\n        # Assign reference trajectory within horizon.\n        t_ref = np.linspace(t, t+self.dt*self.T, num=self.T+1)\n        goal_states = self.reference_generator(t_ref, amp, omega)[0]\n        self.results_dict['goal_states'].append(goal_states)\n        opti.set_value(x_ref, goal_states)\n        if self.x_prev is not None and self.u_prev is not None:\n            x_guess = np.hstack((x_obs, self.x_prev[:,1:]))\n            u_guess = np.hstack((self.u_prev[:,1:], self.u_prev[:,-1,None]))\n            opti.set_initial(x_var, x_guess)\n            opti.set_initial(u_var, u_guess)\n        # Solve the optimization problem.\n        try:\n            sol = opti.solve()\n            x_val, u_val = np.atleast_2d(sol.value(x_var)), np.atleast_2d(sol.value(u_var))\n            self.x_prev = x_val\n            self.u_prev = u_val\n            self.results_dict['horizon_states'].append(deepcopy(self.x_prev))\n            self.results_dict['horizon_inputs'].append(deepcopy(self.u_prev))\n            return_status = True\n        except RuntimeError as e:\n            print(e)\n            return_status = False #opti.return_status()\n            print(\"[Warn]: %s\" % return_status)\n            #raise ValueError()\n\n        return x_val[:,0], u_val[:,0], return_status\n\n\n    def setup_results_dict(self):\n        \"\"\"\n\n        \"\"\"\n        self.results_dict = { 'obs': [],\n                              'reward': [],\n                              'done': [],\n                              'info': [],\n                              'action': [],\n                              'horizon_inputs': [],\n                              'horizon_states': [],\n                              'goal_states': [],\n                              'flat_states': [],\n                              'x_error': [],\n                              'augmented_states': [],\n                              'frames': [],\n                              'state_mse': [],\n                              'common_cost': [],\n                              'state': [],\n                              'state_error': [],\n                              't_wall': []\n                              }\n\n    def reset(self, dynamics=None):\n        self.x_prev = None\n        self.u_prev = None\n\n        # Reset Data Storage.\n        self.setup_results_dict()\n\n        # compute mappings and flat dynamics\n        if dynamics is None:\n            if self.dyn_func is None:\n                self.dyn_func = self.quad.cs_nonlin_dyn_discrete\n        else:\n            self.dyn_func = dynamics\n\n\n        # setup optimizer\n        self.setup_optimizer()", "\n\n"]}
{"filename": "controllers/discrete_socp_filter.py", "chunked_list": ["import numpy as np\nfrom numpy.linalg import norm\nimport cvxpy as cp\nimport torch\n\nclass DiscreteSOCPFilter:\n    def __init__(self, name, quad, beta, d_weight=25.0, input_bound=None, state_bound=None, ctrl=None, gp=None):\n        self.name = name\n        self.quad = quad\n        self.beta = beta\n        self.d_weight = d_weight\n        self.gp = gp\n        if ctrl.P is None:\n            raise ValueError('Controller must have gain matrics P, Q, R, K')\n        else:\n            self.P = ctrl.P\n            self.R = ctrl.R\n            self.Q = ctrl.Q\n            self.K = ctrl.K\n            self.Ad = ctrl.Ad\n            self.Bd = ctrl.Bd\n        self.input_bound = input_bound\n        self.state_bound = state_bound\n        if ctrl is not None:\n            if not(ctrl.name in ['FMPC', 'DLQR']):\n                raise ValueError('Only FMPC or DLQR can be used here for now.')\n            self.ctrl = ctrl\n        else:\n            self.ctrl = None\n\n        # Opt variables and parameters\n        self.X = cp.Variable(shape=(3,))\n        self.A1 = cp.Parameter(shape=(3, 3))\n        self.A2 = cp.Parameter(shape=(3, 3))\n        self.b1 = cp.Parameter(shape=(3,))\n        self.b2 = cp.Parameter(shape=(3,))\n        self.c1 = cp.Parameter(shape=(1, 3))\n        self.c2 = cp.Parameter(shape=(1, 3))\n        self.d1 = cp.Parameter()\n        self.d2 = cp.Parameter()\n        # put into lists\n        As = [self.A1, self.A2]\n        bs = [self.b1, self.b2]\n        cs = [self.c1, self.c2]\n        ds = [self.d1, self.d2]\n        # Add input constraints if supplied\n        if input_bound is not None:\n            A3 = np.zeros((3, 3))\n            A3[0, 0] = 1.0\n            b3 = np.zeros((3, 1))\n            c3 = np.zeros((1, 3))\n            d3 = input_bound\n            As.append(A3)\n            bs.append(b3)\n            cs.append(c3)\n            ds.append(d3)\n        if state_bound is not None:\n            h = state_bound['h']\n            bcon = state_bound['b']\n            phi_p = state_bound['phi_p']\n            self.del_sig = phi_p * np.sqrt(h.T @ self.Bd @ self.Bd.T @ h)\n            self.Astate = cp.Parameter(shape=(3, 3))\n            self.bstate = cp.Parameter(shape=(3,))\n            self.cstate = cp.Parameter(shape=(1, 3))\n            self.dstate = cp.Parameter()\n            #self.Astate = np.zeros((3,3))\n            #self.Astate[0, 0] = 1.0\n            #self.bstate = np.zeros((3, 1))\n            #self.cstate = np.zeros((1, 3))\n            #self.dstate = 0.0\n            As.append(self.Astate)\n            bs.append(self.bstate)\n            cs.append(self.cstate)\n            ds.append(self.dstate)\n        else:\n            self.Astate = None\n            self.bstate = None\n            self.cstate = None\n            self.dstate = None\n        # define cost function\n        self.cost = cp.Parameter(shape=(1, 3))\n        m = len(As)\n        soc_constraints = [\n            cp.SOC(cs[i] @ self.X + ds[i], As[i] @ self.X + bs[i]) for i in range(m)\n        ]\n        self.prob = cp.Problem(cp.Minimize(self.cost @ self.X), soc_constraints)\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, t=None, params=None, **kwargs):\n        \"\"\" Compute u so it can be used in feedback function\"\"\"\n        if gp is None:\n            gp = self.gp\n        if self.ctrl is None:\n            v_des = - self.K.dot((z - z_ref)) + v_ref\n            u, d_sf = self.solve(gp, z, z_ref, v_des, x_init=x_init)\n        else:\n            zd, v_des, return_status = self.ctrl.select_flat_input(z, t, params)\n            zd = np.atleast_2d(zd)\n            u, d_sf = self.solve(gp, zd, z_ref, v_des, x_init=x_init)\n        if 'optimal' in self.prob.status:\n            success = True\n        else:\n            success = False\n        return u, v_des, success, d_sf\n\n    def solve(self, gp_model, z, z_ref, v_des, x_init=np.zeros((3,))):\n        e_k = z - z_ref\n        # Compute state dependent values\n        gam1, gam2, gam3, gam4, gam5 = get_gammas(z, gp_model)\n        w, norm_w = compute_w(e_k, self.Ad, self.Bd, self.P, self.K)\n        #v_nom = -self.K @ e_k + v_des\n        v_nom = v_des\n\n        # Compute cost coefficients\n        cost = compute_cost(gam1, gam2, gam4, v_des)\n        self.cost.value = cost\n\n        # Compute stablity filter coeffs\n        A1, b1, c1, d1 = stab_filter_matrices(gam1, gam2, gam3, gam4, gam5,\n                                              self.Q, self.R, self.P, self.K, self.Bd, e_k,\n                                              norm_w, w,\n                                              self.input_bound, v_nom, self.beta)\n        self.A1.value = A1\n        self.b1.value = b1\n        self.c1.value = c1\n        self.d1.value = d1\n\n        # Compute dummy var mats\n        A2, b2, c2, d2 = dummy_var_matrices(gam2, gam5, self.d_weight)\n        self.A2.value = A2\n        self.b2.value = b2\n        self.c2.value = c2\n        self.d2.value = d2\n\n        # Compute state constraints.\n\n        if self.state_bound is not None:\n\n            Astate, bstate, cstate, dstate = state_con_matrices(z, gam1, gam2, gam3, gam4, gam5,\n                                                                self.state_bound, self.Ad, self.Bd, self.del_sig,\n                                                                self.d_weight)\n\n            self.Astate.value = Astate\n            self.bstate.value = bstate.squeeze()\n            self.cstate.value = cstate\n            self.dstate.value = dstate.squeeze()\n\n        self.X.value = x_init\n        self.prob.solve(solver='MOSEK', warm_start=True, verbose=True) # SCS was used in paper\n        if 'optimal' in self.prob.status:\n            return self.X.value[0], self.X.value[2]\n        else:\n            return 0, 0", "\ndef get_gammas(z, gp_model):\n    query_np = np.hstack((z.T, np.zeros((1, 1)))) # ToDo: Why is this 0 added here?\n    query = torch.from_numpy(query_np).double()\n    gamma1, gamma2, gamma3, gamma4, gamma5 = gp_model.model.compute_gammas(query)\n    gamma1 = gamma1.numpy().squeeze()\n    gamma2 = gamma2.numpy().squeeze()\n    gamma3 = gamma3.numpy().squeeze()\n    gamma4 = gamma4.numpy().squeeze()\n    gamma5 = gamma5.numpy().squeeze()\n    return gamma1, gamma2, gamma3, gamma4, gamma5", "\ndef compute_cost(gam1, gam2, gam4, v_des):\n    cost = np.array([[2 * gam1 * gam2 - 2 * gam2 * v_des.squeeze() + gam4, 0, 1.0]])\n    return cost\ndef compute_w(e_k, Ad, Bd, P, K):\n    w = e_k.T @ (Ad - Bd @ K).T @ P @ Bd\n    return w.squeeze(), np.linalg.norm(w)\n\ndef stab_filter_matrices(gam1,\n                         gam2,\n                         gam3,\n                         gam4,\n                         gam5,\n                         Q, R, P, K, Bd,\n                         e_k,\n                         norm_w, w,\n                         u_max, v_nom, beta):\n    A1, b1 = stab_filter_A1_and_b1(gam3, gam4, gam5, norm_w)\n    c1, d1 = stab_filter_c1_and_d1(gam1, gam2,\n                          Q, R, P, K, Bd,\n                          e_k, w,\n                          u_max, v_nom, beta)\n    return A1, b1, c1, d1", "def stab_filter_matrices(gam1,\n                         gam2,\n                         gam3,\n                         gam4,\n                         gam5,\n                         Q, R, P, K, Bd,\n                         e_k,\n                         norm_w, w,\n                         u_max, v_nom, beta):\n    A1, b1 = stab_filter_A1_and_b1(gam3, gam4, gam5, norm_w)\n    c1, d1 = stab_filter_c1_and_d1(gam1, gam2,\n                          Q, R, P, K, Bd,\n                          e_k, w,\n                          u_max, v_nom, beta)\n    return A1, b1, c1, d1", "\ndef stab_filter_A1_and_b1(gam3,\n                          gam4,\n                          gam5,\n                          norm_w):\n    A1 = np.array([[norm_w*np.sqrt(gam5), 0, 0],\n                   [0, 0, 0],\n                   [0, 0, 0]])\n    b1 = np.array([[norm_w*gam4 / (2 * np.sqrt(gam5))],\n                   [norm_w*np.sqrt(gam3 - 0.25 * gam4 ** 2 / gam5)],\n                   [0]])\n    return A1, b1.squeeze()", "\ndef stab_filter_c1_and_d1(gam1,\n                          gam2,\n                          Q, R, P, K, Bd,\n                          e_k,\n                          w,\n                          u_max, v_nom, beta):\n    d_a = e_k.T @ P @ e_k\n    d_b = e_k.T @ (P - Q - K.T @ R @ K) @ e_k\n    d_c = np.max([(gam1 + gam2*u_max - v_nom)**2, (gam1 + gam2*(-u_max) - v_nom)**2])* Bd.T @ P @ Bd\n    #d_c = 0.0\n    d_d = 2*w*(gam1 - v_nom)\n    d1 = (-1/(2*beta))*(d_a - d_b - d_c + d_d)\n\n    c1 = (-1/(2*beta))*np.array([[2*w*gam2, 1.0, 0.0]])\n\n    return c1, d1.squeeze()", "\ndef dummy_var_matrices(gam2, gam5, d_weight):\n    A2 = np.array([[2.0 * np.sqrt(gam2 ** 2 + gam5), 0, 0],\n                   [0, 2.0 * d_weight, 0],\n                   [0, 0, -1.0]])\n    b2 = np.array([[0], [0], [1.0]])\n    c2 = np.array([[0, 0, 1.0]])\n    d2 = 1\n\n    return A2, b2.squeeze(), c2, d2", "\ndef state_con_matrices(z, gam1, gam2, gam3, gam4, gam5,\n                       state_bound, Ad, Bd, del_sig, d_weight):\n    h = state_bound['h']\n    bcon = state_bound['b']\n    Astate = np.array([[float(del_sig*np.sqrt(gam5)), 0, 0],\n                       [0, 0, 0],\n                       [0, 0, 0]])\n    bstate = np.array([[float(del_sig*gam4 / (2 * np.sqrt(gam5)))],\n                       [float(del_sig*np.sqrt(gam3 - 0.25 * gam4 ** 2 / gam5))],\n                       [0]])\n    cstate = np.array([[float(-h.T @ Bd * gam2), d_weight, 0.0]])\n    dstate = -h.T @ Ad @ z - h.T @ Bd * gam1 + bcon\n    return Astate, bstate, cstate, dstate", ""]}
{"filename": "controllers/__init__.py", "chunked_list": [""]}
{"filename": "controllers/dlqr.py", "chunked_list": ["import numpy as np\nimport scipy as sp\nimport casadi as cs\nimport control\n\nfrom copy import deepcopy\nfrom utils.math_utils import get_cost_weight_matrix, csQuadCost\n\nclass DLQR:\n    def __init__(self,\n                 quad,\n                 dt: float = 0.1,\n                 q_lqr: list = [1],\n                 r_lqr: list = [1],\n                 reference_generator = None,\n                 con_tol: float = 0.0,\n                 bound = None,\n                 **kwargs\n                 ):\n\n\n        # setup env\n        self.quad = quad\n        self.dt = dt\n        self.ny = self.quad.n\n        self.nu = self.quad.m\n        self.nz = self.quad.n\n        self.bound = bound\n        self.name = 'DLQR'\n\n        if reference_generator is None:\n            self.reference_generator = quad.reference_generator\n        else:\n            self.reference_generator = reference_generator\n\n        # Setup controller parameters\n        self.Q = get_cost_weight_matrix(q_lqr, self.nz)\n        self.R = get_cost_weight_matrix(r_lqr, self.nu)\n\n        self.Ad = quad.Ad\n        self.Bd = quad.Bd\n\n        # Compute Gain.\n        self.K, self.P = self.compute_gain()\n\n    def compute_gain(self):\n        K, P, E = control.dlqr(self.Ad, self.Bd, self.Q, self.R)\n        return K, P\n\n    def select_flat_input(self, z, t, params):\n        z_ref, v_ref = self.reference_generator(t, params['Amp'], params['omega'])\n        v_des = - self.K @ (z - z_ref) + v_ref\n        zd = z\n        return_status = True\n        return zd, v_des, return_status\n\n    def compute_feedback_input(self,\n                           gp,\n                           z,\n                           z_ref,\n                           v_ref,\n                           x_init=None,\n                           t=None,\n                           params=None,\n                           **kwargs):\n        _, v_des, _ = self.select_flat_input(z, t, params)\n        u = self.quad.u_from_v(v_des, z)\n        if self.bound is not None:\n            u = np.clip(u, -self.bound, self.bound)\n        return u, v_des, True, 0", "class DLQR:\n    def __init__(self,\n                 quad,\n                 dt: float = 0.1,\n                 q_lqr: list = [1],\n                 r_lqr: list = [1],\n                 reference_generator = None,\n                 con_tol: float = 0.0,\n                 bound = None,\n                 **kwargs\n                 ):\n\n\n        # setup env\n        self.quad = quad\n        self.dt = dt\n        self.ny = self.quad.n\n        self.nu = self.quad.m\n        self.nz = self.quad.n\n        self.bound = bound\n        self.name = 'DLQR'\n\n        if reference_generator is None:\n            self.reference_generator = quad.reference_generator\n        else:\n            self.reference_generator = reference_generator\n\n        # Setup controller parameters\n        self.Q = get_cost_weight_matrix(q_lqr, self.nz)\n        self.R = get_cost_weight_matrix(r_lqr, self.nu)\n\n        self.Ad = quad.Ad\n        self.Bd = quad.Bd\n\n        # Compute Gain.\n        self.K, self.P = self.compute_gain()\n\n    def compute_gain(self):\n        K, P, E = control.dlqr(self.Ad, self.Bd, self.Q, self.R)\n        return K, P\n\n    def select_flat_input(self, z, t, params):\n        z_ref, v_ref = self.reference_generator(t, params['Amp'], params['omega'])\n        v_des = - self.K @ (z - z_ref) + v_ref\n        zd = z\n        return_status = True\n        return zd, v_des, return_status\n\n    def compute_feedback_input(self,\n                           gp,\n                           z,\n                           z_ref,\n                           v_ref,\n                           x_init=None,\n                           t=None,\n                           params=None,\n                           **kwargs):\n        _, v_des, _ = self.select_flat_input(z, t, params)\n        u = self.quad.u_from_v(v_des, z)\n        if self.bound is not None:\n            u = np.clip(u, -self.bound, self.bound)\n        return u, v_des, True, 0", ""]}
{"filename": "controllers/fmpc.py", "chunked_list": ["\"\"\"Flat MPC based on Greef 2018 'Flatness-based Model Predictive Control for Quadrotor Trajectory Tracking'\n\n\"\"\"\n\nimport numpy as np\nimport scipy as sp\nimport control\nimport casadi as cs\nfrom copy import deepcopy\nfrom utils.math_utils import get_cost_weight_matrix, csQuadCost", "from copy import deepcopy\nfrom utils.math_utils import get_cost_weight_matrix, csQuadCost\n\nclass FMPC:\n    def __init__(self,\n                 quad,\n                 horizon: int = 10,\n                 dt: float = 0.1,\n                 q_mpc: list = [1],\n                 r_mpc: list = [1],\n                 solver: str = 'qrqp',\n                 reference_generator = None,\n                 constraints: dict = None,\n                 upper_bounds: dict = None,\n                 lower_bounds: dict = None,\n                 con_tol: float = 0.0,\n                 **kwargs\n                 ):\n\n\n        # setup env\n        self.quad = quad\n        self.solver = solver\n        self.dt = dt\n        self.T = horizon\n        self.ny = self.quad.n\n        self.nu = self.quad.m\n        self.nz = self.quad.n\n        self.name = 'FMPC'\n\n        self.constraints = {}\n        # Handle constraints\n        if upper_bounds is not None and lower_bounds is not None:\n            assert len(upper_bounds.keys()) == len(upper_bounds), 'Provide upper and lower bounds together.'\n            for state in self.quad.state_names:\n                if state in upper_bounds.keys():\n                    self.constraints[state] = {}\n                    self.constraints[state]['upper_bound'] = upper_bounds[state]\n                    self.constraints[state]['lower_bound'] = lower_bounds[state]\n            self.con_tol = con_tol\n\n        if reference_generator is None:\n            self.reference_generator = quad.real_reference_generator\n        else:\n            self.reference_generator = reference_generator\n\n        # Setup controller parameters\n        self.Q = get_cost_weight_matrix(q_mpc, self.nz)\n        self.R = get_cost_weight_matrix(r_mpc, self.nu)\n\n        # compute mappings and flat dynamics\n        self.flat_dyn_func = self.quad.cs_true_flat_dynamics_from_v\n        self.Ad = self.quad.Ad\n        self.Bd = self.quad.Bd\n        self.v_hist = None\n\n        # setup optimizer\n        self.z_prev = None\n        self.v_prev = None\n        self.setup_optimizer()\n        self.P, self.K = self.compute_K_and_P()\n\n    def compute_K_and_P(self):\n        # Equations taken from Borrelli Sec 8.3 but with the opposite sign for K as we are using the convention\n        # u = -Kx and they use u = Kx\n        P = deepcopy(self.Q)*100.0\n        for i in range(self.T):\n            P = self.Ad.T @ P @ self.Ad + self.Q - self.Ad.T @ P @ self.Bd @ np.linalg.pinv(self.Bd.T @ P @ self.Bd + self.R) @ self.Bd.T @ P @ self.Ad\n        K = np.linalg.pinv(self.Bd.T @ P @ self.Bd + self.R) @ self.Bd.T @ P @ self.Ad\n        #K, P, _ = control.dlqr(self.Ad, self.Bd, P, self.R)\n        return P, K\n\n    def setup_optimizer(self):\n        nz, nu = self.nz, self.nu\n        T = self.T\n        # Define optimizer and variables.\n        if self.solver in ['qrqp', 'qpoases']:\n            opti = cs.Opti('conic')\n        else:\n            opti = cs.Opti()\n        z_var = opti.variable(nz, T+1)\n        v_var = opti.variable(nu, T)\n        z_0 = opti.parameter(nz, 1)\n        z_ref = opti.parameter(self.nz, T+1)\n\n        # Dynamics constraints\n        opti.subject_to(z_var[:,0] == z_0)\n        for i in range(T):\n            next_state = self.flat_dyn_func(z=z_var[:,i], v=v_var[:,i])['z_next']\n            opti.subject_to(z_var[:, i+1] == next_state)\n\n        cost = 0\n        for i in range(1,T+1):\n            #cost += csQuadCost(z_var[[0,4],i], z_ref[[0,4],i], self.Q)\n            cost += csQuadCost(z_var[:, i], z_ref[:, i], self.Q)\n        for i in range(T):\n            cost += csQuadCost(v_var[:,i], np.zeros((nu,1)), self.R)\n\n        if not(self.constraints == {}):\n            for state in self.constraints.keys():\n                for i in range(0,T+1):\n                    opti.subject_to(opti.bounded(self.constraints[state]['lower_bound'] + self.con_tol,\n                                                 z_var[self.quad.state_indices[state], i],\n                                                 self.constraints[state]['upper_bound'] - self.con_tol))\n\n        opti.minimize(cost)\n        opts = {\"expand\": True}\n        opts.update({\"print_time\": True})\n        #opts.update({\"qpsol_options\": {\"tol\": 1e-4}})\n        opti.solver(self.solver, opts)\n\n        self.opti_dict = {\n            \"opti\": opti,\n            \"z_var\": z_var,\n            \"v_var\": v_var,\n            \"z_0\": z_0,\n            \"z_ref\": z_ref,\n            \"cost\": cost\n        }\n\n    def compute_feedback_input(self,\n                               gp,\n                               z,\n                               z_ref,\n                               v_ref,\n                               x_init=None,\n                               t=None,\n                               params=None,\n                               **kwargs):\n        if t is None:\n            raise ValueError(\"FMPC needs the current sim time to genereate the reference.\")\n        if params is None:\n            raise ValueError(\"FMPC needs Ampa and Omega to genereate the reference.\")\n        zd, vd, return_status = self.select_flat_input(z, t, params)\n        u = self.quad.cs_u_from_v(z=zd, v=vd)['u'].toarray()\n\n        return u, vd, return_status, 0\n\n    def select_flat_input(self, obs, t, params):\n        amp = params[\"Amp\"]\n        omega = params[\"omega\"]\n        #z_obs = obs[:,None]\n        z_obs = obs\n        if self.z_prev is not None:\n            z_error = self.z_prev[:, 1] - np.squeeze(z_obs)\n            self.results_dict['z_error'].append(deepcopy(z_error))\n        self.results_dict['flat_states'].append(deepcopy(z_obs))\n        opti_dict = self.opti_dict\n        opti = opti_dict[\"opti\"]\n        z_var = opti_dict[\"z_var\"]\n        v_var = opti_dict[\"v_var\"]\n        z_0 = opti_dict[\"z_0\"]\n        z_ref = opti_dict[\"z_ref\"]\n        cost = opti_dict[\"cost\"]\n        # Assign the initial state.\n        opti.set_value(z_0, z_obs)\n        # Assign reference trajectory within horizon.\n        t_ref = np.linspace(t, t+self.dt*self.T, num=self.T+1)\n        goal_states = self.reference_generator(t_ref, amp, omega)[0]\n        self.results_dict['goal_states'].append(goal_states)\n        opti.set_value(z_ref, goal_states)\n        if self.z_prev is not None and self.v_prev is not None:\n            z_guess = np.hstack((z_obs, self.z_prev[:,1:]))\n            v_guess = np.hstack((self.v_prev[:,1:], self.v_prev[:,-1,None]))\n            opti.set_initial(z_var, z_guess)\n            opti.set_initial(v_var, v_guess)\n        # Solve the optimization problem.\n        try:\n            sol = opti.solve()\n            z_val, v_val = np.atleast_2d(sol.value(z_var)), np.atleast_2d(sol.value(v_var))\n            self.z_prev = z_val\n            self.v_prev = v_val\n            self.results_dict['horizon_states'].append(deepcopy(self.z_prev))\n            self.results_dict['horizon_inputs'].append(deepcopy(self.v_prev))\n            return_status = True\n        except RuntimeError as e:\n            print(e)\n            return_status = False #opti.return_status()\n            print(\"[Warn]: %s\" % return_status)\n            #raise ValueError()\n\n        return_status = opti.return_status()\n            #if return_status == 'unknown':\n            #    self.terminate_loop = True\n            #    u_val = self.u_prev\n            #    if u_val is None:\n            #        print('[WARN]: MPC Infeasible first step.')\n            #        u_val = np.zeros((1, self.model.nu))\n            #elif return_status == 'Maximum_Iterations_Exceeded':\n            #    self.terminate_loop = True\n            #    u_val = opti.debug.value(u_var)\n            #elif return_status == 'Search_Direction_Becomes_Too_Small':\n            #    self.terminate_loop = True\n            #    u_val = opti.debug.value(u_var)\n\n        # take first one from solved action sequence\n        z = z_val[:,0].reshape(self.nz, 1)\n        return z, v_val[:,0], return_status\n        #return z_val[:,0], v_val[:,0], return_status\n        #return goal_states[:,1], v_val[:, 0]\n\n    def setup_results_dict(self):\n        \"\"\"\n\n        \"\"\"\n        self.results_dict = { 'obs': [],\n                              'reward': [],\n                              'done': [],\n                              'info': [],\n                              'action': [],\n                              'horizon_inputs': [],\n                              'horizon_states': [],\n                              'goal_states': [],\n                              'flat_states': [],\n                              'z_error': [],\n                              'augmented_states': [],\n                              'frames': [],\n                              'state_mse': [],\n                              'common_cost': [],\n                              'state': [],\n                              'state_error': [],\n                              't_wall': []\n                              }\n\n    def reset(self):\n        self.z_prev = None\n        self.v_prev = None\n\n        # Reset Data Storage.\n        self.setup_results_dict()\n\n        # compute mappings and flat dynamics\n        self.flat_dyn_func = self.quad.cs_true_flat_dynamics_from_v\n\n        # setup optimizer\n        self.setup_optimizer()", "\ndef backward_diff(u, dt):\n    \"\"\" assume u is a list with u_k, u_k-1\"\"\"\n    u_dot = (u[:,0,None] - u[:,1,None])/dt\n    return u_dot\n\ndef circle(start_time, traj_length, sample_time, traj_period, scaling, offset):\n    '''Computes the coordinates of a circle trajectory at time t.\n\n    Args:\n        t (float): The time at which we want to sample one trajectory point.\n        traj_period (float): The period of the trajectory in seconds.\n        scaling (float, optional): Scaling factor for the trajectory.\n\n    Returns:\n        coords_a (float): The position in the first coordinate.\n        coords_b (float): The position in the second coordinate.\n        coords_a_dot (float): The velocity in the first coordinate.\n        coords_b_dot (float): The velocity in the second coordinate.\n    '''\n    times = np.arange(start_time, traj_length, sample_time)\n    traj_freq = 2.0 * np.pi / traj_period\n    z = scaling * np.cos(traj_freq * times) + offset[1]\n    z_dot = -scaling * traj_freq * np.sin(traj_freq * times)\n    z_ddot = -scaling * traj_freq**2 * np.cos(traj_freq * times)\n    z_dddot = scaling * traj_freq**3 * np.sin(traj_freq * times)\n\n    x = scaling * np.sin(traj_freq * times) + offset[0]\n    x_dot = scaling * traj_freq * np.cos(traj_freq * times)\n    x_ddot = -scaling * traj_freq**2 * np.sin(traj_freq * times)\n    x_dddot = -scaling * traj_freq**3 * np.cos(traj_freq * times)\n\n    ref = np.vstack((x,x_dot,x_ddot,x_dddot,z,z_dot,z_ddot,z_dddot))\n\n    return ref", "\n"]}
{"filename": "experiments/input_con_experiments.py", "chunked_list": ["import os\nimport munch\nimport numpy as np\nimport torch\nimport gpytorch\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment", "from utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom controllers.dlqr import DLQR\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n\nfrom learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess", "from learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'tracking_comp',\n           'dt': 0.02,\n           'T': 10.0,\n           'T_test': 10.0,\n           'input_bound': 15.0/180.0*np.pi,", "           'T_test': 10.0,\n           'input_bound': 15.0/180.0*np.pi,\n           #'state_bound': {'h': [1, 0, 0], 'b': 0.25, 'phi_p': 3.0}\n            }\nquad_config = {'thrust': 10,\n               'tau': 0.2,\n               'gamma': 3.0,\n               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,\n                     'tau': 0.05,", "quad_prior_config = {'thrust': 20,\n                     'tau': 0.05,\n                     'gamma': 0.0,\n                     'dt': config['dt']}\ngp_v_from_u_config = {'amp': 0.2,\n                      'omegalist': [0.3, 0.5, 0.7, 0.9],\n                      'sig': 0.0001,\n                      'N': 1000,\n                      'n_train': 500,\n                      'lr': 0.1,", "                      'n_train': 500,\n                      'lr': 0.1,\n                      #'output_dir': None\n                      'output_dir': '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-09-20-47-05_fe35b85/gp_v_from_u'\n                      }\nsigmas = 0.0001\ngpmpc_config = {\n         'amp': 0.2,\n         'omegalist': [0.3, 0.5, 0.7, 0.9],\n         'sig': 0.0001,", "         'omegalist': [0.3, 0.5, 0.7, 0.9],\n         'sig': 0.0001,\n         'num_samples': 5000,\n         'n_train': [2000, 2000, 2000],\n         'lr': [0.05, 0.05, 0.05],\n         'noise':  {'mean': [0.0, 0.0, 0.0], 'std': [sigmas, sigmas, sigmas]},\n         'mpc_prior': {'horizon': 50, #int(1/config['dt']),\n                       'q_mpc': [10.0, 0.1, 0.1],\n                       'r_mpc': [0.1],\n                       'input_bound': config['input_bound'],", "                       'r_mpc': [0.1],\n                       'input_bound': config['input_bound'],\n                       'solver': 'ipopt'\n                       },\n         'input_mask': [1,2,3],\n         'target_mask': [1,2],\n         'pred_kern_size': 200,\n         'gp_output_dir': '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-09-20-47-05_fe35b85/gpmpc_gp'\n         }\nlhs_config = { 'lower_bounds': [-0.01, -2.0, -1.5, -0.6],", "         }\nlhs_config = { 'lower_bounds': [-0.01, -2.0, -1.5, -0.6],\n               'upper_bounds': [0.01, 2.0, 1.5, 0.6]}\nfmpc_config = {'horizon': 50, #int(1/config['dt']),\n              'q_mpc': [10.0, 0.1, 0.1],\n              'r_mpc': [0.1],\n              'solver': 'ipopt'\n}\nsocp_config = {'d_weight': 0,\n               'beta': 2.0}", "socp_config = {'d_weight': 0,\n               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],\n               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {\n               'N': int(config['T_test']/config['dt']),\n               'n': 3,\n               'm': 1,\n               'dt': config['dt'],\n               'Amp': 0.2,", "               'dt': config['dt'],\n               'Amp': 0.2,\n               'omega': 0.6,\n               'ref_type': 'increasing_sine'\n}\nconfig['quad'] = quad_config\nconfig['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config\nconfig['gpmpc'] = gpmpc_config\nconfig['fmpc'] = fmpc_config", "config['gpmpc'] = gpmpc_config\nconfig['fmpc'] = fmpc_config\nconfig['socp'] = socp_config\nconfig['dlqr'] = dlrq_config\nconfig['test_params'] = test_params\nconfig['lhs_samp'] = lhs_config\nconfig = munch.munchify(config)\nset_dir_from_config(config)\nquad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)", "quad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)\n\n\nhorizon = config.gpmpc.mpc_prior.horizon\nq_mpc = config.gpmpc.mpc_prior.q_mpc\nr_mpc = config.gpmpc.mpc_prior.r_mpc\nsolver = config.gpmpc.mpc_prior.solver\nref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)", "ref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)\ninput_bound = config.gpmpc.mpc_prior.input_bound\n\nif config.gp_v_from_u.output_dir is None:\n    gp_inv_new = train_gp_v_from_u(config, quad_prior, quad)\nif config.gpmpc.gp_output_dir is None:\n    #train_gpmpc_gp(config, quad, quad_prior, mpc)\n    train_gpmpc_LHS(config, quad, quad_prior, mpc)\n", "\ninput_mask = config.gpmpc.input_mask\ntarget_mask = config.gpmpc.target_mask\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood(\n    constraint=gpytorch.constraints.GreaterThan(1e-6)).double()\nprior_model = deepcopy(quad_prior.cs_lin_dyn)\ngp_small = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                     likelihood,\n                                     len(target_mask),", "                                     likelihood,\n                                     len(target_mask),\n                                     input_mask=input_mask,\n                                     target_mask=target_mask)\nN_gp_small = config.gpmpc.pred_kern_size\ninterval = int(np.ceil(config.gpmpc.num_samples*0.8/N_gp_small))\ndh = DataHandler.load(os.path.join(config.gpmpc.gp_output_dir, 'data_handler'))\ngp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh.data.train_inputs[::interval,:]),\n                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),\n                              path_to_statedicts=config.gpmpc.gp_output_dir)", "                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),\n                              path_to_statedicts=config.gpmpc.gp_output_dir)\ngp_precict = gp_small.make_casadi_predict_func()\ndyn_func = combine_prior_and_gp(prior_model, gp_precict, input_mask, target_mask)\n# For testing\ngpmpc = MPC(quad=quad_prior,\n            name='GPMPC',\n            horizon=horizon,\n            dt=config.dt,\n            q_mpc=q_mpc,", "            dt=config.dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver,\n            dynamics=dyn_func,\n            reference_generator=ref_gen,\n            input_bound=input_bound)\ngpmpc.reset()\n\nmpc = MPC(quad=quad_prior,", "\nmpc = MPC(quad=quad_prior,\n          name='MPC',\n          horizon=horizon,\n          dt=config.dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver,\n          dynamics=prior_model,\n          reference_generator=ref_gen,", "          dynamics=prior_model,\n          reference_generator=ref_gen,\n          input_bound=input_bound)\nmpc.reset()\nfmpc = FMPC(quad=quad_prior,\n            dt=config.dt,\n            **config.fmpc,\n            reference_generator=ref_gen)\nfmpc.reset()\n", "fmpc.reset()\n\n\ngp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)\ngp_inv.init_with_hyperparam(config.gp_v_from_u.output_dir)\nfmpc_socp = DiscreteSOCPFilter('FMPC+SOCP',\n                               quad_prior,\n                               config.socp.beta,", "                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# Controller Parameters\n", "# Controller Parameters\n\ndlqr = DLQR(quad=quad_prior,\n            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr)\ndlqr_socp = DiscreteSOCPFilter('DLQR+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,", "                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(dlqr),\n                               gp=gp_inv)\n\nexp = Experiment('mpc', quad, [mpc, fmpc, dlqr, fmpc_socp, gpmpc, dlqr_socp], ref_gen, test_params, config)\nexp.run_experiment()\nexp.plot_tracking()", "exp.run_experiment()\nexp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')\nexp.summarize_timings()\nexp.plot_rmse()\n"]}
{"filename": "experiments/comparison_experiments.py", "chunked_list": ["import os\nimport munch\nimport numpy as np\nimport torch\nimport gpytorch\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment", "from utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom controllers.dlqr import DLQR\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n\nfrom learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess", "from learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'tracking_comp',\n           'dt': 0.02,\n           'T': 10.0,\n           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,", "           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,\n           #'state_bound': {'h': [1, 0, 0], 'b': 0.25, 'phi_p': 3.0}\n            }\nquad_config = {'thrust': 10,\n               'tau': 0.2,\n               'gamma': 3.0,\n               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,\n                     'tau': 0.05,", "quad_prior_config = {'thrust': 20,\n                     'tau': 0.05,\n                     'gamma': 0.0,\n                     'dt': config['dt']}\ngp_v_from_u_config = {'amp': 0.2,\n                      'omegalist': [0.3, 0.5, 0.7, 0.9],\n                      'sig': 0.0001,\n                      'N': 1000,\n                      'n_train': 500,\n                      'lr': 0.1,", "                      'n_train': 500,\n                      'lr': 0.1,\n                      #'output_dir': None\n                      'output_dir': '../models/gp_v_from_u'\n                      }\nsigmas = 0.0001\ngpmpc_config = {\n         'amp': 0.2,\n         'omegalist': [0.3, 0.5, 0.7, 0.9],\n         'sig': 0.0001,", "         'omegalist': [0.3, 0.5, 0.7, 0.9],\n         'sig': 0.0001,\n         'num_samples': 5000,\n         'n_train': [2000, 2000, 2000],\n         'lr': [0.05, 0.05, 0.05],\n         'noise':  {'mean': [0.0, 0.0, 0.0], 'std': [sigmas, sigmas, sigmas]},\n         'mpc_prior': {'horizon': 50, #int(1/config['dt']),\n                       'q_mpc': [10.0, 0.1, 0.1],\n                       'r_mpc': [0.1],\n                       'solver': 'ipopt'", "                       'r_mpc': [0.1],\n                       'solver': 'ipopt'\n                       },\n         'input_mask': [1,2,3],\n         'target_mask': [1,2],\n         'pred_kern_size': 200,\n         'gp_output_dir': '../models/gpmpc_gp'\n         }\nlhs_config = { 'lower_bounds': [-0.01, -2.0, -1.5, -0.6],\n               'upper_bounds': [0.01, 2.0, 1.5, 0.6]}", "lhs_config = { 'lower_bounds': [-0.01, -2.0, -1.5, -0.6],\n               'upper_bounds': [0.01, 2.0, 1.5, 0.6]}\nfmpc_config = {'horizon': 50, #int(1/config['dt']),\n              'q_mpc': [10.0, 0.1, 0.1],\n              'r_mpc': [0.1],\n              'solver': 'ipopt'\n}\nsocp_config = {'d_weight': 0,\n               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],", "               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],\n               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {\n               'N': int(config['T_test']/config['dt']),\n               'n': 3,\n               'm': 1,\n               'dt': config['dt'],\n               'Amp': 0.2,\n               'omega': 0.9,", "               'Amp': 0.2,\n               'omega': 0.9,\n               'ref_type': 'increasing_sine'\n}\nconfig['quad'] = quad_config\nconfig['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config\nconfig['gpmpc'] = gpmpc_config\nconfig['fmpc'] = fmpc_config\nconfig['socp'] = socp_config", "config['fmpc'] = fmpc_config\nconfig['socp'] = socp_config\nconfig['dlqr'] = dlrq_config\nconfig['test_params'] = test_params\nconfig['lhs_samp'] = lhs_config\nconfig = munch.munchify(config)\nset_dir_from_config(config)\nquad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)\n", "quad_prior = Quad1D(**quad_prior_config)\n\n\nhorizon = config.gpmpc.mpc_prior.horizon\nq_mpc = config.gpmpc.mpc_prior.q_mpc\nr_mpc = config.gpmpc.mpc_prior.r_mpc\nsolver = config.gpmpc.mpc_prior.solver\nref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)\n\nif config.gp_v_from_u.output_dir is None:\n    gp_inv_new = train_gp_v_from_u(config, quad_prior, quad)", "ref_gen = partial(quad.reference_generator, ref_type=ref_type)\n\nif config.gp_v_from_u.output_dir is None:\n    gp_inv_new = train_gp_v_from_u(config, quad_prior, quad)\nif config.gpmpc.gp_output_dir is None:\n    #train_gpmpc_gp(config, quad, quad_prior, mpc)\n    train_gpmpc_LHS(config, quad, quad_prior, mpc)\n\ninput_mask = config.gpmpc.input_mask\ntarget_mask = config.gpmpc.target_mask", "input_mask = config.gpmpc.input_mask\ntarget_mask = config.gpmpc.target_mask\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood(\n    constraint=gpytorch.constraints.GreaterThan(1e-6)).double()\nprior_model = deepcopy(quad_prior.cs_lin_dyn)\ngp_small = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                     likelihood,\n                                     len(target_mask),\n                                     input_mask=input_mask,", "                                     len(target_mask),\n                                     input_mask=input_mask,\n                                     target_mask=target_mask)\nN_gp_small = config.gpmpc.pred_kern_size\ninterval = int(np.ceil(config.gpmpc.num_samples*0.8/N_gp_small))\ndh = DataHandler.load(os.path.join(config.gpmpc.gp_output_dir, 'data_handler'))\ngp_small.init_with_hyperparam(train_inputs=torch.from_numpy(dh.data.train_inputs[::interval,:]),\n                              train_targets=torch.from_numpy(dh.data.train_targets[::interval,:]),\n                              path_to_statedicts=config.gpmpc.gp_output_dir)\ngp_precict = gp_small.make_casadi_predict_func()", "                              path_to_statedicts=config.gpmpc.gp_output_dir)\ngp_precict = gp_small.make_casadi_predict_func()\ndyn_func = combine_prior_and_gp(prior_model, gp_precict, input_mask, target_mask)\n# For testing\ngpmpc = MPC(quad=quad_prior,\n            name='GPMPC',\n            horizon=horizon,\n            dt=config.dt,\n            q_mpc=q_mpc,\n            r_mpc=r_mpc,", "            q_mpc=q_mpc,\n            r_mpc=r_mpc,\n            solver=solver,\n            dynamics=dyn_func,\n            reference_generator=ref_gen)\ngpmpc.reset()\n\nmpc = MPC(quad=quad_prior,\n          name='MPC',\n          horizon=horizon,", "          name='MPC',\n          horizon=horizon,\n          dt=config.dt,\n          q_mpc=q_mpc,\n          r_mpc=r_mpc,\n          solver=solver,\n          dynamics=prior_model,\n          reference_generator=ref_gen)\nmpc.reset()\nfmpc = FMPC(quad=quad_prior,", "mpc.reset()\nfmpc = FMPC(quad=quad_prior,\n            dt=config.dt,\n            **config.fmpc,\n            reference_generator=ref_gen)\nfmpc.reset()\n\n\ngp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()", "gp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)\ngp_inv.init_with_hyperparam(config.gp_v_from_u.output_dir)\nfmpc_socp = DiscreteSOCPFilter('FMPC+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,", "                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# Controller Parameters\n\ndlqr = DLQR(quad=quad_prior,\n            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,", "            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr)\ndlqr_socp = DiscreteSOCPFilter('DLQR+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(dlqr),", "                               state_bound=None,\n                               ctrl=deepcopy(dlqr),\n                               gp=gp_inv)\n\nexp = Experiment('mpc', quad, [mpc, fmpc, dlqr, fmpc_socp, gpmpc, dlqr_socp], ref_gen, test_params, config)\nexp.run_experiment()\nexp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')\nexp.summarize_timings()\nexp.plot_rmse()", "exp.summarize_timings()\nexp.plot_rmse()\n"]}
{"filename": "experiments/make_paper_plots.py", "chunked_list": ["import os\nimport numpy as np\nimport matplotlib\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['ps.fonttype'] = 42\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set_style('darkgrid')\n\ndef get_data_dict(track_dir):\n    data = np.load(os.path.join(track_dir, 'data.npz'), allow_pickle=True)\n    data_dict = dict(zip(list(data.keys()), (d[()] for d in data.values())))\n    return data_dict", "\ndef get_data_dict(track_dir):\n    data = np.load(os.path.join(track_dir, 'data.npz'), allow_pickle=True)\n    data_dict = dict(zip(list(data.keys()), (d[()] for d in data.values())))\n    return data_dict\ndef make_tracking_plot(track_dir, colors, save_name, labels=None, con=False, paper_dir=None, ctrls=None, error=False):\n    data = get_data_dict(track_dir)\n\n    fig_size = (5,3)\n\n    # Plot the states along the trajectory and compare with reference.\n    units = {0: 'm', 1: 'm/s', 2: 'm/s^2'}\n    fig, ax = plt.subplots(figsize=fig_size)\n    plt_id = 0\n    alpha = 0.7\n    if con:\n        ax.axhline(y=con, color='k', linestyle='solid', label='Constraint')\n    for ctrl_name, ctrl_data in data.items():\n        if ctrls is None or ctrl_name in ctrls:\n            if labels is None:\n                label = ctrl_name\n            else:\n                label = labels[ctrl_name]\n            common_plot_args = { 'label': label, 'color': colors[ctrl_name], 'alpha': alpha}\n            if ctrl_data['infeasible']:\n                inf_ind = ctrl_data['infeasible_index']\n                if error:\n                    ref = ctrl_data['z_ref'][:inf_ind, plt_id]\n                    z_error = ctrl_data['z'][:inf_ind, plt_id] - ref\n                    ax.plot(ctrl_data['t'][:inf_ind,:], z_error, **common_plot_args)\n                    ax.plot(ctrl_data['t'][inf_ind-1,:], z_error[-1], 'rX', alpha=alpha)\n                else:\n                    ax.plot(ctrl_data['t'][:inf_ind,:], ctrl_data['z'][:inf_ind, plt_id], **common_plot_args)\n                    ax.plot(ctrl_data['t'][inf_ind-1,:], ctrl_data['z'][inf_ind-1, plt_id], 'rX', alpha=alpha)\n            else:\n                if error:\n                    z_error = ctrl_data['z_ref'][:, plt_id] - ctrl_data['z'][:, plt_id]\n                    ax.plot(ctrl_data['t'], z_error, **common_plot_args)\n                else:\n                    ax.plot(ctrl_data['t'], ctrl_data['z'][:, plt_id], **common_plot_args)\n    if not(error):\n        ax.plot(ctrl_data['t'], ctrl_data['z_ref'][:, plt_id], '--', label='Reference', color=colors['ref'], zorder=1, alpha=alpha)\n    y_label = f'$z_{plt_id}\\; (' + units[plt_id] + ')$'\n    if error:\n        ax.set_ylabel('Position Error (m)')\n    else:\n        ax.set_ylabel('Horizontal Position (m)')\n    ax.set_xlabel('Time (s)')\n    ax.tick_params(labelsize=10)\n    plt.legend()\n    plt.tight_layout()\n    plt_name = os.path.join(track_dir, save_name )\n    plt.savefig(plt_name)\n    if paper_dir is not None:\n        plt_name = os.path.join(paper_dir, save_name )\n        plt.savefig(plt_name)\n    plt.show()", "\n\ndef make_input_plot(track_dir, colors, save_name, labels=None, con=False, paper_dir=None, ctrls=None, fig_size=None):\n    data = get_data_dict(track_dir)\n    coeff = 180.0/np.pi\n    if fig_size is None:\n        fig_size = (5,3)\n\n    # Plot the states along the trajectory and compare with reference.\n    fig, ax = plt.subplots(figsize=fig_size)\n    plt_id = 0\n    alpha = 0.7\n    if con:\n        ax.axhline(y=con, color='k', linestyle='solid', label='Constraint')\n        ax.axhline(y=-con, color='k', linestyle='solid')\n    for ctrl_name, ctrl_data in data.items():\n        if ctrls is None or ctrl_name in ctrls:\n            if labels is None:\n                label = ctrl_name\n            else:\n                label = labels[ctrl_name]\n            common_plot_args = { 'label': label, 'color': colors[ctrl_name], 'alpha': alpha}\n            if ctrl_data['infeasible']:\n                #raise('Not supported')\n                inf_ind = ctrl_data['infeasible_index']\n                ax.plot(ctrl_data['t'][:inf_ind,:], coeff*ctrl_data['u'][:inf_ind, plt_id], **common_plot_args)\n                ax.plot(ctrl_data['t'][inf_ind-1,:], coeff*ctrl_data['u'][inf_ind-1, plt_id], 'rX', alpha=alpha)\n            else:\n                ax.plot(ctrl_data['t'][:-1,:], coeff*ctrl_data['u'][:, plt_id], **common_plot_args)\n    ax.set_ylabel('Input (deg)')\n    ax.set_xlabel('Time (s)')\n    ax.tick_params(labelsize=10)\n    plt.legend()\n    plt.tight_layout()\n    plt_name = os.path.join(track_dir, save_name )\n    plt.savefig(plt_name)\n    if paper_dir is not None:\n        plt_name = os.path.join(paper_dir, save_name )\n        plt.savefig(plt_name)\n    plt.show()", "\nif __name__ == \"__main__\":\n\n    colors = {'MPC': 'purple',\n              'FMPC': 'lightsalmon',\n              'DLQR':  'skyblue',\n              'FMPC+SOCP': 'orangered',\n              'DLQR+SOCP': 'royalblue',\n              'GPMPC': 'darkgreen',\n              'ref': 'dimgray'\n              }\n    labels = {'MPC': 'MPC',\n              'FMPC': 'FMPC',\n              'DLQR':  'DLQR',\n              'FMPC+SOCP': 'FMPC+SOCP (ours)',\n              'DLQR+SOCP': 'DLQR+SOCP',\n              'GPMPC': 'GPMPC',\n              'ref': 'Reference'\n              }\n    paper_fig_dir = '/home/ahall/Documents/UofT/papers/input_and_stat_constrained_SOCP_filter_overleaf/figs'\n    tracking_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-10-01-49-38_fe35b85'\n    tracking_name = 'tracking.pdf'\n    ctrls = ['MPC', 'DLQR', 'GPMPC', 'FMPC+SOCP']\n    make_tracking_plot(tracking_dir, colors, tracking_name, labels=labels, paper_dir=paper_fig_dir, ctrls=ctrls, error=True)\n    step_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-09-23-47-14_fe35b85'\n    step_name = 'step.pdf'\n    make_tracking_plot(step_dir, colors, step_name, labels=labels, paper_dir=paper_fig_dir)\n    con_step_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-11-13-24-20_7c1ada4'\n    con_step_name = 'constrained_step.pdf'\n    make_tracking_plot(con_step_dir, colors, con_step_name, labels=labels, con=0.51, paper_dir=paper_fig_dir)\n    input_con_step_dir = '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-17-01-52-47_7c1ada4'\n    input_con_step_name = 'input_constrained_step.pdf'\n    colors['DLQR'] = 'brown'\n    labels['DLQR'] = 'DLQR Known'\n    make_tracking_plot(input_con_step_dir, colors, input_con_step_name, labels=labels, con=None, paper_dir=paper_fig_dir)\n    chat_dir = '/experiments/results/response_1-8/saved/seed42_Apr-28-11-25-06_c12bb4c'\n    chat_name = 'chatter.pdf'\n    ctrls = ['DLQR+SOCP', 'FMPC+SOCP']\n    make_input_plot(chat_dir, colors, chat_name, labels=labels, con=30, paper_dir=paper_fig_dir)", ""]}
{"filename": "experiments/__init__.py", "chunked_list": [""]}
{"filename": "experiments/step_comparison.py", "chunked_list": ["import os\nimport munch\nimport numpy as np\nimport torch\nimport gpytorch\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment", "from utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom controllers.dlqr import DLQR\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n\nfrom learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess", "from learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'tracking_comp',\n           'dt': 0.02,\n           'T': 10.0,\n           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,", "           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,\n           #'state_bound': {'h': [1, 0, 0], 'b': 0.25, 'phi_p': 3.0}\n            }\nquad_config = {'thrust': 10,\n               'tau': 0.2,\n               'gamma': 3.0,\n               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,\n                     'tau': 0.05,", "quad_prior_config = {'thrust': 20,\n                     'tau': 0.05,\n                     'gamma': 0.0,\n                     'dt': config['dt']}\ngp_v_from_u_config = {'amp': 0.2,\n                      'omegalist': [0.3, 0.5, 0.7, 0.9],\n                      'sig': 0.0001,\n                      'N': 1000,\n                      'n_train': 500,\n                      'lr': 0.1,", "                      'n_train': 500,\n                      'lr': 0.1,\n                      #'output_dir': None\n                      'output_dir': '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-09-20-47-05_fe35b85/gp_v_from_u'\n                      }\nfmpc_config = {'horizon': 100, #int(1/config['dt']),\n              'q_mpc': [10.0, 0.1, 0.1],\n              'r_mpc': [0.1],\n              'solver': 'ipopt'\n}", "              'solver': 'ipopt'\n}\nsocp_config = {'d_weight': 0,\n               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],\n               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {\n               'N': int(config['T_test']/config['dt']),\n               'n': 3,\n               'm': 1,", "               'n': 3,\n               'm': 1,\n               'dt': config['dt'],\n               'Amp': 0.5,\n               'omega': 5.0,\n               'ref_type': 'step'\n}\nconfig['quad'] = quad_config\nconfig['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config", "config['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config\nconfig['fmpc'] = fmpc_config\nconfig['socp'] = socp_config\nconfig['dlqr'] = dlrq_config\nconfig['test_params'] = test_params\nconfig = munch.munchify(config)\nset_dir_from_config(config)\nquad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)", "quad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)\n\n\nref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)\n\n\nfmpc = FMPC(quad=quad_prior,\n            dt=config.dt,", "fmpc = FMPC(quad=quad_prior,\n            dt=config.dt,\n            **config.fmpc,\n            reference_generator=ref_gen)\nfmpc.reset()\n\n\ngp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)", "likelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)\ngp_inv.init_with_hyperparam(config.gp_v_from_u.output_dir)\nfmpc_socp = DiscreteSOCPFilter('FMPC+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(fmpc),", "                               state_bound=None,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# Controller Parameters\n\ndlqr = DLQR(quad=quad_prior,\n            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr,", "            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr,\n            reference_generator=ref_gen)\ndlqr_socp = DiscreteSOCPFilter('DLQR+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=None,\n                               ctrl=deepcopy(dlqr),", "                               state_bound=None,\n                               ctrl=deepcopy(dlqr),\n                               gp=gp_inv)\n\nexp = Experiment('mpc', quad, [dlqr_socp, fmpc_socp], ref_gen, test_params, config)\nexp.run_experiment()\nexp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')\nexp.summarize_timings()\nexp.plot_rmse()", "exp.summarize_timings()\nexp.plot_rmse()\n"]}
{"filename": "experiments/constrained_step_comparison.py", "chunked_list": ["import os\nimport munch\nimport numpy as np\nimport torch\nimport gpytorch\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment", "from utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom controllers.dlqr import DLQR\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n\nfrom learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess", "from learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'tracking_comp',\n           'dt': 0.02,\n           'T': 10.0,\n           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,", "           'T_test': 10.0,\n           'input_bound': 45.0/180.0*np.pi,\n           'state_bound': {'h': np.array([[1.0, 0, 0]]).T, 'b': 0.51, 'phi_p': 5.0}\n            }\nquad_config = {'thrust': 10,\n               'tau': 0.2,\n               'gamma': 3.0,\n               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,\n                     'tau': 0.05,", "quad_prior_config = {'thrust': 20,\n                     'tau': 0.05,\n                     'gamma': 0.0,\n                     'dt': config['dt']}\ngp_v_from_u_config = {'amp': 0.2,\n                      'omegalist': [0.3, 0.5, 0.7, 0.9],\n                      'sig': 0.0001,\n                      'N': 1000,\n                      'n_train': 500,\n                      'lr': 0.1,", "                      'n_train': 500,\n                      'lr': 0.1,\n                      #'output_dir': None\n                      'output_dir': '/home/ahall/Documents/UofT/code/dsl__projects__flatness_safety_filter/testing/results/tracking_comp/saved/seed42_Mar-09-20-47-05_fe35b85/gp_v_from_u'\n                      }\nfmpc_config = {'horizon': 100, #int(1/config['dt']),\n              'q_mpc': [10.0, 0.1, 0.1],\n              'r_mpc': [0.1],\n              'solver': 'ipopt',\n               'lower_bounds': {'z0': -10.0},", "              'solver': 'ipopt',\n               'lower_bounds': {'z0': -10.0},\n               'upper_bounds': {'z0': config['state_bound']['b']}\n}\nsocp_config = {'d_weight': 0,\n               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],\n               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {\n               'N': int(config['T_test']/config['dt']),", "test_params = {\n               'N': int(config['T_test']/config['dt']),\n               'n': 3,\n               'm': 1,\n               'dt': config['dt'],\n               'Amp': 0.5,\n               'omega': 0.9,\n               'ref_type': 'step'\n}\nconfig['quad'] = quad_config", "}\nconfig['quad'] = quad_config\nconfig['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config\nconfig['fmpc'] = fmpc_config\nconfig['socp'] = socp_config\nconfig['dlqr'] = dlrq_config\nconfig['test_params'] = test_params\nconfig = munch.munchify(config)\nset_dir_from_config(config)", "config = munch.munchify(config)\nset_dir_from_config(config)\nquad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)\n\n\nref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)\n\n", "\n\nfmpc = FMPC(quad=quad_prior,\n            dt=config.dt,\n            **config.fmpc,\n            reference_generator=ref_gen)\nfmpc.reset()\n\n\ngp_type = ZeroMeanAffineGP", "\ngp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)\ngp_inv.init_with_hyperparam(config.gp_v_from_u.output_dir)\nfmpc_socp = DiscreteSOCPFilter('FMPC+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,", "                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=config.state_bound,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# Controller Parameters\n\ndlqr = DLQR(quad=quad_prior,\n            dt=config.dt,", "dlqr = DLQR(quad=quad_prior,\n            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr,\n            reference_generator=ref_gen)\ndlqr_socp = DiscreteSOCPFilter('DLQR+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,", "                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=config.state_bound,\n                               ctrl=deepcopy(dlqr),\n                               gp=gp_inv)\n\nexp = Experiment('mpc', quad, [dlqr_socp, fmpc_socp], ref_gen, test_params, config)\nexp.run_experiment()\nexp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')", "exp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')\nexp.summarize_timings()\nexp.plot_rmse()\n"]}
{"filename": "experiments/constrained_input_step_comparison.py", "chunked_list": ["import os\nimport munch\nimport numpy as np\nimport torch\nimport gpytorch\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment", "from utils.dir_utils import set_dir_from_config\nfrom experiments.experiments import train_gp_v_from_u, train_gpmpc_LHS, Experiment\nfrom quad_1D.quad_1d import Quad1D\nfrom controllers.mpc import MPC\nfrom controllers.fmpc import FMPC\nfrom controllers.dlqr import DLQR\nfrom controllers.discrete_socp_filter import DiscreteSOCPFilter\n\nfrom learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess", "from learning.gpmpc_gp_utils import  ZeroMeanIndependentGPModel, GaussianProcessCollection, DataHandler, combine_prior_and_gp\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\n\nconfig = { 'seed': 42,\n           'output_dir': './results/',\n           'tag': 'tracking_comp',\n           'dt': 0.02,\n           'T': 10.0,\n           'T_test': 10.0,\n           'input_bound': 10.0/180.0*np.pi,", "           'T_test': 10.0,\n           'input_bound': 10.0/180.0*np.pi,\n           'state_bound': {'h': np.array([[1.0, 0, 0]]).T, 'b': 0.51, 'phi_p': 5.0}\n           #'state_bound': None\n            }\nquad_config = {'thrust': 10,\n               'tau': 0.2,\n               'gamma': 3.0,\n               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,", "               'dt': config['dt']}\nquad_prior_config = {'thrust': 20,\n                     'tau': 0.05,\n                     'gamma': 0.0,\n                     'dt': config['dt']}\ngp_v_from_u_config = {'amp': 0.2,\n                      'omegalist': [0.3, 0.5, 0.7, 0.9],\n                      'sig': 0.0001,\n                      'N': 1000,\n                      'n_train': 500,", "                      'N': 1000,\n                      'n_train': 500,\n                      'lr': 0.1,\n                      #'output_dir': None\n                      'output_dir': '../models/gp_v_from_u'\n                      }\nfmpc_config = {'horizon': 100, #int(1/config['dt']),\n              'q_mpc': [50.0, 0.1, 0.1],\n              'r_mpc': [0.1],\n              'solver': 'ipopt',", "              'r_mpc': [0.1],\n              'solver': 'ipopt',\n              'lower_bounds': {'z0': -10.0},\n              'upper_bounds': {'z0': config['state_bound']['b']}\n}\nsocp_config = {'d_weight': 0,\n               'beta': 2.0}\ndlrq_config = {'q_lqr': fmpc_config['q_mpc'],\n               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {", "               'r_lqr': fmpc_config['r_mpc']}\ntest_params = {\n               'N': int(config['T_test']/config['dt']),\n               'n': 3,\n               'm': 1,\n               'dt': config['dt'],\n               'Amp': 0.5,\n               'omega': 0.9,\n               'ref_type': 'step'\n}", "               'ref_type': 'step'\n}\nconfig['quad'] = quad_config\nconfig['quad_prior'] = quad_prior_config\nconfig['gp_v_from_u'] = gp_v_from_u_config\nconfig['fmpc'] = fmpc_config\nconfig['socp'] = socp_config\nconfig['dlqr'] = dlrq_config\nconfig['test_params'] = test_params\nconfig = munch.munchify(config)", "config['test_params'] = test_params\nconfig = munch.munchify(config)\nset_dir_from_config(config)\nquad = Quad1D(**quad_config)\nquad_prior = Quad1D(**quad_prior_config)\n\n\nref_type = config.test_params.ref_type\nref_gen = partial(quad.reference_generator, ref_type=ref_type)\n", "ref_gen = partial(quad.reference_generator, ref_type=ref_type)\n\n\nfmpc = FMPC(quad=quad_prior,\n            dt=config.dt,\n            **config.fmpc,\n            reference_generator=ref_gen)\nfmpc.reset()\n\n", "\n\ngp_type = ZeroMeanAffineGP\nlikelihood_inv = gpytorch.likelihoods.GaussianLikelihood()\ngp_inv = GaussianProcess(gp_type, likelihood_inv, 1, config.gp_v_from_u.output_dir)\ngp_inv.init_with_hyperparam(config.gp_v_from_u.output_dir)\nfmpc_socp = DiscreteSOCPFilter('FMPC+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,", "                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=config.state_bound,\n                               ctrl=deepcopy(fmpc),\n                               gp=gp_inv)\n\n# Controller Parameters\n\ndlqr = DLQR(quad=quad,", "\ndlqr = DLQR(quad=quad,\n            dt=config.dt,\n            q_lqr=config.dlqr.q_lqr,\n            r_lqr=config.dlqr.r_lqr,\n            reference_generator=ref_gen)\ndlqr_socp = DiscreteSOCPFilter('DLQR+SOCP',\n                               quad_prior,\n                               config.socp.beta,\n                               d_weight=config.socp.d_weight,", "                               config.socp.beta,\n                               d_weight=config.socp.d_weight,\n                               input_bound=config.input_bound,\n                               state_bound=config.state_bound,\n                               ctrl=deepcopy(dlqr),\n                               gp=gp_inv)\n\n#exp = Experiment('mpc', quad, [dlqr_socp, fmpc_socp], ref_gen, test_params, config)\nexp = Experiment('mpc', quad, [dlqr, fmpc_socp], ref_gen, test_params, config)\nexp.run_experiment()", "exp = Experiment('mpc', quad, [dlqr, fmpc_socp], ref_gen, test_params, config)\nexp.run_experiment()\nexp.plot_tracking()\nexp.plot_tracking(plot_dims=[0], name='position')\nexp.summarize_timings()\nexp.plot_rmse()\n"]}
{"filename": "experiments/experiments.py", "chunked_list": ["import os.path\nimport munch\nimport torch\nimport gpytorch\nimport sys\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split", "import seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom copy import deepcopy\n\nfrom learning.gp_utils import ZeroMeanAffineGP, GaussianProcess\nfrom learning.gpmpc_gp_utils import DataHandler, ZeroMeanIndependentGPModel, GaussianProcessCollection, get_LHS_samples, generate_samples_into_sequences\nfrom utils.plotting_utils import scatter3d, plot_trained_gp\nfrom quad_1D.expr_utils import feedback_loop\nfrom utils.dir_utils import set_dir_from_config,mkdirs\n\nclass Experiment:\n    def __init__(self, name, test_quad, ctrls, reference_generator, test_params, config):\n            self.name = name\n            self.test_quad = test_quad\n            self.reference_generator = reference_generator\n            self.test_params = test_params\n            self.ctrls = ctrls\n            config['params'] = test_params\n            config['name'] = name\n\n            self.config = config\n            self.results_dict = None\n            self.reset()\n\n    def run_experiment(self, plot_run=False, fig_count=0):\n        for ctrl in self.ctrls:\n            data, fig_count = feedback_loop(\n                self.test_params, # paramters\n                None, # GP model\n                self.test_quad.true_flat_dynamics, # flat dynamics to step with\n                self.reference_generator, # reference\n                ctrl, # FB ctrl\n                secondary_controllers=None, # No comparison\n                online_learning=False,\n                fig_count=fig_count,\n                plot=plot_run,\n                input_bound=self.config.input_bound\n            )\n            self.results_dict[ctrl.name] = data\n        save_name = os.path.join(self.config.output_dir, 'data.npz')\n        np.savez(save_name, **self.results_dict)\n        self.results_dict = munch.munchify(self.results_dict)\n\n    def plot_tracking(self, plot_dims=[0,1,2], fig_count=0, con=False, name=None, size=(10,10), yrange=None):\n        # Plot the states along the trajectory and compare with reference.\n        fig_count += 1\n        n_plots = len(plot_dims)\n        units = {0: 'm', 1: 'm/s', 2: 'm/s^2'}\n        fig, ax = plt.subplots(n_plots, figsize=size)\n        if n_plots == 1:\n            ax = [ax]\n        for plt_id in plot_dims:\n            if con:\n                ax[plt_id].axhline(y=con, color='r', linestyle='solid', label='Constraint')\n            for ctrl_name, ctrl_data in self.results_dict.items():\n                if ctrl_data['infeasible']:\n                    inf_ind = ctrl_data['infeasible_index']\n                    ax[plt_id].plot(ctrl_data.t[:inf_ind,:], ctrl_data.z[:inf_ind, plt_id], label=ctrl_name)\n                    ax[plt_id].plot(ctrl_data.t[inf_ind-1,:], ctrl_data.z[inf_ind-1, plt_id], 'rX')\n                else:\n                    ax[plt_id].plot(ctrl_data.t, ctrl_data.z[:, plt_id], label=ctrl_name)\n            ax[plt_id].plot(ctrl_data.t, ctrl_data.z_ref[:, plt_id], '--k', label='ref')\n            y_label = f'$z_{plt_id}\\; (' + units[plt_id] + ')$'\n            ax[plt_id].set_ylabel(y_label)\n            if yrange:\n                ax[plt_id].set_ylim(yrange[plt_id])\n        ax[-1].set_xlabel('Time (s)')\n        plt.legend()\n        plt.tight_layout()\n        if name is None:\n            plt_name = os.path.join(self.config.output_dir, 'tracking_plot.eps')\n        else:\n            plt_name = os.path.join(self.config.output_dir, name+'.eps')\n        plt.savefig(plt_name)\n        plt.show()\n        return fig_count\n\n\n    def plot_inputs(self, fig_count=0, name=None, units='rad'):\n        if units == 'rad':\n            coeff = 1.0\n        elif units == 'deg':\n            coeff = 180.0/np.pi\n\n        # Plot the states along the trajectory and compare with reference.\n        fig_count += 1\n        fig, ax = plt.subplots(figsize=(10,10))\n        if self.config.input_bound is not None:\n            ax.axhline(y=coeff*self.config.input_bound, color='r', linestyle='solid', label='Constraint')\n            ax.axhline(y=-coeff*self.config.input_bound, color='r', linestyle='solid')\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            if ctrl_data['infeasible']:\n                inf_ind = ctrl_data['infeasible_index']\n                ax.plot(ctrl_data.t[:inf_ind], coeff*ctrl_data.u[:inf_ind], label=ctrl_name)\n                ax.plot(ctrl_data.t[inf_ind-1], coeff*ctrl_data.u[inf_ind-1], 'rX')\n            else:\n                ax.plot(ctrl_data.t[:-1], coeff*ctrl_data.u[:], label=ctrl_name)\n        if units == 'rad':\n            y_label = f'$u$ (rad)'\n        elif units == 'deg':\n            y_label = f'$u$ (deg)'\n        ax.set_ylabel(y_label)\n        ax.set_xlabel('Time (s)')\n        plt.legend()\n        plt.tight_layout()\n        if name is None:\n            plt_name = os.path.join(self.config.output_dir, 'input_plot.eps')\n        else:\n            plt_name = os.path.join(self.config.output_dir, name+'.eps')\n        plt.savefig(plt_name)\n        plt.show()\n        return fig_count\n\n    def summarize_timings(self):\n        headers = [['Algo', 'RMSE_all','RMSE x only','mean (s)', 'std (s)']]\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            rmse = calc_rmse(ctrl_data['z'] - ctrl_data['z_ref'])\n            rmse_x = calc_rmse_x(ctrl_data['z'] - ctrl_data['z_ref'])\n            mean_t = np.mean(ctrl_data['solve_time'][1:])\n            std_t = np.std(ctrl_data['solve_time'][1:])\n            line = [ctrl_name, rmse, rmse_x, mean_t, std_t]\n            headers.append(line)\n        fname = os.path.join(self.config.output_dir,'solve_times.csv')\n        with open(fname, 'w') as fopen:\n            writer = csv.writer(fopen, delimiter=',')\n            writer.writerows(headers)\n\n    def plot_rmse(self):\n        x_pos = list(range(len(self.ctrls)))\n        rmses = []\n        names = []\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            rmse = calc_rmse(ctrl_data['z'] - ctrl_data['z_ref'])\n            rmses.append(rmse)\n            names.append(ctrl_name)\n        fig, ax = plt.subplots()\n        ax.bar(x_pos, rmses, width=1.0, tick_label=names)\n        ax.set_ylabel('RMSE')\n        fname = os.path.join(self.config.output_dir, 'rmse.eps')\n        plt.savefig(fname)\n        plt.show()\n\n\n    def reset(self):\n        self.results_dict = {}", "from utils.dir_utils import set_dir_from_config,mkdirs\n\nclass Experiment:\n    def __init__(self, name, test_quad, ctrls, reference_generator, test_params, config):\n            self.name = name\n            self.test_quad = test_quad\n            self.reference_generator = reference_generator\n            self.test_params = test_params\n            self.ctrls = ctrls\n            config['params'] = test_params\n            config['name'] = name\n\n            self.config = config\n            self.results_dict = None\n            self.reset()\n\n    def run_experiment(self, plot_run=False, fig_count=0):\n        for ctrl in self.ctrls:\n            data, fig_count = feedback_loop(\n                self.test_params, # paramters\n                None, # GP model\n                self.test_quad.true_flat_dynamics, # flat dynamics to step with\n                self.reference_generator, # reference\n                ctrl, # FB ctrl\n                secondary_controllers=None, # No comparison\n                online_learning=False,\n                fig_count=fig_count,\n                plot=plot_run,\n                input_bound=self.config.input_bound\n            )\n            self.results_dict[ctrl.name] = data\n        save_name = os.path.join(self.config.output_dir, 'data.npz')\n        np.savez(save_name, **self.results_dict)\n        self.results_dict = munch.munchify(self.results_dict)\n\n    def plot_tracking(self, plot_dims=[0,1,2], fig_count=0, con=False, name=None, size=(10,10), yrange=None):\n        # Plot the states along the trajectory and compare with reference.\n        fig_count += 1\n        n_plots = len(plot_dims)\n        units = {0: 'm', 1: 'm/s', 2: 'm/s^2'}\n        fig, ax = plt.subplots(n_plots, figsize=size)\n        if n_plots == 1:\n            ax = [ax]\n        for plt_id in plot_dims:\n            if con:\n                ax[plt_id].axhline(y=con, color='r', linestyle='solid', label='Constraint')\n            for ctrl_name, ctrl_data in self.results_dict.items():\n                if ctrl_data['infeasible']:\n                    inf_ind = ctrl_data['infeasible_index']\n                    ax[plt_id].plot(ctrl_data.t[:inf_ind,:], ctrl_data.z[:inf_ind, plt_id], label=ctrl_name)\n                    ax[plt_id].plot(ctrl_data.t[inf_ind-1,:], ctrl_data.z[inf_ind-1, plt_id], 'rX')\n                else:\n                    ax[plt_id].plot(ctrl_data.t, ctrl_data.z[:, plt_id], label=ctrl_name)\n            ax[plt_id].plot(ctrl_data.t, ctrl_data.z_ref[:, plt_id], '--k', label='ref')\n            y_label = f'$z_{plt_id}\\; (' + units[plt_id] + ')$'\n            ax[plt_id].set_ylabel(y_label)\n            if yrange:\n                ax[plt_id].set_ylim(yrange[plt_id])\n        ax[-1].set_xlabel('Time (s)')\n        plt.legend()\n        plt.tight_layout()\n        if name is None:\n            plt_name = os.path.join(self.config.output_dir, 'tracking_plot.eps')\n        else:\n            plt_name = os.path.join(self.config.output_dir, name+'.eps')\n        plt.savefig(plt_name)\n        plt.show()\n        return fig_count\n\n\n    def plot_inputs(self, fig_count=0, name=None, units='rad'):\n        if units == 'rad':\n            coeff = 1.0\n        elif units == 'deg':\n            coeff = 180.0/np.pi\n\n        # Plot the states along the trajectory and compare with reference.\n        fig_count += 1\n        fig, ax = plt.subplots(figsize=(10,10))\n        if self.config.input_bound is not None:\n            ax.axhline(y=coeff*self.config.input_bound, color='r', linestyle='solid', label='Constraint')\n            ax.axhline(y=-coeff*self.config.input_bound, color='r', linestyle='solid')\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            if ctrl_data['infeasible']:\n                inf_ind = ctrl_data['infeasible_index']\n                ax.plot(ctrl_data.t[:inf_ind], coeff*ctrl_data.u[:inf_ind], label=ctrl_name)\n                ax.plot(ctrl_data.t[inf_ind-1], coeff*ctrl_data.u[inf_ind-1], 'rX')\n            else:\n                ax.plot(ctrl_data.t[:-1], coeff*ctrl_data.u[:], label=ctrl_name)\n        if units == 'rad':\n            y_label = f'$u$ (rad)'\n        elif units == 'deg':\n            y_label = f'$u$ (deg)'\n        ax.set_ylabel(y_label)\n        ax.set_xlabel('Time (s)')\n        plt.legend()\n        plt.tight_layout()\n        if name is None:\n            plt_name = os.path.join(self.config.output_dir, 'input_plot.eps')\n        else:\n            plt_name = os.path.join(self.config.output_dir, name+'.eps')\n        plt.savefig(plt_name)\n        plt.show()\n        return fig_count\n\n    def summarize_timings(self):\n        headers = [['Algo', 'RMSE_all','RMSE x only','mean (s)', 'std (s)']]\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            rmse = calc_rmse(ctrl_data['z'] - ctrl_data['z_ref'])\n            rmse_x = calc_rmse_x(ctrl_data['z'] - ctrl_data['z_ref'])\n            mean_t = np.mean(ctrl_data['solve_time'][1:])\n            std_t = np.std(ctrl_data['solve_time'][1:])\n            line = [ctrl_name, rmse, rmse_x, mean_t, std_t]\n            headers.append(line)\n        fname = os.path.join(self.config.output_dir,'solve_times.csv')\n        with open(fname, 'w') as fopen:\n            writer = csv.writer(fopen, delimiter=',')\n            writer.writerows(headers)\n\n    def plot_rmse(self):\n        x_pos = list(range(len(self.ctrls)))\n        rmses = []\n        names = []\n        for ctrl_name, ctrl_data in self.results_dict.items():\n            rmse = calc_rmse(ctrl_data['z'] - ctrl_data['z_ref'])\n            rmses.append(rmse)\n            names.append(ctrl_name)\n        fig, ax = plt.subplots()\n        ax.bar(x_pos, rmses, width=1.0, tick_label=names)\n        ax.set_ylabel('RMSE')\n        fname = os.path.join(self.config.output_dir, 'rmse.eps')\n        plt.savefig(fname)\n        plt.show()\n\n\n    def reset(self):\n        self.results_dict = {}", "\ndef calc_rmse(e):\n    return np.sqrt(np.mean(e**2))\n\ndef calc_rmse_x(e):\n    return calc_rmse(e[:,0])\n\ndef train_gp_v_from_u(config, quad_prior, quad):\n    # Gather training parameters\n    config.gp_v_from_u.output_dir = os.path.join(config.output_dir,'gp_v_from_u')\n    mkdirs(config.gp_v_from_u.output_dir)\n    seed = config.seed\n    np_rnd = np.random.default_rng(seed=seed)\n    dt = config.dt\n    T = config.T\n    Amp = config.gp_v_from_u.amp\n    omegalist = config.gp_v_from_u.omegalist\n    sig = config.gp_v_from_u.sig\n    # Sampling data points\n    N = config.gp_v_from_u.N\n    n_train = config.gp_v_from_u.n_train\n    lr = config.gp_v_from_u.lr\n\n    # Collect Training Data\n    inputs = []\n    targets = []\n    for omega in omegalist:\n        t = np.arange(0,T, dt)\n        z_ref, v_real = quad.reference_generator(t, Amp, omega)\n        u_ref = quad.cs_u_from_v(z=z_ref, v=v_real)['u'].toarray()\n        v_hat = quad_prior.cs_v_from_u(z=z_ref, u=u_ref)['v'].toarray()\n        u_ref_prior = quad_prior.cs_u_from_v(z=z_ref, v=v_hat)['u'].toarray()\n\n        noise = np_rnd.normal(0, sig, size=v_hat.shape)\n        v_hat_noisy = v_real + noise\n        inputs.append(torch.from_numpy(np.vstack((z_ref, u_ref))).double().T)\n        #targets.append(torch.from_numpy(v_real).double().T)\n        targets.append(torch.from_numpy(v_hat_noisy).double().T)\n    inputs = torch.vstack(inputs)\n    targets = torch.vstack(targets)\n\n\n    interval = int(np.ceil(inputs.shape[0]/N))\n    inputs = inputs[::interval, :]\n    targets = targets[::interval, :]\n\n    train_in, test_in, train_tar, test_tar  = train_test_split(inputs, targets, test_size=0.2, random_state=seed)\n\n    # Setup GP\n    gp_type = ZeroMeanAffineGP\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    gp_inv = GaussianProcess(gp_type, likelihood, 1, config.gp_v_from_u.output_dir)\n\n    fname = os.path.join(config.gp_v_from_u.output_dir, 'training_output.txt')\n    orig_stdout = sys.stdout\n    with open(fname,'w', 1) as print_to_file:\n        sys.stdout = print_to_file\n        gp_inv.train(train_in, train_tar.squeeze(), n_train=n_train, learning_rate=lr)\n    sys.stdout = orig_stdout\n\n    means, covs, preds = gp_inv.predict(test_in)\n    errors = means - test_tar.squeeze()\n    abs_errors = torch.abs(errors)\n    rel_errors = abs_errors/torch.abs(test_tar.squeeze())\n\n    #scatter3d(test_in[:,0], test_in[:,1], test_in[:,2], errors)\n\n    # Show Quality on unseen trajectory\n    Amp = 0.2\n    omega = 0.6\n    t = np.arange(0,10, dt)\n    z_test, v_test_real = quad.reference_generator(t, Amp, omega)\n    u_test = quad.cs_u_from_v(z=z_test, v=v_test_real)['u'].toarray()\n    ref_gp_ins = torch.from_numpy(np.vstack((z_test, u_test))).T\n    delv_pred, u_cov, preds = gp_inv.predict(ref_gp_ins)\n    v_test_prior = quad_prior.cs_v_from_u(z=z_test, u=u_test)['v'].toarray()\n    #v_pred = delv_pred.T + v_test_prior\n    v_pred = delv_pred.T\n\n    figcount = plot_trained_gp(v_test_real, v_pred, preds, fig_count=1, show=True)\n\n    likelihood2 = gpytorch.likelihoods.GaussianLikelihood()\n    gp2 = GaussianProcess(gp_type, likelihood2, 1, config.gp_v_from_u.output_dir)\n    gp2.init_with_hyperparam( config.gp_v_from_u.output_dir)\n\n    delv_pred2, u_cov2, preds2 = gp2.predict(ref_gp_ins)\n    v_pred2 = delv_pred2.T\n    plot_trained_gp(v_test_real, v_pred2, preds2, fig_count=figcount, show=True)\n    return gp2", "\ndef train_gpmpc_gp(config, quad, quad_prior, ctrl):\n\n    config.gpmpc.gp_output_dir = os.path.join(config.output_dir,'gpmpc_gp')\n    mkdirs(config.gpmpc.gp_output_dir)\n    seed = config.seed\n    fname = os.path.join(config.gpmpc.gp_output_dir, 'training_output.txt')\n\n    dt = config.dt\n    T = config.T\n    N = int(T/dt)\n\n    # GP params\n    noise = config.gpmpc.noise\n    num_samples = config.gpmpc.num_samples\n    n_train = config.gpmpc.n_train\n    lr = config.gpmpc.lr\n    input_mask = config.gpmpc.input_mask\n    target_mask = config.gpmpc.target_mask\n\n    # Training Traj params\n    Amp = config.gpmpc.amp\n    omega_list =  config.gpmpc.omegalist\n    ctrl.reset()\n\n    reference_generator = quad.reference_generator\n\n    # simulation parameters\n    x_data = []\n    u_data = []\n    params = {}\n    params['N'] = N\n    params['n'] = quad.n\n    params['m'] = quad.m\n    params['dt'] = dt\n    params['Amp'] = Amp\n    for omega in omega_list:\n        params['omega'] = omega\n        fmpc_data_i, fig_count = feedback_loop(\n            params, # paramters\n            None, # GP model\n            quad.true_flat_dynamics, # flat dynamics to step with\n            reference_generator, # reference\n            #prior_lqr_controller, # FB ctrl\n            ctrl, # FB ctrl\n            secondary_controllers=None, # No comparison\n            online_learning=False,\n            fig_count=0,\n            plot=False\n        )\n\n        x_data.append(quad.cs_x_from_z(z=fmpc_data_i['z'].T)['x'].toarray().T)\n        u_data.append(fmpc_data_i['u'])\n        ctrl.reset()\n    prior_model = deepcopy(quad_prior.cs_lin_dyn)\n    save_dir = config.gpmpc.gp_output_dir\n\n    dh = DataHandler(x_data=x_data,\n                     u_data=u_data,\n                     prior_model=prior_model,\n                     save_dir=save_dir,\n                     noise=noise,\n                     num_samples=num_samples)\n    dh.save(config.gpmpc.gp_output_dir)\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n        constraint=gpytorch.constraints.GreaterThan(1e-6),\n    ).double()\n    gp = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                   likelihood,\n                                   len(target_mask),\n                                   input_mask=input_mask,\n                                   target_mask=target_mask,\n                                   )\n\n    orig_stdout = sys.stdout\n    with open(fname,'w', 1) as print_to_file:\n        sys.stdout = print_to_file\n        gp.train(torch.from_numpy(dh.data.train_inputs),\n                 torch.from_numpy(dh.data.train_targets),\n                 torch.from_numpy(dh.data.test_inputs),\n                 torch.from_numpy(dh.data.test_targets),\n                 n_train=n_train,\n                 learning_rate=lr,\n                 gpu=True,\n                 dir=save_dir)\n    sys.stdout = orig_stdout", "\ndef train_gpmpc_LHS(config, quad, quad_prior, ctrl):\n    \"\"\" Using Latin Hypercube Sampling to get training data.\"\"\"\n    config.gpmpc.gp_output_dir = os.path.join(config.output_dir,'gpmpc_gp')\n    mkdirs(config.gpmpc.gp_output_dir)\n    seed = config.seed\n    fname = os.path.join(config.gpmpc.gp_output_dir, 'training_output.txt')\n\n    dt = config.dt\n    T = config.T\n    N = int(T/dt)\n\n    # GP params\n    noise = config.gpmpc.noise\n    num_samples = config.gpmpc.num_samples\n    n_train = config.gpmpc.n_train\n    lr = config.gpmpc.lr\n    input_mask = config.gpmpc.input_mask\n    target_mask = config.gpmpc.target_mask\n\n    # Training Traj params\n    Amp = config.gpmpc.amp\n    omega_list =  config.gpmpc.omegalist\n    ctrl.reset()\n\n    reference_generator = quad.reference_generator\n\n    prior_model = deepcopy(quad_prior.cs_lin_dyn)\n    save_dir = config.gpmpc.gp_output_dir\n\n    LHS_sampler_args = {'lower_bounds': config.lhs_samp.lower_bounds,\n                        'upper_bounds': config.lhs_samp.upper_bounds,\n                        'num_samples': num_samples,\n                        'seed': seed}\n\n    x_data, u_data = generate_samples_into_sequences(get_LHS_samples, LHS_sampler_args, quad)\n    dh = DataHandler(x_data=x_data,\n                     u_data=u_data,\n                     prior_model=prior_model,\n                     save_dir=save_dir,\n                     noise=noise,\n                     num_samples=num_samples)\n    dh.save(config.gpmpc.gp_output_dir)\n    likelihood = gpytorch.likelihoods.GaussianLikelihood(\n        constraint=gpytorch.constraints.GreaterThan(1e-6),\n    ).double()\n    gp = GaussianProcessCollection(ZeroMeanIndependentGPModel,\n                                   likelihood,\n                                   len(target_mask),\n                                   input_mask=input_mask,\n                                   target_mask=target_mask,\n                                   )\n\n    orig_stdout = sys.stdout\n    with open(fname,'w', 1) as print_to_file:\n        sys.stdout = print_to_file\n        gp.train(torch.from_numpy(dh.data.train_inputs),\n                 torch.from_numpy(dh.data.train_targets),\n                 torch.from_numpy(dh.data.test_inputs),\n                 torch.from_numpy(dh.data.test_targets),\n                 n_train=n_train,\n                 learning_rate=lr,\n                 gpu=True,\n                 dir=save_dir)\n    sys.stdout = orig_stdout", ""]}
{"filename": "quad_1D/quad_1d.py", "chunked_list": ["import numpy as np\nimport casadi as cs\nfrom control import lqr\nfrom scipy.linalg import expm\nfrom utils.math_utils import rk_discrete, euler_discrete, discretize_linear_system\n\nclass Quad1D:\n    def __init__(self, thrust, tau, gamma, dt, ref_type='increasing_sine'):\n        self.T = thrust\n        self.tau = tau\n        self.gamma = gamma\n        self.dt = dt\n        self.ref_type = ref_type\n\n        self.n = 3\n        self.m = 1\n        self.state_names = ['z0', 'z1', 'z2']\n        self.state_indices = {'z0': 0,\n                              'z1': 1,\n                              'z2': 2}\n\n        self.A = np.diag(np.ones(self.n-1), k=1)\n        self.B = np.zeros((self.n,self.m))\n        self.B[-1,0] = 1\n\n        M = np.zeros((self.m + self.n, self.m + self.n))\n        M[0:self.n, 0:self.n] = self.A * self.dt\n        M[0:self.n, self.n:self.m + self.n] = self.B * self.dt\n        expM = expm(M)\n        self.Ad = expM[0:self.n, 0:self.n]\n        self.Bd = expM[0:self.n, self.n:]\n\n        self.cs_alpha = self.make_cs_alpha()\n        self.cs_beta = self.make_cs_beta()\n        self.cs_v_from_u = self.make_cs_v_from_u()\n        self.cs_u_from_v = self.make_cs_u_from_v()\n        self.cs_true_flat_dynamics_from_v = self.make_cs_true_flat_dynamics_from_v()\n        self.cs_true_flat_dynamics = self.make_cs_true_flat_dynamics()\n        self.cs_nonlin_dyn, self.cs_nonlin_jac = self.make_cs_nonlinear_dyn()\n        self.cs_nonlin_dyn_discrete = self.make_cs_nonlinear_dyn_discrete()\n        self.cs_lin_dyn, self.Ar, self.Br = self.make_real_lin_dyn()\n        self.cs_x_from_z = self.make_x_from_z()\n        self.cs_z_from_x = self.make_z_from_x()\n        self.cs_true_flat_dyn_from_x_and_u = self.make_cs_true_flat_dyn_from_x_and_u()\n\n\n        self.Q = None\n        self.P = None\n        self.S = None\n        self.K_gain = None\n        self.c3 = None\n\n    def make_cs_nonlinear_dyn(self):\n        x = cs.SX.sym('x')\n        u = cs.SX.sym('u')\n        x_dot = cs.SX.sym('x_dot')\n        theta = cs.SX.sym('theta')\n\n        X = cs.vertcat(x,x_dot,theta)\n        X_dot = cs.vertcat( x_dot,\n                            self.T*cs.sin(theta) - self.gamma*x_dot,\n                            1/self.tau*(u - theta))\n        nonlin_dyn = cs.Function('nonlin_dyn',\n                                 [X,u],\n                                 [X_dot],\n                                 ['x', 'u'],\n                                 ['x_dot'])\n\n        dfdx = cs.jacobian(X_dot, X)\n        dfdu = cs.jacobian(X_dot, u)\n        df_func = cs.Function('df',\n                              [X, u],\n                              [dfdx, dfdu],\n                              ['x', 'u'],\n                              ['dfdx', 'dfdu'])\n\n        return nonlin_dyn, df_func\n\n    def make_real_lin_dyn(self):\n        x_eq = np.zeros((3,1))\n        u_eq = 0.0\n\n        dfdxdfdu = self.cs_nonlin_jac(x=x_eq, u=u_eq)\n        dfdx = dfdxdfdu['dfdx'].toarray()\n        dfdu = dfdxdfdu['dfdu'].toarray()\n\n\n        Ar, Br = discretize_linear_system(dfdx, dfdu, self.dt, exact=True)\n\n        x = cs.SX.sym('x', 3)\n        u = cs.SX.sym('u', 1)\n        x_dot = Ar @ x + Br @ u # Don't need Delta because eq is 0.\n        lin_dyn = cs.Function('lin_dyn',\n                              [x, u],\n                              [x_dot],\n                              ['x0', 'p'],\n                              ['xf'])\n        return lin_dyn, Ar, Br\n\n\n    def make_cs_nonlinear_dyn_discrete(self):\n        nonlin_dyn_discrete = rk_discrete(self.cs_nonlin_dyn, self.n, self.m, self.dt)\n        #nonlin_dyn_discrete = euler_discrete(self.cs_nonlin_dyn, self.n, self.m, self.dt)\n        return nonlin_dyn_discrete\n\n    def alpha(self, z):\n        alpha = self.T/self.tau*np.sqrt((1-((z[2]+self.gamma*z[1])/self.T)**2))\n        return alpha\n\n    def make_cs_alpha(self):\n        z = cs.SX.sym('z', self.n)\n        alpha = self.T/self.tau*cs.sqrt((1-((z[2]+self.gamma*z[1])/self.T)**2))\n        func = cs.Function('alpha',\n                           [z],\n                           [alpha],\n                           ['z'],\n                           ['alpha'])\n        return func\n\n    def beta(self, z):\n        beta = - self.T/self.tau*np.sqrt((1-((z[2]+self.gamma*z[1])/self.T)**2))*np.arcsin((z[2]+self.gamma*z[1])/self.T) \\\n               -self.gamma*(z[2]+self.gamma*z[1]) + self.gamma**2*z[2]\n        return beta\n\n    def make_cs_beta(self):\n        z = cs.SX.sym('z', self.n)\n        beta = - self.T/self.tau*cs.sqrt((1-((z[2]+self.gamma*z[1])/self.T)**2))*cs.arcsin((z[2]+self.gamma*z[1])/self.T) \\\n               -self.gamma*(z[2]+self.gamma*z[1]) + self.gamma**2*z[2]\n        func = cs.Function('beta',\n                           [z],\n                           [beta],\n                           ['z'],\n                           ['beta'])\n        return func\n\n    def v_from_u(self,u, z):\n        v = self.alpha(z)*u + self.beta(z)\n        return v\n\n    def make_cs_v_from_u(self):\n        z = cs.SX.sym('z', self.n)\n        u = cs.SX.sym('u', self.m)\n\n        v = self.cs_alpha(z=z)['alpha']@u + self.cs_beta(z=z)['beta']\n        func = cs.Function('v_from_u',\n                           [z, u],\n                           [v],\n                           ['z', 'u'],\n                           ['v'])\n        return func\n\n\n    def u_from_v(self, v, z):\n        u = (v-self.beta(z))/self.alpha(z)\n        return u\n\n    def make_cs_u_from_v(self):\n        z = cs.SX.sym('z', self.n)\n        v = cs.SX.sym('v', self.m)\n\n        u = (v-self.cs_beta(z=z)['beta'])/self.cs_alpha(z=z)['alpha']\n        func = cs.Function('u_from_v',\n                           [z, v],\n                           [u],\n                           ['z', 'v'],\n                           ['u'])\n        return func\n\n    def true_flat_dynamics_from_v(self, z, v):\n        z_next = self.Ad@z + self.Bd*v\n        return z_next\n\n    def make_cs_true_flat_dynamics_from_v(self):\n        z = cs.SX.sym('z', self.n)\n        v = cs.SX.sym('v', self.m)\n\n        z_next = self.Ad@z + self.Bd*v\n        func = cs.Function('z_next',\n                           [z, v],\n                           [z_next],\n                           ['z', 'v'],\n                           ['z_next'])\n        return func\n\n    def true_flat_dynamics(self, z, u):\n        v = self.v_from_u(u, z)\n        z_next = self.true_flat_dynamics_from_v(z, v)\n        return z_next, v\n\n    def make_cs_true_flat_dynamics(self):\n        z = cs.SX.sym('z', self.n)\n        u = cs.SX.sym('u', self.m)\n\n\n        v = self.cs_v_from_u(z=z, u=u)['v']\n        z_next = self.cs_true_flat_dynamics_from_v(z=z, v=v)['z_next']\n\n        func = cs.Function('z_next',\n                           [z, u],\n                           [z_next, v],\n                           ['z', 'u'],\n                           ['z_next', 'v'])\n        return func\n\n    def make_cs_true_flat_dyn_from_x_and_u(self):\n        x = cs.SX.sym('x', self.n)\n        u = cs.SX.sym('u', self.m)\n        z = self.cs_z_from_x(x=x)['z']\n        z_next = self.cs_true_flat_dynamics(z=z, u=u)['z_next']\n        x_next = self.cs_x_from_z(z=z_next)['x']\n\n        func = cs.Function('x_next',\n                           [x, u],\n                           [x_next],\n                           ['x', 'u'],\n                           ['x_next'])\n        return func\n\n\n    def make_x_from_z(self):\n        z = cs.SX.sym('z', self.n)\n\n        x = cs.vertcat(z[0],\n                       z[1],\n                       cs.arcsin( (z[2] + self.gamma*z[1])/self.T))\n        x_from_z = cs.Function('x_from_z',\n                               [z],\n                               [x],\n                               ['z'],\n                               ['x'])\n        return x_from_z\n\n    def make_z_from_x(self):\n        x = cs.SX.sym('x', self.n)\n\n        z = cs.vertcat(x[0],\n                       x[1],\n                       self.T*cs.sin(x[2]) - self.gamma*x[1])\n        z_from_x = cs.Function('z_from_x',\n                               [x],\n                               [z],\n                               ['x'],\n                               ['z'])\n        return z_from_x\n\n    def reference_generator(self, t, Amp, omega, ref_type=None):\n        \"\"\"\n        Increasing sign such that quad flies in increasing 2D spiral\n        Same oscillation in both x and z\n        \"\"\"\n        if ref_type is None:\n            ref_type = self.ref_type\n\n        # z_ref = np.zeros((8,1))\n        if type(t) == np.ndarray:\n            n = t.shape[0]\n            z_ref = np.zeros((self.n, n))\n            v_ref = np.zeros((self.m, n))\n        else:\n            z_ref = np.zeros((self.n, 1))\n            v_ref = np.zeros((self.m, 1))\n\n        if ref_type == 'increasing_sine':\n            z_ref[0] = Amp * t * np.sin(omega * t)\n            z_ref[1] = Amp * np.sin(omega * t) + Amp * t * omega * np.cos(omega * t)\n            z_ref[2] = 2 * omega * Amp * np.cos(omega * t) - Amp * t * omega ** 2 * np.sin(omega * t)\n            v_ref[0] = -3 * omega ** 2 * Amp * np.sin(omega * t) - Amp * t * omega ** 3 * np.cos(omega * t)\n        elif ref_type == 'increasing_freq':\n            omega = omega*np.exp(0.05*t)\n            z_ref[0] = Amp * np.sin(omega * t)\n            z_ref[1] = Amp * omega * np.cos(omega * t)\n            z_ref[2] =  - Amp * omega ** 2 * np.sin(omega * t)\n            v_ref[0] =  - Amp * omega ** 3 * np.cos(omega * t)\n        elif ref_type == 'step':\n            delta_t = 3.0\n            #z_ref[0] = 0.5 + 0.5*np.tanh((t-delta_t)*omega)\n            #z_ref[1] = 0.5*Amp*omega*(1-np.tanh(omega*(t-delta_t))**2)\n            #z_ref[2] = -Amp*omega**2*(1-np.tanh(omega*(t-delta_t))**2)*np.tanh(omega*(t-delta_t))\n            #v_ref[0] = -Amp*omega**3*(1-np.tanh(omega*(t-delta_t))**2)**2 + 2.0*Amp*omega**3*(1-np.tanh(omega*(t-delta_t))**2)*np.tanh(omega*(t-delta_t))\n            z_ref[0] = Amp*np.heaviside(t-delta_t,1.0)\n            z_ref[1] = 0.0\n            z_ref[2] = 0.0\n            v_ref[0] = 0.0\n\n        else:\n            raise ValueError('Reference type not implemented.')\n\n        return z_ref, v_ref\n\n    def real_reference_generator(self, t, Amp, omega):\n        z_ref, v_ref = self.reference_generator(t, Amp, omega)\n        x_ref = self.cs_x_from_z(z=z_ref)['x'].toarray()\n        u_ref = self.cs_u_from_v(z=z_ref, v=v_ref)['u'].toarray()\n        return x_ref, u_ref\n\n    def lqr_gain_and_ARE_soln(self, Q, R):\n        K_gain, P, S = lqr(self.A, self.B, Q, R)\n        S = Q + K_gain.T.dot(R).dot(K_gain)\n        K_gain = np.asarray(K_gain)\n        S = np.asarray(S)\n        P = np.asarray(P)\n        self.Q = Q\n        self.P = P\n        self.S = S\n        self.K_gain = K_gain\n        self.c3 = np.min(np.linalg.eig(self.S)[0]) / np.max(np.linalg.eig(self.P)[0])", ""]}
{"filename": "quad_1D/__init__.py", "chunked_list": [""]}
{"filename": "quad_1D/gp_utils.py", "chunked_list": ["import GPy\nimport numpy as np\nimport gpytorch\nimport torch\nimport matplotlib.pyplot as plt\nfrom gpytorch.constraints import Positive\n\ndef weighted_distance(x1 : torch.Tensor, x2 : torch.Tensor, L : torch.Tensor) -> torch.Tensor:\n    \"\"\"Computes (x1-x2)^T L (x1-x2)\n    Args:\n        x1 (torch.tensor) : N x n tensor (N is number of samples and n is dimension of vector)\n        x2 (torch.tensor) : M x n tensor\n        L (torch.tensor) : nxn wieght matrix\n\n    Returns:\n        weighted_distances (torch.tensor) : N x M tensor of all the prossible products\n\n    \"\"\"\n    N = x1.shape[0]\n    M = x2.shape[0]\n    n = L.shape[0]\n    # subtract all vectors in x2 from all vectors in x1 (results in NxMxn matrix)\n    diff = x1.unsqueeze(1) - x2\n    diff_T = diff.reshape(N,M,n,1)\n    diff = diff.reshape(N,M,1,n)\n    L = L.reshape(1,1,n,n)\n    L = L.repeat(N,M,1,1)\n    weighted_distances = torch.matmul(diff, torch.matmul(L,diff_T))\n    return weighted_distances.squeeze()", "\ndef squared_exponential(x1,x2,L,var):\n    return var * torch.exp(-0.5 * weighted_distance(x1, x2, L))\n\nclass AffineKernel(gpytorch.kernels.Kernel):\n    is_stationary = False\n    def __init__(self, input_dim,\n                 length_prior=None,\n                 length_constraint=None,\n                 variance_prior=None,\n                 variance_constraint=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.input_dim = input_dim\n        self.register_parameter(\n            name='raw_length', parameter=torch.nn.Parameter(torch.zeros(2*(self.input_dim-1)))\n        )\n        self.register_parameter(\n            name='raw_variance', parameter=torch.nn.Parameter(torch.zeros(2))\n        )\n        # set the parameter constraint to be positive, when nothing is specified\n        if length_constraint is None:\n            length_constraint = Positive()\n        if variance_constraint is None:\n            variance_constraint = Positive()\n        # register the constraints\n        self.register_constraint(\"raw_length\", length_constraint)\n        self.register_constraint(\"raw_variance\", variance_constraint)\n        # set the parameter prior, see\n        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n        if length_prior is not None:\n            self.register_prior(\n                \"length_prior\",\n                length_prior,\n                lambda m: m.length,\n                lambda m, v: m._set_length(v),\n            )\n        if variance_prior is not None:\n            self.register_prior(\n                \"variance_prior\",\n                variance_prior,\n                lambda m: m.variance,\n                lambda m, v: m._set_variance(v),\n            )\n\n    # now set up the 'actual' paramter\n    @property\n    def length(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_length_constraint.transform(self.raw_length)\n\n    @length.setter\n    def length(self, value):\n        return self._set_length(value)\n\n    def _set_length(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_length)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_length=self.raw_length_constraint.inverse_transform(value))\n\n\n    @property\n    def variance(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_variance_constraint.transform(self.raw_variance)\n\n    @variance.setter\n    def variance(self, value):\n        return self._set_variance(value)\n\n    def _set_variance(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_variance)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_variance=self.raw_variance_constraint.inverse_transform(value))\n\n    def kappa_alpha(self,x1, x2):\n        L_alpha = torch.diag(1 / (self.length[0:self.input_dim-1] ** 2))\n        var_alpha = self.variance[0]\n        return squared_exponential(x1, x2, L_alpha, var_alpha)\n\n    def kappa_beta(self, x1, x2):\n        L_beta = torch.diag(1 / (self.length[self.input_dim-1:] ** 2))\n        var_beta = self.variance[1]\n        return squared_exponential(x1, x2, L_beta, var_beta)\n\n    def kappa(self,x1, x2):\n        z1 = x1[:, 0:self.input_dim - 1]\n        u1 = x1[:, -1, None]\n        z2 = x2[:, 0:self.input_dim - 1]\n        u2 = x2[:, -1, None]\n        u_mat = u1.unsqueeze(1) * u2\n        kappa = self.kappa_alpha(z1, z2) + self.kappa_beta(z1, z2) * u_mat.squeeze()\n        if kappa.dim() < 2:\n            return kappa.unsqueeze(0)\n        else:\n            return kappa\n    # this is the kernel function\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        if last_dim_is_batch:\n            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n        kern = self.kappa(x1, x2)\n        if diag:\n            return kern.diag()\n        else:\n            return kern", "\nclass AffineGP(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.input_dim = train_x.shape[1]\n        #self.output_dim = train_y.shape[1]\n        self.output_dim = 1\n        self.n = 1     #output dimension\n        #self.mean_module = gpytorch.means.ConstantMean()\n        self.mean_module = None\n        self.covar_module = AffineKernel(self.input_dim)\n        self.K_plus_noise_inv = None\n\n    def forward(self, x):\n        mean_x = self.mean_module(x) # is this needed for ZeroMean?\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n    def compute_gammas(self, query):\n        return NotImplementedError\n\n    def mean_and_cov_from_gammas(self,query):\n        gamma_1, gamma_2, gamma_3, gamma_4, gamma_5 = self.compute_gammas(query)\n        u = query[:, None, 1]\n        means_from_gamma = gamma_1 + gamma_2.mul(u)\n        covs_from_gamma = gamma_3 + gamma_4.mul(u) + gamma_5.mul(u ** 2) + self.likelihood.noise.detach()\n        upper_from_gamma = means_from_gamma + covs_from_gamma.sqrt() * 2\n        lower_from_gamma = means_from_gamma - covs_from_gamma.sqrt() * 2\n        return means_from_gamma, covs_from_gamma, upper_from_gamma, lower_from_gamma", "\nclass ZeroMeanAffineGP(AffineGP):\n    def __init__(self, train_x, train_y, likelihood):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ZeroMean()\n\n    def compute_gammas(self, query):\n        # Parse inputs\n        with torch.no_grad():\n            n_train_samples = self.train_targets.shape[0]\n            n_query_samples = query.shape[0]\n            zq = query[:,0:self.input_dim-1]\n            uq = query[:,-1,None]\n            z_train = self.train_inputs[0][:,0:self.input_dim-1]\n            u_train = self.train_inputs[0][:,-1,None].tile(n_query_samples)\n            # Precompute useful matrics\n            k_a = self.covar_module.kappa_alpha(zq, z_train)\n            if k_a.dim() == 1:\n                k_a = k_a.unsqueeze(0)\n            k_b = self.covar_module.kappa_beta(zq, z_train).mul(u_train.T)\n            if k_b.dim() == 1:\n                k_b = k_b.unsqueeze(0)\n            Psi = self.train_targets.reshape((n_train_samples,1))\n            # compute gammas (Note: inv_matmul(R, L) = L * inv(K) * R\n            gamma_1 = k_a @ self.K_plus_noise_inv @ Psi\n            gamma_2 = k_b @ self.K_plus_noise_inv @ Psi\n            gamma_3 = torch.diag(self.covar_module.kappa_alpha(zq,zq) - k_a @ self.K_plus_noise_inv @ k_a.T)\n            gamma_4 = torch.diag(-( k_b @ self.K_plus_noise_inv @ k_a.T + k_a @ self.K_plus_noise_inv @ k_b.T))\n            gamma_5 = torch.diag(self.covar_module.kappa_beta(zq,zq) - k_b @ self.K_plus_noise_inv @ k_b.T)\n        return gamma_1, gamma_2, gamma_3.unsqueeze(1), gamma_4.unsqueeze(1), gamma_5.unsqueeze(1)", "\nclass ConstantMeanAffineGP(AffineGP):\n    def __init__(self, train_x, train_y, likelihood, mean_prior=None):\n        \"\"\"Zero mean with Affine Kernel GP model for SISO systems\n\n        Args:\n            train_x (torch.Tensor): input training data (N_samples x input_dim)\n            train_y (torch.Tensor): output training data (N_samples x 1)\n            likelihood (gpytorch.likelihood): Likelihood function\n                (gpytorch.likelihoods.MultitaskGaussianLikelihood)\n            mean_prior (NOT SURE) : prior on the constant mean\n        \"\"\"\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n\n    def compute_gammas(self, query):\n        # Parse inputs\n        with torch.no_grad():\n            n_train_samples = self.train_targets.shape[0]\n            n_query_samples = query.shape[0]\n            zq = query[:,0:self.input_dim-1]\n            uq = query[:,-1,None]\n            z_train = self.train_inputs[0][:,0:self.input_dim-1]\n            u_train = self.train_inputs[0][:,-1,None].tile(n_query_samples)\n            # Precompute useful matrics\n            k_a = self.covar_module.kappa_alpha(zq, z_train)\n            if k_a.dim() == 1:\n                k_a = k_a.unsqueeze(0)\n            k_b = self.covar_module.kappa_beta(zq, z_train).mul(u_train.T)\n            if k_b.dim() == 1:\n                k_b = k_b.unsqueeze(0)\n            Psi = self.train_targets.reshape((n_train_samples,1))\n            # compute gammas (Note: inv_matmul(R, L) = L * inv(K) * R\n            gamma_1 = k_a @ self.K_plus_noise_inv @ (Psi - u_train*self.mean_module.constant)\n            gamma_2 = self.mean_module.constant + k_b @ self.K_plus_noise_inv @ (Psi - u_train*self.mean_module.constant)\n            gamma_3 = torch.diag(self.covar_module.kappa_alpha(zq,zq) - k_a @ self.K_plus_noise_inv @ k_a.T)\n            gamma_4 = torch.diag(-( k_b @ self.K_plus_noise_inv @ k_a.T + k_a @ self.K_plus_noise_inv @ k_b.T))\n            gamma_5 = torch.diag(self.covar_module.kappa_beta(zq,zq) - k_b @ self.K_plus_noise_inv @ k_b.T)\n        return gamma_1, gamma_2, gamma_3.unsqueeze(1), gamma_4.unsqueeze(1), gamma_5.unsqueeze(1)", "\nclass GaussianProcess():\n    def __init__(self, model_type, likelihood, n):\n        \"\"\"\n        Gaussian Process decorator for gpytorch\n        Args:\n            model_type (gpytorch model class): Model class for the GP (ZeroMeanIndependentMultitaskGPModel)\n            likelihood (gpytorch.likelihood): likelihood function\n            n (int): Dimension of input state space\n\n        \"\"\"\n        self.model_type = model_type\n        self.likelihood = likelihood\n        self.m = n\n        self.optimizer = None\n        self.model = None\n\n    def train(self, train_x, train_y, n_train=150):\n        \"\"\"\n        Train the GP using Train_x and Train_y\n        train_x: Torch tensor (dim input x N samples)\n        train_y: Torch tensor (nx x N samples)\n\n        \"\"\"\n        self.n = train_x.shape[1]\n        self.m = 1\n        self.output_dim = 1\n        self.input_dim = train_x.shape[1]\n        if self.model is None:\n            self.model = self.model_type(train_x, train_y, self.likelihood)\n        else:\n            train_x = torch.reshape(train_x, self.model.train_inputs[0].shape)\n            train_x = torch.cat([train_x, self.model.train_inputs[0]])\n            train_y = torch.cat([train_y, self.model.train_targets])\n            self.model.set_train_data(train_x, train_y, False)\n        self.model.double()\n        self.likelihood.double()\n        self.model.train()\n        self.likelihood.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.5)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        for i in range(n_train):\n            self.optimizer.zero_grad()\n            output = self.model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            print('Iter %d/%d - Loss: %.3f' % (i + 1, n_train, loss.item()))\n            self.optimizer.step()\n        # compute inverse covariance plus noise for faster computation later\n        K_lazy = self.model.covar_module(train_x.double())\n        K_lazy_plus_noise = K_lazy.add_diag(self.model.likelihood.noise)\n        n_samples = train_x.shape[0]\n        self.model.K_plus_noise_inv = K_lazy_plus_noise.inv_matmul(torch.eye(n_samples).double())\n\n    def predict(self, x, requires_grad=False, return_pred=True):\n        \"\"\"\n        x : torch.Tensor (input dim X N_samples)\n        Return\n            Predicitons\n            mean : torch.tensor (nx X N_samples)\n            lower : torch.tensor (nx X N_samples)\n            upper : torch.tensor (nx X N_samples)\n        \"\"\"\n        #x = torch.from_numpy(x).double()\n        self.model.eval()\n        self.likelihood.eval()\n        #with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        if type(x) is np.ndarray:\n            x = torch.from_numpy(x).double()\n\n        if requires_grad:\n            predictions = self.likelihood(self.model(x))\n            mean = predictions.mean\n            cov = predictions.covariance_matrix\n        else:\n            with torch.no_grad():\n                predictions = self.likelihood(self.model(x))\n                mean = predictions.mean\n                cov = predictions.covariance_matrix\n        if return_pred:\n            return mean, cov, predictions\n        else:\n            return mean, cov\n\n    def prediction_jacobian(self, query):\n        gammas = self.model.compute_gammas(query)\n        mean_der = gammas[1]\n        cov_der = gammas[4]\n        #mean_der, _ = torch.autograd.functional.jacobian(\n        #                        lambda x: self.predict(x, requires_grad=True, return_pred=False),\n        #                        query.double())\n        #k_query_query = torch.autograd.functional.hessian(\n        #                               lambda x: self.model.covar_module.kappa(x,x), query.double()\n        #)\n        #k_v_v = k_query_query.squeeze()[-1,-1]\n        #k_a_prime = torch.autograd.functional.jacobian(\n        #        lambda x: self.model.covar_module.kappa(x, self.model.train_inputs[0]), query.double()\n        #)\n        #k_a = k_a_prime.squeeze()[:,-1,None]\n        #cov_der = k_v_v - k_a.T @ self.model.K_plus_noise_inv @ k_a #+ self.model.likelihood.noise\n\n        #k_v_v = self.model.covar_module.kappa_beta(query[:,None,0:3], query[:,None,0:3])\n        #u_train = self.model.train_inputs[0][:, -1, None]\n        #k_b = self.model.covar_module.kappa_beta(query[:,None,0:3], self.model.train_inputs[0][:,0:3]).mul(u_train.T)\n        #if k_b.dim() == 1:\n        #    k_b = k_b.unsqueeze(0)\n        ##k_b = self.model.covar_module.kappa_beta(query[:,None,0:3],self.model.train_inputs[0][:,0:3]).unsqueeze(0)\n        #cov_der = k_v_v - k_b @ self.model.K_plus_noise_inv @ k_b.T  #+ self.model.likelihood.noise\n        #cov_der = k_v_v\n\n        return mean_der.detach(), cov_der.detach()\n\n    def plot_trained_gp(self, t, fig_count=0):\n        means, covs, preds = self.predict(self.model.train_inputs[0])\n        lower, upper = preds.confidence_region()\n        fig_count += 1\n        plt.figure(fig_count)\n        plt.fill_between(t, lower.detach().numpy(), upper.detach().numpy(), alpha=0.5, label='95%')\n        plt.plot(t, means, 'r', label='GP Mean')\n        plt.plot(t, self.model.train_targets, '*k', label='Data')\n        plt.legend()\n        plt.title('Fitted GP')\n        plt.xlabel('Time (s)')\n        plt.ylabel('v')\n        plt.show()\n\n        return fig_count", "\n\ndef train_gp(noise, variance_input, n, gp_input, gp_output):\n    gp_kernel = GPy.kern.RBF(input_dim=n, ARD=True, variance=variance_input, lengthscale=1.0)\n\n    gp_model = GPy.models.GPRegression(gp_input, gp_output, gp_kernel, normalizer=None, noise_var=noise)\n    gp_model.rbf.variance.constrain_fixed()\n    gp_model.Gaussian_noise.variance.constrain_fixed(noise)\n    gp_model.optimize(max_iters=1000, optimizer='bfgs', messages=True)\n\n    return gp_model", "\nclass SISOAffineGP:\n    def __init__(self, noise, variance_input, n, gp_input, gp_output):\n        self.model = self.train_gp(noise, variance_input, n, gp_input, gp_output)\n        self.output_dim = self.model.output_dim\n        self.input_dim = self.model.input_dim\n        self.n_samples = gp_input.shape[0]\n\n        self.create_affine_kernels()\n\n        self.X = gp_input[:,0:3]\n        self.U = gp_input[:,-1,np.newaxis]\n        self.UU = self.U @ self.U.T\n\n        self.input = gp_input\n        self.output = gp_output\n\n\n\n        self.K = self.model.kern.K(gp_input) + np.eye(self.n_samples)*noise**2\n        self.K_compute = self.kappa_alpha.K(self.X) + np.multiply(self.UU, self.kappa_beta.K(self.X)) + np.eye(self.n_samples)*noise**2\n        self.K_inv = np.linalg.inv(self.K)\n        self.Psi = gp_output\n\n        self.noise = noise\n        self.variance = variance_input\n\n\n\n    def train_gp(self, noise, variance_input, n, gp_input, gp_output):\n        k_alpha = GPy.kern.RBF(input_dim=n-1, active_dims=[0,1,2], ARD=True, variance=variance_input, lengthscale=1.0,name='rbf_alpha')\n        k_beta = GPy.kern.RBF(input_dim=n-1, active_dims=[0,1,2], ARD=True, variance=variance_input, lengthscale=1.0, name='rbf_beta')\n        k_u = GPy.kern.Linear(input_dim=1, active_dims=[3], ARD=True, name='linear')\n\n\n        k_prod = GPy.kern.Prod([k_u, k_beta])\n        affine_kernel = GPy.kern.Add([k_alpha, k_prod])\n\n        gp_model = GPy.models.GPRegression(gp_input, gp_output, affine_kernel, normalizer=False, noise_var=noise)\n        gp_model.kern.parameters[1].parameters[0].variances.fix(1) # force linear kernel fix variance parameter\n        gp_model.kern.parameters[0].variance.constrain_fixed()\n        gp_model.kern.parameters[1].parameters[1].variance.constrain_fixed()\n        gp_model.optimize(max_iters=10000, optimizer='bfgs', messages=True)\n\n        return gp_model\n\n    def create_affine_kernels(self):\n        params = self.model.param_array\n        var_alpha = params[0]\n        l_alpha = params[1:4]\n        var_beta = params[5]\n        l_beta = params[6:-1]\n        self.kappa_alpha = GPy.kern.RBF(input_dim=self.input_dim - 1,\n                                        ARD=True,\n                                        variance=var_alpha,\n                                        lengthscale=l_alpha,\n                                        name='kappa_alpha')\n        self.kappa_beta = GPy.kern.RBF(input_dim=self.input_dim - 1,\n                                        ARD=True,\n                                        variance=var_beta,\n                                        lengthscale=l_beta,\n                                        name='kappa_beta')\n\n\n    def predict(self,query):\n        x = query[0,np.newaxis,0:self.input_dim-1]\n        u = query[0,np.newaxis,-1,np.newaxis]\n        k_alpha = self.kappa_alpha.K(x,self.X)\n        k_beta = np.multiply(self.kappa_beta.K(x,self.X),self.U.T)\n\n        gamma_1 = k_alpha @ self.K_inv @ self.Psi\n        gamma_2 = k_beta @ self.K_inv @ self.Psi\n        gamma_3 = self.kappa_alpha.K(x,x) - k_alpha @ self.K_inv @ k_alpha.T\n        gamma_4 = -(k_beta @ self.K_inv @ k_alpha.T + k_alpha @ self.K_inv @ k_beta.T)\n        gamma_5 = self.kappa_beta.K(x,x) - k_beta @ self.K_inv @ k_beta.T\n\n        mean = gamma_1 + gamma_2*u\n        cov = gamma_3 + gamma_4*u + gamma_5*u**2\n\n        #cov_test = self.model.kern.K(query,query) - self.model.kern.K(query,self.input) @ np.linalg.inv(self.model.kern.K(self.input,self.input)) @ self.model.kern.K(self.input,query)\n        #param_a = self.kappa_alpha.param_array\n        #param_b = self.kappa_beta.param_array\n        #test = affine_kernel(query,query,param_a, param_b) - \\\n        #        affine_kernel(query,self.input,param_a, param_b) @ np.linalg.inv(affine_kernel(self.input,self.input,param_a, param_b) + np.eye(500)*self.noise**2) @ affine_kernel(self.input, query,param_a, param_b)\n        #mean, cov = self.model.predict(query)\n        return mean, cov", "\ndef affine_kernel(z1, z2, params_a, params_b):\n    variance_a = params_a[0]\n    length_scales_a = params_a[1:]\n    variance_b = params_b[1]\n    length_scales_b = params_b[1:]\n\n    x1 = z1[:,0:-1]\n    x2 = z2[:,0:-1]\n    k_a = se_kernel(x1, x2, variance_a, length_scales_a)\n    k_b = se_kernel_u(z1, z2, variance_b, length_scales_b)\n\n\n    k = k_a + k_b\n    return k", "\ndef se_kernel_u(z1, z2, variance, length_scales):\n    \"\"\"\n    x1 = Nsamples x input\n    x2 = Nsamples x inputs\n    length_scales : size of input\n    \"\"\"\n    N1, n = z1.shape\n    N2, n = z2.shape\n    x1 = z1[:,0:-1]\n    x2 = z2[:,0:-1]\n    u1 = z1[:,-1]\n    u2 = z2[:,-1]\n    L_inv = np.diag(1/length_scales**2)\n    val = np.zeros((N1,N2))\n    for i in range(N1):\n        for j in range(N2):\n            val[i,j] = u1[i]*u2[j]*variance*np.exp(-0.5*(x1[np.newaxis,i,:].T-x2[np.newaxis,j,:].T).T @ L_inv @ (x1[np.newaxis,i,:].T - x2[np.newaxis,j,:].T))\n\n    val = val\n\n    return val", "\ndef se_kernel(x1, x2, variance, length_scales):\n    \"\"\"\n    x1 = Nsamples x input\n    x2 = Nsamples x inputs\n    length_scales : size of input\n    \"\"\"\n    N1, n = x1.shape\n    N2, n = x2.shape\n    L_inv = np.diag(1/length_scales**2)/2.0\n    val = np.zeros((N1,N2))\n    for i in range(N1):\n        for j in range(N2):\n            val[i,j] = variance*np.exp(-0.5*(x1[np.newaxis,i,:].T-x2[np.newaxis,j,:].T).T @ L_inv @ (x1[np.newaxis,i,:].T - x2[np.newaxis,j,:].T))\n    val = val\n\n    return val", "\n\n"]}
{"filename": "quad_1D/expr_utils.py", "chunked_list": ["import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\ndef plot_comparison(title, data_dicts, xkey, ykey, fig_count=0):\n    fig_count += 1\n    plt.figure(fig_count)\n    plt.title(title)\n    for data_dict in data_dicts:\n        x = data_dict[xkey]\n        y = data_dict[ykey]\n        n_subplots = y.shape[1]\n        for i in range(n_subplots):\n            plt_id = int(n_subplots * 100 + 10 + i + 1)\n            plt.subplot(plt_id)\n            plt.plot(x, y[:,i], label=data_dict['primary_name'])\n            plt.xlabel(xkey)\n            plt.ylabel(ykey + '%s' % i)\n    plt.legend()\n    plt.show()\n    return fig_count", "\n\ndef feedback_loop(params,\n                  gp,\n                  true_dynamics,\n                  reference_generator,\n                  primary_controller, # feedbacm function\n                  secondary_controllers=None, # list of comparison controllers\n                  online_learning=False,\n                  fig_count=0,\n                  plot=True,\n                  input_bound: float=None):\n    \"\"\" Run a feedback loop.\n\n    Primary controller is used to determine the input u to be applied to the system.\n    Secondary controllers contains a list of controllers whos inputs are computed\n    for comparison, but not actually applied to the system.\n\n    \"\"\"\n    # Parse Params\n    N = params[\"N\"] # number of time steps\n    n = params[\"n\"] # dimension of inputs\n    m = params[\"m\"] # dimension of outputs\n    dt = params[\"dt\"] # time step\n    Amp = params[\"Amp\"] # Reference traj Amplitude\n    omega = params[\"omega\"] # Reference traj freq\n    if online_learning:\n        n_online = params['n_online']\n        learning_rate = params['learning_rate']\n        variance = params['training_variance']\n        n_train = params['n_train']\n    # Logging\n    u_log = np.zeros((N, 1))\n    if secondary_controllers is not None:\n        n_sec_controllers = len(secondary_controllers)\n        u_secondary_log = np.zeros((N,n_sec_controllers))\n        v_secondary_log = np.zeros((N, n_sec_controllers))\n    z_ref_log = np.zeros((N+1,n))\n    z_log = np.zeros((N+1,n))\n    v_log = np.zeros((N,1))\n    v_des_log = np.zeros((N,1))\n    d_sf_log = np.zeros((N, 1))\n    t_log = np.zeros((N+1, 1))\n    error_log = np.zeros((N,1))\n    solve_time_log = np.zeros((N,1))\n    infeasible_index = N+1\n    infeasible = False\n    z, _ = reference_generator(0, Amp, omega)\n    #z = np.zeros((3,1))\n    u = np.zeros(1)\n    x_init = np.zeros(3)\n    # Main Feedback Loop\n    for i in range(0, N):\n        t = i * dt\n        z_log[i,:] = z.T\n        z_ref, v_ref = reference_generator(t, Amp, omega)\n        x_init[0] = u\n        t1 = time.perf_counter()\n        u, v_des, success, d_sf = primary_controller.compute_feedback_input(gp,\n                                                                            z,\n                                                                            z_ref,\n                                                                            v_ref,\n                                                                            x_init=x_init,\n                                                                            t=t,\n                                                                            params=params)\n        t2 = time.perf_counter()\n        ct = t2 - t1\n        print(\"Step: %s,  min_time: %s, success: %s,  error: %s\" % (\n        i, ct, success, np.sum(np.linalg.norm(z - z_ref))))\n        if secondary_controllers is not None:\n            # compute inputs for secondary controllers for comparisons\n            for ic, sec_controller in enumerate(secondary_controllers):\n                u_secondary_log[i,ic], _, _, _ = sec_controller.compute_feedback_input(gp,\n                                                                                       z,\n                                                                                       z_ref,\n                                                                                       v_ref,\n                                                                                       x_init=x_init,\n                                                                                       t=t,\n                                                                                       params=params)\n            # compute measured flat inputs for secondary controllers for comparisons\n            for ic, sec_controller in enumerate(secondary_controllers):\n                _, v_secondary_log[i, ic] = true_dynamics(z, u_secondary_log[i, ic])\n        if not success or any(np.isnan(np.atleast_1d(u))):\n            infeasible = True\n            infeasible_index = i\n            print(\"%s INFEASIBLE with statues\" % (primary_controller.name))\n            break\n        # step the dynamics\n        if input_bound is not None:\n            u = np.clip(u, -input_bound, input_bound)\n        z, v_meas = true_dynamics(z, u)\n        if any(np.isnan(z)):\n            infeasible = True\n            infeasible_index = i\n            print(\"%s INFEASIBLE with statues\" % (primary_controller.name))\n            break\n        # log\n        t_log[i] = t\n        u_log[i,:] = u\n        v_log[i,:] = v_meas\n        z_ref_log[i,:] = z_ref.T\n        v_des_log[i] = v_des\n        d_sf_log[i] = d_sf\n        solve_time_log[i,:] = ct\n        error_log[i,:] = np.linalg.norm(z-z_ref)\n    z_log[i+1, :] = z.T\n    z_ref, v_ref = reference_generator(t+dt, Amp, omega)\n    t_log[i+1] = t+dt\n    z_ref_log[i+1,:] = z_ref.T\n    if plot:\n        # Plot the states along the trajectory and compare with reference.\n        fig_count += 1\n        plt.figure(fig_count)\n        for i in range(n):\n            plt_id = int(n*100 + m*10 + i + 1)\n            plt.subplot(plt_id)\n            plt.plot(t_log, z_log[:, i], label=primary_controller.name)\n            plt.plot(t_log, z_ref_log[:, i], label='Ref')\n            plt.xlabel('t')\n            plt.ylabel('z%s' % i)\n            plt.title(\" %s FB State z%s Comparisons\" %(primary_controller.name, i))\n        plt.legend()\n        plt.show()\n        # Plot a comparison of the flat inputs.\n        fig_count += 1\n        plt.figure(fig_count)\n        for i in range(m):\n            plt_id = int(int(m) * 100 + 10 + i + 1)\n            plt.subplot(plt_id)\n            plt.plot(t_log[:-1], v_des_log[:, i], label=primary_controller.name)\n            if secondary_controllers is not None:\n                for ic in range(n_sec_controllers):\n                    plt.plot(t_log[:-1], v_secondary_log[:, ic], label=secondary_controllers[ic].name)\n            plt.xlabel('t')\n            plt.ylabel('v%s' % i)\n        plt.title('%s FB Comparison of desired Flat Inputs' % primary_controller.name)\n        plt.legend()\n        plt.show()\n        # Plot a comparison of the real inputs applied to the system.\n        fig_count += 1\n        plt.figure(fig_count)\n        for i in range(m):\n            plt_id = int(int(m) * 100 + 10 + i + 1)\n            plt.subplot(plt_id)\n            plt.plot(t_log[:-1], u_log[:, i], label=primary_controller.name)\n            if secondary_controllers is not None:\n                for ic in range(n_sec_controllers):\n                    plt.plot(t_log[:-1], u_secondary_log[:, ic], label=secondary_controllers[ic].name)\n            plt.xlabel('t')\n            plt.ylabel('u%s' % i)\n        plt.title('%s FB Comparison of Real inputs' % primary_controller.name)\n        plt.legend()\n        plt.show()\n    data_dict = {}\n    data_dict['params'] = params\n    data_dict['t'] = t_log\n    data_dict['u'] = u_log\n    data_dict['v'] = v_log\n    data_dict['z'] = z_log\n    data_dict['z_ref'] = z_ref_log\n    data_dict['v_des'] = v_des_log\n    data_dict['d'] = d_sf_log\n    data_dict['solve_time'] = solve_time_log\n    data_dict['error'] = error_log\n    data_dict['primary_name'] = primary_controller.name\n    data_dict['infeasible'] = infeasible\n    data_dict['infeasible_index'] = infeasible_index\n    if secondary_controllers is not None:\n        data_dict['u_sec'] = u_secondary_log\n        data_dict['v_sec'] = v_secondary_log\n        data_dict['secondary_names'] = {}\n        for ic, sec_controller in enumerate(secondary_controllers):\n            data_dict['secondary_names'][ic] = sec_controller.name\n    return data_dict, fig_count", "\n"]}
{"filename": "quad_1D/controllers.py", "chunked_list": ["import numpy as np\nfrom numpy.linalg import norm\nimport cvxpy as cp\nfrom scipy.optimize import minimize\nfrom robust_lqr import bound_computation\nimport torch\nimport GPy\n\nclass RobustLQR():\n    def __init__(self, name, quad, Q, R, delta, bound=None, eps=1e-3, c0=100.0):\n        self.name = name\n        self.quad = quad\n        self.quad.lqr_gain_and_ARE_soln(Q, R)\n        self.K_gain = self.quad.K_gain\n        self.P = self.quad.P\n        self.S = self.quad.S\n        self.B = self.quad.B\n        self.prob_threshold = np.sqrt(1.0-delta)\n        self.eps = eps\n        self.c0 = c0\n        self.bound = bound\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, **kwargs):\n        v_des = -self.K_gain @ (z - z_ref) + v_ref\n        query = np.hstack((z.T, v_des.T))\n        if type(gp) == GPy.models.gp_regression.GPRegression:\n            u_query = gp.predict(query)\n            u_query_der = gp.predict_jacobian(query)  # this doesn't work for non-stationary kernels!\n            mean = u_query[0][0]\n            var = u_query[1][0]\n            mean_der = u_query_der[0][0][-1]\n            var_der = u_query_der[1][0][-1, -1]\n        else:\n            query = torch.from_numpy(query).double()\n            # compute mean, variance and their derivatives wrt v\n            mean, var, _ = gp.predict(query)\n            mean = mean.numpy().squeeze()\n            var = var.numpy().squeeze()\n            mean_der, var_der = gp.prediction_jacobian(query)\n            mean_der = mean_der.numpy().squeeze()\n            var_der = var_der.numpy().squeeze()\n        #mean_der = mean_der[-1]\n        # compute bound inputs\n        e = z-z_ref\n        w = e.T@self.P@self.B\n        w = w.squeeze()\n        # compute bound\n        c, success = bound_computation(0.0, 1.0+mean_der, var, var_der, 0.0, self.prob_threshold, self.c0)\n        #if abs(w) > self.eps:\n        #    v_rob = -c * np.sign(w)\n        #else:\n        #    v_rob = -c * w / self.eps\n        v_rob = -c * np.sign(w)\n        v = mean + v_rob + v_des\n        # v = mean - c * np.sign(w) + v_des\n        u = self.quad.u_from_v(v, z)\n        if self.bound is not None:\n            u = np.clip(u, -self.bound, self.bound)\n        # saturate u to limits\n        return u, v_des, success, 0", "class RobustLQR():\n    def __init__(self, name, quad, Q, R, delta, bound=None, eps=1e-3, c0=100.0):\n        self.name = name\n        self.quad = quad\n        self.quad.lqr_gain_and_ARE_soln(Q, R)\n        self.K_gain = self.quad.K_gain\n        self.P = self.quad.P\n        self.S = self.quad.S\n        self.B = self.quad.B\n        self.prob_threshold = np.sqrt(1.0-delta)\n        self.eps = eps\n        self.c0 = c0\n        self.bound = bound\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, **kwargs):\n        v_des = -self.K_gain @ (z - z_ref) + v_ref\n        query = np.hstack((z.T, v_des.T))\n        if type(gp) == GPy.models.gp_regression.GPRegression:\n            u_query = gp.predict(query)\n            u_query_der = gp.predict_jacobian(query)  # this doesn't work for non-stationary kernels!\n            mean = u_query[0][0]\n            var = u_query[1][0]\n            mean_der = u_query_der[0][0][-1]\n            var_der = u_query_der[1][0][-1, -1]\n        else:\n            query = torch.from_numpy(query).double()\n            # compute mean, variance and their derivatives wrt v\n            mean, var, _ = gp.predict(query)\n            mean = mean.numpy().squeeze()\n            var = var.numpy().squeeze()\n            mean_der, var_der = gp.prediction_jacobian(query)\n            mean_der = mean_der.numpy().squeeze()\n            var_der = var_der.numpy().squeeze()\n        #mean_der = mean_der[-1]\n        # compute bound inputs\n        e = z-z_ref\n        w = e.T@self.P@self.B\n        w = w.squeeze()\n        # compute bound\n        c, success = bound_computation(0.0, 1.0+mean_der, var, var_der, 0.0, self.prob_threshold, self.c0)\n        #if abs(w) > self.eps:\n        #    v_rob = -c * np.sign(w)\n        #else:\n        #    v_rob = -c * w / self.eps\n        v_rob = -c * np.sign(w)\n        v = mean + v_rob + v_des\n        # v = mean - c * np.sign(w) + v_des\n        u = self.quad.u_from_v(v, z)\n        if self.bound is not None:\n            u = np.clip(u, -self.bound, self.bound)\n        # saturate u to limits\n        return u, v_des, success, 0", "\n\nclass LQR():\n    def __init__(self, name, quad, Q, R, bound=None):\n        self.name = name\n        quad.lqr_gain_and_ARE_soln(Q, R)\n        self.quad = quad\n        self.K_gain = quad.K_gain\n        self.P = quad.P\n        self.S = quad.S\n        self.bound = bound\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, **kwargs):\n        v_des = - self.quad.K_gain.dot((z - z_ref)) + v_ref\n        u = self.quad.u_from_v(v_des, z)\n        if self.bound is not None:\n            u = np.clip(u, -self.bound, self.bound)\n        return u, v_des, True, 0", "\n\nclass ConstrainedQPProblem():\n    def __init__(self, name, upper_bound, lower_bound, quad):\n        self.name = name\n        self.quad = quad\n        self.p = cp.Parameter(shape=(1,1),pos=True)\n        self.q = cp.Parameter(shape=(1,1))\n        self.v_des = cp.Parameter(shape=(1,1))\n        self.u = cp.Variable(shape=(1,1))\n        self.lb = None\n        self.ub = None\n\n        # cost = (self.gamma2**2 + self.gamma5)*self.u**2 + \\\n        #(2 * self.gamma1 * self.gamma2 - 2 * self.gamma2 * self.v_des + self.gamma4) * self.u\n        #cost = self.p*self.u**2  + self.q * self.u\n        cost = self.p@self.u**2 + self.q @ self.u\n        #cost = cp.QuadForm(self.u, self.p) + self.q @ self.u\n        if upper_bound is not None and lower_bound is not None:\n            self.lb = lower_bound <= self.u\n            self.ub = self.u <= upper_bound\n            prob = cp.Problem(cp.Minimize(cost),  # cost\n                              [self.lb, self.ub])  # constraints\n        else:\n            prob = cp.Problem(cp.Minimize(cost))  # Only cost function\n        self.prob = prob\n\n    def solve(self, gp_model, z, v_des, x_init=None):\n        query = np.hstack((z.T, np.zeros((1, 1))))\n        query = torch.from_numpy(query).double()\n        # compute gammas\n        gamma1, gamma2, gamma3, gamma4, gamma5 = gp_model.model.compute_gammas(query)\n        gamma1 = gamma1.numpy().squeeze()\n        gamma2 = gamma2.numpy().squeeze()\n        gamma3 = gamma3.numpy().squeeze()\n        gamma4 = gamma4.numpy().squeeze()\n        gamma5 = gamma5.numpy().squeeze()\n        alpha_quad = self.quad.alpha(z)\n        beta_quad = self.quad.beta(z)\n        gamma1 = gamma1 + beta_quad.squeeze()\n        gamma2 = gamma2 + alpha_quad.squeeze()\n        #self.u = cp.Variable()\n        #cost = (gamma2**2 + gamma5)*self.u**2 + (2*gamma1*gamma2 - 2*gamma2*v_des.squeeze() + gamma4)*self.u\n        #self.prob = cp.Problem(cp.Minimize(self.cost) , [self.lb, self.ub])\n        #print(gamma5)\n        # assign parameters\n        self.p.value = np.array([[gamma2**2 + gamma5]])\n        self.q.value = np.array([[2 * gamma1 * gamma2 - 2 * gamma2 * v_des.squeeze() + gamma4]])\n        #self.gamma1.value = gamma1\n        #self.gamma2.value = gamma2\n        #self.gamma3.value = gamma3\n        #self.gamma4.value = gamma4\n        #self.gamma5.value = gamma5\n        #self.v_des.value = v_des.squeeze()\n\n        #self.u.value = x_init[0]\n        self.prob.solve(solver='MOSEK', warm_start=True, verbose=False)\n        return self.u.value.squeeze()\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, **kwargs):\n        \"\"\" Compute u so it can be used in feedback function\"\"\"\n        v_des = - self.quad.K_gain.dot((z - z_ref)) + v_ref\n        if self.lb is None and self.ub is None:\n            u = unconstrained_closed_form_soln_for_difference_gp(gp, z, v_des, self.quad)\n            success = True\n        else:\n            u = self.solve(gp, z, v_des, x_init=x_init)\n            if 'infeasible' in self.prob.status:\n                success = False\n            else:\n                success = True\n        return u, v_des, success, 0", "\n\nclass SOCPProblem:\n    def __init__(self, name, quad, beta, input_bound=None, state_bound=None, ctrl=None):\n        self.name = name\n        self.quad = quad\n        self.beta = beta\n        if quad.P is not None:\n            self.P = quad.P\n            self.S = quad.S\n            self.Q = quad.Q\n        else:\n            Q = ctrl.Q\n            R = ctrl.R\n            self.quad.lqr_gain_and_ARE_soln(Q, R)\n            self.P = quad.P\n            self.S = quad.S\n            self.Q = quad.Q\n        self.input_bound = input_bound\n        self.state_bound = state_bound\n        self.s_min = np.min(np.linalg.eig(self.S)[0])\n        if ctrl is not None:\n            if not(ctrl.name == 'FMPC'):\n                raise ValueError('Only FMPC can be used here for now.')\n            self.ctrl = ctrl\n        else:\n            self.ctrl = None\n\n        # Opt variables and parameters\n        self.X = cp.Variable(shape=(3,))\n        self.A1 = cp.Parameter(shape=(3, 3))\n        self.A2 = cp.Parameter(shape=(3, 3))\n        self.b1 = cp.Parameter(shape=(3,))\n        self.b2 = cp.Parameter(shape=(3,))\n        self.c1 = cp.Parameter(shape=(1, 3))\n        self.c2 = cp.Parameter(shape=(1, 3))\n        self.d1 = cp.Parameter()\n        self.d2 = cp.Parameter()\n        # put into lists\n        As = [self.A1, self.A2]\n        bs = [self.b1, self.b2]\n        cs = [self.c1, self.c2]\n        ds = [self.d1, self.d2]\n        # Add input constraints if supplied\n        if input_bound is not None:\n            A3 = np.zeros((3, 3))\n            A3[0, 0] = 1.0\n            b3 = np.zeros((3, 1))\n            c3 = np.zeros((1, 3))\n            d3 = input_bound\n            As.append(A3)\n            bs.append(b3)\n            cs.append(c3)\n            ds.append(d3)\n        if state_bound is not None:\n            self.Astate = cp.Parameter(shape=(3, 3))\n            self.bstate = cp.Parameter(shape=(3,))\n            self.cstate = cp.Parameter(shape=(1, 3))\n            self.dstate = cp.Parameter()\n            #self.Astate = np.zeros((3,3))\n            #self.Astate[0, 0] = 1.0\n            #self.bstate = np.zeros((3, 1))\n            #self.cstate = np.zeros((1, 3))\n            #self.dstate = 0.0\n            As.append(self.Astate)\n            bs.append(self.bstate)\n            cs.append(self.cstate)\n            ds.append(self.dstate)\n        else:\n            self.Astate = None\n            self.bstate = None\n            self.cstate = None\n            self.dstate = None\n        # define cost function\n        self.cost = cp.Parameter(shape=(1, 3))\n        m = len(As)\n        soc_constraints = [\n            cp.SOC(cs[i] @ self.X + ds[i], As[i] @ self.X + bs[i]) for i in range(m)\n        ]\n        self.prob = cp.Problem(cp.Minimize(self.cost @ self.X), soc_constraints)\n\n    def solve(self, gp_model, z, z_ref, v_des, x_init=np.zeros((3,))):\n        e = z - z_ref\n        cost, As, bs, cs, ds = socp_constraints_and_cost(gp_model,\n                                                         z,\n                                                         v_des,\n                                                         self.beta,\n                                                         e,\n                                                         self.quad,\n                                                         input_bound=self.input_bound,\n                                                         state_bound=self.state_bound)\n        self.cost.value = cost\n        self.A1.value = As['A1']\n        self.A2.value = As['A2']\n        self.b1.value = bs['b1'].squeeze()\n        self.b2.value = bs['b2'].squeeze()\n        self.c1.value = cs['c1']\n        self.c2.value = cs['c2']\n        self.d1.value = ds['d1']\n        self.d2.value = ds['d2']\n        if self.state_bound is not None:\n            self.Astate.value = As['Astate']\n            self.bstate.value = bs['bstate'].squeeze()\n            self.cstate.value = cs['cstate']\n            self.dstate.value = ds['dstate']\n\n        self.X.value = x_init\n        self.prob.solve(solver='MOSEK', warm_start=True, verbose=True) # SCS was used in paper\n        if 'optimal' in self.prob.status:\n            return self.X.value[0], self.X.value[1]\n        else:\n            return 0, 0\n\n    def compute_feedback_input(self, gp, z, z_ref, v_ref, x_init=None, t=None, params=None, **kwargs):\n        \"\"\" Compute u so it can be used in feedback function\"\"\"\n        if self.ctrl is None:\n            v_des = - self.quad.K_gain.dot((z - z_ref)) + v_ref\n            u, d_sf = self.solve(gp, z, z_ref, v_des, x_init=x_init)\n        else:\n            zd, v_des, return_status = self.ctrl.select_flat_input(z, t, params)\n            zd = np.atleast_2d(zd).T\n            u, d_sf = self.solve(gp, zd, z_ref, v_des, x_init=x_init)\n        if 'optimal' in self.prob.status:\n            success = True\n        else:\n            success = False\n        return u, v_des, success, d_sf", "\ndef socp_constraints_and_cost(gp_model,\n                              z,\n                              v_des,\n                              beta,\n                              e,\n                              quad,\n                              input_bound=None,\n                              state_bound=None,\n                              d_weight=25,\n                              eps=0.1):\n    \"\"\" Setup the SOCP Opt constraint matrices for CVX.\n\n    See following link for more information\n    https://www.cvxpy.org/examples/basic/socp.html\n\n    Args:\n        gp_model (GaussianProcess) : GP model for nonlinear term (z,u) -> v\n        z (np.array): current flat state\n        v_des (np.array): 1x1 desired flat input\n        beta (float): safety factor (number of stds)\n        A (np.array): System state matrix A\n        B (np.array): System input matrix B\n        e (np.array): flat error state\n        P (np.array): Soln to ARE (or DARE)\n        z_ref_dot (np.array): time derivative of input reference\n        s_min (float): smallest eigenvalue of S\n\n    Returns:\n        As (list): list of A socp constraint matrices\n        bs (list): list of b socp constraint matrices\n        cs (list): list of c socp constraint matrices\n        ds (list): list of d socp constraint matrices\n        cost (np.array): array of the cost matrix to be minimized\n\n    \"\"\"\n    query_np = np.hstack((z.T, np.zeros((1, 1))))\n    query = torch.from_numpy(query_np).double()\n    gamma1, gamma2, gamma3, gamma4, gamma5 = gp_model.model.compute_gammas(query)\n    gamma1 = gamma1.numpy().squeeze()\n    gamma2 = gamma2.numpy().squeeze()\n    gamma3 = gamma3.numpy().squeeze()\n    gamma4 = gamma4.numpy().squeeze()\n    gamma5 = gamma5.numpy().squeeze()\n    B = quad.B\n    w = e.T @ quad.P @ B\n    w = w[0, 0]\n    alpha_quad = quad.alpha(z)\n    beta_quad = quad.beta(z)\n    gamma1 = gamma1 + beta_quad.squeeze()\n    gamma2 = gamma2 + alpha_quad.squeeze()\n    gamma1 = gamma1.squeeze()\n    gamma2 = gamma2.squeeze()\n    norm_w = np.absolute(w)\n    if state_bound is not None:\n        h = state_bound['h']\n        bcon = state_bound['b']\n        phi_p = state_bound['phi_p']\n        del_sig = phi_p * np.sqrt(h.T @ quad.Bd @ quad.Bd.T @ h)\n\n    #if norm_w == 0:\n    #    norm_w = 1e-6\n        #w = 1e-6\n    # create constraint matrics\n    #A1 = np.array([[np.sqrt(gamma5), 0, 0],\n    #               [0, 0, 0],\n    #               [0, 0, 0]])\n    #b1 = np.array([[gamma4 / (2 * np.sqrt(gamma5))],\n    #               [np.sqrt(gamma3 - 0.25 * gamma4 ** 2 / gamma5)],\n    #               [0]])\n    A1 = np.array([[norm_w*np.sqrt(gamma5), 0, 0],\n                   [0, 0, 0],\n                   [0, 0, 0]])\n    b1 = np.array([[norm_w*gamma4 / (2 * np.sqrt(gamma5))],\n                   [norm_w*np.sqrt(gamma3 - 0.25 * gamma4 ** 2 / gamma5)],\n                   [0]])\n    #c1 = np.array([[-np.sign(w) * gamma2 / beta, 1 / 2*(norm_w * beta), 0]])\n    #c1 = np.array([[-np.sign(w) * gamma2 / beta, 1 / 2*(norm_w * beta), 0]])\n    #d1 = np.sign(w) * (v_des - gamma1)/beta + e.T @ (quad.S - quad.c3*quad.P) @ e / (2*norm_w*beta)\n    c1 = np.array([[-w * gamma2 / beta, 1 / (2* beta), 0]])\n    d1 = w * (v_des - gamma1)/beta + e.T @ (quad.S - quad.c3*quad.P) @ e / (2*beta)\n\n\n    A2 = np.array([[2 * np.sqrt(gamma2 ** 2 + gamma5), 0, 0],\n                   [0, 2 * d_weight, 0],\n                   [0, 0, -1]])\n    b2 = np.array([[0], [0], [1]])\n    c2 = np.array([[0, 0, 1]])\n    d2 = 1\n    # put into lists\n    As = {'A1': A1, 'A2': A2}\n    bs = {'b1': b1, 'b2': b2}\n    cs = {'c1': c1, 'c2': c2}\n    ds = {'d1': d1.squeeze(), 'd2': d2}\n    # Add input constraints if needed\n    if input_bound is not None:\n        A3 = np.zeros((3, 3))\n        A3[0, 0] = 1.0\n        b3 = np.zeros((3, 1))\n        c3 = np.zeros((1, 3))\n        d3 = input_bound\n        As['A3'] = A3\n        bs['b3'] = b3\n        cs['c3'] = c3\n        ds['d3'] = d3\n    if state_bound is not None:\n        Astate = np.array([[float(del_sig*np.sqrt(gamma5)), 0, 0],\n                           [0, 0, 0],\n                           [0, 0, 0]])\n        bstate = np.array([[float(del_sig*gamma4 / (2 * np.sqrt(gamma5)))],\n                           [float(del_sig*np.sqrt(gamma3 - 0.25 * gamma4 ** 2 / gamma5))],\n                           [0]])\n        cstate = np.array([[float(-h.T @ quad.Bd * gamma2), 0.0, 0.0]])\n        dstate = -h.T @ quad.Ad @ z - h.T @ quad.Bd * gamma1 + bcon\n        As['Astate'] = Astate\n        bs['bstate'] = bstate\n        cs['cstate'] = cstate\n        ds['dstate'] = float(dstate)\n\n    # define cost matrix\n    cost = np.array([[2 * gamma1 * gamma2 - 2 * gamma2 * v_des.squeeze() + gamma4, 0, 1]])\n    return cost, As, bs, cs, ds", "\n\ndef socp_solver(cost, A, b, c, d, x_init=None):\n    \"\"\"socp optimization\"\"\"\n    # optimization variable [u d f].T\n    m = len(A)\n    X = cp.Variable(3)\n    if x_init is not None:\n        X.value = x_init\n    soc_constraints = [\n        cp.SOC(c[i] @ X + d[i], A[i] @ X + b[i].squeeze()) for i in range(m)\n    ]\n    prob = cp.Problem(cp.Minimize(cost @ X), soc_constraints)\n    if x_init is not None:\n        prob.solve(warm_start=True, verbose=True)\n    else:\n        prob.solve()\n    if 'infeasible' in prob.status:\n        return None, prob\n    return X.value[0], X.value[1], prob", "\n\ndef filter_cost(u, gp_model, z, v_des):\n    \"\"\" Computes the error between the predicted mean the nominal input \"\"\"\n    m = gp_model.output_dim\n    u = u.reshape(m, 1)\n\n    query = np.hstack((z.T, u.T))\n\n    # query = np.reshape(np.array([z[0,0], u[0]]),(1,2))\n    v_query = gp_model.predict(query)\n    mean = v_query[0].numpy()\n    sigma2 = v_query[1].numpy()\n\n    cost = (u + mean - v_des).T @ (u + mean - v_des) + sigma2\n    return cost[0]  # + sigma2", "\n\ndef filter_constraint(u, gp_model, z, v_des, w, beta, V1):\n    \"\"\" Computes the probabilistic worst case derivative of the chosen lyapunov safety function\"\"\"\n    m = gp_model.output_dim\n\n    u = u.reshape(m, 1)\n    query = np.hstack((z.T, u.T))\n    v_query = gp_model.predict(query)\n    mean = v_query[0].numpy()\n    sigma2 = v_query[1].numpy()\n    V = w.dot(v_des) - (w.dot(mean) + beta * np.absolute(w) * np.sqrt(sigma2))\n\n    return 2.0 * V[0] - V1[0]", "    # return -1000\n\n\ndef safety_filter(gp_model, z, v_des, w, beta, u0, V1, eps):\n    if np.linalg.norm(w) > eps:\n        con = {'type': 'ineq', 'fun': filter_constraint, 'args': [gp_model, z, v_des, w[0], beta, V1]}\n        # res = minimize(filter_cost, x0=u0, constraints=[con], args=(gp_model, z, v_des),\n        #                method='trust-constr', options={'disp': False})\n        res = minimize(filter_cost, x0=u0, args=(gp_model, z, v_des),\n                       method='trust-constr', options={'disp': False})\n        u = res.x\n\n        if res.constr_violation > 1.0E-7:\n            success = False\n        else:\n            success = True\n\n    else:\n        res = minimize(filter_cost, x0=u0, args=(gp_model, z, v_des), options={'disp': False})\n        u = res.x\n\n        success = True\n\n    # if not(res.success):\n    #    print(\"Success: %s\" % res.success)\n    #    print(res.message)\n    return u, success", "\n\ndef unconstrained_closed_form_soln(gp_model, z, v_des):\n    query = np.hstack((z.T, np.zeros((1, 1))))\n    query = torch.from_numpy(query)\n    gamma1, gamma2, gamma3, gamma4, gamma5 = gp_model.model.compute_gammas(query)\n    v_des = torch.from_numpy(v_des).double()\n    # u = (2*gamma1*v_des - 2*gamma2*gamma3)/(gamma1**2 + gamma3**2)\n    u = -(gamma1 * gamma2 - gamma2 * v_des + 0.5 * gamma4) / (gamma2 ** 2 + gamma5)\n    return u.numpy()", "\n\ndef unconstrained_closed_form_soln_for_difference_gp(gp_model, z, v_des, quad):\n    query = np.hstack((z.T, np.zeros((1, 1))))\n    query = torch.from_numpy(query)\n    gamma1, gamma2, gamma3, gamma4, gamma5 = gp_model.model.compute_gammas(query)\n    gamma1 = gamma1 + torch.from_numpy(quad.beta(z)).double()\n    gamma2 = gamma2 + torch.from_numpy(quad.alpha(z)).double()\n    v_des = torch.from_numpy(v_des).double()\n    u = (-(gamma1 * gamma2) + gamma2 * v_des - 0.5 * gamma4) / (gamma2 ** 2 + gamma5)\n    # u = -(gamma1*(1+gamma2) - (1+gamma2)*v_des+0.5*gamma4)/((1+gamma2)**2 + gamma5)\n    # u = (-gamma1  + v_des) / (1 + gamma2)\n    # u = (-(gamma1 + torch.from_numpy(quad.beta(z)).double()) + v_des) / (gamma2 + torch.from_numpy(quad.alpha(z)).double())\n    return u.numpy()", ""]}
