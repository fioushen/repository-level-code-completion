{"filename": "src/consts.py", "chunked_list": ["\"\"\"\nconsts used throughout the project\n\"\"\"\nCONTEXT_ANSWER_SEP = \"|||\"\nCONTEXT_PREFIX = \"Context: \"\nNUMBERED_CONTEXT_PREFIX = \"Context\"\nFOLLOW_UP_PREFIX = \"Follow up: \"\nKNOWLEDGE_FOLLOW_UP_PREFIX = \"Knowledge Follow up: \"\nREASONING_FOLLOW_UP_PREFIX = \"Reasoning Follow up: \"\nFINAL_ANSWER_PREFIX = \"So the final answer is: \"", "REASONING_FOLLOW_UP_PREFIX = \"Reasoning Follow up: \"\nFINAL_ANSWER_PREFIX = \"So the final answer is: \"\nINTERMEDIATE_ANS_PREFIX = \"Intermediate answer: \"\nQUESTION_PREFIX = \"Question: \"\nCALL_API = True\nPOSITIVE_PREFIX = \"Positive: \"\nNEGATIVE_PREFIX = \"Negative: \"\nSTOP_TOKEN = \"\\n#\"\nSELF_ASK_ANSWER = \"So the final answer is: \"\nFACTOID_ANSWER_PREFIX = \"A: \"", "SELF_ASK_ANSWER = \"So the final answer is: \"\nFACTOID_ANSWER_PREFIX = \"A: \"\nLOGGING_NAME = \"logger\"\n\n# mte\nFULL_MTE_FIELD = \"multi_trace_entailment_full\"\nPRED_MTE_FIELD = \"multi_trace_entailment_prediction\"\nACC_MTE_FIELD = \"acc@mte\"\nNUM_EXAMPLES_FIELD = \"num_examples\"\nNUM_ABSTAINS_FIELD = \"num_abstains\"", "NUM_EXAMPLES_FIELD = \"num_examples\"\nNUM_ABSTAINS_FIELD = \"num_abstains\"\n\n# accuracy feilds\nACC_AT_1_FIELD = \"acc@1\"\nACC_AT_3_FIELD = \"acc@3\"\nACC_AT_MAJORITY_FIELD = \"acc@majority\"\n"]}
{"filename": "src/dataclasses.py", "chunked_list": ["from enum import Enum\nfrom typing import List, Dict\nfrom dataclasses import dataclass\nfrom src.common.config import Config\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.consts import (\n    POSITIVE_PREFIX,\n    NEGATIVE_PREFIX,\n    INTERMEDIATE_ANS_PREFIX,\n    KNOWLEDGE_FOLLOW_UP_PREFIX,", "    INTERMEDIATE_ANS_PREFIX,\n    KNOWLEDGE_FOLLOW_UP_PREFIX,\n    REASONING_FOLLOW_UP_PREFIX,\n    FINAL_ANSWER_PREFIX,\n    FOLLOW_UP_PREFIX,\n    CONTEXT_PREFIX,\n    CONTEXT_ANSWER_SEP,\n    NUMBERED_CONTEXT_PREFIX,\n)\n", ")\n\n\n# Todo: TW - Commented out because of DeBerta in background\n# from src.entailment.entailment import run_entailment\n\n\n@dataclass\nclass Example:\n    qid: str\n    question: str\n    gold_answer: str\n    prev_model_answer: str\n    metadata: Dict", "class Example:\n    qid: str\n    question: str\n    gold_answer: str\n    prev_model_answer: str\n    metadata: Dict\n\n\n@dataclass\nclass BreakDecomposition:\n    decomposition: str\n    intermediate_questions: List[str]\n    intermediate_answers: List[str]\n    final_answer: str\n    intermediate_contexts: List[str]", "@dataclass\nclass BreakDecomposition:\n    decomposition: str\n    intermediate_questions: List[str]\n    intermediate_answers: List[str]\n    final_answer: str\n    intermediate_contexts: List[str]\n\n\n@dataclass\nclass IntermediateQuestionWithAnswer:\n    \"\"\"\n    an intermediate decompotision in the tree\n    \"\"\"\n\n    intermediate_question: str\n    answer: Dict = None  # from gpt-3", "\n@dataclass\nclass IntermediateQuestionWithAnswer:\n    \"\"\"\n    an intermediate decompotision in the tree\n    \"\"\"\n\n    intermediate_question: str\n    answer: Dict = None  # from gpt-3\n", "\n\n@dataclass\nclass Question:\n    \"\"\"\n    a question object\n    \"\"\"\n\n    question: str  # question\n    decompositions: List[str]  # decompositions from gpt\n    intermediate_questions_with_answers: List[\n        IntermediateQuestionWithAnswer\n    ] = None  # intermediate steps with their answers", "\n\n@dataclass\nclass EntailmentResult:\n    entailment: float\n    neutral: float\n    contradiction: float\n\n\n@dataclass\nclass EntailmentResultWithInput:\n    premise: str\n    hypothesis: str\n    entailment_result: EntailmentResult", "\n@dataclass\nclass EntailmentResultWithInput:\n    premise: str\n    hypothesis: str\n    entailment_result: EntailmentResult\n\n\n@dataclass\nclass DecompositionStepV1:\n    question: str\n    answer: str\n    google_answer: str\n    google_answer_long: Dict\n    entailment_result_with_input: EntailmentResultWithInput\n    gpt_3_ans: str", "@dataclass\nclass DecompositionStepV1:\n    question: str\n    answer: str\n    google_answer: str\n    google_answer_long: Dict\n    entailment_result_with_input: EntailmentResultWithInput\n    gpt_3_ans: str\n\n", "\n\n@dataclass\nclass TraceResult:\n    positive: EntailmentResultWithInput\n    negative: EntailmentResultWithInput\n\n\n@dataclass\nclass StatementResult:\n    original_statement: str\n    positive: str\n    negative: str", "@dataclass\nclass StatementResult:\n    original_statement: str\n    positive: str\n    negative: str\n\n\ndef format_ir_decomposition(\n    decomposition: str, entail_facts=False, contexts_first=None\n) -> List[DecompositionStepV1]:\n    \"\"\"format decomposition and store retrieved context(s) for each step\"\"\"\n    if contexts_first is True:\n        # decomposition is in SA format where contexts are appended at beginning: 'Context1:...Context2:...'\n        for i in range(1, 5):\n            decomposition = decomposition.replace(f\"Context{i}:\\n\\n\", f\"Context{i}: \")\n    decomposition_steps = []\n    lines = decomposition.split(\"\\n\")\n    question, answer, gpt_3_ans = None, None, None\n    context = None\n    context_counter = 1 if Config().get(\"decomposition.retrieve_orig_question\") else 0\n    all_contexts = []\n    for line in lines:\n        line = (\n            line + \"  \"\n        )  # add \" \" to avoid discarding traces when the answer is empty\n        if line.startswith(NUMBERED_CONTEXT_PREFIX):\n            if contexts_first is True:\n                context = \": \".join(\n                    line.split(NUMBERED_CONTEXT_PREFIX)[1].split(\":\")[1:]\n                ).strip()\n            else:\n                context = line.split(CONTEXT_PREFIX)[1].strip()\n            all_contexts += [context]\n        if line.startswith(FOLLOW_UP_PREFIX):\n            question = line.split(FOLLOW_UP_PREFIX)[1].strip()\n        if line.startswith(FINAL_ANSWER_PREFIX):\n            decomposition_steps.append(\n                DecompositionStepV1(\n                    question=None,\n                    answer=None,\n                    google_answer=None,\n                    google_answer_long=None,\n                    entailment_result_with_input=None,\n                    gpt_3_ans=line.split(FINAL_ANSWER_PREFIX)[1].strip(),\n                )\n            )\n        if line.startswith(INTERMEDIATE_ANS_PREFIX) and question is not None:\n            answer = line.split(INTERMEDIATE_ANS_PREFIX)[1].strip()\n            answer = \"\" if answer is None else answer\n            context = \"\" if context is None else context\n            if contexts_first is True and context_counter < len(all_contexts):\n                # for contexts first, we need to align preceding contexts with intermediate Qs\n                context = all_contexts[context_counter]\n                context_counter += 1\n            context_and_answer = context + CONTEXT_ANSWER_SEP + answer\n\n            decomposition_steps.append(\n                DecompositionStepV1(\n                    question=question,\n                    answer=context_and_answer,\n                    google_answer=None,\n                    google_answer_long=None,\n                    entailment_result_with_input=None,\n                    gpt_3_ans=None,\n                )\n            )\n            question, answer = None, None\n    return decomposition_steps", "\n\ndef format_statement(statement: str) -> StatementResult:\n    \"\"\" \"\"\"\n    positive, negative = None, None\n    lines = statement.split(\"\\n\")\n    for line in lines:\n        if line.startswith(POSITIVE_PREFIX):\n            positive = line.split(POSITIVE_PREFIX)[1]\n        if line.startswith(NEGATIVE_PREFIX):\n            negative = line.split(NEGATIVE_PREFIX)[1]\n    return StatementResult(\n        original_statement=statement, positive=positive, negative=negative\n    )", "\n\ndef format_decompsition_break(\n    decomposition: BreakDecomposition,\n) -> List[DecompositionStepV1]:\n    decomposition_steps = []\n    for i in range(len(decomposition.intermediate_questions)):\n        decomposition_steps.append(\n            DecompositionStepV1(\n                question=decomposition.intermediate_questions[i],\n                answer=decomposition.intermediate_answers[i + 1],\n                google_answer=None,\n                google_answer_long=None,\n                entailment_result_with_input=None,\n                gpt_3_ans=None,\n            )\n        )\n    decomposition_steps.append(\n        DecompositionStepV1(\n            question=None,\n            answer=None,\n            google_answer=None,\n            google_answer_long=None,\n            entailment_result_with_input=None,\n            gpt_3_ans=decomposition.final_answer,\n        )\n    )\n    return decomposition_steps", "\n\ndef format_decompsition_string(\n    decomposition: str, entail_facts=False\n) -> List[DecompositionStepV1]:\n    \"\"\" \"\"\"\n    decomposition_steps = []\n    lines = decomposition.split(\"\\n\")\n    question, answer, gpt_3_ans = None, None, None\n    for line in lines:\n        if line.startswith(KNOWLEDGE_FOLLOW_UP_PREFIX):\n            question = line.split(KNOWLEDGE_FOLLOW_UP_PREFIX)[1]\n        elif line.startswith(REASONING_FOLLOW_UP_PREFIX):\n            question = line.split(REASONING_FOLLOW_UP_PREFIX)[1]\n        elif line.startswith(FOLLOW_UP_PREFIX):\n            question = line.split(FOLLOW_UP_PREFIX)[1]\n        if line.startswith(FINAL_ANSWER_PREFIX):\n            decomposition_steps.append(\n                DecompositionStepV1(\n                    question=None,\n                    answer=None,\n                    google_answer=None,\n                    google_answer_long=None,\n                    entailment_result_with_input=None,\n                    gpt_3_ans=line.split(FINAL_ANSWER_PREFIX)[1],\n                )\n            )\n        if line.startswith(INTERMEDIATE_ANS_PREFIX) and question is not None:\n            answer = line.split(INTERMEDIATE_ANS_PREFIX)[1]\n\n            # calc answer\n            gooogle_ans_short, entailment_result_with_input, google_ans_long = (\n                None,\n                None,\n                None,\n            )\n            # Todo: TW - Commented out because of DeBerta in background\n            # if entail_facts:\n            #     gooogle_ans_short, google_ans_long = google(question)\n            #     premise = question + \" \" + gooogle_ans_short\n            #     hypothesis = question + \" \" + answer\n            #     entailment = run_entailment(premise, hypothesis)\n            #     entailment_result_with_input = EntailmentResultWithInput(\n            #         premise=premise, hypothesis=hypothesis, entailment_result=entailment\n            #     )\n            decomposition_steps.append(\n                DecompositionStepV1(\n                    question=question,\n                    answer=answer,\n                    google_answer=gooogle_ans_short,\n                    google_answer_long=google_ans_long,\n                    entailment_result_with_input=entailment_result_with_input,\n                    gpt_3_ans=None,\n                )\n            )\n            question, answer = None, None\n    return decomposition_steps", "\n\ndef linearize_decompositions(decompositions: List[DecompositionStepV1]) -> str:\n    \"\"\" \"\"\"\n    res = \"\"\n    valid_decompositions = [d for d in decompositions if d.question]\n    for decomp in valid_decompositions:\n        res += decomp.question + \" \" + decomp.answer + \"\\n\"\n    print(res)\n    return res", "\n\ndef linearize_decompositions_google(decompositions: List[DecompositionStepV1]) -> str:\n    \"\"\" \"\"\"\n    res = \"\"\n    valid_decompositions = [d for d in decompositions if d.question]\n    for decomp in valid_decompositions:\n        if decomp.google_answer is not None:\n            res += decomp.question + \" \" + decomp.google_answer + \"\\n\"\n    print(res)\n    return res", "\n\n@dataclass\nclass QuestionV1:\n    \"\"\"\n    a question object\n    \"\"\"\n\n    question: str  # original question\n    statement: StatementResult  # the parsed statement after calling gpt-3\n    decompositions: List[str]  # decompositions from gpt-3 self-ask\n    decompsition_steps: List[List[DecompositionStepV1]]  # reasoning traces\n    traces_entailments: List[List[TraceResult]]\n\n    def __init__(\n        self,\n        question: str,\n        prompt: str,\n        gpt3_accessor: GptAccessor,\n        model: str,\n        num_decompositions: int = 5,\n    ):\n        self.question = question\n        self.send_question_separately = Config().get(\n            \"decomposition.send_question_separately\"\n        )\n        self.gpt_accessor_indices_with_temperature_0 = Config().get(\n            \"decomposition.gpt_accessor_indices_with_temperature_0\"\n        )\n        self.model = model\n        self.curr_prompt = (\n            prompt if self.send_question_separately else prompt + self.question\n        )\n        self.num_decompositions = num_decompositions\n        self.gpt3_accessor = gpt3_accessor\n        self.statement = None\n        self.decompositions = None\n        self.decompsition_steps = None\n        self.traces_entailments = None\n        self.traces_entailments_google = None\n\n    # Todo: TW - Commented out because of DeBerta in background\n    # def _get_entailment_res(\n    #     self, decomposition_linearized: str, statement: str\n    # ) -> EntailmentResultWithInput:\n    #     entailment = run_entailment(\n    #         premise=decomposition_linearized, hypothesis=statement\n    #     )\n    #     entailment_result_with_input = EntailmentResultWithInput(\n    #         premise=decomposition_linearized,\n    #         hypothesis=statement,\n    #         entailment_result=entailment,\n    #     )\n    #     return entailment_result_with_input\n\n    def populate_statement(self):\n        # gpt_statement_res = call_gpt_statement_generatior(\n        #     statement_generation_prompt + self.question, \"\"\n        # )\n        gpt_statement_res = \"\"\n        self.statement = format_statement(gpt_statement_res)\n\n    def populate_decompositions(self):\n        def get_temp_at_index(index: int) -> float:\n            # check if the index should get temp 0\n            if (\n                self.gpt_accessor_indices_with_temperature_0 is not None\n                and index in self.gpt_accessor_indices_with_temperature_0\n            ):\n                return 0\n            return Config().get(\"decomposition.gpt3_accessor_temperature\")\n\n        print(\"\\nRunning decomposition + retrieval models\")\n        self.decompositions = [\n            self.gpt3_accessor.call_gpt(\n                self.curr_prompt, \"\", get_temp_at_index(i), self.model, i\n            )\n            if not self.send_question_separately\n            else self.gpt3_accessor.call_gpt(\n                self.curr_prompt, \"\", get_temp_at_index(i), self.question, self.model, i\n            )\n            for i in range(self.num_decompositions)\n        ]\n\n    def populate_decomposition_steps(self):\n        # print(\"\\npopulating_decompositions_steps\")\n        self.decompsition_steps = [\n            format_decompsition_break(decomposition)\n            if type(decomposition) == BreakDecomposition\n            else format_decompsition_string(decomposition)\n            for decomposition in self.decompositions\n        ]\n\n    def populate_trace_entailments(self):\n        print(\"\\npopulate_trace_entailments\")\n        trace_results = [\n            [\n                TraceResult(\n                    positive=self._get_entailment_res(\n                        linearize_decompositions(decompsition[: i + 1]),\n                        self.statement.positive,\n                    ),\n                    negative=self._get_entailment_res(\n                        linearize_decompositions(decompsition[: i + 1]),\n                        self.statement.negative,\n                    ),\n                )\n                for i in range(len(decompsition))\n            ]\n            for decompsition in self.decompsition_steps\n        ]\n        self.traces_entailments = trace_results\n\n        print(\"\\npopulate_trace_entailments_google\")\n        self.traces_entailments_google = []\n\n    def populate(self):\n        \"\"\" \"\"\"\n        self.populate_statement()\n        self.populate_decompositions()\n        self.populate_decomposition_steps()", "        # Todo: TW - Commented out because of DeBerta in background\n        # self.populate_trace_entailments()\n\n\nclass Answer(str, Enum):\n    Yes = \"YES\"\n    No = \"NO\"\n    MaybeYes = \"MaybeYes\"\n    MaybeNo = \"MaybeNo\"\n    Donno = \"Donno\"", "\n\n@dataclass\nclass QuestionWithAnswer:\n    question: QuestionV1\n    answers: Dict[str, Answer]\n    gpt_answers: List[str]\n"]}
{"filename": "src/dataset_readers/dataset_readers_factory.py", "chunked_list": ["from typing import Dict, Type\n\nfrom src.common.abstract_factory import AbstractFactory\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\nfrom src.dataset_readers.readers.fermi_reader import FermiReader\n\n# from src.dataset_readers.readers.feverous_reader import FeverousDataReader\n# from src.dataset_readers.readers.hotpotqa_reader import HotpotQADataReader\n# from src.dataset_readers.readers.hover_reader import HoverDataReader\n# from src.dataset_readers.readers.quartz_reader import QuartzDataReader", "# from src.dataset_readers.readers.hover_reader import HoverDataReader\n# from src.dataset_readers.readers.quartz_reader import QuartzDataReader\n# from src.dataset_readers.readers.bamboogle_reader import BamboogleDataReader\nfrom src.dataset_readers.readers.strategy_qa import StrategyQADataReader\nfrom src.dataset_readers.readers.wikihop_reader import WikiHopDataReader\n\n# from src.info_seeking_questions.dataset_reader import DatasetReader\n\n\nclass DatasetReadersFactory(AbstractFactory):\n    \"\"\" \"\"\"\n\n    def get_instance_name_to_class_dict(self) -> Dict[str, Type[DatasetReader]]:\n        return {\n            \"strategy_qa\": StrategyQADataReader,\n            # \"bamboogle\": BamboogleDataReader,\n            # \"quartz\": QuartzDataReader,\n            \"wikihop\": WikiHopDataReader,\n            # \"feverous\": FeverousDataReader,\n            # \"hover\": HoverDataReader,\n            # \"hotpotqa\": HotpotQADataReader,\n            # \"fermi\": FermiReader,\n        }", "\nclass DatasetReadersFactory(AbstractFactory):\n    \"\"\" \"\"\"\n\n    def get_instance_name_to_class_dict(self) -> Dict[str, Type[DatasetReader]]:\n        return {\n            \"strategy_qa\": StrategyQADataReader,\n            # \"bamboogle\": BamboogleDataReader,\n            # \"quartz\": QuartzDataReader,\n            \"wikihop\": WikiHopDataReader,\n            # \"feverous\": FeverousDataReader,\n            # \"hover\": HoverDataReader,\n            # \"hotpotqa\": HotpotQADataReader,\n            # \"fermi\": FermiReader,\n        }", ""]}
{"filename": "src/dataset_readers/readers/hotpotqa_reader.py", "chunked_list": ["from typing import Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\n\nclass HotpotQADataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(\n        self,\n        dataset_path=\"../../../data/full_datasets/hotpotqa/hotpot_dev_fullwiki_v1.json\",\n    ):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.load_json(self.dataset_path)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        return Example(\n            qid=example[\"_id\"],\n            question=example[\"question\"],\n            gold_answer=example[\"answer\"],\n            prev_model_answer=None,\n            metadata=example,\n        )", "\n\n# reader = HotpotQADataReader()\n# reader.read()\n# for ex in reader.get_examples()[:10]:\n#     x = reader.parse_example(ex)\n#     print(x)\n#     print(\"*\"*20)\n# print(len(reader.get_examples()))\n", "# print(len(reader.get_examples()))\n"]}
{"filename": "src/dataset_readers/readers/wikihop_reader.py", "chunked_list": ["from typing import Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\n\nclass WikiHopDataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(\n        self,\n        dataset_path=\"data/test_datasets/2wikihop/dev.json\",\n    ):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.load_json(self.dataset_path)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        # Manually corrected wrong answers in Bamboogle\n        return Example(\n            qid=example[\"_id\"],\n            question=example[\"question\"],\n            gold_answer=example[\"answer\"],\n            prev_model_answer=None,\n            metadata=example,\n        )", ""]}
{"filename": "src/dataset_readers/readers/dataset_reader.py", "chunked_list": ["class DatasetReader:\n    \"\"\"\n    info-seeking dataset reader\n    \"\"\"\n\n    def __init__(self, dataset_path):\n        self.dataset_path = dataset_path\n\n    def read(self, rand_sample=None):\n        raise NotImplementedError(\"Please Implement this method\")\n\n    def parse_example(self, example):\n        raise NotImplementedError(\"Please Implement this method\")", ""]}
{"filename": "src/dataset_readers/readers/strategy_qa.py", "chunked_list": ["from typing import Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\n\nclass StrategyQADataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(self, dataset_path=\"../../../data/full_datasets/strategyqa/sqa.csv\"):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.read_csv_to_dict(self.dataset_path)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        return Example(\n            qid=example[\"qid\"],\n            question=example[\"question\"],\n            gold_answer=example[\"gold_answer\"],\n            prev_model_answer=example[\"model_answer\"],\n            metadata=example,\n        )", ""]}
{"filename": "src/dataset_readers/readers/quartz_reader.py", "chunked_list": ["from typing import List, Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.info_seeking_questions.dataset_reader import DatasetReader\n\n\nclass QuartzDataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(\n        self, dataset_path=\"data/test_datasets/quartz-dataset-v1-aug2019/dev.jsonl\"\n    ):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.load_jsonl(self.dataset_path)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        def _format_choices(choices: List[Dict[str, str]]) -> str:\n            return \" \".join(\n                [\n                    f'{choice[\"label\"]}. {choice[\"text\"]}'\n                    for choice in example[\"question\"][\"choices\"]\n                ]\n            )\n\n        return Example(\n            qid=example[\"id\"],\n            question=example[\"question\"][\"stem\"]\n            + \" \"\n            + _format_choices(example[\"question\"][\"choices\"]),\n            gold_answer=example[\"answerKey\"],\n            prev_model_answer=None,\n            metadata=example,\n        )", ""]}
{"filename": "src/dataset_readers/readers/fermi_reader.py", "chunked_list": ["from typing import List, Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\nfrom src.serpapi.serpapi import get_string_hash\n\n\nclass FermiReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(self, dataset_path=\"data/test_datasets/fermi/train_realfp.json\"):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.train_examples = dataset_utils.load_json(self.dataset_path)\n        self.dev_examples = dataset_utils.load_json(\n            self.dataset_path.replace(\"train\", \"val\")\n        )\n        unique_questions, self.examples = set(), []\n        for x in self.train_examples + self.dev_examples:\n            if x[\"question\"] not in unique_questions:\n                unique_questions.add(x[\"question\"])\n                self.examples.append(x)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        def _format_question(example: Dict) -> str:\n            # check if the there's a $\n            if \"$\" in example[\"answer\"]:\n                return f'{example[\"question\"]} Unit: $.'\n\n            # else check the unit after the space\n            unit_split = example[\"answer\"].split(\" \")\n            if len(unit_split) == 1:\n                return example[\"question\"]\n            else:\n                return f'{example[\"question\"]} Unit: {unit_split[-1]}.'\n\n        return Example(\n            qid=get_string_hash(example[\"question\"]),\n            question=_format_question(example),\n            gold_answer=example[\"answer\"],\n            prev_model_answer=None,\n            metadata=example,\n        )", "class FermiReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(self, dataset_path=\"data/test_datasets/fermi/train_realfp.json\"):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.train_examples = dataset_utils.load_json(self.dataset_path)\n        self.dev_examples = dataset_utils.load_json(\n            self.dataset_path.replace(\"train\", \"val\")\n        )\n        unique_questions, self.examples = set(), []\n        for x in self.train_examples + self.dev_examples:\n            if x[\"question\"] not in unique_questions:\n                unique_questions.add(x[\"question\"])\n                self.examples.append(x)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        def _format_question(example: Dict) -> str:\n            # check if the there's a $\n            if \"$\" in example[\"answer\"]:\n                return f'{example[\"question\"]} Unit: $.'\n\n            # else check the unit after the space\n            unit_split = example[\"answer\"].split(\" \")\n            if len(unit_split) == 1:\n                return example[\"question\"]\n            else:\n                return f'{example[\"question\"]} Unit: {unit_split[-1]}.'\n\n        return Example(\n            qid=get_string_hash(example[\"question\"]),\n            question=_format_question(example),\n            gold_answer=example[\"answer\"],\n            prev_model_answer=None,\n            metadata=example,\n        )", ""]}
{"filename": "src/dataset_readers/readers/bamboogle_reader.py", "chunked_list": ["from typing import Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\n\nclass BamboogleDataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(\n        self,\n        dataset_path=\"data/test_datasets/bamboogle_prerelease_random.csv\",\n    ):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.read_csv_to_dict(self.dataset_path)\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        # Manually corrected wrong answers in Bamboogle\n        corrected_ans = (\n            example[\"Answer_corrected\"]\n            if example[\"Answer_corrected\"] != \"\"\n            else example[\"Answer\"]\n        )\n        return Example(\n            qid=[i for i, x in enumerate(self.examples) if x == example][0],\n            question=example[\"Question\"],\n            gold_answer=corrected_ans,\n            prev_model_answer=None,\n            metadata=example,\n        )", ""]}
{"filename": "src/dataset_readers/readers/feverous_reader.py", "chunked_list": ["from typing import Dict\n\nfrom src.common import dataset_utils\nfrom src.dataclasses import Example\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\n\nPROMPT_CLAIMS = [\n    {\n        \"id\": 1,\n        \"label\": \"Yes\",", "        \"id\": 1,\n        \"label\": \"Yes\",\n        \"claim\": \"Albert Foday, an attacking midfielder and free kick expert (a method of restarting play after an infringement of the laws by the opponent) plays on Kallon FC, which team he played on in 2002-2003 before going to the Mighty Barolle for two years before his return.\",\n    },\n    {\n        \"id\": 2,\n        \"label\": \"Yes\",\n        \"claim\": \"Shattered Angels (a Japanese Manga) had five Absolute Angels, one of which was Cla\u00c3\\xadomh Solais.\",\n    },\n    {", "    },\n    {\n        \"id\": 3,\n        \"label\": \"No\",\n        \"claim\": \"KLLB was a radio station owned by Ford Motor Corporation and licensed in West Jordan, Utah until 2017.\",\n    },\n    {\n        \"id\": 4,\n        \"label\": \"Yes\",\n        \"claim\": \"Mikro globulus belongs to Kingdom: Animalia, Phylum: Gastropoda, Class: Mollusca, and Subclass: Vetigastropoda.\",", "        \"label\": \"Yes\",\n        \"claim\": \"Mikro globulus belongs to Kingdom: Animalia, Phylum: Gastropoda, Class: Mollusca, and Subclass: Vetigastropoda.\",\n    },\n    {\n        \"id\": 5,\n        \"label\": \"No\",\n        \"claim\": \"Valeri Karpin played for CSKA Moscow in the 1986 Soviet Second League season.\",\n    },\n    {\n        \"id\": 6,", "    {\n        \"id\": 6,\n        \"label\": \"Yes\",\n        \"claim\": \"2016 BBC Sports Personality of the Year Award was won by Andy Murray of Tennis (who ranked world No. 1 by the Association of Tennis Professionals (ATP) for 41 weeks, and finished as the year-end No. 1 in 2016), garnering 33.1% of the votes ahead of other sports personalities such as Alistair Brownlee, Nick Skelton, and Mo Farah.\",\n    },\n    {\n        \"id\": 7,\n        \"label\": \"No\",\n        \"claim\": \"Ecstasea is a 585 tonnes yacht built in the Cayman Islands, an autonomous British Overseas Territory in the western Red Sea.\",\n    },", "        \"claim\": \"Ecstasea is a 585 tonnes yacht built in the Cayman Islands, an autonomous British Overseas Territory in the western Red Sea.\",\n    },\n    {\n        \"id\": 8,\n        \"label\": \"Yes\",\n        \"claim\": \"The DOHC Genesis engine has five valves per cylinder as Yamaha adopted the 5-valve concept because it allowed both excellent volumetric efficiency and high rpm.\",\n    },\n    {\n        \"id\": 9,\n        \"label\": \"No\",", "        \"id\": 9,\n        \"label\": \"No\",\n        \"claim\": \"Two episodes of Das unsichtbare Visier are Das Nest im Urwald, which is 89 minutes long, and Depot im Skagerrak, which is 117 minutes long.\",\n    },\n    {\n        \"id\": 10,\n        \"label\": \"No\",\n        \"claim\": \"Robert Craig Kent, born on November 28, 1828 in Wythe County, Virginia, was the 16th Lieutenant Governor of Virginia.\",\n    },\n]", "    },\n]\n\n\nclass FeverousDataReader(DatasetReader):\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def __init__(\n        self,\n        dataset_path=\"../../../data/full_datasets/feverous/feverous_dev_challenges.jsonl\",\n    ):\n        super().__init__(dataset_path=dataset_path)\n        self.examples = None\n\n    def read(self, rand_sample=None):\n        self.examples = dataset_utils.load_jsonl(self.dataset_path)\n        # self.examples = PROMPT_CLAIMS  # todo: delete\n\n    def get_examples(self):\n        return self.examples\n\n    def parse_example(self, example: Dict) -> Example:\n        claim_to_question = f\"Is it true that {example['claim']}?\".replace(\".?\", \"?\")\n        gold_answer = \"Yes\" if example[\"label\"] == \"SUPPORTS\" else \"No\"\n        return Example(\n            qid=example[\"id\"],\n            question=claim_to_question,\n            gold_answer=gold_answer,\n            prev_model_answer=None,\n            metadata=example,\n        )", "\n\n#\n# reader = FeverousDataReader()\n# reader.read()\n# for ex in reader.get_examples()[:10]:\n#     x = reader.parse_example(ex)\n#     print(x)\n#     print(\"*\"*20)\n# print(len(reader.get_examples()))", "#     print(\"*\"*20)\n# print(len(reader.get_examples()))\n"]}
{"filename": "src/serpapi/serpapi.py", "chunked_list": ["import hashlib\nimport os\nimport json\nfrom typing import Dict\n\n\nfrom IPython.utils import io\nfrom serpapi import GoogleSearch\n\nfrom src.common.config import Config", "\nfrom src.common.config import Config\nfrom src.serpapi.wikipedia import get_wikipedia_text\n\n\nREAD_CACHE = True\n\n\ndef google(question):\n    print(f\"Asking google: {question}\")\n\n    params = {\n        \"api_key\": os.getenv(\"SERP_API_KEY\"),\n        \"engine\": \"google\",\n        \"q\": question,\n        \"google_domain\": \"google.com\",\n        \"gl\": \"us\",\n        \"hl\": \"en\",\n    }\n\n    with io.capture_output() as captured:  # disables prints from GoogleSearch\n        print(\"hi man what's up?\")\n        search = GoogleSearch(params)\n        res = search.get_dict()\n\n    answer = None\n    snippet = None\n    title = None\n\n    if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n        answer = res[\"answer_box\"][\"answer\"]\n    if \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n        snippet = res[\"answer_box\"][\"snippet\"]\n        title = res[\"answer_box\"][\"title\"]\n    # elif 'answer_box' in res.keys() and 'snippet_highlighted_words' in res['answer_box'].keys():\n    #     toret = res['answer_box'][\"snippet_highlighted_words\"][0]\n    elif (\n        \"answer_box\" in res.keys()\n        and \"contents\" in res[\"answer_box\"].keys()\n        and \"table\" in res[\"answer_box\"][\"contents\"].keys()\n    ):\n        snippet = res[\"answer_box\"][\"contents\"][\"table\"]\n        title = res[\"answer_box\"][\"title\"]\n    elif \"answer_box\" in res.keys() and \"list\" in res[\"answer_box\"].keys():\n        snippet = res[\"answer_box\"][\"list\"]\n        title = res[\"answer_box\"][\"title\"]\n    elif \"organic_results\" in res and \"snippet\" in res[\"organic_results\"][0].keys():\n        snippet = res[\"organic_results\"][0][\"snippet\"]\n        title = res[\"organic_results\"][0][\"title\"]\n    elif (\n        \"organic_results\" in res\n        and \"rich_snippet_table\" in res[\"organic_results\"][0].keys()\n    ):\n        snippet = res[\"organic_results\"][0][\"rich_snippet_table\"]\n        title = res[\"organic_results\"][0][\"title\"]\n    else:\n        snippet = None\n    if snippet is not None:\n        title = title.replace(\"- Wikipedia\", \"\").strip()\n        toret = f\"{title}: {snippet}\"\n        toret = f\"{toret} So the answer is {answer}.\" if answer is not None else toret\n    else:\n        toret = \"\"\n    return [toret, res]", "def google(question):\n    print(f\"Asking google: {question}\")\n\n    params = {\n        \"api_key\": os.getenv(\"SERP_API_KEY\"),\n        \"engine\": \"google\",\n        \"q\": question,\n        \"google_domain\": \"google.com\",\n        \"gl\": \"us\",\n        \"hl\": \"en\",\n    }\n\n    with io.capture_output() as captured:  # disables prints from GoogleSearch\n        print(\"hi man what's up?\")\n        search = GoogleSearch(params)\n        res = search.get_dict()\n\n    answer = None\n    snippet = None\n    title = None\n\n    if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n        answer = res[\"answer_box\"][\"answer\"]\n    if \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n        snippet = res[\"answer_box\"][\"snippet\"]\n        title = res[\"answer_box\"][\"title\"]\n    # elif 'answer_box' in res.keys() and 'snippet_highlighted_words' in res['answer_box'].keys():\n    #     toret = res['answer_box'][\"snippet_highlighted_words\"][0]\n    elif (\n        \"answer_box\" in res.keys()\n        and \"contents\" in res[\"answer_box\"].keys()\n        and \"table\" in res[\"answer_box\"][\"contents\"].keys()\n    ):\n        snippet = res[\"answer_box\"][\"contents\"][\"table\"]\n        title = res[\"answer_box\"][\"title\"]\n    elif \"answer_box\" in res.keys() and \"list\" in res[\"answer_box\"].keys():\n        snippet = res[\"answer_box\"][\"list\"]\n        title = res[\"answer_box\"][\"title\"]\n    elif \"organic_results\" in res and \"snippet\" in res[\"organic_results\"][0].keys():\n        snippet = res[\"organic_results\"][0][\"snippet\"]\n        title = res[\"organic_results\"][0][\"title\"]\n    elif (\n        \"organic_results\" in res\n        and \"rich_snippet_table\" in res[\"organic_results\"][0].keys()\n    ):\n        snippet = res[\"organic_results\"][0][\"rich_snippet_table\"]\n        title = res[\"organic_results\"][0][\"title\"]\n    else:\n        snippet = None\n    if snippet is not None:\n        title = title.replace(\"- Wikipedia\", \"\").strip()\n        toret = f\"{title}: {snippet}\"\n        toret = f\"{toret} So the answer is {answer}.\" if answer is not None else toret\n    else:\n        toret = \"\"\n    return [toret, res]", "\n\ndef get_sentences(text, max_num_sentences, reverse=None):\n    if text == \"\":\n        return text\n    sentences = text.split(\". \")\n    ret_sentences = \"\"\n    actual_num_sentences = min(max_num_sentences, len(sentences))\n    if reverse:\n        for i in reversed(range(actual_num_sentences)):\n            ret_sentences += f\"{sentences[i]}. \"\n    else:\n        for i in range(actual_num_sentences):\n            ret_sentences += f\"{sentences[i]}. \"\n    return ret_sentences.strip()", "\n\ndef get_first_sentences(text, max_num_sentences):\n    return get_sentences(text, max_num_sentences)\n\n\ndef get_last_sentences(text, max_num_sentences):\n    return get_sentences(text, max_num_sentences, reverse=True)\n\n\ndef get_snippet_wiki_paragraph(wikipage_title, snippet):\n    full_wikipage_text = get_wikipedia_text(wikipage_title)\n    print(f\"Wikipedia title: {wikipage_title}\")\n    print(f\"Google snippet: {snippet}\")\n    try:\n        assert snippet in full_wikipage_text\n        text_before_snippet, text_after_snippet = full_wikipage_text.split(snippet)\n        prev_sentences = get_last_sentences(text_before_snippet, 5).strip()\n        next_sentences = get_first_sentences(text_after_snippet, 5).strip()\n        return f\"{prev_sentences} {snippet} {next_sentences}\"\n    except AssertionError:\n        print(\"* Unable to find snippet in Wikipedia text, return original snippet.\")\n    return snippet", "\n\ndef get_snippet_wiki_paragraph(wikipage_title, snippet):\n    full_wikipage_text = get_wikipedia_text(wikipage_title)\n    print(f\"Wikipedia title: {wikipage_title}\")\n    print(f\"Google snippet: {snippet}\")\n    try:\n        assert snippet in full_wikipage_text\n        text_before_snippet, text_after_snippet = full_wikipage_text.split(snippet)\n        prev_sentences = get_last_sentences(text_before_snippet, 5).strip()\n        next_sentences = get_first_sentences(text_after_snippet, 5).strip()\n        return f\"{prev_sentences} {snippet} {next_sentences}\"\n    except AssertionError:\n        print(\"* Unable to find snippet in Wikipedia text, return original snippet.\")\n    return snippet", "\n\ndef get_question_wiki_snippet(question, cache=None):\n    # google_wikipedia_query = f\"site:en.wikipedia.org '{question}'\"\n    try:\n        cached_query_results = read_google_res_from_cache(query=question)\n        snippet = cached_query_results[\"snippet\"]\n        print(f\"Read from cache for query: {question}, cached snippet: {snippet}\")\n    except IOError:\n        google_wikipedia_query = (\n            f\"en.wikipedia.org {question}\"  # same as Ori's query format\n        )\n        snippet, full_results = google(google_wikipedia_query)\n        # print(f\"full_results: {full_results}\")\n        # print(f\"snippet: {snippet}\")\n        if cache:\n            print(f\"Caching snippet: {snippet}\")\n            cache_google_res(\n                question, {\"snippet\": snippet, \"full_results\": full_results}\n            )\n    clean_snippet = snippet.replace(\"...\", \"\").strip()\n    return clean_snippet", "\n\ndef get_question_google_snippet(question, cache=None):\n    # google_wikipedia_query = f\"site:en.wikipedia.org '{question}'\"\n    try:\n        cached_query_results = read_google_res_from_cache(query=question)\n        snippet = cached_query_results[\"snippet\"]\n        print(f\"Read from cache for query: {question}, cached snippet: {snippet}\")\n    except IOError:\n        google_wikipedia_query = f\"{question}\"  # same as Ori's query format\n        snippet, full_results = google(google_wikipedia_query)\n        # print(f\"full_results: {full_results}\")\n        # print(f\"snippet: {snippet}\")\n        if cache:\n            print(f\"Caching snippet: {snippet}\")\n            cache_google_res(\n                question, {\"snippet\": snippet, \"full_results\": full_results}\n            )\n    clean_snippet = snippet.replace(\"...\", \"\").strip()\n    return clean_snippet", "\n\ndef get_string_hash(query: str) -> str:\n    return hashlib.md5(query.encode()).hexdigest()\n\n\ndef cache_google_res(query: str, res: Dict) -> None:\n    \"\"\"\"\"\"\n    filename = get_string_hash(query)\n    retriever_cache_dir = Config().get(\"decomposition.retriever_cache_dir\")\n    with open(f\"{retriever_cache_dir}/{filename}.json\", \"w\") as json_file:\n        json.dump(res, json_file)", "    # with open(f\"strategy_qa/google_results_2/{filename}.json\", \"w\") as json_file:\n    #     json.dump(res, json_file)\n\n\ndef read_google_res_from_cache(query: str) -> Dict:\n    filename = get_string_hash(query)\n    retriever_cache_dir = Config().get(\"decomposition.retriever_cache_dir\")\n    with open(f\"{retriever_cache_dir}/{filename}.json\", \"r\") as f:\n        data = json.load(f)\n    # with open(f\"strategy_qa/google_results_2/{filename}.json\", \"r\") as f:\n    #     data = json.load(f)\n    return data", ""]}
{"filename": "src/serpapi/wikipedia.py", "chunked_list": ["import wikipedia\nimport re\n\n\ndef get_wikipedia_text(page_name):\n    def clean(text):\n        text = re.sub(r\"==.*?==+\", \"\", text)\n        text = text.replace(\"\\n\", \"\")\n        return text\n\n    def remove_parentheses_from_first_sent(text):\n        \"\"\"remove first sentence parentheses as the info contains redundant spelling info\"\"\"\n\n    # Specify the title of the Wikipedia page\n    wiki = wikipedia.page(page_name)\n    # Extract the plain text content of the page\n    page_text = wiki.content\n    return clean(page_text)", ""]}
{"filename": "src/common/dataset_utils.py", "chunked_list": ["import csv\nfrom tqdm import tqdm\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n\ndef file_as_string(file_path):\n    with open(file_path, \"r\") as file:\n        file_str = file.read()\n    return file_str.strip()", "\n\ndef timestamp_string():\n    now = datetime.now()\n    now = str(now).split(\".\")[0].strip()\n    return str(now).replace(\" \", \"_\").replace(\":\", \"-\")\n\n\ndef chunk_list_to_sublists(original_list, chunk_size):\n    chunked_list = list()\n    for i in range(0, len(original_list), chunk_size):\n        chunked_list.append(original_list[i : i + chunk_size])\n    return chunked_list", "def chunk_list_to_sublists(original_list, chunk_size):\n    chunked_list = list()\n    for i in range(0, len(original_list), chunk_size):\n        chunked_list.append(original_list[i : i + chunk_size])\n    return chunked_list\n\n\ndef read_csv_to_dict(csv_file, encoding=None):\n    # replace NaN values (empty cell) with \"\"\n    dict_from_csv = (\n        pd.read_csv(csv_file, encoding=encoding).fillna(\"\").to_dict(\"records\")\n    )\n    return dict_from_csv", "\n\ndef write_to_json(data, json_file, encoding=None):\n    encoding = \"utf-8\" if encoding is None else encoding\n    with open(json_file, mode=\"w+\", encoding=encoding) as file:\n        json.dump(data, file, indent=4)\n    return True\n\n\ndef load_json(filepath, encoding=None):\n    encoding = \"utf-8\" if encoding is None else encoding\n    with open(filepath, mode=\"r\", encoding=encoding) as reader:\n        text = reader.read()\n    return json.loads(text)", "\ndef load_json(filepath, encoding=None):\n    encoding = \"utf-8\" if encoding is None else encoding\n    with open(filepath, mode=\"r\", encoding=encoding) as reader:\n        text = reader.read()\n    return json.loads(text)\n\n\ndef load_jsonl(filepath, encoding=None):\n    encoding = \"utf-8\" if encoding is None else encoding\n    with open(filepath, \"r\", encoding=encoding) as reader:\n        data = [json.loads(line) for line in reader]\n    return data", "def load_jsonl(filepath, encoding=None):\n    encoding = \"utf-8\" if encoding is None else encoding\n    with open(filepath, \"r\", encoding=encoding) as reader:\n        data = [json.loads(line) for line in reader]\n    return data\n\n\ndef jaccard_similarity(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(list1) + len(list2)) - intersection\n    return float(intersection) / union", "\n\ndef remove_duplicates_from_list(items_list):\n    return list(dict.fromkeys(items_list))\n\n\ndef write_dict_list_to_csv(dict_list, dict_keys_list, output_csv):\n    csv_columns = dict_keys_list\n    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n        writer.writeheader()\n        for i in tqdm(range(len(dict_list))):\n            data = dict_list[i]\n            writer.writerow(data)\n    return True", ""]}
{"filename": "src/common/config.py", "chunked_list": ["global logger\nimport datetime\nimport json\nimport logging\n\n\ntracer = logging.getLogger(\"config\")\ntracer.setLevel(logging.CRITICAL)  # or desired level\ntracer.addHandler(logging.FileHandler(\"indexer.log\"))\n", "tracer.addHandler(logging.FileHandler(\"indexer.log\"))\n\n\nclass Config:\n    \"\"\"A python singleton\"\"\"\n\n    # Usage example\n    # Config().write_log('INFO', 'train_metric', context_dict=elastic_train_metrics)\n\n    class __impl:\n        \"\"\"Implementation of the singleton interface\"\"\"\n\n        def __init__(self):\n            self._start_time = datetime.datetime.utcnow().strftime(\"%d-%m-%y_%H:%M\")\n            self._config = {\"config_path\": \"\", \"use_wandb\": False, \"use_elastic\": False}\n\n        def load(self, config_path):\n            \"\"\"\n            Loads a full config jsonnet\n            \"\"\"\n            with open(config_path) as f:\n                self._config.update(json.load(f))\n\n        def override_dict(self, new_config):\n            \"\"\"\n            Adds a value to the config if missing, or overrides if already in config\n            key: string path in config separated by '.' e.g. training_arguments.num_train_epochs\n            \"\"\"\n            for key, val in new_config.items():\n                tokens = key.split(\".\")\n                config_internal = self._config\n                for token in tokens[:-1]:\n                    if token in config_internal:\n                        config_internal = config_internal[token]\n                    else:\n                        raise (ValueError(f\"could not find {key} in config!\"))\n\n                if tokens[-1] in config_internal:\n                    if val is not None:\n                        if val == \"False\":\n                            config_internal[tokens[-1]] = False\n                        elif val == \"True\":\n                            config_internal[tokens[-1]] = True\n                        else:\n                            config_internal[tokens[-1]] = val\n                else:\n                    raise (ValueError(f\"could not find {key} in config!\"))\n\n        def add_value(self, key, value):\n            \"\"\"\n            Adds a value to the config if missing, or overrides if already in config\n            key: string path in config separated by '.' e.g. training_arguments.num_train_epochs\n            \"\"\"\n            pass\n\n        def iter_on_config(self, d, key, full_key=\"\"):\n            entries = []\n            vals = []\n            val = None\n            for k, v in d.items():\n                if isinstance(v, dict):\n                    if key in v:\n                        vals.append(v[key])\n                        keys.append()\n                        return keys, vals\n                    else:\n                        keys, vals = self.iter_on_config(v, full_key)\n                        for i_k, i_v in zip(key, val):\n                            vals.append(val)\n\n            return full_key + \".\" + key, val\n\n        def get(self, key):\n            \"\"\"\n            Adds a value to the config if missing, or overrides if already in config\n            \"\"\"\n            tokens = key.split(\".\")\n            result = self._config\n            for token in tokens[:-1]:\n                if token in result:\n                    result = result[token]\n            if tokens[-1] in result:\n                return result[tokens[-1]]\n            else:\n                # return self.iter_on_config(self._config, key)\n                tracer.info(f\"could not find {key} in config, return none!\")\n                return None\n\n            return result\n\n    # storage for the instance reference\n    __instance = None\n\n    def __init__(self):\n        \"\"\"Create singleton instance\"\"\"\n        # Check whether we already have an instance\n        if Config.__instance is None:\n            # Create and remember instance\n            Config.__instance = Config.__impl()\n\n        # Store instance reference as the only member in the handle\n        self.__dict__[\"_Singleton__instance\"] = Config.__instance\n\n    def __getattr__(self, attr):\n        \"\"\"Delegate access to implementation\"\"\"\n        return getattr(self.__instance, attr)\n\n    def __setattr__(self, attr, value):\n        \"\"\"Delegate access to implementation\"\"\"\n        return setattr(self.__instance, attr, value)", ""]}
{"filename": "src/common/logger.py", "chunked_list": ["import logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\n\ndef get_logger(LOGGING_NAME=None):\n    return logging.getLogger(LOGGING_NAME)\n"]}
{"filename": "src/common/abstract_factory.py", "chunked_list": ["from abc import ABC\nfrom threading import Lock\nfrom typing import Dict, TypeVar, Generic\n\nfrom src.common.logger import get_logger\n\nlogger = get_logger()\n\nT = TypeVar(\"T\")\n", "T = TypeVar(\"T\")\n\n\nclass AbstractFactory(ABC, Generic[T]):\n    \"\"\"\n    Abstract class to represent a factory.\n    The concrete class factory should only implement the method `get_instance_name_to_class_dict`.\n    By default, Each instance is kept one time (\\singleton), unless one is passing `use_cache=False`.\n    \"\"\"\n\n    def __init__(self):\n        self._instances_dict: Dict[str, T] = {}\n        self._lock = Lock()\n\n    def get_instance_name_to_class_dict(self) -> Dict[str, T]:\n        raise NotImplementedError()\n\n    def get_instance(self, instance_name: str, use_cache: bool = True, *args, **kwargs):\n        cleaned_instance_name = instance_name.lower()\n\n        # check if we already created that instance from before and use it\n        if cleaned_instance_name in self._instances_dict and use_cache:\n            return self._instances_dict[cleaned_instance_name]\n\n        # map from instance name to instance type\n        instance_class = self.get_instance_name_to_class_dict().get(\n            cleaned_instance_name\n        )\n        if not instance_class:\n            raise ValueError(\n                f\"{cleaned_instance_name} does not exist. \"\n                f\"Please choose from {self.get_instance_name_to_class_dict().keys()}\"\n            )\n\n        with self._lock:\n            # check again that the instance was not created from other threads\n            if cleaned_instance_name in self._instances_dict and use_cache:\n                return self._instances_dict[cleaned_instance_name]\n\n            # instantiate the class\n            instance = instance_class.create(*args, **kwargs)\n\n            # save for later use\n            self._instances_dict[cleaned_instance_name] = instance\n\n            return instance", ""]}
{"filename": "src/pred_evaluators/evaluation.py", "chunked_list": ["# these functions are heavily influenced by the HF squad_metrics.py script\ndef normalize_text(s):\n    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n    import string, re\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\n\ndef compute_exact_match(prediction, truth):\n    return int(normalize_text(prediction) == normalize_text(truth))\n\n\ndef compute_f1(prediction, truth):\n    pred_tokens = normalize_text(prediction).split()\n    truth_tokens = normalize_text(truth).split()\n\n    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n\n    common_tokens = set(pred_tokens) & set(truth_tokens)\n\n    # if there are no common tokens then f1 = 0\n    if len(common_tokens) == 0:\n        return 0\n\n    prec = len(common_tokens) / len(pred_tokens)\n    rec = len(common_tokens) / len(truth_tokens)\n\n    return 2 * (prec * rec) / (prec + rec)", ""]}
{"filename": "src/pred_evaluators/evaluators_factory.py", "chunked_list": ["from typing import Dict, Type\n\nfrom src.common.abstract_factory import AbstractFactory\nfrom src.pred_evaluators.pred_evaluators.bamboogle_evaluator import BamboogleEvaluator\nfrom src.pred_evaluators.pred_evaluators.base_evaluator import Evaluator\nfrom src.pred_evaluators.pred_evaluators.em_evaluator import EMEvaluator\nfrom src.pred_evaluators.pred_evaluators.hover_evaluator import HoverEvaluator\nfrom src.pred_evaluators.pred_evaluators.fermi_evaluator import FermiEvaluator\nfrom src.pred_evaluators.pred_evaluators.quartz_em_evaluator import QuartzEMEvaluator\nfrom src.pred_evaluators.pred_evaluators.wikihop_evaluator import WikiHopEvaluator", "from src.pred_evaluators.pred_evaluators.quartz_em_evaluator import QuartzEMEvaluator\nfrom src.pred_evaluators.pred_evaluators.wikihop_evaluator import WikiHopEvaluator\n\n\nclass EvaluatorsFactory(AbstractFactory):\n    \"\"\" \"\"\"\n\n    def get_instance_name_to_class_dict(self) -> Dict[str, Type[Evaluator]]:\n        return {\n            \"em\": EMEvaluator,\n            \"quartz\": QuartzEMEvaluator,\n            \"bamboogle\": BamboogleEvaluator,\n            \"hover\": HoverEvaluator,\n            \"fermi\": FermiEvaluator,\n            \"wikihop\": WikiHopEvaluator,\n        }", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/quartz_em_evaluator.py", "chunked_list": ["class QuartzEMEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred: str, gold: str) -> str:\n        label_split = pred.split(\".\")[0].replace(\"\\n\", \"\")[:1]\n        if len(label_split) == 0:\n            return\n        return label_split[0] == gold", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/bamboogle_evaluator.py", "chunked_list": ["from src.pred_evaluators.evaluation import compute_f1\n\n\nclass BamboogleEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred: str, gold: str) -> str:\n        return compute_f1(pred, gold)", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/hover_evaluator.py", "chunked_list": ["class HoverEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred: str, gold: str) -> str:\n        pred = pred.lower().strip()\n        if not (pred.startswith(\"yes\") or pred.startswith(\"no\")):\n            return 0\n        else:\n            pred = \"yes\" if pred.startswith(\"yes\") else \"no\"\n        return 1 if pred.lower() == gold.lower() else 0", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/wikihop_evaluator.py", "chunked_list": ["import re\nimport string\nfrom collections import Counter\n\n\ndef normalize_answer(s):\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\n\nclass WikiHopEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred: str, gold: str) -> str:\n        normalized_prediction = normalize_answer(pred)\n        normalized_ground_truth = normalize_answer(gold)\n\n        ZERO_METRIC = 0\n\n        if (\n            normalized_prediction in [\"yes\", \"no\", \"noanswer\"]\n            and normalized_prediction != normalized_ground_truth\n        ):\n            return ZERO_METRIC\n        if (\n            normalized_ground_truth in [\"yes\", \"no\", \"noanswer\"]\n            and normalized_prediction != normalized_ground_truth\n        ):\n            return ZERO_METRIC\n\n        prediction_tokens = normalized_prediction.split()\n        ground_truth_tokens = normalized_ground_truth.split()\n        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n        num_same = sum(common.values())\n        if num_same == 0:\n            return ZERO_METRIC\n        precision = 1.0 * num_same / len(prediction_tokens)\n        recall = 1.0 * num_same / len(ground_truth_tokens)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/fermi_evaluator.py", "chunked_list": ["from copy import deepcopy\n\nimport numpy as np\nimport pint\n\nureg = pint.UnitRegistry(system=\"mks\", autoconvert_offset_to_baseunit=True)\n# ureg.load_definitions(\"data/test_datasets/fermi/units.txt\")\n\n\nclass FermiEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def convert_units(self, answer):\n        if type(answer) == str:\n            try:\n                # try converting units\n                original_pint = ureg(answer)\n            except:\n                # keelp as is, can be num or string\n                original_pint = answer\n        else:\n            original_pint = answer\n        # if it's still string, try parsing\n        if type(original_pint) == str:\n            try:\n                original_pint = float(original_pint.replace(\"$\", \"\").split(\" \")[0])\n            except:\n                original_pint = None\n        if original_pint is None:\n            return None, None\n        # this means it's ureg\n        if type(original_pint) not in [float, int]:\n            try:\n                converted_pint = original_pint.to_base_units()\n            except:\n                converted_pint = deepcopy(original_pint)\n            return converted_pint.magnitude, converted_pint.units\n        else:\n            return original_pint, None\n\n    def evaluate(self, pred, gold) -> str:\n        pred = pred.split(\"=\")[-1]\n        gold_split, pred_split = gold.split(\" \"), pred.split(\" \")\n        if len(gold_split) > 1 and len(pred_split) == 1:\n            gold_split_measurement_value = gold_split[-1]\n            if not pred_split[-1].endswith(gold_split_measurement_value):\n                pred += \" \" + gold_split_measurement_value\n\n        y, y_hat = self.convert_units(pred)[0], self.convert_units(gold)[0]\n        conversion_dict = {\"million\": 1e6, \"trillion\": 1e9}\n        for k, v in conversion_dict.items():\n            if k in pred:\n                converted_pred = self.convert_units(pred.replace(k, \"\".strip()))[0]\n                if converted_pred is not None:\n                    y = converted_pred * v\n\n        if type(y) not in [int, float, np.float64] or type(y_hat) not in [\n            int,\n            float,\n            np.float64,\n        ]:\n            return 0\n        if y is None or y_hat is None:\n            return 0\n        if y < 0 or y_hat < 0:\n            return 0\n        if y == 0 and y_hat == 0:\n            return 1\n        elif y == 0 or y_hat == 0:\n            return max(0, 1 - np.abs(np.log10(np.abs(y - y_hat))))\n        # elif y/y_hat == 0:\n        #     return 0\n        try:\n            return max(0, 3 - np.abs(np.log10(y / y_hat))) / 3\n        except:\n            return 0", "\nclass FermiEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def convert_units(self, answer):\n        if type(answer) == str:\n            try:\n                # try converting units\n                original_pint = ureg(answer)\n            except:\n                # keelp as is, can be num or string\n                original_pint = answer\n        else:\n            original_pint = answer\n        # if it's still string, try parsing\n        if type(original_pint) == str:\n            try:\n                original_pint = float(original_pint.replace(\"$\", \"\").split(\" \")[0])\n            except:\n                original_pint = None\n        if original_pint is None:\n            return None, None\n        # this means it's ureg\n        if type(original_pint) not in [float, int]:\n            try:\n                converted_pint = original_pint.to_base_units()\n            except:\n                converted_pint = deepcopy(original_pint)\n            return converted_pint.magnitude, converted_pint.units\n        else:\n            return original_pint, None\n\n    def evaluate(self, pred, gold) -> str:\n        pred = pred.split(\"=\")[-1]\n        gold_split, pred_split = gold.split(\" \"), pred.split(\" \")\n        if len(gold_split) > 1 and len(pred_split) == 1:\n            gold_split_measurement_value = gold_split[-1]\n            if not pred_split[-1].endswith(gold_split_measurement_value):\n                pred += \" \" + gold_split_measurement_value\n\n        y, y_hat = self.convert_units(pred)[0], self.convert_units(gold)[0]\n        conversion_dict = {\"million\": 1e6, \"trillion\": 1e9}\n        for k, v in conversion_dict.items():\n            if k in pred:\n                converted_pred = self.convert_units(pred.replace(k, \"\".strip()))[0]\n                if converted_pred is not None:\n                    y = converted_pred * v\n\n        if type(y) not in [int, float, np.float64] or type(y_hat) not in [\n            int,\n            float,\n            np.float64,\n        ]:\n            return 0\n        if y is None or y_hat is None:\n            return 0\n        if y < 0 or y_hat < 0:\n            return 0\n        if y == 0 and y_hat == 0:\n            return 1\n        elif y == 0 or y_hat == 0:\n            return max(0, 1 - np.abs(np.log10(np.abs(y - y_hat))))\n        # elif y/y_hat == 0:\n        #     return 0\n        try:\n            return max(0, 3 - np.abs(np.log10(y / y_hat))) / 3\n        except:\n            return 0", ""]}
{"filename": "src/pred_evaluators/pred_evaluators/base_evaluator.py", "chunked_list": ["class Evaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred, gold) -> str:\n        raise NotImplementedError(\"Please Implement this method\")\n"]}
{"filename": "src/pred_evaluators/pred_evaluators/em_evaluator.py", "chunked_list": ["class EMEvaluator:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def evaluate(self, pred: str, gold: str) -> str:\n        if type(pred) == str and type(gold) == str:\n            return 1 if pred.lower() == gold.lower() else 0\n        return 1 if pred == gold else 0\n", ""]}
{"filename": "src/opeanai/utils.py", "chunked_list": ["import time\n\nimport openai\nimport os\nimport datetime\nfrom dotenv import load_dotenv\n\nfrom openai.error import RateLimitError, InvalidRequestError\nimport json\n", "import json\n\nfrom src.common.logger import get_logger\nfrom src.serpapi.serpapi import google, get_question_wiki_snippet\n\nlogger = get_logger()\n\n# load openai keys from env, and set a random key at random\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_KEY\")", "load_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_KEY\")\nopen_ai_keys = [openai.api_key]\nlast_time_out_for_keys = {k: datetime.datetime.min for k in open_ai_keys}\nsleep_time_per_key = 30\nprint()\n\n\ndef call_gpt(cur_prompt, stop=\"\\n\"):\n    \"\"\"\n    call the gpt-3 api\n    \"\"\"\n    print(\"calling gpt\")\n    ans = openai.Completion.create(\n        model=\"code-davinci-002\",\n        max_tokens=1,\n        stop=stop,\n        prompt=cur_prompt,\n        temperature=0,\n        logprobs=5,\n    )\n    returned = ans[\"choices\"][0][\"text\"]\n\n    return ans", "def call_gpt(cur_prompt, stop=\"\\n\"):\n    \"\"\"\n    call the gpt-3 api\n    \"\"\"\n    print(\"calling gpt\")\n    ans = openai.Completion.create(\n        model=\"code-davinci-002\",\n        max_tokens=1,\n        stop=stop,\n        prompt=cur_prompt,\n        temperature=0,\n        logprobs=5,\n    )\n    returned = ans[\"choices\"][0][\"text\"]\n\n    return ans", "\n\ndef greenify(input):\n    return \"\\x1b[102m\" + input + \"\\x1b[0m\"\n\n\ndef yellowfy(input):\n    return \"\\x1b[106m\" + input + \"\\x1b[0m\"\n\n", "\n\n#\n# def format_question(question: str, show_question=True) -> str:\n#     \"\"\"\n#     format a question that wil be presented to gpt-3 validator\n#     \"\"\"\n#     # init\n#     res = \"Provide a yes or no answer to the question given the following facts.\\n\"\n#     intermediate_res = []", "#     res = \"Provide a yes or no answer to the question given the following facts.\\n\"\n#     intermediate_res = []\n#\n#     # get a list of facts and intermediate answers\n#     question_liines = question.split(\"\\n\")\n#     facts, intermediate_answers = [], []\n#     for line in question_liines:\n#         if line.startswith(QUESTION_PREFIX):\n#             question = line.split(QUESTION_PREFIX)[1]\n#             res += f\"{question}\\n\"", "#             question = line.split(QUESTION_PREFIX)[1]\n#             res += f\"{question}\\n\"\n#         if line.startswith(FOLLOW_UP_PREFIX):\n#             facts.append(line.split(FOLLOW_UP_PREFIX)[1])\n#         if line.startswith(INTERMEDIATE_ANS_PREFIX):\n#             intermediate_answers.append(line.split(INTERMEDIATE_ANS_PREFIX)[1])\n#\n#     for i, (fact, ans) in enumerate(zip(facts, intermediate_answers)):\n#         if show_question:\n#             res += f\"{i + 1}. {fact} {ans}\\n\"", "#         if show_question:\n#             res += f\"{i + 1}. {fact} {ans}\\n\"\n#             intermediate_res.append(f\"{res}Answer:\\n\")\n#         else:\n#             res += f\"{i + 1}. {ans}\\n\"\n#             intermediate_res.append(f\"{res}Answer:\\n\")\n#\n#     res += \"Answer:\\n\"\n#\n#     return intermediate_res", "#\n#     return intermediate_res\n#\n#\n# def call_gpt3_for_question(curr_question: Question) -> Question:\n#     \"\"\"\n#     calls gpt-3 an populates\n#     \"\"\"\n#     # create the set of questions\n#     intermediate_questions = []", "#     # create the set of questions\n#     intermediate_questions = []\n#     for decmop in curr_question.decompositions:\n#         question_with_decomp = f\"{QUESTION_PREFIX}{curr_question.question}\\n{decmop}\"\n#         intermediate_questions += format_question(question_with_decomp)\n#     qusetion_intermediate_questions = set(intermediate_questions)\n#     qusetion_intermediate_questions_with_answers = []\n#\n#     # print\n#     for intermediate_question in qusetion_intermediate_questions:", "#     # print\n#     for intermediate_question in qusetion_intermediate_questions:\n#         gpt_res = call_gpt(intermediate_question[:-1], \"\\n\")\n#         gpt3_probs = {\n#             k: math.exp(v)\n#             for k, v in gpt_res[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n#             .to_dict()\n#             .items()\n#         }\n#         yes_probs = sum([v for k, v in gpt3_probs.items() if \"yes\" in k.lower()])", "#         }\n#         yes_probs = sum([v for k, v in gpt3_probs.items() if \"yes\" in k.lower()])\n#         no_probs = sum([v for k, v in gpt3_probs.items() if \"no\" in k.lower()])\n#         probs = {\"yes\": yes_probs, \"no\": no_probs, \"other\": 1 - yes_probs - no_probs}\n#         probs = {\n#             **probs,\n#             **{\n#                 \"yes_normalized\": probs[\"yes\"] / (probs[\"yes\"] + probs[\"no\"]),\n#                 \"no_normalized\": probs[\"no\"] / (probs[\"yes\"] + probs[\"no\"]),\n#             },", "#                 \"no_normalized\": probs[\"no\"] / (probs[\"yes\"] + probs[\"no\"]),\n#             },\n#         }\n#         probs\n#         print(probs)\n#         qusetion_intermediate_questions_with_answers.append(\n#             IntermediateQuestionWithAnswer(\n#                 intermediate_question=intermediate_question, answer=probs\n#             )\n#         )", "#             )\n#         )\n#\n#     # set var\n#     curr_question.intermediate_questions_with_answers = (\n#         qusetion_intermediate_questions_with_answers\n#     )\n#     return curr_question\n\n\ndef change_openaikey_and_sleep():\n    \"\"\"\n    if we encountered a time-out, change the key and sleep if necessary\n    \"\"\"\n    # set the date for current time out\n    last_time_out_for_keys[openai.api_key] = datetime.datetime.now()\n\n    # get first time out and calc the time that passed\n    first_timed_out_key = min(last_time_out_for_keys, key=last_time_out_for_keys.get)\n    time_since_first_time_out = (\n        datetime.datetime.now() - last_time_out_for_keys[first_timed_out_key]\n    )\n\n    # change the key to be the one that was last timed out\n    openai.api_key = first_timed_out_key\n    logger.info(f\"switched to openaikey: {openai.api_key}\")\n\n    # sleep if the time that passed between now and when we last used the key is smaller than a threshold, sleep\n    if time_since_first_time_out.seconds < sleep_time_per_key:\n        sleeping_time = sleep_time_per_key - time_since_first_time_out.seconds\n        print(f\"sleeping for {sleeping_time} seconds\")\n        print(last_time_out_for_keys)\n        time.sleep(sleep_time_per_key - time_since_first_time_out.seconds)\n        print(\"finished sleeping\")", "\n\ndef change_openaikey_and_sleep():\n    \"\"\"\n    if we encountered a time-out, change the key and sleep if necessary\n    \"\"\"\n    # set the date for current time out\n    last_time_out_for_keys[openai.api_key] = datetime.datetime.now()\n\n    # get first time out and calc the time that passed\n    first_timed_out_key = min(last_time_out_for_keys, key=last_time_out_for_keys.get)\n    time_since_first_time_out = (\n        datetime.datetime.now() - last_time_out_for_keys[first_timed_out_key]\n    )\n\n    # change the key to be the one that was last timed out\n    openai.api_key = first_timed_out_key\n    logger.info(f\"switched to openaikey: {openai.api_key}\")\n\n    # sleep if the time that passed between now and when we last used the key is smaller than a threshold, sleep\n    if time_since_first_time_out.seconds < sleep_time_per_key:\n        sleeping_time = sleep_time_per_key - time_since_first_time_out.seconds\n        print(f\"sleeping for {sleeping_time} seconds\")\n        print(last_time_out_for_keys)\n        time.sleep(sleep_time_per_key - time_since_first_time_out.seconds)\n        print(\"finished sleeping\")", "\n\ndef gpt_simple_generator(\n    prompt, model=\"code-davinci-002\", stop_condition=\"\\n\", temperature=0, max_tokens=256\n):\n    retries = 6\n    for i in range(retries):\n        try:\n            print(f\"Using: {openai.api_key}\")\n            ans = openai.Completion.create(\n                model=model,\n                max_tokens=max_tokens,\n                stop=stop_condition,\n                prompt=prompt,\n                temperature=temperature,\n                # logprobs=5,\n            )\n            returned = [res[\"text\"] for res in ans[\"choices\"]]\n\n            # if the answer is of size 1, we were prompted with 1 prompt so print it and return\n            if len(returned) == 1:\n                print(greenify(returned[0]), end=\"\")\n                return returned[0], {}\n\n            # else iterate all results and return a list\n            else:\n                for res in returned:\n                    print(greenify(res), end=\"\")\n                return returned, {}\n\n        except RateLimitError as e:\n            print(f\"exception thrown, sleeping...\")\n            print(e)\n            change_openaikey_and_sleep()\n\n        except InvalidRequestError as e:\n            print(\n                \"Invalid request caught, maybe the prompt is too long? Sleeping, plz take a look!\"\n            )\n            time.sleep(30)\n            # return \"\" if type(prompt) == str else [\"\" for _ in range(len(prompt))], {}\n\n        except Exception as e:\n            print(e)\n            time.sleep(90)", "\n\ndef call_gpt_self_ask(cur_prompt, stop):\n    res = \"\"\n    retries = 3\n    # iterate decomposition for 5 steps\n    for i in range(5):\n        # get gpt ans with retries\n        for i in range(retries):\n            try:\n                ans = openai.Completion.create(\n                    model=\"code-davinci-002\",\n                    max_tokens=512,\n                    stop=[\"Context:\", \"#\"],\n                    prompt=cur_prompt,\n                    temperature=0.7,\n                )\n                break\n            except Exception as e:\n                print(\"exception thrown, sleeping...\", e)\n                time.sleep(60)\n                print(\"finished sleeping\")\n\n        # add context\n        returned = ans[\"choices\"][0][\"text\"]\n        res += returned\n        cur_prompt += returned\n        if \"Follow up: \" in returned:\n            question = returned.split(\"Follow up: \")[-1].replace(\"\\n\", \"\")\n            retrieval = get_question_wiki_snippet(question, cache=True)\n            cur_prompt += f\"Context: {retrieval}\\n\"\n            res += f\"Context: {retrieval}\\n\"\n        if \"So the final answer is: \" in returned:\n            print(greenify(res), end=\"\")\n            return res\n        print(greenify(res), end=\"\")\n\n    return res", ""]}
{"filename": "src/experiments/demo.py", "chunked_list": ["import dataclasses\nfrom typing import Dict\n\nfrom src.common.config import Config\nfrom src.dataclasses import QuestionV1\nfrom src.experiments.e2e.run import (\n    _set_entailment_input,\n    _call_gpt_for_entailment_with_batching,\n)\nfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory", ")\nfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory\nfrom src.pred_evaluators.evaluators_factory import EvaluatorsFactory\nfrom src.prompting.prompt_factory import PromptFactoryDict\n\n\ndef run_question(\n    question_text: str,\n    dataset: str,\n    answer: str,\n    model: str = \"code-davinci-002\",\n    num_decomps: int = 3,\n) -> Dict:\n    \"\"\"\n    runs the question, first by populating decomps and then by running mcr\n    \"\"\"\n\n    # load config\n    Config().load(f\"src/config/{dataset}/config_with_retrieval_contexts_first.json\")\n\n    # set up object\n    question = QuestionV1(\n        question=question_text,\n        prompt=PromptFactoryDict[f\"{dataset}_decomposition\"],\n        gpt3_accessor=GptAccessorFactory().get_instance(\n            \"gpt_accessor_with_retrieval_context_first\"\n        ),\n        model=model,\n        num_decompositions=num_decomps,\n    )\n    question.populate()\n\n    # set mcr input\n    question_with_mcr = _set_entailment_input(\n        {\"question\": dataclasses.asdict(question), \"metadata_gold_answer\": answer},\n        PromptFactoryDict[f\"{dataset}_mcr\"],\n        entailment_values={\n            \"shuffle_context\": False,\n            \"use_ir_contexts\": False,\n            \"use_qa_pairs\": True,\n            \"question_prompt_prefix\": \"\",\n            \"stop_condition\": \"#\",\n            \"model\": model,\n        },\n        sa_contexts_first=True,\n        prefix=f\"mcr\",\n    )\n\n    print(\"\\n====================== Running meta-reasoner ======================\")\n    res = _call_gpt_for_entailment_with_batching(\n        [question_with_mcr],\n        EvaluatorsFactory().get_instance(\"bamboogle\"),\n        entailment_values={\n            \"shuffle_context\": False,\n            \"use_ir_contexts\": False,\n            \"use_qa_pairs\": True,\n            \"question_prompt_prefix\": \"\",\n            \"stop_condition\": \"#\",\n            \"model\": model,\n        },\n        prefix=f\"mcr\",\n    )[0]\n    print(f\"F1: {res['mcr_acc@mte']}\")", "\n#\n# dataset = \"strategyqa\"  # 2wikihop/strategyqa\n# question_text = \"Did Brad Peyton need to know about seismology?\"\n# answer = \"yes\"  # gold answer\n#\n# run_question(\n#     question_text=question_text,\n#     dataset=dataset,\n#     answer=answer,", "#     dataset=dataset,\n#     answer=answer,\n#     model=\"code-davinci-002\",\n# )\n"]}
{"filename": "src/experiments/e2e/run.py", "chunked_list": ["\"\"\"\nmain script to run experiments.\n\"\"\"\n\nimport ast\n\nimport numpy as np\nfrom tqdm import tqdm\nimport argparse\nimport dataclasses", "import argparse\nimport dataclasses\nimport json\nimport os\nimport random\nfrom collections import Counter\nfrom datetime import datetime\nfrom typing import Dict, List\nimport pandas as pd\nimport wandb", "import pandas as pd\nimport wandb\n\nfrom src.common.config import Config\nfrom src.common.logger import get_logger\nfrom src.consts import (\n    FULL_MTE_FIELD,\n    PRED_MTE_FIELD,\n    ACC_MTE_FIELD,\n    ACC_AT_1_FIELD,", "    ACC_MTE_FIELD,\n    ACC_AT_1_FIELD,\n    ACC_AT_3_FIELD,\n    ACC_AT_MAJORITY_FIELD,\n    NUM_EXAMPLES_FIELD,\n    NUM_ABSTAINS_FIELD,\n    CONTEXT_ANSWER_SEP,\n)\nfrom src.dataclasses import (\n    QuestionV1,", "from src.dataclasses import (\n    QuestionV1,\n    QuestionWithAnswer,\n    format_decompsition_string,\n    format_ir_decomposition,\n)\nfrom src.dataset_readers.dataset_readers_factory import DatasetReadersFactory\nfrom src.dataset_readers.readers.dataset_reader import DatasetReader\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory", "from src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.gpt3_accessors.gpt_accessor_factory import GptAccessorFactory\nfrom src.opeanai.utils import gpt_simple_generator\nfrom src.pred_evaluators.evaluators_factory import EvaluatorsFactory\nfrom src.pred_evaluators.pred_evaluators.base_evaluator import Evaluator\nfrom src.prompting.prompt_factory import PromptFactoryDict\nfrom src.serpapi.serpapi import get_string_hash, google\nfrom src.serpapi.serpapi import get_question_wiki_snippet\n\nlogger = get_logger()", "\nlogger = get_logger()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        default=\"src/config/2wikihop/config_with_retrieval_contexts_first.json\",\n        help=\"Config file path\",\n    )\n    return parser.parse_args()", "\n\ndef _call_gpt_for_entailment_with_batching(\n    questions: List[Dict],\n    evaluator: Evaluator,\n    entailment_values: Dict,\n    prefix=\"mte\",\n    batch_size: int = 10,\n) -> List[Dict]:\n    \"\"\"\n    call gpt for entailment with batching.\n    will sort the input to batches, call gpt for each batch and return results as a list of questions\n    \"\"\"\n\n    def batch_iterable(iterable, n=1):\n        l = len(iterable)\n        for ndx in range(0, l, n):\n            yield iterable[ndx : min(ndx + n, l)]\n\n    batches = [x for x in batch_iterable(list(questions), batch_size)]\n\n    logger.info(f\"Running entailment with batching.\")\n    for batch in tqdm(batches):\n        input_prompts = [question[f\"{prefix}_entailment_input\"] for question in batch]\n        # call gpt-3\n        gpt_trace_entailment, _ = gpt_simple_generator(\n            prompt=input_prompts,\n            model=entailment_values.get(\"model\"),\n            stop_condition=entailment_values.get(\"stop_condition\"),\n            temperature=0,\n        )\n\n        # change to a list if a string was returned\n        gpt_trace_entailment = (\n            [gpt_trace_entailment]\n            if type(gpt_trace_entailment) == str\n            else gpt_trace_entailment\n        )\n\n        # iterate questions in batch\n        for i, question in enumerate(batch):\n            # add prediction\n            gpt_res = gpt_trace_entailment[i]\n            question[f\"{prefix}_{FULL_MTE_FIELD}\"] = gpt_res\n\n            if \"the answer is:\" in question[f\"{prefix}_{FULL_MTE_FIELD}\"]:\n                # take the final answer\n                final_answer = gpt_res.split(\"the answer is:\")[-1].strip()\n\n                # remove the dot if it's the last character\n                if len(final_answer) and final_answer[-1] == \".\":\n                    final_answer = final_answer[:-1]\n                question[f\"{prefix}_{PRED_MTE_FIELD}\"] = final_answer.replace(\n                    \"\\n\", \"\"\n                ).strip()\n                if \"unknown\" in question[f\"{prefix}_{PRED_MTE_FIELD}\"].lower():\n                    mte_acc = None\n                else:\n                    mte_acc = evaluator.evaluate(\n                        question[f\"{prefix}_{PRED_MTE_FIELD}\"]\n                        .strip()\n                        .replace(\"\\n\", \"\"),\n                        question[\"metadata_gold_answer\"]\n                        if \"metadata\" not in question\n                        else question[\"metadata\"][\"gold_answer\"],\n                    )\n            else:\n                mte_acc = None\n            question[f\"{prefix}_{ACC_MTE_FIELD}\"] = mte_acc\n    return list(questions)", "\n\ndef _set_entailment_input(\n    question: Dict,\n    question_prompt: str,\n    entailment_values: Dict,\n    prefix=\"mte\",\n    seed=0,\n    sa_contexts_first=None,\n) -> Dict:\n    \"\"\"\n    adds gpt input field in place\n    \"\"\"\n\n    def _get_decomps(use_ir: bool) -> List[str]:\n        \"\"\"\n        get the list of facts from decompositions, either the contexts (use_ir) or the QA facts\n        \"\"\"\n        decompositions = []\n\n        # add question ir context if configged\n        if entailment_values.get(\"retrieve_orig_question\"):\n            decompositions.append(\n                [\n                    question[\"question\"][\"question\"]\n                    + \" | \"\n                    + get_question_wiki_snippet(question[\"question\"][\"question\"])\n                ]\n            )\n\n        # limit decomps, for example for ste\n        max_decompositions = entailment_values.get(\"max_decompositions\")\n        if max_decompositions is None:\n            max_decompositions = 100\n\n        for i, decomposition in enumerate(\n            question[\"question\"][\"decompositions\"][:max_decompositions]\n        ):\n            # format decompositions for the context\n            decomp = format_decompsition_string(\n                decomposition.replace(\"Follow up:\\n\\n\", \"Follow up: \")\n                .replace(\"Intermediate answer:\\n\\n\", \"Intermediate answer: \")\n                .replace(\"So the final answer is:\\n\\n\", \"So the final answer is: \")\n            )\n            if use_ir is not None:\n                decomp = format_ir_decomposition(\n                    decomposition.replace(\"Follow up:\\n\\n\", \"Follow up: \")\n                    .replace(\"Intermediate answer:\\n\\n\", \"Intermediate answer: \")\n                    .replace(\"So the final answer is:\\n\\n\", \"So the final answer is: \")\n                    .replace(\"Context:\\n\\n\", \"Context: \"),\n                    contexts_first=sa_contexts_first,\n                )\n            decomp_facts = []\n            for j, decomp_step in enumerate(decomp):\n                decomp_step = dataclasses.asdict(decomp_step)\n\n                # this is when the model decides to give a direct answer (no intermediate steps).\n\n                if j == 0 and decomp_step[\"gpt_3_ans\"]:\n                    decomp_facts.append(\n                        (\n                            question[\"question\"][\"question\"]\n                            + \" \"\n                            + decomp_step[\"gpt_3_ans\"]\n                        )\n                    )\n                # intermediate steps\n                if not decomp_step[\"gpt_3_ans\"]:\n                    if (\n                        decomp_step[\"question\"] is not None\n                        and decomp_step[\"answer\"] is not None\n                        and use_ir is None\n                    ):\n                        decomp_facts.append(\n                            decomp_step[\"question\"] + \" \" + decomp_step[\"answer\"]\n                        )\n                    # print(\"decomp_step: \", decomp_step)  # todo: tw - delete\n                    # print(\"decomposition: \", decomposition)  # todo: tw - delete\n                    # TW - added in case intermediate answer is None\n                    questn = (\n                        \"\"\n                        if decomp_step[\"question\"] is None\n                        else decomp_step[\"question\"]\n                    )\n                    if use_ir is not None:\n                        # when doing entailment on retrieved contexts, answer is comprised of [context][sep][ans]\n                        #   we split it using [sep] to extract just the context (retrieved snippet)\n                        if decomp_step[\"answer\"] is None:\n                            ans = \"\"\n                        else:\n                            retrieved_context, ans = decomp_step[\"answer\"].split(\n                                CONTEXT_ANSWER_SEP\n                            )\n                            ans = \"| \" + retrieved_context\n                        decomp_facts.append(questn + \" \" + ans)\n            decompositions.append(decomp_facts)\n        return decompositions\n\n    retrieved_contexts = _get_decomps(use_ir=True)\n    facts = _get_decomps(use_ir=None)\n    context_list = [d for f in retrieved_contexts for d in f]\n    fact_list = [d for f in facts for d in f]\n    # add the context to the suffix\n    prompt_suffix = \"\\n\"\n    if entailment_values.get(\"shuffle_context\"):\n        seed = random.randint(1, 1000)\n        random.seed(seed)\n        random.shuffle(context_list)\n        random.shuffle(fact_list)\n        question[f\"{prefix}_shuffle_seed\"] = seed\n\n    # add contexts\n    if entailment_values.get(\"use_ir_contexts\"):\n        prompt_suffix += \"\\n\\n\".join(context_list) + \"\\n\"\n        if entailment_values.get(\"use_qa_pairs\"):\n            prompt_suffix += \"\\nDerived facts:\\n\"\n\n    # add qa pairs\n    if entailment_values.get(\"use_qa_pairs\"):\n        prompt_suffix += \"\\n\".join(fact_list) + \"\\n\"\n\n    # add to prompt suffix\n    prompt_suffix += \"\\nQuestion:\" + \"\\n\" + question[\"question\"][\"question\"] + \"\\n\"\n    prompt_suffix += \"\\nAnswer:\"\n    question[f\"{prefix}_entailment_input\"] = question_prompt + prompt_suffix\n\n    return question", "\n\ndef _populate_decompositions(\n    example: Dict,\n    prompt: str,\n    dataset_reader: DatasetReader,\n    gpt_accessor: GptAccessor,\n    evaluator: Evaluator,\n    decomposition_cache_dir: str,\n    num_decompositions: int,\n) -> Dict:\n    \"\"\" \"\"\"\n    example = dataset_reader.parse_example(example)\n    question = QuestionV1(\n        question=example.question,\n        prompt=prompt,\n        gpt3_accessor=gpt_accessor,\n        num_decompositions=num_decompositions,\n    )\n    question.populate()\n\n    gpt_answers = [\n        s[-1].gpt_3_ans[:-1] if s[-1].gpt_3_ans[-1] == \".\" else s[-1].gpt_3_ans\n        for s in question.decompsition_steps\n        if len(s) > 0 and s[-1].gpt_3_ans is not None and len(s[-1].gpt_3_ans) > 0\n    ]\n    question_with_answer = QuestionWithAnswer(\n        question=question, answers=None, gpt_answers=gpt_answers\n    )\n    question_with_answer_dict = dataclasses.asdict(question_with_answer)\n    results = {}\n    if len(question_with_answer_dict[\"gpt_answers\"]) > 0:\n        results[\"acc@1\"] = evaluator.evaluate(\n            question_with_answer_dict[\"gpt_answers\"][0], example.gold_answer\n        )\n        results[\"acc@3\"] = max(\n            [\n                evaluator.evaluate(ans, example.gold_answer)\n                for ans in question_with_answer_dict[\"gpt_answers\"]\n            ]\n        )\n        majority_prediction = Counter(\n            [y.lower() for y in question_with_answer_dict[\"gpt_answers\"]]\n        ).most_common(n=1)[0][0]\n        majority_prediction_at_three = Counter(\n            [y.lower() for y in question_with_answer_dict[\"gpt_answers\"][:3]]\n        ).most_common(n=1)[0][0]\n        results[\"acc@majority\"] = evaluator.evaluate(\n            majority_prediction, example.gold_answer\n        )\n        results[\"acc@majority_3\"] = evaluator.evaluate(\n            majority_prediction_at_three, example.gold_answer\n        )\n\n    else:\n        results[\"acc@1\"] = False\n        results[\"acc@3\"] = False\n        results[\"acc@majority\"] = False\n        results[\"acc@majority_3\"] = False\n    dataset_metadata_fields = Config().get(\"dataset.metadata_fields\")\n    results[\"metadata\"] = {\n        **{k: v for k, v in example.metadata.items() if k in dataset_metadata_fields},\n        **{k: v for k, v in dataclasses.asdict(example).items() if k != \"metadata\"},\n    }\n    logger.info(results)\n    question_with_answer_dict.update(results)\n\n    # save example\n    filename = get_string_hash(example.question)\n    with open(f\"{decomposition_cache_dir}/{filename}.json\", \"w\") as json_file:\n        json.dump(question_with_answer_dict, json_file)\n    return question_with_answer_dict", "\n\ndef _report_results(suffix: str, examples: List[Dict]):\n    \"\"\" \"\"\"\n    questions_with_entailment_df = pd.DataFrame(examples)\n    res = {\n        f\"{NUM_EXAMPLES_FIELD}_{suffix}\": questions_with_entailment_df.shape[0],\n        f\"{NUM_ABSTAINS_FIELD}_{suffix}\": questions_with_entailment_df[\n            f\"mte_{ACC_MTE_FIELD}\"\n        ]\n        .isna()\n        .sum(),\n        f\"{ACC_AT_1_FIELD}_{suffix}\": questions_with_entailment_df[\n            ACC_AT_1_FIELD\n        ].mean(),\n        f\"{ACC_AT_3_FIELD}_{suffix}\": questions_with_entailment_df[\n            ACC_AT_3_FIELD\n        ].mean(),\n        f\"{ACC_AT_MAJORITY_FIELD}_{suffix}\": questions_with_entailment_df[\n            ACC_AT_MAJORITY_FIELD\n        ].mean(),\n        f\"mte_{ACC_MTE_FIELD}_{suffix}\": questions_with_entailment_df[\n            f\"mte_{ACC_MTE_FIELD}\"\n        ].mean(),\n    }\n    for report_func in [logger.info]:\n        report_func(res)", "\n\ndef _save_examples(examples: List[Dict], output_path: str):\n    \"\"\" \"\"\"\n    # format\n    for ex in examples:\n        # flatten metadata\n        if \"metadata\" in ex:\n            for k, v in ex[\"metadata\"].items():\n                ex[f\"metadata_{k}\"] = v\n            del ex[\"metadata\"]\n\n        # flatten decompositions\n        num_decompositions = len(ex[\"question\"][\"decompositions\"])\n        for i in range(num_decompositions):\n            ex[f\"decomposition_{i}\"] = ex[\"question\"][\"decompositions\"][i]\n\n    # save\n    pd.DataFrame(examples).to_csv(output_path)", "\n\ndef _read_csv(file_path: str) -> List[Dict]:\n    \"\"\"\n    read decomps from a csv file\n    \"\"\"\n    logger.info(f\"Reading input data form csv file at path: {file_path}\")\n    df = pd.read_csv(file_path)\n    data = df.to_dict(\"rows\")\n    for x in data:\n        x[\"question\"] = ast.literal_eval(x[\"question\"])\n    num_rows = len(data)\n    logger.info(f\"Read {num_rows} from file\")\n    return data", "\n\ndef _run_decompositions(\n    examples: List[Dict],\n    cache_dir: str,\n    output_dir: str,\n    experiment_unique_name: str,\n    dataset: DatasetReader,\n    prompt: str,\n    gpt_accessor: GptAccessor,\n    evaluator: Evaluator,\n    num_decompositions: int,\n) -> List[Dict]:\n    if cache_dir is None:\n        decomposition_cache_dir = (\n            f\"{output_dir}/{experiment_unique_name}/decompositions\"\n        )\n        os.makedirs(decomposition_cache_dir, exist_ok=True)\n        logger.info(f\"Saving decompositions in: {decomposition_cache_dir}\")\n        questions_with_decompositions = [\n            _populate_decompositions(\n                example,\n                prompt,\n                dataset,\n                gpt_accessor,\n                evaluator,\n                decomposition_cache_dir,\n                num_decompositions,\n            )\n            for example in tqdm(examples)\n        ]\n    else:\n        logger.info(f\"Reading decompositions from: {cache_dir}\")\n        cached_files = [f for f in os.listdir(cache_dir)]\n        cached_decompositions = {}\n        for filename in cached_files:\n            with open(f\"{cache_dir}/{filename}\", \"r\") as f:\n                data = json.load(f)\n                cached_decompositions[filename] = data\n        questions_with_decompositions = cached_decompositions.values()\n    return questions_with_decompositions", "\n\ndef run_experiment(config_path: str):\n    \"\"\" \"\"\"\n    # read config\n    Config().load(config_path)\n    wandb.init(project=\"GRE\", config=Config()._config)\n\n    # start experiment\n    experiment_name = Config().get(\"experiment_name\")\n    logger.info(f\"Starting experiment: {experiment_name}\")\n    output_dir = Config().get(\"output_dir\")\n\n    # datetime\n    timestamp = datetime.now().timestamp()\n    date_time = datetime.fromtimestamp(timestamp)\n    str_date_time = date_time.strftime(\"%d_%m_%Y_%H_%M_%S\")\n    experiment_unique_name = f\"{experiment_name}_{str_date_time}\"\n\n    # read dataset\n    dataset_name = Config().get(\"dataset.name\")\n    logger.info(f\"Reading dataset: {dataset_name}.\")\n    dataset = DatasetReadersFactory().get_instance(dataset_name)\n    dataset.read()\n    examples = dataset.examples\n\n    # get evaluator\n    evaluator_name = Config().get(\"evaluator\")\n    logger.info(f\"Using evaluator: {evaluator_name} to report metrics\")\n    evaluator = EvaluatorsFactory().get_instance(evaluator_name)\n\n    # decomposition settings\n    gpt_accessor_name = Config().get(\"decomposition.gpt3_accessor\")\n    prompt_name = Config().get(\"decomposition.prompt\")\n    logger.info(\n        f\"GPT3 accessor for decompositions: {gpt_accessor_name}, with prompt: {prompt_name}\"\n    )\n    gpt_accessor = GptAccessorFactory().get_instance(gpt_accessor_name)\n    prompt = PromptFactoryDict[prompt_name]\n    num_decompositions = Config().get(\"decomposition.num_decompositions\")\n\n    # abstain settings\n    entailment_prompt_name = list(Config().get(\"entailment\").values())[0].get(\"prompt\")\n    logger.info(f\"Using entailment engine with prompt: {entailment_prompt_name}\")\n    entailment_prompt = PromptFactoryDict[entailment_prompt_name]\n\n    abstain_prompt_name = Config().get(\"abstain.prompt\")\n    logger.info(f\"Using entailment engine with prompt: {entailment_prompt_name}\")\n    abstain_prompt = PromptFactoryDict[abstain_prompt_name]\n\n    # filter examples in prompts\n    examples_not_in_prompts = [\n        e\n        for e in dataset.examples\n        if not (\n            dataset.parse_example(e).question.lower() in prompt.lower()\n            or dataset.parse_example(e).question.lower() in entailment_prompt.lower()\n        )\n    ]\n    num_examples, num_examples_not_in_prompts = len(examples), len(\n        examples_not_in_prompts\n    )\n    num_examples_in_prompt = num_examples - num_examples_not_in_prompts\n    logger.info(\n        f\"Removing {num_examples_in_prompt}/{num_examples} examples. Left with {num_examples_not_in_prompts} examples.\"\n    )\n    examples = examples_not_in_prompts\n\n    # sample examples\n    num_examples = Config().get(\"sampling.num_examples\")\n    if num_examples is not None:\n        sampling_seed = Config().get(\"sampling.seed\")\n        logger.info(f\"Down-sampling to {num_examples} examples.\")\n        random.seed(sampling_seed)\n        examples = random.sample(examples, num_examples)\n\n    # sample from csv\n    examples_csv = Config().get(\"sampling.examples_csv\")\n    if examples_csv is not None:\n        prev_examples = pd.read_csv(examples_csv).to_dict(\"rows\")\n        previous_qids = {x[\"metadata_qid\"] for x in prev_examples}\n\n        # create a dataset with the previous examples\n        prev_examples_dataset, prev_examples_dataset_qids = [], set()\n        for x in examples:\n            if x[\"qid\"] in previous_qids and x[\"qid\"] not in prev_examples_dataset_qids:\n                prev_examples_dataset.append(x)\n                prev_examples_dataset_qids.add(x[\"qid\"])\n\n        # assign the new dataset for the examples\n        examples = prev_examples_dataset\n\n    # retries\n    num_retries = Config().get(\"num_retries\")\n    logger.info(f\"Running with {num_retries} retries.\")\n    examples_with_answers: List[Dict] = []\n    examples_to_answer: List[Dict] = examples  # [207:]  # todo: TW - delete\n\n    # use contexts first format in Self-ask decomposition\n    sa_contexts_first = (\n        Config().get(\"decomposition.gpt3_accessor\")\n        == \"gpt_accessor_with_retrieval_context_first\"\n    )\n\n    for i in range(num_retries + 1):\n        # populate question with decomposition\n        cache_dir = Config().get(\"decomposition.cache_dir\")\n        csv_input_file = Config().get(\"decomposition.csv_input_file\")\n        questions_with_decompositions = (\n            _run_decompositions(\n                examples=examples_to_answer,\n                cache_dir=cache_dir if i == 0 else None,\n                output_dir=output_dir,\n                experiment_unique_name=experiment_unique_name,\n                dataset=dataset,\n                prompt=prompt,\n                gpt_accessor=gpt_accessor,\n                evaluator=evaluator,\n                num_decompositions=num_decompositions,\n            )\n            if csv_input_file is None\n            else _read_csv(csv_input_file)\n        )\n\n        # add input for entailment (for batching)\n        for entailment_prefix, entailment_values in Config().get(\"entailment\").items():\n            # set num iterations\n            iterations = entailment_values.get(\"iterations\")\n            if iterations is None:\n                iterations = 1\n\n            for k in range(iterations):\n                # entailment settings\n                entailment_prompt_name = entailment_values.get(\"prompt\")\n                logger.info(\n                    f\"Using entailment engine with prompt: {entailment_prompt_name}\"\n                )\n                entailment_prompt = PromptFactoryDict[entailment_prompt_name]\n\n                for question in questions_with_decompositions:\n                    _set_entailment_input(\n                        question,\n                        entailment_prompt,\n                        entailment_values,\n                        sa_contexts_first=sa_contexts_first,\n                        prefix=f\"{k}_{entailment_prefix}\",\n                    )\n\n                # call gpt with batching, adds results in place\n                questions_with_entailment = _call_gpt_for_entailment_with_batching(\n                    questions_with_decompositions,\n                    evaluator,\n                    entailment_values,\n                    prefix=f\"{k}_{entailment_prefix}\",\n                )\n        # questions_with_abstain = [\n        #     _populate_entailment(question, abstain_prompt, evaluator, 0, \"abstain\")\n        #     for question in tqdm(list(questions_with_decompositions))\n        # ]\n\n        # save results\n        if output_dir is not None:\n            output_path = f\"{output_dir}/{experiment_unique_name}_{i}.csv\"\n            logger.info(f\"Saving output path to: {output_path}\")\n            _save_examples(questions_with_entailment, output_path)\n\n        # report results\n        _report_results(\n            suffix=f\"_{experiment_name}_iteration={i}\",\n            examples=questions_with_decompositions,\n        )\n\n        # aggregate for next iteration\n\n        examples_with_answers += [\n            q\n            for q in questions_with_entailment\n            if\n            # 'unknown' not in q['abstain_multi_trace_entailment_prediction'].lower()\n            # and\n            q[f\"mte_{ACC_MTE_FIELD}\"] is not None\n        ]\n\n        # get abstain ids and keep only abstain answers in examples_to_answer\n        abstain_ids = {\n            q[\"metadata_qid\"]\n            for q in questions_with_entailment\n            if (\n                q[f\"mte_{ACC_MTE_FIELD}\"]\n                is None\n                # or 'unknown' in q['abstain_multi_trace_entailment_prediction'].lower()\n            )\n        }\n        examples_to_answer = [\n            e for e in examples if dataset.parse_example(e).qid in abstain_ids\n        ]\n        if len(examples_to_answer) == 0:\n            break\n\n    # aggregate after retries loop\n    unanswered_examples = [\n        q for q in questions_with_entailment if q[f\"mte_{ACC_MTE_FIELD}\"] is None\n    ]\n    all_examples = examples_with_answers + unanswered_examples\n    _report_results(suffix=f\"_{experiment_name}_final\", examples=all_examples)\n\n    # save results\n    if output_dir is not None:\n        output_path = f\"{output_dir}/{experiment_unique_name}_final.csv\"\n        logger.info(f\"Saving output path to: {output_path}\")\n        _save_examples(all_examples, output_path)", "\n\nif __name__ == \"__main__\":\n    \"\"\" \"\"\"\n    args = parse_args()\n    run_experiment(args.config_path)\n"]}
{"filename": "src/prompting/prompt_factory.py", "chunked_list": ["from typing import Dict\n\nfrom src.prompting.prompts_to_keep import (\n    wikihop_decompositions_with_retrieval_context_first,\n    wikihop_entailment,\n    strategyqa_decomposition,\n    strategy_mcr,\n)\n\nPromptFactoryDict: Dict[str, str] = {", "\nPromptFactoryDict: Dict[str, str] = {\n    \"2wikihop_decomposition\": wikihop_decompositions_with_retrieval_context_first,\n    \"2wikihop_mcr\": wikihop_entailment,\n    \"strategyqa_decomposition\": strategyqa_decomposition,\n    \"strategyqa_mcr\": strategy_mcr,\n}\n"]}
{"filename": "src/prompting/prompts_to_keep.py", "chunked_list": ["strategyqa_decomposition = \"\"\"Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question, you are given a context which is the top returned google snippet for the question from Wikipedia. If no follow up questions are necessary, answer the question directly.\n#\nContext1: Frost: Frost is a thin layer of ice on a solid surface, which forms from water vapor in an above-freezing atmosphere coming in contact with a solid surface whose ...\nContext2: Graduation: Graduation is the awarding of a diploma to a student by an educational institution. It may also refer to the ceremony that is associated with it.\nContext3: Winter: Winter ; Astronomical season, 22 December \u2013 21 March ; Meteorological season, 1 December \u2013 28/29 February ; Solar (Celtic) season, 1 November \u2013 31 January.\nQuestion: Is it common to see frost during some college commencements?\nAre follow up questions needed here: Yes.\nFollow up: What seasons can you expect to see frost?\nIntermediate answer: Frost is common during the winter.\nFollow up: When is college commencement?", "Intermediate answer: Frost is common during the winter.\nFollow up: When is college commencement?\nIntermediate answer: College commencement ceremonies often happen during the months of December, May, June.\nFollow up: Do any of the months December, May, June occur during the Winter?\nIntermediate answer: December is in the winter.\nSo the final answer is: Yes.\n#\nContext1: War in Vietnam (1945\u20131946): [[\\'Date\\', \\'September 13, 1945 \u2013 March 30, 1946 (6 months, 2 weeks and 3 days)\\'], [\\'Location\\', \\'Southern Vietnam below the 16th parallel\\']]\nContext2: Llama: The gestation period of a llama is 11.5 months (350 days). Dams (female llamas) do not lick off their babies, as they have an attached tongue that does not reach outside of the mouth more than 13 millimetres (1\u20442 inch). Rather, they will nuzzle and hum to their newborns.\nContext3: Logarithm: In mathematics, the logarithm is the inverse function to exponentiation. That means the logarithm of a number x to the base b is the exponent to ... The binary logarithm uses base 2 and is frequently used in computer ...", "Context2: Llama: The gestation period of a llama is 11.5 months (350 days). Dams (female llamas) do not lick off their babies, as they have an attached tongue that does not reach outside of the mouth more than 13 millimetres (1\u20442 inch). Rather, they will nuzzle and hum to their newborns.\nContext3: Logarithm: In mathematics, the logarithm is the inverse function to exponentiation. That means the logarithm of a number x to the base b is the exponent to ... The binary logarithm uses base 2 and is frequently used in computer ...\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nAre follow up questions needed here: Yes.\nFollow up: How long did the vietnam war (1945-1946) last?\nIntermediate answer: The War in Vietnam (1945-46) lasted around 6 months.\nFollow up: How long is the llama gestation period?\nIntermediate answer: The gestation period for a llama is 11.5 months.\nFollow up: What is 2 times 11.5?\nIntermediate answer: 23, which is longer than 6.", "Follow up: What is 2 times 11.5?\nIntermediate answer: 23, which is longer than 6.\nSo the final answer is: No.\n#\nQuestion: Do truck drivers generally also teach math?\nAre follow up questions needed here: No.\nSo the final answer is: No.\n#\nContext1: Princeton University: She graduated from the Dwight-Englewood School in Englewood, New Jersey, in 1983. She went to Princeton University to pursue her bachelor's degree in French literature, where she graduated in 1987. So the answer is Princeton University.\nContext2: Princeton University: Princeton University is a private research university in Princeton, New Jersey. Founded in 1746 in Elizabeth as the College of New Jersey, Princeton is ... It is one of the highest-ranked universities in the world.", "Context1: Princeton University: She graduated from the Dwight-Englewood School in Englewood, New Jersey, in 1983. She went to Princeton University to pursue her bachelor's degree in French literature, where she graduated in 1987. So the answer is Princeton University.\nContext2: Princeton University: Princeton University is a private research university in Princeton, New Jersey. Founded in 1746 in Elizabeth as the College of New Jersey, Princeton is ... It is one of the highest-ranked universities in the world.\nContext3: University of Pennsylvania: It is the fourth-oldest institution of higher education in the United States and is ranked among the highest-regarded universities by numerous organizations and ...\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\nAre follow up questions needed here: Yes.\nFollow up: What college did Brooke Shields go to?\nIntermediate answer: Brooke Shields graduated from Princeton University.\nFollow up: Out of all colleges in the US, how is Princeton University ranked?\nIntermediate answer: Princeton is ranked as the number 1 national college by US news.\nFollow up: Is the ranking of University of Pennsylvania similar to Princeton University?", "Intermediate answer: Princeton is ranked as the number 1 national college by US news.\nFollow up: Is the ranking of University of Pennsylvania similar to Princeton University?\nIntermediate answer: University of Pennsylvania is also ranked at the top 10 national colleges by US news.\nSo the final answer is: Yes.\n#\nContext1: 23 April 1616: William Shakespeare ( bapt. 26 April 1564 \u2013 23 April 1616) was an English playwright, poet and actor. So the answer is 23 April 1616.\nContext2: Email: The original usage in June 1979 occurred in the journal Electronics in reference to the United States Postal Service initiative called E-COM, which was developed in the late 1970s and operated in the early 1980s.\nContext3: List of years: This page indexes the individual years pages. Contents. 1 3rd millennium. 1.1 21st century. 2 2nd millennium. 2.1 20th century; 2.2 19th century ...\nQuestion: Did William Shakespeare ever email his friends?\nAre follow up questions needed here: Yes.", "Question: Did William Shakespeare ever email his friends?\nAre follow up questions needed here: Yes.\nFollow up: When did William Shakespeare die?\nIntermediate answer: William Shakespeare died on 23 April 1616.\nFollow up: When was the email invented?\nIntermediate answer: The email was invented in 1971.\nFollow up: Is 1616 before 1971?\nIntermediate answer: Yes, 1616 is before 1971.\nSo the final answer is: No.\n#", "So the final answer is: No.\n#\nContext1: Hamster: Hamsters are rodents (order Rodentia) belonging to the subfamily Cricetinae, which contains 19 species classified in seven genera. They have become established as popular small pets. The best-known species of hamster is the golden or Syrian hamster (Mesocricetus auratus), which is the type most commonly kept as pets.\nContext2: Predation: Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding ...\nQuestion: Do hamsters provide food for any animals?\nAre follow up questions needed here: Yes.\nFollow up: What types of animals are hamsters?\nIntermediate answer: Hamsters are prey animals.\nFollow up: Do prey animals provide food for any other animals?\nIntermediate answer: Prey animals provide food for predators.", "Follow up: Do prey animals provide food for any other animals?\nIntermediate answer: Prey animals provide food for predators.\nSo the final answer is: Yes.\n#\nQuestion: Do the Druze believe in reincarnation?\nAre follow up questions needed here: No.\nSo the final answer is: Yes.\n#\nContext1: about 1 gram per cubic centimetre: The density of water is about 1 gram per cubic centimetre (62 lb/cu ft): this relationship was originally used to define the gram. So the answer is about 1 gram per cubic centimetre.\nContext2: Gram per cubic centimetre: The density of water is about 1 g/cm3, since the gram was originally defined as the mass of one cubic centimetre of water at its maximum density at 4 \u00b0C.", "Context1: about 1 gram per cubic centimetre: The density of water is about 1 gram per cubic centimetre (62 lb/cu ft): this relationship was originally used to define the gram. So the answer is about 1 gram per cubic centimetre.\nContext2: Gram per cubic centimetre: The density of water is about 1 g/cm3, since the gram was originally defined as the mass of one cubic centimetre of water at its maximum density at 4 \u00b0C.\nQuestion: Would a pear sink in water?\nAre follow up questions needed here: Yes.\nFollow up: What is the density of a pear?\nIntermediate answer: The density of a raw pear is about 0.59 g/cm^3.\nFollow up: What is the density of water?\nIntermediate answer: The density of water is about 1 g/cm^3.\nFollow up: Is 0.59 g/cm^3 greater than 1 g/cm^3?\nIntermediate answer: 0.59 g/cm^3 is less than 1 g/cm^3.", "Follow up: Is 0.59 g/cm^3 greater than 1 g/cm^3?\nIntermediate answer: 0.59 g/cm^3 is less than 1 g/cm^3.\nSo the final answer is: No.\n#\nContext1: Last rites: The last rites, also known as the Commendation of the Dying, are the last prayers and ministrations given to an individual of Christian faith, when possible, shortly before death. They may be administered to those awaiting execution, mortally injured, or terminally ill.\nContext2: Richard Dawkins: Dawkins is an outspoken atheist and a supporter of various atheist, secular, and humanist organisations, including Humanists UK and the Brights movement. Dawkins suggests that atheists should be proud, not apologetic, stressing that atheism is evidence of a healthy, independent mind.\nContext3: Prayer in the Catholic Church: In the Catholic Church, prayer is \"the raising of one\\'s mind and heart to God or the requesting of good things from God.\" It is an act of the moral virtue ...\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nAre follow up questions needed here: Yes.\nFollow up: What are the last Rites?", "Are follow up questions needed here: Yes.\nFollow up: What are the last Rites?\nIntermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.\nFollow up: What are Richard Dawkins religious beliefs?\nIntermediate answer:  Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design.\nFollow up: Would an atheist participate in Catholics prayers?\nIntermediate answer:  It is unlikely that an atheist would participate in Catholics prayers.\nSo the final answer is: Yes.\n#\nContext1: number 1: Hydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. So the answer is number 1.", "#\nContext1: number 1: Hydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. So the answer is number 1.\nContext2: Spice Girls - Simple English Wikipedia, the free encyclopedia: The group has five members. Each member uses a nickname initially given to them: Melanie Chisholm (\"Sporty Spice\"), Emma Bunton (\"Baby Spice\"), Melanie Brown (\"Scary Spice\"), Victoria Beckham (n\u00e9e Adams) (\"Posh Spice\"), and Geri Halliwell (\"Ginger Spice\") .\nContext3: Square number: In mathematics, a square number or perfect square is an integer that is the square of an integer; in other words, it is the product of some integer with ...\nQuestion: Hydrogen's atomic number squared exceeds number of Spice Girls?\nAre follow up questions needed here: Yes.\nFollow up: What is the atomic number of hydrogen?\nIntermediate answer: Hydrogen has an atomic number of 1.\nFollow up: How many people are in the Spice Girls band?\nIntermediate answer: The Spice Girls has 5 members.", "Follow up: How many people are in the Spice Girls band?\nIntermediate answer: The Spice Girls has 5 members.\nFollow up: Is the square of 1 greater than 5?\nIntermediate answer: The square of 1 is 1 which is less than 5.\nSo the final answer is: No.\n#\"\"\"\n\nstrategy_mcr = \"\"\"Given a question and a context, provide a Yes or No answer and explain why. If you are unsure, answer Unknown.\n\n#", "\n#\nContext:\nWhat type of animal is a jellyfish? A jellyfish is a type of invertebrate.\nDo jellyfish have arteries? No, jellyfish do not have arteries. They have a type of nervous system called a nerve net.\nWhat is atherosclerosis? Atherosclerosis is a disease where plaque builds up in the arteries.\nDo jellyfish have arteries? Jellyfish do not have a circulatory system and therefore do not have arteries.\nSo jellyfish don't have atherosclerosis because they don't have arteries? Yes, that is correct.\nWhat is atherosclerosis? Atherosclerosis is a type of heart disease.\nIs an invertebrate susceptible to atherosclerosis? No, invertebrates are not susceptible to atherosclerosis.", "What is atherosclerosis? Atherosclerosis is a type of heart disease.\nIs an invertebrate susceptible to atherosclerosis? No, invertebrates are not susceptible to atherosclerosis.\nWhat is atherosclerosis? Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\n\nQuestion:\nIs a jellyfish safe from atherosclerosis?\n\nAnswer:\nJellyfish do not have a circulatory system and therefore do not have arteries. Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\nSo the answer is: Yes.", "Jellyfish do not have a circulatory system and therefore do not have arteries. Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\nSo the answer is: Yes.\n#\nContext:\nIs 242 mph greater than 130 mph? Yes, 242 mph is greater than 130 mph.\nIs 130 mph greater than 200 mph? No, 130 mph is less than 200 mph.\nWhat is the top wind speed of Hurricane Harvey? Hurricane Harvey had top wind speeds of 130 mph.\nWhat is the top wind speed of a Peregrine falcon? A Peregrine falcon can fly at speeds of up to 200 mph.\nIs 240 mph greater than 130 mph? 240 mph is greater than 130 mph.\nHow fast can Hurricane Harvey winds go? Hurricane Harvey can have wind speeds up to 130 mph.", "Is 240 mph greater than 130 mph? 240 mph is greater than 130 mph.\nHow fast can Hurricane Harvey winds go? Hurricane Harvey can have wind speeds up to 130 mph.\nHow fast can a Peregrine falcon fly? The Peregrine falcon is the fastest bird, and can fly up to 242 mph.\nHow fast can a Peregrine falcon fly? The Peregrine falcon can fly up to 240 mph.\nHow fast can Hurricane Harvey winds travel? Hurricane Harvey winds can travel up to 130 mph.\n\nQuestion:\nCould Hurricane Harvey catch a Peregrine falcon?\n\nAnswer:", "\nAnswer:\nHurricane Harvey had top wind speeds of 130 mph. A Peregrine falcon can fly at speeds of up to 200 mph. 130 mph is less than 200 mph.\nSo the answer is: No.\n#\nContext:\nWho is Donald Trump? Donald Trump is the 45th President of the United States.\nWhat has Donald Trump said about Muslims? Donald Trump has said many things that can be perceived as Islamophobic such as \"Islam hates us\" and \"I think Islam is a problem in the world.\nIs Donald Trump a Muslim? No, Donald Trump is not a Muslim.\nWhat is Islamophobia? Islamophobia is the fear, hatred of, or prejudice against, the Islamic religion or Muslims.", "Is Donald Trump a Muslim? No, Donald Trump is not a Muslim.\nWhat is Islamophobia? Islamophobia is the fear, hatred of, or prejudice against, the Islamic religion or Muslims.\nWhat is the definition of Islamophobia? Islamophobia is the fear or hatred of Islam.\nDid Donald Trump show any fear or hatred of Islam? Yes, Donald Trump has made multiple statements that are critical of Islam.\nWhat is Islamophobia? Islamophobia is the fear or hatred of Muslims.\nDo Donald Trump's comments fit the definition of Islamophobia? Yes, Donald Trump's comments about Islam fit the definition of Islamophobia.\n\nQuestion:\nWas Donald Trump the target of Islamophobia?\n", "Was Donald Trump the target of Islamophobia?\n\nAnswer:\nIslamophobia is the fear or hatred of Islam. Donald Trump is not a Muslim.\nSo the answer is: No.\n#\nContext:\nWhen was Stone Cold Steve Austin born? Stone Cold Steve Austin was born on December 18, 1964.\nWhat is the latest year Stone Cold Steve Austin could have wrestled in? The latest year Stone Cold Steve Austin could have wrestled in is 1999.\nWhat is Stone Cold Steve Austin's birthdate? Stone Cold Steve Austin was born on December 18, 1964.", "What is the latest year Stone Cold Steve Austin could have wrestled in? The latest year Stone Cold Steve Austin could have wrestled in is 1999.\nWhat is Stone Cold Steve Austin's birthdate? Stone Cold Steve Austin was born on December 18, 1964.\nWhat centuries do those years belong to? 1964 and 1989 belong to the 20th century, and 2003 belongs to the 21st century.\nWhat centuries did December 18, 1964 fall in? December 18, 1964 falls in the 20th century.\nDid Stone Cold Steve Austin wrestle in the 20th century? Yes.\nWhat years was Stone Cold Steve Austin born and active as a wrestler? Stone Cold Steve Austin was born in 1964 and was an active wrestler from 1989-2003.\nWhat is the earliest year Stone Cold Steve Austin could have wrestled in? The earliest year Stone Cold Steve Austin could have wrestled in is 1900.\n\nQuestion:\nDid Stone Cold Steve Austin wrestle in two different centuries?", "Question:\nDid Stone Cold Steve Austin wrestle in two different centuries?\n\nAnswer:\nStone Cold Steve Austin was born in 1964 and was an active wrestler from 1989-2003. 1964 and 1989 belong to the 20th century, and 2003 belongs to the 21st century.\nSo the answer is: Yes.\n#\nContext:\nWhat is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility.\nWhat is Christopher Hitchens' religious affiliation? Christopher Hitchens was an atheist.", "What is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility.\nWhat is Christopher Hitchens' religious affiliation? Christopher Hitchens was an atheist.\nWhat is tonsure? Tonsure is a practice of cutting or shaving the hair from the scalp.\nWould Christopher Hitchens be likely to do something as a sign of religious devotion? No.\nWould Christopher Hitchens be religious? Christopher Hitchens was an atheist.\nIs an atheist likely to engage in religious practices? No, an atheist is not likely to engage in religious practices.\nWhat is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility.\nWhat is Christopher Hitchens' religious affiliation? Christopher Hitchens was an atheist.\nDo atheists usually engage in tonsure? No, atheists do not usually engage in tonsure as it is a religious practice.\n", "Do atheists usually engage in tonsure? No, atheists do not usually engage in tonsure as it is a religious practice.\n\nQuestion:\nWould Christopher Hitchens be very unlikely to engage in tonsure?\n\nAnswer:\nTonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility. Christopher Hitchens was an atheist. An atheist is not likely to engage in religious practices.\nSo the answer is: Yes.\n#\nContext:", "#\nContext:\nWhat is the evidence that Neanderthals used arithmetic? There is no direct evidence that Neanderthals used arithmetic, but indirect evidence suggests that they may have had some understanding of basic mathematical concepts.\nWhat is the earliest evidence of arithmetic? The earliest evidence of arithmetic dates back to the ancient Sumerians, who lived in what is now modern-day Iraq.\nDid Neanderthals live before the ancient Sumerians? Yes.\nWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers.\nIs there any reason to believe that Neanderthals could not have used arithmetic? There is no reason to believe that Neanderthals could not have used arithmetic.\nDo we have any evidence that Neanderthals could manipulate numbers? We have no evidence that Neanderthals could manipulate numbers.\nWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers.\nWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the addition, subtraction, multiplication, and division of whole numbers and with the manipulation of fractions and decimals.", "What is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers.\nWhat is arithmetic? Arithmetic is the branch of mathematics that deals with the addition, subtraction, multiplication, and division of whole numbers and with the manipulation of fractions and decimals.\n\nQuestion:\nDid Neanderthals use arithmetic?\n\nAnswer:\nThere is no evidence that Neanderthals used arithmetic. The earliest evidence of arithmetic dates back to the ancient Sumerians who lived after the Neanderthals.\nSo the answer is: No.\n#", "So the answer is: No.\n#\nContext:\"\"\"\n\nwikihop_entailment = \"\"\"Given a question and a context, answer the question and explain why. If you are unsure, answer Unknown.\n#\nContext:\nWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.\nWhere was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba.\nWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.", "Where was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba.\nWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.\nWhere was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba.\nWho is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Mary.\nWhen and where was Mary born? Mary was born in c. 18 BC or September 8 (21), 16 BC Herodian Kingdom of Judea.\n\nQuestion:\nWhere was the wife of Douglas Leiterman born?\n\nAnswer:", "\nAnswer:\nThe wife of Douglas Leiterman is Beryl Fox. Beryl Fox was born in Winnipeg, Manitoba.\nSo the answer is: Winnipeg, Manitoba.\n#\nContext:\nWho is the director of El extra\u00f1o viaje? The director of El extra\u00f1o viaje is Fernando Fern\u00e1n G\u00f3mez.\nWho is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\nWhen was Fernando Fern\u00e1n G\u00f3mez born? Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nWhen was Charles Saunders born? Charles Saunders was born on July 12, 1946.", "When was Fernando Fern\u00e1n G\u00f3mez born? Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nWhen was Charles Saunders born? Charles Saunders was born on July 12, 1946.\nWho is the director of El extra\u00f1o viaje? The director of El extra\u00f1o viaje is Fernando Fern\u00e1n G\u00f3mez.\nWhen was Fernando Fern\u00e1n G\u00f3mez born? Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nWho is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\nWhen was Charles Saunders born? Charles Saunders was born on July 12, 1946.\nWho is the director of El extra\u00f1o viaje? The director of El extra\u00f1o viaje is Fernando Fern\u00e1n G\u00f3mez\nWho is the director of Love in Pawn? The director of Love in Pawn is Charles Saunders.\nWhen was Fernando Fern\u00e1n G\u00f3mez born? Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nWhen was Charles Saunders (director) born? Charles Saunders (director) was born on 8 April 1904.", "When was Fernando Fern\u00e1n G\u00f3mez born? Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nWhen was Charles Saunders (director) born? Charles Saunders (director) was born on 8 April 1904.\n\nQuestion:\nWhich film has the director who was born later, El Extra\u00f1o Viaje or Love In Pawn?\n\nAnswer:\nThe director of El extra\u00f1o viaje is Fernando Fern\u00e1n G\u00f3mez. The director of Love in Pawn is Charles Saunders. Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921. Charles Saunders (director) was born on 8 April 1904.\nSo the answer is: El extra\u00f1o viaje.\n#", "So the answer is: El extra\u00f1o viaje.\n#\nContext:\nWhat year did Blind Shaft come out? Blind Shaft came out in 2003.\nWhat year did The Mask of Fu Manchu come out? The Mask of Fu Manchu came out in 1932.\nWhen did Blind Shaft come out? Blind Shaft came out in 2003.\nWhen did The Mask of Fu Manchu come out? The Mask of Fu Manchu came out on December 2, 1932.\nWhen was the film Blind Shaft released? Blind Shaft was released on 4 February 2004.\nWhen was the film The Mask of Fu Manchu released? The Mask of Fu Manchu was released on December 2, 1932.\n", "When was the film The Mask of Fu Manchu released? The Mask of Fu Manchu was released on December 2, 1932.\n\nQuestion:\nWhich film came out first, Blind Shaft or The Mask Of Fu Manchu?\n\nAnswer:\nBlind Shaft came out in 2003. The Mask of Fu Manchu came out in 1932. \nSo the answer is: The Mask of Fu Manchu.\n#\nContext:", "#\nContext:\nWhen did John V, Prince Of Anhalt-Zerbst's father die? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nWho is the father of John V, Prince of Anhalt-Zerbst? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nWho is the father of John V, Prince of Anhalt-Zerbst? The father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nWhen did Ernest I, Prince of Anhalt-Dessau die? Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\n\nQuestion:", "\nQuestion:\nWhen did John V, Prince Of Anhalt-Zerbst's father die?\n\nAnswer:\nThe father of John V, Prince of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau. Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nSo the answer is: 12 June 1516.\n#\nContext:\nWho is the husband of Catherine of Pomerania? The husband of Catherine of Pomerania is John II, Count Palatine of Neumarkt.", "Context:\nWho is the husband of Catherine of Pomerania? The husband of Catherine of Pomerania is John II, Count Palatine of Neumarkt.\nWho is the father of John II, Count Palatine of Neumarkt? The father of John II, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\nWho are the parents of Rupert III, Elector Palatine? The parents of Rupert III, Elector Palatine are Rupert II, Elector Palatine and Beatrice of Aragon.\nWho is Beatrice of Aragon's father? The father of Beatrice of Aragon is King Ferdinand I of Naples.\nWho is Catherine Of Pomerania, Countess Palatine Of Neumarkt's husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt.\nWho is the father of John I, Count Palatine of Neumarkt? The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\nWho is the father of Rupert III, Elector Palatine? The father of Rupert III, Elector Palatine is Rupert II, Elector Palatine.\nWho is Catherine Of Pomerania, Countess Palatine Of Neumarkt's husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John II, Count of Holstein-Rendsburg.\nWho is the father of John II, Count of Holstein-Rendsburg? The father of John II, Count of Holstein-Rendsburg is Henry II, Count of Holstein-Rendsburg.", "Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt's husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John II, Count of Holstein-Rendsburg.\nWho is the father of John II, Count of Holstein-Rendsburg? The father of John II, Count of Holstein-Rendsburg is Henry II, Count of Holstein-Rendsburg.\n\nQuestion:\nWho is Catherine Of Pomerania, Countess Palatine Of Neumarkt's father-in-law?\n\nAnswer:\nThe husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt. The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\nSo the answer is: Rupert III, Elector Palatine.\n#", "So the answer is: Rupert III, Elector Palatine.\n#\nContext:\nWho is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\nWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\nWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\nWhen did Elio Petri die? Elio Petri died on 10 November 1982.\nWho is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\nWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\nWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.", "Who is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\nWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\nWhen did Elio Petri die? Elio Petri died on 10 November 1982.\nWho is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky.\nWhen did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995.\nWho is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\nWhen did Elio Petri die? Elio Petri died on 10 November 1982.\n\nQuestion:\nWhich film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?", "Question:\nWhich film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\n\nAnswer:\nThe director of Crimen A Las Tres is Luis Saslavsky. The director of The Working Class Goes to Heaven is Elio Petri. Luis Saslavsky died on March 20, 1995. Elio Petri died on 10 November 1982.\nSo the answer is: The Working Class Goes To Heaven.\n#\nContext:\"\"\"\n\nwikihop_decompositions_with_retrieval_context_first = \"\"\"Given the following question, answer it by providing follow up questions and intermediate answers. If no follow up questions are necessary, answer the question directly. You are also provided with the most relevant google snippet for each intermediate question.", "\nwikihop_decompositions_with_retrieval_context_first = \"\"\"Given the following question, answer it by providing follow up questions and intermediate answers. If no follow up questions are necessary, answer the question directly. You are also provided with the most relevant google snippet for each intermediate question.\n#\nContext1: Xawery \u017bu\u0142awski: Polish-Russian War (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery \u017bu\u0142awski based on the novel Polish-Russian War under the white-red flag by Dorota Mas\u0142owska. So the answer is Xawery \u017bu\u0142awski.\nContext2: Xawery \u017bu\u0142awski: Xawery \u017bu\u0142awski ; National Film School in \u0141\u00f3d\u017a \u00b7 1995\u2013present \u00b7 Maria Strzelecka \u00b7 2.\nQuestion: Who is the mother of the director of film Polish-Russian War (Film)?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of the film Polish-Russian War (Film)?\nIntermediate answer: The director of the film Polish-Russian War is Xawery \u017bu\u0142awski.\nFollow up: Who is the mother of Xawery \u017bu\u0142awski?", "Intermediate answer: The director of the film Polish-Russian War is Xawery \u017bu\u0142awski.\nFollow up: Who is the mother of Xawery \u017bu\u0142awski?\nIntermediate answer: The mother of Xawery \u017bu\u0142awski is Ma\u0142gorzata Braunek.\nSo the final answer is: Rick Scott Ma\u0142gorzata Braunek.\n#\nContext1: 2003: Blind Shaft (Chinese: \u76f2\u4e95; pinyin: M\u00e1ngj\u01d0ng) is a 2003 film about a pair of brutal con artists operating in the illegal coal mines of present-day northern China. So the answer is 2003.\nContext2: December 2, 1932: Release and reception. The Mask of Fu Manchu opened in New York on December 2, 1932. The film cost a total of $338,000 and had worldwide rentals of $625,000. It had a profit of $62,000. So the answer is December 2, 1932.\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nAre follow up questions needed here: Yes.\nFollow up: When did Blind Shaft come out?", "Are follow up questions needed here: Yes.\nFollow up: When did Blind Shaft come out?\nIntermediate answer: Blind Shaft came out in 2003.\nFollow up: When did The Mask Of Fu Manchu come out?\nIntermediate answer: The Mask Of Fu Manchu came out in 1932.\nSo the final answer is: The Mask Of Fu Manchu.\n#\nContext1: John V, Prince of Anhalt-Zerbst: John was the second (but eldest surviving) son of Ernest I, Prince of Anhalt-Dessau, by his wife Margarete, daughter of Henry I, Duke of M\u00fcnsterberg-Oels, and granddaughter of George of Pod\u011bbrady, King of Bohemia.\nContext2: 12 June 1516: Ernest I, Prince of Anhalt-Dessau (died Dessau, 12 June 1516), was a German prince of the House of Ascania and ruler of the principality of Anhalt-Dessau. So the answer is 12 June 1516.\nQuestion: When did John V, Prince Of Anhalt-Zerbst's father die?", "Context2: 12 June 1516: Ernest I, Prince of Anhalt-Dessau (died Dessau, 12 June 1516), was a German prince of the House of Ascania and ruler of the principality of Anhalt-Dessau. So the answer is 12 June 1516.\nQuestion: When did John V, Prince Of Anhalt-Zerbst's father die?\nAre follow up questions needed here: Yes.\nFollow up: Who is the father of John V, Prince Of Anhalt-Zerbst?\nIntermediate answer: The father of John V, Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nFollow up: When did Ernest I, Prince of Anhalt-Dessau die?\nIntermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nSo the final answer is: 12 June 1516\n#\nContext1: El extra\u00f1o viaje: El extra\u00f1o viaje (English: The Strange Voyage) is a 1964 Spanish black drama film directed by Fernando Fern\u00e1n G\u00f3mez.", "#\nContext1: El extra\u00f1o viaje: El extra\u00f1o viaje (English: The Strange Voyage) is a 1964 Spanish black drama film directed by Fernando Fern\u00e1n G\u00f3mez.\nContext2: Love in Pawn: Love in Pawn is a 1953 British comedy film directed by Charles Saunders and starring Bernard Braden, Barbara Kelly and Jeannie Carson.\nContext3: 28 August 1921: Fernando Fern\u00e1ndez G\u00f3mez (28 August 1921 \u2013 21 November 2007) better known as Fernando Fern\u00e1n G\u00f3mez was a Spanish actor, screenwriter, film director, theater director and member of the Royal Spanish Academy for seven years. So the answer is 28 August 1921.\nContext4: Charles Saunders (director): Charles Joel Saunders (8 April 1904 \u2013 20 April 1997) was an English film director and screenwriter who began in the industry as a film editor, and who also contributed to television.\nQuestion: Which film has the director who was born later, El Extra\u00f1o Viaje or Love In Pawn?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of El Extra\u00f1o Viaje?\nIntermediate answer: The director of El Extra\u00f1o Viaje is Fernando Fern\u00e1n G\u00f3mez.\nFollow up: Who is the director of Love in Pawn?", "Intermediate answer: The director of El Extra\u00f1o Viaje is Fernando Fern\u00e1n G\u00f3mez.\nFollow up: Who is the director of Love in Pawn?\nIntermediate answer: The director of Love in Pawn is Charles Saunders.\nFollow up: When was Fernando Fern\u00e1n G\u00f3mez born?\nIntermediate answer: Fernando Fern\u00e1n G\u00f3mez was born on 28 August 1921.\nFollow up: When was Charles Saunders (director) born?\nIntermediate answer: Charles Saunders was born on 8 April 1904.\nSo the final answer is: El Extra\u00f1o Viaje.\n#\nContext1: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.", "#\nContext1: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.\nContext2: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407.\nQuestion: Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt's father-in-law?\nAre follow up questions needed here: Yes.\nFollow up: Who is the husband of Catherine of Pomerania, Countess Palatine of Neumarkt?\nIntermediate answer: The husband of Catherine of Pomerania, Countess Palatine of Neumarkt is John, Count Palatine of Neumarkt.\nFollow up: Who is the father of John, Count Palatine of Neumarkt?\nIntermediate answer: The father of John, Count Palatine of Neumarkt is Rupert III of the Palatinate.\nSo the final answer is: Rupert III of the Palatinate.", "Intermediate answer: The father of John, Count Palatine of Neumarkt is Rupert III of the Palatinate.\nSo the final answer is: Rupert III of the Palatinate.\n#\nContext1: Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Crimen a las tres. Directed by, Luis Saslavsky.\nContext2: Elio Petri: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. So the answer is Elio Petri.\nContext3: March 20, 1995: Luis Saslavsky (April 21, 1903 \u2013 March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. So the answer is March 20, 1995.\nContext4: Elio Petri: Final years. In 1981, Petri visited Geneva to direct Arthur Miller\\'s new play The American Clock, with Marcello Mastroianni playing the lead role. Petri died of cancer on 10 November 1982. He was 53 years old.\nQuestion: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Crimen a las tres?", "Are follow up questions needed here: Yes.\nFollow up: Who is the director of Crimen a las tres?\nIntermediate answer: The director of Crimen a las tres is Luis Saslavsky.\nFollow up: Who is the director of The Working Class Goes to Heaven?\nIntermediate answer: The director of The Working Class Goes to Heaven is Elio Petri.\nFollow up: When did Luis Saslavsky die?\nIntermediate answer: Luis Saslavsky died on March 20, 1995.\nFollow up: When did Elio Petri die?\nIntermediate answer: Elio Petri died on 10 November 1982.\nSo the final answer is: The Working Class Goes to Heaven", "Intermediate answer: Elio Petri died on 10 November 1982.\nSo the final answer is: The Working Class Goes to Heaven\n#\"\"\"\n"]}
{"filename": "src/gpt3_accessors/gpt_accessor_factory.py", "chunked_list": ["from typing import Dict, Type\n\nfrom src.common.abstract_factory import AbstractFactory\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_simple import GptAccessorSimple\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_with_retrieval import (\n    GptAccessorWithRetrieval,\n)\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_with_retrieval_context_first import (\n    GptAccessorWithRetrievalContextFirst,", "from src.gpt3_accessors.gpt3_accessors.gpt_accessor_with_retrieval_context_first import (\n    GptAccessorWithRetrievalContextFirst,\n)\n\n\nclass GptAccessorFactory(AbstractFactory):\n    \"\"\" \"\"\"\n\n    def get_instance_name_to_class_dict(self) -> Dict[str, Type[GptAccessor]]:\n        return {\n            \"gpt_accessor_simple\": GptAccessorSimple,\n            \"gpt_accessor_with_retrieval\": GptAccessorWithRetrieval,\n            \"gpt_accessor_with_retrieval_context_first\": GptAccessorWithRetrievalContextFirst,\n        }", ""]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_base.py", "chunked_list": ["class GptAccessor:\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls()\n\n    def call_gpt(\n        self, prompt: str, stop: str, temperature: float, *args, **kwargs\n    ) -> str:\n        raise NotImplementedError(\"Please Implement this method\")\n", ""]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_with_retrieval.py", "chunked_list": ["import time\n\nimport openai\nimport os\n\nfrom src.common.config import Config\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.opeanai.utils import greenify\nfrom src.serpapi.serpapi import (\n    google,", "from src.serpapi.serpapi import (\n    google,\n    get_question_wiki_snippet,\n    get_question_google_snippet,\n)\n\nopenai.api_key = os.getenv(\n    \"OPENAI_KEY\"\n)  # get one from https://openai.com , first few requests are free!\n", ")  # get one from https://openai.com , first few requests are free!\n\n\nclass GptAccessorWithRetrieval(GptAccessor):\n    def call_gpt(self, cur_prompt, stop, temperature):\n        res = \"\"\n        retries = 3\n        # iterate decomposition for 5 steps\n        for i in range(5):\n            # get gpt ans with retries\n            for i in range(retries):\n                try:\n                    ans = openai.Completion.create(\n                        model=\"code-davinci-002\",\n                        max_tokens=512,\n                        stop=[\"Context:\", \"#\"],\n                        prompt=cur_prompt,\n                        temperature=temperature,\n                    )\n                    break\n                except Exception as e:\n                    print(\"exception thrown, sleeping...\", e)\n                    time.sleep(60)\n                    print(\"finished sleeping\")\n\n            # add context\n            returned = ans[\"choices\"][0][\"text\"]\n            res += returned\n            cur_prompt += returned\n            if \"Follow up: \" in returned:\n                question = returned.split(\"Follow up: \")[-1].replace(\"\\n\", \"\")\n                retrieval = get_question_wiki_snippet(question, cache=True)\n                cur_prompt += f\"Context: {retrieval}\\n\"\n                res += f\"Context: {retrieval}\\n\"\n            if \"So the final answer is: \" in returned:\n                print(greenify(res), end=\"\")\n                return res\n            print(greenify(res), end=\"\")\n\n        return res", ""]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_simple.py", "chunked_list": ["import time\n\nimport openai\nimport os\n\nfrom src.common.config import Config\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.opeanai.utils import greenify\n\nopenai.api_key = os.getenv(", "\nopenai.api_key = os.getenv(\n    \"OPENAI_KEY\"\n)  # get one from https://openai.com , first few requests are free!\n\n\nclass GptAccessorSimple(GptAccessor):\n    def call_gpt(self, cur_prompt, stop, temperature):\n        retries = 3\n        # get gpt ans with retries\n        for i in range(retries):\n            try:\n                ans = openai.Completion.create(\n                    model=\"code-davinci-002\",\n                    max_tokens=512,\n                    stop=\"#\",\n                    prompt=cur_prompt,\n                    temperature=temperature,\n                )\n                break\n            except Exception as e:\n                print(\"exception thrown, sleeping...\", e)\n                time.sleep(30)\n                print(\"finished sleeping\")\n\n        # add context\n        returned = ans[\"choices\"][0][\"text\"]\n        print(greenify(returned), end=\"\")\n        return returned", ""]}
{"filename": "src/gpt3_accessors/gpt3_accessors/gpt_accessor_with_retrieval_context_first.py", "chunked_list": ["import time\n\nimport openai\nimport os\n\nfrom src.common.config import Config\nfrom src.gpt3_accessors.gpt3_accessors.gpt_accessor_base import GptAccessor\nfrom src.opeanai.utils import greenify, change_openaikey_and_sleep, gpt_simple_generator\nfrom src.serpapi.serpapi import (\n    google,", "from src.serpapi.serpapi import (\n    google,\n    get_question_wiki_snippet,\n    get_question_google_snippet,\n)\nimport re\n\nopenai.api_key = os.getenv(\n    \"OPENAI_KEY\"\n)  # get one from https://openai.com , first few requests are free!", "    \"OPENAI_KEY\"\n)  # get one from https://openai.com , first few requests are free!\n\n\nFOLLOW_UP_REGEX_PATTERN = r\"Follow up:.*\\n\"\n\n\nclass GptAccessorWithRetrievalContextFirst(GptAccessor):\n    def call_gpt(\n        self, orig_prompt, stop, temperature, orig_question, model, decomposition_index\n    ):\n        print(\n            f\"\\n====================== Decomposition {decomposition_index} ======================\"\n        )\n        contexts = (\n            [get_question_wiki_snippet(orig_question, cache=True)]\n            if Config().get(\"decomposition.retrieve_orig_question\")\n            else []\n        )\n        stop_condition = \"Intermediate answer:\"\n        res = \"\"\n        retries = 6\n        # iterate decomposition for 5 steps\n        for i in range(5):\n            # format input, the input should be:\n            # contexts, followed by intermediate QAs\n            context_formatted = \"\\n\".join(\n                [f\"Context{k+1}: {context}\" for k, context in enumerate(contexts)]\n            )\n            context_formatted = (\n                \"\\n\" + context_formatted\n                if len(context_formatted)\n                else context_formatted\n            )\n            cur_prompt = (\n                orig_prompt\n                + context_formatted\n                + \"\\nQuestion: \"\n                + orig_question\n                + \"\\nAre follow up questions needed here: Yes.\"\n                + res\n            )\n\n            # get gpt ans with retries\n            for i in range(retries):\n                try:\n                    ans = openai.Completion.create(\n                        model=model,\n                        max_tokens=512,\n                        stop=[\"Context:\", \"#\"],\n                        prompt=cur_prompt,\n                        temperature=temperature,\n                    )\n                    break\n                except Exception as e:\n                    print(f\"exception thrown, sleeping...\")\n                    print(e)\n                    change_openaikey_and_sleep()\n\n            # add context\n            returned = ans[\"choices\"][0][\"text\"]\n\n            # check the stop condition, it's different if we generate the first follow up or if we are in an intermediate step\n            # if this is the first follow up, break on the first intermediate question\n            if stop_condition == \"Intermediate answer:\":\n                # add the follow-up question to the prompt and change the stop condition\n                res += returned.split(\"Intermediate answer:\")[0]\n                stop_condition = \"Follow up:\"\n\n            # else, we are in an intermediate step\n            elif stop_condition == \"Follow up:\":\n                # continue until we find the next follow up question\n                # since we want to add retrieved contexts given this question\n                followup_split = re.split(FOLLOW_UP_REGEX_PATTERN, returned)\n\n                # add everything until the next follow up\n                res += followup_split[0]\n\n                # add the next follow up\n                if len(followup_split) > 1:\n                    res += re.findall(FOLLOW_UP_REGEX_PATTERN, returned)[0]\n\n            # make sure the result does not end in a new line\n            if res[-1] == \"\\n\":\n                res = res[:-1]\n\n            # if we are in an intermediate step, add the retrieved context to the list of contexts\n            if \"Follow up: \" in returned:\n                # get the first follow up\n                question = [l for l in returned.split(\"\\n\") if \"Follow up: \" in l][\n                    0\n                ].split(\"Follow up: \")[-1]\n                retrieval = get_question_wiki_snippet(question, cache=True)\n                contexts.append(retrieval)\n\n            # end when the final answer is generated, this means that no follow up questions were asked\n            elif \"So the final answer is: \" in returned:\n                final_res = (\n                    context_formatted\n                    + \"\\nQuestion: \"\n                    + orig_question\n                    + \"\\nAre follow up questions needed here: Yes.\"\n                    + res\n                )\n                print(f\"\\nDecomposition {decomposition_index} result:\")\n                print(greenify(final_res), end=\"\")\n\n                # return the contexts followed by the original question, intermediate QAs and final answer\n                return final_res\n            print(greenify(res), end=\"\")\n            print(\"\\n\")\n\n        return res", ""]}
