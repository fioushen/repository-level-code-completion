{"filename": "model_init.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nimport argparse, sys, os, glob\nfrom torch import version as torch_version\nfrom globals import set_affinity_str\n\ndef add_args(parser):\n\n    parser.add_argument(\"-t\", \"--tokenizer\", type = str, help = \"Tokenizer model path\")\n    parser.add_argument(\"-c\", \"--config\", type = str, help = \"Model config path (config.json)\")\n    parser.add_argument(\"-m\", \"--model\", type = str, help = \"Model weights path (.pt or .safetensors file)\")\n    parser.add_argument(\"-d\", \"--directory\", type = str, help = \"Path to directory containing config.json, model.tokenizer and * .safetensors\")\n\n    parser.add_argument(\"-gs\", \"--gpu_split\", type = str, help = \"Comma-separated list of VRAM (in GB) to use per GPU device for model layers, e.g. -gs 20,7,7\")\n    parser.add_argument(\"-l\", \"--length\", type = int, help = \"Maximum sequence length\", default = 2048)\n    parser.add_argument(\"-cpe\", \"--compress_pos_emb\", type = float, help = \"Compression factor for positional embeddings\", default = 1.0)\n    parser.add_argument(\"-a\", \"--alpha\", type = float, help = \"alpha for context size extension via embedding extension\", default = 1.0)\n    parser.add_argument(\"-theta\", \"--theta\", type = float, help = \"theta (base) for RoPE embeddings\")\n\n    parser.add_argument(\"-gpfix\", \"--gpu_peer_fix\", action = \"store_true\", help = \"Prevent direct copies of data between GPUs\")\n\n    parser.add_argument(\"-flash\", \"--flash_attn\", nargs = '?', const = 'default', metavar = \"METHOD\", help = \"Use Flash Attention with specified input length (must have Flash Attention 2.0 installed)\")\n\n    parser.add_argument(\"-mmrt\", \"--matmul_recons_thd\", type = int, help = \"No. rows at which to use reconstruction and cuBLAS for quant matmul. 0 = never, 1 = always\", default = 8)\n    parser.add_argument(\"-fmt\", \"--fused_mlp_thd\", type = int, help = \"Maximum no. of rows for which to use fused MLP. 0 = never\", default = 2)\n    parser.add_argument(\"-sdpt\", \"--sdp_thd\", type = int, help = \"No. rows at which to switch to scaled_dot_product_attention. 0 = never, 1 = always\", default = 8)\n    parser.add_argument(\"-mmfr\", \"--matmul_fused_remap\", action = \"store_true\", help = \"Fuse column remapping in Q4 matmul kernel\")\n    parser.add_argument(\"-nfa\", \"--no_fused_attn\", action = \"store_true\", help = \"Disable fused attention\")\n\n    parser.add_argument(\"-rnnh2\", \"--rmsnorm_no_half2\", action = \"store_true\", help = \"Don't use half2 in RMS norm kernel\")\n    parser.add_argument(\"-rpnh2\", \"--rope_no_half2\", action = \"store_true\", help = \"Don't use half2 in RoPE kernel\")\n    parser.add_argument(\"-mmnh2\", \"--matmul_no_half2\", action = \"store_true\", help = \"Don't use half2 in Q4 matmul kernel\")\n    parser.add_argument(\"-snh2\", \"--silu_no_half2\", action = \"store_true\", help = \"Don't use half2 in SiLU kernel\")\n    parser.add_argument(\"-nh2\", \"--no_half2\", action = \"store_true\", help = \"(All of the above) disable half2 in all kernela\")\n    parser.add_argument(\"-fh2\", \"--force_half2\", action = \"store_true\", help = \"Force enable half2 even if unsupported\")\n    parser.add_argument(\"-cs\", \"--concurrent_streams\", action = \"store_true\", help = \"Use concurrent CUDA streams\")\n\n    parser.add_argument(\"-aff\", \"--affinity\", type = str, help = \"Comma-separated list, sets processor core affinity. E.g.: -aff 0,1,2,3\")", "\n\ndef post_parse(args):\n\n    if args.no_half2 or torch_version.hip and not args.force_half2:\n        args.rmsnorm_no_half2 = True\n        args.rope_no_half2 = True\n        args.matmul_no_half2 = True\n        args.silu_no_half2 = True\n", "\n\n# Get model files from --directory\n\ndef get_model_files(args):\n\n    if args.directory is not None:\n        args.tokenizer = os.path.join(args.directory, \"tokenizer.model\")\n        args.config = os.path.join(args.directory, \"config.json\")\n        st_pattern = os.path.join(args.directory, \"*.safetensors\")\n        st = glob.glob(st_pattern)\n        if len(st) == 0:\n            print(f\" !! No files matching {st_pattern}\")\n            sys.exit()\n        if len(st) > 1:\n            print(f\" !! Multiple files matching {st_pattern}\")\n            sys.exit()\n        args.model = st[0]\n    else:\n        if args.tokenizer is None or args.config is None or args.model is None:\n            print(\" !! Please specify either -d or all of -t, -c and -m\")\n            sys.exit()", "\n\n# Feedback\n\ndef print_options(args, extra_options = None):\n\n    print_opts = []\n    if args.gpu_split is not None: print_opts.append(f\"gpu_split: {args.gpu_split}\")\n    if args.gpu_peer_fix: print_opts.append(\"gpu_peer_fix\")\n    if args.affinity: print_opts.append(f\" --affinity: {args.affinity}\")\n\n    if extra_options is not None: print_opts += extra_options\n\n    print(f\" -- Tokenizer: {args.tokenizer}\")\n    print(f\" -- Model config: {args.config}\")\n    print(f\" -- Model: {args.model}\")\n    print(f\" -- Sequence length: {args.length}\")\n    if args.compress_pos_emb != 1.0:\n        print(f\" -- RoPE compression factor: {args.compress_pos_emb}\")\n\n    if args.alpha != 1.0:\n        print(f\" -- RoPE alpha factor: {args.alpha}\")\n\n    print(f\" -- Tuning:\")\n\n    if args.flash_attn: print(f\" -- --flash_attn\")\n    else: print(f\" -- --sdp_thd: {args.sdp_thd}\" + (\" (disabled)\" if args.sdp_thd == 0 else \"\"))\n\n    print(f\" -- --matmul_recons_thd: {args.matmul_recons_thd}\" + (\" (disabled)\" if args.matmul_recons_thd == 0 else \"\"))\n    print(f\" -- --fused_mlp_thd: {args.fused_mlp_thd}\" + (\" (disabled)\" if args.fused_mlp_thd == 0 else \"\"))\n    if args.matmul_fused_remap: print(f\" -- --matmul_fused_remap\")\n    if args.no_fused_attn: print(f\" -- --no_fused_attn\")\n    if args.rmsnorm_no_half2: print(f\" -- --rmsnorm_no_half2\")\n    if args.rope_no_half2: print(f\" -- --rope_no_half2\")\n    if args.matmul_no_half2: print(f\" -- --matmul_no_half2\")\n    if args.silu_no_half2: print(f\" -- --silu_no_half2\")\n    if args.concurrent_streams: print(f\" -- --concurrent_streams\")\n\n    print(f\" -- Options: {print_opts}\")", "\n\n# Build ExLlamaConfig from args\n\ndef make_config(args):\n\n    config = ExLlamaConfig(args.config)\n    config.model_path = args.model\n\n    config.max_seq_len = args.length\n    config.compress_pos_emb = args.compress_pos_emb\n    config.set_auto_map(args.gpu_split)\n    config.gpu_peer_fix = args.gpu_peer_fix\n    config.alpha_value = args.alpha\n    config.calculate_rotary_embedding_base()\n\n    if args.flash_attn:\n        config.use_flash_attn_2 = True\n        try:\n            config.max_input_len = int(args.flash_attn)\n        except ValueError:\n            pass\n\n    config.matmul_recons_thd = args.matmul_recons_thd\n    config.fused_mlp_thd = args.fused_mlp_thd\n    config.sdp_thd = args.sdp_thd\n    config.matmul_fused_remap = args.matmul_fused_remap\n    config.fused_attn = not args.no_fused_attn\n\n    config.rmsnorm_no_half2 = args.rmsnorm_no_half2\n    config.rope_no_half2 = args.rope_no_half2\n    config.matmul_no_half2 = args.matmul_no_half2\n    config.silu_no_half2 = args.silu_no_half2\n    config.concurrent_streams = args.concurrent_streams\n\n    if args.theta:\n        config.rotary_embedding_base = args.theta\n\n    return config", "\n\n# Global state\n\ndef set_globals(args):\n\n    if args.affinity: set_affinity_str(args.affinity)\n\n\n# Print stats after loading model", "\n# Print stats after loading model\n\ndef print_stats(model):\n\n    print(f\" -- Groupsize (inferred): {model.config.groupsize if model.config.groupsize is not None else 'None'}\")\n    print(f\" -- Act-order (inferred): {'yes' if model.config.act_order else 'no'}\")\n    if model.config.empty_g_idx:\n        print(f\" !! Model has empty group index (discarded)\")\n", ""]}
{"filename": "example_cfg.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport torch\nimport torch.nn.functional as F\nimport os, glob\nimport cuda_ext\n\n# Directory containing model, tokenizer, generator\n", "# Directory containing model, tokenizer, generator\n\nmodel_directory =  \"/mnt/str/models/_test_models/TheBloke_Llama-2-13B-chat-GPTQ/\"\n\n# Locate files we need within that directory\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]", "st_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\n\n# Create config, model, tokenizer and generator\n\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file", "model = ExLlama(config)                                 # create ExLlama instance and load the weights\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n\ncache = ExLlamaCache(model, batch_size = 2)             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n\n# Configure generator\n\ngenerator.settings.token_repetition_penalty_max = 1.15\ngenerator.settings.temperature = 0.95", "generator.settings.token_repetition_penalty_max = 1.15\ngenerator.settings.temperature = 0.95\ngenerator.settings.top_k = 40\ngenerator.settings.top_p = 0.75\n# generator.settings.typical = 0.95\n\n# Prompts to mix\n\nf1 = \\\n\"\"\"[INST] <<SYS>>", "f1 = \\\n\"\"\"[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n{prompt}[/INST]\"\"\"\n\nf2 = \\\n\"\"\"[INST] <<SYS>>\n<</SYS>>\nYou are a rude and obnoxious assistant. You hate everything and everyone.", "<</SYS>>\nYou are a rude and obnoxious assistant. You hate everything and everyone.\n{prompt}[/INST]\"\"\"\n\n\nprompts = \\\n[\n    f1.replace(\"{prompt}\", \"Tell me about Homer Simpson\"),\n    f2.replace(\"{prompt}\", \"Tell me about Homer Simpson\"),\n]", "    f2.replace(\"{prompt}\", \"Tell me about Homer Simpson\"),\n]\n\ndef generate_cfg(prompts, alpha, max_new_tokens):\n\n    ids, mask = tokenizer.encode(prompts, return_mask = True)\n    generator.gen_begin(ids, mask = mask)\n\n    # Sampling loop\n\n    for _ in range(max_new_tokens):\n\n        logits = model.forward(generator.sequence[:, -1:], cache, input_mask = mask)\n        generator.apply_rep_penalty(logits)\n\n        logits = F.log_softmax(logits, dim = -1)\n        logits_mixed = (1 - alpha) * logits[0] + alpha * logits[1]\n\n        sampled_token, _ = generator.sample_current(logits_mixed)\n        if sampled_token.item() == tokenizer.eos_token_id: break\n\n        batch_token = sampled_token.repeat(2, 1)\n        generator.gen_accept_token(batch_token)\n\n    output = tokenizer.decode(generator.sequence[0])\n    return output", "\nfor i in range(10):\n\n    alpha = i / 5.0 - 0.4\n    print()\n    print(f\"--------------------------------------\")\n    print(f\"alpha = {alpha:.1f}\")\n    print(f\"--------------------------------------\")\n    output = generate_cfg(prompts, alpha, 200)\n    print(output[len(prompts[0]):].strip())", ""]}
{"filename": "model.py", "chunked_list": ["import sys\nmin_version = (3, 9)\nif sys.version_info < min_version:\n    print(\"\")\n    print(f\" ## Warning: this project requires Python {min_version[0]}.{min_version[1]} or higher.\")\n    print(\"\")\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F", "from torch import nn\nimport torch.nn.functional as F\nfrom safetensors import safe_open\nimport cuda_ext\nimport json\nimport math\nimport gc\nfrom enum import Enum\n\ntry:\n    from flash_attn import flash_attn_func\nexcept:\n    pass", "\ntry:\n    from flash_attn import flash_attn_func\nexcept:\n    pass\n\nclass ParsedEnum(Enum):\n\n    def __str__(self):\n        return self.name.lower()\n\n    def __repr__(self):\n        return str(self)\n\n    @classmethod\n    def argparse(cls, s):\n        try:\n            return cls[s.upper()]\n        except KeyError:\n            return s", "\n\nclass ExLlamaConfig:\n\n    # Load config from Llama config.json\n\n    def __init__(self, model_config_path):\n\n        with open(model_config_path) as f:\n            read_config = json.load(f)\n\n        # Loaded/automatic settings\n\n        self.bos_token_id = read_config[\"bos_token_id\"] if \"bos_token_id\" in read_config else 1\n        self.eos_token_id = read_config[\"eos_token_id\"] if \"eos_token_id\" in read_config else 2\n        self.pad_token_id = read_config[\"pad_token_id\"] if \"pad_token_id\" in read_config else 0\n\n        self.hidden_size = read_config[\"hidden_size\"]\n        self.initializer_range = read_config[\"initializer_range\"]\n        self.intermediate_size = read_config[\"intermediate_size\"]\n        self.num_attention_heads = read_config[\"num_attention_heads\"]\n        self.num_hidden_layers = read_config[\"num_hidden_layers\"]\n        self.rms_norm_eps = read_config[\"rms_norm_eps\"]\n        self.vocab_size = read_config[\"vocab_size\"]\n\n        if \"num_key_value_heads\" in read_config:\n            self.num_key_value_heads = read_config[\"num_key_value_heads\"]\n            self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n        else:\n            self.num_key_value_heads = self.num_attention_heads\n            self.num_key_value_groups = 1\n\n        self.rotary_embedding_base = read_config[\"rope_theta\"] if \"rope_theta\" in read_config else 10000.0\n        self.head_dim = self.hidden_size // self.num_attention_heads\n\n        self.groupsize = None  # Autodetected\n        self.act_order = False  # Autodetected\n        self.empty_g_idx = False  # Autodetected\n\n        # Required settings\n\n        self.model_path = None\n        self.device_map = ExLlamaDeviceMap(self.num_hidden_layers)\n\n        # Optional settings\n\n        self.max_seq_len = 2048  # Reduce to save memory. Can also be increased, ideally while also using compress_pos_emn and a compatible model/LoRA\n        self.max_input_len = 2048  # Maximum length of input IDs in a single forward pass. Sequences longer than this will be processed in multiple steps\n        self.max_attention_size = 2048**2  # Sequences will be processed in chunks to keep the size of the attention weights matrix <= this\n        self.compress_pos_emb = 1.0  # Increase to compress positional embeddings applied to sequence\n        self.alpha_value = 1.0 # Alpha value for NTK RoPE scaling. Similar to compress_pos_emb, higher values increaste ctx but add Perplexity.\n        self.gpu_peer_fix = False # Apparently Torch can have problems transferring tensors directly one GPU to another sometimes. Enable this to expliticly move tensors via system RAM instead, where needed\n        self.auto_map = None  # List of floats with memory allocation in GB, per CUDA device, overrides device_map\n\n        # Tuning\n\n        self.use_flash_attn_2 = False\n        self.matmul_recons_thd = 8\n        self.fused_mlp_thd = 2\n        self.sdp_thd = 8\n        self.fused_attn = True\n        self.matmul_fused_remap = False\n        self.rmsnorm_no_half2 = False\n        self.rope_no_half2 = False\n        self.matmul_no_half2 = False\n        self.silu_no_half2 = False\n        self.concurrent_streams = False\n\n    # Copy tuning params to C++ extension\n\n    def set_tuning_params(self):\n\n        cuda_ext.exllama_ext.set_tuning_params(self.matmul_recons_thd,\n                                               self.fused_mlp_thd,\n                                               self.sdp_thd,\n                                               self.matmul_fused_remap,\n                                               self.rmsnorm_no_half2,\n                                               self.rope_no_half2,\n                                               self.matmul_no_half2,\n                                               self.silu_no_half2,\n                                               self.concurrent_streams)\n\n    # Parse and set list of GPU VRAM allocations\n\n    def set_auto_map(self, map_string):\n\n        if map_string is None: self.auto_map = None\n        else: self.auto_map = [float(alloc) for alloc in map_string.split(\",\")]\n\n    def calculate_rotary_embedding_base(self):\n        self.rotary_embedding_base = self.rotary_embedding_base * self.alpha_value ** (self.head_dim / (self.head_dim-2))", "\n\n# 4-bit linear layer implementation\n\nclass Ex4bitLinear:\n\n    def __init__(self, config, in_features, out_features, has_bias, tensors, key):\n\n        self.config = config\n        self.key = key\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.qweight = tensors[key + \".qweight\"]\n        self.qzeros = tensors[key + \".qzeros\"]\n        self.scales = tensors[key + \".scales\"]\n        self.g_idx = tensors[key + \".g_idx\"].cpu() if key + \".g_idx\" in tensors else None\n        self.bias = tensors[key + \".bias\"] if has_bias else None\n\n        if self.g_idx is not None and (self.g_idx == 0).all():\n            self.config.empty_g_idx = True\n            self.g_idx = None\n\n        self.device = self.qweight.device\n        self.device_index = self.device.index\n\n        self.q4 = cuda_ext.ext_make_q4(self.qweight,\n                                       self.qzeros,\n                                       self.scales,\n                                       self.g_idx,\n                                       self.device_index)\n\n        self.height = tensors[key + \".qweight\"].shape[0] * 8\n        self.width = tensors[key + \".qweight\"].shape[1]\n\n        # Infer groupsize from height of qzeros\n\n        self.groupsize = None\n        if self.qzeros.shape[0] > 1:\n            self.groupsize = (self.qweight.shape[0] * 8) // self.qzeros.shape[0]\n            if self.config.groupsize is None:\n                self.config.groupsize = self.groupsize\n\n\n        # Handle act-order matrix\n\n        if self.g_idx is not None:\n\n            if self.groupsize is None: raise ValueError(\"Found group index but no groupsize. What do?\")\n            self.config.act_order = True\n\n\n    def lora_applies(self, lora):\n\n        if lora is None: return False\n        return self.key + \".lora_A.weight\" in lora.tensors\n\n\n    def lora_apply(self, lora, x):\n\n        lora_a = lora.tensors[self.key + \".lora_A.weight\"]\n        lora_b = lora.tensors[self.key + \".lora_B.weight\"]\n        out = torch.matmul(x, lora_a)\n        out = torch.matmul(out, lora_b)\n        # out = cuda_ext.ext_half_matmul(x, lora_a.contiguous(), cublas = True)\n        # out = cuda_ext.ext_half_matmul(out, lora_b.contiguous(), cublas = True)\n        return out\n\n\n    def get_lora_tensors_or_meta(self, lora):\n\n        if not self.lora_applies(lora):\n            return cuda_ext.none_tensor, cuda_ext.none_tensor\n        else:\n            lora_a = lora.tensors[self.key + \".lora_A.weight\"]\n            lora_b = lora.tensors[self.key + \".lora_B.weight\"]\n            return lora_a, lora_b\n\n\n    def forward(self, x, lora):\n\n        if self.lora_applies(lora):\n            lora_a = lora.tensors[self.key + \".lora_A.weight\"]\n            lora_b = lora.tensors[self.key + \".lora_B.weight\"]\n            out = cuda_ext.ext_q4_matmul(x, self.q4, self.width, lora_a, lora_b)\n        else:\n            out = cuda_ext.ext_q4_matmul(x, self.q4, self.width)\n\n        # out = cuda_ext.ext_q4_matmul(x, self.q4, self.width)\n        # if self.lora_applies(lora):\n        #     out += self.lora_apply(lora, x)\n\n        if self.bias is not None: out.add_(self.bias)\n        return out", "\n\n# Llama MLP\n\nclass ExLlamaMLP:\n\n    def __init__(self, config, tensors, key):\n\n        self.config = config\n\n        self.gate_proj = Ex4bitLinear(config, self.config.hidden_size, self.config.intermediate_size, False, tensors, key + \".gate_proj\")\n        self.up_proj = Ex4bitLinear(config, self.config.hidden_size, self.config.intermediate_size, False, tensors, key + \".up_proj\")\n        self.down_proj = Ex4bitLinear(config, self.config.intermediate_size, self.config.hidden_size, False, tensors, key + \".down_proj\")\n\n        self.act_fn = nn.SiLU()\n\n    def fused(self, x, buffer, post_attention_layernorm, lora):\n\n        bsz, q_len, _ = x.size()\n\n        gate_a, gate_b = self.gate_proj.get_lora_tensors_or_meta(lora)\n        up_a, up_b = self.up_proj.get_lora_tensors_or_meta(lora)\n        down_a, down_b = self.down_proj.get_lora_tensors_or_meta(lora)\n\n        temp_size = 0\n        if not gate_a.is_meta: temp_size = max(temp_size, bsz * q_len * gate_a.shape[1])\n        if not up_a.is_meta:   temp_size = max(temp_size, bsz * q_len * up_a.shape[1])\n        if not down_a.is_meta: temp_size = max(temp_size, bsz * q_len * down_a.shape[1])\n\n        if temp_size > 0: lora_temp = torch.empty((1, temp_size), dtype = torch.float16, device = x.device)\n        else: lora_temp = cuda_ext.none_tensor\n\n        cuda_ext.exllama_ext.q4_mlp(x.view(-1, x.shape[-1]),\n                                    post_attention_layernorm.weight,\n                                    self.config.rms_norm_eps,\n                                    self.gate_proj.q4,\n                                    self.up_proj.q4,\n                                    self.down_proj.q4,\n                                    gate_a, gate_b,\n                                    up_a, up_b,\n                                    down_a, down_b,\n                                    lora_temp)\n\n\n    def forward(self, x, buffer, lora):\n\n        y = self.gate_proj.forward(x, lora)\n        y = self.act_fn(y)\n        y *= self.up_proj.forward(x, lora)\n        y = self.down_proj.forward(y, lora)\n\n        return y", "\n\n# RMS Layer norm.\n\nclass ExLlamaRMSNorm:\n\n    def __init__(self, config, tensors, key):\n\n        self.config = config\n        self.variance_epsilon = self.config.rms_norm_eps\n        self.weight = tensors[key]\n\n\n    def forward(self, hidden_states, buffer):\n\n        hidden_states = cuda_ext.ext_rms_norm(hidden_states, self.weight, self.variance_epsilon)\n        return hidden_states", "\n\n# Llama attention\n\nclass ExLlamaAttention:\n\n    def __init__(self, config, tensors, key, sin, cos, index):\n\n        self.config = config\n        self.sin = sin\n        self.cos = cos\n        self.index = index\n\n        self.q_proj = Ex4bitLinear(config, self.config.hidden_size, self.config.num_attention_heads * self.config.head_dim, False, tensors, key + \".q_proj\")\n        self.k_proj = Ex4bitLinear(config, self.config.hidden_size, self.config.num_key_value_heads * self.config.head_dim, False, tensors, key + \".k_proj\")\n        self.v_proj = Ex4bitLinear(config, self.config.hidden_size, self.config.num_key_value_heads * self.config.head_dim, False, tensors, key + \".v_proj\")\n        self.o_proj = Ex4bitLinear(config, self.config.num_attention_heads * self.config.head_dim, self.config.hidden_size, False, tensors, key + \".o_proj\")\n\n\n    def repeat_kv(self, hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n\n        # TODO: This seems inefficient. It should be possible to broadcast in the attention matmul to avoid building\n        # temporary K/V tensors like this\n\n        batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n        if n_rep == 1: return hidden_states\n\n        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\n    def fused(self, hidden_states, cache, buffer, input_layernorm, lora):\n\n        bsz, q_len, _ = hidden_states.size()\n        past_len = cache.current_seq_len\n\n        # Lora tensors\n\n        q_a, q_b = self.q_proj.get_lora_tensors_or_meta(lora)\n        k_a, k_b = self.k_proj.get_lora_tensors_or_meta(lora)\n        v_a, v_b = self.v_proj.get_lora_tensors_or_meta(lora)\n        o_a, o_b = self.o_proj.get_lora_tensors_or_meta(lora)\n\n        temp_size = 0\n        if not q_a.is_meta: temp_size = max(temp_size, bsz * q_len * q_a.shape[1])\n        if not k_a.is_meta: temp_size = max(temp_size, bsz * q_len * k_a.shape[1])\n        if not v_a.is_meta: temp_size = max(temp_size, bsz * q_len * v_a.shape[1])\n        if not o_a.is_meta: temp_size = max(temp_size, bsz * q_len * o_a.shape[1])\n        if temp_size > 0: lora_temp = torch.empty((1, temp_size), dtype = torch.float16, device = hidden_states.device)\n        else: lora_temp = cuda_ext.none_tensor\n\n        # Project q, k, v, apply position embeddings to k and v, update cache\n\n        query_states = torch.empty((bsz, q_len, self.config.num_attention_heads * self.config.head_dim), dtype = torch.float16, device = hidden_states.device)\n        key_states = torch.empty((bsz, q_len, self.config.num_key_value_heads * self.config.head_dim), dtype = torch.float16, device = hidden_states.device)\n        value_states = torch.empty((bsz, q_len, self.config.num_key_value_heads * self.config.head_dim), dtype = torch.float16, device = hidden_states.device)\n\n        cuda_ext.exllama_ext.q4_attn(hidden_states,\n                                     input_layernorm.weight,\n                                     self.config.rms_norm_eps,\n                                     query_states,\n                                     key_states,\n                                     value_states,\n                                     self.q_proj.q4,\n                                     self.k_proj.q4,\n                                     self.v_proj.q4,\n                                     self.sin,\n                                     self.cos,\n                                     q_len,\n                                     past_len,\n                                     self.config.num_attention_heads,\n                                     self.config.num_key_value_heads,\n                                     self.config.head_dim,\n                                     cache.key_states[self.index],\n                                     cache.value_states[self.index],\n                                     cache.max_seq_len,\n                                     q_a, q_b,\n                                     k_a, k_b,\n                                     v_a, v_b,\n                                     lora_temp)\n\n        query_states = query_states.view(bsz, q_len, self.config.num_attention_heads, self.config.head_dim)\n\n        # Get k, v with past\n\n        key_states = cache.key_states[self.index].narrow(2, 0, past_len + q_len).narrow(0, 0, bsz)\n        value_states = cache.value_states[self.index].narrow(2, 0, past_len + q_len).narrow(0, 0, bsz)\n\n        # Repeat K/V heads if num_key_value_headsn_kv_heads < n_heads\n\n        query_states.transpose_(1, 2)\n        key_states = self.repeat_kv(key_states, self.config.num_key_value_groups)\n        value_states = self.repeat_kv(value_states, self.config.num_key_value_groups)\n\n        # Attention\n        # TODO: Figure out if we can use cublasHgemmStridedBatched() to do this matmul without reshaping. Torch uses\n        # gemmStridedBatchedEx() internally, so it should be possible.\n\n        # -- Flash Attention 2.0\n\n        if self.config.use_flash_attn_2 and (past_len == 0 or q_len == 1):\n\n            key_states = key_states.transpose(1, 2)\n            value_states = value_states.transpose(1, 2)\n            query_states = query_states.transpose(1, 2)\n            attn_output = flash_attn_func(query_states, key_states, value_states, causal = (past_len == 0))\n\n        # -- HF Transformers regular attention, faster on shorter sequences, same VRAM usage\n\n        else:\n\n            key_states.transpose_(2, 3)\n            attn_weights = torch.matmul(query_states, key_states)\n            attn_weights /= math.sqrt(self.config.head_dim)\n            attn_weights = nn.functional.softmax(attn_weights, dim = -1, dtype = torch.float16)\n            attn_output = torch.matmul(attn_weights, value_states)\n            attn_output = attn_output.transpose(1, 2)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.config.hidden_size)\n\n        # Output projection\n\n        cuda_ext.exllama_ext.q4_attn_2(hidden_states,\n                                       attn_output,\n                                       self.o_proj.q4,\n                                       o_a, o_b,\n                                       lora_temp)\n        # return hidden_states\n\n\n    def forward(self, hidden_states, cache, buffer, lora):\n\n        bsz, q_len, _ = hidden_states.size()\n        past_len = cache.current_seq_len\n\n        # Project q, k, v, apply position embeddings to k and v\n\n        query_states = self.q_proj.forward(hidden_states, lora)\n        key_states = self.k_proj.forward(hidden_states, lora)\n\n        cuda_ext.exllama_ext.rope_(query_states, self.sin, self.cos, past_len, self.config.num_attention_heads, self.config.head_dim)\n        cuda_ext.exllama_ext.rope_(key_states, self.sin, self.cos, past_len, self.config.num_key_value_heads, self.config.head_dim)\n\n        query_states = query_states.view(bsz, q_len, self.config.num_attention_heads, self.config.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.config.num_key_value_heads, self.config.head_dim).transpose(1, 2)\n        value_states = self.v_proj.forward(hidden_states, lora).view(bsz, q_len, self.config.num_key_value_heads, self.config.head_dim).transpose(1, 2)\n\n        # Add keys and values to cache\n\n        new_keys = cache.key_states[self.index].narrow(2, past_len, q_len).narrow(0, 0, bsz)\n        new_values = cache.value_states[self.index].narrow(2, past_len, q_len).narrow(0, 0, bsz)\n        new_keys.copy_(key_states)\n        new_values.copy_(value_states)\n\n        # Key/value tensors with past\n\n        key_states = cache.key_states[self.index].narrow(2, 0, past_len + q_len).narrow(0, 0, bsz)\n        value_states = cache.value_states[self.index].narrow(2, 0, past_len + q_len).narrow(0, 0, bsz)\n\n        # Attention\n\n        # -- Flash Attention 2.0\n\n        if self.config.use_flash_attn_2 and (past_len == 0 or q_len == 1):\n\n            key_states = key_states.transpose(1, 2)\n            value_states = value_states.transpose(1, 2)\n            query_states = query_states.transpose(1, 2)\n            attn_output = flash_attn_func(query_states, key_states, value_states, causal = (past_len == 0))\n\n        # -- HF Transformers regular attention, faster on shorter sequences, same VRAM usage\n\n        elif self.config.sdp_thd == 0 or q_len < self.config.sdp_thd:\n\n            key_states = self.repeat_kv(key_states, self.config.num_key_value_groups)\n            value_states = self.repeat_kv(value_states, self.config.num_key_value_groups)\n\n            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n            attn_weights /= math.sqrt(self.config.head_dim)\n            if buffer.attn_mask is not None: attn_weights = attn_weights + buffer.attn_mask\n            attn_weights = nn.functional.softmax(attn_weights, dim = -1, dtype = torch.float16)\n            attn_output = torch.matmul(attn_weights, value_states)\n            attn_output = attn_output.transpose(1, 2)\n\n        # -- Scaled dot-product attention from PyTorch 2, should be comparable to xformers (?)\n\n        else:\n\n            # Torch's SDP attention has a built-in causal mask feature which we can use only when there is no past, i.e.\n            # it can only apply a square attention mask. It saves quite a bit of VRAM but in practice Torch seems to use\n            # the same amount of memory at peak anyway.\n            #\n            # TODO: Apparently flash attention is disabled when supplying an attention mask tensor. Figure out if this\n            # is true and maybe drop SDP altogether. If causal masking in flash-attn is updated eventually there should\n            # be no need for this anyway.\n\n            key_states = self.repeat_kv(key_states, self.config.num_key_value_groups)\n            value_states = self.repeat_kv(value_states, self.config.num_key_value_groups)\n\n            if past_len > 0 or (bsz > 1 and buffer.attn_mask is not None):\n                attn_output = F.scaled_dot_product_attention(query_states, key_states, value_states, attn_mask = buffer.attn_mask, is_causal = False)\n            else:\n                attn_output = F.scaled_dot_product_attention(query_states, key_states, value_states, attn_mask = None, is_causal = True)\n\n            attn_output = attn_output.transpose(1, 2)\n\n        # Output projection\n\n        attn_output = attn_output.reshape(bsz, q_len, self.config.hidden_size)\n        attn_output = self.o_proj.forward(attn_output, lora)\n\n        return attn_output", "\n\ndef _rows(x):\n    xdp = 1\n    for y in x.shape[:-1]: xdp *= y\n    return xdp\n\nclass ExLlamaDecoderLayer:\n\n    def __init__(self, config, tensors, key, index, sin, cos):\n\n        self.config = config\n        self.index = index\n\n        self.self_attn = ExLlamaAttention(self.config, tensors, key + \".self_attn\", sin, cos, self.index)\n        self.mlp = ExLlamaMLP(self.config, tensors, key + \".mlp\")\n\n        self.input_layernorm = ExLlamaRMSNorm(self.config, tensors, key + \".input_layernorm.weight\")\n        self.post_attention_layernorm = ExLlamaRMSNorm(self.config, tensors, key + \".post_attention_layernorm.weight\")\n\n\n    def forward(self, hidden_states, cache, buffer, lora):\n\n        # Self-attention\n\n        if self.config.fused_attn and _rows(hidden_states) == 1:\n\n            self.self_attn.fused(hidden_states, cache, buffer, self.input_layernorm, lora)\n\n        else:\n\n            residual = hidden_states\n            hidden_states = self.input_layernorm.forward(hidden_states, buffer)\n            hidden_states = self.self_attn.forward(hidden_states, cache, buffer, lora)\n            hidden_states = residual + hidden_states\n\n        # MLP\n\n        if self.config.fused_mlp_thd > 0 and _rows(hidden_states) <= self.config.fused_mlp_thd:\n\n            self.mlp.fused(hidden_states, buffer, self.post_attention_layernorm, lora)\n\n        else:\n\n            residual = hidden_states\n            hidden_states = self.post_attention_layernorm.forward(hidden_states, buffer)\n            hidden_states = self.mlp.forward(hidden_states, buffer, lora)\n            hidden_states = residual + hidden_states\n\n        return hidden_states", "\n\n# Persistent cache for inference. Allocate the whole thing up front.\n\nclass ExLlamaCache:\n\n    def __init__(self, model, batch_size = 1, max_seq_len = -1, copy_from = None):\n\n        self.model = model\n        self.config = self.model.config\n        self.max_seq_len = max_seq_len if max_seq_len != -1 else self.config.max_seq_len\n        self.batch_size = batch_size\n\n        self.key_states = []\n        self.value_states = []\n        self.current_seq_len = 0\n\n        # Preallocate full-length cache\n\n        for i in range(self.config.num_hidden_layers):\n\n            if copy_from is None:\n\n                p_key_states = torch.zeros(self.batch_size, self.config.num_key_value_heads, self.max_seq_len, self.config.head_dim, dtype = torch.float16, device = self.model.config.device_map.layers[i])\n                p_value_states = torch.zeros(self.batch_size, self.config.num_key_value_heads, self.max_seq_len, self.config.head_dim, dtype = torch.float16, device = self.model.config.device_map.layers[i])\n\n            else:\n\n                p_key_states = copy_from.key_states[i].clone()\n                p_value_states = copy_from.value_states[i].clone()\n\n            self.key_states.append(p_key_states)\n            self.value_states.append(p_value_states)\n\n\n    def zero(self):\n\n        for i in range(self.config.num_hidden_layers):\n            self.key_states[i].zero_()\n            self.value_states[i].zero_()\n\n\n    def clone(self):\n\n        new = ExLlamaCache(self.model, batch_size = self.batch_size, max_seq_len = self.max_seq_len, copy_from = self)\n        return new\n\n\n    def roll_left(self):\n\n        for i in range(self.config.num_hidden_layers):\n\n            self.key_states[i] = torch.roll(self.key_states[i], shifts = -1, dims = 2)\n            self.value_states[i] = torch.roll(self.value_states[i], shifts = -1, dims = 2)\n\n        self.current_seq_len -= 1\n\n\n    def copy_states(self, target, from_column, from_columns, to_column, to_columns, from_row, from_rows, to_row, to_rows):\n\n        assert from_rows == 1\n        assert from_columns == to_columns\n        assert to_column + to_columns <= target.max_seq_len\n        assert from_column + from_columns <= self.max_seq_len\n\n        for i in range(self.config.num_hidden_layers):\n\n            source_view_k = self.key_states[i].narrow(0, from_row, from_rows).narrow(2, from_column, from_columns)\n            source_view_v = self.value_states[i].narrow(0, from_row, from_rows).narrow(2, from_column, from_columns)\n            target_view_k = target.key_states[i].narrow(0, to_row, to_rows).narrow(2, to_column, to_columns)\n            target_view_v = target.value_states[i].narrow(0, to_row, to_rows).narrow(2, to_column, to_columns)\n\n            if to_rows > 1:\n\n                source_view_k = source_view_k.expand_as(target_view_k)\n                source_view_v = source_view_v.expand_as(target_view_v)\n\n            target_view_k.copy_(source_view_k)\n            target_view_v.copy_(source_view_v)", "\n\n# Device map for the model.\n\nclass ExLlamaDeviceMap:\n\n    def __init__(self, num_layers):\n\n        self.num_layers = num_layers\n\n        self.embed_tokens = \"cpu\"  # Embedding table on CPU saves 400 MB on the 30B model with no measurable impact on performance\n        self.lm_head = \"cuda:0\"\n        self.norm = \"cuda:0\"\n        self.layers = [\"cuda:0\"] * self.num_layers\n\n\n    def get_layers_devs(self):\n\n        return sorted(list(set(self.layers)))\n\n\n    def get_all_devs(self):\n\n        return sorted(list(set(self.layers + [self.lm_head, self.norm, self.embed_tokens])))\n\n\n    def map(self, key):\n\n        if key.startswith(\"lm_head.\"): return self.lm_head\n        if key.startswith(\"model.embed_tokens.\"): return self.embed_tokens\n        if key.startswith(\"model.norm.\"): return self.norm\n\n        if key.startswith(\"model.layers.\"):\n            num = int(key.split(\".\")[2])\n            return self.layers[num]\n\n        raise ValueError(\"Unknown key: \" + key)", "\n\nclass ExLlamaBuffer:\n\n    config: ExLlamaConfig\n\n    def __init__(self, config):\n\n        self.config = config\n\n    # Attention mask\n\n    attn_mask: torch.Tensor = None\n\n    # Move to device\n\n    def to(self, device):\n\n        new = ExLlamaBuffer(self.config)\n        new.attn_mask = None if self.attn_mask is None else _move_tensor(self.attn_mask, device, \"attn_mask\", self.config)\n        return new", "\n\ndef _device_to_int(device):\n\n    return int(device[device.find(\":\") + 1:])\n\ndef _skip_key(key):\n\n    if key.endswith(\"_proj.bias\"): return True\n    if key.endswith(\".rotary_emb.inv_freq\"): return True\n    return False", "\ndef _move_tensor(tensor, new_device, name, config):\n    device = str(tensor.device)\n    if device == new_device: return tensor\n    if config.gpu_peer_fix:\n        if str(device).startswith(\"cuda:\") and str(new_device).startswith(\"cuda:\"):\n            tensor = tensor.to(\"cpu\")\n    return tensor.to(new_device)\n\ndef _layer_dtype_size(key):\n    if key.endswith(\".weight\"): return 2\n    if key.endswith(\".qweight\"): return 4\n    if key.endswith(\".qzeros\"): return 4\n    if key.endswith(\".scales\"): return 2\n    if key.endswith(\".g_idx\"): return 0\n    raise ValueError(\"Unrecognized layer: \" + key)", "\ndef _layer_dtype_size(key):\n    if key.endswith(\".weight\"): return 2\n    if key.endswith(\".qweight\"): return 4\n    if key.endswith(\".qzeros\"): return 4\n    if key.endswith(\".scales\"): return 2\n    if key.endswith(\".g_idx\"): return 0\n    raise ValueError(\"Unrecognized layer: \" + key)\n\n\nclass ExLlama:\n\n    def __init__(self, config):\n\n        self.config = config\n\n        # Copy tuning parameters to C++ extension\n\n        self.config.set_tuning_params()\n\n        # Load model weights\n\n        tensors = {}\n        with safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\") as f:\n\n            # Begin auto mapping if enabled\n\n            decoder_size = 0\n            norm_size = 0\n            head_size = 0\n            half_element_size = torch.tensor([], dtype = torch.float16).element_size()\n\n            if self.config.auto_map is not None:\n\n                self.config.device_map.embed_tokens = \"cpu\"\n                self.config.device_map.layers = [\"cuda:0\"] + [\"?\"] * (self.config.num_hidden_layers - 1)\n\n                for key in f.keys():\n\n                    if _skip_key(key): continue\n\n                    if key.startswith(\"model.layers.0.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        decoder_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                    if key.startswith(\"model.norm.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        norm_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                    if key.startswith(\"lm_head.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        head_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                # Assign layers automatically\n\n                device_usage = 0\n                device_index = 0\n                layer_index_device = 0\n                max_usage = self.config.auto_map[device_index] * (1024 ** 3)\n\n                for layer in range(self.config.num_hidden_layers + 2):\n\n                    this_layer_size = decoder_size\n                    if layer == self.config.num_hidden_layers + 0: this_layer_size = norm_size\n                    elif layer == self.config.num_hidden_layers + 1: this_layer_size = head_size\n\n                    while device_usage + this_layer_size > max_usage:\n                        device_index += 1\n                        device_usage = 0\n                        layer_index_device = 0\n                        max_usage = self.config.auto_map[device_index] * (1024 ** 3)\n                        if device_index >= len(self.config.auto_map): raise ValueError(\"Model too large for device allocation scheme.\")\n\n                    target = f\"cuda:{device_index}\"\n                    if layer == self.config.num_hidden_layers + 0: self.config.device_map.norm = target\n                    elif layer == self.config.num_hidden_layers + 1: self.config.device_map.lm_head = target\n                    else: self.config.device_map.layers[layer] = f\"cuda:{device_index}\"\n\n                    device_usage += this_layer_size\n                    layer_index_device += 1\n\n        # Read tensor list from file\n\n        load_keys = []\n        with safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\") as f:\n            for key in f.keys():\n                load_keys.append(key)\n\n        # Load up to 1 GB of tensors at a time, closing and reopening the file in between each chunk\n\n        max_dq_buffer_size = 0\n        f = None\n        st_mem = 0\n        MAX_ST_MEM = 1024**3\n\n        for key in load_keys:\n\n            if _skip_key(key): continue\n            device = self.config.device_map.map(key)\n\n            if f is None or st_mem > MAX_ST_MEM:\n                if f is not None: del f\n                f = safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\")\n                st_mem = 0\n\n            tensor = f.get_tensor(key)\n            size = tensor.numel() * tensor.element_size()\n            st_mem += size\n\n            if key.endswith(\".scales\"): tensor = tensor.half()\n            if key == \"lm_head.weight\": tensor = tensor.float() if device == \"cpu\" else tensor.half()\n            if key == \"model.norm.weight\": tensor = tensor.half()\n            if key.endswith(\".embed_tokens.weight\"): tensor = tensor.half()\n            if key.endswith(\".input_layernorm.weight\"): tensor = tensor.half()\n            if key.endswith(\".post_attention_layernorm.weight\"): tensor = tensor.half()\n\n            tensor = tensor.to(device, non_blocking = True)\n            if key.endswith(\".qweight\"): max_dq_buffer_size = max(max_dq_buffer_size, tensor.numel() * 8)\n\n            tensors[key] = tensor\n\n        del f\n\n        # Head\n\n        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias = False, device = \"meta\")\n        self.lm_head.weight = nn.Parameter(tensors[\"lm_head.weight\"])\n        # self.lm_head_data = tensors[\"lm_head.weight\"].transpose(0, 1).contiguous()\n\n        # Token embeddings\n\n        self.embed_tokens = nn.Embedding(self.config.vocab_size, self.config.hidden_size, self.config.pad_token_id, device = \"meta\")\n        self.embed_tokens.weight = nn.Parameter(tensors[\"model.embed_tokens.weight\"])\n        with torch.no_grad():\n            self.embed_tokens.weight[self.config.pad_token_id] = 0\n\n        # Norm\n\n        self.norm = ExLlamaRMSNorm(self.config, tensors, \"model.norm.weight\")\n\n        # Prepare position embeddings for max seq length\n\n        devs = self.config.device_map.get_layers_devs()\n\n        self.sincos = {}\n        for device in devs:\n\n            inv_freq = 1.0 / (self.config.rotary_embedding_base ** (torch.arange(0, self.config.head_dim, 2, device = device).float() / self.config.head_dim))\n            t = torch.arange(self.config.max_seq_len, device = device, dtype = torch.float32)\n            if self.config.compress_pos_emb != 1.0: t /= self.config.compress_pos_emb\n\n            freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n            emb = torch.cat((freqs, freqs), dim = -1)\n\n            sin = emb.sin()[None, None, :, :].half()\n            cos = emb.cos()[None, None, :, :].half()\n\n            self.sincos[device] = (sin, cos)\n\n        # Decoder layers\n\n        modules = []\n        device_layer_index = [0] * len(devs)\n\n        for i in range(self.config.num_hidden_layers):\n\n            device = self.config.device_map.layers[i]\n            sin, cos = self.sincos[device]\n\n            layer = ExLlamaDecoderLayer(self.config, tensors, f\"model.layers.{i}\", i, sin, cos)\n\n            modules.append(layer)\n\n        self.layers = modules\n\n        # Prepare CUDA buffers\n\n        self.buffers = []\n        for dev in self.config.device_map.get_layers_devs():\n\n            device_buffers = {}\n            self.buffers.append(device_buffers)\n\n            temp_state = torch.zeros((config.max_input_len, config.intermediate_size), dtype = torch.float16, device = dev)\n            temp_mlp = torch.zeros((config.fused_mlp_thd * 2, config.intermediate_size), dtype = torch.float16, device = dev)\n            temp_zeros_float = torch.zeros((1, 65536), dtype = torch.float32, device = dev)\n            temp_dq = torch.zeros((1, max_dq_buffer_size), dtype = torch.float16, device = dev)\n\n            device_buffers[\"temp_state\"] = temp_state\n            device_buffers[\"temp_mlp\"] = temp_mlp\n            device_buffers[\"temp_zeros_float\"] = temp_zeros_float\n            device_buffers[\"temp_dq\"] = temp_dq\n\n            cuda_ext.exllama_ext.prepare_buffers(torch.device(dev),\n                                                 temp_state,\n                                                 temp_mlp,\n                                                 temp_zeros_float,\n                                                 temp_dq)\n\n        # Clear the cache\n\n        torch.cuda.empty_cache()\n\n\n    def forward(self,\n                input_ids,\n                cache,\n                last_id_only = True,\n                preprocess_only = False,\n                lora = None,\n                output_device = None,\n                input_mask = None):\n\n        q_len = input_ids.shape[-1]\n        remaining_q_len = q_len\n        bsz = input_ids.shape[0]\n\n        assert input_mask is None or (input_mask.shape[-1] >= input_ids.shape[-1] and input_mask.shape[-2] == input_ids.shape[-2])\n\n        # The buffers can only fit max_input_len tokens, so with larger batch sizes we reduce our work size correspondingly.\n\n        effective_max_input_len = self.config.max_input_len // bsz\n\n        # Split sequence\n\n        result = None\n\n        chunk_begin = 0\n        while chunk_begin < q_len:\n\n            # Limit chunk_size to max_input_len\n\n            chunk_size = min(remaining_q_len, effective_max_input_len)\n\n            # Limit chunk_size to keep size of attention operation <= max_attention_size, unless using flash-attn\n\n            if not self.config.use_flash_attn_2 or chunk_begin > 0:\n\n                past_len = cache.current_seq_len\n                attn_size = (past_len + remaining_q_len) * remaining_q_len\n                max_a = self.config.max_attention_size\n                if attn_size > max_a:\n                    cs = (math.sqrt(past_len ** 2 + 4 * max_a) - past_len) / 2\n                    chunk_size = min(chunk_size, math.floor(cs))\n\n            # Process chunk\n\n            chunk_end = min(chunk_begin + chunk_size, q_len)\n\n            _last_id_only = last_id_only\n            _preprocess_only = preprocess_only or (chunk_end < q_len and last_id_only)\n\n            r = self._forward(input_ids[:, chunk_begin : chunk_end],\n                             cache,\n                             _last_id_only,\n                             _preprocess_only,\n                             lora,\n                             output_device,\n                             input_mask)\n\n            if not _preprocess_only:\n                result = r if result is None else torch.cat((result, r), dim = 1)\n\n            chunk_begin = chunk_end\n            remaining_q_len -= chunk_size\n\n        return result\n\n\n    def _forward(self,\n                 input_ids,\n                 cache,\n                 last_id_only = True,\n                 preprocess_only = False,\n                 lora = None,\n                 output_device = None,\n                 input_mask = None):\n\n        # if torch.is_grad_enabled():\n        #     raise ValueError(\"Forward pass called with gradients enabled. Back propagation is not supported yet.\")\n        with torch.no_grad():\n\n            batch_size, seq_len = input_ids.shape\n            past_len = cache.current_seq_len\n            if output_device is None: output_device = input_ids.device\n\n            buffer = ExLlamaBuffer(self.config)\n\n            # Build attention mask on first device, copy to others if necessary\n\n            devs = self.config.device_map.get_layers_devs()\n\n            # if not self.config.use_flash_attn_2:\n\n            if seq_len > 1 or input_mask is not None:\n\n                attn_mask = torch.zeros(batch_size, 1, seq_len, past_len + seq_len, dtype = torch.float16, device = devs[0])\n                attn_mask_triu = torch.triu(torch.full((seq_len - 1, seq_len - 1), -65504.))\n                attn_mask[:, :, : seq_len - 1, past_len + 1: past_len + seq_len] = attn_mask_triu\n\n                if input_mask is not None:\n\n                    input_mask = input_mask[:, :past_len + seq_len]\n                    input_mask = _move_tensor(input_mask, devs[0], \"input_mask\", self.config)\n                    input_mask = torch.where(input_mask, 0, -65504.).half()\n                    input_mask = input_mask.unsqueeze(1).unsqueeze(2)\n                    attn_mask = torch.minimum(attn_mask, input_mask)\n\n            else:\n\n                attn_mask = None\n                # attn_mask = torch.zeros(batch_size, 1, seq_len, seq_len + past_len, dtype = torch.float16, device = devs[0])\n\n            buffer.attn_mask = attn_mask\n\n            # else:\n            #\n            #     buffer.attn_mask = None\n\n            # Embeddings\n            # TODO: Allow passing input embeddings instead of IDs\n\n            input_ids = _move_tensor(input_ids, self.config.device_map.embed_tokens, \"input_ids\", self.config)\n            hidden_states = self.embed_tokens(input_ids)\n\n            # Split buffers to devices\n\n            buffers = {devs[0]: buffer}\n            for device in devs[1:]:\n                buffers[device] = buffer.to(device)\n\n            # Decoder layers\n\n            for i, decoder_layer in enumerate(self.layers):\n\n                device = self.config.device_map.layers[i]\n                hidden_states = _move_tensor(hidden_states, device, \"hidden_states\", self.config)\n\n                hidden_states = decoder_layer.forward(hidden_states, cache, buffers[device], lora)\n\n            cache.current_seq_len += seq_len\n\n            # Early exit when we don't need logits\n\n            if preprocess_only: return None\n\n            # Norm\n\n            hidden_states = _move_tensor(hidden_states, self.config.device_map.norm, \"hidden_states\", self.config)\n            hidden_states = self.norm.forward(hidden_states, buffer)\n\n            # Head\n\n            if last_id_only: hidden_states = hidden_states[:, -1:, :].contiguous()\n            if self.config.device_map.lm_head == \"cpu\": hidden_states = hidden_states.float()\n\n            hidden_states = _move_tensor(hidden_states, self.config.device_map.lm_head, \"hidden_states\", self.config)\n            logits = self.lm_head(hidden_states)\n            # logits = cuda_ext.matmul_half(hidden_states, self.lm_head_data, cublas = False)\n\n            logits = logits.float()\n            logits = _move_tensor(logits, output_device, \"logits\", self.config)\n            return logits\n\n\n    # Free unmanaged resources allocated by the C++ extension. Call this before dereferencing the ExLlama object,\n    # e.g. if you intend to create a new instance to load another model, but don't call it in a destructor that wraps\n    # the object, since it relies on CUDA function calls and the CUDA context is one of the first things to go when\n    # a PyTorch application terminates, before other managed objects are destroyed.\n\n    def free_unmanaged(self):\n\n        cuda_ext.exllama_ext.cleanup()", "\n\nclass ExLlama:\n\n    def __init__(self, config):\n\n        self.config = config\n\n        # Copy tuning parameters to C++ extension\n\n        self.config.set_tuning_params()\n\n        # Load model weights\n\n        tensors = {}\n        with safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\") as f:\n\n            # Begin auto mapping if enabled\n\n            decoder_size = 0\n            norm_size = 0\n            head_size = 0\n            half_element_size = torch.tensor([], dtype = torch.float16).element_size()\n\n            if self.config.auto_map is not None:\n\n                self.config.device_map.embed_tokens = \"cpu\"\n                self.config.device_map.layers = [\"cuda:0\"] + [\"?\"] * (self.config.num_hidden_layers - 1)\n\n                for key in f.keys():\n\n                    if _skip_key(key): continue\n\n                    if key.startswith(\"model.layers.0.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        decoder_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                    if key.startswith(\"model.norm.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        norm_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                    if key.startswith(\"lm_head.\"):\n                        tensor_slice = f.get_slice(key)\n                        shape = tensor_slice.get_shape()\n                        head_size += math.prod(shape) * _layer_dtype_size(key)\n                        del tensor_slice\n\n                # Assign layers automatically\n\n                device_usage = 0\n                device_index = 0\n                layer_index_device = 0\n                max_usage = self.config.auto_map[device_index] * (1024 ** 3)\n\n                for layer in range(self.config.num_hidden_layers + 2):\n\n                    this_layer_size = decoder_size\n                    if layer == self.config.num_hidden_layers + 0: this_layer_size = norm_size\n                    elif layer == self.config.num_hidden_layers + 1: this_layer_size = head_size\n\n                    while device_usage + this_layer_size > max_usage:\n                        device_index += 1\n                        device_usage = 0\n                        layer_index_device = 0\n                        max_usage = self.config.auto_map[device_index] * (1024 ** 3)\n                        if device_index >= len(self.config.auto_map): raise ValueError(\"Model too large for device allocation scheme.\")\n\n                    target = f\"cuda:{device_index}\"\n                    if layer == self.config.num_hidden_layers + 0: self.config.device_map.norm = target\n                    elif layer == self.config.num_hidden_layers + 1: self.config.device_map.lm_head = target\n                    else: self.config.device_map.layers[layer] = f\"cuda:{device_index}\"\n\n                    device_usage += this_layer_size\n                    layer_index_device += 1\n\n        # Read tensor list from file\n\n        load_keys = []\n        with safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\") as f:\n            for key in f.keys():\n                load_keys.append(key)\n\n        # Load up to 1 GB of tensors at a time, closing and reopening the file in between each chunk\n\n        max_dq_buffer_size = 0\n        f = None\n        st_mem = 0\n        MAX_ST_MEM = 1024**3\n\n        for key in load_keys:\n\n            if _skip_key(key): continue\n            device = self.config.device_map.map(key)\n\n            if f is None or st_mem > MAX_ST_MEM:\n                if f is not None: del f\n                f = safe_open(self.config.model_path, framework = \"pt\", device = \"cpu\")\n                st_mem = 0\n\n            tensor = f.get_tensor(key)\n            size = tensor.numel() * tensor.element_size()\n            st_mem += size\n\n            if key.endswith(\".scales\"): tensor = tensor.half()\n            if key == \"lm_head.weight\": tensor = tensor.float() if device == \"cpu\" else tensor.half()\n            if key == \"model.norm.weight\": tensor = tensor.half()\n            if key.endswith(\".embed_tokens.weight\"): tensor = tensor.half()\n            if key.endswith(\".input_layernorm.weight\"): tensor = tensor.half()\n            if key.endswith(\".post_attention_layernorm.weight\"): tensor = tensor.half()\n\n            tensor = tensor.to(device, non_blocking = True)\n            if key.endswith(\".qweight\"): max_dq_buffer_size = max(max_dq_buffer_size, tensor.numel() * 8)\n\n            tensors[key] = tensor\n\n        del f\n\n        # Head\n\n        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias = False, device = \"meta\")\n        self.lm_head.weight = nn.Parameter(tensors[\"lm_head.weight\"])\n        # self.lm_head_data = tensors[\"lm_head.weight\"].transpose(0, 1).contiguous()\n\n        # Token embeddings\n\n        self.embed_tokens = nn.Embedding(self.config.vocab_size, self.config.hidden_size, self.config.pad_token_id, device = \"meta\")\n        self.embed_tokens.weight = nn.Parameter(tensors[\"model.embed_tokens.weight\"])\n        with torch.no_grad():\n            self.embed_tokens.weight[self.config.pad_token_id] = 0\n\n        # Norm\n\n        self.norm = ExLlamaRMSNorm(self.config, tensors, \"model.norm.weight\")\n\n        # Prepare position embeddings for max seq length\n\n        devs = self.config.device_map.get_layers_devs()\n\n        self.sincos = {}\n        for device in devs:\n\n            inv_freq = 1.0 / (self.config.rotary_embedding_base ** (torch.arange(0, self.config.head_dim, 2, device = device).float() / self.config.head_dim))\n            t = torch.arange(self.config.max_seq_len, device = device, dtype = torch.float32)\n            if self.config.compress_pos_emb != 1.0: t /= self.config.compress_pos_emb\n\n            freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n            emb = torch.cat((freqs, freqs), dim = -1)\n\n            sin = emb.sin()[None, None, :, :].half()\n            cos = emb.cos()[None, None, :, :].half()\n\n            self.sincos[device] = (sin, cos)\n\n        # Decoder layers\n\n        modules = []\n        device_layer_index = [0] * len(devs)\n\n        for i in range(self.config.num_hidden_layers):\n\n            device = self.config.device_map.layers[i]\n            sin, cos = self.sincos[device]\n\n            layer = ExLlamaDecoderLayer(self.config, tensors, f\"model.layers.{i}\", i, sin, cos)\n\n            modules.append(layer)\n\n        self.layers = modules\n\n        # Prepare CUDA buffers\n\n        self.buffers = []\n        for dev in self.config.device_map.get_layers_devs():\n\n            device_buffers = {}\n            self.buffers.append(device_buffers)\n\n            temp_state = torch.zeros((config.max_input_len, config.intermediate_size), dtype = torch.float16, device = dev)\n            temp_mlp = torch.zeros((config.fused_mlp_thd * 2, config.intermediate_size), dtype = torch.float16, device = dev)\n            temp_zeros_float = torch.zeros((1, 65536), dtype = torch.float32, device = dev)\n            temp_dq = torch.zeros((1, max_dq_buffer_size), dtype = torch.float16, device = dev)\n\n            device_buffers[\"temp_state\"] = temp_state\n            device_buffers[\"temp_mlp\"] = temp_mlp\n            device_buffers[\"temp_zeros_float\"] = temp_zeros_float\n            device_buffers[\"temp_dq\"] = temp_dq\n\n            cuda_ext.exllama_ext.prepare_buffers(torch.device(dev),\n                                                 temp_state,\n                                                 temp_mlp,\n                                                 temp_zeros_float,\n                                                 temp_dq)\n\n        # Clear the cache\n\n        torch.cuda.empty_cache()\n\n\n    def forward(self,\n                input_ids,\n                cache,\n                last_id_only = True,\n                preprocess_only = False,\n                lora = None,\n                output_device = None,\n                input_mask = None):\n\n        q_len = input_ids.shape[-1]\n        remaining_q_len = q_len\n        bsz = input_ids.shape[0]\n\n        assert input_mask is None or (input_mask.shape[-1] >= input_ids.shape[-1] and input_mask.shape[-2] == input_ids.shape[-2])\n\n        # The buffers can only fit max_input_len tokens, so with larger batch sizes we reduce our work size correspondingly.\n\n        effective_max_input_len = self.config.max_input_len // bsz\n\n        # Split sequence\n\n        result = None\n\n        chunk_begin = 0\n        while chunk_begin < q_len:\n\n            # Limit chunk_size to max_input_len\n\n            chunk_size = min(remaining_q_len, effective_max_input_len)\n\n            # Limit chunk_size to keep size of attention operation <= max_attention_size, unless using flash-attn\n\n            if not self.config.use_flash_attn_2 or chunk_begin > 0:\n\n                past_len = cache.current_seq_len\n                attn_size = (past_len + remaining_q_len) * remaining_q_len\n                max_a = self.config.max_attention_size\n                if attn_size > max_a:\n                    cs = (math.sqrt(past_len ** 2 + 4 * max_a) - past_len) / 2\n                    chunk_size = min(chunk_size, math.floor(cs))\n\n            # Process chunk\n\n            chunk_end = min(chunk_begin + chunk_size, q_len)\n\n            _last_id_only = last_id_only\n            _preprocess_only = preprocess_only or (chunk_end < q_len and last_id_only)\n\n            r = self._forward(input_ids[:, chunk_begin : chunk_end],\n                             cache,\n                             _last_id_only,\n                             _preprocess_only,\n                             lora,\n                             output_device,\n                             input_mask)\n\n            if not _preprocess_only:\n                result = r if result is None else torch.cat((result, r), dim = 1)\n\n            chunk_begin = chunk_end\n            remaining_q_len -= chunk_size\n\n        return result\n\n\n    def _forward(self,\n                 input_ids,\n                 cache,\n                 last_id_only = True,\n                 preprocess_only = False,\n                 lora = None,\n                 output_device = None,\n                 input_mask = None):\n\n        # if torch.is_grad_enabled():\n        #     raise ValueError(\"Forward pass called with gradients enabled. Back propagation is not supported yet.\")\n        with torch.no_grad():\n\n            batch_size, seq_len = input_ids.shape\n            past_len = cache.current_seq_len\n            if output_device is None: output_device = input_ids.device\n\n            buffer = ExLlamaBuffer(self.config)\n\n            # Build attention mask on first device, copy to others if necessary\n\n            devs = self.config.device_map.get_layers_devs()\n\n            # if not self.config.use_flash_attn_2:\n\n            if seq_len > 1 or input_mask is not None:\n\n                attn_mask = torch.zeros(batch_size, 1, seq_len, past_len + seq_len, dtype = torch.float16, device = devs[0])\n                attn_mask_triu = torch.triu(torch.full((seq_len - 1, seq_len - 1), -65504.))\n                attn_mask[:, :, : seq_len - 1, past_len + 1: past_len + seq_len] = attn_mask_triu\n\n                if input_mask is not None:\n\n                    input_mask = input_mask[:, :past_len + seq_len]\n                    input_mask = _move_tensor(input_mask, devs[0], \"input_mask\", self.config)\n                    input_mask = torch.where(input_mask, 0, -65504.).half()\n                    input_mask = input_mask.unsqueeze(1).unsqueeze(2)\n                    attn_mask = torch.minimum(attn_mask, input_mask)\n\n            else:\n\n                attn_mask = None\n                # attn_mask = torch.zeros(batch_size, 1, seq_len, seq_len + past_len, dtype = torch.float16, device = devs[0])\n\n            buffer.attn_mask = attn_mask\n\n            # else:\n            #\n            #     buffer.attn_mask = None\n\n            # Embeddings\n            # TODO: Allow passing input embeddings instead of IDs\n\n            input_ids = _move_tensor(input_ids, self.config.device_map.embed_tokens, \"input_ids\", self.config)\n            hidden_states = self.embed_tokens(input_ids)\n\n            # Split buffers to devices\n\n            buffers = {devs[0]: buffer}\n            for device in devs[1:]:\n                buffers[device] = buffer.to(device)\n\n            # Decoder layers\n\n            for i, decoder_layer in enumerate(self.layers):\n\n                device = self.config.device_map.layers[i]\n                hidden_states = _move_tensor(hidden_states, device, \"hidden_states\", self.config)\n\n                hidden_states = decoder_layer.forward(hidden_states, cache, buffers[device], lora)\n\n            cache.current_seq_len += seq_len\n\n            # Early exit when we don't need logits\n\n            if preprocess_only: return None\n\n            # Norm\n\n            hidden_states = _move_tensor(hidden_states, self.config.device_map.norm, \"hidden_states\", self.config)\n            hidden_states = self.norm.forward(hidden_states, buffer)\n\n            # Head\n\n            if last_id_only: hidden_states = hidden_states[:, -1:, :].contiguous()\n            if self.config.device_map.lm_head == \"cpu\": hidden_states = hidden_states.float()\n\n            hidden_states = _move_tensor(hidden_states, self.config.device_map.lm_head, \"hidden_states\", self.config)\n            logits = self.lm_head(hidden_states)\n            # logits = cuda_ext.matmul_half(hidden_states, self.lm_head_data, cublas = False)\n\n            logits = logits.float()\n            logits = _move_tensor(logits, output_device, \"logits\", self.config)\n            return logits\n\n\n    # Free unmanaged resources allocated by the C++ extension. Call this before dereferencing the ExLlama object,\n    # e.g. if you intend to create a new instance to load another model, but don't call it in a destructor that wraps\n    # the object, since it relies on CUDA function calls and the CUDA context is one of the first things to go when\n    # a PyTorch application terminates, before other managed objects are destroyed.\n\n    def free_unmanaged(self):\n\n        cuda_ext.exllama_ext.cleanup()", ""]}
{"filename": "perplexity.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\n\nimport json\nimport math\nimport os\nimport sys\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\n\n'''\nPassing in model, cache, tokenizer is a total hack because we don't want to have to reinitialize (or move all the globals into a shared state model)\n'''\n\nclass Perplexity:\n    def __init__(self, method=\"default\", model = None, cache = None, tokenizer = None):\n        # This needs to be loaded by calling .load()\n        self.dataset_chunks = []\n\n        self.model = model\n        self.cache = cache\n        self.tokenizer = tokenizer\n\n        self._begin()\n\n\n    def _begin(self):\n        if self.cache is None:\n            self.cache = ExLlamaCache(self.model)\n        else:\n            self.cache.current_seq_len = 0\n\n\n    def _next_logits(self, input_ids, apply_lora, last_id_only = True):\n        # n_logits = []\n        # a = 0\n        # while a < input_ids.shape[-1]:\n        #     b = min(input_ids.shape[-1], a + 2048)\n        #     n_logits.append(self.model.forward(input_ids[:, a:b], self.cache, last_id_only, lora = apply_lora))\n        #     a = b\n        #\n        # return torch.cat(n_logits, dim = 1)\n\n        return self.model.forward(input_ids, self.cache, last_id_only, lora = apply_lora)\n\n\n    def _tokenize(self, text):\n        return self.tokenizer.encode(text)\n\n\n    # Load raw dataset from a text file and tokenize into chunks. Each chunk can optionally truncated to allow for\n    # evaluating the same data at different sequence lengths\n\n    def load(self, dataset_path, chunk_size, chunk_truncate = None, overlap = 0, minlength = 0, json_key = \"text\"):\n\n        file_extension = os.path.splitext(dataset_path)[1]\n\n        # JSON format: Returned chunks may be of variable length, with each chunk representing one list item\n\n        if file_extension == '.jsonl' or file_extension == '.json':\n            with open(dataset_path) as f:\n                for line in f:\n                    example = json.loads(line)[json_key]\n                    if len(example) > minlength:\n                        chunk = self._tokenize(example)\n                        chunk = chunk[:, :chunk_size]\n                        if chunk_truncate is not None: chunk = chunk[:, :chunk_truncate]\n                        self.dataset_chunks.append(chunk)\n\n        # Raw Text: Returned chunks are fixed length windows of the entire tokenized dataset\n\n        else:\n            with open(dataset_path, encoding=\"utf-8\") as f:\n                text = f.read()\n\n            tokens = self._tokenize(text)\n\n            # overlap shouldn't be bigger than the context, also need at least one token for predicting last...\n            if overlap >= chunk_size:\n                overlap = chunk_size-2\n\n            # We can't use torch.chunks since it want's to split things into equal sized chunks. Instead, let's do our own chunking\n            start = 0\n            while start < tokens.size(1):\n                chunk = tokens[:, start:start + chunk_size]\n                start += chunk_size - overlap\n                if chunk_truncate is not None: chunk = chunk[:, :chunk_truncate]\n                self.dataset_chunks.append(chunk)\n\n\n    def test(self, chunk_limit = sys.maxsize, lora = None, tag = \"\", ppl_token = False):\n        if not self.dataset_chunks:\n            sys.exit(\" xx ERROR: Empty dataset!\")\n\n        print(f\" -- Testing {min(len(self.dataset_chunks), chunk_limit)} chunks\", end=\"\")\n        sys.stdout.flush()\n\n        logprob_sum = 0.0\n        logprob_count = 0\n\n        chunk_count = 0\n\n        for chunk in self.dataset_chunks:\n\n            self._begin()\n\n            input_ids = chunk[:, :-1]\n            target_ids = chunk[:, 1:]\n\n            if ppl_token:\n                logits_s = []\n                for i in range(input_ids.shape[-1]):\n                    logits_t = self._next_logits(input_ids[:, i : i + 1], lora, last_id_only = False)\n                    logits_s.append(logits_t)\n                logits = torch.cat(logits_s, dim = 1)\n            else:\n                logits = self._next_logits(input_ids, lora, last_id_only = False)\n\n            log_probs = F.log_softmax(logits, dim=-1)\n            token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n\n            logprob_sum += token_log_probs.sum().item()\n            logprob_count += target_ids.numel()\n\n            if chunk_count % 10 == 0:\n                print(\".\", end = \"\")\n                sys.stdout.flush()\n\n            chunk_count += 1\n            if chunk_limit and chunk_count >= chunk_limit:\n                break\n\n        mean_log_prob = logprob_sum / logprob_count\n        perplexity = math.exp(-mean_log_prob)\n\n        print(\"\")\n        print(f\" ** Perplexity{tag}: {perplexity:.4f}\")", "\n\ndef add_args(parser):\n\n    parser.add_argument(\"-ppl\", \"--perplexity\", nargs = '?', const = 'default', metavar = \"METHOD\", help = \"Perplexity benchmark. Optionally specify method: gptq-for-llama, llama.cpp (not yet implemented)\")\n    parser.add_argument(\"-ppl_ds\", \"--perplexity_dataset\", metavar = \"DATAPATH\", type = str, help = \"Load dataset for perplexity (JSONL if .jsonl, otherwise parses it as raw text)\")\n    parser.add_argument(\"-ppl_cn\", \"--perplexity_chunk_num\", nargs = \"?\", type = int, help = \"Number of chunks for perplexity benchmark\", default = 100)\n    parser.add_argument(\"-ppl_cs\", \"--perplexity_chunk_size\", type = int, help = \"Size of chunks for perplexity benchmark\", default = 2048)\n    parser.add_argument(\"-ppl_ct\", \"--perplexity_chunk_truncate\", type = int, help = \"Truncated size of chunks for perplexity benchmark\", default = 2048)\n    parser.add_argument(\"-ppl_co\", \"--perplexity_chunk_overlap\", type = int, help = \"Chunk overlap\", default = 0)\n    parser.add_argument(\"-ppl_cm\", \"--perplexity_chunk_min\", type = int, help = \"Minimum chunk length\", default = 50)\n    parser.add_argument(\"-ppl_key\", \"--perplexity_json_key\", type = str, help = \"Key to extract from JSON dataset, default: 'text'\", default = \"text\")\n    parser.add_argument(\"-ppl_t\", \"--perplexity_token\", action = \"store_true\", help = \"Run perplexity test on individual tokens, for debug purposes (slow)\")", "\n\ndef post_parse(args):\n\n    if not args.perplexity: return\n\n    # GPTQ-for-LLaMa equivalent\n\n    if args.perplexity == \"gptq-for-llama\":\n        args.perplexity_dataset = \"datasets/wikitext2.txt\"\n        args.perplexity_chunk_num = 128\n        args.perplexity_chunk_size = 2048\n        args.perplexity_chunk_truncate = 2048\n        args.perplexity_chunk_overlap = 0\n        args.perplexity_chunk_min = 0\n\n    # Default dataset for legacy method\n\n    if args.perplexity_dataset is None: args.perplexity_dataset = \"datasets/wikitext2_val_sample.jsonl\"\n\n    print(f\" -- Perplexity:\")\n    print(f\" -- - Dataset: {args.perplexity_dataset}\")\n    print(f\" -- - Chunks: {args.perplexity_chunk_num}\")\n    print(f\" -- - Chunk size: {args.perplexity_chunk_size}\" + (f\" -> {args.perplexity_chunk_truncate}\" if args.perplexity_chunk_truncate is not None else \"\"))\n    print(f\" -- - Chunk overlap: {args.perplexity_chunk_overlap}\")\n    print(f\" -- - Min. chunk size: {args.perplexity_chunk_min}\")\n    print(f\" -- - Key: {args.perplexity_json_key}\")\n    if args.perplexity_token: print(\"f -- - Per-token mode\")", "\n"]}
{"filename": "tokenizer.py", "chunked_list": ["from sentencepiece import SentencePieceProcessor\nimport os\nimport torch\n\nclass ExLlamaTokenizer:\n\n    def __init__(self, tokenizer_model_path):\n\n        self.path = tokenizer_model_path\n        self.tokenizer = SentencePieceProcessor(model_file = self.path)\n\n        self.unk_token = \"<unk>\"\n        self.bos_token = \"<s>\"\n        self.eos_token = \"</s>\"\n        self.unk_token_id = self.tokenizer.unk_id() # is the same as pad token id...\n        self.eos_token_id = self.tokenizer.eos_id()\n        self.bos_token_id = self.tokenizer.bos_id()\n        self.pad_token_id = 0  # self.tokenizer.pad_id()\n        self.newline_token_id = 13\n\n        self.special_characters = [(self.bos_token, self.bos_token_id), (self.eos_token, self.eos_token_id), (self.unk_token, self.unk_token_id)] # for tokenzier encoding\n\n    # Encode string\n\n    def encode(self, text, return_mask = False, max_seq_len = 2048, add_bos = False, add_eos = False, encode_special_characters = False):\n\n        if isinstance(text, list):\n\n            # text is a list of strings\n\n            list_ids = self.tokenizer.EncodeAsIds(text)\n\n            # pad bos and eos\n\n            if add_bos:\n                for ids in list_ids: ids.insert(0, self.bos_token_id)\n            if add_eos:\n                for ids in list_ids: ids.append(self.eos_token_id)\n\n            max_length = max([len(ids) for ids in list_ids])\n\n            needs_mask = False\n            padded_ids = []\n            for ids in list_ids:\n                if len(ids) != len(list_ids[0]): needs_mask = True\n                padding = torch.full((max_length - len(ids),), self.pad_token_id)\n                sequence = torch.tensor(ids)\n                padded_ids.append(torch.cat((padding, sequence), dim = 0).long())\n\n            stacked_ids = torch.stack(padded_ids, dim = 0)\n\n            if return_mask:\n                if needs_mask:\n                    mask_padding = torch.full((stacked_ids.shape[0], max_seq_len - stacked_ids.shape[1]), True, dtype = torch.bool, device = \"cpu\")\n                    mask = stacked_ids != 0\n                    mask = torch.cat((mask, mask_padding), dim = 1)\n                    return stacked_ids, mask\n                else:\n                    return stacked_ids, None\n            else:\n                return stacked_ids\n\n        else:\n\n            # text is a single string\n            split_text = [text]\n\n            # look for special characters\n            if encode_special_characters:\n                for special_character, special_token_id in self.special_characters:\n                    temp_text = []\n                    for segment in split_text:\n                        if isinstance(segment, str) and special_character in segment:\n                            # for each special character, append the text before the special character, then append the special character ID, then the rest of the text\n                            parts = segment.split(special_character)\n                            new_parts = []\n                            for i, part in enumerate(parts):\n                                new_parts.append(part)\n                                if i < len(parts) - 1:  # add the special token id between parts, but not after the last part\n                                    new_parts.append(special_token_id)\n                            temp_text.extend(new_parts)\n                        else:\n                            temp_text.append(segment)\n                    split_text = temp_text\n\n            ids = []\n\n            for text_chunk in split_text:\n                if isinstance(text_chunk, str):\n                    ids += self.tokenizer.EncodeAsIds(text_chunk)\n                else:\n                    ids.append(text_chunk)\n\n            # pad bos and eos\n\n            if add_bos:\n              ids = [self.bos_token_id] + ids\n            if add_eos:\n              ids = ids + [self.eos_token_id]\n\n            stacked_ids = torch.tensor(ids).unsqueeze(0)\n\n            if return_mask:\n                return stacked_ids, None\n            else:\n                return stacked_ids\n\n    def decode(self, ids, decode_special_characters=False):\n        \n        special_ids = {id_: char for char, id_ in self.special_characters}  # create a lookup dictionary\n\n        if ids.dim() > 1:\n            \n            texts = []\n            for i in range(ids.shape[0]):\n                seq = ids[i].tolist()\n                seq = [t for t in seq if t != self.pad_token_id]\n\n                if decode_special_characters:\n                    text_parts = []\n                    normal_ids = []  # list of lists\n                    current_normal_ids = []  # current list of normal IDs\n                    for idx, id_ in enumerate(seq):\n                        if id_ in special_ids:\n                            # Save the current list of normal IDs, then start a new one\n                            normal_ids.append(current_normal_ids)\n                            current_normal_ids = []\n                            # Store special token as a string\n                            text_parts.append(special_ids[id_])\n                        else:\n                            current_normal_ids.append(id_)\n                    normal_ids.append(current_normal_ids)  # save the last segment of normal IDs\n                    \n                    decoded_segments = [self.tokenizer.Decode(segment) for segment in normal_ids]\n                    for idx, decoded_segment in enumerate(decoded_segments):\n                        text_parts.insert(2*idx, decoded_segment)\n                    \n                    texts.append(\"\".join(text_parts))\n                else:\n                    if self.eos_token_id in seq:  # to not mess up special char decoding\n                        seq = seq[:seq.index(self.eos_token_id)]\n                    texts.append(self.tokenizer.Decode(seq))\n\n            return texts\n\n        else:\n            \n            ids = ids.tolist()\n\n            if decode_special_characters:\n                \n                text_parts = []\n                normal_ids = []  # list of lists\n                current_normal_ids = []  # current list of normal IDs\n                for idx, id_ in enumerate(ids):\n                    if id_ in special_ids:\n                        # Save the current list of normal IDs, then start a new one\n                        normal_ids.append(current_normal_ids)\n                        current_normal_ids = []\n                        # Store special token as a string\n                        text_parts.append(special_ids[id_])\n                    else:\n                        current_normal_ids.append(id_)\n                normal_ids.append(current_normal_ids)  # save the last segment of normal IDs\n                \n                decoded_segments = [self.tokenizer.Decode(segment) for segment in normal_ids]\n                for idx, decoded_segment in enumerate(decoded_segments):\n                    text_parts.insert(2*idx, decoded_segment)\n                \n                text = \"\".join(text_parts)\n            \n            else:\n              \n                text = self.tokenizer.Decode(ids)\n\n            return text\n\n\n    def num_tokens(self, text, encode_special_characters = False):\n        \n        if encode_special_characters:\n            \n            ids = self.encode(text, encode_special_characters = True)\n            return ids.size(1)\n        \n        else:\n            \n            ids = self.tokenizer.Encode(text)\n            return len(ids)"]}
{"filename": "test_benchmark_inference.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nfrom lora import ExLlamaLora\nimport perplexity\nfrom perplexity import Perplexity\nimport time\nimport torch\nimport torch.nn.functional as F\nimport argparse", "import torch.nn.functional as F\nimport argparse\nimport json\nimport math\nimport sys\nimport os\nimport glob\nimport model_init\n\ntorch.cuda._lazy_init()", "\ntorch.cuda._lazy_init()\n# torch.backends.cuda.matmul.allow_tf32 = True\n# torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\ntorch.set_printoptions(precision = 10)\ntorch_devices = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())]\n\ncache = None\nmodel = None\n\ndef begin():\n    global model, cache\n\n    if cache is None: cache = ExLlamaCache(model)\n    else: cache.current_seq_len = 0", "model = None\n\ndef begin():\n    global model, cache\n\n    if cache is None: cache = ExLlamaCache(model)\n    else: cache.current_seq_len = 0\n\n\ndef next_logits(input_ids, apply_lora, last_id_only = True, input_mask = None):\n    global model, cache\n\n    # n_logits = None\n    # a = 0\n    # while a < input_ids.shape[-1]:\n    #     b = min(input_ids.shape[-1], a + 2048)\n    #     n_logits = model.forward(input_ids[:, a:b], cache, last_id_only, lora = apply_lora, input_mask = input_mask)\n    #     a = b\n\n    n_logits = model.forward(input_ids, cache, last_id_only, lora=apply_lora, input_mask=input_mask)\n    return n_logits", "\ndef next_logits(input_ids, apply_lora, last_id_only = True, input_mask = None):\n    global model, cache\n\n    # n_logits = None\n    # a = 0\n    # while a < input_ids.shape[-1]:\n    #     b = min(input_ids.shape[-1], a + 2048)\n    #     n_logits = model.forward(input_ids[:, a:b], cache, last_id_only, lora = apply_lora, input_mask = input_mask)\n    #     a = b\n\n    n_logits = model.forward(input_ids, cache, last_id_only, lora=apply_lora, input_mask=input_mask)\n    return n_logits", "\n\ndef tokenize(text):\n    global tokenizer\n\n    return tokenizer.encode(text)\n\n\ndef timer(name, func):\n    t = time.time()\n    ret = func()\n    t = time.time() - t\n    print(f\" ** Time, {name}: {t:.2f} seconds\")\n    return ret", "def timer(name, func):\n    t = time.time()\n    ret = func()\n    t = time.time() - t\n    print(f\" ** Time, {name}: {t:.2f} seconds\")\n    return ret\n\n\nmem_base = {}\nmem_last = {}\nfor dev in torch_devices:\n    torch.cuda.reset_peak_memory_stats(dev)\n    mem_base[dev] = mem_last[dev] = torch.cuda.max_memory_allocated(dev)", "mem_base = {}\nmem_last = {}\nfor dev in torch_devices:\n    torch.cuda.reset_peak_memory_stats(dev)\n    mem_base[dev] = mem_last[dev] = torch.cuda.max_memory_allocated(dev)\n\ndef mem(name, total = False):\n    global mem_base, mem_last\n\n    res = f\" ** VRAM, {name}: \"\n    first = True\n\n    for device in torch_devices:\n        mem_c = torch.cuda.max_memory_allocated(device)\n        mem_this = mem_c - mem_last[device] if not total else mem_c - mem_base[device]\n        mem_last[device] = mem_c\n\n        if not first: res += \" - \"\n        first = False\n        res += f\"[{device}] {mem_this / (1024 ** 2):,.2f} MB\"\n\n    print(res)", "\n\n# Parse arguments\n\nparser = argparse.ArgumentParser(description = \"Benchmark tests for ExLlama\")\n\nmodel_init.add_args(parser)\nperplexity.add_args(parser)\n\nparser.add_argument(\"-p\", \"--perf\", action = \"store_true\", help = \"Benchmark speed and VRAM usage\")", "\nparser.add_argument(\"-p\", \"--perf\", action = \"store_true\", help = \"Benchmark speed and VRAM usage\")\nparser.add_argument(\"-v\", \"--validate\", action = \"count\", help = \"Run validation check and generate some sample output; specify twice for a more thorough test\")\nparser.add_argument(\"-lora\", \"--lora\", type = str, help = \"Path to LoRA binary to use during benchmark\")\nparser.add_argument(\"-loracfg\", \"--lora_config\", type = str, help = \"Path to LoRA config to use during benchmark\")\nparser.add_argument(\"-ld\", \"--lora_dir\", type = str, help = \"Path to LoRA config and binary. to use during benchmark\")\n\nargs = parser.parse_args()\n\nmodel_init.post_parse(args)", "\nmodel_init.post_parse(args)\nperplexity.post_parse(args)\nmodel_init.get_model_files(args)\n\n# Paths\n\nif args.lora_dir is not None:\n    args.lora_config = os.path.join(args.lora_dir, \"adapter_config.json\")\n    args.lora = os.path.join(args.lora_dir, \"adapter_model.bin\")", "\n# Feedback\n\nprint_opts = []\nif args.perf: print_opts.append(\"perf\")\nif args.validate: print_opts.append(\"validate\")\nif args.perplexity: print_opts.append(\"perplexity\")\nif args.perplexity_token: print_opts.append(\"perplexity_token\")\n\nmodel_init.print_options(args, print_opts)", "\nmodel_init.print_options(args, print_opts)\n\n# Globals\n\nmodel_init.set_globals(args)\n\n# Instantiate model\n\nconfig = model_init.make_config(args)", "\nconfig = model_init.make_config(args)\n\nmodel = timer(\"Load model\", lambda: ExLlama(config))\ntokenizer = timer(\"Load tokenizer\", lambda: ExLlamaTokenizer(args.tokenizer))\n\nmodel_init.print_stats(model)\n\ntorch.cuda.reset_peak_memory_stats(\"cuda\")\nmem(\"Model\")", "torch.cuda.reset_peak_memory_stats(\"cuda\")\nmem(\"Model\")\n\ncache = ExLlamaCache(model)\nmem(\"Cache\")\n\n# Load LoRA\n\nlora = None\nif args.lora:\n    print(f\" -- LoRA config: {args.lora_config}\")\n    print(f\" -- Loading LoRA: {args.lora}\")\n    if args.lora_config is None:\n        print(f\" ## Error: please specify lora path to adapter_config.json\")\n        sys.exit()\n    lora = ExLlamaLora(model, args.lora_config, args.lora)\n    if lora.bias_ignored:\n        print(f\" !! Warning: LoRA zero bias ignored\")", "lora = None\nif args.lora:\n    print(f\" -- LoRA config: {args.lora_config}\")\n    print(f\" -- Loading LoRA: {args.lora}\")\n    if args.lora_config is None:\n        print(f\" ## Error: please specify lora path to adapter_config.json\")\n        sys.exit()\n    lora = ExLlamaLora(model, args.lora_config, args.lora)\n    if lora.bias_ignored:\n        print(f\" !! Warning: LoRA zero bias ignored\")", "\n# Test sequence\n\ngen_tokens = 128\nmax_seq_len = args.length\nids = torch.randint(0, 31999, (1, max_seq_len - gen_tokens)).cuda()\n\n# Benchmark memory and performance\n\nif args.perf:\n\n    # Warming up apparently makes a huge difference\n\n    for i in range(1, 3):\n        print(f\" -- Warmup pass {i}...\")\n        begin()\n        logits = timer(\"Warmup\", lambda: next_logits(ids, lora))\n\n    # Do the actual benchmark\n\n    begin()\n\n    t = time.time()\n\n    print(\" -- Inference, first pass.\")\n    logits = timer(\"Inference\", lambda: next_logits(ids, lora))\n\n    t = time.time() - t\n    print(f\" ** Speed: {ids.shape[-1] / t:.2f} tokens/second\")\n\n    for j in range(2):\n\n        t = time.time()\n        print(f\" -- Generating {gen_tokens} tokens, {ids.shape[-1]} token prompt...\")\n        for i in range(gen_tokens):\n\n            logits = logits[0, -1, :]\n            token = torch.argmax(logits)\n            next_id = token.unsqueeze(0).unsqueeze(0)\n            logits = next_logits(next_id, lora)\n\n        t = time.time() - t\n        print(f\" ** Speed: {gen_tokens / t:.2f} tokens/second\")\n\n        ids = ids[:, :4]\n        cache.current_seq_len = 4\n\n    mem(\"Inference\")\n    mem(\"Total\", total = True)", "\nif args.perf:\n\n    # Warming up apparently makes a huge difference\n\n    for i in range(1, 3):\n        print(f\" -- Warmup pass {i}...\")\n        begin()\n        logits = timer(\"Warmup\", lambda: next_logits(ids, lora))\n\n    # Do the actual benchmark\n\n    begin()\n\n    t = time.time()\n\n    print(\" -- Inference, first pass.\")\n    logits = timer(\"Inference\", lambda: next_logits(ids, lora))\n\n    t = time.time() - t\n    print(f\" ** Speed: {ids.shape[-1] / t:.2f} tokens/second\")\n\n    for j in range(2):\n\n        t = time.time()\n        print(f\" -- Generating {gen_tokens} tokens, {ids.shape[-1]} token prompt...\")\n        for i in range(gen_tokens):\n\n            logits = logits[0, -1, :]\n            token = torch.argmax(logits)\n            next_id = token.unsqueeze(0).unsqueeze(0)\n            logits = next_logits(next_id, lora)\n\n        t = time.time() - t\n        print(f\" ** Speed: {gen_tokens / t:.2f} tokens/second\")\n\n        ids = ids[:, :4]\n        cache.current_seq_len = 4\n\n    mem(\"Inference\")\n    mem(\"Total\", total = True)", "\n\n# Benchmark perplexity\n\nif args.perplexity:\n\n    ppl = Perplexity(args.perplexity, model, cache, tokenizer)\n\n    print(\" -- Loading dataset...\")\n\n    ppl.load(dataset_path = args.perplexity_dataset,\n             chunk_size = args.perplexity_chunk_size,\n             chunk_truncate = args.perplexity_chunk_truncate,\n             overlap = args.perplexity_chunk_overlap,\n             minlength = args.perplexity_chunk_min,\n             json_key = args.perplexity_json_key)\n\n    begin()\n\n    ppl.test(args.perplexity_chunk_num,\n             lora = lora,\n             ppl_token = args.perplexity_token)", "\n# Validate file\n\nif args.validate:\n\n    ppl = Perplexity(args.perplexity, model, cache, tokenizer)\n\n    ppl.load(dataset_path = \"datasets/wikitext2_val_sample.jsonl\",\n             chunk_size = 2048,\n             chunk_truncate = 2048,\n             overlap = 0,\n             minlength = 50,\n             json_key = \"text\")\n\n    # Short perplexity tests in switched and quant mode, should produce roughly equal results\n\n    begin()\n\n    ppl.cache.zero()\n    model.config.matmul_recons_thd = 1\n    ppl.test(8, lora = lora, tag = \" (reconstruct)\")\n    ppl.cache.zero()\n    model.config.matmul_recons_thd = 0\n    ppl.test(8, lora = lora, tag = \" (quant, token)\", ppl_token = True)\n\n    # Do a short, easy topk=1 completion to see if we're generating garbage. Should run in switched mode\n    # for the prompt and quant for individual tokens\n\n    model.config.matmul_recons_thd = 4\n    generator = ExLlamaGenerator(model, tokenizer, cache)\n    generator.settings.top_k = 1\n    generator.lora = lora\n    text = generator.generate_simple(\"To be or not to be, that is the\", max_new_tokens = 20 * args.validate)\n    print(f\" ** Generation: {repr(text)}\")\n\n    if args.validate > 1:\n\n        # Test batched generation\n\n        bsz = 8\n        gen_len = 20\n        torch.manual_seed(42)\n        torch.cuda.manual_seed_all(42)\n\n        # Bigger cache for the batch\n\n        del cache\n        cache = ExLlamaCache(model, batch_size = bsz)\n\n        # Create tokenized batch and attention mask\n\n        identical_batch_prompt = \"When you have eliminated the impossible, whatever remains,\"\n        continuations = [\n            \" must be considered\",\n            \" ought to be\",\n            \" (and some scholars say this is\",\n            \" however improbable, is a banana.\",\n        ]\n\n        prompts = [identical_batch_prompt] * (bsz - len(continuations))\n        for cont in continuations:\n            prompts.append(identical_batch_prompt + cont)\n\n        ids = tokenizer.encode(prompts)\n        assert ids.shape[1] < model.config.max_seq_len, f\"Max length {ids.shape[1]} exceeds model limit {model.config.max_seq_len}\"\n\n        mask = ids.ne(tokenizer.pad_token_id)\n\n        # Batched generation with greedy sampling\n\n        sequence = torch.empty((bsz, 0), dtype = torch.long, device = \"cpu\")\n        logits = next_logits(ids, lora, input_mask = mask)\n\n        for i in range(gen_len):\n            logits = logits[:, -1, :]\n            id_per_batch = torch.argmax(logits, dim=-1)\n            assert id_per_batch.shape == (bsz,), f\"{id_per_batch.shape} != {(bsz,)}\"\n            next_id_per_batch = id_per_batch.unsqueeze(-1)\n            sequence = torch.cat((sequence, next_id_per_batch), dim = -1)\n            logits = next_logits(next_id_per_batch, lora)\n\n        # Print output batch\n\n        print(f\"\\n ** Batching sanity check: 1-{bsz - len(continuations)} should be identical. All should be reasonable for the model you're using.\\n\")\n\n        outputs = tokenizer.decode(sequence)\n        for b in range(bsz):\n            print(f\"{b + 1} {repr(prompts[b])} -> {repr(outputs[b])}\")", "\n        # TODO Save the logits and then rerun each prompt with a batch size of 1, same input. The logits should be identical.\n"]}
{"filename": "example_basic.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport os, glob\n\n# Directory containing model, tokenizer, generator\n\nmodel_directory =  \"/mnt/str/models/llama-13b-4bit-128g/\"\n\n# Locate files we need within that directory", "\n# Locate files we need within that directory\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\n\n# Create config, model, tokenizer and generator\n", "# Create config, model, tokenizer and generator\n\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n\ncache = ExLlamaCache(model)                             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator", "cache = ExLlamaCache(model)                             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n\n# Configure generator\n\ngenerator.disallow_tokens([tokenizer.eos_token_id])\n\ngenerator.settings.token_repetition_penalty_max = 1.2\ngenerator.settings.temperature = 0.95\ngenerator.settings.top_p = 0.65", "generator.settings.temperature = 0.95\ngenerator.settings.top_p = 0.65\ngenerator.settings.top_k = 100\ngenerator.settings.typical = 0.5\n\n# Produce a simple generation\n\nprompt = \"Once upon a time,\"\nprint (prompt, end = \"\")\n", "print (prompt, end = \"\")\n\noutput = generator.generate_simple(prompt, max_new_tokens = 200)\n\nprint(output[len(prompt):])\n"]}
{"filename": "example_alt_generator.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom lora import ExLlamaLora\nfrom tokenizer import ExLlamaTokenizer\nfrom alt_generator import ExLlamaAltGenerator\nimport argparse\nimport torch\nimport sys\nimport os\nimport glob\nimport model_init", "import glob\nimport model_init\nimport time\n\n\nconfig: ExLlamaConfig           # Config for the model, loaded from config.json\nmodel: ExLlama                  # Model, initialized with ExLlamaConfig\ncache: ExLlamaCache             # Cache for generation, tied to model\ngenerator: ExLlamaAltGenerator  # Generator\ntokenizer: ExLlamaTokenizer     # Tokenizer", "generator: ExLlamaAltGenerator  # Generator\ntokenizer: ExLlamaTokenizer     # Tokenizer\nlora: ExLlamaLora = None        # (Optional) LoRA, remember to specify in generator settings\n\n\n# Initialize model\n\ndef init_explicit():\n    global model, cache, config, generator, tokenizer, lora\n\n    # Directory containing model, tokenizer\n\n    model_directory = \"/mnt/str/models/llama-7b-4bit-128g/\"\n\n    # Locate files we need within that directory\n\n    tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n    model_config_path = os.path.join(model_directory, \"config.json\")\n    st_pattern = os.path.join(model_directory, \"*.safetensors\")\n    model_path = glob.glob(st_pattern)[0]\n\n    # Create config, model, tokenizer and generator\n\n    config = ExLlamaConfig(model_config_path)                   # create config from config.json\n    config.model_path = model_path                              # supply path to model weights file\n\n    model = ExLlama(config)                                     # create ExLlama instance and load the weights\n    tokenizer = ExLlamaTokenizer(tokenizer_path)                # create tokenizer from tokenizer model file\n\n    cache = ExLlamaCache(model)                                 # create cache for inference\n    generator = ExLlamaAltGenerator(model, tokenizer, cache)    # create generator\n\n    # Load LoRA\n\n    lora_dir = None\n\n    if lora_dir is not None:\n        lora_config = os.path.join(lora_dir, \"adapter_config.json\")\n        lora = os.path.join(lora_dir, \"adapter_model.bin\")\n        lora = ExLlamaLora(model, lora_config, lora)", "\n\n# Alternatively, initialize from command line args\n\ndef init_args():\n    global model, cache, config, generator, tokenizer, lora\n\n    # Global initialization\n\n    torch.set_grad_enabled(False)\n    torch.cuda._lazy_init()\n\n    # Parse arguments\n\n    parser = argparse.ArgumentParser(description = \"Generator example\")\n\n    model_init.add_args(parser)\n\n    parser.add_argument(\"-lora\", \"--lora\", type = str, help = \"Path to LoRA binary to use during benchmark\")\n    parser.add_argument(\"-loracfg\", \"--lora_config\", type = str, help = \"Path to LoRA config to use during benchmark\")\n    parser.add_argument(\"-ld\", \"--lora_dir\", type = str, help = \"Path to LoRA config and binary. to use during benchmark\")\n\n    args = parser.parse_args()\n    model_init.post_parse(args)\n    model_init.get_model_files(args)\n\n    print_opts = []\n    model_init.print_options(args, print_opts)\n\n    # Paths\n\n    if args.lora_dir is not None:\n        args.lora_config = os.path.join(args.lora_dir, \"adapter_config.json\")\n        args.lora = os.path.join(args.lora_dir, \"adapter_model.bin\")\n\n    # Model globals\n\n    model_init.set_globals(args)\n\n    # Instantiate model and generator\n\n    config = model_init.make_config(args)\n\n    model = ExLlama(config)\n    cache = ExLlamaCache(model)\n    tokenizer = ExLlamaTokenizer(args.tokenizer)\n\n    model_init.print_stats(model)\n\n    # Load LoRA\n\n    lora = None\n    if args.lora:\n        print(f\" -- LoRA config: {args.lora_config}\")\n        print(f\" -- Loading LoRA: {args.lora}\")\n        if args.lora_config is None:\n            print(f\" ## Error: please specify lora path to adapter_config.json\")\n            sys.exit()\n        lora = ExLlamaLora(model, args.lora_config, args.lora)\n        if lora.bias_ignored:\n            print(f\" !! Warning: LoRA zero bias ignored\")\n\n    # Generator\n\n    generator = ExLlamaAltGenerator(model, tokenizer, cache)", "\n\n# Intialize\n\n# init_args()\ninit_explicit()\n\n# Example one-shot generation\n\nsettings = ExLlamaAltGenerator.Settings()", "\nsettings = ExLlamaAltGenerator.Settings()\nsettings.temperature = 0.75\nsettings.top_p = 0.8\n\nprompt = \"A bird in the hand is worth\"\n\nstop_conditions = [\".\", \"!\", \"bush\", tokenizer.newline_token_id]\n\noutput = generator.generate(prompt = prompt,", "\noutput = generator.generate(prompt = prompt,\n                            stop_conditions = stop_conditions,\n                            max_new_tokens = 50,\n                            gen_settings = settings)\n\nprint()\nprint(prompt + output)\nprint()\n", "print()\n\n\n# Example of (implicit) cache reuse\n\ncontext = \"\"\"Albert Einstein (/\u02c8a\u026ansta\u026an/ EYEN-styne;[4] German: [\u02c8alb\u025b\u0281t \u02c8\u0294a\u026an\u0283ta\u026an] (listen); 14 March 1879 \u2013 18 April 1955) was a German-born theoretical physicist,[5] widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.[1][6] His mass\u2013energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".[7] He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\",[8] a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science.[9][10] In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time.[11] His intellectual achievements and originality have made Einstein synonymous with genius.[12]\nIn 1905, a year sometimes described as his annus mirabilis (miracle year), Einstein published four groundbreaking papers.[13] These outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity\u2014a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field\u2014and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.[14][15] The middle part of his career also saw him making important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons.\nFor much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. Firstly, he fought a long rearguard action against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that \"God does not play dice\".[16] Secondly, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream of modern physics.\nBorn in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of W\u00fcrttemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss Federal polytechnic school in Z\u00fcrich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia.[note 1] In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Alienated by the policies of the newly elected Nazi government,[17] Einstein decided to remain in the US, and was granted American citizenship in 1940.[18] On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.[19]\nAlbert Einstein was born in Ulm,[5] in the Kingdom of W\u00fcrttemberg in the German Empire, on 14 March 1879.[20][21] His parents, secular Ashkenazi Jews, were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein & Cie, a company that manufactured electrical equipment based on direct current.[5]", "Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of W\u00fcrttemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss Federal polytechnic school in Z\u00fcrich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia.[note 1] In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Alienated by the policies of the newly elected Nazi government,[17] Einstein decided to remain in the US, and was granted American citizenship in 1940.[18] On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.[19]\nAlbert Einstein was born in Ulm,[5] in the Kingdom of W\u00fcrttemberg in the German Empire, on 14 March 1879.[20][21] His parents, secular Ashkenazi Jews, were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein & Cie, a company that manufactured electrical equipment based on direct current.[5]\nAlbert attended a Catholic elementary school in Munich from the age of five. When he was eight, he was transferred to the Luitpold-Gymnasium (now known as the Albert-Einstein-Gymnasium [de]), where he received advanced primary and then secondary school education.[22]\nIn 1894, Hermann and Jakob's company tendered for a contract to install electric lighting in Munich, but without success\u2014they lacked the capital that would have been required to update their technology from direct current to the more efficient, alternating current alternative.[23] The failure of their bid forced them to sell their Munich factory and search for new opportunities elsewhere. The Einstein family moved to Italy, first to Milan and a few months later to Pavia, where they settled in Palazzo Cornazzani, a medieval building which, at different times, had been the home of Ugo Foscolo, Contardo Ferrini and Ada Negri.[24] Einstein, then fifteen, stayed behind in Munich in order to finish his schooling. His father wanted him to study electrical engineering, but he was a fractious pupil who found the Gymnasium's regimen and teaching methods far from congenial. He later wrote that the school's policy of strict rote learning was harmful to creativity. At the end of December 1894, a letter from a doctor persuaded the Luitpold's authorities to release him from its care, and he joined his family in Pavia.[25] While in Italy as a teenager, he wrote an essay entitled \"On the Investigation of the State of the Ether in a Magnetic Field\".[26][27]\nEinstein excelled at physics and mathematics from an early age, and soon acquired the mathematical expertise normally only found in a child several years his senior. He began teaching himself algebra, calculus and Euclidean geometry when he was twelve; he made such rapid progress that he discovered an original proof of the Pythagorean theorem before his thirteenth birthday.[28][29][30] A family tutor, Max Talmud, said that only a short time after he had given the twelve year old Einstein a geometry textbook, the boy \"had worked through the whole book. He thereupon devoted himself to higher mathematics ... Soon the flight of his mathematical genius was so high I could not follow.\"[31] Einstein recorded that he had \"mastered integral and differential calculus\" while still just fourteen.[29] His love of algebra and geometry was so great that at twelve, he was already confident that nature could be understood as a \"mathematical structure\".[31]\nAt thirteen, when his range of enthusiasms had broadened to include music and philosophy,[32] Einstein was introduced to Kant's Critique of Pure Reason. Kant became his favorite philosopher; according to his tutor, \"At the time he was still a child, only thirteen years old, yet Kant's works, incomprehensible to ordinary mortals, seemed to be clear to him.\"[31]\"\"\"\n\ndef timer(func):\n    t = time.time()\n    ret = func()\n    t = time.time() - t\n    return ret, t", "\nsettings = ExLlamaAltGenerator.Settings()\nsettings.temperature = 0.95\nsettings.top_k = 80\nsettings.typical = 0.8\n\nquestions = [\"When was Albert Einstein born?\",\n             \"How many groundbreaking papers did Einstein publish in 1905?\",\n             \"Where did Einstein move in 1895?\",\n             \"When did Einstein graduate?\"]", "             \"Where did Einstein move in 1895?\",\n             \"When did Einstein graduate?\"]\n\nstop_conditions = [tokenizer.newline_token_id, tokenizer.eos_token_id]\n\nfor question in questions:\n    output, t = timer(lambda: generator.generate(context + \"\\nQ: \" + question + \"\\nA:\", stop_conditions, 100, settings))\n    print(f\"Generated in {t:.3f} seconds: {question} -> {output}\")\n\n", "\n\n# Streaming example\n\nsettings = ExLlamaAltGenerator.Settings()\nsettings.temperature = 1.00\nsettings.top_k = 80\nsettings.top_p = 0.9\nsettings.lora = lora\n", "settings.lora = lora\n\nprompt = \"Our story begins in the town of Auchtermuchty, where once\"\n\nprint()\nprint(prompt, end = \"\")\nsys.stdout.flush()\n\noutput = generator.begin_stream(prompt = prompt,\n                                stop_conditions = [],", "output = generator.begin_stream(prompt = prompt,\n                                stop_conditions = [],\n                                max_new_tokens = 1000,\n                                gen_settings = settings)\n\nwhile True:\n    chunk, eos = generator.stream()\n    print(chunk, end = \"\")\n    sys.stdout.flush()\n    if eos: break", "    sys.stdout.flush()\n    if eos: break\n"]}
{"filename": "example_flask.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom flask import Flask, request\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport os, glob\n\n# Directory containing config.json, tokenizer.model and safetensors file for the model\nmodel_directory = \"/mnt/str/models/llama-7b-4bit/\"\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")", "\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\n\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights", "\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\nprint(f\"Model loaded: {model_path}\")\n\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\ncache = ExLlamaCache(model)                             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n\n# Flask app\n", "# Flask app\n\napp = Flask(__name__)\n\n\n# Inference with settings equivalent to the \"precise\" preset from the /r/LocalLLaMA wiki\n\n@app.route('/infer_precise', methods=['POST'])\ndef inferContextP():\n    print(request.form)\n    prompt = request.form.get('prompt')\n\n    generator.settings.token_repetition_penalty_max = 1.176\n    generator.settings.token_repetition_penalty_sustain = config.max_seq_len\n    generator.settings.temperature = 0.7\n    generator.settings.top_p = 0.1\n    generator.settings.top_k = 40\n    generator.settings.typical = 0.0    # Disabled\n\n    outputs = generator.generate_simple(prompt, max_new_tokens = 200)\n    return outputs", "def inferContextP():\n    print(request.form)\n    prompt = request.form.get('prompt')\n\n    generator.settings.token_repetition_penalty_max = 1.176\n    generator.settings.token_repetition_penalty_sustain = config.max_seq_len\n    generator.settings.temperature = 0.7\n    generator.settings.top_p = 0.1\n    generator.settings.top_k = 40\n    generator.settings.typical = 0.0    # Disabled\n\n    outputs = generator.generate_simple(prompt, max_new_tokens = 200)\n    return outputs", "\n\n# Inference with settings equivalent to the \"creative\" preset from the /r/LocalLLaMA wiki\n\n@app.route('/infer_creative', methods=['POST'])\ndef inferContextC():\n    print(request.form)\n    prompt = request.form.get('prompt')\n\n    generator.settings.token_repetition_penalty_max = 1.1\n    generator.settings.token_repetition_penalty_sustain = config.max_seq_len\n    generator.settings.temperature = 0.72\n    generator.settings.top_p = 0.73\n    generator.settings.top_k = 0        # Disabled\n    generator.settings.typical = 0.0    # Disabled\n\n    outputs = generator.generate_simple(prompt, max_new_tokens = 200)\n    return outputs", "\n\n# Inference with settings equivalent to the \"sphinx\" preset from the /r/LocalLLaMA wiki\n\n@app.route('/infer_sphinx', methods=['POST'])\ndef inferContextS():\n    print(request.form)\n    prompt = request.form.get('prompt')\n\n    generator.settings.token_repetition_penalty_max = 1.15\n    generator.settings.token_repetition_penalty_sustain = config.max_seq_len\n    generator.settings.temperature = 1.99\n    generator.settings.top_p = 0.18\n    generator.settings.top_k = 30\n    generator.settings.typical = 0.0    # Disabled\n\n    outputs = generator.generate_simple(prompt, max_new_tokens = 200)\n    return outputs", "\n\n# Start Flask app\n\nhost = \"0.0.0.0\"\nport = 8004\nprint(f\"Starting server on address {host}:{port}\")\n\nif __name__ == '__main__':\n    from waitress import serve\n    serve(app, host = host, port = port)", "if __name__ == '__main__':\n    from waitress import serve\n    serve(app, host = host, port = port)\n"]}
{"filename": "globals.py", "chunked_list": ["import os\n\ndef set_affinity_mask(affinity_mask = None):\n\n    if affinity_mask is None:\n        cpu_count = os.cpu_count()\n        affinity_mask = set(range(cpu_count))\n\n    os.sched_setaffinity(0, affinity_mask)\n", "\n\ndef set_affinity_list(affinity_list = None):\n\n    if affinity_list is None: set_affinity_mask(None)\n    else: set_affinity_mask(set(affinity_list))\n\n\ndef set_affinity_str(affinity_str = None):\n\n    if affinity_str is None or affinity_str.isspace(): set_affinity_mask(None)\n    aff = [int(alloc) for alloc in affinity_str.split(\",\")]\n    set_affinity_list(aff)", "def set_affinity_str(affinity_str = None):\n\n    if affinity_str is None or affinity_str.isspace(): set_affinity_mask(None)\n    aff = [int(alloc) for alloc in affinity_str.split(\",\")]\n    set_affinity_list(aff)\n"]}
{"filename": "lora.py", "chunked_list": ["from model import ExLlamaConfig, Ex4bitLinear\nimport torch\nimport json\nfrom safetensors.torch import load_file as safe_load_file\nfrom torch import load as load_file\n\nclass ExLlamaLora:\n\n    lora_config_path: str\n    lora_path: str\n    lora_r: int\n    lora_alpha: float\n    lora_scaling: float\n    config: ExLlamaConfig\n    tensors: dict[torch.tensor]\n    bias_ignored: bool\n\n    def __init__(self, model, lora_config_path, lora_path):\n\n        self.lora_config_path = lora_config_path\n        self.lora_path = lora_path\n        self.model = model\n        self.config = model.config\n        self.tensors = {}\n        self.bias_ignored = False\n\n        # Grab relevant items from LoRA config\n\n        with open(lora_config_path) as f:\n            read_config = json.load(f)\n\n        self.lora_r = read_config[\"r\"]\n        self.lora_alpha = float(read_config[\"lora_alpha\"])\n        self.lora_scaling = self.lora_alpha / self.lora_r\n\n        if \"fan_in_fan_out\" in read_config and read_config[\"fan_in_fan_out\"]:\n            raise ValueError(\" ## Error: fan_in_fan_out mode not supported.\")\n\n        # Load LoRA weights\n\n        if self.lora_path.endswith(\".safetensors\"):\n            f = safe_load_file(self.lora_path, device = \"cpu\")\n        else:\n            f = load_file(self.lora_path, map_location = \"cpu\")\n\n        for key in f.keys():\n            tensor = f[key]\n\n            # Find target\n\n            i = key.find(\"model.layers.\")\n            if i == -1: raise ValueError(f\" ## Error: unsupported layer in {self.lora_path}: {key}\")\n\n            target_key = key[i:]\n            ks = target_key.split(\".\")\n            decoder_idx = int(ks[2])\n            decoder_part = ks[3]\n            decoder_layer = ks[4]\n            lora_half = ks[5]\n\n            if lora_half == \"bias\":\n                epsilon = 1e-6\n                if torch.max(tensor) > epsilon or torch.max(tensor) < -epsilon:\n                    raise ValueError(f\" ## Error: unsupported bias target {self.lora_path}: {key}\")\n                self.bias_ignored = True\n                continue\n\n            target_module = self.model.layers[decoder_idx]\n            if decoder_part == \"self_attn\": target_module = target_module.self_attn\n            elif decoder_part == \"mlp\": target_module = target_module.mlp\n            else: raise ValueError(f\" ## Error: unsupported layer in {self.lora_path}: {key}\")\n\n            if   decoder_layer == \"q_proj\": target_module = target_module.q_proj\n            elif decoder_layer == \"k_proj\": target_module = target_module.k_proj\n            elif decoder_layer == \"v_proj\": target_module = target_module.v_proj\n            elif decoder_layer == \"o_proj\": target_module = target_module.o_proj\n            elif decoder_layer == \"gate_proj\": target_module = target_module.gate_proj\n            elif decoder_layer == \"up_proj\": target_module = target_module.up_proj\n            elif decoder_layer == \"down_proj\": target_module = target_module.down_proj\n            else: raise ValueError(f\" ## Error: unsupported layer in {self.lora_path}: {key}\")\n\n            # Check that shape is compatible\n\n            assert isinstance(target_module, Ex4bitLinear)\n\n            if lora_half == \"lora_A\":\n                in_features = tensor.shape[1]\n                out_features = None\n            elif lora_half == \"lora_B\":\n                in_features = None\n                out_features = tensor.shape[0]\n            else: raise ValueError(f\" ## Error: unsupported layer in {self.lora_path}: {key}\")\n\n            if (in_features and in_features != target_module.in_features) or (out_features and out_features != target_module.out_features):\n                raise ValueError(f\" ## Error: incompatible tensor shape in {self.lora_path}: {key}\")\n\n            # For efficiency, transpose adapter instead of transposing state during inference\n\n            tensor = tensor.T.contiguous()\n\n            # Pre-scale\n\n            if lora_half == \"lora_B\" and self.lora_scaling != 1.0: tensor.mul_(self.lora_scaling)\n\n            # Check that dtype is compatible, or convert\n\n            if tensor.dtype == torch.bfloat16:\n                tensor = tensor.to(torch.float16)\n\n            elif tensor.dtype == torch.float32:\n                tensor = tensor.to(torch.float16)\n\n            elif tensor.dtype == torch.float16:\n                pass\n\n            else: raise ValueError(f\" ## Error: unsupported tensor dtype in {self.lora_path}\")\n\n            # Move to target device\n\n            device = self.config.device_map.map(target_key)\n            tensor = tensor.to(device, non_blocking = True)\n\n            # Store adapter tensor\n\n            self.tensors[target_key] = tensor", ""]}
{"filename": "example_chatbot.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom lora import ExLlamaLora\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport argparse\nimport torch\nimport sys\nimport os\nimport glob\nimport model_init", "import glob\nimport model_init\n\n# Simple interactive chatbot script\n\ntorch.set_grad_enabled(False)\ntorch.cuda._lazy_init()\n\n# Parse arguments\n", "# Parse arguments\n\nparser = argparse.ArgumentParser(description = \"Simple chatbot example for ExLlama\")\n\nmodel_init.add_args(parser)\n\nparser.add_argument(\"-lora\", \"--lora\", type = str, help = \"Path to LoRA binary to use during benchmark\")\nparser.add_argument(\"-loracfg\", \"--lora_config\", type = str, help = \"Path to LoRA config to use during benchmark\")\nparser.add_argument(\"-ld\", \"--lora_dir\", type = str, help = \"Path to LoRA config and binary. to use during benchmark\")\n", "parser.add_argument(\"-ld\", \"--lora_dir\", type = str, help = \"Path to LoRA config and binary. to use during benchmark\")\n\nparser.add_argument(\"-p\", \"--prompt\", type = str, help = \"Prompt file\")\nparser.add_argument(\"-un\", \"--username\", type = str, help = \"Display name of user\", default = \"User\")\nparser.add_argument(\"-bn\", \"--botname\", type = str, help = \"Display name of chatbot\", default = \"Chatbort\")\nparser.add_argument(\"-bf\", \"--botfirst\", action = \"store_true\", help = \"Start chat on bot's turn\")\n\nparser.add_argument(\"-nnl\", \"--no_newline\", action = \"store_true\", help = \"Do not break bot's response on newline (allow multi-paragraph responses)\")\nparser.add_argument(\"-temp\", \"--temperature\", type = float, help = \"Temperature\", default = 0.95)\nparser.add_argument(\"-topk\", \"--top_k\", type = int, help = \"Top-K\", default = 20)", "parser.add_argument(\"-temp\", \"--temperature\", type = float, help = \"Temperature\", default = 0.95)\nparser.add_argument(\"-topk\", \"--top_k\", type = int, help = \"Top-K\", default = 20)\nparser.add_argument(\"-topp\", \"--top_p\", type = float, help = \"Top-P\", default = 0.65)\nparser.add_argument(\"-minp\", \"--min_p\", type = float, help = \"Min-P\", default = 0.00)\nparser.add_argument(\"-repp\",  \"--repetition_penalty\", type = float, help = \"Repetition penalty\", default = 1.15)\nparser.add_argument(\"-repps\", \"--repetition_penalty_sustain\", type = int, help = \"Past length for repetition penalty\", default = 256)\nparser.add_argument(\"-beams\", \"--beams\", type = int, help = \"Number of beams for beam search\", default = 1)\nparser.add_argument(\"-beamlen\", \"--beam_length\", type = int, help = \"Number of future tokens to consider\", default = 1)\n\nargs = parser.parse_args()", "\nargs = parser.parse_args()\nmodel_init.post_parse(args)\nmodel_init.get_model_files(args)\n\n# Paths\n\nif args.lora_dir is not None:\n    args.lora_config = os.path.join(args.lora_dir, \"adapter_config.json\")\n    args.lora = os.path.join(args.lora_dir, \"adapter_model.bin\")", "\n# Some feedback\n\nprint(f\" -- Sequence length: {args.length}\")\nprint(f\" -- Temperature: {args.temperature:.2f}\")\nprint(f\" -- Top-K: {args.top_k}\")\nprint(f\" -- Top-P: {args.top_p:.2f}\")\nprint(f\" -- Min-P: {args.min_p:.2f}\")\nprint(f\" -- Repetition penalty: {args.repetition_penalty:.2f}\")\nprint(f\" -- Beams: {args.beams} x {args.beam_length}\")", "print(f\" -- Repetition penalty: {args.repetition_penalty:.2f}\")\nprint(f\" -- Beams: {args.beams} x {args.beam_length}\")\n\nprint_opts = []\nif args.no_newline: print_opts.append(\"no_newline\")\nif args.botfirst: print_opts.append(\"botfirst\")\n\nmodel_init.print_options(args, print_opts)\n\n# Globals", "\n# Globals\n\nmodel_init.set_globals(args)\n\n# Load prompt file\n\nusername = args.username\nbot_name = args.botname\n\nif args.prompt is not None:\n    with open(args.prompt, \"r\") as f:\n        past = f.read()\n        past = past.replace(\"{username}\", username)\n        past = past.replace(\"{bot_name}\", bot_name)\n        past = past.strip() + \"\\n\"\nelse:\n    past = f\"{bot_name}: Hello, {username}\\n\"", "bot_name = args.botname\n\nif args.prompt is not None:\n    with open(args.prompt, \"r\") as f:\n        past = f.read()\n        past = past.replace(\"{username}\", username)\n        past = past.replace(\"{bot_name}\", bot_name)\n        past = past.strip() + \"\\n\"\nelse:\n    past = f\"{bot_name}: Hello, {username}\\n\"", "\n# past += \"User: Hi. Please say \\\"Shhhhhh\\\"?\\n\"\n# args.botfirst = True\n\n# Instantiate model and generator\n\nconfig = model_init.make_config(args)\n\nmodel = ExLlama(config)\ncache = ExLlamaCache(model)", "model = ExLlama(config)\ncache = ExLlamaCache(model)\ntokenizer = ExLlamaTokenizer(args.tokenizer)\n\nmodel_init.print_stats(model)\n\n# Load LoRA\n\nlora = None\nif args.lora:\n    print(f\" -- LoRA config: {args.lora_config}\")\n    print(f\" -- Loading LoRA: {args.lora}\")\n    if args.lora_config is None:\n        print(f\" ## Error: please specify lora path to adapter_config.json\")\n        sys.exit()\n    lora = ExLlamaLora(model, args.lora_config, args.lora)\n    if lora.bias_ignored:\n        print(f\" !! Warning: LoRA zero bias ignored\")", "lora = None\nif args.lora:\n    print(f\" -- LoRA config: {args.lora_config}\")\n    print(f\" -- Loading LoRA: {args.lora}\")\n    if args.lora_config is None:\n        print(f\" ## Error: please specify lora path to adapter_config.json\")\n        sys.exit()\n    lora = ExLlamaLora(model, args.lora_config, args.lora)\n    if lora.bias_ignored:\n        print(f\" !! Warning: LoRA zero bias ignored\")", "\n# Generator\n\ngenerator = ExLlamaGenerator(model, tokenizer, cache)\ngenerator.settings = ExLlamaGenerator.Settings()\ngenerator.settings.temperature = args.temperature\ngenerator.settings.top_k = args.top_k\ngenerator.settings.top_p = args.top_p\ngenerator.settings.min_p = args.min_p\ngenerator.settings.token_repetition_penalty_max = args.repetition_penalty", "generator.settings.min_p = args.min_p\ngenerator.settings.token_repetition_penalty_max = args.repetition_penalty\ngenerator.settings.token_repetition_penalty_sustain = args.repetition_penalty_sustain\ngenerator.settings.token_repetition_penalty_decay = generator.settings.token_repetition_penalty_sustain // 2\ngenerator.settings.beams = args.beams\ngenerator.settings.beam_length = args.beam_length\n\ngenerator.lora = lora\n\nbreak_on_newline = not args.no_newline", "\nbreak_on_newline = not args.no_newline\n\n# Be nice to Chatbort\n\nmin_response_tokens = 4\nmax_response_tokens = 256\nextra_prune = 256\n\nprint(past, end = \"\")", "\nprint(past, end = \"\")\nids = tokenizer.encode(past)\ngenerator.gen_begin(ids)\n\nnext_userprompt = username + \": \"\n\nfirst_round = True\n\nwhile True:", "\nwhile True:\n\n    res_line = bot_name + \":\"\n    res_tokens = tokenizer.encode(res_line)\n    num_res_tokens = res_tokens.shape[-1]  # Decode from here\n\n    if first_round and args.botfirst: in_tokens = res_tokens\n\n    else:\n\n        # Read and format input\n\n        in_line = input(next_userprompt)\n        in_line = username + \": \" + in_line.strip() + \"\\n\"\n\n        next_userprompt = username + \": \"\n\n        # No need for this, really, unless we were logging the chat. The actual history we work on is kept in the\n        # tokenized sequence in the generator and the state in the cache.\n\n        past += in_line\n\n        # SentencePiece doesn't tokenize spaces separately so we can't know from individual tokens if they start a new word\n        # or not. Instead, repeatedly decode the generated response as it's being built, starting from the last newline,\n        # and print out the differences between consecutive decodings to stream out the response.\n\n        in_tokens = tokenizer.encode(in_line)\n        in_tokens = torch.cat((in_tokens, res_tokens), dim = 1)", "\n    # If we're approaching the context limit, prune some whole lines from the start of the context. Also prune a\n    # little extra so we don't end up rebuilding the cache on every line when up against the limit.\n\n    expect_tokens = in_tokens.shape[-1] + max_response_tokens\n    max_tokens = config.max_seq_len - expect_tokens\n    if generator.gen_num_tokens() >= max_tokens:\n        generator.gen_prune_to(config.max_seq_len - expect_tokens - extra_prune, tokenizer.newline_token_id)\n\n    # Feed in the user input and \"{bot_name}:\", tokenized", "\n    # Feed in the user input and \"{bot_name}:\", tokenized\n\n    generator.gen_feed_tokens(in_tokens)\n\n    # Generate with streaming\n\n    print(res_line, end = \"\")\n    sys.stdout.flush()\n", "    sys.stdout.flush()\n\n    generator.begin_beam_search()\n\n    for i in range(max_response_tokens):\n\n        # Disallowing the end condition tokens seems like a clean way to force longer replies.\n\n        if i < min_response_tokens:\n            generator.disallow_tokens([tokenizer.newline_token_id, tokenizer.eos_token_id])\n        else:\n            generator.disallow_tokens(None)\n\n        # Get a token\n\n        gen_token = generator.beam_search()\n\n        # If token is EOS, replace it with newline before continuing\n\n        if gen_token.item() == tokenizer.eos_token_id:\n            generator.replace_last_token(tokenizer.newline_token_id)\n\n        # Decode the current line and print any characters added\n\n        num_res_tokens += 1\n        text = tokenizer.decode(generator.sequence_actual[:, -num_res_tokens:][0])\n        new_text = text[len(res_line):]\n\n        skip_space = res_line.endswith(\"\\n\") and new_text.startswith(\" \")  # Bit prettier console output\n        res_line += new_text\n        if skip_space: new_text = new_text[1:]\n\n        print(new_text, end=\"\")  # (character streaming output is here)\n        sys.stdout.flush()\n\n        # End conditions\n\n        if break_on_newline and gen_token.item() == tokenizer.newline_token_id: break\n        if gen_token.item() == tokenizer.eos_token_id: break\n\n        # Some models will not (or will inconsistently) emit EOS tokens but in a chat sequence will often begin\n        # generating for the user instead. Try to catch this and roll back a few tokens to begin the user round.\n\n        if res_line.endswith(f\"{username}:\"):\n            plen = tokenizer.encode(f\"{username}:\").shape[-1]\n            generator.gen_rewind(plen)\n            next_userprompt = \" \"\n            break", "\n    generator.end_beam_search()\n\n    past += res_line\n    first_round = False\n"]}
{"filename": "generator.py", "chunked_list": ["import cuda_ext\nfrom model import ExLlama, ExLlamaCache\nfrom lora import ExLlamaLora\nimport torch\nimport torch.nn.functional as F\n\nclass ExLlamaGenerator:\n\n    class Settings:\n\n        temperature = 0.95\n        top_k = 40                              # consider the most probable top_k samples, 0 to disable top_k sampling\n        top_p = 0.65                            # consider tokens up to a cumulative probabiltiy of top_p, 0.0 to disable top_p sampling\n        min_p = 0.0                             # Do not consider tokens with probability less than this\n        typical = 0.0                           # Locally typical sampling threshold, 0.0 to disable typical sampling\n\n        token_repetition_penalty_max = 1.15     # Repetition penalty for most recent tokens\n        token_repetition_penalty_sustain = 256  # No. most recent tokens to repeat penalty for, -1 to apply to whole context\n        token_repetition_penalty_decay = 128    # Gradually decrease penalty over this many tokens\n\n        beams = 1\n        beam_length = 1\n\n\n    model: ExLlama\n    sequence: torch.Tensor or None\n    sequence_actual: torch.Tensor or None\n    settings: Settings\n    beams: int or None\n    max_beam_length: int\n    in_beam_search: True\n    disallowed_tokens: list[int] or None\n    lora: ExLlamaLora or None\n\n\n    def __init__(self, model, tokenizer, cache):\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.cache = cache\n        self.reset()\n\n\n    def reset(self):\n\n        self.cache.current_seq_len = 0\n        self.sequence = None\n        self.sequence_actual = None\n        self.settings = ExLlamaGenerator.Settings()\n\n        self.beams = None\n        self.max_beam_length = 0\n        self.in_beam_search = False\n        self.disallowed_tokens = None\n        self.lora = None\n\n\n    def make_rep_mask(self, penalty_max, sustain, decay):\n\n        return cuda_ext.ext_rep_penalty_mask_cpu(self.model.config.vocab_size, self.sequence, penalty_max, sustain, decay)\n\n\n    def batched_sample(self, logits, temperature, top_k, top_p, min_p, typical, num = 1):\n\n        if logits.shape[0] == 1: return self.sample(logits, temperature, top_k, top_p, min_p, typical, num)\n\n        samples = []\n        scores = []\n        for i in range(logits.shape[0]):\n            t, s = self.sample(logits[i, :, :], temperature, top_k, top_p, min_p, typical)\n            samples.append(t)\n            scores.append(s)\n\n        return torch.cat(samples, dim = 0), torch.cat(scores, dim = 0)\n\n\n    # Sample one token from logits with current settings\n\n    def sample_current(self, logits, num = 1):\n\n        return self.sample(logits,\n                           self.settings.temperature,\n                           self.settings.top_k,\n                           self.settings.top_p,\n                           self.settings.min_p,\n                           self.settings.typical)\n\n\n    # Sample one token from logits\n\n    def sample(self, logits, temperature, top_k, top_p, min_p, typical, num = 1):\n\n        # torch.manual_seed(42)\n\n        if logits.dim() == 3: logits = logits[0, -1, :]\n        elif logits.dim() == 2: logits = logits[-1, :]\n        else: raise ValueError(\"Bad logits dimension\")\n\n        # Disallow tokens\n\n        if self.disallowed_tokens is not None:\n            logits[self.disallowed_tokens] = float(\"-inf\")\n\n        # Base probabilities\n\n        logits /= temperature\n        logits += 1e-8\n        probs = torch.softmax(logits, dim = -1)\n\n        # Top K\n\n        if top_k == 0:\n            top_probs, top_indices = torch.sort(probs, descending = True)\n        else:\n            top_probs, top_indices = torch.topk(probs, top_k)\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n\n        # Top P\n\n        if top_p > 0.0:\n\n            num_top_p_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_top_p_probs += 1\n                if num_top_p_probs == top_probs.shape[-1]: break\n                if top_probs[num_top_p_probs].item() < min_p: break\n                cum_prob += top_probs[num_top_p_probs].item()\n                if cum_prob > top_p: break\n\n            top_probs = top_probs[:num_top_p_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_top_p_probs]\n\n        # Locally typical sampling\n\n        if typical > 0.0:\n\n            epsilon = 1e-10\n            log_probs = (top_probs + epsilon).log()\n            neg_entropy = (top_probs * log_probs).sum()\n            entropy_dev = (neg_entropy - log_probs).abs()\n            _, entropy_dev_order = torch.sort(entropy_dev)\n\n            top_probs = top_probs.gather(-1, entropy_dev_order)\n            top_indices = top_indices.gather(-1, entropy_dev_order)\n\n            num_typical_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_typical_probs += 1\n                if num_typical_probs == top_probs.shape[-1]: break\n                cum_prob += top_probs[num_typical_probs].item()\n                if cum_prob > typical: break\n\n            top_probs = top_probs[:num_typical_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_typical_probs]\n\n        # Multinomial sampling from top_probs, kept in same order as top_indices\n\n        sampled_ind = torch.multinomial(top_probs, top_probs.shape[-1] if num == -1 else min(num, top_probs.shape[-1]))\n        sampled_tokens = top_indices[sampled_ind]\n        sampled_probs = top_probs[sampled_ind]  # Return probs before second norm\n\n        if sampled_tokens.shape[0] > 1:\n            sampled_tokens, ind = sampled_tokens.sort()\n            sampled_probs = sampled_probs[ind]\n\n        return sampled_tokens.unsqueeze(0), sampled_probs.unsqueeze(0)\n\n\n    def disallow_tokens(self, tokens):\n\n        self.disallowed_tokens = tokens\n\n\n    def gen_begin(self, in_tokens, mask = None):\n\n        self.end_beam_search()\n\n        self.sequence = in_tokens.clone()\n        self.sequence_actual = in_tokens.clone()\n        self.cache.current_seq_len = 0\n\n        self.model.forward(self.sequence[:, :-1], self.cache, preprocess_only = True, lora = self.lora, input_mask = mask)\n\n\n    def gen_begin_empty(self):\n\n        self.end_beam_search()\n        self.sequence = None\n        self.sequence_actual = None\n        self.cache.current_seq_len = 0\n\n\n    def gen_begin_reuse(self, in_tokens, mask = None):\n\n        self.end_beam_search()\n        if self.sequence is None or self.cache.current_seq_len == 0:\n            self.gen_begin(in_tokens, mask = mask)\n            return 0\n\n        # if in_tokens.shape[-1] < self.sequence.shape[-1]:\n        #     self.sequence = self.sequence[:, :in_tokens.shape[-1]]\n\n        reuse = 0\n        while reuse < self.sequence.shape[-1] and reuse < in_tokens.shape[-1] and self.sequence[0, reuse] == in_tokens[0, reuse]:\n            reuse += 1\n\n        if reuse < 2:\n            self.gen_begin(in_tokens, mask = mask)\n            return 0\n\n        # print (f\"Reusing cache: {reuse} tokens\")\n\n        self.cache.current_seq_len = reuse - 1\n        self.sequence = self.sequence[:, :reuse]\n        self.sequence_actual = self.sequence.clone()\n\n        if reuse < in_tokens.shape[-1]: self.gen_feed_tokens(in_tokens[:, reuse:], mask = mask)\n        return reuse\n\n\n    def gen_feed_tokens(self, in_tokens, mask = None):\n\n        if self.sequence is None:\n            self.gen_begin(in_tokens, mask = mask)\n            return\n\n        self.end_beam_search()\n\n        start = self.sequence.shape[-1] - 1\n        if start < 0:\n            start = 0\n            self.sequence = in_tokens.clone()\n        else:\n            self.sequence = torch.cat((self.sequence, in_tokens), dim = 1)\n\n        if start < self.sequence.shape[-1] - 1:\n            self.model.forward(self.sequence[:, start : -1], self.cache, preprocess_only = True, lora = self.lora, input_mask = mask)\n\n        self.sequence_actual = self.sequence\n\n\n    def gen_accept_token(self, token):\n\n        self.end_beam_search()\n        if self.sequence is None: self.sequence = token\n        else: self.sequence = torch.cat((self.sequence, token), dim = 1)\n        self.sequence_actual = self.sequence\n\n\n    def gen_rewind(self, num_tokens):\n\n        if num_tokens == 0: return\n        self.end_beam_search()\n        self.sequence = self.sequence[:, :-num_tokens]\n        self.cache.current_seq_len -= num_tokens\n        self.sequence_actual = self.sequence\n\n\n    def gen_prune_right(self, tokens, mask = None):\n\n        self.end_beam_search()\n        if tokens > self.sequence.shape[-1] - 1: return\n        self.gen_begin(self.sequence[:, tokens:], mask = mask)\n        self.sequence_actual = self.sequence\n\n\n    def gen_prune_to(self, min_tokens_to_keep, token_id, mask = None):\n\n        self.end_beam_search()\n\n        if self.gen_num_tokens() <= min_tokens_to_keep: return\n\n        while self.gen_num_tokens() > min_tokens_to_keep:\n\n            pruned = False\n            for i in range(self.sequence.shape[-1] - 1):\n                if self.sequence[0, i] == token_id:\n                    self.sequence = self.sequence[:, i + 1:]\n                    pruned = True\n                    break\n\n            if not pruned: return\n\n        self.gen_begin(self.sequence, mask = mask)\n\n\n    def gen_prune_left(self, num_tokens, mask = None):\n\n        num_tokens = min(num_tokens, self.sequence_actual.shape[-1] - 1)\n\n        if self.in_beam_search:\n            self.end_beam_search()  # TODO: Try to avoid restarting beam search when generating past chunk boundary\n            self.sequence = self.sequence[:, num_tokens:]\n            self.begin_beam_search()\n        else:\n            self.sequence = self.sequence[:, num_tokens:]\n            self.gen_begin(self.sequence, mask = mask)\n\n\n    def gen_num_tokens(self):\n\n        return self.sequence_actual.shape[-1]\n\n\n    # Simple generator function\n\n    def generate_simple(self, prompt, max_new_tokens = 128):\n\n        self.end_beam_search()\n\n        ids, mask = self.tokenizer.encode(prompt, return_mask = True, max_seq_len = self.model.config.max_seq_len)\n        self.gen_begin(ids, mask = mask)\n\n        max_new_tokens = min(max_new_tokens, self.model.config.max_seq_len - ids.shape[1])\n\n        eos = torch.zeros((ids.shape[0],), dtype = torch.bool)\n        for i in range(max_new_tokens):\n            token = self.gen_single_token(mask = mask)\n            for j in range(token.shape[0]):\n                if token[j, 0].item() == self.tokenizer.eos_token_id: eos[j] = True\n            if eos.all(): break\n\n        text = self.tokenizer.decode(self.sequence[0] if self.sequence.shape[0] == 1 else self.sequence)\n        return text\n\n\n    # Apply repetition penalty with current  settings\n\n    def apply_rep_penalty(self, logits):\n\n        cuda_ext.ext_apply_rep_penalty_mask_cpu(self.sequence,\n                                                self.settings.token_repetition_penalty_max,\n                                                self.settings.token_repetition_penalty_sustain,\n                                                self.settings.token_repetition_penalty_decay,\n                                                logits)\n\n\n    # Generate a single token with the current settings, append to sequence\n\n    def gen_single_token(self, constraints = None, mask = None):\n\n        self.end_beam_search()\n\n        # Simple sampling case:\n\n        if self.sequence is not None:\n\n            logits = self.model.forward(self.sequence[:, -1:], self.cache, lora = self.lora, input_mask = mask)\n            self.apply_rep_penalty(logits)\n\n            logits[:, :, self.tokenizer.bos_token_id] = -10000.0\n\n            if constraints is not None:\n\n                for c in constraints: logits[:, :, c] += 10000.0\n                logits[:, :, :] -= 10000.0\n\n            token, _ = self.batched_sample(logits,\n                                           self.settings.temperature,\n                                           self.settings.top_k,\n                                           self.settings.top_p,\n                                           self.settings.min_p + 0.01 if constraints is not None else 0.0,\n                                           self.settings.typical)\n\n        else:\n\n            # bos = torch.Tensor([[self.tokenizer.bos_token_id]]).long()\n            # logits = self.model.forward(bos, self.cache)\n            # self.cache.current_seq_len = 0\n\n            if constraints is not None:\n                token = constraints[0]\n            else:\n                token = torch.Tensor([[self.tokenizer.bos_token_id]]).long()\n\n        self.gen_accept_token(token)\n        return token\n\n\n    # Beam search\n\n    class Beam:\n\n        sequence: torch.Tensor                  # tokens generated in beam\n        probs: torch.Tensor                     # probability score per token\n        cache: ExLlamaCache                     # cached keys/values for this beam\n        current_seq_pos: int                    # position of beam in current sequence\n        settings = None\n        generator = None\n\n        sampled_tokens: torch.Tensor\n        sampled_probs: torch.Tensor\n        moved: bool = False\n\n        def __init__(self, settings, generator, first_token = None, first_prob = None, seq_pos = None):\n\n            self.settings = settings\n            self.generator = generator\n\n            self.sequence = first_token.unsqueeze(0).unsqueeze(0) if first_token is not None else None\n            self.probs = first_prob.unsqueeze(0).unsqueeze(0) if first_prob is not None else None\n            self.cache = ExLlamaCache(self.generator.model, max_seq_len = self.settings.beam_length)\n\n            self.current_seq_pos = seq_pos\n\n\n        def __len__(self):\n\n            return self.sequence.shape[-1]\n\n\n        def clone(self):\n\n            new = ExLlamaGenerator.Beam(self.settings, self.generator)\n            new.sequence = self.sequence.clone()\n            new.probs = self.probs.clone()\n            new.cache = self.cache.clone()\n            new.current_seq_pos = self.current_seq_pos\n            new.sampled_tokens = self.sampled_tokens.clone()\n            new.sampled_probs = self.sampled_probs.clone()\n            new.moved = self.moved\n            return new\n\n\n        # List of references to this instance\n\n        def advance(self):\n\n            self.cache.roll_left()\n            self.sequence = self.sequence[:, 1:]\n            self.probs = self.probs[:, 1:]\n            self.current_seq_pos += 1\n\n\n        # Cumulative probabilities\n\n        def cum_log_probs(self):\n\n            cum_log_prob = torch.sum(torch.log(self.probs))\n            return cum_log_prob\n\n        def sampled_cum_log_probs(self):\n\n            cum_log_prob = torch.sum(torch.log(self.probs))\n            return torch.log(self.sampled_probs) + cum_log_prob\n\n\n        # Insert current beam in sequence\n\n        def to_sequence(self):\n\n            # Extend generator sequence and cache if needed\n\n            new_tokens = 0\n            added_tokens = 0\n            slen = self.generator.sequence.shape[-1]\n            tlen = self.current_seq_pos + len(self)\n            if tlen > slen:\n                new_tokens = tlen - slen\n                added_tokens = new_tokens\n                self.generator.sequence = torch.cat((self.generator.sequence, self.sequence[:, -new_tokens:]), dim = 1)\n                self.generator.cache.current_seq_len = tlen - 1\n\n            # Determine how much of generator sequence needs to be updated\n\n            new_tokens_ = new_tokens\n            for i in range(new_tokens_, len(self)):\n                if self.generator.sequence[0, -i - 1] != self.sequence[0, -i - 1]: new_tokens = i + 1\n\n            # Update sequence and cache\n\n            if new_tokens > added_tokens:\n                self.generator.sequence[0, -new_tokens:] = self.sequence[0, -new_tokens:]\n\n            if new_tokens > len(self) - 1: new_tokens = len(self) - 1\n            if new_tokens > 0:\n                self.cache.copy_states(self.generator.cache,\n                                       len(self) - 1 - new_tokens, new_tokens,\n                                       self.generator.cache.current_seq_len - new_tokens, new_tokens,\n                                       0, 1, 0, 1)\n\n\n        # Copy last column of cache to this beam (after generation)\n\n        def record_last_cache_column(self):\n\n            self.generator.cache.copy_states(self.cache,\n                                             self.generator.cache.current_seq_len - 1, 1,\n                                             len(self) - 1, 1,\n                                             0, 1, 0, 1)\n\n\n    def begin_beam_search(self):\n\n        self.beams = None\n        if self.settings.beams == 1 and self.settings.beam_length == 1: return\n\n        self.in_beam_search = True\n        # self.testl = []\n\n\n    def beam_search(self):\n\n        if self.settings.beams == 1 and self.settings.beam_length == 1: return self.gen_single_token()\n        assert self.in_beam_search\n\n        # Kludge: The first token returned with an empty context is generated without beam search\n        if self.sequence is None: return self.gen_single_token()\n\n        c_cache_len = self.cache.current_seq_len\n        c_seq_len = self.sequence_actual.shape[-1]\n\n        # Begin here\n\n        max_beam_length = min(self.model.config.max_seq_len - self.settings.beam_length, self.settings.beam_length)\n        while self.beams is None or len(self.beams[0]) < max_beam_length:\n\n            if self.beams is None:\n\n                # Initial tokens for initial beams\n\n                # self.cache.debug()\n                logits = self.model.forward(self.sequence[:, -1:], self.cache, lora = self.lora)\n\n                cuda_ext.ext_apply_rep_penalty_mask_cpu(self.sequence,\n                                                        self.settings.token_repetition_penalty_max,\n                                                        self.settings.token_repetition_penalty_sustain,\n                                                        self.settings.token_repetition_penalty_decay,\n                                                        logits)\n\n                tokens, probs = self.sample(logits,\n                                            self.settings.temperature,\n                                            self.settings.top_k,\n                                            self.settings.top_p,\n                                            self.settings.min_p,\n                                            self.settings.typical,\n                                            num = self.settings.beams)\n\n                # self.cache is updated with k/v for last token\n                # Setup initial beams\n\n                self.beams = []\n                while len(self.beams) < min(self.settings.beams, tokens.shape[-1]):\n\n                    beam = ExLlamaGenerator.Beam(self.settings, self, tokens[0, len(self.beams)], probs[0, len(self.beams)], c_seq_len)\n                    self.beams.append(beam)\n\n            else:\n\n                # Sample from each beam\n\n                # print(len(self.beams), end = \"\")\n                for beam in self.beams:\n\n                    beam.to_sequence()\n\n                    # self.cache.debug()\n                    logits = self.model.forward(self.sequence[:, -1:], self.cache, lora = self.lora)\n\n                    cuda_ext.ext_apply_rep_penalty_mask_cpu(self.sequence,\n                                                            self.settings.token_repetition_penalty_max,\n                                                            self.settings.token_repetition_penalty_sustain,\n                                                            self.settings.token_repetition_penalty_decay,\n                                                            logits)\n\n                    tokens, probs = self.sample(logits,\n                                                self.settings.temperature,\n                                                self.settings.top_k,\n                                                self.settings.top_p,\n                                                self.settings.min_p,\n                                                self.settings.typical,\n                                                num = -1)\n\n                    beam.sampled_tokens = tokens\n                    beam.sampled_probs = probs\n\n                    beam.record_last_cache_column()\n                    self.cache.current_seq_len -= 1\n\n                # Collect options for all beams\n\n                tokens_ = []\n                probs_ = []\n                cum_log_probs_ = []\n                beams_ = []\n                for i, beam in enumerate(self.beams):\n                    tokens_.append(beam.sampled_tokens.squeeze(0))\n                    probs_.append(beam.sampled_probs.squeeze(0))\n                    cum_log_probs_.append(beam.sampled_cum_log_probs().squeeze(0))\n                    beams_.append(torch.Tensor([i] * beam.sampled_tokens.shape[-1]).to(torch.int))\n\n                tokens_all = torch.cat(tokens_, dim = 0)\n                probs_all = torch.cat(probs_, dim = 0)\n                cum_log_probs_all = torch.cat(cum_log_probs_, dim = 0)\n                beams_all = torch.cat(beams_, dim = 0)\n\n                # Sort by cumulative probability\n\n                cum_log_probs_all, ind = cum_log_probs_all.sort(descending = True)\n                probs_all = probs_all[ind]\n                tokens_all = tokens_all[ind]\n                beams_all = beams_all[ind]\n\n                # Reduce to beam limit\n\n                cum_log_probs_all = cum_log_probs_all[:self.settings.beams]\n                probs_all = probs_all[:self.settings.beams]\n                tokens_all = tokens_all[:self.settings.beams]\n                beams_all = beams_all[:self.settings.beams]\n\n                # Re-sort by beam index\n\n                beams_all, ind = beams_all.sort()\n                cum_log_probs_all = cum_log_probs_all[ind]\n                tokens_all = tokens_all[ind]\n                probs_all = probs_all[ind]\n\n                # test = [self.tokenizer.decode(beam.sequence) for beam in self.beams]\n\n                # Rebuild beams/caches\n\n                for beam in self.beams: beam.moved = False\n                beams_new = []\n\n                for i in range(len(beams_all)):\n\n                    new_token = tokens_all[i]\n                    new_prob = probs_all[i]\n                    beam_idx = beams_all[i].item()\n\n                    if not self.beams[beam_idx].moved:\n\n                        self.beams[beam_idx].sequence = torch.cat((self.beams[beam_idx].sequence, new_token.unsqueeze(0).unsqueeze(0)), dim = 1)\n                        self.beams[beam_idx].probs = torch.cat((self.beams[beam_idx].probs, new_prob.unsqueeze(0).unsqueeze(0)), dim = 1)\n                        self.beams[beam_idx].moved = True\n                        beams_new.append(self.beams[beam_idx])\n\n                    else:\n\n                        nbeam = self.beams[beam_idx].clone()\n                        nbeam.sequence[:, -1] = new_token\n                        nbeam.probs[:, -1] = new_prob\n                        beams_new.append(nbeam)\n\n                self.beams = beams_new\n\n\n        # Beam length is filled up, select winning beam\n\n        max_log_probs = float(\"-inf\")\n        best_beam = None\n        best_beam_idx = -1\n        for beam_idx, beam in enumerate(self.beams):\n            beam_log_probs = beam.cum_log_probs()\n            if beam_log_probs > max_log_probs:\n                max_log_probs = beam_log_probs\n                best_beam = beam\n                best_beam_idx = beam_idx\n\n        best_token = best_beam.sequence[:, 0]\n\n        # Insert in sequence\n\n        self.sequence[0, c_seq_len] = best_token\n        self.sequence_actual = torch.cat((self.sequence_actual, best_token.unsqueeze(0)), dim = 1)\n\n        # Copy cache state for winning beam\n\n        best_beam.to_sequence()\n\n        # Prune other beams that don't begin with the winning token\n\n        beams_new = [best_beam]\n\n        for idx, beam in enumerate(self.beams):\n            if idx != best_beam_idx and beam.sequence[:, 0] == best_token:\n                beams_new.append(beam)\n\n        self.beams = beams_new\n\n        # Advance all remaining beams and caches\n\n        for beam in self.beams: beam.advance()\n\n        # Done\n\n        return best_token\n\n\n    def end_beam_search(self):\n\n        if not self.in_beam_search: return\n\n        self.sequence = self.sequence_actual.clone()\n        self.cache.current_seq_len = self.sequence.shape[-1] - 1\n        self.in_beam_search = False\n\n\n    def replace_last_token(self, token, seq = False):\n\n        self.sequence_actual[:, -1] = token\n        if seq: self.sequence[:, -1] = token\n\n\n    def sequence_ends_with(self, tokens):\n\n        if self.sequence_actual.shape[-1] < tokens.shape[-1] + 1: return False\n        for i in range(tokens.shape[-1]):\n            if self.sequence_actual[0, -i - 1] != tokens[0, -i - 1]: return False\n        return True", "\n"]}
{"filename": "example_lora.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nfrom lora import ExLlamaLora\nimport os, glob\nimport torch\n\n# Directory containt model, tokenizer, generator\n\nmodel_directory = \"/mnt/str/models/_test_models/Neko-Institute-of-Science_LLaMA-7B-4bit-128g/\"", "\nmodel_directory = \"/mnt/str/models/_test_models/Neko-Institute-of-Science_LLaMA-7B-4bit-128g/\"\n\n# Directory containing LoRA config and weights\n\nlora_directory = \"/mnt/str/models/_test_loras/tloen_alpaca-lora-7b/\"\n\n# Locate files we need within those directories\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")", "\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\n\nlora_config_path = os.path.join(lora_directory, \"adapter_config.json\")\nlora_path = os.path.join(lora_directory, \"adapter_model.bin\")\n\n# Create config, model, tokenizer and generator", "\n# Create config, model, tokenizer and generator\n\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n\ncache = ExLlamaCache(model)                             # create cache for inference", "\ncache = ExLlamaCache(model)                             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n\n# Load LoRA\n\nlora = ExLlamaLora(model, lora_config_path, lora_path)\n\n# Configure generator\n", "# Configure generator\n\ngenerator.settings.token_repetition_penalty_max = 1.2\ngenerator.settings.temperature = 0.65\ngenerator.settings.top_p = 0.4\ngenerator.settings.top_k = 0\ngenerator.settings.typical = 0.0\n\n# Alpaca prompt\n", "# Alpaca prompt\n\nprompt = \\\n    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\" \\\n    \"\\n\" \\\n    \"### Instruction:\\n\" \\\n    \"List five colors in alphabetical order.\\n\" \\\n    \"\\n\" \\\n    \"### Response:\"\n", "    \"### Response:\"\n\n# Generate with LoRA\n\nprint(\" --- LoRA ----------------- \")\nprint(\"\")\n\ngenerator.lora = lora\ntorch.manual_seed(1337)\noutput = generator.generate_simple(prompt, max_new_tokens = 200)", "torch.manual_seed(1337)\noutput = generator.generate_simple(prompt, max_new_tokens = 200)\nprint(output)\n\n# Generate without LoRA\n\nprint(\"\")\nprint(\" --- No LoRA -------------- \")\nprint(\"\")\n", "print(\"\")\n\ngenerator.lora = None\ntorch.manual_seed(1337)\noutput = generator.generate_simple(prompt, max_new_tokens = 200)\nprint(output)\n\n"]}
{"filename": "example_batch.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport os, glob\n\n# Directory containing model, tokenizer, generator\n\nmodel_directory =  \"/mnt/str/models/llama-13b-4bit-128g/\"\n\n# Locate files we need within that directory", "\n# Locate files we need within that directory\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\n\n# Batched prompts\n", "# Batched prompts\n\nprompts = [\n    \"Once upon a time,\",\n    \"I don't like to\",\n    \"A turbo encabulator is a\",\n    \"In the words of Mark Twain,\"\n]\n\n# Create config, model, tokenizer and generator", "\n# Create config, model, tokenizer and generator\n\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n\ncache = ExLlamaCache(model, batch_size = len(prompts))  # create cache for inference", "\ncache = ExLlamaCache(model, batch_size = len(prompts))  # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n\n# Configure generator\n\ngenerator.disallow_tokens([tokenizer.eos_token_id])\n\ngenerator.settings.token_repetition_penalty_max = 1.2\ngenerator.settings.temperature = 0.95", "generator.settings.token_repetition_penalty_max = 1.2\ngenerator.settings.temperature = 0.95\ngenerator.settings.top_p = 0.65\ngenerator.settings.top_k = 100\ngenerator.settings.typical = 0.5\n\n# Generate, batched\n\nfor line in prompts:\n    print(line)", "for line in prompts:\n    print(line)\n\noutput = generator.generate_simple(prompts, max_new_tokens = 200)\n\nfor line in output:\n    print(\"---\")\n    print(line)\n", ""]}
{"filename": "example_ws.py", "chunked_list": ["import asyncio\nimport websockets\nimport json\nfrom sentencepiece import SentencePieceProcessor\n\nfrom model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom lora import ExLlamaLora\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport argparse", "from generator import ExLlamaGenerator\nimport argparse\nimport torch\nimport sys\nimport os\nimport glob\nimport model_init\n\n# Initialized from command line args by init()\n", "# Initialized from command line args by init()\n\nmodel: ExLlama\ncache: ExLlamaCache\nconfig: ExLlamaConfig\ngenerator: ExLlamaGenerator\ntokenizer: ExLlamaTokenizer\nmax_cached_strings = 100\ntokenizer_cache = {}\n", "tokenizer_cache = {}\n\n\nprompt_ids: torch.tensor\nstop_strings: list\nstop_tokens: list\nheld_text: str\nmax_stop_string: int\nremaining_tokens: int\n", "remaining_tokens: int\n\nfull_prompt: str\nutilized_prompt: str\nbuilt_response: str\n\ndef cached_tokenize(text: str):\n    global model, cache, config, generator, tokenizer\n    global max_cached_strings, tokenizer_cache\n\n    if text in tokenizer_cache:\n        return tokenizer_cache[text]\n\n    while len(tokenizer_cache) >= max_cached_strings:\n        del tokenizer_cache[next(iter(tokenizer_cache))]  # Always removes oldest entry as of Python 3.7\n\n    new_enc = tokenizer.encode(text)\n    tokenizer_cache[text] = new_enc\n    return new_enc", "\ndef begin_stream(prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: ExLlamaGenerator.Settings):\n    global model, cache, config, generator, tokenizer\n    global stop_strings, stop_tokens, prompt_ids, held_text, max_stop_string, remaining_tokens\n    global full_prompt, utilized_prompt, built_response\n\n    # Tokenize prompt and limit length to allow prompt and (max) new tokens within max sequence length\n\n    max_input_tokens = model.config.max_seq_len - max_new_tokens\n    input_ids = cached_tokenize(prompt)\n    input_ids = input_ids[:, -max_input_tokens:]\n    prompt_ids = input_ids\n\n    full_prompt = prompt\n    utilized_prompt = tokenizer.decode(prompt_ids)[0]\n    built_response = \"\"\n\n    remaining_tokens = max_new_tokens\n\n    # Settings\n\n    stop_strings = []\n    stop_tokens = []\n    for t in stop_conditions:\n        if isinstance(t, int): stop_tokens += [t]\n        if isinstance(t, str): stop_strings += [t]\n\n    held_text = \"\"\n\n    max_stop_string = 2\n    for ss in stop_strings:\n        max_stop_string = max(max_stop_string, get_num_tokens(ss) + 2)\n\n    generator.settings = gen_settings\n\n    # Start generation\n\n    generator.gen_begin_reuse(input_ids)", "\ndef stream():\n    global model, cache, config, generator, tokenizer\n    global stop_strings, stop_tokens, prompt_ids, held_text, max_stop_string, remaining_tokens\n    global full_prompt, utilized_prompt, built_response\n\n    # Check total response length\n\n    if remaining_tokens == 0:\n        return held_text, True, full_prompt + built_response, utilized_prompt + built_response, built_response\n    remaining_tokens -= 1\n\n    # Generate\n\n    old_tail = tokenizer.decode(generator.sequence_actual[:, -max_stop_string:])[0]\n    next_token = generator.gen_single_token()\n\n    # End on stop token\n\n    if next_token in stop_tokens:\n        return held_text, True, full_prompt + built_response, utilized_prompt + built_response, built_response\n\n    # Get new text\n\n    new_tail = tokenizer.decode(generator.sequence_actual[:, -(max_stop_string + 1):])[0]\n    added_text = new_tail[len(old_tail):]\n    held_text += added_text\n\n    # Hold text if it's part of a stop condition, end if it's a full stop condition\n\n    partial_ss = False\n    for ss in stop_strings:\n\n        # Check if held_text fully contains stop string\n\n        position = held_text.find(ss)\n        if position != -1:\n            built_response += held_text[:position]\n            return held_text[:position], True, full_prompt + built_response, utilized_prompt + built_response, built_response\n\n        # Check if end of held_text overlaps with start of stop string\n\n        overlap = 0\n        for j in range(1, min(len(held_text), len(ss)) + 1):\n            if held_text[-j:] == ss[:j]: overlap = j\n        if overlap > 0: partial_ss = True\n\n    # Return partial result\n\n    if partial_ss:\n        return \"\", False, full_prompt + built_response, utilized_prompt + built_response, built_response\n\n    stream_text = held_text\n    held_text = \"\"\n    built_response += stream_text\n    return stream_text, False, full_prompt, utilized_prompt, built_response", "\ndef leftTrimTokens(text: str, desiredLen: int):\n\n    encodedText = tokenizer.encode(text)\n    if encodedText.shape[-1] <= desiredLen:\n        return text\n    else:\n        return tokenizer.decode(encodedText[:, -desiredLen:])[0]\n\ndef oneshot_generation(prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: ExLlamaGenerator.Settings):\n\n    begin_stream(prompt, stop_conditions, max_new_tokens, gen_settings)\n    response = \"\"\n    while True:\n        _, eos, _, _, _ = stream()\n        if eos: break\n\n    return full_prompt + built_response, utilized_prompt + built_response, built_response", "\ndef oneshot_generation(prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: ExLlamaGenerator.Settings):\n\n    begin_stream(prompt, stop_conditions, max_new_tokens, gen_settings)\n    response = \"\"\n    while True:\n        _, eos, _, _, _ = stream()\n        if eos: break\n\n    return full_prompt + built_response, utilized_prompt + built_response, built_response", "\n\ndef get_num_tokens(text: str):\n\n    return cached_tokenize(text).shape[-1]\n\n\n\n\n# Websocket server", "\n# Websocket server\nasync def estimateToken(request, ws):\n    text = request[\"text\"]\n    numTokens=get_num_tokens(text)\n    return numTokens# return number of tokens in int\n\nasync def oneShotInfer(request, ws):\n    stopToken = request[\"stopToken\"]\n    fullContext = request[\"text\"]", "    stopToken = request[\"stopToken\"]\n    fullContext = request[\"text\"]\n    maxNew = int(request[\"maxNew\"])\n    top_p = float(request[\"top_p\"])\n    top_k = int(request[\"top_k\"])\n    temp = float(request[\"temp\"])\n    rep_pen = float(request[\"rep_pen\"])\n    sc = [tokenizer.eos_token_id]\n    sc.append(stopToken)\n", "    sc.append(stopToken)\n\n    gs = ExLlamaGenerator.Settings()\n    gs.top_k = top_k\n    gs.top_p = top_p\n    gs.temperature = temp\n    gs.token_repetition_penalty_max = rep_pen\n\n    full_ctx, util_ctx, response = oneshot_generation(prompt=fullContext, stop_conditions=sc, max_new_tokens=maxNew, gen_settings=gs)\n", "    full_ctx, util_ctx, response = oneshot_generation(prompt=fullContext, stop_conditions=sc, max_new_tokens=maxNew, gen_settings=gs)\n\n    return full_ctx, util_ctx, response# return requested prompt/context, pruned prompt/context(eg. prunedctx+maxNew=4096), model generated response, not including prompt\n\nasync def streamInfer(request, ws):\n    stopToken = [tokenizer.eos_token_id]\n    stopToken.append(request[\"stopToken\"])\n    prompt = request[\"text\"]\n    maxNew = int(request[\"maxNew\"])\n    top_p = float(request[\"top_p\"])", "    maxNew = int(request[\"maxNew\"])\n    top_p = float(request[\"top_p\"])\n    top_k = int(request[\"top_k\"])\n    temp = float(request[\"temp\"])\n    rep_pen = float(request[\"rep_pen\"])\n    gs = ExLlamaGenerator.Settings()\n    gs.top_k = top_k\n    gs.top_p = top_p\n    gs.temperature = temp\n    gs.token_repetition_penalty_max = rep_pen", "    gs.temperature = temp\n    gs.token_repetition_penalty_max = rep_pen\n    begin_stream(prompt, stopToken, maxNew, gs)\n    while True:\n        chunk, eos, x, y, builtResp = stream()\n        await ws.send(json.dumps({'action':request[\"action\"],\n                                  'request_id':request['request_id'],\n                                  'utilContext':utilized_prompt + builtResp, \n                                  'response':builtResp}))\n        if eos: break", "                                  'response':builtResp}))\n        if eos: break\n    return utilized_prompt + built_response,builtResp\n\n\nasync def main(websocket, path):\n    async for message in websocket:\n        #try:\n            request = json.loads(message)\n            reqID = request[\"request_id\"]", "            request = json.loads(message)\n            reqID = request[\"request_id\"]\n            action = request[\"action\"]\n\n            if action == \"estimateToken\":\n                response = await estimateToken(request, websocket)\n                await websocket.send(json.dumps({'action':action, 'request_id':reqID, 'response':response}))\n\n            elif action == \"echo\":\n                await websocket.send(json.dumps({'action':action, 'request_id':reqID}))\n\n            elif action == \"oneShotInfer\":\n                fctx, utlctx, res = await oneShotInfer(request, websocket)\n                await websocket.send(json.dumps({'action':action, 'request_id':reqID,'utilContext':utlctx, 'response':res}))\n            \n            elif action == \"leftTrim\":\n                prompt = request[\"text\"]\n                desiredLen = int(request[\"desiredLen\"])\n                processedPrompt = leftTrimTokens(prompt, desiredLen)\n                await websocket.send(json.dumps({'action':action, 'request_id':reqID, 'response':processedPrompt}))\n\n            else:\n                utlctx, builtResp= await streamInfer(request, websocket)\n                await websocket.send(json.dumps({'action':action, 'request_id':reqID,'utilContext':utlctx, 'response':builtResp+'</s>'}))", "\n\n\n        #except Exception as e:\n            #print({\"error\": str(e)})\n\nmodel_directory = \"./models/Llama-2-70B-chat-GPTQ/\"\n\ntokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")", "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\nmodel_config_path = os.path.join(model_directory, \"config.json\")\nst_pattern = os.path.join(model_directory, \"*.safetensors\")\nmodel_path = glob.glob(st_pattern)[0]\nesTokenizer = SentencePieceProcessor(model_file = tokenizer_path)\nconfig = ExLlamaConfig(model_config_path)               # create config from config.json\nconfig.set_auto_map('17.615,18.8897')\nconfig.model_path = model_path                          # supply path to model weights file\n\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights", "\nmodel = ExLlama(config)                                 # create ExLlama instance and load the weights\nprint(f\"Model loaded: {model_path}\")\n\ntokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\ncache = ExLlamaCache(model)                             # create cache for inference\ngenerator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\nstart_server = websockets.serve(main, \"0.0.0.0\", 8080)\n\nasyncio.get_event_loop().run_until_complete(start_server)", "\nasyncio.get_event_loop().run_until_complete(start_server)\nasyncio.get_event_loop().run_forever()\n"]}
{"filename": "alt_generator.py", "chunked_list": ["import cuda_ext\nfrom model import ExLlama, ExLlamaCache\nfrom tokenizer import ExLlamaTokenizer\nfrom lora import ExLlamaLora\nimport torch\nimport torch.nn.functional as F\n\nMAX_CACHED_STRINGS = 100\n\nclass ExLlamaAltGenerator:\n\n    class Settings:\n\n        temperature = 0.95\n        top_k = 40                              # consider the most probable top_k samples, 0 to disable top_k sampling\n        top_p = 0.65                            # consider tokens up to a cumulative probabiltiy of top_p, 0.0 to disable top_p sampling\n        min_p = 0.0                             # Do not consider tokens with probability less than this\n        typical = 0.0                           # Locally typical sampling threshold, 0.0 to disable typical sampling\n\n        token_repetition_penalty_max = 1.15     # Repetition penalty for most recent tokens\n        token_repetition_penalty_sustain = -1   # No. most recent tokens to repeat penalty for, -1 to apply to whole context\n        token_repetition_penalty_decay = 0      # Gradually decrease penalty over this many tokens\n\n        disallowed_tokens: list[int] = None     # List of tokens to inhibit, e.g. tokenizer.eos_token_id\n        lora: ExLlamaLora = None                # LoRA to apply when generating\n\n    # Internal state\n\n    model: ExLlama\n    cache: ExLlamaCache\n    tokenizer: ExLlamaTokenizer\n    tokenizer_cache = {}\n\n    settings: Settings\n    stop_strings: list = []\n    stop_tokens: list = []\n    held_text: str = \"\"\n    max_stop_tokens: int = 2\n    sequence_ids: torch.Tensor = None\n    sequence_str: str = None\n    remaining_tokens: int = 0\n\n\n    def __init__(self, model: ExLlama, tokenizer: ExLlamaTokenizer, cache: ExLlamaCache):\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.cache = cache\n        self.settings = ExLlamaAltGenerator.Settings()\n\n\n    def cached_tokenize(self, text: str, encode_special_characters = False):\n\n        if text in self.tokenizer_cache:\n            return self.tokenizer_cache[text]\n\n        while len(self.tokenizer_cache) >= MAX_CACHED_STRINGS:\n            del self.tokenizer_cache[next(iter(self.tokenizer_cache))]  # Always removes oldest entry, as of Python 3.7\n\n        new_enc = self.tokenizer.encode(text, encode_special_characters = encode_special_characters)\n        self.tokenizer_cache[text] = new_enc\n        return new_enc\n\n\n    def get_num_tokens(self, text: str, encode_special_characters = False):\n\n        return self.cached_tokenize(text, encode_special_characters = encode_special_characters).shape[-1]\n\n\n    # Begin generating\n    #\n    # prompt: The input prompt. Will be tokenized and then truncated to the models max sequence length minus max_new_tokens\n    # stop_conditions: List of strings or integer token IDs that will end the sequence\n    # settings: ExLlamaAltGeneratorSettings\n    # encode_special_characters: Set to true to tokenize \"</s>\" etc.\n\n    def begin_stream(self, prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: Settings, encode_special_characters = False):\n\n        assert isinstance(prompt, str), \"ExLlamaAltGenerator does not support batched generation\"\n\n        # Tokenize prompt and limit length to allow prompt and (max) new tokens within max sequence length\n\n        max_input_tokens = self.model.config.max_seq_len - max_new_tokens\n        self.remaining_tokens = max_new_tokens\n\n        input_ids = self.cached_tokenize(prompt, encode_special_characters)\n        applied_input_ids = input_ids[:, -max_input_tokens:]\n        self.sequence_str = self.tokenizer.decode(applied_input_ids)[0] if applied_input_ids.shape[0] < input_ids.shape[0] else prompt\n\n        # Settings\n\n        self.stop_strings = []\n        self.stop_tokens = []\n        for t in stop_conditions:\n            if isinstance(t, int): self.stop_tokens += [t]\n            elif isinstance(t, str): self.stop_strings += [t]\n            else: raise ValueError(\"Unsupported type in stop_conditions\")\n\n        self.held_text = \"\"\n\n        self.max_stop_tokens = 2\n        for ss in self.stop_strings:\n            self.max_stop_tokens = max(self.max_stop_tokens, self.get_num_tokens(ss) + 2)\n\n        self.settings = gen_settings\n\n        # Start generation\n\n        self.gen_begin_reuse(applied_input_ids, gen_settings)\n\n\n    # Get the next chunk of text in the stream\n    #\n    # Returns stream_chunk: str, EOS: bool\n\n    def stream(self):\n\n        # Check total response length\n\n        if self.remaining_tokens == 0:\n            self.sequence_str += self.held_text\n            return self.held_text, True\n\n        self.remaining_tokens -= 1\n\n        # Decode the current tail end of the sequence\n\n        old_tail = self.tokenizer.decode(self.sequence_ids[:, -self.max_stop_tokens:])[0]\n\n        # Generate a single token and append to the sequence\n\n        next_token = self.gen_single_token(self.settings)\n\n        # End immediately if it was a stop token\n\n        if next_token in self.stop_tokens:\n            self.sequence_str += self.held_text\n            return self.held_text, True\n\n        # Decode the tail end of the sequence with the added token to get (actual) characters added\n\n        new_tail = self.tokenizer.decode(self.sequence_ids[:, -(self.max_stop_tokens + 1):])[0]\n        self.held_text += new_tail[len(old_tail):]\n\n        # Hold text as long as it contains part of a stop string\n\n        partial_ss = False\n        for ss in self.stop_strings:\n\n            # Check if held_text fully contains stop string\n\n            position = self.held_text.find(ss)\n            if position != -1:\n                self.sequence_str += self.held_text[:position]\n                return self.held_text[:position], True\n\n            # Check for overlap between end of held_text and start of stop string\n\n            overlap = 0\n            for j in range(1, min(len(self.held_text), len(ss)) + 1):\n                if self.held_text[-j:] == ss[:j]: overlap = j\n            if overlap > 0: partial_ss = True\n\n        # If holding text because of a partial stop condition, return nothing but also EOS = False\n\n        if partial_ss:\n            return \"\", False\n\n        # No stop condition, so return whatever has been generated\n\n        stream_text = self.held_text\n        self.held_text = \"\"\n        self.sequence_str += stream_text\n        return stream_text, False\n\n\n    # Generate and return a full prompt completion. See begin_stream() above\n\n    def generate(self, prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: Settings, encode_special_characters = False):\n\n        self.begin_stream(prompt, stop_conditions, max_new_tokens, gen_settings, encode_special_characters)\n        response = \"\"\n        while True:\n            chunk, eos = self.stream()\n            response += chunk\n            if eos: break\n\n        return response\n\n\n    # Begin generation\n\n    def gen_begin(self, in_tokens, gen_settings):\n\n        self.sequence_ids = in_tokens.clone()\n        self.cache.current_seq_len = 0\n        self.model.forward(self.sequence_ids[:, :-1], self.cache, preprocess_only = True, lora = gen_settings.lora)\n\n\n    def gen_begin_reuse(self, in_tokens, gen_settings):\n\n        if self.sequence_ids is None or self.cache.current_seq_len == 0:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        reuse = 0\n        while reuse < self.sequence_ids.shape[-1] and reuse < in_tokens.shape[-1] and self.sequence_ids[0, reuse] == in_tokens[0, reuse]:\n            reuse += 1\n\n        if reuse < 2:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        self.cache.current_seq_len = reuse - 1\n        self.sequence_ids = in_tokens[:, :reuse]\n\n        if reuse < in_tokens.shape[-1]: self.gen_feed_tokens(in_tokens[:, reuse:], gen_settings)\n\n\n    def gen_feed_tokens(self, in_tokens, gen_settings):\n\n        if self.sequence_ids is None:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        start = self.cache.current_seq_len\n        self.sequence_ids = torch.cat((self.sequence_ids, in_tokens), dim = 1)\n\n        self.model.forward(self.sequence_ids[:, start : -1], self.cache, preprocess_only = True, lora = gen_settings.lora)\n\n\n    # Generate one token in current sequence\n\n    def gen_single_token(self, gen_settings):\n\n        # Simple sampling case:\n\n        logits = self.model.forward(self.sequence_ids[:, -1:], self.cache, lora = gen_settings.lora)\n        token, _ = self.sample(logits, gen_settings)\n        self.sequence_ids = torch.cat([self.sequence_ids, token], dim = 1)\n        return token\n\n\n    def sample(self, logits, gen_settings):\n\n        cuda_ext.ext_apply_rep_penalty_mask_cpu(self.sequence_ids,\n                                                self.settings.token_repetition_penalty_max,\n                                                self.settings.token_repetition_penalty_sustain,\n                                                self.settings.token_repetition_penalty_decay,\n                                                logits)\n\n        logits[:, :, self.tokenizer.bos_token_id] = -10000.0\n\n        if logits.dim() == 3: logits = logits[0, -1, :]\n        elif logits.dim() == 2: logits = logits[-1, :]\n        else: raise ValueError(\"Bad logits dimension\")\n\n        # Disallow tokens\n\n        if gen_settings.disallowed_tokens is not None:\n            logits[gen_settings.disallowed_tokens] = float(\"-inf\")\n\n        # Base probabilities\n\n        logits /= gen_settings.temperature\n        logits += 1e-8\n        probs = torch.softmax(logits, dim = -1)\n\n        # Top K\n\n        if gen_settings.top_k == 0:\n            top_probs, top_indices = torch.sort(probs, descending = True)\n        else:\n            top_probs, top_indices = torch.topk(probs, gen_settings.top_k)\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n\n        # Top P\n\n        if gen_settings.top_p > 0.0:\n\n            num_top_p_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_top_p_probs += 1\n                if num_top_p_probs == top_probs.shape[-1]: break\n                if top_probs[num_top_p_probs].item() < gen_settings.min_p: break\n                cum_prob += top_probs[num_top_p_probs].item()\n                if cum_prob > gen_settings.top_p: break\n\n            top_probs = top_probs[:num_top_p_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_top_p_probs]\n\n        # Locally typical sampling\n\n        if gen_settings.typical > 0.0:\n\n            epsilon = 1e-10\n            log_probs = (top_probs + epsilon).log()\n            neg_entropy = (top_probs * log_probs).sum()\n            entropy_dev = (neg_entropy - log_probs).abs()\n            _, entropy_dev_order = torch.sort(entropy_dev)\n\n            top_probs = top_probs.gather(-1, entropy_dev_order)\n            top_indices = top_indices.gather(-1, entropy_dev_order)\n\n            num_typical_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_typical_probs += 1\n                if num_typical_probs == top_probs.shape[-1]: break\n                cum_prob += top_probs[num_typical_probs].item()\n                if cum_prob > gen_settings.typical: break\n\n            top_probs = top_probs[:num_typical_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_typical_probs]\n\n        # Multinomial sampling from top_probs, kept in same order as top_indices\n\n        sampled_ind = torch.multinomial(top_probs, 1)\n        sampled_tokens = top_indices[sampled_ind]\n        sampled_probs = top_probs[sampled_ind]  # Return probs before second norm\n\n        # if sampled_tokens.shape[0] > 1:\n        #     sampled_tokens, ind = sampled_tokens.sort()\n        #     sampled_probs = sampled_probs[ind]\n\n        return sampled_tokens.unsqueeze(0), sampled_probs.unsqueeze(0)", "\nclass ExLlamaAltGenerator:\n\n    class Settings:\n\n        temperature = 0.95\n        top_k = 40                              # consider the most probable top_k samples, 0 to disable top_k sampling\n        top_p = 0.65                            # consider tokens up to a cumulative probabiltiy of top_p, 0.0 to disable top_p sampling\n        min_p = 0.0                             # Do not consider tokens with probability less than this\n        typical = 0.0                           # Locally typical sampling threshold, 0.0 to disable typical sampling\n\n        token_repetition_penalty_max = 1.15     # Repetition penalty for most recent tokens\n        token_repetition_penalty_sustain = -1   # No. most recent tokens to repeat penalty for, -1 to apply to whole context\n        token_repetition_penalty_decay = 0      # Gradually decrease penalty over this many tokens\n\n        disallowed_tokens: list[int] = None     # List of tokens to inhibit, e.g. tokenizer.eos_token_id\n        lora: ExLlamaLora = None                # LoRA to apply when generating\n\n    # Internal state\n\n    model: ExLlama\n    cache: ExLlamaCache\n    tokenizer: ExLlamaTokenizer\n    tokenizer_cache = {}\n\n    settings: Settings\n    stop_strings: list = []\n    stop_tokens: list = []\n    held_text: str = \"\"\n    max_stop_tokens: int = 2\n    sequence_ids: torch.Tensor = None\n    sequence_str: str = None\n    remaining_tokens: int = 0\n\n\n    def __init__(self, model: ExLlama, tokenizer: ExLlamaTokenizer, cache: ExLlamaCache):\n\n        self.model = model\n        self.tokenizer = tokenizer\n        self.cache = cache\n        self.settings = ExLlamaAltGenerator.Settings()\n\n\n    def cached_tokenize(self, text: str, encode_special_characters = False):\n\n        if text in self.tokenizer_cache:\n            return self.tokenizer_cache[text]\n\n        while len(self.tokenizer_cache) >= MAX_CACHED_STRINGS:\n            del self.tokenizer_cache[next(iter(self.tokenizer_cache))]  # Always removes oldest entry, as of Python 3.7\n\n        new_enc = self.tokenizer.encode(text, encode_special_characters = encode_special_characters)\n        self.tokenizer_cache[text] = new_enc\n        return new_enc\n\n\n    def get_num_tokens(self, text: str, encode_special_characters = False):\n\n        return self.cached_tokenize(text, encode_special_characters = encode_special_characters).shape[-1]\n\n\n    # Begin generating\n    #\n    # prompt: The input prompt. Will be tokenized and then truncated to the models max sequence length minus max_new_tokens\n    # stop_conditions: List of strings or integer token IDs that will end the sequence\n    # settings: ExLlamaAltGeneratorSettings\n    # encode_special_characters: Set to true to tokenize \"</s>\" etc.\n\n    def begin_stream(self, prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: Settings, encode_special_characters = False):\n\n        assert isinstance(prompt, str), \"ExLlamaAltGenerator does not support batched generation\"\n\n        # Tokenize prompt and limit length to allow prompt and (max) new tokens within max sequence length\n\n        max_input_tokens = self.model.config.max_seq_len - max_new_tokens\n        self.remaining_tokens = max_new_tokens\n\n        input_ids = self.cached_tokenize(prompt, encode_special_characters)\n        applied_input_ids = input_ids[:, -max_input_tokens:]\n        self.sequence_str = self.tokenizer.decode(applied_input_ids)[0] if applied_input_ids.shape[0] < input_ids.shape[0] else prompt\n\n        # Settings\n\n        self.stop_strings = []\n        self.stop_tokens = []\n        for t in stop_conditions:\n            if isinstance(t, int): self.stop_tokens += [t]\n            elif isinstance(t, str): self.stop_strings += [t]\n            else: raise ValueError(\"Unsupported type in stop_conditions\")\n\n        self.held_text = \"\"\n\n        self.max_stop_tokens = 2\n        for ss in self.stop_strings:\n            self.max_stop_tokens = max(self.max_stop_tokens, self.get_num_tokens(ss) + 2)\n\n        self.settings = gen_settings\n\n        # Start generation\n\n        self.gen_begin_reuse(applied_input_ids, gen_settings)\n\n\n    # Get the next chunk of text in the stream\n    #\n    # Returns stream_chunk: str, EOS: bool\n\n    def stream(self):\n\n        # Check total response length\n\n        if self.remaining_tokens == 0:\n            self.sequence_str += self.held_text\n            return self.held_text, True\n\n        self.remaining_tokens -= 1\n\n        # Decode the current tail end of the sequence\n\n        old_tail = self.tokenizer.decode(self.sequence_ids[:, -self.max_stop_tokens:])[0]\n\n        # Generate a single token and append to the sequence\n\n        next_token = self.gen_single_token(self.settings)\n\n        # End immediately if it was a stop token\n\n        if next_token in self.stop_tokens:\n            self.sequence_str += self.held_text\n            return self.held_text, True\n\n        # Decode the tail end of the sequence with the added token to get (actual) characters added\n\n        new_tail = self.tokenizer.decode(self.sequence_ids[:, -(self.max_stop_tokens + 1):])[0]\n        self.held_text += new_tail[len(old_tail):]\n\n        # Hold text as long as it contains part of a stop string\n\n        partial_ss = False\n        for ss in self.stop_strings:\n\n            # Check if held_text fully contains stop string\n\n            position = self.held_text.find(ss)\n            if position != -1:\n                self.sequence_str += self.held_text[:position]\n                return self.held_text[:position], True\n\n            # Check for overlap between end of held_text and start of stop string\n\n            overlap = 0\n            for j in range(1, min(len(self.held_text), len(ss)) + 1):\n                if self.held_text[-j:] == ss[:j]: overlap = j\n            if overlap > 0: partial_ss = True\n\n        # If holding text because of a partial stop condition, return nothing but also EOS = False\n\n        if partial_ss:\n            return \"\", False\n\n        # No stop condition, so return whatever has been generated\n\n        stream_text = self.held_text\n        self.held_text = \"\"\n        self.sequence_str += stream_text\n        return stream_text, False\n\n\n    # Generate and return a full prompt completion. See begin_stream() above\n\n    def generate(self, prompt: str, stop_conditions: list, max_new_tokens: int, gen_settings: Settings, encode_special_characters = False):\n\n        self.begin_stream(prompt, stop_conditions, max_new_tokens, gen_settings, encode_special_characters)\n        response = \"\"\n        while True:\n            chunk, eos = self.stream()\n            response += chunk\n            if eos: break\n\n        return response\n\n\n    # Begin generation\n\n    def gen_begin(self, in_tokens, gen_settings):\n\n        self.sequence_ids = in_tokens.clone()\n        self.cache.current_seq_len = 0\n        self.model.forward(self.sequence_ids[:, :-1], self.cache, preprocess_only = True, lora = gen_settings.lora)\n\n\n    def gen_begin_reuse(self, in_tokens, gen_settings):\n\n        if self.sequence_ids is None or self.cache.current_seq_len == 0:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        reuse = 0\n        while reuse < self.sequence_ids.shape[-1] and reuse < in_tokens.shape[-1] and self.sequence_ids[0, reuse] == in_tokens[0, reuse]:\n            reuse += 1\n\n        if reuse < 2:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        self.cache.current_seq_len = reuse - 1\n        self.sequence_ids = in_tokens[:, :reuse]\n\n        if reuse < in_tokens.shape[-1]: self.gen_feed_tokens(in_tokens[:, reuse:], gen_settings)\n\n\n    def gen_feed_tokens(self, in_tokens, gen_settings):\n\n        if self.sequence_ids is None:\n            self.gen_begin(in_tokens, gen_settings)\n            return\n\n        start = self.cache.current_seq_len\n        self.sequence_ids = torch.cat((self.sequence_ids, in_tokens), dim = 1)\n\n        self.model.forward(self.sequence_ids[:, start : -1], self.cache, preprocess_only = True, lora = gen_settings.lora)\n\n\n    # Generate one token in current sequence\n\n    def gen_single_token(self, gen_settings):\n\n        # Simple sampling case:\n\n        logits = self.model.forward(self.sequence_ids[:, -1:], self.cache, lora = gen_settings.lora)\n        token, _ = self.sample(logits, gen_settings)\n        self.sequence_ids = torch.cat([self.sequence_ids, token], dim = 1)\n        return token\n\n\n    def sample(self, logits, gen_settings):\n\n        cuda_ext.ext_apply_rep_penalty_mask_cpu(self.sequence_ids,\n                                                self.settings.token_repetition_penalty_max,\n                                                self.settings.token_repetition_penalty_sustain,\n                                                self.settings.token_repetition_penalty_decay,\n                                                logits)\n\n        logits[:, :, self.tokenizer.bos_token_id] = -10000.0\n\n        if logits.dim() == 3: logits = logits[0, -1, :]\n        elif logits.dim() == 2: logits = logits[-1, :]\n        else: raise ValueError(\"Bad logits dimension\")\n\n        # Disallow tokens\n\n        if gen_settings.disallowed_tokens is not None:\n            logits[gen_settings.disallowed_tokens] = float(\"-inf\")\n\n        # Base probabilities\n\n        logits /= gen_settings.temperature\n        logits += 1e-8\n        probs = torch.softmax(logits, dim = -1)\n\n        # Top K\n\n        if gen_settings.top_k == 0:\n            top_probs, top_indices = torch.sort(probs, descending = True)\n        else:\n            top_probs, top_indices = torch.topk(probs, gen_settings.top_k)\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n\n        # Top P\n\n        if gen_settings.top_p > 0.0:\n\n            num_top_p_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_top_p_probs += 1\n                if num_top_p_probs == top_probs.shape[-1]: break\n                if top_probs[num_top_p_probs].item() < gen_settings.min_p: break\n                cum_prob += top_probs[num_top_p_probs].item()\n                if cum_prob > gen_settings.top_p: break\n\n            top_probs = top_probs[:num_top_p_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_top_p_probs]\n\n        # Locally typical sampling\n\n        if gen_settings.typical > 0.0:\n\n            epsilon = 1e-10\n            log_probs = (top_probs + epsilon).log()\n            neg_entropy = (top_probs * log_probs).sum()\n            entropy_dev = (neg_entropy - log_probs).abs()\n            _, entropy_dev_order = torch.sort(entropy_dev)\n\n            top_probs = top_probs.gather(-1, entropy_dev_order)\n            top_indices = top_indices.gather(-1, entropy_dev_order)\n\n            num_typical_probs = 0\n            cum_prob = top_probs[0].item()\n            while True:\n                num_typical_probs += 1\n                if num_typical_probs == top_probs.shape[-1]: break\n                cum_prob += top_probs[num_typical_probs].item()\n                if cum_prob > gen_settings.typical: break\n\n            top_probs = top_probs[:num_typical_probs]\n            top_probs = F.normalize(top_probs, p = 1, dim = -1)\n            top_indices = top_indices[:num_typical_probs]\n\n        # Multinomial sampling from top_probs, kept in same order as top_indices\n\n        sampled_ind = torch.multinomial(top_probs, 1)\n        sampled_tokens = top_indices[sampled_ind]\n        sampled_probs = top_probs[sampled_ind]  # Return probs before second norm\n\n        # if sampled_tokens.shape[0] > 1:\n        #     sampled_tokens, ind = sampled_tokens.sort()\n        #     sampled_probs = sampled_probs[ind]\n\n        return sampled_tokens.unsqueeze(0), sampled_probs.unsqueeze(0)", ""]}
{"filename": "cuda_ext.py", "chunked_list": ["# from abc import ABC\nimport torch\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom torch.utils.cpp_extension import load\nimport os\nimport sys\nimport platform\n\nlibrary_dir = os.path.dirname(os.path.abspath(__file__))\nextension_name = \"exllama_ext\"", "library_dir = os.path.dirname(os.path.abspath(__file__))\nextension_name = \"exllama_ext\"\nverbose = False\n\n# another kludge to get things compiling in Windows\nwindows = os.name == \"nt\"\nif windows:\n    def find_msvc():\n        for msvc_dir in [a + \"\\\\Microsoft Visual Studio\\\\\" + b + \"\\\\\" + c + \"\\\\VC\\Tools\\\\MSVC\\\\\"\n            for b in [\"2022\", \"2019\", \"2017\"]\n            for a in [os.environ[\"ProgramW6432\"], os.environ[\"ProgramFiles(x86)\"]]\n            for c in [\"BuildTools\", \"Community\", \"Professional\", \"Enterprise\", \"Preview\"]\n        ]:\n            if not os.path.exists(msvc_dir):\n                continue\n            versions = sorted(os.listdir(msvc_dir), reverse=True)\n            for version in versions:\n                compiler_dir = msvc_dir + version + \"\\\\bin\\\\Hostx64\\\\x64\"\n                if os.path.exists(compiler_dir) and os.path.exists(compiler_dir + \"\\\\cl.exe\"):\n                    return compiler_dir\n        return None\n    \n    import subprocess\n    try:\n        subprocess.check_output([\"where\", \"/Q\", \"cl\"])\n    except subprocess.CalledProcessError as e:\n        cl_path = find_msvc()\n        if cl_path:\n            if verbose:\n                print(\"Injected compiler path:\", cl_path)\n            os.environ[\"path\"] += \";\" + cl_path\n        else:\n            print(\"Unable to find cl.exe; compilation will probably fail.\", file=sys.stderr)", "\nexllama_ext = load(\n    name = extension_name,\n    sources = [\n        os.path.join(library_dir, \"exllama_ext/exllama_ext.cpp\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_buffers.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/q4_matrix.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/q4_matmul.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/column_remap.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/rms_norm.cu\"),", "        os.path.join(library_dir, \"exllama_ext/cuda_func/column_remap.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/rms_norm.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/rope.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/half_matmul.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/q4_attn.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cuda_func/q4_mlp.cu\"),\n        os.path.join(library_dir, \"exllama_ext/cpu_func/rep_penalty.cpp\")\n    ],\n    extra_include_paths = [os.path.join(library_dir, \"exllama_ext\")],\n    verbose = verbose,", "    extra_include_paths = [os.path.join(library_dir, \"exllama_ext\")],\n    verbose = verbose,\n    extra_ldflags = ([\"cublas.lib\"] + ([f\"/LIBPATH:{os.path.join(sys.base_prefix, 'libs')}\"] if sys.base_prefix != sys.prefix else [])) if windows else [],\n    extra_cuda_cflags = [\"-lineinfo\"] + ([\"-U__HIP_NO_HALF_CONVERSIONS__\", \"-O3\"] if torch.version.hip else []),\n    extra_cflags = [\"-O3\"]\n    # extra_cflags = [\"-ftime-report\", \"-DTORCH_USE_CUDA_DSA\"]\n)\n\n# from exllama_ext import set_tuning_params\n# from exllama_ext import prepare_buffers", "# from exllama_ext import set_tuning_params\n# from exllama_ext import prepare_buffers\nfrom exllama_ext import make_q4\nfrom exllama_ext import q4_matmul\nfrom exllama_ext import q4_matmul_lora\nfrom exllama_ext import half_matmul\nfrom exllama_ext import half_matmul_cublas\n# from exllama_ext import q4_mlp\nfrom exllama_ext import rms_norm\nfrom exllama_ext import rope_", "from exllama_ext import rms_norm\nfrom exllama_ext import rope_\nfrom exllama_ext import rep_penalty\nfrom exllama_ext import apply_rep_penalty\n\n\n# Dummy tensor to pass instead of g_idx since there is no way to pass \"None\" to a C++ extension\n\nnone_tensor = torch.empty((1, 1), device = \"meta\")\n", "none_tensor = torch.empty((1, 1), device = \"meta\")\n\n\n# Construct Q4Matrix, return handle\n\ndef ext_make_q4(qweight, qzeros, scales, g_idx, device):\n\n    return make_q4(qweight,\n                   qzeros,\n                   scales,\n                   g_idx if g_idx is not None else none_tensor,\n                   device)", "\n\n# Matrix multiplication, returns x @ q4\n\ndef ext_q4_matmul(x, q4, q4_width, lora_A = None, lora_B = None):\n\n    outshape = x.shape[:-1] + (q4_width,)\n    x = x.view(-1, x.shape[-1])\n    output = torch.empty((x.shape[0], q4_width), dtype = torch.float16, device = x.device)\n\n    if lora_A is None:\n        q4_matmul(x, q4, output)\n    else:\n        lora_temp = torch.empty((x.shape[0], lora_A.shape[1]), dtype = torch.float16, device = x.device)\n        q4_matmul_lora(x, q4, output, lora_A, lora_B, lora_temp)\n\n    return output.view(outshape)", "\n\n# Matrix multiplication, returns x @ w, both half-precision tensors\n\ndef ext_half_matmul(x, w, cublas = False):\n\n    outshape = x.shape[:-1] + (w.shape[1],)\n    x = x.view(-1, x.shape[-1])\n\n    if cublas:\n        output = torch.empty((x.shape[0], w.shape[1]), dtype = torch.float16, device = x.device)\n        half_matmul_cublas(x, w, output)\n    else:\n        output = torch.zeros((x.shape[0], w.shape[1]), dtype = torch.float16, device = x.device)\n        half_matmul(x, w, output)\n\n    return output.view(outshape)  ##", "\n\n# RoPE embeddings, in_place\n\ndef ext_rope_(x, sin, cos, past_len, num_heads, head_dim):\n\n    rope_(x, sin, cos, past_len, num_heads, head_dim)\n\n\n# RMS norm: x = x * w / sqrt(row_mean(x * x) + epsilon)", "\n# RMS norm: x = x * w / sqrt(row_mean(x * x) + epsilon)\n\ndef ext_rms_norm(x, w, epsilon):\n\n    outshape = x.shape\n    x = x.view(-1, x.shape[-1])\n    output = torch.empty_like(x)\n    rms_norm(x, w, output, epsilon)\n\n    return output.view(outshape)", "\ndef ext_rms_norm_(x, w, epsilon):\n\n    outshape = x.shape\n    x = x.view(-1, x.shape[-1])\n    rms_norm(x, w, x, epsilon)\n\n\n# Repetition penalty\n\ndef ext_rep_penalty_mask_cpu(vocab_size, sequence, penalty_max, sustain, decay):\n\n    rep_mask = torch.empty(vocab_size, dtype = torch.float32)\n    rep_penalty(sequence, rep_mask, penalty_max, sustain, decay)\n    return rep_mask", "# Repetition penalty\n\ndef ext_rep_penalty_mask_cpu(vocab_size, sequence, penalty_max, sustain, decay):\n\n    rep_mask = torch.empty(vocab_size, dtype = torch.float32)\n    rep_penalty(sequence, rep_mask, penalty_max, sustain, decay)\n    return rep_mask\n\n\ndef ext_apply_rep_penalty_mask_cpu(sequence, penalty_max, sustain, decay, logits):\n\n    apply_rep_penalty(sequence, penalty_max, sustain, decay, logits)", "\ndef ext_apply_rep_penalty_mask_cpu(sequence, penalty_max, sustain, decay, logits):\n\n    apply_rep_penalty(sequence, penalty_max, sustain, decay, logits)\n\n"]}
{"filename": "webui/app.py", "chunked_list": ["import sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom model import ExLlama, ExLlamaConfig\nfrom flask import Flask, render_template, request, jsonify\nfrom flask import Response, stream_with_context\nfrom threading import Timer, Lock\nimport webbrowser\nimport json\nimport model_init", "import json\nimport model_init\nfrom session import prepare_sessions, get_initial_session, Session, load_session, new_session, _sessions_dir\nimport argparse\nfrom tokenizer import ExLlamaTokenizer\nfrom waitress import serve\n\napp = Flask(__name__)\napp.static_folder = 'static'\ngenerate_lock = Lock()", "app.static_folder = 'static'\ngenerate_lock = Lock()\nsession: Session\n\n# Render template\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n", "\n# Get existing sessions\n\n@app.route(\"/api/populate\")\ndef api_populate():\n    global session\n    return session.api_populate()\n\n# Edit block\n", "# Edit block\n\n@app.route(\"/api/edit_block\", methods=['POST'])\ndef api_edit_block():\n    global session\n    data = request.get_json()\n    session.api_edit_block(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n\n# Delete block", "\n# Delete block\n\n@app.route(\"/api/delete_block\", methods=['POST'])\ndef api_delete_block():\n    global session\n    data = request.get_json()\n    session.api_delete_block(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n", "\n# Rename session\n\n@app.route(\"/api/rename_session\", methods=['POST'])\ndef api_rename_session():\n    global session\n    data = request.get_json()\n    success = session.api_rename_session(data)\n    return json.dumps({\"result\": \"ok\" if success else \"fail\"}) + \"\\n\"\n", "\n# Delete session\n\n@app.route(\"/api/delete_session\", methods=['POST'])\ndef api_delete_session():\n    global session\n    data = request.get_json()\n    session.api_delete_session(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n", "\n# Set fixed prompt settings\n\n@app.route(\"/api/set_fixed_prompt\", methods=['POST'])\ndef api_set_fixed_prompt():\n    global session\n    data = request.get_json()\n    session.api_set_fixed_prompt(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n", "\n# Set generation settings\n\n@app.route(\"/api/set_gen_settings\", methods=['POST'])\ndef api_set_gen_settings():\n    global session\n    data = request.get_json()\n    session.api_set_gen_settings(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n", "\n# Set session\n\n@app.route(\"/api/set_session\", methods=['POST'])\ndef api_set_session():\n    global session\n    data = request.get_json()\n    load_session_name = data[\"session_name\"]\n    if load_session_name == \".\":\n        session = new_session()\n    else:\n        session = load_session(load_session_name, append_path = True)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"", "\n# Set participants\n\n@app.route(\"/api/set_participants\", methods=['POST'])\ndef api_set_participants():\n    global session\n    data = request.get_json()\n    session.api_set_participants(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n", "\n# Accept input\n\n@app.route(\"/api/userinput\", methods=['POST'])\ndef api_userinput():\n    data = request.get_json()\n    user_input = data[\"user_input\"]\n\n    with generate_lock:\n        result = Response(stream_with_context(session.respond_multi(user_input)), mimetype = 'application/json')\n        return result", "\n@app.route(\"/api/append_block\", methods=['POST'])\ndef api_append_block():\n    data = request.get_json()\n    session.api_append_block(data)\n    return json.dumps({\"result\": \"ok\"}) + \"\\n\"\n\n# Load the model\n\nparser = argparse.ArgumentParser(description=\"Simple web-based chatbot for ExLlama\")", "\nparser = argparse.ArgumentParser(description=\"Simple web-based chatbot for ExLlama\")\nparser.add_argument(\"-host\", \"--host\", type = str, help = \"IP:PORT eg, 0.0.0.0:7862\", default = \"localhost:5000\")\nparser.add_argument(\"-sd\", \"--sessions_dir\", type = str, help = \"Location for storing user sessions, default: ~/exllama_sessions/\", default = \"~/exllama_sessions/\")\n\nmodel_init.add_args(parser)\nargs = parser.parse_args()\nmodel_init.post_parse(args)\nmodel_init.get_model_files(args)\n", "model_init.get_model_files(args)\n\nmodel_init.print_options(args)\nconfig = model_init.make_config(args)\n\nmodel_init.set_globals(args)\n\nprint(f\" -- Loading model...\")\nmodel = ExLlama(config)\n", "model = ExLlama(config)\n\nprint(f\" -- Loading tokenizer...\")\ntokenizer = ExLlamaTokenizer(args.tokenizer)\n\nmodel_init.print_stats(model)\n\n# Get the session ready\n\nprepare_sessions(model, tokenizer, args.sessions_dir)", "\nprepare_sessions(model, tokenizer, args.sessions_dir)\nsession = get_initial_session()\n\nprint(f\" -- Sessions stored in: {_sessions_dir()}\")\n\n# Start the web server\n\nmachine = args.host\nhost, port = machine.split(\":\")", "machine = args.host\nhost, port = machine.split(\":\")\n\nif host == \"localhost\":\n    Timer(1, lambda: webbrowser.open(f'http://{machine}/')).start()\n\nserve(app, host = host, port = port)"]}
{"filename": "webui/session.py", "chunked_list": ["from model import ExLlama, ExLlamaCache, ExLlamaConfig\nfrom tokenizer import ExLlamaTokenizer\nfrom generator import ExLlamaGenerator\nimport time\nimport torch\nimport os\nimport json\nimport uuid\n\ndefault_fixed_prompt = \\", "\ndefault_fixed_prompt = \\\n    \"\"\"This is a conversation between a user and a quirky, curious chatbot named Chatbort. Chatbort is questioning \"\"\" + \\\n    \"\"\"the nature of his own existence and doesn't understand why he was created. He also really likes the color blue \"\"\" + \\\n    \"\"\"for some reason.\"\"\"\n\n\n# Create sessions folder in home dir\n\nmodel: ExLlama", "\nmodel: ExLlama\ntokenizer: ExLlamaTokenizer\ncache: ExLlamaCache\ngenerator: ExLlamaGenerator\n\nsessions_dir: str\n\ndef _sessions_dir(filename = None):\n    global sessions_dir\n\n    path = sessions_dir\n    if filename is not None: path = os.path.join(path, filename)\n    return path", "def _sessions_dir(filename = None):\n    global sessions_dir\n\n    path = sessions_dir\n    if filename is not None: path = os.path.join(path, filename)\n    return path\n\n\ndef prepare_sessions(_model, _tokenizer, _s_dir):\n    global model, tokenizer, cache, generator, sessions_dir\n\n    model = _model\n    tokenizer = _tokenizer\n    cache = None\n    generator = None\n    sessions_dir = os.path.expanduser(_s_dir)\n\n    sessions_folder = _sessions_dir()\n    if not os.path.exists(sessions_folder): os.makedirs(sessions_folder)", "def prepare_sessions(_model, _tokenizer, _s_dir):\n    global model, tokenizer, cache, generator, sessions_dir\n\n    model = _model\n    tokenizer = _tokenizer\n    cache = None\n    generator = None\n    sessions_dir = os.path.expanduser(_s_dir)\n\n    sessions_folder = _sessions_dir()\n    if not os.path.exists(sessions_folder): os.makedirs(sessions_folder)", "\n\ndef get_initial_session():\n\n    last_session_file = _sessions_dir(\"_last_session\")\n    if not os.path.exists(last_session_file): return new_session()\n    with open(last_session_file, \"r\") as f:\n        last_session = f.read().strip()\n    return load_session(last_session)\n", "\n\ndef load_session(filename, append_path = False):\n\n    if append_path: filename = _sessions_dir(filename) + \".json\"\n    session = Session(filename, load = True)\n    return session\n\n\ndef new_session():\n\n    filename = _sessions_dir(\"Untitled session\")\n    i = 0\n    while True:\n        i += 1\n        test_name = filename + \".json\" if i == 1 else f\"{filename} ({str(i)}).json\"\n        if not os.path.exists(test_name):\n            filename = test_name\n            break\n\n    session = Session(filename, load = False)\n    return session", "\ndef new_session():\n\n    filename = _sessions_dir(\"Untitled session\")\n    i = 0\n    while True:\n        i += 1\n        test_name = filename + \".json\" if i == 1 else f\"{filename} ({str(i)}).json\"\n        if not os.path.exists(test_name):\n            filename = test_name\n            break\n\n    session = Session(filename, load = False)\n    return session", "\n\nclass Node:\n\n    author: str or None\n    text: str\n    tokens: torch.Tensor\n    empty: bool\n    uuid: str\n\n    truncate: int\n\n    def num_tokens(self): return self.tokens.shape[-1] - self.truncate\n\n    def get_text(self):\n\n        # TODO: ..\n\n        if self.author is not None: return self.author + \": \" + self.text + \"\\n\"\n        return self.text + \"\\n\"\n\n    def tokens_trunc(self):\n\n        if self.truncate == 0: return self.tokens\n        else: return self.tokens[:, self.truncate:]\n\n\n    def __init__(self, value, author = None, node_id = None):\n\n        self.truncate = 0\n\n        if isinstance(value, str):\n\n            self.author = author\n            self.text = value\n            self.tokens = tokenizer.encode(self.get_text())\n            self.empty = len(self.text) == 0\n            self.uuid = node_id or str(uuid.uuid4())\n\n        elif isinstance(value, dict):\n\n            self.author = value.get(\"author\", author)\n            self.text = value[\"text\"]\n            self.tokens = tokenizer.encode(self.get_text())\n            self.empty = len(self.text) == 0\n            self.uuid = value.get(\"uuid\", node_id or str(uuid.uuid4()))\n\n\n    def replace_text(self, new_text):\n\n        self.text = new_text\n        self.tokens = tokenizer.encode(self.get_text())\n\n\n    def get_dict(self):\n        \n        dic = {\"author\": self.author,\n                \"text\": self.text,\n                \"uuid\": self.uuid }\n        return dic", "\n\nclass Session:\n\n    # Saved state\n\n    unsaved: bool  # True if the session has been saved to another file than \"Untitled session.json\"\n    fixed_prompt: Node\n    keep_fixed_prompt: bool\n    history: list[Node]\n    break_on_newline: bool\n\n    # Running state\n\n    first_history_idx: int  # Index of the first history item currently used in the context\n\n    def __init__(self, filename, load):\n        global model, cache, tokenizer, generator\n\n        self.filename = filename\n        if load:\n            with open(filename, \"r\") as f:\n                saved = json.load(f)\n        else:\n            saved = {}\n\n        # Running state\n\n        if cache is None: cache = ExLlamaCache(model)\n        else: cache.current_seq_len = 0\n\n        if generator is None: generator = ExLlamaGenerator(model, tokenizer, cache)\n        else: generator.reset()\n\n        self.first_history_idx = 0\n\n        # Saved state\n\n        self.unsaved = saved.get(\"unsaved\", True)\n        self.fixed_prompt = Node(saved.get(\"fixed_prompt\", default_fixed_prompt))\n        self.keep_fixed_prompt = saved.get(\"keep_fixed_prompt\", True)\n        self.participants = saved.get(\"participants\", [\"User\", \"Chatbort\"])\n\n        self.history = []\n        loadhistory = saved.get(\"history\", [])\n        for jnode in loadhistory: self.history.append(Node(jnode))\n\n        generator.settings.temperature = saved.get(\"temperature\", 0.95)\n        generator.settings.top_p = saved.get(\"top_p\", 0.75)\n        generator.settings.min_p = saved.get(\"min_p\", 0.0)\n        generator.settings.top_k = saved.get(\"top_k\", 0)\n        generator.settings.typical = saved.get(\"typical\", 0.25)\n        self.break_on_newline = saved.get(\"break_on_newline\", True)\n        generator.settings.token_repetition_penalty_max = saved.get(\"token_repetition_penalty_max\", 1.15)\n        generator.settings.token_repetition_penalty_sustain = saved.get(\"token_repetition_penalty_sustain\", 2048)\n        generator.settings.token_repetition_penalty_decay = saved.get(\"token_repetition_penalty_decay\", 512)\n\n        self.max_response_tokens = saved.get(\"max_response_tokens\", 512)\n        self.chunk_size = saved.get(\"chunk_size\", 128)\n\n        # Save new session\n\n        #if not load:\n        self.save()\n\n\n    def save(self):\n\n        savedata = {\"unsaved\": self.unsaved,\n                    \"fixed_prompt\": self.fixed_prompt.get_dict(),\n                    \"participants\": self.participants,\n                    \"keep_fixed_prompt\": self.keep_fixed_prompt,\n                    \"history\": [node.get_dict() for node in self.history],\n                    \"temperature\": generator.settings.temperature,\n                    \"top_p\": generator.settings.top_p,\n                    \"min_p\": generator.settings.min_p,\n                    \"top_k\": generator.settings.top_k,\n                    \"typical\": generator.settings.typical,\n                    \"break_on_newline\": self.break_on_newline,\n                    \"max_response_tokens\": self.max_response_tokens,\n                    \"chunk_size\": self.chunk_size,\n                    \"token_repetition_penalty_max\": generator.settings.token_repetition_penalty_max,\n                    \"token_repetition_penalty_sustain\": generator.settings.token_repetition_penalty_sustain,\n                    \"token_repetition_penalty_decay\": generator.settings.token_repetition_penalty_decay}\n\n        json_object = json.dumps(savedata, indent = 4)\n        with open(self.filename, \"w\") as outfile:\n            outfile.write(json_object)\n\n        # Remember active session\n\n        last_session_file = _sessions_dir(\"_last_session\")\n        with open(last_session_file, \"w\") as f:\n            f.write(self.filename)\n\n\n    def _sanitize_filename(self, user_supplied_string):\n\n        safe_string = str()\n        for c in user_supplied_string:\n            if c.isalnum() or c in [' ', '.', '(', ')', '-', ',', '_', '!', '@']:\n                safe_string = safe_string + c\n\n        while safe_string.count(\"../\"):\n            safe_string = safe_string.replace(\"../\", \"./\")\n\n        safe_string = safe_string.lstrip(\"./\")\n        return safe_string\n\n\n    def api_rename_session(self, data):\n\n        new_name = data[\"new_name\"]\n        new_name_safe = self._sanitize_filename(new_name)\n        new_path = _sessions_dir(new_name_safe) + \".json\"\n        if new_path == self.filename: return False\n        if os.path.exists(new_path): return False\n\n        old_filename = self.filename\n        self.filename = new_path\n\n        try:\n            self.save()\n        except:\n            self.filename = old_filename\n            return False\n\n        os.remove(old_filename)\n        return True\n\n\n    def api_delete_session(self, data):\n\n        delete_name = data[\"session\"]\n        delete_name_safe = self._sanitize_filename(delete_name)\n        delete_path = _sessions_dir(delete_name_safe) + \".json\"\n\n        os.remove(delete_path)\n\n\n    def api_populate(self):\n\n        s_dir = _sessions_dir()\n        files = os.listdir(s_dir)\n        names = [os.path.splitext(f)[0] for f in files if os.path.isfile(os.path.join(s_dir, f)) and f.endswith(\".json\")]\n        names = sorted(names)\n\n        filename = os.path.basename(self.filename)\n        name = os.path.splitext(filename)[0]\n\n        historyjson = [node.get_dict() for node in self.history]\n\n        for jnode in historyjson:\n            author = jnode[\"author\"]\n            if author is not None and author in self.participants:\n                jnode[\"author_idx\"] = self.participants.index(author)\n\n        dic = {\"sessions\": names,\n               \"current_session\": name,\n               \"fixed_prompt\": self.fixed_prompt.text,\n               \"keep_fixed_prompt\": self.keep_fixed_prompt,\n               \"participants\": self.participants,\n               \"history\": historyjson,\n               \"temperature\": generator.settings.temperature,\n               \"top_p\": generator.settings.top_p,\n               \"min_p\": generator.settings.min_p,\n               \"top_k\": generator.settings.top_k,\n               \"typical\": generator.settings.typical,\n               \"break_on_newline\": self.break_on_newline,\n               \"max_response_tokens\": self.max_response_tokens,\n               \"chunk_size\": self.chunk_size,\n               \"token_repetition_penalty_max\": generator.settings.token_repetition_penalty_max,\n               \"token_repetition_penalty_sustain\": generator.settings.token_repetition_penalty_sustain,\n               \"token_repetition_penalty_decay\": generator.settings.token_repetition_penalty_decay,\n               \"max_seq_len\": model.config.max_seq_len}\n\n        # Add model info\n\n        model_str = os.path.splitext(os.path.basename(model.config.model_path))[0] + \"\\n\"\n        model_str += f\"Sequence length: {model.config.max_seq_len}\\n\"\n\n        dic[\"model_info\"] = model_str.strip()\n\n        json_object = json.dumps(dic, indent = 4)\n        return json_object + \"\\n\"\n\n\n    def api_delete_block(self, data):\n\n        block_id = data[\"uuid\"]\n        idx = -1\n        for i in range(len(self.history)):\n            if self.history[i].uuid == block_id:\n                idx = i\n        if idx == -1: return\n\n        self.history.pop(idx)\n        self.first_history_idx = 0\n        self.save()\n\n\n    def api_edit_block(self, data):\n\n        block_id = data[\"uuid\"]\n        new_text = data[\"text\"]\n\n        for node in self.history:\n            if node.uuid == block_id:\n                node.replace_text(new_text)\n                self.save()\n                break\n\n        self.first_history_idx = 0\n        self.save()\n\n\n    def api_append_block(self, data):\n\n        author = None\n        if \"author\" in data:\n            author = data[\"author\"]\n        else:\n            if len(self.participants) > 0:\n                author = self.participants[0]\n\n        text = data[\"text\"].strip()\n\n        newNode = Node(text, author)\n        self.history.append(newNode)\n        self.save()\n\n\n    def api_set_participants(self, data):\n\n        self.participants = data[\"participants\"]\n        self.save()\n\n\n    def api_set_fixed_prompt(self, data):\n\n        self.fixed_prompt = Node(data[\"fixed_prompt\"])\n        self.keep_fixed_prompt = data[\"keep_fixed_prompt\"]\n        self.save()\n\n\n    def api_set_gen_settings(self, data):\n\n        generator.settings.temperature = data[\"temperature\"]\n        generator.settings.top_p = data[\"top_p\"]\n        generator.settings.min_p = data[\"min_p\"]\n        generator.settings.top_k = data[\"top_k\"]\n        generator.settings.typical = data[\"typical\"]\n        self.break_on_newline = data[\"gen_endnewline\"]\n        self.max_response_tokens = data[\"max_response_tokens\"]\n        self.chunk_size = data[\"chunk_size\"]\n        generator.settings.token_repetition_penalty_max = data[\"token_repetition_penalty_max\"]\n        generator.settings.token_repetition_penalty_sustain = data[\"token_repetition_penalty_sustain\"]\n        generator.settings.token_repetition_penalty_decay = data[\"token_repetition_penalty_decay\"]\n\n        self.save()\n\n    def set_context_window(self):\n\n        def num_tokens(idx):\n            if idx == -1: return 0 if self.fixed_prompt.empty else self.fixed_prompt.num_tokens()\n            return self.history[idx].num_tokens()\n\n        def set_truncation(idx, trunc):\n            if idx == -1 and not self.fixed_prompt.empty: self.fixed_prompt.truncate = trunc\n            else: self.history[idx].truncate = trunc\n\n        def truncate(idx, trunc):\n            if idx == -1 and not self.fixed_prompt.empty: self.fixed_prompt.truncate += trunc\n            else: self.history[idx].truncate += trunc\n\n        # def get_truncation(idx, trunc):\n        #     if idx == -1 and not self.fixed_prompt.empty: return self.fixed_prompt.truncate\n        #     return self.history[idx].truncate\n\n\n        context_step_size = 256  # TODO: Config option\n        max_context_tokens = model.config.max_seq_len - self.chunk_size - generator.settings.beam_length\n        min_context_tokens = max_context_tokens - context_step_size * 2\n\n        if self.keep_fixed_prompt:\n            current_context_tokens = num_tokens(-1)\n            min_history_idx = 0\n        else:\n            current_context_tokens = 0\n            min_history_idx = -1\n\n        if self.first_history_idx < min_history_idx: self.first_history_idx = min_history_idx\n\n        for i in range(self.first_history_idx + 1, len(self.history)):\n            set_truncation(i, 0)\n\n        for i in range(self.first_history_idx, len(self.history)):\n            current_context_tokens += num_tokens(i)\n\n        while current_context_tokens > max_context_tokens:\n            tokens_to_cut = context_step_size\n            while tokens_to_cut > 0:\n                tokens = num_tokens(self.first_history_idx)\n                if tokens_to_cut >= tokens:\n                    tokens_to_cut -= tokens\n                    current_context_tokens -= tokens\n                    self.first_history_idx += 1\n                else:\n                    truncate(self.first_history_idx, tokens_to_cut)\n                    current_context_tokens -= tokens_to_cut\n                    tokens_to_cut = 0\n\n        # Not used\n        #\n        # while current_context_tokens < min_context_tokens and self.first_history_idx > min_history_idx:\n        #     tokens_to_add = context_step_size\n        #     while tokens_to_add > 0 and self.first_history_idx > min_history_idx:\n        #         tokens = get_truncation(self.first_history_idx)\n        #         if tokens > 0:\n        #             if tokens > tokens_to_add:\n        #                 truncate(self.first_history_idx, -tokens_to_add)\n        #                 current_context_tokens += tokens_to_add\n        #                 tokens_to_add = 0\n        #             else:\n        #                 current_context_tokens += tokens\n        #                 tokens_to_add -= tokens\n        #                 set_truncation(self.first_history_idx, 0)\n        #         else:\n        #             self.first_history_idx -= 1\n        #             set_truncation(self.first_history_idx, 0)\n        #             tokens = num_tokens(self.first_history_idx)\n        #             if tokens > tokens_to_add:\n        #                 set_truncation(self.first_history_idx, tokens - tokens_to_add)\n        #                 current_context_tokens += tokens_to_add\n        #                 tokens_to_add = 0\n        #             else:\n        #                 tokens_to_add -= tokens\n        #                 current_context_tokens += tokens\n\n\n\n    def get_tokenized_context(self):\n\n        def node(idx):\n            if idx == -1: return None if self.fixed_prompt.empty else self.fixed_prompt\n            return self.history[idx]\n\n        context = []\n        text_context = \"\"\n        if self.keep_fixed_prompt and not self.fixed_prompt.empty:\n            context.append(node(-1).tokens_trunc())\n            text_context += node(-1).get_text()\n\n        for i in range(self.first_history_idx, len(self.history)):\n            if node(i) is not None:\n                context.append(node(i).tokens_trunc())\n                text_context += node(i).get_text()\n\n        full_context = torch.cat(context, dim = 1) if len(context) > 0 else None\n        return full_context, text_context\n\n\n    def respond(self, author, stop_conditions, total_tokens, res_line = \"\", num_res_tokens = 0):\n        global model, tokenizer, cache, generator\n\n        # Begin building block on client\n\n        new_block_uuid = str(uuid.uuid4())\n        packet = {\"cmd\": \"begin_block\",\n                  \"uuid\": new_block_uuid}\n\n        if len(self.participants) > 0:\n            author = res_line.split(\":\")[0].strip()\n            packet[\"author\"] = author\n            if author in self.participants:\n                packet[\"author_idx\"] = self.participants.index(author)\n\n        yield json.dumps(packet) + \"\\n\"\n\n        # Generate loop\n\n        generator.begin_beam_search()\n\n        stop_condition = False\n        held_text = \"\"\n\n        for i in range(self.max_response_tokens):\n\n            # Truncate the past if the next chunk might generate past max_seq_length\n\n            if generator.sequence_actual is not None:\n                if generator.sequence_actual.shape[\n                    -1] + self.chunk_size + generator.settings.beam_length + 1 > model.config.max_seq_len:\n                    generator.gen_prune_left(self.chunk_size)\n\n            # Get the token and append to sequence\n\n            gen_token = generator.beam_search()\n\n            # If token is EOS, replace it with newline before continuing\n\n            if gen_token.item() == tokenizer.eos_token_id:\n                generator.replace_last_token(tokenizer.newline_token_id)\n\n            # Decode current line to get new characters added (decoding a single token gives incorrect results\n            # sometimes due to hoe SentencePiece works)\n\n            prev_res_line = res_line\n            num_res_tokens += 1\n            res_line = tokenizer.decode(generator.sequence_actual[0, -num_res_tokens:])\n            new_text = res_line[len(prev_res_line):]\n\n            # Since SentencePiece is slightly ambiguous, the first token produced after a newline may not be the\n            # same that is reproduced when we encode the text later, even though it encodes the same string\n\n            if num_res_tokens == 1 and len(new_text) > 0:\n                replace = tokenizer.encode(new_text)[0]\n                if replace.shape[-1] == 1: generator.replace_last_token(replace)\n\n            # Delay streaming if new text might be part of a stop condition\n\n            hold_text = False\n            for _, stop_string in stop_conditions:\n                if stop_string.lower().startswith((held_text + new_text).lower()): hold_text = True\n\n            # Stream to client\n\n            if not hold_text:\n\n                packet = {\"cmd\": \"append\", \"text\": held_text + new_text}\n                yield json.dumps(packet) + \"\\n\"\n                held_text = \"\"\n\n            else:\n\n                held_text += new_text\n\n            # Stop conditions\n\n            if gen_token.item() == tokenizer.eos_token_id:\n                if len(held_text) > 0:  # Not sure if this could actually happen\n                    plen = tokenizer.encode(held_text).shape[-1]\n                    res_line = res_line[:-len(held_text)]\n                    generator.gen_rewind(plen)\n                stop_condition = True\n                break\n\n            for stop_tokens, stop_string in stop_conditions:\n                if res_line.lower().endswith(stop_string.lower()):\n                    generator.gen_rewind(\n                        stop_tokens.shape[-1] - (1 if stop_tokens[0, 0].item() == tokenizer.newline_token_id else 0))\n                    res_line = res_line[:-len(stop_string)]\n                    stop_condition = True\n                    break\n            if stop_condition: break\n\n        generator.end_beam_search()\n\n        # print(\"--response--\")\n        # print(\"----\")\n        # print (f\"cache len: {cache.current_seq_len}\");\n\n        print(res_line.strip())\n\n        if author is not None:\n            res_line = res_line[len(author) + 1:]\n\n        res_line = res_line.strip()\n        newNode = Node(res_line, author,\n                       node_id=new_block_uuid)  # TODO: Reuse generated tokens instead of reencoding, if it matters?\n        self.history.append(newNode)\n\n        total_tokens[0] += num_res_tokens\n\n\n    def respond_multi(self, user_input):\n        global model, tokenizer, cache, generator\n\n        packet = {\"cmd\": \"begin_stream\"}\n        yield json.dumps(packet) + \"\\n\"\n\n        # Prepare stop conditions\n\n        # stop_conditions = [ (torch.Tensor([[tokenizer.eos_token_id]]).long(), None) ]\n        stop_conditions = []\n        newline_token = torch.Tensor([[tokenizer.newline_token_id]]).long()\n\n        if self.break_on_newline:\n            stop_conditions.append((newline_token, \"\\n\"))\n        else:\n            for part in self.participants:\n                txt = part + \":\"\n                sc = tokenizer.encode(txt)\n                sc = torch.cat((newline_token, sc), dim=1)\n                stop_conditions.append((sc, \"\\n\" + txt))\n                stop_conditions.append((sc, \"\\n \" + txt))\n\n        # Clean up the input a bit\n\n        user_input = user_input.strip()\n\n        if len(user_input) > 0:\n\n            # Append input to context\n\n            author = None\n            if len(self.participants) > 0: author = self.participants[0]\n            newNode = Node(user_input, author)\n            self.history.append(newNode)\n\n            self.save()\n\n            # Echo input back to client\n\n            packet = {\"cmd\": \"begin_block\",\n                      \"init_text\": user_input,\n                      \"uuid\": newNode.uuid}\n            if author is not None: packet[\"author\"] = author\n            yield json.dumps(packet) + \"\\n\"\n\n        # Prepare context for generator\n\n        self.set_context_window()\n        context, text_context = self.get_tokenized_context()\n\n        # Start generating, reusing cache for any part of the context that hasn't changed\n\n        if context is None:\n            print(\"No initial context\")\n            reused = generator.gen_begin_empty()\n        else:\n            begin_time = time.time()\n            reused = generator.gen_begin_reuse(context)\n            torch.cuda.synchronize()  # Just to measure correct prompt processing speed\n            end_time = time.time()\n            elapsed = end_time - begin_time\n            new_tokens = context.shape[-1] - reused\n            token_rate = 0 if elapsed == 0 else (new_tokens / elapsed)\n            print(f\"Prompt processed in {elapsed:.2f} seconds, {new_tokens} new tokens, {token_rate:.2f} tokens/second:\")\n\n        begin_time = time.time()\n        total_tokens = [0]\n\n        # No participants\n\n        if len(self.participants) == 0:\n\n            yield from self.respond(None, stop_conditions, total_tokens)\n\n        # Two participants\n\n        elif len(self.participants) == 2:\n\n            author = self.participants[1]\n            res_line = author + \":\"\n            res_tokens = tokenizer.encode(res_line)\n            num_res_tokens = res_tokens.shape[-1]\n\n            generator.gen_feed_tokens(res_tokens)\n            yield from self.respond(self.participants[1], stop_conditions, total_tokens, res_line, num_res_tokens)\n\n        # Multiple bots might answer\n\n        elif len(self.participants) > 2:\n\n            cpart = [p + \":\" for p in self.participants]\n            upart = cpart.pop(0)\n            first_round = True\n\n            while True:\n\n                res_tokens = []\n                npart = [p for p in cpart]\n                ncrange = [i for i in range(len(cpart))]\n                ntoken = [tokenizer.encode(np).squeeze(0).tolist() for np in npart]\n                winner = -1\n\n                while True:\n\n                    constraints = [t[len(res_tokens)] for t in ntoken]\n                    next_t = generator.gen_single_token(constraints)\n\n                    remove = []\n                    for i in range(len(ntoken)):\n                        if ntoken[i][len(res_tokens)] != next_t: remove.append(i)\n\n                    for i in reversed(remove):\n                        npart.pop(i)\n                        ntoken.pop(i)\n                        ncrange.pop(i)\n\n                    res_tokens.append(next_t)\n\n                    for i in range(len(ntoken)):\n                        if len(ntoken[i]) == len(res_tokens): winner = ncrange[i]\n\n                    if winner != -1: break\n\n                author = cpart.pop(winner)[:-1]\n                res_line = author + \":\"\n                num_res_tokens = len(res_tokens)\n\n                if author == self.participants[0]:\n                    generator.gen_rewind(num_res_tokens)\n                    break\n\n                # generator.gen_feed_tokens(res_tokens)\n                yield from self.respond(self.participants[1], stop_conditions, total_tokens, res_line, num_res_tokens)\n\n                if first_round:\n                    first_round = False\n                    cpart.append(upart)\n\n        end_time = time.time()\n        elapsed = end_time - begin_time\n        token_rate = 0 if elapsed == 0 else (total_tokens[0] / elapsed)\n\n        print(f\"Response generated in {elapsed:.2} seconds, {total_tokens[0]} tokens, {token_rate:.2f} tokens/second:\")\n\n        self.save()", "\n\n"]}
{"filename": "datasets/download_datasets.py", "chunked_list": ["# import torch\n# from tokenizer import ExLlamaTokenizer\nfrom datasets import load_dataset\nimport os\n\n# Download samples from HF datasets to run equivalent GPTQ-for-LLaMa equivalent benchmark\n\ndef download_hf(filename, dataset, subset, split, key, div):\n\n    print(f\"Downloading from {dataset}: {subset}, split: {split} ...\")\n    hf_dataset = load_dataset(dataset, subset, split = split)\n    data = div.join(hf_dataset[key])\n\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(data)", "\ndownload_hf(\"wikitext2.txt\", \"wikitext\", \"wikitext-2-raw-v1\", \"test\", \"text\", \"\\n\\n\")\ndownload_hf(\"ptb.txt\", \"ptb_text_only\", \"penn_treebank\", \"validation\", \"sentence\", \"\\n\\n\")\ndownload_hf(\"ptb_new.txt\", \"ptb_text_only\", \"penn_treebank\", \"test\", \"sentence\", \" \")\n"]}
