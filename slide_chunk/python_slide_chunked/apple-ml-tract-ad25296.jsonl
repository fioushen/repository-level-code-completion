{"filename": "tc_distill_edm.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport copy\nimport functools\nimport pickle\nimport sys\nfrom typing import Dict, Optional\n", "from typing import Dict, Optional\n\nimport torch\nimport torch.nn.functional\nfrom absl import app, flags\n\nimport lib\nfrom lib.distributed import device, device_id\nfrom lib.util import FLAGS, int_str\n", "from lib.util import FLAGS, int_str\n\n# Imports within edm/ are often relative to edm/ so we do this.\nsys.path.append('edm')\nimport dnnlib\nfrom torch_utils import distributed as dist\nfrom torch_utils import misc\n\n\nclass EluDDIM05TCMultiStepx0(lib.train.TrainModel):\n    SIGMA_DATA = 0.5\n    SIGMA_MIN: float = 0.002\n    SIGMA_MAX: float = 80.\n    RHO: float = 7.\n\n    def __init__(self, res: int, timesteps: int, **params):\n        super().__init__(\"EluUNet\", res, timesteps, **params)\n        self.use_imagenet = FLAGS.dataset == \"imagenet64\"\n        self.num_classes = 1000 if self.use_imagenet else 10\n\n        # Setup pretrained model\n        lib.distributed.barrier()\n        if FLAGS.dataset == \"imagenet64\":\n            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\"\n        elif FLAGS.dataset == \"cifar10\":\n            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-uncond-ve.pkl\"\n        else:\n            raise ValueError(\"Only cifar10 or imagenet64 is supported for now.\")\n        with dnnlib.util.open_url(pretrained_url) as f:\n            pretrained = pickle.load(f)['ema']\n        lib.distributed.barrier()\n\n        network_kwargs = self.get_pretrained_cifar10_network_kwargs()\n        if self.use_imagenet:\n            network_kwargs = self.get_pretrained_imagenet_network_kwargs()\n        label_dim = self.num_classes if self.use_imagenet else 0\n        interface_kwargs = dict(img_resolution=res, img_channels=3, label_dim=label_dim)\n        model = dnnlib.util.construct_class_by_name(**network_kwargs, **interface_kwargs)\n        model.train().requires_grad_(True)\n        misc.copy_params_and_buffers(src_module=pretrained, dst_module=model, require_all=False)\n        del pretrained      # save memory\n\n        self.time_schedule = tuple(int(x) for x in self.params.time_schedule.split(','))\n        steps_per_phase = int_str(FLAGS.train_len) / (FLAGS.batch * (len(self.time_schedule) - 1))\n        ema = self.params.ema_residual ** (1 / steps_per_phase)\n        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - ema))\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=self.params.dropout))\n        self.model = lib.distributed.wrap(model)\n        self.model_eval = lib.optim.ModuleEMA(model, momentum=ema).eval().requires_grad_(False).to(device_id())\n        lib.distributed.barrier()\n\n        # Disable dropout noise for teacher\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n        self.self_teacher = lib.optim.ModuleEMA(model, momentum=self.params.sema).to(device_id())\n        self.self_teacher.eval().requires_grad_(False)\n        self.teacher = copy.deepcopy(model).to(device_id())\n        self.teacher.eval().requires_grad_(False)\n\n        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr, weight_decay=0.0)\n\n        # Setup noise schedule\n        sigma = torch.linspace(self.SIGMA_MIN ** (1 / self.RHO),\n                               self.SIGMA_MAX ** (1 / self.RHO), timesteps, dtype=torch.double).pow(self.RHO)\n        sigma = torch.cat([torch.zeros_like(sigma[:1]), sigma])\n        self.register_buffer('sigma', sigma.to(device()))\n        self.timesteps = timesteps\n\n    def get_pretrained_cifar10_network_kwargs(self):\n        network_kwargs = dnnlib.EasyDict()\n        network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n        network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n        network_kwargs.class_name = 'training.networks.EDMPrecond'\n        network_kwargs.augment_dim = 0\n        network_kwargs.update(dropout=0.0, use_fp16=False)\n        return network_kwargs\n\n    def get_pretrained_imagenet_network_kwargs(self):\n        network_kwargs = dnnlib.EasyDict()\n        network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n        network_kwargs.class_name = 'training.networks.EDMPrecond'\n        network_kwargs.update(dropout=0.0, use_fp16=False)\n        return network_kwargs\n\n    @classmethod\n    def c_in(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return (sigma ** 2 + cls.SIGMA_DATA ** 2) ** -0.5\n\n    @classmethod\n    def c_skip(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return (cls.SIGMA_DATA ** 2) / (sigma ** 2 + cls.SIGMA_DATA ** 2)\n\n    @classmethod\n    def c_out(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return sigma * cls.SIGMA_DATA * (cls.SIGMA_DATA ** 2 + sigma ** 2) ** -0.5\n\n    @staticmethod\n    def c_noise(sigma: torch.Tensor) -> torch.Tensor:\n        return 0.25 * sigma.clamp(1e-20).log()\n\n    def forward(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        step = self.timesteps // self.time_schedule[1]\n        shape = n, self.COLORS, self.params.res, self.params.res\n\n        xt = self.sigma[-1] * torch.randn(shape, generator=generator, dtype=torch.double).to(device())\n        class_labels = (torch.eye(self.num_classes, device=device())[torch.randint(self.num_classes, size=[n], device=device())]) if self.use_imagenet else None\n\n        for t in reversed(range(0, self.timesteps, step)):\n            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n            g = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n            x0 = self.model_eval(xt, g[0], class_labels).to(torch.float64)\n            xt = self.post_xt_x0(xt, x0, g[0], g[1])\n\n        return xt.clamp(-1, 1).float()\n\n    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, sigma: torch.Tensor, sigma1: torch.Tensor) -> torch.Tensor:\n        x0 = torch.clip(out, -1., 1.)\n        eps = (xt - x0) / sigma\n        return torch.nan_to_num(x0 + eps * sigma1)\n\n    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n        if self.num_classes == 1000:    # imagenet\n            y = torch.nn.functional.one_hot(y, self.num_classes).to(y.device)\n        else:\n            y = None\n\n        with torch.no_grad():\n\n            step = self.timesteps // self.time_schedule[1]\n            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n            semi_index = torch.randint(step, index.shape, device=device())\n            ix = index - semi_index, (index - semi_index - 1).clamp(1), index - step\n\n            s = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n            noise = torch.randn_like(x).to(device())\n\n            # RK step from teacher\n            xt = x.double() + noise * s[0]\n            x0 = self.teacher(xt, s[0], y)\n            eps = (xt - x0) / s[0]\n            xt_ = xt + (s[1] - s[0]) * eps\n            x0_ = self.teacher(xt_, s[1], y)\n            eps = .5 * (eps + (xt_ - x0_) / s[1])\n            xt_ = xt + (s[1] - s[0]) * eps      # RK target from teacher; no RK needed for sigma_min\n\n            # self-teacher step\n            xt2 = self.post_xt_x0(xt_, self.self_teacher(xt_, s[1], y), s[1], s[2])\n            xt2 += ((semi_index + 1) == step).view(-1, 1, 1, 1) * (xt_ - xt2)   # Only propagate inside phase semi_range\n\n            xt2 = ((xt2 * s[0] - xt * s[2]) / (s[0] - s[2]))\n\n            # Boundary and terminal condition: last time step, no RK and self-teaching needed\n            target_without_precon = torch.where((index - semi_index - 1).view(-1, 1, 1, 1) == 0, x0.double(), xt2.double())\n\n            target = (target_without_precon - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\n        self.opt.zero_grad(set_to_none=True)\n        pred = self.model(xt.float(), s[0].float(), y).double()\n        pred = (pred - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\n        weight = (s[0] ** 2 + self.SIGMA_DATA ** 2) * (self.c_out(s[0]) ** 2) * (s[0] * self.SIGMA_DATA) ** -2\n        loss = (torch.nn.functional.mse_loss(pred.float(), target.float(), reduction='none')).mean((1, 2, 3))\n        loss = (weight.float() * loss).mean()\n\n        loss.backward()\n\n        # LR warmup and clip gradient like EDM paper\n        if self.params.lr_warmup is not None:\n            for g in self.opt.param_groups:\n                g['lr'] = self.params.lr * min(info.samples / max(int_str(self.params.lr_warmup), 1e-8), 1)\n        for param in self.model.parameters():\n            if param.grad is not None:\n                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n\n        self.opt.step()\n        self.self_teacher.update(self.model)\n        self.model_eval.update(self.model)\n        return {'loss/global': loss}", "\nclass EluDDIM05TCMultiStepx0(lib.train.TrainModel):\n    SIGMA_DATA = 0.5\n    SIGMA_MIN: float = 0.002\n    SIGMA_MAX: float = 80.\n    RHO: float = 7.\n\n    def __init__(self, res: int, timesteps: int, **params):\n        super().__init__(\"EluUNet\", res, timesteps, **params)\n        self.use_imagenet = FLAGS.dataset == \"imagenet64\"\n        self.num_classes = 1000 if self.use_imagenet else 10\n\n        # Setup pretrained model\n        lib.distributed.barrier()\n        if FLAGS.dataset == \"imagenet64\":\n            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\"\n        elif FLAGS.dataset == \"cifar10\":\n            pretrained_url = \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-uncond-ve.pkl\"\n        else:\n            raise ValueError(\"Only cifar10 or imagenet64 is supported for now.\")\n        with dnnlib.util.open_url(pretrained_url) as f:\n            pretrained = pickle.load(f)['ema']\n        lib.distributed.barrier()\n\n        network_kwargs = self.get_pretrained_cifar10_network_kwargs()\n        if self.use_imagenet:\n            network_kwargs = self.get_pretrained_imagenet_network_kwargs()\n        label_dim = self.num_classes if self.use_imagenet else 0\n        interface_kwargs = dict(img_resolution=res, img_channels=3, label_dim=label_dim)\n        model = dnnlib.util.construct_class_by_name(**network_kwargs, **interface_kwargs)\n        model.train().requires_grad_(True)\n        misc.copy_params_and_buffers(src_module=pretrained, dst_module=model, require_all=False)\n        del pretrained      # save memory\n\n        self.time_schedule = tuple(int(x) for x in self.params.time_schedule.split(','))\n        steps_per_phase = int_str(FLAGS.train_len) / (FLAGS.batch * (len(self.time_schedule) - 1))\n        ema = self.params.ema_residual ** (1 / steps_per_phase)\n        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - ema))\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=self.params.dropout))\n        self.model = lib.distributed.wrap(model)\n        self.model_eval = lib.optim.ModuleEMA(model, momentum=ema).eval().requires_grad_(False).to(device_id())\n        lib.distributed.barrier()\n\n        # Disable dropout noise for teacher\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n        self.self_teacher = lib.optim.ModuleEMA(model, momentum=self.params.sema).to(device_id())\n        self.self_teacher.eval().requires_grad_(False)\n        self.teacher = copy.deepcopy(model).to(device_id())\n        self.teacher.eval().requires_grad_(False)\n\n        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr, weight_decay=0.0)\n\n        # Setup noise schedule\n        sigma = torch.linspace(self.SIGMA_MIN ** (1 / self.RHO),\n                               self.SIGMA_MAX ** (1 / self.RHO), timesteps, dtype=torch.double).pow(self.RHO)\n        sigma = torch.cat([torch.zeros_like(sigma[:1]), sigma])\n        self.register_buffer('sigma', sigma.to(device()))\n        self.timesteps = timesteps\n\n    def get_pretrained_cifar10_network_kwargs(self):\n        network_kwargs = dnnlib.EasyDict()\n        network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n        network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n        network_kwargs.class_name = 'training.networks.EDMPrecond'\n        network_kwargs.augment_dim = 0\n        network_kwargs.update(dropout=0.0, use_fp16=False)\n        return network_kwargs\n\n    def get_pretrained_imagenet_network_kwargs(self):\n        network_kwargs = dnnlib.EasyDict()\n        network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n        network_kwargs.class_name = 'training.networks.EDMPrecond'\n        network_kwargs.update(dropout=0.0, use_fp16=False)\n        return network_kwargs\n\n    @classmethod\n    def c_in(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return (sigma ** 2 + cls.SIGMA_DATA ** 2) ** -0.5\n\n    @classmethod\n    def c_skip(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return (cls.SIGMA_DATA ** 2) / (sigma ** 2 + cls.SIGMA_DATA ** 2)\n\n    @classmethod\n    def c_out(cls, sigma: torch.Tensor) -> torch.Tensor:\n        return sigma * cls.SIGMA_DATA * (cls.SIGMA_DATA ** 2 + sigma ** 2) ** -0.5\n\n    @staticmethod\n    def c_noise(sigma: torch.Tensor) -> torch.Tensor:\n        return 0.25 * sigma.clamp(1e-20).log()\n\n    def forward(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        step = self.timesteps // self.time_schedule[1]\n        shape = n, self.COLORS, self.params.res, self.params.res\n\n        xt = self.sigma[-1] * torch.randn(shape, generator=generator, dtype=torch.double).to(device())\n        class_labels = (torch.eye(self.num_classes, device=device())[torch.randint(self.num_classes, size=[n], device=device())]) if self.use_imagenet else None\n\n        for t in reversed(range(0, self.timesteps, step)):\n            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n            g = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n            x0 = self.model_eval(xt, g[0], class_labels).to(torch.float64)\n            xt = self.post_xt_x0(xt, x0, g[0], g[1])\n\n        return xt.clamp(-1, 1).float()\n\n    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, sigma: torch.Tensor, sigma1: torch.Tensor) -> torch.Tensor:\n        x0 = torch.clip(out, -1., 1.)\n        eps = (xt - x0) / sigma\n        return torch.nan_to_num(x0 + eps * sigma1)\n\n    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n        if self.num_classes == 1000:    # imagenet\n            y = torch.nn.functional.one_hot(y, self.num_classes).to(y.device)\n        else:\n            y = None\n\n        with torch.no_grad():\n\n            step = self.timesteps // self.time_schedule[1]\n            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n            semi_index = torch.randint(step, index.shape, device=device())\n            ix = index - semi_index, (index - semi_index - 1).clamp(1), index - step\n\n            s = tuple(self.sigma[i].view(-1, 1, 1, 1) for i in ix)\n            noise = torch.randn_like(x).to(device())\n\n            # RK step from teacher\n            xt = x.double() + noise * s[0]\n            x0 = self.teacher(xt, s[0], y)\n            eps = (xt - x0) / s[0]\n            xt_ = xt + (s[1] - s[0]) * eps\n            x0_ = self.teacher(xt_, s[1], y)\n            eps = .5 * (eps + (xt_ - x0_) / s[1])\n            xt_ = xt + (s[1] - s[0]) * eps      # RK target from teacher; no RK needed for sigma_min\n\n            # self-teacher step\n            xt2 = self.post_xt_x0(xt_, self.self_teacher(xt_, s[1], y), s[1], s[2])\n            xt2 += ((semi_index + 1) == step).view(-1, 1, 1, 1) * (xt_ - xt2)   # Only propagate inside phase semi_range\n\n            xt2 = ((xt2 * s[0] - xt * s[2]) / (s[0] - s[2]))\n\n            # Boundary and terminal condition: last time step, no RK and self-teaching needed\n            target_without_precon = torch.where((index - semi_index - 1).view(-1, 1, 1, 1) == 0, x0.double(), xt2.double())\n\n            target = (target_without_precon - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\n        self.opt.zero_grad(set_to_none=True)\n        pred = self.model(xt.float(), s[0].float(), y).double()\n        pred = (pred - self.c_skip(s[0]) * xt) / self.c_out(s[0])\n\n        weight = (s[0] ** 2 + self.SIGMA_DATA ** 2) * (self.c_out(s[0]) ** 2) * (s[0] * self.SIGMA_DATA) ** -2\n        loss = (torch.nn.functional.mse_loss(pred.float(), target.float(), reduction='none')).mean((1, 2, 3))\n        loss = (weight.float() * loss).mean()\n\n        loss.backward()\n\n        # LR warmup and clip gradient like EDM paper\n        if self.params.lr_warmup is not None:\n            for g in self.opt.param_groups:\n                g['lr'] = self.params.lr * min(info.samples / max(int_str(self.params.lr_warmup), 1e-8), 1)\n        for param in self.model.parameters():\n            if param.grad is not None:\n                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n\n        self.opt.step()\n        self.self_teacher.update(self.model)\n        self.model_eval.update(self.model)\n        return {'loss/global': loss}", "\n\ndef check_steps():\n    timesteps = [int(x) for x in FLAGS.time_schedule.split(',')]\n    assert len(timesteps) > 1\n    assert timesteps[0] == FLAGS.timesteps\n    for i in range(len(timesteps) - 1):\n        assert timesteps[i + 1] < timesteps[i]\n\n", "\n\n@lib.distributed.auto_distribute\ndef main(_):\n    check_steps()\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    lib.distributed.barrier()\n    model = EluDDIM05TCMultiStepx0(data.res, FLAGS.timesteps, batch=FLAGS.batch, lr=FLAGS.lr,\n                                   ema_residual=FLAGS.ema_residual, sema=FLAGS.sema, lr_warmup=FLAGS.lr_warmup,\n                                   aug_prob=FLAGS.aug_prob, dropout=FLAGS.dropout, time_schedule=FLAGS.time_schedule)\n    lib.distributed.barrier()\n    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n    train, fid = data.make_dataloaders()\n    model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)", "\n\nif __name__ == '__main__':\n    flags.DEFINE_float('ema_residual', 1e-3, help='Residual for the Exponential Moving Average of model.')\n    flags.DEFINE_float('sema', 0.5, help='Exponential Moving Average of self-teacher.')\n    flags.DEFINE_float('lr', 1e-3, help='Learning rate.')\n    flags.DEFINE_string('lr_warmup', None, help='Warmup for LR in num samples, e.g. 4M')\n    flags.DEFINE_integer('fid_len', 50000, help='Number of samples for FID evaluation.')\n    flags.DEFINE_integer('timesteps', 40, help='Sampling timesteps.')\n    flags.DEFINE_string('time_schedule', None, required=True,\n                        help='Comma separated distillation timesteps, for example: 36,1.')\n    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset. Either cifar10 or imagenet64')\n    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n    flags.DEFINE_float('aug_prob', 0.0, help='Probability of applying data augmentation in training.')\n    flags.DEFINE_float('dropout', 0.0, help='Dropout probability for training.')\n    flags.FLAGS.set_default('report_img_len', '1M')\n    flags.FLAGS.set_default('report_fid_len', '4M')\n    app.run(lib.distributed.main(main))", ""]}
{"filename": "binary_distill.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport copy\nimport functools\nimport math\nimport os\nimport pathlib\nimport shutil", "import pathlib\nimport shutil\nfrom typing import Callable, Dict, Optional\n\nimport torch\nimport torch.nn.functional\nfrom absl import app, flags\n\nimport lib\nfrom lib.distributed import device, device_id, print", "import lib\nfrom lib.distributed import device, device_id, print\nfrom lib.util import FLAGS\nfrom lib.zoo.unet import UNet\n\n\ndef get_model(name: str):\n    if name == 'cifar10':\n        net = UNet(in_channel=3,\n                   channel=256,\n                   emb_channel=1024,\n                   channel_multiplier=[1, 1, 1],\n                   n_res_blocks=3,\n                   attn_rezs=[8, 16],\n                   attn_heads=1,\n                   head_dim=None,\n                   use_affine_time=True,\n                   dropout=0.2,\n                   num_output=1,\n                   resample=True,\n                   num_classes=1)\n    elif name == 'imagenet64':\n        # imagenet model is class conditional\n        net = UNet(in_channel=3,\n                   channel=192,\n                   emb_channel=768,\n                   channel_multiplier=[1, 2, 3, 4],\n                   n_res_blocks=3,\n                   init_rez=64,\n                   attn_rezs=[8, 16, 32],\n                   attn_heads=None,\n                   head_dim=64,\n                   use_affine_time=True,\n                   dropout=0.,\n                   num_output=2,  # predict signal and noise\n                   resample=True,\n                   num_classes=1000)\n    else:\n        raise NotImplementedError(name)\n    return net", "\n\nclass BinaryDistillGoogleModel(lib.train.TrainModel):\n    R_NONE, R_STEP, R_PHASE = 'none', 'step', 'phase'\n    R_ALL = R_NONE, R_STEP, R_PHASE\n\n    def __init__(self, name: str, res: int, timesteps: int, **params):\n        super().__init__(\"GoogleUNet\", res, timesteps, **params)\n        self.num_classes = 1\n        self.shape = 3, res, res\n        self.timesteps = timesteps\n        model = get_model(name)\n        if 'cifar' in name:\n            self.ckpt_path = 'ckpts/cifar_original.pt'\n            self.predict_both = False\n        elif 'imagenet' in name:\n            self.ckpt_path = 'ckpts/imagenet_original.pt'\n            self.predict_both = False\n        elif 'imagenet' in name:\n            self.ckpt_path = 'ckpts/imagenet_original.pt'\n            self.num_classes = 1000\n            self.predict_both = True\n            self.EVAL_COLUMNS = self.EVAL_ROWS = 8\n        else:\n            raise NotImplementedError(name)\n\n        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - self.params.ema))\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n        self.model = lib.distributed.wrap(model)\n        self.model_eval = lib.optim.ModuleEMA(model, momentum=self.params.ema).to(device_id())\n        self.teacher = copy.deepcopy(model).to(device_id())\n        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr)\n        self.register_buffer('phase', torch.zeros((), dtype=torch.long))\n        self.cur_step = self.timesteps // 2\n\n    def initialize_weights_from_teacher(self, logdir: pathlib.Path, teacher_ckpt: Optional[str] = None):\n        teacher_ckpt_path = logdir / 'ckpt/teacher.ckpt'\n        if device_id() == 0:\n            os.makedirs(logdir / 'ckpt', exist_ok=True)\n            shutil.copy2(self.ckpt_path, teacher_ckpt_path)\n\n        lib.distributed.barrier()\n        self.model.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.model_eval.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.self_teacher.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.teacher.load_state_dict(torch.load(teacher_ckpt_path))\n\n    def randn(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        if generator is not None:\n            assert generator.device == torch.device('cpu')\n        return torch.randn((n, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(self.device)\n\n    def call_model(self, model: Callable, xt: torch.Tensor, index: torch.Tensor,\n                   y: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if y is None:\n            return model(xt.float(), index.float()).double()\n        else:\n            return model(xt.float(), index.float(), y.long()).double()\n\n    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        step = self.timesteps // self.cur_step\n        xt = self.randn(samples, generator).to(device_id())\n        if self.num_classes > 1:\n            y = torch.randint(0, self.num_classes, (samples,)).to(xt)\n        else:\n            y = None\n\n        for t in reversed(range(0, self.timesteps, step)):\n            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n            logsnr = tuple(self.logsnr_schedule_cosine(i / self.timesteps).to(xt.double()) for i in ix)\n            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n            x0 = self.call_model(self.model_eval, xt, logsnr[0].repeat(xt.shape[0]), y)\n            xt = self.post_xt_x0(xt, x0, g[0], g[1], clip_x=True)\n        return xt\n\n    @staticmethod\n    def logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n        b = torch.arctan(torch.exp(-0.5 * logsnr_max)).to(t)\n        a = torch.arctan(torch.exp(-0.5 * logsnr_min)).to(t) - b\n        return -2. * torch.log(torch.tan(a * t + b))\n\n    @staticmethod\n    def predict_eps_from_x(z, x, logsnr):\n        \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n        assert logsnr.ndim == x.ndim\n        return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\n    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, g: torch.Tensor, g1: torch.Tensor, clip_x=False) -> torch.Tensor:\n        if self.predict_both:\n            assert out.shape[1] == 6\n            model_x, model_eps = out[:, :3], out[:, 3:]\n            # reconcile the two predictions\n            model_x_eps = (xt - model_eps * (1 - g).sqrt()) * g.rsqrt()\n            wx = 1 - g\n            x0 = wx * model_x + (1. - wx) * model_x_eps\n        else:\n            x0 = out\n        if clip_x:\n            x0 = torch.clip(x0, -1., 1.)\n        eps = (xt - x0 * g.sqrt()) * (1 - g).rsqrt()\n        return torch.nan_to_num(x0 * g1.sqrt() + eps * (1 - g1).sqrt())\n\n    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n        if self.num_classes == 1:\n            y = None\n        else:\n            y = y[:, 0]\n        with torch.no_grad():\n            phase = int(info.progress * (1 - 1e-9) * math.log(self.timesteps, 2))\n            if phase != self.phase:\n                print(f'Refreshing teacher {phase}')\n                self.phase.add_(1)\n                self.teacher.load_state_dict(self.model_eval.module.state_dict())\n                if self.params.reset == self.R_PHASE:\n                    self.model_eval.step.mul_(0)\n                self.cur_step = self.cur_step // 2\n                assert self.cur_step >= 1\n\n            step = self.timesteps // self.cur_step\n            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n            ix = index, index - step // 2, index - step\n            logsnr = tuple(self.logsnr_schedule_cosine(i.double() / self.timesteps).to(x.double()) for i in ix)\n            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n            noise = torch.randn_like(x)\n            xt0 = x.double() * g[0].sqrt() + noise * (1 - g[0]).sqrt()\n            xt1 = self.post_xt_x0(xt0, self.call_model(self.teacher, xt0, logsnr[0], y), g[0], g[1])\n            x_hat = self.call_model(self.teacher, xt1, logsnr[1], y)\n            xt2 = self.post_xt_x0(xt1, x_hat, g[1], g[2])\n            # Find target such that self.post_xt_x0(xt0, target, g[0], g[2]) == xt2\n            target = ((xt0 * (1 - g[2]).sqrt() - xt2 * (1 - g[0]).sqrt()) /\n                      ((g[0] * (1 - g[2])).sqrt() - (g[2] * (1 - g[0])).sqrt()))\n            # use predicted x0 as target when t=0\n            target += (index == step).view(-1, 1, 1, 1) * (x_hat[:, :3] - target)\n\n        self.opt.zero_grad(set_to_none=True)\n        pred = self.call_model(self.model, xt0, logsnr[0], y)\n        if self.predict_both:\n            assert pred.shape[1] == 6\n            model_x, model_eps = pred[:, :3], pred[:, 3:]\n            # reconcile the two predictions\n            model_x_eps = (xt0 - model_eps * (1 - g[0]).sqrt()) * g[0].rsqrt()\n            wx = 1 - g[0]\n            pred_x = wx * model_x + (1. - wx) * model_x_eps\n        else:\n            pred_x = pred\n\n        loss = ((g[0] / (1 - g[0])).clamp(1) * (pred_x - target.detach()).square()).mean(0).sum()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n        self.opt.step()\n        self.model_eval.update(self.model)\n        return {'loss/global': loss, 'stat/timestep': self.cur_step}", "\n\n@ lib.distributed.auto_distribute\ndef main(_):\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    model = BinaryDistillGoogleModel(FLAGS.dataset, data.res, FLAGS.timesteps, reset=FLAGS.reset,\n                                     batch=FLAGS.batch, lr=FLAGS.lr, ema=FLAGS.ema)\n    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n    # resume from previous run (ckpt will be loaded in train.py)\n    if FLAGS.restart_ckpt:\n        lib.distributed.barrier()\n\n    if FLAGS.eval:\n        if not FLAGS.restart_ckpt:\n            model.initialize_weights_from_teacher(logdir, FLAGS.teacher_ckpt)\n        model.eval()\n        with torch.no_grad():\n            generator = torch.Generator(device='cpu')\n            generator.manual_seed(123623113456)\n            model(4, generator)\n    else:\n        train, fid = data.make_dataloaders()\n        if not FLAGS.restart_ckpt:\n            model.initialize_weights_from_teacher(logdir, FLAGS.teacher_ckpt)\n        model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)", "\n\nif __name__ == '__main__':\n    flags.DEFINE_bool('eval', False, help='Whether to run model evaluation.')\n    flags.DEFINE_enum('reset', BinaryDistillGoogleModel.R_NONE, BinaryDistillGoogleModel.R_ALL, help='EMA reset mode.')\n    flags.DEFINE_float('ema', 0.9995, help='Exponential Moving Average of model.')\n    flags.DEFINE_float('lr', 2e-4, help='Learning rate.')\n    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n    flags.FLAGS.set_default('report_img_len', '1M')\n    flags.FLAGS.set_default('report_fid_len', '4M')\n    flags.DEFINE_string('restart_ckpt', None,\n                        help='Trainer checkpoint in the form <taskid>:<ckpt_path> with <ckpt_path> of the form \"ckpt/*.pth\" .')\n    flags.DEFINE_string('teacher_ckpt', None,\n                        help='Teacher checkpoint in the form <taskid>:<ckpt_path> with <ckpt_path> of the form \"ckpt/model_*.ckpt\".')\n    app.run(lib.distributed.main(main))", ""]}
{"filename": "tc_distill.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport copy\nimport functools\nimport os\nimport pathlib\nimport shutil\nfrom typing import Callable, Dict, Optional", "import shutil\nfrom typing import Callable, Dict, Optional\n\nimport torch\nimport torch.nn.functional\nfrom absl import app, flags\n\nimport lib\nfrom lib.distributed import device, device_id, print\nfrom lib.util import FLAGS, int_str", "from lib.distributed import device, device_id, print\nfrom lib.util import FLAGS, int_str\nfrom lib.zoo.unet import UNet\n\n\ndef get_model(name: str):\n    if name == 'cifar10':\n        net = UNet(in_channel=3,\n                   channel=256,\n                   emb_channel=1024,\n                   channel_multiplier=[1, 1, 1],\n                   n_res_blocks=3,\n                   attn_rezs=[8, 16],\n                   attn_heads=1,\n                   head_dim=None,\n                   use_affine_time=True,\n                   dropout=0.2,\n                   num_output=1,\n                   resample=True,\n                   num_classes=1)\n    elif name == 'imagenet64':\n        # imagenet model is class conditional\n        net = UNet(in_channel=3,\n                   channel=192,\n                   emb_channel=768,\n                   channel_multiplier=[1, 2, 3, 4],\n                   n_res_blocks=3,\n                   init_rez=64,\n                   attn_rezs=[8, 16, 32],\n                   attn_heads=None,\n                   head_dim=64,\n                   use_affine_time=True,\n                   dropout=0.,\n                   num_output=2,  # predict signal and noise\n                   resample=True,\n                   num_classes=1000)\n    else:\n        raise NotImplementedError(name)\n    return net", "\n\nclass TCDistillGoogleModel(lib.train.TrainModel):\n    R_NONE, R_STEP, R_PHASE = 'none', 'step', 'phase'\n    R_ALL = R_NONE, R_STEP, R_PHASE\n\n    def __init__(self, name: str, res: int, timesteps: int, **params):\n        super().__init__(\"GoogleUNet\", res, timesteps, **params)\n        self.num_classes = 1\n        self.shape = 3, res, res\n        self.timesteps = timesteps\n        model = get_model(name)\n        if 'cifar' in name:\n            self.ckpt_path = 'ckpts/cifar_original.pt'\n            self.predict_both = False\n        elif 'imagenet' in name:\n            self.ckpt_path = 'ckpts/imagenet_original.pt'\n            self.num_classes = 1000\n            self.predict_both = True\n            self.EVAL_COLUMNS = self.EVAL_ROWS = 8\n        else:\n            raise NotImplementedError(name)\n\n        self.time_schedule = tuple(int(x) for x in self.params.time_schedule.split(','))\n        steps_per_phase = int_str(FLAGS.train_len) / (FLAGS.batch * (len(self.time_schedule) - 1))\n        ema = self.params.ema_residual ** (1 / steps_per_phase)\n        model.apply(functools.partial(lib.nn.functional.set_bn_momentum, momentum=1 - ema))\n        model.apply(functools.partial(lib.nn.functional.set_dropout, p=0))\n        self.model = lib.distributed.wrap(model)\n        self.model_eval = lib.optim.ModuleEMA(model, momentum=ema).to(device_id())\n        self.self_teacher = lib.optim.ModuleEMA(model, momentum=self.params.sema).to(device_id())\n        self.teacher = copy.deepcopy(model).to(device_id())\n        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.params.lr)\n        self.register_buffer('phase', torch.zeros((), dtype=torch.long))\n\n    def initialize_weights_from_teacher(self, logdir: pathlib.Path):\n        teacher_ckpt_path = logdir / 'ckpt/teacher.ckpt'\n        if device_id() == 0:\n            os.makedirs(logdir / 'ckpt', exist_ok=True)\n            shutil.copy2(self.ckpt_path, teacher_ckpt_path)\n\n        lib.distributed.barrier()\n        self.model.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.model_eval.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.self_teacher.module.load_state_dict(torch.load(teacher_ckpt_path))\n        self.teacher.load_state_dict(torch.load(teacher_ckpt_path))\n\n    def randn(self, n: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        if generator is not None:\n            assert generator.device == torch.device('cpu')\n        return torch.randn((n, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(self.device)\n\n    def call_model(self, model: Callable, xt: torch.Tensor, index: torch.Tensor,\n                   y: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if y is None:\n            return model(xt.float(), index.float()).double()\n        else:\n            return model(xt.float(), index.float(), y.long()).double()\n\n    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        step = self.timesteps // self.time_schedule[self.phase.item() + 1]\n        xt = self.randn(samples, generator).to(device_id())\n        if self.num_classes > 1:\n            y = torch.randint(0, self.num_classes, (samples,)).to(xt)\n        else:\n            y = None\n\n        for t in reversed(range(0, self.timesteps, step)):\n            ix = torch.Tensor([t + step]).long().to(device_id()), torch.Tensor([t]).long().to(device_id())\n            logsnr = tuple(self.logsnr_schedule_cosine(i / self.timesteps).to(xt.double()) for i in ix)\n            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n            x0 = self.call_model(self.model_eval, xt, logsnr[0].repeat(xt.shape[0]), y)\n            xt = self.post_xt_x0(xt, x0, g[0], g[1])\n        return xt\n\n    @staticmethod\n    def logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n        b = torch.arctan(torch.exp(-0.5 * logsnr_max)).to(t)\n        a = torch.arctan(torch.exp(-0.5 * logsnr_min)).to(t) - b\n        return -2. * torch.log(torch.tan(a * t + b))\n\n    @staticmethod\n    def predict_eps_from_x(z, x, logsnr):\n        \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n        assert logsnr.ndim == x.ndim\n        return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\n    def post_xt_x0(self, xt: torch.Tensor, out: torch.Tensor, g: torch.Tensor, g1: torch.Tensor) -> torch.Tensor:\n        if self.predict_both:\n            assert out.shape[1] == 6\n            model_x, model_eps = out[:, :3], out[:, 3:]\n            # reconcile the two predictions\n            model_x_eps = (xt - model_eps * (1 - g).sqrt()) * g.rsqrt()\n            wx = 1 - g\n            x0 = wx * model_x + (1. - wx) * model_x_eps\n        else:\n            x0 = out\n        x0 = torch.clip(x0, -1., 1.)\n        eps = (xt - x0 * g.sqrt()) * (1 - g).rsqrt()\n        return torch.nan_to_num(x0 * g1.sqrt() + eps * (1 - g1).sqrt())\n\n    def train_op(self, info: lib.train.TrainInfo, x: torch.Tensor, y: torch.Tensor) -> Dict[str, torch.Tensor]:\n        if self.num_classes == 1:\n            y = None\n        with torch.no_grad():\n            phase = int(info.progress * (1 - 1e-9) * (len(self.time_schedule) - 1))\n            if phase != self.phase:\n                print(f'Refreshing teacher {phase}')\n                self.phase.add_(1)\n                self.teacher.load_state_dict(self.model_eval.module.state_dict())\n                if self.params.reset == self.R_PHASE:\n                    self.model_eval.step.mul_(0)\n            semi_range = self.time_schedule[phase] // self.time_schedule[phase + 1]\n            semi = self.timesteps // self.time_schedule[phase]\n            step = self.timesteps // self.time_schedule[phase + 1]\n            index = torch.randint(1, 1 + (self.timesteps // step), (x.shape[0],), device=device()) * step\n            semi_index = torch.randint(semi_range, index.shape, device=device()) * semi\n            ix = index - semi_index, index - semi_index - semi, index - step\n            logsnr = tuple(self.logsnr_schedule_cosine(i.double() / self.timesteps).to(x.double()) for i in ix)\n            g = tuple(torch.sigmoid(l).view(-1, 1, 1, 1) for l in logsnr)  # Get gamma values\n            noise = torch.randn_like(x)\n            xt0 = x.double() * g[0].sqrt() + noise * (1 - g[0]).sqrt()\n            xt1 = self.post_xt_x0(xt0, self.call_model(self.teacher, xt0, logsnr[0], y), g[0], g[1])\n            xt2 = self.post_xt_x0(xt1, self.call_model(self.self_teacher, xt1, logsnr[1], y), g[1], g[2])\n            xt2 += (semi_index + semi == step).view(-1, 1, 1, 1) * (xt1 - xt2)  # Only propagate inside phase semi_range\n            # Find target such that self.post_xt_x0(xt0, target, g[0], g[2]) == xt2\n            target = ((xt0 * (1 - g[2]).sqrt() - xt2 * (1 - g[0]).sqrt()) /\n                      ((g[0] * (1 - g[2])).sqrt() - (g[2] * (1 - g[0])).sqrt()))\n\n        self.opt.zero_grad(set_to_none=True)\n        pred = self.call_model(self.model, xt0, logsnr[0], y)\n        if self.predict_both:\n            assert pred.shape[1] == 6\n            model_x, model_eps = pred[:, :3], pred[:, 3:]\n            # reconcile the two predictions\n            model_x_eps = (xt0 - model_eps * (1 - g[0]).sqrt()) * g[0].rsqrt()\n            wx = 1 - g[0]\n            pred_x = wx * model_x + (1. - wx) * model_x_eps\n        else:\n            pred_x = pred\n\n        loss = ((g[0] / (1 - g[0])).clamp(1) * (pred_x - target.detach()).square()).mean(0).sum()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.)\n        self.opt.step()\n        self.self_teacher.update(self.model)\n        self.model_eval.update(self.model)\n        return {'loss/global': loss, 'stat/timestep': self.time_schedule[phase + 1]}", "\n\ndef check_steps():\n    timesteps = [int(x) for x in FLAGS.time_schedule.split(',')]\n    assert len(timesteps) > 1\n    for i in range(len(timesteps) - 1):\n        assert timesteps[i + 1] < timesteps[i]\n\n\n@lib.distributed.auto_distribute\ndef main(_):\n    check_steps()\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    model = TCDistillGoogleModel(FLAGS.dataset, data.res, FLAGS.timesteps, reset=FLAGS.reset,\n                                 batch=FLAGS.batch, lr=FLAGS.lr, ema_residual=FLAGS.ema_residual,\n                                 sema=FLAGS.sema, time_schedule=FLAGS.time_schedule)\n    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n    train, fid = data.make_dataloaders()\n    model.initialize_weights_from_teacher(logdir)\n    model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)", "\n@lib.distributed.auto_distribute\ndef main(_):\n    check_steps()\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    model = TCDistillGoogleModel(FLAGS.dataset, data.res, FLAGS.timesteps, reset=FLAGS.reset,\n                                 batch=FLAGS.batch, lr=FLAGS.lr, ema_residual=FLAGS.ema_residual,\n                                 sema=FLAGS.sema, time_schedule=FLAGS.time_schedule)\n    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n    train, fid = data.make_dataloaders()\n    model.initialize_weights_from_teacher(logdir)\n    model.train_loop(train, fid, FLAGS.batch, FLAGS.train_len, FLAGS.report_len, logdir, fid_len=FLAGS.fid_len)", "\n\nif __name__ == '__main__':\n    flags.DEFINE_enum('reset', TCDistillGoogleModel.R_NONE, TCDistillGoogleModel.R_ALL, help='EMA reset mode.')\n    flags.DEFINE_float('ema_residual', 1e-3, help='Residual for the Exponential Moving Average of model.')\n    flags.DEFINE_float('sema', 0.5, help='Exponential Moving Average of self-teacher.')\n    flags.DEFINE_float('lr', 2e-4, help='Learning rate.')\n    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n    flags.DEFINE_string('time_schedule', None, required=True,\n                        help='Comma separated distillation timesteps, for example: 1024,32,1.')\n    flags.DEFINE_string('train_len', '64M', help='Training duration in samples per distillation logstep.')\n    flags.DEFINE_string('report_len', '1M', help='Reporting interval in samples.')\n    flags.FLAGS.set_default('report_img_len', '1M')\n    flags.FLAGS.set_default('report_fid_len', '4M')\n    app.run(lib.distributed.main(main))", ""]}
{"filename": "teacher/download_and_convert_jax.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport os\nimport pathlib\nfrom flax import serialization\nfrom tensorflow.compat.v2.io import gfile\nfrom lib.zoo.unet import UNet\nimport numpy as np", "from lib.zoo.unet import UNet\nimport numpy as np\nimport torch\nimport einops as ei\nfrom absl import app, flags\n\n\ndef to_torch(x):\n    return torch.nn.Parameter(torch.from_numpy(x.copy()))\n", "\n\ndef check_and_convert_gcs_filepath(filepath, raise_if_not_gcs=False):\n    \"\"\"Utility for loading model checkpoints from GCS.\"\"\"\n    local_filepath = filepath.split('/')[-1]\n    if os.path.exists(local_filepath):\n        print('loading from local copy of GCS file: ' + local_filepath)\n    else:\n        print('downloading file from GCS: ' + filepath)\n        os.system('gsutil cp ' + filepath + ' ' + local_filepath)\n    return local_filepath", "\n\ndef restore_from_path(ckpt_path, target):\n    ckpt_path = check_and_convert_gcs_filepath(ckpt_path)\n    with gfile.GFile(ckpt_path, 'rb') as fp:\n        return serialization.from_bytes(target, fp.read())\n\n\ndef convert_conv(module_from, module_to):\n    # PyTorch kernel has shape [outC, inC, kH, kW] and the Flax kernel has shape [kH, kW, inC, outC]\n    module_to.weight = to_torch(module_from['kernel'].transpose(3, 2, 0, 1))\n    module_to.bias = to_torch(module_from['bias'])", "def convert_conv(module_from, module_to):\n    # PyTorch kernel has shape [outC, inC, kH, kW] and the Flax kernel has shape [kH, kW, inC, outC]\n    module_to.weight = to_torch(module_from['kernel'].transpose(3, 2, 0, 1))\n    module_to.bias = to_torch(module_from['bias'])\n\n\ndef convert_conv_after_qkv(module_from, module_to):\n    module_to.weight = to_torch(ei.rearrange(module_from['kernel'], \"nh h f -> f (nh h) 1 1\"))\n    module_to.bias = to_torch(module_from['bias'])\n", "\n\ndef convert_fc(module_from, module_to):\n    # PyTorch kernel has shape [outC, inC] and the Flax kernel has shape [inC, outC]\n    module_to.weight = to_torch(module_from['kernel'].transpose(1, 0))\n    module_to.bias = to_torch(module_from['bias'])\n\n\ndef convert_group_norm(module_from, module_to):\n    module_to.weight = to_torch(module_from['scale'])\n    module_to.bias = to_torch(module_from['bias'])", "def convert_group_norm(module_from, module_to):\n    module_to.weight = to_torch(module_from['scale'])\n    module_to.bias = to_torch(module_from['bias'])\n\n\ndef convert_qkv(module_from_q, module_from_k, module_from_v, module_to):\n    weight = np.concatenate((module_from_q['kernel'], module_from_k['kernel'], module_from_v['kernel']), 2)\n    module_to.weight = to_torch(ei.rearrange(weight, 'f nh h -> (nh h) f 1 1'))\n    bias = np.concatenate((module_from_q['bias'], module_from_k['bias'], module_from_v['bias']), 1)\n    module_to.bias = to_torch(ei.rearrange(bias, 'nh h -> (nh h)'))", "\n\ndef convert1x1conv(module_from, module_to):\n    module_to.weight = to_torch(module_from['kernel'].transpose(1, 0)[:, :, None, None])\n    module_to.bias = to_torch(module_from['bias'])\n\n\ndef convert_res_block(module_from, module_to):\n    convert_group_norm(module_from['norm1'], module_to.norm1)\n    convert_conv(module_from['conv1'], module_to.conv1)\n    convert_fc(module_from['temb_proj'], module_to.time[1])\n    convert_group_norm(module_from['norm2'], module_to.norm2)\n    convert_conv(module_from['conv2'], module_to.conv2)\n    if 'nin_shortcut' in module_from:\n        convert1x1conv(module_from['nin_shortcut'], module_to.skip)", "\n\ndef convert_attention(module_from, module_to):\n    convert_group_norm(module_from['norm'], module_to.norm)\n    convert_qkv(module_from['q'], module_from['k'], module_from['v'], module_to.qkv)\n    convert_conv_after_qkv(module_from['proj_out'], module_to.out)\n\n\ndef convert_down(module_from, module_to, n_down_blocks, n_res_blocks):\n    convert_conv(module_from['conv_in'], module_to[0])\n    module_to_idx = 1\n    for i in range(n_down_blocks):\n        for j in range(n_res_blocks):\n            convert_res_block(module_from[f'down_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n            if f'down_{i}.attn_{j}' in module_from.keys():\n                convert_attention(module_from[f'down_{i}.attn_{j}'], module_to[module_to_idx].attention)\n            module_to_idx += 1\n        # downsample layer is a res block\n        if f'down_{i}.downsample' in module_from.keys():\n            convert_res_block(module_from[f'down_{i}.downsample'], module_to[module_to_idx])\n            module_to_idx += 1\n    assert module_to_idx == len(module_to)", "def convert_down(module_from, module_to, n_down_blocks, n_res_blocks):\n    convert_conv(module_from['conv_in'], module_to[0])\n    module_to_idx = 1\n    for i in range(n_down_blocks):\n        for j in range(n_res_blocks):\n            convert_res_block(module_from[f'down_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n            if f'down_{i}.attn_{j}' in module_from.keys():\n                convert_attention(module_from[f'down_{i}.attn_{j}'], module_to[module_to_idx].attention)\n            module_to_idx += 1\n        # downsample layer is a res block\n        if f'down_{i}.downsample' in module_from.keys():\n            convert_res_block(module_from[f'down_{i}.downsample'], module_to[module_to_idx])\n            module_to_idx += 1\n    assert module_to_idx == len(module_to)", "\n\ndef convert_mid(module_from, module_to):\n    convert_res_block(module_from['mid.block_1'], module_to[0].resblocks)\n    convert_attention(module_from['mid.attn_1'], module_to[0].attention)\n    convert_res_block(module_from['mid.block_2'], module_to[1].resblocks)\n\n\ndef convert_up(module_from, module_to, num_up_blocks, n_res_blocks):\n    module_to_idx = 0\n    for i in reversed(range(num_up_blocks)):\n        for j in range(n_res_blocks + 1):\n            convert_res_block(module_from[f'up_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n            if f'up_{i}.attn_{j}' in module_from.keys():\n                convert_attention(module_from[f'up_{i}.attn_{j}'], module_to[module_to_idx].attention)\n            module_to_idx += 1\n        # upsample layer is a res block\n        if f'up_{i}.upsample' in module_from.keys():\n            convert_res_block(module_from[f'up_{i}.upsample'], module_to[module_to_idx])\n            module_to_idx += 1\n    assert module_to_idx == len(module_to)", "def convert_up(module_from, module_to, num_up_blocks, n_res_blocks):\n    module_to_idx = 0\n    for i in reversed(range(num_up_blocks)):\n        for j in range(n_res_blocks + 1):\n            convert_res_block(module_from[f'up_{i}.block_{j}'], module_to[module_to_idx].resblocks)\n            if f'up_{i}.attn_{j}' in module_from.keys():\n                convert_attention(module_from[f'up_{i}.attn_{j}'], module_to[module_to_idx].attention)\n            module_to_idx += 1\n        # upsample layer is a res block\n        if f'up_{i}.upsample' in module_from.keys():\n            convert_res_block(module_from[f'up_{i}.upsample'], module_to[module_to_idx])\n            module_to_idx += 1\n    assert module_to_idx == len(module_to)", "\n\ndef convert_out(module_from, module_to):\n    convert_group_norm(module_from['norm_out'], module_to[0])\n    convert_conv(module_from['conv_out'], module_to[2])\n\n\ndef convert_time(module_from, module_to):\n    convert_fc(module_from['dense0'], module_to[0])\n    convert_fc(module_from['dense1'], module_to[2])", "\n\ndef convert_class(module_from, module_to):\n    convert_fc(module_from['class_emb'], module_to)\n\n\ndef convert(module_from, module_to, n_down_blocks, n_up_blocks, n_res_blocks, class_conditional=False):\n    # downsample\n    convert_down(module_from['ema_params'], module_to.down, n_down_blocks, n_res_blocks)\n    # mid\n    convert_mid(module_from['ema_params'], module_to.mid)\n    # up\n    convert_up(module_from['ema_params'], module_to.up, n_up_blocks, n_res_blocks)\n    # out\n    convert_out(module_from['ema_params'], module_to.out)\n    # time\n    convert_time(module_from['ema_params'], module_to.time)\n    # class\n    if class_conditional:\n        convert_class(module_from['ema_params'], module_to.class_emb)", "\n\ndef cifar10(path: pathlib.Path):\n    ckpt = restore_from_path('gs://gresearch/diffusion-distillation/cifar_original', None)\n    net = UNet(in_channel=3,\n               channel=256,\n               emb_channel=1024,\n               channel_multiplier=[1, 1, 1],\n               n_res_blocks=3,\n               attn_rezs=[8, 16],\n               attn_heads=1,\n               head_dim=None,\n               use_affine_time=True,\n               dropout=0.2,\n               num_output=1,\n               resample=True,\n               num_classes=1)\n    convert(ckpt, net, n_down_blocks=3, n_up_blocks=3, n_res_blocks=3)\n    # save torch checkpoint\n    torch.save(net.state_dict(), path / 'cifar_original.pt')\n    return net", "\n\ndef imagenet64_conditional(path: pathlib.Path):\n    ckpt = restore_from_path('gs://gresearch/diffusion-distillation/imagenet_original', None)\n    net = UNet(in_channel=3,\n               channel=192,\n               emb_channel=768,\n               channel_multiplier=[1, 2, 3, 4],\n               n_res_blocks=3,\n               init_rez=64,\n               attn_rezs=[8, 16, 32],\n               attn_heads=None,\n               head_dim=64,\n               use_affine_time=True,\n               dropout=0.,\n               num_output=2,  # predict signal and noise\n               resample=True,\n               num_classes=1000)\n    convert(ckpt, net, n_down_blocks=4, n_up_blocks=4, n_res_blocks=3, class_conditional=True)\n    # save torch checkpoint\n    torch.save(net.state_dict(), path / 'imagenet_original.pt')\n    return net", "\n\ndef main(_):\n    path = pathlib.Path(flags.FLAGS.path)\n    os.makedirs(path, exist_ok=True)\n    imagenet64_conditional(path)\n    cifar10(path)\n\n\nif __name__ == '__main__':\n    flags.DEFINE_string('path', './ckpts/', help='Path to save the checkpoints.')\n    app.run(main)", "\nif __name__ == '__main__':\n    flags.DEFINE_string('path', './ckpts/', help='Path to save the checkpoints.')\n    app.run(main)\n"]}
{"filename": "lib/distributed.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\"\"\"\nSingle machine, multi GPU training support\n\"\"\"\n\n__all__ = ['WrapModel', 'auto_distribute', 'barrier', 'device', 'device_id', 'gather_tensor', 'is_master', 'main',\n           'print', 'reduce_dict_mean', 'tqdm', 'tqdm_module', 'tqdm_with', 'trange', 'world_size', 'wrap']", "__all__ = ['WrapModel', 'auto_distribute', 'barrier', 'device', 'device_id', 'gather_tensor', 'is_master', 'main',\n           'print', 'reduce_dict_mean', 'tqdm', 'tqdm_module', 'tqdm_with', 'trange', 'world_size', 'wrap']\n\nimport builtins\nimport contextlib\nimport functools\nimport os\nimport time\nfrom types import SimpleNamespace\nfrom typing import Callable, Dict, Iterable, Optional", "from types import SimpleNamespace\nfrom typing import Callable, Dict, Iterable, Optional\n\nimport torch\nimport torch.distributed\nimport tqdm as tqdm_module\n\nfrom .util import FLAGS, setup\n\n\nclass WrapModel(torch.nn.Module):\n    def __init__(self, m: torch.nn.Module):\n        super().__init__()\n        self.module = m\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)", "\n\nclass WrapModel(torch.nn.Module):\n    def __init__(self, m: torch.nn.Module):\n        super().__init__()\n        self.module = m\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\ndef auto_distribute(f: Callable) -> Callable:\n    \"\"\"Automatically make a function distributed\"\"\"\n\n    @functools.wraps(f)\n    def wrapped(node_rank: Optional[int], world_size: Optional[int], flag_values: SimpleNamespace, *args):\n        if node_rank is None:\n            return f(*args)\n        setup(quiet=True, flags_values=flag_values)\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '12359'\n\n        rank = node_rank\n        torch.distributed.init_process_group('nccl', rank=rank, world_size=torch.cuda.device_count())\n        time.sleep(1)\n        try:\n            return f(*args)\n        finally:\n            torch.distributed.destroy_process_group()\n\n    return wrapped", "\ndef auto_distribute(f: Callable) -> Callable:\n    \"\"\"Automatically make a function distributed\"\"\"\n\n    @functools.wraps(f)\n    def wrapped(node_rank: Optional[int], world_size: Optional[int], flag_values: SimpleNamespace, *args):\n        if node_rank is None:\n            return f(*args)\n        setup(quiet=True, flags_values=flag_values)\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '12359'\n\n        rank = node_rank\n        torch.distributed.init_process_group('nccl', rank=rank, world_size=torch.cuda.device_count())\n        time.sleep(1)\n        try:\n            return f(*args)\n        finally:\n            torch.distributed.destroy_process_group()\n\n    return wrapped", "\n\ndef barrier():\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier()\n\n\ndef device() -> str:\n    return f'cuda:{device_id()}'\n", "\n\ndef device_id() -> int:\n    if not torch.distributed.is_initialized():\n        return 0\n    return torch.distributed.get_rank() % 8\n\n\ndef gather_tensor(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns a concatenated tensor from all the devices.\"\"\"\n    if not torch.distributed.is_initialized():\n        return x\n    x_list = [torch.empty_like(x) for _ in range(world_size())]\n    torch.distributed.all_gather(x_list, x, async_op=False)\n    return torch.cat(x_list, dim=0)", "def gather_tensor(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns a concatenated tensor from all the devices.\"\"\"\n    if not torch.distributed.is_initialized():\n        return x\n    x_list = [torch.empty_like(x) for _ in range(world_size())]\n    torch.distributed.all_gather(x_list, x, async_op=False)\n    return torch.cat(x_list, dim=0)\n\n\ndef is_master() -> bool:\n    return device_id() == 0", "\ndef is_master() -> bool:\n    return device_id() == 0\n\n\ndef main(main_fn: Callable) -> Callable:\n    \"\"\"Main function that automatically handle multiprocessing\"\"\"\n\n    @functools.wraps(main_fn)\n    def wrapped(*args):\n        setup()\n        if torch.cuda.device_count() == 1:\n            return main_fn(None, None, FLAGS, *args)\n        num_gpus = torch.cuda.device_count()\n        torch.multiprocessing.spawn(main_fn, args=(num_gpus, FLAGS, *args), nprocs=num_gpus, join=True)\n\n    return wrapped", "\n\ndef print(*args, **kwargs):\n    if is_master():\n        builtins.print(*args, **kwargs)\n\n\ndef reduce_dict_mean(d: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    \"\"\"Mean reduce the tensor in a dict.\"\"\"\n    if not torch.distributed.is_initialized():\n        return d\n    d = {k: (v if isinstance(v, torch.Tensor) else torch.tensor(v)).to(device_id()) for k, v in d.items()}\n    e = {k: [torch.empty_like(v) for _ in range(world_size())] for k, v in d.items()}\n    # Ideally we should be using all_reduce, but it mysteriously returns incorrect results for the loss\n    [v.wait() for v in [torch.distributed.all_gather(e[k], d[k], async_op=True) for k in d]]\n    return {k: sum(v) / len(v) for k, v in e.items()}", "\n\ndef tqdm(iterable: Iterable, **kwargs) -> Iterable:\n    return tqdm_module.tqdm(iterable, **kwargs)\n\n\ndef tqdm_with(**kwargs) -> Iterable:\n    class Noop:\n        def update(self, *args, **kwargs):\n            pass\n\n    @contextlib.contextmanager\n    def noop():\n        yield Noop()\n\n    return tqdm_module.tqdm(**kwargs)", "\n\ndef trange(*args, **kwargs):\n    return tqdm_module.trange(*args, **kwargs)\n\n\ndef rank() -> int:\n    if not torch.distributed.is_initialized():\n        return 1\n    return torch.distributed.get_rank()", "\n\ndef world_size() -> int:\n    if not torch.distributed.is_initialized():\n        return 1\n    return torch.distributed.get_world_size()\n\n\ndef wrap(m: torch.nn.Module):\n    if not torch.distributed.is_initialized():\n        return WrapModel(m.to(device()))\n    return torch.nn.parallel.DistributedDataParallel(m.to(device_id()), device_ids=[device_id()])", "def wrap(m: torch.nn.Module):\n    if not torch.distributed.is_initialized():\n        return WrapModel(m.to(device()))\n    return torch.nn.parallel.DistributedDataParallel(m.to(device_id()), device_ids=[device_id()])\n"]}
{"filename": "lib/train.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['TrainInfo', 'TrainModel', 'DistillModel']\n\nimport dataclasses\nimport json\nimport pathlib", "import json\nimport pathlib\nimport time\nfrom types import SimpleNamespace\nfrom typing import Callable, Dict, Iterable, List, Optional\n\nimport torch.distributed\nimport torch.nn.functional\nfrom absl import flags\n", "from absl import flags\n\nfrom lib.eval.fid import FID\n\nfrom .distributed import (gather_tensor, is_master, print,\n                          rank, trange, world_size)\nfrom .io import Checkpoint, Summary, SummaryWriter, zip_batch_as_png\nfrom .util import (FLAGS, command_line, int_str, repeater,\n                   report_module_weights, time_format)\n", "                   report_module_weights, time_format)\n\nflags.DEFINE_integer('logstart', 1, help='Logstep at which to start.')\nflags.DEFINE_string('report_fid_len', '16M', help='How often to compute the FID during evaluations.')\nflags.DEFINE_string('report_img_len', '4M', help='How often to sample images during evaluations.')\n\n\n@dataclasses.dataclass\nclass TrainInfo:\n    samples: int\n    progress: float", "class TrainInfo:\n    samples: int\n    progress: float\n\n\nclass TrainModel(torch.nn.Module):\n    COLORS = 3\n    EVAL_ROWS = 16\n    EVAL_COLUMNS = 16\n    model: torch.nn.Module\n    model_eval: torch.nn.Module\n    train_op: Callable[..., Dict[str, torch.Tensor]]\n\n    def __init__(self, arch: str, res: int, timesteps: int, **params):\n        super().__init__()\n        self.params = SimpleNamespace(arch=arch, res=res, timesteps=timesteps, **params)\n        self.register_buffer('logstep', torch.zeros((), dtype=torch.long))\n\n    @property\n    def device(self) -> str:\n        for x in self.model.parameters():\n            return x.device\n\n    @property\n    def logdir(self) -> str:\n        params = '_'.join(f'{k}@{v}' for k, v in sorted(vars(self.params).items()) if k not in ('arch',))\n        return f'{self.__class__.__name__}({self.params.arch})/{params}'\n\n    def __str__(self) -> str:\n        return '\\n'.join((\n            f'{\" Model \":-^80}', str(self.model),\n            f'{\" Parameters \":-^80}', report_module_weights(self.model),\n            f'{\" Config \":-^80}',\n            '\\n'.join(f'{k:20s}: {v}' for k, v in vars(self.params).items())\n        ))\n\n    def save_meta(self, logdir: pathlib.Path, data_logger: Optional[SummaryWriter] = None):\n        if not is_master():\n            return\n        if data_logger is not None:\n            summary = Summary()\n            summary.text('info', f'<pre>{self}</pre>')\n            data_logger.write(summary, 0)\n        (logdir / 'params.json').open('w').write(json.dumps(vars(self.params), indent=4))\n        (logdir / 'model.txt').open('w').write(str(self.model.module))\n        (logdir / 'cmd.txt').open('w').write(command_line())\n\n    def evaluate(self, summary: Summary,\n                 logdir: pathlib.Path,\n                 ckpt: Optional[Checkpoint] = None,\n                 data_fid: Optional[Iterable] = None,\n                 fid_len: int = 0, sample_imgs: bool = True):\n        assert (self.EVAL_ROWS * self.EVAL_COLUMNS) % world_size() == 0\n        self.eval()\n        with torch.no_grad():\n            if sample_imgs:\n                generator = torch.Generator(device='cpu')\n                generator.manual_seed(123623113456 + rank())\n                fixed = self((self.EVAL_ROWS * self.EVAL_COLUMNS) // world_size(), generator)\n                rand = self((self.EVAL_ROWS * self.EVAL_COLUMNS) // world_size())\n                fixed, rand = (gather_tensor(x) for x in (fixed, rand))\n                summary.png('eval/fixed', fixed.view(self.EVAL_ROWS, self.EVAL_COLUMNS, *fixed.shape[1:]))\n                summary.png('eval/random', rand.view(self.EVAL_ROWS, self.EVAL_COLUMNS, *rand.shape[1:]))\n            if fid_len and data_fid:\n                fid = FID(FLAGS.dataset, (self.COLORS, self.params.res, self.params.res))\n                fake_activations, fake_samples = fid.generate_activations_and_samples(self, FLAGS.fid_len)\n                timesteps = self.params.timesteps >> self.logstep.item()\n                zip_batch_as_png(fake_samples, logdir / f'samples_{fid_len}_timesteps_{timesteps}.zip')\n                fidn, fid50 = fid.approximate_fid(fake_activations)\n                summary.scalar(f'eval/fid({fid_len})', fidn)\n                summary.scalar('eval/fid(50000)', fid50)\n                if ckpt:\n                    ckpt.save_file(self.model_eval.module, f'model_{fid50:.5f}.ckpt')\n\n    def train_loop(self,\n                   data_train: Iterable,\n                   data_fid: Optional[Iterable],\n                   batch: int,\n                   train_len: str,\n                   report_len: str,\n                   logdir: pathlib.Path,\n                   *,\n                   fid_len: int = 4096,\n                   keep_ckpts: int = 2):\n        print(self)\n        print(f'logdir: {logdir}')\n        train_len, report_len, report_fid_len, report_img_len = (int_str(x) for x in (\n            train_len, report_len, FLAGS.report_fid_len, FLAGS.report_img_len))\n        assert report_len % batch == 0\n        assert train_len % report_len == 0\n        assert report_fid_len % report_len == 0\n        assert report_img_len % report_len == 0\n        data_train = repeater(data_train)\n        ckpt = Checkpoint(self, logdir, keep_ckpts)\n        start = ckpt.restore()[0]\n        if start:\n            print(f'Resuming training at {start} ({start / (1 << 20):.2f}M samples)')\n\n        with SummaryWriter.create(logdir) as data_logger:\n            if start == 0:\n                self.save_meta(logdir, data_logger)\n\n            for i in range(start, train_len, report_len):\n                self.train()\n                summary = Summary()\n                range_iter = trange(i, i + report_len, batch, leave=False, unit='samples',\n                                    unit_scale=batch,\n                                    desc=f'Training kimg {i >> 10}/{train_len >> 10}')\n                t0 = time.time()\n                for samples in range_iter:\n                    self.train_step(summary, TrainInfo(samples, samples / train_len), next(data_train))\n\n                samples += batch\n                t1 = time.time()\n                summary.scalar('sys/samples_per_sec_train', report_len / (t1 - t0))\n                compute_fid = (samples % report_fid_len == 0) or (samples >= train_len)\n                self.evaluate(summary, logdir, ckpt, data_fid, fid_len=fid_len if compute_fid else 0,\n                              sample_imgs=samples % report_img_len == 0)\n                t2 = time.time()\n                summary.scalar('sys/eval_time', t2 - t1)\n                data_logger.write(summary, samples)\n                ckpt.save(samples)\n                print(f'{samples / (1 << 20):.2f}M/{train_len / (1 << 20):.2f}M samples, '\n                      f'time left {time_format((t2 - t0) * (train_len - samples) / report_len)}\\n{summary}')\n        ckpt.save_file(self.model_eval.module, 'model.ckpt')\n\n    def train_step(self, summary: Summary, info: TrainInfo, batch: List[torch.Tensor]) -> None:\n        device = self.device\n        metrics = self.train_op(info, *[x.to(device, non_blocking=True) for x in batch])\n        summary.from_metrics(metrics)", ""]}
{"filename": "lib/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nfrom . import data  # noqa\nfrom . import distributed  # noqa\nfrom . import eval  # noqa\nfrom . import io  # noqa\nfrom . import nn  # noqa\nfrom . import optim  # noqa", "from . import nn  # noqa\nfrom . import optim  # noqa\nfrom . import train  # noqa\nfrom . import util  # noqa\n"]}
{"filename": "lib/util.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['FLAGS', 'artifact_dir', 'command_line', 'convert_256_to_11', 'cpu_count',\n           'downcast', 'ilog2', 'int_str', 'local_kwargs', 'power_of_2', 'repeater', 'report_module_weights', 'setup',\n           'time_format', 'to_numpy', 'to_png', 'tqdm', 'tqdm_with', 'trange']\n\nimport contextlib", "\nimport contextlib\nimport dataclasses\nimport inspect\nimport io\nimport multiprocessing\nimport os\nimport pathlib\nimport random\nimport re", "import random\nimport re\nimport sys\nfrom types import SimpleNamespace\nfrom typing import Callable, Iterable, Optional, Union\n\nimport absl.flags\nimport numpy as np\nimport torch\nimport torch.backends.cudnn", "import torch\nimport torch.backends.cudnn\nimport tqdm as tqdm_module\nfrom absl import flags\nfrom PIL import Image\n\nFLAGS = SimpleNamespace()\n\nflags.DEFINE_string('logdir', 'e', help='Directory whwer to save logs.')\n", "flags.DEFINE_string('logdir', 'e', help='Directory whwer to save logs.')\n\nSYSTEM_FLAGS = {'?', 'alsologtostderr', 'help', 'helpfull', 'helpshort', 'helpxml', 'log_dir', 'logger_levels',\n                'logtostderr', 'only_check_args', 'pdb', 'pdb_post_mortem', 'profile_file', 'run_with_pdb',\n                'run_with_profiling', 'showprefixforinfo', 'stderrthreshold', 'use_cprofile_for_profiling', 'v',\n                'verbosity'}\n\n@dataclasses.dataclass\nclass MemInfo:\n    total: int  # KB\n    res: int  # KB\n    shared: int  # KB\n\n    @classmethod\n    def query(cls):\n        with open(f'/proc/{os.getpid()}/statm', 'r') as f:\n            return cls(*[int(x) for x in f.read().split(' ')[:3]])\n\n    def __str__(self):\n        gb = 1 << 20\n        return f'Total {self.total / gb:.4f} GB | Res {self.res / gb:.4f} GB | Shared {self.shared / gb:.4f} GB'", "class MemInfo:\n    total: int  # KB\n    res: int  # KB\n    shared: int  # KB\n\n    @classmethod\n    def query(cls):\n        with open(f'/proc/{os.getpid()}/statm', 'r') as f:\n            return cls(*[int(x) for x in f.read().split(' ')[:3]])\n\n    def __str__(self):\n        gb = 1 << 20\n        return f'Total {self.total / gb:.4f} GB | Res {self.res / gb:.4f} GB | Shared {self.shared / gb:.4f} GB'", "\n\ndef artifact_dir(*args) -> pathlib.Path:\n    path = pathlib.Path(FLAGS.logdir)\n    return path.joinpath(*args)\n\n\ndef command_line() -> str:\n    argv = sys.argv[:]\n    rex = re.compile(r'([!|*$#?~&<>{}()\\[\\]\\\\ \"\\'])')\n    cmd = ' '.join(rex.sub(r'\\\\\\1', v) for v in argv)\n    return cmd", "\n\ndef convert_256_to_11(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Lossless conversion of 0,255 interval to -1,1 interval.\"\"\"\n    return x / 128 - 255 / 256\n\n\ndef cpu_count() -> int:\n    return multiprocessing.cpu_count()\n", "\n\ndef downcast(x: Union[np.ndarray, np.dtype]) -> Union[np.ndarray, np.dtype]:\n    \"\"\"Downcast numpy float64 to float32.\"\"\"\n    if isinstance(x, np.dtype):\n        return np.float32 if x == np.float64 else x\n    if x.dtype == np.float64:\n        return x.astype('f')\n    return x\n", "\n\ndef ilog2(x: int) -> int:\n    y = x.bit_length() - 1\n    assert 1 << y == x\n    return y\n\n\ndef int_str(s: str) -> int:\n    p = 1\n    if s.endswith('K'):\n        s, p = s[:-1], 1 << 10\n    elif s.endswith('M'):\n        s, p = s[:-1], 1 << 20\n    elif s.endswith('G'):\n        s, p = s[:-1], 1 << 30\n    return int(float(eval(s)) * p)", "def int_str(s: str) -> int:\n    p = 1\n    if s.endswith('K'):\n        s, p = s[:-1], 1 << 10\n    elif s.endswith('M'):\n        s, p = s[:-1], 1 << 20\n    elif s.endswith('G'):\n        s, p = s[:-1], 1 << 30\n    return int(float(eval(s)) * p)\n", "\n\ndef local_kwargs(kwargs: dict, f: Callable) -> dict:\n    \"\"\"Return the kwargs from dict that are inputs to function f.\"\"\"\n    s = inspect.signature(f)\n    p = s.parameters\n    if next(reversed(p.values())).kind == inspect.Parameter.VAR_KEYWORD:\n        return kwargs\n    if len(kwargs) < len(p):\n        return {k: v for k, v in kwargs.items() if k in p}\n    return {k: kwargs[k] for k in p.keys() if k in kwargs}", "\n\ndef power_of_2(x: int) -> int:\n    \"\"\"Return highest power of 2 <= x\"\"\"\n    return 1 << (x.bit_length() - 1)\n\n\ndef repeater(it: Iterable):\n    \"\"\"Helper function to repeat an iterator in a memory efficient way.\"\"\"\n    while True:\n        for x in it:\n            yield x", "\n\ndef report_module_weights(m: torch.nn.Module):\n    weights = [(k, tuple(v.shape)) for k, v in m.named_parameters()]\n    weights.append((f'Total ({len(weights)})', (sum(np.prod(x[1]) for x in weights),)))\n    width = max(len(x[0]) for x in weights)\n    return '\\n'.join(f'{k:<{width}} {np.prod(s):>10} {str(s):>16}' for k, s in weights)\n\n\ndef setup(seed: Optional[int] = None, quiet: bool = False, flags_values: Optional[SimpleNamespace] = None):\n    if flags_values:\n        for k, v in vars(flags_values).items():\n            setattr(FLAGS, k, v)\n    else:\n        for k in absl.flags.FLAGS:\n            if k not in SYSTEM_FLAGS:\n                setattr(FLAGS, k, getattr(absl.flags.FLAGS, k))\n    torch.backends.cudnn.benchmark = True\n    # os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\n    try:\n        torch.multiprocessing.set_start_method('spawn')\n    except RuntimeError:\n        pass\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    if not quiet:\n        print(f'{\" Flags \":-^79s}')\n        for k in sorted(vars(FLAGS)):\n            print(f'{k:32s}: {getattr(FLAGS, k)}')\n        print(f'{\" System \":-^79s}')\n        for k, v in {'cpus(system)': multiprocessing.cpu_count(),\n                     'cpus(fixed)': cpu_count(),\n                     'multiprocessing.start_method': torch.multiprocessing.get_start_method()}.items():\n            print(f'{k:32s}: {v}')", "\ndef setup(seed: Optional[int] = None, quiet: bool = False, flags_values: Optional[SimpleNamespace] = None):\n    if flags_values:\n        for k, v in vars(flags_values).items():\n            setattr(FLAGS, k, v)\n    else:\n        for k in absl.flags.FLAGS:\n            if k not in SYSTEM_FLAGS:\n                setattr(FLAGS, k, getattr(absl.flags.FLAGS, k))\n    torch.backends.cudnn.benchmark = True\n    # os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'\n    try:\n        torch.multiprocessing.set_start_method('spawn')\n    except RuntimeError:\n        pass\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    if not quiet:\n        print(f'{\" Flags \":-^79s}')\n        for k in sorted(vars(FLAGS)):\n            print(f'{k:32s}: {getattr(FLAGS, k)}')\n        print(f'{\" System \":-^79s}')\n        for k, v in {'cpus(system)': multiprocessing.cpu_count(),\n                     'cpus(fixed)': cpu_count(),\n                     'multiprocessing.start_method': torch.multiprocessing.get_start_method()}.items():\n            print(f'{k:32s}: {v}')", "\n\ndef time_format(t: float) -> str:\n    t = int(t)\n    hours = t // 3600\n    mins = (t // 60) % 60\n    secs = t % 60\n    return f'{hours:02d}:{mins:02d}:{secs:02d}'\n\n\ndef to_numpy(x: Union[np.ndarray, torch.Tensor]):\n    if not isinstance(x, torch.Tensor):\n        return x\n    return x.detach().cpu().numpy()", "\n\ndef to_numpy(x: Union[np.ndarray, torch.Tensor]):\n    if not isinstance(x, torch.Tensor):\n        return x\n    return x.detach().cpu().numpy()\n\n\ndef to_png(x: Union[np.ndarray, torch.Tensor]) -> bytes:\n    \"\"\"Converts numpy array in (C, H, W) or (Rows, Cols, C, H, W) format into PNG format.\"\"\"\n    assert x.ndim in (3, 5)\n    if isinstance(x, torch.Tensor):\n        x = to_numpy(x)\n    if x.ndim == 5:  # Image grid\n        x = np.transpose(x, (2, 0, 3, 1, 4))\n        x = x.reshape((x.shape[0], x.shape[1] * x.shape[2], x.shape[3] * x.shape[4]))  # (C, H, W)\n    if x.dtype in (np.float64, np.float32, np.float16):\n        x = np.transpose(np.round(127.5 * (x + 1)), (1, 2, 0)).clip(0, 255).astype('uint8')\n    elif x.dtype != np.uint8:\n        raise ValueError('Unsupported array type, expecting float or uint8', x.dtype)\n    if x.shape[2] == 1:\n        x = np.broadcast_to(x, x.shape[:2] + (3,))\n    with io.BytesIO() as f:\n        Image.fromarray(x).save(f, 'png')\n        return f.getvalue()", "def to_png(x: Union[np.ndarray, torch.Tensor]) -> bytes:\n    \"\"\"Converts numpy array in (C, H, W) or (Rows, Cols, C, H, W) format into PNG format.\"\"\"\n    assert x.ndim in (3, 5)\n    if isinstance(x, torch.Tensor):\n        x = to_numpy(x)\n    if x.ndim == 5:  # Image grid\n        x = np.transpose(x, (2, 0, 3, 1, 4))\n        x = x.reshape((x.shape[0], x.shape[1] * x.shape[2], x.shape[3] * x.shape[4]))  # (C, H, W)\n    if x.dtype in (np.float64, np.float32, np.float16):\n        x = np.transpose(np.round(127.5 * (x + 1)), (1, 2, 0)).clip(0, 255).astype('uint8')\n    elif x.dtype != np.uint8:\n        raise ValueError('Unsupported array type, expecting float or uint8', x.dtype)\n    if x.shape[2] == 1:\n        x = np.broadcast_to(x, x.shape[:2] + (3,))\n    with io.BytesIO() as f:\n        Image.fromarray(x).save(f, 'png')\n        return f.getvalue()", "\n\ndef tqdm(iterable: Iterable, **kwargs) -> Iterable:\n    return tqdm_module.tqdm(iterable, **kwargs)\n\n\ndef tqdm_with(**kwargs) -> Iterable:\n    class Noop:\n        def update(self, *args, **kwargs):\n            pass\n\n    @contextlib.contextmanager\n    def noop():\n        yield Noop()\n\n    return tqdm_module.tqdm(**kwargs)", "\n\ndef trange(*args, **kwargs):\n    return tqdm_module.trange(*args, **kwargs)\n\n"]}
{"filename": "lib/optim.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport copy\nimport itertools\n\nimport torch\nimport torch.optim.swa_utils\n", "import torch.optim.swa_utils\n\n\nclass ModuleEMA(torch.nn.Module):  # Preferred to PyTorch's builtin because this is pickable\n    def __init__(self, m: torch.nn.Module, momentum: float):\n        super().__init__()\n        self.module = copy.deepcopy(m)\n        self.momentum = momentum\n        self.register_buffer('step', torch.zeros((), dtype=torch.long))\n\n    def update(self, source: torch.nn.Module):\n        self.step.add_(1)\n        decay = (1 - self.momentum) / (1 - self.momentum ** self.step)\n        with torch.no_grad():\n            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n                p_self.add_(p_source - p_self, alpha=decay)\n            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n                if torch.is_floating_point(p_source):\n                    assert torch.is_floating_point(p_self)\n                    p_self.add_(p_source - p_self, alpha=decay)\n                else:\n                    assert not torch.is_floating_point(p_self)\n                    p_self.add_(p_source - p_self)\n\n    def forward(self, *args, **kwargs):\n        return self.module.forward(*args, **kwargs)", "\n\nclass AveragedModel(torch.optim.swa_utils.AveragedModel):\n    def update_parameters(self, model):\n        self_param = itertools.chain(self.module.parameters(), self.module.buffers())\n        model_param = itertools.chain(model.parameters(), model.buffers())\n        for p_swa, p_model in zip(self_param, model_param):\n            device = p_swa.device\n            p_model_ = p_model.detach().to(device)\n            if self.n_averaged == 0:\n                p_swa.detach().copy_(p_model_)\n            else:\n                p_swa.detach().copy_(self.avg_fn(p_swa.detach(), p_model_, self.n_averaged.to(device)))\n        self.n_averaged += 1", "\n\ndef module_exponential_moving_average(model: torch.nn.Module,\n                                      momentum: float) -> torch.optim.swa_utils.AveragedModel:\n    \"\"\"Create an AverageModel using Stochastic Weight Averaging.\n\n    Args:\n        model: the torch Module to average.\n        momentum: the running average momentum coefficient. I found values in 0.9, 0.99, 0.999,\n          0.9999, ... to give good results. The closer to 1 the better, but the longer one needs\n          to train.\n    Returns:\n        torch.optim.swa_utils.AveragedModel module that replicates the model behavior with SWA\n          weights.\n    \"\"\"\n\n    def ema(target: torch.Tensor, source: torch.Tensor, count: int) -> torch.Tensor:\n        mu = 1 - (1 - momentum) / (1 - momentum ** (1 + count))\n        return mu * target + (1 - mu) * source\n\n    return AveragedModel(model, avg_fn=ema)", "\n\nclass HalfLifeEMA(torch.nn.Module):\n    def __init__(self, m: torch.nn.Module, half_life: int = 500000, batch_size: int = 512):\n        \"\"\"\n        EMA Module based of half life (units of samples/images).\n\n        Args:\n            half_life   :   Half life of EMA in units of samples.\n        \"\"\"\n        super().__init__()\n        self.module = copy.deepcopy(m)\n        self.half_life = half_life\n        self.batch_size = batch_size\n\n    def update(self, source: torch.nn.Module):\n        ema_beta = 0.5 ** (self.batch_size / self.half_life)\n\n        with torch.no_grad():\n            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n                p_self.copy_(p_source.lerp(p_self, ema_beta))\n            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n                p_self.copy_(p_source)\n\n    def forward(self, *args, **kwargs):\n        return self.module.forward(*args, **kwargs)", "\n\nclass CopyModule(torch.nn.Module):\n    def __init__(self, m: torch.nn.Module):\n        \"\"\"Copy Module for self-conditioning.\"\"\"\n        super().__init__()\n        self.module = copy.deepcopy(m)\n\n    def update(self, source: torch.nn.Module):\n\n        with torch.no_grad():\n            for p_self, p_source in zip(self.module.parameters(), source.parameters()):\n                p_self.copy_(p_source)\n            for p_self, p_source in zip(self.module.buffers(), source.buffers()):\n                p_self.copy_(p_source)\n\n    def forward(self, *args, **kwargs):\n        return self.module.forward(*args, **kwargs)"]}
{"filename": "lib/io.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['Checkpoint', 'Summary', 'SummaryWriter', 'zip_batch_as_png']\n\nimport enum\nimport io\nimport os", "import io\nimport os\nimport pathlib\nimport zipfile\nfrom time import time\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport imageio\nimport matplotlib.figure\nimport numpy as np", "import matplotlib.figure\nimport numpy as np\nimport torch\nimport torch.nn\nfrom tensorboard.compat.proto import event_pb2, summary_pb2\nfrom tensorboard.summary.writer.event_file_writer import EventFileWriter\nfrom tensorboard.util.tensor_util import make_tensor_proto\n\nfrom .distributed import is_master, print, reduce_dict_mean\nfrom .util import to_numpy, to_png", "from .distributed import is_master, print, reduce_dict_mean\nfrom .util import to_numpy, to_png\n\n\nclass Checkpoint:\n    DIR_NAME: str = 'ckpt'\n    FILE_MATCH: str = '*.pth'\n    FILE_FORMAT: str = '%012d.pth'\n\n    def __init__(self,\n                 model: torch.nn.Module,\n                 logdir: pathlib.Path,\n                 keep_ckpts: int = 0):\n        self.model = model\n        self.logdir = logdir / self.DIR_NAME\n        self.keep_ckpts = keep_ckpts\n\n    @staticmethod\n    def checkpoint_idx(filename: str) -> int:\n        return int(os.path.basename(filename).split('.')[0])\n\n    def restore(self, idx: Optional[int] = None) -> Tuple[int, Optional[pathlib.Path]]:\n        if idx is None:\n            all_ckpts = self.logdir.glob(self.FILE_MATCH)\n            try:\n                idx = self.checkpoint_idx(max(str(x) for x in all_ckpts))\n            except ValueError:\n                return 0, None\n        ckpt = self.logdir / (self.FILE_FORMAT % idx)\n        print(f'Resuming from: {ckpt}')\n        with ckpt.open('rb') as f:\n            self.model.load_state_dict(torch.load(f, map_location='cpu'))\n        return idx, ckpt\n\n    def save(self, idx: int) -> None:\n        if not is_master():  # only save master's state\n            return\n        self.logdir.mkdir(exist_ok=True, parents=True)\n        ckpt = self.logdir / (self.FILE_FORMAT % idx)\n        with ckpt.open('wb') as f:\n            torch.save(self.model.state_dict(), f)\n        old_ckpts = sorted(self.logdir.glob(self.FILE_MATCH), key=str)\n        for ckpt in old_ckpts[:-self.keep_ckpts]:\n            ckpt.unlink()\n\n    def save_file(self, model: torch.nn.Module, filename: str) -> None:\n        if not is_master():  # only save master's state\n            return\n        self.logdir.mkdir(exist_ok=True, parents=True)\n        with (self.logdir / filename).open('wb') as f:\n            torch.save(model.state_dict(), f)", "\nclass Summary(dict):\n    \"\"\"Helper to generate summary_pb2.Summary protobufs.\"\"\"\n\n    # Inspired from https://github.com/google/objax/blob/master/objax/jaxboard.py\n\n    class ProtoMode(enum.Flag):\n        \"\"\"Enum describing what to export to a tensorboard proto.\"\"\"\n\n        IMAGES = enum.auto()\n        VIDEOS = enum.auto()\n        OTHERS = enum.auto()\n        ALL = IMAGES | VIDEOS | OTHERS\n\n    class Scalar:\n        \"\"\"Class for a Summary Scalar.\"\"\"\n\n        def __init__(self, reduce: Callable[[Sequence[float]], float] = np.mean):\n            self.values = []\n            self.reduce = reduce\n\n        def __call__(self):\n            return self.reduce(self.values)\n\n    class Text:\n        \"\"\"Class for a Summary Text.\"\"\"\n\n        def __init__(self, text: str):\n            self.text = text\n\n    class Image:\n        \"\"\"Class for a Summary Image.\"\"\"\n\n        def __init__(self, shape: Tuple[int, int, int], image_bytes: bytes):\n            self.shape = shape  # (C, H, W)\n            self.image_bytes = image_bytes\n\n    class Video:\n        \"\"\"Class for a Summary Video.\"\"\"\n\n        def __init__(self, shape: Tuple[int, int, int], image_bytes: bytes):\n            self.shape = shape  # (C, H, W)\n            self.image_bytes = image_bytes\n\n    def from_metrics(self, metrics: Dict[str, torch.Tensor]):\n        metrics = reduce_dict_mean(metrics)\n        for k, v in metrics.items():\n            v = to_numpy(v)\n            if np.isnan(v):\n                raise ValueError('NaN', k)\n            self.scalar(k, float(v))\n\n    def gif(self, tag: str, imgs: List[np.ndarray]):\n        assert imgs\n        try:\n            height, width, _ = imgs[0].shape\n            vid_save_path = '/tmp/video.gif'\n            imageio.mimsave(vid_save_path, [np.array(img) for i, img in enumerate(imgs) if i % 2 == 0], fps=30)\n            with open(vid_save_path, 'rb') as f:\n                encoded_image_string = f.read()\n            self[tag] = Summary.Video((3, height, width), encoded_image_string)\n        except AttributeError:\n            # the kitchen and hand manipulation envs do not support rendering.\n            return\n\n    def plot(self, tag: str, fig: matplotlib.figure.Figure):\n        byte_data = io.BytesIO()\n        fig.savefig(byte_data, format='png')\n        img_w, img_h = fig.canvas.get_width_height()\n        self[tag] = Summary.Image((4, img_h, img_w), byte_data.getvalue())\n\n    def png(self, tag: str, img: Union[np.ndarray, torch.Tensor]):\n        if img.ndim == 3:\n            shape = (img.shape[2], *img.shape[:2])\n        elif img.ndim == 5:\n            shape = (img.shape[2], img.shape[0] * img.shape[3], img.shape[1] * img.shape[4])\n        else:\n            raise ValueError(f'Unsupported image shape {img.shape}')\n        self[tag] = Summary.Image(shape, to_png(img))\n\n    def scalar(self, tag: str, value: float, reduce: Callable[[Sequence[float]], float] = np.mean):\n        if tag not in self:\n            self[tag] = Summary.Scalar(reduce)\n        self[tag].values.append(value)\n\n    def text(self, tag: str, text: str):\n        self[tag] = Summary.Text(text)\n\n    def proto(self, mode: ProtoMode = ProtoMode.ALL):\n        entries = []\n        for tag, value in self.items():\n            if isinstance(value, Summary.Scalar):\n                if mode & self.ProtoMode.OTHERS:\n                    entries.append(summary_pb2.Summary.Value(tag=tag, simple_value=value()))\n            elif isinstance(value, Summary.Text):\n                if mode & self.ProtoMode.OTHERS:\n                    metadata = summary_pb2.SummaryMetadata(\n                        plugin_data=summary_pb2.SummaryMetadata.PluginData(plugin_name='text'))\n                    entries.append(summary_pb2.Summary.Value(\n                        tag=tag, metadata=metadata,\n                        tensor=make_tensor_proto(values=value.text.encode('utf-8'), shape=(1,))))\n            elif isinstance(value, (Summary.Image, Summary.Video)):\n                if mode & (self.ProtoMode.IMAGES | self.ProtoMode.VIDEOS):\n                    image_summary = summary_pb2.Summary.Image(\n                        encoded_image_string=value.image_bytes,\n                        colorspace=value.shape[0],  # RGBA\n                        height=value.shape[1],\n                        width=value.shape[2])\n                    entries.append(summary_pb2.Summary.Value(tag=tag, image=image_summary))\n            else:\n                raise NotImplementedError(tag, value)\n        return summary_pb2.Summary(value=entries)\n\n    def to_dict(self) -> Dict[str, Any]:\n        entries = {}\n        for tag, value in self.items():\n            if isinstance(value, Summary.Scalar):\n                entries[tag] = float(value())\n            elif isinstance(value, (Summary.Text, Summary.Image, Summary.Video)):\n                pass\n            else:\n                raise NotImplementedError(tag, value)\n        return entries\n\n    def __str__(self) -> str:\n        return '\\n'.join(f'    {k:40s}: {v:.6f}' for k, v in self.to_dict().items())", "\n\nclass SummaryForgetter:\n    \"\"\"Used as placeholder for workers, it basically does nothing.\"\"\"\n\n    def __init__(self,\n                 logdir: pathlib.Path,\n                 queue_size: int = 5,\n                 write_interval: int = 5):\n        self.logdir = logdir\n\n    def write(self, summary: Summary, step: int):\n        pass\n\n    def close(self):\n        \"\"\"Flushes the event file to disk and close the file.\"\"\"\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()", "\n\n# Inspired from https://github.com/google/objax/blob/master/objax/jaxboard.py\nclass SummaryWriter:\n    \"\"\"Writes entries to logdir to be consumed by TensorBoard and Weight & Biases.\"\"\"\n\n    def __init__(self,\n                 logdir: pathlib.Path,\n                 queue_size: int = 5,\n                 write_interval: int = 5):\n        (logdir / 'tb').mkdir(exist_ok=True, parents=True)\n        self.logdir = logdir\n        self.writer = EventFileWriter(logdir / 'tb', queue_size, write_interval)\n        self.writer_image = EventFileWriter(logdir / 'tb', queue_size, write_interval, filename_suffix='images')\n\n    def write(self, summary: Summary, step: int):\n        \"\"\"Add on event to the event file.\"\"\"\n        self.writer.add_event(\n            event_pb2.Event(step=step, summary=summary.proto(summary.ProtoMode.OTHERS),\n                            wall_time=time()))\n        self.writer_image.add_event(\n            event_pb2.Event(step=step, summary=summary.proto(summary.ProtoMode.IMAGES),\n                            wall_time=time()))\n\n    def close(self):\n        \"\"\"Flushes the event file to disk and close the file.\"\"\"\n        self.writer.close()\n        self.writer_image.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    @classmethod\n    def create(cls, logdir: pathlib.Path,\n               queue_size: int = 5,\n               write_interval: int = 5) -> Union[SummaryForgetter, 'SummaryWriter']:\n        if is_master():\n            return cls(logdir, queue_size, write_interval)\n        return SummaryForgetter(logdir, queue_size, write_interval)", "\n\ndef zip_batch_as_png(x: Union[np.ndarray, torch.Tensor], filename: pathlib.Path):\n    if not is_master():\n        return\n    assert x.ndim == 4\n    with zipfile.ZipFile(filename, 'w') as fzip:\n        for i in range(x.shape[0]):\n            with fzip.open(f'{i:06d}.png', 'w') as f:\n                f.write(to_png(x[i]))", ""]}
{"filename": "lib/data/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport os\nimport pathlib\n\nfrom .data import *\nfrom .data_torch import *\n", "from .data_torch import *\n\n\nML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n"]}
{"filename": "lib/data/data.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['Dataset']\n\nfrom typing import Callable\n\nfrom absl import flags", "\nfrom absl import flags\n\nflags.DEFINE_integer('batch', 256, help='Batch size.')\n\n\nclass Dataset:\n    def __init__(self, res: int, make_train: Callable, make_fid: Callable):\n        self.res = res\n        self.make_train = make_train\n        self.make_fid = make_fid", ""]}
{"filename": "lib/data/data_torch.py", "chunked_list": ["# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['make_cifar10', 'make_imagenet64', 'DATASETS']\n\nimport os\nimport pathlib\nfrom typing import Tuple\n", "from typing import Tuple\n\nimport torch\nimport torch.distributed\nimport torch.nn.functional\nimport torchvision.datasets\nimport torchvision.transforms.functional\nfrom lib.util import FLAGS\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose", "from torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose\n\nfrom . import data\n\nML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\n\nclass DatasetTorch(data.Dataset):\n    def make_dataloaders(self, **kwargs) -> Tuple[DataLoader, DataLoader]:\n        batch = FLAGS.batch\n        if torch.distributed.is_initialized():\n            assert batch % torch.distributed.get_world_size() == 0\n            batch //= torch.distributed.get_world_size()\n        return (DataLoader(self.make_train(), shuffle=True, drop_last=True, batch_size=batch,\n                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs),\n                DataLoader(self.make_fid(), shuffle=True, drop_last=True, batch_size=batch,\n                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs))", "class DatasetTorch(data.Dataset):\n    def make_dataloaders(self, **kwargs) -> Tuple[DataLoader, DataLoader]:\n        batch = FLAGS.batch\n        if torch.distributed.is_initialized():\n            assert batch % torch.distributed.get_world_size() == 0\n            batch //= torch.distributed.get_world_size()\n        return (DataLoader(self.make_train(), shuffle=True, drop_last=True, batch_size=batch,\n                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs),\n                DataLoader(self.make_fid(), shuffle=True, drop_last=True, batch_size=batch,\n                           num_workers=4, prefetch_factor=8, persistent_workers=True, **kwargs))", "\n\ndef normalize(x: torch.Tensor) -> torch.Tensor:\n    return 2 * x - 1\n\n\ndef make_cifar10() -> DatasetTorch:\n    transforms = [\n        torchvision.transforms.ToTensor(),\n        normalize,\n    ]\n    transforms_fid = Compose(transforms)\n    transforms_train = Compose(transforms + [torchvision.transforms.RandomHorizontalFlip()])\n    fid = lambda: torchvision.datasets.CIFAR10(str(ML_DATA), train=True, transform=transforms_fid, download=True)\n    train = lambda: torchvision.datasets.CIFAR10(str(ML_DATA), train=True, transform=transforms_train, download=True)\n    return DatasetTorch(32, train, fid)", "\ndef make_imagenet64() -> DatasetTorch:\n    transforms = [\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.CenterCrop(64),\n        normalize,\n    ]\n    transforms_fid = Compose(transforms)\n    transforms_train = Compose(transforms + [torchvision.transforms.RandomHorizontalFlip()])\n    fid = lambda: torchvision.datasets.ImageFolder(str(ML_DATA / \"imagenet\" / \"train\"), transform=transforms_fid)\n    train = lambda: torchvision.datasets.ImageFolder(str(ML_DATA / \"imagenet\" / \"train\"), transform=transforms_train)\n    return DatasetTorch(64, train, fid)", "\n\nDATASETS = {\n    'cifar10': make_cifar10,\n    'imagenet64': make_imagenet64,\n}\n"]}
{"filename": "lib/eval/inception_net.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport os\nimport pathlib\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F", "import torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nimport lib\n\ntry:\n    from torchvision.models.utils import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url", "\n# Inception weights ported to Pytorch from\n# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\nFID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\nFID_WEIGHTS_FILE = 'pt_inception-2015-12-05-6726825d.pth'\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,  # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n                 resize_input=True,\n                 normalize_input=False,\n                 requires_grad=False,\n                 use_fid_inception=True):\n        \"\"\"Build pretrained InceptionV3\n\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, scales the input from range (0, 1) to the range the\n            pretrained Inception network expects, namely (-1, 1)\n        requires_grad : bool\n            If true, parameters of the model require gradients. Possibly useful\n            for finetuning the network\n        use_fid_inception : bool\n            If true, uses the pretrained Inception model used in Tensorflow's\n            FID implementation. If false, uses the pretrained Inception model\n            available in torchvision. The FID Inception model has different\n            weights and a slightly different structure from torchvision's\n            Inception model. If you want to compute FID scores, you are\n            strongly advised to set this parameter to true to get comparable\n            results.\n        \"\"\"\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        if use_fid_inception:\n            inception = fid_inception_v3()\n        else:\n            inception = _inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in\n            range (0, 1)\n\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output\n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.interpolate(x,\n                              size=(299, 299),\n                              mode='bilinear',\n                              align_corners=False)\n\n        if self.normalize_input:\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp", "\n\ndef _inception_v3(*args, **kwargs):\n    \"\"\"Wraps `torchvision.models.inception_v3`\n\n    Skips default weight inititialization if supported by torchvision version.\n    See https://github.com/mseitzer/pytorch-fid/issues/28.\n    \"\"\"\n    try:\n        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n    except ValueError:\n        # Just a caution against weird version strings\n        version = (0,)\n\n    if version >= (0, 6):\n        kwargs['init_weights'] = False\n\n    return torchvision.models.inception_v3(*args, **kwargs)", "\n\ndef fid_inception_v3():\n    \"\"\"Build pretrained Inception model for FID computation\n\n    The Inception model for FID computation uses a different set of weights\n    and has a slightly different structure than torchvision's Inception.\n\n    This method first constructs torchvision's Inception and then patches the\n    necessary parts that are different in the FID Inception model.\n    \"\"\"\n    inception = _inception_v3(num_classes=1008,\n                              aux_logits=False,\n                              pretrained=False)\n    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n    inception.Mixed_7b = FIDInceptionE_1(1280)\n    inception.Mixed_7c = FIDInceptionE_2(2048)\n\n    local_fid_weights = pathlib.Path(lib.data.ML_DATA / os.path.basename(FID_WEIGHTS_URL))\n    if local_fid_weights.is_file():\n        state_dict = torch.load(local_fid_weights)\n    else:\n        state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n    inception.load_state_dict(state_dict)\n    return inception", "\n\nclass FIDInceptionA(torchvision.models.inception.InceptionA):\n    \"\"\"InceptionA block patched for FID computation\"\"\"\n\n    def __init__(self, in_channels, pool_features):\n        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)", "\n\nclass FIDInceptionC(torchvision.models.inception.InceptionC):\n    \"\"\"InceptionC block patched for FID computation\"\"\"\n\n    def __init__(self, in_channels, channels_7x7):\n        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)", "\n\nclass FIDInceptionE_1(torchvision.models.inception.InceptionE):\n    \"\"\"First InceptionE block patched for FID computation\"\"\"\n\n    def __init__(self, in_channels):\n        super(FIDInceptionE_1, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: Tensorflow's average pool does not use the padded zero's in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)", "\n\nclass FIDInceptionE_2(torchvision.models.inception.InceptionE):\n    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n\n    def __init__(self, in_channels):\n        super(FIDInceptionE_2, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: The FID Inception model uses max pooling instead of average\n        # pooling. This is likely an error in this specific Inception\n        # implementation, as other Inception models use average pooling here\n        # (which matches the description in the paper).\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)", ""]}
{"filename": "lib/eval/fid.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\nimport os\nimport pathlib\nfrom typing import Iterable, Tuple\n\nimport numpy as np", "\nimport numpy as np\nimport scipy\nimport torch\nimport torch.nn.functional\nfrom lib.distributed import (barrier, device_id, gather_tensor, is_master,\n                             trange, world_size)\nfrom lib.util import FLAGS, to_numpy\n\nfrom .inception_net import InceptionV3", "\nfrom .inception_net import InceptionV3\n\nML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\n\nclass FID:\n    def __init__(self, dataset: str, shape: Tuple[int, int, int], dims: int = 2048):\n        assert dataset in ('cifar10', 'imagenet64')\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n        self.dims = dims\n        self.shape = shape\n        self.model = InceptionV3([block_idx]).eval().to(device_id())\n        self.post = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1), torch.nn.Flatten())\n        if pathlib.Path(f'{ML_DATA}/{dataset}_activation_mean.npy').exists():\n            self.real_activations_mean = torch.from_numpy(np.load(f'{ML_DATA}/{dataset}_activation_mean.npy'))\n            self.real_activations_std = torch.from_numpy(np.load(f'{ML_DATA}/{dataset}_activation_std.npy'))\n\n    def generate_activations_and_samples(self, model: torch.nn.Module, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        barrier()\n        samples = torch.empty((n, *self.shape))\n        activations = torch.empty((n, self.dims), dtype=torch.double).to(device_id())\n        k = world_size()\n        assert FLAGS.batch % k == 0\n        for i in trange(0, n, FLAGS.batch, desc='Generating FID samples'):\n            p = min(n - i, FLAGS.batch)\n            x = model(FLAGS.batch // k).float()\n            # Discretize to {0,...,255} and project back to [-1,1]\n            x = torch.round(127.5 * (x + 1)).clamp(0, 255) / 127.5 - 1\n            y = self.post(self.model(x)[0])\n            samples[i: i + p] = gather_tensor(x)[:p]\n            activations[i: i + p] = gather_tensor(y)[:p]\n        return activations, samples\n\n    def data_activations(self, iterator: Iterable, n: int, cpu: bool = False) -> torch.Tensor:\n        activations = torch.empty((n, self.dims), dtype=torch.double)\n        if not cpu:\n            activations = activations.to(device_id())\n        k = world_size()\n        it = iter(iterator)\n        for i in trange(0, n, FLAGS.batch, desc='Calculating activations'):\n            x = next(it)[0]\n            p = min((n - i) // k, x.shape[0])\n            y = self.post(self.model(x.to(device_id()))[0])\n            activations[i: i + k * p] = gather_tensor(y[:p]).cpu() if cpu else gather_tensor(y[:p])\n        return activations\n\n    @staticmethod\n    def calculate_activation_statistics(activations: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        return activations.mean(0), torch.cov(activations.T)\n\n    def calculate_fid(self, fake_activations: torch.Tensor) -> float:\n        m_fake, s_fake = self.calculate_activation_statistics(fake_activations)\n        m_real = self.real_activations_mean.to(m_fake)\n        s_real = self.real_activations_std.to(s_fake)\n        return self.calculate_frechet_distance(m_fake, s_fake, m_real, s_real)\n\n    def approximate_fid(self, fake_activations: torch.Tensor, n: int = 50_000) -> Tuple[float, float]:\n        k = fake_activations.shape[0]\n        fid = self.calculate_fid(fake_activations)\n        fid_half = []\n        for it in range(5):\n            sel_fake = np.random.choice(k, k // 2, replace=False)\n            fid_half.append(self.calculate_fid(fake_activations[sel_fake]))\n        fid_half = np.median(fid_half)\n        return fid, fid + (fid_half - fid) * (k / n - 1)\n\n    def calculate_frechet_distance(self, mu1: torch.Tensor, sigma1: torch.Tensor,\n                                   mu2: torch.Tensor, sigma2: torch.Tensor, eps: float = 1e-6) -> float:\n        \"\"\"Numpy implementation of the Frechet Distance.\n        The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n        and X_2 ~ N(mu_2, C_2) is\n                d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n        Stable version by Dougal J. Sutherland.\n        Params:\n        -- mu1   : Numpy array containing the activations of a layer of the\n                   inception net (like returned by the function 'get_predictions')\n                   for generated samples.\n        -- mu2   : The sample mean over activations, precalculated on an\n                   representative data set.\n        -- sigma1: The covariance matrix over activations for generated samples.\n        -- sigma2: The covariance matrix over activations, precalculated on an\n                   representative data set.\n        Returns:\n        --   : The Frechet Distance.\n        \"\"\"\n        if not is_master():\n            return 0\n        mu1, mu2, sigma1, sigma2 = (to_numpy(x) for x in (mu1, mu2, sigma1, sigma2))\n        mu1 = np.atleast_1d(mu1)\n        mu2 = np.atleast_1d(mu2)\n        sigma1 = np.atleast_2d(sigma1)\n        sigma2 = np.atleast_2d(sigma2)\n        assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n        assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n        diff = mu1 - mu2\n\n        # Product might be almost singular\n        covmean = scipy.linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n        if not np.isfinite(covmean).all():\n            print(f'fid calculation produces singular product;  adding {eps} to diagonal of cov estimates')\n            offset = np.eye(sigma1.shape[0]) * eps\n            covmean = scipy.linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n        # Numerical error might give slight imaginary component\n        if np.iscomplexobj(covmean):\n            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n                m = np.max(np.abs(covmean.imag))\n                raise ValueError(f'Imaginary component {m}')\n            covmean = covmean.real\n\n        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)", ""]}
{"filename": "lib/eval/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nfrom .fid import *  # noqa\n"]}
{"filename": "lib/nn/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nfrom . import functional  # noqa\nfrom . import ncsnpp  # noqa\nfrom .nn import *  # noqa\n"]}
{"filename": "lib/nn/nn.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['AutoNorm', 'CondAffinePost', 'CondAffineScaleThenOffset', 'CondLinearlyCombine', 'EMA',\n           'EmbeddingTriangle', 'Residual']\n\nfrom typing import Callable, Optional, Tuple, Sequence\n", "from typing import Callable, Optional, Tuple, Sequence\n\nimport torch\nimport torch.nn\nimport torch.nn.functional\n\nfrom .functional import expand_to\n\n\nclass AutoNorm(torch.nn.Module):\n    def __init__(self, n: int, momentum: float):\n        super().__init__()\n        self.avg = EMA((n,), momentum)\n        self.var = EMA((n,), momentum)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        pad = [1] * (x.ndim - 2)\n        if self.training:\n            reduce = tuple(i for i in range(x.ndim) if i != 1)\n            avg = self.avg(x.mean(reduce))\n            var = self.var((x - x.mean(reduce, keepdims=True)).square().mean(reduce))\n        else:\n            avg, var = self.avg(), self.var()\n\n        return (x - avg.view(1, -1, *pad)) * var.clamp(1e-6).rsqrt().view(1, -1, *pad)\n\n    def denorm(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.training\n        pad = [1] * (x.ndim - 2)\n        avg, var = self.avg(), self.var()\n        return avg.view(1, -1, *pad) + x * var.clamp(1e-6).sqrt().view(1, -1, *pad)", "\nclass AutoNorm(torch.nn.Module):\n    def __init__(self, n: int, momentum: float):\n        super().__init__()\n        self.avg = EMA((n,), momentum)\n        self.var = EMA((n,), momentum)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        pad = [1] * (x.ndim - 2)\n        if self.training:\n            reduce = tuple(i for i in range(x.ndim) if i != 1)\n            avg = self.avg(x.mean(reduce))\n            var = self.var((x - x.mean(reduce, keepdims=True)).square().mean(reduce))\n        else:\n            avg, var = self.avg(), self.var()\n\n        return (x - avg.view(1, -1, *pad)) * var.clamp(1e-6).rsqrt().view(1, -1, *pad)\n\n    def denorm(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.training\n        pad = [1] * (x.ndim - 2)\n        avg, var = self.avg(), self.var()\n        return avg.view(1, -1, *pad) + x * var.clamp(1e-6).sqrt().view(1, -1, *pad)", "\n\nclass CondAffinePost(torch.nn.Module):\n    def __init__(self, ncond: int, nout: int, op: torch.nn.Module, scale: bool = True):\n        super().__init__()\n        self.op = op\n        self.scale = scale\n        self.m = torch.nn.Linear(ncond, nout + (nout if scale else 0))\n        self.cond: Optional[torch.Tensor] = None\n\n    def set_cond(self, x: Optional[torch.Tensor]):\n        self.cond = x if x is None else self.m(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.scale:\n            w, b = expand_to(self.cond, x).chunk(2, dim=1)\n            return self.op(x) * w + b\n        return self.op(x) + expand_to(self.cond, x)", "\n\nclass CondAffineScaleThenOffset(torch.nn.Module):\n    def __init__(self, ncond: int, nin: int, nout: int, op: torch.nn.Module, scale: bool = True):\n        super().__init__()\n        self.op = op\n        self.nin = nin\n        self.scale = scale\n        self.m = torch.nn.Linear(ncond, nout + (nin if scale else 0))\n        self.cond: Optional[torch.Tensor] = None\n\n    def set_cond(self, x: Optional[torch.Tensor]):\n        self.cond = x if x is None else self.m(x)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        cond = expand_to(self.cond, x)\n        if self.scale:\n            w, b = cond[:, :self.nin], cond[:, self.nin:]\n            return self.op(x * w) + b\n        return self.op(x) + cond", "\n\nclass CondLinearlyCombine(torch.nn.Module):\n    def __init__(self, ncond: int, n: int):\n        super().__init__()\n        self.n = n\n        self.mix = torch.nn.Linear(ncond, n)\n        self.cond: Optional[torch.Tensor] = None\n\n    def set_cond(self, x: Optional[torch.Tensor]):\n        self.cond = x if x is None else self.mix(x)\n\n    def forward(self, x: Sequence[torch.Tensor]) -> torch.Tensor:\n        cond = expand_to(self.cond, x[0])\n        return sum(x[i] * cond[:, i:i + 1] for i in range(self.n))", "\n\nclass EMA(torch.nn.Module):\n    def __init__(self, shape: Tuple[int, ...], momentum: float):\n        super().__init__()\n        self.momentum = momentum\n        self.register_buffer('step', torch.zeros((), dtype=torch.long))\n        self.register_buffer('ema', torch.zeros(shape))\n\n    def forward(self, x: Optional[torch.Tensor] = None) -> torch.Tensor:\n        if self.training:\n            self.step.add_(1)\n            mu = 1 - (1 - self.momentum) / (1 - self.momentum ** self.step)\n            self.ema.add_((1 - mu) * (x - self.ema))\n        return self.ema", "\n\nclass EmbeddingTriangle(torch.nn.Module):\n    def __init__(self, dim: int, delta: float):\n        \"\"\"dim number of dimensions for embedding, delta is minimum distance between two values.\"\"\"\n        super().__init__()\n        logres = -torch.tensor(max(2 ** -31, 2 * delta)).log2()\n        logfreqs = torch.nn.functional.pad(torch.linspace(0, logres, dim - 1), (1, 0), mode='constant', value=-1)\n        self.register_buffer('freq', torch.pow(2, logfreqs))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        y = 2 * (x.view(-1, 1) * self.freq).fmod(1)\n        return 2 * (y * (y < 1) + (2 - y) * (y >= 1)) - 1", "\n\nclass Residual(torch.nn.Module):\n    def __init__(self, residual: Callable, skip: Optional[Callable] = None):\n        super().__init__()\n        self.residual = residual\n        self.skip = torch.nn.Identity() if skip is None else skip\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.skip(x) + self.residual(x)", ""]}
{"filename": "lib/nn/ncsnpp/layerspp.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n# This file is adapted and modified from https://github.com/yang-song/score_sde_pytorch.\n\"\"\"Layers for defining NCSN++.\"\"\"\nfrom . import layers\nfrom . import up_or_down_sampling\nimport torch.nn as nn\nimport torch", "import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nconv1x1 = layers.ddpm_conv1x1\nconv3x3 = layers.ddpm_conv3x3\nNIN = layers.NIN\ndefault_init = layers.default_init\n", "default_init = layers.default_init\n\n\nclass GaussianFourierProjection(nn.Module):\n    \"\"\"Gaussian Fourier embeddings for noise levels.\"\"\"\n\n    def __init__(self, embedding_size=256, scale=1.0):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)\n\n    def forward(self, x):\n        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)", "\n\nclass Combine(nn.Module):\n    \"\"\"Combine information from skip connections.\"\"\"\n\n    def __init__(self, dim1, dim2, method='cat'):\n        super().__init__()\n        self.Conv_0 = conv1x1(dim1, dim2)\n        self.method = method\n\n    def forward(self, x, y):\n        h = self.Conv_0(x)\n        if self.method == 'cat':\n            return torch.cat([h, y], dim=1)\n        elif self.method == 'sum':\n            return h + y\n        else:\n            raise ValueError(f'Method {self.method} not recognized.')", "\n\nclass AttnBlockpp(nn.Module):\n    \"\"\"Channel-wise self-attention block. Modified from DDPM.\"\"\"\n\n    def __init__(self, channels, skip_rescale=False, init_scale=0.):\n        super().__init__()\n        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(channels // 4, 32), num_channels=channels,\n                                        eps=1e-6)\n        self.NIN_0 = NIN(channels, channels)\n        self.NIN_1 = NIN(channels, channels)\n        self.NIN_2 = NIN(channels, channels)\n        self.NIN_3 = NIN(channels, channels, init_scale=init_scale)\n        self.skip_rescale = skip_rescale\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        h = self.GroupNorm_0(x)\n        q = self.NIN_0(h)\n        k = self.NIN_1(h)\n        v = self.NIN_2(h)\n\n        w = torch.einsum('bchw,bcij->bhwij', q, k) * (int(C) ** (-0.5))\n        w = torch.reshape(w, (B, H, W, H * W))\n        w = F.softmax(w, dim=-1)\n        w = torch.reshape(w, (B, H, W, H, W))\n        h = torch.einsum('bhwij,bcij->bchw', w, v)\n        h = self.NIN_3(h)\n        if not self.skip_rescale:\n            return x + h\n        else:\n            return (x + h) / np.sqrt(2.)", "\n\nclass Upsample(nn.Module):\n    def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_ch = out_ch if out_ch else in_ch\n        if not fir:\n            if with_conv:\n                self.Conv_0 = conv3x3(in_ch, out_ch)\n        else:\n            if with_conv:\n                self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch, kernel=3, up=True, resample_kernel=fir_kernel, use_bias=True, kernel_init=default_init())\n        self.fir = fir\n        self.with_conv = with_conv\n        self.fir_kernel = fir_kernel\n        self.out_ch = out_ch\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if not self.fir:\n            h = F.interpolate(x, (H * 2, W * 2), 'nearest')\n            if self.with_conv:\n                h = self.Conv_0(h)\n        else:\n            if not self.with_conv:\n                h = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n            else:\n                h = self.Conv2d_0(x)\n\n        return h", "\n\nclass Downsample(nn.Module):\n    def __init__(self, in_ch=None, out_ch=None, with_conv=False, fir=False,\n                 fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_ch = out_ch if out_ch else in_ch\n        if not fir:\n            if with_conv:\n                self.Conv_0 = conv3x3(in_ch, out_ch, stride=2, padding=0)\n        else:\n            if with_conv:\n                self.Conv2d_0 = up_or_down_sampling.Conv2d(in_ch, out_ch, kernel=3, down=True, resample_kernel=fir_kernel, use_bias=True, kernel_init=default_init())\n        self.fir = fir\n        self.fir_kernel = fir_kernel\n        self.with_conv = with_conv\n        self.out_ch = out_ch\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        if not self.fir:\n            if self.with_conv:\n                x = F.pad(x, (0, 1, 0, 1))\n                x = self.Conv_0(x)\n            else:\n                x = F.avg_pool2d(x, 2, stride=2)\n        else:\n            if not self.with_conv:\n                x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n            else:\n                x = self.Conv2d_0(x)\n\n        return x", "\n\nclass ResnetBlockDDPMpp(nn.Module):\n    \"\"\"ResBlock adapted from DDPM.\"\"\"\n\n    def __init__(self, act, in_ch, out_ch=None, temb_dim=None, conv_shortcut=False, dropout=0.1, skip_rescale=False, init_scale=0.):\n        super().__init__()\n        out_ch = out_ch if out_ch else in_ch\n        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n        self.Conv_0 = conv3x3(in_ch, out_ch)\n        if temb_dim is not None:\n            self.Dense_0 = nn.Linear(temb_dim, out_ch)\n            self.Dense_0.weight.data = default_init()(self.Dense_0.weight.data.shape)\n            nn.init.zeros_(self.Dense_0.bias)\n        self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n        self.Dropout_0 = nn.Dropout(dropout)\n        self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n        if in_ch != out_ch:\n            if conv_shortcut:\n                self.Conv_2 = conv3x3(in_ch, out_ch)\n            else:\n                self.NIN_0 = NIN(in_ch, out_ch)\n\n        self.skip_rescale = skip_rescale\n        self.act = act\n        self.out_ch = out_ch\n        self.conv_shortcut = conv_shortcut\n\n    def forward(self, x, temb=None):\n        h = self.act(self.GroupNorm_0(x))\n        h = self.Conv_0(h)\n        if temb is not None:\n            h += self.Dense_0(self.act(temb))[:, :, None, None]\n        h = self.act(self.GroupNorm_1(h))\n        h = self.Dropout_0(h)\n        h = self.Conv_1(h)\n        if x.shape[1] != self.out_ch:\n            if self.conv_shortcut:\n                x = self.Conv_2(x)\n            else:\n                x = self.NIN_0(x)\n        if not self.skip_rescale:\n            return x + h\n        else:\n            return (x + h) / np.sqrt(2.)", "\n\nclass ResnetBlockBigGANpp(nn.Module):\n    def __init__(self, act, in_ch, out_ch=None, temb_dim=None, up=False, down=False,\n                 dropout=0.1, fir=False, fir_kernel=(1, 3, 3, 1),\n                 skip_rescale=True, init_scale=0.):\n        super().__init__()\n\n        out_ch = out_ch if out_ch else in_ch\n        self.GroupNorm_0 = nn.GroupNorm(num_groups=min(in_ch // 4, 32), num_channels=in_ch, eps=1e-6)\n        self.up = up\n        self.down = down\n        self.fir = fir\n        self.fir_kernel = fir_kernel\n\n        self.Conv_0 = conv3x3(in_ch, out_ch)\n        if temb_dim is not None:\n            self.Dense_0 = nn.Linear(temb_dim, out_ch)\n            self.Dense_0.weight.data = default_init()(self.Dense_0.weight.shape)\n            nn.init.zeros_(self.Dense_0.bias)\n\n        self.GroupNorm_1 = nn.GroupNorm(num_groups=min(out_ch // 4, 32), num_channels=out_ch, eps=1e-6)\n        self.Dropout_0 = nn.Dropout(dropout)\n        self.Conv_1 = conv3x3(out_ch, out_ch, init_scale=init_scale)\n        if in_ch != out_ch or up or down:\n            self.Conv_2 = conv1x1(in_ch, out_ch)\n\n        self.skip_rescale = skip_rescale\n        self.act = act\n        self.in_ch = in_ch\n        self.out_ch = out_ch\n\n    def forward(self, x, temb=None):\n        h = self.act(self.GroupNorm_0(x))\n\n        if self.up:\n            if self.fir:\n                h = up_or_down_sampling.upsample_2d(h, self.fir_kernel, factor=2)\n                x = up_or_down_sampling.upsample_2d(x, self.fir_kernel, factor=2)\n            else:\n                h = up_or_down_sampling.naive_upsample_2d(h, factor=2)\n                x = up_or_down_sampling.naive_upsample_2d(x, factor=2)\n        elif self.down:\n            if self.fir:\n                h = up_or_down_sampling.downsample_2d(h, self.fir_kernel, factor=2)\n                x = up_or_down_sampling.downsample_2d(x, self.fir_kernel, factor=2)\n            else:\n                h = up_or_down_sampling.naive_downsample_2d(h, factor=2)\n                x = up_or_down_sampling.naive_downsample_2d(x, factor=2)\n\n        h = self.Conv_0(h)\n        # Add bias to each feature map conditioned on the time embedding\n        if temb is not None:\n            h += self.Dense_0(self.act(temb))[:, :, None, None]\n        h = self.act(self.GroupNorm_1(h))\n        h = self.Dropout_0(h)\n        h = self.Conv_1(h)\n\n        if self.in_ch != self.out_ch or self.up or self.down:\n            x = self.Conv_2(x)\n\n        if not self.skip_rescale:\n            return x + h\n        else:\n            return (x + h) / np.sqrt(2.)", ""]}
{"filename": "lib/nn/ncsnpp/layers.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n# This file is adapted and modified from https://github.com/yang-song/score_sde_pytorch.\nimport math\nimport string\nimport numpy as np\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef variance_scaling(scale, mode, distribution,\n                     in_axis=1, out_axis=0,\n                     dtype=torch.float32,\n                     device='cpu'):\n\n    def _compute_fans(shape, in_axis=1, out_axis=0):\n        receptive_field_size = np.prod(shape) / shape[in_axis] / shape[out_axis]\n        fan_in = shape[in_axis] * receptive_field_size\n        fan_out = shape[out_axis] * receptive_field_size\n        return fan_in, fan_out\n\n    def init(shape, dtype=dtype, device=device):\n        fan_in, fan_out = _compute_fans(shape, in_axis, out_axis)\n        if mode == \"fan_in\":\n            denominator = fan_in\n        elif mode == \"fan_out\":\n            denominator = fan_out\n        elif mode == \"fan_avg\":\n            denominator = (fan_in + fan_out) / 2\n        else:\n            raise ValueError(\n                \"invalid mode for variance scaling initializer: {}\".format(mode))\n        variance = scale / denominator\n        if distribution == \"normal\":\n            return torch.randn(*shape, dtype=dtype, device=device) * np.sqrt(variance)\n        elif distribution == \"uniform\":\n            return (torch.rand(*shape, dtype=dtype, device=device) * 2. - 1.) * np.sqrt(3 * variance)\n        else:\n            raise ValueError(\"invalid distribution for variance scaling initializer\")\n\n    return init", "\n\ndef default_init(scale=1.):\n    \"\"\"Initialize the same way as per DDPM.\"\"\"\n    scale = 1e-10 if scale == 0 else scale\n    return variance_scaling(scale, 'fan_avg', 'uniform')\n\n\ndef get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n    \"\"\"Return timestep embedding for positional embeddings.\"\"\"\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n\n    # magic number 10000 is from transformers\n    emb = math.log(max_positions) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = F.pad(emb, (0, 1), mode='constant')\n\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n\n    return emb", "def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n    \"\"\"Return timestep embedding for positional embeddings.\"\"\"\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n\n    # magic number 10000 is from transformers\n    emb = math.log(max_positions) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = F.pad(emb, (0, 1), mode='constant')\n\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n\n    return emb", "\n\ndef ddpm_conv1x1(in_planes, out_planes, stride=1, bias=True, init_scale=1., padding=0):\n    \"\"\"Return 1x1 convolution with DDPM initialization.\"\"\"\n    conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=padding, bias=bias)\n    conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n    if bias:\n        nn.init.zeros_(conv.bias)\n    return conv\n", "\n\ndef ddpm_conv3x3(in_planes, out_planes, stride=1, bias=True, dilation=1, init_scale=1., padding=1):\n    \"\"\"Return 3x3 convolution with DDPM initialization.\"\"\"\n    conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding,\n                     dilation=dilation, bias=bias)\n    conv.weight.data = default_init(init_scale)(conv.weight.data.shape)\n    if bias:\n        nn.init.zeros_(conv.bias)\n    return conv", "\n\ndef _einsum(a, b, c, x, y):\n    einsum_str = '{},{}->{}'.format(''.join(a), ''.join(b), ''.join(c))\n    return torch.einsum(einsum_str, x, y)\n\n\ndef contract_inner(x, y):\n    \"\"\"Return tensordot(x, y, 1).\"\"\"\n    x_chars = list(string.ascii_lowercase[:len(x.shape)])\n    y_chars = list(string.ascii_lowercase[len(x.shape):len(y.shape) + len(x.shape)])\n    y_chars[0] = x_chars[-1]  # first axis of y and last of x get summed\n    out_chars = x_chars[:-1] + y_chars[1:]\n    return _einsum(x_chars, y_chars, out_chars, x, y)", "\n\nclass NIN(nn.Module):\n    def __init__(self, in_dim, num_units, init_scale=0.1):\n        super().__init__()\n        self.W = nn.Parameter(default_init(scale=init_scale)((in_dim, num_units)), requires_grad=True)\n        self.b = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 3, 1)\n        y = contract_inner(x, self.W) + self.b\n        return y.permute(0, 3, 1, 2)", ""]}
{"filename": "lib/nn/ncsnpp/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#"]}
{"filename": "lib/nn/ncsnpp/up_or_down_sampling.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n# This file is adapted from https://github.com/yang-song/score_sde_pytorch.\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n", "from torch.nn import functional as F\n\n\ndef _setup_kernel(k):\n    k = np.asarray(k, dtype=np.float32)\n    if k.ndim == 1:\n        k = np.outer(k, k)\n    k /= np.sum(k)\n    assert k.ndim == 2\n    assert k.shape[0] == k.shape[1]\n    return k", "\n\ndef _shape(x, dim):\n    return x.shape[dim]\n\n\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n    r\"\"\"Pad, upsample, filter, and downsample a batch of 2D images.\n\n    Performs the following sequence of operations for each channel:\n\n    1. Upsample the input by inserting N-1 zeros after each pixel (`up`).\n\n    2. Pad the image with the specified number of zeros on each side (`padding`).\n       Negative padding corresponds to cropping the image.\n\n    3. Convolve the image with the specified 2D FIR filter (`f`), shrinking it\n       so that the footprint of all output pixels lies within the input image.\n\n    4. Downsample the image by keeping every Nth pixel (`down`).\n\n    This sequence of operations bears close resemblance to scipy.signal.upfirdn().\n    It supports gradients of arbitrary order.\n\n    Args:\n        input:       Float32/float64/float16 input tensor of the shape\n                     `[batch_size, num_channels, in_height, in_width]`.\n        kernel:      Float32 FIR filter of the shape\n                     `[filter_height, filter_width]` called from _setup_kernel.\n        up:          Integer upsampling factor.\n        down:        Integer downsampling factor.\n        pad:         Padding with respect to the upsampled image. list/tuple `[x, y]`.\n\n    Returns:\n        Tensor of the shape `[batch_size, num_channels, out_height, out_width]`.\n    \"\"\"\n    def upfirdn2d_native(\n        input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n    ):\n        _, channel, in_h, in_w = input.shape\n        input = input.reshape(-1, in_h, in_w, 1)\n\n        _, in_h, in_w, minor = input.shape\n        kernel_h, kernel_w = kernel.shape\n\n        out = input.view(-1, in_h, 1, in_w, 1, minor)\n        out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n        out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n\n        out = F.pad(\n            out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n        )\n        out = out[\n            :,\n            max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\n            max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\n            :,\n        ]\n\n        out = out.permute(0, 3, 1, 2)\n        out = out.reshape(\n            [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n        )\n        w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n        out = F.conv2d(out, w)\n        out = out.reshape(\n            -1,\n            minor,\n            in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n            in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n        )\n        out = out.permute(0, 2, 3, 1)\n        out = out[:, ::down_y, ::down_x, :]\n\n        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n\n        return out.view(-1, channel, out_h, out_w)\n\n    out = upfirdn2d_native(\n        input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1]\n    )\n\n    return out", "\n\n# Function ported from StyleGAN2\ndef get_weight(module,\n               shape,\n               weight_var='weight',\n               kernel_init=None):\n    \"\"\"Get/create weight tensor for a convolution or fully-connected layer.\"\"\"\n    return module.param(weight_var, kernel_init, shape)\n", "\n\nclass Conv2d(nn.Module):\n    \"\"\"Conv2d layer with optimal upsampling and downsampling (StyleGAN2).\"\"\"\n\n    def __init__(self, in_ch, out_ch, kernel, up=False, down=False,\n                 resample_kernel=(1, 3, 3, 1),\n                 use_bias=True,\n                 kernel_init=None):\n        super().__init__()\n        assert not (up and down)\n        assert kernel >= 1 and kernel % 2 == 1\n        self.weight = nn.Parameter(torch.zeros(out_ch, in_ch, kernel, kernel))\n        if kernel_init is not None:\n            self.weight.data = kernel_init(self.weight.data.shape)\n        if use_bias:\n            self.bias = nn.Parameter(torch.zeros(out_ch))\n\n        self.up = up\n        self.down = down\n        self.resample_kernel = resample_kernel\n        self.kernel = kernel\n        self.use_bias = use_bias\n\n    def forward(self, x):\n        if self.up:\n            x = upsample_conv_2d(x, self.weight, k=self.resample_kernel)\n        elif self.down:\n            x = conv_downsample_2d(x, self.weight, k=self.resample_kernel)\n        else:\n            x = F.conv2d(x, self.weight, stride=1, padding=self.kernel // 2)\n\n        if self.use_bias:\n            x = x + self.bias.reshape(1, -1, 1, 1)\n\n        return x", "\n\ndef naive_upsample_2d(x, factor=2):\n    _N, C, H, W = x.shape\n    x = torch.reshape(x, (-1, C, H, 1, W, 1))\n    x = x.repeat(1, 1, 1, factor, 1, factor)\n    return torch.reshape(x, (-1, C, H * factor, W * factor))\n\n\ndef naive_downsample_2d(x, factor=2):\n    _N, C, H, W = x.shape\n    x = torch.reshape(x, (-1, C, H // factor, factor, W // factor, factor))\n    return torch.mean(x, dim=(3, 5))", "\ndef naive_downsample_2d(x, factor=2):\n    _N, C, H, W = x.shape\n    x = torch.reshape(x, (-1, C, H // factor, factor, W // factor, factor))\n    return torch.mean(x, dim=(3, 5))\n\n\ndef upsample_conv_2d(x, w, k=None, factor=2, gain=1):\n    \"\"\"Fused `upsample_2d()` followed by `tf.nn.conv2d()`.\n\n    Padding is performed only once at the beginning, not between the\n    operations.\n    The fused op is considerably more efficient than performing the same\n    calculation\n    using standard TensorFlow ops. It supports gradients of arbitrary order.\n    Args:\n        x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n            C]`.\n        w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n            outChannels]`. Grouped convolution can be performed by `inChannels =\n            x.shape[0] // numGroups`.\n        k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n            (separable). The default is `[1] * factor`, which corresponds to\n            nearest-neighbor upsampling.\n        factor:       Integer upsampling factor (default: 2).\n        gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n        Tensor of the shape `[N, C, H * factor, W * factor]` or\n        `[N, H * factor, W * factor, C]`, and same datatype as `x`.\n    \"\"\"\n    assert isinstance(factor, int) and factor >= 1\n\n    # Check weight shape.\n    assert len(w.shape) == 4\n    convH = w.shape[2]\n    convW = w.shape[3]\n    inC = w.shape[1]\n    w.shape[0]\n\n    assert convW == convH\n\n    # Setup filter kernel.\n    if k is None:\n        k = [1] * factor\n    k = _setup_kernel(k) * (gain * (factor ** 2))\n    p = (k.shape[0] - factor) - (convW - 1)\n\n    stride = (factor, factor)\n\n    # Determine data dimensions.\n    stride = [1, 1, factor, factor]\n    output_shape = ((_shape(x, 2) - 1) * factor + convH, (_shape(x, 3) - 1) * factor + convW)\n    output_padding = (output_shape[0] - (_shape(x, 2) - 1) * stride[0] - convH,\n                                        output_shape[1] - (_shape(x, 3) - 1) * stride[1] - convW)\n    assert output_padding[0] >= 0 and output_padding[1] >= 0\n    num_groups = _shape(x, 1) // inC\n\n    # Transpose weights.\n    w = torch.reshape(w, (num_groups, -1, inC, convH, convW))\n    w = w[..., ::-1, ::-1].permute(0, 2, 1, 3, 4)\n    w = torch.reshape(w, (num_groups * inC, -1, convH, convW))\n\n    x = F.conv_transpose2d(x, w, stride=stride, output_padding=output_padding, padding=0)\n\n    return upfirdn2d(x, torch.tensor(k, device=x.device),\n                     pad=((p + 1) // 2 + factor - 1, p // 2 + 1))", "\n\ndef conv_downsample_2d(x, w, k=None, factor=2, gain=1):\n    \"\"\"Fused `tf.nn.conv2d()` followed by `downsample_2d()`.\n\n    Padding is performed only once at the beginning, not between the operations.\n    The fused op is considerably more efficient than performing the same\n    calculation\n    using standard TensorFlow ops. It supports gradients of arbitrary order.\n    Args:\n            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n                C]`.\n            w:            Weight tensor of the shape `[filterH, filterW, inChannels,\n                outChannels]`. Grouped convolution can be performed by `inChannels =\n                x.shape[0] // numGroups`.\n            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to\n                average pooling.\n            factor:       Integer downsampling factor (default: 2).\n            gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n            Tensor of the shape `[N, C, H // factor, W // factor]` or\n            `[N, H // factor, W // factor, C]`, and same datatype as `x`.\n    \"\"\"\n    assert isinstance(factor, int) and factor >= 1\n    _outC, _inC, convH, convW = w.shape\n    assert convW == convH\n    if k is None:\n        k = [1] * factor\n    k = _setup_kernel(k) * gain\n    p = (k.shape[0] - factor) + (convW - 1)\n    s = [factor, factor]\n    x = upfirdn2d(x, torch.tensor(k, device=x.device),\n                  pad=((p + 1) // 2, p // 2))\n    return F.conv2d(x, w, stride=s, padding=0)", "\n\ndef upsample_2d(x, k=None, factor=2, gain=1):\n    r\"\"\"Upsample a batch of 2D images with the given filter.\n\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and upsamples each image with the given filter. The filter is normalized so\n    that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the upsampling factor.\n    Args:\n            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n                C]`.\n            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to\n                nearest-neighbor upsampling.\n            factor:       Integer upsampling factor (default: 2).\n            gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n            Tensor of the shape `[N, C, H * factor, W * factor]`\n    \"\"\"\n    assert isinstance(factor, int) and factor >= 1\n    if k is None:\n        k = [1] * factor\n    k = _setup_kernel(k) * (gain * (factor ** 2))\n    p = k.shape[0] - factor\n    return upfirdn2d(x,torch.tensor(k, device=x.device),\n                     up=factor, pad=((p + 1) // 2 + factor - 1, p // 2))", "\n\ndef downsample_2d(x, k=None, factor=2, gain=1):\n    r\"\"\"Downsample a batch of 2D images with the given filter.\n\n    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]`\n    and downsamples each image with the given filter. The filter is normalized\n    so that\n    if the input pixels are constant, they will be scaled by the specified\n    `gain`.\n    Pixels outside the image are assumed to be zero, and the filter is padded\n    with\n    zeros so that its shape is a multiple of the downsampling factor.\n    Args:\n            x:            Input tensor of the shape `[N, C, H, W]` or `[N, H, W,\n                C]`.\n            k:            FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to\n                average pooling.\n            factor:       Integer downsampling factor (default: 2).\n            gain:         Scaling factor for signal magnitude (default: 1.0).\n    Returns:\n            Tensor of the shape `[N, C, H // factor, W // factor]`\n    \"\"\"\n\n    assert isinstance(factor, int) and factor >= 1\n    if k is None:\n        k = [1] * factor\n    k = _setup_kernel(k) * gain\n    p = k.shape[0] - factor\n    return upfirdn2d(x, torch.tensor(k, device=x.device),\n                     down=factor, pad=((p + 1) // 2, p // 2))", ""]}
{"filename": "lib/nn/functional/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nfrom .functional import *  # noqa\n"]}
{"filename": "lib/nn/functional/functional.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n__all__ = ['expand_to', 'float_index', 'label_smoothing', 'set_bn_momentum', 'set_cond', 'set_dropout',\n           'default', 'log']\n\nfrom typing import Optional\n", "from typing import Optional\n\nimport torch\nimport torch.nn.functional\n\n\ndef expand_to(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    \"\"\"Expand x to the number of dimensions in y.\"\"\"\n    return x.view(x.shape + (1,) * (y.ndim - x.ndim))\n", "\n\ndef float_index(x: torch.Tensor, i: torch.Tensor) -> torch.Tensor:\n    a, b = x[i.long()], x[i.ceil().long()]\n    return a + i.frac() * (b - a)\n\n\ndef label_smoothing(x: torch.Tensor, q: float) -> torch.Tensor:\n    u = torch.zeros_like(x) + 1 / x.shape[-1]\n    return x + q * (u - x)", "\n\ndef set_bn_momentum(m: torch.nn.Module, momentum: float):\n    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n        print('Set momentum for', m)\n        m.momentum = momentum\n\n\ndef set_cond(cond: Optional[torch.Tensor]):\n    def apply_op(m: torch.nn.Module):\n        if hasattr(m, 'set_cond'):\n            m.set_cond(cond)\n\n    return apply_op", "def set_cond(cond: Optional[torch.Tensor]):\n    def apply_op(m: torch.nn.Module):\n        if hasattr(m, 'set_cond'):\n            m.set_cond(cond)\n\n    return apply_op\n\n\ndef set_dropout(m: torch.nn.Module, p: float):\n    if isinstance(m, torch.nn.modules.dropout._DropoutNd):\n        print(f'Set dropout to {p} for', m)\n        m.p = p", "def set_dropout(m: torch.nn.Module, p: float):\n    if isinstance(m, torch.nn.modules.dropout._DropoutNd):\n        print(f'Set dropout to {p} for', m)\n        m.p = p\n\n\ndef default(val, d):\n    if val is not None:\n        return val\n    return d() if callable(d) else d", "\n\ndef log(t, eps=1e-20):\n    return torch.log(t.clamp(min=eps))\n"]}
{"filename": "lib/zoo/unet.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\n# Mostly copied from https://github.com/rosinality/denoising-diffusion-pytorch\n# modified to match https://github.com/google-research/google-research/blob/master/diffusion_distillation/diffusion_distillation/unet.py\n\nimport math\nfrom typing import List, Tuple, Optional", "import math\nfrom typing import List, Tuple, Optional\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nswish = F.silu\n\n\ndef get_timestep_embedding(timesteps, embedding_dim, max_time=1000.):\n    \"\"\"Build sinusoidal embeddings (from Fairseq).\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    Args:\n      timesteps: jnp.ndarray: generate embedding vectors at these timesteps\n      embedding_dim: int: dimension of the embeddings to generate\n      max_time: float: largest time input\n      dtype: data type of the generated embeddings\n    Returns:\n      embedding vectors with shape `(len(timesteps), embedding_dim)`\n    \"\"\"\n    assert len(timesteps.shape) == 1\n    timesteps *= (1000. / max_time)\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :].to(timesteps)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb", "\n\ndef get_timestep_embedding(timesteps, embedding_dim, max_time=1000.):\n    \"\"\"Build sinusoidal embeddings (from Fairseq).\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    Args:\n      timesteps: jnp.ndarray: generate embedding vectors at these timesteps\n      embedding_dim: int: dimension of the embeddings to generate\n      max_time: float: largest time input\n      dtype: data type of the generated embeddings\n    Returns:\n      embedding vectors with shape `(len(timesteps), embedding_dim)`\n    \"\"\"\n    assert len(timesteps.shape) == 1\n    timesteps *= (1000. / max_time)\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :].to(timesteps)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb", "\n\n@torch.no_grad()\ndef variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n\n    if mode == \"fan_in\":\n        scale /= fan_in\n\n    elif mode == \"fan_out\":\n        scale /= fan_out\n\n    else:\n        scale /= (fan_in + fan_out) / 2\n\n    if distribution == \"normal\":\n        std = math.sqrt(scale)\n\n        return tensor.normal_(0, std)\n\n    else:\n        bound = math.sqrt(3 * scale)\n\n        return tensor.uniform_(-bound, bound)", "\n\ndef conv2d(\n        in_channel,\n        out_channel,\n        kernel_size,\n        stride=1,\n        padding=0,\n        bias=True,\n        scale=1,\n        mode=\"fan_avg\",\n):\n    conv = nn.Conv2d(\n        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n    )\n\n    variance_scaling_init_(conv.weight, scale, mode=mode)\n\n    if bias:\n        nn.init.zeros_(conv.bias)\n\n    return conv", "\n\ndef linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n    lin = nn.Linear(in_channel, out_channel)\n\n    variance_scaling_init_(lin.weight, scale, mode=mode)\n    nn.init.zeros_(lin.bias)\n\n    return lin\n", "\n\nclass Swish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return swish(input)\n\n\nclass Upsample(nn.Sequential):\n    def __init__(self, channel):\n        layers = [\n            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n            conv2d(channel, channel, 3, padding=1),\n        ]\n\n        super().__init__(*layers)", "\n\nclass Upsample(nn.Sequential):\n    def __init__(self, channel):\n        layers = [\n            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n            conv2d(channel, channel, 3, padding=1),\n        ]\n\n        super().__init__(*layers)", "\n\nclass Downsample(nn.Sequential):\n    def __init__(self, channel):\n        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n\n        super().__init__(*layers)\n\n\nclass ResBlock(nn.Module):\n    def __init__(\n            self, in_channel, out_channel, time_dim, resample, use_affine_time=False, dropout=0, group_norm=32\n    ):\n        super().__init__()\n\n        self.use_affine_time = use_affine_time\n        self.resample = resample\n        time_out_dim = out_channel\n        time_scale = 1\n\n        if self.use_affine_time:\n            time_out_dim *= 2\n            time_scale = 1e-10\n\n        self.norm1 = nn.GroupNorm(group_norm, in_channel)\n        self.activation1 = Swish()\n        if self.resample:\n            self.updown = {\n                'up':  nn.Upsample(scale_factor=2, mode=\"nearest\"),\n                'down': nn.AvgPool2d(kernel_size=2, stride=2)\n            }[self.resample]\n\n        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n\n        self.time = nn.Sequential(\n            Swish(), linear(time_dim, time_out_dim, scale=time_scale)\n        )\n\n        self.norm2 = nn.GroupNorm(group_norm, out_channel)\n        self.activation2 = Swish()\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n\n        if in_channel != out_channel:\n            self.skip = conv2d(in_channel, out_channel, 1)\n\n        else:\n            self.skip = None\n\n    def forward(self, input, time):\n        batch = input.shape[0]\n        out = self.norm1(input)\n        out = self.activation1(out)\n\n        if self.resample:\n            out = self.updown(out)\n            input = self.updown(input)\n\n        out = self.conv1(out)\n\n        if self.use_affine_time:\n            gamma, beta = self.time(time).view(batch, -1, 1, 1).chunk(2, dim=1)\n            out = (1 + gamma) * self.norm2(out) + beta\n        else:\n            out = out + self.time(time).view(batch, -1, 1, 1)\n            out = self.norm2(out)\n\n        out = self.conv2(self.dropout(self.activation2(out)))\n\n        if self.skip is not None:\n            input = self.skip(input)\n\n        return out + input", "\nclass ResBlock(nn.Module):\n    def __init__(\n            self, in_channel, out_channel, time_dim, resample, use_affine_time=False, dropout=0, group_norm=32\n    ):\n        super().__init__()\n\n        self.use_affine_time = use_affine_time\n        self.resample = resample\n        time_out_dim = out_channel\n        time_scale = 1\n\n        if self.use_affine_time:\n            time_out_dim *= 2\n            time_scale = 1e-10\n\n        self.norm1 = nn.GroupNorm(group_norm, in_channel)\n        self.activation1 = Swish()\n        if self.resample:\n            self.updown = {\n                'up':  nn.Upsample(scale_factor=2, mode=\"nearest\"),\n                'down': nn.AvgPool2d(kernel_size=2, stride=2)\n            }[self.resample]\n\n        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n\n        self.time = nn.Sequential(\n            Swish(), linear(time_dim, time_out_dim, scale=time_scale)\n        )\n\n        self.norm2 = nn.GroupNorm(group_norm, out_channel)\n        self.activation2 = Swish()\n        self.dropout = nn.Dropout(dropout)\n        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n\n        if in_channel != out_channel:\n            self.skip = conv2d(in_channel, out_channel, 1)\n\n        else:\n            self.skip = None\n\n    def forward(self, input, time):\n        batch = input.shape[0]\n        out = self.norm1(input)\n        out = self.activation1(out)\n\n        if self.resample:\n            out = self.updown(out)\n            input = self.updown(input)\n\n        out = self.conv1(out)\n\n        if self.use_affine_time:\n            gamma, beta = self.time(time).view(batch, -1, 1, 1).chunk(2, dim=1)\n            out = (1 + gamma) * self.norm2(out) + beta\n        else:\n            out = out + self.time(time).view(batch, -1, 1, 1)\n            out = self.norm2(out)\n\n        out = self.conv2(self.dropout(self.activation2(out)))\n\n        if self.skip is not None:\n            input = self.skip(input)\n\n        return out + input", "\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channel, n_head=1, head_dim=None, group_norm=32):\n        super().__init__()\n\n        if head_dim is None:\n            assert n_head is not None\n            assert in_channel % n_head == 0\n            self.n_head = n_head\n            self.head_dim = in_channel // n_head\n        else:\n            assert n_head is None\n            assert in_channel % head_dim == 0\n            self.head_dim = head_dim\n            self.n_head = in_channel // head_dim\n\n        self.norm = nn.GroupNorm(group_norm, in_channel)\n        self.qkv = conv2d(in_channel, in_channel * 3, 1)\n        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n\n    def forward(self, input):\n        batch, channel, height, width = input.shape\n        norm = self.norm(input)\n        qkv = self.qkv(norm).view(batch, self.n_head, self.head_dim * 3, height, width)\n        query, key, value = qkv.chunk(3, dim=2)  # bhdyx\n\n        attn = torch.einsum(\n            \"bnchw, bncyx -> bnhwyx\", query, key\n        ).contiguous() / math.sqrt(self.head_dim)\n        attn = attn.view(batch, self.n_head, height, width, -1)\n        attn = torch.softmax(attn, -1)\n        attn = attn.view(batch, self.n_head, height, width, height, width)\n\n        out = torch.einsum(\"bnhwyx, bncyx -> bnchw\", attn, value).contiguous()\n        out = self.out(out.view(batch, channel, height, width))\n\n        return out + input", "\n\nclass ResBlockWithAttention(nn.Module):\n    def __init__(\n            self,\n            in_channel,\n            out_channel,\n            time_dim,\n            dropout,\n            resample,\n            use_attention=False,\n            attention_head: Optional[int] = 1,\n            head_dim: Optional[int] = None,\n            use_affine_time=False,\n            group_norm=32,\n    ):\n        super().__init__()\n        self.resblocks = ResBlock(\n            in_channel, out_channel, time_dim, resample, use_affine_time, dropout, group_norm=group_norm\n        )\n\n        if use_attention:\n            self.attention = SelfAttention(out_channel, n_head=attention_head, head_dim=head_dim, group_norm=group_norm)\n\n        else:\n            self.attention = None\n\n    def forward(self, input, time):\n        out = self.resblocks(input, time)\n\n        if self.attention is not None:\n            out = self.attention(out)\n\n        return out", "\n\nclass UNet(nn.Module):\n    def __init__(\n            self,\n            in_channel: int,\n            channel: int,\n            emb_channel: int,\n            channel_multiplier: List[int],\n            n_res_blocks: int,\n            attn_rezs: List[int],\n            attn_heads: Optional[int],\n            head_dim: Optional[int],\n            use_affine_time: bool = False,\n            dropout: float = 0,\n            num_output: int = 1,\n            resample: bool = False,\n            init_rez: int = 32,\n            logsnr_input_type: str = 'inv_cos',\n            logsnr_scale_range: Tuple[float, float] = (-10., 10.),\n            num_classes: int = 1\n    ):\n        super().__init__()\n\n        self.resample = resample\n        self.channel = channel\n        self.logsnr_input_type = logsnr_input_type\n        self.logsnr_scale_range = logsnr_scale_range\n        self.num_classes = num_classes\n        time_dim = emb_channel\n        group_norm = 32\n\n        n_block = len(channel_multiplier)\n\n        if self.num_classes > 1:\n            self.class_emb = nn.Linear(self.num_classes, time_dim)\n\n        self.time = nn.Sequential(\n            linear(channel, time_dim),\n            Swish(),\n            linear(time_dim, time_dim),\n        )\n\n        down_layers = [conv2d(in_channel, channel, 3, padding=1)]\n        feat_channels = [channel]\n        in_channel = channel\n        cur_rez = init_rez\n        for i in range(n_block):\n            for _ in range(n_res_blocks):\n                channel_mult = channel * channel_multiplier[i]\n\n                down_layers.append(\n                    ResBlockWithAttention(\n                        in_channel,\n                        channel_mult,\n                        time_dim,\n                        dropout,\n                        resample=None,\n                        use_attention=cur_rez in attn_rezs,\n                        attention_head=attn_heads,\n                        head_dim=head_dim,\n                        use_affine_time=use_affine_time,\n                        group_norm=group_norm\n                    )\n                )\n\n                feat_channels.append(channel_mult)\n                in_channel = channel_mult\n\n            if i != n_block - 1:\n                if self.resample:\n                    down_layers.append(ResBlock(\n                        in_channel,\n                        in_channel,\n                        time_dim,\n                        resample='down',\n                        use_affine_time=use_affine_time,\n                        dropout=dropout,\n                        group_norm=group_norm\n                    ))\n                else:\n                    down_layers.append(Downsample(in_channel))\n                cur_rez = cur_rez // 2\n                feat_channels.append(in_channel)\n\n        self.down = nn.ModuleList(down_layers)\n\n        self.mid = nn.ModuleList(\n            [\n                ResBlockWithAttention(\n                    in_channel,\n                    in_channel,\n                    time_dim,\n                    resample=None,\n                    dropout=dropout,\n                    use_attention=True,\n                    attention_head=attn_heads,\n                    head_dim=head_dim,\n                    use_affine_time=use_affine_time,\n                    group_norm=group_norm\n                ),\n                ResBlockWithAttention(\n                    in_channel,\n                    in_channel,\n                    time_dim,\n                    resample=None,\n                    dropout=dropout,\n                    use_affine_time=use_affine_time,\n                    group_norm=group_norm\n                ),\n            ]\n        )\n\n        up_layers = []\n        for i in reversed(range(n_block)):\n            for _ in range(n_res_blocks + 1):\n                channel_mult = channel * channel_multiplier[i]\n\n                up_layers.append(\n                    ResBlockWithAttention(\n                        in_channel + feat_channels.pop(),\n                        channel_mult,\n                        time_dim,\n                        resample=None,\n                        dropout=dropout,\n                        use_attention=cur_rez in attn_rezs,\n                        attention_head=attn_heads,\n                        head_dim=head_dim,\n                        use_affine_time=use_affine_time,\n                        group_norm=group_norm\n                    )\n                )\n\n                in_channel = channel_mult\n\n            if i != 0:\n                if self.resample:\n                    up_layers.append(ResBlock(\n                        in_channel,\n                        in_channel,\n                        time_dim,\n                        resample='up',\n                        use_affine_time=use_affine_time,\n                        dropout=dropout,\n                        group_norm=group_norm\n                    ))\n                else:\n                    up_layers.append(Upsample(in_channel))\n                cur_rez = cur_rez * 2\n\n        self.up = nn.ModuleList(up_layers)\n\n        self.out = nn.Sequential(\n            nn.GroupNorm(group_norm, in_channel),\n            Swish(),\n            conv2d(in_channel, 3 * num_output, 3, padding=1, scale=1e-10),\n        )\n\n    def get_time_embed(self, logsnr):\n        if self.logsnr_input_type == 'linear':\n            logsnr_input = (logsnr - self.logsnr_scale_range[0]) / (self.logsnr_scale_range[1] - self.logsnr_scale_range[0])\n        elif self.logsnr_input_type == 'sigmoid':\n            logsnr_input = torch.sigmoid(logsnr)\n        elif self.logsnr_input_type == 'inv_cos':\n            logsnr_input = (torch.arctan(torch.exp(-0.5 * torch.clip(logsnr, -20., 20.))) / (0.5 * torch.pi))\n        else:\n            raise NotImplementedError(self.logsnr_input_type)\n        time_emb = get_timestep_embedding(logsnr_input, embedding_dim=self.channel, max_time=1.)\n        time_embed = self.time(time_emb)\n        return time_embed\n\n    def forward(self, input, logsnr, y=None):\n        time_embed = self.get_time_embed(logsnr)\n\n        # Class embedding\n        assert self.num_classes >= 1\n        if self.num_classes > 1:\n            y_emb = nn.functional.one_hot(y, num_classes=self.num_classes).float()\n            y_emb = self.class_emb(y_emb)\n            time_embed += y_emb\n        del y\n\n        feats = []\n        out = input\n        for layer in self.down:\n            if isinstance(layer, ResBlockWithAttention):\n                out = layer(out, time_embed)\n            elif isinstance(layer, ResBlock):\n                out = layer(out, time_embed)\n            else:\n                out = layer(out)\n\n            feats.append(out)\n\n        for layer in self.mid:\n            out = layer(out, time_embed)\n\n        for layer in self.up:\n            if isinstance(layer, ResBlockWithAttention):\n                out = layer(torch.cat((out, feats.pop()), 1), time_embed)\n            elif isinstance(layer, ResBlock):\n                out = layer(out, time_embed)\n            else:\n                out = layer(out)\n\n        out = self.out(out)\n\n        return out", ""]}
{"filename": "lib/zoo/__init__.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nfrom . import unet  # noqa"]}
{"filename": "data/image_datasets.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n# Adapted from OpenAI https://github.com/openai/guided-diffusion\nimport math\nimport random\n\nfrom PIL import Image\nimport blobfile as bf", "from PIL import Image\nimport blobfile as bf\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\ndef _list_image_files_recursively(data_dir):\n    results = []\n    for entry in sorted(bf.listdir(data_dir)):\n        full_path = bf.join(data_dir, entry)\n        ext = entry.split(\".\")[-1]\n        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n            results.append(full_path)\n        elif bf.isdir(full_path):\n            results.extend(_list_image_files_recursively(full_path))\n    return results", "\n\nclass ImageDataset(Dataset):\n    def __init__(\n        self,\n        resolution,\n        data_dir,\n        shard=0,\n        num_shards=1,\n        random_crop=False,\n    ):\n        super().__init__()\n        image_paths = _list_image_files_recursively(data_dir)\n        # Assume classes are the first part of the filename,\n        # before an underscore.\n        class_names = [bf.basename(path).split(\"_\")[0] for path in image_paths]\n        sorted_classes = {x: i for i, x in enumerate(sorted(set(class_names)))}\n        classes = [sorted_classes[x] for x in class_names]\n\n        self.resolution = resolution\n        self.local_images = image_paths[shard:][::num_shards]\n        self.local_classes = None if classes is None else classes[shard:][::num_shards]\n        self.random_crop = random_crop\n\n    def __len__(self):\n        return len(self.local_images)\n\n    def __getitem__(self, idx):\n        path = self.local_images[idx]\n        with bf.BlobFile(path, \"rb\") as f:\n            pil_image = Image.open(f)\n            pil_image.load()\n        pil_image = pil_image.convert(\"RGB\")\n\n        if self.random_crop:\n            arr = random_crop_arr(pil_image, self.resolution)\n        else:\n            arr = center_crop_arr(pil_image, self.resolution)\n\n        out_dict = {}\n        if self.local_classes is not None:\n            out_dict[\"y\"] = np.array(self.local_classes[idx], dtype=np.int64)\n        return arr, out_dict[\"y\"]", "\n\ndef center_crop_arr(pil_image, image_size):\n    # We are not on a new enough PIL to support the `reducing_gap`\n    # argument, which uses BOX downsampling at powers of two first.\n    # Thus, we do it by hand to improve downsample quality.\n    while min(*pil_image.size) >= 2 * image_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = image_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = (arr.shape[0] - image_size) // 2\n    crop_x = (arr.shape[1] - image_size) // 2\n    return arr[crop_y:crop_y + image_size, crop_x:crop_x + image_size]", "\n\ndef random_crop_arr(pil_image, image_size, min_crop_frac=0.8, max_crop_frac=1.0):\n    min_smaller_dim_size = math.ceil(image_size / max_crop_frac)\n    max_smaller_dim_size = math.ceil(image_size / min_crop_frac)\n    smaller_dim_size = random.randrange(min_smaller_dim_size, max_smaller_dim_size + 1)\n\n    # We are not on a new enough PIL to support the `reducing_gap`\n    # argument, which uses BOX downsampling at powers of two first.\n    # Thus, we do it by hand to improve downsample quality.\n    while min(*pil_image.size) >= 2 * smaller_dim_size:\n        pil_image = pil_image.resize(\n            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n        )\n\n    scale = smaller_dim_size / min(*pil_image.size)\n    pil_image = pil_image.resize(\n        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n    )\n\n    arr = np.array(pil_image)\n    crop_y = random.randrange(arr.shape[0] - image_size + 1)\n    crop_x = random.randrange(arr.shape[1] - image_size + 1)\n    return arr[crop_y:crop_y + image_size, crop_x:crop_x + image_size]", ""]}
{"filename": "fid/fid_zip.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\"\"\"Compute FID and approximation at 50,000 for zip file of samples.\"\"\"\nimport time\nimport zipfile\n\nimport lib\nimport torch", "import lib\nimport torch\nimport torchvision.transforms.functional\nfrom absl import app, flags\nfrom lib.distributed import auto_distribute, device_id, is_master, world_size\nfrom lib.util import FLAGS\nfrom PIL import Image\n\n\n@auto_distribute\ndef main(argv):\n    def zip_iterator(filename: str, batch: int):\n        with zipfile.ZipFile(filename, 'r') as fzip:\n            x = []\n            fn_list = [fn for fn in fzip.namelist() if fn.endswith('.png')]\n            assert len(fn_list) >= FLAGS.fid_len\n            for fn in fn_list[device_id()::world_size()]:\n                with fzip.open(fn, 'r') as f:\n                    y = torchvision.transforms.functional.to_tensor(Image.open(f))\n                x.append(2 * y - 1)\n                if len(x) == batch:\n                    yield torch.stack(x), None\n                    x = []\n\n    t0 = time.time()\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    fake = (x for x in zip_iterator(argv[1], FLAGS.batch // world_size()))\n    with torch.no_grad():\n        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n        fake_activations = fid.data_activations(fake, FLAGS.fid_len)\n        fid, fid50 = fid.approximate_fid(fake_activations)\n    if is_master():\n        print(f'dataset={FLAGS.dataset}')\n        print(f'fid{FLAGS.fid_len}={fid}')\n        print(f'fid(50000)={fid50}')\n        print(f'time={time.time() - t0}')", "\n@auto_distribute\ndef main(argv):\n    def zip_iterator(filename: str, batch: int):\n        with zipfile.ZipFile(filename, 'r') as fzip:\n            x = []\n            fn_list = [fn for fn in fzip.namelist() if fn.endswith('.png')]\n            assert len(fn_list) >= FLAGS.fid_len\n            for fn in fn_list[device_id()::world_size()]:\n                with fzip.open(fn, 'r') as f:\n                    y = torchvision.transforms.functional.to_tensor(Image.open(f))\n                x.append(2 * y - 1)\n                if len(x) == batch:\n                    yield torch.stack(x), None\n                    x = []\n\n    t0 = time.time()\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    fake = (x for x in zip_iterator(argv[1], FLAGS.batch // world_size()))\n    with torch.no_grad():\n        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n        fake_activations = fid.data_activations(fake, FLAGS.fid_len)\n        fid, fid50 = fid.approximate_fid(fake_activations)\n    if is_master():\n        print(f'dataset={FLAGS.dataset}')\n        print(f'fid{FLAGS.fid_len}={fid}')\n        print(f'fid(50000)={fid50}')\n        print(f'time={time.time() - t0}')", "\n\nif __name__ == '__main__':\n    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n    app.run(lib.distributed.main(main))\n"]}
{"filename": "fid/fid_model.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\n\"\"\"Compute FID and approximation at 50,000 for zip file of samples.\"\"\"\nimport pathlib\nimport time\nfrom types import SimpleNamespace\nfrom typing import Optional\n", "from typing import Optional\n\nimport lib\nimport torch\nfrom absl import app, flags\nfrom lib.distributed import auto_distribute, device_id\nfrom lib.eval.fid import FID\nfrom lib.io import Summary, SummaryWriter, zip_batch_as_png\nfrom lib.util import FLAGS\nfrom lib.zoo.unet import UNet", "from lib.util import FLAGS\nfrom lib.zoo.unet import UNet\n\n\ndef logsnr_schedule_cosine(t, logsnr_min=torch.Tensor([-20.]), logsnr_max=torch.Tensor([20.])):\n    b = torch.arctan(torch.exp(-0.5 * logsnr_max))\n    a = torch.arctan(torch.exp(-0.5 * logsnr_min)) - b\n    return -2. * torch.log(torch.tan(a * t + b))\n\n\ndef predict_eps_from_x(z, x, logsnr):\n    \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n    assert logsnr.ndim == x.ndim\n    return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))", "\n\ndef predict_eps_from_x(z, x, logsnr):\n    \"\"\"eps = (z - alpha*x)/sigma.\"\"\"\n    assert logsnr.ndim == x.ndim\n    return torch.sqrt(1. + torch.exp(logsnr)) * (z - x * torch.rsqrt(1. + torch.exp(-logsnr)))\n\n\ndef predict_x_from_eps(z, eps, logsnr):\n    \"\"\"x = (z - sigma*eps)/alpha.\"\"\"\n    assert logsnr.ndim == eps.ndim\n    return torch.sqrt(1. + torch.exp(-logsnr)) * (z - eps * torch.rsqrt(1. + torch.exp(logsnr)))", "def predict_x_from_eps(z, eps, logsnr):\n    \"\"\"x = (z - sigma*eps)/alpha.\"\"\"\n    assert logsnr.ndim == eps.ndim\n    return torch.sqrt(1. + torch.exp(-logsnr)) * (z - eps * torch.rsqrt(1. + torch.exp(logsnr)))\n\n\nclass ModelFID(torch.nn.Module):\n    COLORS = 3\n\n    def __init__(self, name: str, res: int, timesteps: int, **params):\n        super().__init__()\n        self.name = name\n        if name == 'cifar10':\n            self.model = UNet(in_channel=3,\n                              channel=256,\n                              emb_channel=1024,\n                              channel_multiplier=[1, 1, 1],\n                              n_res_blocks=3,\n                              attn_rezs=[8, 16],\n                              attn_heads=1,\n                              head_dim=None,\n                              use_affine_time=True,\n                              dropout=0.2,\n                              num_output=1,\n                              resample=True,\n                              num_classes=1).to(device_id())\n            self.shape = 3, 32, 32\n            self.mean_type = 'x'\n            self.ckpt_name = 'cifar_original.pt'\n            self.num_classes = 1\n        elif name == 'imagenet64':\n            # imagenet model is class conditional\n            self.model = UNet(in_channel=3,\n                              channel=192,\n                              emb_channel=768,\n                              channel_multiplier=[1, 2, 3, 4],\n                              n_res_blocks=3,\n                              init_rez=64,\n                              attn_rezs=[8, 16, 32],\n                              attn_heads=None,\n                              head_dim=64,\n                              use_affine_time=True,\n                              dropout=0.,\n                              num_output=2,  # predict signal and noise\n                              resample=True,\n                              num_classes=1000).to(device_id())\n            self.shape = 3, 64, 64\n            self.mean_type = 'both'\n            self.ckpt_name = 'imagenet_original.pt'\n            self.num_classes = 1000\n        else:\n            raise NotImplementedError(name)\n        self.params = SimpleNamespace(res=res, timesteps=timesteps, **params)\n        self.timesteps = timesteps\n        self.logstep = 0\n        self.clip_x = True\n\n    @property\n    def logdir(self) -> str:\n        params = ','.join(f'{k}={v}' for k, v in sorted(vars(self.params).items()))\n        return f'{self.__class__.__name__}({params})'\n\n    def initialize_weights(self, logdir: pathlib.Path):\n        self.model.load_state_dict(torch.load(self.params.ckpt))\n\n    def run_model(self, z, logsnr, y=None):\n        if self.mean_type == 'x':\n            model_x = self.model(z.float(), logsnr.float(), y).double()\n            logsnr = logsnr[:, None, None, None]\n        elif self.mean_type == 'both':\n            output = self.model(z.float(), logsnr.float(), y).double()\n            model_x, model_eps = output[:, :3], output[:, 3:]\n            # reconcile the two predictions\n            logsnr = logsnr[:, None, None, None]\n            model_x_eps = predict_x_from_eps(z=z, eps=model_eps, logsnr=logsnr)\n            wx = torch.sigmoid(-logsnr)\n            model_x = wx * model_x + (1. - wx) * model_x_eps\n        else:\n            raise NotImplementedError(self.mean_type)\n\n        # clipping\n        if self.clip_x:\n            model_x = torch.clip(model_x, -1., 1.)\n\n        model_eps = predict_eps_from_x(z=z, x=model_x, logsnr=logsnr)\n        return {'model_x': model_x,\n                'model_eps': model_eps}\n\n    def ddim_step(self, t, z_t, y=None, step=1024):\n        logsnr_t = logsnr_schedule_cosine((t+step) / self.timesteps).to(z_t)\n        logsnr_s = logsnr_schedule_cosine(t / self.timesteps).to(z_t)\n        model_out = self.run_model(z=z_t, logsnr=logsnr_t.repeat(\n            z_t.shape[0]), y=y.to(z_t).long() if y is not None else None)\n        x_pred_t = model_out['model_x']\n        eps_pred_t = model_out['model_eps']\n        stdv_s = torch.sqrt(torch.sigmoid(-logsnr_s))\n        alpha_s = torch.sqrt(torch.sigmoid(logsnr_s))\n        z_s_pred = alpha_s * x_pred_t + stdv_s * eps_pred_t\n        return torch.where(torch.Tensor([t]).to(x_pred_t) == 0, x_pred_t, z_s_pred)\n\n    def sample_loop(self, init_x, y=None, step=1024):\n        # loop over t = num_steps-1, ..., 0\n        image = init_x\n        for t in reversed(range(self.timesteps // step)):\n            image = self.ddim_step(t * step, image, y, step=step)\n        return image\n\n    def forward(self, samples: int, generator: Optional[torch.Generator] = None) -> torch.Tensor:\n        if generator is not None:\n            assert generator.device == torch.device('cpu')\n            init_x = torch.randn((samples, *self.shape), device='cpu', generator=generator, dtype=torch.double).to(device_id())\n        else:\n            init_x = torch.randn((samples, *self.shape), dtype=torch.double).to(device_id())\n        if self.name == 'imagenet64':\n            y = torch.randint(0, self.num_classes, (samples,)).to(device_id())\n        else:\n            y = None\n        return self.sample_loop(init_x, y, step=1 << self.logstep)", "\n\n@auto_distribute\ndef main(_):\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    model = ModelFID(FLAGS.dataset, data.res, FLAGS.timesteps,\n                     batch=FLAGS.batch, fid_len=FLAGS.fid_len, ckpt=FLAGS.ckpt)\n    logdir = lib.util.artifact_dir(FLAGS.dataset, model.logdir)\n\n\n    model.initialize_weights(logdir)\n    model.eval()\n\n    if FLAGS.eval:\n        model.eval()\n        with torch.no_grad():\n            generator = torch.Generator(device='cpu')\n            generator.manual_seed(123623113456)\n            x = model(4, generator)\n        open('debug_fid_model.png', 'wb').write(lib.util.to_png(x.view(2, 2, *x.shape[1:])))\n        import numpy as np\n        np.save('debug_arr_fid_model.npy', x.detach().cpu().numpy())\n        return\n\n    def eval(logstep: int):\n        model.logstep = logstep\n        summary = Summary()\n        t0 = time.time()\n        with torch.no_grad():\n            fid = FID(FLAGS.dataset, (model.COLORS, model.params.res, model.params.res))\n            fake_activations, fake_samples = fid.generate_activations_and_samples(model, FLAGS.fid_len)\n            timesteps = model.params.timesteps >> model.logstep\n            zip_batch_as_png(fake_samples, logdir / f'samples_{FLAGS.fid_len}_timesteps_{timesteps}.zip')\n            fidn, fid50 = fid.approximate_fid(fake_activations)\n        summary.scalar('eval/logstep', logstep)\n        summary.scalar('eval/timesteps', timesteps)\n        summary.scalar(f'eval/fid({FLAGS.fid_len})', fidn)\n        summary.scalar('eval/fid(50000)', fid50)\n        summary.scalar('system/eval_time', time.time() - t0)\n        data_logger.write(summary, logstep)\n        if lib.distributed.is_master():\n            print(f'Logstep {logstep} Timesteps {timesteps}')\n            print(summary)\n\n    with SummaryWriter.create(logdir) as data_logger:\n        if FLAGS.denoise_steps:\n            logstep = lib.util.ilog2(FLAGS.timesteps // FLAGS.denoise_steps)\n            eval(logstep)\n        else:\n            for logstep in range(lib.util.ilog2(FLAGS.timesteps) + 1):\n                eval(logstep)", "\n\nif __name__ == '__main__':\n    flags.DEFINE_bool('eval', False, help='Whether to run model evaluation.')\n    flags.DEFINE_integer('fid_len', 4096, help='Number of samples for FID evaluation.')\n    flags.DEFINE_integer('timesteps', 1024, help='Sampling timesteps.')\n    flags.DEFINE_string('dataset', 'cifar10', help='Dataset.')\n    flags.DEFINE_integer('denoise_steps', None, help='Denoising timesteps.')\n    flags.DEFINE_string('ckpt', None, help='Path to the model checkpoint.')\n    app.run(lib.distributed.main(main))", ""]}
{"filename": "fid/compute_fid_stats.py", "chunked_list": ["#\n# For licensing see accompanying LICENSE file.\n# Copyright (C) 2023 Apple Inc. All Rights Reserved.\n#\nimport os\nimport pathlib\n\nimport lib\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom absl import app, flags\nfrom lib.distributed import auto_distribute\nfrom lib.util import FLAGS, artifact_dir\n\nML_DATA = pathlib.Path(os.getenv('ML_DATA'))\n\n@auto_distribute\ndef main(argv):\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    real = data.make_dataloaders()[1]\n    num_samples = len(real) * FLAGS.batch\n    with torch.no_grad():\n        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n        real_activations = fid.data_activations(real, num_samples, cpu=True)\n        m_real, s_real = fid.calculate_activation_statistics(real_activations)\n    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_mean.npy', m_real.numpy())\n    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_std.npy', s_real.numpy())", "@auto_distribute\ndef main(argv):\n    data = lib.data.DATASETS[FLAGS.dataset]()\n    real = data.make_dataloaders()[1]\n    num_samples = len(real) * FLAGS.batch\n    with torch.no_grad():\n        fid = lib.eval.FID(FLAGS.dataset, (3, data.res, data.res))\n        real_activations = fid.data_activations(real, num_samples, cpu=True)\n        m_real, s_real = fid.calculate_activation_statistics(real_activations)\n    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_mean.npy', m_real.numpy())\n    np.save(f'{ML_DATA}/{FLAGS.dataset}_activation_std.npy', s_real.numpy())", "\n\nif __name__ == '__main__':\n    flags.DEFINE_string('dataset', 'cifar10', help='Training dataset.')\n    app.run(lib.distributed.main(main))\n"]}
