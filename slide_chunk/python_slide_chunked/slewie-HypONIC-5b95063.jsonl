{"filename": "setup.py", "chunked_list": ["import setuptools\n\n\ndef readme():\n    with open('README.md') as f:\n        return f.read()\n\n\nsetuptools.setup(\n    name='hyponic',", "setuptools.setup(\n    name='hyponic',\n    version='0.1.1',\n    author='Vladislav Kulikov, Daniel Satarov, Ivan Chernakov',\n    author_email='v.kulikov@innopolis.university, d.satarov@innopolis.university, i.chernakov@innopolis.university',\n    description='Hyperparameter Optimization with Nature-Inspired Computing',\n    long_description=readme(),\n    long_description_content_type='text/markdown',\n    url='https://github.com/slewie/HypONIC',\n    packages=setuptools.find_packages(),", "    url='https://github.com/slewie/HypONIC',\n    packages=setuptools.find_packages(),\n    install_requires=[\n        'numpy>=1.23.5',\n        'numexpr>=2.8.4',\n        'numba>=0.57.0',\n        'matplotlib>=3.6.3',\n    ],\n    classifiers=[\n        'Programming Language :: Python :: 3.11',", "    classifiers=[\n        'Programming Language :: Python :: 3.11',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n    ],\n    python_requires='>=3.10',\n)\n"]}
{"filename": "documentations/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('../../'))", "sys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('../../'))\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'HypONIC'\ncopyright = '2023, Vladislav Kulikov, Daniel Satarov, Ivan Chernakov'\nauthor = 'Vladislav Kulikov, Daniel Satarov, Ivan Chernakov'\n", "author = 'Vladislav Kulikov, Daniel Satarov, Ivan Chernakov'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n]", "    'sphinx.ext.viewcode',\n]\n\ntemplates_path = ['_templates']\nexclude_patterns = []\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output", "# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = ['_static']\n"]}
{"filename": "hyponic/hyponic.py", "chunked_list": ["from warnings import warn\n\nfrom hyponic.optimizers.swarm_based.PSO import IWPSO\nfrom hyponic.metrics.decorators import METRICS_DICT\nfrom hyponic.utils.problem_identifier import ProblemIdentifier, ProblemType\nfrom hyponic import config\n\nfrom typing import Callable\n\nfrom hyponic.space import Space", "\nfrom hyponic.space import Space\nfrom functools import partial\nimport numpy as np\n\n\nclass HypONIC:\n    \"\"\"\n    HypONIC (Hyperparameter Optimization using Nature-Inspired Computing)\n", "    HypONIC (Hyperparameter Optimization using Nature-Inspired Computing)\n\n    Main class for hyperparameter optimization.\n    \"\"\"\n\n    def __init__(self, model, X, y, metric: Callable | str | None = None, optimizer=None, **kwargs):\n        self.model = model\n        try:\n            self.X = np.array(X)\n            self.y = np.ravel(np.array(y))", "            self.X = np.array(X)\n            self.y = np.ravel(np.array(y))\n        except Exception as e:\n            raise Exception(f\"X and y must be convertible to numpy array. Error: {e}\")\n\n        if isinstance(metric, str):\n            # Try to get metric from the METRICS_DICT\n            self.metric = METRICS_DICT.get(metric, None)\n\n            if self.metric is None:", "\n            if self.metric is None:\n                raise Exception(f\"Metric {metric} is not found.\")\n        elif isinstance(metric, Callable):\n            self.metric = metric\n        elif metric is None:\n            # If metric is None, then try to get metric from the problem type\n            problem_type = ProblemIdentifier(self.y).get_problem_type()\n\n            match problem_type:", "\n            match problem_type:\n                case ProblemType.REGRESSION:\n                    self.metric = METRICS_DICT[\"mse\"]\n                case ProblemType.BINARY_CLASSIFICATION:\n                    self.metric = METRICS_DICT[\"binary_crossentropy\"]\n                case ProblemType.MULTICLASS_CLASSIFICATION:\n                    self.metric = METRICS_DICT[\"log_loss\"]\n        else:\n            raise Exception(f\"Metric {metric} is not found.\")", "        else:\n            raise Exception(f\"Metric {metric} is not found.\")\n\n        try:\n            self.minmax = self.metric.__getattribute__(\"minmax\")\n        except AttributeError:\n            # If a metric does not have minmax attribute,\n            # then it is assumed to be a custom metric and will be minimized by default\n            warn(f\"Metric {metric.__name__} does not have minmax attribute. Minimize by default.\")\n            self.minmax = \"min\"", "            warn(f\"Metric {metric.__name__} does not have minmax attribute. Minimize by default.\")\n            self.minmax = \"min\"\n\n        if kwargs is None:  # Default values for optimizer\n            kwargs = {\"epoch\": 10, \"pop_size\": 10}\n\n        if optimizer is None:\n            self.optimizer = IWPSO(**kwargs)\n        else:\n            self.optimizer = optimizer(**kwargs)", "        else:\n            self.optimizer = optimizer(**kwargs)\n\n        self.hyperparams_optimized = None\n        self.metric_optimized = None\n\n    @staticmethod\n    def warn_not_optimized(func):\n        \"\"\"\n        Decorator that warns if a method is called before optimization.", "        \"\"\"\n        Decorator that warns if a method is called before optimization.\n        \"\"\"\n\n        def wrapper(*args, **kwargs):\n            if args[0].hyperparams_optimized is None:\n                raise Exception(\"Model is not optimized yet. Please call optimize method first\")\n            return func(*args, **kwargs)\n\n        return wrapper", "\n        return wrapper\n\n    def _fitness_wrapper(self, dimensions_names: list, mapping_funcs: dict, values: list) -> float:\n        # Map back to original space\n        hyperparams = {\n            dim: mapping_funcs[dim](val) for dim, val in zip(dimensions_names, values)\n        }\n\n        self.model.set_params(**hyperparams)", "\n        self.model.set_params(**hyperparams)\n        self.model.fit(self.X, self.y)\n\n        y_pred = self.model.predict(self.X)\n        return self.metric(self.y, y_pred)  # TODO: maybe cross-validation could be used instead\n\n    def optimize(self, hyperparams: dict | None = None, verbose=False,\n                 models_config=config.sklearn_models.models_dict) -> (dict, float):\n        print(self.model.__class__)", "                 models_config=config.sklearn_models.models_dict) -> (dict, float):\n        print(self.model.__class__)\n        if hyperparams is None:\n            hyperparams = models_config.get(str(self.model.__class__), dict())\n\n        # Create a space for hyperparameters\n        hyperspace = Space(hyperparams)\n        if verbose:\n            print(\"Successfully created a space for hyperparameters optimization\")\n            print(f\"Using {self.optimizer.__class__.__name__} optimizer\")", "            print(\"Successfully created a space for hyperparameters optimization\")\n            print(f\"Using {self.optimizer.__class__.__name__} optimizer\")\n            print(f\"Metric {self.metric.__name__} is subject to {self.minmax}imization\")\n            print(hyperspace)\n\n        # Map hyperparameters to continuous space\n        mappings_with_bounds = hyperspace.get_continuous_mappings(origins=0)  # Make that all dimensions start from 0\n\n        # Split mappings and bounds\n        mapping_funcs = {}", "        # Split mappings and bounds\n        mapping_funcs = {}\n\n        low_bounds = []\n        highs_bounds = []\n\n        for name in hyperspace.dimensions_names:\n            mapping, (low, high) = mappings_with_bounds[name]\n\n            mapping_funcs[name] = mapping", "\n            mapping_funcs[name] = mapping\n            low_bounds.append(low)\n            highs_bounds.append(high)\n\n        paramspace = {\n            \"fit_func\": partial(self._fitness_wrapper, hyperspace.dimensions_names, mapping_funcs),\n            \"lb\": low_bounds,\n            \"ub\": highs_bounds,\n            \"minmax\": self.minmax", "            \"ub\": highs_bounds,\n            \"minmax\": self.minmax\n        }\n\n        hyperparams_optimized, metric_optimized = self.optimizer.solve(paramspace, verbose=verbose)\n\n        # Map back to the original space\n        hyperparams_optimized = {\n            dim: mapping_funcs[dim](val) for dim, val in zip(hyperspace.dimensions_names, hyperparams_optimized)\n        }", "            dim: mapping_funcs[dim](val) for dim, val in zip(hyperspace.dimensions_names, hyperparams_optimized)\n        }\n\n        self.hyperparams_optimized = hyperparams_optimized\n        self.metric_optimized = metric_optimized\n\n        return hyperparams_optimized, metric_optimized\n\n    @warn_not_optimized\n    def get_optimized_model(self):", "    @warn_not_optimized\n    def get_optimized_model(self):\n        self.model.set_params(**self.hyperparams_optimized)\n        self.model.fit(self.X, self.y)\n        return self.model\n\n    @warn_not_optimized\n    def get_optimized_parameters(self) -> dict:\n        return self.hyperparams_optimized\n", "        return self.hyperparams_optimized\n\n    @warn_not_optimized\n    def get_optimized_metric(self) -> float:\n        return self.metric_optimized\n\n    @warn_not_optimized\n    def visualize_history_fitness(self):\n        self.optimizer.visualize_history_fitness()\n", "        self.optimizer.visualize_history_fitness()\n\n    @warn_not_optimized\n    def visualize_history_time(self):\n        self.optimizer.visualize_history_time()\n"]}
{"filename": "hyponic/space.py", "chunked_list": ["\"\"\"\nThis module contains the Space class, which is used to define the search space.\n\"\"\"\nimport numpy as np\n\nfrom typing import Callable, Any\n\n\nclass Space:\n    \"\"\"\n    A class that represents the search space of a hyperparameter optimization problem.\n    \"\"\"\n    def __init__(self, in_dict: dict[str, Any]):\n        self.__dict = dict\n        self.dimensions = {}\n        self.dimensions_names = []\n\n        for k, v in in_dict.items():\n            # Converting range to list\n            if isinstance(v, range):\n                v = list(v)\n\n            if isinstance(v, Discrete):\n                self.dimensions[k] = v\n            elif isinstance(v, Continuous):\n                self.dimensions[k] = v\n            elif isinstance(v, tuple):\n                if len(v) != 2:\n                    raise ValueError(f\"Value for key {k} is not valid\")\n\n                self.dimensions[k] = Continuous(*v, name=k)\n            elif isinstance(v, list):\n                self.dimensions[k] = Discrete(v, name=k)\n            else:\n                raise ValueError(f\"Value for key {k} is not valid\")\n\n            self.dimensions_names.append(k)\n\n    def get_continuous_mappings(\n            self, scales: dict | int | float = None, origins: dict | int | float = None\n    ) -> dict[str, (Callable, (float, float))]:\n        \"\"\"\n        Returns a function that maps a discrete value to a continuous value.\n        \"\"\"\n\n        if scales is None:\n            scales = {}\n        elif isinstance(scales, (int, float)):\n            scales = {key: scales for key in self.dimensions}\n\n        if origins is None:\n            origins = {}\n        elif isinstance(origins, (int, float)):\n            origins = {key: origins for key in self.dimensions}\n\n        mappings = {}\n        for key in self.dimensions:\n            mappings[key] = self.dimensions[key].get_continuous_mapping(\n                scales.get(key, 1),\n                origins.get(key, None)\n            )\n\n        return mappings\n\n    def map_to_original_space(self, values: list[float]) -> dict[str, Any]:\n        \"\"\"\n        Maps a list of values from the continuous space to the original space.\n        \"\"\"\n        mappings = self.get_continuous_mappings()\n        return {\n            name: mappings[name][0](value) for name, value in zip(self.dimensions_names, values)\n        }\n\n    def __str__(self):\n        if len(self.dimensions) == 0:\n            return \"Hyperparameter Search Space is empty.\"\n\n        # Pretty table of dimensions\n        dim_names = list(self.dimensions.keys())\n        offset = max([len(k) for k in dim_names])\n\n        out_strs = [f\"Hyperparameter Search Space with {len(dim_names)} dimensions:\"]\n        out_strs += [\n            f\"\\t{k}: {'-'*(offset - len(k))} {self.dimensions[k]}\" for k in dim_names\n        ]\n\n        return \"\\n\".join(out_strs)", "class Space:\n    \"\"\"\n    A class that represents the search space of a hyperparameter optimization problem.\n    \"\"\"\n    def __init__(self, in_dict: dict[str, Any]):\n        self.__dict = dict\n        self.dimensions = {}\n        self.dimensions_names = []\n\n        for k, v in in_dict.items():\n            # Converting range to list\n            if isinstance(v, range):\n                v = list(v)\n\n            if isinstance(v, Discrete):\n                self.dimensions[k] = v\n            elif isinstance(v, Continuous):\n                self.dimensions[k] = v\n            elif isinstance(v, tuple):\n                if len(v) != 2:\n                    raise ValueError(f\"Value for key {k} is not valid\")\n\n                self.dimensions[k] = Continuous(*v, name=k)\n            elif isinstance(v, list):\n                self.dimensions[k] = Discrete(v, name=k)\n            else:\n                raise ValueError(f\"Value for key {k} is not valid\")\n\n            self.dimensions_names.append(k)\n\n    def get_continuous_mappings(\n            self, scales: dict | int | float = None, origins: dict | int | float = None\n    ) -> dict[str, (Callable, (float, float))]:\n        \"\"\"\n        Returns a function that maps a discrete value to a continuous value.\n        \"\"\"\n\n        if scales is None:\n            scales = {}\n        elif isinstance(scales, (int, float)):\n            scales = {key: scales for key in self.dimensions}\n\n        if origins is None:\n            origins = {}\n        elif isinstance(origins, (int, float)):\n            origins = {key: origins for key in self.dimensions}\n\n        mappings = {}\n        for key in self.dimensions:\n            mappings[key] = self.dimensions[key].get_continuous_mapping(\n                scales.get(key, 1),\n                origins.get(key, None)\n            )\n\n        return mappings\n\n    def map_to_original_space(self, values: list[float]) -> dict[str, Any]:\n        \"\"\"\n        Maps a list of values from the continuous space to the original space.\n        \"\"\"\n        mappings = self.get_continuous_mappings()\n        return {\n            name: mappings[name][0](value) for name, value in zip(self.dimensions_names, values)\n        }\n\n    def __str__(self):\n        if len(self.dimensions) == 0:\n            return \"Hyperparameter Search Space is empty.\"\n\n        # Pretty table of dimensions\n        dim_names = list(self.dimensions.keys())\n        offset = max([len(k) for k in dim_names])\n\n        out_strs = [f\"Hyperparameter Search Space with {len(dim_names)} dimensions:\"]\n        out_strs += [\n            f\"\\t{k}: {'-'*(offset - len(k))} {self.dimensions[k]}\" for k in dim_names\n        ]\n\n        return \"\\n\".join(out_strs)", "\n\nclass Dimension:\n    def __init__(self, lbound, ubound, name=None):\n        self.name = name\n\n        self._lbound = lbound\n        self._ubound = ubound\n\n    def get_continuous_mapping(self, scale=1, origin=None) -> (Callable, (float, float)):\n        \"\"\"\n        Returns a function that maps set of values to a continuous value\n        \"\"\"\n        assert scale != 0, \"Scale cannot be 0.\"\n\n        if origin is None:\n            origin = self._lbound\n\n        low = origin\n        high = origin + (self._ubound - self._lbound) * scale\n\n        def mapping_func(x):\n            x = np.clip(x, low, high)\n            x = self._lbound + (x - origin) / scale\n            return self.get_value(x)\n\n        return mapping_func, (low, high)\n\n    def get_value(self, x):\n        raise NotImplementedError\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__str__()", "\n\nclass Continuous(Dimension):\n    def __init__(self, low, high, name=None):\n        super().__init__(low, high, name)\n\n    def get_value(self, x):\n        # Note that x is already in the correct range. No need to clip.\n        return x\n\n    def __str__(self):\n        return super().__str__() + f\"({self._lbound}, {self._ubound})\"\n\n    def __repr__(self):\n        return self.__str__()", "\n\nclass Discrete(Dimension):\n    def __init__(self, values, name=None):\n        super().__init__(0, len(values), name)\n        self.values = np.array(values)\n\n    def get_value(self, x):\n        x = np.clip(x, 0, len(self.values) - 1)\n        return self.values[int(x)]\n\n    def __str__(self):\n        return super().__str__() + f\"{self.values}\"\n\n    def __repr__(self):\n        return self.__str__()", ""]}
{"filename": "hyponic/__init__.py", "chunked_list": ["__version__ = '0.1.1'\n\nfrom .hyponic import HypONIC\n\nfrom .metrics.decorators import add_metric_to_dict, add_metric_info,\\\n    add_metric_aliases, minimize_metric, maximize_metric\n\nfrom .metrics.classification import accuracy, precision, recall, f1_score,\\\n    fbeta, confusion_matrix, binary_crossentropy, categorical_crossentropy, log_loss\n", "    fbeta, confusion_matrix, binary_crossentropy, categorical_crossentropy, log_loss\n\nfrom .metrics.regression import mae, mse, rmse, rmsle, r2, adjusted_r2, huber_loss\n\nfrom .optimizers.genetic_based.GA import GA\nfrom .optimizers.physics_based.SA import SA\nfrom .optimizers.swarm_based.ABC import ABC\nfrom .optimizers.swarm_based.ACO import ACO\nfrom .optimizers.swarm_based.PSO import PSO, IWPSO\nfrom .optimizers.swarm_based.GWO import GWO", "from .optimizers.swarm_based.PSO import PSO, IWPSO\nfrom .optimizers.swarm_based.GWO import GWO\nfrom .optimizers.swarm_based.CS import CS\nfrom .optimizers.base_optimizer import BaseOptimizer\n\nfrom .space import Space, Dimension, Continuous, Discrete\n\nfrom .utils.history import History\nfrom .utils.problem_identifier import ProblemIdentifier, ProblemType\n", "from .utils.problem_identifier import ProblemIdentifier, ProblemType\n"]}
{"filename": "hyponic/utils/symtable.py", "chunked_list": ["class Symbol:\n    def __init__(self, symtable, name, *args, **kwargs):\n        self._symtable = symtable\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs  # TODO: Implement kwargs\n\n    def __call__(self, *args):\n        return Symbol(self._symtable, self._name, *args)\n\n    def __repr__(self):\n        return f\"{self._name}({', '.join(repr(arg) for arg in self._args)})\"\n\n    def eval(self):\n        args = [arg.eval() if isinstance(arg, Symbol) else arg for arg in self._args]\n\n        # If all arguments are literals, evaluate the function\n        # Otherwise, evaluate partially\n        if all(not isinstance(arg, Symbol) for arg in args):\n            if self._name in self._symtable.get_symbol_list():\n                return self._symtable.get_symbol(self._name)(*args)\n            else:\n                return self\n        else:\n            # Partially evaluate\n            self._args = args\n            return self", "\n\nclass SymbolTable:\n    def __init__(self):\n        self._symbols = {\n            \"int\": int,\n            \"float\": float,\n            \"add\": lambda x, y: x + y,\n        }\n\n    def get_symbol(self, name):\n        return self._symbols[name]\n\n    def get_symbol_list(self):\n        return self._symbols.keys()\n\n    def add_symbol(self, name, func):\n        self._symbols[name] = func\n\n    def __getattr__(self, name):\n        return Symbol(self, name)", "\n\n# scope = SymbolTable()\n# expr = scope.add(scope.int(1), scope.int(2.5))\n# expr.eval()\n"]}
{"filename": "hyponic/utils/history.py", "chunked_list": ["import numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass History:\n    \"\"\"\n    This class is used to track the history of the optimizer. It tracks the global best, current best, and the time of\n    the epoch.\n    Also, it provides methods to visualize the history of the optimizer.\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.optimizer = kwargs['optimizer']\n        self.epoch = kwargs['epoch']\n        self.population_size = kwargs['population_size']\n\n        self.global_best_list = np.zeros((self.epoch, self.optimizer.dimensions))  # coordinates of the global best\n        self.global_best_fitness_list = np.zeros(self.epoch)  # fitness of the global best\n        self.current_best_list = np.zeros((self.epoch, self.optimizer.dimensions))  # coordinates of the current best\n        self.current_best_fitness_list = np.zeros(self.epoch)  # fitness of the current best\n        self.epoch_time_list = np.zeros(self.epoch)  # time of the current epoch\n\n    def update_history(self, epoch, epoch_time):\n        \"\"\"\n        This method updates the history of the optimizer\n        \"\"\"\n        self.global_best_list[epoch] = self.optimizer.get_best_solution()\n        self.global_best_fitness_list[epoch] = self.optimizer.get_best_score()\n        self.current_best_list[epoch] = self.optimizer.get_best_solution()\n        self.current_best_fitness_list[epoch] = self.optimizer.get_best_score()\n        self.epoch_time_list[epoch] = epoch_time\n\n    def get_history(self) -> dict:\n        \"\"\"\n        This method returns the history of the optimizer\n        \"\"\"\n        return {\n            'global_best_list': self.global_best_list,\n            'global_best_fitness_list': self.global_best_fitness_list,\n            'current_best_list': self.current_best_list,\n            'current_best_fitness_list': self.current_best_fitness_list,\n            'epoch_time_list': self.epoch_time_list\n        }\n\n    def get_global_best(self) -> tuple:\n        \"\"\"\n        This method returns the global best\n        \"\"\"\n        return self.global_best_list[-1], self.global_best_fitness_list[-1]\n\n    def get_current_best(self) -> tuple:\n        \"\"\"\n        This method returns the current best\n        \"\"\"\n        return self.current_best_list[-1], self.current_best_fitness_list[-1]\n\n    @staticmethod\n    def visualize(func):\n        \"\"\"\n        Decorator to visualize the history of the optimizer\n\n        :param func: function to be decorated\n        \"\"\"\n\n        def wrapper(*args, **kwargs):\n            plt.figure(figsize=(12, 8))\n            func(*args, **kwargs)\n            plt.legend()\n            plt.show()\n\n        return wrapper\n\n    @visualize\n    def visualize_fitness(self):\n        \"\"\"\n        This method visualizes the history of the optimizer\n        \"\"\"\n        plt.plot(self.global_best_fitness_list, label='Global Best')\n        plt.plot(self.current_best_fitness_list, label='Current Best')\n        plt.xlabel('Epochs')\n        plt.ylabel('Fitness')\n        plt.title('Fitness vs Epochs')\n\n    @visualize\n    def visualize_time(self):\n        \"\"\"\n        This method visualizes the history of the optimizer\n        \"\"\"\n        plt.plot(self.epoch_time_list, label='Epoch Time')\n        plt.xlabel('Epochs')\n        plt.ylabel('Time')\n        plt.title('Time vs Epochs')\n\n    def is_early_stopping(self, current_epoch, early_stopping: int):\n        \"\"\"\n        This method checks if the early stopping condition is met\n\n        :param current_epoch: current epoch\n        :param early_stopping: number of epochs to wait before stopping the optimization\n\n        :return: boolean\n        \"\"\"\n        if early_stopping is not None and current_epoch >= early_stopping:\n            last_scores = self.global_best_fitness_list[current_epoch - early_stopping + 1:current_epoch + 1]\n            if len(last_scores) == early_stopping and np.all(last_scores == last_scores[0]):\n                return True\n        return False", ""]}
{"filename": "hyponic/utils/problem_identifier.py", "chunked_list": ["import numpy as np\nfrom enum import Enum\n\n\nclass ProblemType(Enum):\n    \"\"\"\n    This class is used to identify the problem type using dataset. It can be used for regression and classification only\n    \"\"\"\n    REGRESSION = 1\n    BINARY_CLASSIFICATION = 2\n    MULTICLASS_CLASSIFICATION = 3", "\n\nclass ProblemIdentifier:\n    \"\"\"\n    This class is used to identify the problem type using dataset. It can be used for regression and classification only\n    \"\"\"\n\n    def __init__(self, y):\n        self.y = y\n\n    def get_problem_type(self):\n        \"\"\"\n        This method identifies the problem type using number of classes and data type of y.\n        If number of classes is 2 and data type of y is int64, then it is a binary classification problem\n        If number of classes is more than 2 and data type of y is int64, then it is a multiclass classification problem\n        If data type of y is not int64, then it is a regression problem\n        \"\"\"\n        target_type = type(self.y[0])\n        number_of_classes = len(np.unique(self.y))\n        if target_type in [np.int64, np.int32, np.int16, np.int8]:\n            if number_of_classes == 2:\n                return ProblemType.BINARY_CLASSIFICATION\n            else:\n                return ProblemType.MULTICLASS_CLASSIFICATION\n        else:\n            return ProblemType.REGRESSION", ""]}
{"filename": "hyponic/utils/__init__.py", "chunked_list": [""]}
{"filename": "hyponic/config/sklearn_models.py", "chunked_list": ["models_dict = {\n    \"<class 'sklearn.svm._classes.SVC'>\": {\n        \"C\": (0.01, 1),\n        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n        \"degree\": range(2, 6),\n    },\n    \"<class 'sklearn.svm._classes.SVR'>\": {\n        \"C\": (0.01, 1),\n        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n        \"degree\": range(2, 6),", "        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n        \"degree\": range(2, 6),\n    },\n    \"<class 'sklearn.ensemble._forest.RandomForestClassifier'>\": {\n        \"n_estimators\": range(10, 100, 10),\n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_depth\": range(1, 10),\n        \"min_samples_split\": range(2, 10),\n        \"min_samples_leaf\": range(1, 10),\n        \"max_features\": [\"sqrt\", \"log2\"],", "        \"min_samples_leaf\": range(1, 10),\n        \"max_features\": [\"sqrt\", \"log2\"],\n    },\n    \"<class 'sklearn.ensemble._forest.RandomForestRegressor'>\": {\n        \"n_estimators\": range(10, 100, 10),\n        \"criterion\": [\"squared_error\", \"absolute_error\"],\n        \"max_depth\": range(1, 10),\n        \"min_samples_split\": range(2, 10),\n        \"min_samples_leaf\": range(1, 10),\n        \"max_features\": [\"sqrt\", \"log2\"],", "        \"min_samples_leaf\": range(1, 10),\n        \"max_features\": [\"sqrt\", \"log2\"],\n    },\n    \"<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\": {\n        'n_neighbors': range(1, 20),\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    },\n    \"<class 'sklearn.neighbors._regression.KNeighborsRegressor'>\": {\n        'n_neighbors': range(1, 20),", "    \"<class 'sklearn.neighbors._regression.KNeighborsRegressor'>\": {\n        'n_neighbors': range(1, 20),\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    },\n    \"<class 'sklearn.tree._classes.DecisionTreeClassifier'>\": {\n        'criterion': ['gini', 'entropy'],\n        'splitter': ['best', 'random'],\n        'max_depth': range(1, 20),\n        'min_samples_split': range(2, 20),", "        'max_depth': range(1, 20),\n        'min_samples_split': range(2, 20),\n        'min_samples_leaf': range(1, 20),\n    },\n    \"<class 'sklearn.tree._classes.DecisionTreeRegressor'>\": {\n        'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n        'splitter': ['best', 'random'],\n        'max_depth': [i for i in range(1, 20)],\n        'min_samples_split': [i for i in range(2, 20)],\n        'min_samples_leaf': [i for i in range(1, 20)]", "        'min_samples_split': [i for i in range(2, 20)],\n        'min_samples_leaf': [i for i in range(1, 20)]\n    },\n    \"<class 'sklearn.linear_model._ridge.Ridge'>\": {\n        'alpha': (0, 20),\n        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n    },\n    \"<class 'sklearn.linear_model._logistic.LogisticRegression'>\": {\n        'C': (0.1, 20),\n        'solver': ['liblinear', 'sag', 'saga'],", "        'C': (0.1, 20),\n        'solver': ['liblinear', 'sag', 'saga'],\n    },\n    \"<class 'sklearn.linear_model._coordinate_descent.Lasso'>\": {\n        'alpha': (0.01, 20),\n        'selection': ['cyclic', 'random'],\n    },\n    \"<class 'sklearn.naive_bayes.GaussianNB'>\": {\n        'var_smoothing': (1e-9, 1e-1),\n    },", "        'var_smoothing': (1e-9, 1e-1),\n    },\n    \"<class 'sklearn.naive_bayes.MultinomialNB'>\": {\n        'alpha': (0.1, 20),\n        'fit_prior': [True, False],\n    },\n    \"<class 'sklearn.naive_bayes.BernoulliNB'>\": {\n        'alpha': (0.1, 20),\n        'fit_prior': [True, False],\n    },", "        'fit_prior': [True, False],\n    },\n    \"<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\": {\n        'loss': ['absolute_error', 'squared_error', 'huber', 'quantile'],\n        'learning_rate': (0.01, 1),\n        'n_estimators': range(10, 100, 10),\n        'criterion': ['friedman_mse', 'squared_error'],\n        'max_depth': range(1, 10),\n        'min_samples_split': range(2, 10),\n    },", "        'min_samples_split': range(2, 10),\n    },\n    \"<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>\": {\n        'learning_rate': (0.01, 1),\n        'n_estimators': range(10, 100, 10),\n        'criterion': ['friedman_mse', 'squared_error'],\n        'max_depth': range(1, 10),\n        'min_samples_split': range(2, 10),\n    },\n    \"<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>\": {", "    },\n    \"<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>\": {\n        'n_estimators': range(10, 100, 10),\n        'learning_rate': (0.01, 1),\n        'algorithm': ['SAMME', 'SAMME.R'],\n    },\n    \"<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>\": {\n        'n_estimators': range(10, 100, 10),\n        'learning_rate': (0.01, 1),\n        'loss': ['linear', 'square', 'exponential'],", "        'learning_rate': (0.01, 1),\n        'loss': ['linear', 'square', 'exponential'],\n    },\n    \"<class 'sklearn.ensemble._bagging.BaggingClassifier'>\": {\n        'n_estimators': range(10, 100, 10),\n        'max_samples': (0.01, 1),\n        'max_features': (0.01, 1),\n        'bootstrap': [True, False],\n        'bootstrap_features': [True, False],\n    },", "        'bootstrap_features': [True, False],\n    },\n    \"<class 'sklearn.ensemble._bagging.BaggingRegressor'>\": {\n        'n_estimators': range(10, 100, 10),\n        'max_samples': (0.01, 1),\n        'max_features': (0.01, 1),\n        'bootstrap': [True, False],\n        'bootstrap_features': [True, False],\n    },\n", "    },\n\n}\n"]}
{"filename": "hyponic/config/__init__.py", "chunked_list": ["from .sklearn_models import models_dict\n"]}
{"filename": "hyponic/metrics/decorators.py", "chunked_list": ["\"\"\"\nThis module contains decorators for metrics.\n\nThe following decorators are available:\n- add_metric_to_dict -- metric is added to the dictionary of metrics (called automatically)\n- add_metric_aliases -- metric can be called by any of the string aliases\n- minimize_metric    -- metric is subject to minimization\n- maximize_metric    -- metric is subject to maximization\n\nThis module also contains a dictionary of metrics to which metrics are added", "\nThis module also contains a dictionary of metrics to which metrics are added\nwhen they are decorated. This dictionary is used by the HypONIC class to\nretrieve the metrics passed to it as the string literals. This is helpful,\nbecause one metric can have multiple aliases, and the user can pass any of\nthem to the HypONIC class as a string.\n\"\"\"\nimport warnings\n\nfrom collections import defaultdict", "\nfrom collections import defaultdict\nfrom typing import Callable\n\n\"\"\"\nA dictionary of aliased metrics to which metrics are added when they are decorated.\n\"\"\"\nMETRICS_DICT = defaultdict()\n\n\ndef add_metric_to_dict(metric: Callable) -> Callable:\n    \"\"\"\n    A decorator that adds the metric to the dictionary of metrics.\n    Called automatically by the minimize_metric and maximize_metric decorators.\n    :param metric: the metric that should be added to the dictionary\n    \"\"\"\n    METRICS_DICT[metric.__name__] = metric\n\n    if not hasattr(metric, \"minmax\"):\n        warnings.warn(f\"Metric {metric.__name__} has no minmax attribute. Using 'min' as default.\")\n        metric.minmax = \"min\"\n\n    return metric", "\n\ndef add_metric_to_dict(metric: Callable) -> Callable:\n    \"\"\"\n    A decorator that adds the metric to the dictionary of metrics.\n    Called automatically by the minimize_metric and maximize_metric decorators.\n    :param metric: the metric that should be added to the dictionary\n    \"\"\"\n    METRICS_DICT[metric.__name__] = metric\n\n    if not hasattr(metric, \"minmax\"):\n        warnings.warn(f\"Metric {metric.__name__} has no minmax attribute. Using 'min' as default.\")\n        metric.minmax = \"min\"\n\n    return metric", "\n\ndef add_metric_aliases(*aliases) -> Callable:\n    \"\"\"\n    A decorator that adds aliases to the metric.\n    :param aliases: a list of aliases for the metric\n    :return: decorated metric\n    \"\"\"\n\n    def decorator(metric: Callable) -> Callable:\n        for alias in aliases:\n            METRICS_DICT[alias] = metric\n\n        return metric\n\n    return decorator", "\n\ndef add_metric_info(info: str) -> Callable:\n    \"\"\"\n    A decorator that adds information about the metric.\n    :param info: information about the metric\n    :return: decorated metric\n    \"\"\"\n\n    def decorator(metric: Callable) -> Callable:\n        metric.info = PrintableProperty(info)\n        return metric\n\n    return decorator", "\n\ndef maximize_metric(metric) -> Callable:\n    \"\"\"\n    A decorator for metrics that should be maximized.\n    :param metric: the metric that should be maximized\n    \"\"\"\n    metric.minmax = \"max\"\n    return add_metric_to_dict(metric)\n", "\n\ndef minimize_metric(metric) -> Callable:\n    \"\"\"\n    A decorator for metrics that should be minimized.\n    :param metric: the metric that should be minimized\n    \"\"\"\n    metric.minmax = \"min\"\n    return add_metric_to_dict(metric)\n", "\n\nclass PrintableProperty:\n    \"\"\"\n    A class for the properties that should either be printed or returned as a string.\n\n    Use case:\n    mse.info()  # will print the information about the metric\n    mse.info    # will return the information about the metric as a string\n    \"\"\"\n\n    def __init__(self, text):\n        self.__text = text\n\n    def __str__(self):\n        return self.__text\n\n    def __repr__(self):\n        return self.__text\n\n    def __call__(self, *args, **kwargs):\n        print(self.__text)\n        return self.__text", ""]}
{"filename": "hyponic/metrics/classification.py", "chunked_list": ["\"\"\"\nThis module contains metrics for evaluating classification models.\n\"\"\"\nfrom hyponic.metrics.decorators import add_metric_info, maximize_metric, minimize_metric\n\nimport numpy as np\nimport numba as nb\n\n\n@maximize_metric", "\n@maximize_metric\n@add_metric_info(\"Accuracy is the fraction of predictions model got right.\")\n@nb.njit\ndef accuracy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    return np.mean(np.equal(y_true, y_pred))\n\n\n@maximize_metric\n@add_metric_info(\"Precision is the fraction of positive predictions that are correct.\")", "@maximize_metric\n@add_metric_info(\"Precision is the fraction of positive predictions that are correct.\")\n@nb.njit\ndef precision(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)\n", "\n\n@maximize_metric\n@add_metric_info(\"Recall is the fraction of positive predictions that are correct\")\n@nb.njit\ndef recall(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    return tp / (tp + fn)", "\n\n@maximize_metric\n@add_metric_info(\"F1 score is the harmonic mean of precision and recall.\")\n@nb.njit\ndef f1_score(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    if p + r == 0:\n        return 0.0\n    return 2 * p * r / (p + r)", "\n\n@maximize_metric\n@add_metric_info(\"F-beta score is the weighted harmonic mean of precision and recall.\")\n@nb.njit\ndef fbeta(y_true: np.array, y_pred: np.array, beta: float = 1.0) -> np.ndarray | float:\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    if p + r == 0:\n        return 0.0\n    return (1 + beta ** 2) * p * r / (beta ** 2 * p + r)", "\n\n@maximize_metric\n@add_metric_info(\n    \"Matthews correlation coefficient is a correlation coefficient between the observed and predicted binary classifications.\")\n@nb.njit\ndef confusion_matrix(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    tn = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n    return np.array([[tp, fp], [fn, tn]])", "\n\n@minimize_metric\n@add_metric_info(\n    \"Log loss is the negative log-likelihood of the true labels given a probabilistic classifier\u2019s predictions.\")\n@nb.njit\ndef log_loss(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    return np.mean(np.log(1 + np.exp(-y_true * y_pred)))\n\n", "\n\n@minimize_metric\n@add_metric_info(\n    \"Binary crossentropy is the negative log-likelihood of the true labels given a probabilistic classifier\u2019s predictions.\")\n# @nb.njit\ndef binary_crossentropy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    # TODO: fix that logarithm can be negative or zero\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n", "\n\n@minimize_metric\n@add_metric_info(\n    \"Categorical crossentropy is the negative log-likelihood of the true labels given a probabilistic classifier\u2019s predictions.\")\n@nb.njit\ndef categorical_crossentropy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    return -np.mean(np.sum(y_true * np.log(y_pred), axis=-1))\n", ""]}
{"filename": "hyponic/metrics/regression.py", "chunked_list": ["\"\"\"\nThis module contains metrics for evaluating regression models.\n\"\"\"\nfrom hyponic.metrics.decorators import add_metric_aliases, add_metric_info, maximize_metric, minimize_metric\n\nimport numpy as np\nimport numexpr as ne\n\n\n@add_metric_aliases(\"mean_absolute_error\")", "\n@add_metric_aliases(\"mean_absolute_error\")\n@add_metric_info(\"Mean Absolute Error (MAE) is the average of the absolute errors.\")\n@minimize_metric\ndef mae(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    ae = ne.evaluate(\"sum(abs(y_true - y_pred))\")  # for technical reasons we have to separate summation and division\n    return ne.evaluate(\"ae / length\", local_dict={\"ae\": ae, \"length\": len(y_true)})\n\n\n@add_metric_aliases(\"mean_squared_error\")", "\n@add_metric_aliases(\"mean_squared_error\")\n@add_metric_info(\"Mean Squared Error (MSE) is the average of the squares of the errors.\")\n@minimize_metric\ndef mse(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    se = ne.evaluate(\"sum((y_true - y_pred) ** 2)\")\n    return ne.evaluate(\"se / length\", local_dict={\"se\": se, \"length\": len(y_true)})\n\n\n@add_metric_aliases(\"root_mean_squared_error\")", "\n@add_metric_aliases(\"root_mean_squared_error\")\n@add_metric_info(\"Root Mean Squared Error (RMSE) is the square root of the average of the squared errors.\")\n@minimize_metric\ndef rmse(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    return np.sqrt(mse(y_true, y_pred))\n\n\n@add_metric_aliases(\"root_mean_squared_log_error\")\n@add_metric_info(\"Root Mean Squared Log Error (RMSLE) is the log of the square root of\"", "@add_metric_aliases(\"root_mean_squared_log_error\")\n@add_metric_info(\"Root Mean Squared Log Error (RMSLE) is the log of the square root of\"\n                 \"the average of the squared errors.\")\n@minimize_metric\ndef rmsle(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    return np.log(rmse(y_true, y_pred))\n\n\n@maximize_metric\n@add_metric_info(\"R2 score is the proportion of the variance in the dependent variable that is predictable\")\ndef r2(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    ss_res = ne.evaluate(\"sum((y_true - y_pred) ** 2)\")\n    sum_y_true = ne.evaluate(\"sum(y_true)\")\n    mean = ne.evaluate(\"sum_y_true / length\", local_dict={\"sum_y_true\": sum_y_true, \"length\": len(y_true)})\n    ss_tot = ne.evaluate(\"sum((y_true - mean) ** 2)\")\n    return ne.evaluate(\"1 - ss_res / ss_tot\")", "@maximize_metric\n@add_metric_info(\"R2 score is the proportion of the variance in the dependent variable that is predictable\")\ndef r2(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    ss_res = ne.evaluate(\"sum((y_true - y_pred) ** 2)\")\n    sum_y_true = ne.evaluate(\"sum(y_true)\")\n    mean = ne.evaluate(\"sum_y_true / length\", local_dict={\"sum_y_true\": sum_y_true, \"length\": len(y_true)})\n    ss_tot = ne.evaluate(\"sum((y_true - mean) ** 2)\")\n    return ne.evaluate(\"1 - ss_res / ss_tot\")\n\n", "\n\n@maximize_metric\n@add_metric_info(\"Adjusted R2 score is the proportion of the variance in the dependent variable that is predictable\")\ndef adjusted_r2(y_true: np.array, y_pred: np.array) -> np.ndarray:\n    if len(y_true.shape) == 1:\n        return r2(y_true, y_pred)\n    return 1 - (1 - r2(y_true, y_pred)) * (len(y_true) - 1) / (len(y_true) - len(y_true[0]) - 1)\n\n", "\n\n@minimize_metric\n@add_metric_info(\"Huber loss is a loss function used in robust regression, that is less sensitive to outliers\")\ndef huber_loss(y_true: np.array, y_pred: np.array, delta: float = 1.0) -> np.ndarray:\n    error = y_true - y_pred\n    return ne.evaluate(\"sum(where(abs(error) <= delta, 0.5 * error ** 2, delta * (abs(error) - 0.5 * delta)))\")\n"]}
{"filename": "hyponic/metrics/__init__.py", "chunked_list": [""]}
{"filename": "hyponic/optimizers/base_optimizer.py", "chunked_list": ["from abc import ABC, abstractmethod\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom hyponic.utils.history import History\nimport time\n\n\nclass BaseOptimizer(ABC):\n    \"\"\"\n    Base class for all optimizers. All optimizers should inherit from this class\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.epoch = None\n        self.population_size = None\n\n        self.function = None\n        self.lb = None\n        self.ub = None\n        self.minmax = None\n\n        self.intervals = None\n        self.dimensions = None\n        self.verbose = None\n        self.mode = None\n        self.n_workers = None\n\n        self.coords = None\n        self.early_stopping = None\n\n    def _before_initialization(self):\n        \"\"\"\n        This method checks if the problem definition is correct\n        \"\"\"\n        if not isinstance(self.epoch, int) or self.epoch < 1:\n            raise ValueError(\"epoch should be a positive integer\")\n\n        if not isinstance(self.population_size, int) or self.population_size < 1:\n            raise ValueError(\"population_size should be a positive integer\")\n\n        if self.mode not in ['single', 'multithread']:\n            raise ValueError(\"mode should be either 'single' or 'multithread'\")\n\n        if self.n_workers < 1:  # TODO: n_workers can be -1, which means use all available cores\n            raise ValueError(\"n_workers should be a positive integer\")\n        if self.early_stopping is not None:\n            if not isinstance(self.early_stopping, int) or self.early_stopping < 1:\n                raise ValueError(\"early_stopping should be a positive integer or None\")\n\n    def _check_initialization(self):\n        \"\"\"\n        This method checks if the problem definition in initialization function is correct\n        \"\"\"\n        if self.lb is None or self.ub is None:\n            raise ValueError(\"lb and ub should be provided\")\n\n        if not isinstance(self.lb, np.ndarray) or not isinstance(self.ub, np.ndarray):\n            raise ValueError(\"lb and ub should be numpy arrays\")\n\n        if self.lb.shape != self.ub.shape:\n            raise ValueError(\"lb and ub should have the same shape\")\n\n        if np.any(self.lb > self.ub):\n            raise ValueError(\"lb should be less than ub\")\n\n        if self.minmax not in ['min', 'max']:\n            raise ValueError(\"minmax should be either 'min' or 'max'\")\n\n        if self.function is None:\n            raise ValueError(\"function should be provided\")\n\n        if not callable(self.function):\n            raise ValueError(\"function should be callable\")\n\n    def initialize(self, problem_dict):\n        \"\"\"\n        Initialize the optimizer with the problem dictionary\n\n        :param problem_dict: dictionary containing the problem definition\n        \"\"\"\n        # Unpack the problem dictionary\n        self.minmax = problem_dict['minmax'] if self.minmax is None else self.minmax\n        self.function = problem_dict[\"fit_func\"]\n        self.lb = np.array(problem_dict[\"lb\"])\n        self.ub = np.array(problem_dict[\"ub\"])\n        self._check_initialization()\n\n        self.intervals = self.ub - self.lb\n        self.dimensions = len(self.lb)\n\n        # TODO: add flag that tracks history or not\n        self.history = History(optimizer=self, epoch=self.epoch, population_size=self.population_size)\n\n        # Create the population\n        self.coords = self._create_population()\n\n    def _create_individual(self):\n        \"\"\"\n        Create an individual\n\n        :return: individual\n        \"\"\"\n        return np.random.uniform(self.lb, self.ub, self.dimensions)\n\n    def _create_population(self):\n        \"\"\"\n        Create a population of size self.population_size\n        This method can be parallelized using multithreading or multiprocessing(but multiprocessing doesn't work now)\n\n        :return: population\n        \"\"\"\n        coords = np.zeros((self.population_size, self.dimensions))\n        if self.mode == 'multithread':\n            with ThreadPoolExecutor(self.n_workers) as executor:\n                list_executor = [executor.submit(self._create_individual) for _ in range(self.population_size)]\n                for f in as_completed(list_executor):\n                    coords[list_executor.index(f)] = f.result()\n        else:\n            coords = np.array([self._create_individual() for _ in range(self.population_size)])\n        return coords\n\n    @abstractmethod\n    def evolve(self, current_epoch):\n        \"\"\"\n        Evolve the population for one epoch\n\n        :param current_epoch: current epoch number\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_best_score(self):\n        \"\"\"\n        Get the best score of the current population\n\n        :return: best score of the fitness function\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_best_solution(self):\n        \"\"\"\n        Get the best solution of the current population\n\n        :return: coordinates of the best solution\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_current_best_score(self):\n        \"\"\"\n        Get the best score of the current population\n\n        :return: current best score of the fitness function\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_current_best_solution(self):\n        \"\"\"\n        Get the best solution of the current population\n\n        :return: current coordinates of the best solution\n        \"\"\"\n        pass\n\n    def _minmax(self):\n        \"\"\"\n        :return: the min or max function, depending on the minmax parameter\n        \"\"\"\n        if self.minmax == 'min':\n            return np.min\n        return np.max\n\n    def _argminmax(self):\n        \"\"\"\n        :return: the argmin or argmax function, depending on the minmax parameter\n        \"\"\"\n        if self.minmax == 'min':\n            return np.argmin\n        return np.argmax\n\n    def solve(self, problem_dict, verbose=False):\n        \"\"\"\n        Solve the problem\n\n        :param problem_dict: dictionary containing the problem definition\n        :param verbose: if True, prints the best score at each epoch\n\n        :return: None\n        \"\"\"\n        if verbose:\n            self.verbose = True\n        self._before_initialization()\n        self.initialize(problem_dict)\n        for current_epoch in range(self.epoch):\n            start = time.time()\n            self.evolve(current_epoch)\n            end = time.time()\n            if self.verbose:\n                print(f'Epoch: {current_epoch}, Best Score: {self.get_best_score()}')\n            self.history.update_history(current_epoch, end - start)\n            if self.history.is_early_stopping(current_epoch, self.early_stopping):\n                if self.verbose:\n                    print(f'Early stopping at epoch {current_epoch}')\n                break\n        return self.get_best_solution(), self.get_best_score()\n\n    def get_history(self):\n        \"\"\"\n        Get the history of the optimizer\n        \"\"\"\n        return self.history.get_history()\n\n    def visualize_history_fitness(self):\n        \"\"\"\n        Visualize the fitness history\n        \"\"\"\n        self.history.visualize_fitness()\n\n    def visualize_history_time(self):\n        \"\"\"\n        Visualize the time history\n        \"\"\"\n        self.history.visualize_time()", ""]}
{"filename": "hyponic/optimizers/__init__.py", "chunked_list": [""]}
{"filename": "hyponic/optimizers/swarm_based/CS.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass CS(BaseOptimizer):\n    \"\"\"\n    Cuckoo Search (CS) algorithm\n\n    Hyperparameters:\n        + pa(float), default=0.25: probability of cuckoo's egg to be discovered\n        + alpha(float), default=0.5: step size\n        + k(float), default=1: Levy multiplication coefficient\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.CS import CS\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> cs = CS(epoch=40, population_size=100, verbose=True, early_stopping=4)\n    >>> cs.solve(problem_dict)\n    >>> print(cs.get_best_score())\n    >>> print(cs.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, verbose: bool = False,\n                 pa: float = 0.25, alpha: float = 0.5, k: float = 1,\n                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        :param pa: probability of cuckoo's egg to be discovered\n        :param alpha: step size\n        :param k: Levy multiplication coefficient\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.pa = pa\n        self.alpha = alpha\n        self.k = k\n\n        self.nests = None\n        self.nests_fitness = None\n        self.cuckoo_coords = None\n\n    def _before_initialization(self):\n        super()._before_initialization()\n        if isinstance(self.pa, float) is False and isinstance(self.pa, int) is False:\n            raise TypeError('pa should be a float or an integer')\n        if isinstance(self.alpha, float) is False and isinstance(self.alpha, int) is False:\n            raise TypeError('alpha should be a float or an integer')\n        if isinstance(self.k, float) is False and isinstance(self.k, int) is False:\n            raise TypeError('k should be a float or an integer')\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n\n        self.nests = self.coords\n        self.nests_fitness = np.array([self.function(self.nests[i]) for i in range(self.population_size)])\n        self.cuckoo_coords = np.random.uniform(self.lb, self.ub, self.dimensions)\n\n    def _levy_flight(self, x):\n        u = np.random.normal(0, 1, size=self.dimensions)\n        v = np.random.normal(0, 1, size=self.dimensions)\n        best_coords = self.nests[self._argminmax()(self.nests_fitness)]\n        return ne.evaluate('x + k * u / (abs(v) ** (1 / 1.5)) * (best_coords - x)', local_dict={\n            'x': x, 'k': self.k, 'u': u, 'v': v, 'best_coords': best_coords\n        })\n\n    def evolve(self, current_epoch):\n        x_new = self._levy_flight(self.cuckoo_coords)\n        self.cuckoo_coords = np.clip(x_new, self.lb, self.ub)\n\n        next_nest = np.random.randint(0, self.population_size)\n        new_fitness = self.function(self.cuckoo_coords)\n        if new_fitness < self.nests_fitness[next_nest]:\n            self.nests[next_nest] = self.cuckoo_coords\n            self.nests_fitness[next_nest] = new_fitness\n        number_of_discovered_eggs = int(self.pa * self.population_size)\n        worst_nests = np.argsort(self.nests_fitness)[-number_of_discovered_eggs:]\n        self.nests[worst_nests] = np.random.uniform(self.lb, self.ub, (number_of_discovered_eggs, self.dimensions))\n        self.nests_fitness[worst_nests] = np.array([self.function(self.nests[i]) for i in worst_nests])\n\n    def get_best_score(self):\n        return self._minmax()(self.nests_fitness)\n\n    def get_best_solution(self):\n        return self.nests[self._argminmax()(self.nests_fitness)]\n\n    def get_current_best_score(self):\n        return self.get_best_score()\n\n    def get_current_best_solution(self):\n        return self.get_best_solution()", ""]}
{"filename": "hyponic/optimizers/swarm_based/__init__.py", "chunked_list": [""]}
{"filename": "hyponic/optimizers/swarm_based/PSO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass PSO(BaseOptimizer):\n    \"\"\"\n    Particle Swarm Optimization (PSO) algorithm\n\n    Hyperparameters:\n        + a1(float), default=0.5: acceleration parameter\n        + a2(float), default=0.5: acceleration parameter\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.PSO import PSO\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> a1, a2 = 0.8, 0.4\n    >>> pso = PSO(epoch=40, population_size=100, verbose=True, early_stopping=4, a1=a1, a2=a2)\n    >>> pso.solve(problem_dict)\n    >>> print(pso.get_best_score())\n    >>> print(pso.get_best_solution())\n    \"\"\"\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, a1: float = 0.5, a2: float = 0.5,\n                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n                 **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        :param a1, a2: acceleration parameter\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.a1 = a1\n        self.a2 = a2\n        self.velocities = None\n        self.p_best = None\n        self.p_best_coords = None\n        self.g_best = None\n        self.g_best_coords = None\n\n    def _before_initialization(self):\n        super()._before_initialization()\n        if isinstance(self.a1, float) is False and isinstance(self.a1, int) is False:\n            raise ValueError(\"a1 should be a float or an integer\")\n        if isinstance(self.a2, float) is False and isinstance(self.a2, int) is False:\n            raise ValueError(\"a2 should be a float or an integer\")\n\n    def initialize(self, problem_dict):\n        # TODO: if lb and ub are not provided, use the default values\n        super().initialize(problem_dict)\n        self.g_best = np.inf if self.minmax == \"min\" else -np.inf\n        max_velocity = ne.evaluate(\"ub - lb\", local_dict={'ub': self.ub, 'lb': self.lb})\n\n        self.velocities = np.random.uniform(-max_velocity, max_velocity, size=(self.population_size, self.dimensions))\n        self.p_best_coords = self.coords\n        self.p_best = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n        self._update_global_best()\n\n    def evolve(self, epoch):\n        self.velocities = self._update_velocity()\n        self.coords = ne.evaluate(\"coords + velocities\",\n                                  local_dict={'coords': self.coords, 'velocities': self.velocities})\n\n        # TODO: if lb or ub is provided, clip the coordinates\n        self.coords = np.clip(self.coords, self.lb, self.ub)\n        fitness = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n        condition = all(self._minmax()(np.concatenate([self.p_best, fitness])) != self.p_best)\n\n        self.p_best_coords = np.where(condition, self.coords, self.p_best_coords)\n\n        self.p_best = ne.evaluate(\"where(condition, fitness, p_best)\", local_dict={'condition': condition,\n                                                                                   'fitness': fitness,\n                                                                                   'p_best': self.p_best})\n        self._update_global_best()\n\n    def _update_velocity(self):\n        r1 = np.random.random()\n        r2 = np.random.random()\n        expr = \"velocities + a1 * r1 * (p_best_coords - coords) + a2 * r2 * (g_best_coords - coords)\"\n        return ne.evaluate(expr,\n                           local_dict={'velocities': self.velocities, 'a1': self.a1, 'a2': self.a2, 'r1': r1, 'r2': r2,\n                                       'p_best_coords': self.p_best_coords, 'coords': self.coords,\n                                       'g_best_coords': self.g_best_coords})\n\n    def _update_global_best(self):\n        if self._minmax()(np.concatenate([self.p_best, [self.g_best]])) != self.g_best:\n            self.g_best = self._minmax()(self.p_best)\n            self.g_best_coords = self.p_best_coords[self._argminmax()(self.p_best)]\n\n    def get_best_score(self):\n        return self.g_best\n\n    def get_best_solution(self):\n        return self.g_best_coords\n\n    def get_current_best_score(self):\n        return self.p_best\n\n    def get_current_best_solution(self):\n        return self.p_best_coords", "\n\nclass IWPSO(PSO):\n    \"\"\"\n    Inertia Weight Particle Swarm Optimization\n\n    Hyperparameters:\n        + a1(float), default=0.5: acceleration parameter\n        + a2(float), default=0.5: acceleration parameter\n        + w(float), default=0.5: inertia weight\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.PSO import IWPSO\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> a1, a2 = 0.8, 0.4\n    >>> w = 0.3\n    >>> iwpso = IWPSO(epoch=40, population_size=100, verbose=True, early_stopping=4, a1=a1, a2=a2, w=w)\n    >>> iwpso.solve(problem_dict)\n    >>> print(iwpso.get_best_score())\n    >>> print(iwpso.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, a1: float = 0.5, a2: float = 0.5,\n                 w: float = 0.8,\n                 verbose: bool = False,\n                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        :param a1, a2: acceleration parameter\n        :param w: inertia\n        \"\"\"\n        super().__init__(epoch, population_size, minmax, a1, a2, verbose, mode, n_workers, early_stopping, **kwargs)\n        self.w = w\n\n    def _before_initialization(self):\n        super()._before_initialization()\n        if isinstance(self.w, float) is False and isinstance(self.w, int) is False:\n            raise ValueError(\"w should be a float or an integer\")\n\n    def _update_velocity(self):\n        r1 = np.random.random()\n        r2 = np.random.random()\n        expr = \"w * velocities + a1 * r1 * (p_best_coords - coords) + a2 * r2 * (g_best_coords - coords)\"\n        return ne.evaluate(expr, local_dict={'w': self.w, 'velocities': self.velocities, 'a1': self.a1, 'a2': self.a2,\n                                             'r1': r1, 'r2': r2, 'p_best_coords': self.p_best_coords,\n                                             'coords': self.coords, 'g_best_coords': self.g_best_coords})", ""]}
{"filename": "hyponic/optimizers/swarm_based/GWO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass GWO(BaseOptimizer):\n    \"\"\"\n    Grey Wolf Optimization (GWO) algorithm\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.GWO import GWO\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> gwo = GWO(epoch=40, population_size=100, verbose=True, early_stopping=4)\n    >>> gwo.solve(problem_dict)\n    >>> print(gwo.get_best_score())\n    >>> print(gwo.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, verbose: bool = False,\n                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.fitness = None\n        self.g_best = None\n        self.g_best_coords = None\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n\n        self.g_best = np.inf if self.minmax == 'min' else -np.inf\n        self.g_best_coords = np.zeros(self.dimensions)\n        self.fitness = np.array([self.function(x) for x in self.coords], dtype=np.float64)\n\n    def evolve(self, current_epoch):\n        a = 2 - current_epoch * (2 / self.epoch)\n\n        for i in range(self.population_size):\n            r1 = np.random.rand()\n            r2 = np.random.rand()\n            A = 2 * a * r1 - a\n            C = 2 * r2\n\n            D = np.abs(C * self.coords[np.random.randint(0, self.population_size)] - self.coords[np.random.randint(0, self.population_size)])\n            coords_new = self.coords[i] + A * D\n            fitness_new = self.function(coords_new)\n            if self._minmax()([fitness_new, self.fitness[i]]) == fitness_new:\n                self.coords[i] = coords_new\n                self.fitness[i] = fitness_new\n\n            if self._minmax()([fitness_new, self.g_best]) == fitness_new:\n                self.g_best = fitness_new\n                self.g_best_coords = coords_new\n\n    def get_best_score(self):\n        return self.g_best\n\n    def get_best_solution(self):\n        return self.g_best_coords\n\n    def get_current_best_score(self):\n        return self._minmax()(self.fitness)\n\n    def get_current_best_solution(self):\n        return self.coords[self._argminmax()(self.fitness)]", "\n"]}
{"filename": "hyponic/optimizers/swarm_based/ABC.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass ABC(BaseOptimizer):\n    \"\"\"\n    Artificial Bee Colony (ABC) algorithm\n\n    Hyperparameters:\n        + limits(int), default=25: the number of trials before abandoning food source\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.ABC import ABC\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> abc = ABC(epoch=40, population_size=100, verbose=True, early_stopping=4)\n    >>> abc.solve(problem_dict)\n    >>> print(abc.get_best_score())\n    >>> print(abc.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, limits=25,\n                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n                 **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        :param limits: the number of trials before abandoning food source\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.limits = limits\n        self.fitness = None\n        self.g_best = None\n        self.g_best_coords = None\n        self.trials = None\n\n    def _before_initialization(self):\n        super()._before_initialization()\n        if not isinstance(self.limits, int) or self.limits < 1:\n            raise ValueError(\"limits should be a positive integer\")\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n        self.g_best = np.inf if self._minmax() == min else -np.inf\n\n        self.fitness = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n\n        self.trials = np.zeros(self.population_size)\n\n    def _coordinate_update_phase(self, i, k):\n        phi = np.random.uniform(-1, 1, self.dimensions)\n        new_coords = ne.evaluate(\"coords + phi * (coords - new_coords)\", local_dict={'coords': self.coords[i],\n                                                                                     'phi': phi,\n                                                                                     'new_coords': self.coords[k]})\n        new_coords = np.clip(new_coords, self.lb, self.ub)\n        new_fitness = self.function(new_coords)\n        if self._minmax()(np.array([self.fitness[i], new_fitness])) != self.fitness[i]:\n            self.coords[i] = new_coords\n            self.fitness[i] = new_fitness\n            self.trials[i] = 0\n        else:\n            self.trials[i] += 1\n\n    def _employed_bees_phase(self):\n        for i in range(self.population_size):\n            k = np.random.choice([j for j in range(self.population_size) if j != i])\n            self._coordinate_update_phase(i, k)\n\n    def _onlooker_bees_phase(self):\n        if np.all(self.fitness == 0):\n            probabilities = np.ones(self.population_size) / self.population_size\n        else:\n            probabilities = self.fitness / np.sum(self.fitness)\n        for i in range(self.population_size):\n            k = np.random.choice([j for j in range(self.population_size)], p=probabilities)\n            self._coordinate_update_phase(i, k)\n\n    def _scout_bees_phase(self):\n        for i in range(self.population_size):\n            if self.trials[i] > self.limits:\n                self.coords[i] = np.random.uniform(self.lb, self.ub, self.dimensions)\n                self.fitness[i] = self.function(self.coords[i])\n                self.trials[i] = 0\n\n    def evolve(self, epoch):\n        self._employed_bees_phase()\n        self._onlooker_bees_phase()\n        self._scout_bees_phase()\n\n        best_index = self._argminmax()(self.fitness)\n        self.g_best = self.fitness[best_index]\n        self.g_best_coords = self.coords[best_index]\n\n    def get_best_score(self):\n        return self.g_best\n\n    def get_best_solution(self):\n        return self.g_best_coords\n\n    def get_current_best_score(self):\n        return self._minmax()(self.fitness)\n\n    def get_current_best_solution(self):\n        best_index = self._argminmax()(self.fitness)\n        return self.coords[best_index]", ""]}
{"filename": "hyponic/optimizers/swarm_based/ACO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\n\n\nclass ACO(BaseOptimizer):\n    \"\"\"\n    Ant Colony Optimization (ACO)\n\n    Hyperparameters:\n        + alpha(float), default=1: the importance of pheromone\n        + beta(float), default=1: the importance of heuristic information\n        + rho(float), default=0.5: the pheromone evaporation rate\n        + q(float), default=1: the pheromone intensity\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.swarm_based.ACO import ACO\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> alpha = 1\n    >>> beta = 1\n    >>> rho = 0.5\n    >>> q = 1\n    >>>\n    >>> aco = ACO(epoch=40, population_size=100, verbose=True, early_stopping=4, alpha=alpha, beta=beta, rho=rho, q=q)\n    >>> aco.solve(problem_dict)\n    >>> print(aco.get_best_score())\n    >>> print(aco.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, alpha: float = 1,\n                 beta: float = 1, rho: float = 0.5, q: float = 1,\n                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n                 **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        :param alpha: the importance of pheromone\n        :param beta: the importance of heuristic information\n        :param rho: the pheromone evaporation rate\n        :param q: the pheromone intensity\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.alpha = alpha\n        self.beta = beta\n        self.rho = rho\n        self.q = q\n\n        self.pheromone = None\n        self.population = None\n        self.scores = None\n        self.best_score = None\n        self.best_solution = None\n\n    def _before_initialization(self):\n        super()._before_initialization()\n        if self.alpha < 0:\n            raise ValueError(\"alpha should be a positive float\")\n        if self.beta < 0:\n            raise ValueError(\"beta should be a positive float\")\n        if self.rho < 0 or self.rho > 1:\n            raise ValueError(\"rho should be a float between 0 and 1\")\n        if self.q < 0:\n            raise ValueError(\"q should be a positive float\")\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n\n        self.pheromone = np.ones((self.population_size, self.dimensions))\n        self.best_score = np.inf if self.minmax == 'min' else -np.inf\n\n        self.population = np.random.uniform(low=self.lb, high=self.ub, size=(self.population_size, self.dimensions))\n        self.scores = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            self.scores[i] = self.function(self.population[i])\n            if self._minmax()(self.scores[i]) < self._minmax()(self.best_score):\n                self.best_score = self.scores[i]\n                self.best_solution = self.population[i]\n\n    def evolve(self, epoch):\n        new_population = np.zeros((self.population_size, self.dimensions))\n        new_scores = np.zeros(self.population_size)\n\n        for i in range(self.population_size):\n            ant_position = np.random.uniform(low=self.lb, high=self.ub, size=self.dimensions)\n\n            for j in range(self.dimensions):\n                pheromone_weights = self.pheromone[:, j] ** self.alpha\n                heuristic_weights = 1 / (np.abs(ant_position[j] - self.population[:, j]) + 1e-6) ** self.beta\n                probs = pheromone_weights * heuristic_weights\n                probs /= np.sum(probs)\n                ant_position[j] = np.random.choice(self.population[:, j], p=probs)\n\n            ant_score = self.function(ant_position)\n            self._update_pheromone(ant_score)\n\n            if self._minmax()(ant_score) < self._minmax()(self.best_score):\n                self.best_solution = ant_position\n                self.best_score = ant_score\n\n            new_population[i] = ant_position\n            new_scores[i] = ant_score\n\n        self.population = new_population\n        self.scores = new_scores\n\n    def get_best_score(self):\n        return self.best_score\n\n    def get_best_solution(self):\n        return self.best_solution\n\n    def get_current_best_score(self):\n        return self._minmax()(self.scores.min())\n\n    def get_current_best_solution(self):\n        return self.population[self.scores.argmin()]\n\n    def _initialize_pheromone(self):\n        self.pheromone = np.ones((self.population_size, self.dimensions))\n\n    def _update_pheromone(self, ant_score):\n        delta_pheromone = np.zeros((self.population_size, self.dimensions))\n        delta_pheromone[:, :] = self.q / ant_score\n        self.pheromone = (1 - self.rho) * self.pheromone + self.rho * delta_pheromone", ""]}
{"filename": "hyponic/optimizers/physics_based/SA.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass SA(BaseOptimizer):\n    \"\"\"\n    Simulated Annealing\n\n    Example\n    ~~~~~~~\n    >>> from hyponic.optimizers.physics_based.SA import SA\n    >>> import numpy as np\n    >>>\n    >>> def sphere(x):\n    >>>     return np.sum(x ** 2)\n    >>>\n    >>> problem_dict = {\n    >>>     'fit_func': sphere,\n    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n    >>>     'minmax': 'min'\n    >>> }\n    >>>\n    >>> sa = SA(epoch=40, population_size=100, verbose=True, early_stopping=4)\n    >>> sa.solve(problem_dict)\n    >>> print(sa.get_best_score())\n    >>> print(sa.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None,\n                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n                 **kwargs):\n        \"\"\"\n        :param epoch: number of iterations\n        :param population_size: number of individuals in the population\n        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n        :param verbose: whether to print the progress, default is False\n        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n        :param n_workers: number of workers to use in multithreading mode\n        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n        stopping is not used\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.currents = None\n        self.best = None\n        self.best_score = None\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n\n        self.currents = np.random.uniform(self.lb, self.ub, (self.population_size, self.dimensions))\n\n        self.best = self.currents[0]\n        self.best_score = self.function(self.best)\n\n    def evolve(self, current_epoch):\n        progress = current_epoch / self.epoch\n        t = max(0.01, min(1, 1 - progress))\n\n        amplitudes = ne.evaluate(\"(_max - _min) * progress * 0.1\",\n                                 local_dict={'_max': np.max(self.intervals), '_min': np.min(self.intervals),\n                                             'progress': progress})\n        deltas = np.random.uniform(-amplitudes / 2, amplitudes / 2, (self.population_size, self.dimensions))\n\n        candidates = ne.evaluate(\"currents + deltas\", local_dict={'currents': self.currents, 'deltas': deltas})\n\n        for idx, candidate in enumerate(candidates):\n            candidate = np.clip(candidate, self.lb, self.ub)\n            candidate_score = self.function(candidate)\n\n            if candidate_score < self.best_score:  # TODO: check if the problem is minimization or maximization\n                self.best = candidate\n                self.best_score = candidate_score\n                self.currents[idx] = candidate\n            else:\n                score_abs_diff = ne.evaluate(\"abs(candidate_score - best_score)\",\n                                             local_dict={'candidate_score': candidate_score,\n                                                         'best_score': self.best_score})\n\n                if np.random.uniform() < np.exp(-score_abs_diff / t):\n                    self.currents[idx] = candidate\n\n    def get_best_score(self):\n        return self.best_score\n\n    def get_best_solution(self):\n        return self.best\n\n    def get_current_best_score(self):\n        return self.function(self.currents[0])\n\n    def get_current_best_solution(self):\n        return self.currents[0]", ""]}
{"filename": "hyponic/optimizers/physics_based/__init__.py", "chunked_list": [""]}
{"filename": "hyponic/optimizers/genetic_based/GA.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\nimport numpy as np\nimport numexpr as ne\n\n\nclass GA(BaseOptimizer):\n    \"\"\"\n    Genetic Algorithm(GA)\n\n    Example\n    ~~~~~~~\n        >>> from hyponic.optimizers.genetic_based.GA import GA\n        >>> import numpy as np\n        >>>\n        >>> def sphere(x):\n        >>>     return np.sum(x ** 2)\n        >>>\n        >>> problem_dict = {\n        >>>     'fit_func': sphere,\n        >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n        >>>     'ub': [5.12, 5, 14, 6, 0.9],\n        >>>     'minmax': 'min'\n        >>> }\n        >>>\n        >>> ga = GA(epoch=40, population_size=100, verbose=True, early_stopping=4)\n        >>> ga.solve(problem_dict)\n        >>> print(ga.get_best_score())\n        >>> print(ga.get_best_solution())\n    \"\"\"\n\n    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None,\n                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n                 **kwargs):\n        \"\"\"\n                :param epoch: number of iterations\n                :param population_size: number of individuals in the population\n                :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n                :param verbose: whether to print the progress, default is False\n                :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n                :param n_workers: number of workers to use in multithreading mode\n                :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n                stopping is not used\n        \"\"\"\n        super().__init__(**kwargs)\n        self.epoch = epoch\n        self.population_size = population_size\n        self.minmax = minmax\n        self.verbose = verbose\n        self.mode = mode\n        self.n_workers = n_workers\n        self.early_stopping = early_stopping\n\n        self.population = None\n        self.scores = None\n        self.best_score = None\n        self.best_solution = None\n\n    def initialize(self, problem_dict):\n        super().initialize(problem_dict)\n\n        self.population = np.random.uniform(low=self.lb, high=self.ub, size=(self.population_size, self.dimensions))\n        self.scores = np.array([self.function(self.population[i]) for i in range(self.population_size)])\n\n        best_idx = self._argminmax()(self.scores)\n        self.best_score = self.scores[best_idx]\n        self.best_solution = self.population[best_idx]\n\n    def evolve(self, epoch):\n        next_population = np.zeros_like(self.population)\n        next_scores = np.zeros(self.population_size)\n\n        # Elitism: keep the best solution from the previous generation\n        best_idx = self._argminmax()(self.scores)\n        next_population[0] = self.population[best_idx]\n        next_scores[0] = self.scores[best_idx]\n\n        # Roulette Wheel Selection\n        fitness = self.scores - np.min(self.scores) if self.minmax == 'min' else np.max(self.scores) - self.scores\n        total_fitness = np.sum(fitness)\n        if total_fitness == 0:\n            probs = np.ones(self.population_size) / self.population_size\n        else:\n            probs = fitness / total_fitness\n        cum_probs = np.cumsum(probs)\n\n        # Crossover and mutation\n        for i in range(1, self.population_size):\n            # Select two parents using roulette wheel selection\n            parent1_idx = np.searchsorted(cum_probs, np.random.rand())\n            parent2_idx = np.searchsorted(cum_probs, np.random.rand())\n\n            # Single point crossover\n            crossover_point = np.random.randint(1, self.dimensions)\n\n            next_population[i, :crossover_point] = self.population[parent1_idx, :crossover_point]\n            next_population[i, crossover_point:] = self.population[parent2_idx, crossover_point:]\n\n            # Mutation\n            mutation_strength = ne.evaluate(\"0.5 * (ub - lb)\", local_dict={\"ub\": self.ub, \"lb\": self.lb})\n\n            next_population[i] += np.random.normal(0, mutation_strength, size=self.dimensions)\n            next_population[i] = np.clip(next_population[i], self.lb, self.ub)\n\n        # evaluate the new population\n        next_scores = np.array([self.function(next_population[i]) for i in range(self.population_size)])\n\n        # update the best solution and score\n        best_idx = self._argminmax()(next_scores)\n        if self._minmax()(next_scores) < self._minmax()(self.scores):\n            self.best_solution = next_population[best_idx]\n            self.best_score = next_scores[best_idx]\n\n        # replace the old population with the new one\n        self.population = next_population\n        self.scores = next_scores\n\n    def get_best_score(self):\n        return self.best_score\n\n    def get_best_solution(self):\n        return self.best_solution\n\n    def get_current_best_score(self):\n        return self._minmax()(self.scores)\n\n    def get_current_best_solution(self):\n        return self.population[self._argminmax()(self.scores)]", ""]}
{"filename": "hyponic/optimizers/genetic_based/__init__.py", "chunked_list": [""]}
{"filename": "examples/automatic_hyperparameters.py", "chunked_list": ["from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_wine\n\nfrom hyponic.hyponic import HypONIC\nfrom hyponic.optimizers.swarm_based.CS import CS\n\nX, y = load_wine(return_X_y=True)\nmodel = GradientBoostingClassifier()\n\noptimizer_kwargs = {", "\noptimizer_kwargs = {\n    \"epoch\": 20,\n    \"pop_size\": 50,\n}\n\nhyponic = HypONIC(model, X, y, optimizer=CS, **optimizer_kwargs)\nhyponic.optimize(verbose=True)\nprint(hyponic.get_optimized_parameters())\nprint(hyponic.get_optimized_metric())", "print(hyponic.get_optimized_parameters())\nprint(hyponic.get_optimized_metric())\nprint(hyponic.get_optimized_model())"]}
{"filename": "examples/random_forrest_simple.py", "chunked_list": ["from sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom hyponic.metrics.regression import mse\nfrom hyponic.hyponic import HypONIC\n\nX, y = load_diabetes(return_X_y=True)\nmodel = RandomForestRegressor()\n\nhyperparams = {", "\nhyperparams = {\n    \"min_samples_split\": (0.01, 0.9),\n    \"min_samples_leaf\": (0.01, 0.9),\n    \"min_weight_fraction_leaf\": (0.0, 0.5),\n    \"min_impurity_decrease\": (0.0, 0.9),\n    \"criterion\": [\"absolute_error\", \"squared_error\"],\n}\n\noptimizer_kwargs = {", "\noptimizer_kwargs = {\n    \"epoch\": 50,\n    \"population_size\": 50\n}\n\nhyponic = HypONIC(model, X, y, mse, **optimizer_kwargs)\nhyponic.optimize(hyperparams, verbose=True)\n\nprint(hyponic.get_optimized_parameters())", "\nprint(hyponic.get_optimized_parameters())\nprint(hyponic.get_optimized_metric())\nprint(hyponic.get_optimized_model())\n"]}
{"filename": "examples/function_optimizing.py", "chunked_list": ["import numpy as np\n\nfrom hyponic.optimizers.swarm_based.ABC import ABC\n\n\ndef sphere_function(x) -> np.ndarray:\n    # min = 0 at x = [0, 0, 0, ...]\n    return np.sum(np.square(x))\n\n\ndef rosenbrock_function(x):\n    # min = 0 at x = [1, 1, 1, ...]\n    return np.sum(100 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2)", "\n\ndef rosenbrock_function(x):\n    # min = 0 at x = [1, 1, 1, ...]\n    return np.sum(100 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2)\n\n\ndef rastrigin_function(x):\n    # min = 0 at x = [0, 0, 0, ...]\n    return np.sum(x ** 2 - 10 * np.cos(2 * np.pi * x) + 10)", "\n\ndef griewank_function(x):\n    # min = 0 at x = [0, 0, 0, ...]\n    return np.sum(x ** 2 / 4000) - np.prod(np.cos(x / np.sqrt(np.arange(1, len(x) + 1)))) + 1\n\n\ndef ackley_function(x):\n    # min = 0 at x = [0, 0, 0, ...]\n    return -20 * np.exp(-0.2 * np.sqrt(np.sum(x ** 2) / len(x))) - np.exp(\n        np.sum(np.cos(2 * np.pi * x)) / len(x)) + 20 + np.exp(1)", "\n\nabc = ABC(epoch=40, population_size=100, verbose=True, early_stopping=4)\nproblem_dict = {\n    'fit_func': ackley_function,\n    'lb': [-5.12, -5, -14, -6, -0.9],\n    'ub': [5.12, 5, 14, 6, 0.9],\n    'minmax': 'min'\n}\nabc.solve(problem_dict)", "}\nabc.solve(problem_dict)\nprint(abc.get_best_score())\nprint(abc.get_best_solution())\n"]}
{"filename": "examples/svm_abc.py", "chunked_list": ["from sklearn.svm import SVC\nfrom sklearn.datasets import load_wine\n\nfrom hyponic.optimizers.swarm_based.ABC import ABC\nfrom hyponic.hyponic import HypONIC\n\nX, y = load_wine(return_X_y=True)\nmodel = SVC()\n\nhyperparams = {", "\nhyperparams = {\n    \"C\": (0.01, 1),\n    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n    \"degree\": [2, 3, 4, 5],\n}\n\noptimizer_kwargs = {\n    \"epoch\": 10,\n    \"pop_size\": 10,", "    \"epoch\": 10,\n    \"pop_size\": 10,\n}\n\nhyponic = HypONIC(model, X, y, optimizer=ABC, **optimizer_kwargs)\nhyponic.optimize(hyperparams, verbose=True)\nprint(hyponic.get_optimized_parameters())\nprint(hyponic.get_optimized_metric())\nprint(hyponic.get_optimized_model())\nhyponic.visualize_history_time()", "print(hyponic.get_optimized_model())\nhyponic.visualize_history_time()\nhyponic.visualize_history_fitness()\n"]}
{"filename": "experiments/test_parallelism.py", "chunked_list": ["\"\"\"\nin this file, we will test the performance of the model with multithreading and without it\n\nResults:\nSingle 1(50, 50): 44.98987\nSingle 2(50, 50): 4.24038\nSingle 3(50, 50): 68.50705\n\n8 threads 1(50, 50): 45.17619\n8 threads 2(50, 50): 3.01542", "8 threads 1(50, 50): 45.17619\n8 threads 2(50, 50): 3.01542\n8 threads 3(50, 50): 65.72666\n\nSingle 2(100, 200): 5.69452\n12 threads 2(100, 200): 6.37028\n\nnumba - 9.080\nnumexpr - 6.85\n\"\"\"", "numexpr - 6.85\n\"\"\"\n\nfrom sklearn.datasets import load_diabetes, load_wine\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom hyponic.metrics.regression import mse\nfrom hyponic.hyponic import HypONIC", "from hyponic.metrics.regression import mse\nfrom hyponic.hyponic import HypONIC\nimport time\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndef give_test_set1():\n    X, y = load_diabetes(return_X_y=True)\n    model = KNeighborsRegressor()\n    hyperparams = {\n        \"n_neighbors\": [i for i in range(1, 10)],\n        \"weights\": [\"uniform\", \"distance\"],\n        \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n        \"leaf_size\": [i for i in range(1, 50)],\n        \"p\": (1, 2)\n    }\n    return X, y, model, hyperparams", "\ndef give_test_set1():\n    X, y = load_diabetes(return_X_y=True)\n    model = KNeighborsRegressor()\n    hyperparams = {\n        \"n_neighbors\": [i for i in range(1, 10)],\n        \"weights\": [\"uniform\", \"distance\"],\n        \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n        \"leaf_size\": [i for i in range(1, 50)],\n        \"p\": (1, 2)\n    }\n    return X, y, model, hyperparams", "\n\ndef give_test_set2():\n    X, y = load_diabetes(return_X_y=True)\n    model = DecisionTreeRegressor()\n    hyperparams = {\n        \"criterion\": [\"absolute_error\", \"friedman_mse\", \"squared_error\"],\n        \"splitter\": [\"best\", \"random\"],\n        \"max_depth\": [i for i in range(1, 50, 5)],\n        \"min_samples_split\": [i for i in range(2, 10)],\n        \"min_samples_leaf\": [i for i in range(1, 10)],\n        \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n    }\n    return X, y, model, hyperparams", "\n\ndef give_test_set3():\n    X, y = load_wine(return_X_y=True)\n    model = RandomForestClassifier()\n    hyperparams = {\n        \"n_estimators\": [i for i in range(1, 100, 5)],\n        \"max_depth\": [i for i in range(1, 50, 5)],\n        \"min_samples_split\": [i for i in range(2, 10)],\n    }\n    return X, y, model, hyperparams", "\n\ndef run(optimizer_kwargs, test_set, num_iter=15):\n    X, y, model, hyperparams = test_set\n    times = np.zeros(num_iter)\n    for i in range(num_iter):\n        hyponic = HypONIC(model, X, y, mse, **optimizer_kwargs)\n        start = time.time()\n        hyponic.optimize(hyperparams)\n        end = time.time()\n        times[i] = end - start\n        print(f\"\\rIteration {i + 1}/{num_iter} done, time: {times[i]}\", end=\"\")\n    print()\n    return np.mean(times)", "\n\ndef test_single():\n    optimizer_kwargs = {\n        \"epoch\": 100,\n        \"pop_size\": 200,\n        \"mode\": \"single\"\n    }\n    print(\"Single thread, KNN regression: \", run(optimizer_kwargs, give_test_set1(), num_iter=10))\n    print(\"Single thread, Decision Tree regression: \", run(optimizer_kwargs, give_test_set2(), num_iter=50))\n    print(\"Single thread, Random Forest classification: \", run(optimizer_kwargs, give_test_set3(), num_iter=5))", "\n\ndef test_threads():\n    n_workers = 12\n    optimizer_kwargs = {\n        \"epoch\": 100,\n        \"pop_size\": 200,\n        \"mode\": \"multithread\",\n        \"n_workers\": n_workers\n    }\n    print(f\"{n_workers} threads, KNN regression: \", run(optimizer_kwargs, give_test_set1(), num_iter=10))\n    print(f\"{n_workers} threads, Decision Tree regression: \", run(optimizer_kwargs, give_test_set2(), num_iter=15))\n    print(f\"{n_workers} threads, Random Forest classification: \", run(optimizer_kwargs, give_test_set3(), num_iter=5))", "\n\nif __name__ == \"__main__\":\n    test_single()\n    test_threads()\n"]}
