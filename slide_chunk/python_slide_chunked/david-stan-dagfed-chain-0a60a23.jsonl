{"filename": "subchain/fabric_api.py", "chunked_list": ["import subprocess\nimport json\nimport sys\nimport time\n\nsys.path.append('../')\n\nfrom common.ipfs import ipfsGetFile\n\n# def queryLocal(lock, taskID, deviceID, currentEpoch, flagSet, localFileName):", "\n# def queryLocal(lock, taskID, deviceID, currentEpoch, flagSet, localFileName):\n#     \"\"\"\n#     Query and download the paras file of local model trained by the device.\n#     \"\"\"\n#     localQuery = subprocess.Popen(args=['../commonComponent/interRun.sh query '+deviceID], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n#     outs, errs = localQuery.communicate(timeout=15)\n#     if localQuery.poll() == 0:\n#         localDetail = json.loads(outs.strip())\n#         if localDetail['epoch'] == currentEpoch and localDetail['taskID'] == taskID:", "#         localDetail = json.loads(outs.strip())\n#         if localDetail['epoch'] == currentEpoch and localDetail['taskID'] == taskID:\n#             print(\"The query result of the \" + deviceID + \" is \", outs.strip())\n#             while 1:\n#                 # localFileName = './clientS/paras/' + taskID + '-' + deviceID + '-epoch-' + str(currentEpoch) + '.pkl'\n#                 outs, stt = ipfsGetFile(localDetail['paras'], localFileName)\n#                 if stt == 0:\n#                     break\n#                 # else:\n#                 #     print(outs.strip())", "#                 # else:\n#                 #     print(outs.strip())\n#             lock.acquire()\n#             t1 = flagSet\n#             t1.add(deviceID)\n#             flagSet = t1\n#             lock.release()\n        # else:\n        #     print('*** This device %s has not updated its model! ***'%(deviceID))\n    # else:", "        #     print('*** This device %s has not updated its model! ***'%(deviceID))\n    # else:\n    #     print(\"Failed to query this device!\", errs)\n\n# def simpleQuery(key):\n#     \"\"\"\n#     Use the only key to query info from fabric network.\n#     \"\"\"\n#     infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query \" + key], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n#     outs, errs = infoQuery.communicate(timeout=15)", "#     infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query \" + key], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n#     outs, errs = infoQuery.communicate(timeout=15)\n#     if infoQuery.poll() == 0:\n#         return outs.strip(), infoQuery.poll()\n#     else:\n#         print(\"*** Failed to query the info of \" + str(key) + \"! ***\" + errs.strip())\n#         return errs.strip(), infoQuery.poll()\n    \ndef query_release():\n    \"\"\"\n    Use the only key to query info from fabric network.\n    \"\"\"\n    infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query_release\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = infoQuery.communicate(timeout=15)\n    if infoQuery.poll() == 0:\n        return outs.strip(), infoQuery.poll()\n    else:\n        print(\"*** Failed to query the info of query_release! ***\" + errs.strip())\n        time.sleep(2)\n        return errs.strip(), infoQuery.poll()", "def query_release():\n    \"\"\"\n    Use the only key to query info from fabric network.\n    \"\"\"\n    infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query_release\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = infoQuery.communicate(timeout=15)\n    if infoQuery.poll() == 0:\n        return outs.strip(), infoQuery.poll()\n    else:\n        print(\"*** Failed to query the info of query_release! ***\" + errs.strip())\n        time.sleep(2)\n        return errs.strip(), infoQuery.poll()", "    \ndef query_task(taskID):\n    \"\"\"\n    Use the only key to query info from fabric network.\n    \"\"\"\n    infoQuery = subprocess.Popen(args=[f\"./hyperledger_invoke.sh query_task {taskID}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = infoQuery.communicate(timeout=15)\n    if infoQuery.poll() == 0:\n        return outs.strip(), infoQuery.poll()\n    else:\n        print(\"*** Failed to query the info of \" + taskID + \"! ***\" + errs.strip())\n        time.sleep(2)\n        return errs.strip(), infoQuery.poll()", "    \ndef query_local(lock, taskID, deviceID, currentEpoch, flagSet, localFileName):\n    \"\"\"\n    Query and download the paras file of local model trained by the device.\n    \"\"\"\n    localQuery = subprocess.Popen(args=[f\"./hyperledger_invoke.sh query_local {deviceID} {taskID}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = localQuery.communicate(timeout=15)\n    if localQuery.poll() == 0:\n        localDetail = json.loads(outs.strip())\n        if localDetail['CurrentEpoch'] == currentEpoch and localDetail['ClientID'] == f\"{taskID}_{deviceID}\":\n            print(f\"The query result of the {deviceID} is {outs.strip()}\")\n            while 1:\n                # localFileName = './clientS/paras/' + taskID + '-' + deviceID + '-epoch-' + str(currentEpoch) + '.pkl'\n                outs, stt = ipfsGetFile(localDetail['LocalParamHash'], localFileName)\n                if stt == 0:\n                    break\n                # else:\n                #     print(outs.strip())\n            lock.acquire()\n            t1 = flagSet\n            t1.add(deviceID)\n            flagSet = t1\n            lock.release()\n        else:\n            print(f\"*** This device {deviceID} has not updated its model! ***\")\n    else:\n        print(\"Failed to query this device!\", errs)"]}
{"filename": "subchain/federated_local.py", "chunked_list": ["import os\nimport shutil\nimport sys\nimport json\nimport time\nimport torch\nimport copy\nimport subprocess\n\nsys.path.append('./ml')", "\nsys.path.append('./ml')\nsys.path.append('../')\n\nfrom common.ipfs import ipfsAddFile\nfrom common.ipfs import ipfsGetFile\n\nfrom ml.utils.settings import BaseSettings\nfrom ml.model_build import model_build\nfrom ml.model_build import model_evaluate", "from ml.model_build import model_build\nfrom ml.model_build import model_evaluate\nfrom ml.models.FedAvg import FedAvg\nfrom ml.models.train_model import LocalUpdate\n\nimport fabric_api\n\nif __name__ == '__main__':\n\n    if os.path.exists('./cache/local'):\n        shutil.rmtree('./cache/local')\n    os.mkdir('./cache/local')\n\n    if os.path.exists('./cache/local/agg'):\n        shutil.rmtree('./cache/local/agg')\n    os.mkdir('./cache/local/agg')\n\n    if os.path.exists('./cache/local/paras'):\n        shutil.rmtree('./cache/local/paras')\n    os.mkdir('./cache/local/paras')\n\n    # build network\n    net_glob, settings, dataset_train, dataset_test, dict_users = model_build(BaseSettings())\n    net_glob.train()\n\n    # with open('../commonComponent/dict_users.pkl', 'rb') as f:\n    #     dict_users = pickle.load(f)\n\n    checkTaskID = ''\n    iteration = 0\n    while 1:\n        taskRelInfo = {}\n        # taskRelease info template {\"taskID\":\"task1994\",\"epoch\":10,\"status\":\"start\",\"usersFrac\":0.1}\n        while 1:\n            taskRelQue, taskRelQueStt = fabric_api.query_release()\n            if taskRelQueStt == 0:\n                taskRelInfo = json.loads(taskRelQue)\n                print('\\n*************************************************************************************')\n                print('Latest task release status is %s!'%taskRelQue.strip())\n                break\n        \n        taskID = taskRelInfo['TaskID']\n        totalEpochs = taskRelInfo['Epochs']\n\n        print(f\"Current task is {taskID}\")\n\n        taskInfo = {}\n        while 1:\n            taskInQue, taskInQueStt = fabric_api.query_task(taskID)\n            if taskInQueStt == 0:\n                taskInfo = json.loads(taskInQue)\n                print('Latest task info is %s!'%taskInQue.strip())\n                print('*************************************************************************************\\n')\n                break\n        if taskInfo['TaskStatus'] == 'done' or checkTaskID == taskID:\n            print('*** %s has been completed! ***\\n'%taskID)\n            time.sleep(5)\n        else:\n            print('\\n******************************* Iteration #%d starting ********************************'%iteration+'\\n')\n            print('Iteration %d starting!'%iteration)\n            print('\\n*************************************************************************************\\n')\n            currentEpoch = int(taskInfo['TaskEpochs']) + 1\n            loss_train = []\n            while currentEpoch <= totalEpochs:\n                \n                while 1:\n                    taskInQueEpoch, taskInQueEpochStt = fabric_api.query_task(taskID)\n                    if taskInQueEpochStt == 0:\n                        taskInfoEpoch = json.loads(taskInQueEpoch)\n                        if int(taskInfoEpoch['TaskEpochs']) == (currentEpoch-1):\n                            print('\\n****************************** Latest status of %s ******************************'%taskID)\n                            print('(In loop) Latest task info is \\n %s!'%taskInQueEpoch)\n                            print('*************************************************************************************\\n')\n                            break\n                        else:\n                            print('\\n*************************** %s has not been updated ***************************'%taskID)\n                            print('(In loop) Latest task info is \\n %s!'%taskInQueEpoch)\n                            print('*************************************************************************************\\n')\n                            time.sleep(10)\n                # download aggregated model in current epoch from ipfs\n                aggBaseModelFile = f\"./cache/local/agg/aggModel-iter-{str(iteration)}-epoch-{str(currentEpoch-1)}.pkl\"\n                while 1:\n                    aggBasMod, aggBasModStt = ipfsGetFile(taskInfoEpoch['ParamHash'], aggBaseModelFile)\n                    if aggBasModStt == 0:\n                        print('\\nThe paras file of aggregated model for epoch %d training has been downloaded!\\n'%(int(taskInfoEpoch['TaskEpochs'])+1))\n                        break\n                    else:\n                        print('\\nFailed to download the paras file of aggregated model for epoch %d training!\\n'%(int(taskInfoEpoch['TaskEpochs'])+1))\n\n                w_glob = net_glob.state_dict()\n                net_glob.load_state_dict(torch.load(aggBaseModelFile))\n\n                selectedDevices = [0, 1, 2, 3, 4]\n                loss_locals = []\n\n                for idx_user in selectedDevices:\n                    local = LocalUpdate(settings, dataset_train, dict_users[idx_user])\n                    w_local, loss_local = local.train(net=copy.deepcopy(net_glob).to(settings.device), user=idx_user)\n                    loss_locals.append(copy.deepcopy(loss_local))\n                    localParamFile = f\"./cache/local/paras/{taskID}-{selectedDevices[idx_user]}-epoch-{str(currentEpoch)}.pkl\"\n                    torch.save(w_local, localParamFile)\n                    while 1:\n                        localParamHash, localAddStt = ipfsAddFile(localParamFile)\n                        if localAddStt == 0:\n                            print('%s has been added to the IPFS network!'%localParamFile)\n                            print('And the hash value of this file is %s'%localParamHash)\n                            break\n                        else:\n                            print('Failed to add %s to the IPFS network!'%localParamFile)\n\n                    while 1:\n                        localRelease = subprocess.Popen(args=[f\"./hyperledger_invoke.sh local {selectedDevices[idx_user]} {taskID} {localParamHash} {str(currentEpoch)}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n                        localOuts, localErrs = localRelease.communicate(timeout=10)\n                        if localRelease.poll() == 0:\n                            print('*** Local model train in epoch ' + str(currentEpoch) + ' of ' + str(selectedDevices[idx_user]) + ' has been uploaded! ***\\n')\n                            break\n                        else:\n                            print(localErrs.strip())\n                            print('*** Failed to release Local model train in epoch ' + str(currentEpoch) + ' of ' + str(selectedDevices[idx_user]) + '! ***\\n')\n                            time.sleep(2)\n                \n                loss_avg = sum(loss_locals) / len(loss_locals)\n                print('Epoch {:3d}, Average loss {:.3f}'.format(currentEpoch, loss_avg))\n                loss_train.append(loss_avg)\n                currentEpoch += 1\n\n            checkTaskID = taskID\n            iteration += 1", ""]}
{"filename": "subchain/client.py", "chunked_list": ["import socket\nimport sys\nimport pathlib\nimport struct\nimport json\n\nBUFFER_SIZE = 1024\n\n\ndef require_tx_from_server(ip_addr, tx_name):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((ip_addr, 65432))\n    except socket.error as msg:\n        print(msg)\n        sys.exit(1)\n    print(s.recv(1024).decode())\n    data = 'requireTx'.encode()\n    s.send(data) # initiate request\n    response = s.recv(1024) # server agrees\n    print(\"Got response from server\", response)\n    data = tx_name.encode()\n    s.send(data) # send tx name\n    rev_file(s, pathlib.Path('./cache/client/txs') / f\"{tx_name}.json\")\n    s.close()", "\ndef require_tx_from_server(ip_addr, tx_name):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((ip_addr, 65432))\n    except socket.error as msg:\n        print(msg)\n        sys.exit(1)\n    print(s.recv(1024).decode())\n    data = 'requireTx'.encode()\n    s.send(data) # initiate request\n    response = s.recv(1024) # server agrees\n    print(\"Got response from server\", response)\n    data = tx_name.encode()\n    s.send(data) # send tx name\n    rev_file(s, pathlib.Path('./cache/client/txs') / f\"{tx_name}.json\")\n    s.close()", "\n\ndef require_tips_from_server(ip_addr):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((ip_addr, 65432))\n    except socket.error as msg:\n        print(msg)\n        sys.exit(1)\n    print(s.recv(1024).decode())\n    data = 'requireTips'.encode()\n    s.send(data) # initiate request\n    rev_file(s, pathlib.Path('./cache/client/pools/tip_pool.json'))\n    s.close()", "\ndef upload_tx_to_server(ip_addr, tx_info):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((ip_addr, 65432))\n    except socket.error as msg:\n        print(msg)\n        sys.exit(1)\n    print(s.recv(1024).decode())\n    data = 'uploadTx'.encode()\n    s.send(data)\n    response = s.recv(BUFFER_SIZE)\n    data = json.dumps(tx_info).encode()\n    s.send(data)\n    s.close()", "    \ndef rev_file(conn, tx_file_path):\n    header_size = struct.calcsize('64si')\n    header = conn.recv(header_size)\n    _, tx_size = struct.unpack('64si', header)\n    print(f\"Size of the block is {tx_size}\")\n    conn.send('header_resp'.encode())\n    with open(tx_file_path, 'wb') as f:\n        bytes_received = 0\n        while not bytes_received >= tx_size:\n            buff = conn.recv(BUFFER_SIZE)\n            bytes_received += len(buff)\n            f.write(buff)", "        "]}
{"filename": "subchain/__init__.py", "chunked_list": [""]}
{"filename": "subchain/shard_run.py", "chunked_list": ["import os\nimport shutil\nimport sys\nimport pathlib\nimport torch\nimport time\nimport uuid\nimport json\nimport random\nimport copy", "import random\nimport copy\nimport subprocess\nimport threading\n\nimport client\nimport fabric_api\n\nsys.path.append('./ml')\nsys.path.append('../')", "sys.path.append('./ml')\nsys.path.append('../')\n# sys.path.append('../../commonComponent')\n\nfrom ml.utils.settings import BaseSettings\nfrom ml.model_build import model_build\nfrom ml.model_build import model_evaluate\nfrom ml.models.FedAvg import FedAvg\n\nfrom common.ipfs import ipfsAddFile", "\nfrom common.ipfs import ipfsAddFile\nfrom common.ipfs import ipfsGetFile\n\n\n\nCACHE_DIR = \"./cache/\"\nCLIENT_DATA_DIR = pathlib.Path(CACHE_DIR) / \"client\"\nTX_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"txs\"\nTIPS_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"pools\"", "TX_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"txs\"\nTIPS_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"pools\"\nPARAMS_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"params\"\nLOCAL_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"local\"\n\ndef main():\n    \n    if os.path.exists(CACHE_DIR) == False:\n        os.mkdir(CACHE_DIR)\n\n    if os.path.exists(CLIENT_DATA_DIR):\n        shutil.rmtree(CLIENT_DATA_DIR)\n    os.mkdir(CLIENT_DATA_DIR)\n\n    if os.path.exists(TX_DATA_DIR):\n        shutil.rmtree(TX_DATA_DIR)\n    os.mkdir(TX_DATA_DIR)\n\n    if os.path.exists(TIPS_DATA_DIR):\n        shutil.rmtree(TIPS_DATA_DIR)\n    os.mkdir(TIPS_DATA_DIR)\n\n    if os.path.exists(PARAMS_DATA_DIR):\n        shutil.rmtree(PARAMS_DATA_DIR)\n    os.mkdir(PARAMS_DATA_DIR)\n\n    if os.path.exists(LOCAL_DATA_DIR):\n        shutil.rmtree(LOCAL_DATA_DIR)\n    os.mkdir(LOCAL_DATA_DIR)\n\n    ## setup\n\n    alpha = 3\n\n    # client.require_tx_from_server(\"localhost\", \"genesis\")\n    # client.require_tips_from_server(\"localhost\")\n\n    net, settings, _, test_dataset, data_user_mapping = model_build(BaseSettings())\n    net_weight = net.state_dict()\n    net_accuracy, _ = model_evaluate(net, net_weight, test_dataset, settings)\n\n    genesisFile = './cache/client/genesis.pkl'\n    torch.save(net_weight, genesisFile)\n\n    while 1:\n        genesisHash, statusCode = ipfsAddFile(genesisFile)\n        if statusCode == 0:\n            print('\\nThe base mode parasfile ' + genesisFile + ' has been uploaded!')\n            print('And the fileHash is ' + genesisHash + '\\n')\n            break\n        else:\n            print('Error: ' + genesisHash)\n            print('\\nFailed to upload the aggregated parasfile ' + genesisFile + ' !\\n')\n\n    genesisTxInfo = {\"approved_tips\": [], \"model_accuracy\": float(net_accuracy), \"param_hash\": genesisHash, \"shard_id\": 0, \"timestamp\": time.time()}\n    client.upload_tx_to_server(\"localhost\", genesisTxInfo)\n\n    time.sleep(1)\n\n    iteration = 0\n    while 1:\n        print(f\"********************* Iteration {iteration} ***************************\")\n\n        taskID = str(uuid.uuid4())[:8]\n\n        apv_tx_cands = []\n        client.require_tips_from_server(\"localhost\") \n        # implement promise later\n        time.sleep(2)\n        with open(\"./cache/client/pools/tip_pool.json\", 'r') as f:\n            tips_dict = json.load(f)\n        \n        if len(tips_dict) <= alpha:\n            apv_tx_cands = list(tips_dict.keys())\n        else:\n            apv_tx_cands = random.sample(tips_dict.keys(), alpha)\n\n        print(f\"The candidates tips are {apv_tx_cands}\")\n\n        apv_tx_cands_dict = {}\n        for apv_tx in apv_tx_cands:\n            apv_tx_file = f\"./cache/client/txs/{apv_tx}.json\"\n            client.require_tx_from_server(\"localhost\", apv_tx)\n            with open(apv_tx_file) as f:\n                tx_info = json.load(f)\n\n            print(tx_info)\n\n            apv_tx_file = f\"./cache/client/params/iter-{iteration}-{apv_tx}.pkl\"\n\n            while 1:\n                status, code = ipfsGetFile(tx_info['param_hash'], apv_tx_file)\n                print('The filehash of this approved trans is ' + tx_info['param_hash'] + ', and the file is ' + apv_tx_file + '!')\n                if code == 0:\n                    print(status.strip())\n                    print('The apv parasfile ' + apv_tx_file + ' has been downloaded!\\n')\n                    break\n                else:\n                    print(status)\n                    print('\\nFailed to download the apv parasfile ' + apv_tx_file + ' !\\n')\n            apv_tx_cands_dict[apv_tx] = float(tx_info['model_accuracy'])\n\n        apv_trans_final = []\n        if len(apv_tx_cands_dict) == alpha:\n            sort_dict = sorted(apv_tx_cands_dict.items(),key=lambda x:x[1],reverse=True)\n            for i in range(alpha - 1):\n                apv_trans_final.append(sort_dict[i][0])\n        else:\n            apv_trans_final = apv_tx_cands\n\n        print(f\"***************************************************\")\n        print(f\"The candidates tips are {apv_tx_cands}\")\n        print(f\"***************************************************\")\n\n        # aggregating approver parameters\n        w_apv_agg = []\n        for apv_tx in apv_trans_final:\n            apv_param_file = f\"./cache/client/params/iter-{iteration}-{apv_tx}.pkl\"\n            net.load_state_dict(torch.load(apv_param_file))\n            w_tmp_iter = net.state_dict()\n            w_apv_agg.append(copy.deepcopy(w_tmp_iter))\n        \n        if len(w_apv_agg) == 1:\n            w_glob = w_apv_agg[0]\n        else:\n            w_glob = FedAvg(w_apv_agg)\n\n        iteration_base_param_file = f\"./cache/client/params/base-iter-{iteration}.pkl\"\n        torch.save(w_glob, iteration_base_param_file)\n\n        base_model_acc, base_model_loss = model_evaluate(net, w_glob, test_dataset, settings)\n\n        print(base_model_acc)\n\n        while 1:\n            basefileHash, baseSttCode = ipfsAddFile(iteration_base_param_file)\n            if baseSttCode == 0:\n                print('\\nThe base mode parasfile ' + iteration_base_param_file + ' has been uploaded!')\n                print('And the fileHash is ' + basefileHash + '\\n')\n                break\n            else:\n                print('Error: ' + basefileHash)\n                print('\\nFailed to uploaded the aggregated parasfile ' + iteration_base_param_file + ' !\\n')\n\n        taskEpochs = settings.epochs\n        taskInitStatus = \"start\"\n\n        ## initiate task release\n        while 1:\n            taskRelease = subprocess.Popen([f\"./hyperledger_invoke.sh release {taskID} {taskEpochs}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n            trOuts, trErrs = taskRelease.communicate(timeout=10)\n            if taskRelease.poll() == 0:\n                print('*** ' + taskID + ' has been released! ***')\n                print('*** And the detail of this task is ' + trOuts.strip() + '! ***\\n')\n                break\n            else:\n                print(trErrs)\n                print('*** Failed to release ' + taskID + ' ! ***\\n')\n                time.sleep(2)\n\n        ## initiate task with base parameter hash\n        while 1:\n            spcAggModelPublish = subprocess.Popen(args=[f\"./hyperledger_invoke.sh aggregated {taskID} 0 training {basefileHash}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n            aggPubOuts, aggPubErrs = spcAggModelPublish.communicate(timeout=10)\n            if spcAggModelPublish.poll() == 0:\n                print('*** The init aggModel of ' + taskID + ' has been published! ***')\n                print('*** And the detail of the init aggModel is ' + aggPubOuts.strip() + ' ! ***\\n')\n                break\n            else:\n                print(aggPubErrs)\n                print('*** Failed to publish the init aggModel of ' + taskID + ' ! ***\\n')\n        \n        # ## wait the local train\n        time.sleep(10)\n        selectedDevices = [0, 1, 2, 3, 4]\n        currentEpoch = 1\n        aggModelAcc = 50.0\n        while (currentEpoch <= settings.epochs):\n            flagList = set(copy.deepcopy(selectedDevices))\n            w_locals = []\n            while (len(flagList) != 0):\n                flagSet = set()\n                ts = []\n                lock = threading.Lock()\n                for deviceID in flagList:\n                    localFileName = f\"./cache/client/local/{taskID}-{deviceID}-epoch-{str(currentEpoch)}.pkl\"\n                    t = threading.Thread(target=fabric_api.query_local,args=(lock,taskID,deviceID,currentEpoch,flagSet,localFileName,))\n                    t.start()\n                    ts.append(t)\n                for t in ts:\n                    t.join()\n                time.sleep(2)\n                flagList = flagList - flagSet\n            for deviceID in selectedDevices:\n                localFileName = f\"./cache/client/local/{taskID}-{deviceID}-epoch-{str(currentEpoch)}.pkl\"\n                \n                ## check the acc of the models trained by selected device & drop the low quality model\n                canddts_dev_pas = torch.load(localFileName,map_location=torch.device('cpu'))\n                acc_canddts_dev, loss_canddts_dev = model_evaluate(net, canddts_dev_pas, test_dataset, settings)\n                acc_canddts_dev = acc_canddts_dev.cpu().numpy().tolist()\n                print(\"Test acc of the model trained by \"+str(deviceID)+\" is \" + str(acc_canddts_dev))\n                if (acc_canddts_dev - aggModelAcc) < -10:\n                    print(str(deviceID)+\" is a malicious device!\")\n                else:\n                    w_locals.append(copy.deepcopy(canddts_dev_pas))\n\n            w_glob = FedAvg(w_locals)\n            aggEchoParasFile = './cache/client/params/aggModel-iter-'+str(iteration)+'-epoch-'+str(currentEpoch)+'.pkl'\n            torch.save(w_glob, aggEchoParasFile)\n\n            # evalute the acc of datatest\n            aggModelAcc, aggModelLoss = model_evaluate(net, w_glob, test_dataset, settings)\n            aggModelAcc = aggModelAcc.cpu().numpy().tolist()\n\n            print(\"\\n************************************\")\n            print(\"Acc of the agg model of Round \"+str(currentEpoch)+\" in iteration \"+str(iteration)+\" is \"+str(aggModelAcc))\n            print(\"************************************\")\n\n            while 1:\n                aggEchoFileHash, sttCodeAdd = ipfsAddFile(aggEchoParasFile)\n                if sttCodeAdd == 0:\n                    print('\\n*************************')\n                    print('The aggregated parasfile ' + aggEchoParasFile + ' has been uploaded!')\n                    print('And the fileHash is ' + aggEchoFileHash + '!')\n                    print('*************************\\n')\n                    break\n                else:\n                    print('Error: ' + aggEchoFileHash)\n                    print('\\nFailed to uploaded the aggregated parasfile ' + aggEchoParasFile + ' !\\n')\n\n            taskStatus = 'training'\n            if currentEpoch == settings.epochs:\n                taskStatus = 'done'\n            while 1:\n                epochAggModelPublish = subprocess.Popen(args=[f\"./hyperledger_invoke.sh aggregated {taskID} {str(currentEpoch)} {taskStatus} {aggEchoFileHash}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n                aggPubOuts, aggPubErrs = epochAggModelPublish.communicate(timeout=10)\n                if epochAggModelPublish.poll() == 0:\n                    print('\\n******************')\n                    print('The info of task ' + taskID + ' is ' + aggPubOuts.strip())\n                    print('The model aggregated in epoch ' + str(currentEpoch) + ' for ' + taskID + ' has been published!')\n                    print('******************\\n')\n                    break\n                else:\n                    print(aggPubErrs)\n                    print('*** Failed to publish the Model aggregated in epoch ' + str(currentEpoch) + ' for ' + taskID + ' ! ***\\n')\n            currentEpoch += 1\n\n        new_tx = {\"approved_tips\": apv_trans_final, \"model_accuracy\": aggModelAcc, \"param_hash\": aggEchoFileHash, \"shard_id\": 1, \"timestamp\": time.time()}\n        # upload the trans to DAG network\n        client.upload_tx_to_server(\"localhost\", new_tx)\n        print('\\n******************************* Transaction upload *******************************')\n        print('The details of this trans are', new_tx)\n        print('The trans generated in the iteration #%d had been uploaded!'%iteration)\n        print('*************************************************************************************\\n')\n        iteration += 1\n        time.sleep(2)", "    # new_tx_01 = {\"approved_tips\": [], \"model_accuracy\": 34.0, \"param_hash\": \"jyjtyjftyj\", \"shard_id\": 1, \"timestamp\": 1683119166.5689557}\n    # new_tx_02 = {\"approved_tips\": [], \"model_accuracy\": 2.0, \"param_hash\": \"asefasef\", \"shard_id\": 0, \"timestamp\": 2345234525.5689557}\n\n    # new_tx_01 = MainchainTransaction(**new_tx_01)\n    # new_tx_02 = MainchainTransaction(**new_tx_02)\n\n    # client.upload_tx_to_server(\"localhost\", new_tx_01)\n    # client.upload_tx_to_server(\"localhost\", new_tx_02)\n\nif __name__ == \"__main__\":\n    main()", "\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "subchain/ml/__init__.py", "chunked_list": [""]}
{"filename": "subchain/ml/model_build.py", "chunked_list": ["import torch\nimport torchvision\nimport copy\nfrom torchvision import datasets\n\nfrom models.Nets import CnnMnist\nfrom models.test_model import test\nfrom utils.sampling import mnist_noniid\nfrom utils.sampling import mnist_iid\nfrom utils.settings import BaseSettings", "from utils.sampling import mnist_iid\nfrom utils.settings import BaseSettings\nfrom models.train_model import LocalUpdate\nfrom models.FedAvg import FedAvg\nfrom utils.datasets import get_dataset\n\nimport os\nimport shutil\n\ndef model_build(settings):\n    settings.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    dataset_train, dataset_test = get_dataset('mnist')\n\n    # sample dataset by individual user\n    if settings.iid:\n        data_user_mapping = mnist_iid(dataset_train, settings.num_users)\n    else:\n        data_user_mapping = mnist_noniid(dataset_train, settings)\n\n    net = CnnMnist(settings=settings).to(device=settings.device)\n    \n    return net, settings, dataset_train, dataset_test, data_user_mapping", "\ndef model_build(settings):\n    settings.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    dataset_train, dataset_test = get_dataset('mnist')\n\n    # sample dataset by individual user\n    if settings.iid:\n        data_user_mapping = mnist_iid(dataset_train, settings.num_users)\n    else:\n        data_user_mapping = mnist_noniid(dataset_train, settings)\n\n    net = CnnMnist(settings=settings).to(device=settings.device)\n    \n    return net, settings, dataset_train, dataset_test, data_user_mapping", "\ndef model_evaluate(net: torch.nn.Module, params, test_set, settings):\n    net.load_state_dict(params)\n    acc_test, loss_test = test(net, test_set, settings=settings)\n    return acc_test, loss_test\n\n# if __name__ == '__main__':\n#     settings = BaseSettings()\n#     if os.path.exists('../data/local'):\n#         shutil.rmtree('../data/local')", "#     if os.path.exists('../data/local'):\n#         shutil.rmtree('../data/local')\n#     os.mkdir('../data/local')\n#     net, settings, train_dataset, test_dataset, data_user_mapping = model_build(settings)\n#     # sum(p.numel() for p in net.parameters() if p.requires_grad)\n#     users = [0, 1, 2, 3, 4]\n#     local_acc = []\n#     local_losses = []\n#     w_glob = net.state_dict()\n#     weightAggFile = lambda epoch : f\"../data/local/aggWeight-epoch-{epoch}.pkl\"", "#     w_glob = net.state_dict()\n#     weightAggFile = lambda epoch : f\"../data/local/aggWeight-epoch-{epoch}.pkl\"\n#     torch.save(w_glob, weightAggFile(0))\n#     for epoch in range(settings.epochs):\n#         w_locals = []\n#         for user in users:\n#             net.load_state_dict(torch.load(weightAggFile(epoch)))\n#             local = LocalUpdate(settings=settings, dataset=train_dataset, idxs=data_user_mapping[user])\n#             w, loss = local.train(net=copy.deepcopy(net).to(settings.device), user=user)\n#             acc, loss = model_evaluate(net, w, test_dataset, settings)", "#             w, loss = local.train(net=copy.deepcopy(net).to(settings.device), user=user)\n#             acc, loss = model_evaluate(net, w, test_dataset, settings)\n#             local_acc.append(acc)\n#             local_losses.append(loss)\n#             w_locals.append(w)\n#         w_glob = FedAvg(w_locals)\n#         torch.save(w_glob, weightAggFile(epoch + 1))\n#         loss_avg = sum(local_losses) / len(local_losses)\n#         acc_avg = sum(local_acc) / len(local_acc)\n#         print('Epoch {:3d}, Average loss {:.3f}'.format(epoch, loss_avg))", "#         acc_avg = sum(local_acc) / len(local_acc)\n#         print('Epoch {:3d}, Average loss {:.3f}'.format(epoch, loss_avg))\n#         print('Epoch {:3d}, Average acc {:.3f}'.format(epoch, acc_avg))\n\n\n#     print(acc, loss)"]}
{"filename": "subchain/ml/utils/sampling.py", "chunked_list": ["import numpy as np\n\ndef cifar_iid(dataset, num_users):\n    \"\"\"\n    Sample I.I.D. client data from CIFAR10 dataset\n    :param dataset:\n    :param num_users:\n    :return: dict of image index\n    \"\"\"\n    num_items = int(len(dataset)/num_users)\n    data_user_mapping, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        data_user_mapping[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n        all_idxs = list(set(all_idxs) - data_user_mapping[i])\n    return data_user_mapping", "\ndef mnist_iid(dataset, num_users):\n    \"\"\"\n    Sample I.I.D. client data from MNIST dataset\n    :param dataset:\n    :param num_users:\n    :return: dict of image index\n    \"\"\"\n    num_items = int(len(dataset)/num_users)\n    data_user_mapping, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        data_user_mapping[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n        all_idxs = list(set(all_idxs) - data_user_mapping[i])\n    return data_user_mapping", "\n\ndef mnist_noniid(dataset, settings):\n    \"\"\"\n    Sample non-I.I.D client data from MNIST dataset\n    :param dataset:\n    :param num_users:\n    :return:\n    \"\"\"\n    dataset_length = len(dataset)\n    dataset_class_length = int(dataset_length / 10)\n    data_user_mapping = {i: np.array([], dtype='int64') for i in range(settings.num_users)}\n    idxs = np.arange(dataset_length)\n    labels = np.array(dataset.targets)\n\n    # sort labels\n    idxs_labels = np.vstack((idxs, labels))\n    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n    idxs = idxs_labels[0,:]\n\n    for user in range(settings.num_users):\n        preffered_class = np.random.choice(np.arange(10), 1, replace=False).item()\n        start = preffered_class * dataset_class_length\n        pref_indexes = idxs_labels[0, np.arange(start, start + dataset_class_length)]\n        pref_indexes_chosen = np.random.choice(\n            pref_indexes,\n            int(dataset_class_length * settings.non_iid_frac),\n            replace=False\n        )\n        data_user_mapping[user] = np.concatenate((data_user_mapping[user], pref_indexes_chosen), axis=0)\n        import copy\n        idxs_labels_del = copy.deepcopy(idxs_labels)\n        idxs_labels_del = np.delete(idxs_labels_del, slice(start, start + dataset_class_length), axis=1)\n        non_preffered_class = np.random.choice(idxs_labels_del[0,:], int(dataset_class_length * (1 - settings.non_iid_frac)))\n        data_user_mapping[user] = np.concatenate((data_user_mapping[user], non_preffered_class), axis=0)\n        np.random.shuffle(data_user_mapping[user])\n\n    return data_user_mapping"]}
{"filename": "subchain/ml/utils/settings.py", "chunked_list": ["class BaseSettings:\n    epochs = 5\n    num_users = 5\n    client_frac = 0.5\n    non_iid_frac = 0.8\n    epochs_local = 3\n    batch_size_local = 10\n    batch_size = 32\n    learning_rate = 1e-3\n    momentum = 0.9\n    weight_decay = 5e-4\n    split = 'user'\n    device = -1\n\n    iid = False\n    num_channels = 3\n    num_classes = 10\n    kernel_size = 5"]}
{"filename": "subchain/ml/utils/__init__.py", "chunked_list": [""]}
{"filename": "subchain/ml/utils/datasets.py", "chunked_list": ["import torch\nfrom torchvision import datasets, transforms\n\ndef get_dataset(type):\n    if type == 'cifar':\n        train_transform = transforms.Compose(\n            [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n            ]\n        )\n        test_transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n            ]\n        )\n        dataset_train = datasets.CIFAR10('../data/cifar10', train=True, download=True, transform=train_transform)\n        dataset_test = datasets.CIFAR10('../data/cifar10', train=False, download=True, transform=test_transform)\n        return dataset_train, dataset_test\n    else:\n        transform = transforms.Compose(\n            [\n                transforms.ToTensor(), \n                transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        )\n        dataset_train = datasets.MNIST('../data/mnist', train=True, download=True, transform=transform)\n        dataset_test = datasets.MNIST('../data/mnist', train=False, download=True, transform=transform)\n        return dataset_train, dataset_test"]}
{"filename": "subchain/ml/models/Nets.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass CnnNet(nn.Module):\n    def __init__(self, settings):\n        super().__init__()\n        self.conv1 = nn.Conv2d(settings.num_channels, 6, settings.kernel_size)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, settings.num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x", "    \nclass CnnMnist(nn.Module):\n    def __init__(self, settings):\n        super(CnnMnist, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x"]}
{"filename": "subchain/ml/models/FedAvg.py", "chunked_list": ["import copy\nimport torch\n\ndef FedAvg(w):\n    w_avg = copy.deepcopy(w[0])\n    for k in w_avg.keys():\n        for i in range(1, len(w)):\n            w_avg[k] += w[i][k]\n        w_avg[k] = torch.div(w_avg[k], len(w))\n    return w_avg"]}
{"filename": "subchain/ml/models/__init__.py", "chunked_list": [""]}
{"filename": "subchain/ml/models/test_model.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\ndef test(net: nn.Module, test_dataset, settings):\n    net.eval()\n    test_loss = 0\n    correct = 0\n    loader = DataLoader(dataset=test_dataset, batch_size=settings.batch_size)\n    for _, (test_dataset, target) in enumerate(loader):\n        test_dataset, target = test_dataset.cuda(), target.cuda()\n        logits = net(test_dataset)\n        test_loss += F.cross_entropy(logits, target=target, reduction='sum').item()\n        y_pred = logits.data.max(1, keepdim=True)[1]\n        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n\n    test_loss /= len(loader.dataset)\n    accuracy = 100.00 * correct / len(loader.dataset)\n    print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, correct, len(loader.dataset), accuracy))\n    return accuracy, test_loss"]}
{"filename": "subchain/ml/models/train_model.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n\nclass DatasetSplit(Dataset):\n    def __init__(self, dataset, idxs):\n        self.dataset = dataset\n        self.idxs = list(idxs)\n\n    def __len__(self):\n        return len(self.idxs)\n\n    def __getitem__(self, item):\n        image, label = self.dataset[self.idxs[item]]\n        return image, label", "class DatasetSplit(Dataset):\n    def __init__(self, dataset, idxs):\n        self.dataset = dataset\n        self.idxs = list(idxs)\n\n    def __len__(self):\n        return len(self.idxs)\n\n    def __getitem__(self, item):\n        image, label = self.dataset[self.idxs[item]]\n        return image, label", "\n\nclass LocalUpdate(object):\n    def __init__(self, settings, dataset=None, idxs=None):\n        self.settings = settings\n        self.loss_func = nn.CrossEntropyLoss()\n        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.settings.batch_size, shuffle=True)\n\n    def train(self, net, user):\n        net.train()\n\n        # train and update\n        optimizer = torch.optim.SGD(\n            net.parameters(),\n            lr=self.settings.learning_rate,\n            momentum=self.settings.momentum,\n            weight_decay=self.settings.weight_decay\n        )\n\n        epoch_loss = []\n        for iter in range(self.settings.epochs_local):\n            batch_loss = []\n            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n                images, labels = images.to(self.settings.device), labels.to(self.settings.device)\n                net.zero_grad()\n                log_probs = net(images)\n                loss = self.loss_func(log_probs, labels)\n                loss.backward()\n                optimizer.step()\n                if batch_idx % 10 == 0:\n                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, device {}, user {}'.format(\n                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n                               100. * batch_idx / len(self.ldr_train), loss.item(), self.settings.device, user))\n                batch_loss.append(loss.item())\n            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)", "\n"]}
{"filename": "common/ipfs.py", "chunked_list": ["import subprocess\n\ndef ipfsGetFile(hashValue, fileName):\n    \"\"\"\n    Use hashValue to download the file from IPFS network.\n    \"\"\"\n    ipfsGet = subprocess.Popen(args=['ipfs get ' + hashValue + ' -o ' + fileName], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = ipfsGet.communicate(timeout=10)\n    if ipfsGet.poll() == 0:\n        return outs.strip(), ipfsGet.poll()\n    else:\n        return errs.strip(), ipfsGet.poll()", "\ndef ipfsAddFile(fileName):\n    \"\"\"\n    Upload the file to IPFS network and return the exclusive fileHash value.\n    \"\"\"\n    ipfsAdd = subprocess.Popen(args=['ipfs add ' + fileName + ' | tr \\' \\' \\'\\\\n\\' | grep Qm'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n    outs, errs = ipfsAdd.communicate(timeout=10)\n    if ipfsAdd.poll() == 0:\n        return outs.strip(), ipfsAdd.poll()\n    else:\n        return errs.strip(), ipfsAdd.poll()"]}
{"filename": "mainchain/server_run.py", "chunked_list": ["import os\nimport shutil\nimport pathlib\n\nimport dag_model.transaction as transaction\nfrom dag_model.dag import DAG\nfrom  dag_socket import server\n\nCACHE_DIR = \"./cache/\"\nSERVER_DATA_DIR = pathlib.Path(CACHE_DIR) / \"server\"", "CACHE_DIR = \"./cache/\"\nSERVER_DATA_DIR = pathlib.Path(CACHE_DIR) / \"server\"\nTX_DATA_DIR = pathlib.Path(SERVER_DATA_DIR) / \"txs\"\nDAG_DATA_DIR = pathlib.Path(SERVER_DATA_DIR) / \"pools\"\n\ndef main():\n    if os.path.exists(CACHE_DIR) == False:\n        os.mkdir(CACHE_DIR)\n\n    if os.path.exists(SERVER_DATA_DIR):\n        shutil.rmtree(SERVER_DATA_DIR)\n    os.mkdir(SERVER_DATA_DIR)\n\n    if os.path.exists(TX_DATA_DIR):\n        shutil.rmtree(TX_DATA_DIR)\n    os.mkdir(TX_DATA_DIR)\n\n    if os.path.exists(DAG_DATA_DIR):\n        shutil.rmtree(DAG_DATA_DIR)\n    os.mkdir(DAG_DATA_DIR)\n\n    server_dag = DAG()\n\n    # genesis_info = \"QmaBYCmzPQ2emuXpVykLDHra7t8tPiU8reFMkbHpN1rRoo\"\n    # genesis_block = transaction.MainchainTransaction(genesis_info)\n    # genesis_block.tx_name = 'genesis'\n    # transaction.tx_save(genesis_block)\n\n    # server_dag.tx_publish(genesis_block)\n    # server_dag.remove_genesis()\n\n    while True:\n        server.create_server_socket(server_dag)", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "mainchain/__init__.py", "chunked_list": [""]}
{"filename": "mainchain/dag_socket/__init__.py", "chunked_list": [""]}
{"filename": "mainchain/dag_socket/server.py", "chunked_list": ["import socket\nimport threading\nimport sys\nimport struct\nimport os\nimport json\nimport pathlib\n\nfrom dag_model.dag import DAG\nimport dag_model.transaction as transaction", "from dag_model.dag import DAG\nimport dag_model.transaction as transaction\n\nBUFFER_SIZE = 1024\n\ndef create_server_socket(dag_obj, num_shards = 5):\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        s.bind((\"\", 65432))\n        s.listen(num_shards)\n        print(\"DAG server created, awaiting connections...\")\n    except socket.error as msg:\n        print(msg)\n        sys.exit(1)\n    while True:\n        conn, addr = s.accept()\n        t = threading.Thread(target=handle_conn, args=(conn, addr, dag_obj))\n        t.start()", "\ndef file_send(conn: socket, file_addr):\n    try:\n        file_header = struct.pack('64si', os.path.basename(file_addr).encode(),\n                                          os.stat(file_addr).st_size)\n        conn.send(file_header) # send header\n        conn.recv(BUFFER_SIZE) # header response\n        with open(file_addr, 'rb') as f:\n            while True:\n                data = f.read(BUFFER_SIZE)\n                if not data:\n                    break\n                conn.send(data)\n    except Exception as e:\n        print(e)", "\ndef handle_conn(conn: socket, addr, dag_obj: DAG):\n    print(f\"Accept new connection from {addr}\")\n    conn.send((f\"Connection accepted on server.\").encode())\n    while 1:\n        msg = conn.recv(1024).decode()\n        if msg == 'requireTx':\n            conn.send('ok'.encode())\n            recv_data_file = conn.recv(BUFFER_SIZE).decode() # which tx is needed\n            tx_file_addr = f\"./cache/server/txs/{recv_data_file}.json\"\n            file_send(conn, tx_file_addr)\n        elif msg == 'requireTips':\n            tips_file_addr = \"./cache/server/pools/tip_pool.json\"\n            file_send(conn, tips_file_addr)\n        elif msg == 'uploadTx':\n            conn.send('ok'.encode())\n            recv_data = conn.recv(BUFFER_SIZE).decode()\n            json_tx_data = json.loads(recv_data)\n            new_tx = transaction.MainchainTransaction(**json_tx_data)\n            transaction.tx_save(new_tx)\n            dag_obj.tx_publish(new_tx)\n            print(f\"The new block {new_tx.tx_name} has been published!\")\n\n    conn.close()"]}
{"filename": "mainchain/dag_model/dag.py", "chunked_list": ["import json\nimport pathlib\n\nfrom dag_model.transaction import MainchainTransaction\n\nclass DAG:\n    def __init__(self, data_address = './cache/server/pools/',\n                       alpha = 3,\n                       freshness = 60,\n                       genesis_name = \"genesis\") -> None:\n        self.data_address = data_address\n        self.alpha = alpha\n        self.freshness = freshness\n        self.active_nodes_pool = dict()\n        self.tip_nodes_pool = dict()\n        self.genesis_name = genesis_name\n\n    def tx_publish(self, tx: MainchainTransaction) -> None:\n        if len(self.active_nodes_pool) == 0:\n            self.genesis_name = tx.tx_name\n\n        self.active_nodes_pool[tx.tx_name] = tx.timestamp\n        self.tip_nodes_pool[tx.tx_name] = tx.timestamp\n        \n        if (len(tx.approved_tips) + self.alpha) < len(self.tip_nodes_pool):\n            for tip in tx.approved_tips:\n                self.tip_nodes_pool.pop(tip, None)\n        if self.genesis_name in tx.approved_tips:\n            self.tip_nodes_pool.pop(self.genesis_name, None)\n\n        with open(pathlib.Path(self.data_address) / 'active_pool.json', 'w') as f:\n            json.dump(self.active_nodes_pool, f)\n        with open(pathlib.Path(self.data_address) / 'tip_pool.json', 'w') as f:\n            json.dump(self.tip_nodes_pool, f)\n\n    def remove_genesis(self):\n        self.tip_nodes_pool.pop('genesis', None)\n        with open(pathlib.Path(self.data_address) / 'tip_pool.json', 'w') as f:\n            json.dump(self.tip_nodes_pool, f)", ""]}
{"filename": "mainchain/dag_model/transaction.py", "chunked_list": ["import time\nimport json\nimport hashlib\nimport pathlib\n\n\"\"\"\nMainchain transaction as described in Fig. 5\nAlso referred as Mainchain Regular Block\n\ntx_hash          - hash of the block (transation)", "\ntx_hash          - hash of the block (transation)\nshard_id         - id of the node (shard) invoking the transaction\napproved_tips    - approve tips set (list of rank)\ntimestamp        - timestamp of the block\nmodel_accuracy   - accuracy of the aggregated model\nparam_hash       - hash of the parameters file\n\n\"\"\"\n\nclass MainchainTransaction:\n    def __init__(self,\n                 param_hash,\n                 timestamp = time.time(),\n                 shard_id = 0,\n                 approved_tips = [],\n                 model_accuracy = 0.0,\n                ) -> None:\n        self.param_hash = param_hash\n        self.timestamp = timestamp\n        self.shard_id = shard_id\n        self.approved_tips = approved_tips\n        self.model_accuracy = model_accuracy\n        self.tx_hash = self.hash()\n        self.tx_name = f\"shard_{self.shard_id}_{str(self.timestamp)}\"\n\n    def hash(self):\n        header = {\n            'shard_id': self.shard_id,\n            'approved_tips': self.approved_tips,\n            'timestamp': self.timestamp,\n        }\n\n        data_to_hash = json.dumps(header,\n                                  default=lambda obj: obj.__dict__,\n                                  sort_keys=True)\n        \n        data_encoded = data_to_hash.encode()\n        return hashlib.sha256(data_encoded).hexdigest()\n    \n    def json_output(self):\n        return {\n            'param_hash': self.param_hash,\n            'timestamp': self.timestamp,\n            'shard_id': self.shard_id,\n            'approved_tips': self.approved_tips,\n            'model_accuracy': self.model_accuracy,\n        }", "\"\"\"\n\nclass MainchainTransaction:\n    def __init__(self,\n                 param_hash,\n                 timestamp = time.time(),\n                 shard_id = 0,\n                 approved_tips = [],\n                 model_accuracy = 0.0,\n                ) -> None:\n        self.param_hash = param_hash\n        self.timestamp = timestamp\n        self.shard_id = shard_id\n        self.approved_tips = approved_tips\n        self.model_accuracy = model_accuracy\n        self.tx_hash = self.hash()\n        self.tx_name = f\"shard_{self.shard_id}_{str(self.timestamp)}\"\n\n    def hash(self):\n        header = {\n            'shard_id': self.shard_id,\n            'approved_tips': self.approved_tips,\n            'timestamp': self.timestamp,\n        }\n\n        data_to_hash = json.dumps(header,\n                                  default=lambda obj: obj.__dict__,\n                                  sort_keys=True)\n        \n        data_encoded = data_to_hash.encode()\n        return hashlib.sha256(data_encoded).hexdigest()\n    \n    def json_output(self):\n        return {\n            'param_hash': self.param_hash,\n            'timestamp': self.timestamp,\n            'shard_id': self.shard_id,\n            'approved_tips': self.approved_tips,\n            'model_accuracy': self.model_accuracy,\n        }", "\n\n# tx fs storage functions\n\n\ndef tx_read(tx_name: str) -> MainchainTransaction:\n    with open(pathlib.Path('./cache/server/txs/') / f\"{tx_name}.json\", 'r') as f:\n        object_params = json.load(f)\n    return MainchainTransaction(**object_params)\n", "\n\ndef tx_save(tx: MainchainTransaction) -> None:\n    tx_file_path = pathlib.Path('./cache/server/txs/') / f\"{tx.tx_name}.json\"\n    try:\n        with open(tx_file_path, 'w') as f:\n            f.write(\n                json.dumps(\n                    tx.json_output(),\n                    default=lambda obj: obj.__dict__,\n                    sort_keys=True,\n                )\n            )\n    except Exception as e:\n        print(e)", ""]}
