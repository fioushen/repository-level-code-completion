{"filename": "tests/test_duckdb_pandas_compare.py", "chunked_list": ["import numpy as np\nimport teehr.queries.pandas as tqk\nimport teehr.queries.duckdb as tqu\nfrom pathlib import Path\n\nTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\nPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\nCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\nGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")", "CROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\nGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\n\ndef test_metric_compare_1():\n    include_metrics = [\n        \"primary_count\",\n        \"secondary_count\",\n        \"primary_minimum\",\n        \"secondary_minimum\",\n        \"primary_maximum\",\n        \"secondary_maximum\",\n        \"primary_average\",\n        \"secondary_average\",\n        \"primary_sum\",\n        \"secondary_sum\",\n        \"primary_variance\",\n        \"secondary_variance\",\n        \"max_value_delta\",\n        \"bias\",\n        \"nash_sutcliffe_efficiency\",\n        \"kling_gupta_efficiency\",\n        \"mean_error\",\n        \"mean_squared_error\",\n        \"root_mean_squared_error\",\n    ]\n    group_by = [\n        \"primary_location_id\",\n        \"reference_time\"\n    ]\n    args = {\n        \"primary_filepath\": PRIMARY_FILEPATH,\n        \"secondary_filepath\": SECONDARY_FILEPATH,\n        \"crosswalk_filepath\": CROSSWALK_FILEPATH,\n        \"geometry_filepath\": GEOMETRY_FILEPATH,\n        \"group_by\": group_by,\n        \"order_by\": [\"primary_location_id\"],\n        \"include_metrics\": include_metrics,\n        \"return_query\": False\n    }\n    pandas_df = tqk.get_metrics(**args)\n    duckdb_df = tqu.get_metrics(**args)\n\n    for m in include_metrics:\n        # print(m)\n        duckdb_np = duckdb_df[m].to_numpy()\n        pandas_np = pandas_df[m].to_numpy()\n        assert np.allclose(duckdb_np, pandas_np)", "\n\ndef test_metric_compare_time_metrics():\n    include_metrics = [\n        \"primary_max_value_time\",\n        \"secondary_max_value_time\",\n        \"max_value_timedelta\",\n    ]\n    group_by = [\n        \"primary_location_id\",\n        \"reference_time\"\n    ]\n    args = {\n        \"primary_filepath\": PRIMARY_FILEPATH,\n        \"secondary_filepath\": SECONDARY_FILEPATH,\n        \"crosswalk_filepath\": CROSSWALK_FILEPATH,\n        \"geometry_filepath\": GEOMETRY_FILEPATH,\n        \"group_by\": group_by,\n        \"order_by\": [\"primary_location_id\"],\n        \"include_metrics\": include_metrics,\n        \"return_query\": False\n    }\n    pandas_df = tqk.get_metrics(**args)\n    duckdb_df = tqu.get_metrics(**args)\n\n    for m in include_metrics:\n        duckdb_np = duckdb_df[m].astype('int64').to_numpy()\n        pandas_np = pandas_df[m].astype('int64').to_numpy()\n        assert np.allclose(duckdb_np, pandas_np)", "\n\nif __name__ == \"__main__\":\n    test_metric_compare_1()\n    test_metric_compare_time_metrics()\n    pass\n"]}
{"filename": "tests/test_remote_nwm_filelist_generation.py", "chunked_list": ["import pandas as pd\nfrom pathlib import Path\nfrom teehr.loading.utils_nwm import build_remote_nwm_filelist\n\n\ndef test_remote_filelist():\n    run = \"analysis_assim\"\n    output_type = \"channel_rt\"\n    t_minus_hours = [2]\n\n    start_date = \"2023-03-18\"\n    ingest_days = 1\n\n    component_paths = build_remote_nwm_filelist(\n        run,\n        output_type,\n        start_date,\n        ingest_days,\n        t_minus_hours,\n    )\n\n    test_list_path = Path(\"tests\", \"data\", \"test_remote_list.csv\")\n    test_df = pd.read_csv(test_list_path)\n    test_list = test_df[\"filename\"].to_list()\n\n    test_list.sort()\n    component_paths.sort()\n\n    assert test_list == component_paths", "\n\nif __name__ == \"__main__\":\n    test_remote_filelist()\n    pass\n"]}
{"filename": "tests/test_duckdb_filter_formatting.py", "chunked_list": ["from datetime import datetime\n\nimport pytest\nimport teehr.models.queries as tmq\nfrom pydantic import ValidationError\nimport teehr.queries.utils as tqu\n\n\ndef test_filter_string():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"=\",\n        value=\"123456\"\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id = '123456'\"", "def test_filter_string():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"=\",\n        value=\"123456\"\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id = '123456'\"\n\n\ndef test_filter_int():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"=\",\n        value=123456\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id = 123456\"", "\n\ndef test_filter_int():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"=\",\n        value=123456\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id = 123456\"", "\n\ndef test_filter_int_gte():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\">=\",\n        value=123456\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id >= 123456\"", "\n\ndef test_filter_int_lt():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"<\",\n        value=123456\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id < 123456\"", "\n\ndef test_filter_float():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"=\",\n        value=123.456\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id = 123.456\"", "\n\ndef test_filter_datetime():\n    filter = tmq.JoinedFilter(\n        column=\"reference_time\",\n        operator=\"=\",\n        value=datetime(2023, 4, 1, 23, 30)\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"sf.reference_time = '2023-04-01 23:30:00'\"", "\n\ndef test_in_filter_string_wrong_operator():\n    with pytest.raises(ValidationError):\n        filter = tmq.JoinedFilter(\n            column=\"secondary_location_id\",\n            operator=\"=\",\n            value=[\"123456\", \"9876\"]\n        )\n        tqu._format_filter_item(filter)", "\n\ndef test_in_filter_string_wrong_value_type():\n    with pytest.raises(ValidationError):\n        filter = tmq.JoinedFilter(\n            column=\"secondary_location_id\",\n            operator=\"in\",\n            value=\"9876\"\n        )\n        tqu._format_filter_item(filter)", "\n\ndef test_in_filter_string():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"in\",\n        value=[\"123456\", \"9876\"]\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id in ('123456','9876')\"", "\n\ndef test_in_filter_int():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"in\",\n        value=[123456, 9876]\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id in (123456,9876)\"", "\n\ndef test_in_filter_float():\n    filter = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"in\",\n        value=[123.456, 98.76]\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"secondary_location_id in (123.456,98.76)\"", "\n\ndef test_in_filter_datetime():\n    filter = tmq.JoinedFilter(\n        column=\"reference_time\",\n        operator=\"in\",\n        value=[datetime(2023, 4, 1, 23, 30), datetime(2023, 4, 2, 23, 30)]\n    )\n    filter_str = tqu._format_filter_item(filter)\n    assert filter_str == \"sf.reference_time in ('2023-04-01 23:30:00','2023-04-02 23:30:00')\"  # noqa", "\n\nif __name__ == \"__main__\":\n    test_filter_string()\n    test_filter_int()\n    test_filter_int_gte()\n    test_filter_int_lt()\n    test_filter_float()\n    test_filter_datetime()\n    test_in_filter_string_wrong_operator()\n    test_in_filter_string_wrong_value_type()\n    test_in_filter_string()\n    test_in_filter_int()\n    test_in_filter_float()\n    test_in_filter_datetime()\n    pass", ""]}
{"filename": "tests/test_pandas_metric_queries.py", "chunked_list": ["import pandas as pd\nimport geopandas as gpd\n# import pytest\n# from pydantic import ValidationError\nimport teehr.queries.pandas as tqk\nfrom pathlib import Path\n\nTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\nPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")", "PRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\nCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\nGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\n\ndef test_metric_query_df():\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        group_by=[\"primary_location_id\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_df2():\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        group_by=[\"primary_location_id\", \"reference_time\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 9\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_filter_df():\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        group_by=[\"primary_location_id\", \"reference_time\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n        filters=[\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"gage-A\"\n            },\n        ]\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_gdf():\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=[\"primary_location_id\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n        include_geometry=True,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert isinstance(query_df, gpd.GeoDataFrame)", "\n\ndef test_metric_query_df_limit_metrics():\n    include_metrics = [\n        \"bias\",\n        \"root_mean_squared_error\",\n        \"nash_sutcliffe_efficiency\",\n        \"kling_gupta_efficiency\",\n        \"mean_error\",\n        \"mean_squared_error\",\n        ]\n    group_by = [\"primary_location_id\"]\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\"],\n        include_metrics=include_metrics,\n        return_query=False,\n        include_geometry=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_df_time_metrics():\n    include_metrics = [\n        \"primary_max_value_time\",\n        \"secondary_max_value_time\",\n        \"max_value_timedelta\"\n    ]\n    group_by = [\"primary_location_id\", \"reference_time\"]\n    query_df = tqk.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\", \"reference_time\"],\n        include_metrics=include_metrics,\n        return_query=False,\n        include_geometry=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 9\n    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n    assert isinstance(query_df, pd.DataFrame)", "\n\nif __name__ == \"__main__\":\n    # test_metric_query_df()\n    # test_metric_query_df2()\n    # test_metric_query_filter_df()\n    # test_metric_query_gdf()\n    # test_metric_query_df_limit_metrics()\n    test_metric_query_df_time_metrics()\n    pass", ""]}
{"filename": "tests/test_duckdb_metric_filter_formatting.py", "chunked_list": ["from datetime import datetime\n\nimport teehr.models.queries as tmq\nimport teehr.queries.utils as tqu\n\n\ndef test_multiple_filters():\n    filter_1 = tmq.JoinedFilter(\n        column=\"secondary_location_id\",\n        operator=\"in\",\n        value=[\"123456\", \"9876543\"]\n    )\n    filter_2 = tmq.JoinedFilter(\n        column=\"reference_time\",\n        operator=\"=\",\n        value=datetime(2023, 1, 1, 0, 0, 0)\n    )\n    filter_str = tqu.filters_to_sql([filter_1, filter_2])\n    assert filter_str == \"WHERE secondary_location_id in ('123456','9876543') AND sf.reference_time = '2023-01-01 00:00:00'\"  # noqa", "\n\ndef test_no_filters():\n    filter_str = tqu.filters_to_sql([])\n    assert filter_str == \"--no where clause\"\n\n\nif __name__ == \"__main__\":\n    test_multiple_filters()\n    test_no_filters()\n    pass", ""]}
{"filename": "tests/test_duckdb_timeseries_queries.py", "chunked_list": ["import pandas as pd\nimport geopandas as gpd\nimport teehr.queries.duckdb as tqu\nfrom pathlib import Path\nfrom datetime import datetime\n\nTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\nPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\nCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")", "SECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\nCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\nGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\n\ndef test_joined_timeseries_query_df():\n    query_df = tqu.get_joined_timeseries(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        order_by=[\"primary_location_id\", \"lead_time\"],\n        return_query=False\n    )\n\n    # print(query_df.info())\n    assert len(query_df) == 3 * 3 * 24\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_joined_timeseries_query_gdf():\n    query_df = tqu.get_joined_timeseries(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        order_by=[\"primary_location_id\", \"lead_time\"],\n        return_query=False,\n        include_geometry=True,\n    )\n\n    # print(query_df.info())\n    assert len(query_df) == 3 * 3 * 24\n    assert isinstance(query_df, gpd.GeoDataFrame)", "\n\ndef test_joined_timeseries_query_df_filter():\n    query_df = tqu.get_joined_timeseries(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        order_by=[\"primary_location_id\", \"lead_time\"],\n        return_query=False,\n        filters=[\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"2022-01-01 00:00:00\"\n            },\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"gage-A\"\n            },\n        ]\n    )\n\n    # print(query_df.info())\n    assert len(query_df) == 24\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_timeseries_query_df():\n    query_df = tqu.get_timeseries(\n        timeseries_filepath=PRIMARY_FILEPATH,\n        order_by=[\"location_id\"],\n        return_query=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 26*3", "\n\ndef test_timeseries_query_df2():\n    query_df = tqu.get_timeseries(\n        timeseries_filepath=SECONDARY_FILEPATH,\n        order_by=[\"location_id\"],\n        return_query=False,\n    )\n    assert len(query_df) == 24*3*3\n", "\n\ndef test_timeseries_query_one_site_df():\n    query_df = tqu.get_timeseries(\n        timeseries_filepath=PRIMARY_FILEPATH,\n        order_by=[\"location_id\"],\n        filters=[{\n            \"column\": \"location_id\",\n            \"operator\": \"=\",\n            \"value\": \"gage-C\"\n        }],\n        return_query=False,\n    )\n    assert len(query_df) == 26", "\n\ndef test_timeseries_query_one_site_one_ref_df():\n    query_df = tqu.get_timeseries(\n        timeseries_filepath=SECONDARY_FILEPATH,\n        order_by=[\"value_time\"],\n        filters=[\n            {\n                \"column\": \"location_id\",\n                \"operator\": \"=\",\n                \"value\": \"fcst-1\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": datetime(2022, 1, 1)\n            },\n        ],\n        return_query=False,\n    )\n    assert len(query_df) == 24", "\n\ndef test_timeseries_char_query_df():\n    query_df = tqu.get_timeseries_chars(\n        timeseries_filepath=PRIMARY_FILEPATH,\n        group_by=[\"location_id\"],\n        order_by=[\"location_id\"],\n        return_query=False,\n    )\n    df = pd.DataFrame(\n        {\n            'location_id': {0: 'gage-A', 1: 'gage-B', 2: 'gage-C'},\n            'count': {0: 26, 1: 26, 2: 26},\n            'min': {0: 0.1, 1: 10.1, 2: 0.0},\n            'max': {0: 5.0, 1: 15.0, 2: 180.0},\n            'average': {\n                0: 1.2038461538461542,\n                1: 11.203846153846156,\n                2: 100.38461538461539\n            },\n            'sum': {\n                0: 31.300000000000008,\n                1: 291.30000000000007,\n                2: 2610.0\n            },\n            'variance': {\n                0: 1.9788313609467447,\n                1: 1.9788313609467456,\n                2: 2726.7751479289923\n            },\n            'max_value_time': {\n                0: pd.Timestamp('2022-01-01 15:00:00'),\n                1: pd.Timestamp('2022-01-01 15:00:00'),\n                2: pd.Timestamp('2022-01-01 06:00:00')\n            }\n        }\n    )\n    assert df.equals(query_df)", "\n\ndef test_timeseries_char_query_df2():\n    query_df = tqu.get_timeseries_chars(\n        timeseries_filepath=SECONDARY_FILEPATH,\n        group_by=[\"location_id\", \"reference_time\"],\n        order_by=[\"location_id\"],\n        return_query=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 9", "\n\ndef test_timeseries_char_query_filter_df():\n    query_df = tqu.get_timeseries_chars(\n        timeseries_filepath=SECONDARY_FILEPATH,\n        group_by=[\"location_id\"],\n        order_by=[\"location_id\"],\n        return_query=False,\n        filters=[\n            {\n                \"column\": \"location_id\",\n                \"operator\": \"=\",\n                \"value\": \"fcst-1\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": datetime(2022, 1, 1)\n            },\n        ],\n    )\n    assert len(query_df) == 1", "\n\nif __name__ == \"__main__\":\n    test_joined_timeseries_query_df()\n    test_joined_timeseries_query_gdf()\n    test_joined_timeseries_query_df_filter()\n    test_timeseries_query_df()\n    test_timeseries_query_df2()\n    test_timeseries_query_one_site_one_ref_df()\n    test_timeseries_char_query_df()\n    test_timeseries_char_query_df2()\n    test_timeseries_char_query_filter_df()\n    pass", ""]}
{"filename": "tests/test_weight_generation.py", "chunked_list": ["import pandas as pd\nfrom pathlib import Path\nfrom teehr.loading.generate_weights import generate_weights_file\n\nTEST_DIR = Path(\"tests\", \"data\")\nTEMPLATE_FILEPATH = Path(TEST_DIR, \"test_template_grid.nc\")\nZONES_FILEPATH = Path(TEST_DIR, \"test_ngen_divides.parquet\")\nWEIGHTS_FILEPATH = Path(TEST_DIR, \"test_weights_results.parquet\")\n\n\ndef test_weights():\n    df = generate_weights_file(\n        zone_polygon_filepath=ZONES_FILEPATH,\n        template_dataset=TEMPLATE_FILEPATH,\n        variable_name=\"RAINRATE\",\n        output_weights_filepath=None,\n        unique_zone_id=\"id\",\n    )\n\n    df_test = pd.read_parquet(WEIGHTS_FILEPATH)\n\n    assert df.equals(df_test)", "\n\ndef test_weights():\n    df = generate_weights_file(\n        zone_polygon_filepath=ZONES_FILEPATH,\n        template_dataset=TEMPLATE_FILEPATH,\n        variable_name=\"RAINRATE\",\n        output_weights_filepath=None,\n        unique_zone_id=\"id\",\n    )\n\n    df_test = pd.read_parquet(WEIGHTS_FILEPATH)\n\n    assert df.equals(df_test)", "\n\nif __name__ == \"__main__\":\n    test_weights()\n    pass\n"]}
{"filename": "tests/test_duckdb_metric_queries.py", "chunked_list": ["import pandas as pd\nimport geopandas as gpd\nimport pytest\nfrom pydantic import ValidationError\nimport teehr.queries.duckdb as tqu\nfrom pathlib import Path\n\nTEST_STUDY_DIR = Path(\"tests\", \"data\", \"test_study\")\nPRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")", "PRIMARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_obs.parquet\")\nSECONDARY_FILEPATH = Path(TEST_STUDY_DIR, \"timeseries\", \"*_fcast.parquet\")\nCROSSWALK_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"crosswalk.parquet\")\nGEOMETRY_FILEPATH = Path(TEST_STUDY_DIR, \"geo\", \"gages.parquet\")\n\n\ndef test_metric_query_str():\n    query_str = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=[\"primary_location_id\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=True\n    )\n    # print(query_str)\n    assert type(query_str) == str", "\n\ndef test_metric_query_df():\n\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        group_by=[\"primary_location_id\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_gdf():\n\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=[\"primary_location_id\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n        include_geometry=True,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert isinstance(query_df, gpd.GeoDataFrame)", "\n\ndef test_metric_query_gdf_2():\n\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=[\"primary_location_id\", \"reference_time\"],\n        order_by=[\"primary_location_id\"],\n        include_metrics=\"all\",\n        return_query=False,\n        include_geometry=True,\n    )\n    # print(query_df)\n    assert len(query_df) == 9\n    assert isinstance(query_df, gpd.GeoDataFrame)", "\n\ndef test_metric_query_gdf_no_geom():\n    with pytest.raises(ValidationError):\n        tqu.get_metrics(\n            primary_filepath=PRIMARY_FILEPATH,\n            secondary_filepath=SECONDARY_FILEPATH,\n            crosswalk_filepath=CROSSWALK_FILEPATH,\n            group_by=[\"primary_location_id\", \"reference_time\"],\n            order_by=[\"primary_location_id\"],\n            include_metrics=\"all\",\n            return_query=False,\n            include_geometry=True,\n        )", "\n\ndef test_metric_query_gdf_missing_group_by():\n    with pytest.raises(ValidationError):\n        tqu.get_metrics(\n            primary_filepath=PRIMARY_FILEPATH,\n            secondary_filepath=SECONDARY_FILEPATH,\n            crosswalk_filepath=CROSSWALK_FILEPATH,\n            geometry_filepath=GEOMETRY_FILEPATH,\n            group_by=[\"reference_time\"],\n            order_by=[\"primary_location_id\"],\n            include_metrics=\"all\",\n            return_query=False,\n            include_geometry=True,\n        )", "\n\ndef test_metric_query_df_2():\n    include_metrics = [\n        \"primary_count\",\n        \"secondary_count\",\n        \"primary_minimum\",\n        \"secondary_minimum\",\n        \"primary_maximum\",\n        \"secondary_maximum\",\n        \"primary_average\",\n        \"secondary_average\",\n        \"primary_sum\",\n        \"secondary_sum\",\n        \"primary_variance\",\n        \"secondary_variance\",\n        \"max_value_delta\",\n        \"bias\",\n        \"nash_sutcliffe_efficiency\",\n        \"kling_gupta_efficiency\",\n        \"mean_error\",\n        \"mean_squared_error\",\n        \"root_mean_squared_error\",\n    ]\n    group_by = [\"primary_location_id\"]\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\"],\n        include_metrics=include_metrics,\n        return_query=False,\n        include_geometry=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_df_time_metrics():\n    include_metrics = [\n        \"primary_max_value_time\",\n        \"secondary_max_value_time\",\n        \"max_value_timedelta\"\n    ]\n    group_by = [\"primary_location_id\", \"reference_time\"]\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\", \"reference_time\"],\n        include_metrics=include_metrics,\n        return_query=False,\n        include_geometry=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 9\n    assert len(query_df.columns) == len(group_by) + len(include_metrics)\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_df_all():\n    group_by = [\"primary_location_id\", \"reference_time\"]\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\", \"reference_time\"],\n        include_metrics=\"all\",\n        return_query=False,\n        include_geometry=False,\n    )\n    # print(query_df)\n    assert len(query_df) == 9\n    assert len(query_df.columns) == len(group_by) + 22\n    assert isinstance(query_df, pd.DataFrame)", "\n\ndef test_metric_query_value_time_filter():\n    group_by = [\"primary_location_id\", \"reference_time\"]\n    query_df = tqu.get_metrics(\n        primary_filepath=PRIMARY_FILEPATH,\n        secondary_filepath=SECONDARY_FILEPATH,\n        crosswalk_filepath=CROSSWALK_FILEPATH,\n        geometry_filepath=GEOMETRY_FILEPATH,\n        group_by=group_by,\n        order_by=[\"primary_location_id\", \"reference_time\"],\n        include_metrics=\"all\",\n        return_query=False,\n        include_geometry=False,\n        filters=[\n            {\n                \"column\": \"value_time\",\n                \"operator\": \">=\",\n                \"value\": f\"{'2022-01-01 13:00:00'}\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \">=\",\n                \"value\": f\"{'2022-01-01 02:00:00'}\"\n            }\n        ],\n    )\n    # print(query_df)\n    assert len(query_df) == 3\n    assert query_df[\"primary_count\"].iloc[0] == 13\n    assert isinstance(query_df, pd.DataFrame)", "\n\nif __name__ == \"__main__\":\n    test_metric_query_str()\n    test_metric_query_df()\n    test_metric_query_gdf()\n    test_metric_query_gdf_2()\n    test_metric_query_gdf_no_geom()\n    test_metric_query_gdf_missing_group_by()\n    test_metric_query_df_2()\n    test_metric_query_df_time_metrics()\n    test_metric_query_df_all()\n    test_metric_query_value_time_filter()\n    pass", ""]}
{"filename": "tests/data/test_study/geo/convert.py", "chunked_list": ["\"\"\"\nThis simple script converts `crosswalk.csv` and `gages.geojson` \nto parquet files.\n\nTo run this:\n```bash\n$ cd cd tests/data/test_study/geo/\n$ python convert.py\n\n```", "\n```\n\"\"\"\nimport pandas as pd\nimport geopandas as gpd\n\nprint(f\"crosswalk.csv\")\ndf = pd.read_csv(\"crosswalk.csv\")\ndf.to_parquet(\"crosswalk.parquet\")\n", "df.to_parquet(\"crosswalk.parquet\")\n\nprint(f\"gages.geojson\")\ngdf = gpd.read_file(\"gages.geojson\")\ngdf.to_parquet(\"gages.parquet\")"]}
{"filename": "tests/data/test_study/timeseries/convert.py", "chunked_list": ["\"\"\"\nThis simple script converts `test_short_fcast.csv` and `test_short_obs.csv` \nto parquet files.\n\nTo run this:\n```bash\n$ cd cd tests/data/test_study/timeseries/\n$ python convert.py\n\n```", "\n```\n\"\"\"\nimport pandas as pd\n\nprint(f\"test_short_fcast.csv\")\ndf = pd.read_csv(\"test_short_fcast.csv\", parse_dates=['reference_time', 'value_time'])\ndf.to_parquet(\"test_short_fcast.parquet\")\nprint(df.info())\n", "print(df.info())\n\nprint(f\"test_short_obs.csv\")\ndf = pd.read_csv(\"test_short_obs.csv\", parse_dates=['reference_time', 'value_time'])\ndf.to_parquet(\"test_short_obs.parquet\")\nprint(df.info())"]}
{"filename": "study_template/study_1/scripts/script.py", "chunked_list": [""]}
{"filename": "study_template/study_1/dashboards/utils.py", "chunked_list": [""]}
{"filename": "src/teehr/__init__.py", "chunked_list": [""]}
{"filename": "src/teehr/queries/pandas.py", "chunked_list": ["import numpy as np\nimport pandas as pd\nimport geopandas as gpd\n# import dask.dataframe as dd\n\nfrom hydrotools.metrics import metrics as hm\n\nfrom typing import List, Union\n\nimport teehr.models.queries as tmq", "\nimport teehr.models.queries as tmq\nimport teehr.queries.duckdb as tqu\n\n\nSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef get_metrics(\n    primary_filepath: str,\n    secondary_filepath: str,\n    crosswalk_filepath: str,\n    group_by: List[str],\n    order_by: List[str],\n    include_metrics: Union[List[tmq.MetricEnum], \"all\"],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n    geometry_filepath: Union[str, None] = None,\n    include_geometry: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Calculate performance metrics using a Pandas or Dask DataFrame.\n\n    Parameters\n    ----------\n    primary_filepath : str\n        File path to the \"observed\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    secondary_filepath : str\n        File path to the \"forecast\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    crosswalk_filepath : str\n        File path to single crosswalk file.\n    group_by : List[str]\n        List of column/field names to group timeseries data by.\n        Must provide at least one.\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    include_metrics = List[str]\n        List of metrics (see below) for allowable list, or \"all\" to return all\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n    include_geometry: bool = True\n        True joins the geometry to the query results.\n        Only works if `primary_location_id`\n        is included as a group_by field.\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Available Metrics\n    -----------------------\n    Basic\n    * primary_count\n    * secondary_count\n    * primary_minimum\n    * secondary_minimum\n    * primary_maximum\n    * secondary_maximum\n    * primary_average\n    * secondary_average\n    * primary_sum\n    * secondary_sum\n    * primary_variance\n    * secondary_variance\n    * max_value_delta\n        max(secondary_value) - max(primary_value)\n    * bias\n        sum(primary_value - secondary_value)/count(*)\n\n    HydroTools Metrics\n    * nash_sutcliffe_efficiency\n    * kling_gupta_efficiency\n    * coefficient_of_extrapolation\n    * coefficient_of_persistence\n    * mean_error\n    * mean_squared_error\n    * root_mean_squared_error\n\n    Time-based Metrics\n    * primary_max_value_time\n    * secondary_max_value_time\n    * max_value_timedelta\n\n    Examples\n    --------\n        group_by = [\"lead_time\", \"primary_location_id\"]\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"'123456'\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"'2022-01-01 00:00'\"\n            },\n            {\n                \"column\": \"lead_time\",\n                \"operator\": \"<=\",\n                \"value\": \"'10 days'\"\n            }\n        ]\n        include_metrics=[\"nash_sutcliffe_efficiency\"]\n    \"\"\"\n\n    mq = tmq.MetricQuery.parse_obj(\n        {\n            \"primary_filepath\": primary_filepath,\n            \"secondary_filepath\": secondary_filepath,\n            \"crosswalk_filepath\": crosswalk_filepath,\n            \"group_by\": group_by,\n            \"order_by\": order_by,\n            \"include_metrics\": include_metrics,\n            \"filters\": filters,\n            \"return_query\": return_query,\n            \"include_geometry\": include_geometry,\n            \"geometry_filepath\": geometry_filepath\n        }\n    )\n\n    if mq.return_query:\n        raise ValueError(\n            \"`return query` is not a valid option \"\n            \"for `dataframe.get_metrics()`.\"\n        )\n\n    # This loads all the timeseries in memory\n    df = tqu.get_joined_timeseries(\n        primary_filepath=mq.primary_filepath,\n        secondary_filepath=mq.secondary_filepath,\n        crosswalk_filepath=mq.crosswalk_filepath,\n        order_by=mq.order_by,\n        filters=mq.filters,\n        return_query=False,\n    )\n\n    # Pandas DataFrame GroupBy approach (works).\n    grouped = df.groupby(mq.group_by, as_index=False)\n\n    calculated_metrics = grouped.apply(\n        calculate_group_metrics,\n        include_metrics=include_metrics\n    )\n\n    # Dask DataFrame GroupBy approach (does not work).\n    # ddf = dd.from_pandas(df, npartitions=4)\n    # calculated_metrics = ddf.groupby(mq.group_by).apply(\n    #     calculate_metrics_on_groups,\n    #     metrics=[\"primary_count\"],\n    #     meta={\"primary_count\": \"int\"}\n    # ).compute()\n\n    if mq.include_geometry:\n        gdf = gpd.read_parquet(mq.geometry_filepath)\n        merged_gdf = gdf.merge(\n            calculated_metrics,\n            left_on=\"id\",\n            right_on=\"primary_location_id\"\n        )\n        return merged_gdf\n\n    return calculated_metrics", "def get_metrics(\n    primary_filepath: str,\n    secondary_filepath: str,\n    crosswalk_filepath: str,\n    group_by: List[str],\n    order_by: List[str],\n    include_metrics: Union[List[tmq.MetricEnum], \"all\"],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n    geometry_filepath: Union[str, None] = None,\n    include_geometry: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Calculate performance metrics using a Pandas or Dask DataFrame.\n\n    Parameters\n    ----------\n    primary_filepath : str\n        File path to the \"observed\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    secondary_filepath : str\n        File path to the \"forecast\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    crosswalk_filepath : str\n        File path to single crosswalk file.\n    group_by : List[str]\n        List of column/field names to group timeseries data by.\n        Must provide at least one.\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    include_metrics = List[str]\n        List of metrics (see below) for allowable list, or \"all\" to return all\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n    include_geometry: bool = True\n        True joins the geometry to the query results.\n        Only works if `primary_location_id`\n        is included as a group_by field.\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Available Metrics\n    -----------------------\n    Basic\n    * primary_count\n    * secondary_count\n    * primary_minimum\n    * secondary_minimum\n    * primary_maximum\n    * secondary_maximum\n    * primary_average\n    * secondary_average\n    * primary_sum\n    * secondary_sum\n    * primary_variance\n    * secondary_variance\n    * max_value_delta\n        max(secondary_value) - max(primary_value)\n    * bias\n        sum(primary_value - secondary_value)/count(*)\n\n    HydroTools Metrics\n    * nash_sutcliffe_efficiency\n    * kling_gupta_efficiency\n    * coefficient_of_extrapolation\n    * coefficient_of_persistence\n    * mean_error\n    * mean_squared_error\n    * root_mean_squared_error\n\n    Time-based Metrics\n    * primary_max_value_time\n    * secondary_max_value_time\n    * max_value_timedelta\n\n    Examples\n    --------\n        group_by = [\"lead_time\", \"primary_location_id\"]\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"'123456'\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"'2022-01-01 00:00'\"\n            },\n            {\n                \"column\": \"lead_time\",\n                \"operator\": \"<=\",\n                \"value\": \"'10 days'\"\n            }\n        ]\n        include_metrics=[\"nash_sutcliffe_efficiency\"]\n    \"\"\"\n\n    mq = tmq.MetricQuery.parse_obj(\n        {\n            \"primary_filepath\": primary_filepath,\n            \"secondary_filepath\": secondary_filepath,\n            \"crosswalk_filepath\": crosswalk_filepath,\n            \"group_by\": group_by,\n            \"order_by\": order_by,\n            \"include_metrics\": include_metrics,\n            \"filters\": filters,\n            \"return_query\": return_query,\n            \"include_geometry\": include_geometry,\n            \"geometry_filepath\": geometry_filepath\n        }\n    )\n\n    if mq.return_query:\n        raise ValueError(\n            \"`return query` is not a valid option \"\n            \"for `dataframe.get_metrics()`.\"\n        )\n\n    # This loads all the timeseries in memory\n    df = tqu.get_joined_timeseries(\n        primary_filepath=mq.primary_filepath,\n        secondary_filepath=mq.secondary_filepath,\n        crosswalk_filepath=mq.crosswalk_filepath,\n        order_by=mq.order_by,\n        filters=mq.filters,\n        return_query=False,\n    )\n\n    # Pandas DataFrame GroupBy approach (works).\n    grouped = df.groupby(mq.group_by, as_index=False)\n\n    calculated_metrics = grouped.apply(\n        calculate_group_metrics,\n        include_metrics=include_metrics\n    )\n\n    # Dask DataFrame GroupBy approach (does not work).\n    # ddf = dd.from_pandas(df, npartitions=4)\n    # calculated_metrics = ddf.groupby(mq.group_by).apply(\n    #     calculate_metrics_on_groups,\n    #     metrics=[\"primary_count\"],\n    #     meta={\"primary_count\": \"int\"}\n    # ).compute()\n\n    if mq.include_geometry:\n        gdf = gpd.read_parquet(mq.geometry_filepath)\n        merged_gdf = gdf.merge(\n            calculated_metrics,\n            left_on=\"id\",\n            right_on=\"primary_location_id\"\n        )\n        return merged_gdf\n\n    return calculated_metrics", "\n\ndef calculate_group_metrics(\n        group: pd.DataFrame,\n        include_metrics: Union[List[str], str]\n):\n    \"\"\"Calculate metrics on a pd.DataFrame.\n\n    Note this approach to calculating metrics is not as fast as\n    `teehr.queries.duckdb.get_metrics()` but is easier to update\n    and contains more metrics.  It also serves as the reference\n    implementation for the duckdb queries.\n\n    Parameters\n    ----------\n    group : pd.DataFrame\n        Represents a population group to calculate the metrics on\n    include_metrics = List[str]\n        List of metrics (see below) for allowable list, or \"all\" to\n        return all\n\n\n    Returns\n    -------\n    calculated_metrics : pd.DataFrame\n\n\n    Available Metrics\n    -----------------------\n    Basic\n    * primary_count\n    * secondary_count\n    * primary_minimum\n    * secondary_minimum\n    * primary_maximum\n    * secondary_maximum\n    * primary_average\n    * secondary_average\n    * primary_sum\n    * secondary_sum\n    * primary_variance\n    * secondary_variance\n    * max_value_delta\n        max(secondary_value) - max(primary_value)\n    * bias\n        sum(primary_value - secondary_value)/count(*)\n\n    HydroTools Metrics\n    * nash_sutcliffe_efficiency\n    * kling_gupta_efficiency\n    * coefficient_of_extrapolation\n    * coefficient_of_persistence\n    * mean_error\n    * mean_squared_error\n    * root_mean_squared_error\n\n    Time-based Metrics\n    * primary_max_value_time\n    * secondary_max_value_time\n    * max_value_timedelta\n\n    \"\"\"\n    data = {}\n\n    # Simple Metrics\n    if include_metrics == \"all\" or \"primary_count\" in include_metrics:\n        data[\"primary_count\"] = len(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_count\" in include_metrics:\n        data[\"secondary_count\"] = len(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"primary_minimum\" in include_metrics:\n        data[\"primary_minimum\"] = np.min(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_minimum\" in include_metrics:\n        data[\"secondary_minimum\"] = np.min(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"primary_maximum\" in include_metrics:\n        data[\"primary_maximum\"] = np.max(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_maximum\" in include_metrics:\n        data[\"secondary_maximum\"] = np.max(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"primary_average\" in include_metrics:\n        data[\"primary_average\"] = np.mean(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_average\" in include_metrics:\n        data[\"secondary_average\"] = np.mean(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"primary_sum\" in include_metrics:\n        data[\"primary_sum\"] = np.sum(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_sum\" in include_metrics:\n        data[\"secondary_sum\"] = np.sum(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"primary_variance\" in include_metrics:\n        data[\"primary_variance\"] = np.var(group[\"primary_value\"])\n\n    if include_metrics == \"all\" or \"secondary_variance\" in include_metrics:\n        data[\"secondary_variance\"] = np.var(group[\"secondary_value\"])\n\n    if include_metrics == \"all\" or \"bias\" in include_metrics:\n        group[\"difference\"] = group[\"primary_value\"] - group[\"secondary_value\"]\n        data[\"bias\"] = np.sum(group[\"difference\"])/len(group)\n\n    if include_metrics == \"all\" or \"max_value_delta\" in include_metrics:\n        data[\"max_value_delta\"] = (\n            np.max(group[\"secondary_value\"])\n            - np.max(group[\"primary_value\"])\n        )\n\n    # HydroTools Forecast Metrics\n    if (\n        include_metrics == \"all\"\n        or \"nash_sutcliffe_efficiency\" in include_metrics\n    ):\n        nse = hm.nash_sutcliffe_efficiency(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"nash_sutcliffe_efficiency\"] = nse\n\n    if include_metrics == \"all\" or \"kling_gupta_efficiency\" in include_metrics:\n        kge = hm.kling_gupta_efficiency(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"kling_gupta_efficiency\"] = kge\n\n    if (\n        include_metrics == \"all\"\n        or \"coefficient_of_extrapolation\" in include_metrics\n    ):\n        coe = hm.coefficient_of_extrapolation(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"coefficient_of_extrapolation\"] = coe\n\n    if (\n        include_metrics == \"all\"\n        or \"coefficient_of_persistence\" in include_metrics\n    ):\n        cop = hm.coefficient_of_persistence(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"coefficient_of_persistence\"] = cop\n\n    if include_metrics == \"all\" or \"mean_error\" in include_metrics:\n        me = hm.mean_error(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"mean_error\"] = me\n\n    if include_metrics == \"all\" or \"mean_squared_error\" in include_metrics:\n        mse = hm.mean_squared_error(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"mean_squared_error\"] = mse\n\n    if (\n        include_metrics == \"all\"\n        or \"root_mean_squared_error\" in include_metrics\n    ):\n        rmse = hm.root_mean_squared_error(\n            group[\"primary_value\"],\n            group[\"secondary_value\"]\n        )\n        data[\"root_mean_squared_error\"] = rmse\n\n    # Time-based Metrics\n    time_indexed_df = group.set_index(\"value_time\")\n    if (\n        include_metrics == \"all\"\n        or \"primary_max_value_time\" in include_metrics\n    ):\n        pmvt = time_indexed_df[\"primary_value\"].idxmax()\n        data[\"primary_max_value_time\"] = pmvt\n\n    if (\n        include_metrics == \"all\"\n        or \"secondary_max_value_time\" in include_metrics\n    ):\n        smvt = time_indexed_df[\"secondary_value\"].idxmax()\n        data[\"secondary_max_value_time\"] = smvt\n\n    if (\n        include_metrics == \"all\"\n        or \"max_value_timedelta\" in include_metrics\n    ):\n        pmvt = time_indexed_df[\"primary_value\"].idxmax()\n        smvt = time_indexed_df[\"secondary_value\"].idxmax()\n        data[\"max_value_timedelta\"] = smvt - pmvt\n    return pd.Series(data)", ""]}
{"filename": "src/teehr/queries/__init__.py", "chunked_list": ["import duckdb"]}
{"filename": "src/teehr/queries/utils.py", "chunked_list": ["import pandas as pd\nimport geopandas as gpd\nimport warnings\n\nfrom collections.abc import Iterable\nfrom datetime import datetime\nfrom typing import List, Union\n\nfrom teehr.models.queries import (\n    JoinedFilter,", "from teehr.models.queries import (\n    JoinedFilter,\n    MetricQuery,\n    JoinedTimeseriesQuery,\n    TimeseriesFilter,\n)\n\nSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef _get_datetime_list_string(values):\n    return [f\"'{v.strftime(SQL_DATETIME_STR_FORMAT)}'\" for v in values]", "\n\ndef _get_datetime_list_string(values):\n    return [f\"'{v.strftime(SQL_DATETIME_STR_FORMAT)}'\" for v in values]\n\n\ndef _format_iterable_value(\n        values: Iterable[Union[str, int, float, datetime]]\n) -> str:\n    \"\"\"Returns an SQL formatted string from list of values.\n\n    Parameters\n    ----------\n    values : Iterable\n        Contains values to be formatted as a string for SQL. Only one type of\n        value (str, int, float, datetime) should be used. First value in list\n        is used to determine value type. Values are not checked for type\n        consistency.\n\n    Returns\n    -------\n    formatted_string : str\n\n    \"\"\"\n\n    # string\n    if isinstance(values[0], str):\n        return f\"\"\"({\",\".join([f\"'{v}'\" for v in values])})\"\"\"\n    # int or float\n    elif (\n        isinstance(values[0], int)\n        or isinstance(values[0], float)\n    ):\n        return f\"\"\"({\",\".join([f\"{v}\" for v in values])})\"\"\"\n    # datetime\n    elif isinstance(values[0], datetime):\n        return f\"\"\"({\",\".join(_get_datetime_list_string(values))})\"\"\"\n    else:\n        warnings.warn(\n            \"treating value as string because didn't know what else to do.\"\n        )\n        return f\"\"\"({\",\".join([f\"'{str(v)}'\" for v in values])})\"\"\"", "\n\ndef _format_filter_item(filter: Union[JoinedFilter, TimeseriesFilter]) -> str:\n    \"\"\"Returns an SQL formatted string for single filter object.\n\n    Parameters\n    ----------\n    filter: models.*Filter\n        A single *Filter object.\n\n    Returns\n    -------\n    formatted_string : str\n\n    \"\"\"\n    column = filter.column\n    if column == \"value_time\":\n        column = \"sf.value_time\"\n    if column == \"reference_time\":\n        column = \"sf.reference_time\"\n\n    if isinstance(filter.value, str):\n        return f\"\"\"{column} {filter.operator} '{filter.value}'\"\"\"\n    elif (\n        isinstance(filter.value, int)\n        or isinstance(filter.value, float)\n    ):\n        return f\"\"\"{column} {filter.operator} {filter.value}\"\"\"\n    elif isinstance(filter.value, datetime):\n        dt_str = filter.value.strftime(SQL_DATETIME_STR_FORMAT)\n        return f\"\"\"{column} {filter.operator} '{dt_str}'\"\"\"\n    elif (\n        isinstance(filter.value, Iterable)\n        and not isinstance(filter.value, str)\n    ):\n        value = _format_iterable_value(filter.value)\n        return f\"\"\"{column} {filter.operator} {value}\"\"\"\n    else:\n        warnings.warn(\n            \"treating value as string because didn't know what else to do.\"\n        )\n        return f\"\"\"{column} {filter.operator} '{str(filter.value)}'\"\"\"", "\n\ndef filters_to_sql(filters: List[JoinedFilter]) -> List[str]:\n    \"\"\"Generate SQL where clause string from filters.\n\n    Parameters\n    ----------\n    filters : List[MetricFilter]\n        A list of MetricFilter objects describing the filters.\n\n    Returns\n    -------\n    where_clause : str\n        A where clause formatted string\n    \"\"\"\n    if len(filters) > 0:\n        filter_strs = []\n        for f in filters:\n            filter_strs.append(_format_filter_item(f))\n        qry = f\"\"\"WHERE {f\" AND \".join(filter_strs)}\"\"\"\n        return qry\n\n    return \"--no where clause\"", "\n\ndef geometry_join_clause(\n        q: Union[MetricQuery, JoinedTimeseriesQuery]\n) -> str:\n    \"\"\"Generate the join clause for\"\"\"\n    if q.include_geometry:\n        return f\"\"\"JOIN read_parquet('{str(q.geometry_filepath)}') gf\n            on pf.location_id = gf.id\n        \"\"\"\n    return \"\"", "\n\ndef geometry_select_clause(\n        q: Union[MetricQuery, JoinedTimeseriesQuery]\n) -> str:\n    if q.include_geometry:\n        return \",gf.geometry as geometry\"\n    return \"\"\n\n\ndef metric_geometry_join_clause(\n        q: Union[MetricQuery, JoinedTimeseriesQuery]\n) -> str:\n    \"\"\"Generate the join clause for\"\"\"\n    if q.include_geometry:\n        return f\"\"\"JOIN read_parquet('{str(q.geometry_filepath)}') gf\n            on primary_location_id = gf.id\n        \"\"\"\n    return \"\"", "\n\ndef metric_geometry_join_clause(\n        q: Union[MetricQuery, JoinedTimeseriesQuery]\n) -> str:\n    \"\"\"Generate the join clause for\"\"\"\n    if q.include_geometry:\n        return f\"\"\"JOIN read_parquet('{str(q.geometry_filepath)}') gf\n            on primary_location_id = gf.id\n        \"\"\"\n    return \"\"", "\n\ndef _join_time_on(join: str, join_to: str, join_on: List[str]):\n    qry = f\"\"\"\n        INNER JOIN {join}\n        ON {f\" AND \".join([f\"{join}.{jo} = {join_to}.{jo}\" for jo in join_on])}\n        AND {join}.n = 1\n    \"\"\"\n    return qry\n", "\n\ndef _join_on(join: str, join_to: str, join_on: List[str]) -> str:\n    qry = f\"\"\"\n        INNER JOIN {join}\n        ON {f\" AND \".join([f\"{join}.{jo} = {join_to}.{jo}\" for jo in join_on])}\n    \"\"\"\n    return qry\n\n\ndef _nse_cte(mq: MetricQuery) -> str:\n    if (\n        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return f\"\"\"\n        ,nse AS (\n            SELECT\n                {\",\".join(mq.group_by)}\n                , value_time\n                , pow(\n                    primary_value - secondary_value, 2\n                ) as primary_minus_secondary_squared\n                , pow(\n                    primary_value\n                    - avg(primary_value)\n                    OVER(PARTITION BY {\",\".join(mq.group_by)}), 2\n                ) as primary_minus_primary_mean_squared\n            FROM joined\n        )\n        \"\"\"\n    return \"\"", "\n\ndef _nse_cte(mq: MetricQuery) -> str:\n    if (\n        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return f\"\"\"\n        ,nse AS (\n            SELECT\n                {\",\".join(mq.group_by)}\n                , value_time\n                , pow(\n                    primary_value - secondary_value, 2\n                ) as primary_minus_secondary_squared\n                , pow(\n                    primary_value\n                    - avg(primary_value)\n                    OVER(PARTITION BY {\",\".join(mq.group_by)}), 2\n                ) as primary_minus_primary_mean_squared\n            FROM joined\n        )\n        \"\"\"\n    return \"\"", "\n\ndef _pmxt_cte(mq: MetricQuery) -> str:\n    if (\n        \"primary_max_value_time\" in mq.include_metrics\n        or \"max_value_timedelta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return f\"\"\"\n            , pmxt AS (\n                SELECT\n                    {\",\".join(mq.group_by)}\n                    , primary_value as value\n                    , value_time\n                    , ROW_NUMBER() OVER(\n                        PARTITION BY {\",\".join(mq.group_by)}\n                        ORDER BY value DESC, value_time\n                    ) as n\n                FROM joined\n            )\n        \"\"\"\n    return \"\"", "\n\ndef _smxt_cte(mq: MetricQuery) -> str:\n    if (\n        \"secondary_max_value_time\" in mq.include_metrics\n        or \"max_value_timedelta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return f\"\"\"\n            , smxt AS (\n            SELECT\n                {\",\".join(mq.group_by)}\n                , secondary_value as value\n                , value_time\n                , ROW_NUMBER() OVER(\n                    PARTITION BY {\",\".join(mq.group_by)}\n                    ORDER BY value DESC, value_time\n                ) as n\n            FROM joined\n        )\n        \"\"\"\n    return \"\"", "\n\ndef _join_nse_cte(mq: MetricQuery) -> str:\n    if (\n        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return f\"\"\"\n            {_join_on(join=\"nse\", join_to=\"joined\", join_on=mq.group_by)}\n            AND nse.value_time = joined.value_time\n        \"\"\"\n    return \"\"", "\n\ndef _select_max_value_timedelta(mq: MetricQuery) -> str:\n    if (\n        \"max_value_timedelta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", smxt.value_time - pmxt.value_time as max_value_timedelta\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_max_value_time(mq: MetricQuery) -> str:\n    if (\n        \"secondary_max_value_time\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", smxt.value_time as secondary_max_value_time\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_max_value_time(mq: MetricQuery) -> str:\n    if (\n        \"primary_max_value_time\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", pmxt.value_time as primary_max_value_time\"\"\"\n    return \"\"\n", "\n\ndef _select_root_mean_squared_error(mq: MetricQuery) -> str:\n    if (\n        \"root_mean_squared_error\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sqrt(sum(power(absolute_difference, 2))/count(*))\n            as root_mean_squared_error\n        \"\"\"\n    return \"\"", "\n\ndef _select_mean_squared_error(mq: MetricQuery) -> str:\n    if (\n        \"mean_squared_error\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sum(power(absolute_difference, 2))/count(*)\n            as mean_squared_error\n        \"\"\"\n    return \"\"", "\n\ndef _select_mean_error(mq: MetricQuery) -> str:\n    if (\n        \"mean_error\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sum(absolute_difference)/count(*) as mean_error\"\"\"\n    return \"\"\n", "\n\ndef _select_kling_gupta_efficiency(mq: MetricQuery) -> str:\n    if (\n        \"kling_gupta_efficiency\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", 1 - sqrt(\n            pow(corr(secondary_value, primary_value) - 1, 2)\n            + pow(stddev(secondary_value)\n                / stddev(primary_value) - 1, 2)\n            + pow(avg(secondary_value) / avg(primary_value) - 1, 2)\n        ) AS kling_gupta_efficiency\n        \"\"\"\n    return \"\"", "\n\ndef _select_nash_sutcliffe_efficiency(mq: MetricQuery) -> str:\n    if (\n        \"nash_sutcliffe_efficiency\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", 1 - (\n                sum(nse.primary_minus_secondary_squared)\n                /sum(nse.primary_minus_primary_mean_squared)\n            ) as nash_sutcliffe_efficiency\n        \"\"\"\n    return \"\"", "\n\ndef _select_bias(mq: MetricQuery) -> str:\n    if (\n        \"bias\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sum(primary_value - secondary_value)/count(*) as bias\"\"\"\n    return \"\"\n", "\n\ndef _select_max_value_delta(mq: MetricQuery) -> str:\n    if (\n        \"max_value_delta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", max(secondary_value) - max(primary_value)\n            as max_value_delta\n        \"\"\"\n    return \"\"", "\n\ndef _select_primary_count(mq: MetricQuery) -> str:\n    if (\n        \"primary_count\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", count(primary_value) as primary_count\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_count(mq: MetricQuery) -> str:\n    if (\n        \"secondary_count\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", count(secondary_value) as secondary_count\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_minimum(mq: MetricQuery) -> str:\n    if (\n        \"primary_minimum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", min(primary_value) as primary_minimum\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_minimum(mq: MetricQuery) -> str:\n    if (\n        \"secondary_minimum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", min(secondary_value) as secondary_minimum\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_maximum(mq: MetricQuery) -> str:\n    if (\n        \"primary_maximum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", max(primary_value) as primary_maximum\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_maximum(mq: MetricQuery) -> str:\n    if (\n        \"secondary_maximum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", max(secondary_value) as secondary_maximum\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_average(mq: MetricQuery) -> str:\n    if (\n        \"primary_average\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", avg(primary_value) as primary_average\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_average(mq: MetricQuery) -> str:\n    if (\n        \"secondary_average\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", avg(secondary_value) as secondary_average\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_sum(mq: MetricQuery) -> str:\n    if (\n        \"primary_sum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sum(primary_value) as primary_sum\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_sum(mq: MetricQuery) -> str:\n    if (\n        \"secondary_sum\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", sum(secondary_value) as secondary_sum\"\"\"\n    return \"\"\n", "\n\ndef _select_primary_variance(mq: MetricQuery) -> str:\n    if (\n        \"primary_variance\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", var_pop(primary_value) as primary_variance\"\"\"\n    return \"\"\n", "\n\ndef _select_secondary_variance(mq: MetricQuery) -> str:\n    if (\n        \"secondary_variance\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return \"\"\", var_pop(secondary_value) as secondary_variance\"\"\"\n    return \"\"\n", "\n\ndef _join_primary_join_max_time(mq: MetricQuery) -> str:\n    if (\n        \"primary_max_value_time\" in mq.include_metrics\n        or \"max_value_timedelta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return _join_time_on(\n            join=\"pmxt\",\n            join_to=\"metrics\",\n            join_on=mq.group_by\n        )\n    return \"\"", "\n\ndef _join_secondary_join_max_time(mq: MetricQuery) -> str:\n    if (\n        \"secondary_max_value_time\" in mq.include_metrics\n        or \"max_value_timedelta\" in mq.include_metrics\n        or mq.include_metrics == \"all\"\n    ):\n        return _join_time_on(\n            join=\"smxt\",\n            join_to=\"metrics\",\n            join_on=mq.group_by\n        )\n    return \"\"", "\n\ndef df_to_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n    \"\"\"Convert pd.DataFrame to gpd.GeoDataFrame.\n\n    When the `geometry` column is read from a parquet file using DuckBD\n    it is a bytearray in the resulting pd.DataFrame.  The `geometry` needs\n    to be convert to bytes before GeoPandas can work with it.  This function\n    does that.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with a `geometry` column that has geometry stored as\n        a bytearray.\n\n    Returns\n    -------\n    gdf : gpd.GeoDataFrame\n        GeoDataFrame with a valid `geometry` column.\n\n    \"\"\"\n    df[\"geometry\"] = gpd.GeoSeries.from_wkb(\n        df[\"geometry\"].apply(lambda x: bytes(x))\n        )\n    return gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=\"geometry\")", "\n\ndef remove_empty_lines(text: str) -> str:\n    \"\"\"Remove empty lines from string.\"\"\"\n    return \"\".join([s for s in text.splitlines(True) if s.strip()])"]}
{"filename": "src/teehr/queries/duckdb.py", "chunked_list": ["import duckdb\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom typing import List, Union\n\nfrom teehr.models.queries import (\n    MetricQuery,\n    JoinedTimeseriesQuery,", "    MetricQuery,\n    JoinedTimeseriesQuery,\n    TimeseriesQuery,\n    TimeseriesCharQuery,\n)\n\nimport teehr.queries.utils as tqu\nimport teehr.models.queries as tmq\n\nSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"", "\nSQL_DATETIME_STR_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n\ndef get_metrics(\n    primary_filepath: str,\n    secondary_filepath: str,\n    crosswalk_filepath: str,\n    group_by: List[str],\n    order_by: List[str],\n    include_metrics: Union[List[tmq.MetricEnum], \"all\"],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n    geometry_filepath: Union[str, None] = None,\n    include_geometry: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Calculate performance metrics using database queries.\n\n    Parameters\n    ----------\n    primary_filepath : str\n        File path to the \"observed\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    secondary_filepath : str\n        File path to the \"forecast\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    crosswalk_filepath : str\n        File path to single crosswalk file.\n    group_by : List[str]\n        List of column/field names to group timeseries data by.\n        Must provide at least one.\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    include_metrics = List[str]\n        List of metrics (see below) for allowable list, or \"all\" to return all\n        Placeholder, currently ignored -> returns \"all\"\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n    include_geometry: bool = True\n        True joins the geometry to the query results.\n        Only works if `primary_location_id`\n        is included as a group_by field.\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Filter, Order By and Group By Fields\n    -----------------------------------\n    * reference_time\n    * primary_location_id\n    * secondary_location_id\n    * primary_value\n    * secondary_value\n    * value_time\n    * configuration\n    * measurement_unit\n    * variable_name\n    * lead_time\n\n    Available Metrics\n    -----------------------\n    Basic\n    * primary_count\n    * secondary_count\n    * primary_minimum\n    * secondary_minimum\n    * primary_maximum\n    * secondary_maximum\n    * primary_average\n    * secondary_average\n    * primary_sum\n    * secondary_sum\n    * primary_variance\n    * secondary_variance\n    * max_value_delta\n        max(secondary_value) - max(primary_value)\n    * bias\n        sum(primary_value - secondary_value)/count(*)\n\n    HydroTools Metrics\n    * nash_sutcliffe_efficiency\n    * kling_gupta_efficiency\n    * coefficient_of_extrapolation\n    * coefficient_of_persistence\n    * mean_error\n    * mean_squared_error\n    * root_mean_squared_error\n\n    Time-based Metrics\n    * primary_max_value_time\n    * secondary_max_value_time\n    * max_value_timedelta\n\n    Examples:\n        group_by = [\"lead_time\", \"primary_location_id\"]\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"'123456'\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"'2022-01-01 00:00'\"\n            },\n            {\n                \"column\": \"lead_time\",\n                \"operator\": \"<=\",\n                \"value\": \"'10 days'\"\n            }\n        ]\n    \"\"\"\n\n    mq = MetricQuery.parse_obj(\n        {\n            \"primary_filepath\": primary_filepath,\n            \"secondary_filepath\": secondary_filepath,\n            \"crosswalk_filepath\": crosswalk_filepath,\n            \"group_by\": group_by,\n            \"order_by\": order_by,\n            \"include_metrics\": include_metrics,\n            \"filters\": filters,\n            \"return_query\": return_query,\n            \"include_geometry\": include_geometry,\n            \"geometry_filepath\": geometry_filepath\n        }\n    )\n\n    query = f\"\"\"\n        WITH joined as (\n            SELECT\n                sf.reference_time\n                , sf.value_time as value_time\n                , sf.location_id as secondary_location_id\n                , sf.value as secondary_value\n                , sf.configuration\n                , sf.measurement_unit\n                , sf.variable_name\n                , pf.value as primary_value\n                , pf.location_id as primary_location_id\n                , sf.value_time - sf.reference_time as lead_time\n                , abs(pf.value - sf.value) as absolute_difference\n            FROM read_parquet('{str(mq.secondary_filepath)}') sf\n            JOIN read_parquet('{str(mq.crosswalk_filepath)}') cf\n                on cf.secondary_location_id = sf.location_id\n            JOIN read_parquet('{str(mq.primary_filepath)}') pf\n                on cf.primary_location_id = pf.location_id\n                and sf.value_time = pf.value_time\n                and sf.measurement_unit = pf.measurement_unit\n                and sf.variable_name = pf.variable_name\n            {tqu.filters_to_sql(mq.filters)}\n        )\n        {tqu._nse_cte(mq)}\n        {tqu._pmxt_cte(mq)}\n        {tqu._smxt_cte(mq)}\n        , metrics AS (\n            SELECT\n                {\",\".join([f\"joined.{gb}\" for gb in mq.group_by])}\n                {tqu._select_primary_count(mq)}\n                {tqu._select_secondary_count(mq)}\n                {tqu._select_primary_minimum(mq)}\n                {tqu._select_secondary_minimum(mq)}\n                {tqu._select_primary_maximum(mq)}\n                {tqu._select_secondary_maximum(mq)}\n                {tqu._select_primary_average(mq)}\n                {tqu._select_secondary_average(mq)}\n                {tqu._select_primary_sum(mq)}\n                {tqu._select_secondary_sum(mq)}\n                {tqu._select_primary_variance(mq)}\n                {tqu._select_secondary_variance(mq)}\n                {tqu._select_max_value_delta(mq)}\n                {tqu._select_bias(mq)}\n                {tqu._select_nash_sutcliffe_efficiency(mq)}\n                {tqu._select_kling_gupta_efficiency(mq)}\n                {tqu._select_mean_error(mq)}\n                {tqu._select_mean_squared_error(mq)}\n                {tqu._select_root_mean_squared_error(mq)}\n            FROM\n                joined\n            {tqu._join_nse_cte(mq)}\n            GROUP BY\n                {\",\".join([f\"joined.{gb}\" for gb in mq.group_by])}\n        )\n        SELECT\n            metrics.*\n            {tqu._select_primary_max_value_time(mq)}\n            {tqu._select_secondary_max_value_time(mq)}\n            {tqu._select_max_value_timedelta(mq)}\n            {tqu.geometry_select_clause(mq)}\n        FROM metrics\n        {tqu.metric_geometry_join_clause(mq)}\n        {tqu._join_primary_join_max_time(mq)}\n        {tqu._join_secondary_join_max_time(mq)}\n        ORDER BY\n            {\",\".join([f\"metrics.{gb}\" for gb in mq.group_by])}\n    ;\"\"\"\n\n    if mq.return_query:\n        return tqu.remove_empty_lines(query)\n\n    df = duckdb.query(query).to_df()\n\n    if mq.include_geometry:\n        return tqu.df_to_gdf(df)\n\n    return df", "\n\ndef get_joined_timeseries(\n    primary_filepath: str,\n    secondary_filepath: str,\n    crosswalk_filepath: str,\n    order_by: List[str],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n    geometry_filepath: Union[str, None] = None,\n    include_geometry: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Retrieve joined timeseries using database query.\n\n    Parameters\n    ----------\n    primary_filepath : str\n        File path to the \"observed\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    secondary_filepath : str\n        File path to the \"forecast\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    crosswalk_filepath : str\n        File path to single crosswalk file.\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n    include_geometry: bool = True\n        True joins the geometry to the query results.\n        Only works if `primary_location_id`.\n        is included as a group_by field.\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Filter and Order By Fields\n    --------------------------\n    * reference_time\n    * primary_location_id\n    * secondary_location_id\n    * primary_value\n    * secondary_value\n    * value_time\n    * configuration\n    * measurement_unit\n    * variable_name\n    * lead_time\n\n    Examples:\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"'123456'\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"'2022-01-01 00:00'\"\n            },\n            {\n                \"column\": \"lead_time\",\n                \"operator\": \"<=\",\n                \"value\": \"'10 days'\"\n            }\n        ]\n    \"\"\"\n\n    jtq = JoinedTimeseriesQuery.parse_obj(\n        {\n            \"primary_filepath\": primary_filepath,\n            \"secondary_filepath\": secondary_filepath,\n            \"crosswalk_filepath\": crosswalk_filepath,\n            \"order_by\": order_by,\n            \"filters\": filters,\n            \"return_query\": return_query,\n            \"include_geometry\": include_geometry,\n            \"geometry_filepath\": geometry_filepath\n        }\n    )\n\n    query = f\"\"\"\n        WITH joined as (\n            SELECT\n                sf.reference_time,\n                sf.value_time,\n                sf.location_id as secondary_location_id,\n                sf.value as secondary_value,\n                sf.configuration,\n                sf.measurement_unit,\n                sf.variable_name,\n                pf.value as primary_value,\n                pf.location_id as primary_location_id,\n                sf.value_time - sf.reference_time as lead_time\n                {tqu.geometry_select_clause(jtq)}\n            FROM read_parquet('{str(jtq.secondary_filepath)}') sf\n            JOIN read_parquet('{str(jtq.crosswalk_filepath)}') cf\n                on cf.secondary_location_id = sf.location_id\n            JOIN read_parquet('{str(jtq.primary_filepath)}') pf\n                on cf.primary_location_id = pf.location_id\n                and sf.value_time = pf.value_time\n                and sf.measurement_unit = pf.measurement_unit\n                and sf.variable_name = pf.variable_name\n            {tqu.geometry_join_clause(jtq)}\n            {tqu.filters_to_sql(jtq.filters)}\n        )\n        SELECT\n            *\n        FROM\n            joined\n        ORDER BY\n            {\",\".join(jtq.order_by)}\n    ;\"\"\"\n\n    if jtq.return_query:\n        return tqu.remove_empty_lines(query)\n\n    df = duckdb.query(query).to_df()\n\n    df[\"primary_location_id\"] = df[\"primary_location_id\"].astype(\"category\")\n    df[\"secondary_location_id\"] = df[\"secondary_location_id\"].astype(\"category\")  # noqa\n    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n\n    if jtq.include_geometry:\n        return tqu.df_to_gdf(df)\n\n    return df", "\n\ndef get_timeseries(\n    timeseries_filepath: str,\n    order_by: List[str],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Retrieve joined timeseries using database query.\n\n    Parameters\n    ----------\n    timeseries_filepath : str\n        File path to the timeseries data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Filter and Order By Fields\n    --------------------------\n    * value_time\n    * location_id\n    * value\n    * measurement_unit\n    * reference_time\n    * configuration\n    * variable_name\n\n    Examples:\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"location_id\",\n                \"operator\": \"in\",\n                \"value\": [12345, 54321]\n            },\n        ]\n    \"\"\"\n    tq = TimeseriesQuery.parse_obj(\n        {\n            \"timeseries_filepath\": timeseries_filepath,\n            \"order_by\": order_by,\n            \"filters\": filters,\n            \"return_query\": return_query\n        }\n    )\n\n    query = f\"\"\"\n        WITH joined as (\n            SELECT\n                sf.reference_time,\n                sf.value_time,\n                sf.location_id,\n                sf.value,\n                sf.configuration,\n                sf.measurement_unit,\n                sf.variable_name\n            FROM\n                read_parquet('{str(tq.timeseries_filepath)}') sf\n            {tqu.filters_to_sql(tq.filters)}\n        )\n        SELECT * FROM\n            joined\n        ORDER BY\n            {\",\".join(tq.order_by)}\n    ;\"\"\"\n\n    if tq.return_query:\n        return tqu.remove_empty_lines(query)\n\n    df = duckdb.query(query).to_df()\n\n    df[\"location_id\"] = df[\"location_id\"].astype(\"category\")\n    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n\n    return df", "\n\ndef get_timeseries_chars(\n    timeseries_filepath: str,\n    group_by: list[str],\n    order_by: List[str],\n    filters: Union[List[dict], None] = None,\n    return_query: bool = False,\n) -> Union[str, pd.DataFrame, gpd.GeoDataFrame]:\n    \"\"\"Retrieve joined timeseries using database query.\n\n    Parameters\n    ----------\n    timeseries_filepath : str\n        File path to the \"observed\" data.  String must include path to file(s)\n        and can include wildcards.  For example, \"/path/to/parquet/*.parquet\"\n    order_by : List[str]\n        List of column/field names to order results by.\n        Must provide at least one.\n    group_by : List[str]\n        List of column/field names to group timeseries data by.\n        Must provide at least one.\n    filters : Union[List[dict], None] = None\n        List of dictionaries describing the \"where\" clause to limit data that\n        is included in metrics.\n    return_query: bool = False\n        True returns the query string instead of the data\n\n    Returns\n    -------\n    results : Union[str, pd.DataFrame, gpd.GeoDataFrame]\n\n    Filter, Group By and Order By Fields\n    ------------------------------------\n    * value_time\n    * location_id\n    * value\n    * measurement_unit\n    * reference_time\n    * configuration\n    * variable_name\n\n    Examples:\n        order_by = [\"lead_time\", \"primary_location_id\"]\n        filters = [\n            {\n                \"column\": \"primary_location_id\",\n                \"operator\": \"=\",\n                \"value\": \"'123456'\"\n            },\n            {\n                \"column\": \"reference_time\",\n                \"operator\": \"=\",\n                \"value\": \"'2022-01-01 00:00'\"\n            },\n            {\n                \"column\": \"lead_time\",\n                \"operator\": \"<=\",\n                \"value\": \"'10 days'\"\n            }\n        ]\n    \"\"\"\n\n    tcq = TimeseriesCharQuery.parse_obj(\n        {\n            \"timeseries_filepath\": timeseries_filepath,\n            \"order_by\": order_by,\n            \"group_by\": group_by,\n            \"filters\": filters,\n            \"return_query\": return_query\n        }\n    )\n\n    join_max_time_on = tqu._join_time_on(\n        join=\"mxt\",\n        join_to=\"chars\",\n        join_on=tcq.group_by\n    )\n\n    query = f\"\"\"\n        WITH fts AS (\n            SELECT sf.* FROM\n            read_parquet('{str(tcq.timeseries_filepath)}') sf\n            {tqu.filters_to_sql(tcq.filters)}\n        ),\n        mxt AS (\n            SELECT\n                {\",\".join(tcq.group_by)}\n                , value\n                , value_time\n                , ROW_NUMBER() OVER(\n                    PARTITION BY {\",\".join(tcq.group_by)}\n                    ORDER BY value DESC, value_time\n                ) as n\n            FROM fts\n        ),\n        chars AS (\n            SELECT\n                {\",\".join(tcq.group_by)}\n                ,count(fts.value) as count\n                ,min(fts.value) as min\n                ,max(fts.value) as max\n                ,avg(fts.value) as average\n                ,sum(fts.value) as sum\n                ,var_pop(fts.value) as variance\n            FROM\n                fts\n            GROUP BY\n                {\",\".join(tcq.group_by)}\n            ORDER BY\n                {\",\".join(tcq.order_by)}\n        )\n        SELECT\n            chars.*\n            ,mxt.value_time as max_value_time\n        FROM chars\n        {join_max_time_on}\n\n    ;\"\"\"\n\n    if tcq.return_query:\n        return tqu.remove_empty_lines(query)\n\n    df = duckdb.query(query).to_df()\n\n    return df", ""]}
{"filename": "src/teehr/models/__init__.py", "chunked_list": [""]}
{"filename": "src/teehr/models/loading.py", "chunked_list": ["from enum import Enum\n\n\nclass ChunkByEnum(str, Enum):\n    day = \"day\"\n    location_id = \"location_id\"\n"]}
{"filename": "src/teehr/models/queries.py", "chunked_list": ["from collections.abc import Iterable\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nfrom pydantic import BaseModel as PydanticBaseModel\nfrom pydantic import validator\nfrom pathlib import Path\n\n\nclass BaseModel(PydanticBaseModel):\n    class Config:\n        arbitrary_types_allowed = True\n        smart_union = True", "\n\nclass BaseModel(PydanticBaseModel):\n    class Config:\n        arbitrary_types_allowed = True\n        smart_union = True\n\n\nclass FilterOperatorEnum(str, Enum):\n    eq = \"=\"\n    gt = \">\"\n    lt = \"<\"\n    gte = \">=\"\n    lte = \"<=\"\n    islike = \"like\"\n    isin = \"in\"", "class FilterOperatorEnum(str, Enum):\n    eq = \"=\"\n    gt = \">\"\n    lt = \"<\"\n    gte = \">=\"\n    lte = \"<=\"\n    islike = \"like\"\n    isin = \"in\"\n\n\nclass MetricEnum(str, Enum):\n    primary_count = \"primary_count\"\n    secondary_count = \"secondary_count\"\n    primary_minimum = \"primary_minimum\"\n    secondary_minimum = \"secondary_minimum\"\n    primary_maximum = \"primary_maximum\"\n    secondary_maximum = \"secondary_maximum\"\n    primary_average = \"primary_average\"\n    secondary_average = \"secondary_average\"\n    primary_sum = \"primary_sum\"\n    secondary_sum = \"secondary_sum\"\n    primary_variance = \"primary_variance\"\n    secondary_variance = \"secondary_variance\"\n    max_value_delta = \"max_value_delta\"\n    bias = \"bias\"\n    nash_sutcliffe_efficiency = \"nash_sutcliffe_efficiency\"\n    kling_gupta_efficiency = \"kling_gupta_efficiency\"\n    mean_error = \"mean_error\"\n    mean_squared_error = \"mean_squared_error\"\n    root_mean_squared_error = \"root_mean_squared_error\"\n    primary_max_value_time = \"primary_max_value_time\"\n    secondary_max_value_time = \"secondary_max_value_time\"\n    max_value_timedelta = \"max_value_timedelta\"", "\n\nclass MetricEnum(str, Enum):\n    primary_count = \"primary_count\"\n    secondary_count = \"secondary_count\"\n    primary_minimum = \"primary_minimum\"\n    secondary_minimum = \"secondary_minimum\"\n    primary_maximum = \"primary_maximum\"\n    secondary_maximum = \"secondary_maximum\"\n    primary_average = \"primary_average\"\n    secondary_average = \"secondary_average\"\n    primary_sum = \"primary_sum\"\n    secondary_sum = \"secondary_sum\"\n    primary_variance = \"primary_variance\"\n    secondary_variance = \"secondary_variance\"\n    max_value_delta = \"max_value_delta\"\n    bias = \"bias\"\n    nash_sutcliffe_efficiency = \"nash_sutcliffe_efficiency\"\n    kling_gupta_efficiency = \"kling_gupta_efficiency\"\n    mean_error = \"mean_error\"\n    mean_squared_error = \"mean_squared_error\"\n    root_mean_squared_error = \"root_mean_squared_error\"\n    primary_max_value_time = \"primary_max_value_time\"\n    secondary_max_value_time = \"secondary_max_value_time\"\n    max_value_timedelta = \"max_value_timedelta\"", "\n\nclass JoinedFilterFieldEnum(str, Enum):\n    value_time = \"value_time\"\n    reference_time = \"reference_time\"\n    secondary_location_id = \"secondary_location_id\"\n    secondary_value = \"secondary_value\"\n    configuration = \"configuration\"\n    measurement_unit = \"measurement_unit\"\n    variable_name = \"variable_name\"\n    primary_value = \"primary_value\"\n    primary_location_id = \"primary_location_id\"\n    lead_time = \"lead_time\"\n    geometry = \"geometry\"", "\n\nclass TimeseriesFilterFieldEnum(str, Enum):\n    value_time = \"value_time\"\n    reference_time = \"reference_time\"\n    location_id = \"location_id\"\n    value = \"value\"\n    configuration = \"configuration\"\n    measurement_unit = \"measurement_unit\"\n    variable_name = \"variable_name\"\n    lead_time = \"lead_time\"\n    geometry = \"geometry\"", "\n\nclass ChunkByEnum(str, Enum):\n    day = \"day\"\n    site = \"site\"\n\n\nclass JoinedFilter(BaseModel):\n    column: JoinedFilterFieldEnum\n    operator: FilterOperatorEnum\n    value: Union[\n        str, int, float, datetime,\n        List[Union[str, int, float, datetime]]\n    ]\n\n    def is_iterable_not_str(obj):\n        if isinstance(obj, Iterable) and not isinstance(obj, str):\n            return True\n        return False\n\n    @validator('value')\n    def in_operator_must_have_iterable(cls, v, values):\n        if cls.is_iterable_not_str(v) and values[\"operator\"] != \"in\":\n            raise ValueError(\"iterable value must be used with 'in' operator\")\n\n        if values[\"operator\"] == \"in\" and not cls.is_iterable_not_str(v):\n            raise ValueError(\n                \"'in' operator can only be used with iterable value\"\n            )\n\n        return v", "\n\nclass TimeseriesFilter(BaseModel):\n    column: TimeseriesFilterFieldEnum\n    operator: FilterOperatorEnum\n    value: Union[\n        str, int, float, datetime,\n        List[Union[str, int, float, datetime]]\n    ]\n\n    def is_iterable_not_str(obj):\n        if isinstance(obj, Iterable) and not isinstance(obj, str):\n            return True\n        return False\n\n    @validator('value')\n    def in_operator_must_have_iterable(cls, v, values):\n        if cls.is_iterable_not_str(v) and values[\"operator\"] != \"in\":\n            raise ValueError(\"iterable value must be used with 'in' operator\")\n\n        if values[\"operator\"] == \"in\" and not cls.is_iterable_not_str(v):\n            raise ValueError(\n                \"'in' operator can only be used with iterable value\"\n            )\n\n        return v", "\n\nclass MetricQuery(BaseModel):\n    primary_filepath: Union[str, Path]\n    secondary_filepath: Union[str, Path]\n    crosswalk_filepath: Union[str, Path]\n    group_by: List[JoinedFilterFieldEnum]\n    order_by: List[JoinedFilterFieldEnum]\n    include_metrics: Union[List[str], str]\n    filters: Optional[List[JoinedFilter]] = []\n    return_query: bool\n    geometry_filepath: Optional[Union[str, Path]]\n    include_geometry: bool\n\n    @validator('include_geometry')\n    def include_geometry_must_group_by_primary_location_id(cls, v, values):\n        if (\n            v is True\n            and JoinedFilterFieldEnum.primary_location_id not in values[\"group_by\"]  # noqa\n        ):\n            raise ValueError(\n                \"`group_by` must contain `primary_location_id` \"\n                \"to include geometry in returned data\"\n            )\n\n        if v is True and not values[\"geometry_filepath\"]:\n            raise ValueError(\n                \"`geometry_filepath` must be provided to include geometry \"\n                \"in returned data\"\n            )\n\n        if JoinedFilterFieldEnum.geometry in values[\"group_by\"] and v is False:\n            raise ValueError(\n                \"group_by contains `geometry` field but `include_geometry` \"\n                \"is False, must be True\"\n            )\n\n        return v\n\n    @validator('filters')\n    def filter_must_be_list(cls, v):\n        if v is None:\n            return []\n        return v", "\n\nclass JoinedTimeseriesQuery(BaseModel):\n    primary_filepath: Union[str, Path]\n    secondary_filepath: Union[str, Path]\n    crosswalk_filepath: Union[str, Path]\n    order_by: List[JoinedFilterFieldEnum]\n    filters: Optional[List[JoinedFilter]] = []\n    return_query: bool\n    geometry_filepath: Optional[Union[str, Path]]\n    include_geometry: bool\n\n    @validator('include_geometry')\n    def include_geometry_must_group_by_primary_location_id(cls, v, values):\n        if v is True and not values[\"geometry_filepath\"]:\n            raise ValueError(\n                \"`geometry_filepath` must be provided to include geometry \"\n                \"in returned data\"\n            )\n\n        return v\n\n    @validator('filters')\n    def filter_must_be_list(cls, v):\n        if v is None:\n            return []\n        return v", "\n\nclass TimeseriesQuery(BaseModel):\n    timeseries_filepath: Union[str, Path]\n    order_by: List[TimeseriesFilterFieldEnum]\n    filters: Optional[List[TimeseriesFilter]] = []\n    return_query: bool\n\n    @validator('filters')\n    def filter_must_be_list(cls, v):\n        if v is None:\n            return []\n        return v", "\n\nclass TimeseriesCharQuery(BaseModel):\n    timeseries_filepath: Union[str, Path]\n    order_by: List[TimeseriesFilterFieldEnum]\n    group_by: List[TimeseriesFilterFieldEnum]\n    filters: Optional[List[TimeseriesFilter]] = []\n    return_query: bool\n\n    @validator('filters')\n    def filter_must_be_list(cls, v):\n        if v is None:\n            return []\n        return v", ""]}
{"filename": "src/teehr/loading/nwm_grid_data.py", "chunked_list": ["from pathlib import Path\nfrom typing import Union, Iterable, Optional, List\nfrom datetime import datetime\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nimport dask\n\nfrom teehr.loading.utils_nwm import (", "\nfrom teehr.loading.utils_nwm import (\n    validate_run_args,\n    build_remote_nwm_filelist,\n    build_zarr_references,\n    get_dataset,\n)\n\nfrom teehr.loading.const_nwm import (\n    NWM22_UNIT_LOOKUP,", "from teehr.loading.const_nwm import (\n    NWM22_UNIT_LOOKUP,\n)\n\n\ndef compute_zonal_mean(\n    da: xr.DataArray, weights_filepath: str\n) -> pd.DataFrame:\n    \"\"\"Compute zonal mean for given zones and weights\"\"\"\n    # Read weights file\n    weights_df = pd.read_parquet(\n        weights_filepath, columns=[\"row\", \"col\", \"weight\", \"zone\"]\n    )\n    # Get variable data\n    arr_2d = da.values[0]\n    arr_2d[arr_2d == da.rio.nodata] = np.nan\n    # Get row/col indices\n    rows = weights_df.row.values\n    cols = weights_df.col.values\n    # Get the values and apply weights\n    var_values = arr_2d[rows, cols]\n    weights_df[\"value\"] = var_values * weights_df.weight.values\n    # Compute mean\n    df = weights_df.groupby(by=\"zone\")[\"value\"].mean().to_frame()\n    df.reset_index(inplace=True)\n\n    return df", "\n\n@dask.delayed\ndef process_single_file(\n    singlefile: str, run: str, variable_name: str, weights_filepath: str\n):\n    \"\"\"Compute zonal mean for a single json reference file and format\n    to a dataframe using the TEEHR data model\"\"\"\n    ds = get_dataset(singlefile)\n    ref_time = ds.reference_time.values[0]\n    nwm22_units = ds[variable_name].attrs[\"units\"]\n    teehr_units = NWM22_UNIT_LOOKUP.get(nwm22_units, nwm22_units)\n    value_time = ds.time.values[0]\n    da = ds[variable_name]\n\n    # Calculate mean areal of selected variable\n    df = compute_zonal_mean(da, weights_filepath)\n\n    df[\"value_time\"] = value_time\n    df[\"reference_time\"] = ref_time\n    df[\"measurement_unit\"] = teehr_units\n    df[\"configuration\"] = run\n    df[\"variable_name\"] = variable_name\n\n    return df", "\n\ndef fetch_and_format_nwm_grids(\n    json_paths: List[str],\n    run: str,\n    variable_name: str,\n    output_parquet_dir: str,\n    zonal_weights_filepath: str,\n) -> None:\n    \"\"\"\n    Reads in the single reference jsons, subsets the NWM data based on\n    provided IDs and formats and saves the data as a parquet files\n    \"\"\"\n    output_parquet_dir = Path(output_parquet_dir)\n    if not output_parquet_dir.exists():\n        output_parquet_dir.mkdir(parents=True)\n\n    # Format file list into a dataframe and group by reference time\n    days = []\n    z_hours = []\n    for path in json_paths:\n        filename = Path(path).name\n        days.append(filename.split(\".\")[1])\n        z_hours.append(filename.split(\".\")[3])\n    df_refs = pd.DataFrame(\n        {\"day\": days, \"z_hour\": z_hours, \"filepath\": json_paths}\n    )\n    gps = df_refs.groupby([\"day\", \"z_hour\"])\n\n    for gp in gps:\n        _, df = gp\n\n        results = []\n        for singlefile in df.filepath.tolist():\n            results.append(\n                process_single_file(\n                    singlefile,\n                    run,\n                    variable_name,\n                    zonal_weights_filepath,\n                )\n            )\n        z_hour_df = pd.concat(dask.compute(results)[0])\n\n        # Save to parquet\n        yrmoday = df.day.iloc[0]\n        z_hour = df.z_hour.iloc[0][1:3]\n        ref_time_str = f\"{yrmoday}T{z_hour}Z\"\n        parquet_filepath = Path(\n            Path(output_parquet_dir), f\"{ref_time_str}.parquet\"\n        )\n        z_hour_df.sort_values([\"zone\", \"value_time\"], inplace=True)\n        z_hour_df.to_parquet(parquet_filepath)", "\n\ndef nwm_grids_to_parquet(\n    run: str,\n    output_type: str,\n    variable_name: str,\n    start_date: Union[str, datetime],\n    ingest_days: int,\n    zonal_weights_filepath: str,\n    json_dir: str,\n    output_parquet_dir: str,\n    t_minus_hours: Optional[Iterable[int]] = None,\n):\n    \"\"\"\n    Fetches NWM gridded data, calculates zonal statistics (mean) of selected\n    variable for given zones, converts and saves to TEEHR tabular format\n\n    Parameters\n    ----------\n    run : str\n        NWM forecast category.\n        (e.g., \"analysis_assim\", \"short_range\", ...)\n    output_type : str\n        Output component of the configuration.\n        (e.g., \"channel_rt\", \"reservoir\", ...)\n    variable_name : str\n        Name of the NWM data variable to download.\n        (e.g., \"streamflow\", \"velocity\", ...)\n    start_date : str or datetime\n        Date to begin data ingest.\n        Str formats can include YYYY-MM-DD or MM/DD/YYYY\n    ingest_days : int\n        Number of days to ingest data after start date\n    zonal_weights_filepath: str\n        Path to the array containing fraction of pixel overlap\n        for each zone\n    json_dir : str\n        Directory path for saving json reference files\n    output_parquet_dir : str\n        Path to the directory for the final parquet files\n    t_minus_hours: Optional[Iterable[int]]\n        Specifies the look-back hours to include if an assimilation\n        run is specified.\n\n    The NWM configuration variables, including run, output_type, and\n    variable_name are stored in the NWM22_RUN_CONFIG dictionary in\n    const_nwm.py.\n\n    Forecast and assimilation data is grouped and saved one file per reference\n    time, using the file name convention \"YYYYMMDDTHHZ\".  The tabular output\n    parquet files follow the timeseries data model described here:\n    https://github.com/RTIInternational/teehr/blob/main/docs/data_models.md#timeseries  # noqa\n    \"\"\"\n    validate_run_args(run, output_type, variable_name)\n\n    component_paths = build_remote_nwm_filelist(\n        run,\n        output_type,\n        start_date,\n        ingest_days,\n        t_minus_hours,\n    )\n\n    json_paths = build_zarr_references(\n        component_paths,\n        json_dir\n    )\n\n    fetch_and_format_nwm_grids(\n        json_paths,\n        run,\n        variable_name,\n        output_parquet_dir,\n        zonal_weights_filepath,\n    )", "\n\nif __name__ == \"__main__\":\n    # Local testing\n    single_filepath = \"/mnt/sf_shared/data/ciroh/nwm.20201218_forcing_short_range_nwm.t00z.short_range.forcing.f001.conus.nc\"  # noqa\n    weights_parquet = \"/mnt/sf_shared/data/ciroh/wbdhuc10_weights.parquet\"\n\n    nwm_grids_to_parquet(\n        \"forcing_analysis_assim\",\n        \"forcing\",\n        \"RAINRATE\",\n        \"2020-12-18\",\n        1,\n        weights_parquet,\n        \"/home/sam/forcing_jsons\",\n        \"/home/sam/forcing_parquet\",\n        [0, 1, 2],\n    )", ""]}
{"filename": "src/teehr/loading/usgs.py", "chunked_list": ["import pandas as pd\n\nfrom typing import List, Union\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom hydrotools.nwis_client.iv import IVDataService\nfrom teehr.models.loading import ChunkByEnum\nfrom pydantic import validate_arguments\n\nDATETIME_STR_FMT = \"%Y-%m-%dT%H:%M:00+0000\"", "\nDATETIME_STR_FMT = \"%Y-%m-%dT%H:%M:00+0000\"\n\n\ndef _filter_to_hourly(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Filter out data not reported on the hour.\"\"\"\n    df.set_index(\"value_time\", inplace=True)\n    df2 = df[\n        df.index.hour.isin(range(0, 24))\n        & (df.index.minute == 0)\n        & (df.index.second == 0)\n    ]\n    df2.reset_index(level=0, allow_duplicates=True, inplace=True)\n    return df2", "\n\ndef _filter_no_data(df: pd.DataFrame, no_data_value=-999) -> pd.DataFrame:\n    \"\"\"Filter out no data values.\"\"\"\n\n    df2 = df[df[\"value\"] != no_data_value]\n    return df2\n\n\ndef _convert_to_si_units(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convert streamflow values from english to metric.\"\"\"\n\n    df[\"value\"] = df[\"value\"] * 0.3048**3\n    df[\"measurement_unit\"] = \"m3/s\"\n    return df", "\ndef _convert_to_si_units(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convert streamflow values from english to metric.\"\"\"\n\n    df[\"value\"] = df[\"value\"] * 0.3048**3\n    df[\"measurement_unit\"] = \"m3/s\"\n    return df\n\n\ndef _datetime_to_date(dt: datetime) -> datetime:\n    \"\"\"Convert datetime to date only\"\"\"\n    dt.replace(\n        hour=0,\n        minute=0,\n        second=0,\n        microsecond=0\n    )\n    return dt", "\ndef _datetime_to_date(dt: datetime) -> datetime:\n    \"\"\"Convert datetime to date only\"\"\"\n    dt.replace(\n        hour=0,\n        minute=0,\n        second=0,\n        microsecond=0\n    )\n    return dt", "\n\ndef _format_df(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Format HydroTools dataframe columns to TEEHR data model.\"\"\"\n\n    df.rename(columns={\"usgs_site_code\": \"location_id\"}, inplace=True)\n    df[\"location_id\"] = \"usgs-\" + df[\"location_id\"].astype(str)\n    df[\"configuration\"] = \"usgs_gage_data\"\n    df[\"reference_time\"] = df[\"value_time\"]\n    return df[[\n        \"location_id\",\n        \"reference_time\",\n        \"value_time\",\n        \"value\",\n        \"variable_name\",\n        \"measurement_unit\",\n        \"configuration\"\n    ]]", "\n\ndef _fetch_usgs(\n    sites: List[str],\n    start_date: datetime,\n    end_date: datetime,\n    filter_to_hourly: bool = True,\n    filter_no_data: bool = True,\n    convert_to_si: bool = True\n) -> pd.DataFrame:\n    \"\"\"Fetch USGS gage data and format to TEEHR format.\"\"\"\n\n    start_dt_str = start_date.strftime(DATETIME_STR_FMT)\n    end_dt_str = (\n        end_date\n        - timedelta(minutes=1)\n    ).strftime(DATETIME_STR_FMT)\n\n    # Retrieve data\n    service = IVDataService(\n        value_time_label=\"value_time\",\n        enable_cache=False\n    )\n    usgs_df = service.get(\n        sites=sites,\n        startDT=start_dt_str,\n        endDT=end_dt_str\n    )\n\n    if filter_to_hourly is True:\n        usgs_df = _filter_to_hourly(usgs_df)\n    if filter_no_data is True:\n        usgs_df = _filter_no_data(usgs_df)\n    if convert_to_si is True:\n        usgs_df = _convert_to_si_units(usgs_df)\n    usgs_df = _format_df(usgs_df)\n\n    # Return the data\n    return usgs_df", "\n\n@validate_arguments()\ndef usgs_to_parquet(\n    sites: List[str],\n    start_date: Union[str, datetime, pd.Timestamp],\n    end_date: Union[str, datetime, pd.Timestamp],\n    output_parquet_dir: Union[str, Path],\n    chunk_by: Union[ChunkByEnum, None] = None,\n    filter_to_hourly: bool = True,\n    filter_no_data: bool = True,\n    convert_to_si: bool = True\n):\n    \"\"\"Fetch USGS gage data and save as a Parquet file.\n\n    Parameters\n    ----------\n    sites : List[str]\n        List of USGS gages sites to fetch.\n        Must be string to preserve the leading 0.\n    start_date : datetime\n        Start time of data to fetch.\n    end_date : datetime\n        End time of data to fetch. Note, since startDt is inclusive for the\n        USGS service, we subtract 1 minute from this time so we don't get\n        overlap between consecutive calls.\n    output_parquet_dir : Union[str, Path]\n        Path of directory where parquet files will be saved.\n    chunk_by : Union[str, None], default = None\n        How to \"chunk\" the fetching and storing of the data.\n        Valid options = [\"day\", \"site\", None]\n    filter_to_hourly: bool = True\n        Return only values that fall on the hour (i.e. drop 15 minute data).\n    filter_no_data: bool = True\n        Filter out -999 values\n    convert_to_si: bool = True\n        Multiplies values by 0.3048**3 and sets `measurement_units` to `m3/s`\n    \"\"\"\n\n    start_date = pd.Timestamp(start_date)\n    end_date = pd.Timestamp(end_date)\n\n    # Check if output_parquet_dir is an existing dir\n    output_dir = Path(output_parquet_dir)\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True)\n\n    # Fetch all at once\n    if chunk_by is None:\n        usgs_df = _fetch_usgs(\n            sites=sites,\n            start_date=start_date,\n            end_date=end_date,\n            filter_to_hourly=filter_to_hourly,\n            filter_no_data=filter_no_data,\n            convert_to_si=convert_to_si\n        )\n        if len(usgs_df) > 0:\n            output_filepath = Path(output_parquet_dir, \"usgs.parquet\")\n            usgs_df.to_parquet(output_filepath)\n            # output_filepath = Path(usgs.output_parquet_dir, \"usgs.csv\")\n            # usgs_df.to_csv(output_filepath)\n            # print(usgs_df)\n\n    if chunk_by == \"day\":\n        # Determine number of days to fetch\n        period_length = timedelta(days=1)\n        start_date = _datetime_to_date(start_date)\n        end_date = _datetime_to_date(end_date)\n        period = end_date - start_date\n        if period < period_length:\n            period = period_length\n        number_of_days = period.days\n\n        # Fetch data in daily batches\n        for day in range(number_of_days):\n\n            # Setup start and end date for fetch\n            start_dt = (start_date + period_length * day)\n            end_dt = (\n                start_date\n                + period_length * (day + 1)\n                - timedelta(minutes=1)\n            )\n            usgs_df = _fetch_usgs(\n                sites=sites,\n                start_date=start_dt,\n                end_date=end_dt,\n                filter_to_hourly=filter_to_hourly,\n                filter_no_data=filter_no_data,\n                convert_to_si=convert_to_si\n            )\n            if len(usgs_df) > 0:\n                output_filepath = Path(\n                    output_parquet_dir,\n                    f\"{start_dt.strftime('%Y-%m-%d')}.parquet\"\n                )\n                usgs_df.to_parquet(output_filepath)\n                # output_filepath = Path(\n                #     usgs.output_parquet_dir,\n                #     f\"{start_dt.strftime('%Y-%m-%d')}.csv\"\n                # )\n                # usgs_df.to_csv(output_filepath)\n                # print(usgs_df)\n\n    if chunk_by == \"location_id\":\n        for site in sites:\n            usgs_df = _fetch_usgs(\n                sites=[site],\n                start_date=start_date,\n                end_date=end_date,\n                filter_to_hourly=filter_to_hourly,\n                filter_no_data=filter_no_data,\n                convert_to_si=convert_to_si\n            )\n            if len(usgs_df) > 0:\n                output_filepath = Path(\n                    output_parquet_dir,\n                    f\"{site}.parquet\"\n                )\n                usgs_df.to_parquet(output_filepath)", "                # output_filepath = Path(\n                #     usgs.output_parquet_dir,\n                #     f\"{site}.csv\"\n                # )\n                # usgs_df.to_csv(output_filepath)\n                # print(usgs_df)\n\n\nif __name__ == \"__main__\":\n    # Examples\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n        chunk_by=\"location_id\"\n    )\n\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n        chunk_by=\"day\"\n    )\n\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n    )\n    pass", "if __name__ == \"__main__\":\n    # Examples\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n        chunk_by=\"location_id\"\n    )\n\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n        chunk_by=\"day\"\n    )\n\n    usgs_to_parquet(\n        sites=[\n            \"02449838\",\n            \"02450825\"\n        ],\n        start_date=datetime(2023, 2, 20),\n        end_date=datetime(2023, 2, 25),\n        output_parquet_dir=Path(Path().home(), \"temp\", \"usgs\"),\n    )\n    pass", ""]}
{"filename": "src/teehr/loading/nwm_point_data.py", "chunked_list": ["from pathlib import Path\nfrom typing import Union, Iterable, Tuple, Optional, List\nfrom datetime import datetime\n\nimport fsspec\nimport xarray as xr\nimport pandas as pd\nfrom kerchunk.combine import MultiZarrToZarr\nimport dask\nimport ujson", "import dask\nimport ujson\n\n\nfrom teehr.loading.utils_nwm import (\n    validate_run_args,\n    build_remote_nwm_filelist,\n    build_zarr_references,\n)\n", ")\n\nfrom teehr.loading.const_nwm import (\n    NWM22_UNIT_LOOKUP\n)\n\n\ndef fetch_and_format_nwm_points(\n    json_paths: List[str],\n    location_ids: Iterable[int],\n    run: str,\n    variable_name: str,\n    output_parquet_dir: str,\n    concat_dims=[\"time\"],\n):\n    \"\"\"Reads in the single reference jsons, subsets the\n        NWM data based on provided IDs and formats and saves\n        the data as a parquet files using Dask.\n\n    Parameters\n    ----------\n    json_paths: list\n        List of the single json reference filepaths\n    location_ids : Iterable[int]\n        Array specifying NWM IDs of interest\n    run : str\n        NWM forecast category\n    variable_name : str\n        Name of the NWM data variable to download\n    output_parquet_dir : str\n        Path to the directory for the final parquet files\n    \"\"\"\n\n    output_parquet_dir = Path(output_parquet_dir)\n    if not output_parquet_dir.exists():\n        output_parquet_dir.mkdir(parents=True)\n\n    # Format file list into a dataframe and group by reference time\n    days = []\n    z_hours = []\n    for path in json_paths:\n        filename = Path(path).name\n        days.append(filename.split(\".\")[1])\n        z_hours.append(filename.split(\".\")[3])\n    df_refs = pd.DataFrame(\n        {\n            \"day\": days,\n            \"z_hour\": z_hours,\n            \"filepath\": json_paths\n        }\n    )\n    gps = df_refs.groupby([\"day\", \"z_hour\"])\n    results = []\n    for gp in gps:\n        results.append(\n            fetch_and_format(\n                gp,\n                location_ids,\n                run,\n                variable_name,\n                output_parquet_dir,\n                concat_dims\n            )\n        )\n    dask.compute(results)", "\n\n@dask.delayed\ndef fetch_and_format(\n    gp: Tuple[Tuple[str, str], pd.DataFrame],\n    location_ids: Iterable[int],\n    run: str,\n    variable_name: str,\n    output_parquet_dir: str,\n    concat_dims: List[str],\n) -> None:\n    \"\"\"Helper function to fetch and format the NWM data using Dask.\n\n    Parameters\n    ----------\n    gp : Tuple[Tuple[str, str], pd.Dataframe],\n        A tuple containing a tuple of (day, z_hour) and a dataframe of\n        reference json filepaths for a specific z_hour.  Results from\n        pandas dataframe.groupby([\"day\", \"z_hour\"])\n    location_ids : Iterable[int]\n        Array specifying NWM IDs of interest\n    run : str\n        NWM forecast category\n    variable_name : str\n        Name of the NWM data variable to download\n    output_parquet_dir : str\n        Path to the directory for the final parquet files\n    concat_dims : list of strings\n        List of dimensions to use when concatenating single file\n        jsons to multifile\n    \"\"\"\n    _, df = gp\n    # Only combine if there is more than one file for this group,\n    # otherwise a warning is thrown\n    if len(df.index) > 1:\n        mzz = MultiZarrToZarr(\n            df.filepath.tolist(),\n            remote_protocol=\"gcs\",\n            remote_options={\"anon\": True},\n            concat_dims=concat_dims,\n        )\n        json = mzz.translate()\n    else:\n        json = ujson.load(open(df.filepath.iloc[0]))\n    fs = fsspec.filesystem(\"reference\", fo=json)\n    m = fs.get_mapper(\"\")\n    ds_nwm_subset = xr.open_zarr(m, consolidated=False, chunks={}).sel(\n        feature_id=location_ids\n    )\n    # Convert to dataframe and do some reformatting\n    df_temp = ds_nwm_subset[variable_name].to_dataframe()\n    df_temp.reset_index(inplace=True)\n    if len(df.index) == 1:\n        df_temp[\"time\"] = ds_nwm_subset.time.values[0]\n    df_temp.rename(\n        columns={\n            variable_name: \"value\",\n            \"time\": \"value_time\",\n            \"feature_id\": \"location_id\",\n        },\n        inplace=True,\n    )\n    df_temp.dropna(subset=[\"value\"], inplace=True)\n    nwm22_units = ds_nwm_subset[variable_name].units\n    teehr_units = NWM22_UNIT_LOOKUP.get(nwm22_units, nwm22_units)\n    df_temp[\"measurement_unit\"] = teehr_units\n    ref_time = ds_nwm_subset.reference_time.values[0]\n    df_temp[\"reference_time\"] = ref_time\n    df_temp[\"configuration\"] = run\n    df_temp[\"variable_name\"] = variable_name\n    df_temp[\"location_id\"] = \"nwm22-\" + df_temp.location_id.astype(int).astype(str)\n    # Save to parquet\n    ref_time_str = pd.to_datetime(ref_time).strftime(\"%Y%m%dT%HZ\")\n    parquet_filepath = Path(output_parquet_dir, f\"{ref_time_str}.parquet\")\n    df_temp.to_parquet(parquet_filepath)", "\n\ndef nwm_to_parquet(\n    run: str,\n    output_type: str,\n    variable_name: str,\n    start_date: Union[str, datetime],\n    ingest_days: int,\n    location_ids: Iterable[int],\n    json_dir: str,\n    output_parquet_dir: str,\n    t_minus_hours: Optional[Iterable[int]] = None,\n):\n    \"\"\"Fetches NWM point data, formats to tabular, and saves to parquet\n\n    Parameters\n    ----------\n    run : str\n        NWM forecast category.\n        (e.g., \"analysis_assim\", \"short_range\", ...)\n    output_type : str\n        Output component of the configuration.\n        (e.g., \"channel_rt\", \"reservoir\", ...)\n    variable_name : str\n        Name of the NWM data variable to download.\n        (e.g., \"streamflow\", \"velocity\", ...)\n    start_date : str or datetime\n        Date to begin data ingest.\n        Str formats can include YYYY-MM-DD or MM/DD/YYYY\n    ingest_days : int\n        Number of days to ingest data after start date\n    location_ids : Iterable[int]\n        Array specifying NWM IDs of interest\n    json_dir : str\n        Directory path for saving json reference files\n    output_parquet_dir : str\n        Path to the directory for the final parquet files\n    t_minus_hours: Optional[Iterable[int]]\n        Specifies the look-back hours to include if an assimilation\n        run is specified.\n\n    The NWM configuration variables, including run, output_type, and\n    variable_name are stored in the NWM22_RUN_CONFIG dictionary in\n    const_nwm.py.\n\n    Forecast and assimilation data is grouped and saved one file per reference\n    time, using the file name convention \"YYYYMMDDTHHZ\".  The tabular output\n    parquet files follow the timeseries data model described here:\n    https://github.com/RTIInternational/teehr/blob/main/docs/data_models.md#timeseries  # noqa\n    \"\"\"\n    validate_run_args(\n        run,\n        output_type,\n        variable_name\n    )\n\n    component_paths = build_remote_nwm_filelist(\n        run,\n        output_type,\n        start_date,\n        ingest_days,\n        t_minus_hours,\n    )\n\n    json_paths = build_zarr_references(\n        component_paths,\n        json_dir\n    )\n\n    fetch_and_format_nwm_points(\n        json_paths,\n        location_ids,\n        run,\n        variable_name,\n        output_parquet_dir,\n    )", ""]}
{"filename": "src/teehr/loading/generate_weights.py", "chunked_list": ["from typing import Union\nfrom pathlib import Path\nimport warnings\n\nimport geopandas as gpd\nimport numpy as np\nimport xarray as xr\nfrom rasterio.transform import rowcol\nimport rasterio\nimport pandas as pd", "import rasterio\nimport pandas as pd\nimport dask\nimport shapely\n\nfrom teehr.loading.utils_nwm import load_gdf\nimport teehr.loading.const_nwm as const_nwm\n\n\n@dask.delayed\ndef vectorize(data_array: xr.DataArray) -> gpd.GeoDataFrame:\n    \"\"\"\n    Convert 2D xarray.DataArray into a geopandas.GeoDataFrame\n\n    Heavily borrowed from GeoCube, see:\n    https://github.com/corteva/geocube/blob/master/geocube/vector.py#L12\n    \"\"\"\n    # nodata mask\n    mask = None\n    if np.isnan(data_array.rio.nodata):\n        mask = ~data_array.isnull()\n    elif data_array.rio.nodata is not None:\n        mask = data_array != data_array.rio.nodata\n\n    # Give all pixels a unique value\n    data_array.values[:, :] = np.arange(0, data_array.values.size).reshape(\n        data_array.shape\n    )\n\n    # vectorize generator\n    vectorized_data = (\n        (value, shapely.geometry.shape(polygon))\n        for polygon, value in rasterio.features.shapes(\n            data_array,\n            transform=data_array.rio.transform(),\n            mask=mask,\n        )\n    )\n    gdf = gpd.GeoDataFrame(\n        vectorized_data,\n        columns=[data_array.name, \"geometry\"],\n        crs=data_array.rio.crs,\n    )\n    xx, yy = np.meshgrid(data_array.x.values, data_array.y.values)\n    gdf[\"x\"] = xx.ravel()\n    gdf[\"y\"] = yy.ravel()\n\n    return gdf", "\n@dask.delayed\ndef vectorize(data_array: xr.DataArray) -> gpd.GeoDataFrame:\n    \"\"\"\n    Convert 2D xarray.DataArray into a geopandas.GeoDataFrame\n\n    Heavily borrowed from GeoCube, see:\n    https://github.com/corteva/geocube/blob/master/geocube/vector.py#L12\n    \"\"\"\n    # nodata mask\n    mask = None\n    if np.isnan(data_array.rio.nodata):\n        mask = ~data_array.isnull()\n    elif data_array.rio.nodata is not None:\n        mask = data_array != data_array.rio.nodata\n\n    # Give all pixels a unique value\n    data_array.values[:, :] = np.arange(0, data_array.values.size).reshape(\n        data_array.shape\n    )\n\n    # vectorize generator\n    vectorized_data = (\n        (value, shapely.geometry.shape(polygon))\n        for polygon, value in rasterio.features.shapes(\n            data_array,\n            transform=data_array.rio.transform(),\n            mask=mask,\n        )\n    )\n    gdf = gpd.GeoDataFrame(\n        vectorized_data,\n        columns=[data_array.name, \"geometry\"],\n        crs=data_array.rio.crs,\n    )\n    xx, yy = np.meshgrid(data_array.x.values, data_array.y.values)\n    gdf[\"x\"] = xx.ravel()\n    gdf[\"y\"] = yy.ravel()\n\n    return gdf", "\n\n@dask.delayed\ndef overlay_zones(\n    grid: gpd.GeoDataFrame, zones: gpd.GeoDataFrame\n) -> gpd.GeoDataFrame:\n    with pd.option_context(\n        \"mode.chained_assignment\", None\n    ):  # to ignore setwithcopywarning\n        grid.loc[:, \"pixel_area\"] = grid.geometry.area\n        overlay_gdf = grid.overlay(zones, keep_geom_type=True)\n        overlay_gdf.loc[:, \"overlay_area\"] = overlay_gdf.geometry.area\n        overlay_gdf.loc[:, \"weight\"] = (\n            overlay_gdf.overlay_area / overlay_gdf.pixel_area\n        )\n    return overlay_gdf", "\n\ndef vectorize_grid(\n    src_da: xr.DataArray,\n    nodata_val: float,\n    vectorize_chunk: float = 40,\n) -> gpd.GeoDataFrame:\n    \"\"\"Vectorize pixels in the template array in chunks using dask\n\n    Note: Parameter vectorize_chunk determines how many pixels will\n    be vectorized at one time\n    (thousands of pixels)\n    \"\"\"\n    src_da = src_da.persist()\n    max_pixels = vectorize_chunk * 1000\n    num_splits = np.ceil(src_da.values.size / max_pixels).astype(int)\n\n    # Prepare each data array\n    if num_splits > 0:\n        da_list = np.array_split(src_da, num_splits)\n        [da.rio.write_nodata(nodata_val, inplace=True) for da in da_list]\n    else:\n        src_da.rio.write_nodata(nodata_val, inplace=True)\n        da_list = [src_da]\n\n    results = []\n    for da_subset in da_list:\n        results.append(vectorize(da_subset))\n    grid_gdf = pd.concat(dask.compute(results)[0])\n    grid_gdf.crs = const_nwm.CONUS_NWM_WKT\n\n    # Reindex to remove duplicates\n    grid_gdf[\"index\"] = np.arange(len(grid_gdf.index))\n    grid_gdf.set_index(\"index\", inplace=True)\n\n    return grid_gdf", "\n\ndef calculate_weights(\n    grid_gdf: gpd.GeoDataFrame,\n    zone_gdf: gpd.GeoDataFrame,\n    overlay_chunk: float = 250,\n) -> gpd.GeoDataFrame:\n    \"\"\"Overlay vectorized pixels and zone polygons, and calculate\n    areal weights, returning a geodataframe\n\n    Note: Parameter overlay_chunk determines the size of the rectangular\n    window that spatially subsets datasets for the operation\n    (thousands of pixels)\n    \"\"\"\n    # Make sure geometries are valid\n    grid_gdf[\"geometry\"] = grid_gdf.geometry.make_valid()\n    zone_gdf[\"geometry\"] = zone_gdf.geometry.make_valid()\n\n    xmin, ymin, xmax, ymax = zone_gdf.total_bounds\n\n    x_steps = np.arange(xmin, xmax, overlay_chunk * 1000)\n    y_steps = np.arange(ymin, ymax, overlay_chunk * 1000)\n\n    x_steps = np.append(x_steps, xmax)\n    y_steps = np.append(y_steps, ymax)\n\n    results = []\n    for i in range(x_steps.size - 1):\n        for j in range(y_steps.size - 1):\n            xmin = x_steps[i]\n            xmax = x_steps[i + 1]\n\n            ymin = y_steps[j]\n            ymax = y_steps[j + 1]\n\n            zone = zone_gdf.cx[xmin:xmax, ymin:ymax]\n            grid = grid_gdf.cx[xmin:xmax, ymin:ymax]\n\n            if len(zone.index) == 0 or len(grid.index) == 0:\n                continue\n            results.append(overlay_zones(grid, zone))\n\n    overlay_gdf = pd.concat(dask.compute(results)[0])\n\n    return overlay_gdf", "\n\ndef generate_weights_file(\n    zone_polygon_filepath: Union[Path, str],\n    template_dataset: Union[str, Path],\n    variable_name: str,\n    output_weights_filepath: Union[str, Path],\n    unique_zone_id: str = None,\n    **read_args: str,\n) -> None:\n    \"\"\"Generate a file of row/col indices and weights for pixels intersecting\n       given zone polyons\n\n    Parameters\n    ----------\n    zone_polygon_filepath : str\n        Path to the polygons geoparquet file\n    template_dataset : str\n        Path to the grid dataset to use as a template\n    variable_name : str\n        Name of the variable within the dataset\n    output_weights_filepath : str\n        Path to the resultant weights file\n    unique_zone_id: str\n        Name of the field in the zone polygon file containing unique IDs\n    save_to_disk: boolean\n        Flag to indicate whether or not to save results to disk\n    read_args: dict, optional\n        Keyword arguments to be passed to GeoPandas read_file(),\n        read_parquet(), and read_feather() methods\n    \"\"\"\n\n    zone_gdf = load_gdf(zone_polygon_filepath, **read_args)\n    zone_gdf = zone_gdf.to_crs(const_nwm.CONUS_NWM_WKT)\n\n    ds = xr.open_dataset(template_dataset)\n    src_da = ds[variable_name]\n    src_da = src_da.rio.write_crs(const_nwm.CONUS_NWM_WKT, inplace=True)\n    grid_transform = src_da.rio.transform()\n    nodata_val = src_da.rio.nodata\n\n    # Get the subset of the grid that intersects the total zone bounds\n    bbox = tuple(zone_gdf.total_bounds)\n    src_da = src_da.sel(x=slice(bbox[0], bbox[2]), y=slice(bbox[1], bbox[3]))[\n        0\n    ]\n    src_da = src_da.astype(\"float32\")\n    src_da[\"x\"] = np.float32(src_da.x.values)\n    src_da[\"y\"] = np.float32(src_da.y.values)\n\n    # Vectorize source grid pixels\n    grid_gdf = vectorize_grid(src_da, nodata_val)\n\n    # Overlay and calculate areal weights of pixels within each zone\n    # Note: Temporarily suppress the dask UserWarning: \"Large object detected\n    #  in task graph\" until a better approach is found\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        weights_gdf = calculate_weights(grid_gdf, zone_gdf)\n    weights_gdf = weights_gdf.drop_duplicates(\n        subset=[\"x\", \"y\", unique_zone_id]\n    )\n\n    # Convert x-y to row-col using original transform\n    rows, cols = rowcol(\n        grid_transform, weights_gdf.x.values, weights_gdf.y.values\n    )\n    weights_gdf[\"row\"] = rows\n    weights_gdf[\"col\"] = cols\n\n    if unique_zone_id:\n        df = weights_gdf[[\"row\", \"col\", \"weight\", unique_zone_id]].copy()\n        df.rename(columns={unique_zone_id: \"zone\"}, inplace=True)\n    else:\n        df = weights_gdf[[\"row\", \"col\", \"weight\"]]\n        df[\"zone\"] = weights_gdf.index.values\n\n    if output_weights_filepath:\n        df.to_parquet(output_weights_filepath)\n        df = None\n\n    return df", "\n\nif __name__ == \"__main__\":\n    # Local testing\n    zone_polygon_filepath = \"/mnt/sf_shared/data/ciroh/nextgen_03S.gpkg\"\n    template_dataset = \"/mnt/sf_shared/data/ciroh/nwm.20201218_forcing_short_range_nwm.t00z.short_range.forcing.f001.conus.nc\"  # noqa\n    variable_name = \"RAINRATE\"\n    unique_zone_id = \"id\"\n    output_weights_filepath = (\n        \"/mnt/sf_shared/data/ciroh/wbdhu10_medium_range_weights.parquet\"\n    )\n    zone_polygon_filepath = (\n        \"/mnt/sf_shared/data/ciroh/test_ngen_divides.parquet\"\n    )\n\n    generate_weights_file(\n        zone_polygon_filepath,\n        template_dataset,\n        variable_name,\n        output_weights_filepath,\n        unique_zone_id,\n    )", ""]}
{"filename": "src/teehr/loading/__init__.py", "chunked_list": [""]}
{"filename": "src/teehr/loading/utils_nwm.py", "chunked_list": ["from pathlib import Path\nfrom typing import Union, Optional, Iterable, List\nfrom datetime import datetime\n\nimport dask\nimport fsspec\nimport ujson  # fast json\nfrom kerchunk.hdf import SingleHdf5ToZarr\nimport pandas as pd\nimport numpy as np", "import pandas as pd\nimport numpy as np\nimport xarray as xr\nimport geopandas as gpd\n\nfrom teehr.loading.const_nwm import (\n    NWM22_RUN_CONFIG,\n    NWM22_ANALYSIS_CONFIG,\n    NWM_BUCKET,\n)", "    NWM_BUCKET,\n)\n\n\ndef load_gdf(filepath: Union[str, Path], **kwargs: str) -> gpd.GeoDataFrame:\n    \"\"\"Load any supported geospatial file type into a gdf using GeoPandas.\"\"\"\n    try:\n        gdf = gpd.read_file(filepath, **kwargs)\n        return gdf\n    except Exception:\n        pass\n    try:\n        gdf = gpd.read_parquet(filepath, **kwargs)\n        return gdf\n    except Exception:\n        pass\n    try:\n        gdf = gpd.read_feather(filepath, **kwargs)\n        return gdf\n    except Exception:\n        raise Exception(\"Unsupported zone polygon file type\")", "\n\ndef parquet_to_gdf(parquet_filepath: str) -> gpd.GeoDataFrame:\n    gdf = gpd.read_parquet(parquet_filepath)\n    return gdf\n\n\ndef np_to_list(t):\n    return [a.tolist() for a in t]\n", "\n\ndef get_dataset(zarr_json: str) -> xr.Dataset:\n    \"\"\"Retrieve a blob from the data service as xarray.Dataset.\n\n    Parameters\n    ----------\n    blob_name: str, required\n        Name of blob to retrieve.\n\n    Returns\n    -------\n    ds : xarray.Dataset\n        The data stored in the blob.\n\n    \"\"\"\n    backend_args = {\n        \"consolidated\": False,\n        \"storage_options\": {\n            \"fo\": zarr_json,\n            \"remote_protocol\": \"gcs\",\n            \"remote_options\": {\"anon\": True},\n        },\n    }\n    ds = xr.open_dataset(\n        \"reference://\",\n        engine=\"zarr\",\n        backend_kwargs=backend_args,\n    )\n\n    return ds", "\n\ndef list_to_np(lst):\n    return tuple([np.array(a) for a in lst])\n\n\n@dask.delayed\ndef gen_json(\n    remote_path: str, fs: fsspec.filesystem, json_dir: Union[str, Path]\n) -> str:\n    \"\"\"Helper function for creating single-file kerchunk reference JSONs.\n\n    Parameters\n    ----------\n    remote_path : str\n        Path to the file in the remote location (ie, GCS bucket)\n    fs : fsspec.filesystem\n        fsspec filesystem mapped to GCS\n    json_dir : str\n        Directory for saving zarr reference json files\n\n    Returns\n    -------\n    str\n        Path to the local zarr reference json file\n    \"\"\"\n    so = dict(\n        mode=\"rb\",\n        anon=True,\n        default_fill_cache=False,\n        default_cache_type=\"first\",  # noqa\n    )\n    with fs.open(remote_path, **so) as infile:\n        h5chunks = SingleHdf5ToZarr(infile, remote_path, inline_threshold=300)\n        p = remote_path.split(\"/\")\n        date = p[3]\n        fname = p[5]\n        outf = str(Path(json_dir, f\"{date}.{fname}.json\"))\n        with open(outf, \"wb\") as f:\n            f.write(ujson.dumps(h5chunks.translate()).encode())\n    return outf", "\n\ndef build_zarr_references(\n    remote_paths: List[str], json_dir: Union[str, Path]\n) -> list[str]:\n    \"\"\"Builds the single file zarr json reference files using kerchunk.\n\n    Parameters\n    ----------\n    remote_paths : List[str]\n        List of remote filepaths\n    json_dir : str or Path\n        Local directory for caching json files\n\n    Returns\n    -------\n    list[str]\n        List of paths to the zarr reference json files\n    \"\"\"\n    json_dir_path = Path(json_dir)\n    if not json_dir_path.exists():\n        json_dir_path.mkdir(parents=True)\n\n    fs = fsspec.filesystem(\"gcs\", anon=True)\n\n    results = []\n    for path in remote_paths:\n        results.append(gen_json(path, fs, json_dir))\n    json_paths = dask.compute(results)[0]\n\n    return sorted(json_paths)", "\n\ndef validate_run_args(run: str, output_type: str, variable: str):\n    \"\"\"Validates user-provided NWMv22 run arguments.\n\n    Parameters\n    ----------\n    run : str\n        Run type/configuration\n    output_type : str\n        Output component of the configuration\n    variable : str\n        Name of the variable to fetch within the output_type\n\n    Raises\n    ------\n    KeyError\n        Invalid key error\n    \"\"\"\n    try:\n        NWM22_RUN_CONFIG[run]\n    except Exception as e:\n        raise ValueError(f\"Invalid RUN entry: {str(e)}\")\n    try:\n        NWM22_RUN_CONFIG[run][output_type]\n    except Exception as e:\n        raise ValueError(f\"Invalid RUN entry: {str(e)}\")\n    if variable not in NWM22_RUN_CONFIG[run][output_type]:\n        raise KeyError(f\"Invalid VARIABLE_NAME entry: {variable}\")", "\n\ndef construct_assim_paths(\n    gcs_dir: str,\n    run: str,\n    output_type: str,\n    dates: pd.DatetimeIndex,\n    t_minus: Iterable[int],\n    run_name_in_filepath: str,\n    cycle_z_hours: Iterable[int],\n    domain: str,\n) -> list[str]:\n    \"\"\"Constructs paths to NWM point assimilation data based on specified\n        parameters.\n\n    This function prioritizes value time over reference time so that only\n    files with value times falling within the specified date range are included\n    in the resulting file list.\n\n    Parameters\n    ----------\n    gcs_dir : str\n        Path to the NWM data on GCS\n    run : str\n        Run type/configuration\n    output_type : str\n        Output component of the configuration\n    dates : pd.DatetimeIndex\n        Range of days to fetch data\n    t_minus : Iterable[int]\n        Collection of lookback hours to include when fetching assimilation data\n    run_name_in_filepath : str\n        Name of the assimilation run as represented in the GCS file.\n        Defined in const_nwm.py\n    cycle_z_hours : Iterable[int]\n        The z-hour of the assimilation run per day. Defined in const_nwm.py\n    domain : str\n        Geographic region covered by the assimilation run.\n        Defined in const_nwm.py\n\n    Returns\n    -------\n    list[str]\n        List of remote filepaths\n    \"\"\"\n    component_paths = []\n\n    for dt in dates:\n        dt_str = dt.strftime(\"%Y%m%d\")\n\n        # Add the values starting from day 1,\n        # skipping value times in the previous day\n        if \"hawaii\" in run:\n            for cycle_hr in cycle_z_hours:\n                for tm in t_minus:\n                    for tm2 in [0, 15, 30, 45]:\n                        if (tm * 100 + tm2) > cycle_hr * 100:\n                            continue\n                        file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.t{cycle_hr:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}{tm2:02d}.{domain}.nc\"  # noqa\n                        component_paths.append(file_path)\n        else:\n            for cycle_hr in cycle_z_hours:\n                for tm in t_minus:\n                    if tm > cycle_hr:\n                        continue\n                    file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.t{cycle_hr:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n                    component_paths.append(file_path)\n\n        # Now add the values from the day following the end day,\n        # whose value times that fall within the end day\n        if \"extend\" in run:\n            for tm in t_minus:\n                dt_add = dt + pd.Timedelta(cycle_hr + 24, unit=\"hours\")\n                hr_add = dt_add.hour\n                if tm > hr_add:\n                    dt_add_str = dt_add.strftime(\"%Y%m%d\")\n                    file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n                    component_paths.append(file_path)\n\n        elif \"hawaii\" in run:\n            for cycle_hr2 in cycle_z_hours:\n                for tm in t_minus:\n                    for tm2 in [0, 15, 30, 45]:\n                        if cycle_hr2 > 0:\n                            dt_add = dt + pd.Timedelta(\n                                cycle_hr + cycle_hr2, unit=\"hours\"\n                            )\n                            hr_add = dt_add.hour\n                            if (tm * 100 + tm2) > hr_add * 100:\n                                dt_add_str = dt_add.strftime(\"%Y%m%d\")\n                                file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}{tm2:02d}.{domain}.nc\"  # noqa\n                                component_paths.append(file_path)\n        else:\n            for cycle_hr2 in cycle_z_hours:\n                for tm in t_minus:\n                    if cycle_hr2 > 0:\n                        dt_add = dt + pd.Timedelta(\n                            cycle_hr + cycle_hr2, unit=\"hours\"\n                        )\n                        hr_add = dt_add.hour\n                        if tm > hr_add:\n                            dt_add_str = dt_add.strftime(\"%Y%m%d\")\n                            file_path = f\"{gcs_dir}/nwm.{dt_add_str}/{run}/nwm.t{hr_add:02d}z.{run_name_in_filepath}.{output_type}.tm{tm:02d}.{domain}.nc\"  # noqa\n                            component_paths.append(file_path)\n\n    return sorted(component_paths)", "\n\ndef build_remote_nwm_filelist(\n    run: str,\n    output_type: str,\n    start_dt: Union[str, datetime],\n    ingest_days: int,\n    t_minus_hours: Optional[Iterable[int]],\n) -> List[str]:\n    \"\"\"Assembles a list of remote NWM files in GCS based on specified user\n        parameters.\n\n    Parameters\n    ----------\n    run : str\n        Run type/configuration\n    output_type : str\n        Output component of the configuration\n    start_dt : str \u201cYYYY-MM-DD\u201d or datetime\n        Date to begin data ingest\n    ingest_days : int\n        Number of days to ingest data after start date\n    t_minus_hours: Iterable[int]\n        Only necessary if assimilation data is requested.\n        Collection of lookback hours to include when fetching assimilation data\n\n    Returns\n    -------\n    list\n        List of remote filepaths (strings)\n    \"\"\"\n    gcs_dir = f\"gcs://{NWM_BUCKET}\"\n    fs = fsspec.filesystem(\"gcs\", anon=True)\n    dates = pd.date_range(start=start_dt, periods=ingest_days, freq=\"1d\")\n\n    if \"assim\" in run:\n        cycle_z_hours = NWM22_ANALYSIS_CONFIG[run][\"cycle_z_hours\"]\n        domain = NWM22_ANALYSIS_CONFIG[run][\"domain\"]\n        run_name_in_filepath = NWM22_ANALYSIS_CONFIG[run][\n            \"run_name_in_filepath\"\n        ]\n        max_lookback = NWM22_ANALYSIS_CONFIG[run][\"num_lookback_hrs\"]\n\n        if max(t_minus_hours) > max_lookback - 1:\n            raise ValueError(\n                f\"The maximum specified t-minus hour exceeds the lookback \"\n                f\"period for this configuration: {run}; max t-minus: \"\n                f\"{max(t_minus_hours)} hrs; \"\n                f\"look-back period: {max_lookback} hrs\"\n            )\n\n        component_paths = construct_assim_paths(\n            gcs_dir,\n            run,\n            output_type,\n            dates,\n            t_minus_hours,\n            run_name_in_filepath,\n            cycle_z_hours,\n            domain,\n        )\n\n    else:\n        component_paths = []\n\n        for dt in dates:\n            dt_str = dt.strftime(\"%Y%m%d\")\n            file_path = f\"{gcs_dir}/nwm.{dt_str}/{run}/nwm.*.{output_type}*\"\n            component_paths.extend(fs.glob(file_path))\n        component_paths = sorted([f\"gcs://{path}\" for path in component_paths])\n\n    return component_paths", ""]}
{"filename": "src/teehr/loading/ngen.py", "chunked_list": ["\"\"\"\nLibrary of code to ingest NGEN outputs to the TEEHR parquet data model format.\n\nThis code would basically just be a *.csv parser the write to parquet files.\nGiven how variable the ngen output can be at this point, it is not worth\nwriting robust converter code.  A few helper functions below.\n\nSee also examples/loading/ngen_to_parquet.ipynb\n\"\"\"\n# import json", "\"\"\"\n# import json\n# import re\n\n# def get_forcing_file_pattern(realization_filepath):\n#     with open(realization_filepath) as f:\n#         j = json.load(f)\n\n#     file_pattern = j[\"global\"][\"forcing\"].get(\"file_pattern\", None)\n#     path = j[\"global\"][\"forcing\"].get(\"path\", None)", "#     file_pattern = j[\"global\"][\"forcing\"].get(\"file_pattern\", None)\n#     path = j[\"global\"][\"forcing\"].get(\"path\", None)\n\n\n# def glob_re(pattern, strings):\n#     return filter(re.compile(pattern).match, strings)\npass\n"]}
{"filename": "src/teehr/loading/nwm21_retrospective.py", "chunked_list": ["import pandas as pd\nimport xarray as xr\nimport fsspec\n# import numpy as np\nfrom datetime import datetime, timedelta\n\nfrom pathlib import Path\nfrom typing import Union, Iterable\n\nfrom teehr.loading.const_nwm import (", "\nfrom teehr.loading.const_nwm import (\n    NWM22_UNIT_LOOKUP\n)\nfrom teehr.models.loading import (\n    ChunkByEnum\n)\n\n# URL = 's3://noaa-nwm-retro-v2-zarr-pds'\n# MIN_DATE = datetime(1993, 1, 1)", "# URL = 's3://noaa-nwm-retro-v2-zarr-pds'\n# MIN_DATE = datetime(1993, 1, 1)\n# MAX_DATE = datetime(2018, 12, 31, 23)\n\nURL = 's3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr/'\nMIN_DATE = pd.Timestamp(1979, 1, 1)\nMAX_DATE = pd.Timestamp(2020, 12, 31, 23)\n\n\ndef _datetime_to_date(dt: datetime) -> datetime:\n    \"\"\"Convert datetime to date only\"\"\"\n    dt.replace(\n        hour=0,\n        minute=0,\n        second=0,\n        microsecond=0\n    )\n    return dt", "\ndef _datetime_to_date(dt: datetime) -> datetime:\n    \"\"\"Convert datetime to date only\"\"\"\n    dt.replace(\n        hour=0,\n        minute=0,\n        second=0,\n        microsecond=0\n    )\n    return dt", "\n\ndef _da_to_df(da: xr.DataArray) -> pd.DataFrame:\n    \"\"\"Format NWM retrospective data to TEEHR format.\"\"\"\n\n    df = da.to_dataframe()\n    df.reset_index(inplace=True)\n    df[\"measurement_unit\"] = NWM22_UNIT_LOOKUP.get(da.units, da.units)\n    df[\"variable_name\"] = da.name\n    df[\"configuration\"] = \"nwm22_retrospective\"\n    df[\"reference_time\"] = df[\"time\"]\n    df.rename(\n        columns={\n            \"time\": \"value_time\",\n            \"feature_id\": \"location_id\",\n            da.name: \"value\"\n        },\n        inplace=True\n    )\n    df.drop(columns=[\"latitude\", \"longitude\"], inplace=True)\n\n    df[\"location_id\"] = \"nwm22-\" + df[\"location_id\"].astype(str)\n    df[\"location_id\"] = df[\"location_id\"].astype(str).astype(\"category\")\n    df[\"measurement_unit\"] = df[\"measurement_unit\"].astype(\"category\")\n    df[\"variable_name\"] = df[\"variable_name\"].astype(\"category\")\n    df[\"configuration\"] = df[\"configuration\"].astype(\"category\")\n\n    return df", "\n\ndef validate_start_end_date(\n    start_date: Union[str, datetime],\n    end_date: Union[str, datetime],\n):\n    if end_date <= start_date:\n        raise ValueError(\"start_date must be before end_date\")\n\n    if start_date < MIN_DATE:\n        raise ValueError(f\"start_date must be on or after {MIN_DATE}\")\n\n    if end_date > MAX_DATE:\n        raise ValueError(f\"end_date must be on or before {MAX_DATE}\")", "\n\ndef nwm_retro_to_parquet(\n    variable_name: str,\n    location_ids: Iterable[int],\n    start_date: Union[str, datetime, pd.Timestamp],\n    end_date: Union[str, datetime, pd.Timestamp],\n    output_parquet_dir: Union[str, Path],\n    chunk_by: Union[ChunkByEnum, None] = None,\n):\n    \"\"\"Fetch NWM retrospective at NWM COMIDs and store as Parquet.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n\n    \"\"\"\n\n    start_date = pd.Timestamp(start_date)\n    end_date = pd.Timestamp(end_date)\n\n    validate_start_end_date(start_date, end_date)\n\n    output_dir = Path(output_parquet_dir)\n    if not output_dir.exists():\n        output_dir.mkdir(parents=True)\n\n    ds = xr.open_zarr(fsspec.get_mapper(URL, anon=True), consolidated=True)\n\n    # Fetch all at once\n    if chunk_by is None:\n        da = ds[variable_name].sel(\n            feature_id=location_ids,\n            time=slice(start_date, end_date)\n        )\n        df = _da_to_df(da)\n        output_filepath = Path(\n            output_parquet_dir,\n            \"nwm22_retrospective.parquet\"\n        )\n        df.to_parquet(output_filepath)\n        # output_filepath = Path(\n        #     output_parquet_dir,\n        #     \"nwm22_retrospective.csv\"\n        # )\n        # df.to_csv(output_filepath)\n        # print(df)\n\n    if chunk_by == \"day\":\n        # Determine number of days to fetch\n        period_length = timedelta(days=1)\n        start_date = _datetime_to_date(start_date)\n        end_date = _datetime_to_date(end_date)\n        period = end_date - start_date\n        if period < period_length:\n            period = period_length\n        number_of_days = period.days\n\n        # Fetch data in daily batches\n        for day in range(number_of_days):\n\n            # Setup start and end date for fetch\n            start_dt = (start_date + period_length * day)\n            end_dt = (\n                start_date\n                + period_length * (day + 1)\n                - timedelta(minutes=1)\n            )\n            da = ds[variable_name].sel(\n                feature_id=location_ids,\n                time=slice(start_dt, end_dt)\n            )\n            df = _da_to_df(da)\n            output_filepath = Path(\n                output_parquet_dir,\n                f\"{start_dt.strftime('%Y-%m-%d')}.parquet\"\n            )\n            df.to_parquet(output_filepath)\n            # output_filepath = Path(\n            #     output_parquet_dir,\n            #     f\"{start_dt.strftime('%Y-%m-%d')}.csv\"\n            # )\n            # df.to_csv(output_filepath)\n            # print(df)\n\n    # fetch data by site\n    if chunk_by == \"location_id\":\n        for location_id in location_ids:\n            da = ds[variable_name].sel(\n                feature_id=location_id,\n                time=slice(start_date, end_date)\n            )\n            df = _da_to_df(da)\n            output_filepath = Path(\n                output_parquet_dir,\n                f\"{location_id}.parquet\"\n            )\n            df.to_parquet(output_filepath)", "            # output_filepath = Path(\n            #     output_parquet_dir,\n            #     f\"{location_id}.csv\"\n            # )\n            # df.to_csv(output_filepath)\n            # print(df)\n\n\nif __name__ == \"__main__\":\n\n    # Examples\n\n    LOCATION_IDS = [7086109, 7040481]\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=\"2000-01-01\",\n        end_date=\"2000-01-02 23:00\",\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\")\n    )\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=datetime(2000, 1, 1),\n        end_date=datetime(2000, 1, 3),\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n        chunk_by=\"day\",\n    )\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=datetime(2000, 1, 1),\n        end_date=datetime(2000, 1, 2, 23),\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n        chunk_by=\"location_id\",\n    )\n    pass", "if __name__ == \"__main__\":\n\n    # Examples\n\n    LOCATION_IDS = [7086109, 7040481]\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=\"2000-01-01\",\n        end_date=\"2000-01-02 23:00\",\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\")\n    )\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=datetime(2000, 1, 1),\n        end_date=datetime(2000, 1, 3),\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n        chunk_by=\"day\",\n    )\n\n    nwm_retro_to_parquet(\n        variable_name='streamflow',\n        start_date=datetime(2000, 1, 1),\n        end_date=datetime(2000, 1, 2, 23),\n        location_ids=LOCATION_IDS,\n        output_parquet_dir=Path(Path().home(), \"temp\", \"nwm22_retrospective\"),\n        chunk_by=\"location_id\",\n    )\n    pass", ""]}
{"filename": "src/teehr/loading/const_nwm.py", "chunked_list": ["import numpy as np\n\nNWM_BUCKET = \"national-water-model\"\n\nNWM22_CHANNEL_RT_VARS = [\n    \"nudge\",\n    \"qBtmVertRunoff\",\n    \"qBucket\",\n    \"qSfcLatRunoff\",\n    \"streamflow\",", "    \"qSfcLatRunoff\",\n    \"streamflow\",\n    \"velocity\",\n]\n\nNWM22_CHANNEL_RT_VARS_NO_DA = [\n    \"nudge\",\n    \"qBucket\",\n    \"qSfcLatRunoff\",\n    \"streamflow\",", "    \"qSfcLatRunoff\",\n    \"streamflow\",\n    \"velocity\",\n]\n\nNWM22_CHANNEL_RT_VARS_LONG = [\"nudge\", \"streamflow\", \"velocity\"]\n\nNWM22_TERRAIN_VARS = [\"sfcheadsubrt\", \"zwattablrt\"]\n\nNWM22_RESERVOIR_VARS = [", "\nNWM22_RESERVOIR_VARS = [\n    \"inflow\",\n    \"outflow\",\n    \"reservoir_assimiated_value\",\n    \"water_sfc_elev\",\n]\n\nNWM22_LAND_VARS_ASSIM = [\n    \"ACCET\",", "NWM22_LAND_VARS_ASSIM = [\n    \"ACCET\",\n    \"ACSNOM\",\n    \"EDIR\",\n    \"FSNO\",\n    \"ISNOW\",\n    \"QRAIN\",\n    \"QSNOW\",\n    \"SNEQV\",\n    \"SNLIQ\",", "    \"SNEQV\",\n    \"SNLIQ\",\n    \"SNOWH\",\n    \"SNOWT_AVG\",\n    \"SOILICE\",\n    \"SOILSAT_TOP\",\n    \"SOIL_M\",\n    \"SOIL_T\",\n]\n", "]\n\nNWM22_LAND_VARS_SHORT = [\n    \"ACCET\",\n    \"SNOWT_AVG\",\n    \"SOILSAT_TOP\",\n    \"FSNO\",\n    \"SNOWH\",\n    \"SNEQV\",\n]", "    \"SNEQV\",\n]\n\nNWM22_LAND_VARS_MEDIUM = [\n    \"FSA\",\n    \"FIRA\",\n    \"GRDFLX\",\n    \"HFX\",\n    \"LH\",\n    \"UGDRNOFF\",", "    \"LH\",\n    \"UGDRNOFF\",\n    \"ACCECAN\",\n    \"ACCEDIR\",\n    \"ACCETRAN\",\n    \"TRAD\",\n    \"SNLIQ\",\n    \"SOIL_T\",\n    \"SOIL_M\",\n    \"SNOWH\",", "    \"SOIL_M\",\n    \"SNOWH\",\n    \"SNEQV\",\n    \"ISNOW\",\n    \"FSNO\",\n    \"ACSNOM\",\n    \"ACCET\",\n    \"CANWAT\",\n    \"SOILICE\",\n    \"SOILSAT_TOP\",", "    \"SOILICE\",\n    \"SOILSAT_TOP\",\n    \"SNOWT_AVG\",\n]\n\nNWM22_LAND_VARS_LONG = [\n    \"UGDRNOFF\",\n    \"SFCRNOFF\",\n    \"SNEQV\",\n    \"ACSNOM\",", "    \"SNEQV\",\n    \"ACSNOM\",\n    \"ACCET\",\n    \"CANWAT\",\n    \"SOILSAT_TOP\",\n    \"SOILSAT\",\n]\n\nNWM22_FORCING_VARS = [\n    \"U2D\",", "NWM22_FORCING_VARS = [\n    \"U2D\",\n    \"V2D\",\n    \"T2D\",\n    \"Q2D\",\n    \"LWDOWN\",\n    \"SWDOWN\",\n    \"RAINRATE\",\n    \"PSFC\",\n]", "    \"PSFC\",\n]\n\nNWM22_RUN_CONFIG = {\n    \"analysis_assim\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_extend\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,", "    \"analysis_assim_extend\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_extend_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_long\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_long_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_hawaii\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,", "    \"analysis_assim_hawaii\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_hawaii_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_puertorico\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"analysis_assim_puertorico_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_ASSIM,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"short_range\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,", "    \"short_range\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"short_range_hawaii\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,", "        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"short_range_puertorico\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"short_range_hawaii_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"short_range_puertorico_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,", "    \"short_range_puertorico_no_da\": {\n        \"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA,\n        \"terrain_rt\": NWM22_TERRAIN_VARS,\n        \"land\": NWM22_LAND_VARS_SHORT,\n        \"reservoir\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem1\": {\n        \"channel_rt_1\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_1\": NWM22_TERRAIN_VARS,\n        \"land_1\": NWM22_LAND_VARS_MEDIUM,", "        \"terrain_rt_1\": NWM22_TERRAIN_VARS,\n        \"land_1\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_1\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem2\": {\n        \"channel_rt_2\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_2\": NWM22_TERRAIN_VARS,\n        \"land_2\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_2\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir_2\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem3\": {\n        \"channel_rt_3\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_3\": NWM22_TERRAIN_VARS,\n        \"land_3\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_3\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem4\": {\n        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS,", "    \"medium_range_mem4\": {\n        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_4\": NWM22_TERRAIN_VARS,\n        \"land_4\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_4\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem5\": {\n        \"channel_rt_5\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_5\": NWM22_TERRAIN_VARS,\n        \"land_5\": NWM22_LAND_VARS_MEDIUM,", "        \"terrain_rt_5\": NWM22_TERRAIN_VARS,\n        \"land_5\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_5\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem6\": {\n        \"channel_rt_6\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_6\": NWM22_TERRAIN_VARS,\n        \"land_6\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_6\": NWM22_RESERVOIR_VARS,\n    },", "        \"reservoir_6\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_mem7\": {\n        \"channel_rt_7\": NWM22_CHANNEL_RT_VARS,\n        \"terrain_rt_7\": NWM22_TERRAIN_VARS,\n        \"land_7\": NWM22_LAND_VARS_MEDIUM,\n        \"reservoir_7\": NWM22_RESERVOIR_VARS,\n    },\n    \"medium_range_no_da\": {\"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA},\n    \"long_range_mem1\": {", "    \"medium_range_no_da\": {\"channel_rt\": NWM22_CHANNEL_RT_VARS_NO_DA},\n    \"long_range_mem1\": {\n        \"channel_rt_1\": NWM22_CHANNEL_RT_VARS_LONG,\n        \"land_1\": NWM22_LAND_VARS_LONG,\n        \"reservoir_1\": NWM22_RESERVOIR_VARS,\n    },\n    \"long_range_mem2\": {\n        \"channel_rt_2\": NWM22_CHANNEL_RT_VARS_LONG,\n        \"land_2\": NWM22_LAND_VARS_LONG,\n        \"reservoir_2\": NWM22_RESERVOIR_VARS,", "        \"land_2\": NWM22_LAND_VARS_LONG,\n        \"reservoir_2\": NWM22_RESERVOIR_VARS,\n    },\n    \"long_range_mem3\": {\n        \"channel_rt_3\": NWM22_CHANNEL_RT_VARS_LONG,\n        \"land_3\": NWM22_LAND_VARS_LONG,\n        \"reservoir_3\": NWM22_RESERVOIR_VARS,\n    },\n    \"long_range_mem4\": {\n        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS_LONG,", "    \"long_range_mem4\": {\n        \"channel_rt_4\": NWM22_CHANNEL_RT_VARS_LONG,\n        \"land_4\": NWM22_LAND_VARS_LONG,\n        \"reservoir_4\": NWM22_RESERVOIR_VARS,\n    },\n    \"forcing_medium_range\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_short_range\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_short_range_hawaii\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_short_range_puertorico\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_analysis_assim\": {\"forcing\": NWM22_FORCING_VARS},", "    \"forcing_short_range_puertorico\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_analysis_assim\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_analysis_assim_extend\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_analysis_assim_hawaii\": {\"forcing\": NWM22_FORCING_VARS},\n    \"forcing_analysis_assim_puertorico\": {\"forcing\": NWM22_FORCING_VARS},\n}\n\nNWM22_ANALYSIS_CONFIG = {\n    \"analysis_assim\": {\n        \"num_lookback_hrs\": 3,", "    \"analysis_assim\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n    \"analysis_assim_no_da\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"conus\",", "        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n    },\n    \"analysis_assim_extend\": {\n        \"num_lookback_hrs\": 28,\n        \"cycle_z_hours\": [16],\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_extend\",\n    },", "        \"run_name_in_filepath\": \"analysis_assim_extend\",\n    },\n    \"analysis_assim_extend_no_da\": {\n        \"num_lookback_hrs\": 28,\n        \"cycle_z_hours\": [16],\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_extend_no_da\",\n    },\n    \"analysis_assim_long\": {\n        \"num_lookback_hrs\": 12,", "    \"analysis_assim_long\": {\n        \"num_lookback_hrs\": 12,\n        \"cycle_z_hours\": np.arange(0, 24, 6),\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_long\",\n    },\n    \"analysis_assim_long_no_da\": {\n        \"num_lookback_hrs\": 12,\n        \"cycle_z_hours\": np.arange(0, 24, 6),\n        \"domain\": \"conus\",", "        \"cycle_z_hours\": np.arange(0, 24, 6),\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_long_no_da\",\n    },\n    \"analysis_assim_hawaii\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"hawaii\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },", "        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n    \"analysis_assim_hawaii_no_da\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_cycle_z_hourstimes\": np.arange(0, 24, 1),\n        \"domain\": \"hawaii\",\n        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n    },\n    \"analysis_assim_puertorico\": {\n        \"num_lookback_hrs\": 3,", "    \"analysis_assim_puertorico\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"puertorico\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n    \"analysis_assim_puertorico_no_da\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"puertorico\",", "        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"puertorico\",\n        \"run_name_in_filepath\": \"analysis_assim_no_da\",\n    },\n    \"forcing_analysis_assim\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },", "        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n    \"forcing_analysis_assim_extend\": {\n        \"num_lookback_hrs\": 28,\n        \"cycle_z_hours\": [16],\n        \"domain\": \"conus\",\n        \"run_name_in_filepath\": \"analysis_assim_extend\",\n    },\n    \"forcing_analysis_assim_hawaii\": {\n        \"num_lookback_hrs\": 3,", "    \"forcing_analysis_assim_hawaii\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"hawaii\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n    \"forcing_analysis_assim_puertorico\": {\n        \"num_lookback_hrs\": 3,\n        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"puertorico\",", "        \"cycle_z_hours\": np.arange(0, 24, 1),\n        \"domain\": \"puertorico\",\n        \"run_name_in_filepath\": \"analysis_assim\",\n    },\n}\n\n\nNWM22_UNIT_LOOKUP = {\"m3 s-1\": \"m3/s\"}\n\n# WKT strings extracted from NWM grids", "\n# WKT strings extracted from NWM grids\nCONUS_NWM_WKT = 'PROJCS[\"Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]], \\\nPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\nPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-97.0],PARAMETER[\"standard_parallel_1\",30.0],\\\nPARAMETER[\"standard_parallel_2\",60.0],PARAMETER[\"latitude_of_origin\",40.0],UNIT[\"Meter\",1.0]]'\n\nHI_NWM_WKT = 'PROJCS[\"Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]],\\\nPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\nPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-157.42],PARAMETER[\"standard_parallel_1\",10.0],\\", "PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\nPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-157.42],PARAMETER[\"standard_parallel_1\",10.0],\\\nPARAMETER[\"standard_parallel_2\",30.0],PARAMETER[\"latitude_of_origin\",20.6],UNIT[\"Meter\",1.0]]'\n\nPR_NWM_WKT = 'PROJCS[\"Sphere_Lambert_Conformal_Conic\",GEOGCS[\"GCS_Sphere\",DATUM[\"D_Sphere\",SPHEROID[\"Sphere\",6370000.0,0.0]],\\\nPRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"false_easting\",0.0],\\\nPARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-65.91],PARAMETER[\"standard_parallel_1\",18.1],\\\nPARAMETER[\"standard_parallel_2\",18.1],PARAMETER[\"latitude_of_origin\",18.1],UNIT[\"Meter\",1.0]]'\n", ""]}
