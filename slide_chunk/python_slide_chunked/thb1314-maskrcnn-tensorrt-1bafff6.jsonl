{"filename": "patch/mmdet/models/dense_heads/base_dense_head.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nfrom abc import ABCMeta, abstractmethod\n\nimport torch\nfrom mmcv.cnn.utils.weight_init import constant_init\nfrom mmcv.ops import batched_nms\nfrom mmcv.runner import BaseModule, force_fp32\n\nfrom mmdet.core.utils import filter_scores_and_topk, select_single_mlvl\nfrom functools import reduce", "from mmdet.core.utils import filter_scores_and_topk, select_single_mlvl\nfrom functools import reduce\n\n\nclass BaseDenseHead(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for DenseHeads.\"\"\"\n\n    def __init__(self, init_cfg=None):\n        super(BaseDenseHead, self).__init__(init_cfg)\n\n    def init_weights(self):\n        super(BaseDenseHead, self).init_weights()\n        # avoid init_cfg overwrite the initialization of `conv_offset`\n        for m in self.modules():\n            # DeformConv2dPack, ModulatedDeformConv2dPack\n            if hasattr(m, 'conv_offset'):\n                constant_init(m.conv_offset, 0)\n\n    @abstractmethod\n    def loss(self, **kwargs):\n        \"\"\"Compute losses of the head.\"\"\"\n        pass\n\n    @force_fp32(apply_to=('cls_scores', 'bbox_preds'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   score_factors=None,\n                   img_metas=None,\n                   cfg=None,\n                   rescale=False,\n                   with_nms=True,\n                   **kwargs):\n        \"\"\"Transform network outputs of a batch into bbox results.\n\n        Note: When score_factors is not None, the cls_scores are\n        usually multiplied by it then obtain the real score used in NMS,\n        such as CenterNess in FCOS, IoU branch in ATSS.\n\n        Args:\n            cls_scores (list[Tensor]): Classification scores for all\n                scale levels, each is a 4D-tensor, has shape\n                (batch_size, num_priors * num_classes, H, W).\n            bbox_preds (list[Tensor]): Box energies / deltas for all\n                scale levels, each is a 4D-tensor, has shape\n                (batch_size, num_priors * 4, H, W).\n            score_factors (list[Tensor], Optional): Score factor for\n                all scale level, each is a 4D-tensor, has shape\n                (batch_size, num_priors * 1, H, W). Default None.\n            img_metas (list[dict], Optional): Image meta info. Default None.\n            cfg (mmcv.Config, Optional): Test / postprocessing configuration,\n                if None, test_cfg would be used.  Default None.\n            rescale (bool): If True, return boxes in original image space.\n                Default False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default True.\n\n        Returns:\n            list[list[Tensor, Tensor]]: Each item in result_list is 2-tuple.\n                The first item is an (n, 5) tensor, where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1. The second item is a\n                (n,) tensor where each item is the predicted class label of\n                the corresponding box.\n        \"\"\"\n        assert len(cls_scores) == len(bbox_preds)\n\n        if score_factors is None:\n            # e.g. Retina, FreeAnchor, Foveabox, etc.\n            with_score_factors = False\n        else:\n            # e.g. FCOS, PAA, ATSS, AutoAssign, etc.\n            with_score_factors = True\n            assert len(cls_scores) == len(score_factors)\n\n        num_levels = len(cls_scores)\n\n        featmap_sizes = [cls_scores[i].shape[-2:] for i in range(num_levels)]\n        mlvl_priors = self.prior_generator.grid_priors(\n            featmap_sizes,\n            dtype=cls_scores[0].dtype,\n            device=cls_scores[0].device)\n\n        result_list = []\n\n        for img_id in range(len(img_metas)):\n            img_meta = img_metas[img_id]\n            cls_score_list = select_single_mlvl(cls_scores, img_id)\n            bbox_pred_list = select_single_mlvl(bbox_preds, img_id)\n            if with_score_factors:\n                score_factor_list = select_single_mlvl(score_factors, img_id)\n            else:\n                score_factor_list = [None for _ in range(num_levels)]\n\n            results = self._get_bboxes_single(cls_score_list, bbox_pred_list,\n                                              score_factor_list, mlvl_priors,\n                                              img_meta, cfg, rescale, with_nms,\n                                              **kwargs)\n            result_list.append(results)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_score_list,\n                           bbox_pred_list,\n                           score_factor_list,\n                           mlvl_priors,\n                           img_meta,\n                           cfg,\n                           rescale=False,\n                           with_nms=True,\n                           **kwargs):\n        \"\"\"Transform outputs of a single image into bbox predictions.\n\n        Args:\n            cls_score_list (list[Tensor]): Box scores from all scale\n                levels of a single image, each item has shape\n                (num_priors * num_classes, H, W).\n            bbox_pred_list (list[Tensor]): Box energies / deltas from\n                all scale levels of a single image, each item has shape\n                (num_priors * 4, H, W).\n            score_factor_list (list[Tensor]): Score factor from all scale\n                levels of a single image, each item has shape\n                (num_priors * 1, H, W).\n            mlvl_priors (list[Tensor]): Each element in the list is\n                the priors of a single level in feature pyramid. In all\n                anchor-based methods, it has shape (num_priors, 4). In\n                all anchor-free methods, it has shape (num_priors, 2)\n                when `with_stride=True`, otherwise it still has shape\n                (num_priors, 4).\n            img_meta (dict): Image meta info.\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n\n        Returns:\n            tuple[Tensor]: Results of detected bboxes and labels. If with_nms\n                is False and mlvl_score_factor is None, return mlvl_bboxes and\n                mlvl_scores, else return mlvl_bboxes, mlvl_scores and\n                mlvl_score_factor. Usually with_nms is False is used for aug\n                test. If with_nms is True, then return the following format\n\n                - det_bboxes (Tensor): Predicted bboxes with shape \\\n                    [num_bboxes, 5], where the first 4 columns are bounding \\\n                    box positions (tl_x, tl_y, br_x, br_y) and the 5-th \\\n                    column are scores between 0 and 1.\n                - det_labels (Tensor): Predicted labels of the corresponding \\\n                    box with shape [num_bboxes].\n        \"\"\"\n        if score_factor_list[0] is None:\n            # e.g. Retina, FreeAnchor, etc.\n            with_score_factors = False\n        else:\n            # e.g. FCOS, PAA, ATSS, etc.\n            with_score_factors = True\n\n        cfg = self.test_cfg if cfg is None else cfg\n        img_shape = img_meta['img_shape']\n        nms_pre = cfg.get('nms_pre', -1)\n\n        mlvl_bboxes = []\n        mlvl_scores = []\n        mlvl_labels = []\n        if with_score_factors:\n            mlvl_score_factors = []\n        else:\n            mlvl_score_factors = None\n        for level_idx, (cls_score, bbox_pred, score_factor, priors) in \\\n                enumerate(zip(cls_score_list, bbox_pred_list,\n                              score_factor_list, mlvl_priors)):\n\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            if with_score_factors:\n                score_factor = score_factor.permute(1, 2,\n                                                    0).reshape(-1).sigmoid()\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                # remind that we set FG labels to [0, num_class-1]\n                # since mmdet v2.0\n                # BG cat_id: num_class\n                scores = cls_score.softmax(-1)[:, :-1]\n\n            # After https://github.com/open-mmlab/mmdetection/pull/6268/,\n            # this operation keeps fewer bboxes under the same `nms_pre`.\n            # There is no difference in performance for most models. If you\n            # find a slight drop in performance, you can set a larger\n            # `nms_pre` than before.\n            results = filter_scores_and_topk(\n                scores, cfg.score_thr, nms_pre,\n                dict(bbox_pred=bbox_pred, priors=priors))\n            scores, labels, keep_idxs, filtered_results = results\n\n            bbox_pred = filtered_results['bbox_pred']\n            priors = filtered_results['priors']\n\n            if with_score_factors:\n                score_factor = score_factor[keep_idxs]\n\n            bboxes = self.bbox_coder.decode(\n                priors, bbox_pred, max_shape=img_shape)\n\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n            mlvl_labels.append(labels)\n            if with_score_factors:\n                mlvl_score_factors.append(score_factor)\n\n        return self._bbox_post_process(mlvl_scores, mlvl_labels, mlvl_bboxes,\n                                       img_meta['scale_factor'], cfg, rescale,\n                                       with_nms, mlvl_score_factors, **kwargs)\n\n    def _bbox_post_process(self,\n                           mlvl_scores,\n                           mlvl_labels,\n                           mlvl_bboxes,\n                           scale_factor,\n                           cfg,\n                           rescale=False,\n                           with_nms=True,\n                           mlvl_score_factors=None,\n                           **kwargs):\n        \"\"\"bbox post-processing method.\n\n        The boxes would be rescaled to the original image scale and do\n        the nms operation. Usually `with_nms` is False is used for aug test.\n\n        Args:\n            mlvl_scores (list[Tensor]): Box scores from all scale\n                levels of a single image, each item has shape\n                (num_bboxes, ).\n            mlvl_labels (list[Tensor]): Box class labels from all scale\n                levels of a single image, each item has shape\n                (num_bboxes, ).\n            mlvl_bboxes (list[Tensor]): Decoded bboxes from all scale\n                levels of a single image, each item has shape (num_bboxes, 4).\n            scale_factor (ndarray, optional): Scale factor of the image arange\n                as (w_scale, h_scale, w_scale, h_scale).\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n            mlvl_score_factors (list[Tensor], optional): Score factor from\n                all scale levels of a single image, each item has shape\n                (num_bboxes, ). Default: None.\n\n        Returns:\n            tuple[Tensor]: Results of detected bboxes and labels. If with_nms\n                is False and mlvl_score_factor is None, return mlvl_bboxes and\n                mlvl_scores, else return mlvl_bboxes, mlvl_scores and\n                mlvl_score_factor. Usually with_nms is False is used for aug\n                test. If with_nms is True, then return the following format\n\n                - det_bboxes (Tensor): Predicted bboxes with shape \\\n                    [num_bboxes, 5], where the first 4 columns are bounding \\\n                    box positions (tl_x, tl_y, br_x, br_y) and the 5-th \\\n                    column are scores between 0 and 1.\n                - det_labels (Tensor): Predicted labels of the corresponding \\\n                    box with shape [num_bboxes].\n        \"\"\"\n        assert len(mlvl_scores) == len(mlvl_bboxes) == len(mlvl_labels)\n\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        mlvl_labels = torch.cat(mlvl_labels)\n\n        if mlvl_score_factors is not None:\n            # TODO\uff1a Add sqrt operation in order to be consistent with\n            #  the paper.\n            mlvl_score_factors = torch.cat(mlvl_score_factors)\n            mlvl_scores = mlvl_scores * mlvl_score_factors\n\n        if with_nms:\n            if mlvl_bboxes.numel() == 0:\n                det_bboxes = torch.cat([mlvl_bboxes, mlvl_scores[:, None]], -1)\n                return det_bboxes, mlvl_labels\n\n            det_bboxes, keep_idxs = batched_nms(mlvl_bboxes, mlvl_scores,\n                                                mlvl_labels, cfg.nms)\n            det_bboxes = det_bboxes[:cfg.max_per_img]\n            det_labels = mlvl_labels[keep_idxs][:cfg.max_per_img]\n            return det_bboxes, det_labels\n        else:\n            return mlvl_bboxes, mlvl_scores, mlvl_labels\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels=None,\n                      gt_bboxes_ignore=None,\n                      proposal_cfg=None,\n                      **kwargs):\n        \"\"\"\n        Args:\n            x (list[Tensor]): Features from FPN.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            proposal_cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used\n\n        Returns:\n            tuple:\n                losses: (dict[str, Tensor]): A dictionary of loss components.\n                proposal_list (list[Tensor]): Proposals of each image.\n        \"\"\"\n        outs = self(x)\n        if gt_labels is None:\n            loss_inputs = outs + (gt_bboxes, img_metas)\n        else:\n            loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)\n        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        if proposal_cfg is None:\n            return losses\n        else:\n            proposal_list = self.get_bboxes(\n                *outs, img_metas=img_metas, cfg=proposal_cfg)\n            return losses, proposal_list\n\n    def simple_test(self, feats, img_metas, rescale=False):\n        \"\"\"Test function without test-time augmentation.\n\n        Args:\n            feats (tuple[torch.Tensor]): Multi-level features from the\n                upstream network, each is a 4D-tensor.\n            img_metas (list[dict]): List of image information.\n            rescale (bool, optional): Whether to rescale the results.\n                Defaults to False.\n\n        Returns:\n            list[tuple[Tensor, Tensor]]: Each item in result_list is 2-tuple.\n                The first item is ``bboxes`` with shape (n, 5),\n                where 5 represent (tl_x, tl_y, br_x, br_y, score).\n                The shape of the second tensor in the tuple is ``labels``\n                with shape (n, ).\n        \"\"\"\n        return self.simple_test_bboxes(feats, img_metas, rescale=rescale)\n\n    @force_fp32(apply_to=('cls_scores', 'bbox_preds'))\n    def onnx_export(self,\n                    cls_scores,\n                    bbox_preds,\n                    score_factors=None,\n                    img_metas=None,\n                    with_nms=True):\n        \"\"\"Transform network output for a batch into bbox predictions.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                with shape (N, num_points * num_classes, H, W).\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_points * 4, H, W).\n            score_factors (list[Tensor]): score_factors for each s\n                cale level with shape (N, num_points * 1, H, W).\n                Default: None.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc. Default: None.\n            with_nms (bool): Whether apply nms to the bboxes. Default: True.\n\n        Returns:\n            tuple[Tensor, Tensor] | list[tuple]: When `with_nms` is True,\n            it is tuple[Tensor, Tensor], first tensor bboxes with shape\n            [N, num_det, 5], 5 arrange as (x1, y1, x2, y2, score)\n            and second element is class labels of shape [N, num_det].\n            When `with_nms` is False, first tensor is bboxes with\n            shape [N, num_det, 4], second tensor is raw score has\n            shape  [N, num_det, num_classes].\n        \"\"\"\n        assert len(cls_scores) == len(bbox_preds)\n\n        num_levels = len(cls_scores)\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        mlvl_priors = self.prior_generator.grid_priors(\n            featmap_sizes,\n            dtype=bbox_preds[0].dtype,\n            device=bbox_preds[0].device)\n\n        mlvl_cls_scores = [cls_scores[i].detach() for i in range(num_levels)]\n        mlvl_bbox_preds = [bbox_preds[i].detach() for i in range(num_levels)]\n\n        assert len(\n            img_metas\n        ) == 1, 'Only support one input image while in exporting to ONNX'\n        img_shape = img_metas[0]['img_shape_for_onnx']\n\n        cfg = self.test_cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_priors)\n        device = cls_scores[0].device\n        batch_size = cls_scores[0].shape[0]\n        # convert to tensor to keep tracing\n        nms_pre_tensor = torch.tensor(\n            cfg.get('nms_pre', -1), device=device, dtype=torch.long)\n\n        # e.g. Retina, FreeAnchor, etc.\n        if score_factors is None:\n            with_score_factors = False\n            mlvl_score_factor = [None for _ in range(num_levels)]\n        else:\n            # e.g. FCOS, PAA, ATSS, etc.\n            with_score_factors = True\n            mlvl_score_factor = [\n                score_factors[i].detach() for i in range(num_levels)\n            ]\n            mlvl_score_factors = []\n\n        mlvl_batch_bboxes = []\n        mlvl_scores = []\n\n        for cls_score, bbox_pred, score_factors, priors in zip(\n                mlvl_cls_scores, mlvl_bbox_preds, mlvl_score_factor,\n                mlvl_priors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n\n            scores = cls_score.permute(0, 2, 3,1)\n            scores_last_shapes = list(map(int, scores.shape[1:]))\n            scores_last_dims = reduce(lambda x, y: x * y, scores_last_shapes, 1)\n            scores = scores.reshape(-1, scores_last_dims // self.cls_out_channels, self.cls_out_channels)\n\n            if self.use_sigmoid_cls:\n                scores = scores.sigmoid()\n                nms_pre_score = scores\n            else:\n                scores = scores.softmax(-1)\n                nms_pre_score = scores\n\n            if with_score_factors:\n                score_factors = score_factors.permute(0, 2, 3, 1)\n                score_factors_last_shapes = list(map(int, score_factors.shape[1:]))\n                score_factors_last_dims = reduce(lambda x, y: x * y, score_factors_last_shapes, 1)\n                score_factors = score_factors.reshape(-1, score_factors_last_dims).sigmoid()\n            bbox_pred = bbox_pred.permute(0, 2, 3,1)\n            bbox_pred_last_shapes = list(map(int, bbox_pred.shape[1:]))\n            bbox_pred_last_dims = reduce(lambda x, y: x * y, bbox_pred_last_shapes, 1)\n            bbox_pred = bbox_pred.reshape(-1, bbox_pred_last_dims // 4, 4)\n\n            \n            priors = priors.unsqueeze(0).expand(batch_size, -1, -1)\n            # Get top-k predictions\n            from mmdet.core.export import get_k_for_topk\n            nms_pre = get_k_for_topk(nms_pre_tensor, int(bbox_pred.shape[1]))\n            if nms_pre > 0 and nms_pre != int(bbox_pred.shape[1]):\n\n                if with_score_factors:\n                    nms_pre_score = (nms_pre_score * score_factors[..., None])\n                else:\n                    nms_pre_score = nms_pre_score\n\n                # Get maximum scores for foreground classes.\n                if self.use_sigmoid_cls:\n                    max_scores, _ = nms_pre_score.max(-1)\n                else:\n                    # remind that we set FG labels to [0, num_class-1]\n                    # since mmdet v2.0\n                    # BG cat_id: num_class\n                    max_scores, _ = nms_pre_score[..., :-1].max(-1)\n                _, topk_inds = max_scores.topk(nms_pre)\n\n                batch_inds = torch.arange(\n                    batch_size, device=bbox_pred.device).view(\n                        -1, 1).expand_as(topk_inds).long()\n                # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501\n                transformed_inds = int(bbox_pred.shape[1]) * batch_inds + topk_inds\n\n                priors = priors.reshape(-1, int(priors.size(-1)))[transformed_inds, :]\n                priors_last_shapes = list(map(int, priors.shape[1:]))\n                priors_last_dims = reduce(lambda x, y: x * y, priors_last_shapes, 1)\n                priors = priors.reshape(-1, priors_last_dims // int(priors.size(-1)), int(priors.size(-1)))\n\n                bbox_pred = bbox_pred.reshape(-1, 4)[transformed_inds, :]\n                bbox_pred_last_shapes = list(map(int, bbox_pred.shape[1:]))\n                bbox_pred_last_dims = reduce(lambda x, y: x * y, bbox_pred_last_shapes, 1)\n                bbox_pred = bbox_pred.reshape(-1, bbox_pred_last_dims // 4, 4)\n\n                scores = scores.reshape(-1, self.cls_out_channels)[transformed_inds, :]\n                scores_last_shapes = list(map(int, scores.shape[1:]))\n                scores_last_dims = reduce(lambda x, y: x * y, scores_last_shapes, 1)\n                scores = scores.reshape(-1, scores_last_dims // self.cls_out_channels, self.cls_out_channels)\n\n                if with_score_factors:\n                    score_factors = score_factors.reshape(-1, 1)[transformed_inds]\n                    score_factors_shapes = list(map(int, score_factors.shape[1:]))\n                    scores_last_dims = reduce(lambda x, y: x * y, score_factors_shapes, 1)\n                    score_factors = score_factors.reshape(-1, scores_last_dims)\n\n            bboxes = self.bbox_coder.decode(priors, bbox_pred, max_shape=img_shape)\n\n            mlvl_batch_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n            if with_score_factors:\n                mlvl_score_factors.append(score_factors)\n\n        batch_bboxes = torch.cat(mlvl_batch_bboxes, dim=1)\n        batch_scores = torch.cat(mlvl_scores, dim=1)\n        \n        if with_score_factors:\n            batch_score_factors = torch.cat(mlvl_score_factors, dim=1)\n\n        # Replace multiclass_nms with ONNX::NonMaxSuppression in deployment\n\n        from mmdet.core.export import add_dummy_nms_for_onnx\n\n        if not self.use_sigmoid_cls:\n            batch_scores = batch_scores[..., :self.num_classes]\n\n        if with_score_factors:\n            batch_scores = batch_scores * (batch_score_factors.unsqueeze(2))\n\n        if with_nms:\n            max_output_boxes_per_class = cfg.nms.get(\n                'max_output_boxes_per_class', 200)\n            iou_threshold = cfg.nms.get('iou_threshold', 0.5)\n            score_threshold = cfg.score_thr\n            nms_pre = cfg.get('deploy_nms_pre', -1)\n            return add_dummy_nms_for_onnx(batch_bboxes, batch_scores,\n                                          max_output_boxes_per_class,\n                                          iou_threshold, score_threshold,\n                                          nms_pre, cfg.max_per_img)\n        else:\n            return batch_bboxes, batch_scores", ""]}
{"filename": "patch/mmdet/models/dense_heads/rpn_head.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\nfrom mmcv.ops import batched_nms\n\nfrom ..builder import HEADS", "\nfrom ..builder import HEADS\nfrom .anchor_head import AnchorHead\n\n\n@HEADS.register_module()\nclass RPNHead(AnchorHead):\n    \"\"\"RPN head.\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        init_cfg (dict or list[dict], optional): Initialization config dict.\n        num_convs (int): Number of convolution layers in the head. Default 1.\n    \"\"\"  # noqa: W605\n\n    def __init__(self,\n                 in_channels,\n                 init_cfg=dict(type='Normal', layer='Conv2d', std=0.01),\n                 num_convs=1,\n                 **kwargs):\n        self.num_convs = num_convs\n        super(RPNHead, self).__init__(\n            1, in_channels, init_cfg=init_cfg, **kwargs)\n\n    def _init_layers(self):\n        \"\"\"Initialize layers of the head.\"\"\"\n        if self.num_convs > 1:\n            rpn_convs = []\n            for i in range(self.num_convs):\n                if i == 0:\n                    in_channels = self.in_channels\n                else:\n                    in_channels = self.feat_channels\n                # use ``inplace=False`` to avoid error: one of the variables\n                # needed for gradient computation has been modified by an\n                # inplace operation.\n                rpn_convs.append(\n                    ConvModule(\n                        in_channels,\n                        self.feat_channels,\n                        3,\n                        padding=1,\n                        inplace=False))\n            self.rpn_conv = nn.Sequential(*rpn_convs)\n        else:\n            self.rpn_conv = nn.Conv2d(\n                self.in_channels, self.feat_channels, 3, padding=1)\n        self.rpn_cls = nn.Conv2d(self.feat_channels,\n                                 self.num_base_priors * self.cls_out_channels,\n                                 1)\n        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_base_priors * 4,\n                                 1)\n\n    def forward_single(self, x):\n        \"\"\"Forward feature map of a single scale level.\"\"\"\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=False)\n        rpn_cls_score = self.rpn_cls(x)\n        rpn_bbox_pred = self.rpn_reg(x)\n        return rpn_cls_score, rpn_bbox_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             img_metas,\n             gt_bboxes_ignore=None):\n        \"\"\"Compute losses of the head.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W)\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n        losses = super(RPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses['loss_cls'], loss_rpn_bbox=losses['loss_bbox'])\n\n    def _get_bboxes_single(self,\n                           cls_score_list,\n                           bbox_pred_list,\n                           score_factor_list,\n                           mlvl_anchors,\n                           img_meta,\n                           cfg,\n                           rescale=False,\n                           with_nms=True,\n                           **kwargs):\n        \"\"\"Transform outputs of a single image into bbox predictions.\n\n        Args:\n            cls_score_list (list[Tensor]): Box scores from all scale\n                levels of a single image, each item has shape\n                (num_anchors * num_classes, H, W).\n            bbox_pred_list (list[Tensor]): Box energies / deltas from\n                all scale levels of a single image, each item has\n                shape (num_anchors * 4, H, W).\n            score_factor_list (list[Tensor]): Score factor from all scale\n                levels of a single image. RPN head does not need this value.\n            mlvl_anchors (list[Tensor]): Anchors of all scale level\n                each item has shape (num_anchors, 4).\n            img_meta (dict): Image meta info.\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n            with_nms (bool): If True, do nms before return boxes.\n                Default: True.\n\n        Returns:\n            Tensor: Labeled boxes in shape (n, 5), where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1.\n        \"\"\"\n        cfg = self.test_cfg if cfg is None else cfg\n        cfg = copy.deepcopy(cfg)\n        img_shape = img_meta['img_shape']\n\n        # bboxes from different level should be independent during NMS,\n        # level_ids are used as labels for batched NMS to separate them\n        level_ids = []\n        mlvl_scores = []\n        mlvl_bbox_preds = []\n        mlvl_valid_anchors = []\n        nms_pre = cfg.get('nms_pre', -1)\n        for level_idx in range(len(cls_score_list)):\n            rpn_cls_score = cls_score_list[level_idx]\n            rpn_bbox_pred = bbox_pred_list[level_idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                # We set FG labels to [0, num_class-1] and BG label to\n                # num_class in RPN head since mmdet v2.5, which is unified to\n                # be consistent with other head since mmdet v2.0. In mmdet v2.0\n                # to v2.4 we keep BG label as 0 and FG label as 1 in rpn head.\n                scores = rpn_cls_score.softmax(dim=1)[:, 0]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n\n\n            anchors = mlvl_anchors[level_idx]\n\n            if 0 < nms_pre < scores.shape[0]:\n                # sort is faster than topk\n                # _, topk_inds = scores.topk(cfg.nms_pre)\n                ranked_scores, rank_inds = scores.sort(descending=True)\n                topk_inds = rank_inds[:nms_pre]\n                scores = ranked_scores[:nms_pre]\n                \n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n\n\n            mlvl_scores.append(scores)\n            mlvl_bbox_preds.append(rpn_bbox_pred)\n            mlvl_valid_anchors.append(anchors)\n            level_ids.append(\n                scores.new_full((scores.size(0), ),\n                                level_idx,\n                                dtype=torch.long))\n        # torch.save({'batch_bboxes':mlvl_bbox_preds, 'batch_scores':mlvl_scores}, 'batch_info.pkl')\n        ret = self._bbox_post_process(mlvl_scores, mlvl_bbox_preds,\n                                       mlvl_valid_anchors, level_ids, cfg,\n                                       img_shape)\n        # torch.save(ret, 'rpn_nms_result.pkl')\n        return ret\n\n    def _bbox_post_process(self, mlvl_scores, mlvl_bboxes, mlvl_valid_anchors,\n                           level_ids, cfg, img_shape, **kwargs):\n        \"\"\"bbox post-processing method.\n\n        Do the nms operation for bboxes in same level.\n\n        Args:\n            mlvl_scores (list[Tensor]): Box scores from all scale\n                levels of a single image, each item has shape\n                (num_bboxes, ).\n            mlvl_bboxes (list[Tensor]): Decoded bboxes from all scale\n                levels of a single image, each item has shape (num_bboxes, 4).\n            mlvl_valid_anchors (list[Tensor]): Anchors of all scale level\n                each item has shape (num_bboxes, 4).\n            level_ids (list[Tensor]): Indexes from all scale levels of a\n                single image, each item has shape (num_bboxes, ).\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, `self.test_cfg` would be used.\n            img_shape (tuple(int)): The shape of model's input image.\n\n        Returns:\n            Tensor: Labeled boxes in shape (n, 5), where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1.\n        \"\"\"\n        scores = torch.cat(mlvl_scores)\n        anchors = torch.cat(mlvl_valid_anchors)\n        rpn_bbox_pred = torch.cat(mlvl_bboxes)\n        proposals = self.bbox_coder.decode(\n            anchors, rpn_bbox_pred, max_shape=img_shape)\n        ids = torch.cat(level_ids)\n        # torch.save({'ids':ids, 'proposals': proposals, 'mlvl_scores': mlvl_scores, 'mlvl_valid_anchors': mlvl_valid_anchors}, 'proposals.pkl')\n\n        if cfg.min_bbox_size >= 0:\n            w = proposals[:, 2] - proposals[:, 0]\n            h = proposals[:, 3] - proposals[:, 1]\n            valid_mask = (w > cfg.min_bbox_size) & (h > cfg.min_bbox_size)\n            if not valid_mask.all():\n                proposals = proposals[valid_mask]\n                scores = scores[valid_mask]\n                ids = ids[valid_mask]\n\n        if proposals.numel() > 0:\n            dets, _ = batched_nms(proposals, scores, ids, cfg.nms)\n        else:\n            return proposals.new_zeros(0, 5)\n\n        return dets[:cfg.max_per_img]\n\n    def onnx_export(self, x, img_metas):\n        \"\"\"Test without augmentation.\n\n        Args:\n            x (tuple[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n            img_metas (list[dict]): Meta info of each image.\n        Returns:\n            Tensor: dets of shape [N, num_det, 5].\n        \"\"\"\n        cls_scores, bbox_preds = self(x)\n\n        assert len(cls_scores) == len(bbox_preds)\n\n        batch_bboxes, batch_scores = super(RPNHead, self).onnx_export(\n            cls_scores, bbox_preds, img_metas=img_metas, with_nms=False)\n        # Use ONNX::NonMaxSuppression in deployment\n        from mmdet.core.export import add_dummy_nms_for_onnx\n        cfg = copy.deepcopy(self.test_cfg)\n        score_threshold = cfg.nms.get('score_thr', 0.0)\n        nms_pre = cfg.get('deploy_nms_pre', -1)\n\n        # Different from the normal forward doing NMS level by level,\n        # we do NMS across all levels when exporting ONNX.\n        num_levels = len(cls_scores)\n        from mmdet.core.export import get_k_for_topk\n        ids_list = []\n        nms_pre_tensor = torch.tensor(cfg.get('nms_pre', -1), device=cls_scores[0].device, dtype=torch.long)\n        for i in range(num_levels):\n            c, h, w = list(map(int, cls_scores[i].shape[1:4]))\n            cur_lvl_length = torch.as_tensor(c * h * w, dtype=torch.long, device=cls_scores[0].device)\n            topk = get_k_for_topk(nms_pre_tensor, cur_lvl_length)\n            if topk > 0 and topk != int(cur_lvl_length):\n                cur_lvl_length = topk\n            ids_list.append(torch.full((1, cur_lvl_length, 1), fill_value = i, dtype=torch.long, device=cls_scores[0].device))\n        idxs = torch.cat(ids_list, dim=1)\n\n        dets, _ = add_dummy_nms_for_onnx(batch_bboxes, batch_scores,\n                                         cfg.max_per_img,\n                                         cfg.nms.iou_threshold,\n                                         score_threshold, nms_pre,\n                                         cfg.max_per_img,\n                                         idxs = idxs)\n        return dets", ""]}
{"filename": "patch/mmdet/models/roi_heads/test_mixins.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport sys\nimport warnings\n\nimport numpy as np\nimport torch\n\nfrom mmdet.core import (bbox2roi, bbox_mapping, merge_aug_bboxes,\n                        merge_aug_masks, multiclass_nms)\n\nif sys.version_info >= (3, 7):\n    from mmdet.utils.contextmanagers import completed", "                        merge_aug_masks, multiclass_nms)\n\nif sys.version_info >= (3, 7):\n    from mmdet.utils.contextmanagers import completed\n\n\nclass BBoxTestMixin:\n\n    if sys.version_info >= (3, 7):\n\n        async def async_test_bboxes(self,\n                                    x,\n                                    img_metas,\n                                    proposals,\n                                    rcnn_test_cfg,\n                                    rescale=False,\n                                    **kwargs):\n            \"\"\"Asynchronized test for box head without augmentation.\"\"\"\n            rois = bbox2roi(proposals)\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            if self.with_shared_head:\n                roi_feats = self.shared_head(roi_feats)\n            sleep_interval = rcnn_test_cfg.get('async_sleep_interval', 0.017)\n\n            async with completed(\n                    __name__, 'bbox_head_forward',\n                    sleep_interval=sleep_interval):\n                cls_score, bbox_pred = self.bbox_head(roi_feats)\n\n            img_shape = img_metas[0]['img_shape']\n            scale_factor = img_metas[0]['scale_factor']\n            det_bboxes, det_labels = self.bbox_head.get_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=rescale,\n                cfg=rcnn_test_cfg)\n            return det_bboxes, det_labels\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_metas,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        \"\"\"Test only det bboxes without augmentation.\n\n        Args:\n            x (tuple[Tensor]): Feature maps of all scale level.\n            img_metas (list[dict]): Image meta info.\n            proposals (List[Tensor]): Region proposals.\n            rcnn_test_cfg (obj:`ConfigDict`): `test_cfg` of R-CNN.\n            rescale (bool): If True, return boxes in original image space.\n                Default: False.\n\n        Returns:\n            tuple[list[Tensor], list[Tensor]]: The first list contains\n                the boxes of the corresponding image in a batch, each\n                tensor has the shape (num_boxes, 5) and last dimension\n                5 represent (tl_x, tl_y, br_x, br_y, score). Each Tensor\n                in the second list is the labels with shape (num_boxes, ).\n                The length of both lists should be equal to batch_size.\n        \"\"\"\n\n        rois = bbox2roi(proposals)\n\n        if rois.shape[0] == 0:\n            batch_size = len(proposals)\n            det_bbox = rois.new_zeros(0, 5)\n            det_label = rois.new_zeros((0, ), dtype=torch.long)\n            if rcnn_test_cfg is None:\n                det_bbox = det_bbox[:, :4]\n                det_label = rois.new_zeros(\n                    (0, self.bbox_head.fc_cls.out_features))\n            # There is no proposal in the whole batch\n            return [det_bbox] * batch_size, [det_label] * batch_size\n\n        bbox_results = self._bbox_forward(x, rois)\n        img_shapes = tuple(meta['img_shape'] for meta in img_metas)\n        scale_factors = tuple(meta['scale_factor'] for meta in img_metas)\n\n        # split batch bbox prediction back to each image\n        cls_score = bbox_results['cls_score']\n        bbox_pred = bbox_results['bbox_pred']\n        num_proposals_per_img = tuple(len(p) for p in proposals)\n        rois = rois.split(num_proposals_per_img, 0)\n        cls_score = cls_score.split(num_proposals_per_img, 0)\n\n        # some detector with_reg is False, bbox_pred will be None\n        if bbox_pred is not None:\n            # TODO move this to a sabl_roi_head\n            # the bbox prediction of some detectors like SABL is not Tensor\n            if isinstance(bbox_pred, torch.Tensor):\n                bbox_pred = bbox_pred.split(num_proposals_per_img, 0)\n            else:\n                bbox_pred = self.bbox_head.bbox_pred_split(\n                    bbox_pred, num_proposals_per_img)\n        else:\n            bbox_pred = (None, ) * len(proposals)\n\n        # apply bbox post-processing to each image individually\n        det_bboxes = []\n        det_labels = []\n        for i in range(len(proposals)):\n            if rois[i].shape[0] == 0:\n                # There is no proposal in the single image\n                det_bbox = rois[i].new_zeros(0, 5)\n                det_label = rois[i].new_zeros((0, ), dtype=torch.long)\n                if rcnn_test_cfg is None:\n                    det_bbox = det_bbox[:, :4]\n                    det_label = rois[i].new_zeros(\n                        (0, self.bbox_head.fc_cls.out_features))\n\n            else:\n                det_bbox, det_label = self.bbox_head.get_bboxes(\n                    rois[i],\n                    cls_score[i],\n                    bbox_pred[i],\n                    img_shapes[i],\n                    scale_factors[i],\n                    rescale=rescale,\n                    cfg=rcnn_test_cfg)\n            det_bboxes.append(det_bbox)\n            det_labels.append(det_label)\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        \"\"\"Test det bboxes with test time augmentation.\"\"\"\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0]['img_shape']\n            scale_factor = img_meta[0]['scale_factor']\n            flip = img_meta[0]['flip']\n            flip_direction = img_meta[0]['flip_direction']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip, flip_direction)\n            rois = bbox2roi([proposals])\n            bbox_results = self._bbox_forward(x, rois)\n            bboxes, scores = self.bbox_head.get_bboxes(\n                rois,\n                bbox_results['cls_score'],\n                bbox_results['bbox_pred'],\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        if merged_bboxes.shape[0] == 0:\n            # There is no proposal in the single image\n            det_bboxes = merged_bboxes.new_zeros(0, 5)\n            det_labels = merged_bboxes.new_zeros((0, ), dtype=torch.long)\n        else:\n            det_bboxes, det_labels = multiclass_nms(merged_bboxes,\n                                                    merged_scores,\n                                                    rcnn_test_cfg.score_thr,\n                                                    rcnn_test_cfg.nms,\n                                                    rcnn_test_cfg.max_per_img)\n        return det_bboxes, det_labels", "\n\nclass MaskTestMixin:\n\n    if sys.version_info >= (3, 7):\n\n        async def async_test_mask(self,\n                                  x,\n                                  img_metas,\n                                  det_bboxes,\n                                  det_labels,\n                                  rescale=False,\n                                  mask_test_cfg=None):\n            \"\"\"Asynchronized test for mask head without augmentation.\"\"\"\n            # image shape of the first image in the batch (only one)\n            ori_shape = img_metas[0]['ori_shape']\n            scale_factor = img_metas[0]['scale_factor']\n            if det_bboxes.shape[0] == 0:\n                segm_result = [[] for _ in range(self.mask_head.num_classes)]\n            else:\n                if rescale and not isinstance(scale_factor,\n                                              (float, torch.Tensor)):\n                    scale_factor = det_bboxes.new_tensor(scale_factor)\n                _bboxes = (\n                    det_bboxes[:, :4] *\n                    scale_factor if rescale else det_bboxes)\n                mask_rois = bbox2roi([_bboxes])\n                mask_feats = self.mask_roi_extractor(\n                    x[:len(self.mask_roi_extractor.featmap_strides)],\n                    mask_rois)\n\n                if self.with_shared_head:\n                    mask_feats = self.shared_head(mask_feats)\n                if mask_test_cfg and mask_test_cfg.get('async_sleep_interval'):\n                    sleep_interval = mask_test_cfg['async_sleep_interval']\n                else:\n                    sleep_interval = 0.035\n                async with completed(\n                        __name__,\n                        'mask_head_forward',\n                        sleep_interval=sleep_interval):\n                    mask_pred = self.mask_head(mask_feats)\n                segm_result = self.mask_head.get_seg_masks(\n                    mask_pred, _bboxes, det_labels, self.test_cfg, ori_shape,\n                    scale_factor, rescale)\n            return segm_result\n\n    def simple_test_mask(self,\n                         x,\n                         img_metas,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        \"\"\"Simple test for mask head without augmentation.\"\"\"\n        # image shapes of images in the batch\n        ori_shapes = tuple(meta['ori_shape'] for meta in img_metas)\n        scale_factors = tuple(meta['scale_factor'] for meta in img_metas)\n\n        if isinstance(scale_factors[0], float):\n            warnings.warn(\n                'Scale factor in img_metas should be a '\n                'ndarray with shape (4,) '\n                'arrange as (factor_w, factor_h, factor_w, factor_h), '\n                'The scale_factor with float type has been deprecated. ')\n            scale_factors = np.array([scale_factors] * 4, dtype=np.float32)\n\n        num_imgs = len(det_bboxes)\n        if all(det_bbox.shape[0] == 0 for det_bbox in det_bboxes):\n            segm_results = [[[] for _ in range(self.mask_head.num_classes)]\n                            for _ in range(num_imgs)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            if rescale:\n                scale_factors = [\n                    torch.as_tensor(scale_factor).to(det_bboxes[0].device)\n                    for scale_factor in scale_factors\n                ]\n            _bboxes = [\n                det_bboxes[i][:, :4] *\n                scale_factors[i] if rescale else det_bboxes[i][:, :4]\n                for i in range(len(det_bboxes))\n            ]\n            mask_rois = bbox2roi(_bboxes)\n            mask_results = self._mask_forward(x, mask_rois)\n            mask_pred = mask_results['mask_pred']\n            # split batch mask prediction back to each image\n            num_mask_roi_per_img = [len(det_bbox) for det_bbox in det_bboxes]\n            mask_preds = mask_pred.split(num_mask_roi_per_img, 0)\n\n            # apply mask post-processing to each image individually\n            segm_results = []\n            for i in range(num_imgs):\n                if det_bboxes[i].shape[0] == 0:\n                    segm_results.append(\n                        [[] for _ in range(self.mask_head.num_classes)])\n                else:\n                    segm_result = self.mask_head.get_seg_masks(\n                        mask_preds[i], _bboxes[i], det_labels[i],\n                        self.test_cfg, ori_shapes[i], scale_factors[i],\n                        rescale)\n                    segm_results.append(segm_result)\n        return segm_results\n\n    def aug_test_mask(self, feats, img_metas, det_bboxes, det_labels):\n        \"\"\"Test for mask head with test time augmentation.\"\"\"\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes)]\n        else:\n            aug_masks = []\n            for x, img_meta in zip(feats, img_metas):\n                img_shape = img_meta[0]['img_shape']\n                scale_factor = img_meta[0]['scale_factor']\n                flip = img_meta[0]['flip']\n                flip_direction = img_meta[0]['flip_direction']\n                _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                       scale_factor, flip, flip_direction)\n                mask_rois = bbox2roi([_bboxes])\n                mask_results = self._mask_forward(x, mask_rois)\n                # convert to numpy array to save memory\n                aug_masks.append(\n                    mask_results['mask_pred'].sigmoid().cpu().numpy())\n            merged_masks = merge_aug_masks(aug_masks, img_metas, self.test_cfg)\n\n            ori_shape = img_metas[0][0]['ori_shape']\n            scale_factor = det_bboxes.new_ones(4)\n            segm_result = self.mask_head.get_seg_masks(\n                merged_masks,\n                det_bboxes,\n                det_labels,\n                self.test_cfg,\n                ori_shape,\n                scale_factor=scale_factor,\n                rescale=False)\n        return segm_result", ""]}
{"filename": "patch/mmdet/models/roi_heads/standard_roi_head.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport torch\n\nfrom mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler\nfrom ..builder import HEADS, build_head, build_roi_extractor\nfrom .base_roi_head import BaseRoIHead\nfrom .test_mixins import BBoxTestMixin, MaskTestMixin\n\n\n@HEADS.register_module()\nclass StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\n    \"\"\"Simplest base roi head including one bbox head and one mask head.\"\"\"\n\n    def init_assigner_sampler(self):\n        \"\"\"Initialize assigner and sampler.\"\"\"\n        self.bbox_assigner = None\n        self.bbox_sampler = None\n        if self.train_cfg:\n            self.bbox_assigner = build_assigner(self.train_cfg.assigner)\n            self.bbox_sampler = build_sampler(\n                self.train_cfg.sampler, context=self)\n\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\n        \"\"\"Initialize ``bbox_head``\"\"\"\n        self.bbox_roi_extractor = build_roi_extractor(bbox_roi_extractor)\n        self.bbox_head = build_head(bbox_head)\n\n    def init_mask_head(self, mask_roi_extractor, mask_head):\n        \"\"\"Initialize ``mask_head``\"\"\"\n        if mask_roi_extractor is not None:\n            self.mask_roi_extractor = build_roi_extractor(mask_roi_extractor)\n            self.share_roi_extractor = False\n        else:\n            self.share_roi_extractor = True\n            self.mask_roi_extractor = self.bbox_roi_extractor\n        self.mask_head = build_head(mask_head)\n\n    def forward_dummy(self, x, proposals):\n        \"\"\"Dummy forward function.\"\"\"\n        # bbox head\n        outs = ()\n        rois = bbox2roi([proposals])\n        if self.with_bbox:\n            bbox_results = self._bbox_forward(x, rois)\n            outs = outs + (bbox_results['cls_score'],\n                           bbox_results['bbox_pred'])\n        # mask head\n        if self.with_mask:\n            mask_rois = rois[:100]\n            mask_results = self._mask_forward(x, mask_rois)\n            outs = outs + (mask_results['mask_pred'], )\n        return outs\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      **kwargs):\n        \"\"\"\n        Args:\n            x (list[Tensor]): list of multi-level img features.\n            img_metas (list[dict]): list of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n            proposals (list[Tensors]): list of region proposals.\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): class indices corresponding to each box\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = self.bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = self.bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        losses = dict()\n        # bbox head forward and loss\n        if self.with_bbox:\n            bbox_results = self._bbox_forward_train(x, sampling_results,\n                                                    gt_bboxes, gt_labels,\n                                                    img_metas)\n            losses.update(bbox_results['loss_bbox'])\n\n        # mask head forward and loss\n        if self.with_mask:\n            mask_results = self._mask_forward_train(x, sampling_results,\n                                                    bbox_results['bbox_feats'],\n                                                    gt_masks, img_metas)\n            losses.update(mask_results['loss_mask'])\n\n        return losses\n\n    def _bbox_forward(self, x, rois):\n        \"\"\"Box head forward function used in both training and testing.\"\"\"\n        # TODO: a more flexible way to decide which feature maps to use\n        bbox_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        if self.with_shared_head:\n            bbox_feats = self.shared_head(bbox_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_feats)\n        return bbox_results\n\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\n                            img_metas):\n        \"\"\"Run forward function and calculate loss for box head in training.\"\"\"\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(x, rois)\n\n        bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,\n                                                  gt_labels, self.train_cfg)\n        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],\n                                        bbox_results['bbox_pred'], rois,\n                                        *bbox_targets)\n\n        bbox_results.update(loss_bbox=loss_bbox)\n        return bbox_results\n\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\n                            img_metas):\n        \"\"\"Run forward function and calculate loss for mask head in\n        training.\"\"\"\n        if not self.share_roi_extractor:\n            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n            mask_results = self._mask_forward(x, pos_rois)\n        else:\n            pos_inds = []\n            device = bbox_feats.device\n            for res in sampling_results:\n                pos_inds.append(\n                    torch.ones(\n                        res.pos_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n                pos_inds.append(\n                    torch.zeros(\n                        res.neg_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n            pos_inds = torch.cat(pos_inds)\n\n            mask_results = self._mask_forward(\n                x, pos_inds=pos_inds, bbox_feats=bbox_feats)\n\n        mask_targets = self.mask_head.get_targets(sampling_results, gt_masks,\n                                                  self.train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = self.mask_head.loss(mask_results['mask_pred'],\n                                        mask_targets, pos_labels)\n\n        mask_results.update(loss_mask=loss_mask, mask_targets=mask_targets)\n        return mask_results\n\n    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):\n        \"\"\"Mask head forward function used in both training and testing.\"\"\"\n        assert ((rois is not None) ^\n                (pos_inds is not None and bbox_feats is not None))\n        if rois is not None:\n            mask_feats = self.mask_roi_extractor(\n                x[:self.mask_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                mask_feats = self.shared_head(mask_feats)\n        else:\n            assert bbox_feats is not None\n            mask_feats = bbox_feats[pos_inds]\n\n        mask_pred = self.mask_head(mask_feats)\n        mask_results = dict(mask_pred=mask_pred, mask_feats=mask_feats)\n        return mask_results\n\n    async def async_simple_test(self,\n                                x,\n                                proposal_list,\n                                img_metas,\n                                proposals=None,\n                                rescale=False):\n        \"\"\"Async test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n\n        det_bboxes, det_labels = await self.async_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = await self.async_test_mask(\n                x,\n                img_metas,\n                det_bboxes,\n                det_labels,\n                rescale=rescale,\n                mask_test_cfg=self.test_cfg.get('mask'))\n            return bbox_results, segm_results\n\n    def simple_test(self,\n                    x,\n                    proposal_list,\n                    img_metas,\n                    proposals=None,\n                    rescale=False):\n        \"\"\"Test without augmentation.\n\n        Args:\n            x (tuple[Tensor]): Features from upstream network. Each\n                has shape (batch_size, c, h, w).\n            proposal_list (list(Tensor)): Proposals from rpn head.\n                Each has shape (num_proposals, 5), last dimension\n                5 represent (x1, y1, x2, y2, score).\n            img_metas (list[dict]): Meta information of images.\n            rescale (bool): Whether to rescale the results to\n                the original image. Default: True.\n\n        Returns:\n            list[list[np.ndarray]] or list[tuple]: When no mask branch,\n            it is bbox results of each image and classes with type\n            `list[list[np.ndarray]]`. The outer list\n            corresponds to each image. The inner list\n            corresponds to each class. When the model has mask branch,\n            it contains bbox results and mask results.\n            The outer list corresponds to each image, and first element\n            of tuple is bbox results, second element is mask results.\n        \"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        # torch.save({'det_bboxes':det_bboxes, 'det_labels':det_labels}, 'det.pkl')\n        bbox_results = [\n            bbox2result(det_bboxes[i], det_labels[i],\n                        self.bbox_head.num_classes)\n            for i in range(len(det_bboxes))\n        ]\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = self.simple_test_mask(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return list(zip(bbox_results, segm_results))\n\n    def aug_test(self, x, proposal_list, img_metas, rescale=False):\n        \"\"\"Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        \"\"\"\n        det_bboxes, det_labels = self.aug_test_bboxes(x, img_metas,\n                                                      proposal_list,\n                                                      self.test_cfg)\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= det_bboxes.new_tensor(\n                img_metas[0][0]['scale_factor'])\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        # det_bboxes always keep the original scale\n        if self.with_mask:\n            segm_results = self.aug_test_mask(x, img_metas, det_bboxes,\n                                              det_labels)\n            return [(bbox_results, segm_results)]\n        else:\n            return [bbox_results]\n\n    def onnx_export(self, x, proposals, img_metas, rescale=False):\n        \"\"\"Test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        det_bboxes, det_labels = self.bbox_onnx_export(\n            x, img_metas, proposals, self.test_cfg, rescale=rescale)\n\n        if not self.with_mask:\n            return det_bboxes, det_labels\n        else:\n            segm_results = self.mask_onnx_export(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return det_bboxes, det_labels, segm_results\n\n    def mask_onnx_export(self, x, img_metas, det_bboxes, det_labels, **kwargs):\n        \"\"\"Export mask branch to onnx which supports batch inference.\n\n        Args:\n            x (tuple[Tensor]): Feature maps of all scale level.\n            img_metas (list[dict]): Image meta info.\n            det_bboxes (Tensor): Bboxes and corresponding scores.\n                has shape [N, num_bboxes, 5].\n            det_labels (Tensor): class labels of\n                shape [N, num_bboxes].\n\n        Returns:\n            Tensor: The segmentation results of shape [N, num_bboxes,\n                image_height, image_width].\n        \"\"\"\n        # image shapes of images in the batch\n\n        if all(det_bbox.shape[0] == 0 for det_bbox in det_bboxes):\n            raise RuntimeError('[ONNX Error] Can not record MaskHead '\n                               'as it has not been executed this time')\n        batch_size = det_bboxes.size(0)\n        # if det_bboxes is rescaled to the original image size, we need to\n        # rescale it back to the testing scale to obtain RoIs.\n        det_bboxes = det_bboxes[..., :4]\n        batch_index = torch.arange(\n            det_bboxes.size(0), device=det_bboxes.device).float().view(\n                -1, 1, 1).expand(det_bboxes.size(0), det_bboxes.size(1), 1)\n        mask_rois = torch.cat([batch_index, det_bboxes], dim=-1)\n        mask_rois = mask_rois.view(-1, 5)\n        mask_results = self._mask_forward(x, mask_rois)\n        mask_pred = mask_results['mask_pred']\n        max_shape = img_metas[0]['img_shape_for_onnx']\n        num_det = int(det_bboxes.shape[1])\n        det_bboxes = det_bboxes.reshape(-1, 4)\n        det_labels = det_labels.reshape(-1)\n        segm_results = self.mask_head.onnx_export(mask_pred, det_bboxes,\n                                                  det_labels, self.test_cfg,\n                                                  max_shape)\n        segm_results = segm_results.reshape(batch_size, num_det, max_shape[0],\n                                            max_shape[1])\n        return segm_results\n\n    def bbox_onnx_export(self, x, img_metas, proposals, rcnn_test_cfg,\n                         **kwargs):\n        \"\"\"Export bbox branch to onnx which supports batch inference.\n\n        Args:\n            x (tuple[Tensor]): Feature maps of all scale level.\n            img_metas (list[dict]): Image meta info.\n            proposals (Tensor): Region proposals with\n                batch dimension, has shape [N, num_bboxes, 5].\n            rcnn_test_cfg (obj:`ConfigDict`): `test_cfg` of R-CNN.\n\n        Returns:\n            tuple[Tensor, Tensor]: bboxes of shape [N, num_bboxes, 5]\n                and class labels of shape [N, num_bboxes].\n        \"\"\"\n        # get origin input shape to support onnx dynamic input shape\n        assert len(\n            img_metas\n        ) == 1, 'Only support one input image while in exporting to ONNX'\n        img_shapes = img_metas[0]['img_shape_for_onnx']\n\n        rois = proposals\n\n        batch_index = torch.arange(\n            rois.size(0), device=rois.device).float().view(-1, 1, 1).expand(\n                rois.size(0), rois.size(1), 1)\n\n        rois = torch.cat([batch_index, rois[..., :4]], dim=-1)\n        batch_size = rois.shape[0]\n        num_proposals_per_img = int(rois.shape[1])\n\n        # Eliminate the batch dimension\n        rois = rois.view(-1, 5)\n        bbox_results = self._bbox_forward(x, rois)\n        cls_score = bbox_results['cls_score']\n        bbox_pred = bbox_results['bbox_pred']\n\n        # Recover the batch dimension\n        rois = rois.reshape(-1, num_proposals_per_img, 5)\n        cls_score = cls_score.reshape(-1, num_proposals_per_img,\n                                      int(cls_score.size(-1)))\n\n        bbox_pred = bbox_pred.reshape(-1, num_proposals_per_img,\n                                      int(bbox_pred.size(-1)))\n        det_bboxes, det_labels = self.bbox_head.onnx_export(\n            rois, cls_score, bbox_pred, img_shapes, cfg=rcnn_test_cfg)\n\n        return det_bboxes, det_labels", "\n@HEADS.register_module()\nclass StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\n    \"\"\"Simplest base roi head including one bbox head and one mask head.\"\"\"\n\n    def init_assigner_sampler(self):\n        \"\"\"Initialize assigner and sampler.\"\"\"\n        self.bbox_assigner = None\n        self.bbox_sampler = None\n        if self.train_cfg:\n            self.bbox_assigner = build_assigner(self.train_cfg.assigner)\n            self.bbox_sampler = build_sampler(\n                self.train_cfg.sampler, context=self)\n\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\n        \"\"\"Initialize ``bbox_head``\"\"\"\n        self.bbox_roi_extractor = build_roi_extractor(bbox_roi_extractor)\n        self.bbox_head = build_head(bbox_head)\n\n    def init_mask_head(self, mask_roi_extractor, mask_head):\n        \"\"\"Initialize ``mask_head``\"\"\"\n        if mask_roi_extractor is not None:\n            self.mask_roi_extractor = build_roi_extractor(mask_roi_extractor)\n            self.share_roi_extractor = False\n        else:\n            self.share_roi_extractor = True\n            self.mask_roi_extractor = self.bbox_roi_extractor\n        self.mask_head = build_head(mask_head)\n\n    def forward_dummy(self, x, proposals):\n        \"\"\"Dummy forward function.\"\"\"\n        # bbox head\n        outs = ()\n        rois = bbox2roi([proposals])\n        if self.with_bbox:\n            bbox_results = self._bbox_forward(x, rois)\n            outs = outs + (bbox_results['cls_score'],\n                           bbox_results['bbox_pred'])\n        # mask head\n        if self.with_mask:\n            mask_rois = rois[:100]\n            mask_results = self._mask_forward(x, mask_rois)\n            outs = outs + (mask_results['mask_pred'], )\n        return outs\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      **kwargs):\n        \"\"\"\n        Args:\n            x (list[Tensor]): list of multi-level img features.\n            img_metas (list[dict]): list of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n            proposals (list[Tensors]): list of region proposals.\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): class indices corresponding to each box\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = self.bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = self.bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        losses = dict()\n        # bbox head forward and loss\n        if self.with_bbox:\n            bbox_results = self._bbox_forward_train(x, sampling_results,\n                                                    gt_bboxes, gt_labels,\n                                                    img_metas)\n            losses.update(bbox_results['loss_bbox'])\n\n        # mask head forward and loss\n        if self.with_mask:\n            mask_results = self._mask_forward_train(x, sampling_results,\n                                                    bbox_results['bbox_feats'],\n                                                    gt_masks, img_metas)\n            losses.update(mask_results['loss_mask'])\n\n        return losses\n\n    def _bbox_forward(self, x, rois):\n        \"\"\"Box head forward function used in both training and testing.\"\"\"\n        # TODO: a more flexible way to decide which feature maps to use\n        bbox_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        if self.with_shared_head:\n            bbox_feats = self.shared_head(bbox_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_feats)\n        return bbox_results\n\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\n                            img_metas):\n        \"\"\"Run forward function and calculate loss for box head in training.\"\"\"\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(x, rois)\n\n        bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,\n                                                  gt_labels, self.train_cfg)\n        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],\n                                        bbox_results['bbox_pred'], rois,\n                                        *bbox_targets)\n\n        bbox_results.update(loss_bbox=loss_bbox)\n        return bbox_results\n\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\n                            img_metas):\n        \"\"\"Run forward function and calculate loss for mask head in\n        training.\"\"\"\n        if not self.share_roi_extractor:\n            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n            mask_results = self._mask_forward(x, pos_rois)\n        else:\n            pos_inds = []\n            device = bbox_feats.device\n            for res in sampling_results:\n                pos_inds.append(\n                    torch.ones(\n                        res.pos_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n                pos_inds.append(\n                    torch.zeros(\n                        res.neg_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n            pos_inds = torch.cat(pos_inds)\n\n            mask_results = self._mask_forward(\n                x, pos_inds=pos_inds, bbox_feats=bbox_feats)\n\n        mask_targets = self.mask_head.get_targets(sampling_results, gt_masks,\n                                                  self.train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = self.mask_head.loss(mask_results['mask_pred'],\n                                        mask_targets, pos_labels)\n\n        mask_results.update(loss_mask=loss_mask, mask_targets=mask_targets)\n        return mask_results\n\n    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):\n        \"\"\"Mask head forward function used in both training and testing.\"\"\"\n        assert ((rois is not None) ^\n                (pos_inds is not None and bbox_feats is not None))\n        if rois is not None:\n            mask_feats = self.mask_roi_extractor(\n                x[:self.mask_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                mask_feats = self.shared_head(mask_feats)\n        else:\n            assert bbox_feats is not None\n            mask_feats = bbox_feats[pos_inds]\n\n        mask_pred = self.mask_head(mask_feats)\n        mask_results = dict(mask_pred=mask_pred, mask_feats=mask_feats)\n        return mask_results\n\n    async def async_simple_test(self,\n                                x,\n                                proposal_list,\n                                img_metas,\n                                proposals=None,\n                                rescale=False):\n        \"\"\"Async test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n\n        det_bboxes, det_labels = await self.async_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = await self.async_test_mask(\n                x,\n                img_metas,\n                det_bboxes,\n                det_labels,\n                rescale=rescale,\n                mask_test_cfg=self.test_cfg.get('mask'))\n            return bbox_results, segm_results\n\n    def simple_test(self,\n                    x,\n                    proposal_list,\n                    img_metas,\n                    proposals=None,\n                    rescale=False):\n        \"\"\"Test without augmentation.\n\n        Args:\n            x (tuple[Tensor]): Features from upstream network. Each\n                has shape (batch_size, c, h, w).\n            proposal_list (list(Tensor)): Proposals from rpn head.\n                Each has shape (num_proposals, 5), last dimension\n                5 represent (x1, y1, x2, y2, score).\n            img_metas (list[dict]): Meta information of images.\n            rescale (bool): Whether to rescale the results to\n                the original image. Default: True.\n\n        Returns:\n            list[list[np.ndarray]] or list[tuple]: When no mask branch,\n            it is bbox results of each image and classes with type\n            `list[list[np.ndarray]]`. The outer list\n            corresponds to each image. The inner list\n            corresponds to each class. When the model has mask branch,\n            it contains bbox results and mask results.\n            The outer list corresponds to each image, and first element\n            of tuple is bbox results, second element is mask results.\n        \"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        # torch.save({'det_bboxes':det_bboxes, 'det_labels':det_labels}, 'det.pkl')\n        bbox_results = [\n            bbox2result(det_bboxes[i], det_labels[i],\n                        self.bbox_head.num_classes)\n            for i in range(len(det_bboxes))\n        ]\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = self.simple_test_mask(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return list(zip(bbox_results, segm_results))\n\n    def aug_test(self, x, proposal_list, img_metas, rescale=False):\n        \"\"\"Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        \"\"\"\n        det_bboxes, det_labels = self.aug_test_bboxes(x, img_metas,\n                                                      proposal_list,\n                                                      self.test_cfg)\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= det_bboxes.new_tensor(\n                img_metas[0][0]['scale_factor'])\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        # det_bboxes always keep the original scale\n        if self.with_mask:\n            segm_results = self.aug_test_mask(x, img_metas, det_bboxes,\n                                              det_labels)\n            return [(bbox_results, segm_results)]\n        else:\n            return [bbox_results]\n\n    def onnx_export(self, x, proposals, img_metas, rescale=False):\n        \"\"\"Test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        det_bboxes, det_labels = self.bbox_onnx_export(\n            x, img_metas, proposals, self.test_cfg, rescale=rescale)\n\n        if not self.with_mask:\n            return det_bboxes, det_labels\n        else:\n            segm_results = self.mask_onnx_export(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return det_bboxes, det_labels, segm_results\n\n    def mask_onnx_export(self, x, img_metas, det_bboxes, det_labels, **kwargs):\n        \"\"\"Export mask branch to onnx which supports batch inference.\n\n        Args:\n            x (tuple[Tensor]): Feature maps of all scale level.\n            img_metas (list[dict]): Image meta info.\n            det_bboxes (Tensor): Bboxes and corresponding scores.\n                has shape [N, num_bboxes, 5].\n            det_labels (Tensor): class labels of\n                shape [N, num_bboxes].\n\n        Returns:\n            Tensor: The segmentation results of shape [N, num_bboxes,\n                image_height, image_width].\n        \"\"\"\n        # image shapes of images in the batch\n\n        if all(det_bbox.shape[0] == 0 for det_bbox in det_bboxes):\n            raise RuntimeError('[ONNX Error] Can not record MaskHead '\n                               'as it has not been executed this time')\n        batch_size = det_bboxes.size(0)\n        # if det_bboxes is rescaled to the original image size, we need to\n        # rescale it back to the testing scale to obtain RoIs.\n        det_bboxes = det_bboxes[..., :4]\n        batch_index = torch.arange(\n            det_bboxes.size(0), device=det_bboxes.device).float().view(\n                -1, 1, 1).expand(det_bboxes.size(0), det_bboxes.size(1), 1)\n        mask_rois = torch.cat([batch_index, det_bboxes], dim=-1)\n        mask_rois = mask_rois.view(-1, 5)\n        mask_results = self._mask_forward(x, mask_rois)\n        mask_pred = mask_results['mask_pred']\n        max_shape = img_metas[0]['img_shape_for_onnx']\n        num_det = int(det_bboxes.shape[1])\n        det_bboxes = det_bboxes.reshape(-1, 4)\n        det_labels = det_labels.reshape(-1)\n        segm_results = self.mask_head.onnx_export(mask_pred, det_bboxes,\n                                                  det_labels, self.test_cfg,\n                                                  max_shape)\n        segm_results = segm_results.reshape(batch_size, num_det, max_shape[0],\n                                            max_shape[1])\n        return segm_results\n\n    def bbox_onnx_export(self, x, img_metas, proposals, rcnn_test_cfg,\n                         **kwargs):\n        \"\"\"Export bbox branch to onnx which supports batch inference.\n\n        Args:\n            x (tuple[Tensor]): Feature maps of all scale level.\n            img_metas (list[dict]): Image meta info.\n            proposals (Tensor): Region proposals with\n                batch dimension, has shape [N, num_bboxes, 5].\n            rcnn_test_cfg (obj:`ConfigDict`): `test_cfg` of R-CNN.\n\n        Returns:\n            tuple[Tensor, Tensor]: bboxes of shape [N, num_bboxes, 5]\n                and class labels of shape [N, num_bboxes].\n        \"\"\"\n        # get origin input shape to support onnx dynamic input shape\n        assert len(\n            img_metas\n        ) == 1, 'Only support one input image while in exporting to ONNX'\n        img_shapes = img_metas[0]['img_shape_for_onnx']\n\n        rois = proposals\n\n        batch_index = torch.arange(\n            rois.size(0), device=rois.device).float().view(-1, 1, 1).expand(\n                rois.size(0), rois.size(1), 1)\n\n        rois = torch.cat([batch_index, rois[..., :4]], dim=-1)\n        batch_size = rois.shape[0]\n        num_proposals_per_img = int(rois.shape[1])\n\n        # Eliminate the batch dimension\n        rois = rois.view(-1, 5)\n        bbox_results = self._bbox_forward(x, rois)\n        cls_score = bbox_results['cls_score']\n        bbox_pred = bbox_results['bbox_pred']\n\n        # Recover the batch dimension\n        rois = rois.reshape(-1, num_proposals_per_img, 5)\n        cls_score = cls_score.reshape(-1, num_proposals_per_img,\n                                      int(cls_score.size(-1)))\n\n        bbox_pred = bbox_pred.reshape(-1, num_proposals_per_img,\n                                      int(bbox_pred.size(-1)))\n        det_bboxes, det_labels = self.bbox_head.onnx_export(\n            rois, cls_score, bbox_pred, img_shapes, cfg=rcnn_test_cfg)\n\n        return det_bboxes, det_labels", ""]}
{"filename": "patch/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nfrom warnings import warn\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, build_conv_layer, build_upsample_layer\nfrom mmcv.ops.carafe import CARAFEPack\nfrom mmcv.runner import BaseModule, ModuleList, auto_fp16, force_fp32", "from mmcv.ops.carafe import CARAFEPack\nfrom mmcv.runner import BaseModule, ModuleList, auto_fp16, force_fp32\nfrom torch.nn.modules.utils import _pair\n\nfrom mmdet.core import mask_target\nfrom mmdet.models.builder import HEADS, build_loss\n\nBYTES_PER_FLOAT = 4\n# TODO: This memory limit may be too much or too little. It would be better to\n# determine it based on available resources.", "# TODO: This memory limit may be too much or too little. It would be better to\n# determine it based on available resources.\nGPU_MEM_LIMIT = 1024**3  # 1 GB memory limit\n\n\n@HEADS.register_module()\nclass FCNMaskHead(BaseModule):\n\n    def __init__(self,\n                 num_convs=4,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 conv_out_channels=256,\n                 num_classes=80,\n                 class_agnostic=False,\n                 upsample_cfg=dict(type='deconv', scale_factor=2),\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 predictor_cfg=dict(type='Conv'),\n                 loss_mask=dict(\n                     type='CrossEntropyLoss', use_mask=True, loss_weight=1.0),\n                 init_cfg=None):\n        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n                                 'behavior, init_cfg is not allowed to be set'\n        super(FCNMaskHead, self).__init__(init_cfg)\n        self.upsample_cfg = upsample_cfg.copy()\n        if self.upsample_cfg['type'] not in [\n                None, 'deconv', 'nearest', 'bilinear', 'carafe'\n        ]:\n            raise ValueError(\n                f'Invalid upsample method {self.upsample_cfg[\"type\"]}, '\n                'accepted methods are \"deconv\", \"nearest\", \"bilinear\", '\n                '\"carafe\"')\n        self.num_convs = num_convs\n        # WARN: roi_feat_size is reserved and not used\n        self.roi_feat_size = _pair(roi_feat_size)\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_out_channels = conv_out_channels\n        self.upsample_method = self.upsample_cfg.get('type')\n        self.scale_factor = self.upsample_cfg.pop('scale_factor', None)\n        self.num_classes = num_classes\n        self.class_agnostic = class_agnostic\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.predictor_cfg = predictor_cfg\n        self.fp16_enabled = False\n        self.loss_mask = build_loss(loss_mask)\n\n        self.convs = ModuleList()\n        for i in range(self.num_convs):\n            in_channels = (\n                self.in_channels if i == 0 else self.conv_out_channels)\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    padding=padding,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg))\n        upsample_in_channels = (\n            self.conv_out_channels if self.num_convs > 0 else in_channels)\n        upsample_cfg_ = self.upsample_cfg.copy()\n        if self.upsample_method is None:\n            self.upsample = None\n        elif self.upsample_method == 'deconv':\n            upsample_cfg_.update(\n                in_channels=upsample_in_channels,\n                out_channels=self.conv_out_channels,\n                kernel_size=self.scale_factor,\n                stride=self.scale_factor)\n            self.upsample = build_upsample_layer(upsample_cfg_)\n        elif self.upsample_method == 'carafe':\n            upsample_cfg_.update(\n                channels=upsample_in_channels, scale_factor=self.scale_factor)\n            self.upsample = build_upsample_layer(upsample_cfg_)\n        else:\n            # suppress warnings\n            align_corners = (None\n                             if self.upsample_method == 'nearest' else False)\n            upsample_cfg_.update(\n                scale_factor=self.scale_factor,\n                mode=self.upsample_method,\n                align_corners=align_corners)\n            self.upsample = build_upsample_layer(upsample_cfg_)\n\n        out_channels = 1 if self.class_agnostic else self.num_classes\n        logits_in_channel = (\n            self.conv_out_channels\n            if self.upsample_method == 'deconv' else upsample_in_channels)\n        self.conv_logits = build_conv_layer(self.predictor_cfg,\n                                            logits_in_channel, out_channels, 1)\n        self.relu = nn.ReLU(inplace=True)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        super(FCNMaskHead, self).init_weights()\n        for m in [self.upsample, self.conv_logits]:\n            if m is None:\n                continue\n            elif isinstance(m, CARAFEPack):\n                m.init_weights()\n            elif hasattr(m, 'weight') and hasattr(m, 'bias'):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.constant_(m.bias, 0)\n\n    @auto_fp16()\n    def forward(self, x):\n        for conv in self.convs:\n            x = conv(x)\n        if self.upsample is not None:\n            x = self.upsample(x)\n            if self.upsample_method == 'deconv':\n                x = self.relu(x)\n        mask_pred = self.conv_logits(x)\n        return mask_pred\n\n    def get_targets(self, sampling_results, gt_masks, rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n        mask_targets = mask_target(pos_proposals, pos_assigned_gt_inds,\n                                   gt_masks, rcnn_train_cfg)\n        return mask_targets\n\n    @force_fp32(apply_to=('mask_pred', ))\n    def loss(self, mask_pred, mask_targets, labels):\n        \"\"\"\n        Example:\n            >>> from mmdet.models.roi_heads.mask_heads.fcn_mask_head import *  # NOQA\n            >>> N = 7  # N = number of extracted ROIs\n            >>> C, H, W = 11, 32, 32\n            >>> # Create example instance of FCN Mask Head.\n            >>> # There are lots of variations depending on the configuration\n            >>> self = FCNMaskHead(num_classes=C, num_convs=1)\n            >>> inputs = torch.rand(N, self.in_channels, H, W)\n            >>> mask_pred = self.forward(inputs)\n            >>> sf = self.scale_factor\n            >>> labels = torch.randint(0, C, size=(N,))\n            >>> # With the default properties the mask targets should indicate\n            >>> # a (potentially soft) single-class label\n            >>> mask_targets = torch.rand(N, H * sf, W * sf)\n            >>> loss = self.loss(mask_pred, mask_targets, labels)\n            >>> print('loss = {!r}'.format(loss))\n        \"\"\"\n        loss = dict()\n        if mask_pred.size(0) == 0:\n            loss_mask = mask_pred.sum()\n        else:\n            if self.class_agnostic:\n                loss_mask = self.loss_mask(mask_pred, mask_targets,\n                                           torch.zeros_like(labels))\n            else:\n                loss_mask = self.loss_mask(mask_pred, mask_targets, labels)\n        loss['loss_mask'] = loss_mask\n        return loss\n\n    def get_seg_masks(self, mask_pred, det_bboxes, det_labels, rcnn_test_cfg,\n                      ori_shape, scale_factor, rescale):\n        \"\"\"Get segmentation masks from mask_pred and bboxes.\n\n        Args:\n            mask_pred (Tensor or ndarray): shape (n, #class, h, w).\n                For single-scale testing, mask_pred is the direct output of\n                model, whose type is Tensor, while for multi-scale testing,\n                it will be converted to numpy array outside of this method.\n            det_bboxes (Tensor): shape (n, 4/5)\n            det_labels (Tensor): shape (n, )\n            rcnn_test_cfg (dict): rcnn testing config\n            ori_shape (Tuple): original image height and width, shape (2,)\n            scale_factor(ndarray | Tensor): If ``rescale is True``, box\n                coordinates are divided by this scale factor to fit\n                ``ori_shape``.\n            rescale (bool): If True, the resulting masks will be rescaled to\n                ``ori_shape``.\n\n        Returns:\n            list[list]: encoded masks. The c-th item in the outer list\n                corresponds to the c-th class. Given the c-th outer list, the\n                i-th item in that inner list is the mask for the i-th box with\n                class label c.\n\n        Example:\n            >>> import mmcv\n            >>> from mmdet.models.roi_heads.mask_heads.fcn_mask_head import *  # NOQA\n            >>> N = 7  # N = number of extracted ROIs\n            >>> C, H, W = 11, 32, 32\n            >>> # Create example instance of FCN Mask Head.\n            >>> self = FCNMaskHead(num_classes=C, num_convs=0)\n            >>> inputs = torch.rand(N, self.in_channels, H, W)\n            >>> mask_pred = self.forward(inputs)\n            >>> # Each input is associated with some bounding box\n            >>> det_bboxes = torch.Tensor([[1, 1, 42, 42 ]] * N)\n            >>> det_labels = torch.randint(0, C, size=(N,))\n            >>> rcnn_test_cfg = mmcv.Config({'mask_thr_binary': 0, })\n            >>> ori_shape = (H * 4, W * 4)\n            >>> scale_factor = torch.FloatTensor((1, 1))\n            >>> rescale = False\n            >>> # Encoded masks are a list for each category.\n            >>> encoded_masks = self.get_seg_masks(\n            >>>     mask_pred, det_bboxes, det_labels, rcnn_test_cfg, ori_shape,\n            >>>     scale_factor, rescale\n            >>> )\n            >>> assert len(encoded_masks) == C\n            >>> assert sum(list(map(len, encoded_masks))) == N\n        \"\"\"\n        if isinstance(mask_pred, torch.Tensor):\n            mask_pred = mask_pred.sigmoid()\n        else:\n            # In AugTest, has been activated before\n            mask_pred = det_bboxes.new_tensor(mask_pred)\n\n        device = mask_pred.device\n        cls_segms = [[] for _ in range(self.num_classes)\n                     ]  # BG is not included in num_classes\n        bboxes = det_bboxes[:, :4]\n        labels = det_labels\n\n        # In most cases, scale_factor should have been\n        # converted to Tensor when rescale the bbox\n        if not isinstance(scale_factor, torch.Tensor):\n            if isinstance(scale_factor, float):\n                scale_factor = np.array([scale_factor] * 4)\n                warn('Scale_factor should be a Tensor or ndarray '\n                     'with shape (4,), float would be deprecated. ')\n            assert isinstance(scale_factor, np.ndarray)\n            scale_factor = torch.Tensor(scale_factor)\n\n        if rescale:\n            img_h, img_w = ori_shape[:2]\n            bboxes = bboxes / scale_factor.to(bboxes)\n        else:\n            w_scale, h_scale = scale_factor[0], scale_factor[1]\n            img_h = np.round(ori_shape[0] * h_scale.item()).astype(np.int32)\n            img_w = np.round(ori_shape[1] * w_scale.item()).astype(np.int32)\n\n        N = len(mask_pred)\n        # The actual implementation split the input into chunks,\n        # and paste them chunk by chunk.\n        if device.type == 'cpu':\n            # CPU is most efficient when they are pasted one by one with\n            # skip_empty=True, so that it performs minimal number of\n            # operations.\n            num_chunks = N\n        else:\n            # GPU benefits from parallelism for larger chunks,\n            # but may have memory issue\n            # the types of img_w and img_h are np.int32,\n            # when the image resolution is large,\n            # the calculation of num_chunks will overflow.\n            # so we need to change the types of img_w and img_h to int.\n            # See https://github.com/open-mmlab/mmdetection/pull/5191\n            num_chunks = int(\n                np.ceil(N * int(img_h) * int(img_w) * BYTES_PER_FLOAT /\n                        GPU_MEM_LIMIT))\n            assert (num_chunks <=\n                    N), 'Default GPU_MEM_LIMIT is too small; try increasing it'\n        chunks = torch.chunk(torch.arange(N, device=device), num_chunks)\n\n        threshold = rcnn_test_cfg.mask_thr_binary\n        im_mask = torch.zeros(\n            N,\n            img_h,\n            img_w,\n            device=device,\n            dtype=torch.bool if threshold >= 0 else torch.uint8)\n\n        if not self.class_agnostic:\n            mask_pred = mask_pred[range(N), labels][:, None]\n\n        for inds in chunks:\n            masks_chunk, spatial_inds = _do_paste_mask(\n                mask_pred[inds],\n                bboxes[inds],\n                img_h,\n                img_w,\n                skip_empty=device.type == 'cpu')\n\n            if threshold >= 0:\n                masks_chunk = (masks_chunk >= threshold).to(dtype=torch.bool)\n            else:\n                # for visualization and debugging\n                masks_chunk = (masks_chunk * 255).to(dtype=torch.uint8)\n\n            im_mask[(inds, ) + spatial_inds] = masks_chunk\n\n        for i in range(N):\n            cls_segms[labels[i]].append(im_mask[i].detach().cpu().numpy())\n        return cls_segms\n\n    def onnx_export(self, mask_pred, det_bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, **kwargs):\n        \"\"\"Get segmentation masks from mask_pred and bboxes.\n\n        Args:\n            mask_pred (Tensor): shape (n, #class, h, w).\n            det_bboxes (Tensor): shape (n, 4/5)\n            det_labels (Tensor): shape (n, )\n            rcnn_test_cfg (dict): rcnn testing config\n            ori_shape (Tuple): original image height and width, shape (2,)\n\n        Returns:\n            Tensor: a mask of shape (N, img_h, img_w).\n        \"\"\"\n\n        mask_pred = mask_pred.sigmoid()\n        bboxes = det_bboxes[:, :4]\n        labels = det_labels\n        # No need to consider rescale and scale_factor while exporting to ONNX\n        img_h, img_w = ori_shape[:2]\n        threshold = rcnn_test_cfg.mask_thr_binary\n        if not self.class_agnostic:\n            box_inds = torch.arange(mask_pred.shape[0])\n            mask_pred = mask_pred[box_inds, labels][:, None]\n        masks, _ = _do_paste_mask(\n            mask_pred, bboxes, img_h, img_w, skip_empty=False)\n        if threshold >= 0:\n            # should convert to float to avoid problems in TRT\n            masks = (masks >= threshold).to(dtype=torch.float)\n        return masks", "\n\ndef _do_paste_mask(masks, boxes, img_h, img_w, skip_empty=True):\n    \"\"\"Paste instance masks according to boxes.\n\n    This implementation is modified from\n    https://github.com/facebookresearch/detectron2/\n\n    Args:\n        masks (Tensor): N, 1, H, W\n        boxes (Tensor): N, 4\n        img_h (int): Height of the image to be pasted.\n        img_w (int): Width of the image to be pasted.\n        skip_empty (bool): Only paste masks within the region that\n            tightly bound all boxes, and returns the results this region only.\n            An important optimization for CPU.\n\n    Returns:\n        tuple: (Tensor, tuple). The first item is mask tensor, the second one\n            is the slice object.\n        If skip_empty == False, the whole image will be pasted. It will\n            return a mask of shape (N, img_h, img_w) and an empty tuple.\n        If skip_empty == True, only area around the mask will be pasted.\n            A mask of shape (N, h', w') and its start and end coordinates\n            in the original image will be returned.\n    \"\"\"\n    # On GPU, paste all masks together (up to chunk size)\n    # by using the entire image to sample the masks\n    # Compared to pasting them one by one,\n    # this has more operations but is faster on COCO-scale dataset.\n    device = masks.device\n    if skip_empty:\n        x0_int, y0_int = torch.clamp(\n            boxes.min(dim=0).values.floor()[:2] - 1,\n            min=0).to(dtype=torch.int32)\n        x1_int = torch.clamp(\n            boxes[:, 2].max().ceil() + 1, max=img_w).to(dtype=torch.int32)\n        y1_int = torch.clamp(\n            boxes[:, 3].max().ceil() + 1, max=img_h).to(dtype=torch.int32)\n    else:\n        x0_int, y0_int = 0, 0\n        x1_int, y1_int = img_w, img_h\n    x0, y0, x1, y1 = torch.split(boxes, 1, dim=1)  # each is Nx1\n\n    N = masks.shape[0]\n\n    img_y = torch.arange(y0_int, y1_int, device=device).to(torch.float32) + 0.5\n    img_x = torch.arange(x0_int, x1_int, device=device).to(torch.float32) + 0.5\n    # \u5f52\u4e00\u5316\n    img_y = (img_y - y0) / (y1 - y0) * 2 - 1\n    img_x = (img_x - x0) / (x1 - x0) * 2 - 1\n    # img_x, img_y have shapes (N, w), (N, h)\n    # IsInf op is not supported with ONNX<=1.7.0\n    if not torch.onnx.is_in_onnx_export():\n        if torch.isinf(img_x).any():\n            inds = torch.where(torch.isinf(img_x))\n            img_x[inds] = 0\n        if torch.isinf(img_y).any():\n            inds = torch.where(torch.isinf(img_y))\n            img_y[inds] = 0\n\n    gx = img_x[:, None, :].expand(N, img_y.size(1), img_x.size(1))\n    gy = img_y[:, :, None].expand(N, img_y.size(1), img_x.size(1))\n    grid = torch.stack([gx, gy], dim=3)\n\n    img_masks = F.grid_sample(\n        masks.to(dtype=torch.float32), grid, align_corners=False)\n\n    if skip_empty:\n        return img_masks[:, 0], (slice(y0_int, y1_int), slice(x0_int, x1_int))\n    else:\n        return img_masks[:, 0], ()", ""]}
{"filename": "patch/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule\n\nfrom mmdet.models.builder import HEADS\nfrom mmdet.models.utils import build_linear_layer\nfrom .bbox_head import BBoxHead\n\n\n@HEADS.register_module()\nclass ConvFCBBoxHead(BBoxHead):\n    r\"\"\"More general bbox head, with shared conv and fc layers and two optional\n    separated branches.\n\n    .. code-block:: none\n\n                                    /-> cls convs -> cls fcs -> cls\n        shared convs -> shared fcs\n                                    \\-> reg convs -> reg fcs -> reg\n    \"\"\"  # noqa: W605\n\n    def __init__(self,\n                 num_shared_convs=0,\n                 num_shared_fcs=0,\n                 num_cls_convs=0,\n                 num_cls_fcs=0,\n                 num_reg_convs=0,\n                 num_reg_fcs=0,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 init_cfg=None,\n                 *args,\n                 **kwargs):\n        super(ConvFCBBoxHead, self).__init__(\n            *args, init_cfg=init_cfg, **kwargs)\n        assert (num_shared_convs + num_shared_fcs + num_cls_convs +\n                num_cls_fcs + num_reg_convs + num_reg_fcs > 0)\n        if num_cls_convs > 0 or num_reg_convs > 0:\n            assert num_shared_fcs == 0\n        if not self.with_cls:\n            assert num_cls_convs == 0 and num_cls_fcs == 0\n        if not self.with_reg:\n            assert num_reg_convs == 0 and num_reg_fcs == 0\n        self.num_shared_convs = num_shared_convs\n        self.num_shared_fcs = num_shared_fcs\n        self.num_cls_convs = num_cls_convs\n        self.num_cls_fcs = num_cls_fcs\n        self.num_reg_convs = num_reg_convs\n        self.num_reg_fcs = num_reg_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        # add shared convs and fcs\n        self.shared_convs, self.shared_fcs, last_layer_dim = \\\n            self._add_conv_fc_branch(\n                self.num_shared_convs, self.num_shared_fcs, self.in_channels,\n                True)\n        self.shared_out_channels = last_layer_dim\n\n        # add cls specific branch\n        self.cls_convs, self.cls_fcs, self.cls_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)\n\n        # add reg specific branch\n        self.reg_convs, self.reg_fcs, self.reg_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)\n\n        if self.num_shared_fcs == 0 and not self.with_avg_pool:\n            if self.num_cls_fcs == 0:\n                self.cls_last_dim *= self.roi_feat_area\n            if self.num_reg_fcs == 0:\n                self.reg_last_dim *= self.roi_feat_area\n\n        self.relu = nn.ReLU(inplace=True)\n        # reconstruct fc_cls and fc_reg since input channels are changed\n        if self.with_cls:\n            if self.custom_cls_channels:\n                cls_channels = self.loss_cls.get_cls_channels(self.num_classes)\n            else:\n                cls_channels = self.num_classes + 1\n            self.fc_cls = build_linear_layer(\n                self.cls_predictor_cfg,\n                in_features=self.cls_last_dim,\n                out_features=cls_channels)\n        if self.with_reg:\n            out_dim_reg = (4 if self.reg_class_agnostic else 4 *\n                           self.num_classes)\n            self.fc_reg = build_linear_layer(\n                self.reg_predictor_cfg,\n                in_features=self.reg_last_dim,\n                out_features=out_dim_reg)\n\n        if init_cfg is None:\n            # when init_cfg is None,\n            # It has been set to\n            # [[dict(type='Normal', std=0.01, override=dict(name='fc_cls'))],\n            #  [dict(type='Normal', std=0.001, override=dict(name='fc_reg'))]\n            # after `super(ConvFCBBoxHead, self).__init__()`\n            # we only need to append additional configuration\n            # for `shared_fcs`, `cls_fcs` and `reg_fcs`\n            self.init_cfg += [\n                dict(\n                    type='Xavier',\n                    distribution='uniform',\n                    override=[\n                        dict(name='shared_fcs'),\n                        dict(name='cls_fcs'),\n                        dict(name='reg_fcs')\n                    ])\n            ]\n\n    def _add_conv_fc_branch(self,\n                            num_branch_convs,\n                            num_branch_fcs,\n                            in_channels,\n                            is_shared=False):\n        \"\"\"Add shared or separable branch.\n\n        convs -> avg pool (optional) -> fcs\n        \"\"\"\n        last_layer_dim = in_channels\n        # add branch specific conv layers\n        branch_convs = nn.ModuleList()\n        if num_branch_convs > 0:\n            for i in range(num_branch_convs):\n                conv_in_channels = (\n                    last_layer_dim if i == 0 else self.conv_out_channels)\n                branch_convs.append(\n                    ConvModule(\n                        conv_in_channels,\n                        self.conv_out_channels,\n                        3,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n            last_layer_dim = self.conv_out_channels\n        # add branch specific fc layers\n        branch_fcs = nn.ModuleList()\n        if num_branch_fcs > 0:\n            # for shared branch, only consider self.with_avg_pool\n            # for separated branches, also consider self.num_shared_fcs\n            if (is_shared\n                    or self.num_shared_fcs == 0) and not self.with_avg_pool:\n                last_layer_dim *= self.roi_feat_area\n            for i in range(num_branch_fcs):\n                fc_in_channels = (\n                    last_layer_dim if i == 0 else self.fc_out_channels)\n                branch_fcs.append(\n                    nn.Linear(fc_in_channels, self.fc_out_channels))\n            last_layer_dim = self.fc_out_channels\n        return branch_convs, branch_fcs, last_layer_dim\n\n    def forward(self, x):\n        # shared part\n        if self.num_shared_convs > 0:\n            for conv in self.shared_convs:\n                x = conv(x)\n\n        if self.num_shared_fcs > 0:\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n\n            x = x.flatten(1)\n\n            for fc in self.shared_fcs:\n                x = self.relu(fc(x))\n        # separate branches\n        x_cls = x\n        x_reg = x\n\n        for conv in self.cls_convs:\n            x_cls = conv(x_cls)\n        if x_cls.dim() > 2:\n            if self.with_avg_pool:\n                x_cls = self.avg_pool(x_cls)\n            x_cls = x_cls.flatten(1)\n        for fc in self.cls_fcs:\n            x_cls = self.relu(fc(x_cls))\n\n        for conv in self.reg_convs:\n            x_reg = conv(x_reg)\n        if x_reg.dim() > 2:\n            if self.with_avg_pool:\n                x_reg = self.avg_pool(x_reg)\n            x_reg = x_reg.flatten(1)\n        for fc in self.reg_fcs:\n            x_reg = self.relu(fc(x_reg))\n\n        cls_score = self.fc_cls(x_cls) if self.with_cls else None\n        bbox_pred = self.fc_reg(x_reg) if self.with_reg else None\n        \n        return cls_score, bbox_pred", "\n@HEADS.register_module()\nclass ConvFCBBoxHead(BBoxHead):\n    r\"\"\"More general bbox head, with shared conv and fc layers and two optional\n    separated branches.\n\n    .. code-block:: none\n\n                                    /-> cls convs -> cls fcs -> cls\n        shared convs -> shared fcs\n                                    \\-> reg convs -> reg fcs -> reg\n    \"\"\"  # noqa: W605\n\n    def __init__(self,\n                 num_shared_convs=0,\n                 num_shared_fcs=0,\n                 num_cls_convs=0,\n                 num_cls_fcs=0,\n                 num_reg_convs=0,\n                 num_reg_fcs=0,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 init_cfg=None,\n                 *args,\n                 **kwargs):\n        super(ConvFCBBoxHead, self).__init__(\n            *args, init_cfg=init_cfg, **kwargs)\n        assert (num_shared_convs + num_shared_fcs + num_cls_convs +\n                num_cls_fcs + num_reg_convs + num_reg_fcs > 0)\n        if num_cls_convs > 0 or num_reg_convs > 0:\n            assert num_shared_fcs == 0\n        if not self.with_cls:\n            assert num_cls_convs == 0 and num_cls_fcs == 0\n        if not self.with_reg:\n            assert num_reg_convs == 0 and num_reg_fcs == 0\n        self.num_shared_convs = num_shared_convs\n        self.num_shared_fcs = num_shared_fcs\n        self.num_cls_convs = num_cls_convs\n        self.num_cls_fcs = num_cls_fcs\n        self.num_reg_convs = num_reg_convs\n        self.num_reg_fcs = num_reg_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        # add shared convs and fcs\n        self.shared_convs, self.shared_fcs, last_layer_dim = \\\n            self._add_conv_fc_branch(\n                self.num_shared_convs, self.num_shared_fcs, self.in_channels,\n                True)\n        self.shared_out_channels = last_layer_dim\n\n        # add cls specific branch\n        self.cls_convs, self.cls_fcs, self.cls_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)\n\n        # add reg specific branch\n        self.reg_convs, self.reg_fcs, self.reg_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)\n\n        if self.num_shared_fcs == 0 and not self.with_avg_pool:\n            if self.num_cls_fcs == 0:\n                self.cls_last_dim *= self.roi_feat_area\n            if self.num_reg_fcs == 0:\n                self.reg_last_dim *= self.roi_feat_area\n\n        self.relu = nn.ReLU(inplace=True)\n        # reconstruct fc_cls and fc_reg since input channels are changed\n        if self.with_cls:\n            if self.custom_cls_channels:\n                cls_channels = self.loss_cls.get_cls_channels(self.num_classes)\n            else:\n                cls_channels = self.num_classes + 1\n            self.fc_cls = build_linear_layer(\n                self.cls_predictor_cfg,\n                in_features=self.cls_last_dim,\n                out_features=cls_channels)\n        if self.with_reg:\n            out_dim_reg = (4 if self.reg_class_agnostic else 4 *\n                           self.num_classes)\n            self.fc_reg = build_linear_layer(\n                self.reg_predictor_cfg,\n                in_features=self.reg_last_dim,\n                out_features=out_dim_reg)\n\n        if init_cfg is None:\n            # when init_cfg is None,\n            # It has been set to\n            # [[dict(type='Normal', std=0.01, override=dict(name='fc_cls'))],\n            #  [dict(type='Normal', std=0.001, override=dict(name='fc_reg'))]\n            # after `super(ConvFCBBoxHead, self).__init__()`\n            # we only need to append additional configuration\n            # for `shared_fcs`, `cls_fcs` and `reg_fcs`\n            self.init_cfg += [\n                dict(\n                    type='Xavier',\n                    distribution='uniform',\n                    override=[\n                        dict(name='shared_fcs'),\n                        dict(name='cls_fcs'),\n                        dict(name='reg_fcs')\n                    ])\n            ]\n\n    def _add_conv_fc_branch(self,\n                            num_branch_convs,\n                            num_branch_fcs,\n                            in_channels,\n                            is_shared=False):\n        \"\"\"Add shared or separable branch.\n\n        convs -> avg pool (optional) -> fcs\n        \"\"\"\n        last_layer_dim = in_channels\n        # add branch specific conv layers\n        branch_convs = nn.ModuleList()\n        if num_branch_convs > 0:\n            for i in range(num_branch_convs):\n                conv_in_channels = (\n                    last_layer_dim if i == 0 else self.conv_out_channels)\n                branch_convs.append(\n                    ConvModule(\n                        conv_in_channels,\n                        self.conv_out_channels,\n                        3,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n            last_layer_dim = self.conv_out_channels\n        # add branch specific fc layers\n        branch_fcs = nn.ModuleList()\n        if num_branch_fcs > 0:\n            # for shared branch, only consider self.with_avg_pool\n            # for separated branches, also consider self.num_shared_fcs\n            if (is_shared\n                    or self.num_shared_fcs == 0) and not self.with_avg_pool:\n                last_layer_dim *= self.roi_feat_area\n            for i in range(num_branch_fcs):\n                fc_in_channels = (\n                    last_layer_dim if i == 0 else self.fc_out_channels)\n                branch_fcs.append(\n                    nn.Linear(fc_in_channels, self.fc_out_channels))\n            last_layer_dim = self.fc_out_channels\n        return branch_convs, branch_fcs, last_layer_dim\n\n    def forward(self, x):\n        # shared part\n        if self.num_shared_convs > 0:\n            for conv in self.shared_convs:\n                x = conv(x)\n\n        if self.num_shared_fcs > 0:\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n\n            x = x.flatten(1)\n\n            for fc in self.shared_fcs:\n                x = self.relu(fc(x))\n        # separate branches\n        x_cls = x\n        x_reg = x\n\n        for conv in self.cls_convs:\n            x_cls = conv(x_cls)\n        if x_cls.dim() > 2:\n            if self.with_avg_pool:\n                x_cls = self.avg_pool(x_cls)\n            x_cls = x_cls.flatten(1)\n        for fc in self.cls_fcs:\n            x_cls = self.relu(fc(x_cls))\n\n        for conv in self.reg_convs:\n            x_reg = conv(x_reg)\n        if x_reg.dim() > 2:\n            if self.with_avg_pool:\n                x_reg = self.avg_pool(x_reg)\n            x_reg = x_reg.flatten(1)\n        for fc in self.reg_fcs:\n            x_reg = self.relu(fc(x_reg))\n\n        cls_score = self.fc_cls(x_cls) if self.with_cls else None\n        bbox_pred = self.fc_reg(x_reg) if self.with_reg else None\n        \n        return cls_score, bbox_pred", "\n\n@HEADS.register_module()\nclass Shared2FCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, fc_out_channels=1024, *args, **kwargs):\n        super(Shared2FCBBoxHead, self).__init__(\n            num_shared_convs=0,\n            num_shared_fcs=2,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)", "\n\n@HEADS.register_module()\nclass Shared4Conv1FCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, fc_out_channels=1024, *args, **kwargs):\n        super(Shared4Conv1FCBBoxHead, self).__init__(\n            num_shared_convs=4,\n            num_shared_fcs=1,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)", ""]}
{"filename": "patch/mmdet/models/detectors/two_stage.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\n\nimport torch\n\nfrom ..builder import DETECTORS, build_backbone, build_head, build_neck\nfrom .base import BaseDetector\n\n\n@DETECTORS.register_module()\nclass TwoStageDetector(BaseDetector):\n    \"\"\"Base class for two-stage detectors.\n\n    Two-stage detectors typically consisting of a region proposal network and a\n    task-specific regression head.\n    \"\"\"\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 roi_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(TwoStageDetector, self).__init__(init_cfg)\n        if pretrained:\n            warnings.warn('DeprecationWarning: pretrained is deprecated, '\n                          'please use \"init_cfg\" instead')\n            backbone.pretrained = pretrained\n        self.backbone = build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = build_neck(neck)\n\n        if rpn_head is not None:\n            rpn_train_cfg = train_cfg.rpn if train_cfg is not None else None\n            rpn_head_ = rpn_head.copy()\n            rpn_head_.update(train_cfg=rpn_train_cfg, test_cfg=test_cfg.rpn)\n            self.rpn_head = build_head(rpn_head_)\n\n        if roi_head is not None:\n            # update train and test cfg here for now\n            # TODO: refactor assigner & sampler\n            rcnn_train_cfg = train_cfg.rcnn if train_cfg is not None else None\n            roi_head.update(train_cfg=rcnn_train_cfg)\n            roi_head.update(test_cfg=test_cfg.rcnn)\n            roi_head.pretrained = pretrained\n            self.roi_head = build_head(roi_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n    @property\n    def with_rpn(self):\n        \"\"\"bool: whether the detector has RPN\"\"\"\n        return hasattr(self, 'rpn_head') and self.rpn_head is not None\n\n    @property\n    def with_roi_head(self):\n        \"\"\"bool: whether the detector has a RoI head\"\"\"\n        return hasattr(self, 'roi_head') and self.roi_head is not None\n\n    def extract_feat(self, img):\n        \"\"\"Directly extract features from the backbone+neck.\"\"\"\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        \"\"\"Used for computing network flops.\n\n        See `mmdetection/tools/analysis_tools/get_flops.py`\n        \"\"\"\n        outs = ()\n        # backbone\n        x = self.extract_feat(img)\n        # rpn\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            outs = outs + (rpn_outs, )\n        proposals = torch.rand(1000, 2).to(img.device)\n        proposals = torch.cat([proposals, proposals + torch.rand(1000, 2).to(img.device)], dim=-1)\n        # roi_head\n        roi_outs = self.roi_head.forward_dummy(x, proposals)\n        outs = outs + (roi_outs, )\n        return outs\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None,\n                      **kwargs):\n        \"\"\"\n        Args:\n            img (Tensor): of shape (N, C, H, W) encoding input images.\n                Typically these should be mean centered and std scaled.\n\n            img_metas (list[dict]): list of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n            gt_labels (list[Tensor]): class indices corresponding to each box\n\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n            proposals : override rpn proposals with custom proposals. Use when\n                `with_rpn` is False.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            proposal_cfg = self.train_cfg.get('rpn_proposal',\n                                              self.test_cfg.rpn)\n            rpn_losses, proposal_list = self.rpn_head.forward_train(\n                x,\n                img_metas,\n                gt_bboxes,\n                gt_labels=None,\n                gt_bboxes_ignore=gt_bboxes_ignore,\n                proposal_cfg=proposal_cfg,\n                **kwargs)\n            losses.update(rpn_losses)\n        else:\n            proposal_list = proposals\n\n        roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,\n                                                 gt_bboxes, gt_labels,\n                                                 gt_bboxes_ignore, gt_masks,\n                                                 **kwargs)\n        losses.update(roi_losses)\n\n        return losses\n\n    async def async_simple_test(self,\n                                img,\n                                img_meta,\n                                proposals=None,\n                                rescale=False):\n        \"\"\"Async test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        x = self.extract_feat(img)\n\n        if proposals is None:\n            proposal_list = await self.rpn_head.async_simple_test_rpn(\n                x, img_meta)\n        else:\n            proposal_list = proposals\n\n        return await self.roi_head.async_simple_test(\n            x, proposal_list, img_meta, rescale=rescale)\n\n    def simple_test(self, img, img_metas, proposals=None, rescale=False):\n        \"\"\"Test without augmentation.\"\"\"\n\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        x = self.extract_feat(img)\n        if proposals is None:\n            proposal_list = self.rpn_head.simple_test_rpn(x, img_metas)\n        else:\n            proposal_list = proposals\n\n        return self.roi_head.simple_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        \"\"\"Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        \"\"\"\n        x = self.extract_feats(imgs)\n        proposal_list = self.rpn_head.aug_test_rpn(x, img_metas)\n        return self.roi_head.aug_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def onnx_export(self, img, img_metas):\n        img_shape = torch._shape_as_tensor(img)[2:]\n        img_metas[0]['img_shape_for_onnx'] = img_shape\n        x = self.extract_feat(img)\n        proposals = self.rpn_head.onnx_export(x, img_metas)\n        if hasattr(self.roi_head, 'onnx_export'):\n            return self.roi_head.onnx_export(x, proposals, img_metas)\n        else:\n            raise NotImplementedError(\n                f'{self.__class__.__name__} can not '\n                f'be exported to ONNX. Please refer to the '\n                f'list of supported models,'\n                f'https://mmdetection.readthedocs.io/en/latest/tutorials/pytorch2onnx.html#list-of-supported-models-exportable-to-onnx'  # noqa E501\n            )", "\n@DETECTORS.register_module()\nclass TwoStageDetector(BaseDetector):\n    \"\"\"Base class for two-stage detectors.\n\n    Two-stage detectors typically consisting of a region proposal network and a\n    task-specific regression head.\n    \"\"\"\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 roi_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(TwoStageDetector, self).__init__(init_cfg)\n        if pretrained:\n            warnings.warn('DeprecationWarning: pretrained is deprecated, '\n                          'please use \"init_cfg\" instead')\n            backbone.pretrained = pretrained\n        self.backbone = build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = build_neck(neck)\n\n        if rpn_head is not None:\n            rpn_train_cfg = train_cfg.rpn if train_cfg is not None else None\n            rpn_head_ = rpn_head.copy()\n            rpn_head_.update(train_cfg=rpn_train_cfg, test_cfg=test_cfg.rpn)\n            self.rpn_head = build_head(rpn_head_)\n\n        if roi_head is not None:\n            # update train and test cfg here for now\n            # TODO: refactor assigner & sampler\n            rcnn_train_cfg = train_cfg.rcnn if train_cfg is not None else None\n            roi_head.update(train_cfg=rcnn_train_cfg)\n            roi_head.update(test_cfg=test_cfg.rcnn)\n            roi_head.pretrained = pretrained\n            self.roi_head = build_head(roi_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n    @property\n    def with_rpn(self):\n        \"\"\"bool: whether the detector has RPN\"\"\"\n        return hasattr(self, 'rpn_head') and self.rpn_head is not None\n\n    @property\n    def with_roi_head(self):\n        \"\"\"bool: whether the detector has a RoI head\"\"\"\n        return hasattr(self, 'roi_head') and self.roi_head is not None\n\n    def extract_feat(self, img):\n        \"\"\"Directly extract features from the backbone+neck.\"\"\"\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        \"\"\"Used for computing network flops.\n\n        See `mmdetection/tools/analysis_tools/get_flops.py`\n        \"\"\"\n        outs = ()\n        # backbone\n        x = self.extract_feat(img)\n        # rpn\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            outs = outs + (rpn_outs, )\n        proposals = torch.rand(1000, 2).to(img.device)\n        proposals = torch.cat([proposals, proposals + torch.rand(1000, 2).to(img.device)], dim=-1)\n        # roi_head\n        roi_outs = self.roi_head.forward_dummy(x, proposals)\n        outs = outs + (roi_outs, )\n        return outs\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None,\n                      **kwargs):\n        \"\"\"\n        Args:\n            img (Tensor): of shape (N, C, H, W) encoding input images.\n                Typically these should be mean centered and std scaled.\n\n            img_metas (list[dict]): list of image info dict where each dict\n                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n\n            gt_labels (list[Tensor]): class indices corresponding to each box\n\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n            proposals : override rpn proposals with custom proposals. Use when\n                `with_rpn` is False.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        \"\"\"\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            proposal_cfg = self.train_cfg.get('rpn_proposal',\n                                              self.test_cfg.rpn)\n            rpn_losses, proposal_list = self.rpn_head.forward_train(\n                x,\n                img_metas,\n                gt_bboxes,\n                gt_labels=None,\n                gt_bboxes_ignore=gt_bboxes_ignore,\n                proposal_cfg=proposal_cfg,\n                **kwargs)\n            losses.update(rpn_losses)\n        else:\n            proposal_list = proposals\n\n        roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,\n                                                 gt_bboxes, gt_labels,\n                                                 gt_bboxes_ignore, gt_masks,\n                                                 **kwargs)\n        losses.update(roi_losses)\n\n        return losses\n\n    async def async_simple_test(self,\n                                img,\n                                img_meta,\n                                proposals=None,\n                                rescale=False):\n        \"\"\"Async test without augmentation.\"\"\"\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        x = self.extract_feat(img)\n\n        if proposals is None:\n            proposal_list = await self.rpn_head.async_simple_test_rpn(\n                x, img_meta)\n        else:\n            proposal_list = proposals\n\n        return await self.roi_head.async_simple_test(\n            x, proposal_list, img_meta, rescale=rescale)\n\n    def simple_test(self, img, img_metas, proposals=None, rescale=False):\n        \"\"\"Test without augmentation.\"\"\"\n\n        assert self.with_bbox, 'Bbox head must be implemented.'\n        x = self.extract_feat(img)\n        if proposals is None:\n            proposal_list = self.rpn_head.simple_test_rpn(x, img_metas)\n        else:\n            proposal_list = proposals\n\n        return self.roi_head.simple_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        \"\"\"Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        \"\"\"\n        x = self.extract_feats(imgs)\n        proposal_list = self.rpn_head.aug_test_rpn(x, img_metas)\n        return self.roi_head.aug_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def onnx_export(self, img, img_metas):\n        img_shape = torch._shape_as_tensor(img)[2:]\n        img_metas[0]['img_shape_for_onnx'] = img_shape\n        x = self.extract_feat(img)\n        proposals = self.rpn_head.onnx_export(x, img_metas)\n        if hasattr(self.roi_head, 'onnx_export'):\n            return self.roi_head.onnx_export(x, proposals, img_metas)\n        else:\n            raise NotImplementedError(\n                f'{self.__class__.__name__} can not '\n                f'be exported to ONNX. Please refer to the '\n                f'list of supported models,'\n                f'https://mmdetection.readthedocs.io/en/latest/tutorials/pytorch2onnx.html#list-of-supported-models-exportable-to-onnx'  # noqa E501\n            )", ""]}
{"filename": "patch/mmdet/core/export/onnx_helper.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport os\nfrom functools import reduce\nimport torch\n\n\ndef dynamic_clip_for_onnx(x1, y1, x2, y2, max_shape):\n    \"\"\"Clip boxes dynamically for onnx.\n\n    Since torch.clamp cannot have dynamic `min` and `max`, we scale the\n      boxes by 1/max_shape and clamp in the range [0, 1].\n\n    Args:\n        x1 (Tensor): The x1 for bounding boxes.\n        y1 (Tensor): The y1 for bounding boxes.\n        x2 (Tensor): The x2 for bounding boxes.\n        y2 (Tensor): The y2 for bounding boxes.\n        max_shape (Tensor or torch.Size): The (H,W) of original image.\n    Returns:\n        tuple(Tensor): The clipped x1, y1, x2, y2.\n    \"\"\"\n    assert isinstance(\n        max_shape,\n        torch.Tensor), '`max_shape` should be tensor of (h,w) for onnx'\n\n    # scale by 1/max_shape\n    x1 = x1 / max_shape[1]\n    y1 = y1 / max_shape[0]\n    x2 = x2 / max_shape[1]\n    y2 = y2 / max_shape[0]\n\n    # clamp [0, 1]\n    x1 = torch.clamp(x1, 0, 1)\n    y1 = torch.clamp(y1, 0, 1)\n    x2 = torch.clamp(x2, 0, 1)\n    y2 = torch.clamp(y2, 0, 1)\n\n    # scale back\n    x1 = x1 * max_shape[1]\n    y1 = y1 * max_shape[0]\n    x2 = x2 * max_shape[1]\n    y2 = y2 * max_shape[0]\n    return x1, y1, x2, y2", "\n\ndef get_k_for_topk(k, size):\n    \"\"\"Get k of TopK for onnx exporting.\n\n    The K of TopK in TensorRT should not be a Tensor, while in ONNX Runtime\n      it could be a Tensor.Due to dynamic shape feature, we have to decide\n      whether to do TopK and what K it should be while exporting to ONNX.\n    If returned K is less than zero, it means we do not have to do\n      TopK operation.\n\n    Args:\n        k (int or Tensor): The set k value for nms from config file.\n        size (Tensor or torch.Size): The number of elements of \\\n            TopK's input tensor\n    Returns:\n        tuple: (int or Tensor): The final K for TopK.\n    \"\"\"\n    ret_k = -1\n    if k <= 0 or size <= 0:\n        return ret_k\n    if torch.onnx.is_in_onnx_export():\n        is_trt_backend = os.environ.get('ONNX_BACKEND') == 'MMCVTensorRT'\n        if is_trt_backend:\n            # TensorRT does not support dynamic K with TopK op\n            if 0 < k < size:\n                ret_k = k\n        else:\n            # Always keep topk op for dynamic input in onnx for ONNX Runtime\n            ret_k = torch.where(k < size, k, size)\n    elif k < size:\n        ret_k = k\n    else:\n        # ret_k is -1\n        pass\n    return ret_k", "\n\ndef add_dummy_nms_for_onnx(boxes,\n                           scores,\n                           max_output_boxes_per_class=1000,\n                           iou_threshold=0.5,\n                           score_threshold=0.05,\n                           pre_top_k=-1,\n                           after_top_k=-1,\n                           labels=None,\n                           idxs=None):\n    \"\"\"Create a dummy onnx::NonMaxSuppression op while exporting to ONNX.\n\n    This function helps exporting to onnx with batch and multiclass NMS op.\n    It only supports class-agnostic detection results. That is, the scores\n    is of shape (N, num_bboxes, num_classes) and the boxes is of shape\n    (N, num_boxes, 4).\n\n    Args:\n        boxes (Tensor): The bounding boxes of shape [N, num_boxes, 4]\n        scores (Tensor): The detection scores of shape\n            [N, num_boxes, num_classes]\n        max_output_boxes_per_class (int): Maximum number of output\n            boxes per class of nms. Defaults to 1000.\n        iou_threshold (float): IOU threshold of nms. Defaults to 0.5\n        score_threshold (float): score threshold of nms.\n            Defaults to 0.05.\n        pre_top_k (bool): Number of top K boxes to keep before nms.\n            Defaults to -1.\n        after_top_k (int): Number of top K boxes to keep after nms.\n            Defaults to -1.\n        labels (Tensor, optional): It not None, explicit labels would be used.\n            Otherwise, labels would be automatically generated using\n            num_classed. Defaults to None.\n\n    Returns:\n        tuple[Tensor, Tensor]: dets of shape [N, num_det, 5]\n            and class labels of shape [N, num_det].\n    \"\"\"\n    max_output_boxes_per_class = torch.LongTensor([max_output_boxes_per_class])\n    iou_threshold = torch.tensor([iou_threshold], dtype=torch.float32)\n    score_threshold = torch.tensor([score_threshold], dtype=torch.float32)\n    batch_size = scores.shape[0]\n    num_class = int(scores.shape[2])\n\n    nms_pre = torch.tensor(pre_top_k, device=scores.device, dtype=torch.long)\n    nms_pre = get_k_for_topk(nms_pre, boxes.shape[1])\n    \n    if nms_pre > 0:\n        max_scores, _ = scores.max(-1)\n        _, topk_inds = max_scores.topk(nms_pre)\n        batch_inds = torch.arange(batch_size).view(-1, 1).expand_as(topk_inds).long()\n        # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501\n        transformed_inds = boxes.shape[1] * batch_inds + topk_inds\n        boxes = boxes.reshape(-1, 4)[transformed_inds, :].reshape(\n            batch_size, -1, 4)\n        scores = scores.reshape(-1, num_class)[transformed_inds, :].reshape(\n            batch_size, -1, num_class)\n        if labels is not None:\n            labels = labels.reshape(-1, 1)[transformed_inds].reshape(\n                batch_size, -1)\n\n    scores = scores.permute(0, 2, 1)\n    num_box = int(boxes.shape[1])\n    # turn off tracing to create a dummy output of nms\n    state = torch._C._get_tracing_state()\n    # dummy indices of nms's output\n    num_fake_det = 2\n    batch_inds = torch.randint(batch_size, (num_fake_det, 1))\n    cls_inds = torch.randint(num_class, (num_fake_det, 1))\n    box_inds = torch.randint(num_box, (num_fake_det, 1))\n    indices = torch.cat([batch_inds, cls_inds, box_inds], dim=1)\n    output = indices\n    setattr(DummyONNXNMSop, 'output', output)\n\n    # open tracing\n    torch._C._set_tracing_state(state)\n    # box shifting\n    boxes_for_nms = boxes\n    if idxs is not None:\n        # [b,1,1]\n        max_coordinate, _ = boxes.max(dim=1, keepdim=True)\n        max_coordinate, _ = max_coordinate.max(dim=2, keepdim=True)\n        # [b,N,1]\n        offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(1).to(boxes))\n        boxes_for_nms = boxes + offsets.view(1, num_box, 1)\n    selected_indices = DummyONNXNMSop.apply(boxes_for_nms, scores,\n                                            max_output_boxes_per_class,\n                                            iou_threshold, score_threshold)\n\n    batch_inds, cls_inds = selected_indices[:, 0], selected_indices[:, 1]\n    box_inds = selected_indices[:, 2]\n    if labels is None:\n        labels = torch.arange(num_class, dtype=torch.long).to(scores.device)\n        labels = labels.view(1, num_class, 1).expand_as(scores)\n    scores = scores.reshape(-1, 1)\n    boxes = boxes.reshape(-1, num_box * 4).repeat(1, num_class).reshape(-1, 4)\n    pos_inds = (num_class * batch_inds + cls_inds) * num_box + box_inds\n    mask = scores.new_zeros(scores.shape)\n    # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501\n    # PyTorch style code: mask[batch_inds, box_inds] += 1\n    mask[pos_inds, :] += 1\n    scores = scores * mask\n    boxes = boxes * mask\n\n    scores_last_shapes = list(map(int, scores.shape))\n    scores_last_dims = reduce(lambda x, y: x * y, scores_last_shapes, 1)\n    scores = scores.reshape(-1, scores_last_dims // int(batch_size))\n\n    boxes_last_shapes = list(map(int, boxes.shape))\n    boxes_last_dims = reduce(lambda x, y: x * y, boxes_last_shapes, 1)\n    boxes = boxes.reshape(-1, boxes_last_dims // 4 // int(batch_size), 4)\n\n    labels_last_shapes = list(map(int, labels.shape))\n    labels_last_dims = reduce(lambda x, y: x * y, labels_last_shapes, 1)\n    labels = labels.reshape(-1, labels_last_dims // int(batch_size))\n\n    nms_after = torch.tensor(after_top_k, device=scores.device, dtype=torch.long)\n    nms_after = get_k_for_topk(nms_after, num_box * num_class)\n\n    if nms_after > 0:\n        _, topk_inds = scores.topk(nms_after)\n        if int(batch_size) == 1:\n            transformed_inds = topk_inds\n        else:\n            batch_inds = torch.arange(batch_size).view(-1, 1).expand_as(topk_inds)\n            # Avoid onnx2tensorrt issue in https://github.com/NVIDIA/TensorRT/issues/1134 # noqa: E501\n            transformed_inds = int(scores.shape[1]) * batch_inds + topk_inds\n        scores = scores.reshape(-1, 1)[transformed_inds, :]\n        scores_last_shapes = list(map(int, scores.shape))\n        scores_last_dims = reduce(lambda x, y: x * y, scores_last_shapes, 1)\n        scores = scores.reshape(-1, scores_last_dims // int(batch_size))\n\n        boxes = boxes.reshape(-1, 4)[transformed_inds, :]\n        boxes_last_shapes = list(map(int, boxes.shape))\n        boxes_last_dims = reduce(lambda x, y: x * y, boxes_last_shapes, 1)\n        boxes = boxes.reshape(-1, boxes_last_dims // 4 // int(batch_size), 4)\n        \n        \n        labels = labels.reshape(-1, 1)[transformed_inds, :]\n        labels_last_shapes = list(map(int, labels.shape))\n        labels_last_dims = reduce(lambda x, y: x * y, labels_last_shapes, 1)\n        labels = labels.reshape(-1, labels_last_dims // int(batch_size))\n\n    scores = scores.unsqueeze(2)\n    dets = torch.cat([boxes, scores], dim=2)\n    return dets, labels", "\n\nclass DummyONNXNMSop(torch.autograd.Function):\n    \"\"\"DummyONNXNMSop.\n\n    This class is only for creating onnx::NonMaxSuppression.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, boxes, scores, max_output_boxes_per_class, iou_threshold,\n                score_threshold):\n\n        return DummyONNXNMSop.output\n\n    @staticmethod\n    def symbolic(g, boxes, scores, max_output_boxes_per_class, iou_threshold,\n                 score_threshold):\n        return g.op(\n            'NonMaxSuppression',\n            boxes,\n            scores,\n            max_output_boxes_per_class,\n            iou_threshold,\n            score_threshold,\n            outputs=1)", ""]}
{"filename": "patch/mmdet/core/export/pytorch2onnx.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nfrom functools import partial\n\nimport mmcv\nimport numpy as np\nimport torch\nfrom mmcv.runner import load_checkpoint\n\n\ndef generate_inputs_and_wrap_model(config_path,\n                                   checkpoint_path,\n                                   input_config,\n                                   cfg_options=None):\n    \"\"\"Prepare sample input and wrap model for ONNX export.\n\n    The ONNX export API only accept args, and all inputs should be\n    torch.Tensor or corresponding types (such as tuple of tensor).\n    So we should call this function before exporting. This function will:\n\n    1. generate corresponding inputs which are used to execute the model.\n    2. Wrap the model's forward function.\n\n    For example, the MMDet models' forward function has a parameter\n    ``return_loss:bool``. As we want to set it as False while export API\n    supports neither bool type or kwargs. So we have to replace the forward\n    method like ``model.forward = partial(model.forward, return_loss=False)``.\n\n    Args:\n        config_path (str): the OpenMMLab config for the model we want to\n            export to ONNX\n        checkpoint_path (str): Path to the corresponding checkpoint\n        input_config (dict): the exactly data in this dict depends on the\n            framework. For MMSeg, we can just declare the input shape,\n            and generate the dummy data accordingly. However, for MMDet,\n            we may pass the real img path, or the NMS will return None\n            as there is no legal bbox.\n\n    Returns:\n        tuple: (model, tensor_data) wrapped model which can be called by\n            ``model(*tensor_data)`` and a list of inputs which are used to\n            execute the model while exporting.\n    \"\"\"\n\n    model = build_model_from_cfg(\n        config_path, checkpoint_path, cfg_options=cfg_options)\n    one_img, one_meta = preprocess_example_input(input_config)\n    tensor_data = [one_img]\n    model.forward = partial(\n        model.forward, img_metas=[[one_meta]], return_loss=False)\n\n    # pytorch has some bug in pytorch1.3, we have to fix it\n    # by replacing these existing op\n    opset_version = 11\n    # put the import within the function thus it will not cause import error\n    # when not using this function\n    try:\n        from mmcv.onnx.symbolic import register_extra_symbolics\n    except ModuleNotFoundError:\n        raise NotImplementedError('please update mmcv to version>=v1.0.4')\n    register_extra_symbolics(opset_version)\n\n    return model, tensor_data", "\ndef generate_inputs_and_wrap_model(config_path,\n                                   checkpoint_path,\n                                   input_config,\n                                   cfg_options=None):\n    \"\"\"Prepare sample input and wrap model for ONNX export.\n\n    The ONNX export API only accept args, and all inputs should be\n    torch.Tensor or corresponding types (such as tuple of tensor).\n    So we should call this function before exporting. This function will:\n\n    1. generate corresponding inputs which are used to execute the model.\n    2. Wrap the model's forward function.\n\n    For example, the MMDet models' forward function has a parameter\n    ``return_loss:bool``. As we want to set it as False while export API\n    supports neither bool type or kwargs. So we have to replace the forward\n    method like ``model.forward = partial(model.forward, return_loss=False)``.\n\n    Args:\n        config_path (str): the OpenMMLab config for the model we want to\n            export to ONNX\n        checkpoint_path (str): Path to the corresponding checkpoint\n        input_config (dict): the exactly data in this dict depends on the\n            framework. For MMSeg, we can just declare the input shape,\n            and generate the dummy data accordingly. However, for MMDet,\n            we may pass the real img path, or the NMS will return None\n            as there is no legal bbox.\n\n    Returns:\n        tuple: (model, tensor_data) wrapped model which can be called by\n            ``model(*tensor_data)`` and a list of inputs which are used to\n            execute the model while exporting.\n    \"\"\"\n\n    model = build_model_from_cfg(\n        config_path, checkpoint_path, cfg_options=cfg_options)\n    one_img, one_meta = preprocess_example_input(input_config)\n    tensor_data = [one_img]\n    model.forward = partial(\n        model.forward, img_metas=[[one_meta]], return_loss=False)\n\n    # pytorch has some bug in pytorch1.3, we have to fix it\n    # by replacing these existing op\n    opset_version = 11\n    # put the import within the function thus it will not cause import error\n    # when not using this function\n    try:\n        from mmcv.onnx.symbolic import register_extra_symbolics\n    except ModuleNotFoundError:\n        raise NotImplementedError('please update mmcv to version>=v1.0.4')\n    register_extra_symbolics(opset_version)\n\n    return model, tensor_data", "\n\ndef build_model_from_cfg(config_path, checkpoint_path, cfg_options=None):\n    \"\"\"Build a model from config and load the given checkpoint.\n\n    Args:\n        config_path (str): the OpenMMLab config for the model we want to\n            export to ONNX\n        checkpoint_path (str): Path to the corresponding checkpoint\n\n    Returns:\n        torch.nn.Module: the built model\n    \"\"\"\n    from mmdet.models import build_detector\n\n    cfg = mmcv.Config.fromfile(config_path)\n    if cfg_options is not None:\n        cfg.merge_from_dict(cfg_options)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None\n    cfg.data.test.test_mode = True\n\n    # build the model\n    cfg.model.train_cfg = None\n    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n    checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n    if 'CLASSES' in checkpoint.get('meta', {}):\n        model.CLASSES = checkpoint['meta']['CLASSES']\n    else:\n        from mmdet.datasets import DATASETS\n        dataset = DATASETS.get(cfg.data.test['type'])\n        assert (dataset is not None)\n        model.CLASSES = dataset.CLASSES\n    model.cpu().eval()\n    return model", "\n\ndef preprocess_example_input(input_config):\n    \"\"\"Prepare an example input image for ``generate_inputs_and_wrap_model``.\n\n    Args:\n        input_config (dict): customized config describing the example input.\n\n    Returns:\n        tuple: (one_img, one_meta), tensor of the example input image and \\\n            meta information for the example input image.\n\n    Examples:\n        >>> from mmdet.core.export import preprocess_example_input\n        >>> input_config = {\n        >>>         'input_shape': (1,3,224,224),\n        >>>         'input_path': 'demo/demo.jpg',\n        >>>         'normalize_cfg': {\n        >>>             'mean': (123.675, 116.28, 103.53),\n        >>>             'std': (58.395, 57.12, 57.375)\n        >>>             }\n        >>>         }\n        >>> one_img, one_meta = preprocess_example_input(input_config)\n        >>> print(one_img.shape)\n        torch.Size([1, 3, 224, 224])\n        >>> print(one_meta)\n        {'img_shape': (224, 224, 3),\n        'ori_shape': (224, 224, 3),\n        'pad_shape': (224, 224, 3),\n        'filename': '<demo>.png',\n        'scale_factor': 1.0,\n        'flip': False}\n    \"\"\"\n    input_path = input_config['input_path']\n    input_shape = input_config['input_shape']\n    one_img = mmcv.imread(input_path)\n    one_img = mmcv.imresize(one_img, input_shape[2:][::-1])\n    show_img = one_img.copy()\n    if 'normalize_cfg' in input_config.keys():\n        normalize_cfg = input_config['normalize_cfg']\n        mean = np.array(normalize_cfg['mean'], dtype=np.float32)\n        std = np.array(normalize_cfg['std'], dtype=np.float32)\n        to_rgb = normalize_cfg.get('to_rgb', True)\n        one_img = mmcv.imnormalize(one_img, mean, std, to_rgb=to_rgb)\n    one_img = one_img.transpose(2, 0, 1)\n    one_img = torch.from_numpy(one_img).unsqueeze(0).float().requires_grad_(\n        True)\n    (_, C, H, W) = input_shape\n    one_meta = {\n        'img_shape': (H, W, C),\n        'ori_shape': (H, W, C),\n        'pad_shape': (H, W, C),\n        'filename': '<demo>.png',\n        'scale_factor': np.ones(4, dtype=np.float32).tolist(),\n        'flip': False,\n        'show_img': torch.as_tensor(show_img),\n        'flip_direction': None\n    }\n\n    return one_img, one_meta", ""]}
{"filename": "patch/mmdet/core/export/model_wrappers.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport os.path as osp\nimport warnings\n\nimport numpy as np\nimport torch\n\nfrom mmdet.core import bbox2result\nfrom mmdet.models import BaseDetector\nimport onnx", "from mmdet.models import BaseDetector\nimport onnx\nimport copy\n\n\nclass DeployBaseDetector(BaseDetector):\n    \"\"\"DeployBaseDetector.\"\"\"\n\n    def __init__(self, class_names, device_id):\n        super(DeployBaseDetector, self).__init__()\n        self.CLASSES = class_names\n        self.device_id = device_id\n\n    def simple_test(self, img, img_metas, **kwargs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def aug_test(self, imgs, img_metas, **kwargs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def extract_feat(self, imgs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def forward_train(self, imgs, img_metas, **kwargs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def val_step(self, data, optimizer):\n        raise NotImplementedError('This method is not implemented.')\n\n    def train_step(self, data, optimizer):\n        raise NotImplementedError('This method is not implemented.')\n\n    def forward_test(self, *, img, img_metas, **kwargs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def async_simple_test(self, img, img_metas, **kwargs):\n        raise NotImplementedError('This method is not implemented.')\n\n    def forward(self, img, img_metas, return_loss=True, **kwargs):\n        outputs = self.forward_test(img, img_metas, **kwargs)\n        batch_dets, batch_labels = outputs[:2]\n        batch_masks = outputs[2] if len(outputs) >= 3 else None\n        batch_size = img[0].shape[0]\n        img_metas = img_metas[0]\n        results = []\n        rescale = kwargs.get('rescale', True)\n        for i in range(batch_size):\n            dets, labels = batch_dets[i], batch_labels[i]\n            det_mask = np.sum(dets, axis=-1) >= 1e-3\n            dets = dets[det_mask]\n            labels = labels[det_mask]\n            if rescale:\n                scale_factor = img_metas[i]['scale_factor']\n\n                if isinstance(scale_factor, (list, tuple, np.ndarray)):\n                    assert len(scale_factor) == 4\n                    scale_factor = np.array(scale_factor)[None, :]  # [1,4]\n                dets[:, :4] /= scale_factor\n\n            if 'border' in img_metas[i]:\n                # offset pixel of the top-left corners between original image\n                # and padded/enlarged image, 'border' is used when exporting\n                # CornerNet and CentripetalNet to onnx\n                x_off = img_metas[i]['border'][2]\n                y_off = img_metas[i]['border'][0]\n                dets[:, [0, 2]] -= x_off\n                dets[:, [1, 3]] -= y_off\n                dets[:, :4] *= (dets[:, :4] > 0).astype(dets.dtype)\n\n            dets_results = bbox2result(dets, labels, len(self.CLASSES))\n\n            if batch_masks is not None:\n                masks = batch_masks[i]\n                masks = masks[det_mask]\n                img_h, img_w = img_metas[i]['img_shape'][:2]\n                ori_h, ori_w = img_metas[i]['ori_shape'][:2]\n                masks = masks[:, :img_h, :img_w]\n                if rescale:\n                    masks = masks.astype(np.float32)\n                    masks = torch.from_numpy(masks)\n                    masks = torch.nn.functional.interpolate(\n                        masks.unsqueeze(0), size=(ori_h, ori_w))\n                    masks = masks.squeeze(0).detach().numpy()\n                if masks.dtype != bool:\n                    masks = masks >= 0.5\n                segms_results = [[] for _ in range(len(self.CLASSES))]\n                for j in range(len(dets)):\n                    segms_results[labels[j]].append(masks[j])\n                results.append((dets_results, segms_results))\n            else:\n                results.append(dets_results)\n        return results", "\n\nclass ONNXRuntimeDetector(DeployBaseDetector):\n    \"\"\"Wrapper for detector's inference with ONNXRuntime.\"\"\"\n\n    def __init__(self, onnx_file, class_names, device_id):\n        # import pickle\n        # with open('class_names.pickle', 'wb') as f:\n        #     pickle.dump(class_names, f)\n\n        super(ONNXRuntimeDetector, self).__init__(class_names, device_id)\n        import onnxruntime as ort\n\n        # get the custom op path\n        ort_custom_op_path = ''\n        try:\n            from mmcv.ops import get_onnxruntime_op_path\n            ort_custom_op_path = get_onnxruntime_op_path()\n        except (ImportError, ModuleNotFoundError):\n            warnings.warn('If input model has custom op from mmcv, \\\n                you may have to build mmcv with ONNXRuntime from source.')\n        session_options = ort.SessionOptions()\n        # register custom op for onnxruntime\n        if osp.exists(ort_custom_op_path):\n            session_options.register_custom_ops_library(ort_custom_op_path)\n        onnx_model = onnx.load(onnx_file)\n        ori_output = copy.deepcopy(onnx_model.graph.output)\n        # \u8f93\u51fa\u6a21\u578b\u6bcf\u5c42\u7684\u8f93\u51fa\n        for node in onnx_model.graph.node:\n            if not (node.op_type in ['NonMaxSuppression', 'Concat', 'TopK', 'Sigmoid', 'Mul'] or \\\n                node.name in ['Reshape_1686', 'onnx::Mul_2265'] or node.output[0] in ['onnx::Mul_2265', \\\n                'onnx::Reshape_2348', 'onnx::Gather_2347', 'onnx::Gather_2337']):\n                continue\n            \n            for output in node.output:\n                if output not in ori_output:\n                    onnx_model.graph.output.extend([onnx.ValueInfoProto(name=output)])\n        sess = ort.InferenceSession(onnx_model.SerializeToString(), session_options)\n        \n        providers = ['CPUExecutionProvider']\n        options = [{}]\n        is_cuda_available = ort.get_device() == 'GPU'\n        if is_cuda_available:\n            providers.insert(0, 'CUDAExecutionProvider')\n            options.insert(0, {'device_id': device_id})\n\n        sess.set_providers(providers, options)\n\n        self.sess = sess\n        self.io_binding = sess.io_binding()\n        self.output_names = [_.name for _ in sess.get_outputs()]\n        print('self.output_names', self.output_names)\n        self.is_cuda_available = is_cuda_available\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        input_data = imgs[0]\n        # set io binding for inputs/outputs\n        device_type = 'cuda' if self.is_cuda_available else 'cpu'\n        if not self.is_cuda_available:\n            input_data = input_data.cpu()\n        self.io_binding.bind_input(\n            name='input',\n            device_type=device_type,\n            device_id=self.device_id,\n            element_type=np.float32,\n            shape=input_data.shape,\n            buffer_ptr=input_data.data_ptr())\n\n        for name in self.output_names:\n            self.io_binding.bind_output(name)\n        # run session to get outputs\n        self.sess.run_with_iobinding(self.io_binding)\n        ort_outputs = self.io_binding.copy_outputs_to_cpu()\n        for idx, ele in enumerate(ort_outputs):\n            if ele.dtype == np.float64:\n                ele = ele.astype(np.float32)\n                ort_outputs[idx] = ele\n        \n        # torch.save(dict(zip(self.output_names, ort_outputs)), 'ort_output.pkl')\n        # for k, v in dict(zip(self.output_names, ort_outputs)).items():\n        #     print(k)\n        #     print(v.shape)\n        #     print(v)\n        #     print('=' * 50)\n        # exit(0)\n        return ort_outputs", "\n\nclass TensorRTDetector(DeployBaseDetector):\n    \"\"\"Wrapper for detector's inference with TensorRT.\"\"\"\n\n    def __init__(self, engine_file, class_names, device_id, output_names=None):\n        super(TensorRTDetector, self).__init__(class_names, device_id)\n        warnings.warn('`output_names` is deprecated and will be removed in '\n                      'future releases.')\n        from mmcv.tensorrt import TRTWraper, load_tensorrt_plugin\n        try:\n            load_tensorrt_plugin()\n        except (ImportError, ModuleNotFoundError):\n            warnings.warn('If input model has custom op from mmcv, \\\n                you may have to build mmcv with TensorRT from source.')\n\n        output_names = ['dets', 'labels']\n        model = TRTWraper(engine_file, ['input'], output_names)\n        with_masks = False\n        # if TensorRT has totally 4 inputs/outputs, then\n        # the detector should have `mask` output.\n        if len(model.engine) == 4:\n            model.output_names = output_names + ['masks']\n            with_masks = True\n        self.model = model\n        self.with_masks = with_masks\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        input_data = imgs[0].contiguous()\n        with torch.cuda.device(self.device_id), torch.no_grad():\n            outputs = self.model({'input': input_data})\n            outputs = [outputs[name] for name in self.model.output_names]\n        outputs = [out.detach().cpu().numpy() for out in outputs]\n        return outputs", ""]}
{"filename": "patch/mmdet/core/visualization/image.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as mask_util\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon", "from matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\n\nfrom mmdet.core.evaluation.panoptic_utils import INSTANCE_OFFSET\nfrom ..mask.structures import bitmap_to_polygon\nfrom ..utils import mask2ndarray\nfrom .palette import get_palette, palette_val\n\n__all__ = [\n    'color_val_matplotlib', 'draw_masks', 'draw_bboxes', 'draw_labels',", "__all__ = [\n    'color_val_matplotlib', 'draw_masks', 'draw_bboxes', 'draw_labels',\n    'imshow_det_bboxes', 'imshow_gt_det_bboxes'\n]\n\nEPS = 1e-2\n\n\ndef color_val_matplotlib(color):\n    \"\"\"Convert various input in BGR order to normalized RGB matplotlib color\n    tuples.\n\n    Args:\n        color (:obj`Color` | str | tuple | int | ndarray): Color inputs.\n\n    Returns:\n        tuple[float]: A tuple of 3 normalized floats indicating RGB channels.\n    \"\"\"\n    color = mmcv.color_val(color)\n    color = [color / 255 for color in color[::-1]]\n    return tuple(color)", "def color_val_matplotlib(color):\n    \"\"\"Convert various input in BGR order to normalized RGB matplotlib color\n    tuples.\n\n    Args:\n        color (:obj`Color` | str | tuple | int | ndarray): Color inputs.\n\n    Returns:\n        tuple[float]: A tuple of 3 normalized floats indicating RGB channels.\n    \"\"\"\n    color = mmcv.color_val(color)\n    color = [color / 255 for color in color[::-1]]\n    return tuple(color)", "\n\ndef _get_adaptive_scales(areas, min_area=800, max_area=30000):\n    \"\"\"Get adaptive scales according to areas.\n\n    The scale range is [0.5, 1.0]. When the area is less than\n    ``'min_area'``, the scale is 0.5 while the area is larger than\n    ``'max_area'``, the scale is 1.0.\n\n    Args:\n        areas (ndarray): The areas of bboxes or masks with the\n            shape of (n, ).\n        min_area (int): Lower bound areas for adaptive scales.\n            Default: 800.\n        max_area (int): Upper bound areas for adaptive scales.\n            Default: 30000.\n\n    Returns:\n        ndarray: The adaotive scales with the shape of (n, ).\n    \"\"\"\n    scales = 0.5 + (areas - min_area) / (max_area - min_area)\n    scales = np.clip(scales, 0.5, 1.0)\n    return scales", "\n\ndef _get_bias_color(base, max_dist=30):\n    \"\"\"Get different colors for each masks.\n\n    Get different colors for each masks by adding a bias\n    color to the base category color.\n    Args:\n        base (ndarray): The base category color with the shape\n            of (3, ).\n        max_dist (int): The max distance of bias. Default: 30.\n\n    Returns:\n        ndarray: The new color for a mask with the shape of (3, ).\n    \"\"\"\n    new_color = base + np.random.randint(\n        low=-max_dist, high=max_dist + 1, size=3)\n    return np.clip(new_color, 0, 255, new_color)", "\n\ndef draw_bboxes(ax, bboxes, color='g', alpha=0.8, thickness=2):\n    \"\"\"Draw bounding boxes on the axes.\n\n    Args:\n        ax (matplotlib.Axes): The input axes.\n        bboxes (ndarray): The input bounding boxes with the shape\n            of (n, 4).\n        color (list[tuple] | matplotlib.color): the colors for each\n            bounding boxes.\n        alpha (float): Transparency of bounding boxes. Default: 0.8.\n        thickness (int): Thickness of lines. Default: 2.\n\n    Returns:\n        matplotlib.Axes: The result axes.\n    \"\"\"\n    polygons = []\n    for i, bbox in enumerate(bboxes):\n        bbox_int = bbox.astype(np.int32)\n        poly = [[bbox_int[0], bbox_int[1]], [bbox_int[0], bbox_int[3]],\n                [bbox_int[2], bbox_int[3]], [bbox_int[2], bbox_int[1]]]\n        np_poly = np.array(poly).reshape((4, 2))\n        polygons.append(Polygon(np_poly))\n    p = PatchCollection(\n        polygons,\n        facecolor='none',\n        edgecolors=color,\n        linewidths=thickness,\n        alpha=alpha)\n    ax.add_collection(p)\n\n    return ax", "\n\ndef draw_labels(ax,\n                labels,\n                positions,\n                scores=None,\n                class_names=None,\n                color='w',\n                font_size=8,\n                scales=None,\n                horizontal_alignment='left'):\n    \"\"\"Draw labels on the axes.\n\n    Args:\n        ax (matplotlib.Axes): The input axes.\n        labels (ndarray): The labels with the shape of (n, ).\n        positions (ndarray): The positions to draw each labels.\n        scores (ndarray): The scores for each labels.\n        class_names (list[str]): The class names.\n        color (list[tuple] | matplotlib.color): The colors for labels.\n        font_size (int): Font size of texts. Default: 8.\n        scales (list[float]): Scales of texts. Default: None.\n        horizontal_alignment (str): The horizontal alignment method of\n            texts. Default: 'left'.\n\n    Returns:\n        matplotlib.Axes: The result axes.\n    \"\"\"\n    for i, (pos, label) in enumerate(zip(positions, labels)):\n        label_text = class_names[\n            label] if class_names is not None else f'class {label}'\n        if scores is not None:\n            label_text += f'|{scores[i]:.02f}'\n        text_color = color[i] if isinstance(color, list) else color\n\n        font_size_mask = font_size if scales is None else font_size * scales[i]\n        ax.text(\n            pos[0],\n            pos[1],\n            f'{label_text}',\n            bbox={\n                'facecolor': 'black',\n                'alpha': 0.8,\n                'pad': 0.7,\n                'edgecolor': 'none'\n            },\n            color=text_color,\n            fontsize=font_size_mask,\n            verticalalignment='top',\n            horizontalalignment=horizontal_alignment)\n\n    return ax", "\n\ndef draw_masks(ax, img, masks, color=None, with_edge=True, alpha=0.8):\n    \"\"\"Draw masks on the image and their edges on the axes.\n\n    Args:\n        ax (matplotlib.Axes): The input axes.\n        img (ndarray): The image with the shape of (3, h, w).\n        masks (ndarray): The masks with the shape of (n, h, w).\n        color (ndarray): The colors for each masks with the shape\n            of (n, 3).\n        with_edge (bool): Whether to draw edges. Default: True.\n        alpha (float): Transparency of bounding boxes. Default: 0.8.\n\n    Returns:\n        matplotlib.Axes: The result axes.\n        ndarray: The result image.\n    \"\"\"\n    taken_colors = set([0, 0, 0])\n    if color is None:\n        random_colors = np.random.randint(0, 255, (masks.size(0), 3))\n        color = [tuple(c) for c in random_colors]\n        color = np.array(color, dtype=np.uint8)\n    polygons = []\n    for i, mask in enumerate(masks):\n        if with_edge:\n            contours, _ = bitmap_to_polygon(mask)\n            polygons += [Polygon(c) for c in contours]\n\n        color_mask = color[i]\n        while tuple(color_mask) in taken_colors:\n            color_mask = _get_bias_color(color_mask)\n        taken_colors.add(tuple(color_mask))\n\n        mask = mask.astype(bool)\n \n        img[mask] = img[mask] * (1 - alpha) + color_mask * alpha\n\n    p = PatchCollection(\n        polygons, facecolor='none', edgecolors='w', linewidths=1, alpha=0.8)\n    ax.add_collection(p)\n\n    return ax, img", "\n\ndef imshow_det_bboxes(img,\n                      bboxes=None,\n                      labels=None,\n                      segms=None,\n                      class_names=None,\n                      score_thr=0,\n                      bbox_color='green',\n                      text_color='green',\n                      mask_color=None,\n                      thickness=2,\n                      font_size=8,\n                      win_name='',\n                      show=True,\n                      wait_time=0,\n                      out_file=None):\n    \"\"\"Draw bboxes and class labels (with scores) on an image.\n\n    Args:\n        img (str | ndarray): The image to be displayed.\n        bboxes (ndarray): Bounding boxes (with scores), shaped (n, 4) or\n            (n, 5).\n        labels (ndarray): Labels of bboxes.\n        segms (ndarray | None): Masks, shaped (n,h,w) or None.\n        class_names (list[str]): Names of each classes.\n        score_thr (float): Minimum score of bboxes to be shown. Default: 0.\n        bbox_color (list[tuple] | tuple | str | None): Colors of bbox lines.\n           If a single color is given, it will be applied to all classes.\n           The tuple of color should be in RGB order. Default: 'green'.\n        text_color (list[tuple] | tuple | str | None): Colors of texts.\n           If a single color is given, it will be applied to all classes.\n           The tuple of color should be in RGB order. Default: 'green'.\n        mask_color (list[tuple] | tuple | str | None, optional): Colors of\n           masks. If a single color is given, it will be applied to all\n           classes. The tuple of color should be in RGB order.\n           Default: None.\n        thickness (int): Thickness of lines. Default: 2.\n        font_size (int): Font size of texts. Default: 13.\n        show (bool): Whether to show the image. Default: True.\n        win_name (str): The window name. Default: ''.\n        wait_time (float): Value of waitKey param. Default: 0.\n        out_file (str, optional): The filename to write the image.\n            Default: None.\n\n    Returns:\n        ndarray: The image with bboxes drawn on it.\n    \"\"\"\n    assert bboxes is None or bboxes.ndim == 2, \\\n        f' bboxes ndim should be 2, but its ndim is {bboxes.ndim}.'\n    assert labels.ndim == 1, \\\n        f' labels ndim should be 1, but its ndim is {labels.ndim}.'\n    assert bboxes is None or bboxes.shape[1] == 4 or bboxes.shape[1] == 5, \\\n        f' bboxes.shape[1] should be 4 or 5, but its {bboxes.shape[1]}.'\n    assert bboxes is None or bboxes.shape[0] <= labels.shape[0], \\\n        'labels.shape[0] should not be less than bboxes.shape[0].'\n    assert segms is None or segms.shape[0] == labels.shape[0], \\\n        'segms.shape[0] and labels.shape[0] should have the same length.'\n    assert segms is not None or bboxes is not None, \\\n        'segms and bboxes should not be None at the same time.'\n\n    img = mmcv.imread(img).astype(np.uint8)\n\n    if score_thr > 0:\n        assert bboxes is not None and bboxes.shape[1] == 5\n        scores = bboxes[:, -1]\n        inds = scores > score_thr\n        bboxes = bboxes[inds, :]\n        labels = labels[inds]\n        if segms is not None:\n            segms = segms[inds, ...]\n\n    img = mmcv.bgr2rgb(img)\n    width, height = img.shape[1], img.shape[0]\n    img = np.ascontiguousarray(img)\n\n    fig = plt.figure(win_name, frameon=False)\n    plt.title(win_name)\n    canvas = fig.canvas\n    dpi = fig.get_dpi()\n    # add a small EPS to avoid precision lost due to matplotlib's truncation\n    # (https://github.com/matplotlib/matplotlib/issues/15363)\n    fig.set_size_inches((width + EPS) / dpi, (height + EPS) / dpi)\n\n    # remove white edges by set subplot margin\n    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n    ax = plt.gca()\n    ax.axis('off')\n\n    max_label = int(max(labels) if len(labels) > 0 else 0)\n    text_palette = palette_val(get_palette(text_color, max_label + 1))\n    text_colors = [text_palette[label] for label in labels]\n\n    num_bboxes = 0\n    if bboxes is not None:\n        num_bboxes = bboxes.shape[0]\n        bbox_palette = palette_val(get_palette(bbox_color, max_label + 1))\n        colors = [bbox_palette[label] for label in labels[:num_bboxes]]\n        draw_bboxes(ax, bboxes, colors, alpha=0.8, thickness=thickness)\n\n        horizontal_alignment = 'left'\n        positions = bboxes[:, :2].astype(np.int32) + thickness\n        areas = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n        scales = _get_adaptive_scales(areas)\n        scores = bboxes[:, 4] if bboxes.shape[1] == 5 else None\n        draw_labels(\n            ax,\n            labels[:num_bboxes],\n            positions,\n            scores=scores,\n            class_names=class_names,\n            color=text_colors,\n            font_size=font_size,\n            scales=scales,\n            horizontal_alignment=horizontal_alignment)\n\n    if segms is not None:\n        mask_palette = get_palette(mask_color, max_label + 1)\n        colors = [mask_palette[label] for label in labels]\n        colors = np.array(colors, dtype=np.uint8)\n        draw_masks(ax, img, segms, colors, with_edge=True)\n\n        if num_bboxes < segms.shape[0]:\n            segms = segms[num_bboxes:]\n            horizontal_alignment = 'center'\n            areas = []\n            positions = []\n            for mask in segms:\n                _, _, stats, centroids = cv2.connectedComponentsWithStats(\n                    mask.astype(np.uint8), connectivity=8)\n                largest_id = np.argmax(stats[1:, -1]) + 1\n                positions.append(centroids[largest_id])\n                areas.append(stats[largest_id, -1])\n            areas = np.stack(areas, axis=0)\n            scales = _get_adaptive_scales(areas)\n            draw_labels(\n                ax,\n                labels[num_bboxes:],\n                positions,\n                class_names=class_names,\n                color=text_colors,\n                font_size=font_size,\n                scales=scales,\n                horizontal_alignment=horizontal_alignment)\n\n    plt.imshow(img)\n\n    stream, _ = canvas.print_to_buffer()\n    buffer = np.frombuffer(stream, dtype='uint8')\n    if sys.platform == 'darwin':\n        width, height = canvas.get_width_height(physical=True)\n    img_rgba = buffer.reshape(height, width, 4)\n    rgb, alpha = np.split(img_rgba, [3], axis=2)\n    img = rgb.astype('uint8')\n    img = mmcv.rgb2bgr(img)\n\n    if show:\n        # We do not use cv2 for display because in some cases, opencv will\n        # conflict with Qt, it will output a warning: Current thread\n        # is not the object's thread. You can refer to\n        # https://github.com/opencv/opencv-python/issues/46 for details\n        if wait_time == 0:\n            plt.show()\n        else:\n            plt.show(block=False)\n            plt.pause(wait_time)\n    if out_file is not None:\n        mmcv.imwrite(img, out_file)\n\n    plt.close()\n\n    return img", "\n\ndef imshow_gt_det_bboxes(img,\n                         annotation,\n                         result,\n                         class_names=None,\n                         score_thr=0,\n                         gt_bbox_color=(61, 102, 255),\n                         gt_text_color=(200, 200, 200),\n                         gt_mask_color=(61, 102, 255),\n                         det_bbox_color=(241, 101, 72),\n                         det_text_color=(200, 200, 200),\n                         det_mask_color=(241, 101, 72),\n                         thickness=2,\n                         font_size=13,\n                         win_name='',\n                         show=True,\n                         wait_time=0,\n                         out_file=None,\n                         overlay_gt_pred=True):\n    \"\"\"General visualization GT and result function.\n\n    Args:\n      img (str | ndarray): The image to be displayed.\n      annotation (dict): Ground truth annotations where contain keys of\n          'gt_bboxes' and 'gt_labels' or 'gt_masks'.\n      result (tuple[list] | list): The detection result, can be either\n          (bbox, segm) or just bbox.\n      class_names (list[str]): Names of each classes.\n      score_thr (float): Minimum score of bboxes to be shown. Default: 0.\n      gt_bbox_color (list[tuple] | tuple | str | None): Colors of bbox lines.\n          If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (61, 102, 255).\n      gt_text_color (list[tuple] | tuple | str | None): Colors of texts.\n          If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (200, 200, 200).\n      gt_mask_color (list[tuple] | tuple | str | None, optional): Colors of\n          masks. If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (61, 102, 255).\n      det_bbox_color (list[tuple] | tuple | str | None):Colors of bbox lines.\n          If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (241, 101, 72).\n      det_text_color (list[tuple] | tuple | str | None):Colors of texts.\n          If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (200, 200, 200).\n      det_mask_color (list[tuple] | tuple | str | None, optional): Color of\n          masks. If a single color is given, it will be applied to all classes.\n          The tuple of color should be in RGB order. Default: (241, 101, 72).\n      thickness (int): Thickness of lines. Default: 2.\n      font_size (int): Font size of texts. Default: 13.\n      win_name (str): The window name. Default: ''.\n      show (bool): Whether to show the image. Default: True.\n      wait_time (float): Value of waitKey param. Default: 0.\n      out_file (str, optional): The filename to write the image.\n          Default: None.\n      overlay_gt_pred (bool): Whether to plot gts and predictions on the\n       same image. If False, predictions and gts will be plotted on two same\n       image which will be concatenated in vertical direction. The image\n       above is drawn with gt, and the image below is drawn with the\n       prediction result. Default: True.\n\n    Returns:\n        ndarray: The image with bboxes or masks drawn on it.\n    \"\"\"\n    assert 'gt_bboxes' in annotation\n    assert 'gt_labels' in annotation\n    assert isinstance(result, (tuple, list, dict)), 'Expected ' \\\n        f'tuple or list or dict, but get {type(result)}'\n\n    gt_bboxes = annotation['gt_bboxes']\n    gt_labels = annotation['gt_labels']\n    gt_masks = annotation.get('gt_masks', None)\n    if gt_masks is not None:\n        gt_masks = mask2ndarray(gt_masks)\n\n    gt_seg = annotation.get('gt_semantic_seg', None)\n    if gt_seg is not None:\n        pad_value = 255  # the padding value of gt_seg\n        sem_labels = np.unique(gt_seg)\n        all_labels = np.concatenate((gt_labels, sem_labels), axis=0)\n        all_labels, counts = np.unique(all_labels, return_counts=True)\n        stuff_labels = all_labels[np.logical_and(counts < 2,\n                                                 all_labels != pad_value)]\n        stuff_masks = gt_seg[None] == stuff_labels[:, None, None]\n        gt_labels = np.concatenate((gt_labels, stuff_labels), axis=0)\n        gt_masks = np.concatenate((gt_masks, stuff_masks.astype(np.uint8)),\n                                  axis=0)\n        # If you need to show the bounding boxes,\n        # please comment the following line\n        # gt_bboxes = None\n\n    img = mmcv.imread(img)\n\n    img_with_gt = imshow_det_bboxes(\n        img,\n        gt_bboxes,\n        gt_labels,\n        gt_masks,\n        class_names=class_names,\n        bbox_color=gt_bbox_color,\n        text_color=gt_text_color,\n        mask_color=gt_mask_color,\n        thickness=thickness,\n        font_size=font_size,\n        win_name=win_name,\n        show=False)\n\n    if not isinstance(result, dict):\n        if isinstance(result, tuple):\n            bbox_result, segm_result = result\n            if isinstance(segm_result, tuple):\n                segm_result = segm_result[0]  # ms rcnn\n        else:\n            bbox_result, segm_result = result, None\n\n        bboxes = np.vstack(bbox_result)\n        labels = [\n            np.full(bbox.shape[0], i, dtype=np.int32)\n            for i, bbox in enumerate(bbox_result)\n        ]\n        labels = np.concatenate(labels)\n\n        segms = None\n        if segm_result is not None and len(labels) > 0:  # non empty\n            segms = mmcv.concat_list(segm_result)\n            segms = mask_util.decode(segms)\n            segms = segms.transpose(2, 0, 1)\n    else:\n        assert class_names is not None, 'We need to know the number ' \\\n                                        'of classes.'\n        VOID = len(class_names)\n        bboxes = None\n        pan_results = result['pan_results']\n        # keep objects ahead\n        ids = np.unique(pan_results)[::-1]\n        legal_indices = ids != VOID\n        ids = ids[legal_indices]\n        labels = np.array([id % INSTANCE_OFFSET for id in ids], dtype=np.int64)\n        segms = (pan_results[None] == ids[:, None, None])\n\n    if overlay_gt_pred:\n        img = imshow_det_bboxes(\n            img_with_gt,\n            bboxes,\n            labels,\n            segms=segms,\n            class_names=class_names,\n            score_thr=score_thr,\n            bbox_color=det_bbox_color,\n            text_color=det_text_color,\n            mask_color=det_mask_color,\n            thickness=thickness,\n            font_size=font_size,\n            win_name=win_name,\n            show=show,\n            wait_time=wait_time,\n            out_file=out_file)\n    else:\n        img_with_det = imshow_det_bboxes(\n            img,\n            bboxes,\n            labels,\n            segms=segms,\n            class_names=class_names,\n            score_thr=score_thr,\n            bbox_color=det_bbox_color,\n            text_color=det_text_color,\n            mask_color=det_mask_color,\n            thickness=thickness,\n            font_size=font_size,\n            win_name=win_name,\n            show=False)\n        img = np.concatenate([img_with_gt, img_with_det], axis=0)\n\n        plt.imshow(img)\n        if show:\n            if wait_time == 0:\n                plt.show()\n            else:\n                plt.show(block=False)\n                plt.pause(wait_time)\n        if out_file is not None:\n            mmcv.imwrite(img, out_file)\n        plt.close()\n\n    return img", ""]}
{"filename": "patch/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\n\nimport mmcv\nimport numpy as np\nimport torch\n\nfrom ..builder import BBOX_CODERS\nfrom .base_bbox_coder import BaseBBoxCoder\n", "from .base_bbox_coder import BaseBBoxCoder\n\n\n@BBOX_CODERS.register_module()\nclass DeltaXYWHBBoxCoder(BaseBBoxCoder):\n    \"\"\"Delta XYWH BBox coder.\n\n    Following the practice in `R-CNN <https://arxiv.org/abs/1311.2524>`_,\n    this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and\n    decodes delta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2).\n\n    Args:\n        target_means (Sequence[float]): Denormalizing means of target for\n            delta coordinates\n        target_stds (Sequence[float]): Denormalizing standard deviation of\n            target for delta coordinates\n        clip_border (bool, optional): Whether clip the objects outside the\n            border of the image. Defaults to True.\n        add_ctr_clamp (bool): Whether to add center clamp, when added, the\n            predicted box is clamped is its center is too far away from\n            the original anchor's center. Only used by YOLOF. Default False.\n        ctr_clamp (int): the maximum pixel shift to clamp. Only used by YOLOF.\n            Default 32.\n    \"\"\"\n\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.),\n                 target_stds=(1., 1., 1., 1.),\n                 clip_border=True,\n                 add_ctr_clamp=False,\n                 ctr_clamp=32):\n        super(BaseBBoxCoder, self).__init__()\n        self.means = target_means\n        self.stds = target_stds\n        self.clip_border = clip_border\n        self.add_ctr_clamp = add_ctr_clamp\n        self.ctr_clamp = ctr_clamp\n\n    def encode(self, bboxes, gt_bboxes):\n        \"\"\"Get box regression transformation deltas that can be used to\n        transform the ``bboxes`` into the ``gt_bboxes``.\n\n        Args:\n            bboxes (torch.Tensor): Source boxes, e.g., object proposals.\n            gt_bboxes (torch.Tensor): Target of the transformation, e.g.,\n                ground-truth boxes.\n\n        Returns:\n            torch.Tensor: Box transformation deltas\n        \"\"\"\n\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = bbox2delta(bboxes, gt_bboxes, self.means, self.stds)\n        return encoded_bboxes\n\n    def decode(self,\n               bboxes,\n               pred_bboxes,\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n        \"\"\"Apply transformation `pred_bboxes` to `boxes`.\n\n        Args:\n            bboxes (torch.Tensor): Basic boxes. Shape (B, N, 4) or (N, 4)\n            pred_bboxes (Tensor): Encoded offsets with respect to each roi.\n               Has shape (B, N, num_classes * 4) or (B, N, 4) or\n               (N, num_classes * 4) or (N, 4). Note N = num_anchors * W * H\n               when rois is a grid of anchors.Offset encoding follows [1]_.\n            max_shape (Sequence[int] or torch.Tensor or Sequence[\n               Sequence[int]],optional): Maximum bounds for boxes, specifies\n               (H, W, C) or (H, W). If bboxes shape is (B, N, 4), then\n               the max_shape should be a Sequence[Sequence[int]]\n               and the length of max_shape should also be B.\n            wh_ratio_clip (float, optional): The allowed ratio between\n                width and height.\n\n        Returns:\n            torch.Tensor: Decoded boxes.\n        \"\"\"\n\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        if pred_bboxes.ndim == 3:\n            assert pred_bboxes.size(1) == bboxes.size(1)\n\n        if pred_bboxes.ndim == 2 and not torch.onnx.is_in_onnx_export():\n            # single image decode\n            decoded_bboxes = delta2bbox(bboxes, pred_bboxes, self.means,\n                                        self.stds, max_shape, wh_ratio_clip,\n                                        self.clip_border, self.add_ctr_clamp,\n                                        self.ctr_clamp)\n        else:\n            if pred_bboxes.ndim == 3 and not torch.onnx.is_in_onnx_export():\n                warnings.warn(\n                    'DeprecationWarning: onnx_delta2bbox is deprecated '\n                    'in the case of batch decoding and non-ONNX, '\n                    'please use \u201cdelta2bbox\u201d instead. In order to improve '\n                    'the decoding speed, the batch function will no '\n                    'longer be supported. ')\n            decoded_bboxes = onnx_delta2bbox(bboxes, pred_bboxes, self.means,\n                                             self.stds, max_shape,\n                                             wh_ratio_clip, self.clip_border,\n                                             self.add_ctr_clamp,\n                                             self.ctr_clamp)\n\n        return decoded_bboxes", "\n\n@mmcv.jit(coderize=True)\ndef bbox2delta(proposals, gt, means=(0., 0., 0., 0.), stds=(1., 1., 1., 1.)):\n    \"\"\"Compute deltas of proposals w.r.t. gt.\n\n    We usually compute the deltas of x, y, w, h of proposals w.r.t ground\n    truth bboxes to get regression target.\n    This is the inverse function of :func:`delta2bbox`.\n\n    Args:\n        proposals (Tensor): Boxes to be transformed, shape (N, ..., 4)\n        gt (Tensor): Gt bboxes to be used as base, shape (N, ..., 4)\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n\n    Returns:\n        Tensor: deltas with shape (N, 4), where columns represent dx, dy,\n            dw, dh.\n    \"\"\"\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0]\n    ph = proposals[..., 3] - proposals[..., 1]\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0]\n    gh = gt[..., 3] - gt[..., 1]\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas", "\n\n@mmcv.jit(coderize=True)\ndef delta2bbox(rois,\n               deltas,\n               means=(0., 0., 0., 0.),\n               stds=(1., 1., 1., 1.),\n               max_shape=None,\n               wh_ratio_clip=16 / 1000,\n               clip_border=True,\n               add_ctr_clamp=False,\n               ctr_clamp=32):\n    \"\"\"Apply deltas to shift/scale base boxes.\n\n    Typically the rois are anchor or proposed bounding boxes and the deltas are\n    network outputs used to shift/scale those boxes.\n    This is the inverse function of :func:`bbox2delta`.\n\n    Args:\n        rois (Tensor): Boxes to be transformed. Has shape (N, 4).\n        deltas (Tensor): Encoded offsets relative to each roi.\n            Has shape (N, num_classes * 4) or (N, 4). Note\n            N = num_base_anchors * W * H, when rois is a grid of\n            anchors. Offset encoding follows [1]_.\n        means (Sequence[float]): Denormalizing means for delta coordinates.\n            Default (0., 0., 0., 0.).\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates. Default (1., 1., 1., 1.).\n        max_shape (tuple[int, int]): Maximum bounds for boxes, specifies\n           (H, W). Default None.\n        wh_ratio_clip (float): Maximum aspect ratio for boxes. Default\n            16 / 1000.\n        clip_border (bool, optional): Whether clip the objects outside the\n            border of the image. Default True.\n        add_ctr_clamp (bool): Whether to add center clamp. When set to True,\n            the center of the prediction bounding box will be clamped to\n            avoid being too far away from the center of the anchor.\n            Only used by YOLOF. Default False.\n        ctr_clamp (int): the maximum pixel shift to clamp. Only used by YOLOF.\n            Default 32.\n\n    Returns:\n        Tensor: Boxes with shape (N, num_classes * 4) or (N, 4), where 4\n           represent tl_x, tl_y, br_x, br_y.\n\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n\n    Example:\n        >>> rois = torch.Tensor([[ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 5.,  5.,  5.,  5.]])\n        >>> deltas = torch.Tensor([[  0.,   0.,   0.,   0.],\n        >>>                        [  1.,   1.,   1.,   1.],\n        >>>                        [  0.,   0.,   2.,  -1.],\n        >>>                        [ 0.7, -1.9, -0.5,  0.3]])\n        >>> delta2bbox(rois, deltas, max_shape=(32, 32, 3))\n        tensor([[0.0000, 0.0000, 1.0000, 1.0000],\n                [0.1409, 0.1409, 2.8591, 2.8591],\n                [0.0000, 0.3161, 4.1945, 0.6839],\n                [5.0000, 5.0000, 5.0000, 5.0000]])\n    \"\"\"\n    num_bboxes, num_classes = deltas.size(0), deltas.size(1) // 4\n    if num_bboxes == 0:\n        return deltas\n\n    deltas = deltas.reshape(-1, 4)\n\n    means = deltas.new_tensor(means).view(1, -1)\n    stds = deltas.new_tensor(stds).view(1, -1)\n    denorm_deltas = deltas * stds + means\n\n    dxy = denorm_deltas[:, :2]\n    dwh = denorm_deltas[:, 2:]\n\n    # Compute width/height of each roi\n    rois_ = rois.repeat(1, num_classes).reshape(-1, 4)\n    pxy = ((rois_[:, :2] + rois_[:, 2:]) * 0.5)\n    pwh = (rois_[:, 2:] - rois_[:, :2])\n\n    dxy_wh = pwh * dxy\n\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    if add_ctr_clamp:\n        dxy_wh = torch.clamp(dxy_wh, max=ctr_clamp, min=-ctr_clamp)\n        dwh = torch.clamp(dwh, max=max_ratio)\n    else:\n        dwh = dwh.clamp(min=-max_ratio, max=max_ratio)\n\n    gxy = pxy + dxy_wh\n    gwh = pwh * dwh.exp()\n    x1y1 = gxy - (gwh * 0.5)\n    x2y2 = gxy + (gwh * 0.5)\n    bboxes = torch.cat([x1y1, x2y2], dim=-1)\n    if clip_border and max_shape is not None:\n        bboxes[..., 0::2].clamp_(min=0, max=max_shape[1])\n        bboxes[..., 1::2].clamp_(min=0, max=max_shape[0])\n    bboxes = bboxes.reshape(num_bboxes, -1)\n    return bboxes", "\n\ndef onnx_delta2bbox(rois,\n                    deltas,\n                    means=(0., 0., 0., 0.),\n                    stds=(1., 1., 1., 1.),\n                    max_shape=None,\n                    wh_ratio_clip=16 / 1000,\n                    clip_border=True,\n                    add_ctr_clamp=False,\n                    ctr_clamp=32):\n    \"\"\"Apply deltas to shift/scale base boxes.\n\n    Typically the rois are anchor or proposed bounding boxes and the deltas are\n    network outputs used to shift/scale those boxes.\n    This is the inverse function of :func:`bbox2delta`.\n\n    Args:\n        rois (Tensor): Boxes to be transformed. Has shape (N, 4) or (B, N, 4)\n        deltas (Tensor): Encoded offsets with respect to each roi.\n            Has shape (B, N, num_classes * 4) or (B, N, 4) or\n            (N, num_classes * 4) or (N, 4). Note N = num_anchors * W * H\n            when rois is a grid of anchors.Offset encoding follows [1]_.\n        means (Sequence[float]): Denormalizing means for delta coordinates.\n            Default (0., 0., 0., 0.).\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates. Default (1., 1., 1., 1.).\n        max_shape (Sequence[int] or torch.Tensor or Sequence[\n            Sequence[int]],optional): Maximum bounds for boxes, specifies\n            (H, W, C) or (H, W). If rois shape is (B, N, 4), then\n            the max_shape should be a Sequence[Sequence[int]]\n            and the length of max_shape should also be B. Default None.\n        wh_ratio_clip (float): Maximum aspect ratio for boxes.\n            Default 16 / 1000.\n        clip_border (bool, optional): Whether clip the objects outside the\n            border of the image. Default True.\n        add_ctr_clamp (bool): Whether to add center clamp, when added, the\n            predicted box is clamped is its center is too far away from\n            the original anchor's center. Only used by YOLOF. Default False.\n        ctr_clamp (int): the maximum pixel shift to clamp. Only used by YOLOF.\n            Default 32.\n\n    Returns:\n        Tensor: Boxes with shape (B, N, num_classes * 4) or (B, N, 4) or\n           (N, num_classes * 4) or (N, 4), where 4 represent\n           tl_x, tl_y, br_x, br_y.\n\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n\n    Example:\n        >>> rois = torch.Tensor([[ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 5.,  5.,  5.,  5.]])\n        >>> deltas = torch.Tensor([[  0.,   0.,   0.,   0.],\n        >>>                        [  1.,   1.,   1.,   1.],\n        >>>                        [  0.,   0.,   2.,  -1.],\n        >>>                        [ 0.7, -1.9, -0.5,  0.3]])\n        >>> delta2bbox(rois, deltas, max_shape=(32, 32, 3))\n        tensor([[0.0000, 0.0000, 1.0000, 1.0000],\n                [0.1409, 0.1409, 2.8591, 2.8591],\n                [0.0000, 0.3161, 4.1945, 0.6839],\n                [5.0000, 5.0000, 5.0000, 5.0000]])\n    \"\"\"\n    means = deltas.new_tensor(means).view(1,\n                                          -1).repeat(1,\n                                                     deltas.size(-1) // 4)\n    stds = deltas.new_tensor(stds).view(1, -1).repeat(1, deltas.size(-1) // 4)\n    if torch.all(means == 0) and torch.all(stds == 1):\n        denorm_deltas = deltas\n    elif torch.all(means == 0):\n        denorm_deltas = deltas * stds\n    elif torch.all(stds == 1):\n        denorm_deltas = deltas + means\n    else:\n        denorm_deltas = deltas * stds + means\n        \n    dx = denorm_deltas[..., 0::4]\n    dy = denorm_deltas[..., 1::4]\n    dw = denorm_deltas[..., 2::4]\n    dh = denorm_deltas[..., 3::4]\n\n    x1, y1 = rois[..., 0], rois[..., 1]\n    x2, y2 = rois[..., 2], rois[..., 3]\n    # Compute center of each roi\n    px = ((x1 + x2) * 0.5).unsqueeze(-1).expand_as(dx)\n    py = ((y1 + y2) * 0.5).unsqueeze(-1).expand_as(dy)\n    # Compute width/height of each roi\n    pw = (x2 - x1).unsqueeze(-1).expand_as(dw)\n    ph = (y2 - y1).unsqueeze(-1).expand_as(dh)\n\n    dx_width = pw * dx\n    dy_height = ph * dy\n\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    if add_ctr_clamp:\n        dx_width = torch.clamp(dx_width, max=ctr_clamp, min=-ctr_clamp)\n        dy_height = torch.clamp(dy_height, max=ctr_clamp, min=-ctr_clamp)\n        dw = torch.clamp(dw, max=max_ratio)\n        dh = torch.clamp(dh, max=max_ratio)\n    else:\n        dw = dw.clamp(min=-max_ratio, max=max_ratio)\n        dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    # Use exp(network energy) to enlarge/shrink each roi\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    # Use network energy to shift the center of each roi\n    gx = px + dx_width\n    gy = py + dy_height\n    # Convert center-xy/width/height to top-left, bottom-right\n    x1 = gx - gw * 0.5\n    y1 = gy - gh * 0.5\n    x2 = gx + gw * 0.5\n    y2 = gy + gh * 0.5\n\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view(deltas.size())\n\n    if clip_border and max_shape is not None:\n        # clip bboxes with dynamic `min` and `max` for onnx\n        if torch.onnx.is_in_onnx_export():\n            from mmdet.core.export import dynamic_clip_for_onnx\n            x1, y1, x2, y2 = dynamic_clip_for_onnx(x1, y1, x2, y2, max_shape)\n            bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view(deltas.size())\n            return bboxes\n        if not isinstance(max_shape, torch.Tensor):\n            max_shape = x1.new_tensor(max_shape)\n        max_shape = max_shape[..., :2].type_as(x1)\n        if max_shape.ndim == 2:\n            assert bboxes.ndim == 3\n            assert max_shape.size(0) == bboxes.size(0)\n\n        min_xy = x1.new_tensor(0)\n        max_xy = torch.cat(\n            [max_shape] * (deltas.size(-1) // 2),\n            dim=-1).flip(-1).unsqueeze(-2)\n        bboxes = torch.where(bboxes < min_xy, min_xy, bboxes)\n        bboxes = torch.where(bboxes > max_xy, max_xy, bboxes)\n\n    return bboxes", ""]}
{"filename": "scripts/codes/004run_on_tensorrt.py", "chunked_list": ["import tensorrt as trt\nimport numpy as np\nimport os\nimport ctypes\n# cuda: https://nvidia.github.io/cuda-python/\n# pycuda: https://documen.tician.de/pycuda/\nimport pycuda.driver as cuda\nimport torch\nimport pycuda.autoinit\nimport glob", "import pycuda.autoinit\nimport glob\nimport mmcv\n\n\n# \u8bbe\u7f6e\u4e00\u4e9b\u5e38\u91cf\nepsilon = 1.0e-2\nnp.random.seed(97)\nlogger = trt.Logger(trt.Logger.ERROR)\ntrt.init_libnvinfer_plugins(logger, '')", "logger = trt.Logger(trt.Logger.ERROR)\ntrt.init_libnvinfer_plugins(logger, '')\nso_files = glob.glob(os.path.join('../relaventTensorRTPlugin/build', '*.so'))\n\nfor so_file in so_files:\n    ctypes.cdll.LoadLibrary(so_file)\n    print('load {} success!'.format(os.path.basename(so_file)))\n\ndef GiB(val):\n    return val * 1 << 30", "def GiB(val):\n    return val * 1 << 30\n\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n    \n    def free(self):\n        self.host = None\n        if self.device is not None:\n            self.device.free()\n            self.device = None\n    \n    def __del__(self):\n        self.free()\n    \n    def __str__(self):\n        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()", "    \n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(ori_inputs, ori_outputs, engine, context, stream):\n    inputs = []\n    outputs = []\n    bindings = []\n    nInput = np.sum([engine.binding_is_input(i) for i in range(engine.num_bindings)])\n    \n    for i, binding in enumerate(engine):\n        size = trt.volume(context.get_binding_shape(i))\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        try:\n            if engine.binding_is_input(binding):\n                ori_mem = ori_inputs[i]\n            else:\n                ori_mem = ori_outputs[i - nInput]\n        except:\n            ori_mem = None\n            \n        if ori_mem is not None:\n            if ori_mem.host.nbytes >= size:\n                host_mem = ori_mem.host\n                device_mem = ori_mem.device\n                # \u907f\u514d\u518d\u6b21\u91ca\u653e\n                ori_mem.device = None\n            else:\n                ori_mem.free()\n                host_mem = cuda.pagelocked_empty(size, dtype)\n                device_mem = cuda.mem_alloc(host_mem.nbytes)\n        else:\n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings", "\n\ndef build_engine(onnx_file_path, enable_fp16=False, max_batch_size=1, max_workspace_size=10, write_engine=True):\n    # \u901a\u8fc7\u52a0\u8f7donnx\u6587\u4ef6\uff0c\u6784\u5efaengine\n    # :param onnx_file_path: onnx\u6587\u4ef6\u8def\u5f84\n    # :return: engine\n    onnx_path = os.path.realpath(onnx_file_path) \n    engine_file_path = \".\".join(onnx_path.split('.')[:-1] + ['engine'])\n    print('engine_file_path', engine_file_path)\n    G_LOGGER = trt.Logger(trt.Logger.INFO)\n    if os.path.exists(engine_file_path):\n        with open(engine_file_path, 'rb') as f, trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(f.read())\n        return engine, engine_file_path\n    explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    with trt.Builder(G_LOGGER) as builder, builder.create_network(explicit_batch) as network, \\\n            trt.OnnxParser(network, G_LOGGER) as parser:\n        \n        config = builder.create_builder_config()\n        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, GiB(max_workspace_size))\n        if enable_fp16:\n            config.set_flag(trt.BuilderFlag.FP16)\n        print('Loading ONNX file from path {} ...'.format(onnx_file_path))\n        with open(onnx_file_path, 'rb') as model:\n            print('Beginning ONNX file parsing')\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                # print(\" parsing error:\", parser.get_error(0).code(), \"\\n\",\n                #       \"function name:\", parser.get_error(0).func(), \"\\n\",\n                #       \"node:\", parser.get_error(0).node(), \"\\n\",\n                #       \"line num:\", parser.get_error(0).line(), \"\\n\",\n                #       \"desc:\", parser.get_error(0).desc())\n                return None, None\n        print('Completed parsing of ONNX file')\n        print('Building an engine from file {}; this may take a while...'.format(onnx_file_path))\n        # \u91cd\u70b9\n        profile = builder.create_optimization_profile()\n        profile.set_shape(\"input\", (1, 3, 800, 1216), (max_batch_size, 3, 800, 1216), (max_batch_size, 3, 800, 1216))\n        config.add_optimization_profile(profile)\n\n        serialized_engine = builder.build_serialized_network(network, config)\n        if not serialized_engine:\n            return None, None\n        print(\"Completed creating Engine\")\n        # \u4fdd\u5b58engine\u6587\u4ef6\n        if write_engine:\n            with open(engine_file_path, \"wb\") as f:\n                f.write(serialized_engine)\n        with trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(serialized_engine)\n        return engine, engine_file_path", "\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]", "\nclass TRTMaskRCNN(object):\n    def __init__(self, engine_or_onnx_path):\n        self.engine_path = engine_or_onnx_path\n        self.logger = trt.Logger(trt.Logger.INFO)\n        self.engine = self._get_engine()\n        self.context = self.engine.create_execution_context()\n        self.stream = cuda.Stream()\n        self.inputs = None\n        self.outputs = None\n\n\n    def _get_engine(self):\n        # If a serialized engine exists, use it instead of building an engine.\n        return build_engine(self.engine_path, enable_fp16=False, max_batch_size=1, write_engine=True)[0]\n\n    def detect(self, image_np_array, cuda_ctx = pycuda.autoinit.context):\n        if cuda_ctx:\n            cuda_ctx.push()\n\n        batch_size = image_np_array.shape[0]\n        # \u52a8\u6001\u8f93\u5165\n        origin_inputshape = self.context.get_binding_shape(0)\n        origin_inputshape[0] = batch_size\n        self.context.set_binding_shape(0, (origin_inputshape))\n        self.context.set_optimization_profile_async(0, self.stream.handle)\n        \n        self.inputs, self.outputs, bindings = allocate_buffers(self.inputs, self.outputs, self.engine, self.context, self.stream)\n        np_type = trt.nptype(self.engine.get_binding_dtype(0))\n        # Do inference\n        self.inputs[0].host = np.ascontiguousarray(image_np_array.astype(np_type))\n        trt_outputs = do_inference(self.context, bindings=bindings, inputs=self.inputs, outputs=self.outputs,\n                                          stream=self.stream)\n        \n        if cuda_ctx:\n            cuda_ctx.pop()\n        \n        nInput = np.sum([self.engine.binding_is_input(i) for i in range(self.engine.num_bindings)])\n        nOutput = self.engine.num_bindings - nInput\n        trt_outputs_dict = dict()\n        \n        for i in range(nOutput):\n            shape = self.context.get_binding_shape(nInput + i)\n            name = self.engine.get_binding_name(nInput + i)\n            trt_outputs_dict[name] = trt_outputs[i].reshape(shape)\n        return trt_outputs_dict\n    \n    def __call__(self, x):\n        return self.detect(x)\n    \n    def __del__(self):\n        del self.inputs\n        del self.outputs\n        del self.stream\n        del self.engine\n        del self.context", "\n\ndef imgpath2pad_array(image_path):\n    \n    image = mmcv.imread(image_path, channel_order='rgb')\n    height, width = image.shape[:2]\n    dst_height, dst_width = 800, 1216\n    resize_scale = min(dst_height / height, dst_width / width)\n    new_height, new_width = round(height * resize_scale), round(width * resize_scale)\n    resize_image = mmcv.imresize(image, (new_width, new_height))\n    # pad_height = dst_height - new_height\n    # pad_width = dst_width - new_width\n    # ((before_1, after_1), ... (before_N, after_N))\n    # pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(image.shape) - 2))\n    return resize_image # np.pad(resize_image, pad_width = pad_val, mode='constant')", "\n\ndef array2tensor(array):\n    dst_height, dst_width = 800, 1216\n    height, width = array.shape[:2]\n    pad_height = dst_height - height\n    pad_width = dst_width - width\n    pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(array.shape) - 2))\n    mean = np.asarray([123.675, 116.28, 103.53], dtype=np.float32).reshape(1,1,3)\n    std = np.asarray([58.395, 57.12, 57.375], dtype=np.float32).reshape(1,1,3)\n    array = (array.astype(np.float32) - mean) / std\n    pad_array = np.pad(array, pad_width = pad_val, mode='constant')\n    return np.transpose(pad_array, axes=[2, 0, 1])", "\ndef main():\n    import mmdet\n\n    trt_maskrcnn = TRTMaskRCNN('../results/mask_rcnn_r50_fpn_2x_coco.onnx')\n\n\n    with open('onnx_output_dict.pkl', 'rb') as f:\n        onnx_outputs = torch.load(f, map_location='cpu')\n    \n    \n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n    cur_dir = os.path.dirname(__file__)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n\n\n    from PIL import Image\n    def get_image_size(filepath):\n        im = Image.open(filepath)\n        width, height = im.size\n        return (width, height)\n\n    image_array = imgpath2pad_array(jpg_image_path)\n    # img_metas = [\n    #     {\n    #         'img_shape':tuple(image_array.shape),\n    #         'ori_shape':get_image_size(jpg_image_path)\n    #     },\n    # ]\n    tensor = array2tensor(image_array)\n    tensor = np.ascontiguousarray(np.expand_dims(tensor, axis=0))\n\n    outputs = trt_maskrcnn.detect(tensor)\n    \n    for k in outputs:\n        diff = (outputs[k] - onnx_outputs[k])\n        print('key: {}, diff: {}'.format(k, np.abs(diff).max()))", "\n\nif __name__ == \"__main__\":\n    main()\n\n"]}
{"filename": "scripts/codes/007run_on_tensorrt_dynamic_shape_fp16.py", "chunked_list": ["import tensorrt as trt\nimport numpy as np\nimport os\nimport ctypes\n# cuda: https://nvidia.github.io/cuda-python/\n# pycuda: https://documen.tician.de/pycuda/\nimport pycuda.driver as cuda\nimport tensorrt as trt\nimport torch\nimport pycuda.autoinit", "import torch\nimport pycuda.autoinit\nimport glob\nimport mmcv\nimport onnx_graphsurgeon as gs\nimport onnx\nfrom mmdet.core import bbox2result\n\n\n# \u8bbe\u7f6e\u4e00\u4e9b\u5e38\u91cf", "\n# \u8bbe\u7f6e\u4e00\u4e9b\u5e38\u91cf\nepsilon = 1.0e-2\nnp.random.seed(97)\nlogger = trt.Logger(trt.Logger.ERROR)\ntrt.init_libnvinfer_plugins(logger, '')\nso_files = glob.glob(os.path.join('../relaventTensorRTPlugin/build', '*.so'))\n\nfor so_file in so_files:\n    ctypes.cdll.LoadLibrary(so_file)\n    print('load {} success!'.format(os.path.basename(so_file)))", "for so_file in so_files:\n    ctypes.cdll.LoadLibrary(so_file)\n    print('load {} success!'.format(os.path.basename(so_file)))\n\ndef GiB(val):\n    return val * 1 << 30\n\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n    \n    def free(self):\n        self.host = None\n        if self.device is not None:\n            self.device.free()\n            self.device = None\n    \n    def __del__(self):\n        self.free()\n    \n    def __str__(self):\n        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()", "    \n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(ori_inputs, ori_outputs, engine, context, stream):\n    inputs = []\n    outputs = []\n    bindings = []\n    nInput = np.sum([engine.binding_is_input(i) for i in range(engine.num_bindings)])\n    \n    for i, binding in enumerate(engine):\n        size = trt.volume(context.get_binding_shape(i))\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        try:\n            if engine.binding_is_input(binding):\n                ori_mem = ori_inputs[i]\n            else:\n                ori_mem = ori_outputs[i - nInput]\n        except:\n            ori_mem = None\n            \n        if ori_mem is not None:\n            if ori_mem.host.nbytes >= size:\n                host_mem = ori_mem.host\n                device_mem = ori_mem.device\n                # \u907f\u514d\u518d\u6b21\u91ca\u653e\n                ori_mem.device = None\n            else:\n                ori_mem.free()\n                host_mem = cuda.pagelocked_empty(size, dtype)\n                device_mem = cuda.mem_alloc(host_mem.nbytes)\n        else:\n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings", "\n\ndef build_engine(onnx_file_path, enable_fp16=False, max_batch_size=2, max_workspace_size=10, write_engine=True):\n    graph = gs.import_onnx(onnx.load(onnx_file_path))\n    precision_name_list = list()\n\n    for node in graph.nodes:\n        #  'Conv', 'Add', 'Sub', 'Mul', 'Exp', 'Sqrt', 'Log'\n        if node.op in ['Concat', 'Add',]:\n            output = node.outputs[0]\n            if output.dtype == np.float32:\n                precision_name_list.append(node.name)\n\n    # exit(0)\n    # \u901a\u8fc7\u52a0\u8f7donnx\u6587\u4ef6\uff0c\u6784\u5efaengine\n    # :param onnx_file_path: onnx\u6587\u4ef6\u8def\u5f84\n    # :return: engine\n    onnx_path = os.path.realpath(onnx_file_path) \n    engine_file_path = \".\".join(onnx_path.split('.')[:-1] + ['engine' if not enable_fp16 else 'fp16.engine'])\n    print('engine_file_path', engine_file_path)\n    G_LOGGER = trt.Logger(trt.Logger.WARNING)\n    if os.path.exists(engine_file_path):\n        with open(engine_file_path, 'rb') as f, trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(f.read())\n        return engine, engine_file_path\n    explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    with trt.Builder(G_LOGGER) as builder, builder.create_network(explicit_batch) as network, \\\n            trt.OnnxParser(network, G_LOGGER) as parser:\n        \n        config = builder.create_builder_config()\n        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, GiB(max_workspace_size))\n        if enable_fp16 and builder.platform_has_fast_fp16:\n            config.set_flag(trt.BuilderFlag.FP16)\n            config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n        print('Loading ONNX file from path {} ...'.format(onnx_file_path))\n        \n\n        with open(onnx_file_path, 'rb') as model:\n            print('Beginning ONNX file parsing')\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return None, None\n        print('Completed parsing of ONNX file')\n        print('Building an engine from file {}; this may take a while...'.format(onnx_file_path))\n        # \u91cd\u70b9\n        profile = builder.create_optimization_profile()\n        profile.set_shape(\"input\", (1, 3, 800, 1216), (max_batch_size, 3, 800, 1216), (max_batch_size, 3, 800, 1216))\n        config.add_optimization_profile(profile)\n\n        if enable_fp16 and builder.platform_has_fast_fp16:\n            for i in range(network.num_layers):\n                \n                layer = network.get_layer(i)\n\n                layer_type = layer.type\n                \n                if layer_type in (trt.LayerType.SHAPE, trt.LayerType.SLICE,\n                                  trt.LayerType.IDENTITY,\n                                  trt.LayerType.SHUFFLE, trt.LayerType.RESIZE):\n                    print(f'{layer.name} passed 1')\n                    continue\n                \n                layer_output_precision = layer.get_output(0).dtype\n                print(f'layer_name: {layer.name}, layer_output_precision: {layer_output_precision}')\n\n                if layer_output_precision in (trt.int32, trt.int8, trt.bool):\n                    print(f'{layer.name} passed 2')\n                    continue\n                \n                \n                if layer.name in precision_name_list:\n                    print(f'layer {layer.name} set fp32 precision mode')\n                    # layer.precision = trt.float32\n                    layer.set_output_type(0, trt.float32)\n                    layer.precision = trt.float32\n        \n        serialized_engine = builder.build_serialized_network(network, config)\n        if not serialized_engine:\n            return None, None\n        print(\"Completed creating Engine\")\n        # \u4fdd\u5b58engine\u6587\u4ef6\n        if write_engine:\n            with open(engine_file_path, \"wb\") as f:\n                f.write(serialized_engine)\n        with trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(serialized_engine)\n        return engine, engine_file_path", "\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]", "\nclass TRTMaskRCNN(object):\n    def __init__(self, engine_or_onnx_path):\n        self.engine_path = engine_or_onnx_path\n        self.logger = trt.Logger(trt.Logger.INFO)\n        self.engine = self._get_engine()\n        self.context = self.engine.create_execution_context()\n        self.stream = cuda.Stream()\n        self.inputs = None\n        self.outputs = None\n\n\n    def _get_engine(self):\n        # If a serialized engine exists, use it instead of building an engine.\n        return build_engine(self.engine_path, enable_fp16=True, max_batch_size=4, write_engine=True)[0]\n\n    def detect(self, image_np_array, cuda_ctx = pycuda.autoinit.context):\n        if cuda_ctx:\n            cuda_ctx.push()\n\n        batch_size = image_np_array.shape[0]\n        # \u52a8\u6001\u8f93\u5165\n        origin_inputshape = self.context.get_binding_shape(0)\n        origin_inputshape[0] = batch_size\n        self.context.set_binding_shape(0, (origin_inputshape))\n        self.context.set_optimization_profile_async(0, self.stream.handle)\n        \n        self.inputs, self.outputs, bindings = allocate_buffers(self.inputs, self.outputs, self.engine, self.context, self.stream)\n        np_type = trt.nptype(self.engine.get_binding_dtype(0))\n        # Do inference\n        self.inputs[0].host = np.ascontiguousarray(image_np_array.astype(np_type))\n        trt_outputs = do_inference(self.context, bindings=bindings, inputs=self.inputs, outputs=self.outputs,\n                                          stream=self.stream)\n        \n        if cuda_ctx:\n            cuda_ctx.pop()\n        \n        nInput = np.sum([self.engine.binding_is_input(i) for i in range(self.engine.num_bindings)])\n        nOutput = self.engine.num_bindings - nInput\n        trt_outputs_dict = dict()\n        \n        for i in range(nOutput):\n            shape = self.context.get_binding_shape(nInput + i)\n            name = self.engine.get_binding_name(nInput + i)\n            trt_outputs_dict[name] = trt_outputs[i].reshape(shape)\n        return trt_outputs_dict\n    \n    def __call__(self, x):\n        return self.detect(x)\n    \n    def __del__(self):\n        del self.inputs\n        del self.outputs\n        del self.stream\n        del self.engine\n        del self.context", "\n\ndef imgpath2pad_array(image_path):\n    \n    image = mmcv.imread(image_path, channel_order='rgb')\n    height, width = image.shape[:2]\n    dst_height, dst_width = 800, 1216\n    resize_scale = min(dst_height / height, dst_width / width)\n    new_height, new_width = round(height * resize_scale), round(width * resize_scale)\n    resize_image = mmcv.imresize(image, (new_width, new_height))\n    # pad_height = dst_height - new_height\n    # pad_width = dst_width - new_width\n    # ((before_1, after_1), ... (before_N, after_N))\n    # pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(image.shape) - 2))\n    return resize_image # np.pad(resize_image, pad_width = pad_val, mode='constant')", "\n\ndef array2tensor(array):\n    dst_height, dst_width = 800, 1216\n    height, width = array.shape[:2]\n    pad_height = dst_height - height\n    pad_width = dst_width - width\n    pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(array.shape) - 2))\n    mean = np.asarray([123.675, 116.28, 103.53], dtype=np.float32).reshape(1,1,3)\n    std = np.asarray([58.395, 57.12, 57.375], dtype=np.float32).reshape(1,1,3)\n    array = (array.astype(np.float32) - mean) / std\n    pad_array = np.pad(array, pad_width = pad_val, mode='constant')\n    return np.transpose(pad_array, axes=[2, 0, 1])", "\ndef main():\n    import mmdet\n\n    trt_maskrcnn = TRTMaskRCNN('../results/mask_rcnn_r50_fpn_2x_coco_dynamic_shape.onnx')\n\n\n    with open('onnx_output_dict.pkl', 'rb') as f:\n        onnx_outputs = torch.load(f, map_location='cpu')\n    \n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n    cur_dir = os.path.dirname(__file__)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n\n\n    from PIL import Image\n    def get_image_size(filepath):\n        im = Image.open(filepath)\n        width, height = im.size\n        return (height, width)\n\n    image_array = imgpath2pad_array(jpg_image_path)\n    tensor = array2tensor(image_array)\n    tensor = np.ascontiguousarray(np.repeat(np.expand_dims(tensor, axis=0), repeats=2, axis=0))\n\n    outputs = trt_maskrcnn.detect(tensor)\n    \n    for k in outputs:\n        val = outputs[k]\n        if np.any(np.isnan(val)):\n            print(k)\n\n\n    from mmdet.core.export.model_wrappers import TensorRTDetector\n    import pickle\n\n    with open('./class_names.pickle', 'rb') as f:\n        class_names = pickle.load(f)\n    trt_model = TensorRTDetector(trt_maskrcnn.engine, class_names, 0)\n    score_thr = 0.3\n    # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n    img_metas = [\n        {\n            'img_shape':tuple(image_array.shape),\n            'ori_shape':get_image_size(jpg_image_path)\n        },\n    ] * 2\n\n    batch_dets = outputs['dets']\n    batch_labels = outputs['labels']\n    batch_masks = outputs['masks']\n    batch_size = batch_dets.shape[0]\n    results = []\n    for i in range(batch_size):\n        # [N, 5], [N]\n        dets, labels = batch_dets[i], batch_labels[i]\n        det_mask = dets[:, -1] >= score_thr\n        dets = dets[det_mask]\n        labels = labels[det_mask]\n\n        \n\n        # [N, 800, 1216]\n        masks = batch_masks[i]\n        masks = masks[det_mask]\n        img_h, img_w = img_metas[i]['img_shape'][:2]\n        ori_h, ori_w = img_metas[i]['ori_shape'][:2]\n\n        scale_factor_w = img_w / ori_w\n        scale_factor_h = img_h / ori_h\n        scale_factor = np.array([scale_factor_w, scale_factor_h, scale_factor_w, scale_factor_h])\n        dets[:, :4] /= scale_factor\n\n        dets_results = bbox2result(dets, labels, len(class_names))\n        # \u53bb\u9664 padding\n        masks = masks[:, :img_h, :img_w]\n        if True:\n            masks = masks.astype(np.float32)\n            masks = torch.from_numpy(masks)\n            masks = torch.nn.functional.interpolate(masks.unsqueeze(0), size=(ori_h, ori_w))\n            masks = masks.squeeze(0).detach().numpy()\n\n        if masks.dtype != bool:\n            masks = masks >= 0.5\n        segms_results = [[] for _ in range(len(class_names))]\n        for j in range(len(dets)):\n            segms_results[labels[j]].append(masks[j])\n        results.append((dets_results, segms_results))\n    \n\n    os.makedirs('../results/', exist_ok=True)\n    trt_model.show_result(\n        jpg_image_path,\n        results[0],\n        score_thr=score_thr,\n        show=True,\n        win_name='TRT_FP16',\n        out_file='../results/trt_fp16_result.png')\n\n    for k in outputs:\n        diff = (outputs[k] - onnx_outputs[k])\n        print('key: {}, shape: {}, diff: {}'.format(k, outputs[k].shape, np.abs(diff).max()))", "\n\nif __name__ == \"__main__\":\n    main()\n\n"]}
{"filename": "scripts/codes/006run_on_tensorrt_dynamic_shape.py", "chunked_list": ["import tensorrt as trt\nimport numpy as np\nimport os\nimport ctypes\n# cuda: https://nvidia.github.io/cuda-python/\n# pycuda: https://documen.tician.de/pycuda/\nimport pycuda.driver as cuda\nimport tensorrt as trt\nimport torch\nimport pycuda.autoinit", "import torch\nimport pycuda.autoinit\nimport glob\nimport mmcv\n\n\n# \u8bbe\u7f6e\u4e00\u4e9b\u5e38\u91cf\nepsilon = 1.0e-2\nnp.random.seed(97)\nlogger = trt.Logger(trt.Logger.ERROR)", "np.random.seed(97)\nlogger = trt.Logger(trt.Logger.ERROR)\ntrt.init_libnvinfer_plugins(logger, '')\nso_files = glob.glob(os.path.join('../relaventTensorRTPlugin/build', '*.so'))\n\nfor so_file in so_files:\n    ctypes.cdll.LoadLibrary(so_file)\n    print('load {} success!'.format(os.path.basename(so_file)))\n\ndef GiB(val):\n    return val * 1 << 30", "\ndef GiB(val):\n    return val * 1 << 30\n\nclass HostDeviceMem(object):\n    def __init__(self, host_mem, device_mem):\n        self.host = host_mem\n        self.device = device_mem\n    \n    def free(self):\n        self.host = None\n        if self.device is not None:\n            self.device.free()\n            self.device = None\n    \n    def __del__(self):\n        self.free()\n    \n    def __str__(self):\n        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n\n    def __repr__(self):\n        return self.__str__()", "    \n\n# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\ndef allocate_buffers(ori_inputs, ori_outputs, engine, context, stream):\n    inputs = []\n    outputs = []\n    bindings = []\n    nInput = np.sum([engine.binding_is_input(i) for i in range(engine.num_bindings)])\n    \n    for i, binding in enumerate(engine):\n        size = trt.volume(context.get_binding_shape(i))\n        dtype = trt.nptype(engine.get_binding_dtype(binding))\n        try:\n            if engine.binding_is_input(binding):\n                ori_mem = ori_inputs[i]\n            else:\n                ori_mem = ori_outputs[i - nInput]\n        except:\n            ori_mem = None\n            \n        if ori_mem is not None:\n            if ori_mem.host.nbytes >= size:\n                host_mem = ori_mem.host\n                device_mem = ori_mem.device\n                # \u907f\u514d\u518d\u6b21\u91ca\u653e\n                ori_mem.device = None\n            else:\n                ori_mem.free()\n                host_mem = cuda.pagelocked_empty(size, dtype)\n                device_mem = cuda.mem_alloc(host_mem.nbytes)\n        else:\n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n        # Append the device buffer to device bindings.\n        bindings.append(int(device_mem))\n        # Append to the appropriate list.\n        if engine.binding_is_input(binding):\n            inputs.append(HostDeviceMem(host_mem, device_mem))\n        else:\n            outputs.append(HostDeviceMem(host_mem, device_mem))\n    return inputs, outputs, bindings", "\n\ndef build_engine(onnx_file_path, enable_fp16 = False, max_batch_size = 1, max_workspace_size = 10, write_engine=True):\n    # \u901a\u8fc7\u52a0\u8f7donnx\u6587\u4ef6\uff0c\u6784\u5efaengine\n    # :param onnx_file_path: onnx\u6587\u4ef6\u8def\u5f84\n    # :return: engine\n    onnx_path = os.path.realpath(onnx_file_path) \n    engine_file_path = \".\".join(onnx_path.split('.')[:-1] + ['engine' if not enable_fp16 else 'fp16.engine'])\n    print('engine_file_path', engine_file_path)\n    G_LOGGER = trt.Logger(trt.Logger.INFO)\n    if os.path.exists(engine_file_path):\n        with open(engine_file_path, 'rb') as f, trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(f.read())\n        return engine, engine_file_path\n    explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    with trt.Builder(G_LOGGER) as builder, builder.create_network(explicit_batch) as network, \\\n            trt.OnnxParser(network, G_LOGGER) as parser:\n        \n        config = builder.create_builder_config()\n        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, GiB(max_workspace_size))\n        if enable_fp16:\n            config.set_flag(trt.BuilderFlag.FP16)\n        print('Loading ONNX file from path {} ...'.format(onnx_file_path))\n        with open(onnx_file_path, 'rb') as model:\n            print('Beginning ONNX file parsing')\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                # print(\" parsing error:\", parser.get_error(0).code(), \"\\n\",\n                #       \"function name:\", parser.get_error(0).func(), \"\\n\",\n                #       \"node:\", parser.get_error(0).node(), \"\\n\",\n                #       \"line num:\", parser.get_error(0).line(), \"\\n\",\n                #       \"desc:\", parser.get_error(0).desc())\n                return None, None\n        print('Completed parsing of ONNX file')\n        print('Building an engine from file {}; this may take a while...'.format(onnx_file_path))\n        # \u91cd\u70b9\n        profile = builder.create_optimization_profile()\n        profile.set_shape(\"input\", (1, 3, 800, 1216), (max_batch_size, 3, 800, 1216), (max_batch_size, 3, 800, 1216))\n        config.add_optimization_profile(profile)\n\n        serialized_engine = builder.build_serialized_network(network, config)\n        if not serialized_engine:\n            return None, None\n        print(\"Completed creating Engine\")\n        # \u4fdd\u5b58engine\u6587\u4ef6\n        if write_engine:\n            with open(engine_file_path, \"wb\") as f:\n                f.write(serialized_engine)\n        with trt.Runtime(G_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(serialized_engine)\n        return engine, engine_file_path", "\n\n# This function is generalized for multiple inputs/outputs.\n# inputs and outputs are expected to be lists of HostDeviceMem objects.\ndef do_inference(context, bindings, inputs, outputs, stream):\n    # Transfer input data to the GPU.\n    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n    # Run inference.\n    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n    # Transfer predictions back from the GPU.\n    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n    # Synchronize the stream\n    stream.synchronize()\n    # Return only the host outputs.\n    return [out.host for out in outputs]", "\nclass TRTMaskRCNN(object):\n    def __init__(self, engine_or_onnx_path):\n        self.engine_path = engine_or_onnx_path\n        self.logger = trt.Logger(trt.Logger.INFO)\n        self.engine = self._get_engine()\n        self.context = self.engine.create_execution_context()\n        self.stream = cuda.Stream()\n        self.inputs = None\n        self.outputs = None\n\n\n    def _get_engine(self):\n        # If a serialized engine exists, use it instead of building an engine.\n        return build_engine(self.engine_path, enable_fp16=False, max_batch_size=4, write_engine=True)[0]\n\n    def detect(self, image_np_array, cuda_ctx = pycuda.autoinit.context):\n        if cuda_ctx:\n            cuda_ctx.push()\n\n        batch_size = image_np_array.shape[0]\n        # \u52a8\u6001\u8f93\u5165\n        origin_inputshape = self.context.get_binding_shape(0)\n        origin_inputshape[0] = batch_size\n        self.context.set_binding_shape(0, (origin_inputshape))\n        self.context.set_optimization_profile_async(0, self.stream.handle)\n        \n        self.inputs, self.outputs, bindings = allocate_buffers(self.inputs, self.outputs, self.engine, self.context, self.stream)\n        np_type = trt.nptype(self.engine.get_binding_dtype(0))\n        # Do inference\n        self.inputs[0].host = np.ascontiguousarray(image_np_array.astype(np_type))\n        trt_outputs = do_inference(self.context, bindings=bindings, inputs=self.inputs, outputs=self.outputs,\n                                          stream=self.stream)\n        \n        if cuda_ctx:\n            cuda_ctx.pop()\n        \n        nInput = np.sum([self.engine.binding_is_input(i) for i in range(self.engine.num_bindings)])\n        nOutput = self.engine.num_bindings - nInput\n        trt_outputs_dict = dict()\n        \n        for i in range(nOutput):\n            shape = self.context.get_binding_shape(nInput + i)\n            name = self.engine.get_binding_name(nInput + i)\n            trt_outputs_dict[name] = trt_outputs[i].reshape(shape)\n        return trt_outputs_dict\n    \n    def __call__(self, x):\n        return self.detect(x)\n    \n    def __del__(self):\n        del self.inputs\n        del self.outputs\n        del self.stream\n        del self.engine\n        del self.context", "\n\ndef imgpath2pad_array(image_path):\n    \n    image = mmcv.imread(image_path, channel_order='rgb')\n    height, width = image.shape[:2]\n    dst_height, dst_width = 800, 1216\n    resize_scale = min(dst_height / height, dst_width / width)\n    new_height, new_width = round(height * resize_scale), round(width * resize_scale)\n    resize_image = mmcv.imresize(image, (new_width, new_height))\n    # pad_height = dst_height - new_height\n    # pad_width = dst_width - new_width\n    # ((before_1, after_1), ... (before_N, after_N))\n    # pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(image.shape) - 2))\n    return resize_image # np.pad(resize_image, pad_width = pad_val, mode='constant')", "\n\ndef array2tensor(array):\n    dst_height, dst_width = 800, 1216\n    height, width = array.shape[:2]\n    pad_height = dst_height - height\n    pad_width = dst_width - width\n    pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(array.shape) - 2))\n    mean = np.asarray([123.675, 116.28, 103.53], dtype=np.float32).reshape(1,1,3)\n    std = np.asarray([58.395, 57.12, 57.375], dtype=np.float32).reshape(1,1,3)\n    array = (array.astype(np.float32) - mean) / std\n    pad_array = np.pad(array, pad_width = pad_val, mode='constant')\n    return np.transpose(pad_array, axes=[2, 0, 1])", "\ndef main():\n    import mmdet\n\n    trt_maskrcnn = TRTMaskRCNN('../results/mask_rcnn_r50_fpn_2x_coco_dynamic_shape.onnx')\n\n\n    with open('onnx_output_dict.pkl', 'rb') as f:\n        onnx_outputs = torch.load(f, map_location='cpu')\n    \n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n    cur_dir = os.path.dirname(__file__)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n\n\n    from PIL import Image\n    def get_image_size(filepath):\n        im = Image.open(filepath)\n        width, height = im.size\n        return (width, height)\n\n    image_array = imgpath2pad_array(jpg_image_path)\n    # img_metas = [\n    #     {\n    #         'img_shape':tuple(image_array.shape),\n    #         'ori_shape':get_image_size(jpg_image_path)\n    #     },\n    # ]\n    tensor = array2tensor(image_array)\n    tensor = np.ascontiguousarray(np.repeat(np.expand_dims(tensor, axis=0), repeats=2, axis=0))\n\n    outputs = trt_maskrcnn.detect(tensor)\n    \n    for k in outputs:\n        diff = (outputs[k] - onnx_outputs[k])\n        print('key: {}, shape: {}, diff: {}'.format(k, outputs[k].shape, np.abs(diff).max()))", "\n\nif __name__ == \"__main__\":\n    main()\n\n"]}
{"filename": "scripts/codes/001infer_maskrcnn_mmdet_api.py", "chunked_list": ["from mmdet.apis import init_detector, inference_detector\nimport os\nimport glob\nimport mmdet\n\n\ndirpath = os.path.dirname\npath_join = os.path.join\nBASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\nMMDET_DIR = dirpath(dirpath(mmdet.__file__))", "BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\nMMDET_DIR = dirpath(dirpath(mmdet.__file__))\nprint('BASE_DIR', BASE_DIR)\nprint('MMDET_DIR', MMDET_DIR)\n\nconfig_file = path_join(BASE_DIR, 'config', 'mask_rcnn_r50_fpn_2x_coco.py')\ncheckpoint_file = path_join(BASE_DIR, 'pretrained_model', 'mask_rcnn_r50_fpn_2x_coco_*.pth')\ncheckpoint_file = glob.glob(checkpoint_file)[0]\n\nmodel = init_detector(config_file, checkpoint_file, device='cpu')", "\nmodel = init_detector(config_file, checkpoint_file, device='cpu')\njpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\ndet_result = inference_detector(model, jpg_image_path)\n\nbasename_woext, ext = os.path.splitext(os.path.basename(jpg_image_path))\nresult_pred_path = path_join('../results', basename_woext + '_result' + ext)\nos.makedirs(dirpath(result_pred_path), exist_ok=True)\n\nmodel.show_result(jpg_image_path, det_result, out_file=result_pred_path)", "\nmodel.show_result(jpg_image_path, det_result, out_file=result_pred_path)\n"]}
{"filename": "scripts/codes/002export_onnx.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport argparse\nimport os.path as osp\nimport warnings\nfrom functools import partial\n\nimport numpy as np\nimport onnx\nimport torch\nfrom mmcv import Config, DictAction", "import torch\nfrom mmcv import Config, DictAction\n\nfrom mmdet.core.export import build_model_from_cfg, preprocess_example_input\nfrom mmdet.core.export.model_wrappers import ONNXRuntimeDetector\n\n\ndef pytorch2onnx(model,\n                 input_img,\n                 input_shape,\n                 normalize_cfg,\n                 opset_version=11,\n                 show=False,\n                 output_file='tmp.onnx',\n                 verify=False,\n                 test_img=None,\n                 do_simplify=False,\n                 dynamic_export=None,\n                 skip_postprocess=False,\n                 force_write=False):\n    \"\"\"\n    force_write\u8868\u793a \u65e0\u8bbaonnx\u6587\u4ef6\u662f\u5426\u5b58\u5728\u90fd\u4f1a\u5f3a\u5236\u66f4\u65b0onnx\n    \"\"\"\n    input_config = {\n        'input_shape': input_shape,\n        'input_path': input_img,\n        'normalize_cfg': normalize_cfg\n    }\n    # prepare input\n    one_img, one_meta = preprocess_example_input(input_config)\n    img_list, img_meta_list = [one_img], [[one_meta]]\n    \n    if skip_postprocess:\n        warnings.warn('Not all models support export onnx without post '\n                      'process, especially two stage detectors!')\n        origin_forward = model.forward\n        model.forward = model.forward_dummy\n        torch.onnx.export(\n            model,\n            one_img,\n            output_file,\n            input_names=['input'],\n            export_params=True,\n            keep_initializers_as_inputs=True,\n            do_constant_folding=True,\n            verbose=show,\n            opset_version=opset_version)\n        model.forward = origin_forward\n        print(f'Successfully exported ONNX model without '\n              f'post process: {output_file}')\n        # return\n    else:\n        # replace original forward function\n        origin_forward = model.forward\n        model.forward = partial(\n            model.forward,\n            img_metas=img_meta_list,\n            return_loss=False,\n            rescale=False)\n\n        output_names = ['dets', 'labels']\n        if model.with_mask:\n            output_names.append('masks')\n        input_name = 'input'\n        dynamic_axes = None\n        if dynamic_export:\n            dynamic_axes = {\n                input_name: {\n                    0: 'batch',\n                },\n                'dets': {\n                    0: 'batch',\n                    1: 'num_dets',\n                },\n                'labels': {\n                    0: 'batch',\n                    1: 'num_dets',\n                },\n            }\n            if model.with_mask:\n                dynamic_axes['masks'] = {0: 'batch', 1: 'num_dets'}\n        if not os.path.exists(output_file) or force_write:\n            torch.onnx.export(\n                model,\n                img_list,\n                output_file,\n                input_names=[input_name],\n                output_names=output_names,\n                export_params=True,\n                keep_initializers_as_inputs=True,\n                do_constant_folding=True,\n                verbose=show,\n                opset_version=opset_version,\n                dynamic_axes=dynamic_axes)\n\n        model.forward = origin_forward\n\n    if do_simplify:\n        import onnxsim\n\n        from mmdet import digit_version\n\n        min_required_version = '0.4.0'\n        assert digit_version(onnxsim.__version__) >= digit_version(\n            min_required_version\n        ), f'Requires to install onnxsim>={min_required_version}'\n\n        model_opt, check_ok = onnxsim.simplify(output_file)\n        if check_ok:\n            onnx.save(model_opt, output_file)\n            print(f'Successfully simplified ONNX model: {output_file}')\n        else:\n            warnings.warn('Failed to simplify ONNX model.')\n    print(f'Successfully exported ONNX model: {output_file}')\n\n    if verify:\n        # check by onnx\n        onnx_model = onnx.load(output_file)\n        onnx.checker.check_model(onnx_model)\n        \n        # wrap onnx model\n        onnx_model = ONNXRuntimeDetector(output_file, model.CLASSES, 0)\n        if dynamic_export:\n            # scale up to test dynamic shape\n            h, w = [int((_ * 1.5) // 32 * 32) for _ in input_shape[2:]]\n            h, w = min(1344, h), min(1344, w)\n            input_config['input_shape'] = (1, 3, h, w)\n\n        if test_img is None:\n            input_config['input_path'] = input_img\n\n        # prepare input once again\n        one_img, one_meta = preprocess_example_input(input_config)\n        img_list, img_meta_list = [one_img], [[one_meta]]\n\n        # get pytorch output\n        with torch.no_grad():\n            pytorch_results = model(\n                img_list,\n                img_metas=img_meta_list,\n                return_loss=False,\n                rescale=True)[0]\n        \n\n        img_list = [_.cuda().contiguous() for _ in img_list]\n        if dynamic_export:\n            img_list = img_list + [_.flip(-1).contiguous() for _ in img_list]\n            img_meta_list = img_meta_list * 2\n        # get onnx output\n        onnx_results = onnx_model(\n            img_list, img_metas=img_meta_list, return_loss=False, rescale=True)[0]\n        # visualize predictions\n        score_thr = 0.3\n\n        out_file_ort, out_file_pt = 'show-ort.png', 'show-pt.png'\n\n        show_img = one_meta['show_img']\n        model.show_result(\n            show_img,\n            pytorch_results,\n            score_thr=score_thr,\n            show=True,\n            win_name='PyTorch',\n            out_file=out_file_pt)\n        onnx_model.show_result(\n            show_img,\n            onnx_results,\n            score_thr=score_thr,\n            show=True,\n            win_name='ONNXRuntime',\n            out_file=out_file_ort)\n\n        # compare a part of result\n        if model.with_mask:\n            compare_pairs = list(zip(onnx_results, pytorch_results))\n        else:\n            compare_pairs = [(onnx_results, pytorch_results)]\n        err_msg = 'The numerical values are different between Pytorch' + \\\n                  ' and ONNX, but it does not necessarily mean the' + \\\n                  ' exported ONNX model is problematic.'\n        # check the numerical value\n        for onnx_res, pytorch_res in compare_pairs:\n            for o_res, p_res in zip(onnx_res, pytorch_res):\n                \n                try:\n                    np.testing.assert_allclose(\n                        o_res, p_res, rtol=0.01, atol=0.1, err_msg=err_msg)\n                except AssertionError as e:\n                    print(e)\n                    print('onnxruntime')\n                    print(o_res)\n                    print('pytorch')\n                    print(p_res)\n                    if hasattr(o_res, 'shape') and hasattr(p_res, 'shape'):\n                        print('onnxruntime', o_res.shape, 'pytorch', p_res.shape)\n                    \n        print('The numerical values are the same between Pytorch and ONNX')", "\n\ndef parse_normalize_cfg(test_pipeline):\n    transforms = None\n    for pipeline in test_pipeline:\n        if 'transforms' in pipeline:\n            transforms = pipeline['transforms']\n            break\n    assert transforms is not None, 'Failed to find `transforms`'\n    norm_config_li = [_ for _ in transforms if _['type'] == 'Normalize']\n    assert len(norm_config_li) == 1, '`norm_config` should only have one'\n    norm_config = norm_config_li[0]\n    return norm_config", "\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert MMDetection models to ONNX')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument('--input-img', type=str, help='Images for input')\n    parser.add_argument(\n        '--show',\n        action='store_true',\n        help='Show onnx graph and detection outputs')\n    parser.add_argument('--output-file', type=str, default='tmp.onnx')\n    parser.add_argument('--opset-version', type=int, default=11)\n    parser.add_argument(\n        '--test-img', type=str, default=None, help='Images for test')\n    parser.add_argument(\n        '--dataset',\n        type=str,\n        default='coco',\n        help='Dataset name. This argument is deprecated and will be removed \\\n        in future releases.')\n    parser.add_argument(\n        '--verify',\n        action='store_true',\n        help='verify the onnx model output against pytorch output')\n    parser.add_argument(\n        '--simplify',\n        action='store_true',\n        help='Whether to simplify onnx model.')\n    parser.add_argument(\n        '--shape',\n        type=int,\n        nargs='+',\n        default=[800, 1216],\n        help='input image size')\n    parser.add_argument(\n        '--mean',\n        type=float,\n        nargs='+',\n        default=[123.675, 116.28, 103.53],\n        help='mean value used for preprocess input data.This argument \\\n        is deprecated and will be removed in future releases.')\n    parser.add_argument(\n        '--std',\n        type=float,\n        nargs='+',\n        default=[58.395, 57.12, 57.375],\n        help='variance value used for preprocess input data. '\n        'This argument is deprecated and will be removed in future releases.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='Override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--dynamic-export',\n        action='store_true',\n        help='Whether to export onnx with dynamic axis.')\n    parser.add_argument(\n        '--skip-postprocess',\n        action='store_true',\n        help='Whether to export model without post process. Experimental '\n        'option. We do not guarantee the correctness of the exported '\n        'model.')\n    args = parser.parse_args()\n    return args", "\n\nif __name__ == '__main__':\n    import os\n    import glob\n    import mmdet\n\n\n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n\n    config_file = path_join(BASE_DIR, 'config', 'mask_rcnn_r50_fpn_2x_coco.py')\n    checkpoint_file = path_join(BASE_DIR, 'pretrained_model', 'mask_rcnn_r50_fpn_2x_coco_*.pth')\n    checkpoint_file = glob.glob(checkpoint_file)[0]\n\n    opset_version = 11\n    try:\n        from mmcv.onnx.symbolic import register_extra_symbolics\n    except ModuleNotFoundError:\n        raise NotImplementedError('please update mmcv to version>=v1.0.4')\n    register_extra_symbolics(opset_version)\n\n    cfg = Config.fromfile(config_file)\n    img_scale = [800, 1216]\n    input_shape = (1, 3, img_scale[0], img_scale[1])\n\n\n    # build the model and load checkpoint\n    model = build_model_from_cfg(config_file, checkpoint_file)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n    normalize_cfg = parse_normalize_cfg(cfg.test_pipeline)\n    basename_woext, ext = os.path.splitext(os.path.basename(config_file))\n\n    onnx_path = os.path.join('..', 'results', basename_woext+'.onnx')\n    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n    # convert model to onnx file\n    pytorch2onnx(\n        model,\n        jpg_image_path,\n        input_shape,\n        normalize_cfg,\n        opset_version=opset_version,\n        show=True,\n        output_file=onnx_path,\n        verify=True,\n        test_img=None,\n        do_simplify=True,\n        dynamic_export=False,\n        skip_postprocess=False,\n        force_write=True)", ""]}
{"filename": "scripts/codes/003run_on_onnx.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport mmcv\nimport numpy as np\nfrom mmcv import Config\nfrom mmdet.datasets.pipelines import Compose\nfrom mmdet.core import bbox2result\nfrom mmdet.datasets import replace_ImageToTensor\nimport onnxruntime as rt\nimport warnings\nimport os", "import warnings\nimport os\nimport torch\n# from mmdet.datasets.builder import bui\n\n\ndef imgpath2pad_array(image_path):\n    \n    image = mmcv.imread(image_path, channel_order='rgb')\n    height, width = image.shape[:2]\n    dst_height, dst_width = 800, 1216\n    resize_scale = min(dst_height / height, dst_width / width)\n    new_height, new_width = round(height * resize_scale), round(width * resize_scale)\n    resize_image = mmcv.imresize(image, (new_width, new_height))\n    # pad_height = dst_height - new_height\n    # pad_width = dst_width - new_width\n    # ((before_1, after_1), ... (before_N, after_N))\n    # pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(image.shape) - 2))\n    return resize_image # np.pad(resize_image, pad_width = pad_val, mode='constant')", "\n\n# pad numpy array and transpose it to tensor memory map\ndef array2tensor(array):\n    dst_height, dst_width = 800, 1216\n    height, width = array.shape[:2]\n    pad_height = dst_height - height\n    pad_width = dst_width - width\n    pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(array.shape) - 2))\n    mean = np.asarray([123.675, 116.28, 103.53], dtype=np.float32).reshape(1,1,3)\n    std = np.asarray([58.395, 57.12, 57.375], dtype=np.float32).reshape(1,1,3)\n    array = (array.astype(np.float32) - mean) / std\n    pad_array = np.pad(array, pad_width = pad_val, mode='constant')\n    return np.transpose(pad_array, axes=[2, 0, 1])", "\n\n\ndef get_onnx_runner(onnx_filepath):\n    # \u6ce8\u518c\u81ea\u5b9a\u4e49\u7b97\u5b50\n    # get the custom op path\n    ort_custom_op_path = ''\n    try:\n        from mmcv.ops import get_onnxruntime_op_path\n        ort_custom_op_path = get_onnxruntime_op_path()\n    except (ImportError, ModuleNotFoundError):\n        warnings.warn('If input model has custom op from mmcv, \\\n            you may have to build mmcv with ONNXRuntime from source.')\n    session_options = rt.SessionOptions()\n    # register custom op for onnxruntime\n    if os.path.exists(ort_custom_op_path):\n        session_options.register_custom_ops_library(ort_custom_op_path)\n\n    sess = rt.InferenceSession(onnx_filepath, session_options)\n    input_names = [item.name for item in sess.get_inputs()]\n    label_names = [item.name for item in sess.get_outputs()]\n    def runner(input_tensors):\n        nonlocal label_names\n        nonlocal input_names\n        pred_onnx = sess.run(label_names, dict(zip(input_names, input_tensors)))\n        return dict(zip(label_names,pred_onnx))\n    return runner", "\nif __name__ == \"__main__\":\n    import os\n    import mmdet\n\n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n\n    cur_dir = os.path.dirname(__file__)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n    \n    from PIL import Image\n    def get_image_size(filepath):\n        im = Image.open(filepath)\n        width, height = im.size\n        return (height, width)\n\n    image_array = imgpath2pad_array(jpg_image_path)\n    dst_path = os.path.join(cur_dir, '..',  'results', \"image_array.jpg\")\n    os.makedirs(dirpath(dst_path), exist_ok=True)\n\n    mmcv.imwrite(image_array[..., ::-1], dst_path)\n    img_metas = [\n        {\n            'img_shape':tuple(image_array.shape),\n            'ori_shape':get_image_size(jpg_image_path)\n        },\n    ]\n    # print(image_array.shape)\n    tensor = array2tensor(image_array)\n    tensor = np.expand_dims(tensor, axis=0)\n    # print(tensor.shape)\n    onnx_filepath = '../results/mask_rcnn_r50_fpn_2x_coco.onnx'\n    runner = get_onnx_runner(onnx_filepath)\n    output_dict = runner([tensor])\n    torch.save(output_dict, 'onnx_output_dict.pkl')\n    \n    import pickle\n    with open('./class_names.pickle', 'rb') as f:\n        class_names = pickle.load(f)\n    score_thr = 0.3\n\n    # print('class_names', class_names)\n    for key, value in output_dict.items():\n        print(key, value.shape)\n\n    batch_dets = output_dict['dets']\n    batch_labels = output_dict['labels']\n    batch_masks = output_dict['masks']\n    batch_size = tensor.shape[0]\n\n    results = []\n    for i in range(batch_size):\n        # [N, 5], [N]\n        dets, labels = batch_dets[i], batch_labels[i]\n        det_mask = dets[:, -1] >= score_thr\n        dets = dets[det_mask]\n        labels = labels[det_mask]\n        \n        # [N, 800, 1216]\n        masks = batch_masks[i]\n        masks = masks[det_mask]\n        img_h, img_w = img_metas[i]['img_shape'][:2]\n        ori_h, ori_w = img_metas[i]['ori_shape'][:2]\n\n        scale_factor_w = img_w / ori_w\n        scale_factor_h = img_h / ori_h\n        scale_factor = np.array([scale_factor_w, scale_factor_h, scale_factor_w, scale_factor_h])\n        dets[:, :4] /= scale_factor\n\n        dets_results = bbox2result(dets, labels, len(class_names))\n        # \u53bb\u9664 padding\n        masks = masks[:, :img_h, :img_w]\n        if True:\n            masks = masks.astype(np.float32)\n            masks = torch.from_numpy(masks)\n            masks = torch.nn.functional.interpolate(masks.unsqueeze(0), size=(ori_h, ori_w))\n            masks = masks.squeeze(0).detach().numpy()\n\n        if masks.dtype != bool:\n            masks = masks >= 0.5\n        segms_results = [[] for _ in range(len(class_names))]\n        for j in range(len(dets)):\n            segms_results[labels[j]].append(masks[j])\n        results.append((dets_results, segms_results))\n\n\n    # \u7ed8\u5236\u63a8\u7406\u7ed3\u679c\n    # wrap onnx model\n    from mmdet.core.export.model_wrappers import ONNXRuntimeDetector\n    onnx_model = ONNXRuntimeDetector(onnx_filepath, class_names, 0)\n    \n    onnx_model.show_result(\n        jpg_image_path,\n        results[0],\n        score_thr=score_thr,\n        show=True,\n        win_name='ONNXRuntime',\n        out_file='../results/onnxruntime_result.png')", ""]}
{"filename": "scripts/codes/polygraphy_data_loader.py", "chunked_list": ["import numpy as np\nimport mmcv\nimport os\n\n\"\"\"\nDefines a `load_data` function that returns a generator yielding\nfeed_dicts so that this script can be used as the argument for\nthe --data-loader-script command-line parameter.\n\"\"\"\n", "\"\"\"\n\nINPUT_SHAPE = (1, 2, 28, 28)\n\ndef imgpath2pad_array(image_path):\n    \n    image = mmcv.imread(image_path, channel_order='rgb')\n    height, width = image.shape[:2]\n    dst_height, dst_width = 800, 1216\n    resize_scale = min(dst_height / height, dst_width / width)\n    new_height, new_width = round(height * resize_scale), round(width * resize_scale)\n    resize_image = mmcv.imresize(image, (new_width, new_height))\n    # pad_height = dst_height - new_height\n    # pad_width = dst_width - new_width\n    # ((before_1, after_1), ... (before_N, after_N))\n    # pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(image.shape) - 2))\n    return resize_image # np.pad(resize_image, pad_width = pad_val, mode='constant')", "\n\ndef array2tensor(array):\n    dst_height, dst_width = 800, 1216\n    height, width = array.shape[:2]\n    pad_height = dst_height - height\n    pad_width = dst_width - width\n    pad_val = ((0, pad_height), (0, pad_width), ) + tuple([(0, 0), ] * (len(array.shape) - 2))\n    mean = np.asarray([123.675, 116.28, 103.53], dtype=np.float32).reshape(1,1,3)\n    std = np.asarray([58.395, 57.12, 57.375], dtype=np.float32).reshape(1,1,3)\n    array = (array.astype(np.float32) - mean) / std\n    pad_array = np.pad(array, pad_width = pad_val, mode='constant')\n    return np.transpose(pad_array, axes=[2, 0, 1])", "\n\ndef load_data():\n    import mmdet\n    dirpath = os.path.dirname\n    path_join = os.path.join\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n\n    image_array = imgpath2pad_array(jpg_image_path)\n\n    tensor = array2tensor(image_array)\n    tensor = np.expand_dims(tensor, axis=0).astype(np.float32)\n    # Still totally real data\n    yield {\"input\": tensor}"]}
{"filename": "scripts/codes/005export_onnx_with_dynamic_shape.py", "chunked_list": ["# Copyright (c) OpenMMLab. All rights reserved.\nimport argparse\nimport os.path as osp\nimport warnings\nfrom functools import partial\n\nimport numpy as np\nimport onnx\nimport torch\nfrom mmcv import Config, DictAction", "import torch\nfrom mmcv import Config, DictAction\n\nfrom mmdet.core.export import build_model_from_cfg, preprocess_example_input\nfrom mmdet.core.export.model_wrappers import ONNXRuntimeDetector\n\n\ndef pytorch2onnx(model,\n                 input_img,\n                 input_shape,\n                 normalize_cfg,\n                 opset_version=11,\n                 show=False,\n                 output_file='tmp.onnx',\n                 verify=False,\n                 test_img=None,\n                 do_simplify=False,\n                 dynamic_export=None,\n                 skip_postprocess=False,\n                 force_write=False):\n\n    input_config = {\n        'input_shape': input_shape,\n        'input_path': input_img,\n        'normalize_cfg': normalize_cfg\n    }\n    # prepare input\n    one_img, one_meta = preprocess_example_input(input_config)\n    img_list, img_meta_list = [one_img], [[one_meta]]\n    \n    if skip_postprocess:\n        warnings.warn('Not all models support export onnx without post '\n                      'process, especially two stage detectors!')\n        origin_forward = model.forward\n        model.forward = model.forward_dummy\n        torch.onnx.export(\n            model,\n            one_img,\n            output_file,\n            input_names=['input'],\n            export_params=True,\n            keep_initializers_as_inputs=True,\n            do_constant_folding=True,\n            verbose=show,\n            opset_version=opset_version)\n        model.forward = origin_forward\n        print(f'Successfully exported ONNX model without '\n              f'post process: {output_file}')\n        # return\n    else:\n        # replace original forward function\n        origin_forward = model.forward\n        model.forward = partial(\n            model.forward,\n            img_metas=img_meta_list,\n            return_loss=False,\n            rescale=False)\n\n        output_names = ['dets', 'labels']\n        if model.with_mask:\n            output_names.append('masks')\n        input_name = 'input'\n        dynamic_axes = None\n        if dynamic_export:\n            dynamic_axes = {\n                input_name: {\n                    0: 'batch',\n                },\n                'dets': {\n                    0: 'batch',\n                },\n                'labels': {\n                    0: 'batch',\n                },\n            }\n            if model.with_mask:\n                dynamic_axes['masks'] = {0: 'batch'}\n        if not os.path.exists(output_file) or force_write:\n            torch.onnx.export(\n                model,\n                img_list,\n                output_file,\n                input_names=[input_name],\n                output_names=output_names,\n                export_params=True,\n                keep_initializers_as_inputs=True,\n                do_constant_folding=True,\n                verbose=show,\n                opset_version=opset_version,\n                dynamic_axes=dynamic_axes)\n\n        model.forward = origin_forward\n\n    if do_simplify:\n        import onnxsim\n\n        from mmdet import digit_version\n\n        min_required_version = '0.4.0'\n        assert digit_version(onnxsim.__version__) >= digit_version(\n            min_required_version\n        ), f'Requires to install onnxsim>={min_required_version}'\n\n        model_opt, check_ok = onnxsim.simplify(output_file)\n        if check_ok:\n            onnx.save(model_opt, output_file)\n            print(f'Successfully simplified ONNX model: {output_file}')\n        else:\n            warnings.warn('Failed to simplify ONNX model.')\n    print(f'Successfully exported ONNX model: {output_file}')\n\n    if verify:\n        # check by onnx\n        onnx_model = onnx.load(output_file)\n        onnx.checker.check_model(onnx_model)\n        \n        # wrap onnx model\n        onnx_model = ONNXRuntimeDetector(output_file, model.CLASSES, 0)\n        # if dynamic_export:\n        #     # scale up to test dynamic shape\n        #     h, w = [int((_ * 1.5) // 32 * 32) for _ in input_shape[2:]]\n        #     h, w = min(1344, h), min(1344, w)\n        #     input_config['input_shape'] = (1, 3, h, w)\n\n        if test_img is None:\n            input_config['input_path'] = input_img\n\n        # prepare input once again\n        one_img, one_meta = preprocess_example_input(input_config)\n        img_list, img_meta_list = [one_img], [[one_meta]]\n\n        # get pytorch output\n        with torch.no_grad():\n            pytorch_results = model(\n                img_list,\n                img_metas=img_meta_list,\n                return_loss=False,\n                rescale=True)[0]\n        \n\n        img_list = [_.cuda().contiguous() for _ in img_list]\n        if dynamic_export:\n            img_list = img_list + [_.flip(-1).contiguous() for _ in img_list]\n            img_meta_list = img_meta_list * 2\n        # get onnx output\n        onnx_results = onnx_model(\n            img_list, img_metas=img_meta_list, return_loss=False, rescale=True)[0]\n        # visualize predictions\n        score_thr = 0.0001\n\n        out_file_ort, out_file_pt = 'show-ort.png', 'show-pt.png'\n\n        show_img = one_meta['show_img']\n        model.show_result(\n            show_img,\n            pytorch_results,\n            score_thr=score_thr,\n            show=True,\n            win_name='PyTorch',\n            out_file=out_file_pt)\n        onnx_model.show_result(\n            show_img,\n            onnx_results,\n            score_thr=score_thr,\n            show=True,\n            win_name='ONNXRuntime',\n            out_file=out_file_ort)\n\n        \n        # compare a part of result\n        if model.with_mask:\n            compare_pairs = list(zip(onnx_results, pytorch_results))\n        else:\n            compare_pairs = [(onnx_results, pytorch_results)]\n        err_msg = 'The numerical values are different between Pytorch' + \\\n                  ' and ONNX, but it does not necessarily mean the' + \\\n                  ' exported ONNX model is problematic.'\n        # check the numerical value\n        for onnx_res, pytorch_res in compare_pairs:\n            for o_res, p_res in zip(onnx_res, pytorch_res):\n                try:\n                    np.testing.assert_allclose(\n                        o_res, p_res, rtol=0.01, atol=0.1, err_msg=err_msg)\n                except AssertionError as e:\n                    print(e)\n                    print('onnxruntime')\n                    print(o_res)\n                    print('pytorch')\n                    print(p_res)\n                    if hasattr(o_res, 'shape') and hasattr(p_res, 'shape'):\n                        print('onnxruntime', o_res.shape, 'pytorch', p_res.shape)\n                    \n        print('The numerical values are the same between Pytorch and ONNX')", "\n\ndef parse_normalize_cfg(test_pipeline):\n    transforms = None\n    for pipeline in test_pipeline:\n        if 'transforms' in pipeline:\n            transforms = pipeline['transforms']\n            break\n    assert transforms is not None, 'Failed to find `transforms`'\n    norm_config_li = [_ for _ in transforms if _['type'] == 'Normalize']\n    assert len(norm_config_li) == 1, '`norm_config` should only have one'\n    norm_config = norm_config_li[0]\n    return norm_config", "\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert MMDetection models to ONNX')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument('--input-img', type=str, help='Images for input')\n    parser.add_argument(\n        '--show',\n        action='store_true',\n        help='Show onnx graph and detection outputs')\n    parser.add_argument('--output-file', type=str, default='tmp.onnx')\n    parser.add_argument('--opset-version', type=int, default=11)\n    parser.add_argument(\n        '--test-img', type=str, default=None, help='Images for test')\n    parser.add_argument(\n        '--dataset',\n        type=str,\n        default='coco',\n        help='Dataset name. This argument is deprecated and will be removed \\\n        in future releases.')\n    parser.add_argument(\n        '--verify',\n        action='store_true',\n        help='verify the onnx model output against pytorch output')\n    parser.add_argument(\n        '--simplify',\n        action='store_true',\n        help='Whether to simplify onnx model.')\n    parser.add_argument(\n        '--shape',\n        type=int,\n        nargs='+',\n        default=[800, 1216],\n        help='input image size')\n    parser.add_argument(\n        '--mean',\n        type=float,\n        nargs='+',\n        default=[123.675, 116.28, 103.53],\n        help='mean value used for preprocess input data.This argument \\\n        is deprecated and will be removed in future releases.')\n    parser.add_argument(\n        '--std',\n        type=float,\n        nargs='+',\n        default=[58.395, 57.12, 57.375],\n        help='variance value used for preprocess input data. '\n        'This argument is deprecated and will be removed in future releases.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='Override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--dynamic-export',\n        action='store_true',\n        help='Whether to export onnx with dynamic axis.')\n    parser.add_argument(\n        '--skip-postprocess',\n        action='store_true',\n        help='Whether to export model without post process. Experimental '\n        'option. We do not guarantee the correctness of the exported '\n        'model.')\n    args = parser.parse_args()\n    return args", "\n\nif __name__ == '__main__':\n    import os\n    import glob\n    import mmdet\n\n\n    dirpath = os.path.dirname\n    path_join = os.path.join\n    BASE_DIR = path_join(dirpath(dirpath(os.path.realpath(__file__))), 'models', 'mask-rcnn')\n    MMDET_DIR = dirpath(dirpath(mmdet.__file__))\n    print('BASE_DIR', BASE_DIR)\n    print('MMDET_DIR', MMDET_DIR)\n\n    config_file = path_join(BASE_DIR, 'config', 'mask_rcnn_r50_fpn_2x_coco.py')\n    checkpoint_file = path_join(BASE_DIR, 'pretrained_model', 'mask_rcnn_r50_fpn_2x_coco_*.pth')\n    checkpoint_file = glob.glob(checkpoint_file)[0]\n\n\n    opset_version = 11\n    try:\n        from mmcv.onnx.symbolic import register_extra_symbolics\n    except ModuleNotFoundError:\n        raise NotImplementedError('please update mmcv to version>=v1.0.4')\n    register_extra_symbolics(opset_version)\n\n    cfg = Config.fromfile(config_file)\n    img_scale = [800, 1216]\n    input_shape = (1, 3, img_scale[0], img_scale[1])\n\n\n    # build the model and load checkpoint\n    model = build_model_from_cfg(config_file, checkpoint_file)\n    jpg_image_path = path_join(MMDET_DIR,  'demo', 'demo.jpg')\n    normalize_cfg = parse_normalize_cfg(cfg.test_pipeline)\n    basename_woext, ext = os.path.splitext(os.path.basename(config_file))\n\n    basename_woext += '_dynamic_shape'\n    onnx_path = os.path.join('..', 'results', basename_woext+'.onnx')\n    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n    # convert model to onnx file\n    pytorch2onnx(\n        model,\n        jpg_image_path,\n        input_shape,\n        normalize_cfg,\n        opset_version=opset_version,\n        show=True,\n        output_file=onnx_path,\n        verify=True,\n        test_img=None,\n        do_simplify=True,\n        dynamic_export=True,\n        skip_postprocess=False,\n        force_write=True)", ""]}
