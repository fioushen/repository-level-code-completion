{"filename": "main_chat.py", "chunked_list": ["import os\nimport dotenv\nimport openai\n\nfrom app.controller import ArxivChatGPT\n\ndotenv.load_dotenv()\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\nif __name__=='__main__':\n    chatgpt = ArxivChatGPT()\n    chatgpt.list_arxiv(num_print=5)\n    chatgpt.select(2)\n\n    # chatgpt.add_arxiv_paper_to_db('2209.10934')\n    # chatgpt.select('2209.10934')\n\n    chatgpt.chat(\"What is the main contribution of this paper?\")\n\n    question = \"What is the main contribution of this paper?\"\n    question = 'What is the key references of this paper?'\n    question = 'To fully understand the content, can you list 5 necessary references to read?'", "\nif __name__=='__main__':\n    chatgpt = ArxivChatGPT()\n    chatgpt.list_arxiv(num_print=5)\n    chatgpt.select(2)\n\n    # chatgpt.add_arxiv_paper_to_db('2209.10934')\n    # chatgpt.select('2209.10934')\n\n    chatgpt.chat(\"What is the main contribution of this paper?\")\n\n    question = \"What is the main contribution of this paper?\"\n    question = 'What is the key references of this paper?'\n    question = 'To fully understand the content, can you list 5 necessary references to read?'", "\n\n# nohup autossh -NT -L 0.0.0.0:443:127.0.0.1:5001 zhangc@localhost > ~/autossh_443_to_5001.log 2>&1 &\n"]}
{"filename": "config.py", "chunked_list": ["import os\n\nimport dotenv\n\ndotenv.load_dotenv()\n\nclass Config:\n    SECRET_KEY = os.environ['SECRET_KEY']\n    SQLALCHEMY_DATABASE_URI = 'sqlite:///'+os.path.abspath(os.environ['SQLITE3_DB_PATH'])\n    SQLALCHEMY_TRACK_MODIFICATIONS = False", ""]}
{"filename": "draft00.py", "chunked_list": ["\n\nimport sqlite3\n\n\nsql_conn0 = sqlite3.connect('arxiv.sqlite3.bak')\n\nsql_conn0.execute(\"SELECT * FROM sqlite_master where type='table'\").fetchall()\n\nsql_conn0 = sqlite3.connect('/public_data/arxiv-gpt/arxiv.sqlite3')", "\nsql_conn0 = sqlite3.connect('/public_data/arxiv-gpt/arxiv.sqlite3')\nsql_conn1 = sqlite3.connect('test.sqlite3')\n\nsql_conn0.execute('SELECT * FROM paper').fetchall()\n\ntmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\ntmp2 = \",\".join(tmp0)\n\npaper_list = [x[1:] for x in sql_conn0.execute(f'SELECT * FROM paper').fetchall()]", "\npaper_list = [x[1:] for x in sql_conn0.execute(f'SELECT * FROM paper').fetchall()]\nsql_conn1.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', paper_list)\n\n"]}
{"filename": "main_crawl.py", "chunked_list": ["# https://arxiv.org/list/cs.PF/recent\n# https://arxiv.org/list/quant-ph/recent\nimport os\nimport datetime\nimport time\nimport sqlite3\nimport dotenv\nimport openai\n\nimport crawl_arxiv", "\nimport crawl_arxiv\n\ndotenv.load_dotenv()\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\n# sqlite3 PaperParseQueue table\n\n\ndef insert_only_user():\n    from werkzeug.security import generate_password_hash\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    tmp0 = os.environ['ONLY_USER_NAME'], 'xxx@email.com', generate_password_hash(os.environ['ONLY_USER_PASS'])\n    sql_conn.execute('INSERT INTO User (username,email,password_hash) VALUES (?,?,?)', tmp0)", "\ndef insert_only_user():\n    from werkzeug.security import generate_password_hash\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    tmp0 = os.environ['ONLY_USER_NAME'], 'xxx@email.com', generate_password_hash(os.environ['ONLY_USER_PASS'])\n    sql_conn.execute('INSERT INTO User (username,email,password_hash) VALUES (?,?,?)', tmp0)\n\n\nif __name__=='__main__':\n    # TODO make it a crontab (run every day)\n    # crawl_arxiv_recent_paper()\n    # _update_existing_arxiv_data()\n\n    # crawl_arxiv.database.init_vector_database()\n\n    recent_url_list = ['https://arxiv.org/list/quant-ph/recent']\n    arxivID_time_list = []\n    last_query_time = None\n    num_paper_limit_one_day = int(os.environ['CRAWL_ONE_DAY_LIMIT'])\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    while True:\n        if (last_query_time is None) or (datetime.datetime.now()-last_query_time).days >= 1:\n            last_query_time = datetime.datetime.now()\n            for url in recent_url_list:\n                crawl_arxiv.crawl.crawl_arxiv_recent_paper(url)\n\n        time.sleep(10) #query every 10 seconds\n        arxiv_list = [x[0] for x in sql_conn.execute('SELECT arxivID FROM paper_parse_queue').fetchall()]\n\n        sql_conn.execute('DELETE FROM paper_parse_queue')\n        for x in arxiv_list:\n            sql_conn.execute('DELETE FROM paper_parse_queue WHERE arxivID = ?', (x,))\n        sql_conn.commit()\n\n        tmp0 = [sql_conn.execute('SELECT arxivID FROM paper WHERE arxivID = ?', (x,)).fetchone() for x in arxiv_list]\n        existed_list = [x[0] for x in tmp0 if x is not None]\n        arxiv_list = list(set(arxiv_list) - set(existed_list))\n\n        if len(arxiv_list)>0:\n            for arxivID in arxiv_list:\n                arxivID_time_list = [x for x in arxivID_time_list if (datetime.datetime.now() - x[1]).days < 1]\n                if len(arxivID_time_list)>num_paper_limit_one_day:\n                    print(f'Limit reached: {len(arxivID_time_list)}')\n                    tmp0 = max(1, 60*60*24 - (datetime.datetime.now()-arxivID_time_list[0][1]).total_seconds())\n                    time.sleep(tmp0)\n                tmp0 = crawl_arxiv.crawl.crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=True)\n                arxivID_time_list.append((arxivID, datetime.datetime.now()))\n    sql_conn.close()", "if __name__=='__main__':\n    # TODO make it a crontab (run every day)\n    # crawl_arxiv_recent_paper()\n    # _update_existing_arxiv_data()\n\n    # crawl_arxiv.database.init_vector_database()\n\n    recent_url_list = ['https://arxiv.org/list/quant-ph/recent']\n    arxivID_time_list = []\n    last_query_time = None\n    num_paper_limit_one_day = int(os.environ['CRAWL_ONE_DAY_LIMIT'])\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    while True:\n        if (last_query_time is None) or (datetime.datetime.now()-last_query_time).days >= 1:\n            last_query_time = datetime.datetime.now()\n            for url in recent_url_list:\n                crawl_arxiv.crawl.crawl_arxiv_recent_paper(url)\n\n        time.sleep(10) #query every 10 seconds\n        arxiv_list = [x[0] for x in sql_conn.execute('SELECT arxivID FROM paper_parse_queue').fetchall()]\n\n        sql_conn.execute('DELETE FROM paper_parse_queue')\n        for x in arxiv_list:\n            sql_conn.execute('DELETE FROM paper_parse_queue WHERE arxivID = ?', (x,))\n        sql_conn.commit()\n\n        tmp0 = [sql_conn.execute('SELECT arxivID FROM paper WHERE arxivID = ?', (x,)).fetchone() for x in arxiv_list]\n        existed_list = [x[0] for x in tmp0 if x is not None]\n        arxiv_list = list(set(arxiv_list) - set(existed_list))\n\n        if len(arxiv_list)>0:\n            for arxivID in arxiv_list:\n                arxivID_time_list = [x for x in arxivID_time_list if (datetime.datetime.now() - x[1]).days < 1]\n                if len(arxivID_time_list)>num_paper_limit_one_day:\n                    print(f'Limit reached: {len(arxivID_time_list)}')\n                    tmp0 = max(1, 60*60*24 - (datetime.datetime.now()-arxivID_time_list[0][1]).total_seconds())\n                    time.sleep(tmp0)\n                tmp0 = crawl_arxiv.crawl.crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=True)\n                arxivID_time_list.append((arxivID, datetime.datetime.now()))\n    sql_conn.close()", ""]}
{"filename": "crawl_arxiv/database.py", "chunked_list": ["import os\nimport math\nimport sqlite3\nimport weaviate\nimport numpy as np\n\nfrom .utils import _MY_REQUEST_HEADERS, download_url_and_save\n\ndef sqlite_insert_paper_list(paper_list):\n    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    for arxivID, x in paper_dict.items():\n        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n        # sql_conn.execute('SELECT * FROM paper').fetchall()\n    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n    tmp2 = \",\".join(tmp0)\n    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n    sql_conn.commit()\n    sql_conn.close()", "def sqlite_insert_paper_list(paper_list):\n    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    for arxivID, x in paper_dict.items():\n        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n        # sql_conn.execute('SELECT * FROM paper').fetchall()\n    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n    tmp2 = \",\".join(tmp0)\n    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n    sql_conn.commit()\n    sql_conn.close()", "\n\ndef sqlite_get_arxivID_by_paper_id(pid):\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    arxivID = sql_conn.execute('SELECT arxivID FROM paper WHERE pid = ?', (pid,)).fetchone()[0]\n    sql_conn.close()\n    return arxivID\n\n\ndef sqlite3_load_all_paper_from():\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n    sql_conn.close()\n    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n    return ret", "\ndef sqlite3_load_all_paper_from():\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n    sql_conn.close()\n    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n    return ret\n\n# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')", "\n# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')\n# for x in sqlite3_load_all_paper_from():\n#     print(x)\n\n\ndef init_sqlite3_paper_parse_queue(remove_if_exist=False):\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    if remove_if_exist:\n        sql_conn.execute('DROP TABLE IF EXISTS paper_parse_queue')\n    cmd = '''create table if not exists paper_parse_queue (\n        ppqid integer primary key,\n        arxivID text\n    )\n    '''\n    sql_conn.execute(cmd)\n    sql_conn.commit()\n    sql_conn.close()", "\n\ndef init_sqlite3_database(remove_if_exist=False):\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    if remove_if_exist:\n        sql_conn.execute('DROP TABLE IF EXISTS paper')\n    cmd = '''create table if not exists paper (\n        pid integer primary key,\n        arxivID text,\n        meta_info_json_path text,\n        pdf_path text,\n        tex_path text,\n        chunk_text_json_path text,\n        num_chunk integer\n    )\n    '''\n    sql_conn.execute(cmd)\n    sql_conn.commit()\n    sql_conn.close()", "\nWeaviate_Paper_schema = {\n    \"class\": \"Paper\",\n    \"description\": \"A collection of arxiv paper\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"", "          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"chunk\",\n        \"description\": \"chunk contents of the paper\",\n        \"dataType\": [\"text\"]\n    },\n    {", "    },\n    {\n        \"name\": \"arxiv_id\",\n        \"description\": \"arxiv ID\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"num_chunk\",\n        \"description\": \"total number of chunk\",", "        \"name\": \"num_chunk\",\n        \"description\": \"total number of chunk\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"num_token\",\n        \"description\": \"number of token\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }", "        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"index\",\n        \"description\": \"index of the chunk\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}", "    }]\n}\n\n\ndef init_vector_database(remove_if_exist=False):\n    client = _get_vector_database(with_openai_api_key=False)\n    tag_exist = 'Paper' in {x['class'] for x in client.schema.get()['classes']}\n    if remove_if_exist and tag_exist:\n        client.schema.delete_class('Paper')\n        tag_exist = False\n    if not tag_exist:\n        client.schema.create_class(Weaviate_Paper_schema)", "\n\n'''\nchunk: text\narxiv_id: string\nindex: int\nnum_token: int\nnum_chunk: int\nvector (ada-002)\n'''", "vector (ada-002)\n'''\n\n\ndef _get_vector_database(with_openai_api_key):\n    tmp0 = weaviate.auth.AuthApiKey(os.environ['WEAVIATE_API_KEY'])\n    if with_openai_api_key:\n        tmp1 = {\"X-OpenAI-Api-Key\": os.environ['OPENAI_API_KEY']} #optional\n    else:\n        tmp1 = None\n    client = weaviate.Client(url=os.environ['WEAVIATE_API_URL'], auth_client_secret=tmp0, additional_headers=tmp1)\n    return client", "\n\ndef vector_database_insert_paper(arxivID, text_chunk_list, vector_list=None):\n    # text_chunk_list(list,tuple(str,int))\n    client = _get_vector_database(with_openai_api_key=True)\n    num_chunk = len(text_chunk_list)\n    uuid_list = []\n    if vector_list is not None:\n        assert len(vector_list)==num_chunk\n    with client.batch as batch:\n        batch.batch_size = 20 #20-100\n        # https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\n        # client.batch.configure(batch_size=10,  dynamic=True, timeout_retries=3)\n        for ind0 in range(num_chunk):\n            tmp0 = dict(arxiv_id=arxivID, num_chunk=num_chunk, index=ind0, chunk=text_chunk_list[ind0][0], num_token=text_chunk_list[ind0][1])\n            tmp1 = vector_list[ind0] if vector_list is not None else None\n            uuid_list.append(batch.add_data_object(tmp0, class_name='Paper', vector=tmp1))\n    return uuid_list", "\n\ndef vector_database_contains_paper(arxivID:str):\n    client = _get_vector_database(with_openai_api_key=False)\n    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n    num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n    return num_chunk>0\n\n\ndef vector_database_retrieve_paper(arxivID:str, index=None):\n    client = _get_vector_database(with_openai_api_key=False)\n    # TODO handle error\n    if index is None:\n        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n        assert num_chunk>0\n        # TODO batch_size=100\n        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n        text_chunk_list = [x['chunk'] for x in tmp1]\n        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n        ret = text_chunk_list\n    else:\n        raise NotImplementedError\n        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        # text_chunk = ''\n        # vector_np = np.zeros(1536, dtype=np.float64)\n        # return text_chunk, vector_np\n    return ret", "\ndef vector_database_retrieve_paper(arxivID:str, index=None):\n    client = _get_vector_database(with_openai_api_key=False)\n    # TODO handle error\n    if index is None:\n        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n        assert num_chunk>0\n        # TODO batch_size=100\n        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n        text_chunk_list = [x['chunk'] for x in tmp1]\n        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n        ret = text_chunk_list\n    else:\n        raise NotImplementedError\n        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        # text_chunk = ''\n        # vector_np = np.zeros(1536, dtype=np.float64)\n        # return text_chunk, vector_np\n    return ret", "\n\ndef vector_database_find_close_chunk(arxivID, message, max_context_len):\n    client = _get_vector_database(with_openai_api_key=True)\n    nearText = {\"concepts\": [message]}\n    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n    result = client.query.get(\"Paper\", [\"chunk\", \"num_token\"]).with_near_text(nearText).with_where(tmp0).with_additional(['certainty']).with_limit(10).do()\n    certainty = [x['_additional']['certainty'] for x in result['data']['Get']['Paper']] #in descending order\n    num_token_list = np.array([x['num_token'] for x in result['data']['Get']['Paper']])\n    chunk_text_str_list = [x['chunk'] for x in result['data']['Get']['Paper']]\n    np.nonzero((num_token_list + 4).cumsum() <= max_context_len)\n    tmp0 = np.nonzero((num_token_list + 4).cumsum() <= max_context_len)[0][-1] + 1\n    ret = chunk_text_str_list[:tmp0]\n    return ret", ""]}
{"filename": "crawl_arxiv/text.py", "chunked_list": ["import os\nimport io\nimport re\nimport gzip\nimport tarfile\nimport sys\nimport time\nimport math\nimport unicodedata\n", "import unicodedata\n\nimport chardet\nimport numpy as np\nimport openai\nimport tiktoken\nimport magic\nimport pylatexenc.latexwalker\nimport pdfminer.high_level\n", "import pdfminer.high_level\n\n\ndef extract_unknown_arxiv_file(file, directory):\n    desc = magic.from_file(file)\n    if desc.startswith('gzip compressed data'):\n        with gzip.open(file, 'rb') as fid:\n            file_byte = fid.read()\n        desc = magic.from_buffer(file_byte[:2048])\n        if desc.startswith('POSIX tar archive'):\n            with tarfile.open(fileobj=io.BytesIO(file_byte), mode='r') as fid:\n                fid.extractall(directory)\n        elif desc.startswith('LaTeX 2e document, ASCII text'):\n            with open(os.path.join(directory, 'main.tex'), 'wb') as fid:\n                fid.write(file_byte)\n        else:\n            print(f'unknown file type \"{file}\": {desc}')\n    else:\n        print(f'unknown file type \"{file}\": {desc}')", "\n\n# TODO replace with re2\n_TEX_INPUT_RE = re.compile(r'\\\\input\\{(.*)\\}')\n\ndef resolve_tex_input(text, directory):\n    while True:\n        tmp0 = _TEX_INPUT_RE.search(text)\n        if tmp0 is None:\n            break\n        else:\n            ind0,ind1 = tmp0.span()\n            file_i = os.path.join(directory, tmp0.group(1))\n            if (not os.path.exists(file_i)) and (not file_i.endswith('.tex')):\n                file_i = os.path.join(directory, tmp0.group(1)+'.tex')\n            if os.path.exists(file_i):\n                with open(file_i, 'r', encoding='utf-8') as fid:\n                    text = text[:ind0] + '\\n' + fid.read() + '\\n' + text[ind1:]\n            else:\n                # replace \\input{xxx} with input{xxx}\n                text = text[:ind0] + text[(ind0+1):]\n    return text", "\n_TEX_COMMENT_RE = re.compile(r'(?<!\\\\)%.*')\n\ndef try_except_make_main_tex_file(directory):\n    texpath_list = [os.path.join(directory, x) for x in os.listdir(directory) if x.endswith('.tex')]\n    text_list = []\n    for texpath_i in texpath_list:\n        try:\n            with open(texpath_i, 'r', encoding='utf-8') as fid:\n                text = fid.read()\n        except UnicodeDecodeError:\n            with open(texpath_i, 'rb') as fid:\n                tmp0 = fid.read()\n            tmp1 = chardet.detect(tmp0)['encoding']\n            print(f'[{texpath_i}] chardet detect encoding: {tmp1}')\n            text = tmp0.decode(tmp1)\n        if r'\\begin{document}' in text:\n            try:\n                text_list.append(resolve_tex_input(text, os.path.dirname(texpath_i)))\n            except Exception as e:\n                print(e)\n                print(f'Error in resolving tex input \"{texpath_i}\"')\n    # TODO handle other .tex files except main.tex\n    if len(text_list)>=1:\n        tmp0 = max(text_list, key=len)\n        ret = _TEX_COMMENT_RE.sub('', tmp0)\n    else:\n        ret = None\n    return ret", "\n\nTIKTOKEN_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n\ndef split_text_into_chunks(text, max_token, tokenizer):\n    sentence_list = text.split('. ')\n    num_token_list = [len(tokenizer.encode(\" \" + x)) for x in sentence_list]\n    ret = []\n    num_token_current = 0\n    chunk = []\n    for sentence, num_token in zip(sentence_list, num_token_list):\n        if num_token_current + num_token > max_token:\n            ret.append(\". \".join(chunk) + \".\")\n            chunk = []\n            num_token_current = 0\n        if num_token > max_token: #TODO possible loss information here\n            continue\n        chunk.append(sentence)\n        num_token_current = num_token_current + num_token + 1\n    ret = [(x,len(tokenizer.encode(x))) for x in ret]\n    return ret", "\n\n# fail data/2304.03250/main.tex\ndef texpath_to_text_chunk(tex_file):\n    # if error, return None\n    with open(tex_file, 'r', encoding='utf-8') as fid:\n        tex_text = fid.read()\n    # split raw text by section\n    # TODO finer split: abstract author title paragraph etc.\n    tmp0 = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n    tmp1 = [x for x in tmp0 if x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='document']\n    if len(tmp1)==0:\n        # [bug] pylatexenc may fail to parse some tex file\n        ind0 = re.search(r'\\\\begin\\{document\\}', tex_text).span()[1]\n        ind1 = re.search(r'\\\\end\\{document\\}', tex_text).span()[0]\n        tex_text = tex_text[ind0:ind1]\n        tex_nodelist = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n    else:\n        assert len(tmp1)==1\n        tex_nodelist = tmp1[0].nodelist\n    # remove reference\n    tex_nodelist = [x for x in tex_nodelist if not (x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='thebibliography')]\n    pos_section = [x.pos for x in tex_nodelist if x.isNodeType(pylatexenc.latexwalker.LatexMacroNode) and x.macroname=='section']\n    # TODO split by paragraph\n    index_split = [tex_nodelist[0].pos] + pos_section + [tex_nodelist[-1].pos + tex_nodelist[-1].len]\n    text_split = [tex_text[x:y] for x,y in zip(index_split[:-1], index_split[1:])]\n\n    ARXIVGPT_MAX_TOKEN_PER_CHUNK = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_CHUNK'])\n    text_chunk_list = [y for x in text_split for y in split_text_into_chunks(x, ARXIVGPT_MAX_TOKEN_PER_CHUNK, TIKTOKEN_tokenizer)]\n    return text_chunk_list", "\n\n\n# TODO make a cache\n# https://stackoverflow.com/a/93029/7290857\ntmp0 = (chr(i) for i in range(sys.maxunicode)) #all character\ntmp1 = ''.join(c for c in tmp0 if (unicodedata.category(c)=='Cc') and c not in '\\t\\n') #all control character\n_CONTROL_CHAR_RE = re.compile('[%s]' % re.escape(tmp1))\n_REMOVE_BEGIN_ARXIV_RE = re.compile(\"$(.?\\n)+\", flags=re.MULTILINE)\n_REMOVE_CONNECTING_RE = re.compile('-\\n', flags=re.MULTILINE)", "_REMOVE_BEGIN_ARXIV_RE = re.compile(\"$(.?\\n)+\", flags=re.MULTILINE)\n_REMOVE_CONNECTING_RE = re.compile('-\\n', flags=re.MULTILINE)\n_REMOVE_LATEXIT_RE = re.compile('latexit(.*)/latexit', flags=re.MULTILINE)\n_REMOVE_NEWLINE_RE = re.compile(r'(\\S)\\n(\\S)', flags=re.MULTILINE)\n_MISC00_RE = re.compile('\ufb00')\n\n\ndef pdfpath_to_text_chunk(pdf_path):\n    # TODO see github/chatpaper how to cleanup pdf\n    assert pdf_path.endswith('.pdf')\n    text_path = pdf_path[:-4] + '.txt'\n\n    with open(pdf_path, 'rb') as fid:\n        text_ori = pdfminer.high_level.extract_text(fid)\n    text0 = _CONTROL_CHAR_RE.sub('', text_ori)\n    text1 = _REMOVE_BEGIN_ARXIV_RE.sub('', text0, count=1)\n    text2 = _REMOVE_LATEXIT_RE.sub('', text1)\n\n    text3 = _REMOVE_CONNECTING_RE.sub('', text2)\n    text4 = _REMOVE_NEWLINE_RE.sub(r'\\1 \\2', text3)\n    text4 = _REMOVE_NEWLINE_RE.sub(r'\\1 \\2', text4) #sometimes we need do this several time\n    text5 = _MISC00_RE.sub('ff', text4)\n\n    text = text5\n\n    ARXIVGPT_MAX_TOKEN_PER_CHUNK = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_CHUNK'])\n    text_chunk_list = split_text_into_chunks(text, ARXIVGPT_MAX_TOKEN_PER_CHUNK, TIKTOKEN_tokenizer)\n    return text_chunk_list", "\n\ndef text_chunk_list_to_numpy_vector(text_chunk_list, batch_size=20):\n    assert all(isinstance(x,str) for x in text_chunk_list)\n    num_chunk = len(text_chunk_list)\n    tmp0 = [x*batch_size for x in range(math.ceil(num_chunk/batch_size) + 1)]\n    text_chunk_list_batch = [text_chunk_list[x:y] for x,y in zip(tmp0,tmp0[1:])]\n    embedding_list = []\n    for batch_i in text_chunk_list_batch:\n        # rate limiting\n        # https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api\n        time.sleep(2.5) #make sure not exceed 20 requests per minutes\n        response = openai.Embedding.create(input=batch_i, engine='text-embedding-ada-002')\n        tmp0 = sorted(response['data'], key=lambda x:x.index)\n        embedding_list += [x.embedding for x in tmp0]\n        print(f'embedding progress: {len(embedding_list)}/{num_chunk}')\n    embedding_np = np.array(embedding_list, dtype=np.float64)\n    return embedding_np", ""]}
{"filename": "crawl_arxiv/__init__.py", "chunked_list": ["# nohup python &\nfrom . import database\nfrom . import text\nfrom . import crawl\n"]}
{"filename": "crawl_arxiv/utils.py", "chunked_list": ["import os\nimport requests\nfrom tqdm import tqdm\n\n_MY_REQUEST_HEADERS = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n\n\ndef download_url_and_save(url, filename=None, directory='.', headers=None, proxies=None):\n    assert os.path.exists(directory)\n    response = requests.get(url, headers=headers, proxies=proxies, stream=True)\n    response.raise_for_status()\n    if filename is None:\n        filepath = os.path.join(directory, url.rsplit('/',1)[1])\n    else:\n        filepath = os.path.join(directory, filename)\n    if not os.path.exists(filepath):\n        tmp_filepath = filepath + '.incomplete'\n        tmp0 = {'total':int(response.headers['content-length']), 'unit':'iB', 'unit_scale':True}\n        with open(tmp_filepath, 'wb') as fid, tqdm(**tmp0) as progress_bar:\n            for x in response.iter_content(chunk_size=1024): #1kiB\n                progress_bar.update(len(x))\n                fid.write(x)\n        os.rename(tmp_filepath, filepath)\n    return filepath", ""]}
{"filename": "crawl_arxiv/crawl.py", "chunked_list": ["import os\nimport re\nimport json\nimport shutil\nimport requests\nimport time\nimport numpy as np\nimport lxml.etree\n\nfrom .utils import download_url_and_save, _MY_REQUEST_HEADERS", "\nfrom .utils import download_url_and_save, _MY_REQUEST_HEADERS\nfrom .text import (extract_unknown_arxiv_file, try_except_make_main_tex_file,\n            texpath_to_text_chunk, pdfpath_to_text_chunk, text_chunk_list_to_numpy_vector)\nfrom .database import sqlite_insert_paper_list, vector_database_insert_paper, vector_database_contains_paper\n\n\ndef crawl_arxiv_meta_info(arxivID):\n    url = 'https://arxiv.org/abs/' + arxivID\n    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n    response.raise_for_status()\n    time.sleep(1) #sleep 1 seconds to avoid being banned\n    html = lxml.etree.HTML(response.content)\n\n    tmp0 = html.xpath('//div[@id=\"abs\"]')\n    assert len(tmp0)==1\n    title = tmp0[0].xpath('./h1[@class=\"title mathjax\"]/text()')[0]\n    author_list = [str(x) for x in tmp0[0].xpath('./div[@class=\"authors\"]/a/text()')]\n    tmp1 = ''.join(tmp0[0].xpath('./blockquote[@class=\"abstract mathjax\"]/text()')).strip()\n    abstract = re.sub('\\n', ' ', tmp1)\n    tmp0 = ''.join(html.xpath('//td[@class=\"tablecell subjects\"]/span/text()'))\n    tmp1 = ''.join(html.xpath('//td[@class=\"tablecell subjects\"]/text()'))\n    subject = tmp0.strip() + tmp1.strip()\n    pdf_url = 'https://arxiv.org' + str(html.xpath('//a[@class=\"abs-button download-pdf\"]/@href')[0])\n    tmp0 = html.xpath('//a[@class=\"abs-button download-format\"]/@href')\n    if len(tmp0)>0:\n        assert str(tmp0[0]).startswith('/format')\n        targz_url = 'https://arxiv.org/e-print' + str(tmp0[0][7:])\n    else:\n        targz_url = ''\n    ret = {'title': title, 'author_list': author_list,'abstract': abstract, 'subject': subject, 'pdf_url': pdf_url, 'targz_url': targz_url}\n    return ret", "\n\ndef crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=False):\n    arxivID = str(arxivID).strip('/')\n    url_abs = 'https://arxiv.org/abs/' + arxivID\n    try:\n        response = requests.get(url_abs, headers=_MY_REQUEST_HEADERS)\n        response.raise_for_status()\n        time.sleep(1) #sleep 1 seconds to avoid being banned\n    except Exception as e:\n        print(e)\n        print(f'[Error][crawl_utils.py/crawl_one_arxiv_paper] fail to crawl url {url_abs}')\n        response = None\n    if response is not None:\n        html = lxml.etree.HTML(response.content)\n        hf_file = lambda *x: os.path.join(os.environ['ARXIV_DIRECTORY'], arxivID, *x)\n        if not os.path.exists(hf_file()):\n            os.makedirs(hf_file())\n        pdf_path = hf_file('main.pdf')\n        meta_info_json_path = hf_file('meta-info.json')\n        targz_path = hf_file('main.tar.gz')\n        tex_path = hf_file('main.tex')\n        chunk_text_json_path = hf_file('chunk-text.json')\n        vector_npy_path = hf_file('chunk-vector.npy')\n\n        if not os.path.exists(meta_info_json_path):\n            print(f'[{arxivID}] crawling meta information')\n            meta_info = crawl_arxiv_meta_info(arxivID)\n            with open(meta_info_json_path, 'w', encoding='utf-8') as fid:\n                json.dump(meta_info, fid, ensure_ascii=False)\n        else:\n            with open(meta_info_json_path, 'r', encoding='utf-8') as fid:\n                meta_info = json.load(fid)\n\n        pdf_url = meta_info['pdf_url']\n        if not os.path.exists(pdf_path):\n            print(f'[{arxivID}] downloading {pdf_url}')\n            download_url_and_save(pdf_url, filename=os.path.basename(pdf_path), directory=hf_file(), headers=_MY_REQUEST_HEADERS)\n            time.sleep(3) #sleep 3 seconds to avoid being banned\n\n        targz_url = meta_info['targz_url']\n        if targz_url!='':\n            if not os.path.exists(targz_path):\n                print(f'[{arxivID}] downloading targz file \"{targz_url}\"')\n                download_url_and_save(targz_url, filename=os.path.basename(targz_path), directory=hf_file(), headers=_MY_REQUEST_HEADERS)\n                time.sleep(3)\n            if not os.path.exists(tex_path):\n                if not os.path.exists(hf_file('untar')):\n                    print(f'[{arxivID}] extract targz file to untar folder')\n                    os.makedirs(hf_file('untar'))\n                    extract_unknown_arxiv_file(targz_path, hf_file('untar'))\n                print(f'[{arxivID}] make main.tex')\n                tex_text = try_except_make_main_tex_file(hf_file('untar'))\n                if tex_text is not None:\n                    with open(tex_path, 'w', encoding='utf-8') as fid:\n                        fid.write(tex_text)\n                shutil.rmtree(hf_file('untar'))\n\n        if not os.path.exists(chunk_text_json_path):\n            text_list = []\n            if os.path.exists(tex_path):\n                print(f'[{arxivID}] convert tex to chunk_text')\n                try:\n                    text_list = texpath_to_text_chunk(tex_path)\n                except Exception as e:\n                    print(e)\n                    print(f'[Error][crawl_utils.py/crawl_one_arxiv_paper] fail to convert tex to chunk_text')\n            if len(text_list)==0:\n                print(f'[{arxivID}] convert pdf to chunk_text')\n                text_list = pdfpath_to_text_chunk(pdf_path)\n            with open(chunk_text_json_path, 'w', encoding='utf-8') as fid:\n                json.dump(text_list, fid, ensure_ascii=False)\n        else:\n            with open(chunk_text_json_path, 'r', encoding='utf-8') as fid:\n                text_list = json.load(fid)\n        num_chunk = len(text_list)\n\n        if os.path.exists(chunk_text_json_path):\n            if (os.environ['ARXIVGPT_SAVE_NUMPY_VECTOR']=='1') and (not os.path.exists(vector_npy_path)):\n                print(f'[{arxivID}] converting chunk_text to numpy vector')\n                embedding_np = text_chunk_list_to_numpy_vector([x[0] for x in text_list])\n                np.save(vector_npy_path, embedding_np)\n            # if vector_database_contains_paper(arxivID):\n            #     print(f'[{arxivID}] vector_database already contains this paper')\n            # else:\n            #     if os.path.exists(vector_npy_path):\n            #         embedding_np = np.load(vector_npy_path)\n            #         assert embedding_np.shape[0] == num_chunk\n            #     else:\n            #         embedding_np = None\n            #     print(f'[{arxivID}] inserting paper into vector database')\n            #     uuid_list = vector_database_insert_paper(arxivID, text_list, embedding_np)\n            # TODO should we save uuid_list to json file?\n\n        ret = dict(arxivID=arxivID, num_chunk=num_chunk, meta_info_json_path=meta_info_json_path, pdf_path=pdf_path, tex_path=tex_path, chunk_text_json_path=chunk_text_json_path)\n        for key in list(ret.keys()):\n            if key.endswith('_path'):\n                value = ret[key]\n                if not os.path.exists(value):\n                    value = ''\n                assert value.startswith(os.environ['ARXIV_DIRECTORY']) or (value=='')\n                ret[key] = value[len(os.environ['ARXIV_DIRECTORY']):].lstrip(os.sep)\n        sqlite_insert_paper_list([ret])\n    else:\n        ret = None\n    return ret", "\ndef crawl_arxiv_recent_paper(url):\n    print(f'crawling {url}')\n    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n    response.raise_for_status()\n    time.sleep(1) #sleep 3 seconds to avoid being banned\n    html = lxml.etree.HTML(response.content)\n    arxivID_list = [str(x.rsplit('/',1)[1]) for x in html.xpath('//dt/span[@class=\"list-identifier\"]/a[@title=\"Abstract\"]/@href')]\n    for x in arxivID_list:\n        crawl_one_arxiv_paper(x, tag_commit_sqlite3=True)", "\n\ndef remove_all_intermidiate_data_in_arxiv(directory):\n    for x in os.listdir(directory):\n        folder = os.path.join(directory, x)\n        if os.path.isdir(folder):\n            tmp0 = os.path.join(folder, 'untar')\n            if os.path.exists(tmp0):\n                shutil.rmtree(tmp0)\n            tmp0 = os.path.join(folder, 'main.tex')\n            if os.path.exists(tmp0):\n                os.remove(tmp0)\n            tmp0 = os.path.join(folder, 'meta-info.json')\n            if os.path.exists(tmp0):\n                os.remove(tmp0)\n            tmp0 = os.path.join(folder, 'chunk-text.json')\n            if os.path.exists(tmp0):\n                os.remove(tmp0)", "            # tmp0 = os.path.join(folder, 'chunk-vector.npy')\n            # if os.path.exists(tmp0):\n            #     os.remove(tmp0)\n\n\ndef _update_existing_arxiv_data():\n    directory = os.environ['ARXIV_DIRECTORY']\n    arxivID_list = [x for x in os.listdir(directory) if os.path.isdir(os.path.join(directory,x))]\n    for x in arxivID_list:\n        crawl_one_arxiv_paper(x, tag_commit_sqlite3=True)", ""]}
{"filename": "history_file/draft00.py", "chunked_list": ["import os\nimport time\nimport pickle\nimport dotenv\nimport requests\nfrom tqdm import tqdm\nimport lxml.etree\nimport tiktoken\nimport openai\nimport openai.embeddings_utils", "import openai\nimport openai.embeddings_utils\nimport numpy as np\n\nfrom utils import download_url_and_save, convert_pdf_to_text, _MY_REQUEST_HEADERS, NaiveChatGPT\n\ndotenv.load_dotenv()\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\nif not os.path.exists('data'):\n    os.makedirs('data')", "\nif not os.path.exists('data'):\n    os.makedirs('data')\n\nchatgpt = NaiveChatGPT()\n\ndef get_arxiv_recent_url_pdf():\n    url = \"https://arxiv.org/list/quant-ph/recent\"\n    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n    response.raise_for_status()\n    z0 = lxml.etree.HTML(response.content)\n    ret = ['https://arxiv.org'+str(x) for x in z0.xpath('//a[@title=\"Download PDF\"]/@href')]\n    # TODO title author abstract\n    return ret", "\ndef download_arxiv_pdf(url_pdf_list, directory='data'):\n    # TODO download and read from tex file\n    for url_i in url_pdf_list:\n        print(url_i)\n        filename = url_i.rsplit('/',1)[1] + '.pdf'\n        filepath = os.path.join(directory, filename)\n        if not os.path.exists(filepath):\n            download_url_and_save(url_i, filename=filename, directory=directory, headers=_MY_REQUEST_HEADERS)\n            time.sleep(3) #sleep 3 seconds to avoid being banned\n        else:\n            print(f'{filename} already downloaded, skip it')", "\n\ndef split_text_into_chunks(text, max_token, tokenizer):\n    sentence_list = text.split('. ')\n    num_token_list = [len(tokenizer.encode(\" \" + x)) for x in sentence_list]\n    ret = []\n    num_token_current = 0\n    chunk = []\n    for sentence, num_token in zip(sentence_list, num_token_list):\n        if num_token_current + num_token > max_token:\n            ret.append(\". \".join(chunk) + \".\")\n            chunk = []\n            num_token_current = 0\n        if num_token > max_token: #TODO possible loss information here\n            continue\n        chunk.append(sentence)\n        num_token_current = num_token_current + num_token + 1\n    ret = [(x,len(tokenizer.encode(x))) for x in ret]\n    return ret", "\n\ndef get_arxiv_text_embedding(txt_file, max_tokens=None, tokenizer=None):\n    # (ret0) chunked_text_list (list,tuple(text:str, num_token:int))\n    # (ret1) embedding_np (np.float64, (N,1536))\n    assert txt_file.endswith('.txt')\n    pkl_file = txt_file[:-4] + '.pkl'\n    if os.path.exists(pkl_file):\n        with open(pkl_file, 'rb') as fid:\n            tmp0 = pickle.load(fid)\n            chunked_list = tmp0['chunked_list']\n            embedding_np = tmp0['embedding_np']\n    else:\n        assert max_tokens is not None\n        with open(txt_file, 'r', encoding='utf-8') as fid:\n            text = fid.read()\n        chunked_list = split_text_into_chunks(text, max_tokens, tokenizer)\n        embbeding_list = []\n        # TODO rate limiting\n        # https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api\n        for x,_ in tqdm(chunked_list):\n            time.sleep(2.5) #make sure not exceed 20 requests per minutes\n            tmp0 = np.array(openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'], dtype=np.float64)\n            embbeding_list.append(tmp0)\n        embedding_np = np.stack(embbeding_list, axis=0)\n        with open(pkl_file, 'wb') as fid:\n            tmp0 = dict(chunked_list=chunked_list, embedding_np=embedding_np)\n            pickle.dump(tmp0, fid)\n    return chunked_list, embedding_np", "\n\n_openai_qa_template = (\"Answer the question based on the context below, and if the question can't be answered based on the context, \"\n            \"say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\")\n\ndef answer_question(question, text_list, embedding_np, max_context_len=1800, tag_print_context=False):\n    \"\"\"\n    Answer a question based on the most similar context from the dataframe texts\n    text_list(list, tuple(text:str, num_token:int))\n    TODO response_max_tokens=150\n    \"\"\"\n    assert all(len(x)==2 for x in text_list)\n    assert len(text_list) == embedding_np.shape[0]\n    text_len_list = np.array([x[1] for x in text_list])\n    text_str_list = [x[0] for x in text_list]\n\n    q_embedding = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n    distance = np.array(openai.embeddings_utils.distances_from_embeddings(q_embedding, embedding_np, distance_metric='cosine')) # 0: cloest\n    ind0 = np.argsort(distance)\n    tmp0 = np.nonzero((text_len_list[ind0] + 4).cumsum() > max_context_len)[0].min()\n    context_text_list = [text_str_list[x] for x in ind0[:tmp0]]\n    context_text = \"\\n\\n###\\n\\n\".join(context_text_list)\n    if tag_print_context:\n        print(f\"Context:\\n{context_text}\\n\\n\")\n    prompt = _openai_qa_template.format(context=context_text, question=question)\n    try:\n        chatgpt.reset()\n        ret = chatgpt.chat(prompt, tag_print=False, tag_return=True)\n    except Exception as e:\n        print(e)\n        ret = \"\"\n    return ret", "\n\n\nurl_i = 'https://arxiv.org/pdf/2209.10934'\n\nurl_pdf_list = get_arxiv_recent_url_pdf()\ndownload_arxiv_pdf(url_pdf_list)\n\npdf_file_list = [os.path.join('data',x) for x in os.listdir('data') if x.endswith('.pdf')]\nconvert_pdf_to_text(pdf_file_list)", "pdf_file_list = [os.path.join('data',x) for x in os.listdir('data') if x.endswith('.pdf')]\nconvert_pdf_to_text(pdf_file_list)\n\n\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\nmax_token_per_chunk = 300\n\ntxt_file_list = [os.path.join('data',x) for x in os.listdir('data') if x.endswith('.txt')]\n\ntxt_file = 'data/2304.01190.txt'", "\ntxt_file = 'data/2304.01190.txt'\nchunked_list, embedding_np = get_arxiv_text_embedding(txt_file, max_token_per_chunk, tokenizer)\n\nquestion = \"What is the main contribution of this paper?\"\nquestion = 'What is the key references of this paper?'\nquestion = 'To fully understand the content, can you list 5 necessary references to read?'\nanswer = answer_question(question, chunked_list, embedding_np, tag_print_context=True)\nprint(answer)\n", "print(answer)\n"]}
{"filename": "history_file/draft02.py", "chunked_list": ["import os\nimport io\nimport magic\nimport time\nimport re\nimport gzip\nimport tarfile\nimport lxml.etree\nimport requests\nfrom tqdm import tqdm", "import requests\nfrom tqdm import tqdm\nimport pylatexenc.latexwalker\n\nfrom utils import _MY_REQUEST_HEADERS, download_url_and_save\n\n# TODO save to database\n\ndef get_arxiv_recent_targz_url():\n    # TODO title author abstract\n    url = \"https://arxiv.org/list/quant-ph/recent\"\n    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n    response.raise_for_status()\n    html = lxml.etree.HTML(response.content)\n    # TODO print if targz file is missing\n    tmp0 = [str(x) for x in html.xpath('//a[@title=\"Other formats\"]/@href')]\n    tmp1 = [str(x) for x in html.xpath('//a[@title=\"Abstract\"]/@href')]\n    assert all(x.startswith('/format/') for x in tmp0)\n    assert all(x.startswith('/abs/') for x in tmp1)\n    tmp2 = set(x[5:] for x in tmp1) - set(x[8:] for x in tmp0)\n    if len(tmp2)>0:\n        print('these arxiv keys are missing targz file:', tmp2)\n    ret = ['https://arxiv.org/e-print/'+x[8:] for x in tmp0]\n    return ret", "def get_arxiv_recent_targz_url():\n    # TODO title author abstract\n    url = \"https://arxiv.org/list/quant-ph/recent\"\n    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n    response.raise_for_status()\n    html = lxml.etree.HTML(response.content)\n    # TODO print if targz file is missing\n    tmp0 = [str(x) for x in html.xpath('//a[@title=\"Other formats\"]/@href')]\n    tmp1 = [str(x) for x in html.xpath('//a[@title=\"Abstract\"]/@href')]\n    assert all(x.startswith('/format/') for x in tmp0)\n    assert all(x.startswith('/abs/') for x in tmp1)\n    tmp2 = set(x[5:] for x in tmp1) - set(x[8:] for x in tmp0)\n    if len(tmp2)>0:\n        print('these arxiv keys are missing targz file:', tmp2)\n    ret = ['https://arxiv.org/e-print/'+x[8:] for x in tmp0]\n    return ret", "\n\ndef extract_unknown_arxiv_file(file, directory):\n    desc = magic.from_file(file)\n    if desc.startswith('gzip compressed data'):\n        with gzip.open(file, 'rb') as fid:\n            file_byte = fid.read()\n        desc = magic.from_buffer(file_byte[:2048])\n        if desc.startswith('POSIX tar archive'):\n            with tarfile.open(fileobj=io.BytesIO(file_byte), mode='r') as fid:\n                fid.extractall(directory)\n        elif desc.startswith('LaTeX 2e document, ASCII text'):\n            with open(os.path.join(directory, 'main.tex'), 'wb') as fid:\n                fid.write(file_byte)\n        else:\n            print(f'unknown file type \"{file}\": {desc}')\n    else:\n        print(f'unknown file type \"{file}\": {desc}')", "\n\ndef download_tex_targz_file(url_list, directory='data'):\n    tex_file_list = []\n    for url_i in url_list:\n        arxiv_key = url_i.rsplit('/',1)[1]\n        hf_path = lambda *x: os.path.join(directory, arxiv_key, *x)\n        if not os.path.exists(hf_path()):\n            os.makedirs(hf_path())\n        if not os.path.exists(hf_path('untar')):\n            os.makedirs(hf_path('untar'))\n        targz_filename = arxiv_key + '.tar.gz'\n        if os.path.exists(hf_path(targz_filename)):\n            print(f'{targz_filename} already exists, skip downloading')\n        else:\n            print(f'downloading {url_i}')\n            download_url_and_save(url_i, filename=targz_filename, directory=hf_path(), headers=_MY_REQUEST_HEADERS)\n            time.sleep(2.5) #avoid being banned\n        tag_has_tex = len([x for x in os.listdir(hf_path('untar')) if x.endswith('.tex')])>1\n        if not tag_has_tex:\n            extract_unknown_arxiv_file(hf_path(targz_filename), hf_path('untar'))\n        tmp0 = [hf_path('untar',x) for x in os.listdir(hf_path('untar')) if x.endswith('.tex')]\n        if len(tmp0)==0:\n            print(f'no tex file for {arxiv_key}')\n        else:\n            if len(tmp0)>1:\n                print(f'more than one tex file in {arxiv_key}, choose the largest one in size')\n                tmp0 = sorted(tmp0, key=os.path.getsize, reverse=True)[0]\n            else:\n                tmp0 = tmp0[0]\n            tex_file_list.append(tmp0)\n    return tex_file_list", "\n\ndef get_tex_split_text(tex_file):\n    # if error, return None\n    with open(tex_file, 'r', encoding='utf-8') as fid:\n        tex_text = fid.read()\n    # split raw text by section\n    # TODO finer split: abstract author title paragraph etc.\n    tmp0 = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n    tmp1 = [x for x in tmp0 if x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='document']\n    if len(tmp1)==0:\n        tmp0 = re.search(r'\\\\begin\\{document\\}', tex_text)\n        tmp1 = re.search(r'\\\\end\\{document\\}', tex_text)\n        if (tmp0 is None) or (tmp1 is None):\n            return None\n        ind0 = tmp0.span()[1]\n        ind1 = tmp1.span()[0]\n        tex_text = tex_text[ind0:ind1]\n        tex_nodelist = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n    else:\n        assert len(tmp1)==1\n        tex_nodelist = tmp1[0].nodelist\n    # remove reference\n    tex_nodelist = [x for x in tex_nodelist if not (x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='thebibliography')]\n    pos_section = [x.pos for x in tex_nodelist if x.isNodeType(pylatexenc.latexwalker.LatexMacroNode) and x.macroname=='section']\n    # TODO split by paragraph\n    index_split = [tex_nodelist[0].pos] + pos_section + [tex_nodelist[-1].pos + tex_nodelist[-1].len]\n    text_split = [tex_text[x:y] for x,y in zip(index_split[:-1], index_split[1:])]\n    return text_split", "\n'''\ndata\n\u251c\u2500\u2500 2101.00001\n\u2502   \u251c\u2500\u2500 2101.00001.pdf\n\u2502   \u251c\u2500\u2500 2101.00001.txt\n\u2502   \u251c\u2500\u2500 2101.00001.tar.gz\n\u2502   \u251c\u2500\u2500 untar\n\u2502   \u2502   \u251c\u2500\u2500 2101.00001v1\n'''", "\u2502   \u2502   \u251c\u2500\u2500 2101.00001v1\n'''\n\nurl_list = get_arxiv_recent_targz_url()\n\ntex_file_list = download_tex_targz_file(url_list)\nzc0 = [get_tex_split_text(x) for x in tqdm(tex_file_list)]\nzc0 = get_tex_split_text(tex_file_list[13])\n# TODO 2303.17399 input\n", "# TODO 2303.17399 input\n\ntex_file = tex_file_list[13]\n\n# TODO download 2209.10934 (PureB) tex\n\n# url_i = 'https://arxiv.org/e-print//2303.17565'\n\n# arxiv_key = url_i.rsplit('/',1)[1]\n# hf_path = lambda *x: os.path.join('data', arxiv_key, *x)", "# arxiv_key = url_i.rsplit('/',1)[1]\n# hf_path = lambda *x: os.path.join('data', arxiv_key, *x)\n# if not os.path.exists(hf_path()):\n#     os.makedirs(hf_path())\n# targz_filename = arxiv_key + '.tar.gz' #if exists, then skip all below\n# download_url_and_save(url_i, filename=targz_filename, directory=hf_path(), headers=_MY_REQUEST_HEADERS)\n\n\n# tmp0 = hf_path('untar')\n# if os.path.isdir(tmp0):", "# tmp0 = hf_path('untar')\n# if os.path.isdir(tmp0):\n#     os.rmdir(tmp0)\n# with tarfile.open(hf_path(targz_filename), 'r') as fid:\n#     fid.extractall(tmp0)\n\n# tmp0 = [x for x in os.listdir(hf_path('untar')) if x.endswith('.tex')]\n# assert len(tmp0) == 1\n# tex_file = hf_path('untar', tmp0[0])\n", "# tex_file = hf_path('untar', tmp0[0])\n\n# with open(tex_file, 'r', encoding='utf-8') as fid:\n#     tex_text = fid.read()\n"]}
{"filename": "history_file/draft01.py", "chunked_list": ["# python chat_arxiv.py --query \"chatgpt robot\" --page_num 2 --max_results 3 --days 2\n\n# paper_list = reader1.get_arxiv_web(args=args, page_num=args.page_num, days=args.days)\n\n# titles, links, dates = self.get_all_titles_from_web(args.query, page_num=page_num, days=days)\n\ndef get_all_titles_from_web(self, keyword, page_num=1, days=1):\n    title_list, link_list, date_list = [], [], []\n    for page in range(page_num):\n        url = self.get_url(keyword, page)  # \u6839\u636e\u5173\u952e\u8bcd\u548c\u9875\u7801\u751f\u6210\u94fe\u63a5\n        titles, links, dates = self.get_titles(url, days)  # \u6839\u636e\u94fe\u63a5\u83b7\u53d6\u8bba\u6587\u6807\u9898\n        if not titles:  # \u5982\u679c\u6ca1\u6709\u83b7\u53d6\u5230\u4efb\u4f55\u6807\u9898\uff0c\u8bf4\u660e\u5df2\u7ecf\u5230\u8fbe\u6700\u540e\u4e00\u9875\uff0c\u9000\u51fa\u5faa\u73af\n            break\n        for title_index, title in enumerate(titles):  # \u904d\u5386\u6bcf\u4e2a\u6807\u9898\uff0c\u5e76\u6253\u5370\u51fa\u6765\n            print(page, title_index, title, links[title_index], dates[title_index])\n        title_list.extend(titles)\n        link_list.extend(links)\n        date_list.extend(dates)\n    print(\"-\" * 40)\n    return title_list, link_list, date_list", "\n\ndef get_url(self, keyword, page):\n    base_url = \"https://arxiv.org/search/?\"\n    params = {\n        \"query\": keyword,\n        \"searchtype\": \"all\",  # \u641c\u7d22\u6240\u6709\u5b57\u6bb5\n        \"abstracts\": \"show\",  # \u663e\u793a\u6458\u8981\n        \"order\": \"-announced_date_first\",  # \u6309\u65e5\u671f\u964d\u5e8f\u6392\u5e8f\n        \"size\": 50  # \u6bcf\u9875\u663e\u793a50\u6761\u7ed3\u679c\n    }\n    if page > 0:\n        params[\"start\"] = page * 50  # \u8bbe\u7f6e\u8d77\u59cb\u4f4d\u7f6e\n    return base_url + requests.compat.urlencode(params)", "\n"]}
{"filename": "app/Form.py", "chunked_list": ["from flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, BooleanField, SubmitField, SelectField, TextAreaField, IntegerField\nfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo,InputRequired\n\nfrom . import controller\nfrom .models import User\n\n# login form, used in login page\nclass LoginForm(FlaskForm):\n    username = StringField(\"Username\", validators = [DataRequired()], default='NOT REQUIRED')\n    password = PasswordField(\"Password\", validators = [DataRequired()])\n    remember_me = BooleanField(\"Remember Me\")\n    submit = SubmitField(\"Sign In\")", "class LoginForm(FlaskForm):\n    username = StringField(\"Username\", validators = [DataRequired()], default='NOT REQUIRED')\n    password = PasswordField(\"Password\", validators = [DataRequired()])\n    remember_me = BooleanField(\"Remember Me\")\n    submit = SubmitField(\"Sign In\")\n\n# registration form, used to get registration information\n# class RegistrationForm(FlaskForm):\n#     username = StringField('Username', validators=[DataRequired()])\n#     email = StringField('Email', validators=[DataRequired(), Email()])", "#     username = StringField('Username', validators=[DataRequired()])\n#     email = StringField('Email', validators=[DataRequired(), Email()])\n#     password = PasswordField('Password', validators=[DataRequired()])\n#     password2 = PasswordField('Repeat Password', validators=[DataRequired(), EqualTo('password')])\n#     submit = SubmitField('Register')\n#     # make sure user name is not empty or duplicate\n#     def validate_username(self, username):\n#         user = User.query.filter_by(username=username.data).first()\n#         if user is not None:\n#             raise ValidationError('Please use a different username.')", "#         if user is not None:\n#             raise ValidationError('Please use a different username.')\n#     # make sure email is not empty or duplicate\n#     def validate_email(self, email):\n#         user = User.query.filter_by(email=email.data).first()\n#         if user is not None:\n#             raise ValidationError('Please use a different email address.')\n\n# form for chat room\nclass messageForm(FlaskForm):\n    message = TextAreaField('Message', validators=[DataRequired()])\n    submit = SubmitField('Send')", "# form for chat room\nclass messageForm(FlaskForm):\n    message = TextAreaField('Message', validators=[DataRequired()])\n    submit = SubmitField('Send')\n\n"]}
{"filename": "app/_init.py", "chunked_list": ["from flask import Flask\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\nfrom flask_login import LoginManager\n\nfrom config import Config #root directory\n\n\napp = Flask(__name__.split('.',1)[0])\napp.config.from_object(Config)", "app = Flask(__name__.split('.',1)[0])\napp.config.from_object(Config)\ndb = SQLAlchemy(app)\nmigrate = Migrate(app, db)\nlogin = LoginManager(app)\nlogin.login_view = 'login'\n"]}
{"filename": "app/models.py", "chunked_list": ["import os\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom flask_login import UserMixin\nfrom hashlib import md5\nimport sqlite3\nimport json\n\nfrom ._init import login, db\n\n#the tables of sql and related caculations are wirtten here", "\n#the tables of sql and related caculations are wirtten here\n\nclass User(UserMixin,db.Model):\n    uid = db.Column(db.Integer, primary_key=True)\n    username = db.Column(db.String(64), index=True, unique=True)\n    email = db.Column(db.String(120), index=True, unique=True)\n    password_hash = db.Column(db.String(128))\n    Is_adm = db.Column(db.Integer)\n    message = db.relationship('message', backref='User', lazy='dynamic')\n    def set_password(self, password):\n        self.password_hash = generate_password_hash(password)\n\n    def check_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\n    def __repr__(self):\n        return '<User {}>'.format(self.username)\n\n    def get_id(self):\n        return self.uid\n\n    def get_user_name(self):\n        return self.username\n\n    def if_adm(self):\n        return self.Is_adm", "\n@login.user_loader\ndef load_user(id):\n    return User.query.get(int(id))\n\n# paper table\nclass paper(db.Model):\n    pid = db.Column(db.Integer, primary_key=True)\n    arxivID = db.Column(db.String(50), unique=True)\n    meta_info_json_path = db.Column(db.String(100), unique=True)\n    pdf_path = db.Column(db.String(100), unique=True, default='')\n    tex_path = db.Column(db.String(100), unique=True, default='')\n    chunk_text_json_path = db.Column(db.String(100), default='')\n    num_chunk = db.Column(db.Integer, default=0)\n    message = db.relationship('message', backref='paper', lazy='dynamic')\n    def __repr__(self):\n        return '<paper {}>'.format(self.arxivID)\n\n    def get_id(self):\n        return self.pid\n    def get_arxivID(self):\n        return self.arxivID\n\n    def get_title(self):\n        with open(os.path.join(os.environ['ARXIV_DIRECTORY'], self.meta_info_json_path), 'r') as fid:\n            meta_info = json.load(fid)\n        return meta_info['title']\n\n    def get_pdf_url(self):\n        with open(os.path.join(os.environ['ARXIV_DIRECTORY'], self.meta_info_json_path), 'r') as fid:\n            meta_info = json.load(fid)\n        return meta_info['pdf_url']", "\n\n# message table\nclass message(db.Model):\n    mid = db.Column(db.Integer, primary_key=True)\n    pid = db.Column(db.Integer, db.ForeignKey('paper.pid'))\n    uid = db.Column(db.Integer, db.ForeignKey('user.uid'))\n    content = db.Column(db.String(500))\n    time = db.Column(db.DateTime)\n    def __repr__(self):\n        return '<\"message\": {},'.format(self.content)+'\"time\": {}'.format(self.time)+\">\"", "\nclass paper_parse_queue(db.Model):\n    ppqid = db.Column(db.Integer, primary_key=True) #useless\n    arxivID = db.Column(db.String(50), unique=True)\n\n# init_db() is used to initialize the database, it should be called only once\ndef init_db():\n    # create the database and the db table\n    db.drop_all()\n    db.create_all()\n    db.session.commit()\n    # insert some test data\n    conn = sqlite3.connect('app.db')\n    print('opened database successfully')\n    c = conn.cursor()\n    sql_query = \"UPDATE paper SET  meta_info_json_path = 'arxiv//test_paper1//meta-info.json' where pid = 1;\"\n    c.execute(sql_query)\n    conn.commit()\n    sql_query = \"UPDATE paper SET  meta_info_json_path = 'arxiv//test_paper2//meta-info.json' where pid = 2;\"\n    c.execute(sql_query)\n    conn.commit()\n    # show the table\n    contents = c.execute(\"SELECT * FROM paper\")", ""]}
{"filename": "app/__init__.py", "chunked_list": ["from ._init import app,db,migrate,login\nfrom . import routes\nfrom . import models\nfrom . import Form\nfrom . import controller\n"]}
{"filename": "app/routes.py", "chunked_list": ["import os\nfrom flask import render_template, flash, redirect, url_for, request, send_from_directory, jsonify\nfrom flask_login import logout_user, login_user, current_user, login_required\nfrom werkzeug.urls import url_parse\nfrom flask_wtf import FlaskForm\nfrom wtforms import SelectField, SubmitField\nfrom wtforms.validators import ValidationError, DataRequired\nimport sqlalchemy.sql.expression\n\n", "\n\nfrom .models import User, paper, paper_parse_queue\nfrom ._init import app, db\nfrom . import controller\nfrom .Form import LoginForm, messageForm #RegistrationForm\n\n\n# root page\n@app.route('/')", "# root page\n@app.route('/')\n@app.route('/index')\ndef index():\n    if current_user.is_authenticated:\n        return redirect(url_for('welcome'))\n    return render_template('Application.html')\n\n# route for login page\n@app.route('/Login', methods=['POST', 'GET'])\ndef login():\n    if current_user.is_authenticated:\n        return redirect(url_for('welcome'))\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(username=os.environ['ONLY_USER_NAME']).first()\n        if user is None or not user.check_password(form.password.data):\n            flash('Invalid username or password')\n            return redirect(url_for('login'))\n        login_user(user, remember=form.remember_me.data)\n        next_page = request.args.get('next')\n        if not next_page or url_parse(next_page).netloc != '':\n            next_page = url_for('welcome')\n        if user.Is_adm ==1:\n            next_page = url_for('AdminWelcome')\n        return redirect(next_page)\n    return render_template('Can_log.html', title='Sign In', form=form)", "# route for login page\n@app.route('/Login', methods=['POST', 'GET'])\ndef login():\n    if current_user.is_authenticated:\n        return redirect(url_for('welcome'))\n    form = LoginForm()\n    if form.validate_on_submit():\n        user = User.query.filter_by(username=os.environ['ONLY_USER_NAME']).first()\n        if user is None or not user.check_password(form.password.data):\n            flash('Invalid username or password')\n            return redirect(url_for('login'))\n        login_user(user, remember=form.remember_me.data)\n        next_page = request.args.get('next')\n        if not next_page or url_parse(next_page).netloc != '':\n            next_page = url_for('welcome')\n        if user.Is_adm ==1:\n            next_page = url_for('AdminWelcome')\n        return redirect(next_page)\n    return render_template('Can_log.html', title='Sign In', form=form)", "\n# route for welcome page\n@app.route('/welcome', methods=['GET', 'POST'])\n@login_required\ndef welcome():\n    # tmp0 = list(paper.query.limit(10))\n    # https://stackoverflow.com/a/60815/7290857\n    arxivID_list = db.session.execute(db.select(paper.arxivID).order_by(sqlalchemy.sql.expression.func.random()).limit(10)).all()\n    tmp0 = [paper.query.filter_by(arxivID=x[0]).first() for x in arxivID_list]\n    return render_template('welcome.html', title='Home', user=current_user, paper_list=tmp0)", "\n# route for welcome page of administrator\n@app.route('/AdminWelcome', methods=['GET', 'POST'])\ndef AdminWelcome():\n    return render_template('AdminWelcome.html')\n\n# route for logout, redirect them to root page\n@app.route('/logout')\ndef logout():\n    logout_user()\n    return redirect(url_for('index'))", "def logout():\n    logout_user()\n    return redirect(url_for('index'))\n\n# # route for register\n# @app.route('/register', methods=['GET', 'POST'])\n# def register():\n#     form = RegistrationForm()\n#     if form.validate_on_submit():\n#         user = User(username=form.username.data, email=form.email.data)", "#     if form.validate_on_submit():\n#         user = User(username=form.username.data, email=form.email.data)\n#         user.set_password(form.password.data)\n#         db.session.add(user)\n#         db.session.commit()\n#         if current_user.is_authenticated:\n#             flash('Add a user')\n#         else:\n#             flash('Congratulations, you are now a registered user!')\n#         return redirect(url_for('login'))", "#             flash('Congratulations, you are now a registered user!')\n#         return redirect(url_for('login'))\n#     return render_template('Register.html', title='Register', form=form)\n\n# route for get paper\n@app.route('/paper/<arxivID>', methods=['GET', 'POST'])\n@login_required\ndef get_paper(arxivID):\n    tmp0 = paper.query.filter_by(arxivID=arxivID).first()\n    return render_template('paper.html', paper=tmp0, title='paper_chat')", "\n@app.route('/search', methods=['POST'])\n@login_required\ndef search_paper():\n    arxivID = str(request.form.get('arxivID')).strip()\n    tmp0 = paper.query.filter_by(arxivID=arxivID).first()\n    if tmp0 is None:\n        # TODO: add this arxivID to paper_parse_queue table\n        db.session.add(paper_parse_queue(arxivID=arxivID))\n        db.session.commit()\n        flash(\"No such paper yet! we will add it soon! please search again 1 minute later (usually).\")\n        ret = redirect(url_for('welcome'))\n    else:\n        ret = redirect(url_for('get_paper', arxivID=arxivID))\n    return ret", "\n# route for chat response\n@app.route('/chat_response', methods=['GET', 'POST'])\n@login_required\ndef chat_response():\n    if request.method == 'POST':\n        prompt = request.form['prompt']\n        res = {}\n        res['answer'] = controller.reply_message(current_user.get_id(), request.form['arxivID'], prompt)\n    return jsonify(res), 200", "\n# route to remove, add and make other users admin\n@app.route('/User_management', methods=['GET', 'POST'])\n@login_required\ndef User_management():\n    if current_user.if_adm() != 1:\n        return redirect(url_for('welcome'))\n    users = User.query.all()\n\n    return render_template('User_management.html', title='User_management', Users_list=users)", "\n# get user id for delete\n@app.route('/delete/<user_id>')\n@login_required\ndef delete_user(user_id):\n    if current_user.if_adm() != 1:\n        return redirect(url_for('welcome'))\n    controller.delete_user(user_id)\n    flash(\"User\"+str(user_id)+ \" has been deleted!\")\n    return redirect(url_for('User_management'))", "\n# make user an admin\n@app.route('/make_admin/<user_id>')\n@login_required\ndef make_admin(user_id):\n    if current_user.if_adm() != 1:\n        return redirect(url_for('welcome'))\n    controller.make_admin(user_id)\n    flash(\"User\"+str(user_id)+ \" now is admin!\")\n    return redirect(url_for('User_management'))", "\n\n"]}
{"filename": "app/controller/_old_version.py", "chunked_list": ["import sqlite3\nimport time\n\nfrom .gpt_utils import ArxivChatGPT\nfrom .database_utils import sqlite_get_arxivID_by_paper_id\n\n# The functions are used to interact with the database\n\ndef reply_message(user_id, arxivID, content):\n    # conn = sqlite3.connect('app.db')\n    # print('opened database successfully')\n    # c = conn.cursor()\n    # sql_query = \"INSERT INTO message (pid, uid, content, time) VALUES (\" + str(paper_id) + \", \" + str(user_id) + \", '\" + content + \"', datetime('now'))\"\n    # c.execute(sql_query)\n    # conn.commit()\n    # print('successfully inserted')\n    chatgpt = ArxivChatGPT(use_local_npy=True)\n    chatgpt.select(arxivID, print_meta_info=False)\n    ret = chatgpt.chat(content, tag_print=False, tag_return=True)\n    return ret", "def reply_message(user_id, arxivID, content):\n    # conn = sqlite3.connect('app.db')\n    # print('opened database successfully')\n    # c = conn.cursor()\n    # sql_query = \"INSERT INTO message (pid, uid, content, time) VALUES (\" + str(paper_id) + \", \" + str(user_id) + \", '\" + content + \"', datetime('now'))\"\n    # c.execute(sql_query)\n    # conn.commit()\n    # print('successfully inserted')\n    chatgpt = ArxivChatGPT(use_local_npy=True)\n    chatgpt.select(arxivID, print_meta_info=False)\n    ret = chatgpt.chat(content, tag_print=False, tag_return=True)\n    return ret", "\n\ndef get_papers(pid):\n    #connect to database\n    conn = sqlite3.connect('app.db')\n    print('opened database successfully')\n    c = conn.cursor()\n    cursor = c.execute(\"SELECT * FROM paper where pid = \" + str(pid))\n    for row in cursor:\n        cur_tup = (row[0])\n    return cur_tup", "\n# delete a user using his/her id\ndef delete_user(user_id):\n    conn = sqlite3.connect('app.db')\n    print('opened database successfully')\n    c = conn.cursor()\n    sql_query = \"DELETE FROM User where id = \" + str(user_id)\n    c.execute(sql_query)\n    conn.commit()\n    sql_query = \"DELETE FROM answer where user_id = \" + str(user_id)\n    c.execute(sql_query)\n    conn.commit()\n    print('successfully deleted')", "\n# make a user administrator\ndef make_admin(user_id):\n    conn = sqlite3.connect('app.db')\n    print('opened database successfully')\n    c = conn.cursor()\n    sql_query = \"UPDATE User SET Is_adm = 1 WHERE id = \" + str(user_id)\n    c.execute(sql_query)\n    conn.commit()\n    print('updated successfully')", "\n"]}
{"filename": "app/controller/__init__.py", "chunked_list": ["from ._old_version import get_papers, delete_user, make_admin, reply_message\n\nfrom .gpt_utils import ArxivChatGPT\n"]}
{"filename": "app/controller/gpt_utils.py", "chunked_list": ["import os\nimport json\nimport requests\nfrom tqdm import tqdm\nimport openai\nimport openai.embeddings_utils\nimport numpy as np\n\nfrom .database_utils import sqlite3_load_all_paper_from, vector_database_find_close_chunk\n# from .crawl_utils import crawl_one_arxiv_paper", "from .database_utils import sqlite3_load_all_paper_from, vector_database_find_close_chunk\n# from .crawl_utils import crawl_one_arxiv_paper\n\n# caller's duty to set openai.api_key\nclass NaiveChatGPT:\n    def __init__(self) -> None:\n        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n        self.response = None #for debug only\n\n    def reset(self):\n        self.message_list = self.message_list[:1]\n\n    def chat(self, message='', tag_print=True, tag_return=False):\n        message = str(message)\n        if message: #skip if empty\n            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n            tmp0 = self.response.choices[0].message.content\n            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n            if tag_print:\n                print(tmp0)\n            if tag_return:\n                return tmp0", "\n\nclass ContextChatGPT:\n    def __init__(self):\n        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the context below, \"\n                              \"and if the question can't be answered based on the context, say \\\"I don't know\\\"\"},]\n        self.response = None\n\n    def reset(self):\n        self.message_list = self.message_list[:1]\n\n    def set_context(self, context, use_gpt_reply=False):\n        tmp0 = '\\nAbove is some context, no need to reply and just acknowledge it with \"...\"'\n        self.message_list.append({'role':'user', 'content': context+tmp0})\n        if use_gpt_reply:\n            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n            tmp0 = self.response.choices[0].message.content\n            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n        else:\n            self.message_list.append({\"role\": \"assistant\", \"content\": '...'})\n\n    def chat(self, message, tag_print=True, tag_return=False):\n        message = str(message)\n        if message: #skip if empty\n            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n            tmp0 = self.response.choices[0].message.content\n            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n            if tag_print:\n                print(tmp0)\n            if tag_return:\n                return tmp0", "\n\nclass ArxivChatGPT:\n    def __init__(self, use_local_npy=False):\n        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the context below, \"\n                              \"and if the question can't be answered based on the context, say \\\"I don't know\\\"\"},]\n        self.response = None\n\n        self._db_paper_list = sqlite3_load_all_paper_from()\n        self.arxivID_list = [x['arxivID'] for x in self._db_paper_list]\n\n        self._db_paper_i = None\n        self.use_local_npy = use_local_npy\n\n    # def add_arxiv_paper_to_db(self, arxivID):\n    #     assert isinstance(arxivID, str)\n    #     crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=True)\n    #     self._db_paper_list = sqlite3_load_all_paper_from()\n    #     self.arxivID_list = [x['arxivID'] for x in self._db_paper_list]\n\n    def list_arxiv(self, num_print=-1):\n        db_paper_list = self._db_paper_list\n        if num_print>0:\n            db_paper_list = db_paper_list[:num_print]\n        for ind0,x in enumerate(db_paper_list):\n            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], x['meta_info_json_path'])\n            with open(tmp0, 'r') as f:\n                meta_info = json.load(f)\n            print(f'[{ind0}]', x['arxivID'], meta_info['title'])\n\n    def select(self, index, print_meta_info=True):\n        if isinstance(index, str):\n            arxivID = index\n            if arxivID not in self.arxivID_list:\n                print(f'Error: {arxivID} not in arxivID_list')\n                return\n            index = self.arxivID_list.index(arxivID)\n        else:\n            if (index<0) or (index>=len(self.arxivID_list)):\n                print(f'Error: index {index} out of range [0, {len(self.arxivID_list)-1}]')\n                return\n        self._db_paper_i = self._db_paper_list[index]\n        if print_meta_info:\n            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['meta_info_json_path'])\n            with open(tmp0, 'r') as f:\n                meta_info = json.load(f)\n            for key,value in meta_info.items():\n                print(f'[{key}]: {value}')\n        self.reset()\n\n    def reset(self):\n        self.message_list = self.message_list[:1]\n\n    def set_context(self, context, use_gpt_reply=False):\n        tmp0 = '\\nAbove is some context, no need to reply and just acknowledge it with \"...\"'\n        self.message_list.append({'role':'user', 'content': context+tmp0})\n        if use_gpt_reply:\n            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n            tmp0 = self.response.choices[0].message.content\n            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n        else:\n            self.message_list.append({\"role\": \"assistant\", \"content\": '...'})\n\n    def _find_related_chunk(self, message):\n        max_context_len = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_QA'])\n        if self.use_local_npy:\n            q_embedding = openai.Embedding.create(input=message, engine='text-embedding-ada-002')['data'][0]['embedding']\n            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['chunk_text_json_path'])\n            assert os.path.exists(tmp0)\n            with open(tmp0, 'r') as fid:\n                chunk_text_list = json.load(fid)\n                chunk_text_len_list = np.array([x[1] for x in chunk_text_list])\n                chunk_text_str_list = [x[0] for x in chunk_text_list]\n            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['arxivID'], 'chunk-vector.npy')\n            assert os.path.exists(tmp0)\n            embedding_np = np.load(tmp0)\n            distance = np.array(openai.embeddings_utils.distances_from_embeddings(q_embedding, embedding_np, distance_metric='cosine')) # 0: cloest\n            ind0 = np.argsort(distance)\n            tmp0 = np.nonzero((chunk_text_len_list[ind0] + 4).cumsum() > max_context_len)[0].min()\n            context_text_list = [chunk_text_str_list[x] for x in ind0[:tmp0]]\n        else:\n            context_text_list = vector_database_find_close_chunk(self._db_paper_i['arxivID'], message, max_context_len)\n        for x in context_text_list:\n            self.set_context(x, use_gpt_reply=False)\n\n    def chat(self, message, tag_reset=True, tag_print=True, tag_return=False):\n        assert self._db_paper_i is not None\n        if tag_reset:\n            self.reset()\n        message = str(message)\n        if message: #skip if empty\n            self._find_related_chunk(message)\n            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n            tmp0 = self.response.choices[0].message.content\n            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n            if tag_print:\n                print(tmp0)\n            if tag_return:\n                return tmp0", ""]}
{"filename": "app/controller/database_utils.py", "chunked_list": ["import os\nimport math\nimport sqlite3\nimport weaviate\nimport numpy as np\n\n# TODO\n\ndef sqlite_insert_paper_list(paper_list):\n    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    for arxivID, x in paper_dict.items():\n        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n        # sql_conn.execute('SELECT * FROM paper').fetchall()\n    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n    tmp2 = \",\".join(tmp0)\n    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n    sql_conn.commit()\n    sql_conn.close()", "def sqlite_insert_paper_list(paper_list):\n    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    for arxivID, x in paper_dict.items():\n        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n        # sql_conn.execute('SELECT * FROM paper').fetchall()\n    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n    tmp2 = \",\".join(tmp0)\n    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n    sql_conn.commit()\n    sql_conn.close()", "\n\ndef sqlite_get_arxivID_by_paper_id(pid):\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    arxivID = sql_conn.execute('SELECT arxivID FROM paper WHERE pid = ?', (pid,)).fetchone()[0]\n    sql_conn.close()\n    return arxivID\n\n\ndef sqlite3_load_all_paper_from():\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n    sql_conn.close()\n    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n    return ret", "\ndef sqlite3_load_all_paper_from():\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n    sql_conn.close()\n    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n    return ret\n\n# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')", "\n# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')\n# for x in sqlite3_load_all_paper_from():\n#     print(x)\n\n\ndef init_sqlite3_database(remove_if_exist=False):\n    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n    if remove_if_exist:\n        sql_conn.execute('DROP TABLE IF EXISTS paper')\n    cmd = '''create table if not exists paper (\n        pid integer primary key,\n        arxivID text,\n        meta_info_json_path text,\n        pdf_path text,\n        tex_path text,\n        chunk_text_json_path text,\n        num_chunk integer\n    )\n    '''\n    sql_conn.execute(cmd)\n    sql_conn.commit()\n    sql_conn.close()", "\nWeaviate_Paper_schema = {\n    \"class\": \"Paper\",\n    \"description\": \"A collection of arxiv paper\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n          \"model\": \"ada\",\n          \"modelVersion\": \"002\",\n          \"type\": \"text\"", "          \"modelVersion\": \"002\",\n          \"type\": \"text\"\n        }\n    },\n    \"properties\": [{\n        \"name\": \"chunk\",\n        \"description\": \"chunk contents of the paper\",\n        \"dataType\": [\"text\"]\n    },\n    {", "    },\n    {\n        \"name\": \"arxiv_id\",\n        \"description\": \"arxiv ID\",\n        \"dataType\": [\"string\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"num_chunk\",\n        \"description\": \"total number of chunk\",", "        \"name\": \"num_chunk\",\n        \"description\": \"total number of chunk\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"num_token\",\n        \"description\": \"number of token\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }", "        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    },\n    {\n        \"name\": \"index\",\n        \"description\": \"index of the chunk\",\n        \"dataType\": [\"int\"],\n        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n    }]\n}", "    }]\n}\n\n\ndef init_vector_database(remove_if_exist=False):\n    client = _get_vector_database(with_openai_api_key=False)\n    tag_exist = 'Paper' in {x['class'] for x in client.schema.get()['classes']}\n    if remove_if_exist and tag_exist:\n        client.schema.delete_class('Paper')\n        tag_exist = False\n    if not tag_exist:\n        client.schema.create_class(Weaviate_Paper_schema)", "\n\n'''\nchunk: text\narxiv_id: string\nindex: int\nnum_token: int\nnum_chunk: int\nvector (ada-002)\n'''", "vector (ada-002)\n'''\n\n\ndef _get_vector_database(with_openai_api_key):\n    tmp0 = weaviate.auth.AuthApiKey(os.environ['WEAVIATE_API_KEY'])\n    if with_openai_api_key:\n        tmp1 = {\"X-OpenAI-Api-Key\": os.environ['OPENAI_API_KEY']} #optional\n    else:\n        tmp1 = None\n    client = weaviate.Client(url=os.environ['WEAVIATE_API_URL'], auth_client_secret=tmp0, additional_headers=tmp1)\n    return client", "\n\ndef vector_database_insert_paper(arxivID, text_chunk_list, vector_list=None):\n    # text_chunk_list(list,tuple(str,int))\n    client = _get_vector_database(with_openai_api_key=True)\n    num_chunk = len(text_chunk_list)\n    uuid_list = []\n    if vector_list is not None:\n        assert len(vector_list)==num_chunk\n    with client.batch as batch:\n        batch.batch_size = 20 #20-100\n        # https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\n        # client.batch.configure(batch_size=10,  dynamic=True, timeout_retries=3)\n        for ind0 in range(num_chunk):\n            tmp0 = dict(arxiv_id=arxivID, num_chunk=num_chunk, index=ind0, chunk=text_chunk_list[ind0][0], num_token=text_chunk_list[ind0][1])\n            tmp1 = vector_list[ind0] if vector_list is not None else None\n            uuid_list.append(batch.add_data_object(tmp0, class_name='Paper', vector=tmp1))\n    return uuid_list", "\n\ndef vector_database_contains_paper(arxivID:str):\n    client = _get_vector_database(with_openai_api_key=False)\n    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n    num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n    return num_chunk>0\n\n\ndef vector_database_retrieve_paper(arxivID:str, index=None):\n    client = _get_vector_database(with_openai_api_key=False)\n    # TODO handle error\n    if index is None:\n        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n        assert num_chunk>0\n        # TODO batch_size=100\n        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n        text_chunk_list = [x['chunk'] for x in tmp1]\n        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n        ret = text_chunk_list\n    else:\n        raise NotImplementedError\n        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        # text_chunk = ''\n        # vector_np = np.zeros(1536, dtype=np.float64)\n        # return text_chunk, vector_np\n    return ret", "\ndef vector_database_retrieve_paper(arxivID:str, index=None):\n    client = _get_vector_database(with_openai_api_key=False)\n    # TODO handle error\n    if index is None:\n        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n        assert num_chunk>0\n        # TODO batch_size=100\n        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n        text_chunk_list = [x['chunk'] for x in tmp1]\n        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n        ret = text_chunk_list\n    else:\n        raise NotImplementedError\n        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n        # text_chunk = ''\n        # vector_np = np.zeros(1536, dtype=np.float64)\n        # return text_chunk, vector_np\n    return ret", "\n\ndef vector_database_find_close_chunk(arxivID, message, max_context_len):\n    client = _get_vector_database(with_openai_api_key=True)\n    nearText = {\"concepts\": [message]}\n    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n    result = client.query.get(\"Paper\", [\"chunk\", \"num_token\"]).with_near_text(nearText).with_where(tmp0).with_additional(['certainty']).with_limit(10).do()\n    certainty = [x['_additional']['certainty'] for x in result['data']['Get']['Paper']] #in descending order\n    num_token_list = np.array([x['num_token'] for x in result['data']['Get']['Paper']])\n    chunk_text_str_list = [x['chunk'] for x in result['data']['Get']['Paper']]\n    np.nonzero((num_token_list + 4).cumsum() <= max_context_len)\n    tmp0 = np.nonzero((num_token_list + 4).cumsum() <= max_context_len)[0][-1] + 1\n    ret = chunk_text_str_list[:tmp0]\n    return ret", ""]}
