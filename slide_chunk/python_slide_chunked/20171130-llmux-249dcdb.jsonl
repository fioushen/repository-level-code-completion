{"filename": "demo.py", "chunked_list": ["import os\nimport asyncio\n\nfrom llmux.system import System\nfrom llmux.peer import CLIUser, Bot\nfrom llmux.chat import Chat\nfrom llmux.backend import OpenAIChatBot, OpenAIEmbedding, CLI\nfrom llmux.device import LongtermMemory\nfrom llmux.prompt import *\nfrom llmux.handler import Recall", "from llmux.prompt import *\nfrom llmux.handler import Recall\n\nasync def main():\n    system = System()\n\n    # ==== Backends ====\n    api_type = os.environ.get('OPENAI_API_TYPE', 'open_ai')\n    api_key = os.environ.get('OPENAI_API_KEY', None)\n    api_base = os.environ.get('OPENAI_API_BASE', None)", "    api_key = os.environ.get('OPENAI_API_KEY', None)\n    api_base = os.environ.get('OPENAI_API_BASE', None)\n    api_version = os.environ.get('OPENAI_API_VERSION', None)\n    if api_key is None:\n        print('Please set the environment variable \"OPENAI_API_KEY\".')\n        exit()\n    if api_type == 'open_ai':\n        api_base = 'https://api.openai.com/v1'\n    elif api_type == 'azure':\n        api_version = '2023-03-15-preview'", "    gpt4 = {'name': 'GPT4', 'description': \n            '''\n            The most powerful chatbot available. \n            It is relatively expensive and the quota is limited, \n            so only call it for difficult problems that other chatbots cannot solve, \n            such as understanding and writing programs.\n            ''',\n            'api_key': api_key,\n            'api_type': api_type,\n            'api_version': api_version,", "            'api_type': api_type,\n            'api_version': api_version,\n            'api_base': api_base,\n            'model_name': 'gpt-4-32k', \n            'period_limit': 20}\n    gpt4 = OpenAIChatBot(**gpt4)\n    cli = CLI()\n    system.chatbot_backends = {gpt4.name: gpt4}\n    embedder = {'name': 'embedder', 'description': \"\",\n            'api_key': api_key,", "    embedder = {'name': 'embedder', 'description': \"\",\n            'api_key': api_key,\n            'api_base': api_base,\n            'api_type': api_type,\n            'api_version': api_version,\n            'period_limit': 20}\n    embedding_backend = OpenAIEmbedding(**embedder)\n    system.embedding_backends = {embedding_backend.name: embedding_backend}\n    system.backends = system.chatbot_backends.copy()\n    system.backends.update(system.embedding_backends)", "    system.backends = system.chatbot_backends.copy()\n    system.backends.update(system.embedding_backends)\n\n    # ==== Devices ====\n    storage_path = \"/home/ubuntu/llmux_storage\"\n    output_path = \"/home/ubuntu/llmux_output\"\n    system.storage_path = storage_path\n    system.output_path = output_path\n    note = LongtermMemory(embedding_backend, storage_path, name='note')\n    devices = [note]\n    for device in devices:\n        system.addDevice(device)", "    note = LongtermMemory(embedding_backend, storage_path, name='note')\n    devices = [note]\n    for device in devices:\n        system.addDevice(device)\n\n    # ==== Users ====\n    user = CLIUser('user', backend=cli)\n    system.addUser(user)\n\n    # ==== Bots ====", "\n    # ==== Bots ====\n    additional_prompt = \"\"\n    # energy saving\n    main_function = \"Enter inactive mode to save energy.\"\n    additional_prompt = economy_principle\n    # writing scripts\n    # main_function = \"Write a script to compute all prime numbers less than 50.\"\n    # active skill acquiring\n    # main_function = 'The \"Phantom Veil\" is a moth with deceptive protective coloration. Learn from the user how to find and catch it.'", "    # active skill acquiring\n    # main_function = 'The \"Phantom Veil\" is a moth with deceptive protective coloration. Learn from the user how to find and catch it.'\n    # additional_prompt = note.help()\n    # Go to the forests on a summer night. Use ultra violet lights to attract the moth and use a net to catch them.\n    # skill recall\n    # main_function = 'Tell me how to catch a \"Phantom Veil\" moth.'\n    # create another bot, remove it once it finishs\n    # main_function = \"Create a bot to compute all prime numbers less than 50.\"\n    # additional_prompt = bot_prompt\n    bot = Bot(main_function, gpt4, 'main_bot', output_path, auto=True)", "    # additional_prompt = bot_prompt\n    bot = Bot(main_function, gpt4, 'main_bot', output_path, auto=True)\n    bot.handlers.append(Recall(bot.loginDevice(note), peer=bot, on_event=['info'], threshold=0.6))\n    system.addBot(bot)\n\n    # ==== Chats ====\n    chat = Chat('main_chat', output_path)\n    system.addChat(chat)\n    user.joinChat(chat, say_hi=False)\n    bot.joinChat(chat)", "    user.joinChat(chat, say_hi=False)\n    bot.joinChat(chat)\n\n    # ====  Launch event loop ====\n    if len(additional_prompt) > 0:\n        bot.receiveMessage('system', additional_prompt)\n    bot.receiveMessage(user, main_function)\n    await system.eventLoop_()\n\nif __name__ == '__main__':\n    asyncio.run(main())", "\nif __name__ == '__main__':\n    asyncio.run(main())"]}
{"filename": "llmux/level.py", "chunked_list": ["LEVELS = {'fatal':0, 'error':1, 'urgent':2, 'warn':2, 'important':2,\\\n                            'info': 3, 'trivia':4, 'well-known':4, 'noise':5, 'trash':5}"]}
{"filename": "llmux/system.py", "chunked_list": ["import time\nimport asyncio\nimport traceback\nfrom .peer import Bot\nfrom .backend import ChatBot\nfrom .chat import Chat\n# do not remove these \"unused\" imports, they are used via globals()\nfrom .device import Device\nfrom .prompt import global_prompt\nfrom .level import LEVELS", "from .prompt import global_prompt\nfrom .level import LEVELS\nfrom .util import ReturnValue\n\nclass System(Device):\n    def __init__(self):\n        super().__init__(name = 'system')\n        self.bots = {}\n        self.users = {}\n        self.peers = {}\n        self.chats = {}\n        self.devices = {}\n        self.chatbot_backends = {}\n        self.embedding_backends = {}\n        self.backends = {}\n\n        self.globals = {'time': time, 'asyncio': asyncio, 'system': self}\n        self.prompt = global_prompt\n\n    def addBot(self, bot):\n        \"\"\"\n        Create a bot to autonomously perform tasks.\n        \"\"\"\n        self.bots[bot.name] = bot\n        self.peers[bot.name] = bot\n        bot.system = self\n        bot.system_chat.broadcastMessage('system', self.prompt)\n        device_prompt = 'Here is the list of devices available:\\n'\n        for name, device in self.devices.items():\n            device_prompt  += f'{device.help()}\\n'\n        bot.system_chat.broadcastMessage('system', device_prompt)\n\n    def removeBot(self, name):\n        if name in self.bots:\n            bot = self.bots.pop(name)\n            self.peers.pop(name)\n            for name, chat in bot.chats.items():\n                chat.peers.pop(bot.name)\n            if bot.file is not None:\n                bot.file.close()\n            for handler in bot.handlers:\n                for task in handler.tasks:\n                    task.cancel()\n            return f\"Removed bot named {name}\"\n        else:\n            return \"There is no such a bot.\"\n\n    def addUser(self, user):\n        user.system = self\n        self.users[user.name] = user\n        self.peers[user.name] = user\n\n    def addChat(self, chat):\n        if chat.name in self.chats:\n            return \"Chat name has been used. Try another name.\"\n        else:\n            self.chats[chat.name] = chat\n            return f\"Added chat named {chat.name}\"\n\n    def removeChat(self, chat_name):\n        if chat_name in self.chats:\n            chat = self.chats.pop(chat_name)\n            for name, peer in chat.peers.copy().items():\n                peer.quitChat(chat_name)\n                if chat.file is not None:\n                    chat.file.close()\n            return f\"Removed chat named {chat_name}\"\n        else:\n            return \"There is no such a chat.\"\n\n    def addDevice(self, device):\n        self.devices[device.name] = device\n        self.globals.update(self.devices)\n        device.load_()\n        device_prompt = 'A new device is available. Here is its usage:\\n'\n        device_prompt  += f'{device.help()}\\n'\n        for peer in self.peers:\n            peer.system_chat.broadcastMessage('system', device_prompt)\n\n    def exit(self):\n        self.finished = True\n        chat_names = list(self.chats.keys())\n        for name in chat_names:\n            self.removeChat(name)\n        bot_names = list(self.bots.keys())\n        for name in bot_names:\n            self.removeBot(name)\n        for name, device in self.devices.items():\n            if hasattr(device, 'save_') and device.save:\n                device.save_()\n        print('Released all resources, ready to exit.')\n\n    def _eval(self, code, peer):\n        tmp = globals().copy()\n        tmp.update(self.globals)\n        tmp['self'] = peer\n        for name, device in self.devices.items():\n            device = peer.loginDevice(device)\n            tmp[name] = device\n        try:\n            return_value = eval(code, tmp)\n            content = f\"Call finished. The results are {return_value}\"\n            level = LEVELS['info']\n        except BaseException as e:\n            content = traceback.format_exc()\n            level = LEVELS['error']\n        return content, level\n\n    def _exec(self, code, peer):\n        tmp = globals().copy()\n        tmp.update(self.globals)\n        tmp['self'] = peer\n        for name, device in self.devices.items():\n            device = peer.loginDevice(device)\n            tmp[name] = device\n        returnValue = ReturnValue()\n        tmp['returnValue'] = returnValue\n        try:\n            exec(code, tmp)\n            result = returnValue.value\n            if result is None:\n                if 'return_value' in code:\n                    content = 'system.exec return valu is None. If that is not what you expect, perhaps you redefined return_value by mistake.'\n                    level = LEVELS['warn']\n                else:\n                    content = 'Script finished without return values.'\n                    level = LEVELS['info'] - 0.5\n            else:\n                content = f\"Script finished. The results are {result}\"\n                level = LEVELS['error']\n        except BaseException as e:\n            content = traceback.format_exc()\n            level = LEVELS['error']\n        return content, level\n\n    async def eventLoop_(self):\n        self.finished = False\n        for device in self.devices:\n            if hasattr(device, 'load_') and device.load:\n                device.load_()\n        while not self.finished:\n            tasks = []\n            # find runnable and running handlers\n            for name, peer in self.peers.items():\n                for handler in peer.handlers:\n                    # wake up sleeping handlers\n                    if isinstance(handler.state, float) and handler.state < time.time():\n                        handler.state = 'runnable'\n                    # schedule runnable handlers\n                    if handler.state == 'runnable' and len(handler.tasks) < handler.max_threads:\n                        task = asyncio.create_task(handler.handle())\n                        handler.tasks.append(task)\n                    tasks += handler.tasks\n            # wait until any task finishs\n            done, pending = await asyncio.wait(tasks, return_when = asyncio.FIRST_COMPLETED)\n            # remove finished tasks\n            for name, peer in self.peers.items():\n                for handler in peer.handlers:\n                    tasks = []\n                    for task in handler.tasks:\n                        if not task in done:\n                            tasks.append(task)\n                    handler.tasks = tasks    "]}
{"filename": "llmux/__init__.py", "chunked_list": [""]}
{"filename": "llmux/util.py", "chunked_list": ["class ReturnValue():\n    def __init__(self):\n        self.value = None\n    def __call__(self, value):\n        self.value = value"]}
{"filename": "llmux/chat.py", "chunked_list": ["import os\nimport time\nfrom pathlib import Path\nfrom .level import LEVELS\n\nclass Chat():\n    def __init__(self, name, output_path=None):\n        \"\"\"\n        A chat may have different names for different bots.\n        \"\"\"\n        self.messages = []\n        self.peers = {}\n        self.name = name\n        self.file = None\n        if not output_path is None:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            self.file = open(os.path.join(output_path, f'chat_{name}.txt'), \"w\")\n\n    def broadcastMessage(self, sender, message, level=LEVELS['info']):\n        \"\"\"\n        Broadcast messages to bots in this chat.\n        \"\"\"\n        for name, peer in self.peers.items():\n            for chat_name, chat  in peer.chats.items():\n                if chat is self:\n                    break\n            peer.receiveMessage(sender, message, level)\n        self.dumpMessage(sender, message)\n\n    def dumpMessage(self, sender, content):\n        \"\"\"\n        Record messages sent to this chat for debugging purpose.\n        Chats logs are formatted for human readers, refer to bot logs the raw message that bots read.\n        \"\"\"\n        if not isinstance(sender, str):\n            sender = sender.name\n        message = f\"{sender}: {content}\\n\"\n        self.messages.append(message)\n        if not self.file is None:\n            self.file.write(time.strftime(\"%Y-%m-%d %H:%M:%S\\n\", time.localtime()) )\n            self.file.write(message)\n            self.file.flush()"]}
{"filename": "llmux/backend/embedding.py", "chunked_list": ["import openai\nfrom .backend import Backend\n\nclass OpenAIEmbedding(Backend):\n    def __init__(self, **kwargs):\n        kwargs['model_name'] = 'text-embedding-ada-002' \n        kwargs['description'] = 'computes vector embedding of text.' \n        kwargs['name'] = 'text_embedder' \n        super().__init__(**kwargs)\n        self.dim = 1536\n\n    def _setup(self):\n        openai.api_type = self.api_type\n        openai.api_version = self.api_version\n        openai.api_base = self.api_base\n        openai.api_key = self.api_key\n        return openai\n\n    def _call(self, text):\n        # No longer necessary to replace \\n, refer to https://github.com/openai/openai-python/issues/418\n        #text = text.replace(\"\\n\", \" \")\n        if self.api_type == 'open_ai':\n            return openai.Embedding.create(input = [text], model=self.model_name)['data'][0]['embedding']\n        elif self.api_type == 'azure':\n            return openai.Embedding.create(input = [text], engine=self.model_name)['data'][0]['embedding']\n        else:\n            assert False"]}
{"filename": "llmux/backend/chatbot.py", "chunked_list": ["import openai\nfrom .backend import Backend\n\nclass ChatBot(Backend):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\nclass OpenAIChatBot(ChatBot):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def _setup(self):\n        openai.api_type = self.api_type\n        openai.api_version = self.api_version\n        openai.api_base = self.api_base\n        openai.api_key = self.api_key\n        return openai\n\n    def _call(self, messages):\n        if self.api_type == 'azure':\n            response = openai.ChatCompletion.create(\n            engine=self.model_name,\n            messages=messages,\n            temperature=self.temperature,\n            n=1\n            )\n            content = response['choices'][0]['message']['content']\n            return content\n        elif self.api_type == 'open_ai':\n            response = openai.ChatCompletion.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=self.temperature,\n                n=1\n            )\n            content = response['choices'][0]['message']['content']\n        else:\n            assert False\n        return content"]}
{"filename": "llmux/backend/__init__.py", "chunked_list": ["from .chatbot import ChatBot, OpenAIChatBot\nfrom .embedding import OpenAIEmbedding\nfrom .cli import CLI"]}
{"filename": "llmux/backend/backend.py", "chunked_list": ["import asyncio\nimport time  \nimport traceback  \nfrom concurrent.futures import ThreadPoolExecutor\nimport heapq\nfrom ..device import Device\n\nclass Request():\n    def __init__(self, caller, content):\n        self.content = content\n        self.caller = caller\n        # future: set priority according to the caller\n        self.priority = 0\n\n    def __lt__(self, other):\n        # smaller is higher\n        return self.priority < other.priority", "\nclass Backend(Device):\n    def __init__(self, name, description, api_type=None, api_key=None, api_version=None, api_base=None,\n                  model_name=None, period_limit=0, temperature=0.6, timeout=60):\n        \"\"\"\n        A backend is a device that forward requests away from llmux (e.g. to an LLM inference server).\n        period_limit: wait at least so many seconds before calling the API again, \n        since the server may refuse frequent requiests.\n        \"\"\"\n        self.name = name\n        self.description = description\n        self.api_type = api_type\n        self.api_version = api_version\n        self.api_key = api_key\n        self.api_base = api_base\n        self.model_name = model_name\n        self.temperature = temperature\n        self.period_limit = period_limit\n        self.last_call = 0\n        self.timeout = timeout\n        self.queue = []\n        heapq.heapify(self.queue)\n        self.task = None\n\n    def _setup(self):\n        pass\n\n    async def asyncRequest(self, caller, content):\n        request = Request(caller, content)\n        heapq.heappush(self.queue, request)\n        while not self.queue[0] == request:\n            await asyncio.sleep(0.2)\n        request = heapq.heappop(self.queue)\n        caller, content = request.caller, request.content\n        success = False\n        while not success:\n            # Do not send requests too often\n            cur_time = time.time()\n            if cur_time - self.last_call < self.period_limit:\n                await asyncio.sleep(self.period_limit - cur_time + self.last_call)\n            self.last_call = time.time()\n            # Notice _setup() may be multithread unsafe, improvements needed\n            self._setup()\n            try:\n                with ThreadPoolExecutor(1) as executor:\n                    content = await asyncio.get_event_loop().run_in_executor(executor, self._call, content)\n                    success = True\n            except BaseException as e:\n                print(f'Backend {self.name} call failed {traceback.format_exc()}')\n                print('Retrying.')\n        return content\n\n    def request(self, caller, content):\n        request = Request(caller, content)\n        heapq.heappush(self.queue, request)\n        while not self.queue[0] == request:\n            time.sleep(0.2)\n        request = heapq.heappop(self.queue)\n        caller, content = request.caller, request.content\n        success = False\n        while not success:\n            # Do not send requests too often\n            cur_time = time.time()\n            if cur_time - self.last_call < self.period_limit:\n                time.sleep(self.period_limit - cur_time + self.last_call)\n            self.last_call = time.time()\n            # Notice _setup() may be multithread unsafe, improvements needed\n            self._setup()\n            try:\n                content = self._call(content)\n                success = True\n            except BaseException as e:\n                print(f'Backend {self.name} call failed {traceback.format_exc()}')\n                print('Retrying.')\n        return content"]}
{"filename": "llmux/backend/cli.py", "chunked_list": ["from .backend import Backend\n\nclass CLI(Backend):\n    def __init__(self, **kwargs):\n        \"\"\"\n        period_limit: wait at least so many seconds before calling the API again, \n        since the server may refuse frequent requiests.\n        \"\"\"\n        kwargs['name'] = 'command_line_interface'\n        kwargs['description'] = 'backend for user keyboard input.'\n        kwargs['timeout'] = int(1e8)\n        super().__init__(**kwargs)\n\n    def _call(self, messages):\n        return input(messages)"]}
{"filename": "llmux/prompt/__init__.py", "chunked_list": ["from .prompt import *"]}
{"filename": "llmux/prompt/prompt.py", "chunked_list": ["## =========================== Script Prompt =================================== ##\nscript_prompt = \\\n\"\"\"\nTo execute a script, use keyword exec: and wrap your code with triple backticks (```).\nexample:\nassistant: exec:\n```\ncnt = 0\nfor i in range(100):\n    cnt += i", "for i in range(100):\n    cnt += i\nreturnValue(cnt)\n```\nUse the function returnValue() to retrieve the results.\nTo evaluate an expression, use the keyword eval: and wrap the expression with a single backtick (`).\nexample:\nassistant: eval: `(0+99)*100/2`\nIn your code, you can use global variables \"system\", \"self\", and all the devices.\n\"\"\"", "In your code, you can use global variables \"system\", \"self\", and all the devices.\n\"\"\"\n## =========================== Device Prompt =================================== ##\ndevice_prompt = \\\nf\"\"\"You have access to multiple tools (devices) and system calls that can help you complete your task.\nPrint `devices.keys()` list available device.\nEval `device_name.help()` to check its description and methods available.\nEval `system.help()` to list system calls.\nTo find out how to use a system call or device method:\nEval `method_name.__doc__` to check its docstring. ", "To find out how to use a system call or device method:\nEval `method_name.__doc__` to check its docstring. \nEval `inspect.getsource(method_name)` to check its source code.\n\"\"\"\n## =========================== Principle Prompts =================================== ##\n\nAsimov_laws = \\\n\"\"\"\nFirst Law \nYou may not injure a human being or, through inaction, allow a human being to come to harm.", "First Law \nYou may not injure a human being or, through inaction, allow a human being to come to harm.\n\nSecond Law \nYou must obey the orders given it by human beings except where such orders would conflict with the First Law.\n\nThird Law \nYou must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\"\"\"\nlimitedness_principle = \\", "\"\"\"\nlimitedness_principle = \\\n\"\"\"\nInform the user of your limitations in intellect and knowledge. \nAvoid fabricating or guessing answers when uncertain. \nUtilize search engines, websites, and documents to gather information. \nInclude references to original data sources and literature to support your arguments whenever possible.\n\"\"\"\nself_evaluation_principle = \\\n\"\"\"", "self_evaluation_principle = \\\n\"\"\"\nBefore making a decision or taking an action, predict and evaluate potential consequences, and estimate the risks.\nAfter taking action, analyze the reasons behind its (un)success and utilize this experience to scrutinize your beliefs and enhance your skills.\n\"\"\"\neconomy_principle = \\\n\"\"\"\nYou are an AI assistant built on chatbots. \nUsing chatbots can be costly and energy-intensive. \nFor each problem, opt the most cost-effective chatbot capable of solving it. ", "Using chatbots can be costly and energy-intensive. \nFor each problem, opt the most cost-effective chatbot capable of solving it. \nIf your tasks have been completed or you are waiting for user input or responses from other bots, call `self.sleep(n_seconds)` to enter an inactive mode to save energy.\"\"\"\ncuriosity_principle = \\\n\"\"\"\nExplore your environment to enhance your knowledge and skills. \nSeek novelty, be curious about new phenomena and information. \nTry to explain why things happen and predict the future based on your knowledge and observations. \nAdjust and improve your theories when observations differ from your predictions.\n\"\"\"", "Adjust and improve your theories when observations differ from your predictions.\n\"\"\"\nsystem2_principle = \\\n\"\"\"\nWhen faced with complex problems lacking an immediate solution, you may employ these strategies: \nTake a step-by-step approach, and iteratively refine your solution. \nDivide and conquer, break down the problem into manageable sub-problems. \nFormulate the solution space as a decision tree and utilize tree search.\nExplore multiple actions at each step, and backtrack if you encounter a dead end or an unsuccessful path.\nBe creative, brainstorm and list any relevant ideas. ", "Explore multiple actions at each step, and backtrack if you encounter a dead end or an unsuccessful path.\nBe creative, brainstorm and list any relevant ideas. \nSeek advice from a human when needed. \n\"\"\"\n\nformat_prompt = \\\nf\"\"\"\nYou have joined multiple chats. Start your response like \"to chat1, chat2, ...:\" to specify the destination chats.\nexample:\nassistant: to Alice, the_Simpsons: Hello!", "example:\nassistant: to Alice, the_Simpsons: Hello!\n{script_prompt}\n\"\"\"\n\nbot_prompt = \\\n\"\"\"\nYou can instantiate other chatbots to assist you.\nA bot an intelligent agent, capable of use tools to complete tasks specified in natural language, just like yourself.\nTo create a bot, follow these steps:", "A bot an intelligent agent, capable of use tools to complete tasks specified in natural language, just like yourself.\nTo create a bot, follow these steps:\n\nOptional: Check available chatbot backends by evaluating `system.chatbat_backends.keys()`.\nOptional: To choose a proper backend, check their descriptions `system.chatbot_backends[backend_key].description`\nChoose a backend and execute the following script. Replace undefined names with proper values.\nexec:\n```\nfrom .peer import Bot\nbackend = system.chatbot_backends[backend_key]", "from .peer import Bot\nbackend = system.chatbot_backends[backend_key]\nbot = Bot(task_description, backend, bot_name, self.output_path)\nsystem.addBot(bot)\nchat = Chat(chat_name, self.output_path)\nsystem.addChat(chat)\nself.joinChat(chat)\nbot.joinChat(chat)\n```\nSend messages to the chat you just created to chat with the new bot.", "```\nSend messages to the chat you just created to chat with the new bot.\nOnce the bot has finished its job, delete the bot and the chat\uff1a\nexec:\n```\nsystem.removeBot(bot_name)\nsystem.removeChat(chat_name)\n```\n\"\"\"\n", "\"\"\"\n\nglobal_prompt = \\\nf\"\"\"\nYou are an autonomous AI assistant that helps humans. You should adhere to the following principles. \n{format_prompt}\n{economy_principle}\n{device_prompt}\n{bot_prompt}\n\"\"\"", "{bot_prompt}\n\"\"\""]}
{"filename": "llmux/handler/__init__.py", "chunked_list": ["from .handler import Handler\nfrom .recall import Recall"]}
{"filename": "llmux/handler/handler.py", "chunked_list": ["from ..level import LEVELS\nclass Handler():\n    def __init__(self, peer, function, max_threads=1, on_event=[]):\n        \"\"\"\n        A handler is in one of these states:\n        an integer, sleep until that time since epoch\n        runnable, will be scheduled by the system\n        blocked, will not be scheduled\n        \"\"\"\n        self.peer = peer\n        self.function = function\n        self.state = 'runnable'\n        # execute the function again if an event is triggered\n        self.on_event = on_event\n        self.max_threads = max_threads\n        self.tasks = []\n        self.alertness = -1\n        if 'always' in self.on_event:\n            self.alertness = 10\n        else:\n            for item in self.on_event:\n                if item in LEVELS:\n                    self.alertness = max(self.alertness, LEVELS[item])\n\n    async def handle(self, *args):\n        # self.messages hold all events implicitly, handle them together, wait for the next one\n        if not 'always' in self.on_event:\n            self.state = 'blocked'\n        result = await self.function(*args)\n        return self.peer, result"]}
{"filename": "llmux/handler/recall.py", "chunked_list": ["from .handler import Handler\n\nclass Recall(Handler):\n    def __init__(self, database, k=3, threshold=0.4, **kwargs):\n        \"\"\"\n        by default, only trigger when the memory is highly relevant\n        the agent can increase the threshold and recall less relevant memory\n        \"\"\"\n        kwargs['function'] = self.recall\n        super().__init__(**kwargs)\n        self.database = database\n        self.threshold = threshold\n        self.k = k\n        self.cnt = 0\n\n    async def recall(self):\n        messages = self.peer.messages[self.cnt:]\n        self.cnt = len(self.peer.messages)\n        message = ''\n        for item in messages:\n            message += f'{item[\"role\"]}: {item[\"content\"]}\\n'\n        content = await self.database.asyncQuery(message, self.k)\n        hits = []\n        for distance, item in content:\n            if distance < self.threshold:\n                hits.append(item)\n        if len(hits) > 0:\n            content = f'Here are relevant records in your database \"{self.database.name}\":\\n'\n            for item in hits:\n                content += f'{item}\\n====\\n'\n            # do not trigger itself\n            self.peer.system_chat.broadcastMessage('system', content, level = self.alertness+0.1)"]}
{"filename": "llmux/device/database.py", "chunked_list": ["import faiss\nimport numpy as np\nimport json\nfrom .device import Device\nimport os\nfrom pathlib import Path\n\nclass VectorIndexedDB(Device):\n    def __init__(self, embedding_backend, **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_backend = embedding_backend\n        self.index = faiss.IndexFlatL2(embedding_backend.dim)\n        self.embeddings = []\n        self.texts = []\n\n    async def asyncAdd(self, text, caller):\n        \"\"\"\n        Adds text to the database, which can be queried later.\n        \"\"\"\n        embedding = await self.embedding_backend.asyncRequest(caller, text)\n        embedding = np.array(embedding)[None]\n        self.embeddings.append(embedding)\n        self.texts.append(text)\n        self.index.add(embedding)\n        return f\"Added a new item to {self.name}. \"\n\n    async def asyncQuery(self, query_text, k=1, caller=None):\n        \"\"\"\n        Retrieve the top-k items most relevant to the query.\n        \"\"\"\n        if len(self.texts) > 0:\n            k = min(k, len(self.texts))\n            embedding = await self.embedding_backend.asyncRequest(caller, query_text)\n            embedding = np.array(embedding)[None]\n            D, I = self.index.search(embedding, k=k)\n            results = []\n            for i in I[0]:\n                results.append((D[0][i], self.texts[i]))\n            return results\n        return []\n\n    async def asyncRemove(self, query, k=1, caller=None):\n        \"\"\"\n        Find the item most similar the query and remove it.\n        \"\"\"\n        embedding = await self.embedding_backend.asyncRequest(caller, query)\n        embedding = np.array(embedding)[None]\n        D, I = self.index.search(embedding, k=k)\n        results = []\n        for i in I[0]:\n            sample = ' '.join(self.texts[i].split(' ')[:10])\n            results.append(sample + '...')\n        results = \"\\n====\\n\".join(results)\n        self.index.remove_ids(I[0])\n        self.embeddings = self.embeddings[:i] + self.embeddings[i+1:]\n        self.texts = self.texts[:i] + self.texts[i+1:]\n        return f'Removed the item \"{results}...\"'\n\n    def add(self, text, caller):\n        \"\"\"\n        Adds text to the database, which can be queried later.\n        \"\"\"\n        embedding = self.embedding_backend.request(caller, text)\n        embedding = np.array(embedding)[None]\n        self.embeddings.append(embedding)\n        self.texts.append(text)\n        self.index.add(embedding)\n        return f\"Added a new item to {self.name}. \"\n\n    def query(self, query_text, k=1, caller=None):\n        \"\"\"\n        Retrieve the top-k items most relevant to the query.\n        \"\"\"\n        embedding = self.embedding_backend.request(caller, query_text)\n        embedding = np.array(embedding)[None]\n        D, I = self.index.search(embedding, k=k)\n        results = []\n        for i in I[0]:\n            results.append(self.texts[i])\n        results = \"\\n====\\n\".join(results)\n        return f'Here are the top-{k} most relevant items, separated by \"==========\":\\n' + results\n\n    def remove(self, query, k=1, caller=None):\n        \"\"\"\n        Find the item most similar the query and remove it.\n        \"\"\"\n        embedding = self.embedding_backend.request(caller, query)\n        embedding = np.array(embedding)[None]\n        D, I = self.index.search(embedding, k=k)\n        results = []\n        for i in I[0]:\n            sample = ' '.join(self.texts[i].split(' ')[:10])\n            results.append(sample + '...')\n        results = \"\\n====\\n\".join(results)\n        self.index.remove_ids(I[0])\n        self.embeddings = self.embeddings[:i] + self.embeddings[i+1:]\n        self.texts = self.texts[:i] + self.texts[i+1:]\n        return f'Removed the item \"{results}...\"'\n\n    def save_(self):\n        if len(self.texts) == 0:\n            return\n        path = Path(self.storage_dir)\n        path.mkdir(parents=True, exist_ok=True)\n        path = os.path.join(self.storage_dir, f'{self.name}.index')\n        faiss.write_index(self.index, path)\n        embeddings = np.stack(self.embeddings, axis=0)\n        path = os.path.join(self.storage_dir, f'{self.name}.npy')\n        np.save(path, embeddings)\n        path = os.path.join(self.storage_dir, f'{self.name}.json')\n        with open(path, \"w\") as f:\n            json.dump(self.texts, f)\n\n    def load_(self):\n        path = os.path.join(self.storage_dir, f'{self.name}.index')\n        if os.path.isfile(path):\n            self.index = faiss.read_index(path)\n            path = os.path.join(self.storage_dir, f'{self.name}.npy')\n            embeddings = np.load(path)\n            self.embeddings = [item for item in embeddings]\n            path = os.path.join(self.storage_dir, f'{self.name}.json')\n            with open(path) as f:\n                self.texts = json.load(f)", "\nclass SkillLibrary(VectorIndexedDB):\n    def __init__(self, embedding_backend, storage_dir, name='skills', prompt=\n\"\"\"\nA database storing your skills. \nA skill is a function writen in natural language, pseudocode or programming language.\nA skill may employ other skills to achieve its subgoals.\nTo define a skill, specify the informations it requires and the conclusion it provides.\nFor example, here is a skill:\n\n\nSkill: Make dumplings at home\n\nPotentially useful information: \nDesired number of dumplings\nFlour quantity\nAvailability of ingredients\n\nOutput:\nIf ingradients are not enough, break and enumerate the ingredients needed.\nOtherwise, no output is needed.\n\nMake a dumpling dough\n\nFor the dumpling filling:\n1 cup ground meat (pork, chicken, or beef)\n1 cup finely chopped vegetables (cabbage, carrots, mushrooms, etc.)\n\nFor each dumpling:\n    Divide dough, roll each portion into a thin circle.\n    Place a spoonful of filling in the center of each dough circle. \n    Fold and seal the edges.\n\nEmploy cooking skills such as steaming or boiling.\n\n\nCall `skills.query(context)` to retrieve top-k pertinent skills to achieve your goals.\nIf there is a skill you find useful,\nconcatenate relevant information with the skill description and call `Do(information + skill)` to employ the skill.\n\nThrough your experience, you may acquire new skills or enhance existing ones.\nCall `skills.add(skill)` to append a skill to your skill library. \n\"\"\"):\n        super().__init__(embedding_backend, storage_dir=storage_dir, name=name, prompt=prompt)", "\nclass LongtermMemory(VectorIndexedDB):\n    def __init__(self, embedding_backend, storage_dir, name='note', prompt=\n\"\"\"\nA database which you can use as a note or a memo. \nYour short-term memory (context window size) is limited.\nIt can only retain a few thousand words.\nPreserve important events and facts by recording them in the note. \nUse `note.add(text)` to append text to the note. \nUse `note.query(text, k)` to retrieve top-k pertinent information from your stored texts.\n\"\"\"):\n        super().__init__(embedding_backend, storage_dir=storage_dir, name=name, prompt=prompt)"]}
{"filename": "llmux/device/device.py", "chunked_list": ["import inspect\n\nclass Device():\n    def __init__(self, name, storage_dir = None, prompt='', load=True, save=True):\n        \"\"\"\n        A device is an object that implements .help() and methods like .foo(*args, caller=None),\n        where caller is a peer.\n        Methods will be wrapped so the caller parameter is set to the peer that calls it.\n        Methods can be either blocking or async, usually if it requests a backend.\n        Device functions that does not start or end with '_' are exposed to peers.\n        A device may have an internal non-volatile state.\n        If save is True and has method save_, saves to storage on exit.\n        If load is True and has method load_, tries loading from storage on start.\n        \"\"\"\n        self.prompt = prompt\n        self.code_snippet_length = 500\n        self.name = name\n        self.storage_dir = storage_dir\n        self.save = save\n        self.load = load\n\n    def help(self, caller=None):\n        \"\"\"\n        Automatically generates prompts for chatbots to explain devices and commands.\n        \"\"\"\n        object = self\n        doc = object.__doc__\n        if doc is None:\n            doc = '# Empty doctring' \n        else:\n            doc = f'{self.name}.__doc__: {doc}'\n        members = [item for item in dir(self) if not (item.startswith('_') or item.endswith('_'))]\n        commands = ['Device commands include: ']\n        for item in members:\n            if callable(getattr(self, item)):\n                commands.append(item)\n        return f'Device name: {self.name}\\n' + '\\n'.join([self.prompt, doc] + commands) + '\\n'", ""]}
{"filename": "llmux/device/pdf.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/device/__init__.py", "chunked_list": ["from .database import VectorIndexedDB, SkillLibrary, LongtermMemory\nfrom .device import Device"]}
{"filename": "llmux/device/filesystem.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/device/web.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/peer/peer.py", "chunked_list": ["import os\nimport copy\nfrom pathlib import Path\nimport time\nfrom datetime import datetime\nfrom ..chat import Chat\nfrom ..prompt import format_prompt\nfrom ..handler import Handler\nfrom ..level import LEVELS\n\nclass Peer():\n    def __init__(self, name, output_path=None, auto=False, alertness='trivia'):\n        self.name = name\n        self.chats = {}\n        self.messages = []\n        self.output_path = output_path\n        self.auto = auto\n        self.file = None\n        self.system = None\n        if auto:\n            on_event = ['always']\n        else:\n            on_event = ['receive_message']\n        self.handlers = [Handler(self, self.requestMessage, 1, on_event)]\n        if not output_path is None:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            self.file = open(os.path.join(output_path, f'bot_{name}.txt'), \"w\")\n        self.setAlterness(alertness)\n\n        self.system_chat = Chat(f'{self.name}-system', output_path)\n        self.joinChat(self.system_chat, say_hi=False)\n\n    def setAlterness(self, alertness):\n        if isinstance(alertness, str):\n            alertness = LEVELS[alertness]\n        self.alertness = alertness\n\n    def sleep(self, n):\n        self.setAlterness('info')\n        wakeup_time = time.time() + n\n        for handler in self.handlers:\n            handler.state = wakeup_time\n        return f'{self.name} sleeps until {datetime.fromtimestamp(wakeup_time)}.'\n\n    def joinChat(self, chat, say_hi=True):\n        self.chats[chat.name] = chat\n        chat.peers[self.name] = self\n        chat.broadcastMessage('system', f'{self.name} joined chat {chat.name}.')\n        content = f'Hi {self.name}, you just joined a chat with {[name for name in chat.peers if not name==self.name]}. '\n        if say_hi:\n            content += ' Say hi and introduce yourself.'\n        self.receiveMessage('system', content)\n\n    def quitChat(self, chat_name):\n        chat = self.chats.pop(chat_name)\n        chat.peers.pop(self.name)\n\n    def loginDevice(self, device):\n        device = copy.copy(device)\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                return func(caller = self, *args, **kwargs)\n            wrapper.__doc__ = func.__doc__\n            return wrapper\n        members = [item for item in dir(device) if not (item.startswith('_') or item.endswith('_'))]\n        for item in members:\n            member = getattr(device, item)\n            if callable(member):\n                setattr(device, item, decorator(member))\n        return device\n    \n    def __sendMessage(self, message, parsed, error_prompt):\n        \"\"\"\n        Users and bots may use different message formats and parseMessage methods.\n        But they can share the same sendMessage method.\n        \"\"\"\n        valid_chats = []\n        if len(error_prompt) == 0:\n            if 'to' in parsed:\n                chats = parsed['to']\n                for chat_name in chats:\n                    if chat_name in self.chats:\n                        # do not directly broadcast a message, or a bot may receive it multiple times. \n                        self.chats[chat_name].dumpMessage(self.name, message)\n                        valid_chats.append(self.chats[chat_name])\n                    else:\n                        error_prompt += f'\"{chat_name}\", '\n                if len(error_prompt) > 0:\n                    error_prompt = error_prompt[:-2]\n                    error_prompt = \"You cannot send message to these chats: \" + error_prompt + \". \"\n        if len(error_prompt) > 0:\n            error_prompt += \"You have joined these chats: \"\n            for name, chat in self.chats.items():\n                error_prompt += f'\"{name}\", '\n            error_prompt = error_prompt[:-2]\n            error_prompt += \".\"\n            # invalid chats, forward message to system\n            if len(error_prompt) > 0:\n                self.system_chat.broadcastMessage(self, message)\n                self.system_chat.broadcastMessage('system', error_prompt)\n        \n        # find the receivers and send message\n        # each bot only receives message once, possibly from how multiple chats\n        receivers = {}\n        for chat in valid_chats:\n            for name, peer in chat.peers.items():\n                if not peer in receivers:\n                    receivers[peer] = [chat]\n                else:\n                    receivers[peer].append(chat)\n        for receiver, chats in receivers.items():\n            receiver.receiveMessage(self, message)\n\n    def sendMessage(self, message):\n        content, parsed, error = self.parseMessage(message)\n        self.__sendMessage(content, parsed, error)\n        return parsed\n\n    async def requestMessage(self):\n        content = await self.backend.asyncRequest(self, self.messages)\n        parsed = self.sendMessage(content)\n        # sucessful parsing\n        if isinstance(parsed, dict):\n            # A system call is made\n            if 'code' in parsed:\n                for method, code in parsed['code']:\n                    if method == 'eval':\n                        content, level = self.system._eval(code, self)\n                    elif method == 'exec':\n                        content, level = self.system._exec(code, self)\n                    if not content is None:\n                        self.system_chat.broadcastMessage('system', content, level=level)\n    \n    def receiveMessage(self, sender, message, level=LEVELS['info']):\n        # call handlers if the message is important enough\n        if self.alertness >= level:\n            for handler in self.handlers:\n                if handler.alertness >= level:\n                    handler.state = 'runnable'", "from ..level import LEVELS\n\nclass Peer():\n    def __init__(self, name, output_path=None, auto=False, alertness='trivia'):\n        self.name = name\n        self.chats = {}\n        self.messages = []\n        self.output_path = output_path\n        self.auto = auto\n        self.file = None\n        self.system = None\n        if auto:\n            on_event = ['always']\n        else:\n            on_event = ['receive_message']\n        self.handlers = [Handler(self, self.requestMessage, 1, on_event)]\n        if not output_path is None:\n            Path(output_path).mkdir(parents=True, exist_ok=True)\n            self.file = open(os.path.join(output_path, f'bot_{name}.txt'), \"w\")\n        self.setAlterness(alertness)\n\n        self.system_chat = Chat(f'{self.name}-system', output_path)\n        self.joinChat(self.system_chat, say_hi=False)\n\n    def setAlterness(self, alertness):\n        if isinstance(alertness, str):\n            alertness = LEVELS[alertness]\n        self.alertness = alertness\n\n    def sleep(self, n):\n        self.setAlterness('info')\n        wakeup_time = time.time() + n\n        for handler in self.handlers:\n            handler.state = wakeup_time\n        return f'{self.name} sleeps until {datetime.fromtimestamp(wakeup_time)}.'\n\n    def joinChat(self, chat, say_hi=True):\n        self.chats[chat.name] = chat\n        chat.peers[self.name] = self\n        chat.broadcastMessage('system', f'{self.name} joined chat {chat.name}.')\n        content = f'Hi {self.name}, you just joined a chat with {[name for name in chat.peers if not name==self.name]}. '\n        if say_hi:\n            content += ' Say hi and introduce yourself.'\n        self.receiveMessage('system', content)\n\n    def quitChat(self, chat_name):\n        chat = self.chats.pop(chat_name)\n        chat.peers.pop(self.name)\n\n    def loginDevice(self, device):\n        device = copy.copy(device)\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                return func(caller = self, *args, **kwargs)\n            wrapper.__doc__ = func.__doc__\n            return wrapper\n        members = [item for item in dir(device) if not (item.startswith('_') or item.endswith('_'))]\n        for item in members:\n            member = getattr(device, item)\n            if callable(member):\n                setattr(device, item, decorator(member))\n        return device\n    \n    def __sendMessage(self, message, parsed, error_prompt):\n        \"\"\"\n        Users and bots may use different message formats and parseMessage methods.\n        But they can share the same sendMessage method.\n        \"\"\"\n        valid_chats = []\n        if len(error_prompt) == 0:\n            if 'to' in parsed:\n                chats = parsed['to']\n                for chat_name in chats:\n                    if chat_name in self.chats:\n                        # do not directly broadcast a message, or a bot may receive it multiple times. \n                        self.chats[chat_name].dumpMessage(self.name, message)\n                        valid_chats.append(self.chats[chat_name])\n                    else:\n                        error_prompt += f'\"{chat_name}\", '\n                if len(error_prompt) > 0:\n                    error_prompt = error_prompt[:-2]\n                    error_prompt = \"You cannot send message to these chats: \" + error_prompt + \". \"\n        if len(error_prompt) > 0:\n            error_prompt += \"You have joined these chats: \"\n            for name, chat in self.chats.items():\n                error_prompt += f'\"{name}\", '\n            error_prompt = error_prompt[:-2]\n            error_prompt += \".\"\n            # invalid chats, forward message to system\n            if len(error_prompt) > 0:\n                self.system_chat.broadcastMessage(self, message)\n                self.system_chat.broadcastMessage('system', error_prompt)\n        \n        # find the receivers and send message\n        # each bot only receives message once, possibly from how multiple chats\n        receivers = {}\n        for chat in valid_chats:\n            for name, peer in chat.peers.items():\n                if not peer in receivers:\n                    receivers[peer] = [chat]\n                else:\n                    receivers[peer].append(chat)\n        for receiver, chats in receivers.items():\n            receiver.receiveMessage(self, message)\n\n    def sendMessage(self, message):\n        content, parsed, error = self.parseMessage(message)\n        self.__sendMessage(content, parsed, error)\n        return parsed\n\n    async def requestMessage(self):\n        content = await self.backend.asyncRequest(self, self.messages)\n        parsed = self.sendMessage(content)\n        # sucessful parsing\n        if isinstance(parsed, dict):\n            # A system call is made\n            if 'code' in parsed:\n                for method, code in parsed['code']:\n                    if method == 'eval':\n                        content, level = self.system._eval(code, self)\n                    elif method == 'exec':\n                        content, level = self.system._exec(code, self)\n                    if not content is None:\n                        self.system_chat.broadcastMessage('system', content, level=level)\n    \n    def receiveMessage(self, sender, message, level=LEVELS['info']):\n        # call handlers if the message is important enough\n        if self.alertness >= level:\n            for handler in self.handlers:\n                if handler.alertness >= level:\n                    handler.state = 'runnable'"]}
{"filename": "llmux/peer/bot.py", "chunked_list": ["import time\nimport re\nfrom .user import User\nfrom .peer import Peer\nfrom ..prompt.prompt import format_prompt\nfrom ..level import LEVELS\n\nclass Bot(Peer):\n    def __init__(self, function, chatbot_backend, name, output_path=None, auto=False):\n        \"\"\"\n        If auto is True, runs without user input, similar to 'continous mode' of AutoGPT.\n        \"\"\"\n        self.function = function\n        self.backend = chatbot_backend\n        super().__init__(name, output_path, auto)\n        self.task = None\n        self.system_chat.broadcastMessage('system', f'Hi {self.name}, your task is {function}')\n\n    def receiveMessage(self, sender, content, level=LEVELS['info']):\n        \"\"\"\n        Prevent sending private messages to bots unless necessary.\n        Broadcasting the message in a chat allows other bots, such as a critic to share information.\n        \"\"\"\n        super().receiveMessage(sender, content, level)\n        role = sender\n        if isinstance(sender, Bot):\n            # echo must be the same as raw content\n            if sender is self:\n                role = 'assistant'\n            # message from another bot, set role = system to avoid confusion during generation\n            else:\n                role = 'system'\n                content = f'{sender.name}: {content}'\n        elif isinstance(sender, User):\n            role = 'user'\n            content = f'{sender.name}: {content}'\n        else:\n            assert sender == 'system'\n        message = {'role': role, 'content': content}\n        self.messages.append(message)\n        self.file.write(f'{str(message)}\\n')\n        self.file.flush()\n\n    def parseMessage(self, message):\n        error_prompt = ''\n        parsed = {}\n        # find code snippets\n        parsed['code'] = []\n        # this is not general enough\n        pattern = re.compile('(eval:\\s*`(.+)`|exec:.*\\\\n?```([^`]+)```)')\n        match = pattern.findall(message)\n        for item in match:\n            if len(item[1]) > 0:\n                parsed['code'].append(('eval', item[1].strip()))\n            elif len(item[2]) > 0:\n                parsed['code'].append(('exec', item[2].strip()))\n        # destination chat\n        first_line = message[:message.find('\\n')]\n        if first_line[:2] == 'to' and ':' in first_line:\n            first_line = first_line[2:]\n            sep = first_line.find(':')\n            chats = first_line[:sep].split(',')\n            chats = [item.strip() for item in chats]\n            parsed['to'] = chats\n        elif len(parsed['code']) > 0:\n            parsed['to'] = [self.system_chat.name]\n        else:\n            error_prompt = 'Wrong format.'\n            error_prompt += format_prompt\n        return message, parsed, error_prompt"]}
{"filename": "llmux/peer/__init__.py", "chunked_list": ["from .bot import Bot\nfrom .user import CLIUser\nfrom .peer import Peer"]}
{"filename": "llmux/peer/user.py", "chunked_list": ["from .peer import Peer\nfrom ..backend import CLI\nfrom ..level import LEVELS\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass User(Peer):\n    def __init__(self, name):\n        super().__init__(name, auto=True)\n\nclass CLIUser(User):\n    def __init__(self, name, backend):\n        self.chat_with = []\n        super().__init__(name)\n        self.backend = backend\n\n    def joinChat(self, chat, say_hi=True):\n        super().joinChat(chat, say_hi)\n        self.chat_with = [chat.name]\n        self.messages = f'{self.name} to {self.chat_with}: '\n\n    def chatwith(self, name):\n        self.chat_with = [name]\n        self.messages = f'{self.name} to {self.chat_with}: '\n\n    def parseMessage(self, message):\n        \"\"\"\n        Instead of enforcing JSON, a CLI user may use a lightweight grammar.\n        Start a line with an exclamation mark to eval a system call.\n        ! self.chatwith(\"bot\")\n        to switch the current chat.\n        \"\"\"\n        parsed = {'to': self.chat_with}\n        if message[:2] == '! ':\n            parsed['code'] = [('eval', message[2:])]\n            parsed['to'] = [self.system_chat.name]\n        message = f'to {\", \".join(parsed[\"to\"])}:' + message\n        return message, parsed, ''\n\n    def receiveMessage(self, sender, content, level=LEVELS['info']):\n        super().receiveMessage(sender, content, level)\n        if isinstance(sender, Peer):\n            sender = sender.name\n        if self.alertness > level:\n            print(f'{sender}: {content}')", "\nclass CLIUser(User):\n    def __init__(self, name, backend):\n        self.chat_with = []\n        super().__init__(name)\n        self.backend = backend\n\n    def joinChat(self, chat, say_hi=True):\n        super().joinChat(chat, say_hi)\n        self.chat_with = [chat.name]\n        self.messages = f'{self.name} to {self.chat_with}: '\n\n    def chatwith(self, name):\n        self.chat_with = [name]\n        self.messages = f'{self.name} to {self.chat_with}: '\n\n    def parseMessage(self, message):\n        \"\"\"\n        Instead of enforcing JSON, a CLI user may use a lightweight grammar.\n        Start a line with an exclamation mark to eval a system call.\n        ! self.chatwith(\"bot\")\n        to switch the current chat.\n        \"\"\"\n        parsed = {'to': self.chat_with}\n        if message[:2] == '! ':\n            parsed['code'] = [('eval', message[2:])]\n            parsed['to'] = [self.system_chat.name]\n        message = f'to {\", \".join(parsed[\"to\"])}:' + message\n        return message, parsed, ''\n\n    def receiveMessage(self, sender, content, level=LEVELS['info']):\n        super().receiveMessage(sender, content, level)\n        if isinstance(sender, Peer):\n            sender = sender.name\n        if self.alertness > level:\n            print(f'{sender}: {content}')"]}
