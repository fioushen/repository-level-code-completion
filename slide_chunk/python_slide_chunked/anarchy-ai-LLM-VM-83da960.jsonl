{"filename": "quickstart_REBEL.py", "chunked_list": ["# import our client\nfrom llm_vm.client import Client\nimport os\n\n# Instantiate the client specifying which LLM you want to use\n\nclient = Client(big_model='chat_gpt', small_model='gpt')\n\n# Put in your prompt and go!\nresponse = client.complete(prompt = 'Is it warmer in Paris or Timbuktu and what are the temperatures in either city?',", "# Put in your prompt and go!\nresponse = client.complete(prompt = 'Is it warmer in Paris or Timbuktu and what are the temperatures in either city?',\n                           context='',\n                           openai_key=os.getenv(\"LLM_VM_OPENAI_API_KEY\"), #for REBEL we need an OpenAI key\n                           tools=\n                           [{'description': 'Find the weather at a location and returns it in celcius.',\n                            'dynamic_params': {\"latitude\": 'latitude of as a float',\"longitude\": 'the longitude as a float'},\n                            'method': 'GET',\n                            'url': \"https://api.open-meteo.com/v1/forecast\",\n                            'static_params': {'current_weather': 'true'}}]) #No tools by default, so you have to add your own", "                            'url': \"https://api.open-meteo.com/v1/forecast\",\n                            'static_params': {'current_weather': 'true'}}]) #No tools by default, so you have to add your own\nprint(response)\n\n"]}
{"filename": "quickstart_finetune.py", "chunked_list": ["#! /usr/bin/env python3\n# import our client\nfrom llm_vm.client import Client\nimport os\nfrom llm_vm.config import settings\n# Instantiate the client specifying which LLM you want to use\nclient = Client(big_model='chat_gpt', small_model='pythia')\n\n# Put in your prompt and go!\nresponse = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the currency in myanmmar\",", "# Put in your prompt and go!\nresponse = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the currency in myanmmar\",\n                           openai_key=settings.openai_api_key,\n                           temperature=0.0,\n                           data_synthesis=True,\n                           finetune=True,)\nprint(response)\n\n# response = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the economic situation in France\",\n#                            openai_key=settings.openai_api_key,", "# response = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the economic situation in France\",\n#                            openai_key=settings.openai_api_key,\n#                            temperature=0.0,\n#                            data_synthesis=True,\n#                            finetune=True,)\n# print(response)\n# response = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the currency in myanmmar\",\n#                            openai_key=settings.openai_api_key,\n#                            temperature=0.0,\n#                            data_synthesis=True,", "#                            temperature=0.0,\n#                            data_synthesis=True,\n#                            finetune=True,)\n# print(response)\n# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "quickstart_finetune_load.py", "chunked_list": ["#! /usr/bin/env python3\n# import our client\nfrom llm_vm.client import Client\nimport os\n\n# Instantiate the client specifying which LLM you want to use\nclient = Client(big_model='pythia')\n\n# specify the file name of the finetuned model to load\nmodel_name = '<filename_of_your_model>.pt'", "# specify the file name of the finetuned model to load\nmodel_name = '<filename_of_your_model>.pt'\nclient.load_finetune(model_name)\n\n# Put in your prompt and go!\nresponse = client.complete(prompt = 'What is anarchy?',\n                           context='')\nprint(response)\n# Anarchy is a political system in which the state is abolished and the people are free...\n", "# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "test_llm_vm.py", "chunked_list": ["# import llm_vm.client as l\nimport os, requests, json\n\nopenai_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n\nurl = \"http://localhost:3002/v1/complete\"\njson_payload = {\"prompt\": \"what is the economic situation in canada?\",\n                \"context\": \"\",\n                \"temperature\": 0.0,\n                \"openai_key\": openai_key,", "                \"temperature\": 0.0,\n                \"openai_key\": openai_key,\n                # \"finetune\": True,\n                }\n\nresponse = requests.post(url, data=json.dumps(json_payload))\nprint(response.text)\n"]}
{"filename": "quickstart.py", "chunked_list": ["#! /usr/bin/env python3\n# import our client\nfrom llm_vm.client import Client\nimport os\n\n# Instantiate the client specifying which LLM you want to use\n\nclient = Client(big_model='pythia', small_model='neo')\n\n# Put in your prompt and go!", "\n# Put in your prompt and go!\nresponse = client.complete(prompt = 'What is anarchy?',\n                           context='')\nprint(response)\n# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "tests/test_data_synthesis.py", "chunked_list": ["from dotenv import load_dotenv\nimport os\nimport openai\nimport data_synthesis\nfrom optimize import *\nfrom llm_vm.client import Client\n\nclient = Client(big_model='gpt', small_model='neo')\nif __name__ == \"__main__\":\n    try:\n        load_dotenv()\n    except:\n        pass\n    openai.api_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n    print(\"key:\", openai.api_key)\n\n\n    data_synthesizer = data_synthesis.DataSynthesis(0.87, 50)\n\n    # for one-shot prompt\n    prompt = \"What is the currency in myanmmar?\"\n    response = client.complete(prompt=prompt, context = \"\", openai_key=\"\",temperature = 0.0)[\"completion\"]\n    print(f\"Prompt: {prompt} /nResponse: {response}\")\n\n    # for k-shot prompt\n    prompt_list = [\"What is the currency in Madagascar?\", \"What is the currency in myanmmar?\", \"What is the currency in Morocco?\"]\n    response_list = []\n    for p in prompt_list:\n        res = client.complete(prompt=p, context = \"\", openai_key=\"\", temperature = 0.0)\n        response_list.append(res[\"completion\"])\n\n\n\n\n    data_synthesizer.data_synthesis(client.optimizer, prompt_list, response_list,openai_key=\"\", temperature=0.0)", "if __name__ == \"__main__\":\n    try:\n        load_dotenv()\n    except:\n        pass\n    openai.api_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n    print(\"key:\", openai.api_key)\n\n\n    data_synthesizer = data_synthesis.DataSynthesis(0.87, 50)\n\n    # for one-shot prompt\n    prompt = \"What is the currency in myanmmar?\"\n    response = client.complete(prompt=prompt, context = \"\", openai_key=\"\",temperature = 0.0)[\"completion\"]\n    print(f\"Prompt: {prompt} /nResponse: {response}\")\n\n    # for k-shot prompt\n    prompt_list = [\"What is the currency in Madagascar?\", \"What is the currency in myanmmar?\", \"What is the currency in Morocco?\"]\n    response_list = []\n    for p in prompt_list:\n        res = client.complete(prompt=p, context = \"\", openai_key=\"\", temperature = 0.0)\n        response_list.append(res[\"completion\"])\n\n\n\n\n    data_synthesizer.data_synthesis(client.optimizer, prompt_list, response_list,openai_key=\"\", temperature=0.0)", ""]}
{"filename": "tests/test_optimize.py", "chunked_list": ["from dotenv import load_dotenv\nimport os\nimport openai\nimport sys\n\nfrom llm_vm.completion.optimize import *\n\nhaskell = '''\ndef call_gpt(state, cur_prompt: str, stop: str, max_tokens = 20, quality = \"best\", temperature = 0.0):\n    if state.verbose > 1:", "def call_gpt(state, cur_prompt: str, stop: str, max_tokens = 20, quality = \"best\", temperature = 0.0):\n    if state.verbose > 1:\n        print_op(\"\\nGPT input for {\" +stop + \"} \"+ str(len(cur_prompt)) + \".\")\n    if state.verbose > 2:\n        print_op(prepPrintPromptContext(cur_prompt))\n    ask_tokens = max_tokens + len(cur_prompt) / 2.7\n    if state.verbose > 0:\n        print_op(\"ASK_TOKENS:\", ask_tokens)\n    if (ask_tokens) > 2049:\n        quality = 'best'", "    if (ask_tokens) > 2049:\n        quality = 'best'\n    model = { 'best' : (\"text-davinci-003\", 0.02),\n              'okay' : (\"text-curie-001\", 0.002),\n             }[quality]\n    def calcCost(p):\n        return (len(p) / 2700.0) * model[1]\n    cost = calcCost(cur_prompt)\n    try:\n        ans = openai.Completion.create(", "    try:\n        ans = openai.Completion.create(\n            model=model[0],\n            max_tokens=max_tokens,\n            stop=stop,\n            prompt=cur_prompt,\n            temperature=temperature\n        )\n    except Exception as e:\n        print_op(\"WTF:\", e)", "    except Exception as e:\n        print_op(\"WTF:\", e)\n        state.price += cost\n        return \"OpenAI is down!\"\n    response_text = ans['choices'][0]['text']\n    simpleprice = model[1] * ans['usage']['total_tokens'] / 1000\n    if state.verbose > 0:\n        print_op(\"SimplePrice: $\"+str(simpleprice))\n    state.price += simpleprice\n    if state.verbose > 2:", "    state.price += simpleprice\n    if state.verbose > 2:\n        print_op(\"GPT output:\")\n        print_op(prepPrintPromptContext(response_text))\n        print_op(\"GPT output fin.\\n\")\n    return response_text\ndef delete_file(file_name):\n    location = os.getcwd()\n    path = os.path.join(location, file_name)\n    os.remove(path)", "    path = os.path.join(location, file_name)\n    os.remove(path)\n    return True\ndef call_ChatGPT(cur_prompt, stop = None, max_tokens = 20, temperature = 0.2, gpt4 = False):\n    ans = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0301\" if not gpt4 else 'gpt-4',\n        max_tokens=max_tokens,\n        stop=stop,\n        messages=cur_prompt,\n        temperature=temperature)", "        messages=cur_prompt,\n        temperature=temperature)\n    return ans['choices'][0]['message']['content']\n    return response_text\n\ndef call_gpt(cur_prompt: str, stop: str, max_tokens = 20, quality = \"best\", temperature = 0.0, model = \"text-davinci-003\"):\n    ans = openai.Completion.create(\n        model=model,\n        max_tokens=max_tokens,\n        stop=stop,", "        max_tokens=max_tokens,\n        stop=stop,\n        prompt=cur_prompt,\n        temperature=temperature\n    )\n    return ans['choices'][0]['text']\nimport gzip\nimport json\ndef create_jsonl_file(data_list: list, file_name: str, compress: bool = True) -> None:\n    \"\"\"", "def create_jsonl_file(data_list: list, file_name: str, compress: bool = True) -> None:\n    \"\"\"\n    Method saves list of dicts into jsonl file.\n    :param data: (list) list of dicts to be stored,\n    :param filename: (str) path to the output file. If suffix .jsonl is not given then methods appends\n        .jsonl suffix into the file.\n    :param compress: (bool) should file be compressed into a gzip archive?\n    \"\"\"\n    sjsonl = '.jsonl'\n    sgz = '.gz'", "    sjsonl = '.jsonl'\n    sgz = '.gz'\n    # Check filename\n    if not file_name.endswith(sjsonl):\n        file_name = file_name + sjsonl\n    # Save data\n    if compress:\n        file_name = file_name + sgz\n        with gzip.open(file_name, 'w') as compressed:\n            for ddict in data_list:", "        with gzip.open(file_name, 'w') as compressed:\n            for ddict in data_list:\n                jout = json.dumps(ddict) + '\\n'\n                jout = jout.encode('utf-8')\n                compressed.write(jout)\n    else:\n        with open(file_name, 'w') as out:\n            for ddict in data_list:\n                jout = json.dumps(ddict) + '\\n'\n                out.write(jout)", "                jout = json.dumps(ddict) + '\\n'\n                out.write(jout)\n    return file_name, open(file_name, \"rb\")\n'''\n\ndef run_test_stub():\n    try:\n        load_dotenv()\n    except:\n        pass\n    openai_api_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n    openai.api_key =openai_api_key\n    # anarchy_key = os.getenv('LLM_VM_ANARCHY_KEY')\n    print(\"key:\", openai.api_key[0:5])\n    optimizer = LocalOptimizer(MIN_TRAIN_EXS=1,openai_key=openai_api_key)\n    #optimizer = HostedOptimizer(openai_key = openai.api_key,\n    #                            anarchy_key = anarchy_key,\n    #                            MIN_TRAIN_EXS=2)\n    i = 0\n    optimizer.complete(\"Answer question Q. \",\"Q: What is the currency in myanmmar\", \\\n                 temperature = 0.0, data_synthesis = True,\\\n                 min_examples_for_synthesis=0,finetune=True)", "\nif __name__ == \"__main__\":\n    run_test_stub()\n    '''\n    for h in haskell.splitlines():\n        print(\"At: \", i)\n        try:\n            print(optimizer.complete(\"Please convert this line to some haskell:\", h + \"\\nHaskell:\", max_tokens = 100, temperature = 0.0))\n        except Exception as e:\n            print('E:', e)\n\n        time.sleep(2)\n        if i > 3 and i < 20:\n            time.sleep(120)\n        i += 1\n    '''", ""]}
{"filename": "src/llm_vm/onsite_llm.py", "chunked_list": ["import abc\nfrom abc import ABC,abstractmethod\nimport openai\nimport math\nfrom transformers import (\n    AutoModelForMaskedLM,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    BertTokenizer,\n    OPTForCausalLM,", "    BertTokenizer,\n    OPTForCausalLM,\n    BloomForCausalLM,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    GPTNeoForCausalLM,\n    GPTNeoXForCausalLM,\n    GPT2Tokenizer,\n    DataCollatorForLanguageModeling,\n    TrainingArguments,", "    DataCollatorForLanguageModeling,\n    TrainingArguments,\n    Trainer)\nimport time\nfrom datetime import datetime\nimport tempfile\nimport json\nimport os\nimport torch\n", "import torch\n\n\n__private_key_value_models_map =  {}\n# []   {\n#         \"opt\": Small_Local_OPT,\n#         \"bloom\": Small_Local_Bloom,\n#         \"neo\": Small_Local_Neo,\n#         \"llama\": Small_Local_LLama,\n#         \"pythia\": Small_Local_Pythia,", "#         \"llama\": Small_Local_LLama,\n#         \"pythia\": Small_Local_Pythia,\n#         \"gpt\": GPT3,\n#         \"chat_gpt\": Chat_GPT,\n#         \"flan\" : Small_Local_Flan_T5,\n#         \"pythia\" : Small_Local_Pythia,\n#         }\n\ndef RegisterModelClass(name):\n    def regClass(cls):\n        __private_key_value_models_map[name]=cls \n    return regClass", "def RegisterModelClass(name):\n    def regClass(cls):\n        __private_key_value_models_map[name]=cls \n    return regClass\n\nmodel_keys_registered = __private_key_value_models_map.keys()        \n# Dictionary of models to be loaded in ModelConfig\ndef load_model_closure(model_name):\n    models = __private_key_value_models_map\n    return models[model_name]", "\n# this is a hack till we add dynaconf or something?\nif os.name == \"nt\":\n    homepath = os.path.join('C:\\\\','Users',os.getlogin())\nelse:\n    homepath = os.environ.get(\"HOME\")\n\nmodel_path_default = os.path.join( homepath , \".llm_vm\", \"models\")\nos.makedirs(model_path_default, exist_ok = True)\n\ndef create_jsonl_file(data_list):\n    out = tempfile.TemporaryFile('w+')\n    for a,b in data_list:\n        out.write(json.dumps({'prompt': a, 'completion': b}) + \"\\n\")\n    out.seek(0)\n    return out", "os.makedirs(model_path_default, exist_ok = True)\n\ndef create_jsonl_file(data_list):\n    out = tempfile.TemporaryFile('w+')\n    for a,b in data_list:\n        out.write(json.dumps({'prompt': a, 'completion': b}) + \"\\n\")\n    out.seek(0)\n    return out\n\n\nclass FinetuningDataset(torch.utils.data.Dataset):\n    def __init__(self,iterable_dataset,length):\n        self.dataset = list(iterable_dataset)\n        self.length = length\n    def __len__(self):\n        return self.length\n    def __getitem__(self, idx):\n        return self.dataset[idx]", "\n\nclass FinetuningDataset(torch.utils.data.Dataset):\n    def __init__(self,iterable_dataset,length):\n        self.dataset = list(iterable_dataset)\n        self.length = length\n    def __len__(self):\n        return self.length\n    def __getitem__(self, idx):\n        return self.dataset[idx]", "\nclass Base_Onsite_LLM(ABC):\n    def __init__(self,model_uri=None,tokenizer_kw_args={},model_kw_args={}):\n        if model_uri != None :\n            self.model_uri= model_uri\n        if model_uri is None and self.model_uri is None:\n            raise ValueError('A very specific bad thing happened.')\n        self.model_name : str = self.model_uri.split('/')[-1] # our default for deriving model name\n        self.model=self.model_loader(**model_kw_args)\n        self.tokenizer=self.tokenizer_loader(**tokenizer_kw_args)\n\n    @property\n    @abstractmethod\n    def model_uri(self):\n        pass\n\n    @model_uri.setter\n    def model_uri(self,val):\n        self.model_uri=val # check if this is correct\n\n    # model_name : str = self.model_uri.split('/')[-1]\n\n    @abstractmethod\n    def model_loader(self):\n        pass\n\n    @abstractmethod\n    def tokenizer_loader(self):\n        pass\n\n    def load_finetune(self, model_filename):\n        self.model.load_state_dict(torch.load(os.path.join(model_path_default,\"finetuned_models\", self.model_name, model_filename)))\n\n\n    def generate(self,prompt,max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n        \"\"\"\n        This function uses the class's llm and tokenizer to generate a response given a user's prompt\n\n        Parameters:\n            prompt (str): Prompt to send to LLM\n            max_length (int): Optional parameter limiting response length\n\n\n        Returns:\n            str: LLM Generated Response\n\n        Example:\n           >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n           I think it takes about a week for the apple to grow.\n        \"\"\"\n        inputs=self.tokenizer(prompt,return_tensors=\"pt\")\n        generate_ids=self.model.generate(inputs.input_ids,max_length=max_length)\n        resp= self.tokenizer.batch_decode(generate_ids,skip_special_tokens=True,clean_up_tokenization_spaces=False)[0]\n        # need to drop the len(prompt) prefix with these sequences generally\n        # because they include the prompt.\n        return resp[len(prompt):]\n\n    def finetune(self,data, optimizer, c_id):\n        def asynctune():\n            old_model = optimizer.storage.get_model(c_id)\n            if old_model is not None:\n                self.model.load_state_dict(torch.load(old_model))\n            untokenized_final_dataset = []\n            for prompt,response in data:\n                untokenized_final_dataset.append(prompt + response)\n            tokenized_final_dataset = map(self.tokenizer,untokenized_final_dataset)\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n            optimizer.storage.set_training_in_progress(c_id, True)\n            training_args = TrainingArguments(\n                output_dir=os.path.join(model_path_default,\"finetuned_models\",),\n                evaluation_strategy=\"epoch\",\n                learning_rate=2e-5,\n                per_device_train_batch_size = 1,\n                per_device_eval_batch_size = 1,\n                num_train_epochs=1,\n                weight_decay=0.01,\n                report_to= \"none\",\n            )\n            test_set = FinetuningDataset(tokenized_final_dataset,len(untokenized_final_dataset))\n\n            trainer = Trainer(\n                model=self.model,\n                args=training_args,\n                train_dataset=test_set,\n                eval_dataset=test_set,\n                data_collator=data_collator,\n            )\n            os.makedirs(os.path.join(model_path_default,\"finetuned_models\", self.model_name), exist_ok=True)\n            if tokenized_final_dataset:\n                trainer.train()\n                eval_results = trainer.evaluate()\n            optimizer.storage.set_training_in_progress(c_id, False)\n\n            if os.name == \"nt\":\n                timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n            else:\n                timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n            new_model = os.path.join(model_path_default,\"finetuned_models\",self.model_name, timestamp + '_' + self.model_name + \".pt\" )\n            open(new_model,\"a\")\n            torch.save(self.model.state_dict(), new_model) # the model in memory is different now\n            self.model_name = self.model_name + \"_ft_\"+  timestamp\n            optimizer.storage.set_model(c_id, new_model)\n            return math.exp(eval_results['eval_loss']) #perplexity is the metric we use for finetuning measurement\n        return asynctune\n\n\n    def finetune_immediately(self):\n        finetune()()", "\n\"\"\"\nthis factorization isn't necessarily the greatest, nor should it be viewed\nas likely being more general, aside from covering hugging face transformers\n\"\"\"\n@RegisterModelClass(\"pythia\")\nclass Small_Local_Pythia(Base_Onsite_LLM):\n    \"\"\"\n    This is a class for ElutherAI's Pythia-70m LLM\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n    # def __init__(self,**kwargs):\n    #     # self.model_uri =\n    #     super().__init__(kwargs) ## this line is required\n    model_uri = \"EleutherAI/pythia-70m-deduped\"\n    def model_loader(self):\n        return GPTNeoXForCausalLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return AutoTokenizer.from_pretrained(self.model_uri)", "\n\n@RegisterModelClass(\"opt\")\nclass Small_Local_OPT(Base_Onsite_LLM):\n\n    \"\"\"\n    This is a class for Facebook's OPT-350m LLM\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n    model_uri=\"facebook/opt-350m\"\n    def model_loader(self):\n        return OPTForCausalLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return AutoTokenizer.from_pretrained(self.model_uri)", "\n@RegisterModelClass(\"bloom\")\nclass Small_Local_Bloom(Base_Onsite_LLM):\n\n    \"\"\"\n    This is a class for BigScience's bloom-560 LLM\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n    model_uri=\"bigscience/bloom-560m\"\n\n    def model_loader(self):\n        return BloomForCausalLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return AutoTokenizer.from_pretrained(self.model_uri)", "\n@RegisterModelClass(\"neo\")\nclass Small_Local_Neo(Base_Onsite_LLM):\n\n    \"\"\"\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n    model_uri=\"EleutherAI/gpt-neo-1.3B\"\n\n    def model_loader(self):\n        return GPTNeoForCausalLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return GPT2Tokenizer.from_pretrained(self.model_uri)", "\n@RegisterModelClass(\"llama\")\nclass Small_Local_LLama(Base_Onsite_LLM):\n\n    \"\"\"\n    This is a class for Openlm-Research's open_llama-3b LLM\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n    model_uri=\"openlm-research/open_llama_3b_v2\"\n\n    def model_loader(self):\n        return LlamaForCausalLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return LlamaTokenizer.from_pretrained(self.model_uri)", "\n@RegisterModelClass(\"flan\")# our yummiest model based on similarity to food\nclass Small_Local_Flan_T5(Base_Onsite_LLM):\n\n    \"\"\"\n    This is a class for Google's flan-t5 LLM\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n\n    model_uri=\"google/flan-t5-small\"\n    def model_loader(self):\n        return AutoModelForSeq2SeqLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return AutoTokenizer.from_pretrained(self.model_uri)", "\n@RegisterModelClass(\"bert\")\nclass Small_Local_BERT(Base_Onsite_LLM):\n\n    \"\"\"\n    This is a class for BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n    The base model needs finetuning in almost all cases.\n\n    Attributes:\n        model_uri (str): Hugging Face Endpoint for LLM\n        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n        model (LLM): The large language model\n\n    Methods:\n        model_loader: Loads the LLM into memory\n        tokenizer_loader: Loads the tokenizer into memory\n        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n    \"\"\"\n\n    model_uri = \"bert-base-cased\"\n    def model_loader(self):\n        return AutoModelForMaskedLM.from_pretrained(self.model_uri)\n    def tokenizer_loader(self):\n        return BertTokenizer.from_pretrained(self.model_uri)", "@RegisterModelClass(\"gpt\")\nclass GPT3:\n\n    \"\"\"\n    This is a class for openAI's completion endpoint\n\n    Methods:\n        generate: Generates a response from a given prompt with OpenAI's completion endpoint\n    \"\"\"\n\n    def generate(self,prompt, max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n        \"\"\"\n        This function uses openAI's API to generate a response from the prompt\n\n        Parameters:\n            prompt (str): Prompt to send to LLM\n            max_length (int): Optional parameter limiting response length\n\n\n        Returns:\n            str: LLM Generated Response\n\n        Example:\n            >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n            It typically takes about 100-200 days...\n        \"\"\"\n\n        ans = openai.Completion.create(prompt= prompt, model=\"text-davinci-003\", **kwargs)\n        return ans['choices'][0]['text']\n\n\n    def finetune(self, dataset, optimizer, c_id):\n        old_model = optimizer.storage.get_model(c_id)\n        training_file = create_jsonl_file(dataset)\n        upload_response = openai.File.create(file=training_file, purpose=\"fine-tune\")\n        training_file.close()\n        fine_tuning_job = openai.FineTune.create(training_file= upload_response.id)\n\n        print(f\"Fine-tuning job created: {fine_tuning_job}\", flush=True)\n        global job_id # global state isn't great, but thats interrupt handlers\n        job_id = fine_tuning_job[\"id\"]\n        while True:\n            fine_tuning_status = openai.FineTune.retrieve(id=job_id)\n            status = fine_tuning_status[\"status\"]\n            print(f\"Fine-tuning job status: {status}\")\n            if status in [\"succeeded\", \"completed\", \"failed\"]:\n                break\n            time.sleep(30)\n        job_id = None #\n        new_model_id = fine_tuning_status.fine_tuned_model\n\n        print(\"New_model_id: \", new_model_id, flush=True)\n\n        optimizer.storage.set_model(c_id, new_model_id)\n        optimizer.storage.set_training_in_progress(c_id, False)\n        if old_model is not None:\n            openai.Model.delete(old_model)", "\n@RegisterModelClass(\"chat_gpt\")\nclass Chat_GPT:\n    \"\"\"\n    This is a class for openAI's gpt-3.5-turbo LLM\n\n    Methods:\n        generate: Generates a response from a given prompt through OpenAI's endpoint\n    \"\"\"\n\n    def generate(self,prompt, max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n        \"\"\"\n        This function uses openAI's API to generate a response from the prompt\n\n        Parameters:\n            prompt (str): Prompt to send to LLM\n            max_length (int): Optional parameter limiting response length\n\n\n        Returns:\n            str: LLM Generated Response\n\n        Example:\n            >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n            It typically takes about 100-200 days...\n        \"\"\"\n        cur_prompt = [{'role': \"system\", 'content' : prompt}]\n        ans = openai.ChatCompletion.create(\n            messages=cur_prompt,\n            model=\"gpt-3.5-turbo-0301\",\n            **kwargs)\n        return ans['choices'][0]['message']['content']\n\n    def finetune(self, dataset, optimizer, c_id):\n        print(\"fine tuning isn't supported by OpenAI on this model\")\n        exit()", "        # old_model = optimizer.storage.get_model(c_id)\n        # training_file = create_jsonl_file(dataset)\n        # upload_response = openai.File.create(file=training_file, purpose=\"fine-tune\")\n        # training_file.close()\n        # fine_tuning_job = openai.FineTune.create(training_file= upload_response.id)\n\n        # print(f\"Fine-tuning job created: {fine_tuning_job}\", flush=True)\n        # global job_id # global state isn't great, but thats interrupt handlers\n        # job_id = fine_tuning_job[\"id\"]\n        # while True:", "        # job_id = fine_tuning_job[\"id\"]\n        # while True:\n        #     fine_tuning_status = openai.FineTune.retrieve(id=job_id)\n        #     status = fine_tuning_status[\"status\"]\n        #     print(f\"Fine-tuning job status: {status}\")\n        #     if status in [\"succeeded\", \"completed\", \"failed\"]:\n        #         break\n        #     time.sleep(30)\n        # job_id = None #\n        # new_model_id = fine_tuning_status.fine_tuned_model", "        # job_id = None #\n        # new_model_id = fine_tuning_status.fine_tuned_model\n\n        # print(\"New_model_id: \", new_model_id, flush=True)\n\n        # optimizer.storage.set_model(c_id, new_model_id)\n        # optimizer.storage.set_training_in_progress(c_id, False)\n        # if old_model is not None:\n        #     openai.Model.delete(old_model)\n", "        #     openai.Model.delete(old_model)\n"]}
{"filename": "src/llm_vm/config.py", "chunked_list": ["import re\nimport os\nimport argparse\nfrom dynaconf import Dynaconf\nfrom xdg import XDG_CONFIG_HOME\nfrom llm_vm.onsite_llm import model_keys_registered\nfrom llm_vm.data_path import project_root\n\n# Parse CLI arguments\nparser = argparse.ArgumentParser()", "# Parse CLI arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('-b', '--big_model', type=str, help='Big LLM Model.')\nparser.add_argument('-p', '--port', type=int,      help='Port Number.')\nparser.add_argument('-s', '--small_model',type=str,help='Small LLM Model.')\nparser.add_argument('-H', '--host', type=str,      help='Host Address.')\nparser.add_argument('-K', '--openai_key',type=str,help='OpenAI api key')\nargs = parser.parse_args()\n\n# Set the CLI argument values to environment variables if they are present\nif args.big_model is not None:\n    os.environ['LLM_VM_BIG_MODEL'] = args.big_model", "\n# Set the CLI argument values to environment variables if they are present\nif args.big_model is not None:\n    os.environ['LLM_VM_BIG_MODEL'] = args.big_model\nif args.port is not None:\n    os.environ['LLM_VM_PORT'] = str(args.port)\nif args.small_model is not None:\n    os.environ['LLM_VM_SMALL_MODEL'] = args.small_model\nif args.host is not None:\n    os.environ['LLM_VM_HOST'] = args.host\nif \"openai_api_key\" in args:\n    os.environ['LLM_VM_OPENAI_API_KEY'] = args.openai_api_key", "if args.host is not None:\n    os.environ['LLM_VM_HOST'] = args.host\nif \"openai_api_key\" in args:\n    os.environ['LLM_VM_OPENAI_API_KEY'] = args.openai_api_key\n\n\n# project_root = os.path.abspath(os.getcwd())\n\nprint(\"Project Root: \" + project_root)\n", "print(\"Project Root: \" + project_root)\n\n\n# default to the local file, and look in XDG if we can't find it\nconfig_files = [\n    os.path.join(project_root, \"settings.default.toml\"),\n    os.path.join(XDG_CONFIG_HOME, \"settings.toml\"),\n    os.path.join(project_root, \"settings.toml\"),\n]\n", "]\n\nsettings = Dynaconf(\n    settings_file=config_files,\n    load_dotenv=True,\n    dotenv_path=\".env\",\n    envvar_prefix=\"LLM_VM\",\n)\n\n# making MODELS_AVAILABLE a set because it will be used for membership testing", "\n# making MODELS_AVAILABLE a set because it will be used for membership testing\nMODELS_AVAILABLE = set(model_keys_registered)\n\nif settings.big_model not in MODELS_AVAILABLE:\n    print(settings.big_model + \" is an invalid Model selection for Big LLM Model\")\n    exit()\n\nif settings.small_model not in MODELS_AVAILABLE:\n    print(settings.small_model + \" is an invalid Model selection for Small LLM Model\")\n    exit()", "if settings.small_model not in MODELS_AVAILABLE:\n    print(settings.small_model + \" is an invalid Model selection for Small LLM Model\")\n    exit()\n\n# do we want to do this early test and exit?\n# if settings.small_model is \"chat_gpt\":\n#     print(\"openai currently doesn't support fine tuning chat_gpt, aka gpt3.5-turbo, exiting\")\n#     exit()\n\ndef isOpenAIModel(str):\n    str ==\"gpt\" or str ==\"chat_gpt\"", "\ndef isOpenAIModel(str):\n    str ==\"gpt\" or str ==\"chat_gpt\"\n\nif settings.openai_api_key is None and (isOpenAIModel(settings.small_model) or isOpenAIModel(settings.big_model)):\n    print(\"Error: you must have an OpenAI API key set via config files, ./settings.default.toml or via environment variable \")\n    print(\"LLM_VM_OPEN_AI_API,if you wish to use their models. Exiting\")\n    exit()\n\n# check args.host is a valid IP address", "\n# check args.host is a valid IP address\npattern = r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$'\nif not re.match(pattern, settings.host):\n    print(\"Invalid IP address. Reverting to the default host, 127.0.0.1\")\n    settings.host = '127.0.0.1'\nelse:\n    # validates each number in the IP address is between 0-255\n    octets = settings.host.split('.')\n    valid_ip = True\n    for octet in octets:\n        if not 0 <= int(octet) <= 255:\n            valid_ip = False\n            break\n    if not valid_ip:\n        print(\"Invalid IP address range. Reverting to the default host, 127.0.0.1\")\n        settings.host = '127.0.0.1'", "\n\nassert settings.big_model in MODELS_AVAILABLE, f\"{settings.big_model} is not a valid Model selection for Big LLM Model\"\nassert settings.small_model in MODELS_AVAILABLE, f\"{settings.small_model} is not a valid Model selection for Small LLM Model\"\nassert 1024 < settings.port < 65535, f\"{settings.port} is an invalid port number\"\n"]}
{"filename": "src/llm_vm/client.py", "chunked_list": ["import openai\nimport llm_vm.onsite_llm as llms\nfrom llm_vm.onsite_llm import load_model_closure\nfrom llm_vm.agents.REBEL import agent\nfrom llm_vm.completion.optimize import LocalOptimizer\nimport llm_vm.config as conf\nimport os\n\n\nif conf.settings.big_model is None:\n    default_big_model = \"chat_gpt\"\nelse:\n    default_big_model= conf.settings.big_model", "\nif conf.settings.big_model is None:\n    default_big_model = \"chat_gpt\"\nelse:\n    default_big_model= conf.settings.big_model\n\nif conf.settings.small_model is not  None:\n    default_small_model= \"pythia\"\nelse:    \n    default_small_model = conf.settings.small_model", "\n    \n\n\nclass Client:\n    \"\"\"\n    This is a class sets up a Local optimizer around a LLM for easy access.\n\n    Attributes:\n        optimizer (LocalOptimizer): Anarchy LLM Optimizer\n        openai_key (str): API key for OpenAI access\n\n    Methods:\n        complete: The Anarchy completion endpoint giving access to a specified LLM\n    \"\"\"\n\n    def __init__(self, big_model = default_big_model, small_model =default_small_model,big_model_config={},small_model_config={}):\n        \"\"\"\n        This __init__ function allows the user to specify which LLM they want to use upon instantiation.\n\n        Parameters:\n            big_model (str): Name of LLM to be used as a reliable source\n            small_model (str): Name of small model to be used for fine-tuning\n\n        Returns:\n            None\n\n        Example:\n            >>> client = Client(big_model = 'neo')\n        \"\"\"\n        self.teacher = load_model_closure(big_model)(**big_model_config)\n        self.student = load_model_closure(small_model)(**small_model_config)\n\n        ## FIXME, do something like $HOME/.llm_vm/finetuned_models/\n        if os.path.isdir(\"finetuned_models\") == False:\n            os.mkdir(\"finetuned_models\")\n        # Specify the model strategy the user will use\n        # MODELCONFIG = models.ModelConfig(big_model=big_model, small_model=small_model)\n        print(\"Using model: \" + big_model) # announce the primary LLM that is generating results\n\n        # These functions allow for proper initialization of the optimizer\n        # def CALL_BIG(prompt, max_len=256, **kwargs):\n\n        #     return self.teacher.generate(prompt, max_len,**kwargs)\n\n        def CALL_SMALL(prompt, max_len=256, **kwargs):\n\n            return self.student.generate(prompt, max_len,**kwargs)\n\n        # load the optimizer into object memory for use by the complete function\n        self.optimizer = LocalOptimizer(MIN_TRAIN_EXS=2,openai_key=None, call_big=self.CALL_BIG, call_small= CALL_SMALL,\n                                        big_model = self.teacher, small_model = self.student)\n        self.rebel_agent = None # only initialized on first use \n\n    # These functions allow for proper initialization of the optimizer\n    def CALL_BIG(self, prompt, max_len=256, **kwargs):\n\n        return self.teacher.generate(prompt, max_len,**kwargs)\n\n    def complete(self, prompt,\n                 context,\n                 openai_key = None,\n                 finetune=False,\n                 data_synthesis = False,\n                 temperature=0,\n                 stoptoken = None,\n                 tools = None):\n        \"\"\"\n        This function is Anarchy's completion entry point\n\n        Parameters:\n            prompt (str): Prompt to send to LLM for generation\n            context (str): Context to send to the LLM for generation\n            finetune (bool): Boolean value that begins fine tuning when set to True\n            data_synthesis (bool): Boolean value to determine whether data should be synthesized for fine-tuning or not\n            temperature (float): An analog\n\n\n        Returns:\n            str: LLM Generated Response\n\n        Example:\n           >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n           How long does it take for an apple tree to grow?\n        \"\"\"\n        static_context = context\n        dynamic_prompt = prompt\n        use_rebel_agent = False\n        kwargs = {}\n\n        kwargs.update({\"temperature\":temperature})\n\n        if openai_key is not None:\n            self.openai_key = openai_key\n\n        if stoptoken is not None:\n            kwargs.update({\"stop\":stoptoken})\n\n        if tools is not None:\n            if type(tools) != list:\n                return Exception(\"Wrong data type for tools. Should be a list\")\n            else:\n                final_tools=[]\n                for i in tools:\n                    temp_tool_dict = {}\n                    temp_args_dict = {}\n                    temp_tool_dict.update({\"description\":i[\"description\"]})\n                    temp_tool_dict.update({\"dynamic_params\":i[\"dynamic_params\"]})\n                    temp_tool_dict.update({\"method\":i[\"method\"]})\n                    temp_args_dict.update({\"url\":i[\"url\"]})\n                    temp_args_dict.update({\"params\":{}})\n                    for j in i[\"static_params\"].keys():\n                        temp_args_dict[\"params\"].update({j:i[\"static_params\"][j]})\n                    for k in i[\"dynamic_params\"].keys():\n                        temp_args_dict[\"params\"].update({k:\"{\"+k+\"}\"})\n                    temp_tool_dict.update({\"args\":temp_args_dict})\n                    final_tools.append(temp_tool_dict)\n                if self.rebel_agent is None:\n                    self.rebel_agent = agent.Agent(\"\", [], verbose=1) # initialize only if tools registered\n                self.rebel_agent.set_tools(final_tools)\n                use_rebel_agent = True\n\n        try:\n            if openai_key is not None:\n                openai.api_key = self.openai_key\n        except:\n            return  {\"status\":0, \"resp\":\"Issue with OpenAI key\"}\n\n        self.optimizer.openai_key = openai.api_key\n        # self.agent.set_api_key(openai.api_key,\"OPENAI_API_KEY\") # \n        if os.getenv(\"OPENAI_API_KEY\") is None and use_rebel_agent==True :\n            print(\"warning: you need OPENAI_API_KEY environment variable for \")\n\n        try:\n            if not use_rebel_agent:\n                completion = self.optimizer.complete(static_context,dynamic_prompt,data_synthesis=data_synthesis,finetune = finetune, **kwargs)\n            else:\n                completion = self.rebel_agent.run(static_context+dynamic_prompt,[])[0]\n        except Exception as e:\n            return {\"status\":0, \"resp\": str(e)}\n        return {\"completion\":completion, \"status\": 200}\n\n    def load_finetune(self, model_filename=None):\n        self.teacher.load_finetune(model_filename)", "\n"]}
{"filename": "src/llm_vm/data_path.py", "chunked_list": ["import os\n\n\n\nfrom pathlib import Path\n\n\npath = Path(__file__)\nproject_root= str(path.parent.absolute())\n\nif __name__ == 'main':\n    print(parent_root)", "project_root= str(path.parent.absolute())\n\nif __name__ == 'main':\n    print(parent_root)"]}
{"filename": "src/llm_vm/agents/agent_interface.py", "chunked_list": ["\"\"\"\nThis file has been temporarily repurposed as an interface for users to\ncall any of the three agents (REBEL, BACKWARD_CHAINING, and FLAT) and interact with them.\nRunning this file prompts the user to choose any of the agents and ask it questions.\n\"\"\"\nimport llm_vm.agents.REBEL.agent as REBEL\nimport llm_vm.agents.FLAT.agent as FLAT\nimport os\nkey = os.getenv(\"LLM_VM_OPENAI_API_KEY\")\n\ndef call_agent():\n    print(\"Try out any agent!\")\n\n    # stores user input for which agent to try out\n    model_choice = 0\n\n    # times that the user was prompted to choose a valid model\n    times_asked_for_model = 0\n\n    # if user enters invalid choice, prompt for input until valid\n    while True:\n        model_choice = input(\"[1] FLAT\\n[2] REBEL\\nChoose your agent:\")\n        try:\n            # try to cast the input to an integer\n            model_choice = int(model_choice)\n\n            if model_choice not in range (1, 4):\n                print(\"=====Please enter 1, or 2!=====\")\n            else:\n                # user has entered a valid input\n                break\n        except:\n            print(\"=====Please enter 1 or 2!=====\")\n\n    # FLAT\n    if model_choice == 1:\n        # TODO: Add agent call here when FLAT is fixed\n        tools =  [{'method': 'GET', \"dynamic_params\": { 'location': 'This string indicates the geographic area to be used when searching for businesses. \\\n    Examples: \"New York City\", \"NYC\", \"350 5th Ave, New York, NY 10118\".', 'term': 'Search term, e.g. \"food\" or \"restaurants\". The \\\n    term may also be the business\\'s name, such as \"Starbucks\"', 'price': 'Pricing levels to filter the search result with: 1 = \\\n    $, 2 = $$, 3 = $$$, 4 = $$$$. The price filter can be a list of comma delimited pricing levels. e.g., \"1, 2, 3\" will filter the \\\n    results to show the ones that are $, $$, or $$$.'}, \"description\":\"This tool searches for a business on yelp.  It's useful for finding restaurants and \\\n    whatnot.\", 'args' :{'url': 'https://api.yelp.com/v3/businesses/search', 'cert': '', 'json': {}, 'params': {'limit': '1',\n                                                                                                              'open_now': 'true', 'location': '{location}', 'term': '{term}', 'price': '{price}'}, 'data': {},\n                       'headers': {'authorization': '',\n                                   'accept': 'application/json'}}}]\n        agent = FLAT.Agent(key, tools, verbose=1)\n    elif model_choice == 2:\n        tools = REBEL.buildExampleTools()\n        agent = REBEL.Agent(key, tools, verbose = 1)\n\n\n    pass\n\n    mem = []\n    last = \"\"\n    while True:\n        inp = input(last+\"Human: \")\n        ret = agent.run(inp, mem)\n        mem = ret[1]\n        last = \"AI: \"+str(ret[0])+ \"\\n\"", "key = os.getenv(\"LLM_VM_OPENAI_API_KEY\")\n\ndef call_agent():\n    print(\"Try out any agent!\")\n\n    # stores user input for which agent to try out\n    model_choice = 0\n\n    # times that the user was prompted to choose a valid model\n    times_asked_for_model = 0\n\n    # if user enters invalid choice, prompt for input until valid\n    while True:\n        model_choice = input(\"[1] FLAT\\n[2] REBEL\\nChoose your agent:\")\n        try:\n            # try to cast the input to an integer\n            model_choice = int(model_choice)\n\n            if model_choice not in range (1, 4):\n                print(\"=====Please enter 1, or 2!=====\")\n            else:\n                # user has entered a valid input\n                break\n        except:\n            print(\"=====Please enter 1 or 2!=====\")\n\n    # FLAT\n    if model_choice == 1:\n        # TODO: Add agent call here when FLAT is fixed\n        tools =  [{'method': 'GET', \"dynamic_params\": { 'location': 'This string indicates the geographic area to be used when searching for businesses. \\\n    Examples: \"New York City\", \"NYC\", \"350 5th Ave, New York, NY 10118\".', 'term': 'Search term, e.g. \"food\" or \"restaurants\". The \\\n    term may also be the business\\'s name, such as \"Starbucks\"', 'price': 'Pricing levels to filter the search result with: 1 = \\\n    $, 2 = $$, 3 = $$$, 4 = $$$$. The price filter can be a list of comma delimited pricing levels. e.g., \"1, 2, 3\" will filter the \\\n    results to show the ones that are $, $$, or $$$.'}, \"description\":\"This tool searches for a business on yelp.  It's useful for finding restaurants and \\\n    whatnot.\", 'args' :{'url': 'https://api.yelp.com/v3/businesses/search', 'cert': '', 'json': {}, 'params': {'limit': '1',\n                                                                                                              'open_now': 'true', 'location': '{location}', 'term': '{term}', 'price': '{price}'}, 'data': {},\n                       'headers': {'authorization': '',\n                                   'accept': 'application/json'}}}]\n        agent = FLAT.Agent(key, tools, verbose=1)\n    elif model_choice == 2:\n        tools = REBEL.buildExampleTools()\n        agent = REBEL.Agent(key, tools, verbose = 1)\n\n\n    pass\n\n    mem = []\n    last = \"\"\n    while True:\n        inp = input(last+\"Human: \")\n        ret = agent.run(inp, mem)\n        mem = ret[1]\n        last = \"AI: \"+str(ret[0])+ \"\\n\"", "\nif __name__ == \"__main__\":\n    call_agent()\n"]}
{"filename": "src/llm_vm/agents/REBEL/bothandler.py", "chunked_list": ["import openai\nimport re\n\n\ndef tool_picker(tools_list, question, starting_tool_num):\n\n    tools=\"\"\n    prompt= '''\n{tools}\nWhich tool (number only), if any, would you use to answer the following question:\n\n{question}\n    '''\n    count=0\n    tools_list =  tools_list[starting_tool_num:]\n    for i in tools_list:\n        tools+=\"tool \"+str(count)+\": \"+str(i[\"description\"])+\"\\n\"\n        count+=1\n    tools+=\"tool \"+str(count)+\": \"+str(\"Use this tool when the question can be answered without using any tool or if the question is a greeting or a casual conversation. Also if we need to convert languages other than english, use this tool.\")+\"\\n\"\n    prompt=prompt.format(**{\"tools\":tools,\"question\":question})\n    prompt += \"\\n\\nYOU JUST ANSWER WITH A NUMBER.\"\n    ans = openai.Completion.create(\n        model=\"text-davinci-003\",\n        max_tokens=256,\n        stop=None,\n        prompt=prompt,\n        temperature=0.4\n    )\n    \n    try:\n        return (calcCost(prompt),str(int(re.sub(\"[^0-9]\", \"\",ans['choices'][0]['text'].strip()))+starting_tool_num))\n    except:\n        return  (calcCost(prompt),1+starting_tool_num)", "\ndef calcCost(p):\n            return (len(p) / 2700.0) * 0.02\n\ndef question_split(question,tools_list,memory):\n    tools=\"\"\n    starting_tool_num=2\n    count=starting_tool_num\n    for i in range(len(tools_list)-starting_tool_num):\n        tools+=\"tool \"+str(i+starting_tool_num)+\": \"+str(tools_list[starting_tool_num+i][\"description\"])+\"\\n\"\n        count+=1\n    tools+=\"tool \"+str(count)+\": \"+str(\"Use this tool when the question can be answered without using any tool or if the question is a greeting or a casual conversation.\")+\"\\n\"\n\n    prompt='''\nTools we have access to =\ntool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\ntool 2: Find the driving distance and time to travel between two cities.\ntool 3: Find the weather at a location and returns it in celcius.\n\nQ=\"Is the distance between London and Paris larger than the distance between Boston and LA?\"\nLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\nWhat is the distance from London to Paris?, What is the distance from Boston to LA?\n\nTools we have access to =\ntool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\ntool 2: Find the driving distance and time to travel between two cities.\ntool 3: Find the weather at a location and returns it in celcius.\n\nQ=\"Who was the maternal grandfather of george washington?\"\nLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\nWho was george washington's mother?,Who was her father?\n\nTools we have access to =\ntool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\ntool 2: Find the driving distance and time to travel between two cities.\ntool 3: Find the weather at a location and returns it in celcius.\n\nQ:\"What is the currency in India\"\nLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\n\nTools we have access to =\ntool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\ntool 2: Find the driving distance and time to travel between two cities.\ntool 3: Find the weather at a location and returns it in celcius.\n\nQ:\"If I am in Miami how far am I from Atlanta?\"\nLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\n\nTools we have access to =\n{tools}\nQ= \"{question}\"\nLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n    '''\n    prompt=prompt.format(**{\"question\":question,\"tools\":tools,\"memory\":memory})\n    ans = openai.Completion.create(\n    model=\"text-davinci-003\",\n    max_tokens=256,\n    stop=None,\n    prompt=prompt,\n    temperature=0.2\n    )\n    ans=ans['choices'][0]['text'].replace(\"\\n\",\"\").split(\",\")\n    return (calcCost(prompt),ans)", "\n\ndef memory_check(memory,question):\n\n    prompt='''\nQ: \"What's up?\"\n\nIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. yes\n\nQ: \"What color is the sky\"\n\nIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. yes\n\nQ: \"What is the temperature in Portland?\"\n\nIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. no\n\nMemory:\n{memory}\n\nQ: \"{question}\"\n\nIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no.\n    '''\n    prompt=prompt.format(**{\"memory\":memory,\"question\":question})\n    ans = openai.Completion.create(\n                model=\"text-davinci-003\",\n                max_tokens=256,\n                stop=None,\n                prompt=prompt,\n                temperature=0.4\n            )\n\n    ans=ans['choices'][0]['text']\n\n    if \"yes\" in ans.lower():\n        return (calcCost(prompt),True)\n    else:\n        return (calcCost(prompt),False)", "\ndef replace_variables_for_values(my_dict: dict, dynamic_keys, ignore_key: str = \"_______\"):\n    replaced_dict = {}\n    for key, value in my_dict.items():\n        if (key == ignore_key):\n            continue;\n        formatted_key = key.format(**dynamic_keys)\n        if (isinstance(value, dict)):\n            formatted_value = replace_variables_for_values(value, dynamic_keys)\n        elif (isinstance(value, list)):\n            formatted_value = []\n            for item in value:\n                formatted_value += replace_variables_for_values(item, dynamic_keys)\n        else:\n            try:\n                formatted_value = value.format(**dynamic_keys)\n            except:\n                formatted_value = value\n        replaced_dict[formatted_key] = formatted_value\n    return replaced_dict", "#print(question_split(\"\"))\n"]}
{"filename": "src/llm_vm/agents/REBEL/agent.py", "chunked_list": ["\"\"\"\nThis Python script runs the REBEL agent, which takes into account of multiple questions within a\nsingle user input.\nThe user is prompted to enter a question, to which the AI responds.\nIf multiple questions are present, the script splits these into different subquestions and retrieves\nanswers from the respective APIs.\nThe script also takes into account of previous prompts as history, in case the user may\nenter related questions later.\nThe user will also see messages regarding which API for information was chosen and the\nprice of the API call.", "The user will also see messages regarding which API for information was chosen and the\nprice of the API call.\n\"\"\"\n\nimport os\nimport sys\n\n# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(current_dir))", "# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(current_dir))\n# utils_dir = os.path.join(grandparent_dir, \"utils/\")\n# sys.path.append(utils_dir)\n\nfrom llm_vm.utils.keys import *\nfrom llm_vm.utils.labels import *\nfrom llm_vm.utils.print_types import *\nfrom llm_vm.utils.tools import *\nfrom urllib.parse import urlencode", "from llm_vm.utils.tools import *\nfrom urllib.parse import urlencode\nimport urllib.parse as urlparse\nimport json\nimport random\n\n# UNCOMMENT THESE IMPORTS SO THAT SUBQUESTION PORTION CAN WORK (NLP CURRENTLY NOT RECOGNIZED)\nimport spacy\n\nnlp = None #spacy.load(\"en_core_web_md\")", "\nnlp = None #spacy.load(\"en_core_web_md\")\n\nfrom math import sqrt, pow, exp\n\ntry:\n    from .bothandler import (\n        question_split,\n        tool_picker,\n        memory_check,\n        replace_variables_for_values,\n    )\n\nexcept:\n    from bothandler import (\n        question_split,\n        tool_picker,\n        memory_check,\n        replace_variables_for_values,\n    )", "\ntry:\n    from .utils import *\nexcept:\n    from utils import *\n\n\ndef prepPrintPromptContext(p):\n    return \"--> \" + p.replace(\"\\n\", \"\\n--> \") if len(p) > 0 else \"\"\n", "\n\ndef print_op(*kargs, **kwargs):\n    print(*kargs, **kwargs, flush=True)\n\n\nrandom_fixed_seed = random.Random(4)\n\nQUALITY = 0.2\n", "QUALITY = 0.2\n\n\ndef buildExampleTools(tools=GENERIC_TOOLS):\n    new_tools = tools\n\n    # wolfram\n    new_tools[0][\"examples\"] = [\n        (\n            [\n                (\n                    \"What's the most popular spot to vacation if you are in Germany?\",\n                    \"Mallorca\",\n                ),\n                (\"What's the date\", \"July 7, 2009\"),\n            ],\n            \"How crowded is it there now?\",\n            '{\"q\": \"How many tourists visit Mallorca each July?\"}',\n        ),\n        (\n            [\n                (\"What is the circumference of a basketball?\", \"750mm\"),\n            ],\n            \"What is the volume of a basketball?\",\n            '{\"q\": \"What is the volume of a ball with circumference 750mm?\"}',\n        ),\n        (\n            [\n                (\"What's the fastest a ford explorer can drive?\", \"160mph\"),\n            ],\n            \"What is that multiplied by seventy?\",\n            '{\"q\": \"160 x 70\"}',\n        ),\n        (\n            [],\n            \"Is 5073 raised to the 3rd power divisible by 73?\",\n            '{\"q\": \"Is 5073^3 divisible by 73?\"}',\n        ),\n    ]\n\n    # geopy\n    new_tools[1][\"examples\"] = [\n        (\n            [\n                (\n                    \"I feel like taking a drive today to zurich can you help?\",\n                    \"Yes!  Where are you now?\",\n                ),\n            ],\n            \"I'm in Paris.\",\n            '{\"origins\": \"Paris\", \"destinations\": \"Zurich\"}',\n        ),\n        (\n            [],\n            \"How long would it take to get between South Africa and Kenya.\",\n            '{\"origins\": \"South Africa\", \"destinations\": \"Kenya\"}',\n        ),\n        (\n            [\n                (\"Where do elephant seals mate?\", \"San Luis Obispo\"),\n                (\"Thats really cool!\", \"Damn right.\"),\n                (\"Where do they migrate each year?\", \"Alaska\"),\n            ],\n            \"How many miles would they travel while doing that?\",\n            '{\"origins\": \"San Luis Obispo\", \"destinations\": \"Alaska\"}',\n        ),\n    ]\n\n    # weather\n    new_tools[2][\"examples\"] = [\n        (\n            [\n                (\"Are you allergic to seafood?\", \"I'm just an AI. I have no body.\"),\n                (\"What city is Obama in?\", \"NYC\"),\n                (\"What is the latitude of NYC?\", \"40.7128\u00b0 N\"),\n                (\n                    \"What is the longitude of NYC?\",\n                    \"The longitude is 73\u00b0 56' 6.8712'' W\",\n                ),\n            ],\n            \"What is the weather in NYC?\",\n            '{\"longitude\": 73.56, \"latitude\": 40.41728}',\n        ),\n        (\n            [\n                (\"What is the longitude of Milan?\", \"9.1900\u00b0 E\"),\n                (\"What is the latitude of Milan?\", \"45.4642\u00b0 N\"),\n            ],\n            \"What is the weather in Milan?\",\n            '{\"longitude\": 45.4642, \"latitude\": 9.1900}',\n        ),\n    ]\n    return new_tools", "\n\ndef squared_sum(x):\n    \"\"\"return 3 rounded square rooted value\"\"\"\n\n    return round(sqrt(sum([a * a for a in x])), 3)\n\n\ndef cos_similarity(x, y):\n    \"\"\"return cosine similarity between two lists\"\"\"\n\n    numerator = sum(a * b for a, b in zip(x, y))\n    denominator = squared_sum(x) * squared_sum(y)\n    return round(numerator / float(denominator), 3)", "def cos_similarity(x, y):\n    \"\"\"return cosine similarity between two lists\"\"\"\n\n    numerator = sum(a * b for a, b in zip(x, y))\n    denominator = squared_sum(x) * squared_sum(y)\n    return round(numerator / float(denominator), 3)\n\n\nclass Agent:\n    def __init__(self, openai_key, tools, bot_str=\"\", verbose=4):\n        self.verbose = verbose\n        self.price = 0\n        self.tools = []\n        self.set_tools(buildExampleTools()+tools)\n        if bot_str == \"\":\n            self.bot_str = bot_str\n        else:\n            self.bot_str = \"<GLOBAL>\" + bot_str + \"<GLOBAL>\"\n        self.nlp=spacy.load(\"en_core_web_md\")\n        # set all the API resource keys to make calls\n        set_api_key(openai_key, \"OPENAI_API_KEY\")\n\n    def makeToolDesc(self, tool_id):\n        \"\"\"\n        Creates the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        tool_id\n            the tool's enum value as specified in the DefaultTools class\n\n        Returns\n        ----------\n        String\n            a formatted string with tags containing the tool_id, description and params\n        \"\"\"\n\n        tool = self.tools[tool_id]\n        params = (\n            \"{\"\n            + \", \".join(\n                ['\"' + l + '\": ' + v for l, v in tool[\"dynamic_params\"].items()]\n            )\n            + \"}\"\n            if tool[\"dynamic_params\"] != \"\"\n            else \"{}\"\n        )\n        return f\"\"\"<TOOL>\n    <{TOOL_ID}>{str(tool_id)}</{TOOL_ID}>\n    <{DESCRIPTION}>{tool['description']}</{DESCRIPTION}>\n    <{PARAMS}>{params}</{PARAMS}>\n    </TOOL>\"\"\"\n\n    def set_tools(self, tools):\n        \"\"\"\n        Adds all the available tools when the class is initialized.\n\n        Parameters\n        ----------\n        tools\n            a list of available tools. Each tool is defined within a Dict.\n\n        Returns\n        ----------\n        None\n        \"\"\"\n\n        for tool in tools:\n            if not \"args\" in tool:\n                tool[\"args\"] = {}\n            if not \"method\" in tool:\n                tool[\"method\"] = \"GET\"\n            if not \"examples\" in tool:\n                tool[\"examples\"] = []\n            if not \"dynamic_params\" in tool:\n                tool[\"dynamic_params\"] = {}\n            self.tools += [tool]\n\n    def use_tool(self, tool, gpt_suggested_input, question, memory, facts, query=\"\"):\n        \"\"\"\n        Calls the appropriate API based on the tool that's in use.\n\n        Parameters\n        ----------\n        tool\n            an integer refencing the selected tool\n        gpt_suggested_input\n            the input params for the selected tool as specified by the gpt response\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        query\n            the value of the ai_response_prompt key for the selected tool. If ai_response_prompt key is not available this will have same value as question parameter\n\n        Returns\n        ----------\n        String\n            Response text after calling API tool and passing result into ChatGPT prompt\n        \"\"\"\n\n        return tool_api_call(\n            self, tool, gpt_suggested_input, question, memory, facts, query=query\n        )\n\n    def run(self, question, memory):\n        \"\"\"\n        Runs the Agent on the given inputs\n\n        Parameters\n        ----------\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n\n        Returns\n        ----------\n        Tuple\n            a tuple containing the Agent's answer and a list of the conversation history\n        \"\"\"\n\n        self.price = 0\n\n        thought, facts = self.promptf(\n            question,\n            memory,\n            [],\n            0\n        )\n\n        if self.verbose > -1:\n            print_big(\"GPT-3.5 Price = ~{:.1f} cents\".format(self.price * 100))\n\n        # return (answer, memory + [(question, answer)], calls, debug_return, has_friendly_tags)\n        return (thought, memory + [(question, thought)])\n\n    def makeInteraction(self, p, a, Q=\"HUMAN\", A=\"AI\", INTERACTION=INTERACTION):\n        \"\"\"\n        Formats the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        p\n            the question being asked\n        a\n            the gpt response\n        Q\n            the entity asking the question. Could be HUMAN or AI.\n        A\n            the entity answering the question. Could be HUMAN or AI.\n        INTERACTION\n            the type of interaction. Could be HUMAN-AI or AI-AI.\n\n        Returns\n        ----------\n        String\n            a formatted string with tags containing the type of interaction, the question and the gpt response\n        \"\"\"\n\n        return f\"<>{INTERACTION}:<{Q}>{p}</{Q}>\\n<{A}>\" + (\n            f\"{a}</{A}>\\n</>\" if a is not None else \"\"\n        )\n\n    def make_sub(\n        self,\n        tools,\n        memory,\n        facts,\n        question,\n        subq,\n        answer_label,\n        toolEx,\n        tool_to_use=None,\n        bot_str=\"\",\n        quality=\"best\",\n        max_tokens=20,\n    ):\n        \"\"\"\n        Formats the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        tools\n            a list of available tools. Each tool is defined within a Dict.\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        question\n            the user's input question\n        subq\n            a lambda function that returns a question that's used to prompt gpt for the suggested input for the selected tool\n        answer_label\n            the format of the response\n        toolEx\n            a lambda function that returns an example for the selected tool\n        tool_to_use\n            an integer referencing the selected tool\n        bot_str\n            a string to be appended to the gpt prompt\n        quality\n            could be either \"okay\"-text-curie-001 or \"best\"-text-davinci-003\n        max_tokens\n            maximum number of tokens to be generated\n\n        Returns\n        ----------\n        String\n            a gpt text response containing the suggested input for the selected tool\n        \"\"\"\n        tool_context = \"\".join([self.makeToolDesc(t) for t, _ in tools])\n        mem = \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                for p, a in memory\n            ]\n        ) + \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                for p, a in facts\n            ]\n        )\n\n        def makeQuestion(memory, question, tool=None):\n            return (\n                f\"\\n\\n<{EXAMPLE}>\\n\"\n                + f\"<{QUESTION}>{question}</{QUESTION}>\\n\"\n                + f\"<{THOUGHT}><{PROMPT}>{subq(tool)}</{PROMPT}>\\n<{RESPONSE} ty={answer_label}>\\n\"\n            )\n\n        examples = []\n        for tool_id, tool in tools:\n            for tool_example in tool[\"examples\"]:\n                examples += [\n                    makeQuestion(tool_example[0], tool_example[1], tool_id)\n                    + toolEx(tool_id, tool_example)\n                    + f\"</{RESPONSE}></{THOUGHT}></{EXAMPLE}>\"\n                ]\n\n        random_fixed_seed.shuffle(examples)\n\n        cur_question = f\"<{CONVERSATION}>{mem}</{CONVERSATION}>\" + makeQuestion(\n            memory, question, tool=tool_to_use\n        )\n        prompt = MSG(\"system\", \"You are a good and helpful assistant.\")\n        prompt += MSG(\n            \"user\",\n            \"<TOOLS>\" + tool_context + \"</TOOLS>\" + \"\".join(examples) + cur_question,\n        )\n        return call_ChatGPT(\n            self, prompt, stop=f\"</{RESPONSE}>\", max_tokens=max_tokens\n        ).strip()\n\n    def promptf(self, question, memory, facts, level, split_allowed=True, max_level=3):\n        \"\"\"\n        Formats the gpt prompt to include conversation history, facts and logs responses to the console\n\n        Parameters\n        ----------\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        level\n            param indicating the current recursive level\n\n        split_allowed\n            a boolean to allow the question to be split into sub-questions if set to True\n\n        max_level\n            param that indicates the maximum recursive level we want to allow.\n\n        Returns\n        ----------\n        Tuple\n            a tuple containing the gpt response and a list with the conversation history\n        \"\"\"\n\n        mem = \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                for p, a in memory\n            ]\n        ) + \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                for p, a in facts\n            ]\n        )\n\n        if split_allowed:\n            subq = question_split(question, self.tools, mem)\n\n            subq_final = []\n\n            if len(subq[1]) == 1:\n                split_allowed = False\n\n            else:\n                for i in subq[1]:\n                    if cos_similarity(self.nlp(question).vector, self.nlp(i).vector) < 0.98:\n                        subq_final.append(i)\n                    else:\n                        split_allowed = False\n\n            if level==max_level:\n                split_allowed = False\n\n            self.price += subq[0]\n            new_facts = []\n\n            for i in range(len(subq_final)):\n                _, new_facts = self.promptf(\n                    subq_final[i],\n                    memory,\n                    facts,\n                    level+1,\n                    split_allowed=split_allowed,\n                )\n                facts = facts + new_facts\n                mem = \"\".join(\n                    [\n                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                        for p, a in memory\n                    ]\n                ) + \"\".join(\n                    [\n                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                        for p, a in facts\n                    ]\n                )\n\n        answer_in_memory = memory_check(mem, question)\n        self.price += answer_in_memory[0]\n        answer_in_memory = answer_in_memory[1]\n        if answer_in_memory:\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n            )\n            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n            print_big(\"NO DATA TOOLS USED, ANSWERED FROM MEMORY\")\n            return (a.replace(\"\\n\", \"\"), [(question, a)])\n\n        tool_to_use = tool_picker(self.tools, question, 3)\n\n\n        print_big(\n            \"\".join(\n                [\n                    f'{\"\u2713\" if self.tools.index(tool) == tool_to_use else \" \"} {self.tools.index(tool)}.- {tool[\"description\"]}\\n'\n                    for tool in self.tools[3:0]\n                ]\n            )\n            + f\"\\n> Question: {question}\\n> Raw answer: '{tool_to_use}'\\n> Tool ID: {tool_to_use}\",\n            \"LIST OF DATA TOOLS\",\n        )\n\n        self.price += tool_to_use[0]\n        try:\n            tool_to_use = int(tool_to_use[1])\n        except:\n            tool_to_use = len(self.tools)\n        if tool_to_use >= len(self.tools):\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem\n                + \"\\nQ:\"\n                + question\n                + \"\\nUsing this information, what is the answer to Q?\",\n            )\n            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\n            return (a.replace(\"\\n\", \"\"), [(question, a)])\n        tool_input = self.make_sub(\n            list(enumerate(self.tools)),\n            memory,\n            facts,\n            question,\n            lambda t: \"What should the input for tool \" + str(t) + \" be to answer Q?\",\n            \"JSON\",\n            lambda t, ex: ex[2],\n            tool_to_use=tool_to_use,\n            quality=\"best\" if self.price < QUALITY else \"okay\",\n            max_tokens=200,\n        )\n\n        query = question\n        if \"ai_response_prompt\" in self.tools[tool_to_use].keys():\n            query = self.tools[tool_to_use][\"ai_response_prompt\"]\n        try:\n            answer = self.use_tool(\n                self.tools[tool_to_use], tool_input, question, memory, facts, query=query\n            )\n        except:\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n            )\n            answer = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\n\n        return (answer, [(question, answer)])", "class Agent:\n    def __init__(self, openai_key, tools, bot_str=\"\", verbose=4):\n        self.verbose = verbose\n        self.price = 0\n        self.tools = []\n        self.set_tools(buildExampleTools()+tools)\n        if bot_str == \"\":\n            self.bot_str = bot_str\n        else:\n            self.bot_str = \"<GLOBAL>\" + bot_str + \"<GLOBAL>\"\n        self.nlp=spacy.load(\"en_core_web_md\")\n        # set all the API resource keys to make calls\n        set_api_key(openai_key, \"OPENAI_API_KEY\")\n\n    def makeToolDesc(self, tool_id):\n        \"\"\"\n        Creates the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        tool_id\n            the tool's enum value as specified in the DefaultTools class\n\n        Returns\n        ----------\n        String\n            a formatted string with tags containing the tool_id, description and params\n        \"\"\"\n\n        tool = self.tools[tool_id]\n        params = (\n            \"{\"\n            + \", \".join(\n                ['\"' + l + '\": ' + v for l, v in tool[\"dynamic_params\"].items()]\n            )\n            + \"}\"\n            if tool[\"dynamic_params\"] != \"\"\n            else \"{}\"\n        )\n        return f\"\"\"<TOOL>\n    <{TOOL_ID}>{str(tool_id)}</{TOOL_ID}>\n    <{DESCRIPTION}>{tool['description']}</{DESCRIPTION}>\n    <{PARAMS}>{params}</{PARAMS}>\n    </TOOL>\"\"\"\n\n    def set_tools(self, tools):\n        \"\"\"\n        Adds all the available tools when the class is initialized.\n\n        Parameters\n        ----------\n        tools\n            a list of available tools. Each tool is defined within a Dict.\n\n        Returns\n        ----------\n        None\n        \"\"\"\n\n        for tool in tools:\n            if not \"args\" in tool:\n                tool[\"args\"] = {}\n            if not \"method\" in tool:\n                tool[\"method\"] = \"GET\"\n            if not \"examples\" in tool:\n                tool[\"examples\"] = []\n            if not \"dynamic_params\" in tool:\n                tool[\"dynamic_params\"] = {}\n            self.tools += [tool]\n\n    def use_tool(self, tool, gpt_suggested_input, question, memory, facts, query=\"\"):\n        \"\"\"\n        Calls the appropriate API based on the tool that's in use.\n\n        Parameters\n        ----------\n        tool\n            an integer refencing the selected tool\n        gpt_suggested_input\n            the input params for the selected tool as specified by the gpt response\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        query\n            the value of the ai_response_prompt key for the selected tool. If ai_response_prompt key is not available this will have same value as question parameter\n\n        Returns\n        ----------\n        String\n            Response text after calling API tool and passing result into ChatGPT prompt\n        \"\"\"\n\n        return tool_api_call(\n            self, tool, gpt_suggested_input, question, memory, facts, query=query\n        )\n\n    def run(self, question, memory):\n        \"\"\"\n        Runs the Agent on the given inputs\n\n        Parameters\n        ----------\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n\n        Returns\n        ----------\n        Tuple\n            a tuple containing the Agent's answer and a list of the conversation history\n        \"\"\"\n\n        self.price = 0\n\n        thought, facts = self.promptf(\n            question,\n            memory,\n            [],\n            0\n        )\n\n        if self.verbose > -1:\n            print_big(\"GPT-3.5 Price = ~{:.1f} cents\".format(self.price * 100))\n\n        # return (answer, memory + [(question, answer)], calls, debug_return, has_friendly_tags)\n        return (thought, memory + [(question, thought)])\n\n    def makeInteraction(self, p, a, Q=\"HUMAN\", A=\"AI\", INTERACTION=INTERACTION):\n        \"\"\"\n        Formats the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        p\n            the question being asked\n        a\n            the gpt response\n        Q\n            the entity asking the question. Could be HUMAN or AI.\n        A\n            the entity answering the question. Could be HUMAN or AI.\n        INTERACTION\n            the type of interaction. Could be HUMAN-AI or AI-AI.\n\n        Returns\n        ----------\n        String\n            a formatted string with tags containing the type of interaction, the question and the gpt response\n        \"\"\"\n\n        return f\"<>{INTERACTION}:<{Q}>{p}</{Q}>\\n<{A}>\" + (\n            f\"{a}</{A}>\\n</>\" if a is not None else \"\"\n        )\n\n    def make_sub(\n        self,\n        tools,\n        memory,\n        facts,\n        question,\n        subq,\n        answer_label,\n        toolEx,\n        tool_to_use=None,\n        bot_str=\"\",\n        quality=\"best\",\n        max_tokens=20,\n    ):\n        \"\"\"\n        Formats the tool description to contain the relevant tags according to the dynamic params\n\n        Parameters\n        ----------\n        tools\n            a list of available tools. Each tool is defined within a Dict.\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        question\n            the user's input question\n        subq\n            a lambda function that returns a question that's used to prompt gpt for the suggested input for the selected tool\n        answer_label\n            the format of the response\n        toolEx\n            a lambda function that returns an example for the selected tool\n        tool_to_use\n            an integer referencing the selected tool\n        bot_str\n            a string to be appended to the gpt prompt\n        quality\n            could be either \"okay\"-text-curie-001 or \"best\"-text-davinci-003\n        max_tokens\n            maximum number of tokens to be generated\n\n        Returns\n        ----------\n        String\n            a gpt text response containing the suggested input for the selected tool\n        \"\"\"\n        tool_context = \"\".join([self.makeToolDesc(t) for t, _ in tools])\n        mem = \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                for p, a in memory\n            ]\n        ) + \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                for p, a in facts\n            ]\n        )\n\n        def makeQuestion(memory, question, tool=None):\n            return (\n                f\"\\n\\n<{EXAMPLE}>\\n\"\n                + f\"<{QUESTION}>{question}</{QUESTION}>\\n\"\n                + f\"<{THOUGHT}><{PROMPT}>{subq(tool)}</{PROMPT}>\\n<{RESPONSE} ty={answer_label}>\\n\"\n            )\n\n        examples = []\n        for tool_id, tool in tools:\n            for tool_example in tool[\"examples\"]:\n                examples += [\n                    makeQuestion(tool_example[0], tool_example[1], tool_id)\n                    + toolEx(tool_id, tool_example)\n                    + f\"</{RESPONSE}></{THOUGHT}></{EXAMPLE}>\"\n                ]\n\n        random_fixed_seed.shuffle(examples)\n\n        cur_question = f\"<{CONVERSATION}>{mem}</{CONVERSATION}>\" + makeQuestion(\n            memory, question, tool=tool_to_use\n        )\n        prompt = MSG(\"system\", \"You are a good and helpful assistant.\")\n        prompt += MSG(\n            \"user\",\n            \"<TOOLS>\" + tool_context + \"</TOOLS>\" + \"\".join(examples) + cur_question,\n        )\n        return call_ChatGPT(\n            self, prompt, stop=f\"</{RESPONSE}>\", max_tokens=max_tokens\n        ).strip()\n\n    def promptf(self, question, memory, facts, level, split_allowed=True, max_level=3):\n        \"\"\"\n        Formats the gpt prompt to include conversation history, facts and logs responses to the console\n\n        Parameters\n        ----------\n        question\n            the user's input question\n        memory\n            a list of tuples containing the conversation history\n        facts\n            a list containing factual answers generated for each sub-question\n        level\n            param indicating the current recursive level\n\n        split_allowed\n            a boolean to allow the question to be split into sub-questions if set to True\n\n        max_level\n            param that indicates the maximum recursive level we want to allow.\n\n        Returns\n        ----------\n        Tuple\n            a tuple containing the gpt response and a list with the conversation history\n        \"\"\"\n\n        mem = \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                for p, a in memory\n            ]\n        ) + \"\".join(\n            [\n                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                for p, a in facts\n            ]\n        )\n\n        if split_allowed:\n            subq = question_split(question, self.tools, mem)\n\n            subq_final = []\n\n            if len(subq[1]) == 1:\n                split_allowed = False\n\n            else:\n                for i in subq[1]:\n                    if cos_similarity(self.nlp(question).vector, self.nlp(i).vector) < 0.98:\n                        subq_final.append(i)\n                    else:\n                        split_allowed = False\n\n            if level==max_level:\n                split_allowed = False\n\n            self.price += subq[0]\n            new_facts = []\n\n            for i in range(len(subq_final)):\n                _, new_facts = self.promptf(\n                    subq_final[i],\n                    memory,\n                    facts,\n                    level+1,\n                    split_allowed=split_allowed,\n                )\n                facts = facts + new_facts\n                mem = \"\".join(\n                    [\n                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n                        for p, a in memory\n                    ]\n                ) + \"\".join(\n                    [\n                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n                        for p, a in facts\n                    ]\n                )\n\n        answer_in_memory = memory_check(mem, question)\n        self.price += answer_in_memory[0]\n        answer_in_memory = answer_in_memory[1]\n        if answer_in_memory:\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n            )\n            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n            print_big(\"NO DATA TOOLS USED, ANSWERED FROM MEMORY\")\n            return (a.replace(\"\\n\", \"\"), [(question, a)])\n\n        tool_to_use = tool_picker(self.tools, question, 3)\n\n\n        print_big(\n            \"\".join(\n                [\n                    f'{\"\u2713\" if self.tools.index(tool) == tool_to_use else \" \"} {self.tools.index(tool)}.- {tool[\"description\"]}\\n'\n                    for tool in self.tools[3:0]\n                ]\n            )\n            + f\"\\n> Question: {question}\\n> Raw answer: '{tool_to_use}'\\n> Tool ID: {tool_to_use}\",\n            \"LIST OF DATA TOOLS\",\n        )\n\n        self.price += tool_to_use[0]\n        try:\n            tool_to_use = int(tool_to_use[1])\n        except:\n            tool_to_use = len(self.tools)\n        if tool_to_use >= len(self.tools):\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem\n                + \"\\nQ:\"\n                + question\n                + \"\\nUsing this information, what is the answer to Q?\",\n            )\n            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\n            return (a.replace(\"\\n\", \"\"), [(question, a)])\n        tool_input = self.make_sub(\n            list(enumerate(self.tools)),\n            memory,\n            facts,\n            question,\n            lambda t: \"What should the input for tool \" + str(t) + \" be to answer Q?\",\n            \"JSON\",\n            lambda t, ex: ex[2],\n            tool_to_use=tool_to_use,\n            quality=\"best\" if self.price < QUALITY else \"okay\",\n            max_tokens=200,\n        )\n\n        query = question\n        if \"ai_response_prompt\" in self.tools[tool_to_use].keys():\n            query = self.tools[tool_to_use][\"ai_response_prompt\"]\n        try:\n            answer = self.use_tool(\n                self.tools[tool_to_use], tool_input, question, memory, facts, query=query\n            )\n        except:\n            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n            prompt += MSG(\n                \"user\",\n                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n            )\n            answer = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\n\n        return (answer, [(question, answer)])", "\ndef rebel_main():\n    tools = buildExampleTools()\n\n\n    label = Agent(os.getenv(\"LLM_VM_OPENAI_API_KEY\"), tools, verbose=1)\n    conversation_history = []\n    last = \"\"\n    while True:\n        inp = input(last + \"Human: \")\n        return_value = label.run(inp, conversation_history)\n        conversation_history = return_value[1]\n        last = \"AI: \" + str(return_value[0]) + \"\\n\"", "\nif __name__ == \"__main__\":\n    rebel_main()\n"]}
{"filename": "src/llm_vm/agents/REBEL/test_agent.py", "chunked_list": ["import json\nimport openai\nfrom agent import Agent\nimport time\nimport os, sys\n\ngoogle_tool = {\n               'description': \"Returns the result of a google search result about information on the internet. Good for retrieving information about the world and live information.\",\n               'dynamic_params': {\"q\": 'The natural language input query'},\n               'method': 'GET',", "               'dynamic_params': {\"q\": 'The natural language input query'},\n               'method': 'GET',\n               'args': {'url': \"https://www.googleapis.com/customsearch/v1\",\n                         'params': {'key': \"Enter Key Here\",\n                                    'cx' : 'Enter CX Here',\n                                    'q': '{q}'}\n                        }\n               }\n\nclass suppress_output:\n    def __init__(self, suppress_stdout=False, suppress_stderr=False):\n        self.suppress_stdout = suppress_stdout\n        self.suppress_stderr = suppress_stderr\n        self._stdout = None\n        self._stderr = None\n\n    def __enter__(self):\n        devnull = open(os.devnull, \"w\")\n        if self.suppress_stdout:\n            self._stdout = sys.stdout\n            sys.stdout = devnull\n\n        if self.suppress_stderr:\n            self._stderr = sys.stderr\n            sys.stderr = devnull\n\n    def __exit__(self, *args):\n        if self.suppress_stdout:\n            sys.stdout = self._stdout\n        if self.suppress_stderr:\n            sys.stderr = self._stderr", "\nclass suppress_output:\n    def __init__(self, suppress_stdout=False, suppress_stderr=False):\n        self.suppress_stdout = suppress_stdout\n        self.suppress_stderr = suppress_stderr\n        self._stdout = None\n        self._stderr = None\n\n    def __enter__(self):\n        devnull = open(os.devnull, \"w\")\n        if self.suppress_stdout:\n            self._stdout = sys.stdout\n            sys.stdout = devnull\n\n        if self.suppress_stderr:\n            self._stderr = sys.stderr\n            sys.stderr = devnull\n\n    def __exit__(self, *args):\n        if self.suppress_stdout:\n            sys.stdout = self._stdout\n        if self.suppress_stderr:\n            sys.stderr = self._stderr", "\nopenai.api_key = \"\"\nagent = Agent(openai.api_key,[], verbose=-1)\nagent.set_tools([google_tool])\n\nf = open(\"compositional_celebrities.json\")\ndata = json.load(f)\ncategory = []\nq_and_a={}\nfor i in data[\"data\"]:\n    if i[\"category\"] in ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']:\n        if i[\"category\"] not in category:\n            category.append(i[\"category\"])\n            q_and_a.update({i[\"category\"]:[(i[\"Question\"],i[\"Answer\"])]})\n        else:\n            q_and_a[i[\"category\"]].append((i[\"Question\"],i[\"Answer\"]))", "q_and_a={}\nfor i in data[\"data\"]:\n    if i[\"category\"] in ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']:\n        if i[\"category\"] not in category:\n            category.append(i[\"category\"])\n            q_and_a.update({i[\"category\"]:[(i[\"Question\"],i[\"Answer\"])]})\n        else:\n            q_and_a[i[\"category\"]].append((i[\"Question\"],i[\"Answer\"]))\n\nop = ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']\nfor j in range(0,len(op)):\n    print(j,op[j])", "\nop = ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']\nfor j in range(0,len(op)):\n    print(j,op[j])\nidx=int(input())\n\nchoice=op[idx]\nfor i in range(100):\n    with suppress_output(suppress_stdout=True, suppress_stderr=True):\n        try:\n            a=agent.run(q_and_a[choice][i][0],[])\n        except Exception as e:\n            a = [e,\"\"]\n    print(str(i)+\"|\",str(a[0])+\"|\",q_and_a[choice][i][1])", ""]}
{"filename": "src/llm_vm/agents/REBEL/utils.py", "chunked_list": ["import requests\nimport traceback\nimport os\nimport random\nimport concurrent.futures\nimport openai\nimport urllib.request\nimport json\nimport string\nimport re", "import string\nimport re\nimport sys\n\n\n\ndef flatten(a):\n    return sum(a, [])\n\n\ndef print_op(*kargs, **kwargs):\n    print(*kargs, **kwargs, flush=True)", "\n\ndef print_op(*kargs, **kwargs):\n    print(*kargs, **kwargs, flush=True)\n\n\ndef prepPrintPromptContext(p):\n    return \"--> \" + p.replace(\"\\n\", \"\\n--> \") if len(p) > 0 else \"\"\n\n\ndef MSG(u, c):\n    return [{\"role\": u, \"content\": c}]", "\n\ndef MSG(u, c):\n    return [{\"role\": u, \"content\": c}]\n\n\ndef call_ChatGPT(state, cur_prompt, stop=None, max_tokens=20, temperature=0.2):\n    if state.verbose > 1:\n        print_op(\"\\nGPT input for {\" + str(stop) + \"} \" + str(len(cur_prompt)) + \".\")\n    if state.verbose > 2:\n        print_op(str(cur_prompt))\n\n    ppt = 0.002\n\n    def calcCost(p):\n        chars = sum((len(a[\"content\"]) for a in p))\n        if state.verbose > 0:\n            print_op(\"ASK_CHARS:\", chars)\n        c = (chars / 2700.0) * ppt\n        if state.verbose > 2:\n            print_op(\"PrePrice: $\" + str(c))\n        return c\n\n    cost = calcCost(cur_prompt)\n    try:\n        ans = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0301\",\n            max_tokens=max_tokens,\n            stop=stop,\n            messages=cur_prompt,\n            temperature=temperature,\n        )\n\n    except Exception as e:\n        state.price += cost\n        traceback.print_exc()\n        print_op(\"Error:\", e)\n\n        return \"OpenAI is down!\"\n\n    price = ppt * ans[\"usage\"][\"total_tokens\"] / 1000\n    if state.verbose > 0:\n        print_op(\"Price: $\" + str(price))\n    state.price += price\n    response_text = ans[\"choices\"][0][\"message\"][\"content\"]\n\n    if state.verbose > 2:\n        print_op(\"GPT output:\")\n        print_op(prepPrintPromptContext(response_text))\n        print_op(\"GPT output fin.\\n\")\n\n    return response_text", "\n\ndef call_gpt(\n    state, cur_prompt: str, stop: str, max_tokens=20, quality=\"best\", temperature=0.0\n):\n    if state.verbose > 1:\n        print_op(\"\\nGPT input for {\" + stop + \"} \" + str(len(cur_prompt)) + \".\")\n    if state.verbose > 2:\n        print_op(prepPrintPromptContext(cur_prompt))\n\n    ask_tokens = max_tokens + len(cur_prompt) / 2.7\n\n    if state.verbose > 0:\n        print_op(\"ASK_TOKENS:\", ask_tokens)\n\n    if (ask_tokens) > 2049:\n        quality = \"best\"\n\n    model = {\n        \"best\": (\"text-davinci-003\", 0.02),\n        \"okay\": (\"text-curie-001\", 0.002),\n    }[quality]\n\n    def calcCost(p):\n        return (len(p) / 2700.0) * model[1]\n\n    cost = calcCost(cur_prompt)\n\n    try:\n        ans = openai.Completion.create(\n            model=model[0],\n            max_tokens=max_tokens,\n            stop=stop,\n            prompt=cur_prompt,\n            temperature=temperature,\n        )\n    except Exception as e:\n        print_op(\"WTF:\", e)\n        state.price += cost\n        return \"OpenAI is down!\"\n\n    response_text = ans[\"choices\"][0][\"text\"]\n\n    simpleprice = model[1] * ans[\"usage\"][\"total_tokens\"] / 1000\n    if state.verbose > 0:\n        print_op(\"SimplePrice: $\" + str(simpleprice))\n    state.price += simpleprice\n\n    if state.verbose > 2:\n        print_op(\"GPT output:\")\n        print_op(prepPrintPromptContext(response_text))\n        print_op(\"GPT output fin.\\n\")\n\n    return response_text", "\n\ndef deep_fmap(lambdaFunc, json_data):\n    print_op(json_data)\n    if isinstance(json_data, list):\n        print_op(\"##LIST\")\n        return list(map(lambda listItem: deep_fmap(lambdaFunc, listItem), json_data))\n    elif isinstance(json_data, tuple):\n        print_op(\"##TUPLE\")\n        return tuple(map(lambda tupleItem: deep_fmap(lambdaFunc, tupleItem), json_data))\n    elif isinstance(json_data, dict):\n        print_op(\"##DICT\")\n        return {lambdaFunc(k): deep_fmap(lambdaFunc, v) for k, v in json_data.items()}\n    else:\n        print_op(\"##SIMPLE\")\n        return lambdaFunc(json_data)", "\n\ndef replace_variables_for_values(\n    my_dict: dict, dynamic_keys, ignore_key: str = \"_______\"\n):\n    replaced_dict = {}\n    for key, value in my_dict.items():\n        if key == ignore_key:\n            continue\n        formatted_key = key.format(**dynamic_keys)\n        if isinstance(value, dict):\n            formatted_value = replace_variables_for_values(value, dynamic_keys)\n        elif isinstance(value, list):\n            formatted_value = []\n            for item in value:\n                formatted_value += replace_variables_for_values(item, dynamic_keys)\n        else:\n            try:\n                formatted_value = value.format(**dynamic_keys)\n            except:\n                formatted_value = value\n        replaced_dict[formatted_key] = formatted_value\n    return replaced_dict", "\n\ndef tool_api_call(self, tool, gpt_suggested_input, question, memory, facts, query=\"\"):\n    if query == \"\":\n        query = question\n\n    if gpt_suggested_input[0] != \"{\":\n        gpt_suggested_input = \"{\" + gpt_suggested_input\n    if gpt_suggested_input[-1] != \"}\":\n        gpt_suggested_input += \"}\"\n\n    if self.verbose > 1:\n        print_op(\"GPT SUGGESTED INPUT:\", gpt_suggested_input)\n\n    parsed_gpt_suggested_input = json.loads(gpt_suggested_input)\n    # make sure all of the suggested fields exist in the tool desc\n    for i in parsed_gpt_suggested_input.keys():\n        if i not in tool[\"dynamic_params\"].keys():\n            raise Exception(\"Bad Generated Input\")\n    tool_args = replace_variables_for_values(tool[\"args\"], parsed_gpt_suggested_input)\n\n    url = tool_args[\"url\"]\n    if self.verbose > -1:\n        print_op(tool[\"method\"] + \":\", url)\n\n    if \"auth\" in tool_args and isinstance(tool_args[\"auth\"], dict):\n        auths = list(tool_args[\"auth\"].items())\n        if len(auths) > 0:\n            tool_args[\"auth\"] = list(tool_args[\"auth\"].items())[0]\n        else:\n            del tool_args[\"auth\"]\n\n    # Remove those parameters that are not part of Python's `requests` function.\n    tool_args.pop(\"jsonParams\", None)\n    tool_args.pop(\"urlParams\", None)\n\n    if self.verbose > -1:\n        print_op(\"ARGS: \", tool_args)\n\n    resp = (requests.get if tool[\"method\"] == \"GET\" else requests.post)(**tool_args)\n\n    # print_op(\"FINAL URL: (\" + tool[\"method\"] + \") \", resp.url)\n\n    actual_call = str(tool_args)\n\n    if resp.status_code in [404, 401, 500]:\n        er = \" => \" + str(resp.status_code)\n        return \"This tool isn't working currently\" + er\n\n    ret = str(resp.text)\n    if self.verbose > 4:\n        print(ret)\n    try:\n        ret = str(json.loads(ret))\n    except:\n        pass\n\n    if len(ret) > 10000:\n        ret = ret[0:10000]\n    mem = \"\".join(\n        [\n            self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n            for p, a in memory\n        ]\n    ) + \"\".join(\n        [self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\") for p, a in facts]\n    )\n\n    prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n    prompt += MSG(\n        \"user\",\n        mem\n        + \"\\nQ:\"\n        + query\n        + \"\\n An api call about Q returned:\\n\"\n        + ret\n        + \"\\nUsing this information, what is the answer to Q?\",\n    )\n    a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n    return a", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent.py", "chunked_list": ["import os\nimport sys\nimport re\nimport random\nfrom llm_vm.agents.FLAT.agent_helper.business_logic import promptf\nfrom llm_vm.utils.labels import *\nfrom llm_vm.agents.FLAT.agent_helper.utils import *\nfrom llm_vm.agents.FLAT.agent_helper.tools import *\nfrom llm_vm.utils.keys import *\n", "from llm_vm.utils.keys import *\n\n\nrandom_fixed_seed = random.Random(4)\n\nclass Agent:\n    def __init__(self, openai_key, tools = None, bot_instructions = \"\", verbose = 4):\n        self.verbose = verbose\n        self.set_tools((GENERIC_TOOLS + tools) if tools else GENERIC_TOOLS)\n        self.bot_instructions = f\"<{L_BOT_INSTRUCTIONS}>{bot_instructions}<{L_BOT_INSTRUCTIONS}>\" if bot_instructions else \"\"\n\n        # set the openai key to make calls to the API\n        set_api_key(openai_key)\n    def set_tools(self, tools):\n        self.tools = []\n        for tool in tools:\n            if not 'args' in tool:\n                tool['args'] = {}\n            if not 'method' in tool:\n                tool['method'] = \"GET\"\n            if not 'examples' in tool:\n                tool['examples'] = []\n            if not 'dynamic_params' in tool:\n                tool['dynamic_params'] = {}\n            if not 'id' in tool:\n                tool['id'] = len(self.tools) + 1\n\n            self.tools += [tool]\n\n\n    def run(self, question: str, memory: TupleList) -> Tuple[str, TupleList, list, DebugCallList]:\n        try:\n            answer, _, calls, debug_return, price = promptf(\n                question,\n                memory,\n                self.tools,\n                self.bot_instructions,\n                self.verbose\n            )\n\n            # Remove xml tags from answer, if any. Important, because this is trade secret.\n            answer, has_friendly_tags = remove_tags_from_html_string(answer)\n\n        except Exception as e:\n            print(e, flush=True)\n            print(\"Main thread exception: \", e, flush=True)\n            answer, calls, debug_return, price, has_friendly_tags = \"Error: \" + str(e), [], [], 0, False\n\n\n        if self.verbose > -1:\n            print_big(\"GPT-3.5 Price = ~{:.1f} cents\".format(price * 100))\n\n        return (answer, memory + [(question, answer)], calls, debug_return, has_friendly_tags)", "\ndef flat_main():\n    # tools = [{'method': 'GET',\"description\":\"use this tool to find the price of stocks\",'args' : {\"url\":\"https://finnhub.io/api/v1/quote\",'params': { 'token' :'cfi1v29r01qq9nt1nu4gcfi1v29r01qq9nt1nu50'} },\"dynamic_params\":{\"symbol\":\"the symbol of the stock\"}}]\n    tools =  [{'method': 'GET', \"dynamic_params\": { 'location': 'This string indicates the geographic area to be used when searching for businesses. \\\n    Examples: \"New York City\", \"NYC\", \"350 5th Ave, New York, NY 10118\".', 'term': 'Search term, e.g. \"food\" or \"restaurants\". The \\\n    term may also be the business\\'s name, such as \"Starbucks\"', 'price': 'Pricing levels to filter the search result with: 1 = \\\n    $, 2 = $$, 3 = $$$, 4 = $$$$. The price filter can be a list of comma delimited pricing levels. e.g., \"1, 2, 3\" will filter the \\\n    results to show the ones that are $, $$, or $$$.'}, \"description\":\"This tool searches for a business on yelp.  It's useful for finding restaurants and \\\n    whatnot.\", 'args' :{'url': 'https://api.yelp.com/v3/businesses/search', 'cert': '', 'json': {}, 'params': {'limit': '1',\n                                                                                                              'open_now': 'true', 'location': '{location}', 'term': '{term}', 'price': '{price}'}, 'data': {},\n                       'headers': {'authorization': 'Bearer OaEqVSw9OV6llVnvh9IJo92ZCnseQ9tftnUUVwjYXTNzPxDjxRafYkz99oJKI9WHEwUYkiwULXjoBcLJm7JhHj479Xqv6C0lKVXS7N91ni-nRWpGomaPkZ6Z1T0GZHYx',\n                                   'accept': 'application/json'}}}]\n\n    label = Agent(os.getenv(\"LLM_VM_OPENAI_API_KEY\"), tools, verbose=4)\n    conversation_history = []\n    last = \"\"\n    while True:\n        inp = input(last+\"Human: \")\n        return_value = label.run(inp, conversation_history)\n        conversation_history = return_value[1]\n        print(return_value[2])\n        last = \"AI: \"+str(return_value[0]) + \"\\n\"", "\n# print_op(google(' {\"question\": \"\"}'))\nif __name__ == \"__main__\":\n    flat_main()\n"]}
{"filename": "src/llm_vm/agents/FLAT/typings.py", "chunked_list": ["from typing import Tuple, List, Dict, TypedDict, Union, Any, Optional\nfrom enum import Enum\n\nclass OpenAIModel(Enum):\n    CURIE = \"curie\"\n    CURIE_TEXT = \"text-curie-001\"\n    FAST_DAVINCI = \"code-cushman-002\"\n    DAVINCI = \"davinci\"\n    DAVINCI_TEXT = \"text-davinci-003\"\n\nclass LLMCallType(Enum):\n    OPENAI_COMPLETION = \"openai-completion\"\n    OPENAI_CHAT = \"openai-chat\"", "\nclass LLMCallType(Enum):\n    OPENAI_COMPLETION = \"openai-completion\"\n    OPENAI_CHAT = \"openai-chat\"\n    # ... other models\n\nclass DecisionStep(Enum):\n    SPLIT = \"split_questions\"\n    INPUT = \"guess_api_input\"\n    FROM_MEMORY = \"use_memory\"\n    TOOL_PICKER = \"tool_picker\"", "\nclass DefaultTools(Enum):\n    I_DONT_KNOW = -1\n    ANSWER_FROM_MEMORY = 0\n\n    WOLFRAM = 1\n    DIRECTIONS = 2\n    WEATHER = 3\n\n    ## Always set this to the latest ID. This is an integer, not an enum!\n    __LAST_ID__: int = 4", "\nclass DefaultTrainingTools(Enum):\n    TRAIN_SEND_EMAIL = 4\n    TRAIN_CREATE_DOCUMENT = 5\n    TRAIN_SHARE_DOCUMENT = 6\n    TRAIN_BOOK_TABLE = 7\n    TRAIN_FIND_RESTAURANT = 8\n    TRAIN_NEWS = 9\n    TRAIN_ORDER_FOOD = 10\n    TRAIN_BOOK_APPOINTMENT = 11\n    TRAIN_AVAILABLE_APPOINTMENT = 12\n    TRAIN_FIND_FLIGHT = 13", "\nTupleList = List[Tuple[str, str]]\n\nclass ToolTypeArgs(TypedDict):\n    url: str\n    params: Optional[dict]\n    json: Optional[dict]\n    jsonParams: Optional[dict]\n    auth: Optional[dict]\n    cert: Optional[str]\n    data: Optional[dict]", "\nclass SingleTool(TypedDict):\n    description: str\n    dynamic_params: dict\n    id: int\n    args: ToolTypeArgs\n    ai_response_prompt: Union[str, None]\n\nToolList = List[SingleTool]\n# ToolList = NewType(\"ToolList\", List[SingleTool])", "ToolList = List[SingleTool]\n# ToolList = NewType(\"ToolList\", List[SingleTool])\nEnumeratedToolList = List[Tuple[int,SingleTool]]\n\nALL_TOOLS_MAP: Dict[str, Union[DefaultTools, DefaultTrainingTools]] = {\n    \"dont_know\": DefaultTools.I_DONT_KNOW,\n    \"memory\": DefaultTools.ANSWER_FROM_MEMORY,\n    \"weather\": DefaultTools.WEATHER,\n    \"wolfram\": DefaultTools.WOLFRAM,\n    \"directions\": DefaultTools.DIRECTIONS,", "    \"wolfram\": DefaultTools.WOLFRAM,\n    \"directions\": DefaultTools.DIRECTIONS,\n    \"send_email\": DefaultTrainingTools.TRAIN_SEND_EMAIL,\n    \"create_doc\": DefaultTrainingTools.TRAIN_CREATE_DOCUMENT,\n    \"share_doc\": DefaultTrainingTools.TRAIN_SHARE_DOCUMENT,\n    \"book_table\": DefaultTrainingTools.TRAIN_BOOK_TABLE,\n    \"find_restaurant\": DefaultTrainingTools.TRAIN_FIND_RESTAURANT,\n    \"news\": DefaultTrainingTools.TRAIN_NEWS,\n    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,", "    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,\n    \"check_appointment\": DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT,\n    \"find_flight\": DefaultTrainingTools.TRAIN_FIND_FLIGHT\n}\n\nclass QuestionSplitModelJSONData(TypedDict):\n    mem: List[List[str]]\n    question: str\n    answer: List[str]", "\nclass QuestionSplitModelData(TypedDict):\n    mem: Union[TupleList, None]\n    question: str\n    answer: Union[List[str], None]\n    tools: Union[ToolList, None]\n\nclass AnswerFromMemoryModelData(TypedDict):\n    mem: Union[TupleList, None]\n    question: str\n    thought: Union[str, None]\n    answer: Union[str, None]", "\nclass QuestionSplitInputModel(TypedDict):\n    model_name: str\n    openai_model: OpenAIModel\n    data: List[QuestionSplitModelData]\n\nclass PromptModelEntry(TypedDict):\n    prompt: str\n    completion: str\n\nclass LLMCallParams(TypedDict):\n    llm: LLMCallType\n    model: Tuple[str, Union[str, None]]\n    prompt: str\n    temperature: float\n    max_tokens: int\n    stop: str", "\nclass LLMCallParams(TypedDict):\n    llm: LLMCallType\n    model: Tuple[str, Union[str, None]]\n    prompt: str\n    temperature: float\n    max_tokens: int\n    stop: str\n\nLLMCallReturnType = Tuple[str, float]", "\nLLMCallReturnType = Tuple[str, float]\n\nclass LLMTrainingResult(TypedDict):\n    elapsed_time_s: int\n    model_name: str\n    model_files: List[str]\n\nclass LLMTrainingResultMeta(TypedDict):\n    OPENAI_KEY: str\n    DATE: str\n    TIMESTAMP: int", "class LLMTrainingResultMeta(TypedDict):\n    OPENAI_KEY: str\n    DATE: str\n    TIMESTAMP: int\n\nLLModels = Union[Dict[str, LLMTrainingResult], LLMTrainingResultMeta]\n\n\nclass ToolInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    description: str\n    question: str\n    answer: dict\n    params: dict", "class ToolInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    description: str\n    question: str\n    answer: dict\n    params: dict\nclass ToolInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    description: str\n    question: str\n    answer: dict\n    params: dict", "\nclass ToolInputModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[ToolInputModelData]\n\nclass ToolpickerInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    thought: Optional[str]\n    question: str\n    answer: str\nclass ToolpickerInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    thought: Optional[str]\n    question: str\n    answer: int\n    tools: Optional[ToolList]", "class ToolpickerInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    thought: Optional[str]\n    question: str\n    answer: int\n    tools: Optional[ToolList]\n\nclass ToolpickerInputModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[ToolpickerInputModelData]", "\nclass AnswerInMemoryModelData(TypedDict):\n    mem: Optional[TupleList]\n    facts: Optional[TupleList]\n    question: str\n    answer: Optional[bool]\n\nclass AnswerInMemoryModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[AnswerInMemoryModelData]", "\nclass DebugCalls(TypedDict):\n    response: str\n    gpt_suggested_params: Any\n    url: str\n    raw_api_return: str\n\nDebugCallList = List[DebugCalls]\n", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/bothandler.py", "chunked_list": ["import re\nimport openai\nimport random\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\n\nfrom llm_vm.agents.FLAT.agent_helper.tools import *\nfrom llm_vm.agents.FLAT.agent_helper.tool_utils import *\nfrom llm_vm.agents.FLAT.agent_helper.utils import *\n", "from llm_vm.agents.FLAT.agent_helper.utils import *\n\nfrom llm_vm.agents.FLAT.models.get_decision_model import get_newest_decision_model\nfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.tool_picker_model_data import tool_input_data\nfrom llm_vm.agents.FLAT.models.utils.question_split_model.question_split_model_data import question_splitter_data\n\n# openai.api_key = OPENAI_DEFAULT_KEY\n\n\ndef question_split(question_to_split, use_fine_tuned_model = True):\n\n    question_data = question_splitter_data[\"data\"].copy()\n    random.shuffle(question_data)\n    NUMBER_OF_EXAMPLES = 6\n    question_data = question_data[:NUMBER_OF_EXAMPLES]\n    use_fine_tuned_model = False\n    prompt, prompt_stop = splitter_prompt(\n        question_data +\n        [question_to_split]\n    )\n\n    if use_fine_tuned_model:\n            custom_model, custom_model_key = get_newest_decision_model(DecisionStep.SPLIT)\n    else:\n        custom_model, custom_model_key = OpenAIModel.DAVINCI_TEXT.value, None\n\n    sub_questions_str, price = call_llm({\n        \"llm\": LLMCallType.OPENAI_COMPLETION,\n        \"model\": (custom_model, custom_model_key),\n        \"max_tokens\": 256,\n        \"stop\": prompt_stop,\n        \"prompt\": prompt,\n        \"temperature\": 0.2,\n    })\n\n    sub_questions, main_question = tidy_up_subquestions(\n        sub_questions_str.replace(prompt_stop, \"\"),\n        question_to_split[\"question\"]\n    )\n\n    return sub_questions, main_question, price", "\ndef question_split(question_to_split, use_fine_tuned_model = True):\n\n    question_data = question_splitter_data[\"data\"].copy()\n    random.shuffle(question_data)\n    NUMBER_OF_EXAMPLES = 6\n    question_data = question_data[:NUMBER_OF_EXAMPLES]\n    use_fine_tuned_model = False\n    prompt, prompt_stop = splitter_prompt(\n        question_data +\n        [question_to_split]\n    )\n\n    if use_fine_tuned_model:\n            custom_model, custom_model_key = get_newest_decision_model(DecisionStep.SPLIT)\n    else:\n        custom_model, custom_model_key = OpenAIModel.DAVINCI_TEXT.value, None\n\n    sub_questions_str, price = call_llm({\n        \"llm\": LLMCallType.OPENAI_COMPLETION,\n        \"model\": (custom_model, custom_model_key),\n        \"max_tokens\": 256,\n        \"stop\": prompt_stop,\n        \"prompt\": prompt,\n        \"temperature\": 0.2,\n    })\n\n    sub_questions, main_question = tidy_up_subquestions(\n        sub_questions_str.replace(prompt_stop, \"\"),\n        question_to_split[\"question\"]\n    )\n\n    return sub_questions, main_question, price", "\n\ndef pick_tool(tools_list, question, conversation_history, use_fine_tuned_model = True, debug_prompt = False):\n    tool_data = tool_input_data[\"data\"].copy()\n    random.shuffle(tool_data)\n    NUMBER_OF_EXAMPLES = 6\n    tool_data = tool_data[:NUMBER_OF_EXAMPLES]\n    use_fine_tuned_model = False\n\n\n    prompt, stop = toolpicker_prompt(\n        tool_data + [{\"question\": question, \"mem\": conversation_history}],\n        tools_list,\n    )\n\n    if debug_prompt:\n        print_big(prompt, \"toolpicker prompt\")\n\n    if use_fine_tuned_model:\n        custom_model, custom_model_key = get_newest_decision_model(\n            DecisionStep.TOOL_PICKER)\n    else:\n        custom_model, custom_model_key = OpenAIModel.DAVINCI_TEXT.value, None\n\n    suggested_tool_str, price = call_llm({\n        \"llm\": LLMCallType.OPENAI_COMPLETION,\n        \"model\": (custom_model, custom_model_key),\n        \"max_tokens\": 10,\n        \"stop\": stop,\n        \"prompt\": prompt,\n        \"temperature\": 0,\n    })\n\n    number_match_regex = \"-?[0-9]+\"\n\n    try:\n        valid_tool_ids = [t[\"id\"] for t in tools_list] + \\\n            [DefaultTools.ANSWER_FROM_MEMORY.value, DefaultTools.I_DONT_KNOW.value]\n        parsed_suggested_tool_str = re.findall(\n            number_match_regex, suggested_tool_str)\n        if len(parsed_suggested_tool_str) == 0:\n            raise Exception(\"No matching ID found in response\")\n\n        suggested_tool_id = int(parsed_suggested_tool_str[0])\n        if suggested_tool_id not in valid_tool_ids:\n            raise Exception(\n                f\"Invalid tool id: {suggested_tool_id} // {','.join([str(v) for v in valid_tool_ids])}\")\n\n        tools_list.sort(key=lambda t: t[\"id\"])\n        print_big(\n            \"\".join([f'{\"\u2713\" if tool[\"id\"] == suggested_tool_id else \" \"} {tool[\"id\"]}.- {tool[\"description\"]}\\n' for tool in tools_list]) +\n            f\"\\n> Question: {question}\\n> Raw answer: '{suggested_tool_str}'\\n> Tool ID: {suggested_tool_id}\",\n            \"LIST OF DATA TOOLS\"\n        )\n\n        return suggested_tool_id, price\n    except Exception as e:\n        print_big(f'Exception parsing tool_id: \"{suggested_tool_str}\"')\n        print(e, flush=True)\n        return DefaultTools.I_DONT_KNOW.value, price", "\n\ndef check_can_answer_from_memory(question: str, memory: TupleList = [], facts: TupleList = []) -> Tuple[bool, float]:\n\n    memory_prompt, memory_stop = create_memory_prompt(\n        # answer_from_memory_data[\"data\"] +\n        [{\"question\": question, \"mem\": memory, \"facts\": facts}]\n    )\n\n    ans, price = call_llm({\n        \"llm\": LLMCallType.OPENAI_COMPLETION,\n        \"model\": get_newest_decision_model(DecisionStep.FROM_MEMORY),\n        \"max_tokens\": 10,\n        \"stop\": memory_stop,\n        \"prompt\": memory_prompt,\n        \"temperature\": 0.1,\n    })\n\n    if \"yes\" in ans.lower():\n        return True, price\n    else:\n        return False, price", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/replacer.py", "chunked_list": ["import re\nfrom llm_vm.utils.keys import DICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS\n\n###\n# Use this regex to find if a variable is a pure interpolation.\n# For @example, { \"name\": \"{my_name}\" } is pure, since there's nothing else besides the interpolation\n# For @example, { \"Ticker\": \"TickerID={ticker_id}|historical=1\" } is NOT pure, since there is extra text\ndef __is_pure_interpolation (key: str) -> bool:\n    return len(re.findall(DICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS, key)) == 1\n", "\n###\n# Use this to get the key of a pure interpolation ===> '{a}' => 'a', '{abc_def-345}' => 'abc_def-345'\ndef __get_pure_key (key: str) -> str:\n    return key[1:-1]\n\n\ndef replace_variables_for_values(my_dict: dict, dynamic_keys: dict, ignore_key: str = \"_______\") -> any:\n    def format_simple_value(value: str) -> any:\n        try:\n            if __is_pure_interpolation(value):\n                formatted_value = dynamic_keys[__get_pure_key(value)]\n            else:\n                formatted_value = value.format(**dynamic_keys)\n        except Exception as e:\n            formatted_value = value\n\n        return formatted_value\n\n    replaced_dict = {}\n    for key, value in my_dict.items():\n        if (key == ignore_key):\n            continue\n        formatted_key = key.format(**dynamic_keys)\n        if (isinstance(value, dict)):\n            if (not isinstance(value, list) and not isinstance(value, dict)):\n                formatted_value = format_simple_value(value)\n            else:\n                formatted_value = replace_variables_for_values(value, dynamic_keys, ignore_key)\n        elif (isinstance(value, list)):\n            formatted_value = []\n            for item in value:\n                if (not isinstance(item, list) and not isinstance(item, dict)):\n                    formatted_value.append(format_simple_value(item))\n                else:\n                    formatted_value.append(replace_variables_for_values(item, dynamic_keys, ignore_key))\n        else:\n            formatted_value = format_simple_value(value)\n        replaced_dict[formatted_key] = formatted_value\n    return replaced_dict", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/utils.py", "chunked_list": ["import traceback\nimport os\nimport sys\nimport re\nimport openai\n\n# Get the current file's directory to grab the python files with common functionality in the utils/ folder\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\ngrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\nutils_dir = os.path.join(grandparent_dir, 'utils/')", "grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\nutils_dir = os.path.join(grandparent_dir, 'utils/')\nsys.path.append(utils_dir)\n\nfrom llm_vm.utils.labels import *\nimport random\nfrom Levenshtein import distance as lev\nfrom llm_vm.utils.typings_llm import *\nfrom bs4 import BeautifulSoup\n", "from bs4 import BeautifulSoup\n\n\nrandom_fixed_seed = random.Random(4)\n\ndef print_op(*kargs, **kwargs):\n    print(*kargs, **kwargs, flush=True)\n\ndef verbose_answer(data, answer):\n    return f'''<{L_ANSWER_DATA}>{str(data)}</{L_ANSWER_DATA}><{L_ANSWER_SUMMARY}>{answer}</{L_ANSWER_SUMMARY}>'''", "def verbose_answer(data, answer):\n    return f'''<{L_ANSWER_DATA}>{str(data)}</{L_ANSWER_DATA}><{L_ANSWER_SUMMARY}>{answer}</{L_ANSWER_SUMMARY}>'''\n\ndef remove_tags_from_html_string(html_string):\n    # if not isinstance(html_string, str):\n    #     return \"\"\n    # return re.compile(r'<[^>]+>').sub(\"\", html_string), False\n\n\n    soup = BeautifulSoup(html_string, 'html.parser')\n    has_friendly_tags: bool = False\n\n    # Find all tags that are not in the list of tags to preserve\n    for tag in soup.find_all(True):\n        if tag.name in ['ul', 'ol', 'li', 'a', 'pre', 'code', 'kbd', 'i']:\n            has_friendly_tags = True\n        elif tag.name in ['style', 'script']:\n            tag.extract() # remove tag and content\n        else:\n            tag.unwrap() # remove tag, but preserve content\n\n    # Get the stripped HTML string\n    return str(soup).strip(\"\\n \"), has_friendly_tags", "\ndef print_big(data, label = \"\"):\n    def do_format(x) -> str:\n        formatted_title = \"======#====== {:20} ======#======\\n\"\n        if len(x) >= 20:\n            return formatted_title.format(x)\n        else:\n            return formatted_title.format((int((20 - len(x)) / 2) * \" \") + x)\n    try:\n        if len(label):\n            print(do_format(str(label).upper()), data, flush=True)\n        else:\n            print(do_format(str(data)), flush=True)\n\n    except:\n        print(label, flush=True)", "\n\n# Tolerance: If string A can be converted into B in more than {tolerance} steps, they are considered different.\ndef remove_similars(similars_list, tolerance = 3):\n    unique_list = []\n    sanitized_list = list(map(lambda item: item.strip(\" \").lower(), similars_list))\n    similars_len = len(similars_list)\n\n    for i in range(similars_len):\n        has_duplicate: bool = False\n        for j in range(i+1, similars_len):\n            if lev(sanitized_list[i], sanitized_list[j], score_cutoff=tolerance) < tolerance:\n                has_duplicate = True\n                break\n        if has_duplicate == False:\n            unique_list.append(similars_list[i])\n    return unique_list", "\ndef make_interaction_request(human_question, ai_response, data, Q = L_QUESTION, A = L_ANSWER, D = L_ANSWER_DATA, INTERACTION = L_INTERACTION):\n\n    interaction_text = f\"<{INTERACTION}>\\n<{Q}>{human_question}</{Q}>\\n\"\n\n    if data and len(data) and isinstance(data, str):\n        interaction_text += f'''<{D}>{data}</{D}>'''\n\n    interaction_text += f'''<{A}>{ai_response if ai_response else ''}'''\n\n    stop = f\"</{A}>\\n</{INTERACTION}>\"\n\n    return interaction_text, stop", "\ndef make_interaction(human_question, ai_response, data = '',\n                     Q = L_QUESTION,\n                     A = L_ANSWER,\n                     D = L_ANSWER_DATA,\n                     INTERACTION = L_INTERACTION):\n    text, stop = make_interaction_request(human_question, ai_response, data, Q, A, D, INTERACTION)\n    return text + stop\n\ndef get_tool_by_id(tools, tool_id):\n    for tool in tools:\n        if tool['id'] == tool_id:\n            return tool\n    raise Exception(f\"No tool corresponds to id='{tool_id}'\")", "\ndef get_tool_by_id(tools, tool_id):\n    for tool in tools:\n        if tool['id'] == tool_id:\n            return tool\n    raise Exception(f\"No tool corresponds to id='{tool_id}'\")\n\ndef tidy_up_subquestions(subquestions_str, main_question):\n    sub_questions_raw = subquestions_str.replace(\"\\n\", \"\").split(SUBQ_SPLITTER)\n\n    # First, clear the subquestions of extra symbols that might be confusing\n    sub_questions_raw = list(filter(lambda q: len(q) > 1, [q.strip() for q in sub_questions_raw]))\n    # then, remove similar subquestions\n    unique_subquestions = remove_similars(sub_questions_raw)\n\n    # if we only have one sub question, we don't need to \"summarise\" at the end => main question == subquestion.\n    if len(unique_subquestions) == 1:\n        return [main_question], None\n    else:\n        # to make sure, remove similar subquestions again because we add the main question\n        # sub_questions = remove_similars(sub_questions + [question])\n        return unique_subquestions, main_question", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/business_logic.py", "chunked_list": ["import json\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.utils import *\nfrom llm_vm.agents.FLAT.agent_helper.bothandler import question_split, pick_tool, check_can_answer_from_memory, generate_convo_history, prompt_for_answer, get_newest_decision_model\nfrom llm_vm.agents.FLAT.agent_helper.use_tool import use_tool\nfrom llm_vm.agents.FLAT.agent_helper.tool_utils import make_tool_input_case\n\n# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))", "# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n# utils_dir = os.path.join(grandparent_dir, 'utils/')\n# sys.path.append(utils_dir)\n\nfrom labels import *\nfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\n\n__MAX_RETRIES_GUESS_INPUT = 3\n", "__MAX_RETRIES_GUESS_INPUT = 3\n\n\ndef __get_tool_input(\n    tool: SingleTool,\n    mem: TupleList,\n    question: str,\n    verbose: int,\n    max_tokens: int = 20,\n    temperature: float = 0.0\n) -> Tuple[dict, float]:\n\n    gpt_prompt, gpt_prompt_stop = make_tool_input_case(\n        mem,\n        question,\n        None,\n        tool_descr=tool[\"description\"],\n        tool_params=tool[\"dynamic_params\"],\n        wrapper_tag=L_INTERACTION\n    )\n\n    if verbose > 3:\n        print_big(gpt_prompt, f\"GPT PROMPT [tool={tool['id']}]\")\n\n    gpt_suggested_input, price = call_llm({\n        \"llm\": LLMCallType.OPENAI_COMPLETION,\n        \"model\": get_newest_decision_model(DecisionStep.INPUT),\n        \"max_tokens\": max_tokens,\n        \"prompt\": gpt_prompt,\n        \"stop\": gpt_prompt_stop,\n        \"temperature\": temperature,\n    })\n\n    try:\n        if gpt_suggested_input[0] != \"{\":\n            gpt_suggested_input = \"{\" + gpt_suggested_input\n        if gpt_suggested_input[-1] != \"}\":\n            gpt_suggested_input += \"}\"\n\n        parsed_gpt_suggested_input: dict = json.loads(gpt_suggested_input)\n        if verbose > 1:\n            _has_keys = len(parsed_gpt_suggested_input.keys()) > 0\n            print_big(\n                parsed_gpt_suggested_input if _has_keys else \"(( no input ))\", \"GPT SUGGESTED INPUT\")\n        return parsed_gpt_suggested_input, price\n    except:\n        print_big(gpt_suggested_input, \"INVALID GPT INPUT\")\n        # {\"$ PARSING FAILED $\": str(e), \"$ REASON $\": \"malformed JSON\"}\n        return {}, price", "\n\ndef promptf(\n    question: str,\n    factual_memory: TupleList,\n    tools: ToolList,\n    bot_instructions: str,\n    verbose: int,\n    split_allowed: bool = True,\n    force_answer_from_memory: bool = False\n) -> Tuple[str, TupleList, List[str], DebugCallList, float]:\n    price_accumulator = 0\n\n    if split_allowed:\n        conversation_memory: TupleList = []\n        sub_questions, main_question, price = question_split({\n            \"mem\": factual_memory,\n            \"question\": question,\n            # \"tools\": tools\n        })\n        price_accumulator += price\n\n        if verbose > 0:\n            if len(sub_questions) == 1:\n                print_big(f\"NO SPLIT REQUIRED\")\n            else:\n                print_big(f'''Question:\\n- {main_question}\\n\\nSubquestions:\\n''' + (\n                    \"- \" + \"\\n- \".join(sub_questions)), f\"SPLIT\")\n\n        calls: List[str] = []\n        debug_return: DebugCallList = []\n\n        for i in range(len(sub_questions)):\n            answer, new_facts, new_calls, new_debug_return, price = promptf(\n                sub_questions[i],\n                factual_memory,\n                tools,\n                bot_instructions,\n                verbose,\n                split_allowed=False\n            )\n            calls = calls + new_calls\n            factual_memory = factual_memory + new_facts\n            debug_return = debug_return + new_debug_return\n            price_accumulator += price\n            conversation_memory.append((sub_questions[i], answer))\n\n        if main_question:\n            print_big(main_question, \"TRYING TO ANSWER MAIN QUESTION\")\n            final_answer, _, _, _, price = promptf(\n                main_question,\n                # last round, we give the non-verbose convo memory.\n                conversation_memory,\n                [],\n                bot_instructions,\n                verbose,\n                split_allowed=False,\n                force_answer_from_memory=True\n            )\n            price_accumulator += price\n            conversation_memory.append((main_question, final_answer))\n\n            answer = final_answer\n            print_big(answer, \"FINAL ANSWER\")\n            print_big(\"=========#####=========\")\n\n        return answer, [], calls, debug_return, price_accumulator\n\n    if len(tools):\n        best_tool_id, price_picker = pick_tool(tools, question, factual_memory)\n        price_accumulator += price_picker\n        cannot_use_tools_to_answer = best_tool_id in [\n            DefaultTools.ANSWER_FROM_MEMORY.value, DefaultTools.I_DONT_KNOW.value]\n    else:\n        best_tool_id = DefaultTools.ANSWER_FROM_MEMORY.value if force_answer_from_memory else DefaultTools.I_DONT_KNOW.value\n        cannot_use_tools_to_answer = True\n\n    # Only in the cases where we DON'T have a tool (answer_from_tool == False) we check if we can answer from memory.\n    can_answer_from_memory = force_answer_from_memory\n    if cannot_use_tools_to_answer and not force_answer_from_memory:\n        can_answer_from_memory, price_check = check_can_answer_from_memory(\n            question, memory=factual_memory)\n        price_accumulator += price_check\n\n    print_big(\n        f\"MEM ({can_answer_from_memory}) / TOOL ({not cannot_use_tools_to_answer})\")\n\n    if cannot_use_tools_to_answer and can_answer_from_memory:\n\n        cur_prompt = (\n            bot_instructions +\n            generate_convo_history(memory=factual_memory) +\n            f\"\\n\\n\" +\n            f\"\\n[SYSTEM]: Use <{L_ANSWER_DATA}/> to answer better, but follow the format of <{L_ANSWER_SUMMARY}/>\\n\" +\n            prompt_for_answer(question)\n        )\n\n        answer, price = call_llm({\n            \"llm\": LLMCallType.OPENAI_COMPLETION,\n            \"model\": OpenAIModel.DAVINCI_TEXT.value if \"quality\" == \"best\" else OpenAIModel.CURIE_TEXT.value,\n            \"max_tokens\": 200,\n            \"prompt\": cur_prompt,\n            \"stop\": None,\n            \"temperature\": 0.1,\n        })\n\n        price_accumulator += price\n        if verbose > 0:\n            print_op(\"Question:  \", question)\n            print_op(\"AI Answer: \", answer)\n\n        return answer, [(question, answer)], [], [], price_accumulator\n\n    conversation_history = generate_convo_history(facts=factual_memory)\n    # No information in the Conversation memory AND no tool --> Using AI General Knowledge Base.\n    if best_tool_id in [DefaultTools.I_DONT_KNOW.value, DefaultTools.ANSWER_FROM_MEMORY.value]:\n\n        if (best_tool_id == DefaultTools.I_DONT_KNOW.value):\n            current_prompt = f\"{bot_instructions}\\n\" if bot_instructions else \"\" \\\n                + (f\"<{L_CONVERSATION}>{conversation_history}</{L_CONVERSATION}>\\n\" if len(conversation_history) else \"\") \\\n                + f\"<{L_QUESTION}>{question}</{L_QUESTION}>\\n\" \\\n                + f\"<{L_THOUGHT}>Maybe not enough information to answer. If that's the case, say why.</{L_THOUGHT}>\\n\" \\\n                + f\"<{L_ANSWER}>\"\n        else:\n            current_prompt = f\"{bot_instructions}\\n\" if bot_instructions else \"\" \\\n                + (f\"<{L_CONVERSATION}>{conversation_history}</{L_CONVERSATION}>\\n\" if len(conversation_history) else \"\") \\\n                + f\"<{L_QUESTION}>{question}</{L_QUESTION}>\\n\" \\\n                + f\"<{L_ANSWER}>\"\n\n        a, price = call_llm({\n            \"llm\": LLMCallType.OPENAI_COMPLETION,\n            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n            \"max_tokens\": 500,\n            \"prompt\": current_prompt,\n            \"stop\": f\"</{L_ANSWER}>\",\n            \"temperature\": 0.1,\n        })\n        price_accumulator += price\n        return a, [(question, a)], [], [], price_accumulator\n\n    best_tool: SingleTool = get_tool_by_id(tools, best_tool_id)\n\n    if \"dynamic_params\" not in best_tool:\n        best_tool[\"dynamic_params\"] = {}\n\n    dynamic_params_to_fill = len(best_tool[\"dynamic_params\"].keys())\n    # only try to guess the \"dynamic_params\" if we have any\n    if dynamic_params_to_fill == 0:\n        tool_input = {}\n    else:\n        # Try more than once to get the right input.\n        for attempt_to_find_input in range(__MAX_RETRIES_GUESS_INPUT):\n            tool_input, price = __get_tool_input(\n                best_tool,\n                factual_memory,\n                question,\n                verbose,\n                max_tokens=200,\n                # higher temperature on subsequent attempts\n                temperature=min(0.15 * attempt_to_find_input, 0.5)\n            )\n            price_accumulator += price\n\n            filled_dynamic_params = len(tool_input.keys())\n            if filled_dynamic_params == dynamic_params_to_fill:\n                break\n\n        # If, besides all attempts, we still don't have the right input...\n        if filled_dynamic_params != dynamic_params_to_fill:\n            raise Exception(\n                f\"Wrong input! Not enough parameters (actual: {filled_dynamic_params}, expected: {dynamic_params_to_fill})\")\n\n    try:\n\n        response_instructions = question\n\n        if \"ai_response_prompt\" in best_tool.keys():\n            response_instructions = best_tool[\"ai_response_prompt\"]\n\n        answer, actual_url, raw_api_return, price = use_tool(\n            best_tool,\n            tool_input,\n            question,\n            factual_memory,\n            verbose,\n            bot_instructions,\n            response_instructions\n        )\n        price_accumulator += price\n    except Exception as e:\n        print_big(e, \"EXCEPTION CAUGHT! [#1]\")\n        cur_prompt_text = f\"\\n\"\\\n            f\"{bot_instructions}\\n\" \\\n            f\"{conversation_history}\\n\" \\\n            f\"--------\\n\" + \\\n            prompt_for_answer(question)\n\n        a, price = call_llm({\n            \"llm\": LLMCallType.OPENAI_COMPLETION,\n            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n            \"max_tokens\": 500,\n            \"prompt\": cur_prompt_text,\n        })\n        price_accumulator += price\n        return a, [(question, a)], [], [], price_accumulator\n\n    if verbose > 0:\n        print_big(\"RESULT OF ANSWER\")\n        print_op(\"Question:  \", question)\n        print_op(\"Tool Input:\", tool_input)\n        print_op(\"AI Answer: \", answer)\n\n    return answer, \\\n        [(question, verbose_answer(raw_api_return, answer))], \\\n        [actual_url], \\\n        [{\"response\": \"\", \"gpt_suggested_params\": tool_input, \"url\": actual_url, \"raw_api_return\": raw_api_return}], \\\n        price_accumulator", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/tools.py", "chunked_list": ["import os\nimport sys\n\n# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n# utils_dir = os.path.join(grandparent_dir, 'utils/')\n# sys.path.append(utils_dir)\n\nfrom keys import *", "\nfrom keys import *\nfrom labels import *\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.utils import verbose_answer\n\nCUSTOM_TOOL_ANSWER_EMBEDDING = \"/answer_embedding\"\n\n\ndef __get_generic_tools():\n    # # wolfram\n\n    WOLFRAM_KEY = os.getenv(\"LLM_VM_WOLFRAM_KEY\")\n    GOOGLE_MAPS_KEY = os.getenv(\"LLM_VM_GOOGLE_MAPS_KEY\")\n    wolfram_tool = {\n        'description': \"Useful to query questions about people, events, anything that can change, complicated math, live data retrieval, current date and other data.\",\n        # {'description': \"The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\",\n        'id': DefaultTools.WOLFRAM.value,\n        'dynamic_params': {\"input\": 'The natural language input query'},\n        'method': 'GET',\n        'args': {\n            'url': \"http://api.wolframalpha.com/v2/query\",\n            'params': {'appid': WOLFRAM_KEY, 'input': '{input}'}\n        },\n    }\n\n    # geopy\n    directions_tool = {\n        'description': \"Find the driving distance and time to travel between two cities.\",\n        'id': DefaultTools.DIRECTIONS.value,\n        'dynamic_params': {\"origins\": 'the origin city', \"destinations\": 'the destination city'},\n        'method': 'GET',\n        'args': {\n            'url': \"https://maps.googleapis.com/maps/api/distancematrix/json\",\n            'params': {\n                'key': GOOGLE_MAPS_KEY,\n                'origins': '{origins}',\n                'destinations': '{destinations}'\n            }\n        }\n    }\n\n    # weather\n    weather_tool = {\n        'description': 'Useful to get the weather.',\n        'ai_response_prompt': 'Returns temperature in celcius',\n        'id': DefaultTools.WEATHER.value,\n        'dynamic_params': {\n            \"latitude\": 'latitude of the location, a float',\n            \"longitude\": 'the longitude of the location, a float'\n        },\n        \"ai_response_prompt\": '''the value of \"current_weather.weathercode\" means:\n0: Clear sky\n1, 2, 3: Mainly clear, partly cloudy, and overcast\n45, 48: Fog and depositing rime fog\n51, 53, 55: Drizzle\n56, 57: Freezing Drizzle\n61, 63, 65: Rain\n66, 67: Freezing Rain\n71, 73, 75: Snow fall\n77: Snow grains\n80, 81, 82: Rain showers\n85, 86: Snow showers slight and heavy\n95: Thunderstorm\n96, 99: Thunderstorm with slight and heavy hail\n\nDo not return the weathercode as a number, instead use the description from list above''',\n        'method': 'GET',\n        'args': {\n            'url': \"https://api.open-meteo.com/v1/forecast\",\n            'params': {\n                'current_weather': 'true',\n                'latitude': '{latitude}',\n                'longitude': '{longitude}'\n            }\n        }\n    }\n\n    # return [wolfram_tool, directions_tool, weather_tool]\n    return [weather_tool]", "\ndef __get_generic_tools():\n    # # wolfram\n\n    WOLFRAM_KEY = os.getenv(\"LLM_VM_WOLFRAM_KEY\")\n    GOOGLE_MAPS_KEY = os.getenv(\"LLM_VM_GOOGLE_MAPS_KEY\")\n    wolfram_tool = {\n        'description': \"Useful to query questions about people, events, anything that can change, complicated math, live data retrieval, current date and other data.\",\n        # {'description': \"The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\",\n        'id': DefaultTools.WOLFRAM.value,\n        'dynamic_params': {\"input\": 'The natural language input query'},\n        'method': 'GET',\n        'args': {\n            'url': \"http://api.wolframalpha.com/v2/query\",\n            'params': {'appid': WOLFRAM_KEY, 'input': '{input}'}\n        },\n    }\n\n    # geopy\n    directions_tool = {\n        'description': \"Find the driving distance and time to travel between two cities.\",\n        'id': DefaultTools.DIRECTIONS.value,\n        'dynamic_params': {\"origins\": 'the origin city', \"destinations\": 'the destination city'},\n        'method': 'GET',\n        'args': {\n            'url': \"https://maps.googleapis.com/maps/api/distancematrix/json\",\n            'params': {\n                'key': GOOGLE_MAPS_KEY,\n                'origins': '{origins}',\n                'destinations': '{destinations}'\n            }\n        }\n    }\n\n    # weather\n    weather_tool = {\n        'description': 'Useful to get the weather.',\n        'ai_response_prompt': 'Returns temperature in celcius',\n        'id': DefaultTools.WEATHER.value,\n        'dynamic_params': {\n            \"latitude\": 'latitude of the location, a float',\n            \"longitude\": 'the longitude of the location, a float'\n        },\n        \"ai_response_prompt\": '''the value of \"current_weather.weathercode\" means:\n0: Clear sky\n1, 2, 3: Mainly clear, partly cloudy, and overcast\n45, 48: Fog and depositing rime fog\n51, 53, 55: Drizzle\n56, 57: Freezing Drizzle\n61, 63, 65: Rain\n66, 67: Freezing Rain\n71, 73, 75: Snow fall\n77: Snow grains\n80, 81, 82: Rain showers\n85, 86: Snow showers slight and heavy\n95: Thunderstorm\n96, 99: Thunderstorm with slight and heavy hail\n\nDo not return the weathercode as a number, instead use the description from list above''',\n        'method': 'GET',\n        'args': {\n            'url': \"https://api.open-meteo.com/v1/forecast\",\n            'params': {\n                'current_weather': 'true',\n                'latitude': '{latitude}',\n                'longitude': '{longitude}'\n            }\n        }\n    }\n\n    # return [wolfram_tool, directions_tool, weather_tool]\n    return [weather_tool]", "\n\nGENERIC_TOOLS = __get_generic_tools()\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/use_tool.py", "chunked_list": ["import os\nimport sys\n\nimport requests\nfrom llama_index import Document, GPTTreeIndex\nfrom llm_vm.agents.FLAT.agent_helper.utils import print_op, make_interaction, print_big\nfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\nfrom llm_vm.agents.FLAT.agent_helper.replacer import replace_variables_for_values\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.bothandler import prompt_for_answer, prompt_for_instructions", "from llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.bothandler import prompt_for_answer, prompt_for_instructions\n\n# # Get the current file's directory to grab the python files with common functionality in the utils/ folder\n# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n# utils_dir = os.path.join(grandparent_dir, 'utils/')\n# sys.path.append(utils_dir)\n\nfrom llm_vm.utils.labels import *", "\nfrom llm_vm.utils.labels import *\nfrom llm_vm.agents.FLAT.agent_helper.tools import CUSTOM_TOOL_ANSWER_EMBEDDING\nimport json\n\ndef __format_tool_url (url: str) -> str:\n    if CUSTOM_TOOL_ANSWER_EMBEDDING in url:\n        return \"CHAT.DEV Embeddings Service\"\n    return url\n", "\n\ndef use_tool(\n    tool: SingleTool,\n    gpt_suggested_input: dict,\n    question: str,\n    memory: TupleList,\n    verbose: int,\n    bot_instructions: str,\n    response_instructions: str = \"\"\n) -> Tuple[str, str, Any, float]:\n    price = 0\n\n    # make sure all of the suggested fields exist in the tool desc\n    for i in gpt_suggested_input.keys():\n        if i not in tool[\"dynamic_params\"].keys():\n            raise Exception(\"Bad Generated Input\")\n\n    try:\n        tool_args = replace_variables_for_values(\n            tool[\"args\"],\n            gpt_suggested_input\n        )\n    except Exception as e:\n        print_big(e, \"EXCEPTION\");\n        tool_args = {}\n\n    url = tool_args['url']\n\n    if 'auth' in tool_args and isinstance(tool_args['auth'], dict):\n        auths = list(tool_args['auth'].items())\n        if len(auths) > 0:\n            tool_args['auth'] = list(tool_args['auth'].items())[0]\n        else:\n            del tool_args['auth']\n\n    # Remove those parameters that are not part of Python's `requests` function.\n    tool_args.pop(\"jsonParams\", None)\n    tool_args.pop(\"urlParams\", None)\n\n    # print_big(tool_args, \"ARGS\")\n    request_function = None\n    method = tool['method'].upper()\n    if method == \"PUT\":\n        request_function = requests.put\n    elif method == \"POST\":\n        request_function = requests.post\n    elif method == \"PATCH\":\n        request_function = requests.patch\n    elif method == \"DELETE\":\n        request_function = requests.delete\n    elif method == \"GET\":\n        request_function = requests.get\n    else:\n        request_function = requests.get\n\n    resp = request_function(**tool_args)\n    print_big(resp.url, \"FINAL URL: (\" + tool[\"method\"] + \") \")\n\n    api_call_args = str(tool_args)\n    return_value = str(resp.text)\n\n    formatted_url = __format_tool_url(str(url))\n\n    if verbose > -1:\n        pass\n        #commented out, causes test cases to fail because the return json of some wolfram alpha searches contains character encodings that charmap does not know.\n        # leads to error being thrown.\n        # print_op(\"URL Response:\", ret)\n\n    try:\n        # seems pointless, but it formats the resulting json without spaces.\n        # If the response isn't json, it does nothing.\n        return_value_json = json.loads(return_value)\n        return_value = str(return_value_json)\n    except:\n        return_value_json = {\"response\": return_value}\n        pass\n\n    if resp.status_code in [400, 401, 404, 500]:\n        er = \" \u2192 \" + str(resp.status_code)\n        return \"This tool isn't working currently\" + er, formatted_url + er, return_value_json, price\n\n\n    summarised_ret: str = \"\"\n    if len(return_value) > 8000:\n        document = Document(return_value)\n        try:\n            if verbose > 2:\n                print_op(\"### Summarising with GPT Tree Index ...\")\n            summarised_ret = GPTTreeIndex([document]).query(\n                response_instructions, response_mode=\"tree_summarize\")\n        except:\n            return \"OpenAI is down!\", formatted_url, None, price\n\n        if verbose > 2:\n            print_op(\"SUMMARISED:\", summarised_ret)\n\n        if str(summarised_ret).find(\"ANSWER: \") == 0:\n            summarised_ret = str(return_value)[len(\"ANSWER: X.\"):].strip(\" .\")\n\n    else:\n\n        mem = \"\".join([make_interaction(p, a, INTERACTION = L_INT_HUMAN) for p,a in memory]) \\\n\n        # THIS WORKS VERY WELL, but using <XML> tags here does not work.\n        summarise_prompt, stop = f\"{bot_instructions}<{L_CONVERSATION}>{mem}</{L_CONVERSATION}>\\n\\\nDescription: a JSON object contains:\\n{return_value}\\nUsing this information, what is the [{L_ANSWER}] to [{L_QUESTION}]?\\n \\\n{prompt_for_instructions(response_instructions)}{prompt_for_answer(question)}\", None\n\n        if verbose > 3:\n            print_big(summarise_prompt, \"SUMMARY OF JSON DECODE\")\n\n        summarised_ret, price = call_llm({\n            \"llm\": LLMCallType.OPENAI_COMPLETION,\n            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n            \"max_tokens\": 256,\n            \"stop\": stop,\n            \"prompt\": summarise_prompt,\n            \"temperature\": 0.1,\n        })\n        print_big(summarised_ret, \"SUMMARISED ANSWER\")\n\n    return summarised_ret, formatted_url, return_value_json, price", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/tool_utils.py", "chunked_list": ["import json\nimport os\nimport sys\n\n# # Get the current file's directory to grab the python files with common functionality in the utils/ folder\n# current_dir = os.path.dirname(os.path.abspath(__file__))\n# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n# utils_dir = os.path.join(grandparent_dir, 'utils/')\n# sys.path.append(utils_dir)\n", "# sys.path.append(utils_dir)\n\nfrom llm_vm.utils.labels import *\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.utils import make_interaction\nfrom datetime import datetime\nfrom llm_vm.agents.FLAT.agent_helper.tools import GENERIC_TOOLS\nfrom random import sample, shuffle\n\n\ndef __create_tool_tag(tool: SingleTool) -> str:\n    return f'<{L_TOOL} id=\"{tool[\"id\"]}\" description=\"{tool[\"description\"]}\"/>'", "\n\ndef __create_tool_tag(tool: SingleTool) -> str:\n    return f'<{L_TOOL} id=\"{tool[\"id\"]}\" description=\"{tool[\"description\"]}\"/>'\n\n__DEFAULT_TOOLS = \"\\n\".join(\n    [__create_tool_tag(tool)\n     for tool in GENERIC_TOOLS]\n)\n\ndef __tools_we_have_access_to(tools: str = __DEFAULT_TOOLS) -> str:\n    return f\"\"\"<{L_TOOL_LIST}>{tools}</{L_TOOL_LIST}>\"\"\"", ")\n\ndef __tools_we_have_access_to(tools: str = __DEFAULT_TOOLS) -> str:\n    return f\"\"\"<{L_TOOL_LIST}>{tools}</{L_TOOL_LIST}>\"\"\"\n\n\ndef __split_into_subquestions_instructions(extra: str = \"\") -> str:\n    return f\"\"\"<{L_SPLIT_INSTRUCTIONS}>Look at the tools we have access to. If needed, split <{L_QUESTION}/> into subquestions, so each <{L_QUESTION}/> can be answered with one use of one tool. Make as few subquestions as possible, comma-separated, avoid duplicates, and return nothing else.{extra}</{L_SPLIT_INSTRUCTIONS}>\"\"\"\n\ndef make_tool_desc(tool: SingleTool):\n    params = \"{\" + \", \".join(['\"'+l + '\": '+v for l, v in tool['dynamic_params'].items()]\n                               )+\"}\" if tool['dynamic_params'] != \"\" else \"{}\"\n    return f'''<{L_TOOL} id=\"{tool['id']}\">\n  <{L_DESCRIPTION}>{tool['description']}</{L_DESCRIPTION}>\n  <{L_PARAMS}>{params}</{L_PARAMS}>\n</{L_TOOL}>\n'''", "\ndef make_tool_desc(tool: SingleTool):\n    params = \"{\" + \", \".join(['\"'+l + '\": '+v for l, v in tool['dynamic_params'].items()]\n                               )+\"}\" if tool['dynamic_params'] != \"\" else \"{}\"\n    return f'''<{L_TOOL} id=\"{tool['id']}\">\n  <{L_DESCRIPTION}>{tool['description']}</{L_DESCRIPTION}>\n  <{L_PARAMS}>{params}</{L_PARAMS}>\n</{L_TOOL}>\n'''\n\ndef generate_convo_history(memory: TupleList = [], facts: TupleList = []) -> str:\n    if len(memory) + len(facts) == 0:\n        return \"\"\n\n    return (\n        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_HUMAN) for p, a in memory]) + \\\n        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_AI) for p, a in facts])\n    )", "\ndef generate_convo_history(memory: TupleList = [], facts: TupleList = []) -> str:\n    if len(memory) + len(facts) == 0:\n        return \"\"\n\n    return (\n        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_HUMAN) for p, a in memory]) + \\\n        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_AI) for p, a in facts])\n    )\n\ndef prompt_for_instructions(instructions: str) -> str:\n    if instructions and len(instructions):\n        return f'''[{L_BOT_INSTRUCTIONS}]: To summarize the JSON, remember:\\n {instructions}\\n\\n\\n'''\n    else:\n        return \"\"", "\ndef prompt_for_instructions(instructions: str) -> str:\n    if instructions and len(instructions):\n        return f'''[{L_BOT_INSTRUCTIONS}]: To summarize the JSON, remember:\\n {instructions}\\n\\n\\n'''\n    else:\n        return \"\"\ndef prompt_for_answer(question: str) -> str:\n    return f'''[{L_QUESTION}]: {question}\\n[{L_ANSWER}]: ''';\n\n## ONLY for training purposes!\ndef get_training_tool_subset (\n    list_of_tools: ToolList,\n    tool_id_always_returned: Union[int, None],\n    max_num_elements: int = 6\n) -> ToolList:\n    # ONLY when we don't know, we return any elements.\n    if tool_id_always_returned == None \\\n       or tool_id_always_returned == DefaultTools.I_DONT_KNOW.value \\\n       or tool_id_always_returned == DefaultTools.ANSWER_FROM_MEMORY.value:\n        return sample(list_of_tools, max_num_elements - 1)\n\n    specific_tool = next(tool for tool in list_of_tools if tool[\"id\"] == tool_id_always_returned)\n    subset_of_tools = sample(list_of_tools, max_num_elements - 1)\n    # make sure it doesn't have our specific_tool inside\n    subset_of_tools = [tool for tool in subset_of_tools if tool[\"id\"] != tool_id_always_returned] + [specific_tool]\n\n    # VERY IMPORTANT - otherwise the model thinks that the last answer is always the correct one.\n    shuffle(subset_of_tools)\n\n    return subset_of_tools", "\n## ONLY for training purposes!\ndef get_training_tool_subset (\n    list_of_tools: ToolList,\n    tool_id_always_returned: Union[int, None],\n    max_num_elements: int = 6\n) -> ToolList:\n    # ONLY when we don't know, we return any elements.\n    if tool_id_always_returned == None \\\n       or tool_id_always_returned == DefaultTools.I_DONT_KNOW.value \\\n       or tool_id_always_returned == DefaultTools.ANSWER_FROM_MEMORY.value:\n        return sample(list_of_tools, max_num_elements - 1)\n\n    specific_tool = next(tool for tool in list_of_tools if tool[\"id\"] == tool_id_always_returned)\n    subset_of_tools = sample(list_of_tools, max_num_elements - 1)\n    # make sure it doesn't have our specific_tool inside\n    subset_of_tools = [tool for tool in subset_of_tools if tool[\"id\"] != tool_id_always_returned] + [specific_tool]\n\n    # VERY IMPORTANT - otherwise the model thinks that the last answer is always the correct one.\n    shuffle(subset_of_tools)\n\n    return subset_of_tools", "\n\ndef splitter_prompt(elements: List[QuestionSplitModelData]) -> Tuple[str, str]:\n\n    stopper: str = f'''</{L_ANSWER}>\n</{L_SPLIT}>'''\n\n    last_stop = None;\n\n    response: str = f'''\\\n{__split_into_subquestions_instructions()}'''\n\n    for element in elements:\n        response += f'''\\\n<{L_SPLIT}>\n{\n    __tools_we_have_access_to(\"\".join([__create_tool_tag(tool) for tool in element[\"tools\"]])) if (\"tools\" in element and element[\"tools\"] and isinstance(element[\"tools\"], list)) else \"\"\n}{\n    f\"\"\"<{L_CONVERSATION}>{\"\".join([make_interaction(q,a) for (q,a) in element[\"mem\"]])}</{L_CONVERSATION}>\"\"\" if (\"mem\" in element and isinstance(element[\"mem\"], list)) else \"\"\n}\n<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\n<{L_ANSWER}>'''\n\n        if \"answer\" in element and element[\"answer\"]:\n            response += f'''{SUBQ_SPLITTER.join(element[\"answer\"])}{stopper}\\n'''\n            last_stop = None\n        else:\n            last_stop = stopper\n\n    return response, last_stop", "\n\ndef toolpicker_prompt(elements: List[ToolpickerInputModelData], tools: ToolList) -> str:\n\n    shuffled_tools = tools.copy()\n    shuffle(shuffled_tools)\n    tools_html = \"\\n\".join([__create_tool_tag(_tool) for _tool in shuffled_tools])\n    last_stop = None\n\n    prompt: str = f'''<{L_BOT_INSTRUCTIONS}><{L_DESCRIPTION}>\nWe have a list of tools in <{L_TOOL_LIST}/>, and a question <{L_QUESTION}/> that we want to answer.\nWhich of the <{L_TOOL}/> in <{L_TOOL_LIST}/> can be used to best answer <{L_QUESTION}/>?\nAnswer only with the 'id' of the most relevant <{L_TOOL}/> (i.e.: {\", \".join([str(t['id']) for t in shuffled_tools])}).\nAnswer {DefaultTools.ANSWER_FROM_MEMORY.value} if the answer is in <{L_CONVERSATION}/>.\nAnswer {DefaultTools.I_DONT_KNOW.value} if no tool can be used to answer <{L_QUESTION}/>.\n</{L_DESCRIPTION}>\n\n<{L_TOOL_LIST}>\n{tools_html}\n</{L_TOOL_LIST}>\n</{L_BOT_INSTRUCTIONS}>\n'''\n\n    for element in elements:\n\n        prompt += f'''<{L_INTERACTION}>'''\n\n        prompt += f'''<{L_CONVERSATION}>\n            {\"\".join([make_interaction(mem_q, mem_a) for mem_q, mem_a in (element[\"mem\"])\n        ])}</{L_CONVERSATION}>''' if \"mem\" in element and isinstance(element[\"mem\"], list) and len(element[\"mem\"]) else \"\"\n\n        prompt += f'''{\n      f'<{L_THOUGHT}>{element[\"thought\"]}</{L_THOUGHT}>' if \"thought\" in element and element[\"thought\"] else \"\"\n}\n<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\n<{L_ANSWER}>'''\n\n        stopper = f'''</{L_ANSWER}>\\n</{L_INTERACTION}>'''\n\n        if \"answer\" in element:\n            prompt += f'''{element[\"answer\"]}{stopper}\\n'''\n            last_stop = None\n        else:\n            last_stop = stopper\n\n    return prompt, last_stop", "\n\ndef create_memory_prompt(elements: List[AnswerInMemoryModelData]) -> Tuple[str, str]:\n    do_you_know_the_answer = f\"<{L_BOT_INSTRUCTIONS}>Is the answer to <{L_QUESTION}/> found in the memory or in your knowledge base already? Answer with a yes or no.</{L_BOT_INSTRUCTIONS}>\"\n\n    stop = f\"</{L_ANSWER}></{L_INTERACTION}>\"\n\n    last_stop = None\n\n    prompt: str = f'''{do_you_know_the_answer}\\n'''\n\n    for element in elements:\n        conversation_history = generate_convo_history(\n            element[\"mem\"] if \"mem\" in element and element[\"mem\"] else [],\n            element[\"facts\"] if \"facts\" in element and element[\"facts\"] else []\n        )\n        prompt += f'''<{L_INTERACTION}>\\n'''\n        if conversation_history:\n            prompt += f'''\\\n<{L_CONVERSATION}>\n    {conversation_history}\n</{L_CONVERSATION}>\\n'''\n        prompt += f'''<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\\n<{L_ANSWER}>'''\n\n        if \"answer\" in element and isinstance(element[\"answer\"], bool):\n            prompt += f'''{\"yes\" if element[\"answer\"] else \"no\"}{stop}'''\n            last_stop = None\n        else:\n            last_stop = stop\n            break\n\n    return prompt, last_stop", "\n\n\n\ndef make_tool_input_case(\n    q_facts: TupleList,\n    q_question: str,\n    answer_json_input: Union[dict, None],\n    tool_descr: str,\n    tool_params: dict,\n    answer_label: str = \"JSON\",\n    wrapper_tag: str = L_EXAMPLE,\n) -> Tuple[str, Union[str, None]]:\n    system_instructions: str = f'''\nWe have the following {answer_label}: <{answer_label}>{json.dumps(tool_params)}</{answer_label}>. Each key describes what its value is.\\n\nThe {answer_label} is used where: <{L_DESCRIPTION}>{tool_descr}</{L_DESCRIPTION}>\\n\nWe need a {answer_label} object with the same keys as <{answer_label}/> to answer <{L_QUESTION}/>{f\"\"\" (we can use the <{L_CONVERSATION}/> for contextual info as well)\"\"\" if len(q_facts) else \"\"}. Must be a valid {answer_label}!'''\n\n    stopper: str = f\"</{L_ANSWER}>\\n</{wrapper_tag}>\"\n\n    mem = generate_convo_history(facts=q_facts)\n    return f\"\\n<{wrapper_tag}>\\n\" \\\n        + f\"  <{L_BOT_INSTRUCTIONS}>{system_instructions}</{L_BOT_INSTRUCTIONS}>\" \\\n        + (f\"  <{L_CONVERSATION}>{mem}\\n</{L_CONVERSATION}>\\n\" if mem else \"\") \\\n        + f\"  <{L_QUESTION}>{q_question}</{L_QUESTION}>\\n\" \\\n        + f\"  <{L_ANSWER} type=\\\"{answer_label}\\\">\"\\\n        + (( f\"{json.dumps(answer_json_input)}{stopper}\") if answer_json_input else \"\"), None if answer_json_input else stopper", ""]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/requests/call_llm.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.requests.call_open_ai import call_open_ai\n\ndef call_llm(llm_request: LLMCallParams) -> LLMCallReturnType:\n    if llm_request['llm'] == LLMCallType.OPENAI_CHAT or llm_request['llm'] == LLMCallType.OPENAI_COMPLETION:\n        try:\n            return call_open_ai(llm_request)\n        except Exception as e:\n            print(\"OpenAI call failed\", e)\n            return \"OpenAI is down!\", 0", "\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/requests/call_open_ai.py", "chunked_list": ["import openai\nimport os\nfrom llm_vm.utils.typings_llm import *\n\ndef __tokens_to_dollars(usage) -> float:\n    return float(usage)/1000*0.02\n\ndef call_open_ai(request: LLMCallParams) -> LLMCallReturnType:\n    model = request['model']\n    max_tokens = request['max_tokens']\n    stop = request['stop'] if 'stop' in request else None\n    cur_prompt = request['prompt']\n    temperature = request['temperature'] if 'temperature' in request else 0.0\n    chat = request['llm']\n\n    if isinstance(model, tuple):\n        model, api_key = model[0], model[1]\n    else:\n        api_key = False\n\n    if api_key:\n        if api_key[0] == '$':\n            api_key = False\n\n\n    current_key = None\n    if api_key:\n        current_key = openai.api_key\n        os.environ[\"OPENAI_API_KEY\"] = api_key\n        openai.api_key = api_key\n    else:\n        api_key = os.getenv(\"LLM_VM_OPENAI_API_KEY\")\n        openai.api_key = api_key\n\n    if chat == LLMCallType.OPENAI_CHAT:\n        response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo-0301\",\n                max_tokens=max_tokens,\n                stop=stop,\n                messages=cur_prompt,\n                temperature=temperature\n            )\n        response_text = response['choices'][0]['message']['content']\n    elif chat == LLMCallType.OPENAI_COMPLETION:\n\n        response = openai.Completion.create(\n                model=model,\n                max_tokens=max_tokens,\n                stop=stop,\n                prompt=cur_prompt,\n                temperature=temperature\n            )\n\n        response_text = response['choices'][0]['text']\n\n    if current_key:\n        openai.api_key = current_key\n        os.environ[\"OPENAI_API_KEY\"] = current_key\n\n    usage = response[\"usage\"][\"total_tokens\"]\n    price = __tokens_to_dollars(usage)\n    return response_text.strip(\"\\n \"), price", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/get_decision_model.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\nimport os\nimport json\n\ndef get_newest_decision_model(model: DecisionStep, default_model = OpenAIModel.DAVINCI_TEXT) -> Tuple[str, Union[str, None]]:\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_name = os.path.join(current_dir, 'models.json')\n\n    model_name = model.value\n    default_model_name = default_model.value\n    print(file_name)\n    file = open(file_name)\n\n    data: List[LLModels] = json.load(file);\n\n    for llm_model in reversed(data):\n        if model.value in llm_model and llm_model[model_name]:\n            print(f\"### {model_name} model will be used (version {llm_model['DATE']})\", flush=True)\n            return llm_model[model_name]['model_name'], llm_model['OPENAI_KEY']\n\n    print(f\"Model {model_name} not found: will use {default_model_name}\", flush=True)\n    return default_model_name, None", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_input_model/get_tool_input_as_jsonl.py", "chunked_list": ["import json\nfrom llm_vm.agents.FLAT.agent_helper.tool_utils import create_memory_prompt, make_tool_input_case\nfrom llm_vm.agents.FLAT.agent_helper.labels import *\nfrom typings_llm import *\nfrom random import shuffle\nfrom llm_vm.agents.FLAT.models.utils.tool_input_model.tool_input_model_data import tool_input_data\n\ndef __get_tool_input_jsonl(data: List[ToolInputModelData]) -> List[PromptModelEntry]:\n    jsonl_entries: List[PromptModelEntry] = []\n\n    for entry in data:\n        prompt, stopper = make_tool_input_case(\n            entry[\"mem\"],\n            entry[\"question\"],\n            None,\n            entry[\"description\"],\n            entry[\"params\"],\n        )\n\n        jsonl_entries.append({\n            \"prompt\": prompt,\n            \"completion\": json.dumps(entry[\"answer\"]) + stopper\n        })\n    shuffled_examples = jsonl_entries.copy()\n    shuffle(shuffled_examples)\n    return shuffled_examples", "\n\ndef tool_input_jsonl() -> Tuple[PromptModelEntry, str]:\n    return (\n        __get_tool_input_jsonl(tool_input_data[\"data\"]),\n        tool_input_data[\"openai_model\"].value\n    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_input_model/tool_input_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\nimport os\nimport json\nfrom llm_vm.agents.FLAT.agent_helper.utils import verbose_answer\n\ndef __get_tool_input_model(model: OpenAIModel) -> ToolInputModel:\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/tool_input_data.json'))\n\n    json_data: List[ToolInputModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\n    def __get_mem_tuple(json_mem: List[List[Union[str, dict]]]) -> TupleList:\n        mem: TupleList = []\n        for m in json_mem:\n            if len(m) == 3:\n                # when the \"mem\" has also the previous JSON response\n                mem.append((m[0], verbose_answer(m[2], m[1])))\n            elif len(m) == 2:\n                # when the \"mem\" has only question/answer\n                mem.append((m[0], m[1]))\n        return mem\n\n    return {\n        \"openai_model\": model,\n        \"data\": [{\n            \"mem\": __get_mem_tuple(t[\"mem\"]) if \"mem\" in t else [],\n            \"question\": t[\"question\"],\n            \"answer\": t[\"answer\"],\n            \"description\": t[\"description\"],\n            \"params\": t[\"params\"]\n        } for t in json_data]\n    }", "\n\ntool_input_data: ToolInputModel = __get_tool_input_model(OpenAIModel.DAVINCI)\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/answer_from_memory_model/answer_from_memory_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\nanswer_from_memory_data: AnswerInMemoryModel = {\n    \"openai_model\": OpenAIModel.DAVINCI,\n    \"data\": [\n        {\n            \"question\": \"What's the time?\",\n            \"answer\": False\n        },\n        {", "        },\n        {\n            \"mem\": [\n                (\"Whats day is it today?\", \"Today is Sunday, December 15th, 2027. The time is 11:33AM\")\n            ],\n            \"question\": \"What's the time?\",\n            \"answer\": True\n        },\n        {\n            \"question\": \"How are you feeling?\",", "        {\n            \"question\": \"How are you feeling?\",\n            \"answer\": True\n        },\n        {\n            \"question\": \"What color is the sky?\",\n            \"answer\": True\n        },\n        {\n            \"question\": \"What is the temperature in Portland?\",", "        {\n            \"question\": \"What is the temperature in Portland?\",\n            \"answer\": False\n        },\n        {\n            \"mem\": [\n               (\"What is the weather in Portland?\", \"It is sunny, 22C, with clear skies. Theres 40% chance of rain after 18:00hs\")\n            ],\n            \"question\": \"What is the temperature in Portland?\",\n            \"answer\": True", "            \"question\": \"What is the temperature in Portland?\",\n            \"answer\": True\n        },\n        {\n            \"question\": \"What day is it today?\",\n            \"answer\": False\n        },\n        {\n            \"question\": \"What's the name of this place?\",\n            \"answer\": False", "            \"question\": \"What's the name of this place?\",\n            \"answer\": False\n        }\n    ]\n}\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/answer_from_memory_model/get_asm_as_jsonl.py", "chunked_list": ["from llm_vm.agents.FLAT.agent_helper.tool_utils import create_memory_prompt\nfrom llm_vm.agents.FLAT.agent_helper.labels import *\nfrom typings_llm import *\nfrom llm_vm.agents.FLAT.models.utils.answer_from_memory_model.answer_from_memory_model_data import answer_from_memory_data\n\ndef __construct_answer_from_memory_jsonl(data: List[QuestionSplitModelData]) -> List[PromptModelEntry]:\n    jsonl_entries: List[PromptModelEntry] = []\n    for entry in data:\n        prompt, stopper = create_memory_prompt([{\n            \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n            \"question\": entry[\"question\"],\n            \"answer\": entry[\"answer\"]\n        }])\n\n        jsonl_entries.append({\n            \"prompt\": prompt,\n            \"completion\": (\"yes\" if entry[\"answer\"] else \"no\") + (stopper if stopper else \"\")\n        })\n    return jsonl_entries", "\n\ndef answer_from_memory_jsonl() -> Tuple[PromptModelEntry, str]:\n    return (\n        __construct_answer_from_memory_jsonl(answer_from_memory_data[\"data\"]),\n        answer_from_memory_data[\"openai_model\"].value\n    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/get_training_tools.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\n# Retuns the tools, in a random order, and with randomised IDs.\ndef get_randomised_training_tools (\n    generic_tools: ToolList,\n    shuffled_by: int = 0,\n    shuffle_by_modulo: int = 1e9\n) -> ToolList:\n\n    def __shuffled(tool_id: int) -> int:\n        return int((tool_id + shuffled_by) % shuffle_by_modulo)\n\n    shuffled_generic_tools: ToolList = [{\"id\": __shuffled(t[\"id\"]), \"description\": t[\"description\"]} for t in generic_tools]\n\n    tools: ToolList = shuffled_generic_tools + [\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_SEND_EMAIL.value),\n            \"description\": \"Use this tool to send an email to someone\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_BOOK_TABLE.value),\n            \"description\": \"Useful to book a table or slot at a restaurant\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_CREATE_DOCUMENT.value),\n            \"description\": \"Create a document on google drive\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_SHARE_DOCUMENT.value),\n            \"description\": \"Useful to share an online document with someone. You need the email\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_FIND_RESTAURANT.value),\n            \"description\": \"Use this tool to find a specific bar, caf\u00e9 or restaurant, or to search for places to eat around a certain location.\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_NEWS.value),\n            \"description\": \"Use this tool to get news and current events\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT.value),\n            \"description\": \"Use this tool to book an appointment, meeting or doctor, through a calendar.\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT.value),\n            \"description\": \"Use this tool to get the available time slots for meetings, doctors, appointments, etc.\"\n        },\n        {\n            \"id\": __shuffled(DefaultTrainingTools.TRAIN_ORDER_FOOD.value),\n            \"description\": \"Use this tool to order food and get it delivered.\"\n        }\n    ]\n\n    return tools", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/get_tp_as_jsonl.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.agent_helper.labels import *\nfrom llm_vm.agents.FLAT.agent_helper.tool_utils import toolpicker_prompt\nfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.tool_picker_model_data import tool_input_data\n\ndef __construct_tool_picker_jsonl(data: List[ToolpickerInputModelData]) -> List[PromptModelEntry]:\n    jsonl_entries: List[PromptModelEntry] = []\n    for entry in data:\n        prompt, stopper = toolpicker_prompt(\n            [{\n                \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n                \"question\": entry[\"question\"],\n                \"thought\": entry[\"thought\"] if \"thought\" in entry else None\n            }],\n            entry[\"tools\"]\n        )\n        jsonl_entries.append({\n            \"prompt\": prompt,\n            \"completion\": str(entry[\"answer\"]) + stopper\n        })\n    return jsonl_entries", "\n\ndef tool_picker_jsonl() -> Tuple[PromptModelEntry, str]:\n    return (\n        __construct_tool_picker_jsonl(tool_input_data[\"data\"]),\n        tool_input_data[\"openai_model\"].value\n    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/tool_picker_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\nimport json\nimport os\nfrom llm_vm.agents.FLAT.agent_helper.tool_utils import get_training_tool_subset\nfrom llm_vm.agents.FLAT.agent_helper.tools import GENERIC_TOOLS\nfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.get_training_tools import get_randomised_training_tools\nfrom random import randrange\n\ndef __use_tool(tool_id: int, shuffle_value: int = 0, shuffle_modulo: int = 1e9) -> int:\n    return int((tool_id + shuffle_value) % shuffle_modulo)", "def __use_tool(tool_id: int, shuffle_value: int = 0, shuffle_modulo: int = 1e9) -> int:\n    return int((tool_id + shuffle_value) % shuffle_modulo)\n\n# Get a random subset of the tools, which WILL INCLUDE the given tool_id.\n# -> The tools are shuffled to prevent the model from finding patterns\n# -> The ids are changed to prevent the model from remembering tools (i.e. Tool=X is always ID=Y)\ndef __get_random_tool_subset(tool_id: int, shuffle_value: int = 0, shuffle_modulo: int = 1e9) -> str:\n    return get_training_tool_subset(\n        get_randomised_training_tools(GENERIC_TOOLS, shuffle_value, shuffle_modulo),\n        tool_id\n    )", "\ndef __get_toolpicker_model(tools_input_model: ToolpickerInputModel) -> ToolpickerInputModel:\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/toolpicker_data.json'))\n    json_data: List[QuestionSplitModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\n    tools_input_model[\"data\"] = json_data\n    complete_tools: List[ToolpickerInputModelData] = []\n    for t in tools_input_model[\"data\"]:\n        rand_value = randrange(0, len(tools_input_model[\"data\"]))\n        answer_key = t[\"answer\"]\n        answer_value = ALL_TOOLS_MAP[answer_key].value if answer_key in ALL_TOOLS_MAP else None\n        if (answer_value == None):\n            raise Exception(\"Invalid tool name/value: \" + answer_key)\n\n        # tool_id_to_use = __use_tool(answer_value, rand_value) if answer_value > 0 else answer_value\n        tool_id_to_use = 0\n        complete_tools.append({\n            \"question\": t[\"question\"],\n            \"thought\": t[\"thought\"] if \"thought\" in t else \"\",\n            \"mem\": [(a[0], a[1]) for a in t[\"mem\"]] if \"mem\" in t else [],\n            \"answer\": tool_id_to_use,\n            \"tools\": __get_random_tool_subset(tool_id_to_use, rand_value)\n        })\n\n    complete_model: ToolpickerInputModel = {\n        \"openai_model\": tools_input_model[\"openai_model\"],\n        \"data\": complete_tools\n    }\n    return complete_model;", "\ntool_input_data: ToolpickerInputModel = __get_toolpicker_model({\n    \"openai_model\": OpenAIModel.DAVINCI\n})\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/question_split_model/question_split_model_data.py", "chunked_list": ["import json\nimport os\nfrom llm_vm.utils.typings_llm import *\n\ndef __question_splitter_data() -> QuestionSplitInputModel:\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/question_split_data.json'))\n\n    json_data: List[QuestionSplitModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\n    question_data: List[QuestionSplitModelData] = [\n        {\n            \"mem\": [(a[0], a[1]) for a in q[\"mem\"]] if \"mem\" in q else [],\n            \"question\": q[\"question\"],\n            \"answer\": q[\"answer\"]\n        }\n        for q in json_data\n    ]\n\n    return {\n        \"openai_model\": OpenAIModel.DAVINCI,\n        \"data\": question_data\n    }", "\nquestion_splitter_data = __question_splitter_data();\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/question_split_model/get_qs_as_jsonl.py", "chunked_list": ["from llm_vm.agents.FLAT.agent_helper.tool_utils import splitter_prompt\nfrom llm_vm.agents.FLAT.agent_helper.labels import *\nfrom llm_vm.typings_llm import *\nfrom llm_vm.agents.FLAT.models.utils.question_split_model.question_split_model_data import question_splitter_data\n\ndef __construct_question_split_jsonl(data: List[QuestionSplitModelData]) -> List[PromptModelEntry]:\n    jsonl_entries: List[PromptModelEntry] = []\n    for entry in data:\n        prompt, stopper = splitter_prompt([{\n            \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n            \"tools\": entry[\"tools\"] if \"tools\" in entry else None,\n            \"question\": entry[\"question\"]\n        }])\n        jsonl_entries.append({\n            \"prompt\": prompt,\n            \"completion\": SUBQ_SPLITTER.join(entry[\"answer\"]) + stopper\n        })\n    return jsonl_entries", "\n\ndef question_splitter_jsonl() -> Tuple[PromptModelEntry, str]:\n    return (\n        __construct_question_split_jsonl(question_splitter_data[\"data\"]),\n        question_splitter_data[\"openai_model\"].value\n    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/persist_models.py", "chunked_list": ["import json\nfrom llm_vm.utils.typings_llm import *\nimport os\nimport time\n\ndef persist_models(models: LLModels, openai_key: str, is_test: bool = False) -> None:\n    if is_test:\n        return\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_name = os.path.join(current_dir, '../models.json')\n\n    model_json: List[LLModels] = []\n    with open(file_name, \"r\") as output_file:\n        loaded_json = json.loads(output_file.read() or \"[]\")\n        model_json: List[LLModels] = loaded_json if loaded_json else []\n\n    with open(file_name, \"w\") as output_file:\n        models[\"TIMESTAMP\"] = int(time.time())\n        models[\"DATE\"] = time.asctime()\n        models[\"OPENAI_KEY\"] = openai_key\n        model_json.append(models)\n        json.dump(model_json, output_file, indent=4)", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/delete_model.py", "chunked_list": ["import openai\n\ndef delete_model (model_name: str) -> None:\n    ## DELETE MODELS\n    try:\n        # model = openai.Model.retrieve()\n        openai.Model.delete(model_name)\n    except Exception as e:\n        print(\"Could not delete model because: \" + str(e))\n\n\n    try:\n        # Delete all files:\n        files = openai.File.list()[\"data\"]\n        [openai.File.delete(file[\"id\"]) for file in files];\n    except Exception as e:\n        print(\"Could not delete file because: \" + str(e))", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/check_model_status.py", "chunked_list": ["import openai\nfrom llm_vm.utils.typings_llm import *\n\ndef check_model_status (\n    job_id: str,\n    label: str\n) -> Dict:\n    finetuned_model = openai.FineTune.retrieve(id=job_id)\n\n    print(\n        label,\n        finetuned_model[\"status\"],\n        flush=True\n    )\n\n    return finetuned_model", ""]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/upload_model.py", "chunked_list": ["import json\nimport openai\nimport time\nimport os\nfrom llm_vm.utils.typings_llm import *\nfrom llm_vm.agents.FLAT.models.helpers.check_model_status import check_model_status\n\n__START_LABEL    = \" \ud83c\udf9b\ufe0f  Creating model...\"\n__PROGRESS_LABEL = \" \u23f3 Processing\"\n__FINISHED_LABEL = \"\\n \ud83c\udf89 Finished!\" + \\", "__PROGRESS_LABEL = \" \u23f3 Processing\"\n__FINISHED_LABEL = \"\\n \ud83c\udf89 Finished!\" + \\\n                   \"\\n \ud83d\udd17\"\n\ndef upload_model (\n    data: PromptModelEntry,\n    openai_model: str,\n    file_name: str,\n    is_test: bool = False\n) -> LLMTrainingResult:\n    start_of_execution = time.time()\n\n    def progress_label(label: str) -> str:\n        ellapsed_s = int(time.time() - start_of_execution)\n        return f\"{label} '{file_name}'\\t ({'%dm %02ds' % (ellapsed_s / 60, ellapsed_s % 60)}) \"\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    file_path = os.path.join(current_dir, f'./../data/{file_name}.jsonl')\n\n    with open(file_path, \"w\") as output_file:\n        output_file.truncate(0)\n        for entry in data:\n            json.dump(entry, output_file)\n            output_file.write(\"\\n\")\n\n    if is_test:\n        return {\n            \"model_name\": file_name + \"_EXXX\",\n            \"model_files\": [],\n            \"elapsed_time_s\": int(time.time() - start_of_execution)\n        }\n\n    upload_response = openai.File.create(\n        file=open(file_path, \"rb\"),\n        purpose='fine-tune'\n    )\n\n    file_id = upload_response.id\n\n    fine_tune_response = openai.FineTune.create(training_file=file_id, model=openai_model)\n    job_id = fine_tune_response[\"id\"]\n    print(__START_LABEL, f'''job_id: {job_id}, model: {file_name}''', flush=True)\n\n    finetuned_model = {\"status\": None}\n    while finetuned_model[\"status\"] not in [\"succeeded\", \"failed\"]:\n        time.sleep(15)\n        finetuned_model = check_model_status(job_id, progress_label(__PROGRESS_LABEL))\n\n    print(progress_label(__FINISHED_LABEL), finetuned_model[\"fine_tuned_model\"], flush=True)\n\n    related_files = (\n        [file[\"id\"] for file in finetuned_model[\"result_files\"]] + \\\n        [file[\"id\"] for file in finetuned_model[\"training_files\"]]\n    )\n\n    return {\n        \"model_name\": finetuned_model[\"fine_tuned_model\"],\n        \"model_files\": related_files,\n        \"elapsed_time_s\": int(time.time() - start_of_execution)\n    }", ""]}
{"filename": "src/llm_vm/utils/typings_llm.py", "chunked_list": ["\"\"\"\nThis file contains various machine learning classes and variables\nfor training and API calls to OpenAI.\n\"\"\"\n\nfrom typing import Tuple, List, Dict, TypedDict, Union, Any, Optional\nfrom enum import Enum\n\nclass OpenAIModel(Enum):\n    CURIE = \"curie\"\n    CURIE_TEXT = \"text-curie-001\"\n    FAST_DAVINCI = \"code-cushman-002\"\n    DAVINCI = \"davinci\"\n    DAVINCI_TEXT = \"text-davinci-003\"", "class OpenAIModel(Enum):\n    CURIE = \"curie\"\n    CURIE_TEXT = \"text-curie-001\"\n    FAST_DAVINCI = \"code-cushman-002\"\n    DAVINCI = \"davinci\"\n    DAVINCI_TEXT = \"text-davinci-003\"\n\nclass LLMCallType(Enum):\n    OPENAI_COMPLETION = \"openai-completion\"\n    OPENAI_CHAT = \"openai-chat\"", "    # ... other models\n\nclass DecisionStep(Enum):\n    SPLIT = \"split_questions\"\n    INPUT = \"guess_api_input\"\n    FROM_MEMORY = \"use_memory\"\n    TOOL_PICKER = \"tool_picker\"\n\nclass DefaultTools(Enum):\n    I_DONT_KNOW = -1\n    ANSWER_FROM_MEMORY = 0\n\n    WOLFRAM = 1\n    DIRECTIONS = 2\n    WEATHER = 3\n\n    ## Always set this to the latest ID. This is an integer, not an enum!\n    __LAST_ID__: int = 4", "class DefaultTools(Enum):\n    I_DONT_KNOW = -1\n    ANSWER_FROM_MEMORY = 0\n\n    WOLFRAM = 1\n    DIRECTIONS = 2\n    WEATHER = 3\n\n    ## Always set this to the latest ID. This is an integer, not an enum!\n    __LAST_ID__: int = 4", "\nclass DefaultTrainingTools(Enum):\n    TRAIN_SEND_EMAIL = 4\n    TRAIN_CREATE_DOCUMENT = 5\n    TRAIN_SHARE_DOCUMENT = 6\n    TRAIN_BOOK_TABLE = 7\n    TRAIN_FIND_RESTAURANT = 8\n    TRAIN_NEWS = 9\n    TRAIN_ORDER_FOOD = 10\n    TRAIN_BOOK_APPOINTMENT = 11\n    TRAIN_AVAILABLE_APPOINTMENT = 12\n    TRAIN_FIND_FLIGHT = 13", "\nTupleList = List[Tuple[str, str]]\n\nclass ToolTypeArgs(TypedDict):\n    url: str\n    params: Optional[dict]\n    json: Optional[dict]\n    jsonParams: Optional[dict]\n    auth: Optional[dict]\n    cert: Optional[str]\n    data: Optional[dict]", "\nclass SingleTool(TypedDict):\n    description: str\n    dynamic_params: dict\n    id: int\n    args: ToolTypeArgs\n    ai_response_prompt: Union[str, None]\n\nToolList = List[SingleTool]\n# ToolList = NewType(\"ToolList\", List[SingleTool])", "ToolList = List[SingleTool]\n# ToolList = NewType(\"ToolList\", List[SingleTool])\nEnumeratedToolList = List[Tuple[int,SingleTool]]\n\nALL_TOOLS_MAP: Dict[str, Union[DefaultTools, DefaultTrainingTools]] = {\n    \"dont_know\": DefaultTools.I_DONT_KNOW,\n    \"memory\": DefaultTools.ANSWER_FROM_MEMORY,\n    \"weather\": DefaultTools.WEATHER,\n    \"wolfram\": DefaultTools.WOLFRAM,\n    \"directions\": DefaultTools.DIRECTIONS,", "    \"wolfram\": DefaultTools.WOLFRAM,\n    \"directions\": DefaultTools.DIRECTIONS,\n    \"send_email\": DefaultTrainingTools.TRAIN_SEND_EMAIL,\n    \"create_doc\": DefaultTrainingTools.TRAIN_CREATE_DOCUMENT,\n    \"share_doc\": DefaultTrainingTools.TRAIN_SHARE_DOCUMENT,\n    \"book_table\": DefaultTrainingTools.TRAIN_BOOK_TABLE,\n    \"find_restaurant\": DefaultTrainingTools.TRAIN_FIND_RESTAURANT,\n    \"news\": DefaultTrainingTools.TRAIN_NEWS,\n    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,", "    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,\n    \"check_appointment\": DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT,\n    \"find_flight\": DefaultTrainingTools.TRAIN_FIND_FLIGHT\n}\n\nclass QuestionSplitModelJSONData(TypedDict):\n    mem: List[List[str]]\n    question: str\n    answer: List[str]", "\nclass QuestionSplitModelData(TypedDict):\n    mem: Union[TupleList, None]\n    question: str\n    answer: Union[List[str], None]\n    tools: Union[ToolList, None]\n\nclass AnswerFromMemoryModelData(TypedDict):\n    mem: Union[TupleList, None]\n    question: str\n    thought: Union[str, None]\n    answer: Union[str, None]", "\nclass QuestionSplitInputModel(TypedDict):\n    model_name: str\n    openai_model: OpenAIModel\n    data: List[QuestionSplitModelData]\n\nclass PromptModelEntry(TypedDict):\n    prompt: str\n    completion: str\n\nclass LLMCallParams(TypedDict):\n    llm: LLMCallType\n    model: Tuple[str, Union[str, None]]\n    prompt: str\n    temperature: float\n    max_tokens: int\n    stop: str", "\nclass LLMCallParams(TypedDict):\n    llm: LLMCallType\n    model: Tuple[str, Union[str, None]]\n    prompt: str\n    temperature: float\n    max_tokens: int\n    stop: str\n\nLLMCallReturnType = Tuple[str, float]", "\nLLMCallReturnType = Tuple[str, float]\n\nclass LLMTrainingResult(TypedDict):\n    elapsed_time_s: int\n    model_name: str\n    model_files: List[str]\n\nclass LLMTrainingResultMeta(TypedDict):\n    OPENAI_KEY: str\n    DATE: str\n    TIMESTAMP: int", "class LLMTrainingResultMeta(TypedDict):\n    OPENAI_KEY: str\n    DATE: str\n    TIMESTAMP: int\n\nLLModels = Union[Dict[str, LLMTrainingResult], LLMTrainingResultMeta]\n\n\nclass ToolInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    description: str\n    question: str\n    answer: dict\n    params: dict", "class ToolInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    description: str\n    question: str\n    answer: dict\n    params: dict\nclass ToolInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    description: str\n    question: str\n    answer: dict\n    params: dict", "\nclass ToolInputModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[ToolInputModelData]\n\nclass ToolpickerInputModelJSONData(TypedDict):\n    mem: Optional[List[str]]\n    thought: Optional[str]\n    question: str\n    answer: str\nclass ToolpickerInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    thought: Optional[str]\n    question: str\n    answer: int\n    tools: Optional[ToolList]", "class ToolpickerInputModelData(TypedDict):\n    mem: Optional[TupleList]\n    thought: Optional[str]\n    question: str\n    answer: int\n    tools: Optional[ToolList]\n\nclass ToolpickerInputModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[ToolpickerInputModelData]", "\nclass AnswerInMemoryModelData(TypedDict):\n    mem: Optional[TupleList]\n    facts: Optional[TupleList]\n    question: str\n    answer: Optional[bool]\n\nclass AnswerInMemoryModel(TypedDict):\n    openai_model: OpenAIModel\n    data: List[AnswerInMemoryModelData]", "\nclass DebugCalls(TypedDict):\n    response: str\n    gpt_suggested_params: Any\n    url: str\n    raw_api_return: str\n\nDebugCallList = List[DebugCalls]\n", ""]}
{"filename": "src/llm_vm/utils/labels.py", "chunked_list": ["\"\"\"\nThis file contains a list of constants that represent tags for JSON objects.\n\"\"\"\n\n# REBEL & BACKWARD_CHAINING LABELS\nTOOL_ID = \"ID\"\nDESCRIPTION =\"DESC\"\nPARAMS = \"PARAMS\"\n\nINTERACTION = \"QA\"", "\nINTERACTION = \"QA\"\nEXAMPLE = \"CASE\"\nCONVERSATION = \"MEM\"\n\nQUESTION = \"Q\"\nTHOUGHT = \"THOUGHT\"\nPROMPT = \"P\"\nRESPONSE = \"A\"\n", "RESPONSE = \"A\"\n\n# FLAT LABELS\nL_DESCRIPTION =\"DESC\"\nL_PARAMS = \"PARAMS\"\nL_BOT_INSTRUCTIONS = \"SYSTEM\"\n\nL_INTERACTION = \"INTERACTION\"\nL_INTERACTIONS = \"INTERACTIONS\"\nL_EXAMPLE = \"EXAMPLE\"", "L_INTERACTIONS = \"INTERACTIONS\"\nL_EXAMPLE = \"EXAMPLE\"\nL_EXAMPLES = \"EXAMPLES\"\nL_CONVERSATION = \"CONVERSATION-HISTORY\"\n\nL_THOUGHT = \"THOUGHT\"\nL_TOOL_LIST = \"TOOLS\"\nL_TOOL = \"TOOL\"\nL_TOOL_PICKER = \"PICK-TOOL\"\nL_QUESTION = \"QUESTION\"", "L_TOOL_PICKER = \"PICK-TOOL\"\nL_QUESTION = \"QUESTION\"\nL_ANSWER = \"ANSWER\"\nL_ANSWER_DATA = \"DATA\"\nL_ANSWER_SUMMARY = \"SUMMARY\"\n\nL_SPLIT = \"SPLIT\"\nL_SPLIT_INSTRUCTIONS = \"INSTRUCTIONS\"\n\nSUBQ_SPLITTER = \"|||\"", "\nSUBQ_SPLITTER = \"|||\"\n\n# Human-AI interaction\nL_INT_HUMAN = \"HUMAN-AI\"\n# AI-AI interaction\nL_INT_AI = \"AI-AI\"\n\nQUALITY = 0.2\n", "QUALITY = 0.2\n\n\n\n"]}
{"filename": "src/llm_vm/utils/tools.py", "chunked_list": ["\"\"\"\nThis file contains common tools (JSON API request objects) that are used\nfor all the agents to retrieve information for questions across various topics.\n\"\"\"\n\nimport os\nimport sys\n\n# Get the current file's directory to grab the python files with common functionality in the utils/ folder\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))", "# Get the current file's directory to grab the python files with common functionality in the utils/ folder\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\ngrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\nutils_dir = os.path.join(grandparent_dir, 'utils/')\nsys.path.append(utils_dir)\n\nfrom llm_vm.utils.keys import *\nfrom llm_vm.utils.labels import *\nfrom llm_vm.utils.typings_llm import *\n", "from llm_vm.utils.typings_llm import *\n\nCUSTOM_TOOL_ANSWER_EMBEDDING = \"/answer_embedding\"\n\n\ndef __get_example_tools():\n    # wolfram\n    google_tool = {\n               'description': \"The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\",\n               'dynamic_params': {\"q\": 'The natural language input query'},\n               'method': 'GET',\n               'args': {'url': \"https://www.googleapis.com/customsearch/v1\",\n                         'params': {'key': \"\",\n                                    'cx' : \"\",\n                                    'q': '{q}'}\n                        }\n               }\n\n    # geopy\n    directions_tool = {'description': \"Find the driving distance and time to travel between two cities.\",\n               'dynamic_params': {\"origins\": 'the origin city', \"destinations\": 'the destination city'},\n               'method': 'GET',\n               'args': {'url': \"https://maps.googleapis.com/maps/api/distancematrix/json\",\n                         'params': {'key': \"\",\n                                    'origins': '{origins}',\n                                    'destinations': '{destinations}'}\n                        }}\n\n    # weather\n    weather_tool = {'description': 'Find the weather at a location and returns it in celcius.',\n               'dynamic_params': {\"latitude\": 'latitude of as a float',\n                                  \"longitude\": 'the longitude as a float'},\n               'method': 'GET',\n               'args': {'url': \"https://api.open-meteo.com/v1/forecast\",\n                         'params': {'current_weather': 'true',\n                                    'latitude': '{latitude}',\n                                    'longitude': '{longitude}'\n                                    }}\n               }\n    return [google_tool, directions_tool, weather_tool]", "\n\nGENERIC_TOOLS = __get_example_tools()\n"]}
{"filename": "src/llm_vm/utils/keys.py", "chunked_list": ["\"\"\"\nAPI Key Definitions and Regex Pattern for Pure Interpolations.\n\nThis section defines and retrieves the API keys required for various services from the environment variables.\nIt also includes a regular expression pattern used to identify pure interpolations.\n\nAPI Key Definitions:\n- OPENAI_DEFAULT_KEY: The API key for OPENAI.\n- GOOGLE_MAPS_KEY: The API key for Google Maps.\n- SERPAPI_KEY: The API key for SERPAPI (Anarchy AI)", "- GOOGLE_MAPS_KEY: The API key for Google Maps.\n- SERPAPI_KEY: The API key for SERPAPI (Anarchy AI)\n- WOLFRAM_KEY: The API key for Wolfram.\n\n\nNote: The corresponding environment variables should be set before running\nany of the agents. Set the environment variables without quotes.\nE.g.: WOLFRAM_KEY=an-encypted-key\n\"\"\"\nimport os", "\"\"\"\nimport os\nimport openai\n\n\n\n###\n# Use this regex to find if a variable is a pure interpolation.\n# For @example, { \"name\": \"{my_name}\" } is pure, since there's nothing else besides the interpolation\n# For @example, { \"Ticker\": \"TickerID={ticker_id}|historical=1\" } is NOT pure, since there is extra text", "# For @example, { \"name\": \"{my_name}\" } is pure, since there's nothing else besides the interpolation\n# For @example, { \"Ticker\": \"TickerID={ticker_id}|historical=1\" } is NOT pure, since there is extra text\nDICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS = \"^\\{[a-zA-Z0-9\\.\\-_]+\\}$\"\n\n\ndef set_api_key(key, key_type=\"OPENAI_API_KEY\"):\n    \"\"\"\n    Set the API key in the environment variable.\n\n    Parameters:\n    - key:  The API key to set. Defaults to OPENAI_DEFAULT_KEY.\n            There are four options:\n                - OPENAI_DEFAULT_KEY\n                - GOOGLE_MAPS_KEY\n                - GOOGLE_KEY\n                - GOOGLE_CX\n    - key_type: The type of API key. Defaults to \"OPENAI_API_KEY\".\n                There are four options:\n                - OPENAI_API_KEY\n                - GOOGLE_MAPS_KEY\n                - GOOGLE_KEY\n                - GOOGLE_CX\n    \"\"\"\n    if not os.getenv(key_type):\n        os.environ[key_type]=key\n\n        if key_type == \"OPENAI_API_KEY\":\n            openai.api_key=str(key)", ""]}
{"filename": "src/llm_vm/utils/print_types.py", "chunked_list": ["\"\"\"\nThis file contains a list of common functions across\nthe different agents that format and print output to the CLI.\n\"\"\"\n\ndef print_big(data, label = \"\"):\n    def do_format(x) -> str:\n        formatted_title = \"======#====== {:20} ======#======\\n\"\n        if len(x) >= 20:\n            return formatted_title.format(x)\n        else:\n            return formatted_title.format((int((20 - len(x)) / 2) * \" \") + x)\n    try:\n        if len(label):\n            print(do_format(str(label).upper()), data, flush=True)\n        else:\n            print(do_format(str(data)), flush=True)\n\n    except:\n        print(label, flush=True)", ""]}
{"filename": "src/llm_vm/server/main.py", "chunked_list": ["import flask\nfrom flask import request, jsonify\nimport json\nimport time\nimport traceback\nimport importlib\nimport openai\nimport os\nimport hashlib\nimport sys", "import hashlib\nimport sys\nfrom flask_cors import CORS\nfrom contextlib import contextmanager\nimport llm_vm.server.routes as routes\nfrom llm_vm.config import settings\n\napp = flask.Flask(__name__)\nCORS(app)\napp.config[\"DEBUG\"] = True", "CORS(app)\napp.config[\"DEBUG\"] = True\n\n# Register_blueprint from routes to load API\napp.register_blueprint(routes.bp)\n\ndef server_entry_point(host = '127.0.0.1', port = 3002):\n    \"\"\"\n    This function launches the server with specified parameters\n\n     Parameters:\n         host (str): Network IP address\n         port (int): Port Number\n\n     Returns:\n         None\n\n     Example:\n         >>> server_entry_point(port = 3002)\n    \"\"\"\n    app.run(host = host,port = port)", "\ndef cli():\n    \"\"\"\n     This function is the entry point for the project and allows the user to specify an option network address and port number when launching from the cli\n\n     Parameters:\n         None\n\n     Returns:\n         None\n\n    \"\"\"\n    port = settings.port\n    if port > 65535:\n        print('Port defined out of range, defaulting to 3002')\n        port = 3002\n    server_entry_point(host = settings.host, port = port)", "\nif __name__ == '__main__':\n    cli()\n"]}
{"filename": "src/llm_vm/server/routes.py", "chunked_list": ["from flask import request, Blueprint\nimport json\nimport os\nimport openai\nfrom llm_vm.agents.REBEL import agent\nfrom llm_vm.client import Client\nfrom llm_vm.config import settings\n# load optimizer for endpoint use\n# optimizer = LocalOptimizer(MIN_TRAIN_EXS=2,openai_key=None)\n", "# optimizer = LocalOptimizer(MIN_TRAIN_EXS=2,openai_key=None)\n\nclient = Client( big_model=settings.big_model, small_model=settings.small_model)\n\nprint('optimizer loaded')\n\nbp = Blueprint('bp',__name__)\n\n@bp.route('/', methods=['GET'])\ndef home():\n    return '''home'''", "@bp.route('/', methods=['GET'])\ndef home():\n    return '''home'''\n\n@bp.route('/v1/complete', methods=['POST'])\ndef optimizing_complete():\n    rebel_agent = agent.Agent(\"\", [], verbose=1)\n    data = json.loads(request.data)\n    static_context = data[\"context\"]\n    dynamic_prompt = data[\"prompt\"]\n    data_synthesis = False\n    finetune = False\n    use_rebel_agent = False\n    kwargs = {}\n    if \"openai_key\" not in data.keys():\n        return {\"status\":0, \"resp\":\"No OpenAI key provided\"}\n\n    if \"temperature\" in data.keys():\n        if type(data[\"temperature\"]) != float and type(data[\"temperature\"]) != int:\n            return {\"status\":0, \"resp\":\"Wrong Data Type for temperature\"}\n        else:\n            kwargs.update({\"temperature\":data[\"temperature\"]})\n\n    if \"stoptoken\" in data.keys():\n\n        if type(data[\"stoptoken\"]) != str and type(data[\"stoptoken\"]) != list:\n            # stop can either be a string or array of strings\n            return {\"status\":0, \"resp\":\"Wrong Data Type for stop\"}\n        elif type(data[\"stoptoken\"]) == list:\n            if len(data[\"stoptoken\"]) > 4:\n                return {\"status\":0, \"resp\":\"Too many stop tokens in array limit to 4 or less\"}\n            # check that every element in the list is a string\n            for j in data[\"stoptoken\"]:\n                if type(j) != str:\n                    return {\"status\":0, \"resp\":\"Wrong Data Type for stop\"}\n            kwargs.update({\"stop\": data[\"stoptoken\"]})\n        else:\n            kwargs.update({\"stop\":data[\"stoptoken\"]})\n\n    if \"data_synthesis\" in data.keys():\n        if type(data[\"data_synthesis\"])==bool:\n            data_synthesis = data[\"data_synthesis\"]\n        else:\n            return {\"status\":0, \"resp\":\"Wrong Data Type for data_synthesis\"}\n\n    if \"finetune\" in data.keys():\n        if type(data[\"finetune\"])==bool:\n            finetune = data[\"finetune\"]\n        else:\n            return {\"status\":0, \"resp\":\"Wrong Data Type for finetune\"}\n\n    if \"tools\" in data.keys():\n        if type(data[\"tools\"]) != list:\n            return {\"status\":0, \"resp\":\"Wrong data type for tools list\"}\n        else:\n            tools=[]\n            for i in data[\"tools\"]:\n                temp_tool_dict = {}\n                temp_args_dict = {}\n                temp_tool_dict.update({\"description\":i[\"description\"]})\n                temp_tool_dict.update({\"dynamic_params\":i[\"dynamic_params\"]})\n                temp_tool_dict.update({\"method\":i[\"method\"]})\n                temp_args_dict.update({\"url\":i[\"url\"]})\n                temp_args_dict.update({\"params\":{}})\n                for j in i[\"static_params\"].keys():\n                    temp_args_dict[\"params\"].update({j:i[\"static_params\"][j]})\n                for k in i[\"dynamic_params\"].keys():\n                    temp_args_dict[\"params\"].update({k:\"{\"+k+\"}\"})\n                temp_tool_dict.update({\"args\":temp_args_dict})\n                tools.append(temp_tool_dict)\n            rebel_agent.set_tools(tools)\n            use_rebel_agent = True\n    try:\n        openai.api_key = data[\"openai_key\"]\n    except:\n        return  {\"status\":0, \"resp\":\"Issue with OpenAI key\"}\n\n    # optimizer.openai_key = openai.api_key\n    agent.set_api_key(openai.api_key,\"OPENAI_API_KEY\")\n    try:\n        if not use_rebel_agent:\n            completion = client.complete(static_context,dynamic_prompt,openai_key=openai.api_key, data_synthesis=data_synthesis,finetune = finetune, **kwargs)\n        else:\n            completion = rebel_agent.run(static_context+dynamic_prompt,[])[0]\n    except Exception as e:\n        return {\"status\":0, \"resp\": str(e)}\n\n    # return {\"completion\":completion, \"status\": 200}\n    return completion", ""]}
{"filename": "src/llm_vm/completion/data_synthesis.py", "chunked_list": ["import json\nimport sys\nfrom sentence_transformers import SentenceTransformer, util\nimport openai\n\n\nclass DataSynthesis:\n     def __init__(self, variance, examples_to_generate):\n         self.variance = variance\n         self.examples_to_generate = examples_to_generate\n     def data_synthesis(self, optimizer, prompt, response, example_delim=\"<Datum-Separator/>\", openai_key=None, semantic_sim=True, **kwargs):\n        \"\"\"\n        This method generates QA pairs using the larger LLM to be used as training data for fine-tuning the smaller LLM.\n\n        Parameters\n        ----------\n        - optimizer (class): The Optimizer class to use for fine-tuning. Could be either LocalOptimizer or HostedOptimizer.\n        - prompt (str | list ): A question to be used as a one-shot QA example for the larger LLM prompt.\n        - response (str | list): A verified answer to the provided prompt question to be used in the one-shot QA example.\n        - example_delim (str): A unique XML tag used to separate the generated JSON examples. Default value is \"<Datum-Separator/>\".\n        - semantic_sim (bool): Option to use semantic similarity to filter duplicate generations. Default value is True.\n        - **kwargs: Additional keyword arguments to be passed into the `call_big` method.\n\n        Returns\n        ----------\n        - List: A list of tuples containing the QA pairs to be used for fine-tuning.\n        \"\"\"\n        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        final_prompt = None\n        if type(prompt) is str:\n            final_prompt = '{\"prompt\": \"' +prompt+'\"  , \"response\": \"' +response+'\" }'+example_delim\n        elif type(prompt) is list:\n            json_str = \"\"\n            for idx,p in enumerate(prompt):\n               example_str = '{\"prompt\": \"' + p +'\"  , \"response\": \"' + str(response[idx]) +'\" }'+example_delim+'\\n'\n               json_str += example_str\n            final_prompt = json_str\n        final_prompt = \"Generate 50 more jsons like the ones below. Use \"+example_delim+\" as a delimeter between JSONs.\\n\" + final_prompt\n        print(final_prompt)\n        data = None\n        openai.api_key=openai_key\n        response=openai.Completion.create(prompt=final_prompt,model=\"text-davinci-003\",max_tokens=1000,temperature=1).choices[0].text\n        datapoints = []\n        print(\"REPLY\"+response,file=sys.stderr)\n        split_response = response.split(sep=example_delim)\n        print(f\"Generated {len(split_response)}/{self.examples_to_generate} examples.\", file=sys.stderr )\n        if semantic_sim:\n            num_responses = len(split_response)\n            batch_responses = []\n            for idx in range(0, len(split_response), 10):\n                batch_str = None\n                if len(split_response) - idx >= 10:\n                    batch_str = \"\".join(split_response[idx : idx + 10])\n                else:\n                    batch_str = \"\".join(split_response[idx : len(split_response)])\n                batch_responses.append(batch_str)\n            embeddings = model.encode(batch_responses, convert_to_tensor=True)\n            cosine_scores = util.cos_sim(embeddings, embeddings)\n            duplicate_idx = []\n            for row_idx, row in enumerate(cosine_scores):\n                for i, score in enumerate(row):\n                    if score >= self.variance and score < 0.99:\n                        duplicate_idx.append((row_idx, i))\n\n            deleted_idx = []\n            for duplicate in duplicate_idx:\n                example_idx, match_idx = duplicate\n                if example_idx in deleted_idx or match_idx in deleted_idx:\n                    continue\n                else:\n                    split_idx = (example_idx + 1) * 10\n                    if split_idx < len(split_response):\n                        del split_response[split_idx - 10 : split_idx]\n                    else:\n                        del split_response[split_idx - 10 : len(split_response)]\n                    deleted_idx.append(example_idx)\n\n            print(\n                f\"Found {len(split_response)} valid examples. Removed {num_responses - len(split_response)} duplicate examples.\",\n                file=sys.stderr,\n            )\n        datum_failure = 0\n        bad_key_failure =0\n        resp_filter = {}\n\n        for d in split_response:\n            print(d)\n\n            try:\n                the_data = json.loads(d.replace(\"\\n\",\"\"))\n                the_tuple = (the_data[\"prompt\"],the_data[\"response\"])\n                if the_tuple in resp_filter:\n                    continue   # dont save a response if its already happened\n                resp_filter[the_tuple]=True  # for now we're treating the (Q,A) pair as a single value\n                datapoints.append(the_tuple)\n            except json.decoder.JSONDecodeError as err:\n                print(F'data_synthesis response parsing failed with: { err } \\nExpected a valid JSON Object but received {type(d)} of length {len(d)}',file=sys.stderr)\n                datum_failure+=1\n            except LookupError as err : # i have no evidence that this will happen\n                print(F'data_synthesis key lookup failed with: { err }',file=sys.stderr)\n                bad_key_failure +=1\n        print(F'Out of { len(split_response)} response objects, {datum_failure} were not valid json \\n\\\n            and {bad_key_failure} were missing a key',file=sys.stderr)\n        return datapoints", ""]}
{"filename": "src/llm_vm/completion/optimize.py", "chunked_list": ["import openai\nimport traceback\nimport threading\nimport time\nimport os\nimport sys\nimport signal\nimport json\nimport tempfile\nimport abc", "import tempfile\nimport abc\nimport requests\nimport hashlib\nimport pickle\n#we need to package-ify so this works\nimport llm_vm.completion.data_synthesis as data_synthesis\nimport inspect\n\n", "\n\njob_id = None # we want to be able to cancel a fine_tune if you kill the program\n\ndef exit_handler(signum, frame):\n\n    if (None != job_id):\n        print(\"cancelling fine-tune if applicable\")\n        openai.FineTune.cancel(id=job_id)\n\n    print(\"user interrupt, exiting\")\n    sys.exit()", "\nsignal.signal(signal.SIGINT, exit_handler)\n\n\ndef generate_hash(input_string):\n    sha256_hash = hashlib.sha256()\n    sha256_hash.update(str(input_string).encode('utf-8'))\n    return int(sha256_hash.hexdigest(), 16) % 10**18\n\ndef asyncStart(foo):\n    t = [None, None]\n    def new_thread():\n        t[0] = foo()\n    t[1] = threading.Thread(target=new_thread)\n    t[1].start()\n    return t", "\ndef asyncStart(foo):\n    t = [None, None]\n    def new_thread():\n        t[0] = foo()\n    t[1] = threading.Thread(target=new_thread)\n    t[1].start()\n    return t\n\ndef asyncAwait(t):\n    t[1].join()\n    return t[0]", "\ndef asyncAwait(t):\n    t[1].join()\n    return t[0]\n\n\nclass local_ephemeral:\n\n    def __init__(self):\n        self.training_store = {}\n\n    def get_data(self, c_id):\n        self.init_if_null(c_id)\n        return self.training_store[c_id][\"data\"]\n\n    def load_data(self,file):\n        #using pickle files right now, in the future we will have to use databases\n        self.training_store = pickle.load(file)\n\n    def store_data(self,file):\n         #using pickle files right now, in the future we will have to use databases\n        pickle.dump(self.training_store,file)\n\n    def add_example(self, c_id, example):\n        self.init_if_null(c_id)\n        self.training_store[c_id][\"data\"] += [example]\n\n    def set_training_in_progress(self, c_id, is_training):\n        self.init_if_null(c_id)\n        self.training_store[c_id][\"is_training\"] = is_training\n\n    def get_training_in_progress_set_true(self, c_id):\n        self.init_if_null(c_id)\n        # TODO: this could cause a concurrency bug when distributed!\n        self.training_store[c_id]['lock'].acquire()\n        old_val = self.training_store[c_id][\"is_training\"]\n        if not old_val:\n            self.training_store[c_id][\"is_training\"] = True\n        self.training_store[c_id]['lock'].release()\n        return old_val\n\n    def set_model(self, c_id, model_id):\n        self.init_if_null(c_id)\n        self.training_store[c_id][\"model\"] = model_id\n\n    def get_model(self, c_id):\n        self.init_if_null(c_id)\n        return self.training_store[c_id][\"model\"]\n\n    def init_if_null(self, c_id):\n        if not c_id in self.training_store:\n            self.training_store[c_id] = { \"is_training\": False,\n                                          'lock' : threading.Lock(),\n                                          \"data\": [],\n                                          \"model\": None }", "\n\n\nclass Optimizer:\n    @abc.abstractmethod\n    def complete(self, stable_context, dynamic_prompt, **kwargs):\n        \"\"\"\n        Runs a completion using the string stable_context+dynamic_prompt.  Returns an optional training closure to use if the\n        caller decides that the completion was particularly good.\n\n        This method first checks if a model exists for the stable_context. If it does, it uses the model to complete the prompt.\n        It then checks if the number of training examples is less than the maximum allowable. If it is, or if a model wasn't\n        previously found, it retrieves the best completion for the prompt using a larger model, adds a new datapoint for training,\n        and potentially fine-tunes a new model using the updated data, storing the new model if successful.\n\n        The function returns the best completion (either generated by the stored model or the larger model).\n\n        This can not handle cases where either stable_context or dynamic_prompt are just whitespace!\n\n        Parameters:\n        ----------\n        - stable_context (str): Stable contextual data to use as a basis for training.\n        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\n        Returns:\n        ----------\n        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n        \"\"\"", "\n\nclass HostedOptimizer(Optimizer):\n    def __init__(self, anarchy_key, openai_key, MIN_TRAIN_EXS=20, MAX_TRAIN_EXS = 2000):\n        self.anarchy_key = anarchy_key\n        self.openai_key = openai_key\n        self.MIN_TRAIN_EXS = MIN_TRAIN_EXS\n        self.MAX_TRAIN_EXS = MAX_TRAIN_EXS\n\n    def complete(self, stable_context, dynamic_prompt, **kwargs):\n        \"\"\"\n        TODO: Runs the optimizing completion process on anarchy's hosted server with persistence.\n\n        Parameters:\n        ----------\n        - stable_context (str): Stable contextual data to use as a basis for training.\n        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\n        Returns:\n        ----------\n        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n        \"\"\"\n        url = \"https://api.chat.dev/completion/optimizing\"\n        payload = {**kwargs,\n                   'stable_context': stable_context,\n                   'dynamic_prompt': dynamic_prompt,\n                   'anarchy_key' : self.anarchy_key,\n                   'openai_key' : self.openai_key,\n                   'MIN_TRAIN_EXS' : self.MIN_TRAIN_EXS,\n                   'MAX_TRAIN_EXS' : self.MAX_TRAIN_EXS\n                   }\n        headers = {'Authorization': f'Bearer {self.anarchy_key}'}\n\n        print(\"Payload: \", payload)\n        try:\n            response = requests.post(url, json=payload, headers=headers)\n            response.raise_for_status()  # Raise an exception for 4XX and 5XX status codes\n            return response.json()['completion']\n        except requests.exceptions.RequestException as e:\n            print(\"Error occurred:\", e)", "\n\nclass LocalOptimizer(Optimizer):\n    def __init__(self, storage=local_ephemeral(), MIN_TRAIN_EXS = 1, MAX_TRAIN_EXS = 2000, call_small = None , call_big = None , big_model = None, small_model = None, openai_key=\"\"):\n        self.storage = storage\n        self.MIN_TRAIN_EXS = MIN_TRAIN_EXS\n        self.MAX_TRAIN_EXS = MAX_TRAIN_EXS\n        self.call_small = call_small\n        self.call_big = call_big\n        self.big_model = big_model\n        self.small_model = small_model\n        self.openai_key = openai_key\n        self.data_synthesizer = data_synthesis.DataSynthesis(0.87, 50)\n\n    def complete(self, stable_context, dynamic_prompt, data_synthesis = False, finetune = False, **kwargs):\n        openai.api_key = self.openai_key\n        completion, train = self.complete_delay_train(stable_context, dynamic_prompt, run_data_synthesis=data_synthesis, **kwargs)\n        print(finetune,flush=True)\n        if finetune:\n            train()\n        return completion\n\n    def complete_delay_train(self, stable_context, dynamic_prompt, run_data_synthesis = False, min_examples_for_synthesis = 1 ,c_id = None, **kwargs):\n        \"\"\"\n        Runs a completion using the string stable_context+dynamic_prompt.  Returns an optional training closure to use if the\n        caller decides that the completion was particularly good.\n\n        This method first checks if a model exists for the stable_context. If it does, it uses the model to complete the prompt.\n        It then checks if the number of training examples is less than the maximum allowable. If it is, or if a model wasn't\n        previously found, it retrieves the best completion for the prompt using a larger model, adds a new datapoint for training,\n        and potentially fine-tunes a new model using the updated data, storing the new model if successful.\n\n        The function returns the best completion (either generated by the stored model or the larger model), and a closure\n        function that encapsulates the fine-tuning process for potential execution at a later time.\n\n        Parameters:\n        ----------\n        - stable_context (str): Stable contextual data to use as a basis for training.\n        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n        - c_id (str): To be used if multiple users could share the same stable_contexts so that we don't leak data.  If its None, defaults to all possible context.\n        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\n        Returns:\n        ----------\n        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n        - succeed_train (function): A closure function that encapsulates the fine-tuning process, ready for\n        execution at a later time.  If you pass it a completion, it will use that, otherwise it will use the completion from the \"best\" model.\n        \"\"\"\n        assert dynamic_prompt.strip() != \"\" or stable_context.strip() != \"\"\n        assert self.call_big is not None and self.call_small is not None and self.big_model is not None and self.small_model is not None\n        if stable_context.strip() == \"\" :\n            print(\"Running with an empty context\")\n\n        prompt = (stable_context + dynamic_prompt).strip()\n        c_id_repr = str({'stable_context' : stable_context,\n                    'args' : kwargs,\n                    'MIN_TRAIN_EXS' : self.MIN_TRAIN_EXS,\n                    'MAX_TRAIN_EXS' : self.MAX_TRAIN_EXS,\n                    'call_small' : str(self.small_model).split(' ')[0], # HACKS\n                    'call_big' : str(self.big_model).split(' ')[0],\n                    }) if c_id is None else c_id\n        c_id = generate_hash(c_id_repr)\n        completion = None\n\n        model = self.storage.get_model(c_id)\n        # this gives us the model_id\n        if model is not None:\n            print(\"Using the new model:\", model, flush=True)\n            completion = self.call_small(prompt = dynamic_prompt.strip(), model=model, **kwargs)\n\n        training_exs = self.storage.get_data(c_id)\n\n        best_completion_promise = None\n        succeed_train = None\n        if len(training_exs) < self.MAX_TRAIN_EXS:\n            def promiseCompletion():\n                best_completion = self.call_big(prompt, **kwargs)\n\n                def actual_train(use_completion = None):\n\n                    train_completion = best_completion if use_completion is None else use_completion\n                    new_datapoint = (dynamic_prompt.strip(), train_completion)\n                    self.storage.add_example(c_id, new_datapoint)\n\n                    if run_data_synthesis:\n                        if len(self.storage.get_data(c_id)) < min_examples_for_synthesis:\n                            print(\"Data synthesis is not available right now, need more examples in storage.\")\n                        else:\n                            for j in self.data_synthesizer.data_synthesis(self,prompt,best_completion,openai_key=self.openai_key, **kwargs):\n                                self.storage.add_example(c_id, j)\n                    training_exs = self.storage.get_data(c_id)\n                    print(training_exs)\n                    print(\"Considering Fine-tuning\", flush=True)\n\n                    if len(training_exs) >= self.MIN_TRAIN_EXS and not self.storage.get_training_in_progress_set_true(c_id):\n                        print(\"Actually Fine-tuning\", flush=True)\n                        print(\"Training examples:\",str(len(training_exs)))\n                        asyncStart(self.small_model.finetune(training_exs,self,c_id))\n                return (best_completion, actual_train)\n\n            best_completion_promise = asyncStart(promiseCompletion)\n\n            if completion is None:\n                # crazy story: succeed_train gets set before this anyway if it makes sense to set it!\n                completion, succeed_train = asyncAwait(best_completion_promise)\n\n            else:\n                _, succeed_train = asyncAwait(best_completion_promise)\n            print(completion)\n\n\n        def succeed_train_closure(use_completion = None):\n            def promise():\n                if succeed_train is not None:\n                    return succeed_train(use_completion)\n                if best_completion_promise is not None:\n                    try:\n                        return asyncAwait(best_completion)[1](use_completion)\n                    except:\n                        return\n            return asyncStart(promise)\n\n        return completion, succeed_train_closure", "\n\n\n\ndef create_jsonl_file(data_list):\n    out = tempfile.TemporaryFile('w+')\n    for a,b in data_list:\n        out.write(json.dumps({'prompt': a, 'completion': b}) + \"\\n\")\n    out.seek(0)\n    return out", "\n\n"]}
