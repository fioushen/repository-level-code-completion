{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nREQUIRES = \"\"\"\nruamel.yaml\npandas\nscipy\nscikit-learn\npyro-api==0.1.2\npyro-ppl==1.8.0\nnumba", "pyro-ppl==1.8.0\nnumba\n\"\"\"\n\ndef get_install_requires():\n    reqs = [req for req in REQUIRES.split(\"\\n\") if len(req) > 0]\n    return reqs\n\n\nwith open(\"README.md\", encoding=\"utf-8\") as f:\n    readme = f.read()", "\nwith open(\"README.md\", encoding=\"utf-8\") as f:\n    readme = f.read()\n\n\ndef do_setup():\n    setup(\n        name=\"opengsl\",\n        version=\"0.0.5\",\n        description=\"A comprehensive benchmark for Graph Structure Learning.\",\n        url=\"https://github.com/OpenGSL/OpenGSL\",\n        author='Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou',\n        long_description=readme,\n        long_description_content_type=\"text/markdown\",\n        install_requires=get_install_requires(),\n        python_requires=\">=3.7.0\",\n        packages=find_packages(),\n        include_package_data=True,\n        keywords=[\"AI\", \"GNN\", \"graph structure learning\"],\n        classifiers=[\n            \"Programming Language :: Python :: 3.7\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Science/Research\",\n        ]\n    )", "\n\nif __name__ == \"__main__\":\n    do_setup()"]}
{"filename": "paper/main_results.py", "chunked_list": ["import argparse\nimport os\nimport sys\nimport pandas as pd\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data', type=str, default='cora',\n                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',", "                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n                             'blogcatalog', 'flickr'], help='dataset')\nparser.add_argument('--method', type=str, default='gcn', choices=['gcn', 'appnp', 'gt', 'gat', 'prognn', 'gen',\n                                                                  'gaug', 'idgl', 'grcn', 'sgc', 'jknet', 'slaps',\n                                                                  'gprgnn', 'nodeformer', 'segsl', 'sublime',\n                                                                  'stable', 'cogsl', 'lpa', 'link', 'linkx', 'wsgnn'], help=\"Select methods\")\nparser.add_argument('--config', type=str, default=None)\nparser.add_argument('--debug', action='store_true')", "parser.add_argument('--config', type=str, default=None)\nparser.add_argument('--debug', action='store_true')\nparser.add_argument('--gpu', type=str, default='0', help=\"Visible GPU\")\nargs = parser.parse_args()\n\nos.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\nfrom opengsl.config import load_conf\nfrom opengsl.data import Dataset\nfrom opengsl import ExpManager", "from opengsl.data import Dataset\nfrom opengsl import ExpManager\nfrom opengsl.method import *\n\nif args.config is None:\n    conf = load_conf(method=args.method, dataset=args.data)\nelse:\n    conf = load_conf(args.config)\nconf.analysis['save_graph'] = False\nprint(conf)", "conf.analysis['save_graph'] = False\nprint(conf)\n\ndataset = Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\n\n\nmethod = eval('{}Solver(conf, dataset)'.format(args.method.upper()))\nexp = ExpManager(method,  save_path='records')\nacc_save, std_save = exp.run(n_runs=10, debug=args.debug)\n\nif not os.path.exists('results'):\n    os.makedirs('results')", "acc_save, std_save = exp.run(n_runs=10, debug=args.debug)\n\nif not os.path.exists('results'):\n    os.makedirs('results')\nif os.path.exists('results/performance.csv'):\n    records = pd.read_csv('results/performance.csv')\n    records.loc[len(records)] = {'method':args.method, 'data':args.data, 'acc':acc_save, 'std':std_save}\n    records.to_csv('results/performance.csv', index=False)\nelse:\n    records = pd.DataFrame([[args.method, args.data, acc_save, std_save]], columns=['method', 'data', 'acc', 'std'])\n    records.to_csv('results/performance.csv', index=False)"]}
{"filename": "paper/generalizability.py", "chunked_list": ["import argparse\nimport os\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data', type=str, default='cora',\n                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n                             'blogcatalog', 'flickr'], help='dataset')", "                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n                             'blogcatalog', 'flickr'], help='dataset')\nparser.add_argument('--gsl', type=str, default='grcn', choices=['gt', 'prognn', 'gen', 'gaug', 'idgl', 'grcn', 'slaps',  'nodeformer', 'segsl', 'sublime', 'stable', 'cogsl'], help=\"Select methods\")\nparser.add_argument('--gnn', type=str, default='gcn', choices=['gcn', 'sgc', 'jknet', 'appnp', 'gprgnn', 'lpa', 'link'])\nparser.add_argument('--debug', action='store_true')\nparser.add_argument('--gpu', type=str, default='0', help=\"Visible GPU\")\nargs = parser.parse_args()\nos.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\nimport opengsl", "\nimport opengsl\n\nconf = opengsl.config.load_conf(method=args.gnn, dataset=args.data)\n# specify some settings\nconf.analysis['load_graph'] = True\nconf.analysis['load_graph_path'] = 'results/graph/{}'.format(args.gsl)\nif args.gsl in ['sublime', 'idgl']:\n    conf.dataset['normalize'] = False\nelse:\n    conf.dataset['normalize'] = True", "if args.gsl in ['grcn', 'sublime', 'idgl']:\n    conf.dataset['add_loop'] = False\nelse:\n    conf.dataset['add_loop'] = True\nprint(conf)\ndataset = opengsl.data.Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\nmethod = eval('opengsl.method.{}(conf, dataset)'.format(args.gnn))\nexp = opengsl.ExpManager(method,  save_path='records')\nexp.run(n_runs=1, debug=args.debug)", "exp.run(n_runs=1, debug=args.debug)"]}
{"filename": "paper/homophily.py", "chunked_list": ["import argparse\nimport os\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data', type=str, default='cora',\n                    choices=['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho',\n                             'coauthorcs', 'coauthorph', 'amazon-ratings', 'questions', 'chameleon-filtered',\n                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n                             'blogcatalog', 'flickr'], help='dataset')", "                             'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'penn94',\n                             'blogcatalog', 'flickr'], help='dataset')\nparser.add_argument('--method', type=str, default='gcn', choices=['gcn', 'appnp', 'gt', 'gat', 'prognn', 'gen',\n                                                                  'gaug', 'idgl', 'grcn', 'sgc', 'jknet', 'slaps',\n                                                                  'gprgnn', 'nodeformer', 'segsl', 'sublime',\n                                                                  'stable', 'cogsl', 'lpa', 'link', 'linkx'], help=\"Select methods\")\nargs = parser.parse_args()\n\nimport opengsl\nimport numpy as np", "import opengsl\nimport numpy as np\nimport torch\n\nconf = opengsl.config.load_conf(method=args.method, dataset=args.data)\nprint(conf)\ndata = opengsl.data.Dataset(args.data, feat_norm=conf.dataset['feat_norm'], path='data')\n\nfill = None\nh = []", "fill = None\nh = []\nprint(opengsl.utils.get_homophily(data.labels.cpu(), data.adj.to_dense().cpu(), type='edge', fill=fill))\nfor i in range(10):\n    adj = torch.load(os.path.join('results/graph/{}'.format(args.method), '{}_{}_{}.pth'.format(args.data, 0, i)))\n    h.append(opengsl.utils.get_homophily(data.labels.cpu(), adj.cpu(), type='edge', fill=fill))\n    print(h)\nh = np.array(h)\nprint(f'{h.mean():.4f} \u00b1 {h.std():.4f}')", "print(f'{h.mean():.4f} \u00b1 {h.std():.4f}')"]}
{"filename": "opengsl/__init__.py", "chunked_list": ["from .expmanager.ExpManager import ExpManager\nfrom . import data as data\nfrom . import method as method\nfrom . import config as config\nfrom . import utils as utils"]}
{"filename": "opengsl/utils/logger.py", "chunked_list": ["import torch\n\nclass Logger(object):\n    \"\"\"\n    Logger Class.\n\n    Parameters\n    ----------\n    runs : int\n        Total experimental runs.\n    \"\"\"\n    def __init__(self, runs):\n        self.results = [[] for _ in range(runs)]\n\n    def add_result(self, run, result_dict):\n        '''\n        Add performance of a new run.\n\n        Parameters\n        ----------\n        run : int\n            Id of the new run.\n        result_dict : dict\n            A dict containing training, valid and test performances.\n\n        '''\n        assert \"train\" in result_dict.keys()\n        assert \"valid\" in result_dict.keys()\n        assert \"test\"  in result_dict.keys()\n        assert run >= 0 and run < len(self.results)\n        self.results[run].append(result_dict[\"train\"])\n        self.results[run].append(result_dict[\"valid\"])\n        self.results[run].append(result_dict[\"test\"])\n\n    def print_statistics(self, run=None):\n        '''\n        Function to output the statistics.\n\n        Parameters\n        ----------\n        run : int\n            Id of a run. If not specified, output the statistics of all runs.\n\n        Returns\n        -------\n            The statistics of a given run or all runs.\n\n        '''\n        if run is not None:\n            result = 100 * torch.tensor(self.results[run])\n            print(f'Run {run + 1:02d}:')\n            print(f'Highest Train: {result[0]:.2f}')\n            print(f'Highest Valid: {result[1]:.2f}')\n            print(f'   Final Test: {result[2]:.2f}')\n            return  result[2]\n        else:\n            best_result = 100 * torch.tensor(self.results)\n\n            print(f'All runs:')\n            r = best_result[:, 0]\n            print(f'Highest Train: {r.mean():.2f} \u00b1 {r.std():.2f}')\n            r = best_result[:, 1]\n            print(f'Highest Valid: {r.mean():.2f} \u00b1 {r.std():.2f}')\n            r = best_result[:, 2]\n            print(f'   Final Test: {r.mean():.2f} \u00b1 {r.std():.2f}')\n            return r.mean(), r.std()"]}
{"filename": "opengsl/utils/__init__.py", "chunked_list": ["from opengsl.utils.utils import accuracy\nfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor\nfrom opengsl.utils.utils import sparse_tensor_to_scipy_sparse\nfrom opengsl.utils.utils import set_seed\nfrom opengsl.utils.utils import get_node_homophily\nfrom opengsl.utils.utils import get_edge_homophily\nfrom opengsl.utils.utils import get_homophily\nfrom opengsl.utils.utils import get_adjusted_homophily\nfrom opengsl.utils.utils import get_label_informativeness", "from opengsl.utils.utils import get_label_informativeness"]}
{"filename": "opengsl/utils/utils.py", "chunked_list": ["import dgl.random\nimport torch\nimport os\nimport numpy as np\nimport scipy.sparse as sp\nimport random\n\n\ndef accuracy(labels, logits):\n    '''\n    Compute the accuracy score given true labels and predicted labels.\n\n    Parameters\n    ----------\n    labels: np.array\n        Ground truth labels.\n    logits : np.array\n        Predicted labels.\n\n    Returns\n    -------\n    accuracy : np.float\n        The Accuracy score.\n\n    '''\n    return np.sum(logits.argmax(1)==labels)/len(labels)", "def accuracy(labels, logits):\n    '''\n    Compute the accuracy score given true labels and predicted labels.\n\n    Parameters\n    ----------\n    labels: np.array\n        Ground truth labels.\n    logits : np.array\n        Predicted labels.\n\n    Returns\n    -------\n    accuracy : np.float\n        The Accuracy score.\n\n    '''\n    return np.sum(logits.argmax(1)==labels)/len(labels)", "\n\ndef scipy_sparse_to_sparse_tensor(sparse_mx):\n    '''\n    Convert a scipy sparse matrix to a torch sparse tensor.\n\n    Parameters\n    ----------\n    sparse_mx : scipy.sparse_matrix\n        Sparse matrix to convert.\n\n    Returns\n    -------\n    sparse_tensor: torch.Tensor in sparse form\n        A tensor stored in sparse form.\n    '''\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)", "\n\ndef sparse_tensor_to_scipy_sparse(sparse_tensor):\n    '''\n    Convert a torch sparse tensor to a scipy sparse matrix.\n\n    Parameters\n    ----------\n    sparse_tensor : torch.Tensor in sparse form\n        A tensor stored in sparse form to convert.\n\n    Returns\n    -------\n    sparse_mx : scipy.sparse_matrix\n        Sparse matrix.\n\n    '''\n    sparse_tensor = sparse_tensor.cpu()\n    row = sparse_tensor.coalesce().indices()[0].numpy()\n    col = sparse_tensor.coalesce().indices()[1].numpy()\n    values = sparse_tensor.coalesce().values().numpy()\n    return sp.coo_matrix((values, (row, col)), shape=sparse_tensor.shape)", "\n\ndef set_seed(seed):\n    '''\n    Set seed to make sure the results can be repetitive.\n\n    Parameters\n    ----------\n    seed : int\n        Random seed to set.\n    '''\n    dgl.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False", "\n\ndef get_node_homophily(label, adj):\n    '''\n    Calculate the node homophily of a graph.\n\n    Parameters\n    ----------\n    label : torch.tensor\n        The ground truth labels.\n    adj : torch.tensor\n        The adjacency matrix in dense form.\n\n    Returns\n    -------\n    homophily : torch.float\n        The node homophily of the graph.\n\n    '''\n    label = label.cpu().numpy()\n    adj = adj.cpu().numpy()\n    num_node = len(label)\n    label = label.repeat(num_node).reshape(num_node, -1)\n    n = (np.multiply((label == label.T), adj)).sum(axis=1)\n    d = adj.sum(axis=1)\n    homos = []\n    for i in range(num_node):\n        if d[i] > 0:\n            homos.append(n[i] * 1. / d[i])\n    return np.mean(homos)", "\n\ndef get_edge_homophily(label, adj):\n    '''\n    Calculate the node homophily of a graph.\n\n    Parameters\n    ----------\n    label : torch.tensor\n        The ground truth labels.\n    adj : torch.tensor\n        The adjacency matrix in dense form.\n\n    Returns\n    -------\n    homophily : torch.float\n        The edge homophily of the graph.\n\n    '''\n    num_edge = adj.sum()\n    cnt = 0\n    for i, j in adj.nonzero():\n        if label[i] == label[j]:\n            cnt += adj[i, j]\n    return cnt/num_edge", "\n\ndef get_homophily(label, adj, type='node', fill=None):\n    '''\n    Calculate node or edge homophily of a graph.\n\n    Parameters\n    ----------\n    label : torch.tensor\n        The ground truth labels.\n    adj : torch.tensor\n        The adjacency matrix in dense form.\n    type : str\n        This decides whether to calculate node homo or edge homo.\n    fill : str\n        The value to fill in the diagonal of `adj`. If set to `None`, the operation won't be done.\n\n    Returns\n    -------\n    homophily : np.float\n        The node or edge homophily of a graph.\n\n    '''\n    if fill:\n        np.fill_diagonal(adj, fill)\n    return eval('get_'+type+'_homophily(label, adj)')", "\n\ndef get_adjusted_homophily(_label, adj):\n    '''\n    Calculate adjusted homophily of a graph.\n\n    Parameters\n    ----------\n    _label : torch.tensor\n        The ground truth labels.\n    adj : torch.tensor\n        The adjacency matrix in dense form.\n\n    Returns\n    -------\n    homophily : np.float\n        The adjusted homophily of a graph.\n\n    '''\n    label = _label.long()\n    labels = label.max() + 1\n    d = adj.sum(1)\n    E = d.sum()\n    D = torch.zeros(labels)\n    for i in range(adj.shape[0]):\n        D[label[i]] += d[i]\n\n    h_edge = get_edge_homophily(label, adj)\n    sum_pk = ((D / E) ** 2).sum()\n\n    return (h_edge - sum_pk) / (1 - sum_pk)", "\n\ndef get_label_informativeness(_label, adj):\n    '''\n    Calculate label informativeness of a graph.\n\n    Parameters\n    ----------\n    _label : torch.tensor\n        The ground truth labels.\n    adj : torch.tensor\n        The adjacency matrix in dense form.\n\n    Returns\n    -------\n    label_informativeness : np.float\n        The label informativeness of a graph.\n    '''\n    label = _label.long()\n    labels = label.max() + 1\n    LI_1 = 0\n    LI_2 = 0\n\n    p = torch.zeros((labels, labels))\n    for i, j in adj.nonzero():\n        p[label[i]][label[j]] = p[label[i]][label[j]] + adj[i][j]\n\n    d = adj.sum(1)\n    E = d.sum()\n    D = torch.zeros(labels)\n\n    for i in range(adj.shape[0]):\n        D[label[i]] = D[label[i]] + d[i]\n\n    for i in range(labels):\n        for j in range(labels):\n            p[i][j] = p[i][j] / E\n\n    p_ = D / E\n    LI_2 = (p_ * torch.log(p_)).sum()\n    for i in range(labels):\n        for j in range(labels):\n            if (p[i][j] != 0):\n                LI_1 += p[i][j] * torch.log(p[i][j] / (p_[i] * p_[j]))\n\n    return -LI_1 / LI_2"]}
{"filename": "opengsl/utils/recorder.py", "chunked_list": ["class Recorder:\n    \"\"\"\n    Recorder Class.\n\n    This records the performances of epochs in a single run. It determines whether the training has improved based\n    on the provided `criterion` and determines whether the earlystop `patience` has been achieved.\n\n    Parameters\n    ----------\n    patience : int\n        The maximum epochs to keep training since last improvement.\n    criterion : str\n        The criterion to determine whether whether the training has improvement.\n        - ``None``: Improvement will be considered achieved in any case.\n        - ``loss``: Improvement will be considered achieved when loss decreases.\n        - ``metric``: Improvement will be considered achieved when metric increases.\n        - ``either``: Improvement will be considered achieved if either loss decreases or metric increases.\n        - ``both``: Improvement will be considered achieved if both loss decreases and metric increases.\n    \"\"\"\n    def __init__(self, patience=100, criterion=None):\n        self.patience = patience\n        self.criterion = criterion\n        self.best_loss = 100\n        self.best_metric = 0\n        self.wait = 0\n\n    def add(self, loss_val, metric_val):\n        '''\n        Function to add the loss and metric of a new epoch.\n\n        Parameters\n        ----------\n        loss_val : float\n        metric_val : float\n\n        Returns\n        -------\n        flag : bool\n            Whether improvement has been achieved in the epoch.\n        flag_earlystop: bool\n            Whether training needs earlystopping.\n        '''\n        flag = False\n        if self.criterion is None:\n            flag = True\n        elif self.criterion == 'loss':\n            flag = loss_val < self.best_loss\n        elif self.criterion == 'metric':\n            flag = metric_val > self.best_metric\n        elif self.criterion == 'either':\n            flag = loss_val < self.best_loss or metric_val > self.best_metric\n        elif self.criterion == 'both':\n            flag = loss_val < self.best_loss and metric_val > self.best_metric\n        else:\n            raise NotImplementedError\n\n        if flag:\n            self.best_metric = metric_val\n            self.best_loss = loss_val\n            self.wait = 0\n        else:\n            self.wait += 1\n\n        flag_earlystop = self.patience and self.wait >= self.patience\n\n        return flag, flag_earlystop"]}
{"filename": "opengsl/config/__init__.py", "chunked_list": ["from opengsl.config.util import load_conf, save_conf"]}
{"filename": "opengsl/config/util.py", "chunked_list": ["import ruamel.yaml as yaml\nimport argparse\nimport os\n\n\ndef load_conf(path:str = None, method:str = None, dataset:str = None):\n    '''\n    Function to load config file.\n\n    Parameters\n    ----------\n    path : str\n        Path to load config file. Load default configuration if set to `None`.\n    method : str\n        Name of the used mathod. Necessary if ``path`` is set to `None`.\n    dataset : str\n        Name of the corresponding dataset. Necessary if ``path`` is set to `None`.\n\n    Returns\n    -------\n    conf : argparse.Namespace\n        The config file converted to Namespace.\n\n    '''\n    if path == None and method == None:\n        raise KeyError\n    if path == None and dataset == None:\n        raise KeyError\n    if path == None:\n        method_name = ['gcn', 'sgc', 'gat', 'jknet', 'appnp', 'gprgnn', 'prognn', 'idgl', 'grcn', 'gaug', 'slaps',\n                       'gen', 'gt', 'nodeformer', 'cogsl', 'sublime', 'stable', 'segsl', 'lpa', 'link', 'wsgnn']\n        data_name = ['cora', 'pubmed', 'citeseer','blogcatalog', 'flickr', 'amazon-ratings', 'questions', 'minesweeper', 'roman-empire', 'wiki-cooc']\n\n        assert method in method_name\n        assert dataset in data_name\n        dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"config\")\n        if method in [\"link\", \"lpa\"]:\n            path = os.path.join(dir, method, method+\".yaml\")\n        else:\n            path = os.path.join(dir, method, method+'_'+dataset+\".yaml\")\n\n        if os.path.exists(path) == False:\n            raise KeyError(\"The configuration file is not provided.\")\n    \n    conf = open(path, \"r\").read()\n    conf = yaml.safe_load(conf)\n    conf = argparse.Namespace(**conf)\n\n    return conf", "\n\ndef save_conf(path, conf):\n    '''\n    Function to save the config file.\n\n    Parameters\n    ----------\n    path : str\n        Path to save config file.\n    conf : dict\n        The config dict.\n\n    '''\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(vars(conf), f)"]}
{"filename": "opengsl/data/__init__.py", "chunked_list": ["from .dataset.dataset import Dataset\nfrom . import preprocess as preprocess"]}
{"filename": "opengsl/data/dataset/hetero_load.py", "chunked_list": ["import numpy as np\nimport os\nimport torch\nimport dgl\nimport urllib.request\n\ndef hetero_load(name, path='./data/hetero_data'):\n    file_name = f'{name.replace(\"-\", \"_\")}.npz'\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(os.path.join(path, file_name)):\n        download(file_name, path)\n    data = np.load(os.path.join(path, f'{name.replace(\"-\", \"_\")}.npz'))\n    node_features = torch.tensor(data['node_features'])\n    labels = torch.tensor(data['node_labels'])\n    edges = torch.tensor(data['edges'])\n    train_masks = torch.tensor(data['train_masks'])\n    val_masks = torch.tensor(data['val_masks'])\n    test_masks = torch.tensor(data['test_masks'])\n\n    train_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in train_masks]\n    val_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in val_masks]\n    test_indices = [torch.nonzero(x, as_tuple=False).squeeze().numpy() for x in test_masks]\n\n\n    n_nodes = node_features.shape[0]\n    graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(node_features), idtype=torch.long)\n    graph = dgl.to_bidirected(graph)\n    adj = graph.adj()\n\n    num_classes = len(labels.unique())\n    num_targets = 1 if num_classes == 2 else num_classes\n    if num_targets == 1:\n        labels = labels.float()\n\n\n    return node_features, adj, labels, (train_indices, val_indices, test_indices)", "\ndef download(name, path):\n    url = 'https://github.com/OpenGSL/HeterophilousDatasets/raw/main/data/'\n    try:\n        print('Downloading', url+name)\n        urllib.request.urlretrieve(url + name, os.path.join(path, name))\n        print('Done!')\n    except:\n        raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n\nif __name__ == '__main__':\n    print(hetero_load('minesweeper', 'tmp'))", "\nif __name__ == '__main__':\n    print(hetero_load('minesweeper', 'tmp'))"]}
{"filename": "opengsl/data/dataset/pyg_load.py", "chunked_list": ["'''\nload data via pyg\n'''\n\nfrom torch_geometric.datasets import Planetoid, Amazon, Coauthor, WikiCS, WikipediaNetwork, WebKB, Actor, AttributedGraphDataset\nimport os\n\n\ndef pyg_load_dataset(name, path='./data/'):\n    dic = {'cora': 'Cora',\n           'citeseer': 'CiteSeer',\n           'pubmed': 'PubMed',\n           'amazoncom': 'Computers',\n           'amazonpho': 'Photo',\n           'coauthorcs': 'CS',\n           'coauthorph': 'Physics',\n           'wikics': 'WikiCS',\n           'chameleon': 'Chameleon',\n           'squirrel': 'Squirrel',\n           'cornell': 'Cornell',\n           'texas': 'Texas',\n           'wisconsin': 'Wisconsin',\n           'actor': 'Actor',\n           'blogcatalog':'blogcatalog',\n           'flickr':'flickr'}\n    name = dic[name]\n\n    if name in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n        dataset = Planetoid(root=os.path.join(path, name), name=name)\n    elif name in [\"Computers\", \"Photo\"]:\n        dataset = Amazon(root=os.path.join(path, name), name=name)\n    elif name in [\"CS\", \"Physics\"]:\n        dataset = Coauthor(root=os.path.join(path, name), name=name)\n    elif name in ['WikiCS']:\n        dataset = WikiCS(root=os.path.join(path, name))\n    elif name in ['Chameleon', 'Squirrel', 'Crocodile']:\n        dataset = WikipediaNetwork(root=os.path.join(path, name), name=name)\n    elif name in ['Cornell', 'Texas', 'Wisconsin']:\n        dataset = WebKB(root=os.path.join(path, name), name=name)\n    elif name == 'Actor':\n        dataset = Actor(root=os.path.join(path, name))\n    elif name in ['blogcatalog', 'flickr']:\n        dataset = AttributedGraphDataset(root=os.path.join(path, name), name=name)\n    else:\n        exit(\"wrong dataset\")\n    return dataset", "def pyg_load_dataset(name, path='./data/'):\n    dic = {'cora': 'Cora',\n           'citeseer': 'CiteSeer',\n           'pubmed': 'PubMed',\n           'amazoncom': 'Computers',\n           'amazonpho': 'Photo',\n           'coauthorcs': 'CS',\n           'coauthorph': 'Physics',\n           'wikics': 'WikiCS',\n           'chameleon': 'Chameleon',\n           'squirrel': 'Squirrel',\n           'cornell': 'Cornell',\n           'texas': 'Texas',\n           'wisconsin': 'Wisconsin',\n           'actor': 'Actor',\n           'blogcatalog':'blogcatalog',\n           'flickr':'flickr'}\n    name = dic[name]\n\n    if name in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n        dataset = Planetoid(root=os.path.join(path, name), name=name)\n    elif name in [\"Computers\", \"Photo\"]:\n        dataset = Amazon(root=os.path.join(path, name), name=name)\n    elif name in [\"CS\", \"Physics\"]:\n        dataset = Coauthor(root=os.path.join(path, name), name=name)\n    elif name in ['WikiCS']:\n        dataset = WikiCS(root=os.path.join(path, name))\n    elif name in ['Chameleon', 'Squirrel', 'Crocodile']:\n        dataset = WikipediaNetwork(root=os.path.join(path, name), name=name)\n    elif name in ['Cornell', 'Texas', 'Wisconsin']:\n        dataset = WebKB(root=os.path.join(path, name), name=name)\n    elif name == 'Actor':\n        dataset = Actor(root=os.path.join(path, name))\n    elif name in ['blogcatalog', 'flickr']:\n        dataset = AttributedGraphDataset(root=os.path.join(path, name), name=name)\n    else:\n        exit(\"wrong dataset\")\n    return dataset"]}
{"filename": "opengsl/data/dataset/dataset.py", "chunked_list": ["import torch\nfrom .pyg_load import pyg_load_dataset\nfrom .hetero_load import hetero_load\nfrom .split import get_split\nfrom opengsl.data.preprocess.normalize import normalize\nimport numpy as np\nfrom opengsl.data.preprocess.control_homophily import control_homophily\nimport pickle\nimport os\nimport urllib.request", "import os\nimport urllib.request\n\n\nclass Dataset:\n    '''\n    Dataset Class.\n    This class loads, preprocesses and splits various datasets.\n\n    Parameters\n    ----------\n    data : str\n        The name of dataset.\n    feat_norm : bool\n        Whether to normalize the features.\n    verbose : bool\n        Whether to print statistics.\n    n_splits : int\n        Number of data splits.\n    homophily_control : float\n        The homophily ratio `control homophily` receives. If set to `None`, the original adj will be kept unchanged.\n    path : str\n        Path to save dataset files.\n    '''\n\n    def __init__(self, data, feat_norm=False, verbose=True, n_splits=1, homophily_control=None, path='./data/'):\n        self.name = data\n        self.path = path\n        self.device = torch.device('cuda')\n        self.prepare_data(data, feat_norm, verbose)\n        self.split_data(n_splits, verbose)\n        if homophily_control:\n            self.adj = control_homophily(self.adj, self.labels.cpu().numpy(), homophily_control)\n\n    def prepare_data(self, ds_name, feat_norm=False, verbose=True):\n        '''\n        Function to Load various datasets.\n        Homophilous datasets are loaded via pyg, while heterophilous datasets are loaded with `hetero_load`.\n        The results are saved as `self.feats, self.adj, self.labels, self.train_masks, self.val_masks, self.test_masks`.\n        Noth that `self.adj` is undirected and has no self loops.\n\n        Parameters\n        ----------\n        ds_name : str\n            The name of dataset.\n        feat_norm : bool\n            Whether to normalize the features.\n        verbose : bool\n            Whether to print statistics.\n\n        '''\n        if ds_name in ['cora', 'pubmed', 'citeseer', 'amazoncom', 'amazonpho', 'coauthorcs', 'coauthorph', 'blogcatalog',\n                       'flickr']:\n            self.data_raw = pyg_load_dataset(ds_name, path=self.path)\n            self.g = self.data_raw[0]\n            self.feats = self.g.x  # unnormalized\n            if ds_name == 'flickr':\n                self.feats = self.feats.to_dense()\n            self.n_nodes = self.feats.shape[0]\n            self.dim_feats = self.feats.shape[1]\n            self.labels = self.g.y\n            self.adj = torch.sparse.FloatTensor(self.g.edge_index, torch.ones(self.g.edge_index.shape[1]),\n                                                [self.n_nodes, self.n_nodes])\n            self.n_edges = self.g.num_edges/2\n            self.n_classes = self.data_raw.num_classes\n\n            self.feats = self.feats.to(self.device)\n            self.labels = self.labels.to(self.device)\n            self.adj = self.adj.to(self.device)\n            # normalize features\n            if feat_norm:\n                self.feats = normalize(self.feats, style='row')\n\n        elif ds_name in ['amazon-ratings', 'questions', 'chameleon-filtered', 'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'tolokers']:\n            self.feats, self.adj, self.labels, self.splits = hetero_load(ds_name, path=self.path)\n\n            self.feats = self.feats.to(self.device)\n            self.labels = self.labels.to(self.device)\n            self.adj = self.adj.to(self.device)\n            self.n_nodes = self.feats.shape[0]\n            self.dim_feats = self.feats.shape[1]\n            self.n_edges = len(self.adj.coalesce().values())/2\n            if feat_norm:\n                self.feats = normalize(self.feats, style='row')\n                # exit(0)\n            self.n_classes = len(self.labels.unique())\n\n        else:\n            print('dataset not implemented')\n            exit(0)\n\n        if verbose:\n            print(\"\"\"----Data statistics------'\n                #Nodes %d\n                #Edges %d\n                #Classes %d\"\"\" %\n                  (self.n_nodes, self.n_edges, self.n_classes))\n\n        self.num_targets = self.n_classes\n        if self.num_targets == 2:\n            self.num_targets = 1\n\n    def split_data(self, n_splits, verbose=True):\n        '''\n        Function to conduct data splitting for various datasets.\n\n        Parameters\n        ----------\n        n_splits : int\n            Number of data splits.\n        verbose : bool\n            Whether to print statistics.\n\n        '''\n        self.train_masks = []\n        self.val_masks = []\n        self.test_masks = []\n        if self.name in ['blogcatalog', 'flickr']:\n            def load_obj(file_name):\n                with open(file_name, 'rb') as f:\n                    return pickle.load(f)\n            def download(name):\n                url = 'https://github.com/zhao-tong/GAug/raw/master/data/graphs/'\n                try:\n                    print('Downloading', url + name)\n                    urllib.request.urlretrieve(url + name, os.path.join(self.path, name))\n                    print('Done!')\n                except:\n                    raise Exception(\n                        '''Download failed! Make sure you have stable Internet connection and enter the right name''')\n\n            split_file = self.name + '_tvt_nids.pkl'\n            if not os.path.exists(os.path.join(self.path, split_file)):\n                download(split_file)\n            train_indices, val_indices, test_indices = load_obj(os.path.join(self.path, split_file))\n            for i in range(n_splits):\n                self.train_masks.append(train_indices)\n                self.val_masks.append(val_indices)\n                self.test_masks.append(test_indices)\n\n        elif self.name in ['coauthorcs', 'coauthorph', 'amazoncom', 'amazonpho']:\n            for i in range(n_splits):\n                np.random.seed(i)\n                train_indices, val_indices, test_indices = get_split(self.labels.cpu().numpy(), train_examples_per_class=20, val_examples_per_class=30)  # \u9ed8\u8ba4\u91c7\u53d620-30-rest\u8fd9\u79cd\u5212\u5206\n                self.train_masks.append(train_indices)\n                self.val_masks.append(val_indices)\n                self.test_masks.append(test_indices)\n        elif self.name in ['cora', 'citeseer', 'pubmed']:\n            for i in range(n_splits):\n                self.train_masks.append(torch.nonzero(self.g.train_mask, as_tuple=False).squeeze().numpy())\n                self.val_masks.append(torch.nonzero(self.g.val_mask, as_tuple=False).squeeze().numpy())\n                self.test_masks.append(torch.nonzero(self.g.test_mask, as_tuple=False).squeeze().numpy())\n        elif self.name in ['amazon-ratings', 'questions', 'chameleon-filtered', 'squirrel-filtered', 'minesweeper', 'roman-empire', 'wiki-cooc', 'tolokers']:\n            assert n_splits < 10 , 'n_splits > splits provided'\n            self.train_masks = self.splits[0][:n_splits]\n            self.val_masks = self.splits[1][:n_splits]\n            self.test_masks = self.splits[2][:n_splits]\n        # elif ds_name in ['penn94']:\n        #     train_indices = self.splits[seed]['train']\n        #     val_indices = self.splits[seed]['valid']\n        #     test_indices = self.splits[seed]['test']\n        #     self.train_mask = generate_mask_tensor(sample_mask(train_indices, self.n_nodes))\n        #     self.val_mask = generate_mask_tensor(sample_mask(val_indices, self.n_nodes))\n        #     self.test_mask = generate_mask_tensor(sample_mask(test_indices, self.n_nodes))\n        else:\n            print('dataset not implemented')\n            exit(0)\n\n        if verbose:\n            print(\"\"\"----Split statistics of %d splits------'\n                #Train samples %d\n                #Val samples %d\n                #Test samples %d\"\"\" %\n                  (n_splits, len(self.train_masks[0]), len(self.val_masks[0]), len(self.test_masks[0])))"]}
{"filename": "opengsl/data/dataset/split.py", "chunked_list": ["'''\nThis file is to split Coauthor/Amazon dataset\n'''\n\nimport numpy as np\n\ndef sample_per_class(labels, num_examples_per_class, forbidden_indices=None):\n    num_samples = len(labels)\n    num_classes = labels.max() + 1\n    sample_indices_per_class = {index: [] for index in range(num_classes)}\n\n    # get indices sorted by class\n    for class_index in range(num_classes):\n        for sample_index in range(num_samples):\n            if labels[sample_index] == class_index:\n                if forbidden_indices is None or sample_index not in forbidden_indices:\n                    sample_indices_per_class[class_index].append(sample_index)\n\n    # get specified number of indices for each class\n    return np.concatenate(\n        [np.random.choice(sample_indices_per_class[class_index], num_examples_per_class, replace=False)\n         for class_index in range(len(sample_indices_per_class))\n         ])", "\ndef get_split(labels, train_examples_per_class=None, val_examples_per_class=None, test_examples_per_class=None,\n              train_size=None, val_size=None, test_size=None):\n    num_samples = len(labels)\n    num_classes = labels.max() + 1\n    remaining_indices = list(range(num_samples))\n\n    if train_examples_per_class is not None:\n        train_indices = sample_per_class(labels, train_examples_per_class)\n    else:\n        # select train examples with no respect to class distribution\n        train_indices = np.random.choice(remaining_indices, train_size, replace=False)\n\n    if val_examples_per_class is not None:\n        val_indices = sample_per_class(labels, val_examples_per_class, forbidden_indices=train_indices)\n    else:\n        remaining_indices = np.setdiff1d(remaining_indices, train_indices)\n        val_indices = np.random.choice(remaining_indices, val_size, replace=False)\n\n    forbidden_indices = np.concatenate((train_indices, val_indices))\n\n    if test_examples_per_class is not None:\n        test_indices = sample_per_class(labels, test_examples_per_class, forbidden_indices=forbidden_indices)\n    elif test_size is not None:\n        remaining_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n        test_indices = np.random.choice(remaining_indices, test_size, replace=False)\n    else:\n        test_indices = np.setdiff1d(remaining_indices, forbidden_indices)\n\n    # assert that there are no duplicates in sets\n    assert len(set(train_indices)) == len(train_indices)\n    assert len(set(val_indices)) == len(val_indices)\n    assert len(set(test_indices)) == len(test_indices)\n    # assert sets are mutually exclusive\n    assert len(set(train_indices) - set(val_indices)) == len(set(train_indices))\n    assert len(set(train_indices) - set(test_indices)) == len(set(train_indices))\n    assert len(set(val_indices) - set(test_indices)) == len(set(val_indices))\n    if test_size is None and test_examples_per_class is None:\n        # all indices must be part of the split\n        assert len(np.concatenate((train_indices, val_indices, test_indices))) == num_samples\n\n    return train_indices, val_indices, test_indices"]}
{"filename": "opengsl/data/dataset/__init__.py", "chunked_list": ["from opengsl.data.dataset.dataset import Dataset"]}
{"filename": "opengsl/data/preprocess/normalize.py", "chunked_list": ["import torch\nimport numpy as np\nimport scipy.sparse as sp\nfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor ,sparse_tensor_to_scipy_sparse\n\n\ndef normalize(mx, style='symmetric', add_loop=True, sparse=False):\n    '''\n    Normalize the feature matrix or adj matrix.\n\n    Parameters\n    ----------\n    mx : torch.tensor\n        Feature matrix or adj matrix to normalize.\n    style: str\n        If set as ``row``, `mx` will be row-wise normalized.\n        If set as ``symmetric``, `mx` will be normalized as in GCN.\n\n    add_loop : bool\n        Whether to add self loop.\n    sparse : bool\n        Whether the matrix is stored in sparse form. The returned tensor will be the same form.\n\n    Returns\n    -------\n    normalized_mx : torch.tensor\n        The normalized matrix.\n\n    '''\n    if style == 'row':\n        return row_nomalize(mx)\n    elif style == 'symmetric':\n        if sparse:\n            return normalize_sp_tensor(mx, add_loop)\n        else:\n            return normalize_tensor(mx, add_loop)", "\n\ndef row_nomalize(mx):\n    \"\"\"Row-normalize sparse matrix.\n    \"\"\"\n    device = mx.device\n    mx = mx.cpu().numpy()\n    r_sum = np.array(mx.sum(1))\n    r_inv = np.power(r_sum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    mx = torch.tensor(mx).to(device)\n    return mx", "\n\ndef normalize_tensor(adj, add_loop=True):\n    device = adj.device\n    adj_loop = adj + torch.eye(adj.shape[0]).to(device) if add_loop else adj\n    rowsum = adj_loop.sum(1)\n    r_inv = rowsum.pow(-1/2).flatten()\n    r_inv[torch.isinf(r_inv)] = 0.\n    r_mat_inv = torch.diag(r_inv)\n    A = r_mat_inv @ adj_loop\n    A = A @ r_mat_inv\n    return A", "\n\ndef normalize_sp_tensor(adj, add_loop=True):\n    device = adj.device\n    adj = sparse_tensor_to_scipy_sparse(adj)\n    adj = normalize_sp_matrix(adj, add_loop)\n    adj = scipy_sparse_to_sparse_tensor(adj).to(device)\n    return adj\n\n\ndef normalize_sp_matrix(adj, add_loop=True):\n    mx = adj + sp.eye(adj.shape[0]) if add_loop else adj\n    rowsum = np.array(mx.sum(1))\n    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n    new = mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n    return new", "\n\ndef normalize_sp_matrix(adj, add_loop=True):\n    mx = adj + sp.eye(adj.shape[0]) if add_loop else adj\n    rowsum = np.array(mx.sum(1))\n    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n    new = mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n    return new"]}
{"filename": "opengsl/data/preprocess/__init__.py", "chunked_list": ["from opengsl.data.preprocess.normalize import normalize\nfrom opengsl.data.preprocess.control_homophily import control_homophily"]}
{"filename": "opengsl/data/preprocess/control_homophily.py", "chunked_list": ["from opengsl.utils.utils import get_homophily\nimport numpy as np\n\n\ndef control_homophily(adj, labels, homophily):\n    '''\n    Control the homophily of original structure by adding edges.\n    More ways to add perturbations will be implemented soon.\n\n    Parameters\n    ----------\n    adj : torch.tensor\n        The original structure in sparse form.\n    labels : torch.tensor\n        Ground truth labels.\n    homophily : float\n        Homophily ratio.\n\n    Returns\n    -------\n    new_adj : torch.tensor\n        The perturbed structure in sparse form.\n\n    '''\n    np.random.seed(0)\n    # change homophily through adding edges\n    adj = adj.to_dense()\n    n_edges = adj.sum()/2\n    n_nodes = len(labels)\n    homophily_orig = get_homophily(labels, adj, 'edge')\n    # print(homophily_orig)\n    if homophily<homophily_orig:\n        # add noisy edges\n        n_add_edges = int(n_edges*homophily_orig/homophily-n_edges)\n        while n_add_edges>0:\n            u = np.random.randint(0, n_nodes)\n            vs = np.arange(0, n_nodes)[labels!=labels[u]]\n            v = np.random.choice(vs)\n            if adj[u, v]==0:\n                adj[u,v]=adj[v,u]=1\n                n_add_edges-=1\n    if homophily>homophily_orig:\n        # add helpful edges\n        n_add_edges = int(n_edges*(1-homophily_orig)/(1-homophily)-n_edges)\n        while n_add_edges > 0:\n            u = np.random.randint(0, n_nodes)\n            vs = np.arange(0, n_nodes)[labels==labels[u]]\n            v = np.random.choice(vs)\n            if u==v:\n                continue\n            if adj[u,v]==0:\n                adj[u,v]=adj[v,u]=1\n                n_add_edges -= 1\n    return adj.to_sparse()", "\n\n\n"]}
{"filename": "opengsl/expmanager/__init__.py", "chunked_list": [""]}
{"filename": "opengsl/expmanager/ExpManager.py", "chunked_list": ["import torch\nfrom opengsl.utils.utils import set_seed\nfrom opengsl.utils.logger import Logger\nfrom opengsl.config.util import save_conf\nimport os\nimport time as time\n\n\nclass ExpManager:\n    '''\n    Experiment management class to enable running multiple experiment,\n    loading learned structures and saving results.\n\n    Parameters\n    ----------\n    solver : opengsl.method.Solver\n        Solver of the method to solve the task.\n    n_splits : int\n        Number of data splits to run experiment on.\n    n_runs : int\n        Number of experiment runs each split.\n    save_path : str\n        Path to save the config file.\n    debug : bool\n        Whether to print statistics during training.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.dataset.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gcn', 'cora')\n    >>> # create solver\n    >>> import opengsl.method.SGCSolver\n    >>> solver = SGCSolver(conf, dataset)\n    >>>\n    >>> import opengsl.ExpManager\n    >>> exp = ExpManager(solver)\n    >>> exp.run(n_runs=10, debug=True)\n\n    '''\n    def __init__(self, solver=None, save_path=None):\n        self.solver = solver\n        self.conf = solver.conf\n        self.method = solver.method_name\n        self.dataset = solver.dataset\n        self.data = self.dataset.name\n        self.device = torch.device('cuda')\n        # you can change random seed here\n        self.train_seeds = [i for i in range(400)]\n        self.split_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        self.save_path = None\n        self.save_graph_path = None\n        self.load_graph_path = None\n        if save_path:\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            self.save_path = save_path\n        if 'save_graph' in self.conf.analysis and self.conf.analysis['save_graph']:\n            assert 'save_graph_path' in self.conf.analysis and self.conf.analysis['save_graph_path'] is not None, 'Specify the path to save graph'\n            self.save_graph_path = os.path.join(self.conf.analysis['save_graph_path'], self.method)\n        if 'load_graph' in self.conf.analysis and self.conf.analysis['load_graph']:\n            assert 'load_graph_path' in self.conf.analysis and self.conf.analysis[\n                'load_graph_path'] is not None, 'Specify the path to load graph'\n            self.load_graph_path = self.conf.analysis['load_graph_path']\n        assert self.save_graph_path is None or self.load_graph_path is None, 'GNN does not save graph, GSL does not load graph'\n\n    def run(self, n_splits=1, n_runs=1, debug=False):\n        total_runs = n_runs * n_splits\n        assert n_splits <= len(self.split_seeds)\n        assert total_runs <= len(self.train_seeds)\n        logger = Logger(runs=total_runs)\n        for i in range(n_splits):\n            succeed = 0\n            for j in range(400):\n                idx = i * n_runs + j\n                print(\"Exp {}/{}\".format(idx, total_runs))\n                set_seed(self.train_seeds[idx])\n\n                # load graph\n                if self.load_graph_path:\n                    self.solver.adj = torch.load(os.path.join(self.load_graph_path,'{}_{}_{}.pth'.format(self.data, i, self.train_seeds[idx]))).to(self.device)\n                    if self.conf.dataset['sparse']:\n                        self.solver.adj = self.solver.adj.to_sparse()\n                # run an exp\n                try:\n                    result, graph = self.solver.run_exp(split=i, debug=debug)\n                except ValueError:\n                    continue\n                logger.add_result(succeed, result)\n\n                # save graph\n                if self.save_graph_path:\n                    if not os.path.exists(self.save_graph_path):\n                        os.makedirs(self.save_graph_path)\n                    torch.save(graph.cpu(), os.path.join(self.save_graph_path, '{}_{}_{}.pth'.format(self.data, i, self.train_seeds[succeed])))\n                succeed += 1\n                if succeed == total_runs:\n                    break\n        self.acc_save, self.std_save = logger.print_statistics()\n        if self.save_path:\n            save_conf(os.path.join(self.save_path, '{}-{}-'.format(self.method, self.data) +\n                                   time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime()) + '.yaml'), self.conf)\n            \n        return float(self.acc_save), float(self.std_save)", "class ExpManager:\n    '''\n    Experiment management class to enable running multiple experiment,\n    loading learned structures and saving results.\n\n    Parameters\n    ----------\n    solver : opengsl.method.Solver\n        Solver of the method to solve the task.\n    n_splits : int\n        Number of data splits to run experiment on.\n    n_runs : int\n        Number of experiment runs each split.\n    save_path : str\n        Path to save the config file.\n    debug : bool\n        Whether to print statistics during training.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.dataset.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gcn', 'cora')\n    >>> # create solver\n    >>> import opengsl.method.SGCSolver\n    >>> solver = SGCSolver(conf, dataset)\n    >>>\n    >>> import opengsl.ExpManager\n    >>> exp = ExpManager(solver)\n    >>> exp.run(n_runs=10, debug=True)\n\n    '''\n    def __init__(self, solver=None, save_path=None):\n        self.solver = solver\n        self.conf = solver.conf\n        self.method = solver.method_name\n        self.dataset = solver.dataset\n        self.data = self.dataset.name\n        self.device = torch.device('cuda')\n        # you can change random seed here\n        self.train_seeds = [i for i in range(400)]\n        self.split_seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        self.save_path = None\n        self.save_graph_path = None\n        self.load_graph_path = None\n        if save_path:\n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n            self.save_path = save_path\n        if 'save_graph' in self.conf.analysis and self.conf.analysis['save_graph']:\n            assert 'save_graph_path' in self.conf.analysis and self.conf.analysis['save_graph_path'] is not None, 'Specify the path to save graph'\n            self.save_graph_path = os.path.join(self.conf.analysis['save_graph_path'], self.method)\n        if 'load_graph' in self.conf.analysis and self.conf.analysis['load_graph']:\n            assert 'load_graph_path' in self.conf.analysis and self.conf.analysis[\n                'load_graph_path'] is not None, 'Specify the path to load graph'\n            self.load_graph_path = self.conf.analysis['load_graph_path']\n        assert self.save_graph_path is None or self.load_graph_path is None, 'GNN does not save graph, GSL does not load graph'\n\n    def run(self, n_splits=1, n_runs=1, debug=False):\n        total_runs = n_runs * n_splits\n        assert n_splits <= len(self.split_seeds)\n        assert total_runs <= len(self.train_seeds)\n        logger = Logger(runs=total_runs)\n        for i in range(n_splits):\n            succeed = 0\n            for j in range(400):\n                idx = i * n_runs + j\n                print(\"Exp {}/{}\".format(idx, total_runs))\n                set_seed(self.train_seeds[idx])\n\n                # load graph\n                if self.load_graph_path:\n                    self.solver.adj = torch.load(os.path.join(self.load_graph_path,'{}_{}_{}.pth'.format(self.data, i, self.train_seeds[idx]))).to(self.device)\n                    if self.conf.dataset['sparse']:\n                        self.solver.adj = self.solver.adj.to_sparse()\n                # run an exp\n                try:\n                    result, graph = self.solver.run_exp(split=i, debug=debug)\n                except ValueError:\n                    continue\n                logger.add_result(succeed, result)\n\n                # save graph\n                if self.save_graph_path:\n                    if not os.path.exists(self.save_graph_path):\n                        os.makedirs(self.save_graph_path)\n                    torch.save(graph.cpu(), os.path.join(self.save_graph_path, '{}_{}_{}.pth'.format(self.data, i, self.train_seeds[succeed])))\n                succeed += 1\n                if succeed == total_runs:\n                    break\n        self.acc_save, self.std_save = logger.print_statistics()\n        if self.save_path:\n            save_conf(os.path.join(self.save_path, '{}-{}-'.format(self.method, self.data) +\n                                   time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime()) + '.yaml'), self.conf)\n            \n        return float(self.acc_save), float(self.std_save)", ""]}
{"filename": "opengsl/method/solver.py", "chunked_list": ["import torch\nfrom copy import deepcopy\nimport time\nfrom ..utils.utils import accuracy\nfrom ..utils.recorder import Recorder\nfrom sklearn.metrics import roc_auc_score\nimport torch.nn.functional as F\n\n\nclass Solver:\n    '''\n    Base solver class to conduct a single experiment. It defines the abstract training procedures\n    which can be overwritten in subclass solver for each method.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Configuration file.\n    dataset : opengsl.data.Dataset\n        Dataset to be conduct an experiment on.\n\n    Attributes\n    ----------\n    conf : argparse.Namespace\n        Configuration file.\n    dataset : opengsl.data.Dataset\n        Dataset to be conduct an experiment on.\n    model : nn.Module\n        Model of the method.\n    loss_fn : function\n        Loss function, either `F.binary_cross_entropy_with_logits` or `F.cross_entropy`.\n    metric : functrion\n        Metric function, either 'roc_auc_score' or 'accuracy'.\n\n    '''\n    def __init__(self, conf, dataset):\n        self.dataset = dataset\n        \n        self.conf = conf\n        self.device = torch.device('cuda')\n        self.n_nodes = dataset.n_nodes\n        self.dim_feats = dataset.dim_feats\n        self.num_targets = dataset.num_targets\n        self.n_classes = dataset.n_classes\n        self.model = None\n        self.loss_fn = F.binary_cross_entropy_with_logits if self.num_targets == 1 else F.cross_entropy\n        self.metric = roc_auc_score if self.num_targets == 1 else accuracy\n        self.model = None\n\n        self.feats = dataset.feats\n        self.adj = dataset.adj if self.conf.dataset['sparse'] else dataset.adj.to_dense()\n        self.labels = dataset.labels\n        self.train_masks = dataset.train_masks\n        self.val_masks = dataset.val_masks\n        self.test_masks = dataset.test_masks\n\n    def run_exp(self, split=None, debug=False):\n        '''\n        Function to start an experiment.\n\n        Parameters\n        ----------\n        split : int\n            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure. `None` for GNN methods.\n        '''\n        self.set(split)\n        return self.learn(debug)\n\n    def set(self, split):\n        '''\n        This conducts necessary operations for an experiment, including the setting specified split,\n        variables to record statistics, models.\n\n        Parameters\n        ----------\n        split : int\n            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n\n        '''\n        if split is None:\n            print('split set to default 0.')\n            split=0\n        assert split<len(self.train_masks), 'error, split id is larger than number of splits'\n        self.train_mask = self.train_masks[split]\n        self.val_mask = self.val_masks[split]\n        self.test_mask = self.test_masks[split]\n        self.total_time = 0\n        self.best_val_loss = 10\n        self.weights = None\n        self.best_graph = None\n        self.result = {'train': 0, 'valid': 0, 'test': 0}\n        self.start_time = time.time()\n        self.recoder = Recorder(self.conf.training['patience'], self.conf.training['criterion'])\n        self.set_method()\n\n    def set_method(self):\n        '''\n        This sets model and other members, which is overwritten for each method.\n\n        '''\n        self.model = None\n        self.optim = None\n\n    def learn(self, debug=False):\n        '''\n        This is the common learning procedure, which is overwritten for special learning procedure.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure. `None` for GNN methods.\n        '''\n\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            output = self.model(self.input_distributer())\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(),\n                                    output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val = self.evaluate(self.val_mask)\n            flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\n            # save\n            if flag:\n                improve = '*'\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n            elif flag_earlystop:\n                break\n\n\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, None\n\n    def evaluate(self, val_mask):\n        '''\n        This is the common evaluation procedure, which is overwritten for special evaluation procedure.\n\n        Parameters\n        ----------\n        val_mask : torch.tensor\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output = self.model(self.input_distributer())\n        logits = output[val_mask]\n        labels = self.labels[val_mask]\n        loss = self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def input_distributer(self):\n        '''\n        This distributes different input in `learn` for different methods, which is overwritten for each method.\n        '''\n        return None\n\n    def test(self):\n        '''\n        This is the common test procedure, which is overwritten for special test procedure.\n\n        Returns\n        -------\n        loss : float\n            Test loss.\n        metric : float\n            Test metric.\n        '''\n        self.model.load_state_dict(self.weights)\n        return self.evaluate(self.test_mask)", "\nclass Solver:\n    '''\n    Base solver class to conduct a single experiment. It defines the abstract training procedures\n    which can be overwritten in subclass solver for each method.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Configuration file.\n    dataset : opengsl.data.Dataset\n        Dataset to be conduct an experiment on.\n\n    Attributes\n    ----------\n    conf : argparse.Namespace\n        Configuration file.\n    dataset : opengsl.data.Dataset\n        Dataset to be conduct an experiment on.\n    model : nn.Module\n        Model of the method.\n    loss_fn : function\n        Loss function, either `F.binary_cross_entropy_with_logits` or `F.cross_entropy`.\n    metric : functrion\n        Metric function, either 'roc_auc_score' or 'accuracy'.\n\n    '''\n    def __init__(self, conf, dataset):\n        self.dataset = dataset\n        \n        self.conf = conf\n        self.device = torch.device('cuda')\n        self.n_nodes = dataset.n_nodes\n        self.dim_feats = dataset.dim_feats\n        self.num_targets = dataset.num_targets\n        self.n_classes = dataset.n_classes\n        self.model = None\n        self.loss_fn = F.binary_cross_entropy_with_logits if self.num_targets == 1 else F.cross_entropy\n        self.metric = roc_auc_score if self.num_targets == 1 else accuracy\n        self.model = None\n\n        self.feats = dataset.feats\n        self.adj = dataset.adj if self.conf.dataset['sparse'] else dataset.adj.to_dense()\n        self.labels = dataset.labels\n        self.train_masks = dataset.train_masks\n        self.val_masks = dataset.val_masks\n        self.test_masks = dataset.test_masks\n\n    def run_exp(self, split=None, debug=False):\n        '''\n        Function to start an experiment.\n\n        Parameters\n        ----------\n        split : int\n            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure. `None` for GNN methods.\n        '''\n        self.set(split)\n        return self.learn(debug)\n\n    def set(self, split):\n        '''\n        This conducts necessary operations for an experiment, including the setting specified split,\n        variables to record statistics, models.\n\n        Parameters\n        ----------\n        split : int\n            Specify the idx of a split among mutiple splits. Set to 0 if not specified.\n\n        '''\n        if split is None:\n            print('split set to default 0.')\n            split=0\n        assert split<len(self.train_masks), 'error, split id is larger than number of splits'\n        self.train_mask = self.train_masks[split]\n        self.val_mask = self.val_masks[split]\n        self.test_mask = self.test_masks[split]\n        self.total_time = 0\n        self.best_val_loss = 10\n        self.weights = None\n        self.best_graph = None\n        self.result = {'train': 0, 'valid': 0, 'test': 0}\n        self.start_time = time.time()\n        self.recoder = Recorder(self.conf.training['patience'], self.conf.training['criterion'])\n        self.set_method()\n\n    def set_method(self):\n        '''\n        This sets model and other members, which is overwritten for each method.\n\n        '''\n        self.model = None\n        self.optim = None\n\n    def learn(self, debug=False):\n        '''\n        This is the common learning procedure, which is overwritten for special learning procedure.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure. `None` for GNN methods.\n        '''\n\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            output = self.model(self.input_distributer())\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(),\n                                    output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val = self.evaluate(self.val_mask)\n            flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\n            # save\n            if flag:\n                improve = '*'\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n            elif flag_earlystop:\n                break\n\n\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, None\n\n    def evaluate(self, val_mask):\n        '''\n        This is the common evaluation procedure, which is overwritten for special evaluation procedure.\n\n        Parameters\n        ----------\n        val_mask : torch.tensor\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output = self.model(self.input_distributer())\n        logits = output[val_mask]\n        labels = self.labels[val_mask]\n        loss = self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def input_distributer(self):\n        '''\n        This distributes different input in `learn` for different methods, which is overwritten for each method.\n        '''\n        return None\n\n    def test(self):\n        '''\n        This is the common test procedure, which is overwritten for special test procedure.\n\n        Returns\n        -------\n        loss : float\n            Test loss.\n        metric : float\n            Test metric.\n        '''\n        self.model.load_state_dict(self.weights)\n        return self.evaluate(self.test_mask)", "\n\n\n\n\n"]}
{"filename": "opengsl/method/__init__.py", "chunked_list": ["'''\nmethod_name = ['gcn', 'sgc', 'gat', 'jknet', 'appnp', 'gprgnn',\n'prognn', 'idgl', 'grcn', 'gaug', 'slaps', 'gen', 'gt', 'nodeformer', 'cogsl', 'sublime', 'stable', 'segsl']\n'''\nfrom opengsl.method.solver import Solver\nfrom opengsl.method.gnnsolver import GCNSolver\nfrom opengsl.method.gnnsolver import SGCSolver\nfrom opengsl.method.gnnsolver import GATSolver\nfrom opengsl.method.gnnsolver import JKNetSolver\nfrom opengsl.method.gnnsolver import APPNPSolver", "from opengsl.method.gnnsolver import JKNetSolver\nfrom opengsl.method.gnnsolver import APPNPSolver\nfrom opengsl.method.gnnsolver import GPRGNNSolver\nfrom opengsl.method.gnnsolver import LINKSolver\nfrom opengsl.method.gnnsolver import LPASolver\nfrom opengsl.method.gslsolver import WSGNNSolver\nfrom opengsl.method.gslsolver import PROGNNSolver\nfrom opengsl.method.gslsolver import IDGLSolver\nfrom opengsl.method.gslsolver import GRCNSolver\nfrom opengsl.method.gslsolver import GAUGSolver", "from opengsl.method.gslsolver import GRCNSolver\nfrom opengsl.method.gslsolver import GAUGSolver\nfrom opengsl.method.gslsolver import SLAPSSolver\nfrom opengsl.method.gslsolver import GENSolver\nfrom opengsl.method.gslsolver import GTSolver\nfrom opengsl.method.gslsolver import NODEFORMERSolver\nfrom opengsl.method.gslsolver import COGSLSolver\nfrom opengsl.method.gslsolver import SUBLIMESolver\nfrom opengsl.method.gslsolver import STABLESolver\nfrom opengsl.method.gslsolver import SEGSLSolver", "from opengsl.method.gslsolver import STABLESolver\nfrom opengsl.method.gslsolver import SEGSLSolver"]}
{"filename": "opengsl/method/gslsolver.py", "chunked_list": ["import scipy.sparse as sp\nfrom sklearn.metrics.pairwise import cosine_similarity as cos\nimport numpy as np\nimport random\nfrom copy import deepcopy\nfrom .models.gcn import GCN\nfrom .models.gnn_modules import APPNP\nfrom .models.grcn import GRCN\nfrom .models.gaug import GAug, eval_edge_pred, MultipleOptimizer, get_lr_schedule_by_sigmoid\nfrom .models.gen import EstimateAdj as GENEstimateAdj, prob_to_adj", "from .models.gaug import GAug, eval_edge_pred, MultipleOptimizer, get_lr_schedule_by_sigmoid\nfrom .models.gen import EstimateAdj as GENEstimateAdj, prob_to_adj\nfrom .models.idgl import IDGL, sample_anchors, diff, compute_anchor_adj\nfrom .models.prognn import PGD, prox_operators, EstimateAdj, feature_smoothing\nfrom .models.gt import GT\nfrom .models.slaps import SLAPS\nfrom .models.nodeformer import NodeFormer, adj_mul\nfrom .models.segsl import knn_maxE1, add_knn, get_weight, get_adj_matrix, PartitionTree, get_community, reshape\nfrom .models.sublime import torch_sparse_to_dgl_graph, FGP_learner, ATT_learner, GNN_learner, MLP_learner, GCL, get_feat_mask, split_batch, dgl_graph_to_torch_sparse, GCN_SUB\nfrom .models.stable import DGI, preprocess_adj, aug_random_edge, get_reliable_neighbors", "from .models.sublime import torch_sparse_to_dgl_graph, FGP_learner, ATT_learner, GNN_learner, MLP_learner, GCL, get_feat_mask, split_batch, dgl_graph_to_torch_sparse, GCN_SUB\nfrom .models.stable import DGI, preprocess_adj, aug_random_edge, get_reliable_neighbors\nfrom .models.cogsl import CoGSL\nfrom .models.wsgnn import WSGNN, ELBONCLoss\nimport torch\nimport torch.nn.functional as F\nimport time\nfrom .solver import Solver\nfrom ..utils.utils import get_homophily, sparse_tensor_to_scipy_sparse, scipy_sparse_to_sparse_tensor\nfrom opengsl.data.preprocess.normalize import normalize, normalize_sp_matrix", "from ..utils.utils import get_homophily, sparse_tensor_to_scipy_sparse, scipy_sparse_to_sparse_tensor\nfrom opengsl.data.preprocess.normalize import normalize, normalize_sp_matrix\nfrom ..utils.recorder import Recorder\nimport dgl\nimport copy\nimport os\nfrom os.path import dirname\n\n\nclass GRCNSolver(Solver):\n    '''\n        A solver to train, evaluate, test GRCN in a run.\n\n        Parameters\n        ----------\n        conf : argparse.Namespace\n            Config file.\n        dataset : opengsl.data.Dataset\n            The dataset.\n\n        Attributes\n        ----------\n        method_name : str\n            The name of the method.\n\n        Examples\n        --------\n        >>> # load dataset\n        >>> import opengsl.dataset\n        >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n        >>> # load config file\n        >>> import opengsl.config.load_conf\n        >>> conf = opengsl.config.load_conf('grcn', 'cora')\n        >>>\n        >>> solver = GRCNSolver(conf, dataset)\n        >>> # Conduct a experiment run.\n        >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n        '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"grcn\"\n        print(\"Solver Version : [{}]\".format(\"grcn\"))\n        edge_index = self.adj.coalesce().indices().cpu()\n        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n        edges = torch.cat([edge_index, loop_edge_index], dim=1)\n        self.adj = torch.sparse.FloatTensor(edges, torch.ones(edges.shape[1]), [self.n_nodes, self.n_nodes]).to(self.device).coalesce()\n\n\n    def learn(self, debug=False):\n        '''\n        Learning process of GRCN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim1.zero_grad()\n            self.optim2.zero_grad()\n\n            # forward and backward\n            output, _ = self.model(self.feats, self.adj)\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim1.step()\n            self.optim2.step()\n\n            # Evaluate\n            loss_val, acc_val, adj = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                self.total_time = time.time() - self.start_time\n                improve = '*'\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n                if self.conf.analysis['save_graph']:\n                    self.best_graph = deepcopy(adj.to_dense())\n\n            # print\n\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test, _ = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def evaluate(self, test_mask):\n        '''\n        Evaluation procedure of GRCN.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        adj : torch.tensor        \n            The learned structure.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output, adj = self.model(self.feats, self.adj)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GRCN(self.n_nodes, self.dim_feats, self.num_targets, self.device, self.conf).to(self.device)\n        self.optim1 = torch.optim.Adam(self.model.base_parameters(), lr=self.conf.training['lr'],\n                                       weight_decay=self.conf.training['weight_decay'])\n        self.optim2 = torch.optim.Adam(self.model.graph_parameters(), lr=self.conf.training['lr_graph'])", "\nclass GRCNSolver(Solver):\n    '''\n        A solver to train, evaluate, test GRCN in a run.\n\n        Parameters\n        ----------\n        conf : argparse.Namespace\n            Config file.\n        dataset : opengsl.data.Dataset\n            The dataset.\n\n        Attributes\n        ----------\n        method_name : str\n            The name of the method.\n\n        Examples\n        --------\n        >>> # load dataset\n        >>> import opengsl.dataset\n        >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n        >>> # load config file\n        >>> import opengsl.config.load_conf\n        >>> conf = opengsl.config.load_conf('grcn', 'cora')\n        >>>\n        >>> solver = GRCNSolver(conf, dataset)\n        >>> # Conduct a experiment run.\n        >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n        '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"grcn\"\n        print(\"Solver Version : [{}]\".format(\"grcn\"))\n        edge_index = self.adj.coalesce().indices().cpu()\n        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n        edges = torch.cat([edge_index, loop_edge_index], dim=1)\n        self.adj = torch.sparse.FloatTensor(edges, torch.ones(edges.shape[1]), [self.n_nodes, self.n_nodes]).to(self.device).coalesce()\n\n\n    def learn(self, debug=False):\n        '''\n        Learning process of GRCN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim1.zero_grad()\n            self.optim2.zero_grad()\n\n            # forward and backward\n            output, _ = self.model(self.feats, self.adj)\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim1.step()\n            self.optim2.step()\n\n            # Evaluate\n            loss_val, acc_val, adj = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                self.total_time = time.time() - self.start_time\n                improve = '*'\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n                if self.conf.analysis['save_graph']:\n                    self.best_graph = deepcopy(adj.to_dense())\n\n            # print\n\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test, _ = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def evaluate(self, test_mask):\n        '''\n        Evaluation procedure of GRCN.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        adj : torch.tensor        \n            The learned structure.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output, adj = self.model(self.feats, self.adj)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GRCN(self.n_nodes, self.dim_feats, self.num_targets, self.device, self.conf).to(self.device)\n        self.optim1 = torch.optim.Adam(self.model.base_parameters(), lr=self.conf.training['lr'],\n                                       weight_decay=self.conf.training['weight_decay'])\n        self.optim2 = torch.optim.Adam(self.model.graph_parameters(), lr=self.conf.training['lr_graph'])", "\n\nclass GAUGSolver(Solver):\n    '''\n    A solver to train, evaluate, test GAug in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gaug', 'cora')\n    >>>\n    >>> solver = GAUGSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_strcuture = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gaug\"\n        print(\"Solver Version : [{}]\".format(\"gaug\"))\n        self.normalized_adj = normalize(self.adj, sparse=True).to(self.device)\n        self.adj_orig = (self.adj.to_dense() + torch.eye(self.n_nodes).to(self.device))  # adj with self loop\n        self.conf = conf\n\n    def pretrain_ep_net(self, norm_w, pos_weight, n_epochs, debug=False):\n        \"\"\" pretrain the edge prediction network \"\"\"\n        optimizer = torch.optim.Adam(self.model.ep_net.parameters(), lr=self.conf.training['lr'])\n        self.model.train()\n        for epoch in range(n_epochs):\n            t = time.time()\n            optimizer.zero_grad()\n            adj_logits = self.model.ep_net(self.feats, self.normalized_adj)\n            loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, self.adj_orig, pos_weight=pos_weight)\n            if not self.conf.gsl['gae']:\n                mu = self.model.ep_net.mean\n                lgstd = self.model.ep_net.logstd\n                kl_divergence = 0.5/adj_logits.size(0) * (1 + 2*lgstd - mu**2 - torch.exp(2*lgstd)).sum(1).mean()\n                loss -= kl_divergence\n            loss.backward()\n            optimizer.step()\n            adj_pred = torch.sigmoid(adj_logits.detach()).cpu()\n            ep_auc, ep_ap = eval_edge_pred(adj_pred, self.val_edges, self.edge_labels)\n            if debug:\n                print('EPNet pretrain, Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | auc {:.4f} | ap {:.4f}'\n                                 .format(epoch+1, time.time()-t, loss.item(), ep_auc, ep_ap))\n\n    def pretrain_nc_net(self, n_epochs, debug=False):\n        \"\"\" pretrain the node classification network \"\"\"\n        optimizer = torch.optim.Adam(self.model.nc_net.parameters(),\n                                     lr=self.conf.training['lr'],\n                                     weight_decay=self.conf.training['weight_decay'])\n        # loss function for node classification\n        for epoch in range(n_epochs):\n            t = time.time()\n            improve = ''\n            self.model.train()\n            optimizer.zero_grad()\n\n            # forward and backward\n            hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            optimizer.step()\n\n            # evaluate\n            self.model.eval()\n            with torch.no_grad():\n                hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n                loss_val = self.loss_fn(output[self.val_mask], self.labels[self.val_mask])\n            acc_val = self.metric(self.labels[self.val_mask].cpu().numpy(), output[self.val_mask].detach().cpu().numpy())\n            if acc_val > self.result['valid']:\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                improve = '*'\n                self.weights = deepcopy(self.model.state_dict())\n                self.best_graph = self.adj.to_dense()\n\n            # print\n            if debug:\n                print(\"NCNet pretrain, Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n    def learn(self, debug=False):\n        '''\n        Learning process of GAUG.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        patience_step = 0\n\n        # prepare\n        adj_t = self.adj_orig\n        norm_w = adj_t.shape[0]**2 / float((adj_t.shape[0]**2 - adj_t.sum()) * 2)\n        pos_weight = torch.FloatTensor([float(adj_t.shape[0]**2 - adj_t.sum()) / adj_t.sum()]).to(self.device)\n\n        # pretrain\n        self.pretrain_ep_net(norm_w, pos_weight, self.conf.training['pretrain_ep'], debug)\n        self.pretrain_nc_net(self.conf.training['pretrain_nc'], debug)\n\n        # train\n        optims = MultipleOptimizer(torch.optim.Adam(self.model.ep_net.parameters(),\n                                                    lr=self.conf.training['lr']),\n                                   torch.optim.Adam(self.model.nc_net.parameters(),\n                                                    lr=self.conf.training['lr'],\n                                                    weight_decay=self.conf.training['weight_decay']))\n        # get the learning rate schedule for the optimizer of ep_net if needed\n        if self.conf.training['warmup']:\n            ep_lr_schedule = get_lr_schedule_by_sigmoid(self.conf.training['n_epochs'], self.conf.training['lr'], self.conf.training['warmup'])\n\n        for epoch in range(self.conf.training['n_epochs']):\n            t = time.time()\n            improve = ''\n            # update the learning rate for ep_net if needed\n            if self.conf.training['warmup']:\n                optims.update_lr(0, ep_lr_schedule[epoch])\n\n            self.model.train()\n            optims.zero_grad()\n\n            # forward and backward\n            output, adj_logits, adj_new = self.model(self.feats, self.normalized_adj, self.adj_orig)\n            loss_train = nc_loss = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            ep_loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, self.adj_orig, pos_weight=pos_weight)\n            loss_train += self.conf.training['beta'] * ep_loss\n            loss_train.backward()\n            optims.step()\n\n            # validate\n            self.model.eval()\n            with torch.no_grad():\n                hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))   # the author proposed to validate and test on the original adj\n                loss_val = self.loss_fn(output[self.val_mask], self.labels[self.val_mask])\n            acc_val = self.metric(self.labels[self.val_mask].cpu().numpy(), output[self.val_mask].detach().cpu().numpy())\n            adj_pred = torch.sigmoid(adj_logits.detach()).cpu()\n            ep_auc, ep_ap = eval_edge_pred(adj_pred, self.val_edges, self.edge_labels)\n\n            # save\n            if acc_val > self.result['valid']:\n                self.total_time = time.time() - self.start_time\n                improve = '*'\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n                self.best_graph = adj_new.clone().detach()\n                patience_step = 0\n            else:\n                patience_step += 1\n                if patience_step == self.conf.training['patience']:\n                    print('Early stop!')\n                    break\n\n            # print\n            if debug:\n                print(\"Training, Epoch {:05d} | Time(s) {:.4f}\".format(epoch+1, time.time() -t))\n                print('    EP Loss {:.4f} | EP AUC {:.4f} | EP AP {:.4f}'.format(ep_loss, ep_auc, ep_ap))\n                print('    Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}'.format(nc_loss, acc_train, loss_val, acc_val, improve))\n\n        # test\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        self.model.load_state_dict(self.weights)\n        with torch.no_grad():\n            hidden, output = self.model.nc_net((self.feats, self.normalized_adj, False))\n            loss_test = self.loss_fn(output[self.test_mask], self.labels[self.test_mask])\n        acc_test = self.metric(self.labels[self.test_mask].cpu().numpy(), output[self.test_mask].detach().cpu().numpy())\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\n        return self.result, self.best_graph\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        # sample edges\n        if self.labels.size(0) > 5000:\n            edge_frac = 0.01\n        else:\n            edge_frac = 0.1\n        adj_matrix = sp.csr_matrix(self.adj.to_dense().cpu().numpy())\n        adj_matrix.setdiag(1)  # the original code samples 10%(1%) of the total edges(with self loop)\n        n_edges_sample = int(edge_frac * adj_matrix.nnz / 2)\n        # sample negative edges\n        neg_edges = []\n        added_edges = set()\n        while len(neg_edges) < n_edges_sample:\n            i = np.random.randint(0, adj_matrix.shape[0])\n            j = np.random.randint(0, adj_matrix.shape[0])\n            if i == j:\n                continue\n            if adj_matrix[i, j] > 0:\n                continue\n            if (i, j) in added_edges:\n                continue\n            neg_edges.append([i, j])\n            added_edges.add((i, j))\n            added_edges.add((j, i))\n        neg_edges = np.asarray(neg_edges)\n        # sample positive edges\n        nz_upper = np.array(sp.triu(adj_matrix, k=1).nonzero()).T\n        np.random.shuffle(nz_upper)\n        pos_edges = nz_upper[:n_edges_sample]\n        self.val_edges = np.concatenate((pos_edges, neg_edges), axis=0)\n        self.edge_labels = np.array([1] * n_edges_sample + [0] * n_edges_sample)\n\n        self.model = GAug(self.dim_feats, self.num_targets, self.conf).to(self.device)", "\n\nclass GENSolver(Solver):\n    '''\n    A solver to train, evaluate, test GEN in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gen', 'cora')\n    >>>\n    >>> solver = GENSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gen\"\n        print(\"Solver Version : [{}]\".format(\"gen\"))\n        self.homophily = get_homophily(self.labels.cpu(), self.adj.to_dense().cpu(), type='node')\n\n    def knn(self, feature):\n        # Generate a knn graph for input feature matrix. Note that the graph contains self loop.\n        adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.int64)\n        dist = cos(feature.detach().cpu().numpy())\n        col = np.argpartition(dist, -(self.conf.gsl['k'] + 1), axis=1)[:, -(self.conf.gsl['k'] + 1):].flatten()\n        adj[np.arange(self.n_nodes).repeat(self.conf.gsl['k'] + 1), col] = 1\n        return adj\n\n    def train_gcn(self, iter, adj, debug=False):\n        if debug:\n            print('==== Iteration {:04d} ===='.format(iter+1))\n        t = time.time()\n        improve_1 = ''\n        best_loss_val = 10\n        best_acc_val = 0\n        normalized_adj = normalize(adj, sparse=True)\n        for epoch in range(self.conf.training['n_epochs']):\n            improve_2 = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            hidden_output, output = self.model((self.feats, normalized_adj, False))\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val, hidden_output, output = self.evaluate(self.val_mask, normalized_adj)\n\n            # save\n            if acc_val > best_acc_val:\n                best_acc_val = acc_val\n                best_loss_val = loss_val\n                improve_2 = '*'\n                if acc_val > self.result['valid']:\n                    self.total_time = time.time()-self.start_time\n                    improve_1 = '*'\n                    self.best_val_loss = loss_val\n                    self.result['valid'] = acc_val\n                    self.result['train'] = acc_train\n                    self.best_iter = iter+1\n                    self.hidden_output = hidden_output\n                    self.output = output if len(output.shape)>1 else output.unsqueeze(1)\n                    self.output = F.log_softmax(self.output, dim=1)\n                    self.weights = deepcopy(self.model.state_dict())\n                    self.best_graph = deepcopy(adj)\n\n            # print\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\n        print('Iteration {:04d} | Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(iter+1,time.time()-t, best_loss_val, best_acc_val, improve_1))\n\n    def structure_learning(self, iter):\n        t=time.time()\n        self.estimator.reset_obs()\n        self.estimator.update_obs(self.knn(self.feats))   # 2\n        self.estimator.update_obs(self.knn(self.hidden_output))   # 3\n        self.estimator.update_obs(self.knn(self.output))   # 4\n        alpha, beta, O, Q, iterations = self.estimator.EM(self.output.max(1)[1].detach().cpu().numpy(), self.conf.gsl['tolerance'])\n        adj = torch.tensor(prob_to_adj(Q, self.conf.gsl['threshold']),dtype=torch.float32, device=self.device).to_sparse()\n        print('Iteration {:04d} | Time(s) {:.4f} | EM step {:04d}'.format(iter+1,time.time()-t,self.estimator.count))\n        return adj\n\n    def learn(self, debug=False):\n        '''\n        Learning process of GEN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        adj = self.adj\n\n        for iter in range(self.conf.training['n_iters']):\n            self.train_gcn(iter, adj, debug)\n            adj = self.structure_learning(iter)\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test, _, _ = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def evaluate(self, test_mask, normalized_adj):\n        '''\n        Evaluation procedure of GEN.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n        normalized_adj : torch.tensor\n            Adjacency matrix.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        hidden_output : torch.tensor        \n            Hidden output of the model.\n        output : torch.tensor        \n            Output of the model.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            hidden_output, output = self.model((self.feats, normalized_adj, False))\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), hidden_output, output\n\n    def test(self):\n        '''\n        Test procedure of GEN.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        hidden_output : torch.tensor        \n            Hidden output of the model.\n        output : torch.tensor        \n            Output of the model.\n        '''\n        self.model.load_state_dict(self.weights)\n        normalized_adj = normalize(self.best_graph, sparse=True)\n        return self.evaluate(self.test_mask, normalized_adj)\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        if self.conf.model['type']=='gcn':\n            self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                             self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n                             self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n                             self.conf.model['input_layer'], self.conf.model['output_layer'], weight_initializer='glorot',\n                             bias_initializer='zeros').to(self.device)\n        elif self.conf.model['type']=='appnp':\n            self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets,\n                               dropout=self.conf.model['dropout'], K=self.conf.model['K'],\n                               alpha=self.conf.model['alpha']).to(self.device)\n        self.estimator = GENEstimateAdj(self.n_classes, self.adj.to_dense(), self.train_mask, self.labels, self.homophily)\n        self.optim = torch.optim.Adam(self.model.parameters(),\n                                      lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])\n        self.best_iter = 0\n        self.hidden_output = None\n        self.output = None", "\n\nclass IDGLSolver(Solver):\n    '''\n    A solver to train, evaluate, test IDGL in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('idgl', 'cora')\n    >>>\n    >>> solver = IDGLSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"idgl\"\n        print(\"Solver Version : [{}]\".format(\"idgl\"))\n        self.conf = conf\n        self.run_epoch = self._scalable_run_whole_epoch if self.conf.model['scalable_run'] else self._run_whole_epoch\n        if self.conf.model['scalable_run']:\n            self.normalized_adj = normalize(self.adj, sparse=True)\n        else:\n            self.adj = self.adj.to_dense()\n            self.normalized_adj = normalize(self.adj, sparse=False)\n\n    def _run_whole_epoch(self, mode='train', debug=False):\n\n        # prepare\n        training = mode == 'train'\n        if mode == 'train':\n            idx = self.train_mask\n        elif mode == 'valid':\n            idx = self.val_mask\n        else:\n            idx = self.test_mask\n        self.model.train(training)\n        network = self.model\n\n        # The first iter\n        features = F.dropout(self.feats, self.conf.gsl['feat_adj_dropout'], training=training)\n        init_node_vec = features\n        init_adj = self.normalized_adj\n        cur_raw_adj, cur_adj = network.learn_graph(network.graph_learner, init_node_vec, self.conf.gsl['graph_skip_conn'], graph_include_self=self.conf.gsl['graph_include_self'], init_adj=init_adj)\n        # cur_raw_adj\u662f\u6839\u636e\u8f93\u5165Z\u76f4\u63a5\u4ea7\u751f\u7684adj, cur_adj\u662f\u524d\u8005\u5f52\u4e00\u5316\u5e76\u548c\u539f\u59cbadj\u52a0\u6743\u6c42\u548c\u7684\u7ed3\u679c\n        cur_raw_adj = F.dropout(cur_raw_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n        cur_adj = F.dropout(cur_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n        node_vec, output = network.encoder(init_node_vec, cur_adj)\n        score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n        loss1 = self.loss_fn(output[idx], self.labels[idx])\n        loss1 += self.get_graph_loss(cur_raw_adj, init_node_vec)\n        first_raw_adj, first_adj = cur_raw_adj, cur_adj\n\n        # the following iters\n        if training:\n            eps_adj = float(self.conf.gsl['eps_adj'])\n        else:\n            eps_adj = float(self.conf.gsl['test_eps_adj'])\n        pre_raw_adj = cur_raw_adj\n        pre_adj = cur_adj\n        loss = 0\n        iter_ = 0\n        while (iter_ == 0 or diff(cur_raw_adj, pre_raw_adj, first_raw_adj).item() > eps_adj) and iter_ < self.conf.training['max_iter']:\n            iter_ += 1\n            pre_adj = cur_adj\n            pre_raw_adj = cur_raw_adj\n            cur_raw_adj, cur_adj = network.learn_graph(network.graph_learner2, node_vec, self.conf.gsl['graph_skip_conn'], graph_include_self=self.conf.gsl['graph_include_self'], init_adj=init_adj)\n            update_adj_ratio = self.conf.gsl['update_adj_ratio']\n            cur_adj = update_adj_ratio * cur_adj + (1 - update_adj_ratio) * first_adj   # \u8fd9\u91cc\u4f3c\u4e4e\u548c\u8bba\u6587\u4e2d\u6709\u4e9b\u51fa\u5165\uff1f\uff1f\n            node_vec, output = network.encoder(init_node_vec, cur_adj, self.conf.gsl['gl_dropout'])\n            score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n            loss += self.loss_fn(output[idx], self.labels[idx])\n            loss += self.get_graph_loss(cur_raw_adj, init_node_vec)\n\n        if iter_ > 0:\n            loss = loss / iter_ + loss1\n        else:\n            loss = loss1\n\n        if training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        return loss, score, cur_adj\n\n    def _scalable_run_whole_epoch(self, mode='train', debug=False):\n\n        # prepare\n        training = mode == 'train'\n        if mode == 'train':\n            idx = self.train_mask\n        elif mode == 'valid':\n            idx = self.val_mask\n        else:\n            idx = self.test_mask\n        self.model.train(training)\n        network = self.model\n\n        # init\n        init_adj = self.normalized_adj\n        features = F.dropout(self.feats, self.conf.gsl['feat_adj_dropout'], training=training)\n        init_node_vec = features\n        init_anchor_vec, sampled_node_idx = sample_anchors(init_node_vec, self.conf.model['num_anchors'])\n\n        # the first iter\n        # Compute n x s node-anchor relationship matrix\n        cur_node_anchor_adj = network.learn_graph(network.graph_learner, init_node_vec, anchor_features=init_anchor_vec)\n        # Compute s x s anchor graph\n        cur_anchor_adj = compute_anchor_adj(cur_node_anchor_adj)\n        cur_node_anchor_adj = F.dropout(cur_node_anchor_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n        cur_anchor_adj = F.dropout(cur_anchor_adj, self.conf.gsl['feat_adj_dropout'], training=training)\n\n        first_init_agg_vec, init_agg_vec, node_vec, output = network.encoder(init_node_vec, init_adj, cur_node_anchor_adj, self.conf.gsl['graph_skip_conn'])\n        anchor_vec = node_vec[sampled_node_idx]\n        first_node_anchor_adj, first_anchor_adj = cur_node_anchor_adj, cur_anchor_adj\n        score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n        loss1 = self.loss_fn(output[idx], self.labels[idx])\n        loss1 += self.get_graph_loss(cur_anchor_adj, init_anchor_vec)\n\n        # the following iters\n        if training:\n            eps_adj = float(self.conf.gsl['eps_adj'])\n        else:\n            eps_adj = float(self.conf.gsl['test_eps_adj'])\n\n        pre_node_anchor_adj = cur_node_anchor_adj\n        loss = 0\n        iter_ = 0\n        while (iter_ == 0 or diff(cur_node_anchor_adj, pre_node_anchor_adj, cur_node_anchor_adj).item() > eps_adj) and iter_ < self.conf.training['max_iter']:\n            iter_ += 1\n            pre_node_anchor_adj = cur_node_anchor_adj\n            # Compute n x s node-anchor relationship matrix\n            cur_node_anchor_adj = network.learn_graph(network.graph_learner2, node_vec, anchor_features=anchor_vec)\n            # Compute s x s anchor graph\n            cur_anchor_adj = compute_anchor_adj(cur_node_anchor_adj)\n\n            update_adj_ratio = self.conf.gsl['update_adj_ratio']\n            _, _, node_vec, output = network.encoder(init_node_vec, init_adj, cur_node_anchor_adj, self.conf.gsl['graph_skip_conn'],\n                                           first=False, first_init_agg_vec=first_init_agg_vec, init_agg_vec=init_agg_vec, update_adj_ratio=update_adj_ratio,\n                                           dropout=self.conf.gsl['gl_dropout'], first_node_anchor_adj=first_node_anchor_adj)\n            anchor_vec = node_vec[sampled_node_idx]\n\n            score = self.metric(self.labels[idx].cpu().numpy(), output[idx].detach().cpu().numpy())\n            loss += self.loss_fn(output[idx], self.labels[idx])\n\n            loss += self.get_graph_loss(cur_anchor_adj, init_anchor_vec)\n\n        if iter_ > 0:\n            loss = loss / iter_ + loss1\n        else:\n            loss = loss1\n        if training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        return loss, score, cur_anchor_adj\n\n    def get_graph_loss(self, out_adj, features):\n        # Graph regularization\n        graph_loss = 0\n        L = torch.diagflat(torch.sum(out_adj, -1)) - out_adj\n        graph_loss += self.conf.training['smoothness_ratio'] * torch.trace(torch.mm(features.transpose(-1, -2), torch.mm(L, features))) / int(np.prod(out_adj.shape))\n        ones_vec = torch.ones(out_adj.size(-1)).to(self.device)\n        graph_loss += -self.conf.training['degree_ratio'] * torch.mm(ones_vec.unsqueeze(0), torch.log(torch.mm(out_adj, ones_vec.unsqueeze(-1)) + 1e-12)).squeeze() / out_adj.shape[-1]\n        graph_loss += self.conf.training['sparsity_ratio'] * torch.sum(torch.pow(out_adj, 2)) / int(np.prod(out_adj.shape))\n        return graph_loss\n\n    def learn(self, debug=False):\n        '''\n        Learning process of IDGL.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        wait = 0\n\n        for epoch in range(self.conf.training['max_epochs']):\n            t = time.time()\n            improve = ''\n\n            # training phase\n            loss_train, acc_train, _ = self.run_epoch(mode='train', debug=debug)\n\n            # validation phase\n            with torch.no_grad():\n                loss_val, acc_val, adj = self.run_epoch(mode='valid', debug=debug)\n\n            if loss_val < self.best_val_loss:\n                wait = 0\n                self.total_time = time.time()-self.start_time\n                self.best_val_loss = loss_val\n                self.weights = deepcopy(self.model.state_dict())\n                self.result['train'] = acc_train\n                self.result['valid'] = acc_val\n                improve = '*'\n                self.best_graph = deepcopy(adj.clone().detach())\n            else:\n                wait += 1\n                if wait == self.conf.training['patience']:\n                    print('Early stop!')\n                    break\n\n            # print\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        # test\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        self.model.load_state_dict(self.weights)\n        with torch.no_grad():\n            loss_test, acc_test, _ = self.run_epoch(mode='test', debug=debug)\n        self.result['test']=acc_test\n        print(acc_test)\n        return self.result, self.best_graph\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = IDGL(self.conf, self.dim_feats, self.num_targets).to(self.device)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])", "\n\nclass PROGNNSolver(Solver):\n    '''\n    A solver to train, evaluate, test ProGNN in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('prognn', 'cora')\n    >>>\n    >>> solver = PROGNNSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"prognn\"\n        print(\"Solver Version : [{}]\".format(\"prognn\"))\n        self.adj = self.adj.to_dense()\n\n    def train_gcn(self, epoch, debug=False):\n        normalized_adj = self.estimator.normalize()\n\n        t = time.time()\n        improve = ''\n        self.model.train()\n        self.optimizer.zero_grad()\n\n        # forward and backward\n        output = self.model((self.feats, normalized_adj, False))[-1]\n        loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n        acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n        loss_train.backward()\n        self.optimizer.step()\n\n        # evaluate\n        loss_val, acc_val = self.evaluate(self.val_mask, normalized_adj)\n        flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\n        # save best model\n        if flag:\n            self.total_time = time.time()-self.start_time\n            self.improve = True\n            self.best_val_loss = loss_val\n            self.result['train'] = acc_train\n            self.result['valid'] = acc_val\n            improve = '*'\n            self.best_graph = self.estimator.estimated_adj.clone().detach()\n            self.weights = deepcopy(self.model.state_dict())\n\n        #print\n        if debug:\n            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                epoch+1, time.time() -t, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n    def train_adj(self, epoch, debug=False):\n        estimator = self.estimator\n        t = time.time()\n        improve = ''\n        estimator.train()\n        self.optimizer_adj.zero_grad()\n\n        loss_l1 = torch.norm(estimator.estimated_adj, 1)\n        loss_fro = torch.norm(estimator.estimated_adj - self.adj, p='fro')\n        normalized_adj = estimator.normalize()\n\n        if self.conf.gsl['lambda_']:\n            loss_smooth_feat = feature_smoothing(estimator.estimated_adj, self.feats)\n        else:\n            loss_smooth_feat = 0 * loss_l1\n\n        output = self.model((self.feats, normalized_adj, False))[-1]\n        loss_gcn = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n        acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n\n        #loss_symmetric = torch.norm(estimator.estimated_adj - estimator.estimated_adj.t(), p=\"fro\")\n        #loss_differential =  loss_fro + self.conf.gamma * loss_gcn + self.conf.lambda_ * loss_smooth_feat + args.phi * loss_symmetric\n        loss_differential = loss_fro + self.conf.gsl['gamma'] * loss_gcn + self.conf.gsl['lambda_'] * loss_smooth_feat\n        loss_differential.backward()\n        self.optimizer_adj.step()\n        # we finish the optimization of the differential part above, next we need to do the optimization of loss_l1 and loss_nuclear\n\n        loss_nuclear =  0 * loss_fro\n        if self.conf.gsl['beta'] != 0:\n            self.optimizer_nuclear.zero_grad()\n            self.optimizer_nuclear.step()\n            loss_nuclear = prox_operators.nuclear_norm\n\n        self.optimizer_l1.zero_grad()\n        self.optimizer_l1.step()\n\n        total_loss = loss_fro \\\n                     + self.conf.gsl['gamma'] * loss_gcn \\\n                     + self.conf.gsl['alpha'] * loss_l1 \\\n                     + self.conf.gsl['beta'] * loss_nuclear\n                     #+ self.conf.phi * loss_symmetric\n\n        estimator.estimated_adj.data.copy_(torch.clamp(estimator.estimated_adj.data, min=0, max=1))\n\n        # evaluate\n        self.model.eval()\n        normalized_adj = estimator.normalize()\n        loss_val, acc_val = self.evaluate(self.val_mask, normalized_adj)\n        flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\n        # save the best model\n        if flag:\n            self.total_time = time.time()-self.start_time\n            self.improve = True\n            self.best_val_loss = loss_val\n            self.result['train'] = acc_train\n            self.result['valid'] = acc_val\n            improve = '*'\n            self.best_graph = estimator.estimated_adj.clone().detach()\n            self.weights = deepcopy(self.model.state_dict())\n\n        #print\n        if debug:\n            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(adj) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                epoch+1, time.time() - t, total_loss.item(), loss_val, acc_val, improve))\n\n    def learn(self, debug=False):\n        '''\n        Learning process of PROGNN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        for epoch in range(self.conf.training['n_epochs']):\n            for i in range(int(self.conf.training['outer_steps'])):\n                self.train_adj(epoch, debug=debug)\n\n            for i in range(int(self.conf.training['inner_steps'])):\n                self.train_gcn(epoch, debug=debug)\n\n            # we use earlystopping here as prognn is very slow\n            if self.improve:\n                self.wait = 0\n                self.improve = False\n            else:\n                self.wait += 1\n                if self.wait == self.conf.training['patience_iter']:\n                    print('Early stop!')\n                    break\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def evaluate(self, test_mask, normalized_adj):\n        '''\n        Evaluation procedure of PROGNN.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n        normalized_adj : torch.tensor\n            Adjacency matrix.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.eval()\n        self.estimator.eval()\n        with torch.no_grad():\n            logits = self.model((self.feats, normalized_adj, False))[-1]\n        logits = logits[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def test(self):\n        '''\n        Test procedure of PROGNN.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.load_state_dict(self.weights)\n        self.estimator.estimated_adj.data.copy_(self.best_graph)\n        normalized_adj = self.estimator.normalize()\n        return self.evaluate(self.test_mask, normalized_adj)\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        if self.conf.model['type'] == 'gcn':\n            self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                             self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n                             self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n                             self.conf.model['input_layer'], self.conf.model['output_layer'],\n                             weight_initializer='uniform').to(self.device)\n        else:\n            self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets,\n                               dropout=self.conf.model['dropout'], K=self.conf.model['K'],\n                               alpha=self.conf.model['alpha']).to(self.device)\n        self.estimator = EstimateAdj(self.adj, symmetric=self.conf.gsl['symmetric'], device=self.device).to(self.device)\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                          weight_decay=self.conf.training['weight_decay'])\n        self.optimizer_adj = torch.optim.SGD(self.estimator.parameters(), momentum=0.9, lr=self.conf.training['lr_adj'])\n        self.optimizer_l1 = PGD(self.estimator.parameters(), proxs=[prox_operators.prox_l1],\n                                lr=self.conf.training['lr_adj'], alphas=[self.conf.gsl['alpha']])\n        self.optimizer_nuclear = PGD(self.estimator.parameters(), proxs=[prox_operators.prox_nuclear_cuda],\n                                     lr=self.conf.training['lr_adj'], alphas=[self.conf.gsl['beta']])\n\n        self.wait = 0", "\n\nclass GTSolver(Solver):\n    '''\n    A solver to train, evaluate, test GT in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gt', 'cora')\n    >>>\n    >>> solver = GTSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gt\"\n        print(\"Solver Version : [{}]\".format(\"gt\"))\n        # prepare dgl graph\n        edges = self.adj.coalesce().indices().cpu()\n        self.graph = dgl.graph((edges[0], edges[1]), num_nodes=self.n_nodes, idtype=torch.int)\n        self.graph = dgl.add_self_loop(self.graph).to(self.device)\n\n\n    def learn(self, debug=False):\n        '''\n        Learning process of GRCN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        0 : constant\n        '''\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            x, output, _ = self.model(self.feats, self.graph, self.labels.cpu())\n\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val, _ = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                improve = '*'\n                self.weights = deepcopy(self.model.state_dict())\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n\n            # print\n\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test, homo_heads = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, 0\n\n    def evaluate(self, test_mask, graph_analysis=False):\n        '''\n        Evaluation procedure of GT.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n        graph_analysis : bool\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        homo_heads\n\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            x, output, homo_heads = self.model(self.feats, self.graph, self.labels.cpu(), graph_analysis)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss = self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), homo_heads\n\n    def test(self):\n        '''\n        Test procedure of GT.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        homo_heads\n        '''\n        self.model.load_state_dict(self.weights)\n        return self.evaluate(self.test_mask, graph_analysis=self.conf.analysis['graph_analysis'])\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GT(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                   self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm_type'],\n                   self.conf.model['n_heads'], self.conf.model['act'], input_layer=self.conf.model['input_layer'],\n                        ff=self.conf.model['ff'], output_layer=self.conf.model['output_layer'],\n                        use_norm=self.conf.model['use_norm'], use_redisual=self.conf.model['use_residual'],\n                        hidden_dim_multiplier=self.conf.model['hidden_dim_multiplier']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                 weight_decay=self.conf.training['weight_decay'])", "\n\nclass SLAPSSolver(Solver):\n    '''\n        A solver to train, evaluate, test SLAPS in a run.\n\n        Parameters\n        ----------\n        conf : argparse.Namespace\n            Config file.\n        dataset : opengsl.data.Dataset\n            The dataset.\n\n        Attributes\n        ----------\n        method_name : str\n            The name of the method.\n\n        Examples\n        --------\n        >>> # load dataset\n        >>> import opengsl.dataset\n        >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n        >>> # load config file\n        >>> import opengsl.config.load_conf\n        >>> conf = opengsl.config.load_conf('slaps', 'cora')\n        >>>\n        >>> solver = SLAPSSolver(conf, dataset)\n        >>> # Conduct a experiment run.\n        >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n        '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"slaps\"\n        print(\"Solver Version : [{}]\".format(\"slaps\"))\n\n    def learn(self, debug=False):\n        '''\n        Learning process of SLAPS.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n            \n            # forward and backward\n            output, loss_dae, adj = self.model(self.feats)\n            if epoch < self.conf.training['n_epochs'] // self.conf.training['epoch_d']:\n                self.model.gcn_c.eval()\n                loss_train = self.conf.training['lamda'] * loss_dae\n            else:\n                loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask]) + self.conf.training['lamda'] * loss_dae \n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n            \n            # Evaluate\n            loss_val, acc_val = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                self.total_time = time.time() - self.start_time\n                improve = '*'\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.state_dict())\n                self.best_graph = adj.clone()\n\n            #print\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n    \n    def evaluate(self, test_mask):\n        '''\n        Evaluation procedure of SLAPS.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output, _, _ = self.model(self.feats)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss = self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = SLAPS(self.n_nodes, self.dim_feats, self.num_targets, self.feats, self.device, self.conf).to(self.device)\n        self.optim = torch.optim.Adam([\n            {'params': self.model.gcn_c.parameters(), 'lr': self.conf.training['lr'], 'weight_decay': self.conf.training['weight_decay']},\n            {'params': self.model.gcn_dae.parameters(), 'lr': self.conf.training['lr_dae'], 'weight_decay': self.conf.training['weight_decay_dae']}\n        ])", "\n\nclass NODEFORMERSolver(Solver):\n    '''\n    A solver to train, evaluate, test Nodeformer in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('nodeoformer', 'cora')\n    >>>\n    >>> solver = NODEFORMERSolverSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"nodeformer\"\n        print(\"Solver Version : [{}]\".format(\"nodeformer\"))\n        edge_index = self.adj.coalesce().indices().cpu()\n        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n        adj = torch.cat([edge_index, loop_edge_index], dim=1).to(self.device)\n        self.adjs = []\n        self.adjs.append(adj)\n        for i in range(conf.model['rb_order'] - 1):  # edge_index of high order adjacency\n            adj = adj_mul(adj, adj, self.n_nodes)\n            self.adjs.append(adj)\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = NodeFormer(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, num_layers=self.conf.model['n_layers'], dropout=self.conf.model['dropout'],\n                           num_heads=self.conf.model['n_heads'], use_bn=self.conf.model['use_bn'], nb_random_features=self.conf.model['M'],\n                           use_gumbel=self.conf.model['use_gumbel'], use_residual=self.conf.model['use_residual'], use_act=self.conf.model['use_act'],\n                           use_jk=self.conf.model['use_jk'],\n                           nb_gumbel_sample=self.conf.model['K'], rb_order=self.conf.model['rb_order'], rb_trans=self.conf.model['rb_trans']).to(self.device)\n        self.model.reset_parameters()\n        self.optim = torch.optim.Adam(self.model.parameters(), weight_decay=self.conf.training['weight_decay'], lr=self.conf.training['lr'])\n\n    def learn(self, debug=False):\n        '''\n        Learning process of Nodeformer.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        0 : constant\n        '''\n\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            output, link_loss = self.model(self.feats, self.adjs, self.conf.model['tau'])\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(),\n                                    output[self.train_mask].detach().cpu().numpy())\n            loss_train -= self.conf.training['lambda'] * sum(link_loss) / len(link_loss)\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val, = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                improve = '*'\n                self.weights = deepcopy(self.model.state_dict())\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n\n            # print\n\n            if debug:\n                print(\n                    \"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                        epoch + 1, time.time() - t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, 0\n\n    def evaluate(self, test_mask):\n        '''\n        Evaluation procedure of NODEFORMER.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            output, _ = self.model(self.feats, self.adjs, self.conf.model['tau'])\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())", "\n\nclass SEGSLSolver(Solver):\n    '''\n    A solver to train, evaluate, test SEGSL in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('segsl', 'cora')\n    >>>\n    >>> solver = SEGSLSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"segsl\"\n        print(\"Solver Version : [{}]\".format(\"segsl\"))\n\n    def learn(self, debug=False):\n        '''\n        Learning process of SEGSL.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        adj = self.adj.to_dense()\n        adj.fill_diagonal_(1)\n        adj = adj.to_sparse()\n\n        for iter in range(self.conf.training['n_iters']):\n            logits = self.train_gcn(iter, adj, debug)\n            adj = self.structure_learning(logits, adj)\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test, _, _ = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def train_gcn(self, iter, adj, debug=False):\n        self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                         self.conf.model['dropout'], self.conf.model['input_dropout']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(),\n                                      lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])\n\n        if debug:\n            print('==== Iteration {:04d} ===='.format(iter+1))\n        t = time.time()\n        improve_1 = ''\n        best_loss_val = 10\n        best_acc_val = 0\n        normalized_adj = normalize(adj, add_loop=False, sparse=True)\n        for epoch in range(self.conf.training['n_epochs']):\n            improve_2 = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            hidden_output, output = self.model((self.feats, normalized_adj, False))\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val, hidden_output, output = self.evaluate(self.val_mask, normalized_adj)\n\n            # save\n            if acc_val > best_acc_val:\n                best_acc_val = acc_val\n                best_loss_val = loss_val\n                improve_2 = '*'\n                if acc_val > self.result['valid']:\n                    self.total_time = time.time()-self.start_time\n                    improve_1 = '*'\n                    self.best_val_loss = loss_val\n                    self.result['valid'] = acc_val\n                    self.result['train'] = acc_train\n                    self.best_iter = iter+1\n                    self.weights = deepcopy(self.model.state_dict())\n                    self.best_graph = deepcopy(adj)\n\n            # print\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\n        print('Iteration {:04d} | Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(iter+1,time.time()-t, best_loss_val, best_acc_val, improve_1))\n\n        return output\n\n    def structure_learning(self, logits, adj):\n        edge_index = adj.coalesce().indices().t()\n\n        k = knn_maxE1(edge_index, logits)  # edge index\u5bf9\u79f0\u6709\u81ea\u73af\n        edge_index_2 = add_knn(k, logits, edge_index)\n        weight = get_weight(logits, edge_index_2)\n        adj_matrix = get_adj_matrix(self.n_nodes, edge_index_2, weight)\n\n        code_tree = PartitionTree(adj_matrix=np.array(adj_matrix))\n        code_tree.build_coding_tree(self.conf.gsl['se'])\n\n        community, isleaf = get_community(code_tree)\n        new_edge_index = reshape(community, code_tree, isleaf,\n                                 self.conf.gsl['k'])\n        new_edge_index_2 = reshape(community, code_tree, isleaf,\n                                   self.conf.gsl['k'])\n        new_edge_index = torch.cat(\n            (new_edge_index.t(), new_edge_index_2.t()), dim=0)\n        new_edge_index, unique_idx = torch.unique(\n            new_edge_index, return_counts=True, dim=0)\n        new_edge_index = new_edge_index[unique_idx != 1].t()\n        add_num = int(new_edge_index.shape[1])\n\n        new_edge_index = torch.cat(\n            (new_edge_index.t(), edge_index.cpu()), dim=0)\n        new_edge_index = torch.unique(new_edge_index, dim=0)\n        new_edge_index = new_edge_index.t()\n        new_weight = get_weight(logits, new_edge_index.t())\n        _, delete_idx = torch.topk(new_weight,\n                                   k=add_num,\n                                   largest=False)\n        delete_mask = torch.ones(\n            new_edge_index.t().shape[0]).bool()\n        delete_mask[delete_idx] = False\n        new_edge_index = new_edge_index.t()[delete_mask].t()  # \u5f97\u5230\u65b0\u7684edge_index\u4e86\n\n        graph = dgl.graph((new_edge_index[0], new_edge_index[1]),\n                          num_nodes=self.n_nodes).to(self.device)\n        graph = dgl.remove_self_loop(graph)\n        graph = dgl.add_self_loop(graph)\n        adj = graph.adj().to(self.device)\n        return adj\n\n    def evaluate(self, test_mask, normalized_adj):\n        '''\n        Evaluation procedure of SEGSL.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n        normalized_adj : torch.tensor\n            Adjacency matrix.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        hidden_output : torch.tensor        \n            Hidden output of the model.\n        output : torch.tensor        \n           Output of the model.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            hidden_output, output = self.model((self.feats, normalized_adj, False))\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy()), hidden_output, output\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.best_iter = 0\n\n    def test(self):\n        '''\n        Test procedure of SEGSL.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        hidden_output : torch.tensor        \n            Hidden output of the model.\n        output : torch.tensor        \n           Output of the model.\n        '''\n        self.model.load_state_dict(self.weights)\n        normalized_adj = normalize(self.best_graph, add_loop=False, sparse=True)\n        return self.evaluate(self.test_mask, normalized_adj)", "\n\nclass SUBLIMESolver(Solver):\n    '''\n    A solver to train, evaluate, test SUBLIME in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('sublime', 'cora')\n    >>>\n    >>> solver = SUBLIMESolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"sublime\"\n        print(\"Solver Version : [{}]\".format(\"sublime\"))\n\n    def loss_gcl(self, model, graph_learner, features, anchor_adj):\n\n        # view 1: anchor graph\n        if self.conf.maskfeat_rate_anchor:\n            mask_v1, _ = get_feat_mask(features, self.conf.maskfeat_rate_anchor)\n            features_v1 = features * (1 - mask_v1)\n        else:\n            features_v1 = copy.deepcopy(features)\n\n        z1, _ = model(features_v1, anchor_adj, 'anchor')\n\n        # view 2: learned graph\n        if self.conf.maskfeat_rate_learner:\n            mask, _ = get_feat_mask(features, self.conf.maskfeat_rate_learner)\n            features_v2 = features * (1 - mask)\n        else:\n            features_v2 = copy.deepcopy(features)\n\n        learned_adj = graph_learner(features)   # \u8fd9\u4e2alearned adj\u662f\u6709\u81ea\u73af\u7684\n        if not self.conf.sparse:\n            learned_adj = (learned_adj + learned_adj.T) / 2\n            learned_adj = normalize(learned_adj, add_loop=False, sparse=False)\n\n        z2, _ = model(features_v2, learned_adj, 'learner')\n\n        # compute loss\n        if self.conf.contrast_batch_size:\n            node_idxs = list(range(features.shape[0]))\n            batches = split_batch(node_idxs, self.conf.contrast_batch_size)\n            loss = 0\n            for batch in batches:\n                weight = len(batch) / features.shape[0]\n                loss += model.calc_loss(z1[batch], z2[batch]) * weight\n        else:\n            loss = model.calc_loss(z1, z2)\n\n        return loss, learned_adj\n\n    def train_gcn(self, adj, debug=False):\n        model = GCN_SUB(nfeat=self.dim_feats, nhid=self.conf.hidden_dim_cls, nclass=self.num_targets,\n                        n_layers=self.conf.n_layers_cls, dropout=self.conf.dropout_cls,\n                        dropout_adj=self.conf.dropedge_cls, sparse=self.conf.sparse).to(self.device)\n        optim = torch.optim.Adam(model.parameters(), lr=self.conf.lr_cls, weight_decay=self.conf.w_decay_cls)\n        t = time.time()\n        improve_1 = ''\n        best_loss_val = 10\n        best_acc_val = 0\n        for epoch in range(self.conf.epochs_cls):\n            improve_2 = ''\n            t0 = time.time()\n            model.train()\n            optim.zero_grad()\n\n            # forward and backward\n            output = model(self.feats, adj)\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            optim.step()\n\n            # Evaluate\n            loss_val, acc_val = self.evaluate(model, self.val_mask, adj)\n\n            # save\n            if acc_val > best_acc_val:\n                best_acc_val = acc_val\n                best_loss_val = loss_val\n                improve_2 = '*'\n                if acc_val > self.result['valid']:\n                    self.total_time = time.time()-self.start_time\n                    improve_1 = '*'\n                    self.best_val_loss = loss_val\n                    self.result['valid'] = acc_val\n                    self.result['train'] = acc_train\n                    self.weights = deepcopy(model.state_dict())\n                    current_adj = dgl_graph_to_torch_sparse(adj).to_dense() if self.conf.sparse else adj\n                    self.best_graph = deepcopy(current_adj)\n                    self.best_graph_test = deepcopy(adj)\n\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve_2))\n\n\n        print('Time(s) {:.4f} | Loss(val):{:.4f} | Acc(val):{:.4f} | {}'.format(time.time()-t, best_loss_val, best_acc_val, improve_1))\n\n    def evaluate(self, model, test_mask, adj):\n        '''\n        Evaluation procedure of GRCN.\n\n        Parameters\n        ----------\n        model : torch.nn.Module\n            model.\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n        adj : torch.tensor\n            Adjacency matrix.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        model.eval()\n        with torch.no_grad():\n            output = model(self.feats, adj)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss=self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def test(self):\n        '''\n        Test procedure of SUBLIME.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        metric : float\n            Evaluation metric.\n        '''\n        model = GCN_SUB(nfeat=self.dim_feats, nhid=self.conf.hidden_dim_cls, nclass=self.num_targets,\n                        n_layers=self.conf.n_layers_cls, dropout=self.conf.dropout_cls,\n                        dropout_adj=self.conf.dropedge_cls, sparse=self.conf.sparse).to(self.device)\n        model.load_state_dict(self.weights)\n        adj = self.best_graph_test\n        return self.evaluate(model, self.test_mask, adj)\n\n    def learn(self, debug=False):\n        '''\n        Learning process of SUBLIME.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        anchor_adj = normalize(self.anchor_adj_raw, add_loop=False, sparse=self.conf.sparse)\n\n        if self.conf.sparse:\n            anchor_adj_torch_sparse = copy.deepcopy(anchor_adj)\n            anchor_adj = torch_sparse_to_dgl_graph(anchor_adj)\n\n        for epoch in range(1, self.conf.epochs + 1):\n\n            # Contrastive Learning\n            self.model.train()\n            self.graph_learner.train()\n\n            loss, Adj = self.loss_gcl(self.model, self.graph_learner, self.feats, anchor_adj)   # Adj\u662f\u6709\u81ea\u73af\u4e14normalized\n\n            self.optimizer_cl.zero_grad()\n            self.optimizer_learner.zero_grad()\n            loss.backward()\n            self.optimizer_cl.step()\n            self.optimizer_learner.step()\n\n            # Structure Bootstrapping\n            if (1 - self.conf.tau) and (self.conf.c == 0 or epoch % self.conf.c == 0):\n                if self.conf.sparse:\n                    learned_adj_torch_sparse = dgl_graph_to_torch_sparse(Adj).to(self.device)\n                    anchor_adj_torch_sparse = anchor_adj_torch_sparse * self.conf.tau \\\n                                              + learned_adj_torch_sparse * (1 - self.conf.tau)\n                    anchor_adj = torch_sparse_to_dgl_graph(anchor_adj_torch_sparse)\n                else:\n                    anchor_adj = anchor_adj * self.conf.tau + Adj.detach() * (1 - self.conf.tau)\n\n            if debug:\n                print(\"Epoch {:05d} | CL Loss {:.4f}\".format(epoch, loss.item()))\n\n            # Evaluate via Node Classification\n            if epoch % self.conf.eval_freq == 0:\n                self.model.eval()\n                self.graph_learner.eval()\n                f_adj = Adj\n\n                if self.conf.sparse:\n                    f_adj.edata['w'] = f_adj.edata['w'].detach()\n                else:\n                    f_adj = f_adj.detach()\n\n                self.train_gcn(f_adj, debug)\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        if self.conf.sparse:\n            self.anchor_adj_raw = self.adj\n        else:\n            self.anchor_adj_raw = self.adj.to_dense()\n        anchor_adj = normalize(self.anchor_adj_raw, add_loop=False, sparse=self.conf.sparse)\n        if self.conf.type_learner == 'fgp':\n            self.graph_learner = FGP_learner(self.feats.cpu(), self.conf.k, self.conf.sim_function, 6, self.conf.sparse)\n        elif self.conf.type_learner == 'mlp':\n            self.graph_learner = MLP_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n                                 self.conf.activation_learner)\n        elif self.conf.type_learner == 'att':\n            self.graph_learner = ATT_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n                                      self.conf.activation_learner)\n        elif self.conf.type_learner == 'gnn':\n            self.graph_learner = GNN_learner(2, self.feats.shape[1], self.conf.k, self.conf.sim_function, 6, self.conf.sparse,\n                                 self.conf.activation_learner, anchor_adj)\n        self.graph_learner = self.graph_learner.to(self.device)\n        self.model = GCL(nlayers=self.conf.n_layers, in_dim=self.dim_feats, hidden_dim=self.conf.n_hidden,\n                    emb_dim=self.conf.n_embed, proj_dim=self.conf.n_proj,\n                    dropout=self.conf.dropout, dropout_adj=self.conf.dropedge_rate, sparse=self.conf.sparse,\n                         conf=self.conf).to(self.device)\n        self.optimizer_cl = torch.optim.Adam(self.model.parameters(), lr=self.conf.lr, weight_decay=self.conf.wd)\n        self.optimizer_learner = torch.optim.Adam(self.graph_learner.parameters(), lr=self.conf.lr,\n                                             weight_decay=self.conf.wd)", "\n\nclass STABLESolver(Solver):\n    '''\n    A solver to train, evaluate, test Stable in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('stable', 'cora')\n    >>>\n    >>> solver = STABLESolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"stable\"\n        print(\"Solver Version : [{}]\".format(\"stable\"))\n        self.adj = sparse_tensor_to_scipy_sparse(self.adj)\n        self.processed_adj = preprocess_adj(self.feats.cpu().numpy(), self.adj, threshold=self.conf.jt)\n\n    def pretrain(self, debug=False):\n\n        # generate 2 augment views\n        adj_delete = self.adj - self.processed_adj\n        aug_adj1 = aug_random_edge(self.processed_adj, adj_delete=adj_delete, recover_percent=self.conf.recover_percent)  # random drop edges\n        aug_adj2 = aug_random_edge(self.processed_adj, adj_delete=adj_delete, recover_percent=self.conf.recover_percent)  # random drop edges\n        sp_adj = normalize_sp_matrix(self.processed_adj+(sp.eye(self.n_nodes) * self.conf.beta),\n                                  add_loop=False)\n        sp_aug_adj1 = normalize_sp_matrix(aug_adj1 + (sp.eye(self.n_nodes) * self.conf.beta),\n                                  add_loop=False)\n        sp_aug_adj2 = normalize_sp_matrix(aug_adj2 + (sp.eye(self.n_nodes) * self.conf.beta),\n                                  add_loop=False)\n        sp_adj = scipy_sparse_to_sparse_tensor(sp_adj).to(self.device)\n        sp_aug_adj1 = scipy_sparse_to_sparse_tensor(sp_aug_adj1).to(self.device)\n        sp_aug_adj2 = scipy_sparse_to_sparse_tensor(sp_aug_adj2).to(self.device)\n\n        # contrastive learning\n        weights = None\n        wait = 0\n        best = 1e9\n        best_t = 0\n        b_xent = torch.nn.BCEWithLogitsLoss()\n        for epoch in range(self.conf.pretrain['n_epochs']):\n            self.model.train()\n            self.optim.zero_grad()\n\n            idx = np.random.permutation(self.n_nodes)\n            shuf_fts = self.feats.unsqueeze(0)[:, idx, :]\n\n            lbl_1 = torch.ones(1, self.n_nodes)\n            lbl_2 = torch.zeros(1, self.n_nodes)\n            lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n\n            logits = self.model(self.feats.unsqueeze(0), shuf_fts, sp_adj, sp_aug_adj1, sp_aug_adj2)\n            loss = b_xent(logits, lbl)\n            if debug:\n                print(loss)\n\n            if loss < best:\n                best = loss\n                best_t = epoch\n                wait = 0\n                weights = copy.deepcopy(self.model.state_dict())\n            else:\n                wait+=1\n            if wait == self.conf.pretrain['patience']:\n                print('Early stopping!')\n                break\n\n            loss.backward()\n            self.optim.step()\n\n        print('Loading {}th epoch'.format(best_t))\n        self.model.load_state_dict(weights)\n\n        return self.model.embed(self.feats.unsqueeze(0), sp_adj)\n\n    def train_gcn(self, feats, adj, debug=False):\n        def evaluate(model, test_mask):\n            model.eval()\n            with torch.no_grad():\n                output = model((feats, adj, True))\n            logits = output[test_mask]\n            labels = self.labels[test_mask]\n            loss = self.loss_fn(logits, labels)\n            return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n        def test(model):\n            return evaluate(model, self.test_mask)\n\n\n        model = GCN(self.conf.n_embed, self.conf.n_hidden, self.num_targets, self.conf.n_layers, self.conf.dropout).to(self.device)\n        optim = torch.optim.Adam(model.parameters(), lr=self.conf.lr, weight_decay=self.conf.weight_decay)\n        best_loss_val = 10\n        for epoch in range(self.conf.n_epochs):\n            improve = ''\n            t0 = time.time()\n            model.train()\n            optim.zero_grad()\n\n            # forward and backward\n            output = model((feats, adj, True))\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            optim.step()\n\n            # Evaluate\n            loss_val, acc_val = evaluate(model, self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                improve = '*'\n                self.total_time = time.time() - self.start_time\n                best_loss_val = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                weights = deepcopy(model.state_dict())\n\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        model.load_state_dict(weights)\n        loss_test, acc_test = test(model)\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result\n\n    def learn(self, debug=False):\n        '''\n        Learning process of STABLE.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        embeds = self.pretrain(debug)\n        embeds = embeds.squeeze(dim=0)\n\n        # prunue the graph\n        adj_clean = preprocess_adj(embeds.cpu().numpy(), self.adj, jaccard=False, threshold=self.conf.cos)\n        adj_clean = scipy_sparse_to_sparse_tensor(adj_clean).to(self.device).to_dense()\n        # add k neighbors\n        get_reliable_neighbors(adj_clean, embeds, k=self.conf.k, degree_threshold=self.conf.threshold)\n        # \u5f97\u5230\u7684\u662f0-1 \u65e0\u81ea\u73af\u7684\u56fe\n\n        normalized_adj_clean = normalize(adj_clean, sparse=False)   # \u672a\u4f7f\u7528\u8bba\u6587\u4e2d\u5bf9\u5f52\u4e00\u5316\u7684\u6539\u8fdb\n        result = self.train_gcn(embeds, normalized_adj_clean, debug)\n        return result, adj_clean\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = DGI(self.dim_feats, self.conf.n_embed, 'prelu').to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.pretrain['lr'], weight_decay=self.conf.pretrain['weight_decay'])", "        \n        \nclass COGSLSolver(Solver):\n    '''\n    A solver to train, evaluate, test CoGSL in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('cogsl', 'cora')\n    >>>\n    >>> solver = COGSLSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"cogsl\"\n        print(\"Solver Version : [{}]\".format(\"CoGSL\"))\n        edge_index = self.adj.coalesce().indices().cpu()\n        loop_edge_index = torch.stack([torch.arange(self.n_nodes), torch.arange(self.n_nodes)])\n        edges = torch.cat([edge_index, loop_edge_index], dim=1)\n        self.adj = torch.sparse.FloatTensor(edges, torch.ones(edges.shape[1]), [self.n_nodes, self.n_nodes]).to(\n            self.device).coalesce()\n        if self.conf.dataset['init'] :\n            _view1 = eval(\"self.\"+self.conf.dataset[\"name_view1\"]+\"()\")\n            self.view1_indices = self.get_indices(self.conf.dataset[\"view1_indices\"], _view1, self.conf.dataset[\"view1_k\"])\n            _view2 = eval(\"self.\"+self.conf.dataset[\"name_view2\"]+\"()\")\n            self.view2_indices = self.get_indices(self.conf.dataset[\"view2_indices\"], _view2, self.conf.dataset[\"view2_k\"])\n        else:\n            _view1 = sp.load_npz(self.conf.dataset['view1_path'])\n            _view2 = sp.load_npz(self.conf.dataset['view2_path'])\n            self.view1_indices = torch.load(self.conf.dataset['view1_indices_path'])\n            self.view2_indices = torch.load(self.conf.dataset['view2_indices_path'])\n        self.view1 = scipy_sparse_to_sparse_tensor(normalize_sp_matrix(_view1, False))\n        self.view2 = scipy_sparse_to_sparse_tensor(normalize_sp_matrix(_view2, False))\n        self.loss_fn = F.binary_cross_entropy if self.num_targets == 1 else F.nll_loss\n        #self.train_mask = np.load('/root/dataset/citeseer/train.npy')\n        #self.valid_mask = np.load('/root/dataset/citeseer/val.npy')\n        #self.test_mask = np.load('/root/dataset/citeseer/test.npy')\n        #print(self.view1_indices.shape)\n        #print(self.view1.shape)\n        #print(self.view2_indices.shape)\n        #print(self.view2.shape)\n\n    def view_knn(self):\n        adj = np.zeros((self.n_nodes, self.n_nodes), dtype=np.int64)\n        dist = cos(self.feats.cpu())\n        col = np.argpartition(dist, -(self.conf.dataset['knn_k'] + 1), axis=1)[:, -(self.conf.dataset['knn_k'] + 1):].flatten()\n        adj[np.arange(self.n_nodes).repeat(self.conf.dataset['knn_k'] + 1), col] = 1\n        return sp.coo_matrix(adj)\n\n    def view_adj(self):\n        return sparse_tensor_to_scipy_sparse(self.adj)\n\n    def view_diff(self):\n        adj = sparse_tensor_to_scipy_sparse(self.adj)\n        at = normalize_sp_matrix(adj,False)\n        result = self.conf.dataset['diff_alpha'] * sp.linalg.inv(sp.eye(adj.shape[0]) - (1 - self.conf.dataset['diff_alpha']) * at)\n        return result\n\n    def view_sub(self):\n        adj = sparse_tensor_to_scipy_sparse(self.adj)\n        adj_ = sp.triu(sp.coo_matrix(adj), 1)\n        adj_cand = np.array(adj_.nonzero())\n        dele_num = int(self.conf.dataset['sub_rate'] * adj_cand.shape[1])\n        adj_sele = np.random.choice(np.arange(adj_cand.shape[1]), dele_num, replace=False)\n        adj_sele = adj_cand[:, adj_sele]\n        adj_new = sp.coo_matrix((np.ones(adj_sele.shape[1]), (adj_sele[0, :], adj_sele[1, :])), shape=adj_.shape)\n        adj_new = adj_new + adj_new.T + sp.eye(adj_new.shape[0])\n        return adj_new\n\n\n\n    def get_khop_indices(self, k, view):\n        view = (view.A > 0).astype(\"int32\")\n        view_ = view\n        for i in range(1, k):\n            view_ = (np.matmul(view_, view.T)>0).astype(\"int32\")\n        view_ = torch.tensor(view_).to_sparse()\n        #print(view_)\n        return view_.indices()\n\n    def topk(self, k, _adj):\n        adj = _adj.todense()\n        pos = np.zeros(adj.shape)\n        for i in range(len(adj)):\n            one = adj[i].nonzero()[1]\n            if len(one)>k:\n                oo = np.argsort(-adj[i, one])\n                sele = one[oo[0,:k]]\n                pos[i, sele] = adj[i, sele]\n            else:\n                pos[i, one] = adj[i, one]\n        return pos\n\n    def get_indices(self, val, adj, k):\n        if (k == 0):\n            return self.get_khop_indices(val, sp.coo_matrix((adj)))\n        else:\n            kn = self.topk(k, adj)\n            return self.get_khop_indices(val, sp.coo_matrix((kn)))\n\n    def train_mi(self, x, views):\n        vv1, vv2, v1v2 = self.model.get_mi_loss(x, views)\n        return self.conf.model['mi_coe'] * v1v2 + (vv1 + vv2) * (1 - self.conf.model['mi_coe']) / 2\n\n    def loss_acc(self, output, y):\n        loss = self.loss_fn(output, y)\n        acc = self.metric(y.cpu().numpy(),output.detach().cpu().numpy())\n        return loss, acc\n\n    def train_cls(self):\n        new_v1, new_v2 = self.model.get_view(self.view1, self.view1_indices, self.view2, self.view2_indices, self.n_nodes, self.feats)\n        logits_v1, logits_v2, prob_v1, prob_v2 = self.model.get_cls_loss(new_v1, new_v2, self.feats)\n        curr_v = self.model.get_fusion(new_v1, prob_v1, new_v2, prob_v2)\n        logits_v = self.model.get_v_cls_loss(curr_v, self.feats)\n\n        views = [curr_v, new_v1, new_v2]\n\n        loss_v1, _ = self.loss_acc(logits_v1[self.train_mask], self.labels[self.train_mask])\n        loss_v2, _ = self.loss_acc(logits_v2[self.train_mask], self.labels[self.train_mask])\n        loss_v, _ = self.loss_acc(logits_v[self.train_mask], self.labels[self.train_mask])\n        return self.conf.model['cls_coe'] * loss_v + (loss_v1 + loss_v2) * (1 - self.conf.model['cls_coe']) / 2, views\n\n\n    def learn(self, debug=False):\n        '''\n        Learning process of CoGSL.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        graph : torch.tensor\n            The learned structure.\n        '''\n        self.best_acc_val = 0\n        self.best_loss_val = 1e9\n        self.best_test = 0\n        self.best_v_cls_weight = None\n        torch.autograd.set_detect_anomaly(True)\n\n        for epoch in range(self.conf.training['main_epoch']):\n            curr = np.log(1 + self.conf.training['temp_r'] * epoch)\n            curr = min(max(0.05, curr), 0.1)\n            for inner_ne in range(self.conf.training['inner_ne_epoch']):\n                self.model.train()\n                self.opti_ve.zero_grad()\n                cls_loss, views = self.train_cls()\n                mi_loss = self.train_mi(self.feats, views)\n                loss = cls_loss - curr * mi_loss\n                #with torch.autograd.detect_anomaly():\n                loss.backward()\n                self.opti_ve.step()\n            self.scheduler.step()\n            for inner_cls in range(self.conf.training['inner_cls_epoch']):\n                self.model.train()\n                self.opti_cls.zero_grad()\n                cls_loss, _ = self.train_cls()\n                #with torch.autograd.detect_anomaly():\n                cls_loss.backward()\n                self.opti_cls.step()\n\n            for inner_mi in range(self.conf.training['inner_mi_epoch']):\n                self.model.train()\n                self.opti_mi.zero_grad()\n                _, views = self.train_cls()\n                mi_loss = self.train_mi(self.feats, views)\n                mi_loss.backward()\n                self.opti_mi.step()\n\n            self.model.eval()\n            _, views = self.train_cls()\n            self.view = views[0]\n\n            loss_val, acc_val = self.evaluate(self.val_mask)\n            loss_train, acc_train = self.evaluate(self.train_mask)\n\n            if acc_val >= self.best_acc_val and self.best_loss_val > loss_val:\n                \n                self.best_acc_val = max(acc_val, self.best_acc_val)\n                self.best_loss_val = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n                self.weights = deepcopy(self.model.cls.encoder_v.state_dict())\n                self.best_graph = views[0]\n            print(\"EPOCH \",epoch, \"\\tCUR_LOSS_VAL \", loss_val, \"\\tCUR_ACC_Val \", acc_val, \"\\tBEST_ACC_VAL \", self.best_acc_val)\n        self.total_time = time.time() - self.start_time\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        #test_f1_macro, test_f1_micro, auc  = self.test()\n        self.result['test'] = acc_test\n        #print(\"Test_Macro: \", test_f1_macro, \"\\tTest_Micro: \", test_f1_micro, \"\\tAUC: \", auc)\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, self.best_graph.to_dense()\n\n\n    def evaluate(self, test_mask):\n        '''\n        Evaluation procedure of CoGSL.\n\n        Parameters\n        ----------\n        test_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the data set.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        '''\n        logits = self.model.get_v_cls_loss(self.view, self.feats)\n\n        return self.loss_acc(logits[test_mask], self.labels[test_mask])\n\n    def set_method(self):\n        self.model = CoGSL(self.dim_feats, self.conf.model['cls_hid_1'], self.num_targets, self.conf.model['gen_hid'],\n                           self.conf.model['mi_hid_1'], self.conf.model['com_lambda_v1'], self.conf.model['com_lambda_v2'],\n                           self.conf.model['lam'], self.conf.model['alpha'], self.conf.model['cls_dropout'],\n                           self.conf.model['ve_dropout'], self.conf.model['tau'], self.conf.dataset['pyg'],\n                           self.conf.dataset['big'], self.conf.dataset['batch'], self.conf.dataset['name']).to(self.device)\n        self.opti_ve = torch.optim.Adam(self.model.ve.parameters(), lr=self.conf.training['ve_lr'], weight_decay=self.conf.training['ve_weight_decay'])\n        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opti_ve, 0.99)\n        self.opti_cls = torch.optim.Adam(self.model.cls.parameters(), lr=self.conf.training['cls_lr'], weight_decay=self.conf.training['cls_weight_decay'])\n        self.opti_mi = torch.optim.Adam(self.model.mi.parameters(), lr=self.conf.training['mi_lr'], weight_decay=self.conf.training['mi_weight_decay'])\n\n        self.view1 = self.view1.to(self.device)\n        \n        self.view2 = self.view2.to(self.device)\n        self.view1_indices = self.view1_indices.to(self.device)\n        self.view2_indices = self.view2_indices.to(self.device)\n\n    #def gen_auc_mima(self, logits, label):\n    #        preds = torch.argmax(logits, dim=1)\n    #        test_f1_macro = f1_score(label.cpu(), preds.cpu(), average='macro')\n    #        test_f1_micro = f1_score(label.cpu(), preds.cpu(), average='micro')\n    #        \n    #        best_proba = F.softmax(logits, dim=1)\n    #        if logits.shape[1] != 2:\n    #            auc = roc_auc_score(y_true=label.detach().cpu().numpy(),\n    #                                                    y_score=best_proba.detach().cpu().numpy(),\n    #                                                    multi_class='ovr'\n    #                                                    )\n    #        else:\n    #            auc = roc_auc_score(y_true=label.detach().cpu().numpy(),\n    #                                                    y_score=best_proba[:,1].detach().cpu().numpy()\n    #                                                    )\n    #        return test_f1_macro, test_f1_micro, auc\n\n    def test(self):\n        '''\n        Test procedure of CoGSL.\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        '''\n        self.model.cls.encoder_v.load_state_dict(self.weights)\n        self.model.eval()\n        self.view = self.best_graph\n\n        return self.evaluate(self.test_mask)", "\n\n\nclass WSGNNSolver(Solver):\n    '''\n    A solver to train, evaluate, test WSGNN in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('wsgnn', 'cora')\n    >>>\n    >>> solver = WSGNNSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, new_structure = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = 'wsgnn'\n        self.edge_index = self.adj.coalesce().indices()\n\n\n    def learn(self, debug=False):\n        '''\n        Learning process of WSGNN.\n\n        Parameters\n        ----------\n        debug : bool\n            Whether to print statistics during training.\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        '''    \n        best_node_val = 0\n        best_node_test = 0\n        best_node_epoch = -1\n        for epoch in range(self.conf.training['n_epochs']):\n            self.model.train()\n            self.optimizer.zero_grad()\n\n            p_y, _, q_y, _ = self.model(self.feats, self.n_nodes, self.edge_index)\n            p_y = torch.nn.functional.log_softmax(p_y, dim=1)\n            q_y = torch.nn.functional.log_softmax(q_y, dim=1)\n            mask = torch.zeros(self.n_nodes, dtype=bool)\n            mask[self.train_mask] = 1\n            loss = self.criterion(self.labels, mask, p_y, q_y, )\n            loss.backward()\n            self.optimizer.step()\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), q_y[self.train_mask].detach().cpu().numpy()) \n            loss_val, acc_val = self.evaluate(self.val_mask)\n            flag, flag_earlystop = self.recoder.add(loss_val, acc_val)\n\n            if flag:\n                self.total_time = time.time() - self.start_time\n                best_loss = loss_val\n                self.result['train'] = acc_train\n                self.result['valid'] = acc_val\n                self.weights = deepcopy(self.model.state_dict())\n\n            if (epoch + 1) % 10 == 0:\n                print(f'Epoch: {epoch:02d}, '\n                    f'Loss: {loss:.4f}, '\n                    f'Train_acc: {100 * acc_train:.2f}%, '\n                    f'Valid_acc: {100 * acc_val:.2f}%, ')\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, None\n\n\n\n    def evaluate(self, val_mask):\n        '''\n        Evaluation procedure of CoGSL.\n\n        Parameters\n        ----------\n        val_mask : torch.tensor\n\n        Returns\n        -------\n        loss : float\n            Evaluation loss.\n        '''\n        self.model.eval()\n        with torch.no_grad():\n            p_y, _, q_y, _ = self.model(self.feats, self.n_nodes, self.edge_index)\n        p_y = torch.nn.functional.log_softmax(p_y, dim=1)\n        q_y = torch.nn.functional.log_softmax(q_y, dim=1)\n        mask = torch.zeros(self.n_nodes, dtype=bool)\n        mask[val_mask] = 1\n        loss = self.criterion(self.labels, mask, p_y, q_y)\n        acc = self.metric(self.labels[val_mask].cpu().numpy(), q_y[val_mask].detach().cpu().numpy())\n        return loss, acc\n\n    def set_method(self):\n        self.model = WSGNN(self.conf.model['graph_skip_conn'], self.conf.model['n_hidden'], self.conf.model['dropout'],self.conf.model['n_layers'], \n                           self.conf.model['graph_learn_num_pers'], self.conf.model['mlp_layers'], self.conf.model['no_bn'], self.dim_feats,self.n_nodes,self.num_targets).to(self.device)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])\n        self.criterion = ELBONCLoss()"]}
{"filename": "opengsl/method/gnnsolver.py", "chunked_list": ["from .models.gnn_modules import SGC, LPA, MLP, LINK, LINKX, APPNP, GPRGNN, GAT\nimport time\nfrom .models.gcn import GCN\nfrom .models.jknet import JKNet\nfrom .solver import Solver\nimport torch\nfrom copy import deepcopy\nfrom opengsl.data.preprocess.normalize import normalize\n\n\nclass SGCSolver(Solver):\n    '''\n    A solver to train, evaluate, test SGC in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('sgc', 'cora')\n    >>>\n    >>> solver = SGCSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"sgc\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.feats, self.normalized_adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = SGC(self.dim_feats, self.num_targets, self.conf.model['n_layers']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                           weight_decay=self.conf.training['weight_decay'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])", "\n\nclass SGCSolver(Solver):\n    '''\n    A solver to train, evaluate, test SGC in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('sgc', 'cora')\n    >>>\n    >>> solver = SGCSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"sgc\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.feats, self.normalized_adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = SGC(self.dim_feats, self.num_targets, self.conf.model['n_layers']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                           weight_decay=self.conf.training['weight_decay'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])", "\n\nclass GCNSolver(Solver):\n    '''\n    A solver to train, evaluate, test GCN in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gcn', 'cora')\n    >>>\n    >>> solver = GCNSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gcn\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        True : constant bool\n        '''\n        return self.feats, self.normalized_adj, True\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GCN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                    self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n                    self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n                    self.conf.model['input_layer'], self.conf.model['output_layer']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])", "\n\nclass LPASolver(Solver):\n    '''\n    A solver to test LPA in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('lpa', 'cora')\n    >>>\n    >>> solver = LPASolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"lpa\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.labels : torch.tensor\n            Node labels.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        self.train_mask : torch.tensor\n            A boolean tensor indicating whether the node is in the training set.\n        '''\n        return self.labels, self.normalized_adj, self.train_mask\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = LPA(self.conf.model['n_layers'], self.conf.model['alpha']).to(self.device)\n        self.normalize = normalize if self.conf.dataset['normalize'] else lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, add_loop=self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])\n\n    def learn(self, split=None, debug=False):\n        '''\n        Learning process of LPA.\n\n        Parameters\n        ----------\n        debug : bool\n\n        Returns\n        -------\n        result : dict\n            A dict containing train, valid and test metrics.\n        0 : constant\n\n        '''\n        y_pred = self.model(self.input_distributer())\n        loss_test = self.loss_fn(y_pred[self.test_mask], self.labels[self.test_mask])\n        acc_test = self.metric(self.labels[self.test_mask].cpu().numpy(), y_pred[self.test_mask].detach().cpu().numpy())\n\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n        return self.result, 0", "\n\nclass MLPSolver(Solver):\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"mlp\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        '''\n        return self.feats\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = MLP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])", "\n\nclass LINKSolver(Solver):\n    '''\n    A solver to train, evaluate, test LINK in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('link', 'cora')\n    >>>\n    >>> solver = LINKSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"link\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.adj : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = LINK(self.n_nodes, self.num_targets).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])", "\n\nclass LINKXSolver(Solver):\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"linkx\"\n\n    def input_distributer(self):\n        return self.feats, self.adj\n\n    def set_method(self):\n        self.model = LINKX(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'], self.n_nodes).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])", "\n\nclass APPNPSolver(Solver):\n    '''\n    A solver to train, evaluate, test APPNP in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('appnp', 'cora')\n    >>>\n    >>> solver = APPNPSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"appnp\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.feats, self.normalized_adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = APPNP(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, dropout=self.conf.model['dropout'],\n                      K=self.conf.model['K'], alpha=self.conf.model['alpha']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                 weight_decay=self.conf.training['weight_decay'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], sparse=self.conf.dataset['sparse'])", "\n\nclass JKNetSolver(Solver):\n    '''\n    A solver to train, evaluate, test JKNet in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('jknet', 'cora')\n    >>>\n    >>> solver = JKNetSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"jknet\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        True : constant bool\n        '''\n        return self.feats, self.normalized_adj, True\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = JKNet(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                           self.conf.model['dropout'], self.conf.model['input_dropout'], self.conf.model['norm'],\n                           self.conf.model['n_linear'], self.conf.model['spmm_type'], self.conf.model['act'],\n                           self.conf.model['general'],\n                           self.conf.model['input_layer'], self.conf.model['output_layer']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                      weight_decay=self.conf.training['weight_decay'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], self.conf.dataset['sparse'])", "\n\nclass GPRGNNSolver(Solver):\n    '''\n    A solver to train, evaluate, test GPRGNN in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gprgnn', 'cora')\n    >>>\n    >>> solver = GPRGNNSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gprgnn\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.normalized_adj : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.feats, self.normalized_adj\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GPRGNN(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, dropout=self.conf.model['dropout'],\n                      dprate=self.conf.model['dprate'], K=self.conf.model['K'], alpha=self.conf.model['alpha'], init=self.conf.model['init']).to(self.device)\n        self.optim = torch.optim.Adam([{\n                'params': self.model.lin1.parameters(),\n                'weight_decay': self.conf.training['weight_decay'], 'lr': self.conf.training['lr']\n            }, {\n                'params': self.model.lin2.parameters(),\n                'weight_decay': self.conf.training['weight_decay'], 'lr': self.conf.training['lr']\n            }, {\n                'params': self.model.temp,\n                'weight_decay': 0.0, 'lr': self.conf.training['lr']\n            }], lr=self.conf.training['lr'])\n        if self.conf.dataset['normalize']:\n            self.normalize = normalize\n        else:\n            self.normalize = lambda x, y: x\n        self.normalized_adj = self.normalize(self.adj, self.conf.dataset['add_loop'], self.conf.dataset['sparse'])", "\n\nclass GATSolver(Solver):\n    '''\n    A solver to train, evaluate, test GAT in a run.\n\n    Parameters\n    ----------\n    conf : argparse.Namespace\n        Config file.\n    dataset : opengsl.data.Dataset\n        The dataset.\n\n    Attributes\n    ----------\n    method_name : str\n        The name of the method.\n\n    Examples\n    --------\n    >>> # load dataset\n    >>> import opengsl.dataset\n    >>> dataset = opengsl.data.Dataset('cora', feat_norm=True)\n    >>> # load config file\n    >>> import opengsl.config.load_conf\n    >>> conf = opengsl.config.load_conf('gat', 'cora')\n    >>>\n    >>> solver = GATSolver(conf, dataset)\n    >>> # Conduct a experiment run.\n    >>> acc, _ = solver.run_exp(split=0, debug=True)\n    '''\n    def __init__(self, conf, dataset):\n        super().__init__(conf, dataset)\n        self.method_name = \"gat\"\n\n    def input_distributer(self):\n        '''\n        Function to ditribute input to GNNs, automatically called in function `learn`.\n\n        Returns\n        -------\n        self.feats : torch.tensor\n            Node features.\n        self.edge_index : torch.tensor\n            Adjacency matrix.\n        '''\n        return self.feats, self.edge_index\n\n    def set_method(self):\n        '''\n        Function to set the model and necessary variables for each run, automatically called in function `set`.\n\n        '''\n        self.model = GAT(self.dim_feats, self.conf.model['n_hidden'], self.num_targets, self.conf.model['n_layers'],\n                         n_heads=self.conf.model['n_heads'], dropout=self.conf.model['dropout']).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                           weight_decay=self.conf.training['weight_decay'])\n        # prepare edge index\n        self.edge_index = self.adj.coalesce().indices()", "\n"]}
{"filename": "opengsl/method/models/wsgnn.py", "chunked_list": ["from torch_sparse import SparseTensor\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n# from torch_geometric.nn import GCNConv, GATConv, APPNP\nfrom torch_geometric.nn import GCNConv, GATConv\nimport torch_sparse\n# from .gnn_modules import APPNP\nfrom .gcn import GCN", "# from .gnn_modules import APPNP\nfrom .gcn import GCN\n\nclass GraphLearner(nn.Module):\n    def __init__(self, input_size, num_pers=16):\n        super(GraphLearner, self).__init__()\n        self.weight_tensor = torch.Tensor(num_pers, input_size)\n        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\n    def reset_parameters(self):\n        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\n    def forward(self, context):\n        expand_weight_tensor = self.weight_tensor.unsqueeze(1)\n        context_fc = context.unsqueeze(0) * expand_weight_tensor\n        context_norm = F.normalize(context_fc, p=2, dim=-1)\n        attention = torch.matmul(context_norm, context_norm.transpose(-1, -2)).mean(0)\n        mask = (attention > 0).detach().float()\n        attention = attention * mask + 0 * (1 - mask)\n\n        return attention", "\nclass MLP(nn.Module):\n    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout=.5, use_bn=False):\n        super(MLP, self).__init__()\n        self.use_bn = use_bn\n        self.lins = nn.ModuleList()\n        self.bns = nn.ModuleList()\n        if num_layers == 1:\n            # just linear layer i.e. logistic regression\n            self.lins.append(nn.Linear(in_channels, out_channels))\n        else:\n            self.lins.append(nn.Linear(in_channels, hidden_channels))\n            self.bns.append(nn.BatchNorm1d(hidden_channels))\n            for _ in range(num_layers - 2):\n                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n                self.bns.append(nn.BatchNorm1d(hidden_channels))\n            self.lins.append(nn.Linear(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for lin in self.lins:\n            lin.reset_parameters()\n        for bn in self.bns:\n            bn.reset_parameters()\n\n    def forward(self, x):\n        for i, lin in enumerate(self.lins[:-1]):\n            x = lin(x)\n            x = F.relu(x, inplace=True)\n            if self.use_bn:\n                x = self.bns[i](x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lins[-1](x)\n        return x", "\n\n# class DenseAPPNP(nn.Module):\n#     def __init__(self, K, alpha):\n#         super().__init__()\n#         self.K = K\n#         self.alpha = alpha\n\n#     def forward(self, x, adj_t):\n#         h = x", "#     def forward(self, x, adj_t):\n#         h = x\n#         for k in range(self.K):\n#             if adj_t.is_sparse:\n#                 x = torch_sparse.spmm(adj_t, x)\n#             else:\n#                 x = torch.matmul(adj_t, x)\n#             x = x * (1 - self.alpha)\n#             x += self.alpha * h\n#         return x", "#             x += self.alpha * h\n#         return x\n\n\n# class Dense_APPNP_Net(nn.Module):\n#     def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, K=10, alpha=.1):\n#         super(Dense_APPNP_Net, self).__init__()\n#         self.lin1 = nn.Linear(in_channels, hidden_channels)\n#         self.lin2 = nn.Linear(hidden_channels, out_channels)\n#         self.prop1 = DenseAPPNP(K, alpha)", "#         self.lin2 = nn.Linear(hidden_channels, out_channels)\n#         self.prop1 = DenseAPPNP(K, alpha)\n#         self.dropout = dropout\n\n#     def reset_parameters(self):\n#         self.lin1.reset_parameters()\n#         self.lin2.reset_parameters()\n\n#     def forward(self, x, adj_t):\n#         x = F.dropout(x, p=self.dropout, training=self.training)", "#     def forward(self, x, adj_t):\n#         x = F.dropout(x, p=self.dropout, training=self.training)\n#         x = F.relu(self.lin1(x))\n#         x = F.dropout(x, p=self.dropout, training=self.training)\n#         x = self.lin2(x)\n#         x = self.prop1(x, adj_t)\n#         return x\n\nINF = 1e20\nVERY_SMALL_NUMBER = 1e-12", "INF = 1e20\nVERY_SMALL_NUMBER = 1e-12\n\nclass QModel(nn.Module):\n    def __init__(self, graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, d, n, c):\n        super(QModel, self).__init__()\n        self.graph_skip_conn = graph_skip_conn\n        # self.encoder = APPNP(d,nhid,c,dropout,hops,alpha)\n        self.encoder = GCN(d, nhid, c, n_layers, dropout)\n        # self.encoder = Dense_APPNP_Net(in_channels=d,\n        #                                hidden_channels=nhid,\n        #                                out_channels=c,\n        #                                dropout=dropout,\n        #                                K=hops,\n        #                                alpha=alpha)\n\n        self.graph_learner1 = GraphLearner(input_size=d, num_pers=graph_learn_num_pers)\n        self.graph_learner2 = GraphLearner(input_size=2 * c, num_pers=graph_learn_num_pers)\n\n    def reset_parameters(self):\n        self.encoder.reset_parameters()\n        self.graph_learner1.reset_parameters()\n        self.graph_learner2.reset_parameters()\n\n    def learn_graph(self, graph_learner, node_features, graph_skip_conn=None, init_adj=None):\n        raw_adj = graph_learner(node_features)\n        adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=VERY_SMALL_NUMBER)\n        adj = graph_skip_conn * init_adj + (1 - graph_skip_conn) * adj\n\n        return raw_adj, adj\n\n    def forward(self, feats, n_node, edge_index):\n        node_features = feats\n        train_index = edge_index\n        edge_weight = None\n        _edge_index, edge_weight = gcn_norm(\n            train_index, edge_weight, n_node, False,\n            dtype=node_features.dtype)\n        row, col = _edge_index\n        init_adj_sparse = SparseTensor(row=col, col=row, value=edge_weight,\n                                       sparse_sizes=(n_node, n_node))\n        init_adj = init_adj_sparse.to_dense()\n\n        raw_adj_1, adj_1 = self.learn_graph(self.graph_learner1, node_features, self.graph_skip_conn, init_adj)\n        node_vec_1 = self.encoder([node_features, adj_1, True])\n\n        node_vec_2 = self.encoder([node_features, init_adj, True])\n        raw_adj_2, adj_2 = self.learn_graph(self.graph_learner2, torch.cat([node_vec_1, node_vec_2], dim=1),\n                                            self.graph_skip_conn, init_adj)\n\n        output = 0.5 * node_vec_1 + 0.5 * node_vec_2\n        adj = 0.5 * adj_1 + 0.5 * adj_2\n\n        return output, adj", "\n\nclass PModel(nn.Module):\n    def __init__(self, nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c):\n        super(PModel, self).__init__()\n        # self.encoder1 = APPNP(d,nhid,c,dropout,hops,alpha)\n        self.encoder1 = GCN(d, nhid, c, n_layers, dropout)\n        # self.encoder1 = Dense_APPNP_Net(in_channels=d,\n        #                                 hidden_channels=nhid,\n        #                                 out_channels=c,\n        #                                 dropout=dropout,\n        #                                 K=hops,\n        #                                 alpha=alpha)\n\n        self.encoder2 = MLP(in_channels=d,\n                            hidden_channels=nhid,\n                            out_channels=c,\n                            num_layers=mlp_layers,\n                            dropout=dropout,\n                            use_bn=not no_bn)\n\n        self.graph_learner1 = GraphLearner(input_size=d, num_pers=graph_learn_num_pers)\n        self.graph_learner2 = GraphLearner(input_size=2 * c, num_pers=graph_learn_num_pers)\n\n    def reset_parameters(self):\n        self.encoder1.reset_parameters()\n        self.encoder2.reset_parameters()\n        self.graph_learner.reset_parameters()\n        self.graph_learner2.reset_parameters()\n\n    def learn_graph(self, graph_learner, node_features):\n        raw_adj = graph_learner(node_features)\n        adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=VERY_SMALL_NUMBER)\n\n        return raw_adj, adj\n\n    def forward(self, feats):\n        node_features = feats\n\n        raw_adj_1, adj_1 = self.learn_graph(self.graph_learner1, node_features)\n        node_vec_1 = self.encoder1([node_features, adj_1, True])\n\n        node_vec_2 = self.encoder2(node_features)\n        raw_adj_2, adj_2 = self.learn_graph(self.graph_learner2, torch.cat([node_vec_1, node_vec_2], dim=1))\n\n        output = 0.5 * node_vec_1 + 0.5 * node_vec_2\n        adj = 0.5 * adj_1 + 0.5 * adj_2\n\n        return output, adj", "\n\nclass WSGNN(nn.Module):\n    def __init__(self, graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c):\n        super(WSGNN, self).__init__()\n        self.P_Model = PModel(nhid, dropout, n_layers, graph_learn_num_pers, mlp_layers, no_bn, d, n, c)\n        self.Q_Model = QModel(graph_skip_conn, nhid, dropout, n_layers, graph_learn_num_pers, d, n, c)\n\n    def reset_parameters(self):\n        self.P_Model.reset_parameters()\n        self.Q_Model.reset_parameters()\n\n    def forward(self, feats, n_node, edge_index):\n        q_y, q_a = self.Q_Model.forward(feats, n_node, edge_index)\n        p_y, p_a = self.P_Model.forward(feats)\n        return p_y, p_a, q_y, q_a", "\nclass ELBONCLoss(nn.Module):\n    def __init__(self):\n        super(ELBONCLoss, self).__init__()\n\n    def forward(self, labels, train_mask, log_p_y, log_q_y):\n        y_obs = labels[train_mask]\n        log_p_y_obs = log_p_y[train_mask]\n        p_y_obs = torch.exp(log_p_y_obs)\n        log_p_y_miss = log_p_y[train_mask == 0]\n        p_y_miss = torch.exp(log_p_y_miss)\n        log_q_y_obs = log_q_y[train_mask]\n        q_y_obs = torch.exp(log_q_y_obs)\n        log_q_y_miss = log_q_y[train_mask == 0]\n        q_y_miss = torch.exp(log_q_y_miss)\n        loss_p_y = F.nll_loss(log_p_y_obs, y_obs) - torch.mean(q_y_miss * log_p_y_miss)\n        loss_q_y = torch.mean(q_y_miss * log_q_y_miss)\n\n        loss_y_obs = 10 * F.nll_loss(log_q_y_obs, y_obs)\n\n        loss = loss_p_y + loss_q_y + loss_y_obs\n\n        return loss"]}
{"filename": "opengsl/method/models/prognn.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nimport scipy.sparse as sp\n\n\nclass PGD(Optimizer):\n    \"\"\"Proximal gradient descent.\n\n    Parameters\n    ----------\n    params : iterable\n        iterable of parameters to optimize or dicts defining parameter groups\n    proxs : iterable\n        iterable of proximal operators\n    alpha : iterable\n        iterable of coefficients for proximal gradient descent\n    lr : float\n        learning rate\n    momentum : float\n        momentum factor (default: 0)\n    weight_decay : float\n        weight decay (L2 penalty) (default: 0)\n    dampening : float\n        dampening for momentum (default: 0)\n\n    \"\"\"\n\n    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n        defaults = dict(lr=lr, momentum=0, dampening=0,\n                        weight_decay=0, nesterov=False)\n\n\n        super(PGD, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            group.setdefault('proxs', proxs)\n            group.setdefault('alphas', alphas)\n\n    def __setstate__(self, state):\n        super(PGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('nesterov', False)\n            group.setdefault('proxs', proxs)\n            group.setdefault('alphas', alphas)\n\n    def step(self, delta=0, closure=None):\n        for group in self.param_groups:\n            lr = group['lr']\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            proxs = group['proxs']\n            alphas = group['alphas']\n\n            # apply the proximal operator to each parameter in a group\n            for param in group['params']:\n                for prox_operator, alpha in zip(proxs, alphas):\n                    # param.data.add_(lr, -param.grad.data)\n                    # param.data.add_(delta)\n                    param.data = prox_operator(param.data, alpha=alpha*lr)", "class PGD(Optimizer):\n    \"\"\"Proximal gradient descent.\n\n    Parameters\n    ----------\n    params : iterable\n        iterable of parameters to optimize or dicts defining parameter groups\n    proxs : iterable\n        iterable of proximal operators\n    alpha : iterable\n        iterable of coefficients for proximal gradient descent\n    lr : float\n        learning rate\n    momentum : float\n        momentum factor (default: 0)\n    weight_decay : float\n        weight decay (L2 penalty) (default: 0)\n    dampening : float\n        dampening for momentum (default: 0)\n\n    \"\"\"\n\n    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n        defaults = dict(lr=lr, momentum=0, dampening=0,\n                        weight_decay=0, nesterov=False)\n\n\n        super(PGD, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            group.setdefault('proxs', proxs)\n            group.setdefault('alphas', alphas)\n\n    def __setstate__(self, state):\n        super(PGD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('nesterov', False)\n            group.setdefault('proxs', proxs)\n            group.setdefault('alphas', alphas)\n\n    def step(self, delta=0, closure=None):\n        for group in self.param_groups:\n            lr = group['lr']\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            proxs = group['proxs']\n            alphas = group['alphas']\n\n            # apply the proximal operator to each parameter in a group\n            for param in group['params']:\n                for prox_operator, alpha in zip(proxs, alphas):\n                    # param.data.add_(lr, -param.grad.data)\n                    # param.data.add_(delta)\n                    param.data = prox_operator(param.data, alpha=alpha*lr)", "\n\nclass ProxOperators():\n    \"\"\"Proximal Operators.\n    \"\"\"\n\n    def __init__(self):\n        self.nuclear_norm = None\n\n    def prox_l1(self, data, alpha):\n        \"\"\"Proximal operator for l1 norm.\n        \"\"\"\n        data = torch.mul(torch.sign(data), torch.clamp(torch.abs(data)-alpha, min=0))\n        return data\n\n    def prox_nuclear(self, data, alpha):\n        \"\"\"Proximal operator for nuclear norm (trace norm).\n        \"\"\"\n        U, S, V = np.linalg.svd(data.cpu())\n        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n        self.nuclear_norm = S.sum()\n        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n\n        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n        return torch.matmul(torch.matmul(U, diag_S), V)\n\n    def prox_nuclear_truncated_2(self, data, alpha, k=50):\n        import tensorly as tl\n        tl.set_backend('pytorch')\n        U, S, V = tl.truncated_svd(data.cpu(), n_eigenvecs=k)\n        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n        self.nuclear_norm = S.sum()\n        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n\n        S = torch.clamp(S-alpha, min=0)\n\n        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n        # U = torch.spmm(U, diag_S)\n        # V = torch.matmul(U, V)\n\n        # make diag_S sparse matrix\n        indices = torch.tensor((range(0, len(S)), range(0, len(S)))).cuda()\n        values = S\n        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size((len(S), len(S))))\n        V = torch.spmm(diag_S, V)\n        V = torch.matmul(U, V)\n        return V\n\n    def prox_nuclear_truncated(self, data, alpha, k=50):\n        indices = torch.nonzero(data).t()\n        values = data[indices[0], indices[1]] # modify this based on dimensionality\n        data_sparse = sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()))\n        U, S, V = sp.linalg.svds(data_sparse, k=k)\n        U, S, V = torch.FloatTensor(U).cuda(), torch.FloatTensor(S).cuda(), torch.FloatTensor(V).cuda()\n        self.nuclear_norm = S.sum()\n        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n        return torch.matmul(torch.matmul(U, diag_S), V)\n\n    def prox_nuclear_cuda(self, data, alpha):\n\n        U, S, V = torch.svd(data)\n        # self.nuclear_norm = S.sum()\n        # print(f\"rank = {len(S.nonzero())}\")\n        self.nuclear_norm = S.sum()\n        S = torch.clamp(S-alpha, min=0)\n        indices = torch.tensor([range(0, U.shape[0]),range(0, U.shape[0])]).cuda()\n        values = S\n        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size(U.shape))\n        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n        # print(f\"rank_after = {len(diag_S.nonzero())}\")\n        V = torch.spmm(diag_S, V.t_())\n        V = torch.matmul(U, V)\n        return V", "\n\nclass EstimateAdj(nn.Module):\n    \"\"\"Provide a pytorch parameter matrix for estimated\n    adjacency matrix and corresponding operations.\n    \"\"\"\n\n    def __init__(self, adj, symmetric=False, device='cpu'):\n        super(EstimateAdj, self).__init__()\n        n = len(adj)\n        self.estimated_adj = nn.Parameter(torch.FloatTensor(n, n))\n        self._init_estimation(adj)\n        self.symmetric = symmetric\n        self.device = device\n\n    def _init_estimation(self, adj):\n        with torch.no_grad():\n            n = len(adj)\n            self.estimated_adj.data.copy_(adj)\n\n    def forward(self):\n        return self.estimated_adj\n\n    def normalize(self):\n\n        if self.symmetric:\n            adj = (self.estimated_adj + self.estimated_adj.t()) / 2\n        else:\n            adj = self.estimated_adj\n\n        normalized_adj = self._normalize(adj + torch.eye(adj.shape[0]).to(self.device))\n        return normalized_adj\n\n    def _normalize(self, mx):\n        rowsum = mx.sum(1)\n        r_inv = rowsum.pow(-1/2).flatten()\n        r_inv[torch.isinf(r_inv)] = 0.\n        r_mat_inv = torch.diag(r_inv)\n        mx = r_mat_inv @ mx\n        mx = mx @ r_mat_inv\n        return mx", "\n\ndef feature_smoothing(adj, X):\n    adj = (adj.t() + adj)/2\n    rowsum = adj.sum(1)\n    r_inv = rowsum.flatten()\n    D = torch.diag(r_inv)\n    L = D - adj\n\n    r_inv = r_inv  + 1e-3\n    r_inv = r_inv.pow(-1/2).flatten()\n    r_inv[torch.isinf(r_inv)] = 0.\n    r_mat_inv = torch.diag(r_inv)\n    # L = r_mat_inv @ L\n    L = r_mat_inv @ L @ r_mat_inv\n\n    XLXT = torch.matmul(torch.matmul(X.t(), L), X)\n    loss_smooth_feat = torch.trace(XLXT)\n    return loss_smooth_feat", "\n\nprox_operators = ProxOperators()\n"]}
{"filename": "opengsl/method/models/segsl.py", "chunked_list": ["import torch\nimport dgl\nimport copy\nimport math\nimport heapq\nimport numba as nb\nimport numpy as np\n\n\ndef add_knn(k, node_embed, edge_index):\n    # \u8fd9\u91cc\u7528cosine\uff0c\u548c\u8bba\u6587\u4e2d\u4e0d\u4e00\u81f4\n    knn_g = dgl.knn_graph(node_embed,\n                          k,\n                          algorithm='bruteforce-sharemem',\n                          dist='cosine')\n    knn_g = dgl.add_reverse_edges(knn_g)\n    knn_edge_index = knn_g.edges()\n    knn_edge_index = torch.cat(\n        (knn_edge_index[0].reshape(1, -1), knn_edge_index[1].reshape(1, -1)),\n        dim=0)\n    knn_edge_index = knn_edge_index.t()\n    edge_index_2 = torch.cat((edge_index, knn_edge_index), dim=0)\n    edge_index_2 = torch.unique(edge_index_2, dim=0)\n    return edge_index_2", "\ndef add_knn(k, node_embed, edge_index):\n    # \u8fd9\u91cc\u7528cosine\uff0c\u548c\u8bba\u6587\u4e2d\u4e0d\u4e00\u81f4\n    knn_g = dgl.knn_graph(node_embed,\n                          k,\n                          algorithm='bruteforce-sharemem',\n                          dist='cosine')\n    knn_g = dgl.add_reverse_edges(knn_g)\n    knn_edge_index = knn_g.edges()\n    knn_edge_index = torch.cat(\n        (knn_edge_index[0].reshape(1, -1), knn_edge_index[1].reshape(1, -1)),\n        dim=0)\n    knn_edge_index = knn_edge_index.t()\n    edge_index_2 = torch.cat((edge_index, knn_edge_index), dim=0)\n    edge_index_2 = torch.unique(edge_index_2, dim=0)\n    return edge_index_2", "\n\ndef calc_e1(adj: torch.Tensor):\n    adj = adj - torch.diag_embed(torch.diag(adj))\n    degree = adj.sum(dim=1)\n    vol = adj.sum()\n    idx = degree.nonzero().reshape(-1)\n    g = degree[idx]\n    return -((g / vol) * torch.log2(g / vol)).sum()\n", "\n\ndef get_adj_matrix(node_num, edge_index, weight) -> torch.Tensor:\n    adj_matrix = torch.zeros((node_num, node_num))\n    adj_matrix[edge_index.t()[0], edge_index.t()[1]] = weight.float()\n    adj_matrix = adj_matrix - torch.diag_embed(torch.diag(adj_matrix))  #\u53bb\u9664\u5bf9\u89d2\u7ebf\n    return adj_matrix\n\n\ndef get_weight(node_embedding, edge_index):\n    node_num = node_embedding.shape[0]\n    links = node_embedding[edge_index]\n    weight = []\n    for i in range(links.shape[0]):\n        # print(links[i])\n        # weight.append(torch.corrcoef(links[i])[0, 1])\n        weight.append(np.corrcoef(links[i].cpu())[0, 1])\n    weight = torch.tensor(weight) + 1\n    weight[torch.isnan(weight)] = 0\n    M = weight.mean() / (2 * node_num)\n    weight = weight + M\n    return weight", "\ndef get_weight(node_embedding, edge_index):\n    node_num = node_embedding.shape[0]\n    links = node_embedding[edge_index]\n    weight = []\n    for i in range(links.shape[0]):\n        # print(links[i])\n        # weight.append(torch.corrcoef(links[i])[0, 1])\n        weight.append(np.corrcoef(links[i].cpu())[0, 1])\n    weight = torch.tensor(weight) + 1\n    weight[torch.isnan(weight)] = 0\n    M = weight.mean() / (2 * node_num)\n    weight = weight + M\n    return weight", "\n\ndef knn_maxE1(edge_index: torch.Tensor, node_embedding: torch.Tensor):\n    old_e1 = 0\n    node_num = node_embedding.shape[0]\n    k = 1\n    while k < 50:\n        edge_index_k = add_knn(k, node_embedding, edge_index)\n        weight = get_weight(node_embedding, edge_index_k)\n        # e1 = calc_e1(edge_index_k, weight)\n        adj = get_adj_matrix(node_num, edge_index_k, weight)\n        e1 = calc_e1(adj)\n        if e1 - old_e1 > 0.1:\n            k += 5\n        elif e1 - old_e1 > 0.01:\n            k += 3\n        elif e1 - old_e1 > 0.001:\n            k += 1\n        else:\n            break\n        old_e1 = e1\n    print(f'max1SE k: {k}')\n    return k", "\n\ndef get_id():\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n\ndef graph_parse(adj_matrix):\n    g_num_nodes = adj_matrix.shape[0]\n    adj_table = {}\n    VOL = 0\n    node_vol = []\n    for i in range(g_num_nodes):\n        n_v = 0\n        adj = set()\n        for j in range(g_num_nodes):\n            if adj_matrix[i, j] != 0:\n                n_v += adj_matrix[i, j]\n                VOL += adj_matrix[i, j]\n                adj.add(j)\n        adj_table[i] = adj\n        node_vol.append(n_v)\n    return g_num_nodes, VOL, node_vol, adj_table", "\ndef graph_parse(adj_matrix):\n    g_num_nodes = adj_matrix.shape[0]\n    adj_table = {}\n    VOL = 0\n    node_vol = []\n    for i in range(g_num_nodes):\n        n_v = 0\n        adj = set()\n        for j in range(g_num_nodes):\n            if adj_matrix[i, j] != 0:\n                n_v += adj_matrix[i, j]\n                VOL += adj_matrix[i, j]\n                adj.add(j)\n        adj_table[i] = adj\n        node_vol.append(n_v)\n    return g_num_nodes, VOL, node_vol, adj_table", "\n\n@nb.jit(nopython=True)\ndef cut_volume(adj_matrix, p1, p2):\n    c12 = 0\n    for i in range(len(p1)):\n        for j in range(len(p2)):\n            c = adj_matrix[p1[i], p2[j]]\n            if c != 0:\n                c12 += c\n    return c12", "\n\ndef LayerFirst(node_dict, start_id):\n    stack = [start_id]\n    while len(stack) != 0:\n        node_id = stack.pop(0)\n        yield node_id\n        if node_dict[node_id].children:\n            for c_id in node_dict[node_id].children:\n                stack.append(c_id)", "\n\ndef merge(new_ID, id1, id2, cut_v, node_dict):\n    new_partition = node_dict[id1].partition + node_dict[id2].partition\n    v = node_dict[id1].vol + node_dict[id2].vol\n    g = node_dict[id1].g + node_dict[id2].g - 2 * cut_v\n    child_h = max(node_dict[id1].child_h, node_dict[id2].child_h) + 1\n    new_node = PartitionTreeNode(ID=new_ID,\n                                 partition=new_partition,\n                                 children={id1, id2},\n                                 g=g,\n                                 vol=v,\n                                 child_h=child_h,\n                                 child_cut=cut_v)\n    node_dict[id1].parent = new_ID\n    node_dict[id2].parent = new_ID\n    node_dict[new_ID] = new_node", "\n\ndef compressNode(node_dict, node_id, parent_id):\n    p_child_h = node_dict[parent_id].child_h\n    node_children = node_dict[node_id].children\n    node_dict[parent_id].child_cut += node_dict[node_id].child_cut\n    node_dict[parent_id].children.remove(node_id)\n    node_dict[parent_id].children = node_dict[parent_id].children.union(\n        node_children)\n    for c in node_children:\n        node_dict[c].parent = parent_id\n    com_node_child_h = node_dict[node_id].child_h\n    node_dict.pop(node_id)\n\n    if (p_child_h - com_node_child_h) == 1:\n        while True:\n            max_child_h = max([\n                node_dict[f_c].child_h for f_c in node_dict[parent_id].children\n            ])\n            if node_dict[parent_id].child_h == (max_child_h + 1):\n                break\n            node_dict[parent_id].child_h = max_child_h + 1\n            parent_id = node_dict[parent_id].parent\n            if parent_id is None:\n                break", "\n\ndef child_tree_deepth(node_dict, nid):\n    node = node_dict[nid]\n    deepth = 0\n    while node.parent is not None:\n        node = node_dict[node.parent]\n        deepth += 1\n    deepth += node_dict[nid].child_h\n    return deepth", "\n\ndef CompressDelta(node1, p_node):\n    a = node1.child_cut\n    v1 = node1.vol + 1\n    v2 = p_node.vol + 1\n    return a * math.log2(v2 / v1)\n\n\ndef CombineDelta(node1, node2, cut_v, g_vol):\n    v1 = node1.vol + 1\n    v2 = node2.vol + 1\n    g1 = node1.g + 1\n    g2 = node2.g + 1\n    v12 = v1 + v2\n    return ((v1 - g1) * math.log2(v12 / v1) + (v2 - g2) * math.log2(v12 / v2) -\n            2 * cut_v * math.log2(g_vol / v12)) / g_vol", "\ndef CombineDelta(node1, node2, cut_v, g_vol):\n    v1 = node1.vol + 1\n    v2 = node2.vol + 1\n    g1 = node1.g + 1\n    g2 = node2.g + 1\n    v12 = v1 + v2\n    return ((v1 - g1) * math.log2(v12 / v1) + (v2 - g2) * math.log2(v12 / v2) -\n            2 * cut_v * math.log2(g_vol / v12)) / g_vol\n", "\n\nclass PartitionTreeNode:\n    def __init__(self,\n                 ID,\n                 partition,\n                 vol,\n                 g,\n                 children: set = None,\n                 parent=None,\n                 child_h=0,\n                 child_cut=0):\n        self.ID = ID\n        self.partition = partition\n        self.parent = parent\n        self.children = children\n        self.vol = vol\n        self.g = g\n        self.merged = False\n        self.child_h = child_h  #\u4e0d\u5305\u62ec\u8be5\u8282\u70b9\u7684\u5b50\u6811\u9ad8\u5ea6\n        self.child_cut = child_cut\n\n    def __str__(self):\n        return \"{\" + \"{}:{}\".format(self.__class__.__name__,\n                                    self.gatherAttrs()) + \"}\"\n\n    def gatherAttrs(self):\n        return \",\".join(\"{}={}\".format(k, getattr(self, k))\n                        for k in self.__dict__.keys())", "\n\nclass PartitionTree:\n    def __init__(self, adj_matrix):\n        self.adj_matrix = adj_matrix\n        self.tree_node = {}\n        self.g_num_nodes, self.VOL, self.node_vol, self.adj_table = graph_parse(\n            adj_matrix)\n        self.id_g = get_id()\n        self.leaves = []\n        self.build_leaves()\n\n    def build_leaves(self):\n        for vertex in range(self.g_num_nodes):\n            ID = next(self.id_g)\n            v = self.node_vol[vertex]\n            leaf_node = PartitionTreeNode(ID=ID,\n                                          partition=[vertex],\n                                          g=v,\n                                          vol=v)\n            self.tree_node[ID] = leaf_node\n            self.leaves.append(ID)\n\n    def build_sub_leaves(self, node_list, p_vol):\n        subgraph_node_dict = {}\n        ori_ent = 0\n        for vertex in node_list:\n            ori_ent += -(self.tree_node[vertex].g / self.VOL)\\\n                       * math.log2((self.tree_node[vertex].vol+1)/(p_vol+1))\n            sub_n = set()\n            vol = 0\n            for vertex_n in node_list:\n                c = self.adj_matrix[vertex, vertex_n]\n                if c != 0:\n                    vol += c\n                    sub_n.add(vertex_n)\n            sub_leaf = PartitionTreeNode(ID=vertex,\n                                         partition=[vertex],\n                                         g=vol,\n                                         vol=vol)\n            subgraph_node_dict[vertex] = sub_leaf\n            self.adj_table[vertex] = sub_n\n\n        return subgraph_node_dict, ori_ent\n\n    def build_root_down(self):\n        root_child = self.tree_node[self.root_id].children\n        subgraph_node_dict = {}\n        ori_en = 0\n        g_vol = self.tree_node[self.root_id].vol\n        for node_id in root_child:\n            node = self.tree_node[node_id]\n            ori_en += -(node.g / g_vol) * math.log2((node.vol + 1) / g_vol)\n            new_n = set()\n            for nei in self.adj_table[node_id]:\n                if nei in root_child:\n                    new_n.add(nei)\n            self.adj_table[node_id] = new_n\n\n            new_node = PartitionTreeNode(ID=node_id,\n                                         partition=node.partition,\n                                         vol=node.vol,\n                                         g=node.g,\n                                         children=node.children)\n            subgraph_node_dict[node_id] = new_node\n\n        return subgraph_node_dict, ori_en\n\n    def entropy(self, node_dict=None):\n        if node_dict is None:\n            node_dict = self.tree_node\n        ent = 0\n        for node_id, node in node_dict.items():\n            if node.parent is not None:\n                node_p = node_dict[node.parent]\n                node_vol = node.vol + 1\n                node_g = node.g\n                node_p_vol = node_p.vol + 1\n                ent += -(node_g / self.VOL) * math.log2(node_vol / node_p_vol)\n        return ent\n\n    def __build_k_tree(\n        self,\n        g_vol,\n        nodes_dict: dict,\n        k=None,\n    ):\n        min_heap = []\n        cmp_heap = []\n        nodes_ids = nodes_dict.keys()\n        new_id = None\n        for i in nodes_ids:\n            for j in self.adj_table[i]:\n                if j > i:\n                    n1 = nodes_dict[i]\n                    n2 = nodes_dict[j]\n                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n                        cut_v = self.adj_matrix[n1.partition[0],\n                                                n2.partition[0]]\n                    else:\n                        cut_v = cut_volume(self.adj_matrix,\n                                           p1=np.array(n1.partition),\n                                           p2=np.array(n2.partition))\n                    diff = CombineDelta(nodes_dict[i], nodes_dict[j], cut_v,\n                                        g_vol)\n                    heapq.heappush(min_heap, (diff, i, j, cut_v))\n        unmerged_count = len(nodes_ids)\n        while unmerged_count > 1:\n            if len(min_heap) == 0:\n                break\n            diff, id1, id2, cut_v = heapq.heappop(min_heap)\n            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n                continue\n            nodes_dict[id1].merged = True\n            nodes_dict[id2].merged = True\n            new_id = next(self.id_g)\n            merge(new_id, id1, id2, cut_v, nodes_dict)\n            self.adj_table[new_id] = self.adj_table[id1].union(\n                self.adj_table[id2])\n            for i in self.adj_table[new_id]:\n                self.adj_table[i].add(new_id)\n            #compress delta\n            if nodes_dict[id1].child_h > 0:\n                heapq.heappush(cmp_heap, [\n                    CompressDelta(nodes_dict[id1], nodes_dict[new_id]), id1,\n                    new_id\n                ])\n            if nodes_dict[id2].child_h > 0:\n                heapq.heappush(cmp_heap, [\n                    CompressDelta(nodes_dict[id2], nodes_dict[new_id]), id2,\n                    new_id\n                ])\n            unmerged_count -= 1\n\n            for ID in self.adj_table[new_id]:\n                if not nodes_dict[ID].merged:\n                    n1 = nodes_dict[ID]\n                    n2 = nodes_dict[new_id]\n                    cut_v = cut_volume(self.adj_matrix, np.array(n1.partition),\n                                       np.array(n2.partition))\n\n                    new_diff = CombineDelta(nodes_dict[ID], nodes_dict[new_id],\n                                            cut_v, g_vol)\n                    heapq.heappush(min_heap, (new_diff, ID, new_id, cut_v))\n        root = new_id\n\n        if unmerged_count > 1:\n            #combine solitary node\n            assert len(min_heap) == 0\n            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n            new_child_h = max([nodes_dict[i].child_h\n                               for i in unmerged_nodes]) + 1\n\n            new_id = next(self.id_g)\n            new_node = PartitionTreeNode(ID=new_id,\n                                         partition=list(nodes_ids),\n                                         children=unmerged_nodes,\n                                         vol=g_vol,\n                                         g=0,\n                                         child_h=new_child_h)\n            nodes_dict[new_id] = new_node\n\n            for i in unmerged_nodes:\n                nodes_dict[i].merged = True\n                nodes_dict[i].parent = new_id\n                if nodes_dict[i].child_h > 0:\n                    heapq.heappush(cmp_heap, [\n                        CompressDelta(nodes_dict[i], nodes_dict[new_id]), i,\n                        new_id\n                    ])\n            root = new_id\n\n        if k is not None:\n            while nodes_dict[root].child_h > k:\n                diff, node_id, p_id = heapq.heappop(cmp_heap)\n                if child_tree_deepth(nodes_dict, node_id) <= k:\n                    continue\n                children = nodes_dict[node_id].children\n                compressNode(nodes_dict, node_id, p_id)\n                if nodes_dict[root].child_h == k:\n                    break\n                for e in cmp_heap:\n                    if e[1] == p_id:\n                        if child_tree_deepth(nodes_dict, p_id) > k:\n                            e[0] = CompressDelta(nodes_dict[e[1]],\n                                                 nodes_dict[e[2]])\n                    if e[1] in children:\n                        if nodes_dict[e[1]].child_h == 0:\n                            continue\n                        if child_tree_deepth(nodes_dict, e[1]) > k:\n                            e[2] = p_id\n                            e[0] = CompressDelta(nodes_dict[e[1]],\n                                                 nodes_dict[p_id])\n                heapq.heapify(cmp_heap)\n        return root\n\n    def check_balance(self, node_dict, root_id):\n        root_c = copy.deepcopy(node_dict[root_id].children)\n        for c in root_c:\n            if node_dict[c].child_h == 0:\n                self.single_up(node_dict, c)\n\n    def single_up(self, node_dict, node_id):\n        new_id = next(self.id_g)\n        p_id = node_dict[node_id].parent\n        grow_node = PartitionTreeNode(ID=new_id,\n                                      partition=node_dict[node_id].partition,\n                                      parent=p_id,\n                                      children={node_id},\n                                      vol=node_dict[node_id].vol,\n                                      g=node_dict[node_id].g)\n        node_dict[node_id].parent = new_id\n        node_dict[p_id].children.remove(node_id)\n        node_dict[p_id].children.add(new_id)\n        node_dict[new_id] = grow_node\n        node_dict[new_id].child_h = node_dict[node_id].child_h + 1\n        self.adj_table[new_id] = self.adj_table[node_id]\n        for i in self.adj_table[node_id]:\n            self.adj_table[i].add(new_id)\n\n    def root_down_delta(self):\n        if len(self.tree_node[self.root_id].children) < 3:\n            return 0, None, None\n        subgraph_node_dict, ori_entropy = self.build_root_down()\n        g_vol = self.tree_node[self.root_id].vol\n        new_root = self.__build_k_tree(g_vol=g_vol,\n                                       nodes_dict=subgraph_node_dict,\n                                       k=2)\n        self.check_balance(subgraph_node_dict, new_root)\n\n        new_entropy = self.entropy(subgraph_node_dict)\n        delta = (ori_entropy - new_entropy) / len(\n            self.tree_node[self.root_id].children)\n        return delta, new_root, subgraph_node_dict\n\n    def leaf_up_entropy(self, sub_node_dict, sub_root_id, node_id):\n        ent = 0\n        for sub_node_id in LayerFirst(sub_node_dict, sub_root_id):\n            if sub_node_id == sub_root_id:\n                sub_node_dict[sub_root_id].vol = self.tree_node[node_id].vol\n                sub_node_dict[sub_root_id].g = self.tree_node[node_id].g\n\n            elif sub_node_dict[sub_node_id].child_h == 1:\n                node = sub_node_dict[sub_node_id]\n                inner_vol = node.vol - node.g\n                partition = node.partition\n                ori_vol = sum(self.tree_node[i].vol for i in partition)\n                ori_g = ori_vol - inner_vol\n                node.vol = ori_vol\n                node.g = ori_g\n                node_p = sub_node_dict[node.parent]\n                ent += -(node.g / self.VOL) * math.log2(\n                    (node.vol + 1) / (node_p.vol + 1))\n            else:\n                node = sub_node_dict[sub_node_id]\n                node.g = self.tree_node[sub_node_id].g\n                node.vol = self.tree_node[sub_node_id].vol\n                node_p = sub_node_dict[node.parent]\n                ent += -(node.g / self.VOL) * math.log2(\n                    (node.vol + 1) / (node_p.vol + 1))\n        return ent\n\n    def leaf_up(self):\n        h1_id = set()\n        h1_new_child_tree = {}\n        id_mapping = {}\n        for l in self.leaves:\n            p = self.tree_node[l].parent\n            h1_id.add(p)\n        delta = 0\n        for node_id in h1_id:\n            candidate_node = self.tree_node[node_id]\n            sub_nodes = candidate_node.partition\n            if len(sub_nodes) == 1:\n                id_mapping[node_id] = None\n            if len(sub_nodes) == 2:\n                id_mapping[node_id] = None\n            if len(sub_nodes) >= 3:\n                sub_g_vol = candidate_node.vol - candidate_node.g\n                subgraph_node_dict, ori_ent = self.build_sub_leaves(\n                    sub_nodes, candidate_node.vol)\n                sub_root = self.__build_k_tree(g_vol=sub_g_vol,\n                                               nodes_dict=subgraph_node_dict,\n                                               k=2)\n                self.check_balance(subgraph_node_dict, sub_root)\n                new_ent = self.leaf_up_entropy(subgraph_node_dict, sub_root,\n                                               node_id)\n                delta += (ori_ent - new_ent)\n                h1_new_child_tree[node_id] = subgraph_node_dict\n                id_mapping[node_id] = sub_root\n        delta = delta / self.g_num_nodes\n        return delta, id_mapping, h1_new_child_tree\n\n    def leaf_up_update(self, id_mapping, leaf_up_dict):\n        for node_id, h1_root in id_mapping.items():\n            if h1_root is None:\n                children = copy.deepcopy(self.tree_node[node_id].children)\n                for i in children:\n                    self.single_up(self.tree_node, i)\n            else:\n                h1_dict = leaf_up_dict[node_id]\n                self.tree_node[node_id].children = h1_dict[h1_root].children\n                for h1_c in h1_dict[h1_root].children:\n                    assert h1_c not in self.tree_node\n                    h1_dict[h1_c].parent = node_id\n                h1_dict.pop(h1_root)\n                self.tree_node.update(h1_dict)\n        self.tree_node[self.root_id].child_h += 1\n\n    def root_down_update(self, new_id, root_down_dict):\n        self.tree_node[self.root_id].children = root_down_dict[new_id].children\n        for node_id in root_down_dict[new_id].children:\n            assert node_id not in self.tree_node\n            root_down_dict[node_id].parent = self.root_id\n        root_down_dict.pop(new_id)\n        self.tree_node.update(root_down_dict)\n        self.tree_node[self.root_id].child_h += 1\n\n    def build_coding_tree(self, k=2, mode='v2'):\n        if k == 1:\n            return\n        if mode == 'v1' or k is None:\n            self.root_id = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n        elif mode == 'v2':\n            self.root_id = self.__build_k_tree(self.VOL, self.tree_node, k=2)\n            self.check_balance(self.tree_node, self.root_id)\n\n            if self.tree_node[self.root_id].child_h < 2:\n                self.tree_node[self.root_id].child_h = 2\n\n            flag = 0\n            while self.tree_node[self.root_id].child_h < k:\n                if flag == 0:\n                    leaf_up_delta, id_mapping, leaf_up_dict = self.leaf_up()\n                    root_down_delta, new_id, root_down_dict = self.root_down_delta(\n                    )\n\n                elif flag == 1:\n                    leaf_up_delta, id_mapping, leaf_up_dict = self.leaf_up()\n                elif flag == 2:\n                    root_down_delta, new_id, root_down_dict = self.root_down_delta(\n                    )\n                else:\n                    raise ValueError\n\n                if leaf_up_delta < root_down_delta:\n                    # print('root down')\n                    # root down update and recompute root down delta\n                    flag = 2\n                    self.root_down_update(new_id, root_down_dict)\n\n                else:\n                    # leaf up update\n                    # print('leave up')\n                    flag = 1\n                    # print(self.tree_node[self.root_id].child_h)\n                    self.leaf_up_update(id_mapping, leaf_up_dict)\n                    # print(self.tree_node[self.root_id].child_h)\n\n                    # update root down leave nodes' children\n                    if root_down_delta != 0:\n                        for root_down_id, root_down_node in root_down_dict.items(\n                        ):\n                            if root_down_node.child_h == 0:\n                                root_down_node.children = self.tree_node[\n                                    root_down_id].children\n        count = 0\n        for _ in LayerFirst(self.tree_node, self.root_id):\n            count += 1\n        assert len(self.tree_node) == count\n\n    def get_community(self):\n        '''\n        \u9700\u8981\u5148\u8fdb\u884c\u7f16\u7801\u6811\u7684\u6784\u5efa build_coding_tree\n        k: k\u7ef4\u7ed3\u6784\u71b5\n        \u8fd4\u56detensor\u7c7b\u578b,\u53ea\u8fd4\u56de\u7b2c\u4e8c\u5c42\u7684\u793e\u533a\u5212\u5206\n        '''\n        root_id = self.root_id\n        partition_id = self.tree_node[root_id].children\n        community = torch.zeros(len(self.adj_matrix[0]), dtype=torch.int32)\n        community_id = 0\n        for e in partition_id:\n            partition_node = torch.tensor(self.tree_node[e].partition)\n            community[partition_node] = community_id\n            community_id += 1\n        return community\n\n    def get_community_3(self):\n        '''\n        \u9700\u8981\u5148\u8fdb\u884c\u7f16\u7801\u6811\u7684\u6784\u5efa build_coding_tree\n        k: k\u7ef4\u7ed3\u6784\u71b5\n        \u8fd4\u56de\u7b2c\u4e09\u5c42\u7684\u5212\u5206\n        '''\n        root_id = self.root_id\n        partition_id = self.tree_node[root_id].children\n        partition_2 = set()\n        for e in partition_id:\n            if self.tree_node[e].children == None:\n                partition_2.add(e)\n            else:\n                partition_2.symmetric_difference_update(\n                    self.tree_node[e].children)\n        partition_id = partition_2\n        community = torch.zeros(len(self.adj_matrix[0]), dtype=torch.int32)\n        community_id = 0\n        for e in partition_id:\n            partition_node = torch.tensor(self.tree_node[e].partition)\n            community[partition_node] = community_id\n            community_id += 1\n        return community\n\n    def deduct_se(self, leaf_id, root_id=None):\n        '''\n        \u9700\u5148\u8c03\u7528build_code_tree\u6784\u5efa\u7f16\u7801\u6811!\n        root_id = None\u65f6\u8868\u793a\u6839\u8282\u70b9\u4e3a\u7f16\u7801\u6811\u6839\u8282\u70b9\n        '''\n        node_dict = self.tree_node\n        path_id = [leaf_id]\n        current_id = leaf_id\n        while True:\n            parent_id = node_dict[current_id].parent\n            if parent_id == root_id:\n                break\n            if node_dict[parent_id].partition == node_dict[\n                    current_id].partition:\n                current_id = parent_id\n                path_id[-1] = current_id\n                continue\n            path_id.append(parent_id)\n            current_id = parent_id\n        if root_id == None:\n            path_id = path_id[0:-1]  #\u53bb\u9664\u6839\u8282\u70b9\n        g = []\n        vol = []\n        parent_vol = []\n        for e in path_id:\n            g.append(node_dict[e].g)\n            vol.append(node_dict[e].vol)\n            parent_vol.append(node_dict[node_dict[e].parent].vol)\n        g = torch.tensor(g)\n        vol = torch.tensor(vol) + 1\n        parent_vol = torch.tensor(parent_vol) + 1\n        deduct_se = -(g / self.VOL * torch.log2(vol / parent_vol)).sum()\n        return deduct_se\n\n    def LCA(self):\n        node_dict = self.tree_node\n        root_id = self.root_id\n        tree_node_num = max(node_dict.keys()) + 1\n        first = torch.zeros(tree_node_num)\n        height = torch.zeros(tree_node_num)\n        visited = torch.zeros(tree_node_num)\n        euler = []\n        stack = [root_id]\n        h = 0\n        while stack:\n            node_id = stack.pop()\n            euler.append(node_id)\n            if visited[node_id]:\n                h -= 1\n                continue\n            visited[node_id] = True\n            height[node_id] = h\n            h += 1\n            first[node_id] = len(euler) - 1\n            child = node_dict[node_id].children\n            if child is None:\n                continue\n            child = list(child)\n            if len(child) == 1 and node_dict[\n                    child[0]].partition == node_dict[node_id].partition:\n                child = child[0]\n                while True:\n                    first[child] = first[node_id]\n                    child = node_dict[child].children\n                    if child is None:\n                        break\n                    child = list(child)[0]\n                continue\n            for e in child[::-1]:\n                stack.append(node_id)\n                stack.append(e)\n            # stack.append(child[0])\n        self.first = first\n        self.height = height\n        self.euler = euler\n\n    def query_LCA(self, id1, id2):\n        tmp = torch.tensor([self.first[id1], self.first[id2]])\n        begin, end = tmp.min(), tmp.max()\n        interval = self.euler[begin.int():end.int() + 1]\n        height_interval = self.height[interval]\n        return interval[height_interval.argmin().int()]", "\n\ndef get_community(code_tree: PartitionTree):\n    node_dict = code_tree.tree_node\n    root_id = code_tree.root_id\n    tree_node_num = max(node_dict.keys()) + 1\n    isleaf = torch.zeros(tree_node_num, dtype=torch.bool)\n    stack = [root_id]\n    while stack:\n        node_id = stack.pop()\n        child = node_dict[node_id].children\n        if child is None:\n            isleaf[node_id] = True\n            continue\n        child = list(child)\n        for e in child[::-1]:\n            stack.append(e)\n        # stack.append(child[0])\n    community = []\n    for current_id in range(tree_node_num):\n        if isleaf[current_id]:\n            while True:\n                parent_id = node_dict[current_id].parent\n                if node_dict[parent_id].partition == node_dict[\n                        current_id].partition:\n                    isleaf[parent_id] = True\n                    current_id = parent_id\n                break\n    for e in node_dict.keys():\n        if not isleaf[e]:\n            community.append(e)\n    return community, isleaf", "\n\ndef get_sedict(community: list, code_tree: PartitionTree):\n    node_dict = code_tree.tree_node\n    se_dict = {}\n    for community_id in community:\n        node_list = list(node_dict[community_id].children)\n        se = torch.zeros(len(node_list))\n        for i, e in enumerate(node_list):\n            e = node_dict[e]\n            e: PartitionTreeNode\n            se[i] = -(e.g / code_tree.VOL) * torch.log2(\n                torch.tensor(\n                    (e.vol + 1) /\n                    (node_dict[e.parent].vol + 1))) + code_tree.deduct_se(\n                        community_id, None)\n            # se[i] = -(e.g / code_tree.VOL) * torch.log2(\n            #     torch.tensor((e.vol + 1) / (node_dict[e.parent].vol + 1)))\n        se = torch.softmax(se.float(), dim=0)\n        se_dict[community_id] = se\n    return se_dict", "\n\ndef select_link(community_id: int, code_tree: PartitionTree,\n                isleaf: torch.Tensor, se_dict):\n    node_dict = code_tree.tree_node\n    node_list = list(node_dict[community_id].children)\n    node_dict = code_tree.tree_node\n    se = se_dict[community_id]\n    id1, id2 = torch.multinomial(se, num_samples=2, replacement=True)\n    link_id1 = node_list[id1]\n    link_id2 = node_list[id2]\n    link_id = [link_id1, link_id2]\n    return link_id", "\n\ndef select_leaf(node_id, code_tree: PartitionTree, isleaf: torch.Tensor,\n                se_dict):\n    # print(node_id)\n    node_dict = code_tree.tree_node\n    while not isleaf[node_id]:\n        node_list = list(node_dict[node_id].children)\n        if len(node_list) > 1:  #\u907f\u514d\u53ea\u6709\u4e00\u4e2a\u5b57\u8282\u70b9\u7684\u975e\u53f6\u5b50\u8282\u70b9\n            se = se_dict[node_id]\n            # print(se)\n            id = torch.multinomial(se, num_samples=1, replacement=False)\n            node_id = node_list[id]\n        node_id = node_list[0]\n    return (node_dict[node_id].partition)[0]", "\n\ndef reshape(community: list, code_tree: PartitionTree, isleaf: torch.Tensor,\n            k):\n    se_dict = {}\n    edge_index = []\n    node_dict = code_tree.tree_node\n    # for k, v in code_tree.tree_node.items():\n    #     print(k, v.__dict__)\n    se_dict = get_sedict(community, code_tree)\n\n    for community_id in community:\n        node_list = list(node_dict[community_id].children)\n        if len(node_list) == 1:\n            continue\n        prefer_edge_num = round(k * len(node_list))\n        for i in range(prefer_edge_num):\n            id1, id2 = select_link(community_id, code_tree, isleaf, se_dict)\n            edge_index.append([\n                select_leaf(id1, code_tree, isleaf, se_dict),\n                select_leaf(id2, code_tree, isleaf, se_dict)\n            ])\n    edge_index = torch.tensor(edge_index)\n    edge_index = torch.cat((edge_index, torch.flip(edge_index, dims=[1])),\n                              dim=0)\n    edge_index = torch.unique(edge_index, dim=0)\n    return edge_index.t()", ""]}
{"filename": "opengsl/method/models/stable.py", "chunked_list": ["import torch.nn as nn\nimport torch\nimport scipy.sparse as sp\nimport numpy as np\nimport copy\nimport random\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndef preprocess_adj(features, adj, threshold=0.03, jaccard=True):\n    \"\"\"Drop dissimilar edges.(Faster version using numba)\n    \"\"\"\n    if not sp.issparse(adj):\n        adj = sp.csr_matrix(adj)\n\n    adj_triu = sp.triu(adj, format='csr')\n\n    if sp.issparse(features):\n        features = features.todense().A  # make it easier for njit processing\n\n    if jaccard:\n        removed_cnt = dropedge_jaccard(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n                                       threshold=threshold)\n    else:\n        removed_cnt = dropedge_cosine(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n                                      threshold=threshold)\n    print('removed %s edges in the original graph' % removed_cnt)\n    modified_adj = adj_triu + adj_triu.transpose()\n    return modified_adj", "\ndef preprocess_adj(features, adj, threshold=0.03, jaccard=True):\n    \"\"\"Drop dissimilar edges.(Faster version using numba)\n    \"\"\"\n    if not sp.issparse(adj):\n        adj = sp.csr_matrix(adj)\n\n    adj_triu = sp.triu(adj, format='csr')\n\n    if sp.issparse(features):\n        features = features.todense().A  # make it easier for njit processing\n\n    if jaccard:\n        removed_cnt = dropedge_jaccard(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n                                       threshold=threshold)\n    else:\n        removed_cnt = dropedge_cosine(adj_triu.data, adj_triu.indptr, adj_triu.indices, features,\n                                      threshold=threshold)\n    print('removed %s edges in the original graph' % removed_cnt)\n    modified_adj = adj_triu + adj_triu.transpose()\n    return modified_adj", "\n\ndef dropedge_jaccard(A, iA, jA, features, threshold):\n    removed_cnt = 0\n    for row in range(len(iA)-1):\n        for i in range(iA[row], iA[row+1]):\n            # print(row, jA[i], A[i])\n            n1 = row\n            n2 = jA[i]\n            a, b = features[n1], features[n2]\n            intersection = np.count_nonzero(a*b)\n            J = intersection * 1.0 / (np.count_nonzero(a) + np.count_nonzero(b) - intersection)\n\n            if J < threshold:\n                A[i] = 0\n                # A[n2, n1] = 0\n                removed_cnt += 1\n    return removed_cnt", "\n\ndef dropedge_cosine(A, iA, jA, features, threshold):\n    removed_cnt = 0\n    for row in range(len(iA)-1):\n        for i in range(iA[row], iA[row+1]):\n            # print(row, jA[i], A[i])\n            n1 = row\n            n2 = jA[i]\n            a, b = features[n1], features[n2]\n            inner_product = (a * b).sum()\n            C = inner_product / (np.sqrt(np.square(a).sum()) * np.sqrt(np.square(b).sum()) + 1e-8)\n            if C <= threshold:\n                A[i] = 0\n                # A[n2, n1] = 0\n                removed_cnt += 1\n    return removed_cnt", "\n\ndef aug_random_edge(input_adj, adj_delete, recover_percent=0.2):\n    percent = recover_percent\n    adj_delete = sp.tril(adj_delete)\n    row_idx, col_idx = adj_delete.nonzero()\n    edge_num = int(len(row_idx))\n    add_edge_num = int(edge_num * percent)\n    print(\"the number of recovering edges: {:04d}\" .format(add_edge_num))\n    aug_adj = copy.deepcopy(input_adj.todense().tolist())\n\n    edge_list = [(i, j) for i, j in zip(row_idx, col_idx)]\n    edge_idx = [i for i in range(edge_num)]\n    add_idx = random.sample(edge_idx, add_edge_num)\n\n    for i in add_idx:\n        aug_adj[edge_list[i][0]][edge_list[i][1]] = 1\n        aug_adj[edge_list[i][1]][edge_list[i][0]] = 1\n\n\n    aug_adj = np.matrix(aug_adj)\n    aug_adj = sp.csr_matrix(aug_adj)\n    return aug_adj", "\n\ndef get_reliable_neighbors(adj, features, k, degree_threshold):\n    degree = adj.sum(dim=1)\n    degree_mask = degree > degree_threshold\n    assert degree_mask.sum().item() >= k\n    sim = cosine_similarity(features.to('cpu'))\n    sim = torch.FloatTensor(sim).to('cuda')\n    sim[:, degree_mask == False] = 0\n    _, top_k_indices = sim.topk(k=k, dim=1)\n    for i in range(adj.shape[0]):\n        adj[i][top_k_indices[i]] = 1\n        adj[i][i] = 0\n    return", "\n\nclass DGI(nn.Module):\n    def __init__(self, n_in, n_h, activation):\n        super(DGI, self).__init__()\n        self.gcn = GCN_DGI(n_in, n_h, activation)\n        self.read = AvgReadout()\n        self.sigm = nn.Sigmoid()\n        self.disc = Discriminator(n_h)\n\n    # (features, shuf_fts, aug_features1, aug_features2,\n    #  sp_adj if sparse else adj,\n    #  sp_aug_adj1 if sparse else aug_adj1,\n    #  sp_aug_adj2 if sparse else aug_adj2,\n    #  sparse, None, None, None, aug_type=aug_type\n    def forward(self, seq1, seq2, adj, aug_adj1, aug_adj2):\n        h_0 = self.gcn(seq1, adj)\n\n        h_1 = self.gcn(seq1, aug_adj1)\n        h_3 = self.gcn(seq1, aug_adj2)\n\n        c_1 = self.read(h_1)\n        c_1 = self.sigm(c_1)\n\n        c_3 = self.read(h_3)\n        c_3 = self.sigm(c_3)\n\n        h_2 = self.gcn(seq2, adj)\n\n        ret1 = self.disc(c_1, h_0, h_2)\n        ret2 = self.disc(c_3, h_0, h_2)\n\n        ret = ret1 + ret2   # \u8fd9\u91cc\u5b9e\u9645\u4e0a\u4e0d\u7b26\u5408\u516c\u5f0f\n        return ret\n\n    # Detach the return variables\n    def embed(self, seq, adj):\n        h_1 = self.gcn(seq, adj)\n        # c = self.read(h_1)\n\n        return h_1.detach()", "\n\nclass GCN_DGI(nn.Module):\n    def __init__(self, in_ft, out_ft, act, bias=True):\n        super(GCN_DGI, self).__init__()\n        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n        self.act = nn.PReLU() if act == 'prelu' else act\n\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n            self.bias.data.fill_(0.0)\n        else:\n            self.register_parameter('bias', None)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    # Shape of seq: (batch, nodes, features)\n    def forward(self, seq, adj):\n        seq_fts = self.fc(seq)\n        out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n        if self.bias is not None:\n            out += self.bias\n\n        return self.act(out)", "\n\nclass AvgReadout(nn.Module):\n    def __init__(self):\n        super(AvgReadout, self).__init__()\n\n    def forward(self, seq):\n        return torch.mean(seq, 1)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c, h_pl, h_mi):\n        # \u5224\u65ad\u4e00\u4e2aview\u548c\u539fadj\u3001\u6270\u52a8adj\u7684\u5173\u7cfb [1, 2*n_nodes]\n        c_x = torch.unsqueeze(c, 1)\n        c_x = c_x.expand_as(h_pl)\n        tmp = self.f_k(h_pl, c_x)\n        sc_1 = torch.squeeze(tmp, 2)\n        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n\n        logits = torch.cat((sc_1, sc_2), 1)\n\n        return logits", "\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c, h_pl, h_mi):\n        # \u5224\u65ad\u4e00\u4e2aview\u548c\u539fadj\u3001\u6270\u52a8adj\u7684\u5173\u7cfb [1, 2*n_nodes]\n        c_x = torch.unsqueeze(c, 1)\n        c_x = c_x.expand_as(h_pl)\n        tmp = self.f_k(h_pl, c_x)\n        sc_1 = torch.squeeze(tmp, 2)\n        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n\n        logits = torch.cat((sc_1, sc_2), 1)\n\n        return logits"]}
{"filename": "opengsl/method/models/jknet.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n\nclass GraphConvolution(nn.Module):\n\n    def __init__(self, in_features, out_features, dropout=0.5, n_linear=1, bias=True, spmm_type=1, act='relu'):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.mlp = nn.ModuleList()\n        self.mlp.append(nn.Linear(in_features, out_features, bias=bias))\n        for i in range(n_linear-1):\n            self.mlp.append(nn.Linear(out_features, out_features, bias=bias))\n        self.dropout = dropout\n        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n        self.act = eval('F.'+act) if not act=='identity' else lambda x: x\n\n\n    def forward(self, input, adj):\n        \"\"\" Graph Convolutional Layer forward function\n        \"\"\"\n        x = self.spmm(adj, input)\n        for i in range(len(self.mlp)-1):\n            x = self.mlp[i](x)\n            x = self.act(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.mlp[-1](x)\n        x = self.act(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        return x\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "\n\nclass JKNet(nn.Module):\n\n    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm=None, n_linear=1, spmm_type=0, act='relu', general='concat', input_layer=True, output_layer=True):\n\n        super(JKNet, self).__init__()\n\n        self.nfeat = nfeat\n        self.nclass = nclass\n        self.n_layers = n_layers\n        self.input_layer = input_layer\n        self.output_layer = output_layer\n        self.n_linear = n_linear\n        self.norm_flag = norm['flag']\n        self.norm_type = eval(\"nn.\"+norm['norm_type'])\n        self.general_aggregation = eval('self.'+general)\n        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n        if input_layer:\n            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n            self.input_drop = nn.Dropout(input_dropout)\n        if output_layer:\n            self.output_linear = nn.Linear(in_features=nhid * n_layers, out_features=nclass)\n            self.output_normalization = self.norm_type(nhid)\n        self.convs = nn.ModuleList()\n        if self.norm_flag:\n            self.norms = nn.ModuleList()\n        else:\n            self.norms = None\n\n        for i in range(n_layers):\n            if i == 0 and not self.input_layer:\n                in_hidden = nfeat\n            else:\n                in_hidden = nhid\n\n            out_hidden = nhid\n\n            self.convs.append(GraphConvolution(in_hidden, out_hidden, dropout, n_linear, spmm_type=spmm_type, act=act))\n            if self.norm_flag:\n                self.norms.append(self.norm_type(in_hidden))\n\n        if (general == 'concat'):\n            self.last_layer = nn.Linear(nhid * n_layers, nclass)\n        else :\n            self.last_layer = nn.Linear(nhid, nclass) \n        if (general == 'LSMT'):\n            self.lstm = nn.LSTM(nhid, (n_layers * nhid) // 2, bidirectional=True, batch_first=True)\n            self.attn = nn.Linear(2 * ((n_layers * nhid) // 2), 1)\n\n\n    def forward(self, input):\n        x=input[0]\n        adj=input[1]\n        only_z=input[2]\n        layer_outputs = []\n        if self.input_layer:\n            x = self.input_linear(x)\n            x = self.input_drop(x)\n            x = self.act(x)\n\n        for i, layer in enumerate(self.convs):\n            if self.norm_flag:\n                x_res = self.norms[i](x)\n                x_res = layer(x_res, adj)\n                x = x + x_res\n            else:\n                x = layer(x,adj)\n            layer_outputs.append(x)\n        mid = self.general_aggregation(layer_outputs)\n        if self.output_layer:\n            x = self.output_normalization(x)\n        x = self.last_layer(mid).squeeze(1)\n        if only_z:\n            return x\n        else:\n            return mid, x\n\n    def concat(self, layer_outputs):\n        return torch.cat(layer_outputs, dim=1)\n    \n    def maxpool(self, layer_outputs):\n        return torch.max(torch.stack(layer_outputs, dim=0), dim=0)[0]\n    \n    def LSMT(self, layer_outputs):\n        x = torch.stack(layer_outputs, dim=1)\n        alpha, _ = self.lstm(x)\n        alpha = self.attn(alpha).squeeze(-1)\n        alpha = torch.softmax(alpha, dim=-1).unsqueeze(-1)\n        return (x * alpha).sum(dim=1)", ""]}
{"filename": "opengsl/method/models/cogsl.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#from module.view_estimator import View_Estimator\n#from module.cls import Classification\n#from module.mi_nce import MI_NCE\n#from module.fusion import Fusion\nfrom torch_geometric.nn import GCNConv\nfrom torch_sparse import SparseTensor\n", "from torch_sparse import SparseTensor\n\n#from .gcn import GraphConvolution\nimport numpy as np\n\nclass GCN_two_pyg(nn.Module):\n    def __init__(self, input_dim, hid_dim1, hid_dim2, dropout=0., activation=\"relu\"):\n        super(GCN_two_pyg, self).__init__()\n        self.conv1 = GCNConv(input_dim, hid_dim1)\n        self.conv2 = GCNConv(hid_dim1, hid_dim2)\n\n        self.dropout = dropout\n        assert activation in [\"relu\", \"leaky_relu\", \"elu\"]\n        self.activation = getattr(F, activation)\n\n    def forward(self, feature, adj):\n        adj = SparseTensor.from_torch_sparse_coo_tensor(adj)\n        x1 = self.activation(self.conv1(feature, adj))\n        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n        x2 = self.conv2(x1, adj)\n        return x2", "\nclass GCN_one_pyg(nn.Module):\n    def __init__(self, in_ft, out_ft, bias=True, activation=None):\n        super(GCN_one_pyg, self).__init__()\n        self.conv1 = GCNConv(in_ft, out_ft)\n        self.activation = activation\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n            self.bias.data.fill_(0.0)\n        else:\n            self.register_parameter('bias', None)\n\n    def forward(self, feat, adj):\n        # print(adj)\n        # exit(0)\n        adj = SparseTensor.from_torch_sparse_coo_tensor(adj)\n        out = self.conv1(feat, adj)\n        if self.bias is not None:\n            out += self.bias\n        if self.activation is not None:\n            out = self.activation(out)\n        return out", "\nclass GCN_two(nn.Module):\n    def __init__(self, input_dim, hid_dim1, hid_dim2, dropout=0., activation=\"relu\"):\n        super(GCN_two, self).__init__()\n        self.conv1 = GCN_one(input_dim, hid_dim1)\n        self.conv2 = GCN_one(hid_dim1, hid_dim2)\n\n        self.dropout = dropout\n        assert activation in [\"relu\", \"leaky_relu\", \"elu\"]\n        self.activation = getattr(F, activation)\n\n    def forward(self, feature, adj):\n        x1 = self.activation(self.conv1(feature, adj))\n        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n        x2 = self.conv2(x1, adj)\n        return x2  # F.log_softmax(x2, dim=1)", "\n\nclass GCN_one(nn.Module):\n    def __init__(self, in_ft, out_ft, bias=True, activation=None):\n        super(GCN_one, self).__init__()\n        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n        self.activation = activation\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n            self.bias.data.fill_(0.0)\n        else:\n            self.register_parameter('bias', None)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, feat, adj):\n        adj = adj.to_dense()\n        feat = self.fc(feat)\n        out = torch.spmm(adj, feat)\n        if self.bias is not None:\n            out += self.bias\n        if self.activation is not None:\n            out = self.activation(out)\n        return out", "\n#class two_layer_GCN(nn.Module):\n#    def __init__(self, num_feature, cls_hid_1, num_class, dropout = 0.5, act = 'relu'):\n#        super(two_layer_GCN, self).__init__()\n#        self.layer1 = GraphConvolution(num_feature, cls_hid_1, dropout, act= act)\n#        self.layer2 = GraphConvolution(cls_hid_1, num_class, dropout=0, last_layer=True)\n#\n#    def forward(self, feature, adj):\n#        x = self.layer1(feature, adj)\n#        x = self.layer2(x,adj)", "#        x = self.layer1(feature, adj)\n#        x = self.layer2(x,adj)\n#        return x\n\nclass Classification(nn.Module):\n    def __init__(self, num_feature, cls_hid_1, num_class, dropout, pyg):\n        super(Classification, self).__init__()\n        if pyg==False:\n            self.encoder_v1 = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n            self.encoder_v2 = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n            self.encoder_v = GCN_two(num_feature, cls_hid_1, num_class, dropout)\n        else:\n            #print(\"pyg\")\n            self.encoder_v1 = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n            self.encoder_v2 = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n            self.encoder_v = GCN_two_pyg(num_feature, cls_hid_1, num_class, dropout)\n        #self.encoder_v = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n        #self.encoder_v1 = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n        #self.encoder_v2 = two_layer_GCN(num_feature, cls_hid_1, num_class, dropout)\n\n    def forward(self, feat, view, flag):\n        if flag == \"v1\":\n            prob = F.softmax(self.encoder_v1(feat, view), dim=1)\n        elif flag == \"v2\":\n            prob = F.softmax(self.encoder_v2(feat, view), dim=1)\n        elif flag == \"v\":\n            prob = F.softmax(self.encoder_v(feat, view), dim=1)\n        return prob", "\nclass Contrast:\n    def __init__(self, tau):\n        self.tau = tau\n\n    def sim(self, z1, z2):\n        z1_norm = torch.norm(z1, dim=-1, keepdim=True)\n        z2_norm = torch.norm(z2, dim=-1, keepdim=True)\n        dot_numerator = torch.mm(z1, z2.t())\n        dot_denominator = torch.mm(z1_norm, z2_norm.t())\n        sim_matrix = torch.exp(dot_numerator / dot_denominator / self.tau)\n        return sim_matrix\n\n    def cal(self, z1_proj, z2_proj):\n        matrix_z1z2 = self.sim(z1_proj, z2_proj)\n        matrix_z2z1 = matrix_z1z2.t()\n\n        matrix_z1z2 = matrix_z1z2 / (torch.sum(matrix_z1z2, dim=1).view(-1, 1) + 1e-8)\n        lori_v1v2 = -torch.log(matrix_z1z2.diag()+1e-8).mean()\n\n        matrix_z2z1 = matrix_z2z1 / (torch.sum(matrix_z2z1, dim=1).view(-1, 1) + 1e-8)\n        lori_v2v1 = -torch.log(matrix_z2z1.diag()+1e-8).mean()\n        return (lori_v1v2 + lori_v2v1) / 2", "\nclass Fusion(nn.Module):\n    def __init__(self, lam, alpha, name):\n        super(Fusion, self).__init__()\n        self.lam = lam\n        self.alpha = alpha\n        self.name = name\n\n    def get_weight(self, prob):\n        if len(prob.shape)==1:\n            prob = torch.stack([prob, 1 - prob],dim=1)\n        out, _ = prob.topk(2, dim=1, largest=True, sorted=True)\n        fir = out[:, 0]\n        sec = out[:, 1]\n        w = torch.exp(self.alpha*(self.lam*torch.log(fir+1e-8) + (1-self.lam)*torch.log(fir-sec+1e-8)))\n        return w\n\n    def forward(self, v1, prob_v1, v2, prob_v2):\n        w_v1 = self.get_weight(prob_v1)\n        w_v2 = self.get_weight(prob_v2)\n        beta_v1 = w_v1 / (w_v1 + w_v2)\n        beta_v2 = w_v2 / (w_v1 + w_v2)\n        if self.name not in [\"citeseer\", \"digits\", \"polblogs\", \"cora\"]:\n            beta_v1 = beta_v1.diag().to_sparse()\n            beta_v2 = beta_v2.diag().to_sparse()\n            v = torch.sparse.mm(beta_v1, v1) + torch.sparse.mm(beta_v2, v2)\n            return v\n        else :\n            beta_v1 = beta_v1.unsqueeze(1)\n            beta_v2 = beta_v2.unsqueeze(1)\n            v = beta_v1 * v1.to_dense() + beta_v2 * v2.to_dense()\n            return v.to_sparse()", "\nclass GenView(nn.Module):\n    def __init__(self, num_feature, hid, com_lambda, dropout, pyg):\n        super(GenView, self).__init__()\n        if pyg == False:\n            self.gen_gcn = GCN_one(num_feature, hid, activation=nn.ReLU())\n        else:\n            self.gen_gcn = GCN_one_pyg(num_feature, hid, activation=nn.ReLU())  \n        #self.gen_gcn = GraphConvolution(num_feature, hid, dropout=0)\n        self.gen_mlp = nn.Linear(2 * hid, 1)\n        nn.init.xavier_normal_(self.gen_mlp.weight, gain=1.414)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.com_lambda = com_lambda\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, v_ori, feat, v_indices, num_node):\n        emb = self.gen_gcn(feat, v_ori)\n        f1 = emb[v_indices[0]]\n        f2 = emb[v_indices[1]]\n        ff = torch.cat([f1, f2], dim=-1)\n        temp = self.gen_mlp(self.dropout(ff)).reshape(-1)\n        \n        z_matrix = torch.sparse.FloatTensor(v_indices, temp, (num_node, num_node))\n        pi = torch.sparse.softmax(z_matrix, dim=1)\n        gen_v = v_ori + self.com_lambda * pi \n        return gen_v", "\n\nclass View_Estimator(nn.Module):\n    def __init__(self, num_feature, gen_hid, com_lambda_v1, com_lambda_v2, dropout, pyg, big):\n        super(View_Estimator, self).__init__()\n        self.v1_gen = GenView(num_feature, gen_hid, com_lambda_v1, dropout, pyg)\n        self.v2_gen = GenView(num_feature, gen_hid, com_lambda_v2, dropout, pyg)\n        if big:\n            self.normalize = self.normalize1\n        else:\n            self.normalize = self.normalize2\n\n    def normalize1(self, adj):\n        return (adj + adj.t())\n\n    def normalize2(self, mx):\n        mx = mx + mx.t() + torch.eye(mx.shape[0]).to(mx.device).to_sparse()\n        mx = mx.to_dense()\n        rowsum = mx.sum(1) + 1e-6  # avoid NaN\n        r_inv = rowsum.pow(-1 / 2).flatten()\n        r_inv[torch.isinf(r_inv)] = 0.\n        r_mat_inv = torch.diag(r_inv)\n        mx = r_mat_inv @ mx\n        mx = mx @ r_mat_inv\n        return mx.to_sparse()\n\n    def forward(self, view1, view1_indices, view2, view2_indices, num_nodes, feats):\n        new_v1 = self.normalize(self.v1_gen(view1, feats, view1_indices, num_nodes))\n        new_v2 = self.normalize(self.v2_gen(view2, feats, view2_indices, num_nodes))\n        return new_v1, new_v2", "\nclass MI_NCE(nn.Module):\n    def __init__(self, num_feature, mi_hid_1, tau, pyg, big, batch):\n        super(MI_NCE, self).__init__()\n        if pyg == False:\n            self.gcn = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n            self.gcn1 = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n            self.gcn2 = GCN_one(num_feature, mi_hid_1, activation=nn.PReLU())\n        else:\n            #print(\"pyg\")\n            self.gcn = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n            self.gcn1 = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n            self.gcn2 = GCN_one_pyg(num_feature, mi_hid_1, activation=nn.PReLU())\n        #self.gcn = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n        #self.gcn1 = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n        #self.gcn2 = GraphConvolution(num_feature, mi_hid_1, act='relu', dropout=0)\n        self.proj = nn.Sequential(\n            nn.Linear(mi_hid_1, mi_hid_1),\n            nn.ELU(),\n            nn.Linear(mi_hid_1, mi_hid_1)\n        )\n        self.con = Contrast(tau)\n        self.big = big\n        self.batch = batch\n\n    def forward(self, views, feat):\n        v_emb = self.proj(self.gcn(feat, views[0]))\n        v1_emb = self.proj(self.gcn1(feat, views[1]))\n        v2_emb = self.proj(self.gcn2(feat, views[2]))\n        # if dataset is so big, we will randomly sample part of nodes to perform MI estimation\n        if self.big == True:\n            idx = np.random.choice(feat.shape[0], self.batch, replace=False)\n            idx.sort()\n            v_emb = v_emb[idx]\n            v1_emb = v1_emb[idx]\n            v2_emb = v2_emb[idx]\n            \n        vv1 = self.con.cal(v_emb, v1_emb)\n        vv2 = self.con.cal(v_emb, v2_emb)\n        v1v2 = self.con.cal(v1_emb, v2_emb)\n\n        return vv1, vv2, v1v2", "\nclass CoGSL(nn.Module):\n    def __init__(self, num_feature, cls_hid_1, num_class, gen_hid, mi_hid_1,\n                 com_lambda_v1, com_lambda_v2, lam, alpha, cls_dropout, ve_dropout, tau, pyg, big, batch, name):\n        super(CoGSL, self).__init__()\n        self.cls = Classification(num_feature, cls_hid_1, num_class, cls_dropout, pyg)\n        self.ve = View_Estimator(num_feature, gen_hid, com_lambda_v1, com_lambda_v2, ve_dropout, pyg, big)\n        self.mi = MI_NCE(num_feature, mi_hid_1, tau, pyg, big, batch)\n        self.fusion = Fusion(lam, alpha, name)\n        \n\n    def get_view(self, view1, view1_indices, view2, view2_indices, num_nodes, feats):\n        new_v1, new_v2 = self.ve(view1, view1_indices, view2, view2_indices, num_nodes, feats)\n        return new_v1, new_v2\n\n    def get_mi_loss(self, feat, views):\n        mi_loss = self.mi(views, feat)\n        return mi_loss\n\n    def get_cls_loss(self, v1, v2, feat):\n        prob_v1 = self.cls(feat, v1, \"v1\")\n        prob_v2 = self.cls(feat, v2, \"v2\")\n        logits_v1 = torch.log(prob_v1 + 1e-8)\n        logits_v2 = torch.log(prob_v2 + 1e-8)\n        return logits_v1.squeeze(1), logits_v2.squeeze(1), prob_v1.squeeze(1), prob_v2.squeeze(1)\n\n    def get_v_cls_loss(self, v, feat):\n        logits = torch.log(self.cls(feat, v, \"v\") + 1e-8)\n        return logits.squeeze(1)\n\n    def get_fusion(self, v1, prob_v1, v2, prob_v2):\n        v = self.fusion(v1, prob_v1, v2, prob_v2)\n        return v"]}
{"filename": "opengsl/method/models/sublime.py", "chunked_list": ["import dgl\nimport torch\nimport torch.nn as nn\nfrom sklearn.neighbors import kneighbors_graph\nfrom .gcn import GCN\nfrom .gnn_modules import APPNP\nimport dgl.function as fn\nimport numpy as np\nimport torch.nn.functional as F\nimport copy", "import torch.nn.functional as F\nimport copy\nEOS = 1e-10\n\n\ndef get_feat_mask(features, mask_rate):\n    feat_node = features.shape[1]\n    mask = torch.zeros(features.shape)\n    samples = np.random.choice(feat_node, size=int(feat_node * mask_rate), replace=False)\n    mask[:, samples] = 1\n    return mask.cuda(), samples", "\n\ndef split_batch(init_list, batch_size):\n    groups = zip(*(iter(init_list),) * batch_size)\n    end_list = [list(i) for i in groups]\n    count = len(init_list) % batch_size\n    end_list.append(init_list[-count:]) if count != 0 else end_list\n    return end_list\n\n\ndef dgl_graph_to_torch_sparse(dgl_graph):\n    values = dgl_graph.edata['w'].cpu().detach()\n    rows_, cols_ = dgl_graph.edges()\n    indices = torch.cat((torch.unsqueeze(rows_, 0), torch.unsqueeze(cols_, 0)), 0).cpu()\n    torch_sparse_mx = torch.sparse.FloatTensor(indices, values)\n    return torch_sparse_mx", "\n\ndef dgl_graph_to_torch_sparse(dgl_graph):\n    values = dgl_graph.edata['w'].cpu().detach()\n    rows_, cols_ = dgl_graph.edges()\n    indices = torch.cat((torch.unsqueeze(rows_, 0), torch.unsqueeze(cols_, 0)), 0).cpu()\n    torch_sparse_mx = torch.sparse.FloatTensor(indices, values)\n    return torch_sparse_mx\n\n\ndef torch_sparse_to_dgl_graph(torch_sparse_mx):\n    torch_sparse_mx = torch_sparse_mx.coalesce()\n    indices = torch_sparse_mx.indices()\n    values = torch_sparse_mx.values()\n    rows_, cols_ = indices[0,:], indices[1,:]\n    dgl_graph = dgl.graph((rows_, cols_), num_nodes=torch_sparse_mx.shape[0], device='cuda')\n    dgl_graph.edata['w'] = values.detach().cuda()\n    return dgl_graph", "\n\ndef torch_sparse_to_dgl_graph(torch_sparse_mx):\n    torch_sparse_mx = torch_sparse_mx.coalesce()\n    indices = torch_sparse_mx.indices()\n    values = torch_sparse_mx.values()\n    rows_, cols_ = indices[0,:], indices[1,:]\n    dgl_graph = dgl.graph((rows_, cols_), num_nodes=torch_sparse_mx.shape[0], device='cuda')\n    dgl_graph.edata['w'] = values.detach().cuda()\n    return dgl_graph", "\n\ndef nearest_neighbors_pre_elu(X, k, metric, i):\n    # \u8fd9\u4e2a\u521d\u59cb\u5316\u6709\u70b9\u4e0d\u7406\u89e3\n    adj = kneighbors_graph(X, k, metric=metric)\n    adj = np.array(adj.todense(), dtype=np.float32)\n    adj += np.eye(adj.shape[0])\n    adj = adj * i - i\n    return adj\n", "\n\ndef knn_fast(X, k, b):\n    X = F.normalize(X, dim=1, p=2)\n    index = 0\n    values = torch.zeros(X.shape[0] * (k + 1)).cuda()\n    rows = torch.zeros(X.shape[0] * (k + 1)).cuda()\n    cols = torch.zeros(X.shape[0] * (k + 1)).cuda()\n    norm_row = torch.zeros(X.shape[0]).cuda()\n    norm_col = torch.zeros(X.shape[0]).cuda()\n    while index < X.shape[0]:\n        if (index + b) > (X.shape[0]):\n            end = X.shape[0]\n        else:\n            end = index + b\n        sub_tensor = X[index:index + b]\n        similarities = torch.mm(sub_tensor, X.t())\n        vals, inds = similarities.topk(k=k + 1, dim=-1)\n        values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n        cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n        rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)\n        norm_row[index: end] = torch.sum(vals, dim=1)\n        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n        index += b\n    norm = norm_row + norm_col\n    rows = rows.long()\n    cols = cols.long()\n    values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n    return rows, cols, values", "\n\ndef apply_non_linearity(tensor, non_linearity, i):\n    if non_linearity == 'elu':\n        return F.elu(tensor * i - i) + 1\n    elif non_linearity == 'relu':\n        return F.relu(tensor)\n    elif non_linearity == 'none':\n        return tensor\n    else:\n        raise NameError('We dont support the non-linearity yet')", "\n\ndef cal_similarity_graph(node_embeddings):\n    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n    return similarity_graph\n\n\ndef top_k(raw_graph, K):\n    values, indices = raw_graph.topk(k=int(K), dim=-1)\n    assert torch.max(indices) < raw_graph.shape[1]\n    mask = torch.zeros(raw_graph.shape).cuda()\n    mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.\n\n    mask.requires_grad = False\n    sparse_graph = raw_graph * mask\n    return sparse_graph", "\n\nclass GCNConv_dgl(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(GCNConv_dgl, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    def forward(self, x, g):\n        with g.local_scope():\n            g.ndata['h'] = self.linear(x)\n            g.update_all(fn.u_mul_e('h', 'w', 'm'), fn.sum(msg='m', out='h'))\n            return g.ndata['h']", "\n\nclass Attentive(nn.Module):\n    def __init__(self, isize):\n        super(Attentive, self).__init__()\n        self.w = nn.Parameter(torch.ones(isize))\n\n    def forward(self, x):\n        return x @ torch.diag(self.w)\n", "\n\nclass FGP_learner(nn.Module):\n    def __init__(self, features, k, knn_metric, i, sparse):\n        super(FGP_learner, self).__init__()\n\n        self.k = k\n        self.knn_metric = knn_metric\n        self.i = i\n        self.sparse = sparse\n\n        self.Adj = nn.Parameter(\n            torch.from_numpy(nearest_neighbors_pre_elu(features, self.k, self.knn_metric, self.i)))\n\n    def forward(self, h):\n        if not self.sparse:\n            Adj = F.elu(self.Adj) + 1\n        else:\n            Adj = self.Adj.coalesce()\n            Adj.values = F.elu(Adj.values()) + 1\n        return Adj", "\n\nclass ATT_learner(nn.Module):\n    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, mlp_act):\n        super(ATT_learner, self).__init__()\n\n        self.i = i\n        self.layers = nn.ModuleList()\n        for _ in range(nlayers):\n            self.layers.append(Attentive(isize))\n        self.k = k\n        self.knn_metric = knn_metric\n        self.non_linearity = 'relu'\n        self.sparse = sparse\n        self.mlp_act = mlp_act\n\n    def internal_forward(self, h):\n        for i, layer in enumerate(self.layers):\n            h = layer(h)\n            if i != (len(self.layers) - 1):\n                if self.mlp_act == \"relu\":\n                    h = F.relu(h)\n                elif self.mlp_act == \"tanh\":\n                    h = F.tanh(h)\n        return h\n\n    def forward(self, features):\n        if self.sparse:\n            embeddings = self.internal_forward(features)\n            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n            rows_ = torch.cat((rows, cols))\n            cols_ = torch.cat((cols, rows))\n            values_ = torch.cat((values, values))\n            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n            adj.edata['w'] = values_\n            return adj\n        else:\n            embeddings = self.internal_forward(features)\n            embeddings = F.normalize(embeddings, dim=1, p=2)\n            similarities = cal_similarity_graph(embeddings)\n            similarities = top_k(similarities, self.k + 1)\n            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n            return similarities", "\n\nclass MLP_learner(nn.Module):\n    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, act):\n        super(MLP_learner, self).__init__()\n\n        self.layers = nn.ModuleList()\n        if nlayers == 1:\n            self.layers.append(nn.Linear(isize, isize))\n        else:\n            self.layers.append(nn.Linear(isize, isize))\n            for _ in range(nlayers - 2):\n                self.layers.append(nn.Linear(isize, isize))\n            self.layers.append(nn.Linear(isize, isize))\n\n        self.input_dim = isize\n        self.output_dim = isize\n        self.k = k\n        self.knn_metric = knn_metric\n        self.non_linearity = 'relu'\n        self.param_init()\n        self.i = i\n        self.sparse = sparse\n        self.act = act\n\n    def internal_forward(self, h):\n        for i, layer in enumerate(self.layers):\n            h = layer(h)\n            if i != (len(self.layers) - 1):\n                if self.act == \"relu\":\n                    h = F.relu(h)\n                elif self.act == \"tanh\":\n                    h = F.tanh(h)\n        return h\n\n    def param_init(self):\n        for layer in self.layers:\n            layer.weight = nn.Parameter(torch.eye(self.input_dim))\n\n    def forward(self, features):\n        if self.sparse:\n            embeddings = self.internal_forward(features)\n            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n            rows_ = torch.cat((rows, cols))\n            cols_ = torch.cat((cols, rows))\n            values_ = torch.cat((values, values))\n            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n            adj.edata['w'] = values_\n            return adj\n        else:\n            embeddings = self.internal_forward(features)\n            embeddings = F.normalize(embeddings, dim=1, p=2)\n            similarities = cal_similarity_graph(embeddings)\n            similarities = top_k(similarities, self.k + 1)\n            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n            return similarities", "\n\nclass GNN_learner(nn.Module):\n    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, mlp_act, adj):\n        super(GNN_learner, self).__init__()\n\n        self.adj = adj\n        self.layers = nn.ModuleList()\n        if nlayers == 1:\n            self.layers.append(GCNConv_dgl(isize, isize))\n        else:\n            self.layers.append(GCNConv_dgl(isize, isize))\n            for _ in range(nlayers - 2):\n                self.layers.append(GCNConv_dgl(isize, isize))\n            self.layers.append(GCNConv_dgl(isize, isize))\n\n        self.input_dim = isize\n        self.output_dim = isize\n        self.k = k\n        self.knn_metric = knn_metric\n        self.non_linearity = 'relu'\n        self.param_init()\n        self.i = i\n        self.sparse = sparse\n        self.mlp_act = mlp_act\n\n    def internal_forward(self, h):\n        for i, layer in enumerate(self.layers):\n            h = layer(h, self.adj)\n            if i != (len(self.layers) - 1):\n                if self.mlp_act == \"relu\":\n                    h = F.relu(h)\n                elif self.mlp_act == \"tanh\":\n                    h = F.tanh(h)\n        return h\n\n    def param_init(self):\n        for layer in self.layers:\n            layer.weight = nn.Parameter(torch.eye(self.input_dim))\n\n    def forward(self, features):\n        if self.sparse:\n            embeddings = self.internal_forward(features)\n            rows, cols, values = knn_fast(embeddings, self.k, 1000)\n            rows_ = torch.cat((rows, cols))\n            cols_ = torch.cat((cols, rows))\n            values_ = torch.cat((values, values))\n            values_ = apply_non_linearity(values_, self.non_linearity, self.i)\n            adj = dgl.graph((rows_, cols_), num_nodes=features.shape[0], device='cuda')\n            adj.edata['w'] = values_\n            return adj\n        else:\n            embeddings = self.internal_forward(features)\n            embeddings = F.normalize(embeddings, dim=1, p=2)\n            similarities = cal_similarity_graph(embeddings)\n            similarities = top_k(similarities, self.k + 1)\n            similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n            return similarities", "\n\nclass GraphEncoder(nn.Module):\n    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, sparse, conf=None):\n\n        super(GraphEncoder, self).__init__()\n        self.dropout = dropout\n        self.sparse = sparse\n        self.gnn_encoder_layers = nn.ModuleList()\n        if sparse:\n            self.gnn_encoder_layers.append(GCNConv_dgl(in_dim, hidden_dim))\n            for _ in range(nlayers - 2):\n                self.gnn_encoder_layers.append(GCNConv_dgl(hidden_dim, hidden_dim))\n            self.gnn_encoder_layers.append(GCNConv_dgl(hidden_dim, emb_dim))\n        else:\n            if conf.model['type']=='gcn':\n                self.model = GCN(nfeat=in_dim, nhid=hidden_dim, nclass=emb_dim, n_layers=nlayers, dropout=dropout,\n                                 input_layer=False, output_layer=False, spmm_type=0)\n            elif conf.model['type']=='appnp':\n                self.model = APPNP(in_dim, hidden_dim, emb_dim,\n                                    dropout=conf.model['dropout'], K=conf.model['K'],\n                                    alpha=conf.model['alpha'])\n        self.proj_head = nn.Sequential(nn.Linear(emb_dim, proj_dim), nn.ReLU(inplace=True),\n                                           nn.Linear(proj_dim, proj_dim))\n\n    def forward(self, x, Adj_):\n\n        if self.sparse:\n            for conv in self.gnn_encoder_layers[:-1]:\n                x = conv(x, Adj_)\n                x = F.relu(x)\n                x = F.dropout(x, p=self.dropout, training=self.training)\n            x = self.gnn_encoder_layers[-1](x, Adj_)\n        else:\n            x = self.model((x, Adj_, True))\n        z = self.proj_head(x)\n        return z, x", "\n\nclass GCL(nn.Module):\n    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj, sparse, conf=None):\n        super(GCL, self).__init__()\n\n        self.encoder = GraphEncoder(nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, sparse, conf)\n        self.dropout_adj = dropout_adj\n        self.sparse = sparse\n\n    def forward(self, x, Adj_, branch=None):\n\n        # edge dropping\n        if self.sparse:\n            if branch == 'anchor':\n                Adj = copy.deepcopy(Adj_)\n            else:\n                Adj = Adj_\n            Adj.edata['w'] = F.dropout(Adj.edata['w'], p=self.dropout_adj, training=self.training)\n        else:\n            Adj = F.dropout(Adj_, p=self.dropout_adj, training=self.training)\n\n        # get representations\n        z, embedding = self.encoder(x, Adj)\n        return z, embedding\n\n    @staticmethod\n    def calc_loss(x, x_aug, temperature=0.2):\n        batch_size, _ = x.size()\n        x_abs = x.norm(dim=1)\n        x_aug_abs = x_aug.norm(dim=1)\n\n        sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)   # \u8ba1\u7b97\u7684\u662fcos\u76f8\u4f3c\u5ea6\n        sim_matrix = torch.exp(sim_matrix / temperature)\n        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n        loss_0 = pos_sim / (sim_matrix.sum(dim=0) - pos_sim)\n        loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n\n        loss_0 = - torch.log(loss_0).mean()\n        loss_1 = - torch.log(loss_1).mean()\n        loss = (loss_0 + loss_1) / 2.0\n        return loss", "\n\nclass GCN_SUB(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, dropout_adj=0.5, sparse=0):\n        super(GCN_SUB, self).__init__()\n        self.layers = nn.ModuleList()\n        self.sparse = sparse\n        self.dropout_adj_p = dropout_adj\n        self.dropout = dropout\n\n        if sparse:\n            self.layers.append(GCNConv_dgl(nfeat, nhid))\n            for _ in range(n_layers - 2):\n                self.layers.append(GCNConv_dgl(nhid, nhid))\n            self.layers.append(GCNConv_dgl(nhid, nclass))\n        else:\n            self.model = GCN(nfeat=nfeat, nhid=nhid, nclass=nclass, n_layers=n_layers, dropout=dropout,\n                             input_layer=False, output_layer=False, spmm_type=0)\n\n    def forward(self, x, Adj):\n\n        if self.sparse:\n            Adj = copy.deepcopy(Adj)\n            Adj.edata['w'] = F.dropout(Adj.edata['w'], p=self.dropout_adj_p, training=self.training)\n        else:\n            Adj = F.dropout(Adj, p=self.dropout_adj_p, training=self.training)\n\n        if self.sparse:\n            for i, conv in enumerate(self.layers[:-1]):\n                x = conv(x, Adj)\n                x = F.relu(x)\n                x = F.dropout(x, p=self.dropout, training=self.training)\n            x = self.layers[-1](x, Adj)\n            return x.squeeze(1)\n        else:\n            return self.model((x, Adj, True))", "\n\n\n"]}
{"filename": "opengsl/method/models/grcn.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\n# from .GCN3 import GraphConvolution, GCN\nfrom .gcn import GCN\nfrom .gnn_modules import APPNP\n\n\nclass GCNConv_diag(torch.nn.Module):\n    '''\n    A GCN convolution layer of diagonal matrix multiplication\n    '''\n    def __init__(self, input_size):\n        super(GCNConv_diag, self).__init__()\n        self.W = torch.nn.Parameter(torch.ones(input_size))\n        # inds = torch.stack([torch.arange(input_size), torch.arange(input_size)]).to(device)\n        # self.mW = torch.sparse.FloatTensor(inds, self.W, torch.Size([input_size,input_size]))\n        self.input_size = input_size\n\n    def forward(self, input, A):\n        hidden = input @ torch.diag(self.W)\n        # hidden = torch.sparse.mm(self.mW, input.t()).t()\n        output = torch.sparse.mm(A, hidden)\n        return output", "\n\nclass GRCN(torch.nn.Module):\n\n    def __init__(self, num_nodes, num_features, num_classes, device, conf):\n        super(GRCN, self).__init__()\n        self.num_nodes = num_nodes\n        self.num_features = num_features\n        if conf.model['type'] == 'gcn':\n            self.conv_task = GCN(num_features, conf.model['n_hidden'], num_classes, conf.model['n_layers'],\n                                 conf.model['dropout'], conf.model['input_dropout'], conf.model['norm'],\n                                 conf.model['n_linear'], conf.model['spmm_type'], conf.model['act'],\n                                 conf.model['input_layer'], conf.model['output_layer'])\n        else:\n            self.conv_task = APPNP(num_features, conf.model['n_hidden'], num_classes,\n                               dropout=conf.model['dropout'], K=conf.model['K_APPNP'],\n                               alpha=conf.model['alpha'], spmm_type=1)\n        self.model_type = conf.gsl['model_type']\n        if conf.gsl['model_type'] == 'diag':\n            self.conv_graph = GCNConv_diag(num_features)\n            self.conv_graph2 = GCNConv_diag(num_features)\n        else:\n            self.conv_graph = GCN(num_features, conf.gsl['n_hidden_1'], conf.gsl['n_hidden_2'], conf.gsl['n_layers'],\n                             conf.gsl['dropout'], conf.gsl['input_dropout'], conf.gsl['norm'],\n                             conf.gsl['n_linear'], conf.gsl['spmm_type'], conf.gsl['act'],\n                             conf.gsl['input_layer'], conf.gsl['output_layer'])\n\n        self.K = conf.gsl['K']\n        self.mask = None\n        self.Adj_new = None\n        self._normalize = conf.gsl['normalize']   # \u7528\u6765\u51b3\u5b9a\u662f\u5426\u5bf9node embedding\u8fdb\u884cnormalize\n        self.device = device\n\n    def graph_parameters(self):\n        if self.model_type == 'diag':\n            return list(self.conv_graph.parameters()) + list(self.conv_graph2.parameters())\n        else:\n            return list(self.conv_graph.parameters())\n\n\n    def base_parameters(self):\n        return list(self.conv_task.parameters())\n\n    def cal_similarity_graph(self, node_embeddings):\n        # \u4e00\u4e2a2head\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\n        # similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n        similarity_graph = torch.mm(node_embeddings[:, :int(self.num_features/2)], node_embeddings[:, :int(self.num_features/2)].t())\n        similarity_graph += torch.mm(node_embeddings[:, int(self.num_features/2):], node_embeddings[:, int(self.num_features/2):].t())\n        return similarity_graph\n\n    def normalize(self, adj):\n        adj = adj.coalesce()\n        inv_sqrt_degree = 1. / (torch.sqrt(torch.sparse.sum(adj, dim=1).values()) + 1e-10)\n        D_value = inv_sqrt_degree[adj.indices()[0]] * inv_sqrt_degree[adj.indices()[1]]\n        new_values = adj.values() * D_value\n        return torch.sparse.FloatTensor(adj.indices(), new_values, adj.size()).to(self.device)\n\n    def _sparse_graph(self, raw_graph, K):\n        values, indices = raw_graph.topk(k=int(K), dim=-1)\n        assert torch.sum(torch.isnan(values)) == 0\n        assert torch.max(indices) < raw_graph.shape[1]\n\n        inds = torch.stack([torch.arange(raw_graph.shape[0]).view(-1,1).expand(-1,int(K)).contiguous().view(1,-1)[0].to(self.device),\n                             indices.view(1,-1)[0]])\n        inds = torch.cat([inds, torch.stack([inds[1], inds[0]])], dim=1)\n        values = torch.cat([values.view(1,-1)[0], values.view(1,-1)[0]])\n        return inds, values\n\n    def _node_embeddings(self, input, Adj):\n        norm_Adj = self.normalize(Adj)\n        if self.model_type == 'diag':\n            node_embeddings = torch.tanh(self.conv_graph(input, norm_Adj))\n            node_embeddings = self.conv_graph2(node_embeddings, norm_Adj)\n        else:\n            node_embeddings = self.conv_graph((input, norm_Adj, True))\n        if self._normalize:\n            node_embeddings = F.normalize(node_embeddings, dim=1, p=2)\n        return node_embeddings\n\n    def forward(self, input, Adj):\n        Adj.requires_grad = False\n        node_embeddings = self._node_embeddings(input, Adj)\n        Adj_new = self.cal_similarity_graph(node_embeddings)\n\n        Adj_new_indices, Adj_new_values = self._sparse_graph(Adj_new, self.K)\n        new_inds = torch.cat([Adj.indices(), Adj_new_indices], dim=1)\n        new_values = torch.cat([Adj.values(), Adj_new_values])\n        Adj_new = torch.sparse.FloatTensor(new_inds, new_values, Adj.size()).to(self.device)\n        Adj_new_norm = self.normalize(Adj_new)\n\n        # x = self.conv1(input, Adj_new_norm)\n        # x = F.dropout(F.relu(x), training=self.training, p=self.dropout)\n        # x = self.conv2(x, Adj_new_norm)\n        _, x = self.conv_task((input, Adj_new_norm, False))\n\n        return x, Adj_new", ""]}
{"filename": "opengsl/method/models/gt.py", "chunked_list": ["'''\nThis is the GT model from UniMP [https://arxiv.org/pdf/2009.03509.pdf]\n'''\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom dgl import ops\nfrom dgl.nn.functional import edge_softmax\nimport torch.nn.functional as F\nimport scipy.sparse as sp", "import torch.nn.functional as F\nimport scipy.sparse as sp\nfrom ...utils.utils import get_homophily\nfrom opengsl.utils.utils import scipy_sparse_to_sparse_tensor\n\n\nclass TransformerAttentionModule(nn.Module):\n    def __init__(self, dim, dim_out, num_heads, dropout):\n        super().__init__()\n\n        assert dim % num_heads == 0, 'Dimension mismatch: hidden_dim should be a multiple of num_heads.'\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.attn_query = nn.Linear(in_features=dim, out_features=dim)\n        self.attn_key = nn.Linear(in_features=dim, out_features=dim)\n        self.attn_value = nn.Linear(in_features=dim, out_features=dim)\n\n        self.output_linear = nn.Linear(in_features=dim, out_features=dim_out)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, graph, labels=None, graph_analysis=False):\n        queries = self.attn_query(x)\n        keys = self.attn_key(x)\n        values = self.attn_value(x)\n\n        queries = queries.reshape(-1, self.num_heads, self.head_dim)\n        keys = keys.reshape(-1, self.num_heads, self.head_dim)\n        values = values.reshape(-1, self.num_heads, self.head_dim)\n\n        attn_scores = ops.u_dot_v(graph, queries, keys) / self.head_dim ** 0.5\n        attn_probs = edge_softmax(graph, attn_scores)\n\n        x = ops.u_mul_e_sum(graph, values, attn_probs)\n        x = x.reshape(-1, self.dim)\n\n        x = self.output_linear(x)\n        x = self.dropout(x)\n\n        if graph_analysis:\n            assert labels is not None, 'error'\n            homophily = self.compute_homo(graph, attn_probs, labels)\n            return x, homophily\n        return x, 0\n\n    def compute_homo(self, graph, attn_weights, labels):\n        '''\n        Args:\n            graph: dgl graph\n            attn_weights: [n_edges, n_heads, 1], attention weights learned by a transformer layer\n\n        Returns:\n            homophily: [n_heads] the homophily of attention weights of each head\n\n        '''\n\n        n_heads = self.num_heads\n        n_nodes = graph.num_nodes()\n        homophily = np.zeros(n_heads)\n        edges = graph.edges()\n        row = edges[0].cpu().numpy()\n        col = edges[1].cpu().numpy()\n        # values = adj.coalesce().values().numpy()\n        # return sp.coo_matrix((values, (row, col)), shape=adj.shape)\n        for i in range(n_heads):\n            values = attn_weights.squeeze()[:, i].cpu().detach().numpy()\n            adj = sp.coo_matrix((values, (row, col)), shape=(n_nodes, n_nodes))\n            adj = scipy_sparse_to_sparse_tensor(adj)\n            homophily[i] = get_homophily(labels, adj)\n        return homophily", "\n\nclass FeedForwardModule(nn.Module):\n    def __init__(self, dim, hidden_dim_multiplier, dropout, act):\n        super().__init__()\n        input_dim = int(dim)\n        hidden_dim = int(dim * hidden_dim_multiplier)\n        self.linear_1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n        self.dropout_1 = nn.Dropout(p=dropout)\n        self.act = eval('F.' + act) if not act == 'identity' else lambda x: x\n        self.linear_2 = nn.Linear(in_features=hidden_dim, out_features=dim)\n        self.dropout_2 = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.dropout_1(x)\n        x = self.act(x)\n        x = self.linear_2(x)\n        x = self.dropout_2(x)\n\n        return x", "\n\nclass GT(nn.Module):\n\n    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm_type='LayerNorm',\n                 num_heads=8, act='relu', input_layer=False, output_layer=False, ff=False, hidden_dim_multiplier=2,\n                 use_norm=False, use_redisual=False):\n\n        super(GT, self).__init__()\n        self.nfeat = nfeat\n        self.nclass = nclass\n        self.n_layers = n_layers\n        self.input_layer = input_layer\n        self.output_layer = output_layer\n        self.use_norm = use_norm\n        self.use_residual = use_redisual\n        self.ff = ff\n        self.norm_type = eval('nn.' + norm_type)\n        self.act = eval('F.' + act) if not act == 'identity' else lambda x: x\n        if input_layer:\n            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n            self.input_drop = nn.Dropout(input_dropout)\n        if output_layer:\n            self.output_linear = nn.Linear(in_features=nhid, out_features=nclass)\n            self.output_normalization = self.norm_type(nhid)\n        self.trans = nn.ModuleList()\n        if self.use_norm:\n            self.norms_1 = nn.ModuleList()\n        if self.ff:\n            self.ffns = nn.ModuleList()\n            if self.use_norm:\n                self.norms_2 = nn.ModuleList()\n        for i in range(n_layers):\n            if i == 0 and not self.input_layer:\n                in_hidden = nfeat\n            else:\n                in_hidden = nhid\n            if i == n_layers - 1 and not self.output_layer:\n                out_hidden = nclass\n            else:\n                out_hidden = nhid\n            self.trans.append(TransformerAttentionModule(in_hidden, out_hidden, num_heads, dropout))\n            if self.use_norm:\n                self.norms_1.append(self.norm_type(in_hidden))\n            if self.ff:\n                self.ffns.append(FeedForwardModule(in_hidden, hidden_dim_multiplier, dropout, act))\n                if self.use_norm:\n                    self.norms_2.append(self.norm_type(in_hidden))\n\n    def forward(self, x, graph, labels=None, graph_analysis=False):\n        if self.input_layer:\n            x = self.input_linear(x)\n            x = self.input_drop(x)\n            x = self.act(x)\n\n        homo_heads = []\n        for i, layer in enumerate(self.trans):\n\n            x_res = self.norms_1[i](x) if self.use_norm else x\n            x_res, homophily = layer(x_res, graph, labels, graph_analysis)\n            x = x + x_res if self.use_residual else x_res\n\n            if self.ff:\n                x_res = self.norms_2[i](x) if self.use_norm else x\n                x_res = self.ffns[i](x_res)\n                x = x + x_res if self.use_residual else x_res\n            if i == self.n_layers - 1:\n                mid = x\n            if graph_analysis:\n                homo_heads.append(homophily)\n\n        if self.output_layer:\n            if self.use_norm:\n                x = self.output_normalization(x)\n            x = self.output_linear(x)\n        return mid, x.squeeze(1), homo_heads", "\n\nclass GraphTransformerAttn(nn.Module):\n    def __init__(self, dim, dim_out, num_heads, concat=True):\n        super().__init__()\n\n        self.dim = dim\n        self.dim_out = dim_out\n        self.dim_inner = dim_out * num_heads\n        self.num_heads = num_heads\n        self.concat = concat\n\n        self.attn_query = nn.Linear(in_features=dim, out_features=self.dim_inner)\n        self.attn_key = nn.Linear(in_features=dim, out_features=self.dim_inner)\n        self.attn_value = nn.Linear(in_features=dim, out_features=self.dim_inner)\n\n    def forward(self, x, graph, labels=None, graph_analysis=False):\n        queries = self.attn_query(x)\n        keys = self.attn_key(x)\n        values = self.attn_value(x)\n\n        queries = queries.reshape(-1, self.num_heads, self.dim_out)\n        keys = keys.reshape(-1, self.num_heads, self.dim_out)\n        values = values.reshape(-1, self.num_heads, self.dim_out)\n\n        attn_scores = ops.u_dot_v(graph, queries, keys) / self.dim_out ** 0.5\n        attn_probs = edge_softmax(graph, attn_scores)\n\n        x = ops.u_mul_e_sum(graph, values, attn_probs)\n        if self.concat:\n            x = x.reshape(-1, self.dim_inner)\n        else:\n            x = torch.mean(x, dim=1)\n\n        if graph_analysis:\n            assert labels is not None, 'error'\n            homophily = self.compute_homo(graph, attn_probs, labels)\n            return x, homophily\n        return x\n\n    def compute_homo(self, graph, attn_weights, labels):\n        '''\n        Args:\n            graph: dgl graph\n            attn_weights: [n_edges, n_heads, 1], attention weights learned by a transformer layer\n\n        Returns:\n            homophily: [n_heads] the homophily of attention weights of each head\n\n        '''\n\n        n_heads = self.num_heads\n        n_nodes = graph.num_nodes()\n        homophily = np.zeros(n_heads)\n        edges = graph.edges()\n        row = edges[0].cpu().numpy()\n        col = edges[1].cpu().numpy()\n        # values = adj.coalesce().values().numpy()\n        # return sp.coo_matrix((values, (row, col)), shape=adj.shape)\n        for i in range(n_heads):\n            values = attn_weights.squeeze()[:, i].cpu().detach().numpy()\n            adj = sp.coo_matrix((values, (row, col)), shape=(n_nodes, n_nodes))\n            adj = scipy_sparse_to_sparse_tensor(adj)\n            homophily[i] = get_homophily(labels, adj)\n        return homophily", "\n\nclass GatedResidual(nn.Module):\n    \"\"\" This is the implementation of Eq (5), i.e., gated residual connection between block.\n    \"\"\"\n    def __init__(self, dim_in, dim_out, only_gate=False):\n        super().__init__()\n        self.lin_res = nn.Linear(dim_in, dim_out)\n        self.proj = nn.Sequential(\n            nn.Linear(dim_out * 3, 1, bias = False),\n            nn.Sigmoid()\n        )\n        self.norm = nn.LayerNorm(dim_out)\n        self.non_lin = nn.ReLU()\n        self.only_gate = only_gate\n\n    def forward(self, x, res):\n        res = self.lin_res(res)\n        gate_input = torch.cat((x, res, x - res), dim = -1)\n        gate = self.proj(gate_input) # Eq (5), this is beta in the paper\n        if self.only_gate: # This is for Eq (6), a case when normalizaton and non linearity is not used.\n            return x * gate + res * (1 - gate)\n        return self.non_lin(self.norm(x * gate + res * (1 - gate)))", "\n\nclass GraphTransformerModel(nn.Module):\n    \"\"\" This is the overall architecture of the model.\n    \"\"\"\n\n    def __init__(\n            self,\n            n_feats,\n            n_class,\n            n_hidden,\n            n_layers,\n            n_heads=8,\n    ):\n        super().__init__()\n        self.n_feats = n_feats\n        self.n_class = n_class\n        self.n_hidden = n_hidden\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n\n        self.layers = nn.ModuleList()\n\n        self.input_layer = nn.Linear(n_feats, n_hidden)\n\n        assert n_hidden % n_heads == 0\n\n        for i in range(n_layers):\n            if i < n_layers - 1:\n                self.layers.append(nn.ModuleList([\n                    GraphTransformerAttn(n_hidden, dim_out=int(n_hidden / n_heads), num_heads=n_heads),\n                    GatedResidual(n_hidden, n_hidden)\n                ]))\n            else:\n                self.layers.append(nn.ModuleList([\n                    GraphTransformerAttn(n_hidden, dim_out=n_class, num_heads=n_heads, concat=False),\n                    GatedResidual(n_hidden, n_class, only_gate=True)\n                ]))\n\n    def forward(self, input):\n        x=input[0]\n        graph=input[1]\n\n        x = self.input_layer(x)\n\n        for trans_block in self.layers:\n            trans, trans_residual = trans_block\n            x = trans_residual(trans(x, graph), x)\n\n        return x", "\n\nif __name__ == '__main__':\n    model = GT(1000, 128, 7, 5, 0.5, 0)\n    print(model)"]}
{"filename": "opengsl/method/models/gcn.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn.dense.linear import Linear\n\n\nclass GraphConvolution(nn.Module):\n\n    def __init__(self, in_features, out_features, dropout=0.5, n_linear=1, bias=True, spmm_type=1, act='relu',\n                 last_layer=False, weight_initializer=None, bias_initializer=None):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.mlp = nn.ModuleList()\n        self.mlp.append(Linear(in_features, out_features, bias=bias, weight_initializer=weight_initializer, bias_initializer=bias_initializer))\n        for i in range(n_linear-1):\n            self.mlp.append(Linear(out_features, out_features, bias=bias, weight_initializer=weight_initializer, bias_initializer=bias_initializer))\n        self.dropout = dropout\n        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n        self.last_layer = last_layer\n\n\n    def forward(self, input, adj):\n        \"\"\" Graph Convolutional Layer forward function\n        \"\"\"\n        x = self.spmm(adj, input)\n        for i in range(len(self.mlp)-1):\n            x = self.mlp[i](x)\n            x = self.act(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.mlp[-1](x)\n        if not self.last_layer:\n            x = self.act(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        return x\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "\n\nclass GCN(nn.Module):\n\n    def __init__(self, nfeat, nhid, nclass, n_layers=5, dropout=0.5, input_dropout=0.0, norm=None, n_linear=1,\n                 spmm_type=0, act='relu', input_layer=False, output_layer=False, weight_initializer=None,\n                 bias_initializer=None, bias=True):\n\n        super(GCN, self).__init__()\n\n        self.nfeat = nfeat\n        self.nclass = nclass\n        self.n_layers = n_layers\n        self.input_layer = input_layer\n        self.output_layer = output_layer\n        self.n_linear = n_linear\n        if norm is None:\n            norm = {'flag':False, 'norm_type':'LayerNorm'}\n        self.norm_flag = norm['flag']\n        self.norm_type = eval('nn.'+norm['norm_type'])\n        self.act = eval('F.'+act) if not act == 'identity' else lambda x: x\n        if input_layer:\n            self.input_linear = nn.Linear(in_features=nfeat, out_features=nhid)\n            self.input_drop = nn.Dropout(input_dropout)\n        if output_layer:\n            self.output_linear = nn.Linear(in_features=nhid, out_features=nclass)\n            self.output_normalization = self.norm_type(nhid)\n        self.convs = nn.ModuleList()\n        if self.norm_flag:\n            self.norms = nn.ModuleList()\n        else:\n            self.norms = None\n\n        for i in range(n_layers):\n            if i == 0 and not self.input_layer:\n                in_hidden = nfeat\n            else:\n                in_hidden = nhid\n            if i == n_layers - 1 and not self.output_layer:\n                out_hidden = nclass\n            else:\n                out_hidden = nhid\n\n            self.convs.append(GraphConvolution(in_hidden, out_hidden, dropout, n_linear, spmm_type=spmm_type, act=act,\n                                               weight_initializer=weight_initializer, bias_initializer=bias_initializer,\n                                               bias=bias))\n            if self.norm_flag:\n                self.norms.append(self.norm_type(in_hidden))\n        self.convs[-1].last_layer = True\n\n    def forward(self, input):\n        x=input[0]\n        adj=input[1]\n        only_z=input[2]\n        if self.input_layer:\n            x = self.input_linear(x)\n            x = self.input_drop(x)\n            x = self.act(x)\n\n        for i, layer in enumerate(self.convs):\n            if self.norm_flag:\n                x_res = self.norms[i](x)\n                x_res = layer(x_res, adj)\n                x = x + x_res\n            else:\n                x = layer(x,adj)\n            if i == self.n_layers - 1:\n                mid = x\n\n        if self.output_layer:\n            x = self.output_normalization(x)\n            x = self.output_linear(x).squeeze(1)\n        if only_z:\n            return x.squeeze(1)\n        else:\n            return mid, x.squeeze(1)"]}
{"filename": "opengsl/method/models/__init__.py", "chunked_list": [""]}
{"filename": "opengsl/method/models/gcn_idgl.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n'''\nThis GCN is only used for IGDL for its changeable dropout.\n'''\n", "'''\n\n\nclass GraphConvolution(nn.Module):\n\n    def __init__(self, in_features, out_features, with_bias=True, batch_norm=False):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        if with_bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.bn = nn.BatchNorm1d(out_features) if batch_norm else None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def init_params(self):\n        # initialize weights with xavier uniform and biases with all zeros.\n        # This is more recommended than the upper one.\n        for param in self.parameters():\n            if len(param.size()) == 2:\n                nn.init.xavier_uniform_(param)\n            else:\n                nn.init.constant_(param, 0.0)\n\n    def forward(self, input, adj, batch_norm=True):\n        \"\"\" Graph Convolutional Layer forward function\n        \"\"\"\n        if input.data.is_sparse:\n            support = torch.spmm(input, self.weight)\n        else:\n            support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            output = output + self.bias\n        if self.bn is not None and batch_norm:\n            output = self.compute_bn(output)\n        return output\n\n    def compute_bn(self, x):\n        if len(x.shape) == 2:\n            return self.bn(x)\n        else:\n            return self.bn(x.view(-1, x.size(-1))).view(x.size())\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "\n\nclass GCN(nn.Module):\n\n    def __init__(self, nfeat, nhid, nclass, n_layers=2, dropout=0.5, with_bias=True, batch_norm=False):\n\n        super(GCN, self).__init__()\n\n        self.nfeat = nfeat\n        self.hidden_sizes = [nhid]\n        self.nclass = nclass\n        self.n_layers = n_layers\n        self.layers = nn.ModuleList()\n        self.layers.append(GraphConvolution(nfeat, nhid, with_bias=with_bias, batch_norm=batch_norm))\n        for i in range(n_layers-2):\n            self.layers.append(GraphConvolution(nhid, nhid, with_bias=with_bias, batch_norm=batch_norm))\n        self.layers.append(GraphConvolution(nhid, nclass, with_bias=with_bias, batch_norm=False))\n        self.dropout = dropout\n        self.with_bias = with_bias\n        self.batch_norm = batch_norm\n\n    def forward(self, x, adj, dropout=None):\n        for i, layer in enumerate(self.layers[:-1]):\n            x = F.relu(layer(x, adj))\n            x = F.dropout(x, dropout if dropout else self.dropout, training=self.training)\n        output = self.layers[-1](x, adj)\n        return x, output\n\n    def initialize(self):\n        \"\"\"Initialize parameters of GCN.\n        \"\"\"\n        for i in range(self.n_layers):\n            self.layers[i].reset_parameters()"]}
{"filename": "opengsl/method/models/gaug.py", "chunked_list": ["from .gcn import GCN\nfrom .gnn_modules import APPNP\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pyro as pyro\nfrom opengsl.data.preprocess.normalize import normalize\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n", "from sklearn.metrics import roc_auc_score, average_precision_score\n\n\nclass VGAE(nn.Module):\n    \"\"\" GAE/VGAE as edge prediction model \"\"\"\n    def __init__(self, dim_feats, conf):\n        super(VGAE, self).__init__()\n        self.gae = conf.gsl['gae']\n        # self.gcn_base = GraphConvolution(dim_feats, dim_h, with_bias=False)\n        # self.gcn_mean = GraphConvolution(dim_h, dim_z, with_bias=False)\n        # self.gcn_logstd = GraphConvolution(dim_h, dim_z, with_bias=False)\n        self.conv_graph = GCN(dim_feats, conf.gsl['n_hidden'], conf.gsl['n_embed'], conf.gsl['n_layers'],\n                              conf.gsl['dropout'], conf.gsl['input_dropout'], conf.gsl['norm'],\n                              conf.gsl['n_linear'], conf.gsl['spmm_type'], conf.gsl['act'],\n                              conf.gsl['input_layer'], conf.gsl['output_layer'], bias=False,\n                              weight_initializer='glorot')\n\n    def forward(self, feats, adj):\n        # GCN encoder\n        # hidden = self.gcn_base(feats, adj)\n        # self.mean = F.relu(self.gcn_mean(hidden, adj))\n        _, mean = self.conv_graph((feats, adj, False))\n        mean = F.relu(mean)\n        if self.gae:\n            # GAE (no sampling at bottleneck)\n            Z = mean\n        else:\n            # VGAE\n            # self.logstd = F.relu(self.gcn_logstd(hidden, adj))\n            # gaussian_noise = torch.randn_like(self.mean)\n            # sampled_Z = gaussian_noise*torch.exp(self.logstd) + self.mean\n            # Z = sampled_Z\n            pass\n        # inner product decoder\n        adj_logits = Z @ Z.T\n        return adj_logits", "\n\nclass GAug(nn.Module):\n    def __init__(self,\n                 dim_feats,\n                 n_classes,\n                 conf):\n        super(GAug, self).__init__()\n        self.temperature = conf.gsl['temperature']\n        self.alpha = conf.gsl['alpha']\n        # edge prediction network\n        self.ep_net = VGAE(dim_feats, conf)\n        # node classification network\n        # self.nc_net = GCN(dim_feats, dim_h, n_classes, dropout=dropout)\n        if conf.model['type']=='gcn':\n            self.nc_net = GCN(dim_feats, conf.model['n_hidden'], n_classes, conf.model['n_layers'], conf.model['dropout'],\n                              conf.model['input_dropout'], conf.model['norm'], conf.model['n_linear'],\n                              conf.model['spmm_type'], conf.model['act'], conf.model['input_layer'],\n                              conf.model['output_layer'], weight_initializer='glorot', bias_initializer='zeros')\n        elif conf.model['type']=='appnp':\n            self.nc_net = APPNP(dim_feats, conf.model['n_hidden'], n_classes,\n                               dropout=conf.model['dropout'], K=conf.model['K'],\n                               alpha=conf.model['alpha'])\n    def sample_adj(self, adj_logits):\n        \"\"\" sample an adj from the predicted edge probabilities of ep_net \"\"\"\n        edge_probs = adj_logits / torch.max(adj_logits)\n        # sampling\n\n        # print(adj_logits)\n        # print(edge_probs)\n        adj_sampled = pyro.distributions.RelaxedBernoulliStraightThrough(temperature=self.temperature, probs=edge_probs).rsample()\n        # making adj_sampled symmetric\n        adj_sampled = adj_sampled.triu(1)\n        adj_sampled = adj_sampled + adj_sampled.T\n        return adj_sampled\n\n    def sample_adj_add_bernoulli(self, adj_logits, adj_orig, alpha):\n        edge_probs = adj_logits / torch.max(adj_logits)\n        edge_probs = alpha*edge_probs + (1-alpha)*adj_orig\n        # sampling\n        adj_sampled = pyro.distributions.RelaxedBernoulliStraightThrough(temperature=self.temperature, probs=edge_probs).rsample()\n        # making adj_sampled symmetric\n        adj_sampled = adj_sampled.triu(1)\n        adj_sampled = adj_sampled + adj_sampled.T\n        return adj_sampled\n\n    def forward(self, feats, adj, adj_orig):\n        # print(feats)\n        # print(adj)\n        adj_logits = self.ep_net(feats, adj)\n        if self.alpha == 1:\n            adj_new = self.sample_adj(adj_logits)\n        else:\n            adj_new = self.sample_adj_add_bernoulli(adj_logits, adj_orig, self.alpha)\n        adj_new_normed = normalize(adj_new)\n        hidden, output = self.nc_net((feats, adj_new_normed, False))\n        return output, adj_logits, adj_new", "\n\ndef eval_edge_pred(adj_pred, val_edges, edge_labels):\n    logits = adj_pred[val_edges.T]\n    logits = np.nan_to_num(logits)\n    roc_auc = roc_auc_score(edge_labels, logits)\n    ap_score = average_precision_score(edge_labels, logits)\n    return roc_auc, ap_score\n\n\nclass MultipleOptimizer():\n    \"\"\" a class that wraps multiple optimizers \"\"\"\n    def __init__(self, *op):\n        self.optimizers = op\n\n    def zero_grad(self):\n        for op in self.optimizers:\n            op.zero_grad()\n\n    def step(self):\n        for op in self.optimizers:\n            op.step()\n\n    def update_lr(self, op_index, new_lr):\n        \"\"\" update the learning rate of one optimizer\n        Parameters: op_index: the index of the optimizer to update\n                    new_lr:   new learning rate for that optimizer \"\"\"\n        for param_group in self.optimizers[op_index].param_groups:\n            param_group['lr'] = new_lr", "\n\nclass MultipleOptimizer():\n    \"\"\" a class that wraps multiple optimizers \"\"\"\n    def __init__(self, *op):\n        self.optimizers = op\n\n    def zero_grad(self):\n        for op in self.optimizers:\n            op.zero_grad()\n\n    def step(self):\n        for op in self.optimizers:\n            op.step()\n\n    def update_lr(self, op_index, new_lr):\n        \"\"\" update the learning rate of one optimizer\n        Parameters: op_index: the index of the optimizer to update\n                    new_lr:   new learning rate for that optimizer \"\"\"\n        for param_group in self.optimizers[op_index].param_groups:\n            param_group['lr'] = new_lr", "\n\ndef get_lr_schedule_by_sigmoid(n_epochs, lr, warmup):\n    \"\"\" schedule the learning rate with the sigmoid function.\n    The learning rate will start with near zero and end with near lr \"\"\"\n    factors = torch.FloatTensor(np.arange(n_epochs))\n    factors = ((factors / factors[-1]) * (warmup * 2)) - warmup\n    factors = torch.sigmoid(factors)\n    # range the factors to [0, 1]\n    factors = (factors - factors[0]) / (factors[-1] - factors[0])\n    lr_schedule = factors * lr\n    return lr_schedule"]}
{"filename": "opengsl/method/models/nodeformer.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.utils import degree\n\nBIG_CONSTANT = 1e8\n\n\ndef adj_mul(adj_i, adj, N):\n    adj_i_sp = torch.sparse_coo_tensor(adj_i, torch.ones(adj_i.shape[1], dtype=torch.float).to(adj.device), (N, N))\n    adj_sp = torch.sparse_coo_tensor(adj, torch.ones(adj.shape[1], dtype=torch.float).to(adj.device), (N, N))\n    adj_j = torch.sparse.mm(adj_i_sp, adj_sp)\n    adj_j = adj_j.coalesce().indices()\n    return adj_j", "\n\ndef adj_mul(adj_i, adj, N):\n    adj_i_sp = torch.sparse_coo_tensor(adj_i, torch.ones(adj_i.shape[1], dtype=torch.float).to(adj.device), (N, N))\n    adj_sp = torch.sparse_coo_tensor(adj, torch.ones(adj.shape[1], dtype=torch.float).to(adj.device), (N, N))\n    adj_j = torch.sparse.mm(adj_i_sp, adj_sp)\n    adj_j = adj_j.coalesce().indices()\n    return adj_j\n\n\ndef create_projection_matrix(m, d, seed=0):\n    block_list = []\n    current_seed = seed\n    torch.manual_seed(current_seed)\n    unstructured_block = torch.randn((d, d))\n    q, _ = torch.qr(unstructured_block)\n    q = torch.t(q)\n    block_list.append(q[0:m])\n    final_matrix = torch.vstack(block_list)\n\n    current_seed += 1\n    torch.manual_seed(current_seed)\n    multiplier = torch.norm(torch.randn((m, d)), dim=1)\n\n    return torch.matmul(torch.diag(multiplier), final_matrix)", "\n\ndef create_projection_matrix(m, d, seed=0):\n    block_list = []\n    current_seed = seed\n    torch.manual_seed(current_seed)\n    unstructured_block = torch.randn((d, d))\n    q, _ = torch.qr(unstructured_block)\n    q = torch.t(q)\n    block_list.append(q[0:m])\n    final_matrix = torch.vstack(block_list)\n\n    current_seed += 1\n    torch.manual_seed(current_seed)\n    multiplier = torch.norm(torch.randn((m, d)), dim=1)\n\n    return torch.matmul(torch.diag(multiplier), final_matrix)", "\n\ndef softmax_kernel_transformation(data, is_query, projection_matrix=None, numerical_stabilizer=0.000001):\n    data_normalizer = 1.0 / torch.sqrt(torch.sqrt(torch.tensor(data.shape[-1], dtype=torch.float32)))   # \u4e0d\u592a\u7406\u89e3\u8fd9\u4e00\u884c\n    data = data_normalizer * data\n    ratio = 1.0 / torch.sqrt(torch.tensor(projection_matrix.shape[0], dtype=torch.float32))\n    data_dash = torch.einsum(\"bnhd,md->bnhm\", data, projection_matrix)\n    diag_data = torch.square(data)\n    diag_data = torch.sum(diag_data, dim=len(data.shape)-1)\n    diag_data = diag_data / 2.0\n    diag_data = torch.unsqueeze(diag_data, dim=len(data.shape)-1)\n    last_dims_t = len(data_dash.shape) - 1\n    attention_dims_t = len(data_dash.shape) - 3\n    if is_query:\n        # \u4e0b\u9762\u7684\u51e0\u884c\u4e5f\u4e0d\u7406\u89e3\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data - torch.max(data_dash, dim=last_dims_t, keepdim=True)[0]) + numerical_stabilizer\n        )\n    else:\n        data_dash = ratio * (\n            torch.exp(data_dash - diag_data - torch.max(torch.max(data_dash, dim=last_dims_t, keepdim=True)[0],\n                    dim=attention_dims_t, keepdim=True)[0]) + numerical_stabilizer\n        )\n    return data_dash", "\n\ndef numerator(qs, ks, vs):\n    kvs = torch.einsum(\"nbhm,nbhd->bhmd\", ks, vs) # kvs refers to U_k in the paper\n    return torch.einsum(\"nbhm,bhmd->nbhd\", qs, kvs)\n\n\ndef denominator(qs, ks):\n    all_ones = torch.ones([ks.shape[0]]).to(qs.device)\n    ks_sum = torch.einsum(\"nbhm,n->bhm\", ks, all_ones) # ks_sum refers to O_k in the paper\n    return torch.einsum(\"nbhm,bhm->nbh\", qs, ks_sum)", "\n\ndef numerator_gumbel(qs, ks, vs):\n    kvs = torch.einsum(\"nbhkm,nbhd->bhkmd\", ks, vs) # kvs refers to U_k in the paper\n    return torch.einsum(\"nbhm,bhkmd->nbhkd\", qs, kvs)\n\n\ndef denominator_gumbel(qs, ks):\n    all_ones = torch.ones([ks.shape[0]]).to(qs.device)\n    ks_sum = torch.einsum(\"nbhkm,n->bhkm\", ks, all_ones) # ks_sum refers to O_k in the paper\n    return torch.einsum(\"nbhm,bhkm->nbhk\", qs, ks_sum)", "\n\ndef kernelized_softmax(query, key, value, projection_matrix=None, edge_index=None, tau=0.25):\n    '''\n    fast computation of all-pair attentive aggregation with linear complexity\n    input: query/key/value [B, N, H, D]\n    return: updated node emb, attention weight (for computing edge loss)\n    B = graph number (always equal to 1 in Node Classification), N = node number, H = head number,\n    M = random feature dimension, D = hidden size\n    '''\n    query = query / math.sqrt(tau)\n    key = key / math.sqrt(tau)\n    query_prime = softmax_kernel_transformation(query, True, projection_matrix) # [B, N, H, M]\uff0c \u53ea\u6709softmax_kernel_transformation\u4e00\u79cd\n    key_prime = softmax_kernel_transformation(key, False, projection_matrix) # [B, N, H, M]\n    query_prime = query_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n    key_prime = key_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n    value = value.permute(1, 0, 2, 3) # [N, B, H, D]\n\n    # compute updated node emb, this step requires O(N)\n    z_num = numerator(query_prime, key_prime, value)\n    z_den = denominator(query_prime, key_prime)\n\n    z_num = z_num.permute(1, 0, 2, 3)  # [B, N, H, D]\n    z_den = z_den.permute(1, 0, 2)\n    z_den = torch.unsqueeze(z_den, len(z_den.shape))\n    z_output = z_num / z_den # [B, N, H, D]\n\n    # \u4e00\u5b9a\u662fTrue\n    start, end = edge_index\n    query_end, key_start = query_prime[end], key_prime[start] # [E, B, H, M]\n    edge_attn_num = torch.einsum(\"ebhm,ebhm->ebh\", query_end, key_start) # [E, B, H]\n    edge_attn_num = edge_attn_num.permute(1, 0, 2) # [B, E, H]\n    attn_normalizer = denominator(query_prime, key_prime) # [N, B, H]\n    edge_attn_dem = attn_normalizer[end]  # [E, B, H]\n    edge_attn_dem = edge_attn_dem.permute(1, 0, 2) # [B, E, H]\n    A_weight = edge_attn_num / edge_attn_dem # [B, E, H]\n\n    return z_output, A_weight", "\n\ndef kernelized_gumbel_softmax(query, key, value, projection_matrix=None, edge_index=None,\n                                K=10, tau=0.25):\n    '''\n    fast computation of all-pair attentive aggregation with linear complexity\n    input: query/key/value [B, N, H, D]\n    return: updated node emb, attention weight (for computing edge loss)\n    B = graph number (always equal to 1 in Node Classification), N = node number, H = head number,\n    M = random feature dimension, D = hidden size, K = number of Gumbel sampling\n    '''\n    query = query / math.sqrt(tau)\n    key = key / math.sqrt(tau)\n    query_prime = softmax_kernel_transformation(query, True, projection_matrix) # [B, N, H, M]\n    key_prime = softmax_kernel_transformation(key, False, projection_matrix) # [B, N, H, M]\n    query_prime = query_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n    key_prime = key_prime.permute(1, 0, 2, 3) # [N, B, H, M]\n    value = value.permute(1, 0, 2, 3) # [N, B, H, D]\n\n    # compute updated node emb, this step requires O(N)\n    gumbels = (\n        -torch.empty(key_prime.shape[:-1]+(K, ), memory_format=torch.legacy_contiguous_format).exponential_().log()\n    ).to(query.device) / tau # [N, B, H, K]\n    key_t_gumbel = key_prime.unsqueeze(3) * gumbels.exp().unsqueeze(4) # [N, B, H, K, M]\n    z_num = numerator_gumbel(query_prime, key_t_gumbel, value) # [N, B, H, K, D]\n    z_den = denominator_gumbel(query_prime, key_t_gumbel) # [N, B, H, K]\n\n    z_num = z_num.permute(1, 0, 2, 3, 4) # [B, N, H, K, D]\n    z_den = z_den.permute(1, 0, 2, 3) # [B, N, H, K]\n    z_den = torch.unsqueeze(z_den, len(z_den.shape))\n    z_output = torch.mean(z_num / z_den, dim=3) # [B, N, H, D]\n\n    start, end = edge_index\n    query_end, key_start = query_prime[end], key_prime[start] # [E, B, H, M]\n    edge_attn_num = torch.einsum(\"ebhm,ebhm->ebh\", query_end, key_start) # [E, B, H]\n    edge_attn_num = edge_attn_num.permute(1, 0, 2) # [B, E, H]\n    attn_normalizer = denominator(query_prime, key_prime) # [N, B, H]\n    edge_attn_dem = attn_normalizer[end]  # [E, B, H]\n    edge_attn_dem = edge_attn_dem.permute(1, 0, 2) # [B, E, H]\n    A_weight = edge_attn_num / edge_attn_dem # [B, E, H]\n\n    return z_output, A_weight", "\n\ndef add_conv_relational_bias(x, edge_index, b, trans='sigmoid'):\n    '''\n    compute updated result by the relational bias of input adjacency\n    the implementation is similar to the Graph Convolution Network with a (shared) scalar weight for each edge\n    '''\n    row, col = edge_index\n    d_in = degree(col, x.shape[1]).float()\n    d_norm_in = (1. / d_in[col]).sqrt()\n    d_out = degree(row, x.shape[1]).float()\n    d_norm_out = (1. / d_out[row]).sqrt()\n    conv_output = []\n    for i in range(x.shape[2]):\n        if trans == 'sigmoid':\n            b_i = F.sigmoid(b[i])\n        elif trans == 'identity':\n            b_i = b[i]\n        else:\n            raise NotImplementedError\n        value = torch.ones_like(row) * b_i * d_norm_in * d_norm_out\n        adj_i = SparseTensor(row=col, col=row, value=value, sparse_sizes=(x.shape[1], x.shape[1]))\n        conv_output.append(matmul(adj_i, x[:, :, i]) )  # [B, N, D]\n    conv_output = torch.stack(conv_output, dim=2) # [B, N, H, D]\n    return conv_output", "\n\nclass NodeFormerConv(nn.Module):\n    '''\n    one layer of NodeFormer that attentive aggregates all nodes over a latent graph\n    return: node embeddings for next layer, edge loss at this layer\n    '''\n    def __init__(self, in_channels, out_channels, num_heads, nb_random_features=10, use_gumbel=True,\n                 nb_gumbel_sample=10, rb_order=0, rb_trans='sigmoid'):\n        super(NodeFormerConv, self).__init__()\n        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n        self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n        self.Wo = nn.Linear(out_channels * num_heads, out_channels)\n        if rb_order >= 1:\n            self.b = torch.nn.Parameter(torch.FloatTensor(rb_order, num_heads), requires_grad=True)\n\n        self.out_channels = out_channels\n        self.num_heads = num_heads\n        self.nb_random_features = nb_random_features\n        self.use_gumbel = use_gumbel\n        self.nb_gumbel_sample = nb_gumbel_sample\n        self.rb_order = rb_order\n        self.rb_trans = rb_trans\n\n    def reset_parameters(self):\n        self.Wk.reset_parameters()\n        self.Wq.reset_parameters()\n        self.Wv.reset_parameters()\n        self.Wo.reset_parameters()\n        if self.rb_order >= 1:\n            if self.rb_trans == 'sigmoid':\n                torch.nn.init.constant_(self.b, 0.1)\n            elif self.rb_trans == 'identity':\n                torch.nn.init.constant_(self.b, 1.0)\n\n    def forward(self, z, adjs, tau):\n        B, N = z.size(0), z.size(1)\n        query = self.Wq(z).reshape(-1, N, self.num_heads, self.out_channels)\n        key = self.Wk(z).reshape(-1, N, self.num_heads, self.out_channels)\n        value = self.Wv(z).reshape(-1, N, self.num_heads, self.out_channels)\n\n        dim = query.shape[-1]\n        seed = torch.ceil(torch.abs(torch.sum(query) * BIG_CONSTANT)).to(torch.int32)\n        projection_matrix = create_projection_matrix(\n            self.nb_random_features, dim, seed=seed).to(query.device)\n\n        # compute all-pair message passing update and attn weight on input edges, requires O(N) or O(N + E)\n        if self.use_gumbel and self.training:  # only using Gumbel noise for training\n            z_next, weight = kernelized_gumbel_softmax(query,key,value,projection_matrix,adjs[0], self.nb_gumbel_sample,\n                                                       tau)\n        else:\n            z_next, weight = kernelized_softmax(query, key, value, projection_matrix, adjs[0], tau)\n        # compute update by relational bias of input adjacency, requires O(E)\n        for i in range(self.rb_order):\n            z_next += add_conv_relational_bias(value, adjs[i], self.b[i], self.rb_trans)\n\n        # aggregate results of multiple heads\n        z_next = self.Wo(z_next.flatten(-2, -1))\n\n        row, col = adjs[0]\n        d_in = degree(col, query.shape[1]).float()\n        d_norm = 1. / d_in[col]\n        d_norm_ = d_norm.reshape(1, -1, 1).repeat(1, 1, weight.shape[-1])\n        link_loss = torch.mean(weight.log() * d_norm_)\n\n        return z_next, link_loss", "\n\nclass NodeFormer(nn.Module):\n    '''\n    NodeFormer model implementation\n    return: predicted node labels, a list of edge losses at every layer\n    '''\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=4, dropout=0.0,\n                 nb_random_features=30, use_bn=True, use_gumbel=True, use_residual=True, use_act=False, use_jk=False,\n                 nb_gumbel_sample=10, rb_order=0, rb_trans='sigmoid'):\n        super(NodeFormer, self).__init__()\n\n        self.convs = nn.ModuleList()\n        self.fcs = nn.ModuleList()\n        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n        self.bns = nn.ModuleList()\n        self.bns.append(nn.LayerNorm(hidden_channels))\n        for i in range(num_layers):\n            self.convs.append(\n                NodeFormerConv(hidden_channels, hidden_channels, num_heads=num_heads,\n                               nb_random_features=nb_random_features, use_gumbel=use_gumbel,\n                               nb_gumbel_sample=nb_gumbel_sample, rb_order=rb_order, rb_trans=rb_trans))\n            self.bns.append(nn.LayerNorm(hidden_channels))\n\n        if use_jk:\n            self.fcs.append(nn.Linear(hidden_channels * num_layers + hidden_channels, out_channels))\n        else:\n            self.fcs.append(nn.Linear(hidden_channels, out_channels))\n\n        self.dropout = dropout\n        self.activation = F.elu\n        self.use_bn = use_bn\n        self.use_residual = use_residual\n        self.use_act = use_act\n        self.use_jk = use_jk\n\n\n    def reset_parameters(self):\n        for conv in self.convs:\n            conv.reset_parameters()\n        for bn in self.bns:\n            bn.reset_parameters()\n        for fc in self.fcs:\n            fc.reset_parameters()\n\n    def forward(self, x, adjs, tau=1.0):\n        x = x.unsqueeze(0) # [B, N, H, D], B=1 denotes number of graph\n        layer_ = []\n        link_loss_ = []\n        z = self.fcs[0](x)\n        if self.use_bn:\n            z = self.bns[0](z)\n        z = self.activation(z)\n        z = F.dropout(z, p=self.dropout, training=self.training)\n        layer_.append(z)\n\n        for i, conv in enumerate(self.convs):\n            z, link_loss = conv(z, adjs, tau)\n            link_loss_.append(link_loss)\n            if self.use_residual:\n                z += layer_[i]\n            if self.use_bn:\n                z = self.bns[i+1](z)\n            if self.use_act:\n                z = self.activation(z)\n            z = F.dropout(z, p=self.dropout, training=self.training)\n            layer_.append(z)\n\n        if self.use_jk: # use jk connection for each layer\n            z = torch.cat(layer_, dim=-1)\n\n        x_out = self.fcs[-1](z).squeeze(0)\n\n        return x_out.squeeze(1), link_loss_"]}
{"filename": "opengsl/method/models/gen.py", "chunked_list": ["import numpy as np\nfrom collections import Counter\n\n\nclass EstimateAdj:\n    def __init__(self, n_classes, adj, train_mask, labels, homophily):\n        self.num_class = n_classes\n        self.num_node = adj.shape[0]\n        self.idx_train = train_mask\n        self.label = labels.cpu().numpy()\n        self.adj = adj.cpu().numpy()\n\n        self.output = None\n        self.iterations = 0\n        self.count = 0\n\n        self.homophily = homophily\n\n    def reset_obs(self):\n        self.count = 0\n        self.N = 0\n        self.E = np.zeros((self.num_node, self.num_node), dtype=np.int64)\n\n    def update_obs(self, graph):\n        self.E += graph\n        self.N += 1\n\n    def revise_pred(self):\n        # For the training node, GT is used, and for the unlabeled node, the predicted label is used\n        self.output[self.idx_train] = self.label[self.idx_train]\n\n    def E_step(self, Q):\n        \"\"\"Run the Expectation(E) step of the EM algorithm.\n        Parameters\n        ----------\n        Q:\n            The current estimation that each edge is actually present (numpy.array)\n\n        Returns\n        ----------\n        alpha:\n            The estimation of true-positive rate (float)\n        beta\uff1a\n            The estimation of false-positive rate (float)\n        O:\n            The estimation of network model parameters (numpy.array)\n        \"\"\"\n        # Temporary variables to hold the numerators and denominators of alpha and beta\n        an = Q * self.E\n        an = np.triu(an, 1).sum()\n        bn = (1 - Q) * self.E\n        bn = np.triu(bn, 1).sum()\n        ad = Q * self.N\n        ad = np.triu(ad, 1).sum()\n        bd = (1 - Q) * self.N\n        bd = np.triu(bd, 1).sum()\n\n        # Calculate alpha, beta\n        alpha = an * 1. / (ad)\n        beta = bn * 1. / (bd)\n\n        O = np.zeros((self.num_class, self.num_class))\n\n        n = []\n        counter = Counter(self.output)\n        for i in range(self.num_class):\n            n.append(counter[i])\n\n        a = self.output.repeat(self.num_node).reshape(self.num_node, -1)\n        for j in range(self.num_class):\n            c = (a == j)\n            for i in range(j + 1):\n                b = (a == i)\n                O[i, j] = np.triu((b & c.T) * Q, 1).sum()\n                if i == j:\n                    O[j, j] = 2. / (n[j] * (n[j] - 1)) * O[j, j]\n                else:\n                    O[i, j] = 1. / (n[i] * n[j]) * O[i, j]\n        return (alpha, beta, O)\n\n    def M_step(self, alpha, beta, O):\n        \"\"\"Run the Maximization(M) step of the EM algorithm.\n        \"\"\"\n        O += O.T - np.diag(O.diagonal())   #\u4f7f\u6709\u5bf9\u89d2\u5143\u7d20\u7684\u4e0a\u4e09\u89d2\u53d8\u4e3a\u5bf9\u79f0\u77e9\u9635\n\n        row = self.output.repeat(self.num_node)\n        col = np.tile(self.output, self.num_node)\n        tmp = O[row, col].reshape(self.num_node, -1)\n\n        p1 = tmp * np.power(alpha, self.E) * np.power(1 - alpha, self.N - self.E)\n        p2 = (1 - tmp) * np.power(beta, self.E) * np.power(1 - beta, self.N - self.E)\n        Q = p1 * 1. / (p1 + p2 * 1.)\n        return Q\n\n    def EM(self, output, tolerance=.000001):\n        \"\"\"Run the complete EM algorithm.\n        Parameters\n        ----------\n        tolerance:\n            Determine the tolerance in the variantions of alpha, beta and O, which is acceptable to stop iterating (float)\n        seed:\n            seed for np.random.seed (int)\n\n        Returns\n        ----------\n        iterations:\n            The number of iterations to achieve the tolerance on the parameters (int)\n        \"\"\"\n        # Record previous values to confirm convergence\n        alpha_p = 0\n        beta_p = 0\n\n        self.output = output\n        self.revise_pred()\n\n        # Do an initial E-step with random alpha, beta and O\n        # Beta must be smalller than alpha\n        beta, alpha = np.sort(np.random.rand(2))\n        O = np.triu(np.random.rand(self.num_class, self.num_class))   #\u6709\u5bf9\u89d2\u5143\u7d20\u7684\u4e0a\u4e09\u89d2\n\n        # Calculate initial Q\n        Q = self.M_step(alpha, beta, O)\n\n        while abs(alpha_p - alpha) > tolerance or abs(beta_p - beta) > tolerance:\n            alpha_p = alpha\n            beta_p = beta\n            alpha, beta, O = self.E_step(Q)\n            Q = self.M_step(alpha, beta, O)\n            self.iterations += 1\n            self.count += 1\n            #print(self.iterations,alpha,beta)\n\n        if self.homophily > 0.5:\n            # Make sure that the initial edge will be preserved\n            Q += self.adj\n        return (alpha, beta, O, Q, self.iterations)", "\n\ndef get_homophily(label, adj):\n    num_node = len(label)\n    label = label.repeat(num_node).reshape(num_node, -1)\n    n = np.triu((label == label.T) & (adj == 1)).sum(axis=0)\n    d = np.triu(adj).sum(axis=0)\n    homos = []\n    for i in range(num_node):\n        if d[i] > 0:\n            homos.append(n[i] * 1./d[i])\n    return np.mean(homos)", "\n\ndef prob_to_adj(mx, threshold):\n    mx = np.triu(mx, 1)   # the 2 steps here del the self loop\n    mx += mx.T\n    adj = np.zeros_like(mx)\n    adj[mx > threshold] = 1\n    return adj\n", ""]}
{"filename": "opengsl/method/models/slaps.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\n# from .GCN3 import GraphConvolution, GCN\nimport math\nimport dgl\nfrom .gcn import GCN, GraphConvolution\nfrom sklearn.neighbors import kneighbors_graph\nfrom .gnn_modules import APPNP\nimport numpy as np\n\ndef apply_non_linearity(tensor, non_linearity, i):\n    if non_linearity == 'elu':\n        return F.elu(tensor * i - i) + 1\n    elif non_linearity == 'relu':\n        return F.relu(tensor)\n    elif non_linearity == 'none':\n        return tensor\n    else:\n        raise NameError('We dont support the non-linearity yet')", "import numpy as np\n\ndef apply_non_linearity(tensor, non_linearity, i):\n    if non_linearity == 'elu':\n        return F.elu(tensor * i - i) + 1\n    elif non_linearity == 'relu':\n        return F.relu(tensor)\n    elif non_linearity == 'none':\n        return tensor\n    else:\n        raise NameError('We dont support the non-linearity yet')", "\n\ndef nearest_neighbors(X, k, metric):\n    adj = kneighbors_graph(X, k, metric=metric)\n    adj = np.array(adj.todense(), dtype=np.float32)\n    adj += np.eye(adj.shape[0])\n    return adj\n\ndef normalize(adj, mode):\n    EOS = 1e-10\n    if mode == \"sym\":\n        inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n        return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n    elif mode == \"row\":\n        inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n        return inv_degree[:, None] * adj\n    else:\n        exit(\"wrong norm mode\")", "def normalize(adj, mode):\n    EOS = 1e-10\n    if mode == \"sym\":\n        inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n        return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n    elif mode == \"row\":\n        inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n        return inv_degree[:, None] * adj\n    else:\n        exit(\"wrong norm mode\")", "\n\n\ndef symmetrize(adj):  # only for non-sparse\n    return (adj + adj.T) / 2\n\ndef cal_similarity_graph(node_embeddings):\n    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n    return similarity_graph\n", "\n\ndef top_k(raw_graph, K):\n    values, indices = raw_graph.topk(k=int(K), dim=-1)\n    assert torch.max(indices) < raw_graph.shape[1]\n    mask = torch.zeros(raw_graph.shape, device='cuda')\n    mask[torch.arange(raw_graph.shape[0], device='cuda').view(-1, 1), indices] = 1.\n\n    mask.requires_grad = False\n    sparse_graph = raw_graph * mask\n    return sparse_graph", "\n\ndef knn_fast(X, k, b):\n    X = F.normalize(X, dim=1, p=2)\n    index = 0\n    values = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n    rows = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n    cols = torch.zeros(X.shape[0] * (k + 1), device='cuda')\n    norm_row = torch.zeros(X.shape[0], device='cuda')\n    norm_col = torch.zeros(X.shape[0], device='cuda')\n    while index < X.shape[0]:\n        if (index + b) > (X.shape[0]):\n            end = X.shape[0]\n        else:\n            end = index + b\n        sub_tensor = X[index:index + b]\n        similarities = torch.mm(sub_tensor, X.t())\n        vals, inds = similarities.topk(k=k + 1, dim=-1)\n        values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n        cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n        rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end, device='cuda').view(-1, 1).repeat(1, k + 1).view(-1)\n        norm_row[index: end] = torch.sum(vals, dim=1)\n        norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n        index += b\n    norm = norm_row + norm_col\n    rows = rows.long()\n    cols = cols.long()\n    values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n    return rows, cols, values", "\nclass MLP(torch.nn.Module):\n    def __init__(self, nlayers, isize, hsize, osize, features, mlp_epochs, k, knn_metric, non_linearity, i, mlp_act):\n        super(MLP, self).__init__()\n\n        self.layers = torch.nn.ModuleList()\n        if nlayers == 1:\n            self.layers.append(torch.nn.Linear(isize, hsize))\n        else:\n            self.layers.append(torch.nn.Linear(isize, hsize))\n            for _ in range(nlayers - 2):\n                self.layers.append(torch.nn.Linear(hsize, hsize))\n            self.layers.append(torch.nn.Linear(hsize, osize))\n\n        self.input_dim = isize\n        self.output_dim = osize\n        self.features = features\n        self.mlp_epochs = mlp_epochs\n        self.k = k\n        self.knn_metric = knn_metric\n        self.non_linearity = non_linearity\n        self.i = i\n        self.mlp_act = mlp_act\n        self.mlp_knn_init()\n\n    def internal_forward(self, h):\n        for i, layer in enumerate(self.layers):\n            h = layer(h)\n            if i != (len(self.layers) - 1):\n                if self.mlp_act == \"relu\":\n                    h = F.relu(h)\n                elif self.mlp_act == \"tanh\":\n                    h = F.tanh(h)\n        return h\n\n    def mlp_knn_init(self):\n        self.layers.to(self.features.device)\n        if self.input_dim == self.output_dim:\n            print(\"MLP full\")\n            for layer in self.layers:\n                layer.weight = torch.nn.Parameter(torch.eye(self.input_dim))\n        else:\n            optimizer = torch.optim.Adam(self.parameters(), 0.01)\n            labels = torch.from_numpy(nearest_neighbors(self.features.cpu(), self.k, self.knn_metric)).cuda()\n            for epoch in range(1, self.mlp_epochs):\n                self.train()\n                logits = self.forward(self.features)\n                loss = F.mse_loss(logits, labels, reduction='sum')\n                if epoch % 10 == 0:\n                    print(\"MLP loss\", loss.item())\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n    def forward(self, features):\n        embeddings = self.internal_forward(features)\n        embeddings = F.normalize(embeddings, dim=1, p=2)\n        similarities = cal_similarity_graph(embeddings)\n        similarities = top_k(similarities, self.k + 1)\n        similarities = apply_non_linearity(similarities, self.non_linearity, self.i)\n        return similarities", "\nclass GCN_DAE(torch.nn.Module):\n    def __init__(self, cfg_model, nlayers, in_dim, hidden_dim, nclasses, dropout, dropout_adj, features, k, knn_metric, i_,\n                 non_linearity, normalization, mlp_h, mlp_epochs, mlp_act):\n        super(GCN_DAE, self).__init__()\n\n        if cfg_model['type'] == 'gcn':\n            self.layers = GCN(in_dim, hidden_dim, nclasses, n_layers=nlayers, dropout=dropout, spmm_type=1)\n        elif cfg_model['type'] == 'appnp':\n            self.layers = APPNP(in_dim, hidden_dim, nclasses, spmm_type=1,\n                               dropout=dropout, K=cfg_model['appnp_k'], alpha=cfg_model['appnp_alpha'])\n\n        self.dropout_adj = torch.nn.Dropout(p=dropout_adj)\n        self.normalization = normalization\n\n        self.graph_gen = MLP(2, features.shape[1], math.floor(math.sqrt(features.shape[1] * mlp_h)),\n                                mlp_h, features, mlp_epochs, k, knn_metric, non_linearity, i_,\n                                mlp_act).cuda()\n\n    def get_adj(self, h):\n        Adj_ = self.graph_gen(h)\n        Adj_ = symmetrize(Adj_)\n        Adj_ = normalize(Adj_, self.normalization)\n        return Adj_\n\n    def forward(self, features, x):  # x corresponds to masked_features\n        Adj_ = self.get_adj(features)\n        Adj = self.dropout_adj(Adj_)\n\n        x = self.layers((x, Adj, True))\n\n        return x, Adj_", "\nclass GCN_C(torch.nn.Module):\n    def __init__(self, cfg_model, in_channels, hidden_channels, out_channels, num_layers, dropout, dropout_adj):\n        super(GCN_C, self).__init__()\n\n        if cfg_model['type'] == 'gcn':\n            self.layers = GCN(in_channels, hidden_channels, out_channels, n_layers=num_layers, dropout=dropout, spmm_type=1)\n        elif cfg_model['type'] == 'appnp':\n            self.layers = APPNP(in_channels, hidden_channels, out_channels, spmm_type=1,\n                               dropout=dropout, K=cfg_model['appnp_k'], alpha=cfg_model['appnp_alpha'])\n\n        self.dropout_adj = torch.nn.Dropout(p=dropout_adj)\n\n    def forward(self, x, adj_t):\n        Adj = self.dropout_adj(adj_t)\n\n        x = self.layers((x, Adj, True))\n        return x", "\n\n\nclass SLAPS(torch.nn.Module):\n    def __init__(self, num_nodes, num_features, num_classes, features, device, conf):\n        super(SLAPS, self).__init__()\n        self.num_nodes = num_nodes\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.device = device\n        self.conf = conf\n\n        self.gcn_dae = GCN_DAE(self.conf.model, nlayers=self.conf.model['nlayers_adj'], in_dim=num_features, hidden_dim=self.conf.model['hidden_adj'], nclasses=num_features,\n                             dropout=self.conf.model['dropout1'], dropout_adj=self.conf.model['dropout_adj1'],\n                             features=features, k=self.conf.model['k'], knn_metric=self.conf.model['knn_metric'], i_=self.conf.model['i'],\n                             non_linearity=self.conf.model['non_linearity'], normalization=self.conf.model['normalization'], mlp_h=self.num_features,\n                             mlp_epochs=self.conf.model['mlp_epochs'], mlp_act=self.conf.model['mlp_act'])\n        self.gcn_c = GCN_C(self.conf.model, in_channels=num_features, hidden_channels=self.conf.model['hidden'], out_channels=num_classes,\n                            num_layers=self.conf.model['nlayers'], dropout=self.conf.model['dropout2'], dropout_adj=self.conf.model['dropout_adj2'])\n\n\n    def forward(self, features):\n        loss_dae, Adj = self.get_loss_masked_features(features)\n        logits = self.gcn_c(features, Adj)\n        if len(logits.shape) > 1:\n            logits = logits.squeeze(1)\n        \n        return logits, loss_dae, Adj\n\n    def get_loss_masked_features(self, features):\n        if self.conf.dataset['feat_type'] == 'binary':\n            mask = self.get_random_mask_binary(features, self.conf.training['ratio'], self.conf.training['nr'])\n            masked_features = features * (1 - mask)\n\n            logits, Adj = self.gcn_dae(features, masked_features)\n            indices = mask > 0\n            loss = F.binary_cross_entropy_with_logits(logits[indices], features[indices], reduction='mean')\n        elif self.conf.dataset['feat_type'] == 'continuous':\n            mask = self.get_random_mask_continuous(features, self.conf.training['ratio'])\n            # noise = torch.normal(0.0, 1.0, size=features.shape).cuda()\n            # masked_features = features + (noise * mask)\n            masked_features = features * (1 - mask)\n\n            logits, Adj = self.gcn_dae(features, masked_features)\n            indices = mask > 0\n            loss = F.binary_cross_entropy_with_logits(logits[indices], features[indices], reduction='mean')\n        else:\n            raise ValueError(\"Wrong feat_type in dataset_configure.\")\n    \n        return loss, Adj\n    \n    def get_random_mask_binary(self, features, r, nr):\n        nones = torch.sum(features > 0.0).float()\n        nzeros = features.shape[0] * features.shape[1] - nones\n        pzeros = nones / nzeros / r * nr\n        probs = torch.zeros(features.shape, device='cuda')\n        probs[features == 0.0] = pzeros\n        probs[features > 0.0] = 1 / r\n        mask = torch.bernoulli(probs)\n        return mask\n\n    def get_random_mask_continuous(self, features, r):\n        probs = torch.full(features.shape, 1 / r, device='cuda')\n        mask = torch.bernoulli(probs)\n        return mask", ""]}
{"filename": "opengsl/method/models/idgl.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .gcn_idgl import GCN\n\n\nclass AnchorGCNLayer(nn.Module):\n    def __init__(self, in_features, out_features, with_bias=False, batch_norm=True):\n        super(AnchorGCNLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        if with_bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.bn = nn.BatchNorm1d(out_features) if batch_norm else None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj, anchor_mp=True):\n        if input.data.is_sparse:\n            support = torch.spmm(input, self.weight)\n        else:\n            support = torch.mm(input, self.weight)\n\n        if anchor_mp:\n            node_anchor_adj = adj\n            node_norm = node_anchor_adj / torch.clamp(torch.sum(node_anchor_adj, dim=-2, keepdim=True), min=1e-12)\n            anchor_norm = node_anchor_adj / torch.clamp(torch.sum(node_anchor_adj, dim=-1, keepdim=True), min=1e-12)\n            output = torch.matmul(anchor_norm, torch.matmul(node_norm.transpose(-1, -2), support))\n        else:\n            output = torch.spmm(adj, support)\n        if self.bias is not None:\n            output = output + self.bias\n        return output\n\n    def compute_bn(self, x):\n        if len(x.shape) == 2:\n            return self.bn(x)\n        else:\n            return self.bn(x.view(-1, x.size(-1))).view(x.size())", "\n\nclass AnchorGCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, n_layers=3, dropout=0.5, with_bias=False, batch_norm=True):\n        super(AnchorGCN, self).__init__()\n        self.nfeat = nfeat\n        self.hidden_sizes = [nhid]\n        self.nclass = nclass\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.with_bias = with_bias\n        self.batch_norm = batch_norm\n\n        self.layers = nn.ModuleList()\n        self.layers.append(AnchorGCNLayer(nfeat, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\n        for _ in range(n_layers - 2):\n            self.layers.append(AnchorGCNLayer(nhid, nhid, with_bias=with_bias, batch_norm=batch_norm))\n\n        self.layers.append(AnchorGCNLayer(nhid, nclass, with_bias=with_bias, batch_norm=False))\n\n\n    def forward(self, x, init_adj, cur_node_anchor_adj, graph_skip_conn, first=True, first_init_agg_vec=None,\n                init_agg_vec=None, update_adj_ratio=None, dropout=None, first_node_anchor_adj=None):\n\n        if dropout is None:\n            dropout = self.dropout\n\n        # layer 1\n        first_vec = self.layers[0](x, cur_node_anchor_adj, anchor_mp=True)\n        if first:\n            init_agg_vec = self.layers[0](x, init_adj, anchor_mp=False)\n        else:\n            first_vec = update_adj_ratio * first_vec + (1 - update_adj_ratio) * first_init_agg_vec\n        node_vec = (1-graph_skip_conn)*first_vec+graph_skip_conn*init_agg_vec\n        if self.batch_norm:\n            node_vec = self.layers[0].compute_bn(node_vec)\n        node_vec = F.dropout(torch.relu(node_vec), dropout, training=self.training)\n\n        # layer 2-n-1\n        for encoder in self.layers[1:-1]:\n\n            mid_cur_agg_vec = encoder(node_vec, cur_node_anchor_adj, anchor_mp=True)\n            if not first:\n                mid_cur_agg_vec = update_adj_ratio*mid_cur_agg_vec+(1-update_adj_ratio)*encoder(node_vec,first_node_anchor_adj,anchor_mp=True)\n            node_vec = (1 - graph_skip_conn) * mid_cur_agg_vec + graph_skip_conn * encoder(node_vec, init_adj, anchor_mp=False)\n            if self.batch_norm:\n                node_vec = encoder.compute_bn(node_vec)\n            node_vec = F.dropout(torch.relu(node_vec), dropout, training=self.training)\n\n        # layer n\n        cur_agg_vec = self.layers[-1](node_vec, cur_node_anchor_adj, anchor_mp=True)\n        if not first:\n            cur_agg_vec = update_adj_ratio * cur_agg_vec + (1-update_adj_ratio) * self.layers[-1](node_vec, first_node_anchor_adj, anchor_mp=True)\n        output = (1 - graph_skip_conn) * cur_agg_vec + graph_skip_conn * self.layers[-1](node_vec, init_adj, anchor_mp=False)\n        output = F.log_softmax(output, dim=-1).squeeze(1)\n        return first_vec, init_agg_vec, node_vec, output", "\n\nclass GraphLearner(nn.Module):\n    def __init__(self, input_size, topk=None, epsilon=None, num_pers=16):\n        super(GraphLearner, self).__init__()\n        self.topk = topk\n        self.epsilon = epsilon\n\n        self.weight_tensor = torch.Tensor(num_pers, input_size)\n        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n\n    def forward(self, context, anchor=None):\n        # return a new adj according to the representation gived\n\n        expand_weight_tensor = self.weight_tensor.unsqueeze(1)\n        if len(context.shape) == 3:\n            expand_weight_tensor = expand_weight_tensor.unsqueeze(1)\n\n        context_fc = context.unsqueeze(0) * expand_weight_tensor  # (num_pers, num_node, dim)\n        context_norm = F.normalize(context_fc, p=2, dim=-1)\n\n        if anchor is None:\n            attention = torch.matmul(context_norm, context_norm.transpose(-1, -2)).mean(0)   # (num_node, num_node)\n            markoff_value = 0\n        else:\n            anchors_fc = anchor.unsqueeze(0) * expand_weight_tensor\n            anchors_norm = F.normalize(anchors_fc, p=2, dim=-1)\n            attention = torch.matmul(context_norm, anchors_norm.transpose(-1, -2)).mean(0)  # (num_node, num_anchor)\n            markoff_value = 0\n\n        if self.epsilon is not None:\n            attention = self.build_epsilon_neighbourhood(attention, self.epsilon, markoff_value)\n\n        if self.topk is not None:\n            attention = self.build_knn_neighbourhood(attention, self.topk, markoff_value)\n        return attention\n\n    def build_knn_neighbourhood(self, attention, topk, markoff_value):\n        device = attention.device\n        topk = min(topk, attention.size(-1))\n        knn_val, knn_ind = torch.topk(attention, topk, dim=-1)\n        weighted_adjacency_matrix = (markoff_value * torch.ones_like(attention)).scatter_(-1, knn_ind, knn_val).to(device)\n        return weighted_adjacency_matrix\n\n    def build_epsilon_neighbourhood(self, attention, epsilon, markoff_value):\n        mask = (attention > epsilon).detach().float()\n        weighted_adjacency_matrix = attention * mask + markoff_value * (1 - mask)\n        return weighted_adjacency_matrix", "\n\nclass IDGL(nn.Module):\n    def __init__(self, conf, nfeat, nclass):\n        super(IDGL, self).__init__()\n        self.nfeat = nfeat\n        self.nclass = nclass\n        self.hidden_size = conf.model['n_hidden']\n        self.dropout = conf.model['dropout']\n        self.scalable_run = conf.model['scalable_run'] if 'scalable_run' in conf.model else False\n        self.feat_adj_dropout = conf.gsl['feat_adj_dropout']\n\n        gcn_module = AnchorGCN if self.scalable_run else GCN\n        self.encoder = gcn_module(nfeat=nfeat, nhid=conf.model['n_hidden'], nclass=nclass, n_layers=conf.model['n_layers'], dropout=conf.model['dropout'], batch_norm=conf.model['norm'])\n        self.graph_learner = GraphLearner(nfeat,\n                                        topk=conf.gsl['graph_learn_topk'],\n                                        epsilon=conf.gsl['graph_learn_epsilon'],\n                                        num_pers=conf.gsl['graph_learn_num_pers'])\n\n        self.graph_learner2 = GraphLearner(self.hidden_size,\n                                        topk=conf.gsl['graph_learn_topk2'],\n                                        epsilon=conf.gsl['graph_learn_epsilon2'],\n                                        num_pers=conf.gsl['graph_learn_num_pers'])\n\n    def learn_graph(self, graph_learner, node_features, graph_skip_conn=None, graph_include_self=False, init_adj=None, anchor_features=None):\n        device = node_features.device\n\n        if self.scalable_run:\n            node_anchor_adj = graph_learner(node_features, anchor_features)\n            return node_anchor_adj\n\n        else:\n            raw_adj = graph_learner(node_features)\n\n            assert raw_adj.min().item() >= 0\n            adj = raw_adj / torch.clamp(torch.sum(raw_adj, dim=-1, keepdim=True), min=1e-12)   # \u5f52\u4e00\u5316\n\n            if graph_skip_conn in (0, None):\n                if graph_include_self:\n                    adj = adj + torch.eye(adj.size(0)).to(device)\n            else:\n                adj = graph_skip_conn * init_adj + (1 - graph_skip_conn) * adj\n\n            return raw_adj, adj\n\n    def forward(self, node_features, init_adj=None):\n        node_features = F.dropout(node_features, self.feat_adj_dropout, training=self.training)\n        raw_adj, adj = self.learn_graph(self.graph_learner, node_features, self.graph_skip_conn, init_adj=init_adj)\n        adj = F.dropout(adj, self.feat_adj_dropout, training=self.training)\n        node_vec = self.encoder(node_features, adj)\n        output = F.log_softmax(node_vec, dim=-1).squeeze(1)\n        return output, adj", "\n\ndef sample_anchors(node_vec, s):\n    idx = torch.randperm(node_vec.size(0))[:s]\n    return node_vec[idx], idx\n\n\ndef diff(X, Y, Z):\n    assert X.shape == Y.shape\n    diff_ = torch.sum(torch.pow(X - Y, 2))\n    norm_ = torch.sum(torch.pow(Z, 2))\n    diff_ = diff_ / torch.clamp(norm_, min=1e-12)\n    return diff_", "\n\ndef compute_anchor_adj(node_anchor_adj, anchor_mask=None):\n    '''Can be more memory-efficient'''\n    anchor_node_adj = node_anchor_adj.transpose(-1, -2)   # (num_anchor, num_node)\n    anchor_norm = torch.clamp(anchor_node_adj.sum(dim=-2), min=1e-12) ** -1\n    # anchor_adj = torch.matmul(anchor_node_adj, torch.matmul(torch.diag(anchor_norm), node_anchor_adj))\n    anchor_adj = torch.matmul(anchor_node_adj, anchor_norm.unsqueeze(-1) * node_anchor_adj)\n\n    markoff_value = 0\n    if anchor_mask is not None:\n        anchor_adj = anchor_adj.masked_fill_(1 - anchor_mask.byte().unsqueeze(-1), markoff_value)\n        anchor_adj = anchor_adj.masked_fill_(1 - anchor_mask.byte().unsqueeze(-2), markoff_value)\n\n    return anchor_adj"]}
{"filename": "opengsl/method/models/gnn_modules.py", "chunked_list": ["import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn.functional import one_hot\nfrom torch_geometric.nn import GATConv, GATv2Conv\n\n\nclass SGC(nn.Module):\n    def __init__(self, n_feat, n_class, K=2):\n        super(SGC, self).__init__()\n        self.K = K\n        self.W = nn.Linear(n_feat, n_class)\n        self.features = None\n\n    def sgc_precompute(self, features, adj):\n        for i in range(self.K):\n            features = torch.spmm(adj, features)\n            self.features = features\n\n    def forward(self, input):\n        # adj is normalized and sparse\n        # compute feature propagation only once\n        x=input[0]\n        adj=input[1]\n        if self.features is None:\n            self.sgc_precompute(x, adj)\n        return self.W(self.features).squeeze(1)", "\n\nclass LPA(nn.Module):\n    def __init__(self, n_layers=3, alpha=0.9):\n        super(LPA, self).__init__()\n        self.n_layers = n_layers\n        self.alpha = alpha\n\n    def forward(self, input):\n        # adj is normalized(without self-loop) and sparse\n        # check the format of y\n        y = input[0]\n        adj = input[1]\n        mask = input[2]\n        if not len(y.shape) == 2:\n            # convert to one hot labels\n            y = one_hot(y).float()\n        if mask is not None:\n            out = torch.zeros_like(y)\n            out[mask] = y[mask]\n        res = (1 - self.alpha) * out\n        for _ in range(self.n_layers):\n            out = torch.spmm(adj, out)\n            out = out * self.alpha + res\n            out.clamp_(0., 1.)\n        return out", "\n\nclass LINK(nn.Module):\n    \"\"\" logistic regression on adjacency matrix \"\"\"\n\n    def __init__(self, num_nodes, out_channels):\n        super(LINK, self).__init__()\n        self.W = nn.Linear(num_nodes, out_channels)\n        self.num_nodes = num_nodes\n\n    def reset_parameters(self):\n        self.W.reset_parameters()\n\n    def forward(self, adj):\n        logits = self.W(adj)\n        return logits.squeeze(1)", "\n\nclass MLP(nn.Module):\n    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n                 dropout=.5):\n        super(MLP, self).__init__()\n        self.lins = nn.ModuleList()\n        self.bns = nn.ModuleList()\n        if num_layers == 1:\n            # just linear layer i.e. logistic regression\n            self.lins.append(nn.Linear(in_channels, out_channels))\n        else:\n            self.lins.append(nn.Linear(in_channels, hidden_channels))\n            self.bns.append(nn.BatchNorm1d(hidden_channels))\n            for _ in range(num_layers - 2):\n                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n                self.bns.append(nn.BatchNorm1d(hidden_channels))\n            self.lins.append(nn.Linear(hidden_channels, out_channels))\n\n        self.dropout = dropout\n\n    def reset_parameters(self):\n        for lin in self.lins:\n            lin.reset_parameters()\n        for bn in self.bns:\n            bn.reset_parameters()\n\n    def forward(self, feats):\n        x = feats\n        for i, lin in enumerate(self.lins[:-1]):\n            x = lin(x)\n            x = F.relu(x, inplace=True)\n            x = self.bns[i](x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lins[-1](x)\n        return x", "\n\nclass LINKX(nn.Module):\n    \"\"\" \n    adapted from \n    a = MLP_1(A), x = MLP_2(X), MLP_3(sigma(W_1[a, x] + a + x))\n    \"\"\"\n\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, num_nodes, dropout=.5, cache=False, inner_activation=False, inner_dropout=False, init_layers_A=1, init_layers_X=1):\n        super(LINKX, self).__init__()\n        self.mlpA = MLP(num_nodes, hidden_channels, hidden_channels, init_layers_A, dropout=0)\n        self.mlpX = MLP(in_channels, hidden_channels, hidden_channels, init_layers_X, dropout=0)\n        self.W = nn.Linear(2*hidden_channels, hidden_channels)\n        self.mlp_final = MLP(hidden_channels, hidden_channels, out_channels, num_layers, dropout=dropout)\n        self.in_channels = in_channels\n        self.num_nodes = num_nodes\n        self.A = None\n        self.inner_activation = inner_activation\n        self.inner_dropout = inner_dropout\n\n    def reset_parameters(self):\n        self.mlpA.reset_parameters()\n        self.mlpX.reset_parameters()\n        self.W.reset_parameters()\n        self.mlp_final.reset_parameters()\n\n    def forward(self, input):\n        feat=input[0]\n        A=input[1]\n        xA = self.mlpA(A)\n        xX = self.mlpX(feat)\n        x = torch.cat((xA, xX), axis=-1)\n        x = self.W(x)\n        if self.inner_dropout:\n            x = F.dropout(x)\n        if self.inner_activation:\n            x = F.relu(x)\n        x = F.relu(x + xA + xX)\n        x = self.mlp_final(x)\n\n        return x", "\n\nclass APPNP(nn.Module):\n    '''\n    APPNP Implementation\n    Weight decay on the first layer and dropout on adj are not used.\n    '''\n    def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, K=10, alpha=.1, spmm_type=0):\n        super(APPNP, self).__init__()\n        self.lin1 = nn.Linear(in_channels, hidden_channels)\n        self.lin2 = nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n        self.K = K\n        self.alpha = alpha\n        self.spmm = [torch.spmm, torch.sparse.mm][spmm_type]\n\n    def reset_parameters(self):\n        self.lin1.reset_parameters()\n        self.lin2.reset_parameters()\n\n    def forward(self, input):\n        # adj is normalized and sparse\n        x=input[0]\n        adj=input[1]\n        only_z = input[2] if len(input) > 2 else True\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = F.relu(self.lin1(x))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin2(x)\n        z = x\n        for i in range(self.K):\n            z = (1-self.alpha)*self.spmm(adj,z)+self.alpha*x\n        # x = self.prop1(x, edge_index)\n        if only_z:\n            return z.squeeze(1)\n        else:\n            return z, z.squeeze(1)", "\n\nclass GPRGNN(nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, dropout=.5, dprate=.5, K=10, alpha=.1, init='SGC'):\n        super(GPRGNN, self).__init__()\n        self.lin1 = nn.Linear(in_channels, hidden_channels)\n        self.lin2 = nn.Linear(hidden_channels, out_channels)\n        self.dropout = dropout\n        self.dprate = dprate\n        self.K = K\n        self.alpha = alpha\n\n        assert init in ['SGC', 'PPR', 'NPPR', 'Random']\n        if init == 'SGC':\n            # SGC-like, note that in this case, alpha has to be a integer. It means where the peak at when initializing GPR weights.\n            TEMP = torch.zeros(K+1)\n            TEMP[0] = 1.0\n        elif init == 'PPR':\n            # PPR-like\n            TEMP = alpha*(1-alpha)**torch.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif init == 'NPPR':\n            # Negative PPR\n            TEMP = (alpha)**torch.arange(K+1)\n            TEMP = TEMP/torch.sum(torch.abs(TEMP))\n        elif init == 'Random':\n            # Random\n            bound = torch.sqrt(3/(K+1))\n            TEMP = torch.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/torch.sum(torch.abs(TEMP))\n\n\n        self.temp = nn.Parameter(TEMP)\n\n    def reset_parameters(self):\n        self.lin1.reset_parameters()\n        self.lin2.reset_parameters()\n        torch.nn.init.zeros_(self.temp)\n        self.temp.data[self.k] = self.alpha*(1-self.alpha)**torch.arange(self.K+1)\n        self.temp.data[-1] = (1-self.alpha)**self.K\n\n    def forward(self, input):\n        # adj is normalized and sparse\n        x=input[0]\n        adj=input[1]\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = F.relu(self.lin1(x))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.lin2(x)\n        \n        x = F.dropout(x, p=self.dprate, training=self.training)\n        z = x*self.temp[0]\n        for i in range(self.K):\n            x = torch.spmm(adj,x)\n            z = z + self.temp[i+1]*x\n        return z.squeeze(1)", "\n\nclass GAT(nn.Module):\n    def __init__(self, n_feat, n_hidden, n_class, n_layers, n_heads, dropout):\n        super(GAT, self).__init__()\n        self.convs = nn.ModuleList()\n        self.convs.append(GATConv(n_feat, n_hidden, heads=n_heads, dropout=dropout))\n        for i in range(n_layers-2):\n            self.convs.append(GATConv(n_hidden*n_heads, n_hidden, heads=n_heads, dropout=dropout))\n        self.convs.append((GATConv(n_hidden*n_heads, n_class, heads=n_heads, dropout=dropout, concat=False)))\n        self.dropout = dropout\n\n    def forward(self, input):\n        x = input[0]\n        edge_index = input[1]\n\n        x = F.dropout(x, training=self.training, p=self.dropout)\n\n        for i in range(len(self.convs)-1):\n            x = self.convs[i](x, edge_index)\n            x = F.dropout(x, training=self.training, p=self.dropout)\n            x = F.elu(x)\n        x = self.convs[-1](x, edge_index)\n        return x.squeeze(1)"]}
{"filename": "examples/gcn_cora.py", "chunked_list": ["import opengsl\n\nconf = opengsl.config.load_conf(method=\"gcn\", dataset=\"cora\")\ndataset = opengsl.data.Dataset(\"cora\", n_splits=1, feat_norm=conf.dataset['feat_norm'])\nsolver = opengsl.method.GCNSolver(conf,dataset)\nexp = opengsl.ExpManager(solver)\nexp.run(n_runs = 10)\n"]}
{"filename": "examples/customized_gsl.py", "chunked_list": ["import opengsl\nimport time\nfrom copy import deepcopy\nimport torch\n\n\nclass GSL_Model(torch.nn.Module):\n    def __init__(self, in_dim, output_dim):\n        super(GSL_Model, self).__init__()\n        self.layer = torch.nn.Linear(in_dim, output_dim)\n        \"\"\"\n        init\n        \"\"\"\n\n    def forward(self, input, adj):\n        x = self.layer(input)\n        return x", "\n\nclass GSL(opengsl.method.Solver):\n    def __init__(self, conf, dataset):\n        '''\n        Create a solver for gsl to train, evaluate, test in a run.\n        Parameters\n        ----------\n        conf : config file\n        dataset: dataset object containing all things about dataset\n        '''\n        super().__init__(conf, dataset)\n        self.method_name = \"gsl\"\n        print(\"Solver Version : [{}]\".format(\"gsl\"))\n\n    def learn(self, debug=False):\n        for epoch in range(self.conf.training['n_epochs']):\n            improve = ''\n            t0 = time.time()\n            self.model.train()\n            self.optim.zero_grad()\n\n            # forward and backward\n            output = self.model(self.feats, self.adj)\n\n            loss_train = self.loss_fn(output[self.train_mask], self.labels[self.train_mask])\n            acc_train = self.metric(self.labels[self.train_mask].cpu().numpy(), output[self.train_mask].detach().cpu().numpy())\n            loss_train.backward()\n            self.optim.step()\n\n            # Evaluate\n            loss_val, acc_val = self.evaluate(self.val_mask)\n\n            # save\n            if acc_val > self.result['valid']:\n                improve = '*'\n                self.weights = deepcopy(self.model.state_dict())\n                self.total_time = time.time() - self.start_time\n                self.best_val_loss = loss_val\n                self.result['valid'] = acc_val\n                self.result['train'] = acc_train\n\n            if debug:\n                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n                    epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n\n        print('Optimization Finished!')\n        print('Time(s): {:.4f}'.format(self.total_time))\n        loss_test, acc_test = self.test()\n        self.result['test'] = acc_test\n        print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))\n\n        return self.result, 0\n\n    def evaluate(self, test_mask):\n        self.model.eval()\n        with torch.no_grad():\n            output = self.model(self.feats, self.adj)\n        logits = output[test_mask]\n        labels = self.labels[test_mask]\n        loss = self.loss_fn(logits, labels)\n        return loss, self.metric(labels.cpu().numpy(), logits.detach().cpu().numpy())\n\n    def test(self):\n        self.model.load_state_dict(self.weights)\n        return self.evaluate(self.test_mask)\n\n    def set_method(self):\n        self.model = GSL_Model(self.dim_feats, self.num_targets).to(self.device)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.conf.training['lr'],\n                                 weight_decay=self.conf.training['weight_decay'])", "\n\nif __name__ == \"__main__\":\n    \n    conf = opengsl.config.load_conf(path=\"./configs/gsl_cora.yaml\")\n    dataset = opengsl.data.Dataset(\"cora\", n_splits=1, feat_norm=conf.dataset['feat_norm'])\n    solver = GSL(conf,dataset)\n\n\n    exp = opengsl.ExpManager(solver)\n    exp.run(n_runs = 10)"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../../'))\n", "sys.path.insert(0, os.path.abspath('../../'))\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'OpenGSL'\ncopyright = '2023, Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou'\nauthor = 'Zhiyao Zhou, Sheng Zhou, Bochao Mao, Xuanyi Zhou'\nrelease = 'v0.0.4'\n", "release = 'v0.0.4'\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',", "    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.githubpages',\n]\n", "]\n\ntemplates_path = ['_templates']\nexclude_patterns = []\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\n\nimport sphinx_rtd_theme\n# html_theme = 'alabaster'\nhtml_theme = \"sphinx_rtd_theme\"\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_static_path = ['_static']"]}
