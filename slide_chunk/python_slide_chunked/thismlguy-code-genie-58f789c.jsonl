{"filename": "code_genie/_cache.py", "chunked_list": ["import json\nimport logging\nimport os\nfrom hashlib import blake2b\nfrom tempfile import mkdtemp\nfrom typing import Dict, List, Optional\n\nfrom pydantic import BaseModel\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\nclass _MetaValue(BaseModel):\n    id: str\n    instructions: List[str]\n    inputs: List[str]\n\n    class Config:\n        frozen = True\n\n    def toJson(self):\n        return json.dumps(self, default=lambda o: o.__dict__)", "\n\nclass _CacheValue(_MetaValue):\n    code: str\n\n\nclass _CacheManager:\n    DEFAULT_NAME = \"_genie_cache.json\"\n    META_NAME = \"_meta.json\"\n    DEFAULT_CACHE_DIR = mkdtemp()\n\n    def __init__(self, cache_dir: Optional[str] = None):\n        self.cache_dir, self.meta_path = self._check_cache_dir(cache_dir or self.DEFAULT_CACHE_DIR)\n\n    @classmethod\n    def _set_cache_dir(cls, cache_dir: str):\n        cls.DEFAULT_CACHE_DIR = cache_dir\n\n    @classmethod\n    def reload(cls, filepath: str):\n        return cls(filepath)\n\n    def _check_cache_dir(self, name: str):\n        # if dir doesn't exist, create it\n        if not os.path.exists(name):\n            os.makedirs(name)\n        return name, os.path.join(name, self.META_NAME)\n\n    @staticmethod\n    def _json_decoder(schema):\n        def decoder(obj):\n            try:\n                return schema.parse_obj(obj)\n            except:\n                return obj\n\n        return decoder\n\n    @staticmethod\n    def _json_encoder(obj):\n        try:\n            return obj.dict()\n        except:\n            return obj\n\n    def _load_meta(self) -> Dict[str, _MetaValue]:\n        if not os.path.exists(self.meta_path):\n            return {}\n        with open(self.meta_path, \"r\") as f:\n            meta_data: Dict[str, _MetaValue] = json.load(f, object_hook=self._json_decoder(_MetaValue))\n        return meta_data\n\n    def _load_code(self, id: str) -> str:\n        with open(self._get_filename(id), \"r\") as f:\n            return f.read()\n\n    @classmethod\n    def _consistent_hash(cls, value: str) -> str:\n        h = blake2b()\n        h.update(bytes(value, \"utf-8\"))\n        return h.hexdigest()\n\n    def _get_filename(self, id: str) -> str:\n        return os.path.join(self.cache_dir, f\"{id}.py\")\n\n    def update(self, key: str, value: _CacheValue):\n        _cache_meta = self._load_meta()\n        key_hash = self._consistent_hash(key)\n\n        # if key is present in, delete the file which is currently cashed\n        if key_hash in _cache_meta:\n            filename = self._get_filename(_cache_meta[key_hash].id)\n            if os.path.exists(filename):\n                os.remove(filename)\n\n        # add new cash entry\n        _cache_meta[key_hash] = value\n        # write to code file\n        with open(self._get_filename(value.id), \"w\") as f:\n            f.write(value.code)\n        # update metadata\n        with open(self.meta_path, \"w\") as f:\n            json.dump(_cache_meta, f, indent=4, default=self._json_encoder)\n\n    def get(self, key: str) -> Optional[_CacheValue]:\n        _cache_meta = self._load_meta()\n        meta = _cache_meta.get(self._consistent_hash(key), None)\n        if meta is not None:\n            try:\n                code = self._load_code(meta.id)\n                return _CacheValue(code=code, id=meta.id, instructions=meta.instructions, inputs=meta.inputs)\n            except FileNotFoundError:\n                return None\n        return None\n\n    def num_items(self):\n        return len(self._load_meta())\n\n    def get_all_code_segments(self) -> Dict[str, str]:\n        _cache_meta = self._load_meta()\n        code_segments = {}\n        for meta in _cache_meta.values():\n            code_segments[meta.id] = self._load_code(meta.id)\n        return code_segments", ""]}
{"filename": "code_genie/client.py", "chunked_list": ["import os\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom uuid import UUID\n\nimport requests\nfrom pydantic import BaseModel\n\n\nclass GetExecutableRequest(BaseModel):\n    instructions: Union[str, List[str]]\n    inputs: Dict[str, str]\n    allowed_imports: Optional[List[str]] = None", "class GetExecutableRequest(BaseModel):\n    instructions: Union[str, List[str]]\n    inputs: Dict[str, str]\n    allowed_imports: Optional[List[str]] = None\n\n\nclass GetPandasExecutableRequest(GetExecutableRequest):\n    inputs: Optional[Dict[str, str]] = None\n    columns: Optional[List[str]] = None\n", "\n\nclass GetExecutableResponse(BaseModel):\n    id: UUID\n    code: str\n    fn_name: str\n\n\nclass Client:\n    TOKEN_ENV_VAR = \"CODE_GENIE_TOKEN\"\n    URL = \"https://code-scribe-pzj44qvhfa-el.a.run.app\"\n    ENDPOINT = \"get-executable/generic\"\n\n    def __init__(self, token: Optional[str] = None):\n        self._token = token or os.environ[self.TOKEN_ENV_VAR]\n\n    def _get_response(self, endpoint, data):\n        headers = {\"token\": self._token, \"Content-Type\": \"application/json\"}\n        response = requests.post(url=f\"{self.URL}/{endpoint}\", data=data.json(), headers=headers)\n        # if error found, raise the error\n        response.raise_for_status()\n        return GetExecutableResponse.parse_obj(response.json())\n\n    def get(self, instructions: List[str], inputs: Dict[str, str]) -> str:\n        request = GetExecutableRequest(instructions=instructions, inputs=inputs, allowed_imports=[])\n        # send a request with given data\n        response = self._get_response(self.ENDPOINT, request)\n        return response.code", "class Client:\n    TOKEN_ENV_VAR = \"CODE_GENIE_TOKEN\"\n    URL = \"https://code-scribe-pzj44qvhfa-el.a.run.app\"\n    ENDPOINT = \"get-executable/generic\"\n\n    def __init__(self, token: Optional[str] = None):\n        self._token = token or os.environ[self.TOKEN_ENV_VAR]\n\n    def _get_response(self, endpoint, data):\n        headers = {\"token\": self._token, \"Content-Type\": \"application/json\"}\n        response = requests.post(url=f\"{self.URL}/{endpoint}\", data=data.json(), headers=headers)\n        # if error found, raise the error\n        response.raise_for_status()\n        return GetExecutableResponse.parse_obj(response.json())\n\n    def get(self, instructions: List[str], inputs: Dict[str, str]) -> str:\n        request = GetExecutableRequest(instructions=instructions, inputs=inputs, allowed_imports=[])\n        # send a request with given data\n        response = self._get_response(self.ENDPOINT, request)\n        return response.code", ""]}
{"filename": "code_genie/pipeline.py", "chunked_list": ["import json\nimport os\nimport time\nfrom typing import Any, Dict, List, Optional, TypeVar, Union\n\nfrom pydantic import BaseModel, validator\n\nfrom code_genie.genie import Genie, GenieResult\nfrom code_genie.io import (\n    BigQueryToDataframeSource,", "from code_genie.io import (\n    BigQueryToDataframeSource,\n    BoolArg,\n    CsvToDataFrameSource,\n    DataFrameToCsvSink,\n    IntArg,\n    StringArg,\n)\nfrom code_genie.io.argument import GenieArgument\nfrom code_genie.io.base import GenieSource", "from code_genie.io.argument import GenieArgument\nfrom code_genie.io.base import GenieSource\n\nSource = TypeVar(\"Source\", CsvToDataFrameSource, BigQueryToDataframeSource)\nSink = TypeVar(\"Sink\", bound=DataFrameToCsvSink)\nArgument = TypeVar(\"Argument\", StringArg, IntArg, BoolArg)\n\n\nclass PipelineStep(BaseModel):\n    genie_result: GenieResult\n    \"\"\"Result of the genie which should be run in this step\"\"\"\n    data: Optional[Union[Source, GenieResult]] = None\n    \"\"\"Data to be passed to the genie for computation. This could be either a data source or a previous genie result\"\"\"\n    additional_inputs: Optional[Dict[str, Union[Source, GenieResult, Argument]]] = None\n    \"\"\"Set this value for each additional input to the genie. The dictionary key should be the name of the input\n    and the value could be one of the 3 things: \n    1. A genie data source\n    2. A genie result from a previous step\n    3. A constant value to be passed as an argument to the pipeline\"\"\"\n    sink: Optional[Sink] = None\n    \"\"\"If the output of this step needs to be exported, then a sink can be provided here\"\"\"", "class PipelineStep(BaseModel):\n    genie_result: GenieResult\n    \"\"\"Result of the genie which should be run in this step\"\"\"\n    data: Optional[Union[Source, GenieResult]] = None\n    \"\"\"Data to be passed to the genie for computation. This could be either a data source or a previous genie result\"\"\"\n    additional_inputs: Optional[Dict[str, Union[Source, GenieResult, Argument]]] = None\n    \"\"\"Set this value for each additional input to the genie. The dictionary key should be the name of the input\n    and the value could be one of the 3 things: \n    1. A genie data source\n    2. A genie result from a previous step\n    3. A constant value to be passed as an argument to the pipeline\"\"\"\n    sink: Optional[Sink] = None\n    \"\"\"If the output of this step needs to be exported, then a sink can be provided here\"\"\"", "\n\nclass GeniePipeline(BaseModel):\n    name: str\n    \"\"\"Name of the pipeline\"\"\"\n    version: str\n    \"\"\"Version of the pipeline\"\"\"\n    cache_dir: str\n    \"\"\"Directory where the genies being used in this pipeline are cached. The pipeline will also be cached at the \n    same location\"\"\"\n    steps: List[PipelineStep]\n    \"\"\"List of steps in the pipeline\"\"\"\n\n    def add(self, step: PipelineStep):\n        \"\"\"Add a step to the pipeline\"\"\"\n        return self.copy(update={\"steps\": self.steps + [step]})\n\n    @validator(\"steps\")\n    def _validate_steps(cls, v: List[PipelineStep]):\n        # base_input of the first step should be a genie source\n        if v[0].data is None:\n            raise ValueError(f\"base_input_source of the first step should be set, found None\")\n        # there should be atleast 1 sink\n        if all(step.sink is None for step in v):\n            raise ValueError(\"atleast one of the steps should have a sink; none found\")\n        return v\n\n    def _get_filepath(self, filename: str):\n        if not filename.endswith(\"json\"):\n            filename += \".json\"\n        return os.path.join(self.cache_dir, filename)\n\n    def export(self, filename: str):\n        \"\"\"Export the pipeline as a json file\"\"\"\n        with open(self._get_filepath(filename), \"w\") as f:\n            json.dump(self.dict(), f)\n\n    @classmethod\n    def load(cls, filepath: str):\n        \"\"\"Load the pipeline from a json file\"\"\"\n        with open(filepath, \"r\") as f:\n            return cls.parse_obj(json.load(f))\n\n    @classmethod\n    def _get_cached_genie_result(cls, step_id: str, genie_id: str, cached_results: Dict[str, GenieResult]):\n        if genie_id not in cached_results:\n            raise ValueError(\n                f\"Error in step id id: {step_id}; You are attempting to use the results of genie with id: {genie_id} \"\n                f\"in this step but the genie has not been run in a previous step. Add a step before this step in the \"\n                f\"pipeline to run genie id {genie_id}.\"\n            )\n        return cached_results[genie_id].result\n\n    def run(self, args: Dict[str, Any]):\n        \"\"\"Run the pipeline using the value of the arguments passed. Note that all arguments which do not have a\n        default value needs to be passed here for the pipeline to run.\"\"\"\n        cached_genie_results: Dict[str, GenieResult] = {}\n        for i, step in enumerate(self.steps):\n            print(f\"Running step {i+1}: {step.genie_result.id}\")\n            # initialize timer\n            start_time = time.time()\n            step_id = step.genie_result.id\n            # get the base input\n            if isinstance(step.data, GenieSource):\n                base_input = step.data.get(**args)\n            elif isinstance(step.data, GenieResult):\n                base_input = self._get_cached_genie_result(\n                    step_id=step_id, genie_id=step.data.id, cached_results=cached_genie_results\n                )\n            else:\n                raise ValueError(f\"Invalid type for base_input: {type(step.data)}\")\n\n            # get the additional inputs\n            additional_inputs = {}\n            for name, add_input in (step.additional_inputs or {}).items():\n                if isinstance(add_input, GenieSource):\n                    additional_inputs[name] = add_input.get(**args)\n                if isinstance(add_input, GenieResult):\n                    additional_inputs[name] = self._get_cached_genie_result(step_id, add_input.id, cached_genie_results)\n                if isinstance(add_input, GenieArgument):\n                    additional_inputs[name] = add_input.get(**args)\n\n            # run the genie\n            genie = Genie(data=base_input)\n            genie_result = genie.run(step.genie_result.code, additional_inputs)\n            cached_genie_results[step_id] = GenieResult(\n                id=step_id, result=genie_result, code=step.genie_result.code, cache_dir=self.cache_dir\n            )\n\n            # write the output\n            if step.sink is not None:\n                step.sink.put(genie_result, **args)\n            end_time = time.time()\n            print(f\"\\tCompleted in {end_time - start_time:.1f} seconds\")", ""]}
{"filename": "code_genie/genie.py", "chunked_list": ["import random\nimport re\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport pandas as pd\nfrom pydantic import BaseModel\n\nfrom code_genie._cache import _CacheManager, _CacheValue\nfrom code_genie.client import Client\n", "from code_genie.client import Client\n\n\nclass GenieResult(BaseModel):\n    \"\"\"The result of a genie execution\"\"\"\n\n    id: str\n    \"\"\"ID of the genie, this would also be the filename used for storing the generated code in the cache.\"\"\"\n\n    code: str\n    \"\"\"The code generated by the genie\"\"\"\n\n    cache_dir: str\n    \"\"\"The cache directory used by the genie\"\"\"\n\n    result: Any = None\n    \"\"\"The result of the execution; None if no result was returned\"\"\"\n\n    class Config:\n        # always exclude result from json export\n        fields = {\"result\": {\"exclude\": True}}\n        frozen = True", "\n\nclass Genie:\n    _hash_sep = \"::\"\n\n    def __init__(\n        self,\n        data: Optional[Any] = None,\n        client: Optional[Client] = None,\n        cache_dir: Optional[str] = None,\n        copy_data_before_use: bool = True,\n    ):\n        \"\"\"Initialize a genie instance\n\n        Args:\n            data: a base dataset whose attributes will be used to generate the code. the result will be determined\n                by running this data over the code\n            client: an instance of the client to use for making requests to the api. if not provided, a new instance\n                will be created.\n            cache_dir: if provided, the code generated by the genie will be cached in this directory. if not\n                provided, the global default is used. it is recommended to use set_cache_dir() method to set this.\n            copy_data_before_use: if True, the data will be copied before passing through generated code. this is to\n                prevent the data from being modified inplace by the code. the data passed should have a copy() method\n                implemented. if False, the data will be passed as is. this is faster but can lead to unexpected results\n\n        Returns:\n            A callable which can be used to execute the code generated by the genie.\n        \"\"\"\n        self.data = data\n        self._base_key = self._get_data_key(data)\n        self._cache = _CacheManager(cache_dir)\n        self.copy_data_before_use = copy_data_before_use\n        if copy_data_before_use:\n            # check data should have a copy method\n            if not hasattr(data, \"copy\"):\n                raise ValueError(\n                    \"data should have a copy method implemented if copy_data_before_use is True\",\n                    \"Set it to False if you want to continue using the genie\",\n                )\n        self._client = client or Client()\n\n    def plz(\n        self,\n        instructions: Optional[Union[str, List[str]]],\n        additional_inputs: Optional[Dict[str, Any]] = None,\n        override: bool = False,\n        update_base_input: bool = False,\n    ) -> GenieResult:\n        \"\"\"Generate code for a new task\n\n        Args:\n            instructions: text instructions on the task required to be performed. use the keywords in inputs argument\n                to refer to the inputs.\n            additional_inputs: a dictionary of inputs to the function. the keys are the names of the inputs and the\n                values are small description of the inputs.\n            override: if a genie has been generated before with the same args, then it will be loaded from cache be\n                default. set override to True to make a new API call and recreate the genie.\n            update_base_input: if True, the base data will be replaced by the result of executing the code. this is used\n                if we are making a permanent update to the input and want to use the updated input moving forward.\n\n        Returns:\n            A GenieResult instance which contains attributes:\n                - result: the result of executing the code\n                - id: the id of the genie\n                - code: the code generated by the genie\n                - cache_dir: the directory where the code is cached. the code will be cached in a file named\n                    \"cache_dir/<id>.py\n        \"\"\"\n        if isinstance(instructions, str):\n            instructions = [instructions]\n\n        # check cache\n        cache_key = self._get_hash_str(instructions, additional_inputs)\n        cache_value = self._cache.get(cache_key)\n\n        # case: reading from cache\n        if (not override) and (cache_value is not None):\n            code, id = cache_value.code, cache_value.id\n            print(f\"Loading cached genie id: {id}, set override = True to rerun\")\n        else:\n            # case: creating new genie\n            inputs = self._combine_inputs(additional_inputs)\n            code = self._get_code(instructions, inputs)\n            id = self._generate_id(code)\n            self._update_cache(code, cache_key, instructions, inputs, id)\n        return self._get_result(code, additional_inputs, update_base_input, id)\n\n    def _update_cache(\n        self, code: str, cache_key: str, instructions: Optional[Union[str, List[str]]], inputs: Dict[str, Any], id: str\n    ):\n        self._cache.update(\n            cache_key,\n            _CacheValue(code=code, id=id, instructions=instructions, inputs=list(inputs.keys())),\n        )\n        print(f\"Genie cached with id: {id}\")\n\n    def _get_result(self, code, additional_inputs, update_base_input, id: Optional[str] = None):\n        # create executor and return results\n        result = self.run(code, additional_inputs)\n\n        if update_base_input:\n            if result is None:\n                raise ValueError(f\"result of genie is None, cannot update base input\")\n            self.data = result\n        id = id or self._generate_id(code)\n        return GenieResult(id=id, code=code, cache_dir=self._cache.cache_dir, result=result)\n\n    def custom(\n        self, code: str, additional_inputs: Optional[Dict[str, Any]] = None, update_base_input: bool = False\n    ) -> GenieResult:\n        \"\"\"Define a custom genie with user defined code segment. The first argument of the function should be the\n        base input of the genie.\n        Note that this code should define a stand alone function, ie, it should not depend on any external variables\n        or functions or imports. If any additional packages are required, you need to import them in the code segment\n        itself.\n\n        Args:\n            code: the code segment defining a single function to be used to process data.\n            additional_inputs: a dictionary of inputs to the function. the keys are the names of the inputs and the\n                values are small description of the inputs.\n            update_base_input: if True, the base data will be replaced by the result of executing the code. this is used\n                if we are making a permanent update to the input and want to use the updated input moving forward.\n\n        Returns:\n            A GenieResult instance which contains attributes:\n                - result: the result of executing the code\n                - id: the id of the genie\n                - code: the code generated by the genie\n                - cache_dir: the directory where the code is cached. the code will be cached in a file named\n                    \"cache_dir/<id>.py\n        \"\"\"\n        # proxy instructions as the code entered\n        instructions = [code]\n        cache_key = self._get_hash_str(instructions, additional_inputs)\n        id = self._generate_id(code)\n        self._update_cache(code, cache_key, instructions, inputs=self._combine_inputs(additional_inputs), id=id)\n        return self._get_result(code, additional_inputs, update_base_input)\n\n    def run(self, code: str, additional_inputs: Dict[str, Any]):\n        executor = self._extract_executable(code)\n        try:\n            return executor(**self._combine_inputs(additional_inputs, copy_base_input=True))\n        except Exception as e:\n            raise RuntimeError(f\"Failed to execute code segment: \\n\\n{code}\") from e\n\n    @classmethod\n    def _extract_fn_name(cls, code: str):\n        # find function name from code block and substitute with function_name\n        match = re.search(\"def\\s+(.*)\\(.*\\).*:\", code)\n        if match is None:\n            raise RuntimeError(f\"Failed to extract function from code block: {code}\")\n        return match.group(1)\n\n    def _combine_inputs(\n        self, additional_inputs: Optional[Dict[str, Any]], copy_base_input: bool = False\n    ) -> Dict[str, Any]:\n        data = self.data.copy() if (copy_base_input and self.copy_data_before_use) else self.data\n        return {self._base_key: data, **(additional_inputs or {})}\n\n    @staticmethod\n    def _create_input_str(x):\n        if isinstance(x, pd.DataFrame):\n            return f\"pandas dataframes with columns: {x.columns}\"\n        return f\"{type(x)}\"\n\n    def _get_code(self, instructions: List[str], inputs: Dict[str, Any]) -> str:\n        input_str = {key: self._create_input_str(value) for key, value in inputs.items()}\n        return self._client.get(instructions=instructions, inputs=input_str)\n\n    @classmethod\n    def _extract_executable(cls, code: str) -> Callable:\n        # define function in memory\n        fn_name = cls._extract_fn_name(code)\n        mem = {}\n        exec(code, mem)\n        return mem[fn_name]\n\n    @classmethod\n    def _generate_id(cls, code: str) -> str:\n        fn_name = cls._extract_fn_name(code)\n        # use fn name with random 5 digit suffix\n        return f\"{fn_name}_{random.randint(10000, 99999)}\"\n\n    @classmethod\n    def _list_to_str(cls, l: List[str]) -> str:\n        return cls._hash_sep.join(l)\n\n    @classmethod\n    def _inputs_to_str(cls, d: Dict[str, Any]) -> str:\n        sorted_keys = sorted(d.keys())\n        return cls._hash_sep.join([f\"{k}={type(d[k])}\" for k in sorted_keys])\n\n    def _get_hash_str(self, instructions: List[str], additional_inputs: Optional[Dict[str, Any]]) -> str:\n        hash_strings = [\n            self._list_to_str(instructions),\n            self._inputs_to_str(self._combine_inputs(additional_inputs)),\n        ]\n        return self._hash_sep.join(hash_strings)\n\n    def read_cache(self) -> Dict[str, str]:\n        \"\"\"Read all the code segments in the cache directory set in the current genie instance\n\n        Returns:\n            A dictionary with keys as the genie ids and values as the code segments\n        \"\"\"\n        return self._cache.get_all_code_segments()\n\n    @staticmethod\n    def _get_data_key(data):\n        if isinstance(data, pd.DataFrame):\n            return \"df\"\n        return \"data\"", ""]}
{"filename": "code_genie/__init__.py", "chunked_list": ["__version__ = \"0.4.0\"\n\nfrom code_genie._cache import _CacheManager\nfrom code_genie.genie import Genie\n"]}
{"filename": "code_genie/io/base.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Any, Dict, TypeVar\n\nfrom pydantic import BaseModel\n\nfrom code_genie.io.argument import GenieArgument\n\nARG = TypeVar(\"ARG\", bound=GenieArgument)\n\n\nclass GenieSource(ABC, BaseModel):\n    @abstractmethod\n    def get(self, **kwargs: Dict[str, GenieArgument]):\n        raise NotImplemented", "\n\nclass GenieSource(ABC, BaseModel):\n    @abstractmethod\n    def get(self, **kwargs: Dict[str, GenieArgument]):\n        raise NotImplemented\n\n\nclass GenieSink(ABC, BaseModel):\n    @abstractmethod\n    def put(self, data: Any, **kwargs: Dict[str, GenieArgument]):\n        ...", "class GenieSink(ABC, BaseModel):\n    @abstractmethod\n    def put(self, data: Any, **kwargs: Dict[str, GenieArgument]):\n        ...\n"]}
{"filename": "code_genie/io/local.py", "chunked_list": ["from typing import Dict\n\nimport pandas as pd\n\nfrom code_genie.io.argument import BoolArg, GenieArgument, StringArg\nfrom code_genie.io.base import GenieSink, GenieSource\n\n\nclass DataFrameToCsvSink(GenieSink):\n    path: StringArg\n    \"\"\"Path to the local file where data is to be exported.\"\"\"\n    index: BoolArg = BoolArg(name=\"export-pd-index\", default_value=False)\n    \"\"\"Whether to export the index of the dataframe.\"\"\"\n    reset_index: BoolArg = BoolArg(name=\"reset-pd-index\", default_value=True)\n    \"\"\"Whether to reset the index of the dataframe before exporting.\"\"\"\n\n    def put(self, data: pd.DataFrame, **kwargs):\n        if self.reset_index.get(**kwargs):\n            data = data.reset_index()\n        data.to_csv(self.path.get(**kwargs), index=self.index.get(**kwargs))", "class DataFrameToCsvSink(GenieSink):\n    path: StringArg\n    \"\"\"Path to the local file where data is to be exported.\"\"\"\n    index: BoolArg = BoolArg(name=\"export-pd-index\", default_value=False)\n    \"\"\"Whether to export the index of the dataframe.\"\"\"\n    reset_index: BoolArg = BoolArg(name=\"reset-pd-index\", default_value=True)\n    \"\"\"Whether to reset the index of the dataframe before exporting.\"\"\"\n\n    def put(self, data: pd.DataFrame, **kwargs):\n        if self.reset_index.get(**kwargs):\n            data = data.reset_index()\n        data.to_csv(self.path.get(**kwargs), index=self.index.get(**kwargs))", "\n\nclass CsvToDataFrameSource(GenieSource):\n    path: StringArg\n    \"\"\"Path to the local file from where data is to be imported from.\"\"\"\n    kwargs: Dict[str, GenieArgument] = {}\n    \"\"\"Any arguments to be passed to the pandas read_csv method.\"\"\"\n\n    def get(self, **kwargs):\n        return pd.read_csv(self.path.get(**kwargs), **{k: v.get(**kwargs) for k, v in self.kwargs.items()})", ""]}
{"filename": "code_genie/io/gcp.py", "chunked_list": ["from abc import ABC\nfrom typing import Dict\n\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\nfrom code_genie.io.argument import GenieArgument, StringArg\nfrom code_genie.io.base import GenieSource\n\n\nclass _WithCredentials(GenieSource, ABC):\n    _default_key_path_env_var: str = \"GOOGLE_APPLICATION_CREDENTIALS\"\n\n    key_path: StringArg = StringArg(env_var=_default_key_path_env_var)\n    \"\"\"Path to the GCP key file.\"\"\"\n\n    def _bq_client(self, **kwargs):\n        credentials = service_account.Credentials.from_service_account_file(\n            self.key_path.get(**kwargs),\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n        return bigquery.Client(credentials=credentials, project=credentials.project_id)", "\n\nclass _WithCredentials(GenieSource, ABC):\n    _default_key_path_env_var: str = \"GOOGLE_APPLICATION_CREDENTIALS\"\n\n    key_path: StringArg = StringArg(env_var=_default_key_path_env_var)\n    \"\"\"Path to the GCP key file.\"\"\"\n\n    def _bq_client(self, **kwargs):\n        credentials = service_account.Credentials.from_service_account_file(\n            self.key_path.get(**kwargs),\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n        return bigquery.Client(credentials=credentials, project=credentials.project_id)", "\n\nclass BigQueryToDataframeSource(_WithCredentials):\n    query: str\n    \"\"\"Query to be used to read data from BQ\"\"\"\n    query_args: Dict[str, GenieArgument]\n    \"\"\"Any arguments to be used in the query. These will be passed to the query using the format method.\"\"\"\n\n    def get(self, **kwargs: Dict[str, GenieArgument]):\n        query = self.query\n        if self.query_args:\n            query = query.format(**{k: v.get(**kwargs) for k, v in self.query_args.items()})\n        return self._bq_client(**kwargs).query(query).result().to_dataframe()", ""]}
{"filename": "code_genie/io/argument.py", "chunked_list": ["import os\nimport warnings\nfrom abc import ABC\nfrom typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, root_validator, validator\n\n\nclass GenieArgument(ABC, BaseModel):\n    \"\"\"Define an argument to the pipeline. This would be used to create the deployment setup\"\"\"\n\n    name: Optional[str] = None\n    \"\"\"Name of the argument, should only contain numbers, letters, dash and underscores; should start with a letter.\n    If a name is provided, then the value of this argument can be set when running the script.\n    If name is not provided, then default_value must be provided and the value of this argument cannot be set while \n    running the pipeline.\"\"\"\n\n    default_value: Optional[Any] = None  # set custom validator in the subclass\n    \"\"\"Default value of the argument if not provided\"\"\"\n\n    env_var: Optional[str] = None\n    \"\"\"Name of the environment variable from which this argument should be read if not provided\"\"\"\n\n    @validator(\"name\")\n    def name_valid(cls, v):\n        if v is None:\n            return v\n        if not isinstance(v, str):\n            raise ValueError(f\"name must be a string, found: {v}\")\n        # check v contains only letters, numbers and underscores\n        if not v.replace(\"_\", \"\").replace(\"-\", \"\").isalnum():\n            raise ValueError(f\"name must only contain letters, numbers, dash and underscores, found: {v}\")\n        if not v[0].isalpha():\n            raise ValueError(f\"name must start with a letter, found: {v}\")\n        return v\n\n    @root_validator\n    def name_or_default_value(cls, values):\n        if \"name\" not in values and \"default_value\" not in values:\n            raise ValueError(\"Either name or default_value must be provided, both are none\")\n        return values\n\n    def get(self, **kwargs: Dict[str, Any]):\n        \"\"\"Resolve the value of the arg from the dictionary of arguments passed to the pipeline. Resolution is done\n        using the following precedence:\n        1. value set by the pipeline object\n        2. env_var\n        3. default_value\n        \"\"\"\n        value = kwargs.get(self.name, None)\n        if value is not None:\n            return value\n        if self.env_var is not None:\n            value = os.getenv(self.env_var)\n            if value is None:\n                warnings.warn(\n                    f\"No environment variable {self.env_var} found for argument: {self.name}\"\n                    f\" falling back to default value\"\n                )\n            else:\n                return value\n        if self.default_value is not None:\n            return self.default_value\n        raise ValueError(f\"None of value or default_value or env_var provided for argument: {self.name}\")", "class GenieArgument(ABC, BaseModel):\n    \"\"\"Define an argument to the pipeline. This would be used to create the deployment setup\"\"\"\n\n    name: Optional[str] = None\n    \"\"\"Name of the argument, should only contain numbers, letters, dash and underscores; should start with a letter.\n    If a name is provided, then the value of this argument can be set when running the script.\n    If name is not provided, then default_value must be provided and the value of this argument cannot be set while \n    running the pipeline.\"\"\"\n\n    default_value: Optional[Any] = None  # set custom validator in the subclass\n    \"\"\"Default value of the argument if not provided\"\"\"\n\n    env_var: Optional[str] = None\n    \"\"\"Name of the environment variable from which this argument should be read if not provided\"\"\"\n\n    @validator(\"name\")\n    def name_valid(cls, v):\n        if v is None:\n            return v\n        if not isinstance(v, str):\n            raise ValueError(f\"name must be a string, found: {v}\")\n        # check v contains only letters, numbers and underscores\n        if not v.replace(\"_\", \"\").replace(\"-\", \"\").isalnum():\n            raise ValueError(f\"name must only contain letters, numbers, dash and underscores, found: {v}\")\n        if not v[0].isalpha():\n            raise ValueError(f\"name must start with a letter, found: {v}\")\n        return v\n\n    @root_validator\n    def name_or_default_value(cls, values):\n        if \"name\" not in values and \"default_value\" not in values:\n            raise ValueError(\"Either name or default_value must be provided, both are none\")\n        return values\n\n    def get(self, **kwargs: Dict[str, Any]):\n        \"\"\"Resolve the value of the arg from the dictionary of arguments passed to the pipeline. Resolution is done\n        using the following precedence:\n        1. value set by the pipeline object\n        2. env_var\n        3. default_value\n        \"\"\"\n        value = kwargs.get(self.name, None)\n        if value is not None:\n            return value\n        if self.env_var is not None:\n            value = os.getenv(self.env_var)\n            if value is None:\n                warnings.warn(\n                    f\"No environment variable {self.env_var} found for argument: {self.name}\"\n                    f\" falling back to default value\"\n                )\n            else:\n                return value\n        if self.default_value is not None:\n            return self.default_value\n        raise ValueError(f\"None of value or default_value or env_var provided for argument: {self.name}\")", "\n\nclass StringArg(GenieArgument):\n    @validator(\"default_value\")\n    def must_be_str(cls, v):\n        if v is not None and not isinstance(v, str):\n            raise ValueError(f\"default_value must be a string, found: {v}\")\n        return v\n\n\nclass IntArg(GenieArgument):\n    @validator(\"default_value\")\n    def must_be_int(cls, v):\n        if v is not None and not isinstance(v, int):\n            raise ValueError(f\"default_value must be an int, found: {v}\")\n        return v", "\n\nclass IntArg(GenieArgument):\n    @validator(\"default_value\")\n    def must_be_int(cls, v):\n        if v is not None and not isinstance(v, int):\n            raise ValueError(f\"default_value must be an int, found: {v}\")\n        return v\n\n\nclass BoolArg(GenieArgument):\n    @validator(\"default_value\")\n    def must_be_bool(cls, v):\n        if v is not None and not isinstance(v, bool):\n            raise ValueError(f\"default_value must be a bool, found: {v}\")\n        return v", "\n\nclass BoolArg(GenieArgument):\n    @validator(\"default_value\")\n    def must_be_bool(cls, v):\n        if v is not None and not isinstance(v, bool):\n            raise ValueError(f\"default_value must be a bool, found: {v}\")\n        return v\n", ""]}
{"filename": "code_genie/io/__init__.py", "chunked_list": ["from code_genie.io.argument import BoolArg, IntArg, StringArg\nfrom code_genie.io.gcp import BigQueryToDataframeSource\nfrom code_genie.io.local import CsvToDataFrameSource, DataFrameToCsvSink\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_pipeline.py", "chunked_list": ["import os\n\nimport pandas as pd\nimport pytest\n\nfrom code_genie import Genie\nfrom code_genie.io import IntArg, StringArg\nfrom code_genie.io.local import CsvToDataFrameSource, DataFrameToCsvSink\nfrom code_genie.pipeline import GeniePipeline, PipelineStep\n", "from code_genie.pipeline import GeniePipeline, PipelineStep\n\n\n@pytest.fixture(scope=\"module\")\ndef pipeline_cache_dir() -> str:\n    # path to _cache dir in same directory as this file\n    return os.path.join(os.path.dirname(__file__), \"_pipeline_cache\")\n\n\n@pytest.fixture(scope=\"module\")\ndef df() -> pd.DataFrame:\n    return pd.DataFrame({\"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [\"a\"] * 5 + [\"b\"] * 5})", "\n@pytest.fixture(scope=\"module\")\ndef df() -> pd.DataFrame:\n    return pd.DataFrame({\"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [\"a\"] * 5 + [\"b\"] * 5})\n\n\n@pytest.fixture(scope=\"module\")\ndef df_eval() -> pd.DataFrame:\n    return pd.DataFrame({\"x\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], \"y\": [\"c\"] * 5 + [\"d\"] * 5})\n", "\n\n@pytest.fixture(scope=\"module\")\ndef df_path(pipeline_cache_dir, df) -> str:\n    filepath = os.path.join(pipeline_cache_dir, \"data\", \"df.csv\")\n    df.to_csv(filepath, index=False)\n    return filepath\n\n\n@pytest.fixture(scope=\"module\")\ndef df_eval_path(pipeline_cache_dir, df_eval) -> str:\n    filepath = os.path.join(pipeline_cache_dir, \"data\", \"df_eval.csv\")\n    df_eval.to_csv(filepath, index=False)\n    return filepath", "\n@pytest.fixture(scope=\"module\")\ndef df_eval_path(pipeline_cache_dir, df_eval) -> str:\n    filepath = os.path.join(pipeline_cache_dir, \"data\", \"df_eval.csv\")\n    df_eval.to_csv(filepath, index=False)\n    return filepath\n\n\ndef test_pipeline(client, pipeline_cache_dir, df, df_path, df_eval_path):\n    # create a genie to be converted into a pipeline\n    genie = Genie(data=df, cache_dir=pipeline_cache_dir, client=client)\n    multiplier = 2\n    gr_grp = genie.plz(\"create a df with mean values of x grouped by y\")\n    genie = Genie(data=gr_grp.result, cache_dir=pipeline_cache_dir, client=client)\n    gr_mul = genie.plz(\"multiply values of x by multiplier\", additional_inputs={\"multiplier\": multiplier})\n    result_values = gr_mul.result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n    assert result_values == {\"a\": 3.0 * multiplier, \"b\": 8.0 * multiplier}\n\n    # create a pipeline which takes the input as a df, runs the genie and then stores the result locally\n    source_path_key = \"df-source-path\"\n    sink_path_key = \"df-sink-path\"\n    multiplier_key = \"multiplier\"\n    source = CsvToDataFrameSource(path=StringArg(name=source_path_key))\n    sink = DataFrameToCsvSink(path=StringArg(name=sink_path_key))\n    steps = [\n        PipelineStep(\n            genie_result=gr_grp,\n            data=source,\n        ),\n        PipelineStep(\n            genie_result=gr_mul,\n            data=gr_grp,\n            additional_inputs={multiplier_key: IntArg(name=multiplier_key)},\n            sink=sink,\n        ),\n    ]\n    pipeline = GeniePipeline(name=\"test-pipe\", version=\"1\", cache_dir=pipeline_cache_dir, steps=steps)\n    pipeline.export(\"test-pipe.json\")\n\n    pipeline_imported = GeniePipeline.load(os.path.join(pipeline_cache_dir, \"test-pipe.json\"))\n    assert pipeline_imported == pipeline\n\n    # now we will run this pipeline with a new df and multiplier and check results\n    sink_path = os.path.join(pipeline_cache_dir, \"data\", \"df_eval_result.csv\")\n    pipeline_imported.run({source_path_key: df_eval_path, multiplier_key: 5, sink_path_key: sink_path})\n\n    # read eval df and make sure it is correct\n    df_eval_result = pd.read_csv(sink_path)\n    result_values_eval = df_eval_result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n    assert result_values_eval == {\"c\": 150.0, \"d\": 400.0}", "def test_pipeline(client, pipeline_cache_dir, df, df_path, df_eval_path):\n    # create a genie to be converted into a pipeline\n    genie = Genie(data=df, cache_dir=pipeline_cache_dir, client=client)\n    multiplier = 2\n    gr_grp = genie.plz(\"create a df with mean values of x grouped by y\")\n    genie = Genie(data=gr_grp.result, cache_dir=pipeline_cache_dir, client=client)\n    gr_mul = genie.plz(\"multiply values of x by multiplier\", additional_inputs={\"multiplier\": multiplier})\n    result_values = gr_mul.result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n    assert result_values == {\"a\": 3.0 * multiplier, \"b\": 8.0 * multiplier}\n\n    # create a pipeline which takes the input as a df, runs the genie and then stores the result locally\n    source_path_key = \"df-source-path\"\n    sink_path_key = \"df-sink-path\"\n    multiplier_key = \"multiplier\"\n    source = CsvToDataFrameSource(path=StringArg(name=source_path_key))\n    sink = DataFrameToCsvSink(path=StringArg(name=sink_path_key))\n    steps = [\n        PipelineStep(\n            genie_result=gr_grp,\n            data=source,\n        ),\n        PipelineStep(\n            genie_result=gr_mul,\n            data=gr_grp,\n            additional_inputs={multiplier_key: IntArg(name=multiplier_key)},\n            sink=sink,\n        ),\n    ]\n    pipeline = GeniePipeline(name=\"test-pipe\", version=\"1\", cache_dir=pipeline_cache_dir, steps=steps)\n    pipeline.export(\"test-pipe.json\")\n\n    pipeline_imported = GeniePipeline.load(os.path.join(pipeline_cache_dir, \"test-pipe.json\"))\n    assert pipeline_imported == pipeline\n\n    # now we will run this pipeline with a new df and multiplier and check results\n    sink_path = os.path.join(pipeline_cache_dir, \"data\", \"df_eval_result.csv\")\n    pipeline_imported.run({source_path_key: df_eval_path, multiplier_key: 5, sink_path_key: sink_path})\n\n    # read eval df and make sure it is correct\n    df_eval_result = pd.read_csv(sink_path)\n    result_values_eval = df_eval_result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n    assert result_values_eval == {\"c\": 150.0, \"d\": 400.0}", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\n\nimport pytest\nfrom dotenv import load_dotenv\n\nfrom code_genie.client import Client\n\n\n@pytest.fixture(scope=\"module\")\ndef client():\n    # path to 2 directories above current file\n    path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \".env\")\n    load_dotenv(path)\n    return Client()", "@pytest.fixture(scope=\"module\")\ndef client():\n    # path to 2 directories above current file\n    path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \".env\")\n    load_dotenv(path)\n    return Client()\n\n\n@pytest.fixture(scope=\"module\")\ndef cache_dir():\n    # path to _cache dir in same directory as this file\n    return os.path.join(os.path.dirname(__file__), \"_cache\")", "@pytest.fixture(scope=\"module\")\ndef cache_dir():\n    # path to _cache dir in same directory as this file\n    return os.path.join(os.path.dirname(__file__), \"_cache\")\n"]}
{"filename": "tests/test_genie.py", "chunked_list": ["import pandas as pd\nimport pytest\n\nfrom code_genie import Genie\n\n\n@pytest.fixture(scope=\"session\")\ndef df():\n    return pd.DataFrame({\"x\": [1, 2, 3, 1, 2, 3], \"y\": [4, 5, 6, 4, 5, 6]})\n", "\n\ndef test_math(client):\n    # use genie to get a method to add 2 numbers\n    genie = Genie(data=[10, 20, 30], client=client)\n\n    # call the method\n    assert genie.plz(instructions=\"add all inputs\").result == 60\n    assert genie.plz(instructions=\"multiply all inputs\").result == 6000\n", "\n\ndef test_math_additional_inputs(client):\n    # use genie to get a method to add 2 numbers\n    genie = Genie(data=4, client=client, copy_data_before_use=False)\n\n    # call the method\n    additional_input = {\"y\": 2}\n    assert genie.plz(instructions=\"add data and y\", additional_inputs=additional_input).result == 6\n    assert genie.plz(instructions=\"multiply data and y\", additional_inputs=additional_input).result == 8", "\n\ndef test_pd_mean(client, df):\n    genie = Genie(data=df, client=client)\n\n    # call the method\n    assert genie.plz(instructions=\"sum of mean of x and y\").result == 7\n    assert set(genie.plz(instructions=\"distinct values of x\").result) == {1, 2, 3}\n\n\ndef test_custom(client, df):\n    genie = Genie(data=df, client=client)\n\n    # call the method\n    code = \"\"\"\ndef run(df):\n    return df.x.unique() \n    \"\"\"\n    assert set(genie.custom(code=code).result) == {1, 2, 3}", "\n\ndef test_custom(client, df):\n    genie = Genie(data=df, client=client)\n\n    # call the method\n    code = \"\"\"\ndef run(df):\n    return df.x.unique() \n    \"\"\"\n    assert set(genie.custom(code=code).result) == {1, 2, 3}", "\n\ndef test_cache(client):\n    # use genie to get a method to add 2 numbers\n    genie = Genie(data=[1, 2, 3, 4], client=client)\n\n    # call the method\n    gr_1 = genie.plz(instructions=\"add all elements of input\")\n    assert gr_1.result == 10\n\n    gr_2 = genie.plz(instructions=\"add all elements of input\")\n    assert gr_2.result == 10\n    assert gr_1.id == gr_2.id\n\n    gr_3 = genie.plz(instructions=\"add all elements of input\", override=True)\n    assert gr_3.result == 10\n    assert gr_3.id != gr_1.id", "\n\ndef test_pd_copy_before_use(client):\n    df = pd.DataFrame({\"x\": [1, 2, 3, 1, 2, 3], \"y\": [4, 5, 6, 4, 5, 6]})\n    genie = Genie(data=df, client=client)\n\n    # call the method\n    gr = genie.plz(instructions=\"drop column x\")\n    assert set(gr.result.columns) == {\"y\"}\n    assert set(df.columns) == {\"x\", \"y\"}", ""]}
{"filename": "tests/_pipeline_cache/multiply_df_98613.py", "chunked_list": ["import pandas as pd\n\n\ndef multiply_df(df: pd.DataFrame, multiplier: int) -> pd.DataFrame:\n    df[\"x\"] = df[\"x\"] * multiplier\n    return df\n"]}
{"filename": "tests/_pipeline_cache/create_means_df_73453.py", "chunked_list": ["import pandas as pd\n\n\ndef create_means_df(df):\n    means_df = df.groupby(\"y\").mean()\n    return means_df\n"]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nimport os\nimport sys", "import os\nimport sys\n\nsys.path.insert(0, os.path.abspath('..'))\n\nproject = 'code-genie'\ncopyright = '2023, Aarshay Jain'\nauthor = 'Aarshay Jain'\n\n# -- General configuration ---------------------------------------------------", "\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = ['sphinx.ext.napoleon',\n              'sphinx.ext.autodoc',\n              'sphinx.ext.viewcode',\n              'sphinx.ext.autosummary',\n              \"nbsphinx\",\n              \"sphinx_gallery.load_style\",", "              \"nbsphinx\",\n              \"sphinx_gallery.load_style\",\n              \"sphinxcontrib.autodoc_pydantic\"]\ntemplates_path = ['_templates']\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = False\n", "napoleon_numpy_docstring = False\n\n# autodoc-pydantic settings\nautodoc_pydantic_model_show_json = False\nautodoc_pydantic_model_show_config = False\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'sphinx_rtd_theme'", "\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = ['_static']\n"]}
{"filename": "docs/notebooks/_cache_starter/remote_ratio_by_work_years_96078.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport pandas as pd\n\ndef remote_ratio_by_work_years(df):\n    \"\"\"\n    This function creates a bar chart of percentage counts of remote_ratio grouped by work_years.\n\n    Parameters:\n    df (pd.DataFrame): pandas dataframe with columns 'work_year', 'experience_level', 'employment_type',\n                        'job_title', 'salary', 'employee_residence', 'remote_ratio', 'company_location', \n                        'company_size'.\n                        \n    Returns:\n    None (displays a bar chart)\n    \"\"\"\n    # Group by work year and remote ratio to get counts\n    counts = df.groupby(['work_year', 'remote_ratio']).size().reset_index(name='count')\n    \n    # Group by work year to get total count for each year\n    total_counts = counts.groupby('work_year').agg({'count': 'sum'}).reset_index()\n    total_counts.rename(columns={'count': 'total_count'}, inplace=True)\n    \n    # Merge counts with total counts\n    counts = counts.merge(total_counts, on=['work_year'])\n    \n    # Calculate percentage counts\n    counts['percentage_count'] = counts['count'] / counts['total_count'] * 100\n    \n    # Pivot the table to create bar chart\n    counts_pivot = counts.pivot(index='work_year', columns='remote_ratio', values='percentage_count')\n    \n    # Create the bar chart\n    ax = counts_pivot.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_xlabel('Work Years')\n    ax.set_ylabel('Percentage Count')\n    ax.set_title('Remote Ratio by Work Years')\n    plt.show()", ""]}
{"filename": "docs/notebooks/_cache_starter/plot_salary_distribution_51861.py", "chunked_list": ["import seaborn as sns\n\ndef plot_salary_distribution(df):\n    sns.set(style=\"whitegrid\")\n    sns.displot(data=df, x=\"salary\", hue=\"work_year\", kind=\"kde\", fill=True)\n"]}
{"filename": "docs/notebooks/_cache_starter/pie_chart_47446.py", "chunked_list": ["import matplotlib.pyplot as plt\n\ndef pie_chart(df):\n    plt.pie(df['work_year'].value_counts(), labels=df['work_year'].unique())\n    plt.title('Distribution of work year')\n    plt.show()"]}
{"filename": "docs/notebooks/_cache_starter/unique_remote_ratio_counts_63145.py", "chunked_list": ["def unique_remote_ratio_counts(df):\n    print(df[\"remote_ratio\"].value_counts())\n"]}
{"filename": "docs/notebooks/_cache_starter/count_job_designations_83350.py", "chunked_list": ["def count_job_designations(df):\n    num_job_designations = df['job_title'].nunique()\n    return num_job_designations"]}
{"filename": "docs/notebooks/_cache_starter/process_dataframe_88292.py", "chunked_list": ["import pandas as pd\n\ndef process_dataframe(df):\n    df = df.drop(columns=['salary', 'salary_currency'])\n    df = df.rename(columns={\"salary_in_usd\": \"salary\"})\n    return df"]}
{"filename": "docs/notebooks/_cache_starter/remote_ratio_mapper_69067.py", "chunked_list": ["import pandas as pd\n\ndef remote_ratio_mapper(df):\n    df['remote_ratio'].replace({0: 'No Remote Work', 50: 'Partially Remote', 100: 'Fully Remote'}, inplace=True)\n    return df\n"]}
{"filename": "docs/notebooks/_cache_starter/plot_emp_type_experience_level_31165.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_emp_type_experience_level(df):\n    emp_types = df['employment_type'].unique()\n    fig, axes = plt.subplots(1, len(emp_types), figsize=(15,5))\n    for i, emp_type in enumerate(emp_types):\n        data = df[df['employment_type'] == emp_type]['experience_level'].value_counts()\n        axes[i].bar(data.index, data.values)\n        axes[i].set_title(emp_type)\n        axes[i].tick_params(axis='x', rotation=45)\n    plt.show()", ""]}
{"filename": "docs/notebooks/_cache_starter/map_experience_level_48687.py", "chunked_list": ["import pandas as pd\n\ndef map_experience_level(df):\n    mapping = {'EN': 'Entry-level / Junior', 'MI': 'Mid-level / Intermediate', \n               'SE': 'Senior-level / Expert', 'EX': 'Executive-level / Director'}\n    df['experience_level'] = df['experience_level'].map(mapping)\n    return df"]}
{"filename": "docs/notebooks/_cache_starter/count_missing_values_26226.py", "chunked_list": ["import pandas as pd\n\ndef count_missing_values(df):\n    return df.isnull().sum()\n"]}
{"filename": "docs/notebooks/_cache_starter/make_wordcloud_51022.py", "chunked_list": ["import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef make_wordcloud(df):\n    # define the frequency of each job title\n    job_title_freq = df['job_title'].value_counts(normalize=True)\n\n    # create the wordcloud object\n    wc = WordCloud(width=800, height=600, background_color='white', max_words=50)\n\n    # generate the wordcloud\n    wc.generate_from_frequencies(job_title_freq)\n\n    # plot the wordcloud\n    plt.figure(figsize=(12, 8))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()", ""]}
{"filename": "docs/notebooks/_cache_starter/unique_experience_count_96626.py", "chunked_list": ["import pandas as pd\n\ndef unique_experience_count(df):\n    experience_counts = df['experience_level'].value_counts()\n    print(experience_counts)\n"]}
{"filename": "docs/notebooks/_cache_starter/top_jobs_41202.py", "chunked_list": ["import matplotlib.pyplot as plt\n\ndef top_jobs(df):\n    top_jobs = df['job_title'].value_counts()[:15] # Get top 15 most frequently occurring job designations\n    plt.bar(top_jobs.index, top_jobs.values) # Make bar chart\n    plt.xticks(rotation=90) # Rotate labels\n    for i, v in enumerate(top_jobs.values):\n        plt.text(i, v+5, str(v), ha='center') # Print count on top of each bar\n    plt.show() \n", ""]}
{"filename": "docs/notebooks/_cache_starter/create_boxplot_24032.py", "chunked_list": ["import matplotlib.pyplot as plt\n\ndef create_boxplot(df):\n    plt.boxplot(df['salary'])\n    plt.show()\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/aggregate_data_92888.py", "chunked_list": ["import pandas as pd\n\ndef aggregate_data(df):\n    df_filtered = df[df['hit_type']=='PAGE']\n    df_grouped = df_filtered.groupby('page_path').agg({'hit_type':'count','bounces':'mean'})\n    df_grouped.columns = ['views','exit_rate']\n    df_sorted = df_grouped.sort_values(by='views', ascending=False)\n    return df_sorted\n", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/group_action_type_27490.py", "chunked_list": ["import pandas as pd\n\ndef group_action_type(df):\n    # group data by action_type and count number of rows in each group\n    action_counts = df.groupby('action_type').size().reset_index(name='count')\n    \n    # remove action_type other than 1, 2, 5, 6\n    action_counts = action_counts[action_counts['action_type'].isin([1, 2, 5, 6])]\n    \n    # replace the action_type values as:\n    # 1: Click on product list page\n    # 2: Product details page\n    # 5: Checkout\n    # 6: Purchase Complete\n    action_counts['action_type'] = action_counts['action_type'].replace({1: 'Click on product list page', 2: 'Product details page', 5: 'Checkout', 6: 'Purchase Complete'})\n    \n    return action_counts", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/get_page_bounce_rate_81411.py", "chunked_list": ["import pandas as pd\n\ndef get_page_bounce_rate(df):\n    hit1_page_df = df[(df[\"hit_number\"] == 1) & (df[\"hit_type\"] == \"PAGE\")]\n    page_agg_df = hit1_page_df.groupby(\"page_path\").agg(\n        views=pd.NamedAgg(column=\"page_path\", aggfunc=\"count\"),\n        bounce_rate=pd.NamedAgg(column=\"bounces\", aggfunc=lambda x: sum(x)/len(x))\n    ).sort_values(\"views\", ascending=False)\n    return page_agg_df\n", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_hit_type_49083.py", "chunked_list": ["import pandas as pd\n\ndef extract_hit_type(df):\n    \n    # Loop through the hits column and extract the hit type from the dictionary\n    hit_types = []\n    for row in df['hits']:\n        hit_type = row['type']\n        hit_types.append(hit_type)\n    \n    # Create a new column called hit_type with the extracted hit types\n    df['hit_type'] = hit_types\n    \n    return df", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_hit_number_77980.py", "chunked_list": ["import pandas as pd\n\ndef extract_hit_number(df):\n    df['hit_number'] = df['hits'].apply(lambda x: x.get('hitNumber'))\n    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/run_51794.py", "chunked_list": ["\ndef run(df):\n    df[\"hit_type\"] = df[\"hits\"].apply(lambda x: x[\"type\"])\n    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_action_type_93153.py", "chunked_list": ["import pandas as pd\n\ndef extract_action_type(df):\n    hits_list = []\n    \n    for row in df.itertuples(index=False):\n        hits_dict = row.hits\n        if 'eCommerceAction' in hits_dict.keys():\n            ecommerce_dict = hits_dict['eCommerceAction']\n            if 'action_type' in ecommerce_dict.keys():\n                hits_dict.update({'action_type': ecommerce_dict['action_type']})\n            else:\n                hits_dict.update({'action_type': None})\n        else:\n            hits_dict.update({'action_type': None})\n        hits_list.append(hits_dict)\n    \n    hits_df = pd.DataFrame(hits_list)\n\n    df = pd.concat([df, hits_df['action_type']], axis=1)\n    return df", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/run_20093.py", "chunked_list": ["\ndef run(df):\n    df[\"action_type\"] = df[\"hits\"].apply(lambda x: int(x[\"eCommerceAction\"][\"action_type\"]))\n    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_page_path_78866.py", "chunked_list": ["import pandas as pd\n\ndef extract_page_path(df):\n    # extract page path\n    df['page_path'] = df['hits'].apply(lambda x: x['page']['pagePath'])\n    \n    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/separate_hits_70651.py", "chunked_list": ["import pandas as pd\n\ndef separate_hits(df):\n    \"\"\"\n    Input:\n    df: pandas dataframe with columns: Index(['visitorId', 'visitNumber', 'visitId', 'visitStartTime', 'date',\n       'totals', 'trafficSource', 'device', 'geoNetwork', 'customDimensions',\n       'hits', 'fullVisitorId', 'userId', 'clientId', 'channelGrouping',\n       'socialEngagementType'],\n      dtype='object')\n    \n    Output:\n    new_df: pandas dataframe with separate rows for each item in the hits column and the same value for each item in the totals column\n    \"\"\"\n    \n    # create new dataframe with separate row for each item in hits column\n    new_df = df.explode('hits')\n    \n    # drop columns except hits and totals\n    new_df = new_df[['hits', 'totals']]\n    \n    return new_df", ""]}
{"filename": "docs/notebooks/_cache_google_analytics/add_bounces_column_93508.py", "chunked_list": ["import pandas as pd\n\ndef add_bounces_column(df):\n    df['bounces'] = df['totals'].apply(lambda x: True if 'bounces' in x and x['bounces'] == 1 else False)\n    return df\n"]}
