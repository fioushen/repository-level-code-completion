{"filename": "scripts/process_zip/process_zip.py", "chunked_list": ["import uuid\nimport zipfile\nimport os\nimport json\nimport argparse\nimport asyncio\n\nfrom models.models import Document, DocumentMetadata, Source\nfrom datastore.datastore import DataStore\nfrom datastore.factory import get_datastore", "from datastore.datastore import DataStore\nfrom datastore.factory import get_datastore\nfrom services.extract_metadata import extract_metadata_from_document\nfrom services.file import extract_text_from_filepath\nfrom services.pii_detection import screen_text_for_pii\n\nDOCUMENT_UPSERT_BATCH_SIZE = 50\n\n\nasync def process_file_dump(", "\nasync def process_file_dump(\n    filepath: str,\n    datastore: DataStore,\n    custom_metadata: dict,\n    screen_for_pii: bool,\n    extract_metadata: bool,\n):\n    # create a ZipFile object and extract all the files into a directory named 'dump'\n    with zipfile.ZipFile(filepath) as zip_file:\n        zip_file.extractall(\"dump\")", "    # create a ZipFile object and extract all the files into a directory named 'dump'\n    with zipfile.ZipFile(filepath) as zip_file:\n        zip_file.extractall(\"dump\")\n\n    documents = []\n    skipped_files = []\n    # use os.walk to traverse the dump directory and its subdirectories\n    for root, dirs, files in os.walk(\"dump\"):\n        for filename in files:\n            if len(documents) % 20 == 0:\n                print(f\"Processed {len(documents)} documents\")\n\n            filepath = os.path.join(root, filename)\n\n            try:\n                extracted_text = extract_text_from_filepath(filepath)\n                print(f\"extracted_text from {filepath}\")\n\n                # create a metadata object with the source and source_id fields\n                metadata = DocumentMetadata(\n                    source=Source.file,\n                    source_id=filename,\n                )\n\n                # update metadata with custom values\n                for key, value in custom_metadata.items():\n                    if hasattr(metadata, key):\n                        setattr(metadata, key, value)\n\n                # screen for pii if requested\n                if screen_for_pii:\n                    pii_detected = screen_text_for_pii(extracted_text)\n                    # if pii detected, print a warning and skip the document\n                    if pii_detected:\n                        print(\"PII detected in document, skipping\")\n                        skipped_files.append(\n                            filepath\n                        )  # add the skipped file to the list\n                        continue\n\n                # extract metadata if requested\n                if extract_metadata:\n                    # extract metadata from the document text\n                    extracted_metadata = extract_metadata_from_document(\n                        f\"Text: {extracted_text}; Metadata: {str(metadata)}\"\n                    )\n                    # get a Metadata object from the extracted metadata\n                    metadata = DocumentMetadata(**extracted_metadata)\n\n                # create a document object with a random id, text and metadata\n                document = Document(\n                    id=str(uuid.uuid4()),\n                    text=extracted_text,\n                    metadata=metadata,\n                )\n                documents.append(document)\n            except Exception as e:\n                # log the error and continue with the next file\n                print(f\"Error processing {filepath}: {e}\")\n                skipped_files.append(filepath)  # add the skipped file to the list", "\n    # do this in batches, the upsert method already batches documents but this allows\n    # us to add more descriptive logging\n    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n        # Get the text of the chunks in the current batch\n        batch_documents = [doc for doc in documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]]\n        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n        print(\"documents: \", documents)\n        await datastore.upsert(batch_documents)\n", "\n    # delete all files in the dump directory\n    for root, dirs, files in os.walk(\"dump\", topdown=False):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            os.remove(filepath)\n        for dirname in dirs:\n            dirpath = os.path.join(root, dirname)\n            os.rmdir(dirpath)\n", "\n    # delete the dump directory\n    os.rmdir(\"dump\")\n\n    # print the skipped files\n    print(f\"Skipped {len(skipped_files)} files due to errors or PII detection\")\n    for file in skipped_files:\n        print(file)\n\n", "\n\nasync def main():\n    # parse the command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--filepath\", required=True, help=\"The path to the file dump\")\n    parser.add_argument(\n        \"--custom_metadata\",\n        default=\"{}\",\n        help=\"A JSON string of key-value pairs to update the metadata of the documents\",", "        default=\"{}\",\n        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n    )\n    parser.add_argument(\n        \"--screen_for_pii\",\n        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to the PII detection function (using a language model)\",\n    )\n    parser.add_argument(", "    )\n    parser.add_argument(\n        \"--extract_metadata\",\n        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n    )\n    args = parser.parse_args()\n\n    # get the arguments", "\n    # get the arguments\n    filepath = args.filepath\n    custom_metadata = json.loads(args.custom_metadata)\n    screen_for_pii = args.screen_for_pii\n    extract_metadata = args.extract_metadata\n\n    # initialize the db instance once as a global variable\n    datastore = await get_datastore()\n    # process the file dump", "    datastore = await get_datastore()\n    # process the file dump\n    await process_file_dump(\n        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n", ""]}
{"filename": "scripts/process_jsonl/process_jsonl.py", "chunked_list": ["import uuid\nimport json\nimport argparse\nimport asyncio\n\nfrom models.models import Document, DocumentMetadata, Source\nfrom datastore.datastore import DataStore\nfrom datastore.factory import get_datastore\nfrom services.extract_metadata import extract_metadata_from_document\nfrom services.pii_detection import screen_text_for_pii", "from services.extract_metadata import extract_metadata_from_document\nfrom services.pii_detection import screen_text_for_pii\n\nDOCUMENT_UPSERT_BATCH_SIZE = 50\n\n\nasync def process_jsonl_dump(\n    filepath: str,\n    datastore: DataStore,\n    custom_metadata: dict,", "    datastore: DataStore,\n    custom_metadata: dict,\n    screen_for_pii: bool,\n    extract_metadata: bool,\n):\n    # open the jsonl file as a generator of dictionaries\n    with open(filepath) as jsonl_file:\n        data = [json.loads(line) for line in jsonl_file]\n\n    documents = []", "\n    documents = []\n    skipped_items = []\n    # iterate over the data and create document objects\n    for item in data:\n        if len(documents) % 20 == 0:\n            print(f\"Processed {len(documents)} documents\")\n\n        try:\n            # get the id, text, source, source_id, url, created_at and author from the item\n            # use default values if not specified\n            id = item.get(\"id\", None)\n            text = item.get(\"text\", None)\n            source = item.get(\"source\", None)\n            source_id = item.get(\"source_id\", None)\n            url = item.get(\"url\", None)\n            created_at = item.get(\"created_at\", None)\n            author = item.get(\"author\", None)\n\n            if not text:\n                print(\"No document text, skipping...\")\n                continue\n\n            # create a metadata object with the source, source_id, url, created_at and author\n            metadata = DocumentMetadata(\n                source=source,\n                source_id=source_id,\n                url=url,\n                created_at=created_at,\n                author=author,\n            )\n\n            # update metadata with custom values\n            for key, value in custom_metadata.items():\n                if hasattr(metadata, key):\n                    setattr(metadata, key, value)\n\n            # screen for pii if requested\n            if screen_for_pii:\n                pii_detected = screen_text_for_pii(text)\n                # if pii detected, print a warning and skip the document\n                if pii_detected:\n                    print(\"PII detected in document, skipping\")\n                    skipped_items.append(item)  # add the skipped item to the list\n                    continue\n\n            # extract metadata if requested\n            if extract_metadata:\n                # extract metadata from the document text\n                extracted_metadata = extract_metadata_from_document(\n                    f\"Text: {text}; Metadata: {str(metadata)}\"\n                )\n                # get a Metadata object from the extracted metadata\n                metadata = DocumentMetadata(**extracted_metadata)\n\n            # create a document object with the id, text and metadata\n            document = Document(\n                id=id,\n                text=text,\n                metadata=metadata,\n            )\n            documents.append(document)\n        except Exception as e:\n            # log the error and continue with the next item\n            print(f\"Error processing {item}: {e}\")\n            skipped_items.append(item)  # add the skipped item to the list", "\n    # do this in batches, the upsert method already batches documents but this allows\n    # us to add more descriptive logging\n    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n        # Get the text of the chunks in the current batch\n        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]\n        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n        await datastore.upsert(batch_documents)\n\n    # print the skipped items", "\n    # print the skipped items\n    print(f\"Skipped {len(skipped_items)} items due to errors or PII detection\")\n    for item in skipped_items:\n        print(item)\n\n\nasync def main():\n    # parse the command-line arguments\n    parser = argparse.ArgumentParser()", "    # parse the command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--filepath\", required=True, help=\"The path to the jsonl dump\")\n    parser.add_argument(\n        \"--custom_metadata\",\n        default=\"{}\",\n        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n    )\n    parser.add_argument(\n        \"--screen_for_pii\",", "    parser.add_argument(\n        \"--screen_for_pii\",\n        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to the PII detection function (using a language model)\",\n    )\n    parser.add_argument(\n        \"--extract_metadata\",\n        default=False,\n        type=bool,", "        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n    )\n    args = parser.parse_args()\n\n    # get the arguments\n    filepath = args.filepath\n    custom_metadata = json.loads(args.custom_metadata)\n    screen_for_pii = args.screen_for_pii", "    custom_metadata = json.loads(args.custom_metadata)\n    screen_for_pii = args.screen_for_pii\n    extract_metadata = args.extract_metadata\n\n    # initialize the db instance once as a global variable\n    datastore = await get_datastore()\n    # process the jsonl dump\n    await process_jsonl_dump(\n        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n    )", "        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"]}
{"filename": "scripts/process_json/process_json.py", "chunked_list": ["import uuid\nimport json\nimport argparse\nimport asyncio\n\nfrom models.models import Document, DocumentMetadata, Source\nfrom datastore.datastore import DataStore\nfrom datastore.factory import get_datastore\nfrom services.extract_metadata import extract_metadata_from_document\nfrom services.pii_detection import screen_text_for_pii", "from services.extract_metadata import extract_metadata_from_document\nfrom services.pii_detection import screen_text_for_pii\n\nDOCUMENT_UPSERT_BATCH_SIZE = 50\n\n\nasync def process_json_dump(\n    filepath: str,\n    datastore: DataStore,\n    custom_metadata: dict,", "    datastore: DataStore,\n    custom_metadata: dict,\n    screen_for_pii: bool,\n    extract_metadata: bool,\n):\n    # load the json file as a list of dictionaries\n    with open(filepath) as json_file:\n        data = json.load(json_file)\n\n    documents = []", "\n    documents = []\n    skipped_items = []\n    # iterate over the data and create document objects\n    for item in data:\n        if len(documents) % 20 == 0:\n            print(f\"Processed {len(documents)} documents\")\n\n        try:\n            # get the id, text, source, source_id, url, created_at and author from the item\n            # use default values if not specified\n            id = item.get(\"id\", None)\n            text = item.get(\"text\", None)\n            source = item.get(\"source\", None)\n            source_id = item.get(\"source_id\", None)\n            url = item.get(\"url\", None)\n            created_at = item.get(\"created_at\", None)\n            author = item.get(\"author\", None)\n\n            if not text:\n                print(\"No document text, skipping...\")\n                continue\n\n            # create a metadata object with the source, source_id, url, created_at and author\n            metadata = DocumentMetadata(\n                source=source,\n                source_id=source_id,\n                url=url,\n                created_at=created_at,\n                author=author,\n            )\n            print(\"metadata: \", str(metadata))\n\n            # update metadata with custom values\n            for key, value in custom_metadata.items():\n                if hasattr(metadata, key):\n                    setattr(metadata, key, value)\n\n            # screen for pii if requested\n            if screen_for_pii:\n                pii_detected = screen_text_for_pii(text)\n                # if pii detected, print a warning and skip the document\n                if pii_detected:\n                    print(\"PII detected in document, skipping\")\n                    skipped_items.append(item)  # add the skipped item to the list\n                    continue\n\n            # extract metadata if requested\n            if extract_metadata:\n                # extract metadata from the document text\n                extracted_metadata = extract_metadata_from_document(\n                    f\"Text: {text}; Metadata: {str(metadata)}\"\n                )\n                # get a Metadata object from the extracted metadata\n                metadata = DocumentMetadata(**extracted_metadata)\n\n            # create a document object with the id or a random id, text and metadata\n            document = Document(\n                id=id or str(uuid.uuid4()),\n                text=text,\n                metadata=metadata,\n            )\n            documents.append(document)\n        except Exception as e:\n            # log the error and continue with the next item\n            print(f\"Error processing {item}: {e}\")\n            skipped_items.append(item)  # add the skipped item to the list", "\n    # do this in batches, the upsert method already batches documents but this allows\n    # us to add more descriptive logging\n    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n        # Get the text of the chunks in the current batch\n        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]\n        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n        print(\"documents: \", documents)\n        await datastore.upsert(batch_documents)\n", "\n    # print the skipped items\n    print(f\"Skipped {len(skipped_items)} items due to errors or PII detection\")\n    for item in skipped_items:\n        print(item)\n\n\nasync def main():\n    # parse the command-line arguments\n    parser = argparse.ArgumentParser()", "    # parse the command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--filepath\", required=True, help=\"The path to the json dump\")\n    parser.add_argument(\n        \"--custom_metadata\",\n        default=\"{}\",\n        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n    )\n    parser.add_argument(\n        \"--screen_for_pii\",", "    parser.add_argument(\n        \"--screen_for_pii\",\n        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to the PII detection function (using a language model)\",\n    )\n    parser.add_argument(\n        \"--extract_metadata\",\n        default=False,\n        type=bool,", "        default=False,\n        type=bool,\n        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n    )\n    args = parser.parse_args()\n\n    # get the arguments\n    filepath = args.filepath\n    custom_metadata = json.loads(args.custom_metadata)\n    screen_for_pii = args.screen_for_pii", "    custom_metadata = json.loads(args.custom_metadata)\n    screen_for_pii = args.screen_for_pii\n    extract_metadata = args.extract_metadata\n\n    # initialize the db instance once as a global variable\n    datastore = await get_datastore()\n    # process the json dump\n    await process_json_dump(\n        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n    )", "        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/datastore/providers/weaviate/test_weaviate_datastore.py", "chunked_list": ["import pytest\nfrom fastapi.testclient import TestClient\nfrom weaviate import Client\nimport weaviate\nimport os\nfrom models.models import DocumentMetadataFilter\nfrom server.main import app\nfrom datastore.providers.weaviate_datastore import (\n    SCHEMA,\n    WeaviateDataStore,", "    SCHEMA,\n    WeaviateDataStore,\n    extract_schema_properties,\n)\nimport logging\nfrom loguru import logger\nfrom _pytest.logging import LogCaptureFixture\n\nBEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n", "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n\nclient = TestClient(app)\nclient.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n\n\n@pytest.fixture\ndef weaviate_client():\n    host = os.getenv(\"WEAVIATE_HOST\", \"http://localhost\")\n    port = os.getenv(\"WEAVIATE_PORT\", \"8080\")\n    client = Client(f\"{host}:{port}\")\n\n    yield client\n\n    client.schema.delete_all()", "\n\n@pytest.fixture\ndef test_db(weaviate_client, documents):\n    weaviate_client.schema.delete_all()\n    weaviate_client.schema.create_class(SCHEMA)\n\n    response = client.post(\"/upsert\", json={\"documents\": documents})\n\n    if response.status_code != 200:\n        raise Exception(\n            f\"Could not upsert to test client.\\nStatus Code: {response.status_code}\\nResponse:\\n{response.json()}\"\n        )\n\n    yield client", "\n\n@pytest.fixture\ndef documents():\n    documents = []\n\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    texts = [\n        \"lorem ipsum dolor sit amet\",\n        \"consectetur adipiscing elit\",\n        \"sed do eiusmod tempor incididunt\",\n    ]\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    sources = [\"chat\", \"email\", \"email\"]\n    created_at = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"2021-01-21T10:00:00-02:00\",\n    ]\n\n    for i in range(3):\n        documents.append(\n            {\n                \"id\": ids[i],\n                \"text\": texts[i],\n                \"metadata\": {\n                    \"source\": sources[i],\n                    \"source_id\": \"5325\",\n                    \"url\": \"http://example.com\",\n                    \"created_at\": created_at[i],\n                    \"author\": authors[i],\n                },\n            }\n        )\n\n    no_metadata_doc = {\n        \"id\": \"jkl_012\",\n        \"text\": \"no metadata\",\n    }\n\n    documents.append(no_metadata_doc)\n\n    partial_metadata_doc = {\n        \"id\": \"mno_345\",\n        \"text\": \"partial metadata\",\n        \"metadata\": {\n            \"source\": \"file\",\n        },\n    }\n\n    documents.append(partial_metadata_doc)\n\n    yield documents", "\n\n@pytest.fixture\ndef mock_env_public_access(monkeypatch):\n    monkeypatch.setattr(\n        \"datastore.providers.weaviate_datastore.WEAVIATE_USERNAME\", None\n    )\n    monkeypatch.setattr(\n        \"datastore.providers.weaviate_datastore.WEAVIATE_PASSWORD\", None\n    )", "\n\n@pytest.fixture\ndef mock_env_resource_owner_password_flow(monkeypatch):\n    monkeypatch.setattr(\n        \"datastore.providers.weaviate_datastore.WEAVIATE_SCOPES\",\n        [\"schema:read\", \"schema:write\"],\n    )\n    monkeypatch.setattr(\n        \"datastore.providers.weaviate_datastore.WEAVIATE_USERNAME\", \"admin\"\n    )\n    monkeypatch.setattr(\n        \"datastore.providers.weaviate_datastore.WEAVIATE_PASSWORD\", \"abc123\"\n    )", "\n\n@pytest.fixture\ndef caplog(caplog: LogCaptureFixture):\n    handler_id = logger.add(caplog.handler, format=\"{message}\")\n    yield caplog\n    logger.remove(handler_id)\n\n\n@pytest.mark.parametrize(", "\n@pytest.mark.parametrize(\n    \"document_id\", [(\"abc_123\"), (\"9a253e0b-d2df-5c2e-be6d-8e9b1f4ae345\")]\n)\ndef test_upsert(weaviate_client, document_id):\n    weaviate_client.schema.delete_all()\n    weaviate_client.schema.create_class(SCHEMA)\n\n    text = \"\"\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. \n    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem \n    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, \n    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. \n    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed \n    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, \n    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum \n    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.\n    \"\"\"\n    source = \"email\"\n    source_id = \"5325\"\n    url = \"http://example.com\"\n    created_at = \"2022-12-16T08:00:00+01:00\"\n    author = \"Max Mustermann\"\n\n    documents = {\n        \"documents\": [\n            {\n                \"id\": document_id,\n                \"text\": text,\n                \"metadata\": {\n                    \"source\": source,\n                    \"source_id\": source_id,\n                    \"url\": url,\n                    \"created_at\": created_at,\n                    \"author\": author,\n                },\n            }\n        ]\n    }\n\n    response = client.post(\"/upsert\", json=documents)\n\n    assert response.status_code == 200\n    assert response.json() == {\"ids\": [document_id]}\n\n    properties = [\n        \"chunk_id\",\n        \"document_id\",\n        \"source\",\n        \"source_id\",\n        \"url\",\n        \"created_at\",\n        \"author\",\n    ]\n\n    where_filter = {\n        \"path\": [\"document_id\"],\n        \"operator\": \"Equal\",\n        \"valueString\": document_id,\n    }\n\n    weaviate_doc = (\n        weaviate_client.query.get(\"OpenAIDocument\", properties)\n        .with_additional(\"vector\")\n        .with_where(where_filter)\n        .with_sort({\"path\": [\"chunk_id\"], \"order\": \"asc\"})\n        .do()\n    )\n\n    weaviate_docs = weaviate_doc[\"data\"][\"Get\"][\"OpenAIDocument\"]\n\n    assert len(weaviate_docs) == 2\n\n    for i, weaviate_doc in enumerate(weaviate_docs):\n        assert weaviate_doc[\"chunk_id\"] == f\"{document_id}_{i}\"\n\n        assert weaviate_doc[\"document_id\"] == document_id\n\n        assert weaviate_doc[\"source\"] == source\n        assert weaviate_doc[\"source_id\"] == source_id\n        assert weaviate_doc[\"url\"] == url\n        assert weaviate_doc[\"created_at\"] == created_at\n        assert weaviate_doc[\"author\"] == author\n\n        assert weaviate_doc[\"_additional\"][\"vector\"]", "\n\ndef test_upsert_no_metadata(weaviate_client):\n    weaviate_client.schema.delete_all()\n    weaviate_client.schema.create_class(SCHEMA)\n\n    no_metadata_doc = {\n        \"id\": \"jkl_012\",\n        \"text\": \"no metadata\",\n    }\n\n    metadata_properties = [\n        \"source\",\n        \"source_id\",\n        \"url\",\n        \"created_at\",\n        \"author\",\n    ]\n\n    response = client.post(\"/upsert\", json={\"documents\": [no_metadata_doc]})\n\n    assert response.status_code == 200\n\n    weaviate_doc = weaviate_client.query.get(\"OpenAIDocument\", metadata_properties).do()\n\n    weaviate_doc = weaviate_doc[\"data\"][\"Get\"][\"OpenAIDocument\"][0]\n\n    for _, metadata_value in weaviate_doc.items():\n        assert metadata_value is None", "\n\n@pytest.mark.parametrize(\n    \"test_document, expected_status_code\",\n    [\n        ({\"id\": \"abc_123\", \"text\": \"some text\"}, 200),\n        ({\"id\": \"abc_123\"}, 422),\n        ({\"text\": \"some text\"}, 200),\n    ],\n)\ndef test_upsert_invalid_documents(weaviate_client, test_document, expected_status_code):\n    weaviate_client.schema.delete_all()\n    weaviate_client.schema.create_class(SCHEMA)\n\n    response = client.post(\"/upsert\", json={\"documents\": [test_document]})\n\n    assert response.status_code == expected_status_code", "    ],\n)\ndef test_upsert_invalid_documents(weaviate_client, test_document, expected_status_code):\n    weaviate_client.schema.delete_all()\n    weaviate_client.schema.create_class(SCHEMA)\n\n    response = client.post(\"/upsert\", json={\"documents\": [test_document]})\n\n    assert response.status_code == expected_status_code\n", "\n\n@pytest.mark.parametrize(\n    \"query, expected_num_results\",\n    [\n        ({\"query\": \"consectetur adipiscing\", \"top_k\": 3}, 3),\n        ({\"query\": \"consectetur adipiscing elit\", \"filter\": {\"source\": \"email\"}}, 2),\n        (\n            {\n                \"query\": \"sed do eiusmod tempor\",", "            {\n                \"query\": \"sed do eiusmod tempor\",\n                \"filter\": {\n                    \"start_date\": \"2020-01-01T00:00:00Z\",\n                    \"end_date\": \"2022-12-31T00:00:00Z\",\n                },\n            },\n            1,\n        ),\n        (", "        ),\n        (\n            {\n                \"query\": \"some random query\",\n                \"filter\": {\"start_date\": \"2009-01-01T00:00:00Z\"},\n                \"top_k\": 3,\n            },\n            2,\n        ),\n        (", "        ),\n        (\n            {\n                \"query\": \"another random query\",\n                \"filter\": {\"end_date\": \"1929-12-31T00:00:00Z\"},\n                \"top_k\": 3,\n            },\n            1,\n        ),\n    ],", "        ),\n    ],\n)\ndef test_query(test_db, query, expected_num_results):\n    queries = {\"queries\": [query]}\n\n    response = client.post(\"/query\", json=queries)\n    assert response.status_code == 200\n\n    num_docs = response.json()[\"results\"][0][\"results\"]\n    assert len(num_docs) == expected_num_results", "\n\ndef test_delete(test_db, weaviate_client, caplog):\n    caplog.set_level(logging.DEBUG)\n\n    delete_request = {\"ids\": [\"def_456\"]}\n\n    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n    assert response.status_code == 200\n    assert response.json()[\"success\"]\n    assert weaviate_client.data_object.get()[\"totalResults\"] == 4\n\n    client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n    assert \"Failed to delete\" in caplog.text\n    caplog.clear()\n\n    delete_request = {\"filter\": {\"source\": \"email\"}}\n\n    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n    assert response.status_code == 200\n    assert response.json()[\"success\"]\n    assert weaviate_client.data_object.get()[\"totalResults\"] == 3\n\n    client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n    assert \"Failed to delete\" in caplog.text\n\n    delete_request = {\"delete_all\": True}\n\n    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n    assert response.status_code == 200\n    assert response.json()[\"success\"]\n    assert not weaviate_client.data_object.get()[\"objects\"]", "\n\ndef test_access_with_username_password(mock_env_resource_owner_password_flow):\n    auth_credentials = WeaviateDataStore._build_auth_credentials()\n\n    assert isinstance(auth_credentials, weaviate.auth.AuthClientPassword)\n\n\ndef test_public_access(mock_env_public_access):\n    auth_credentials = WeaviateDataStore._build_auth_credentials()\n\n    assert auth_credentials is None", "def test_public_access(mock_env_public_access):\n    auth_credentials = WeaviateDataStore._build_auth_credentials()\n\n    assert auth_credentials is None\n\n\ndef test_extract_schema_properties():\n    class_schema = {\n        \"class\": \"Question\",\n        \"description\": \"Information from a Jeopardy! question\",\n        \"properties\": [\n            {\n                \"dataType\": [\"text\"],\n                \"description\": \"The question\",\n                \"name\": \"question\",\n            },\n            {\n                \"dataType\": [\"text\"],\n                \"description\": \"The answer\",\n                \"name\": \"answer\",\n            },\n            {\n                \"dataType\": [\"text\"],\n                \"description\": \"The category\",\n                \"name\": \"category\",\n            },\n        ],\n        \"vectorizer\": \"text2vec-openai\",\n    }\n    results = extract_schema_properties(class_schema)\n    assert results == {\"question\", \"answer\", \"category\"}", "\n\ndef test_reuse_schema(weaviate_client, caplog):\n    caplog.set_level(logging.DEBUG)\n\n    weaviate_client.schema.delete_all()\n\n    WeaviateDataStore()\n    assert \"Creating index\" in caplog.text\n\n    WeaviateDataStore()\n    assert \"Will reuse this schema\" in caplog.text", "\n\ndef test_build_date_filters():\n    filter = DocumentMetadataFilter(\n        document_id=None,\n        source=None,\n        source_id=None,\n        author=None,\n        start_date=\"2020-01-01T00:00:00Z\",\n        end_date=\"2022-12-31T00:00:00Z\",\n    )\n    actual_result = WeaviateDataStore.build_filters(filter)\n    expected_result = {\n        \"operator\": \"And\",\n        \"operands\": [\n            {\n                \"path\": [\"created_at\"],\n                \"operator\": \"GreaterThanEqual\",\n                \"valueDate\": \"2020-01-01T00:00:00Z\",\n            },\n            {\n                \"path\": [\"created_at\"],\n                \"operator\": \"LessThanEqual\",\n                \"valueDate\": \"2022-12-31T00:00:00Z\",\n            },\n        ],\n    }\n\n    assert actual_result == expected_result", "\n\n@pytest.mark.parametrize(\n    \"test_input, expected_result\",\n    [\n        (\"abc_123\", False),\n        (\"b2e4133c-c956-5684-bbf5-584e50ec3647\", True),  # version 5\n        (\"f6179953-11d8-4ee0-9af8-e51e00dbf727\", True),  # version 4\n        (\"16fe8165-3c08-348f-a015-a8bb31e26b5c\", True),  # version 3\n        (\"bda85f97-be72-11ed-9291-00000000000a\", False),  # version 1", "        (\"16fe8165-3c08-348f-a015-a8bb31e26b5c\", True),  # version 3\n        (\"bda85f97-be72-11ed-9291-00000000000a\", False),  # version 1\n    ],\n)\ndef test_is_valid_weaviate_id(test_input, expected_result):\n    actual_result = WeaviateDataStore._is_valid_weaviate_id(test_input)\n    assert actual_result == expected_result\n\n\ndef test_upsert_same_docid(test_db, weaviate_client):\n    def get_doc_by_document_id(document_id):\n        properties = [\n            \"chunk_id\",\n            \"document_id\",\n            \"source\",\n            \"source_id\",\n            \"url\",\n            \"created_at\",\n            \"author\",\n        ]\n        where_filter = {\n            \"path\": [\"document_id\"],\n            \"operator\": \"Equal\",\n            \"valueString\": document_id,\n        }\n\n        results = (\n            weaviate_client.query.get(\"OpenAIDocument\", properties)\n            .with_additional(\"id\")\n            .with_where(where_filter)\n            .with_sort({\"path\": [\"chunk_id\"], \"order\": \"asc\"})\n            .do()\n        )\n\n        return results[\"data\"][\"Get\"][\"OpenAIDocument\"]\n\n    def build_upsert_payload(document):\n        return {\"documents\": [document]}\n\n    # upsert a new document\n    # this is a document that has 2 chunks and\n    # the source is email\n    doc_id = \"abc_123\"\n    text = \"\"\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. \n    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem \n    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, \n    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. \n    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed \n    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, \n    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum \n    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.\n    \"\"\"\n\n    document = {\n        \"id\": doc_id,\n        \"text\": text,\n        \"metadata\": {\"source\": \"email\"},\n    }\n\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 2\n    for chunk in weaviate_doc:\n        assert chunk[\"source\"] == \"email\"\n\n    # now update the source to file\n    # user still has to specify the text\n    # because test is a required field\n    document[\"metadata\"][\"source\"] = \"file\"\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 2\n    for chunk in weaviate_doc:\n        assert chunk[\"source\"] == \"file\"\n\n    # now update the text so that it is only 1 chunk\n    # user does not need to specify metadata\n    # since it is optional\n    document[\"text\"] = \"This is a short text\"\n    document.pop(\"metadata\")\n\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 1\n\n    # TODO: Implement update function\n    # but the source should still be file\n    # but it is None right now because an\n    # update function is out of scope\n    assert weaviate_doc[0][\"source\"] is None", "\ndef test_upsert_same_docid(test_db, weaviate_client):\n    def get_doc_by_document_id(document_id):\n        properties = [\n            \"chunk_id\",\n            \"document_id\",\n            \"source\",\n            \"source_id\",\n            \"url\",\n            \"created_at\",\n            \"author\",\n        ]\n        where_filter = {\n            \"path\": [\"document_id\"],\n            \"operator\": \"Equal\",\n            \"valueString\": document_id,\n        }\n\n        results = (\n            weaviate_client.query.get(\"OpenAIDocument\", properties)\n            .with_additional(\"id\")\n            .with_where(where_filter)\n            .with_sort({\"path\": [\"chunk_id\"], \"order\": \"asc\"})\n            .do()\n        )\n\n        return results[\"data\"][\"Get\"][\"OpenAIDocument\"]\n\n    def build_upsert_payload(document):\n        return {\"documents\": [document]}\n\n    # upsert a new document\n    # this is a document that has 2 chunks and\n    # the source is email\n    doc_id = \"abc_123\"\n    text = \"\"\"\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. \n    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem \n    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, \n    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. \n    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed \n    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, \n    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum \n    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.\n    \"\"\"\n\n    document = {\n        \"id\": doc_id,\n        \"text\": text,\n        \"metadata\": {\"source\": \"email\"},\n    }\n\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 2\n    for chunk in weaviate_doc:\n        assert chunk[\"source\"] == \"email\"\n\n    # now update the source to file\n    # user still has to specify the text\n    # because test is a required field\n    document[\"metadata\"][\"source\"] = \"file\"\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 2\n    for chunk in weaviate_doc:\n        assert chunk[\"source\"] == \"file\"\n\n    # now update the text so that it is only 1 chunk\n    # user does not need to specify metadata\n    # since it is optional\n    document[\"text\"] = \"This is a short text\"\n    document.pop(\"metadata\")\n\n    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n    assert response.status_code == 200\n    weaviate_doc = get_doc_by_document_id(doc_id)\n    assert len(weaviate_doc) == 1\n\n    # TODO: Implement update function\n    # but the source should still be file\n    # but it is None right now because an\n    # update function is out of scope\n    assert weaviate_doc[0][\"source\"] is None", ""]}
{"filename": "tests/datastore/providers/zilliz/test_zilliz_datastore.py", "chunked_list": ["# from pathlib import Path\n# from dotenv import find_dotenv, load_dotenv\n# env_path = Path(\".\") / \"zilliz.env\"\n# load_dotenv(dotenv_path=env_path, verbose=True)\n\nimport pytest\nfrom models.models import (\n    DocumentChunkMetadata,\n    DocumentMetadataFilter,\n    DocumentChunk,", "    DocumentMetadataFilter,\n    DocumentChunk,\n    Query,\n    QueryWithEmbedding,\n    Source,\n)\nfrom datastore.providers.zilliz_datastore import (\n    OUTPUT_DIM,\n    ZillizDataStore,\n)", "    ZillizDataStore,\n)\n\n\n@pytest.fixture\ndef zilliz_datastore():\n    return ZillizDataStore()\n\n\n@pytest.fixture\ndef document_chunk_one():\n    doc_id = \"zerp\"\n    doc_chunks = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"lorem ipsum dolor sit amet\",\n        \"consectetur adipiscing elit\",\n        \"sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"2021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks.append(chunk)\n\n    return {doc_id: doc_chunks}", "\n@pytest.fixture\ndef document_chunk_one():\n    doc_id = \"zerp\"\n    doc_chunks = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"lorem ipsum dolor sit amet\",\n        \"consectetur adipiscing elit\",\n        \"sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"2021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks.append(chunk)\n\n    return {doc_id: doc_chunks}", "\n\n@pytest.fixture\ndef document_chunk_two():\n    doc_id_1 = \"zerp\"\n    doc_chunks_1 = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"1lorem ipsum dolor sit amet\",\n        \"2consectetur adipiscing elit\",\n        \"3sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"3021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id_1,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks_1.append(chunk)\n\n    doc_id_2 = \"merp\"\n    doc_chunks_2 = []\n\n    ids = [\"jkl_123\", \"lmn_456\", \"opq_789\"]\n    texts = [\n        \"3sdsc efac feas sit qweas\",\n        \"4wert sdfas fdsc\",\n        \"52dsc fdsf eiusmod asdasd incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"4929-10-28T09:30:00-05:00\",\n        \"5009-01-03T16:39:57-08:00\",\n        \"6021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3, 6)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id_2,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks_2.append(chunk)\n\n    return {doc_id_1: doc_chunks_1, doc_id_2: doc_chunks_2}", "\n\n@pytest.mark.asyncio\nasync def test_upsert(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    assert 3 == zilliz_datastore.col.num_entities\n", "    assert 3 == zilliz_datastore.col.num_entities\n\n\n@pytest.mark.asyncio\nasync def test_reload(zilliz_datastore, document_chunk_one, document_chunk_two):\n    await zilliz_datastore.delete(delete_all=True)\n\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()", "    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    assert 3 == zilliz_datastore.col.num_entities\n    new_store = ZillizDataStore()\n    another_in = {i: document_chunk_two[i] for i in document_chunk_two if i != res[0]}\n    res = await new_store._upsert(another_in)\n    new_store.col.flush()\n    assert 6 == new_store.col.num_entities\n    query = QueryWithEmbedding(\n        query=\"lorem\",", "    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=10,\n        embedding=[0.5] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n    assert 1 == len(query_results)\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_upsert_and_query_all(zilliz_datastore, document_chunk_two):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_two)\n    assert res == list(document_chunk_two.keys())\n    zilliz_datastore.col.flush()\n\n    # Num entities currently doesnt track deletes\n    query = QueryWithEmbedding(", "    # Num entities currently doesnt track deletes\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0.5] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 6 == len(query_results[0].results)", "    assert 1 == len(query_results)\n    assert 6 == len(query_results[0].results)\n\n\n@pytest.mark.asyncio\nasync def test_query_accuracy(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()", "    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=1,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)", "\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert 0 == query_results[0].results[0].score\n    assert \"abc_123\" == query_results[0].results[0].id\n\n\n@pytest.mark.asyncio\nasync def test_query_filter(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)", "async def test_query_filter(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=1,\n        embedding=[0] * OUTPUT_DIM,\n        filter=DocumentMetadataFilter(", "        embedding=[0] * OUTPUT_DIM,\n        filter=DocumentMetadataFilter(\n            start_date=\"2000-01-03T16:39:57-08:00\", end_date=\"2010-01-03T16:39:57-08:00\"\n        ),\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert 0 != query_results[0].results[0].score", "    assert 1 == len(query_results[0].results)\n    assert 0 != query_results[0].results[0].score\n    assert \"def_456\" == query_results[0].results[0].id\n\n\n@pytest.mark.asyncio\nasync def test_delete_with_date_filter(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())", "    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    await zilliz_datastore.delete(\n        filter=DocumentMetadataFilter(\n            end_date=\"2009-01-03T16:39:57-08:00\",\n        )\n    )\n\n    query = QueryWithEmbedding(", "\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)", "    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert \"ghi_789\" == query_results[0].results[0].id\n\n\n@pytest.mark.asyncio\nasync def test_delete_with_source_filter(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())", "    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    await zilliz_datastore.delete(\n        filter=DocumentMetadataFilter(\n            source=Source.email,\n        )\n    )\n\n    query = QueryWithEmbedding(", "\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 2 == len(query_results[0].results)", "    assert 1 == len(query_results)\n    assert 2 == len(query_results[0].results)\n    assert \"def_456\" == query_results[0].results[0].id\n\n\n@pytest.mark.asyncio\nasync def test_delete_with_document_id_filter(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())", "    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    await zilliz_datastore.delete(\n        filter=DocumentMetadataFilter(\n            document_id=res[0],\n        )\n    )\n    query = QueryWithEmbedding(\n        query=\"lorem\",", "    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 0 == len(query_results[0].results)\n", "    assert 0 == len(query_results[0].results)\n\n\n@pytest.mark.asyncio\nasync def test_delete_with_document_id(zilliz_datastore, document_chunk_one):\n    await zilliz_datastore.delete(delete_all=True)\n    res = await zilliz_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    zilliz_datastore.col.flush()\n    await zilliz_datastore.delete([res[0]])", "    zilliz_datastore.col.flush()\n    await zilliz_datastore.delete([res[0]])\n\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await zilliz_datastore._query(queries=[query])\n", "    query_results = await zilliz_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 0 == len(query_results[0].results)\n\n\n# if __name__ == '__main__':\n#     import sys\n#     import pytest\n#     pytest.main(sys.argv)", "#     import pytest\n#     pytest.main(sys.argv)\n"]}
{"filename": "tests/datastore/providers/milvus/test_milvus_datastore.py", "chunked_list": ["# from pathlib import Path\n# from dotenv import find_dotenv, load_dotenv\n# env_path = Path(\".\") / \"milvus.env\"\n# load_dotenv(dotenv_path=env_path, verbose=True)\n\nimport pytest\nfrom models.models import (\n    DocumentChunkMetadata,\n    DocumentMetadataFilter,\n    DocumentChunk,", "    DocumentMetadataFilter,\n    DocumentChunk,\n    Query,\n    QueryWithEmbedding,\n    Source,\n)\nfrom datastore.providers.milvus_datastore import (\n    OUTPUT_DIM,\n    MilvusDataStore,\n)", "    MilvusDataStore,\n)\n\n\n@pytest.fixture\ndef milvus_datastore():\n    return MilvusDataStore()\n\n\n@pytest.fixture\ndef document_chunk_one():\n    doc_id = \"zerp\"\n    doc_chunks = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"lorem ipsum dolor sit amet\",\n        \"consectetur adipiscing elit\",\n        \"sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"2021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks.append(chunk)\n\n    return {doc_id: doc_chunks}", "\n@pytest.fixture\ndef document_chunk_one():\n    doc_id = \"zerp\"\n    doc_chunks = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"lorem ipsum dolor sit amet\",\n        \"consectetur adipiscing elit\",\n        \"sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"2021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks.append(chunk)\n\n    return {doc_id: doc_chunks}", "\n\n@pytest.fixture\ndef document_chunk_two():\n    doc_id_1 = \"zerp\"\n    doc_chunks_1 = []\n\n    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n    texts = [\n        \"1lorem ipsum dolor sit amet\",\n        \"2consectetur adipiscing elit\",\n        \"3sed do eiusmod tempor incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"1929-10-28T09:30:00-05:00\",\n        \"2009-01-03T16:39:57-08:00\",\n        \"3021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id_1,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks_1.append(chunk)\n\n    doc_id_2 = \"merp\"\n    doc_chunks_2 = []\n\n    ids = [\"jkl_123\", \"lmn_456\", \"opq_789\"]\n    texts = [\n        \"3sdsc efac feas sit qweas\",\n        \"4wert sdfas fdsc\",\n        \"52dsc fdsf eiusmod asdasd incididunt\",\n    ]\n    sources = [Source.email, Source.file, Source.chat]\n    source_ids = [\"foo\", \"bar\", \"baz\"]\n    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n    created_ats = [\n        \"4929-10-28T09:30:00-05:00\",\n        \"5009-01-03T16:39:57-08:00\",\n        \"6021-01-21T10:00:00-02:00\",\n    ]\n    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n    embeddings = [[x] * OUTPUT_DIM for x in range(3, 6)]\n\n    for i in range(3):\n        chunk = DocumentChunk(\n            id=ids[i],\n            text=texts[i],\n            metadata=DocumentChunkMetadata(\n                document_id=doc_id_2,\n                source=sources[i],\n                source_id=source_ids[i],\n                url=urls[i],\n                created_at=created_ats[i],\n                author=authors[i],\n            ),\n            embedding=embeddings[i],  # type: ignore\n        )\n\n        doc_chunks_2.append(chunk)\n\n    return {doc_id_1: doc_chunks_1, doc_id_2: doc_chunks_2}", "\n\n@pytest.mark.asyncio\nasync def test_upsert(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    assert 3 == milvus_datastore.col.num_entities\n   ", "    assert 3 == milvus_datastore.col.num_entities\n   \n\n\n@pytest.mark.asyncio\nasync def test_reload(milvus_datastore, document_chunk_one, document_chunk_two):\n    await milvus_datastore.delete(delete_all=True)\n\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())", "    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    assert 3 == milvus_datastore.col.num_entities\n    new_store = MilvusDataStore()\n    another_in = {i:document_chunk_two[i] for i in document_chunk_two if i!=res[0]}\n    res = await new_store._upsert(another_in)\n    new_store.col.flush()\n    assert 6 == new_store.col.num_entities\n    query = QueryWithEmbedding(", "    assert 6 == new_store.col.num_entities\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=10,\n        embedding=[0.5] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n    assert 1 == len(query_results)\n\n    ", "\n    \n\n\n\n@pytest.mark.asyncio\nasync def test_upsert_query_all(milvus_datastore, document_chunk_two):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_two)\n    assert res == list(document_chunk_two.keys())", "    res = await milvus_datastore._upsert(document_chunk_two)\n    assert res == list(document_chunk_two.keys())\n    milvus_datastore.col.flush()\n\n    # Num entities currently doesnt track deletes\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=10,\n        embedding=[0.5] * OUTPUT_DIM,\n    )", "        embedding=[0.5] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 6 == len(query_results[0].results)\n\n    \n\n", "\n\n@pytest.mark.asyncio\nasync def test_query_accuracy(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    query = QueryWithEmbedding(\n        query=\"lorem\",", "    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=1,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert 0 == query_results[0].results[0].score", "    assert 1 == len(query_results[0].results)\n    assert 0 == query_results[0].results[0].score\n    assert \"abc_123\" == query_results[0].results[0].id\n    \n\n\n@pytest.mark.asyncio\nasync def test_query_filter(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)", "    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=1,\n        embedding=[0] * OUTPUT_DIM,\n        filter=DocumentMetadataFilter(\n            start_date=\"2000-01-03T16:39:57-08:00\", end_date=\"2010-01-03T16:39:57-08:00\"", "        filter=DocumentMetadataFilter(\n            start_date=\"2000-01-03T16:39:57-08:00\", end_date=\"2010-01-03T16:39:57-08:00\"\n        ),\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert 0 != query_results[0].results[0].score\n    assert \"def_456\" == query_results[0].results[0].id", "    assert 0 != query_results[0].results[0].score\n    assert \"def_456\" == query_results[0].results[0].id\n    \n\n\n@pytest.mark.asyncio\nasync def test_delete_with_date_filter(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())", "    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    await milvus_datastore.delete(\n        filter=DocumentMetadataFilter(\n            end_date=\"2009-01-03T16:39:57-08:00\",\n        )\n    )\n\n    query = QueryWithEmbedding(", "\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)", "    assert 1 == len(query_results)\n    assert 1 == len(query_results[0].results)\n    assert \"ghi_789\" == query_results[0].results[0].id\n    \n\n\n@pytest.mark.asyncio\nasync def test_delete_with_source_filter(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)", "    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    await milvus_datastore.delete(\n        filter=DocumentMetadataFilter(\n            source=Source.email,\n        )\n    )\n", "    )\n\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)", "\n    assert 1 == len(query_results)\n    assert 2 == len(query_results[0].results)\n    assert \"def_456\" == query_results[0].results[0].id\n    \n\n\n@pytest.mark.asyncio\nasync def test_delete_with_document_id_filter(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)", "async def test_delete_with_document_id_filter(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    await milvus_datastore.delete(\n        filter=DocumentMetadataFilter(\n            document_id=res[0],\n        )\n    )", "        )\n    )\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)", "\n    assert 1 == len(query_results)\n    assert 0 == len(query_results[0].results)\n    \n\n\n@pytest.mark.asyncio\nasync def test_delete_with_document_id(milvus_datastore, document_chunk_one):\n    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)", "    await milvus_datastore.delete(delete_all=True)\n    res = await milvus_datastore._upsert(document_chunk_one)\n    assert res == list(document_chunk_one.keys())\n    milvus_datastore.col.flush()\n    await milvus_datastore.delete([res[0]])\n\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=9,\n        embedding=[0] * OUTPUT_DIM,", "        top_k=9,\n        embedding=[0] * OUTPUT_DIM,\n    )\n    query_results = await milvus_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 0 == len(query_results[0].results)\n    \n\n", "\n\n# if __name__ == '__main__':\n#     import sys\n#     import pytest\n#     pytest.main(sys.argv)\n"]}
{"filename": "tests/datastore/providers/qdrant/test_qdrant_datastore.py", "chunked_list": ["from typing import Dict, List\n\nimport pytest\nimport qdrant_client\nfrom qdrant_client.http.models import PayloadSchemaType\n\nfrom datastore.providers.qdrant_datastore import QdrantDataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,", "    DocumentChunk,\n    DocumentChunkMetadata,\n    QueryWithEmbedding,\n    DocumentMetadataFilter,\n    Source,\n)\n\n\ndef create_embedding(non_zero_pos: int, size: int) -> List[float]:\n    vector = [0.0] * size\n    vector[non_zero_pos % size] = 1.0\n    return vector", "def create_embedding(non_zero_pos: int, size: int) -> List[float]:\n    vector = [0.0] * size\n    vector[non_zero_pos % size] = 1.0\n    return vector\n\n\n@pytest.fixture\ndef qdrant_datastore() -> QdrantDataStore:\n    return QdrantDataStore(\n        collection_name=\"documents\", vector_size=5, recreate_collection=True\n    )", "\n\n@pytest.fixture\ndef client() -> qdrant_client.QdrantClient:\n    return qdrant_client.QdrantClient()\n\n\n@pytest.fixture\ndef initial_document_chunks() -> Dict[str, List[DocumentChunk]]:\n    first_doc_chunks = [\n        DocumentChunk(\n            id=f\"first-doc-{i}\",\n            text=f\"Lorem ipsum {i}\",\n            metadata=DocumentChunkMetadata(),\n            embedding=create_embedding(i, 5),\n        )\n        for i in range(4, 7)\n    ]\n    return {\n        \"first-doc\": first_doc_chunks,\n    }", "def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:\n    first_doc_chunks = [\n        DocumentChunk(\n            id=f\"first-doc-{i}\",\n            text=f\"Lorem ipsum {i}\",\n            metadata=DocumentChunkMetadata(),\n            embedding=create_embedding(i, 5),\n        )\n        for i in range(4, 7)\n    ]\n    return {\n        \"first-doc\": first_doc_chunks,\n    }", "\n\n@pytest.fixture\ndef document_chunks() -> Dict[str, List[DocumentChunk]]:\n    first_doc_chunks = [\n        DocumentChunk(\n            id=f\"first-doc_{i}\",\n            text=f\"Lorem ipsum {i}\",\n            metadata=DocumentChunkMetadata(\n                source=Source.email, created_at=\"2023-03-05\", document_id=\"first-doc\"\n            ),\n            embedding=create_embedding(i, 5),\n        )\n        for i in range(3)\n    ]\n    second_doc_chunks = [\n        DocumentChunk(\n            id=f\"second-doc_{i}\",\n            text=f\"Dolor sit amet {i}\",\n            metadata=DocumentChunkMetadata(\n                created_at=\"2023-03-04\", document_id=\"second-doc\"\n            ),\n            embedding=create_embedding(i + len(first_doc_chunks), 5),\n        )\n        for i in range(2)\n    ]\n    return {\n        \"first-doc\": first_doc_chunks,\n        \"second-doc\": second_doc_chunks,\n    }", "\n\n@pytest.mark.asyncio\nasync def test_datastore_creates_payload_indexes(\n    qdrant_datastore,\n    client,\n):\n    collection_info = client.get_collection(collection_name=\"documents\")\n\n    assert 2 == len(collection_info.payload_schema)", "\n    assert 2 == len(collection_info.payload_schema)\n    assert \"created_at\" in collection_info.payload_schema\n    created_at = collection_info.payload_schema[\"created_at\"]\n    assert PayloadSchemaType.INTEGER == created_at.data_type\n    assert \"metadata.document_id\" in collection_info.payload_schema\n    document_id = collection_info.payload_schema[\"metadata.document_id\"]\n    assert PayloadSchemaType.KEYWORD == document_id.data_type\n\n", "\n\n@pytest.mark.asyncio\nasync def test_upsert_creates_all_points(\n    qdrant_datastore,\n    client,\n    document_chunks,\n):\n    document_ids = await qdrant_datastore._upsert(document_chunks)\n", "    document_ids = await qdrant_datastore._upsert(document_chunks)\n\n    assert 2 == len(document_ids)\n    assert 5 == client.count(collection_name=\"documents\").count\n\n\n@pytest.mark.asyncio\nasync def test_upsert_does_not_remove_existing_documents_but_store_new(\n    qdrant_datastore,\n    client,", "    qdrant_datastore,\n    client,\n    initial_document_chunks,\n    document_chunks,\n):\n    \"\"\"\n    This test ensures calling ._upsert no longer removes the existing document chunks,\n    as they are currently removed in the .upsert method directly.\n    \"\"\"\n    # Fill the database with document chunks before running the actual test", "    \"\"\"\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(initial_document_chunks)\n\n    await qdrant_datastore._upsert(document_chunks)\n\n    assert 8 == client.count(collection_name=\"documents\").count\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_query_returns_all_on_single_query(qdrant_datastore, document_chunks):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    query = QueryWithEmbedding(\n        query=\"lorem\",\n        top_k=5,\n        embedding=[0.5, 0.5, 0.5, 0.5, 0.5],", "        top_k=5,\n        embedding=[0.5, 0.5, 0.5, 0.5, 0.5],\n    )\n    query_results = await qdrant_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert \"lorem\" == query_results[0].query\n    assert 5 == len(query_results[0].results)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_query_returns_closest_entry(qdrant_datastore, document_chunks):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    query = QueryWithEmbedding(\n        query=\"ipsum\",\n        top_k=1,", "        query=\"ipsum\",\n        top_k=1,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )\n    query_results = await qdrant_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert \"ipsum\" == query_results[0].query\n    assert 1 == len(query_results[0].results)\n    first_document_chunk = query_results[0].results[0]", "    assert 1 == len(query_results[0].results)\n    first_document_chunk = query_results[0].results[0]\n    assert 0.0 <= first_document_chunk.score <= 1.0\n    assert Source.email == first_document_chunk.metadata.source\n    assert \"2023-03-05\" == first_document_chunk.metadata.created_at\n    assert \"first-doc\" == first_document_chunk.metadata.document_id\n\n\n@pytest.mark.asyncio\nasync def test_query_filter_by_document_id_returns_this_document_chunks(", "@pytest.mark.asyncio\nasync def test_query_filter_by_document_id_returns_this_document_chunks(\n    qdrant_datastore, document_chunks\n):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    first_query = QueryWithEmbedding(\n        query=\"dolor\",\n        filter=DocumentMetadataFilter(document_id=\"first-doc\"),", "        query=\"dolor\",\n        filter=DocumentMetadataFilter(document_id=\"first-doc\"),\n        top_k=5,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )\n    second_query = QueryWithEmbedding(\n        query=\"dolor\",\n        filter=DocumentMetadataFilter(document_id=\"second-doc\"),\n        top_k=5,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],", "        top_k=5,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )\n    query_results = await qdrant_datastore._query(queries=[first_query, second_query])\n\n    assert 2 == len(query_results)\n    assert \"dolor\" == query_results[0].query\n    assert \"dolor\" == query_results[1].query\n    assert 3 == len(query_results[0].results)\n    assert 2 == len(query_results[1].results)", "    assert 3 == len(query_results[0].results)\n    assert 2 == len(query_results[1].results)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"start_date\", [\"2023-03-05T00:00:00\", \"2023-03-05\"])\nasync def test_query_start_date_converts_datestring(\n    qdrant_datastore,\n    document_chunks,\n    start_date,", "    document_chunks,\n    start_date,\n):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    query = QueryWithEmbedding(\n        query=\"sit amet\",\n        filter=DocumentMetadataFilter(start_date=start_date),\n        top_k=5,", "        filter=DocumentMetadataFilter(start_date=start_date),\n        top_k=5,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )\n    query_results = await qdrant_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 3 == len(query_results[0].results)\n\n", "\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"end_date\", [\"2023-03-04T00:00:00\", \"2023-03-04\"])\nasync def test_query_end_date_converts_datestring(\n    qdrant_datastore,\n    document_chunks,\n    end_date,\n):\n    # Fill the database with document chunks before running the actual test", "):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    query = QueryWithEmbedding(\n        query=\"sit amet\",\n        filter=DocumentMetadataFilter(end_date=end_date),\n        top_k=5,\n        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )", "        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n    )\n    query_results = await qdrant_datastore._query(queries=[query])\n\n    assert 1 == len(query_results)\n    assert 2 == len(query_results[0].results)\n\n\n@pytest.mark.asyncio\nasync def test_delete_removes_by_ids(", "@pytest.mark.asyncio\nasync def test_delete_removes_by_ids(\n    qdrant_datastore,\n    client,\n    document_chunks,\n):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    await qdrant_datastore.delete(ids=[\"first-doc\"])", "\n    await qdrant_datastore.delete(ids=[\"first-doc\"])\n\n    assert 2 == client.count(collection_name=\"documents\").count\n\n\n@pytest.mark.asyncio\nasync def test_delete_removes_by_document_id_filter(\n    qdrant_datastore,\n    client,", "    qdrant_datastore,\n    client,\n    document_chunks,\n):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    await qdrant_datastore.delete(\n        filter=DocumentMetadataFilter(document_id=\"first-doc\")\n    )", "        filter=DocumentMetadataFilter(document_id=\"first-doc\")\n    )\n\n    assert 2 == client.count(collection_name=\"documents\").count\n\n\n@pytest.mark.asyncio\nasync def test_delete_removes_all(\n    qdrant_datastore,\n    client,", "    qdrant_datastore,\n    client,\n    document_chunks,\n):\n    # Fill the database with document chunks before running the actual test\n    await qdrant_datastore._upsert(document_chunks)\n\n    await qdrant_datastore.delete(delete_all=True)\n\n    assert 0 == client.count(collection_name=\"documents\").count", "\n    assert 0 == client.count(collection_name=\"documents\").count\n"]}
{"filename": "server/main.py", "chunked_list": ["import os\nimport uvicorn\nfrom fastapi import FastAPI, File, HTTPException, Depends, Body, UploadFile\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.staticfiles import StaticFiles\n\nfrom models.api import (\n    DeleteRequest,\n    DeleteResponse,\n    QueryRequest,", "    DeleteResponse,\n    QueryRequest,\n    QueryResponse,\n    UpsertRequest,\n    UpsertResponse,\n)\nfrom datastore.factory import get_datastore\nfrom services.file import get_document_from_file\n\n", "\n\napp = FastAPI()\napp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n\n# Create a sub-application, in order to access just the query endpoint in an OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\nsub_app = FastAPI(\n    title=\"Retrieval Plugin API\",\n    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n    version=\"1.0.0\",", "    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n    version=\"1.0.0\",\n    servers=[{\"url\": \"https://ask-lex-ti7zy.ondigitalocean.app\"}],\n)\napp.mount(\"/sub\", sub_app)\n\nbearer_scheme = HTTPBearer()\nBEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")\nassert BEARER_TOKEN is not None\n", "assert BEARER_TOKEN is not None\n\n\ndef validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):\n    if credentials.scheme != \"Bearer\" or credentials.credentials != BEARER_TOKEN:\n        raise HTTPException(status_code=401, detail=\"Invalid or missing token\")\n    return credentials\n\n\n@app.post(", "\n@app.post(\n    \"/upsert-file\",\n    response_model=UpsertResponse,\n)\nasync def upsert_file(\n    file: UploadFile = File(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    document = await get_document_from_file(file)", "):\n    document = await get_document_from_file(file)\n\n    try:\n        ids = await datastore.upsert([document])\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=f\"str({e})\")\n", "\n\n@app.post(\n    \"/upsert\",\n    response_model=UpsertResponse,\n)\nasync def upsert(\n    request: UpsertRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\n", "\n\n@app.post(\n    \"/query\",\n    response_model=QueryResponse,\n)\nasync def query_main(\n    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@sub_app.post(\n    \"/query\",\n    response_model=QueryResponse,\n    description='Accepts an array of search query objects, each with a natural language query string (\"query\") and an optional metadata filter (\"filter\"). Filters are not necessary in most cases, but can sometimes help refine search results based on criteria such as document source or time period. Send multiple queries to compare information from different sources or break down complex questions into sub-questions. If you receive a ResponseTooLargeError, try splitting up the queries into multiple calls to this endpoint.',\n)\nasync def query(\n    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),", "    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.delete(\n    \"/delete\",\n    response_model=DeleteResponse,\n)\nasync def delete(\n    request: DeleteRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )\n    try:\n        success = await datastore.delete(\n            ids=request.ids,\n            filter=request.filter,\n            delete_all=request.delete_all,\n        )\n        return DeleteResponse(success=success)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.on_event(\"startup\")\nasync def startup():\n    global datastore\n    datastore = await get_datastore()\n\n\ndef start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)", "def start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
{"filename": "datastore/__init__.py", "chunked_list": [""]}
{"filename": "datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional\nimport asyncio\n\nfrom models.models import (\n    Document,\n    DocumentChunk,\n    DocumentMetadataFilter,\n    Query,\n    QueryResult,", "    Query,\n    QueryResult,\n    QueryWithEmbedding,\n)\nfrom services.chunks import get_document_chunks\nfrom services.openai import get_embeddings\n\n\nclass DataStore(ABC):\n    async def upsert(\n        self, documents: List[Document], chunk_token_size: Optional[int] = None\n    ) -> List[str]:\n        \"\"\"\n        Takes in a list of documents and inserts them into the database.\n        First deletes all the existing vectors with the document id (if necessary, depends on the vector db), then inserts the new ones.\n        Return a list of document ids.\n        \"\"\"\n        # Delete any existing vectors for documents with the input document ids\n        await asyncio.gather(\n            *[\n                self.delete(\n                    filter=DocumentMetadataFilter(\n                        document_id=document.id,\n                    ),\n                    delete_all=False,\n                )\n                for document in documents\n                if document.id\n            ]\n        )\n\n        chunks = get_document_chunks(documents, chunk_token_size)\n\n        return await self._upsert(chunks)\n\n    @abstractmethod\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n\n        raise NotImplementedError\n\n    async def query(self, queries: List[Query]) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        # get a list of of just the queries from the Query list\n        query_texts = [query.query for query in queries]\n        query_embeddings = get_embeddings(query_texts)\n        # hydrate the queries with embeddings\n        queries_with_embeddings = [\n            QueryWithEmbedding(**query.dict(), embedding=embedding)\n            for query, embedding in zip(queries, query_embeddings)\n        ]\n        return await self._query(queries_with_embeddings)\n\n    @abstractmethod\n    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Multiple parameters can be used at once.\n        Returns whether the operation was successful.\n        \"\"\"\n        raise NotImplementedError", "class DataStore(ABC):\n    async def upsert(\n        self, documents: List[Document], chunk_token_size: Optional[int] = None\n    ) -> List[str]:\n        \"\"\"\n        Takes in a list of documents and inserts them into the database.\n        First deletes all the existing vectors with the document id (if necessary, depends on the vector db), then inserts the new ones.\n        Return a list of document ids.\n        \"\"\"\n        # Delete any existing vectors for documents with the input document ids\n        await asyncio.gather(\n            *[\n                self.delete(\n                    filter=DocumentMetadataFilter(\n                        document_id=document.id,\n                    ),\n                    delete_all=False,\n                )\n                for document in documents\n                if document.id\n            ]\n        )\n\n        chunks = get_document_chunks(documents, chunk_token_size)\n\n        return await self._upsert(chunks)\n\n    @abstractmethod\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n\n        raise NotImplementedError\n\n    async def query(self, queries: List[Query]) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        # get a list of of just the queries from the Query list\n        query_texts = [query.query for query in queries]\n        query_embeddings = get_embeddings(query_texts)\n        # hydrate the queries with embeddings\n        queries_with_embeddings = [\n            QueryWithEmbedding(**query.dict(), embedding=embedding)\n            for query, embedding in zip(queries, query_embeddings)\n        ]\n        return await self._query(queries_with_embeddings)\n\n    @abstractmethod\n    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Multiple parameters can be used at once.\n        Returns whether the operation was successful.\n        \"\"\"\n        raise NotImplementedError", ""]}
{"filename": "datastore/factory.py", "chunked_list": ["from datastore.datastore import DataStore\nimport os\n\n\nasync def get_datastore() -> DataStore:\n    datastore = os.environ.get(\"DATASTORE\")\n    assert datastore is not None\n\n    match datastore:\n        case \"pinecone\":", "    match datastore:\n        case \"pinecone\":\n            from datastore.providers.pinecone_datastore import PineconeDataStore\n\n            return PineconeDataStore()\n        case \"weaviate\":\n            from datastore.providers.weaviate_datastore import WeaviateDataStore\n\n            return WeaviateDataStore()\n        case \"milvus\":", "            return WeaviateDataStore()\n        case \"milvus\":\n            from datastore.providers.milvus_datastore import MilvusDataStore\n\n            return MilvusDataStore()\n        case \"zilliz\":\n            from datastore.providers.zilliz_datastore import ZillizDataStore\n\n            return ZillizDataStore()\n        case \"redis\":", "            return ZillizDataStore()\n        case \"redis\":\n            from datastore.providers.redis_datastore import RedisDataStore\n\n            return await RedisDataStore.init()\n        case \"qdrant\":\n            from datastore.providers.qdrant_datastore import QdrantDataStore\n\n            return QdrantDataStore()\n        case _:", "            return QdrantDataStore()\n        case _:\n            raise ValueError(f\"Unsupported vector database: {datastore}\")\n"]}
{"filename": "datastore/providers/pinecone_datastore.py", "chunked_list": ["import os\nfrom typing import Any, Dict, List, Optional\nimport pinecone\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\nimport asyncio\n\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,", "    DocumentChunk,\n    DocumentChunkMetadata,\n    DocumentChunkWithScore,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,\n    Source,\n)\nfrom services.date import to_unix_timestamp\n", "from services.date import to_unix_timestamp\n\n# Read environment variables for Pinecone configuration\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\nPINECONE_ENVIRONMENT = os.environ.get(\"PINECONE_ENVIRONMENT\")\nPINECONE_INDEX = os.environ.get(\"PINECONE_INDEX\")\nassert PINECONE_API_KEY is not None\nassert PINECONE_ENVIRONMENT is not None\nassert PINECONE_INDEX is not None\n", "assert PINECONE_INDEX is not None\n\n# Initialize Pinecone with the API key and environment\npinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n\n# Set the batch size for upserting vectors to Pinecone\nUPSERT_BATCH_SIZE = 100\n\n\nclass PineconeDataStore(DataStore):\n    def __init__(self):\n        # Check if the index name is specified and exists in Pinecone\n        if PINECONE_INDEX and PINECONE_INDEX not in pinecone.list_indexes():\n\n            # Get all fields in the metadata object in a list\n            fields_to_index = list(DocumentChunkMetadata.__fields__.keys())\n\n            # Create a new index with the specified name, dimension, and metadata configuration\n            try:\n                print(\n                    f\"Creating index {PINECONE_INDEX} with metadata config {fields_to_index}\"\n                )\n                pinecone.create_index(\n                    PINECONE_INDEX,\n                    dimension=1536,  # dimensionality of OpenAI ada v2 embeddings\n                    metadata_config={\"indexed\": fields_to_index},\n                )\n                self.index = pinecone.Index(PINECONE_INDEX)\n                print(f\"Index {PINECONE_INDEX} created successfully. Indexed fields are {fields_to_index}\")\n            except Exception as e:\n                print(f\"Error creating index {PINECONE_INDEX}: {e}\")\n                raise e\n        elif PINECONE_INDEX and PINECONE_INDEX in pinecone.list_indexes():\n            # Connect to an existing index with the specified name\n            try:\n                print(f\"Connecting to existing index {PINECONE_INDEX}\")\n                self.index = pinecone.Index(PINECONE_INDEX)\n                print(f\"Connected to index {PINECONE_INDEX} successfully\")\n            except Exception as e:\n                print(f\"Error connecting to index {PINECONE_INDEX}: {e}\")\n                raise e\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a dict from document id to list of document chunks and inserts them into the index.\n        Return a list of document ids.\n        \"\"\"\n        raise NotImplementedError\n        # Initialize a list of ids to return\n        doc_ids: List[str] = []\n        # Initialize a list of vectors to upsert\n        vectors = []\n        # Loop through the dict items\n        for doc_id, chunk_list in chunks.items():\n            # Append the id to the ids list\n            doc_ids.append(doc_id)\n            print(f\"Upserting document_id: {doc_id}\")\n            for chunk in chunk_list:\n                # Create a vector tuple of (id, embedding, metadata)\n                # Convert the metadata object to a dict with unix timestamps for dates\n                pinecone_metadata = self._get_pinecone_metadata(chunk.metadata)\n                # Add the text and document id to the metadata dict\n                pinecone_metadata[\"text\"] = chunk.text\n                pinecone_metadata[\"document_id\"] = doc_id\n                vector = (chunk.id, chunk.embedding, pinecone_metadata)\n                vectors.append(vector)\n\n        # Split the vectors list into batches of the specified size\n        batches = [\n            vectors[i : i + UPSERT_BATCH_SIZE]\n            for i in range(0, len(vectors), UPSERT_BATCH_SIZE)\n        ]\n        # Upsert each batch to Pinecone\n        for batch in batches:\n            try:\n                print(f\"Upserting batch of size {len(batch)}\")\n                self.index.upsert(vectors=batch)\n                print(f\"Upserted batch successfully\")\n            except Exception as e:\n                print(f\"Error upserting batch: {e}\")\n                raise e\n\n        return doc_ids\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n\n        # Define a helper coroutine that performs a single query and returns a QueryResult\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n            print(f\"Query: {query.query}\")\n\n            # Convert the metadata filter object to a dict with pinecone filter expressions\n            pinecone_filter = self._get_pinecone_filter(query.filter)\n\n            try:\n                # Query the index with the query embedding, filter, and top_k\n                query_response = self.index.query(\n                    # namespace=namespace,\n                    top_k=query.top_k,\n                    vector=query.embedding,\n                    filter=pinecone_filter,\n                    include_metadata=True,\n                )\n            except Exception as e:\n                print(f\"Error querying index: {e}\")\n                raise e\n\n            query_results: List[DocumentChunkWithScore] = []\n            for result in query_response.matches:\n                score = result.score\n                metadata = result.metadata\n                # Remove document id and text from metadata and store it in a new variable\n                metadata_without_text = (\n                    {key: value for key, value in metadata.items() if key != \"text\"}\n                    if metadata\n                    else None\n                )\n\n                # If the source is not a valid Source in the Source enum, set it to None\n                if (\n                    metadata_without_text\n                    and \"source\" in metadata_without_text\n                    and metadata_without_text[\"source\"] not in Source.__members__\n                ):\n                    metadata_without_text[\"source\"] = None\n\n                # Create a document chunk with score object with the result data\n                result = DocumentChunkWithScore(\n                    id=result.id,\n                    score=score,\n                    text=metadata[\"text\"] if metadata and \"text\" in metadata else None,\n                    metadata=metadata_without_text,\n                )\n                query_results.append(result)\n            return QueryResult(query=query.query, results=query_results)\n\n        # Use asyncio.gather to run multiple _single_query coroutines concurrently and collect their results\n        results: List[QueryResult] = await asyncio.gather(\n            *[_single_query(query) for query in queries]\n        )\n\n        return results\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything from the index.\n        \"\"\"\n        raise NotImplementedError\n        # Delete all vectors from the index if delete_all is True\n        if delete_all == True:\n            try:\n                print(f\"Deleting all vectors from index\")\n                self.index.delete(delete_all=True)\n                print(f\"Deleted all vectors successfully\")\n                return True\n            except Exception as e:\n                print(f\"Error deleting all vectors: {e}\")\n                raise e\n\n        # Convert the metadata filter object to a dict with pinecone filter expressions\n        pinecone_filter = self._get_pinecone_filter(filter)\n        # Delete vectors that match the filter from the index if the filter is not empty\n        if pinecone_filter != {}:\n            try:\n                print(f\"Deleting vectors with filter {pinecone_filter}\")\n                self.index.delete(filter=pinecone_filter)\n                print(f\"Deleted vectors with filter successfully\")\n            except Exception as e:\n                print(f\"Error deleting vectors with filter: {e}\")\n                raise e\n\n        # Delete vectors that match the document ids from the index if the ids list is not empty\n        if ids != None and len(ids) > 0:\n            try:\n                print(f\"Deleting vectors with ids {ids}\")\n                pinecone_filter = {\"document_id\": {\"$in\": ids}}\n                self.index.delete(filter=pinecone_filter)  # type: ignore\n                print(f\"Deleted vectors with ids successfully\")\n            except Exception as e:\n                print(f\"Error deleting vectors with ids: {e}\")\n                raise e\n\n        return True\n\n    def _get_pinecone_filter(\n        self, filter: Optional[DocumentMetadataFilter] = None\n    ) -> Dict[str, Any]:\n        if filter is None:\n            return {}\n\n        pinecone_filter = {}\n\n        # For each field in the MetadataFilter, check if it has a value and add the corresponding pinecone filter expression\n        # For start_date and end_date, uses the $gte and $lte operators respectively\n        # For other fields, uses the $eq operator\n        for field, value in filter.dict().items():\n            if value is not None:\n                if field == \"start_date\":\n                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n                    pinecone_filter[\"date\"][\"$gte\"] = to_unix_timestamp(value)\n                elif field == \"end_date\":\n                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n                    pinecone_filter[\"date\"][\"$lte\"] = to_unix_timestamp(value)\n                else:\n                    pinecone_filter[field] = value\n\n        return pinecone_filter\n\n    def _get_pinecone_metadata(\n        self, metadata: Optional[DocumentChunkMetadata] = None\n    ) -> Dict[str, Any]:\n        if metadata is None:\n            return {}\n\n        pinecone_metadata = {}\n\n        # For each field in the Metadata, check if it has a value and add it to the pinecone metadata dict\n        # For fields that are dates, convert them to unix timestamps\n        for field, value in metadata.dict().items():\n            if value is not None:\n                if field in [\"created_at\"]:\n                    pinecone_metadata[field] = to_unix_timestamp(value)\n                else:\n                    pinecone_metadata[field] = value\n\n        return pinecone_metadata", "\nclass PineconeDataStore(DataStore):\n    def __init__(self):\n        # Check if the index name is specified and exists in Pinecone\n        if PINECONE_INDEX and PINECONE_INDEX not in pinecone.list_indexes():\n\n            # Get all fields in the metadata object in a list\n            fields_to_index = list(DocumentChunkMetadata.__fields__.keys())\n\n            # Create a new index with the specified name, dimension, and metadata configuration\n            try:\n                print(\n                    f\"Creating index {PINECONE_INDEX} with metadata config {fields_to_index}\"\n                )\n                pinecone.create_index(\n                    PINECONE_INDEX,\n                    dimension=1536,  # dimensionality of OpenAI ada v2 embeddings\n                    metadata_config={\"indexed\": fields_to_index},\n                )\n                self.index = pinecone.Index(PINECONE_INDEX)\n                print(f\"Index {PINECONE_INDEX} created successfully. Indexed fields are {fields_to_index}\")\n            except Exception as e:\n                print(f\"Error creating index {PINECONE_INDEX}: {e}\")\n                raise e\n        elif PINECONE_INDEX and PINECONE_INDEX in pinecone.list_indexes():\n            # Connect to an existing index with the specified name\n            try:\n                print(f\"Connecting to existing index {PINECONE_INDEX}\")\n                self.index = pinecone.Index(PINECONE_INDEX)\n                print(f\"Connected to index {PINECONE_INDEX} successfully\")\n            except Exception as e:\n                print(f\"Error connecting to index {PINECONE_INDEX}: {e}\")\n                raise e\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a dict from document id to list of document chunks and inserts them into the index.\n        Return a list of document ids.\n        \"\"\"\n        raise NotImplementedError\n        # Initialize a list of ids to return\n        doc_ids: List[str] = []\n        # Initialize a list of vectors to upsert\n        vectors = []\n        # Loop through the dict items\n        for doc_id, chunk_list in chunks.items():\n            # Append the id to the ids list\n            doc_ids.append(doc_id)\n            print(f\"Upserting document_id: {doc_id}\")\n            for chunk in chunk_list:\n                # Create a vector tuple of (id, embedding, metadata)\n                # Convert the metadata object to a dict with unix timestamps for dates\n                pinecone_metadata = self._get_pinecone_metadata(chunk.metadata)\n                # Add the text and document id to the metadata dict\n                pinecone_metadata[\"text\"] = chunk.text\n                pinecone_metadata[\"document_id\"] = doc_id\n                vector = (chunk.id, chunk.embedding, pinecone_metadata)\n                vectors.append(vector)\n\n        # Split the vectors list into batches of the specified size\n        batches = [\n            vectors[i : i + UPSERT_BATCH_SIZE]\n            for i in range(0, len(vectors), UPSERT_BATCH_SIZE)\n        ]\n        # Upsert each batch to Pinecone\n        for batch in batches:\n            try:\n                print(f\"Upserting batch of size {len(batch)}\")\n                self.index.upsert(vectors=batch)\n                print(f\"Upserted batch successfully\")\n            except Exception as e:\n                print(f\"Error upserting batch: {e}\")\n                raise e\n\n        return doc_ids\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n\n        # Define a helper coroutine that performs a single query and returns a QueryResult\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n            print(f\"Query: {query.query}\")\n\n            # Convert the metadata filter object to a dict with pinecone filter expressions\n            pinecone_filter = self._get_pinecone_filter(query.filter)\n\n            try:\n                # Query the index with the query embedding, filter, and top_k\n                query_response = self.index.query(\n                    # namespace=namespace,\n                    top_k=query.top_k,\n                    vector=query.embedding,\n                    filter=pinecone_filter,\n                    include_metadata=True,\n                )\n            except Exception as e:\n                print(f\"Error querying index: {e}\")\n                raise e\n\n            query_results: List[DocumentChunkWithScore] = []\n            for result in query_response.matches:\n                score = result.score\n                metadata = result.metadata\n                # Remove document id and text from metadata and store it in a new variable\n                metadata_without_text = (\n                    {key: value for key, value in metadata.items() if key != \"text\"}\n                    if metadata\n                    else None\n                )\n\n                # If the source is not a valid Source in the Source enum, set it to None\n                if (\n                    metadata_without_text\n                    and \"source\" in metadata_without_text\n                    and metadata_without_text[\"source\"] not in Source.__members__\n                ):\n                    metadata_without_text[\"source\"] = None\n\n                # Create a document chunk with score object with the result data\n                result = DocumentChunkWithScore(\n                    id=result.id,\n                    score=score,\n                    text=metadata[\"text\"] if metadata and \"text\" in metadata else None,\n                    metadata=metadata_without_text,\n                )\n                query_results.append(result)\n            return QueryResult(query=query.query, results=query_results)\n\n        # Use asyncio.gather to run multiple _single_query coroutines concurrently and collect their results\n        results: List[QueryResult] = await asyncio.gather(\n            *[_single_query(query) for query in queries]\n        )\n\n        return results\n\n    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything from the index.\n        \"\"\"\n        raise NotImplementedError\n        # Delete all vectors from the index if delete_all is True\n        if delete_all == True:\n            try:\n                print(f\"Deleting all vectors from index\")\n                self.index.delete(delete_all=True)\n                print(f\"Deleted all vectors successfully\")\n                return True\n            except Exception as e:\n                print(f\"Error deleting all vectors: {e}\")\n                raise e\n\n        # Convert the metadata filter object to a dict with pinecone filter expressions\n        pinecone_filter = self._get_pinecone_filter(filter)\n        # Delete vectors that match the filter from the index if the filter is not empty\n        if pinecone_filter != {}:\n            try:\n                print(f\"Deleting vectors with filter {pinecone_filter}\")\n                self.index.delete(filter=pinecone_filter)\n                print(f\"Deleted vectors with filter successfully\")\n            except Exception as e:\n                print(f\"Error deleting vectors with filter: {e}\")\n                raise e\n\n        # Delete vectors that match the document ids from the index if the ids list is not empty\n        if ids != None and len(ids) > 0:\n            try:\n                print(f\"Deleting vectors with ids {ids}\")\n                pinecone_filter = {\"document_id\": {\"$in\": ids}}\n                self.index.delete(filter=pinecone_filter)  # type: ignore\n                print(f\"Deleted vectors with ids successfully\")\n            except Exception as e:\n                print(f\"Error deleting vectors with ids: {e}\")\n                raise e\n\n        return True\n\n    def _get_pinecone_filter(\n        self, filter: Optional[DocumentMetadataFilter] = None\n    ) -> Dict[str, Any]:\n        if filter is None:\n            return {}\n\n        pinecone_filter = {}\n\n        # For each field in the MetadataFilter, check if it has a value and add the corresponding pinecone filter expression\n        # For start_date and end_date, uses the $gte and $lte operators respectively\n        # For other fields, uses the $eq operator\n        for field, value in filter.dict().items():\n            if value is not None:\n                if field == \"start_date\":\n                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n                    pinecone_filter[\"date\"][\"$gte\"] = to_unix_timestamp(value)\n                elif field == \"end_date\":\n                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n                    pinecone_filter[\"date\"][\"$lte\"] = to_unix_timestamp(value)\n                else:\n                    pinecone_filter[field] = value\n\n        return pinecone_filter\n\n    def _get_pinecone_metadata(\n        self, metadata: Optional[DocumentChunkMetadata] = None\n    ) -> Dict[str, Any]:\n        if metadata is None:\n            return {}\n\n        pinecone_metadata = {}\n\n        # For each field in the Metadata, check if it has a value and add it to the pinecone metadata dict\n        # For fields that are dates, convert them to unix timestamps\n        for field, value in metadata.dict().items():\n            if value is not None:\n                if field in [\"created_at\"]:\n                    pinecone_metadata[field] = to_unix_timestamp(value)\n                else:\n                    pinecone_metadata[field] = value\n\n        return pinecone_metadata", ""]}
{"filename": "datastore/providers/redis_datastore.py", "chunked_list": ["import asyncio\nimport logging\nimport os\nimport re\nimport json\nimport redis.asyncio as redis\nimport numpy as np\n\nfrom redis.commands.search.query import Query as RediSearchQuery\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType", "from redis.commands.search.query import Query as RediSearchQuery\nfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\nfrom redis.commands.search.field import (\n    TagField,\n    TextField,\n    NumericField,\n    VectorField,\n)\nfrom typing import Dict, List, Optional\nfrom datastore.datastore import DataStore", "from typing import Dict, List, Optional\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentMetadataFilter,\n    DocumentChunkWithScore,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,\n)", "    QueryWithEmbedding,\n)\nfrom services.date import to_unix_timestamp\n\n# Read environment variables for Redis\nREDIS_HOST = os.environ.get(\"REDIS_HOST\", \"localhost\")\nREDIS_PORT = int(os.environ.get(\"REDIS_PORT\", 6379))\nREDIS_PASSWORD = os.environ.get(\"REDIS_PASSWORD\")\nREDIS_INDEX_NAME = os.environ.get(\"REDIS_INDEX_NAME\", \"index\")\nREDIS_DOC_PREFIX = os.environ.get(\"REDIS_DOC_PREFIX\", \"doc\")", "REDIS_INDEX_NAME = os.environ.get(\"REDIS_INDEX_NAME\", \"index\")\nREDIS_DOC_PREFIX = os.environ.get(\"REDIS_DOC_PREFIX\", \"doc\")\nREDIS_DISTANCE_METRIC = os.environ.get(\"REDIS_DISTANCE_METRIC\", \"COSINE\")\nREDIS_INDEX_TYPE = os.environ.get(\"REDIS_INDEX_TYPE\", \"FLAT\")\nassert REDIS_INDEX_TYPE in (\"FLAT\", \"HNSW\")\n\n# OpenAI Ada Embeddings Dimension\nVECTOR_DIMENSION = 1536\n\n# RediSearch constants", "\n# RediSearch constants\nREDIS_DEFAULT_ESCAPED_CHARS = re.compile(r\"[,.<>{}\\[\\]\\\\\\\"\\':;!@#$%^&*()\\-+=~\\/ ]\")\nREDIS_SEARCH_SCHEMA = {\n    \"document_id\": TagField(\"$.document_id\", as_name=\"document_id\"),\n    \"metadata\": {\n        # \"source_id\": TagField(\"$.metadata.source_id\", as_name=\"source_id\"),\n        \"source\": TagField(\"$.metadata.source\", as_name=\"source\"),\n        # \"author\": TextField(\"$.metadata.author\", as_name=\"author\"),\n        # \"created_at\": NumericField(\"$.metadata.created_at\", as_name=\"created_at\"),", "        # \"author\": TextField(\"$.metadata.author\", as_name=\"author\"),\n        # \"created_at\": NumericField(\"$.metadata.created_at\", as_name=\"created_at\"),\n    },\n    \"embedding\": VectorField(\n        \"$.embedding\",\n        REDIS_INDEX_TYPE,\n        {\n            \"TYPE\": \"FLOAT64\",\n            \"DIM\": VECTOR_DIMENSION,\n            \"DISTANCE_METRIC\": REDIS_DISTANCE_METRIC,", "            \"DIM\": VECTOR_DIMENSION,\n            \"DISTANCE_METRIC\": REDIS_DISTANCE_METRIC,\n            \"INITIAL_CAP\": 500,\n        },\n        as_name=\"embedding\",\n    ),\n}\n\n\ndef unpack_schema(d: dict):", "\ndef unpack_schema(d: dict):\n    for v in d.values():\n        if isinstance(v, dict):\n            yield from unpack_schema(v)\n        else:\n            yield v\n\n\nclass RedisDataStore(DataStore):", "\nclass RedisDataStore(DataStore):\n    def __init__(self, client: redis.Redis):\n        self.client = client\n        # Init default metadata with sentinal values in case the document written has no metadata\n        self._default_metadata = {\n            field: \"_null_\" for field in REDIS_SEARCH_SCHEMA[\"metadata\"]\n        }\n\n    ### Redis Helper Methods ###", "\n    ### Redis Helper Methods ###\n\n    @classmethod\n    async def init(cls):\n        \"\"\"\n        Setup the index if it does not exist.\n        \"\"\"\n        try:\n            # Connect to the Redis Client", "        try:\n            # Connect to the Redis Client\n            logging.info(\"Connecting to Redis\")\n            client = redis.Redis(\n                host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD\n            )\n        except Exception as e:\n            logging.error(f\"Error setting up Redis: {e}\")\n            raise e\n", "            raise e\n\n        try:\n            # Check for existence of RediSearch Index\n            await client.ft(REDIS_INDEX_NAME).info()\n            logging.info(f\"RediSearch index {REDIS_INDEX_NAME} already exists\")\n        except:\n            # Create the RediSearch Index\n            logging.info(f\"Creating new RediSearch index {REDIS_INDEX_NAME}\")\n            definition = IndexDefinition(", "            logging.info(f\"Creating new RediSearch index {REDIS_INDEX_NAME}\")\n            definition = IndexDefinition(\n                prefix=[REDIS_DOC_PREFIX], index_type=IndexType.JSON\n            )\n            fields = list(unpack_schema(REDIS_SEARCH_SCHEMA))\n            await client.ft(REDIS_INDEX_NAME).create_index(\n                fields=fields, definition=definition\n            )\n        return cls(client)\n", "        return cls(client)\n\n    @staticmethod\n    def _redis_key(document_id: str, chunk_id: str) -> str:\n        \"\"\"\n        Create the JSON key for document chunks in Redis.\n\n        Args:\n            document_id (str): Document Identifier\n            chunk_id (str): Chunk Identifier", "            document_id (str): Document Identifier\n            chunk_id (str): Chunk Identifier\n\n        Returns:\n            str: JSON key string.\n        \"\"\"\n        return f\"doc:{document_id}:chunk:{chunk_id}\"\n\n    @staticmethod\n    def _escape(value: str) -> str:", "    @staticmethod\n    def _escape(value: str) -> str:\n        \"\"\"\n        Escape filter value.\n\n        Args:\n            value (str): Value to escape.\n\n        Returns:\n            str: Escaped filter value for RediSearch.", "        Returns:\n            str: Escaped filter value for RediSearch.\n        \"\"\"\n\n        def escape_symbol(match) -> str:\n            value = match.group(0)\n            return f\"\\\\{value}\"\n\n        return REDIS_DEFAULT_ESCAPED_CHARS.sub(escape_symbol, value)\n", "        return REDIS_DEFAULT_ESCAPED_CHARS.sub(escape_symbol, value)\n\n    def _get_redis_chunk(self, chunk: DocumentChunk) -> dict:\n        \"\"\"\n        Convert DocumentChunk into a JSON object for storage\n        in Redis.\n\n        Args:\n            chunk (DocumentChunk): Chunk of a Document.\n", "            chunk (DocumentChunk): Chunk of a Document.\n\n        Returns:\n            dict: JSON object for storage in Redis.\n        \"\"\"\n        # Convert chunk -> dict\n        data = chunk.__dict__\n        metadata = chunk.metadata.__dict__\n        data[\"chunk_id\"] = data.pop(\"id\")\n", "        data[\"chunk_id\"] = data.pop(\"id\")\n\n        # Prep Redis Metadata\n        redis_metadata = dict(self._default_metadata)\n        if metadata:\n            for field, value in metadata.items():\n                if value:\n                    if field == \"created_at\":\n                        redis_metadata[field] = to_unix_timestamp(value)  # type: ignore\n                    else:", "                        redis_metadata[field] = to_unix_timestamp(value)  # type: ignore\n                    else:\n                        redis_metadata[field] = value\n        data[\"metadata\"] = redis_metadata\n        return data\n\n    def _get_redis_query(self, query: QueryWithEmbedding) -> RediSearchQuery:\n        \"\"\"\n        Convert a QueryWithEmbedding into a RediSearchQuery.\n", "        Convert a QueryWithEmbedding into a RediSearchQuery.\n\n        Args:\n            query (QueryWithEmbedding): Search query.\n\n        Returns:\n            RediSearchQuery: Query for RediSearch.\n        \"\"\"\n        query_str: str = \"\"\n        filter_str: str = \"\"", "        query_str: str = \"\"\n        filter_str: str = \"\"\n\n        # RediSearch field type to query string\n        def _typ_to_str(typ, field, value) -> str:  # type: ignore\n            if isinstance(typ, TagField):\n                return f\"@{field}:{{{self._escape(value)}}} \"\n            elif isinstance(typ, TextField):\n                return f\"@{field}:{self._escape(value)} \"\n            elif isinstance(typ, NumericField):", "                return f\"@{field}:{self._escape(value)} \"\n            elif isinstance(typ, NumericField):\n                num = to_unix_timestamp(value)\n                match field:\n                    case \"start_date\":\n                        return f\"@{field}:[{num} +inf] \"\n                    case \"end_date\":\n                        return f\"@{field}:[-inf {num}] \"\n\n        # Build filter", "\n        # Build filter\n        if query.filter:\n            for field, value in query.filter.__dict__.items():\n                if not value:\n                    continue\n                if field in REDIS_SEARCH_SCHEMA:\n                    filter_str += _typ_to_str(REDIS_SEARCH_SCHEMA[field], field, value)\n                elif field in REDIS_SEARCH_SCHEMA[\"metadata\"]:\n                    if field == \"source\":  # handle the enum", "                elif field in REDIS_SEARCH_SCHEMA[\"metadata\"]:\n                    if field == \"source\":  # handle the enum\n                        value = value.value\n                    filter_str += _typ_to_str(\n                        REDIS_SEARCH_SCHEMA[\"metadata\"][field], field, value\n                    )\n                elif field in [\"start_date\", \"end_date\"]:\n                    filter_str += _typ_to_str(\n                        REDIS_SEARCH_SCHEMA[\"metadata\"][\"created_at\"], field, value\n                    )", "                        REDIS_SEARCH_SCHEMA[\"metadata\"][\"created_at\"], field, value\n                    )\n\n        # Postprocess filter string\n        filter_str = filter_str.strip()\n        filter_str = filter_str if filter_str else \"*\"\n\n        # Prepare query string\n        query_str = (\n            f\"({filter_str})=>[KNN {query.top_k} @embedding $embedding as score]\"", "        query_str = (\n            f\"({filter_str})=>[KNN {query.top_k} @embedding $embedding as score]\"\n        )\n        return (\n            RediSearchQuery(query_str)\n            .sort_by(\"score\")\n            .paging(0, query.top_k)\n            .dialect(2)\n        )\n", "        )\n\n    async def _redis_delete(self, keys: List[str]):\n        \"\"\"\n        Delete a list of keys from Redis.\n\n        Args:\n            keys (List[str]): List of keys to delete.\n        \"\"\"\n        # Delete the keys", "        \"\"\"\n        # Delete the keys\n        await asyncio.gather(*[self.client.delete(key) for key in keys])\n\n    #######\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.", "        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n        # Initialize a list of ids to return\n        doc_ids: List[str] = []\n\n        # Loop through the dict items\n        for doc_id, chunk_list in chunks.items():\n\n            # Append the id to the ids list", "\n            # Append the id to the ids list\n            doc_ids.append(doc_id)\n\n            # Write all chunks associated with a document\n            n = min(len(chunk_list), 50)\n            semaphore = asyncio.Semaphore(n)\n\n            async def _write(chunk: DocumentChunk):\n                async with semaphore:", "            async def _write(chunk: DocumentChunk):\n                async with semaphore:\n                    # Get redis key and chunk object\n                    key = self._redis_key(doc_id, chunk.id)  # type: ignore\n                    data = self._get_redis_chunk(chunk)\n                    await self.client.json().set(key, \"$\", data)\n\n            # Concurrently gather writes\n            await asyncio.gather(*[_write(chunk) for i, chunk in enumerate(chunk_list)])\n        return doc_ids", "            await asyncio.gather(*[_write(chunk) for i, chunk in enumerate(chunk_list)])\n        return doc_ids\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and\n        returns a list of query results with matching document chunks and scores.", "        Takes in a list of queries with embeddings and filters and\n        returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        # Prepare results object\n        results: List[QueryResult] = []\n\n        # Use asyncio for concurrent search\n        n = min(len(queries), 50)\n        semaphore = asyncio.Semaphore(n)\n", "        semaphore = asyncio.Semaphore(n)\n\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n            logging.info(f\"Query: {query.query}\")\n            # Extract Redis query\n            redis_query: RediSearchQuery = self._get_redis_query(query)\n            # Perform a single query\n            async with semaphore:\n                embedding = np.array(query.embedding, dtype=np.float64).tobytes()\n                # Get vector query response from Redis", "                embedding = np.array(query.embedding, dtype=np.float64).tobytes()\n                # Get vector query response from Redis\n                query_response = await self.client.ft(REDIS_INDEX_NAME).search(\n                    redis_query, {\"embedding\": embedding}  # type: ignore\n                )\n                return query_response\n\n        # Concurrently gather query results\n        logging.info(f\"Gathering {len(queries)} query results\", flush=True)  # type: ignore\n        query_responses = await asyncio.gather(", "        logging.info(f\"Gathering {len(queries)} query results\", flush=True)  # type: ignore\n        query_responses = await asyncio.gather(\n            *[_single_query(query) for query in queries]\n        )\n\n        # Iterate through responses and construct results\n        for query, query_response in zip(queries, query_responses):\n\n            # Iterate through nearest neighbor documents\n            query_results: List[DocumentChunkWithScore] = []", "            # Iterate through nearest neighbor documents\n            query_results: List[DocumentChunkWithScore] = []\n            for doc in query_response.docs:\n                # Create a document chunk with score object with the result data\n                doc_json = json.loads(doc.json)\n                result = DocumentChunkWithScore(\n                    id=doc_json[\"metadata\"][\"document_id\"],\n                    score=doc.score,\n                    text=doc_json[\"text\"],\n                    metadata=doc_json[\"metadata\"],", "                    text=doc_json[\"text\"],\n                    metadata=doc_json[\"metadata\"],\n                )\n                query_results.append(result)\n\n            # Add to overall results\n            results.append(QueryResult(query=query.query, results=query_results))\n        return results\n\n    async def _find_keys(self, pattern: str) -> List[str]:", "\n    async def _find_keys(self, pattern: str) -> List[str]:\n        return await self.client.keys(pattern=pattern)\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:", "        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Returns whether the operation was successful.\n        \"\"\"\n        # Delete all vectors from the index if delete_all is True\n        if delete_all:\n            try:\n                logging.info(f\"Deleting all documents from index\")", "            try:\n                logging.info(f\"Deleting all documents from index\")\n                await self.client.ft(REDIS_INDEX_NAME).dropindex(True)\n                logging.info(f\"Deleted all documents successfully\")\n                return True\n            except Exception as e:\n                logging.info(f\"Error deleting all documents: {e}\")\n                raise e\n\n        # Delete by filter", "\n        # Delete by filter\n        if filter:\n            # TODO - extend this to work with other metadata filters?\n            if filter.document_id:\n                try:\n                    keys = await self._find_keys(f\"{REDIS_DOC_PREFIX}:{filter.document_id}:*\")\n                    await self._redis_delete(keys)\n                    logging.info(f\"Deleted document {filter.document_id} successfully\")\n                except Exception as e:", "                    logging.info(f\"Deleted document {filter.document_id} successfully\")\n                except Exception as e:\n                    logging.info(f\"Error deleting document {filter.document_id}: {e}\")\n                    raise e\n\n        # Delete by explicit ids (Redis keys)\n        if ids:\n            try:\n                logging.info(f\"Deleting document ids {ids}\")\n                keys = []", "                logging.info(f\"Deleting document ids {ids}\")\n                keys = []\n                # find all keys associated with the document ids\n                for document_id in ids:\n                    doc_keys = await self._find_keys(pattern=f\"{REDIS_DOC_PREFIX}:{document_id}:*\")\n                    keys.extend(doc_keys)\n                # delete all keys\n                logging.info(f\"Deleting {len(keys)} keys from Redis\")\n                await self._redis_delete(keys)\n            except Exception as e:", "                await self._redis_delete(keys)\n            except Exception as e:\n                logging.info(f\"Error deleting ids: {e}\")\n                raise e\n\n        return True\n"]}
{"filename": "datastore/providers/zilliz_datastore.py", "chunked_list": ["import os\nimport asyncio\n\nfrom typing import Dict, List, Optional\nfrom pymilvus import (\n    Collection,\n    connections,\n    utility,\n    FieldSchema,\n    DataType,", "    FieldSchema,\n    DataType,\n    CollectionSchema,\n)\nfrom uuid import uuid4\n\n\nfrom services.date import to_unix_timestamp\nfrom datastore.datastore import DataStore\nfrom models.models import (", "from datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,\n    Source,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,\n    DocumentChunkWithScore,\n)", "    DocumentChunkWithScore,\n)\n\nZILLIZ_COLLECTION = os.environ.get(\"ZILLIZ_COLLECTION\") or \"c\" + uuid4().hex\nZILLIZ_URI = os.environ.get(\"ZILLIZ_URI\")\nZILLIZ_USER = os.environ.get(\"ZILLIZ_USER\")\nZILLIZ_PASSWORD = os.environ.get(\"ZILLIZ_PASSWORD\")\nZILLIZ_USE_SECURITY = False if ZILLIZ_PASSWORD is None else True\n\n", "\n\nUPSERT_BATCH_SIZE = 100\nOUTPUT_DIM = 1536\n\n\nclass Required:\n    pass\n\n", "\n\n# The fields names that we are going to be storing within Zilliz Cloud, the field declaration for schema creation, and the default value\nSCHEMA = [\n    (\n        \"pk\",\n        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n        Required,\n    ),\n    (", "    ),\n    (\n        \"embedding\",\n        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=OUTPUT_DIM),\n        Required,\n    ),\n    (\n        \"text\",\n        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n        Required,", "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n        Required,\n    ),\n    (\n        \"document_id\",\n        FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\n        \"source_id\",", "    (\n        \"source_id\",\n        FieldSchema(name=\"source_id\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\n        \"id\",\n        FieldSchema(\n            name=\"id\",\n            dtype=DataType.VARCHAR,", "            name=\"id\",\n            dtype=DataType.VARCHAR,\n            max_length=65535,\n        ),\n        \"\",\n    ),\n    (\n        \"source\",\n        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",", "        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\"url\", FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=65535), \"\"),\n    (\"created_at\", FieldSchema(name=\"created_at\", dtype=DataType.INT64), -1),\n    (\n        \"author\",\n        FieldSchema(name=\"author\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),", "        \"\",\n    ),\n]\n\n\nclass ZillizDataStore(DataStore):\n    def __init__(self, create_new: Optional[bool] = False):\n        \"\"\"Create a Zilliz DataStore.\n\n        The Zilliz Datastore allows for storing your indexes and metadata within a Zilliz Cloud instance.\n\n        Args:\n            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.\n        \"\"\"\n\n        # # TODO: Auto infer the fields\n        # non_string_fields = [('embedding', List[float]), ('created_at', int)]\n        # fields_to_index = list(DocumentChunkMetadata.__fields__.keys())\n        # fields_to_index = list(DocumentChunk.__fields__.keys())\n\n        # Check if the connection already exists\n        try:\n            i = [\n                connections.get_connection_addr(x[0])\n                for x in connections.list_connections()\n            ].index({\"address\": ZILLIZ_URI, \"user\": ZILLIZ_USER})\n            self.alias = connections.list_connections()[i][0]\n        except ValueError:\n            # Connect to the Zilliz instance using the passed in Enviroment variables\n            self.alias = uuid4().hex\n            connections.connect(alias=self.alias, uri=ZILLIZ_URI, user=ZILLIZ_USER, password=ZILLIZ_PASSWORD, secure=ZILLIZ_USE_SECURITY)  # type: ignore\n\n        self._create_collection(create_new)  # type: ignore\n\n    def _create_collection(self, create_new: bool) -> None:\n        \"\"\"Create a collection based on enviroment and passed in variables.\n\n        Args:\n            create_new (bool): Whether to overwrite if collection already exists.\n        \"\"\"\n\n        # If the collection exists and create_new is True, drop the existing collection\n        if utility.has_collection(ZILLIZ_COLLECTION, using=self.alias) and create_new:\n            utility.drop_collection(ZILLIZ_COLLECTION, using=self.alias)\n\n        # Check if the collection doesnt exist\n        if utility.has_collection(ZILLIZ_COLLECTION, using=self.alias) is False:\n            # If it doesnt exist use the field params from init to create a new schem\n            schema = [field[1] for field in SCHEMA]\n            schema = CollectionSchema(schema)\n            # Use the schema to create a new collection\n            self.col = Collection(\n                ZILLIZ_COLLECTION,\n                schema=schema,\n                consistency_level=\"Strong\",\n                using=self.alias,\n            )\n        else:\n            # If the collection exists, point to it\n            self.col = Collection(ZILLIZ_COLLECTION, consistency_level=\"Strong\", using=self.alias)  # type: ignore\n\n        # If no index on the collection, create one\n        if len(self.col.indexes) == 0:\n            i_p = {\"metric_type\": \"L2\", \"index_type\": \"AUTOINDEX\", \"params\": {}}\n            self.col.create_index(\"embedding\", index_params=i_p)\n\n        self.col.load()\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"Upsert chunks into the datastore.\n\n        Args:\n            chunks (Dict[str, List[DocumentChunk]]): A list of DocumentChunks to insert\n\n        Raises:\n            e: Error in upserting data.\n\n        Returns:\n            List[str]: The document_id's that were inserted.\n        \"\"\"\n        # The doc id's to return for the upsert\n        doc_ids: List[str] = []\n        # List to collect all the insert data\n        insert_data = [[] for _ in range(len(SCHEMA) - 1)]\n        # Go through each document chunklist and grab the data\n        for doc_id, chunk_list in chunks.items():\n            # Append the doc_id to the list we are returning\n            doc_ids.append(doc_id)\n            # Examine each chunk in the chunklist\n            for chunk in chunk_list:\n                # Extract data from the chunk\n                list_of_data = self._get_values(chunk)\n                # Check if the data is valid\n                if list_of_data is not None:\n                    # Append each field to the insert_data\n                    for x in range(len(insert_data)):\n                        insert_data[x].append(list_of_data[x])\n        # Slice up our insert data into batches\n        batches = [\n            insert_data[i : i + UPSERT_BATCH_SIZE]\n            for i in range(0, len(insert_data), UPSERT_BATCH_SIZE)\n        ]\n\n        # Attempt to insert each batch into our collection\n        for batch in batches:\n            # Check if empty batch\n            if len(batch[0]) != 0:\n                try:\n                    print(f\"Upserting batch of size {len(batch[0])}\")\n                    self.col.insert(batch)\n                    print(f\"Upserted batch successfully\")\n                except Exception as e:\n                    print(f\"Error upserting batch: {e}\")\n                    raise e\n\n        # This setting perfoms flushes after insert. Small insert == bad to use\n        # self.col.flush()\n\n        return doc_ids\n\n    def _get_values(self, chunk: DocumentChunk) -> List[any] | None:  # type: ignore\n        \"\"\"Convert the chunk into a list of values to insert whose indexes align with fields.\n\n        Args:\n            chunk (DocumentChunk): The chunk to convert.\n\n        Returns:\n            List (any): The values to insert.\n        \"\"\"\n        # Convert DocumentChunk and its sub models to dict\n        values = chunk.dict()\n        # Unpack the metadata into the same dict\n        meta = values.pop(\"metadata\")\n        values.update(meta)\n\n        # Convert date to int timestamp form\n        if values[\"created_at\"]:\n            values[\"created_at\"] = to_unix_timestamp(values[\"created_at\"])\n\n        # If source exists, change from Source object to the string value it holds\n        if values[\"source\"]:\n            values[\"source\"] = values[\"source\"].value\n        # List to collect data we will return\n        ret = []\n        # Grab data responding to each field excluding the hidden auto pk field\n        for key, _, default in SCHEMA[1:]:\n            # Grab the data at the key and default to our defaults set in init\n            x = values.get(key) or default\n            # If one of our required fields is missing, ignore the entire entry\n            if x is Required:\n                print(\"Chunk \" + values[\"id\"] + \" missing \" + key + \" skipping\")\n                return None\n            # Add the corresponding value if it passes the tests\n            ret.append(x)\n        return ret\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"Query the QueryWithEmbedding against the ZillizDocumentSearch\n\n        Search the embedding and its filter in the collection.\n\n        Args:\n            queries (List[QueryWithEmbedding]): The list of searches to perform.\n\n        Returns:\n            List[QueryResult]: Results for each search.\n        \"\"\"\n        # Async to perform the query, adapted from pinecone implementation\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n\n            filter = None\n            # Set the filter to expression that is valid for Zilliz\n            if query.filter != None:\n                # Either a valid filter or None will be returned\n                filter = self._get_filter(query.filter)\n\n            # Perform our search\n            res = self.col.search(\n                data=[query.embedding],\n                anns_field=\"embedding\",\n                param={\"metric_type\": \"L2\", \"params\": {}},\n                limit=query.top_k,\n                expr=filter,\n                output_fields=[\n                    field[0] for field in SCHEMA[2:]\n                ],  # Ignoring pk, embedding\n            )\n            # Results that will hold our DocumentChunkWithScores\n            results = []\n            # Parse every result for our search\n            for hit in res[0]:  # type: ignore\n                # The distance score for the search result, falls under DocumentChunkWithScore\n                score = hit.score\n                # Our metadata info, falls under DocumentChunkMetadata\n                metadata = {}\n                # Grab the values that correspond to our fields, ignore pk and embedding.\n                for x in [field[0] for field in SCHEMA[2:]]:\n                    metadata[x] = hit.entity.get(x)\n                # If the source isnt valid, conver to None\n                if metadata[\"source\"] not in Source.__members__:\n                    metadata[\"source\"] = None\n                # Text falls under the DocumentChunk\n                text = metadata.pop(\"text\")\n                # Id falls under the DocumentChunk\n                ids = metadata.pop(\"id\")\n                chunk = DocumentChunkWithScore(\n                    id=ids,\n                    score=score,\n                    text=text,\n                    metadata=DocumentChunkMetadata(**metadata),\n                )\n                results.append(chunk)\n\n            # TODO: decide on doing queries to grab the embedding itself, slows down performance as double query occurs\n\n            return QueryResult(query=query.query, results=results)\n\n        results: List[QueryResult] = await asyncio.gather(\n            *[_single_query(query) for query in queries]\n        )\n        return results\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"Delete the entities based either on the chunk_id of the vector,\n\n        Args:\n            ids (Optional[List[str]], optional): The document_ids to delete. Defaults to None.\n            filter (Optional[DocumentMetadataFilter], optional): The filter to delet by. Defaults to None.\n            delete_all (Optional[bool], optional): Whether to drop the collection and recreate it. Defaults to None.\n        \"\"\"\n        # If deleting all, drop and create the new collection\n        if delete_all:\n            # Release the collection from memory\n            self.col.release()\n            # Drop the collection\n            self.col.drop()\n            # Recreate the new collection\n            self._create_collection(True)\n            return True\n\n        # Keep track of how many we have deleted for later printing\n        delete_count = 0\n\n        # Check if empty ids\n        if ids != None:\n            if len(ids) != 0:\n                # Add quotation marks around the string format id\n                ids = ['\"' + str(id) + '\"' for id in ids]\n                # Query for the pk's of entries that match id's\n                ids = self.col.query(f\"document_id in [{','.join(ids)}]\")\n                # Convert to list of pks\n                ids = [str(entry[\"pk\"]) for entry in ids]  # type: ignore\n                # Check to see if there are valid pk's to delete\n                if len(ids) != 0:\n                    # Delete the entries for each pk\n                    res = self.col.delete(f\"pk in [{','.join(ids)}]\")\n                    # Incremet our deleted count\n                    delete_count += int(res.delete_count)  # type: ignore\n\n        # Check if empty filter\n        if filter != None:\n            # Convert filter to Zilliz expression\n            filter = self._get_filter(filter)  # type: ignore\n            # Check if there is anything to filter\n            if len(filter) != 0:  # type: ignore\n                # Query for the pk's of entries that match filter\n                filter = self.col.query(filter)  # type: ignore\n                # Convert to list of pks\n                filter = [str(entry[\"pk\"]) for entry in filter]  # type: ignore\n                # Check to see if there are valid pk's to delete\n                if len(filter) != 0:  # type: ignore\n                    # Delete the entries\n                    res = self.col.delete(f\"pk in [{','.join(filter)}]\")  # type: ignore\n                    # Increment our delete count\n                    delete_count += int(res.delete_count)  # type: ignore\n\n        # This setting perfoms flushes after delete. Small delete == bad to use\n        # self.col.flush()\n\n        return True\n\n    def _get_filter(self, filter: DocumentMetadataFilter) -> Optional[str]:\n        \"\"\"Converts a DocumentMetdataFilter to the expression that Zilliz takes.\n\n        Args:\n            filter (DocumentMetadataFilter): The Filter to convert to Zilliz expression.\n\n        Returns:\n            Optional[str]: The filter if valid, otherwise None.\n        \"\"\"\n        filters = []\n        # Go through all the fields and thier values\n        for field, value in filter.dict().items():\n            # Check if the Value is empty\n            if value is not None:\n                # Convert start_date to int and add greater than or equal logic\n                if field == \"start_date\":\n                    filters.append(\n                        \"(created_at >= \" + str(to_unix_timestamp(value)) + \")\"\n                    )\n                # Convert end_date to int and add less than or equal logic\n                elif field == \"end_date\":\n                    filters.append(\n                        \"(created_at <= \" + str(to_unix_timestamp(value)) + \")\"\n                    )\n                # Convert Source to its string value and check equivalency\n                elif field == \"source\":\n                    filters.append(\"(\" + field + ' == \"' + str(value.value) + '\")')\n                # Check equivalency of rest of string fields\n                else:\n                    filters.append(\"(\" + field + ' == \"' + str(value) + '\")')\n        # Join all our expressions with `and``\n        return \" and \".join(filters)", ""]}
{"filename": "datastore/providers/weaviate_datastore.py", "chunked_list": ["# TODO\nimport asyncio\nfrom typing import Dict, List, Optional\nfrom loguru import logger\nfrom weaviate import Client\nimport weaviate\nimport os\nimport uuid\n\nfrom weaviate.util import generate_uuid5", "\nfrom weaviate.util import generate_uuid5\n\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,", "    QueryResult,\n    QueryWithEmbedding,\n    DocumentChunkWithScore,\n    Source,\n)\n\n\nWEAVIATE_HOST = os.environ.get(\"WEAVIATE_HOST\", \"http://127.0.0.1\")\nWEAVIATE_PORT = os.environ.get(\"WEAVIATE_PORT\", \"8080\")\nWEAVIATE_USERNAME = os.environ.get(\"WEAVIATE_USERNAME\", None)", "WEAVIATE_PORT = os.environ.get(\"WEAVIATE_PORT\", \"8080\")\nWEAVIATE_USERNAME = os.environ.get(\"WEAVIATE_USERNAME\", None)\nWEAVIATE_PASSWORD = os.environ.get(\"WEAVIATE_PASSWORD\", None)\nWEAVIATE_SCOPES = os.environ.get(\"WEAVIATE_SCOPE\", None)\nWEAVIATE_INDEX = os.environ.get(\"WEAVIATE_INDEX\", \"OpenAIDocument\")\n\nWEAVIATE_BATCH_SIZE = int(os.environ.get(\"WEAVIATE_BATCH_SIZE\", 20))\nWEAVIATE_BATCH_DYNAMIC = os.environ.get(\"WEAVIATE_BATCH_DYNAMIC\", False)\nWEAVIATE_BATCH_TIMEOUT_RETRIES = int(os.environ.get(\"WEAVIATE_TIMEOUT_RETRIES\", 3))\nWEAVIATE_BATCH_NUM_WORKERS = int(os.environ.get(\"WEAVIATE_BATCH_NUM_WORKERS\", 1))", "WEAVIATE_BATCH_TIMEOUT_RETRIES = int(os.environ.get(\"WEAVIATE_TIMEOUT_RETRIES\", 3))\nWEAVIATE_BATCH_NUM_WORKERS = int(os.environ.get(\"WEAVIATE_BATCH_NUM_WORKERS\", 1))\n\nSCHEMA = {\n    \"class\": WEAVIATE_INDEX,\n    \"description\": \"The main class\",\n    \"properties\": [\n        {\n            \"name\": \"chunk_id\",\n            \"dataType\": [\"string\"],", "            \"name\": \"chunk_id\",\n            \"dataType\": [\"string\"],\n            \"description\": \"The chunk id\",\n        },\n        {\n            \"name\": \"document_id\",\n            \"dataType\": [\"string\"],\n            \"description\": \"The document id\",\n        },\n        {", "        },\n        {\n            \"name\": \"text\",\n            \"dataType\": [\"text\"],\n            \"description\": \"The chunk's text\",\n        },\n        {\n            \"name\": \"source\",\n            \"dataType\": [\"string\"],\n            \"description\": \"The source of the data\",", "            \"dataType\": [\"string\"],\n            \"description\": \"The source of the data\",\n        },\n        {\n            \"name\": \"source_id\",\n            \"dataType\": [\"string\"],\n            \"description\": \"The source id\",\n        },\n        {\n            \"name\": \"url\",", "        {\n            \"name\": \"url\",\n            \"dataType\": [\"string\"],\n            \"description\": \"The source url\",\n        },\n        {\n            \"name\": \"created_at\",\n            \"dataType\": [\"date\"],\n            \"description\": \"Creation date of document\",\n        },", "            \"description\": \"Creation date of document\",\n        },\n        {\n            \"name\": \"author\",\n            \"dataType\": [\"string\"],\n            \"description\": \"Document author\",\n        },\n    ],\n}\n", "}\n\n\ndef extract_schema_properties(schema):\n    properties = schema[\"properties\"]\n\n    return {property[\"name\"] for property in properties}\n\n\nclass WeaviateDataStore(DataStore):\n    def handle_errors(self, results: Optional[List[dict]]) -> List[str]:\n        if not self or not results:\n            return []\n\n        error_messages = []\n        for result in results:\n            if (\n                \"result\" not in result\n                or \"errors\" not in result[\"result\"]\n                or \"error\" not in result[\"result\"][\"errors\"]\n            ):\n                continue\n            for message in result[\"result\"][\"errors\"][\"error\"]:\n                error_messages.append(message[\"message\"])\n                logger.exception(message[\"message\"])\n\n        return error_messages\n\n    def __init__(self):\n        auth_credentials = self._build_auth_credentials()\n\n        url = f\"{WEAVIATE_HOST}:{WEAVIATE_PORT}\"\n\n        logger.debug(\n            f\"Connecting to weaviate instance at {url} with credential type {type(auth_credentials).__name__}\"\n        )\n        self.client = Client(url, auth_client_secret=auth_credentials)\n        self.client.batch.configure(\n            batch_size=WEAVIATE_BATCH_SIZE,\n            dynamic=WEAVIATE_BATCH_DYNAMIC,  # type: ignore\n            callback=self.handle_errors,  # type: ignore\n            timeout_retries=WEAVIATE_BATCH_TIMEOUT_RETRIES,\n            num_workers=WEAVIATE_BATCH_NUM_WORKERS,\n        )\n\n        if self.client.schema.contains(SCHEMA):\n            current_schema = self.client.schema.get(WEAVIATE_INDEX)\n            current_schema_properties = extract_schema_properties(current_schema)\n\n            logger.debug(\n                f\"Found index {WEAVIATE_INDEX} with properties {current_schema_properties}\"\n            )\n            logger.debug(\"Will reuse this schema\")\n        else:\n            new_schema_properties = extract_schema_properties(SCHEMA)\n            logger.debug(\n                f\"Creating index {WEAVIATE_INDEX} with properties {new_schema_properties}\"\n            )\n            self.client.schema.create_class(SCHEMA)\n\n    @staticmethod\n    def _build_auth_credentials():\n        if WEAVIATE_USERNAME and WEAVIATE_PASSWORD:\n            return weaviate.auth.AuthClientPassword(\n                WEAVIATE_USERNAME, WEAVIATE_PASSWORD, WEAVIATE_SCOPES\n            )\n        else:\n            return None\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n        doc_ids = []\n\n        with self.client.batch as batch:\n            for doc_id, doc_chunks in chunks.items():\n                logger.debug(f\"Upserting {doc_id} with {len(doc_chunks)} chunks\")\n                for doc_chunk in doc_chunks:\n                    # we generate a uuid regardless of the format of the document_id because\n                    # weaviate needs a uuid to store each document chunk and\n                    # a document chunk cannot share the same uuid\n                    doc_uuid = generate_uuid5(doc_chunk, WEAVIATE_INDEX)\n                    metadata = doc_chunk.metadata\n                    doc_chunk_dict = doc_chunk.dict()\n                    doc_chunk_dict.pop(\"metadata\")\n                    for key, value in metadata.dict().items():\n                        doc_chunk_dict[key] = value\n                    doc_chunk_dict[\"chunk_id\"] = doc_chunk_dict.pop(\"id\")\n                    doc_chunk_dict[\"source\"] = (\n                        doc_chunk_dict.pop(\"source\").value\n                        if doc_chunk_dict[\"source\"]\n                        else None\n                    )\n                    embedding = doc_chunk_dict.pop(\"embedding\")\n\n                    batch.add_data_object(\n                        uuid=doc_uuid,\n                        data_object=doc_chunk_dict,\n                        class_name=WEAVIATE_INDEX,\n                        vector=embedding,\n                    )\n\n                doc_ids.append(doc_id)\n            batch.flush()\n        return doc_ids\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n            logger.debug(f\"Query: {query.query}\")\n            if not hasattr(query, \"filter\") or not query.filter:\n                result = (\n                    self.client.query.get(\n                        WEAVIATE_INDEX,\n                        [\n                            \"chunk_id\",\n                            \"document_id\",\n                            \"text\",\n                            \"source\",\n                            \"source_id\",\n                            \"url\",\n                            \"created_at\",\n                            \"author\",\n                        ],\n                    )\n                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n                    .with_limit(query.top_k)  # type: ignore\n                    .with_additional([\"score\", \"vector\"])\n                    .do()\n                )\n            else:\n                filters_ = self.build_filters(query.filter)\n                result = (\n                    self.client.query.get(\n                        WEAVIATE_INDEX,\n                        [\n                            \"chunk_id\",\n                            \"document_id\",\n                            \"text\",\n                            \"source\",\n                            \"source_id\",\n                            \"url\",\n                            \"created_at\",\n                            \"author\",\n                        ],\n                    )\n                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n                    .with_where(filters_)\n                    .with_limit(query.top_k)  # type: ignore\n                    .with_additional([\"score\", \"vector\"])\n                    .do()\n                )\n\n            query_results: List[DocumentChunkWithScore] = []\n            response = result[\"data\"][\"Get\"][WEAVIATE_INDEX]\n\n            for resp in response:\n                result = DocumentChunkWithScore(\n                    id=resp[\"chunk_id\"],\n                    text=resp[\"text\"],\n                    embedding=resp[\"_additional\"][\"vector\"],\n                    score=resp[\"_additional\"][\"score\"],\n                    metadata=DocumentChunkMetadata(\n                        document_id=resp[\"document_id\"] if resp[\"document_id\"] else \"\",\n                        source=Source(resp[\"source\"]),\n                        source_id=resp[\"source_id\"],\n                        url=resp[\"url\"],\n                        created_at=resp[\"created_at\"],\n                        author=resp[\"author\"],\n                    ),\n                )\n                query_results.append(result)\n            return QueryResult(query=query.query, results=query_results)\n\n        return await asyncio.gather(*[_single_query(query) for query in queries])\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        # TODO\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Returns whether the operation was successful.\n        \"\"\"\n        if delete_all:\n            logger.debug(f\"Deleting all vectors in index {WEAVIATE_INDEX}\")\n            self.client.schema.delete_all()\n            return True\n\n        if ids:\n            operands = [\n                {\"path\": [\"document_id\"], \"operator\": \"Equal\", \"valueString\": id}\n                for id in ids\n            ]\n\n            where_clause = {\"operator\": \"Or\", \"operands\": operands}\n\n            logger.debug(f\"Deleting vectors from index {WEAVIATE_INDEX} with ids {ids}\")\n            result = self.client.batch.delete_objects(\n                class_name=WEAVIATE_INDEX, where=where_clause, output=\"verbose\"\n            )\n\n            if not bool(result[\"results\"][\"successful\"]):\n                logger.debug(\n                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n                )\n\n        if filter:\n            where_clause = self.build_filters(filter)\n\n            logger.debug(\n                f\"Deleting vectors from index {WEAVIATE_INDEX} with filter {where_clause}\"\n            )\n            result = self.client.batch.delete_objects(\n                class_name=WEAVIATE_INDEX, where=where_clause\n            )\n\n            if not bool(result[\"results\"][\"successful\"]):\n                logger.debug(\n                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n                )\n\n        return True\n\n    @staticmethod\n    def build_filters(filter):\n        if filter.source:\n            filter.source = filter.source.value\n\n        operands = []\n        filter_conditions = {\n            \"source\": {\n                \"operator\": \"Equal\",\n                \"value\": \"query.filter.source.value\",\n                \"value_key\": \"valueString\",\n            },\n            \"start_date\": {\"operator\": \"GreaterThanEqual\", \"value_key\": \"valueDate\"},\n            \"end_date\": {\"operator\": \"LessThanEqual\", \"value_key\": \"valueDate\"},\n            \"default\": {\"operator\": \"Equal\", \"value_key\": \"valueString\"},\n        }\n\n        for attr, value in filter.__dict__.items():\n            if value is not None:\n                filter_condition = filter_conditions.get(\n                    attr, filter_conditions[\"default\"]\n                )\n                value_key = filter_condition[\"value_key\"]\n\n                operand = {\n                    \"path\": [\n                        attr\n                        if not (attr == \"start_date\" or attr == \"end_date\")\n                        else \"created_at\"\n                    ],\n                    \"operator\": filter_condition[\"operator\"],\n                    value_key: value,\n                }\n\n                operands.append(operand)\n\n        return {\"operator\": \"And\", \"operands\": operands}\n\n    @staticmethod\n    def _is_valid_weaviate_id(candidate_id: str) -> bool:\n        \"\"\"\n        Check if candidate_id is a valid UUID for weaviate's use\n\n        Weaviate supports UUIDs of version 3, 4 and 5. This function checks if the candidate_id is a valid UUID of one of these versions.\n        See https://weaviate.io/developers/weaviate/more-resources/faq#q-are-there-restrictions-on-uuid-formatting-do-i-have-to-adhere-to-any-standards\n        for more information.\n        \"\"\"\n        acceptable_version = [3, 4, 5]\n\n        try:\n            result = uuid.UUID(candidate_id)\n            if result.version not in acceptable_version:\n                return False\n            else:\n                return True\n        except ValueError:\n            return False", "\nclass WeaviateDataStore(DataStore):\n    def handle_errors(self, results: Optional[List[dict]]) -> List[str]:\n        if not self or not results:\n            return []\n\n        error_messages = []\n        for result in results:\n            if (\n                \"result\" not in result\n                or \"errors\" not in result[\"result\"]\n                or \"error\" not in result[\"result\"][\"errors\"]\n            ):\n                continue\n            for message in result[\"result\"][\"errors\"][\"error\"]:\n                error_messages.append(message[\"message\"])\n                logger.exception(message[\"message\"])\n\n        return error_messages\n\n    def __init__(self):\n        auth_credentials = self._build_auth_credentials()\n\n        url = f\"{WEAVIATE_HOST}:{WEAVIATE_PORT}\"\n\n        logger.debug(\n            f\"Connecting to weaviate instance at {url} with credential type {type(auth_credentials).__name__}\"\n        )\n        self.client = Client(url, auth_client_secret=auth_credentials)\n        self.client.batch.configure(\n            batch_size=WEAVIATE_BATCH_SIZE,\n            dynamic=WEAVIATE_BATCH_DYNAMIC,  # type: ignore\n            callback=self.handle_errors,  # type: ignore\n            timeout_retries=WEAVIATE_BATCH_TIMEOUT_RETRIES,\n            num_workers=WEAVIATE_BATCH_NUM_WORKERS,\n        )\n\n        if self.client.schema.contains(SCHEMA):\n            current_schema = self.client.schema.get(WEAVIATE_INDEX)\n            current_schema_properties = extract_schema_properties(current_schema)\n\n            logger.debug(\n                f\"Found index {WEAVIATE_INDEX} with properties {current_schema_properties}\"\n            )\n            logger.debug(\"Will reuse this schema\")\n        else:\n            new_schema_properties = extract_schema_properties(SCHEMA)\n            logger.debug(\n                f\"Creating index {WEAVIATE_INDEX} with properties {new_schema_properties}\"\n            )\n            self.client.schema.create_class(SCHEMA)\n\n    @staticmethod\n    def _build_auth_credentials():\n        if WEAVIATE_USERNAME and WEAVIATE_PASSWORD:\n            return weaviate.auth.AuthClientPassword(\n                WEAVIATE_USERNAME, WEAVIATE_PASSWORD, WEAVIATE_SCOPES\n            )\n        else:\n            return None\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n        doc_ids = []\n\n        with self.client.batch as batch:\n            for doc_id, doc_chunks in chunks.items():\n                logger.debug(f\"Upserting {doc_id} with {len(doc_chunks)} chunks\")\n                for doc_chunk in doc_chunks:\n                    # we generate a uuid regardless of the format of the document_id because\n                    # weaviate needs a uuid to store each document chunk and\n                    # a document chunk cannot share the same uuid\n                    doc_uuid = generate_uuid5(doc_chunk, WEAVIATE_INDEX)\n                    metadata = doc_chunk.metadata\n                    doc_chunk_dict = doc_chunk.dict()\n                    doc_chunk_dict.pop(\"metadata\")\n                    for key, value in metadata.dict().items():\n                        doc_chunk_dict[key] = value\n                    doc_chunk_dict[\"chunk_id\"] = doc_chunk_dict.pop(\"id\")\n                    doc_chunk_dict[\"source\"] = (\n                        doc_chunk_dict.pop(\"source\").value\n                        if doc_chunk_dict[\"source\"]\n                        else None\n                    )\n                    embedding = doc_chunk_dict.pop(\"embedding\")\n\n                    batch.add_data_object(\n                        uuid=doc_uuid,\n                        data_object=doc_chunk_dict,\n                        class_name=WEAVIATE_INDEX,\n                        vector=embedding,\n                    )\n\n                doc_ids.append(doc_id)\n            batch.flush()\n        return doc_ids\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n            logger.debug(f\"Query: {query.query}\")\n            if not hasattr(query, \"filter\") or not query.filter:\n                result = (\n                    self.client.query.get(\n                        WEAVIATE_INDEX,\n                        [\n                            \"chunk_id\",\n                            \"document_id\",\n                            \"text\",\n                            \"source\",\n                            \"source_id\",\n                            \"url\",\n                            \"created_at\",\n                            \"author\",\n                        ],\n                    )\n                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n                    .with_limit(query.top_k)  # type: ignore\n                    .with_additional([\"score\", \"vector\"])\n                    .do()\n                )\n            else:\n                filters_ = self.build_filters(query.filter)\n                result = (\n                    self.client.query.get(\n                        WEAVIATE_INDEX,\n                        [\n                            \"chunk_id\",\n                            \"document_id\",\n                            \"text\",\n                            \"source\",\n                            \"source_id\",\n                            \"url\",\n                            \"created_at\",\n                            \"author\",\n                        ],\n                    )\n                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n                    .with_where(filters_)\n                    .with_limit(query.top_k)  # type: ignore\n                    .with_additional([\"score\", \"vector\"])\n                    .do()\n                )\n\n            query_results: List[DocumentChunkWithScore] = []\n            response = result[\"data\"][\"Get\"][WEAVIATE_INDEX]\n\n            for resp in response:\n                result = DocumentChunkWithScore(\n                    id=resp[\"chunk_id\"],\n                    text=resp[\"text\"],\n                    embedding=resp[\"_additional\"][\"vector\"],\n                    score=resp[\"_additional\"][\"score\"],\n                    metadata=DocumentChunkMetadata(\n                        document_id=resp[\"document_id\"] if resp[\"document_id\"] else \"\",\n                        source=Source(resp[\"source\"]),\n                        source_id=resp[\"source_id\"],\n                        url=resp[\"url\"],\n                        created_at=resp[\"created_at\"],\n                        author=resp[\"author\"],\n                    ),\n                )\n                query_results.append(result)\n            return QueryResult(query=query.query, results=query_results)\n\n        return await asyncio.gather(*[_single_query(query) for query in queries])\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        # TODO\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Returns whether the operation was successful.\n        \"\"\"\n        if delete_all:\n            logger.debug(f\"Deleting all vectors in index {WEAVIATE_INDEX}\")\n            self.client.schema.delete_all()\n            return True\n\n        if ids:\n            operands = [\n                {\"path\": [\"document_id\"], \"operator\": \"Equal\", \"valueString\": id}\n                for id in ids\n            ]\n\n            where_clause = {\"operator\": \"Or\", \"operands\": operands}\n\n            logger.debug(f\"Deleting vectors from index {WEAVIATE_INDEX} with ids {ids}\")\n            result = self.client.batch.delete_objects(\n                class_name=WEAVIATE_INDEX, where=where_clause, output=\"verbose\"\n            )\n\n            if not bool(result[\"results\"][\"successful\"]):\n                logger.debug(\n                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n                )\n\n        if filter:\n            where_clause = self.build_filters(filter)\n\n            logger.debug(\n                f\"Deleting vectors from index {WEAVIATE_INDEX} with filter {where_clause}\"\n            )\n            result = self.client.batch.delete_objects(\n                class_name=WEAVIATE_INDEX, where=where_clause\n            )\n\n            if not bool(result[\"results\"][\"successful\"]):\n                logger.debug(\n                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n                )\n\n        return True\n\n    @staticmethod\n    def build_filters(filter):\n        if filter.source:\n            filter.source = filter.source.value\n\n        operands = []\n        filter_conditions = {\n            \"source\": {\n                \"operator\": \"Equal\",\n                \"value\": \"query.filter.source.value\",\n                \"value_key\": \"valueString\",\n            },\n            \"start_date\": {\"operator\": \"GreaterThanEqual\", \"value_key\": \"valueDate\"},\n            \"end_date\": {\"operator\": \"LessThanEqual\", \"value_key\": \"valueDate\"},\n            \"default\": {\"operator\": \"Equal\", \"value_key\": \"valueString\"},\n        }\n\n        for attr, value in filter.__dict__.items():\n            if value is not None:\n                filter_condition = filter_conditions.get(\n                    attr, filter_conditions[\"default\"]\n                )\n                value_key = filter_condition[\"value_key\"]\n\n                operand = {\n                    \"path\": [\n                        attr\n                        if not (attr == \"start_date\" or attr == \"end_date\")\n                        else \"created_at\"\n                    ],\n                    \"operator\": filter_condition[\"operator\"],\n                    value_key: value,\n                }\n\n                operands.append(operand)\n\n        return {\"operator\": \"And\", \"operands\": operands}\n\n    @staticmethod\n    def _is_valid_weaviate_id(candidate_id: str) -> bool:\n        \"\"\"\n        Check if candidate_id is a valid UUID for weaviate's use\n\n        Weaviate supports UUIDs of version 3, 4 and 5. This function checks if the candidate_id is a valid UUID of one of these versions.\n        See https://weaviate.io/developers/weaviate/more-resources/faq#q-are-there-restrictions-on-uuid-formatting-do-i-have-to-adhere-to-any-standards\n        for more information.\n        \"\"\"\n        acceptable_version = [3, 4, 5]\n\n        try:\n            result = uuid.UUID(candidate_id)\n            if result.version not in acceptable_version:\n                return False\n            else:\n                return True\n        except ValueError:\n            return False", ""]}
{"filename": "datastore/providers/milvus_datastore.py", "chunked_list": ["import os\nimport asyncio\n\nfrom typing import Dict, List, Optional\nfrom pymilvus import (\n    Collection,\n    connections,\n    utility,\n    FieldSchema,\n    DataType,", "    FieldSchema,\n    DataType,\n    CollectionSchema,\n    MilvusException,\n)\nfrom uuid import uuid4\n\n\nfrom services.date import to_unix_timestamp\nfrom datastore.datastore import DataStore", "from services.date import to_unix_timestamp\nfrom datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentChunkMetadata,\n    Source,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,\n    DocumentChunkWithScore,", "    QueryWithEmbedding,\n    DocumentChunkWithScore,\n)\n\nMILVUS_COLLECTION = os.environ.get(\"MILVUS_COLLECTION\") or \"c\" + uuid4().hex\nMILVUS_HOST = os.environ.get(\"MILVUS_HOST\") or \"localhost\"\nMILVUS_PORT = os.environ.get(\"MILVUS_PORT\") or 19530\nMILVUS_USER = os.environ.get(\"MILVUS_USER\")\nMILVUS_PASSWORD = os.environ.get(\"MILVUS_PASSWORD\")\nMILVUS_USE_SECURITY = False if MILVUS_PASSWORD is None else True", "MILVUS_PASSWORD = os.environ.get(\"MILVUS_PASSWORD\")\nMILVUS_USE_SECURITY = False if MILVUS_PASSWORD is None else True\n\nUPSERT_BATCH_SIZE = 100\nOUTPUT_DIM = 1536\n\n\nclass Required:\n    pass\n", "\n\n# The fields names that we are going to be storing within Milvus, the field declaration for schema creation, and the default value\nSCHEMA = [\n    (\n        \"pk\",\n        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n        Required,\n    ),\n    (", "    ),\n    (\n        \"embedding\",\n        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=OUTPUT_DIM),\n        Required,\n    ),\n    (\n        \"text\",\n        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n        Required,", "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n        Required,\n    ),\n    (\n        \"document_id\",\n        FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\n        \"source_id\",", "    (\n        \"source_id\",\n        FieldSchema(name=\"source_id\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\n        \"id\",\n        FieldSchema(\n            name=\"id\",\n            dtype=DataType.VARCHAR,", "            name=\"id\",\n            dtype=DataType.VARCHAR,\n            max_length=65535,\n        ),\n        \"\",\n    ),\n    (\n        \"source\",\n        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",", "        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),\n    (\"url\", FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=65535), \"\"),\n    (\"created_at\", FieldSchema(name=\"created_at\", dtype=DataType.INT64), -1),\n    (\n        \"author\",\n        FieldSchema(name=\"author\", dtype=DataType.VARCHAR, max_length=65535),\n        \"\",\n    ),", "        \"\",\n    ),\n]\n\n\nclass MilvusDataStore(DataStore):\n    def __init__(\n        self,\n        create_new: Optional[bool] = False,\n        index_params: Optional[dict] = None,\n        search_params: Optional[dict] = None,\n    ):\n        \"\"\"Create a Milvus DataStore.\n\n        The Milvus Datastore allows for storing your indexes and metadata within a Milvus instance.\n\n        Args:\n            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.\n            index_params (Optional[dict], optional): Custom index params to use. Defaults to None.\n            search_params (Optional[dict], optional): Custom search params to use. Defaults to None.\n        \"\"\"\n\n        # # TODO: Auto infer the fields\n        # non_string_fields = [('embedding', List[float]), ('created_at', int)]\n        # fields_to_index = list(DocumentChunkMetadata.__fields__.keys())\n        # fields_to_index = list(DocumentChunk.__fields__.keys())\n\n        # Set the index_params to passed in or the default\n        self.index_params = index_params\n\n        # The default search params\n        self.default_search_params = {\n            \"IVF_FLAT\": {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n            \"IVF_SQ8\": {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n            \"IVF_PQ\": {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n            \"HNSW\": {\"metric_type\": \"L2\", \"params\": {\"ef\": 10}},\n            \"RHNSW_FLAT\": {\"metric_type\": \"L2\", \"params\": {\"ef\": 10}},\n            \"RHNSW_SQ\": {\"metric_type\": \"L2\", \"params\": {\"ef\": 10}},\n            \"RHNSW_PQ\": {\"metric_type\": \"L2\", \"params\": {\"ef\": 10}},\n            \"IVF_HNSW\": {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10, \"ef\": 10}},\n            \"ANNOY\": {\"metric_type\": \"L2\", \"params\": {\"search_k\": 10}},\n            \"AUTOINDEX\": {\"metric_type\": \"L2\", \"params\": {}},\n        }\n\n        # Check if the connection already exists\n        try:\n            i = [\n                connections.get_connection_addr(x[0])\n                for x in connections.list_connections()\n            ].index({\"host\": MILVUS_HOST, \"port\": MILVUS_PORT})\n            self.alias = connections.list_connections()[i][0]\n        except ValueError:\n            # Connect to the Milvus instance using the passed in Enviroment variables\n            self.alias = uuid4().hex\n            connections.connect(\n                alias=self.alias,\n                host=MILVUS_HOST,\n                port=MILVUS_PORT,\n                user=MILVUS_USER,  # type: ignore\n                password=MILVUS_PASSWORD,  # type: ignore\n                secure=MILVUS_USE_SECURITY,\n            )\n\n        self._create_collection(create_new)  # type: ignore\n\n        index_params = self.index_params or {}\n\n        # Use in the passed in search params or the default for the specified index\n        self.search_params = (\n            search_params or self.default_search_params[index_params[\"index_type\"]]\n        )\n\n    def _create_collection(self, create_new: bool) -> None:\n        \"\"\"Create a collection based on enviroment and passed in variables.\n\n        Args:\n            create_new (bool): Whether to overwrite if collection already exists.\n        \"\"\"\n\n        # If the collection exists and create_new is True, drop the existing collection\n        if utility.has_collection(MILVUS_COLLECTION, using=self.alias) and create_new:\n            utility.drop_collection(MILVUS_COLLECTION, using=self.alias)\n\n        # Check if the collection doesnt exist\n        if utility.has_collection(MILVUS_COLLECTION, using=self.alias) is False:\n            # If it doesnt exist use the field params from init to create a new schem\n            schema = [field[1] for field in SCHEMA]\n            schema = CollectionSchema(schema)\n            # Use the schema to create a new collection\n            self.col = Collection(\n                MILVUS_COLLECTION,\n                schema=schema,\n                consistency_level=\"Strong\",\n                using=self.alias,\n            )\n        else:\n            # If the collection exists, point to it\n            self.col = Collection(\n                MILVUS_COLLECTION, consistency_level=\"Strong\", using=self.alias\n            )  # type: ignore\n\n        # If no index on the collection, create one\n        if len(self.col.indexes) == 0:\n            if self.index_params != None:\n                # Create an index on the 'embedding' field with the index params found in init\n                self.col.create_index(\"embedding\", index_params=self.index_params)\n            else:\n                # If no index param supplied, to first create an HNSW index for Milvus\n                try:\n                    print(\"Attempting creation of Milvus default index\")\n                    i_p = {\n                        \"metric_type\": \"L2\",\n                        \"index_type\": \"HNSW\",\n                        \"params\": {\"M\": 8, \"efConstruction\": 64},\n                    }\n\n                    self.col.create_index(\"embedding\", index_params=i_p)\n                    self.index_params = i_p\n                    print(\"Creation of Milvus default index succesful\")\n                # If create fails, most likely due to being Zilliz Cloud instance, try to create an AutoIndex\n                except MilvusException:\n                    print(\"Attempting creation of Zilliz Cloud default index\")\n                    i_p = {\"metric_type\": \"L2\", \"index_type\": \"AUTOINDEX\", \"params\": {}}\n                    self.col.create_index(\"embedding\", index_params=i_p)\n                    self.index_params = i_p\n                    print(\"Creation of Zilliz Cloud default index succesful\")\n        # If an index already exists, grab its params\n        else:\n            self.index_params = self.col.indexes[0].to_dict()['index_param']\n            \n\n\n        self.col.load()\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"Upsert chunks into the datastore.\n\n        Args:\n            chunks (Dict[str, List[DocumentChunk]]): A list of DocumentChunks to insert\n\n        Raises:\n            e: Error in upserting data.\n\n        Returns:\n            List[str]: The document_id's that were inserted.\n        \"\"\"\n        # The doc id's to return for the upsert\n        doc_ids: List[str] = []\n        # List to collect all the insert data\n        insert_data = [[] for _ in range(len(SCHEMA) - 1)]\n        # Go through each document chunklist and grab the data\n        for doc_id, chunk_list in chunks.items():\n            # Append the doc_id to the list we are returning\n            doc_ids.append(doc_id)\n            # Examine each chunk in the chunklist\n            for chunk in chunk_list:\n                # Extract data from the chunk\n                list_of_data = self._get_values(chunk)\n                # Check if the data is valid\n                if list_of_data is not None:\n                    # Append each field to the insert_data\n                    for x in range(len(insert_data)):\n                        insert_data[x].append(list_of_data[x])\n        # Slice up our insert data into batches\n        batches = [\n            insert_data[i : i + UPSERT_BATCH_SIZE]\n            for i in range(0, len(insert_data), UPSERT_BATCH_SIZE)\n        ]\n\n        # Attempt to insert each batch into our collection\n        for batch in batches:\n            if len(batch[0]) != 0:\n                try:\n                    print(f\"Upserting batch of size {len(batch[0])}\")\n                    self.col.insert(batch)\n                    print(f\"Upserted batch successfully\")\n                except Exception as e:\n                    print(f\"Error upserting batch: {e}\")\n                    raise e\n\n        # This setting perfoms flushes after insert. Small insert == bad to use\n        # self.col.flush()\n\n        return doc_ids\n\n    def _get_values(self, chunk: DocumentChunk) -> List[any] | None:  # type: ignore\n        \"\"\"Convert the chunk into a list of values to insert whose indexes align with fields.\n\n        Args:\n            chunk (DocumentChunk): The chunk to convert.\n\n        Returns:\n            List (any): The values to insert.\n        \"\"\"\n        # Convert DocumentChunk and its sub models to dict\n        values = chunk.dict()\n        # Unpack the metadata into the same dict\n        meta = values.pop(\"metadata\")\n        values.update(meta)\n\n        # Convert date to int timestamp form\n        if values[\"created_at\"]:\n            values[\"created_at\"] = to_unix_timestamp(values[\"created_at\"])\n\n        # If source exists, change from Source object to the string value it holds\n        if values[\"source\"]:\n            values[\"source\"] = values[\"source\"].value\n        # List to collect data we will return\n        ret = []\n        # Grab data responding to each field excluding the hidden auto pk field\n        for key, _, default in SCHEMA[1:]:\n            # Grab the data at the key and default to our defaults set in init\n            x = values.get(key) or default\n            # If one of our required fields is missing, ignore the entire entry\n            if x is Required:\n                print(\"Chunk \" + values[\"id\"] + \" missing \" + key + \" skipping\")\n                return None\n            # Add the corresponding value if it passes the tests\n            ret.append(x)\n        return ret\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"Query the QueryWithEmbedding against the MilvusDocumentSearch\n\n        Search the embedding and its filter in the collection.\n\n        Args:\n            queries (List[QueryWithEmbedding]): The list of searches to perform.\n\n        Returns:\n            List[QueryResult]: Results for each search.\n        \"\"\"\n        # Async to perform the query, adapted from pinecone implementation\n        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n\n            filter = None\n            # Set the filter to expression that is valid for Milvus\n            if query.filter != None:\n                # Either a valid filter or None will be returned\n                filter = self._get_filter(query.filter)\n\n            # Perform our search\n            res = self.col.search(\n                data=[query.embedding],\n                anns_field=\"embedding\",\n                param=self.search_params,\n                limit=query.top_k,\n                expr=filter,\n                output_fields=[\n                    field[0] for field in SCHEMA[2:]\n                ],  # Ignoring pk, embedding\n            )\n            # Results that will hold our DocumentChunkWithScores\n            results = []\n            # Parse every result for our search\n            for hit in res[0]:  # type: ignore\n                # The distance score for the search result, falls under DocumentChunkWithScore\n                score = hit.score\n                # Our metadata info, falls under DocumentChunkMetadata\n                metadata = {}\n                # Grab the values that correspond to our fields, ignore pk and embedding.\n                for x in [field[0] for field in SCHEMA[2:]]:\n                    metadata[x] = hit.entity.get(x)\n                # If the source isnt valid, conver to None\n                if metadata[\"source\"] not in Source.__members__:\n                    metadata[\"source\"] = None\n                # Text falls under the DocumentChunk\n                text = metadata.pop(\"text\")\n                # Id falls under the DocumentChunk\n                ids = metadata.pop(\"id\")\n                chunk = DocumentChunkWithScore(\n                    id=ids,\n                    score=score,\n                    text=text,\n                    metadata=DocumentChunkMetadata(**metadata),\n                )\n                results.append(chunk)\n\n            # TODO: decide on doing queries to grab the embedding itself, slows down performance as double query occurs\n\n            return QueryResult(query=query.query, results=results)\n\n        results: List[QueryResult] = await asyncio.gather(\n            *[_single_query(query) for query in queries]\n        )\n        return results\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"Delete the entities based either on the chunk_id of the vector,\n\n        Args:\n            ids (Optional[List[str]], optional): The document_ids to delete. Defaults to None.\n            filter (Optional[DocumentMetadataFilter], optional): The filter to delet by. Defaults to None.\n            delete_all (Optional[bool], optional): Whether to drop the collection and recreate it. Defaults to None.\n        \"\"\"\n        # If deleting all, drop and create the new collection\n        if delete_all:\n            # Release the collection from memory\n            self.col.release()\n            # Drop the collection\n            self.col.drop()\n            # Recreate the new collection\n            self._create_collection(True)\n            return True\n\n        # Keep track of how many we have deleted for later printing\n        delete_count = 0\n\n        # Check if empty ids\n        if ids != None:\n            if len(ids) != 0:\n                # Add quotation marks around the string format id\n                ids = ['\"' + str(id) + '\"' for id in ids]\n                # Query for the pk's of entries that match id's\n                ids = self.col.query(f\"document_id in [{','.join(ids)}]\")\n                # Convert to list of pks\n                ids = [str(entry[\"pk\"]) for entry in ids]  # type: ignore\n                # Check to see if there are valid pk's to delete\n                if len(ids) != 0:\n                    # Delete the entries for each pk\n                    res = self.col.delete(f\"pk in [{','.join(ids)}]\")\n                    # Incremet our deleted count\n                    delete_count += int(res.delete_count)  # type: ignore\n\n        # Check if empty filter\n        if filter != None:\n            # Convert filter to milvus expression\n            filter = self._get_filter(filter)  # type: ignore\n            # Check if there is anything to filter\n            if len(filter) != 0:  # type: ignore\n                # Query for the pk's of entries that match filter\n                filter = self.col.query(filter)  # type: ignore\n                # Convert to list of pks\n                filter = [str(entry[\"pk\"]) for entry in filter]  # type: ignore\n                # Check to see if there are valid pk's to delete\n                if len(filter) != 0:  # type: ignore\n                    # Delete the entries\n                    res = self.col.delete(f\"pk in [{','.join(filter)}]\")  # type: ignore\n                    # Increment our delete count\n                    delete_count += int(res.delete_count)  # type: ignore\n\n        # This setting perfoms flushes after delete. Small delete == bad to use\n        # self.col.flush()\n\n        return True\n\n    def _get_filter(self, filter: DocumentMetadataFilter) -> Optional[str]:\n        \"\"\"Converts a DocumentMetdataFilter to the expression that Milvus takes.\n\n        Args:\n            filter (DocumentMetadataFilter): The Filter to convert to Milvus expression.\n\n        Returns:\n            Optional[str]: The filter if valid, otherwise None.\n        \"\"\"\n        filters = []\n        # Go through all the fields and thier values\n        for field, value in filter.dict().items():\n            # Check if the Value is empty\n            if value is not None:\n                # Convert start_date to int and add greater than or equal logic\n                if field == \"start_date\":\n                    filters.append(\n                        \"(created_at >= \" + str(to_unix_timestamp(value)) + \")\"\n                    )\n                # Convert end_date to int and add less than or equal logic\n                elif field == \"end_date\":\n                    filters.append(\n                        \"(created_at <= \" + str(to_unix_timestamp(value)) + \")\"\n                    )\n                # Convert Source to its string value and check equivalency\n                elif field == \"source\":\n                    filters.append(\"(\" + field + ' == \"' + str(value.value) + '\")')\n                # Check equivalency of rest of string fields\n                else:\n                    filters.append(\"(\" + field + ' == \"' + str(value) + '\")')\n        # Join all our expressions with `and``\n        return \" and \".join(filters)", ""]}
{"filename": "datastore/providers/qdrant_datastore.py", "chunked_list": ["import os\nimport uuid\nfrom typing import Dict, List, Optional\n\nfrom grpc._channel import _InactiveRpcError\nfrom qdrant_client.http.exceptions import UnexpectedResponse\nfrom qdrant_client.http.models import PayloadSchemaType\n\nfrom datastore.datastore import DataStore\nfrom models.models import (", "from datastore.datastore import DataStore\nfrom models.models import (\n    DocumentChunk,\n    DocumentMetadataFilter,\n    QueryResult,\n    QueryWithEmbedding,\n    DocumentChunkWithScore,\n)\nfrom qdrant_client.http import models as rest\n", "from qdrant_client.http import models as rest\n\nimport qdrant_client\n\nfrom services.date import to_unix_timestamp\n\nQDRANT_URL = os.environ.get(\"QDRANT_URL\", \"http://localhost\")\nQDRANT_PORT = os.environ.get(\"QDRANT_PORT\", \"6333\")\nQDRANT_GRPC_PORT = os.environ.get(\"QDRANT_GRPC_PORT\", \"6334\")\nQDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")", "QDRANT_GRPC_PORT = os.environ.get(\"QDRANT_GRPC_PORT\", \"6334\")\nQDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")\nQDRANT_COLLECTION = os.environ.get(\"QDRANT_COLLECTION\", \"document_chunks\")\n\n\nclass QdrantDataStore(DataStore):\n    UUID_NAMESPACE = uuid.UUID(\"3896d314-1e95-4a3a-b45a-945f9f0b541d\")\n\n    def __init__(\n        self,\n        collection_name: Optional[str] = None,\n        vector_size: int = 1536,\n        distance: str = \"Cosine\",\n        recreate_collection: bool = False,\n    ):\n        \"\"\"\n        Args:\n            collection_name: Name of the collection to be used\n            vector_size: Size of the embedding stored in a collection\n            distance:\n                Any of \"Cosine\" / \"Euclid\" / \"Dot\". Distance function to measure\n                similarity\n        \"\"\"\n        self.client = qdrant_client.QdrantClient(\n            url=QDRANT_URL,\n            port=int(QDRANT_PORT),\n            grpc_port=int(QDRANT_GRPC_PORT),\n            api_key=QDRANT_API_KEY,\n            prefer_grpc=True,\n        )\n        self.collection_name = collection_name or QDRANT_COLLECTION\n\n        # Set up the collection so the points might be inserted or queried\n        self._set_up_collection(vector_size, distance, recreate_collection)\n\n    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n        \"\"\"\n        Takes in a list of document chunks and inserts them into the database.\n        Return a list of document ids.\n        \"\"\"\n        points = [\n            self._convert_document_chunk_to_point(chunk)\n            for _, chunks in chunks.items()\n            for chunk in chunks\n        ]\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points,  # type: ignore\n            wait=True,\n        )\n        return list(chunks.keys())\n\n    async def _query(\n        self,\n        queries: List[QueryWithEmbedding],\n    ) -> List[QueryResult]:\n        \"\"\"\n        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n        \"\"\"\n        search_requests = [\n            self._convert_query_to_search_request(query) for query in queries\n        ]\n        results = self.client.search_batch(\n            collection_name=self.collection_name,\n            requests=search_requests,\n        )\n        return [\n            QueryResult(\n                query=query.query,\n                results=[\n                    self._convert_scored_point_to_document_chunk_with_score(point)\n                    for point in result\n                ],\n            )\n            for query, result in zip(queries, results)\n        ]\n\n    async def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        filter: Optional[DocumentMetadataFilter] = None,\n        delete_all: Optional[bool] = None,\n    ) -> bool:\n        \"\"\"\n        Removes vectors by ids, filter, or everything in the datastore.\n        Returns whether the operation was successful.\n        \"\"\"\n        if ids is None and filter is None and delete_all is None:\n            raise ValueError(\n                \"Please provide one of the parameters: ids, filter or delete_all.\"\n            )\n\n        if delete_all:\n            points_selector = rest.Filter()\n        else:\n            points_selector = self._convert_metadata_filter_to_qdrant_filter(\n                filter, ids\n            )\n\n        response = self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=points_selector,  # type: ignore\n        )\n        return \"COMPLETED\" == response.status\n\n    def _convert_document_chunk_to_point(\n        self, document_chunk: DocumentChunk\n    ) -> rest.PointStruct:\n        created_at = (\n            to_unix_timestamp(document_chunk.metadata.created_at)\n            if document_chunk.metadata.created_at is not None\n            else None\n        )\n        return rest.PointStruct(\n            id=self._create_document_chunk_id(document_chunk.id),\n            vector=document_chunk.embedding,  # type: ignore\n            payload={\n                \"id\": document_chunk.id,\n                \"text\": document_chunk.text,\n                \"metadata\": document_chunk.metadata.dict(),\n                \"created_at\": created_at,\n            },\n        )\n\n    def _create_document_chunk_id(self, external_id: Optional[str]) -> str:\n        if external_id is None:\n            return uuid.uuid4().hex\n        return uuid.uuid5(self.UUID_NAMESPACE, external_id).hex\n\n    def _convert_query_to_search_request(\n        self, query: QueryWithEmbedding\n    ) -> rest.SearchRequest:\n        return rest.SearchRequest(\n            vector=query.embedding,\n            filter=self._convert_metadata_filter_to_qdrant_filter(query.filter),\n            limit=query.top_k,  # type: ignore\n            with_payload=True,\n            with_vector=False,\n        )\n\n    def _convert_metadata_filter_to_qdrant_filter(\n        self,\n        metadata_filter: Optional[DocumentMetadataFilter] = None,\n        ids: Optional[List[str]] = None,\n    ) -> Optional[rest.Filter]:\n        if metadata_filter is None and ids is None:\n            return None\n\n        must_conditions, should_conditions = [], []\n\n        # Filtering by document ids\n        if ids and len(ids) > 0:\n            for document_id in ids:\n                should_conditions.append(\n                    rest.FieldCondition(\n                        key=\"metadata.document_id\",\n                        match=rest.MatchValue(value=document_id),\n                    )\n                )\n\n        # Equality filters for the payload attributes\n        if metadata_filter:\n            meta_attributes_keys = {\n                \"document_id\": \"metadata.document_id\",\n                \"source\": \"metadata.source\",\n                \"source_id\": \"metadata.source_id\",\n                \"author\": \"metadata.author\",\n            }\n\n            for meta_attr_name, payload_key in meta_attributes_keys.items():\n                attr_value = getattr(metadata_filter, meta_attr_name)\n                if attr_value is None:\n                    continue\n\n                must_conditions.append(\n                    rest.FieldCondition(\n                        key=payload_key, match=rest.MatchValue(value=attr_value)\n                    )\n                )\n\n            # Date filters use range filtering\n            start_date = metadata_filter.start_date\n            end_date = metadata_filter.end_date\n            if start_date or end_date:\n                gte_filter = (\n                    to_unix_timestamp(start_date) if start_date is not None else None\n                )\n                lte_filter = (\n                    to_unix_timestamp(end_date) if end_date is not None else None\n                )\n                must_conditions.append(\n                    rest.FieldCondition(\n                        key=\"created_at\",\n                        range=rest.Range(\n                            gte=gte_filter,\n                            lte=lte_filter,\n                        ),\n                    )\n                )\n\n        if 0 == len(must_conditions) and 0 == len(should_conditions):\n            return None\n\n        return rest.Filter(must=must_conditions, should=should_conditions)\n\n    def _convert_scored_point_to_document_chunk_with_score(\n        self, scored_point: rest.ScoredPoint\n    ) -> DocumentChunkWithScore:\n        payload = scored_point.payload or {}\n        return DocumentChunkWithScore(\n            id=payload.get(\"id\"),\n            text=scored_point.payload.get(\"text\"),  # type: ignore\n            metadata=scored_point.payload.get(\"metadata\"),  # type: ignore\n            embedding=scored_point.vector,  # type: ignore\n            score=scored_point.score,\n        )\n\n    def _set_up_collection(\n        self, vector_size: int, distance: str, recreate_collection: bool\n    ):\n        distance = rest.Distance[distance.upper()]\n\n        if recreate_collection:\n            self._recreate_collection(distance, vector_size)\n\n        try:\n            collection_info = self.client.get_collection(self.collection_name)\n            current_distance = collection_info.config.params.vectors.distance  # type: ignore\n            current_vector_size = collection_info.config.params.vectors.size  # type: ignore\n\n            if current_distance != distance:\n                raise ValueError(\n                    f\"Collection '{self.collection_name}' already exists in Qdrant, \"\n                    f\"but it is configured with a similarity '{current_distance.name}'. \"\n                    f\"If you want to use that collection, but with a different \"\n                    f\"similarity, please set `recreate_collection=True` argument.\"\n                )\n\n            if current_vector_size != vector_size:\n                raise ValueError(\n                    f\"Collection '{self.collection_name}' already exists in Qdrant, \"\n                    f\"but it is configured with a vector size '{current_vector_size}'. \"\n                    f\"If you want to use that collection, but with a different \"\n                    f\"vector size, please set `recreate_collection=True` argument.\"\n                )\n        except (UnexpectedResponse, _InactiveRpcError):\n            self._recreate_collection(distance, vector_size)\n\n    def _recreate_collection(self, distance: rest.Distance, vector_size: int):\n        self.client.recreate_collection(\n            self.collection_name,\n            vectors_config=rest.VectorParams(\n                size=vector_size,\n                distance=distance,\n            ),\n        )\n\n        # Create the payload index for the document_id metadata attribute, as it is\n        # used to delete the document related entries\n        self.client.create_payload_index(\n            self.collection_name,\n            field_name=\"metadata.document_id\",\n            field_type=PayloadSchemaType.KEYWORD,\n        )\n\n        # Create the payload index for the created_at attribute, to make the lookup\n        # by range filters faster\n        self.client.create_payload_index(\n            self.collection_name,\n            field_name=\"created_at\",\n            field_schema=PayloadSchemaType.INTEGER,\n        )", ""]}
{"filename": "models/models.py", "chunked_list": ["from pydantic import BaseModel\nfrom typing import List, Optional\nfrom enum import Enum\n\n\nclass Source(str, Enum):\n    email = \"email\"\n    file = \"file\"\n    chat = \"chat\"\n", "\n\nclass DocumentMetadata(BaseModel):\n    source: Optional[Source] = None\n    source_id: Optional[str] = None\n    url: Optional[str] = None\n    created_at: Optional[str] = None\n    author: Optional[str] = None\n    title: Optional[str] = None\n", "\n\nclass DocumentChunkMetadata(DocumentMetadata):\n    document_id: Optional[str] = None\n\n\nclass DocumentChunk(BaseModel):\n    id: Optional[str] = None\n    text: str\n    metadata: DocumentChunkMetadata\n    embedding: Optional[List[float]] = None", "\n\nclass DocumentChunkWithScore(DocumentChunk):\n    score: float\n\n\nclass Document(BaseModel):\n    id: Optional[str] = None\n    text: str\n    metadata: Optional[DocumentMetadata] = None", "\n\nclass DocumentWithChunks(Document):\n    chunks: List[DocumentChunk]\n\n\nclass DocumentMetadataFilter(BaseModel):\n    document_id: Optional[str] = None\n    source: Optional[Source] = None\n    source_id: Optional[str] = None\n    author: Optional[str] = None\n    start_date: Optional[str] = None  # any date string format\n    end_date: Optional[str] = None  # any date string format\n    title: Optional[str] = None", "\n\nclass Query(BaseModel):\n    query: str\n    filter: Optional[DocumentMetadataFilter] = None\n    top_k: Optional[int] = 3\n\n\nclass QueryWithEmbedding(Query):\n    embedding: List[float]", "class QueryWithEmbedding(Query):\n    embedding: List[float]\n\n\nclass QueryResult(BaseModel):\n    query: str\n    results: List[DocumentChunkWithScore]\n"]}
{"filename": "models/api.py", "chunked_list": ["from models.models import (\n    Document,\n    DocumentChunkWithScore,\n    DocumentMetadataFilter,\n    Query,\n    QueryResult,\n)\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n", "from typing import List, Optional\n\n\nclass UpsertRequest(BaseModel):\n    documents: List[Document]\n    chunk_token_size: Optional[int] = 400\n    chunk_overlap: Optional[int] = 20\n\n\nclass UpsertResponse(BaseModel):\n    ids: List[str]", "\nclass UpsertResponse(BaseModel):\n    ids: List[str]\n\n\nclass QueryRequest(BaseModel):\n    queries: List[Query]\n\n\nclass QueryResponse(BaseModel):\n    results: List[QueryResult]", "\nclass QueryResponse(BaseModel):\n    results: List[QueryResult]\n\n\nclass DeleteRequest(BaseModel):\n    ids: Optional[List[str]] = None\n    filter: Optional[DocumentMetadataFilter] = None\n    delete_all: Optional[bool] = False\n", "\n\nclass DeleteResponse(BaseModel):\n    success: bool\n"]}
{"filename": "services/chunks.py", "chunked_list": ["from typing import Dict, List, Optional, Tuple\nimport uuid\nfrom models.models import Document, DocumentChunk, DocumentChunkMetadata\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nimport tiktoken\n\nfrom services.openai import get_embeddings\n\n# Global variables", "\n# Global variables\ntokenizer = tiktoken.get_encoding(\n    \"cl100k_base\"\n)  # The encoding scheme to use for tokenization\n\n# create the length function\ndef tiktoken_len(text):\n    tokens = tokenizer.encode(\n        text,\n        disallowed_special=()\n    )\n    return len(tokens)", "\n# Constants\nCHUNK_SIZE = 200  # The target size of each text chunk in tokens\nMIN_CHUNK_SIZE_CHARS = 350  # The minimum size of each text chunk in characters\nMIN_CHUNK_LENGTH_TO_EMBED = 30  # Discard chunks shorter than this\nEMBEDDINGS_BATCH_SIZE = 128  # The number of embeddings to request at a time\nMAX_NUM_CHUNKS = 10000  # The maximum number of chunks to generate from a text\n\n\n\ndef get_text_chunks(text: str, chunk_token_size: Optional[int]) -> List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries.\n\n    Args:\n        text: The text to split into chunks.\n        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\n    Returns:\n        A list of text chunks, each of which is a string of ~CHUNK_SIZE tokens.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Use the provided chunk token size or the default one\n    chunk_size = chunk_token_size or CHUNK_SIZE\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks < MAX_NUM_CHUNKS:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[:chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text from the remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        last_punctuation = max(\n            chunk_text.rfind(\".\"),\n            chunk_text.rfind(\"?\"),\n            chunk_text.rfind(\"!\"),\n            chunk_text.rfind(\"\\n\"),\n        )\n\n        # If there is a punctuation mark, and the last punctuation index is before MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation > MIN_CHUNK_SIZE_CHARS\n            and last_punctuation > MIN_CHUNK_SIZE_CHARS\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Remove any newline characters and strip any leading or trailing whitespace\n        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n\n        if len(chunk_text) > MIN_CHUNK_LENGTH_TO_EMBED:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text)\n\n        # Remove the tokens corresponding to the chunk text from the remaining tokens\n        tokens = tokens[len(tokenizer.encode(chunk_text, disallowed_special=())) :]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # Handle the remaining tokens\n    if tokens:\n        remaining_text = tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\n        if len(remaining_text) > MIN_CHUNK_LENGTH_TO_EMBED:\n            chunks.append(remaining_text)\n\n    return chunks", "\n\ndef get_text_chunks(text: str, chunk_token_size: Optional[int]) -> List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries.\n\n    Args:\n        text: The text to split into chunks.\n        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\n    Returns:\n        A list of text chunks, each of which is a string of ~CHUNK_SIZE tokens.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Use the provided chunk token size or the default one\n    chunk_size = chunk_token_size or CHUNK_SIZE\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks < MAX_NUM_CHUNKS:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[:chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text from the remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        last_punctuation = max(\n            chunk_text.rfind(\".\"),\n            chunk_text.rfind(\"?\"),\n            chunk_text.rfind(\"!\"),\n            chunk_text.rfind(\"\\n\"),\n        )\n\n        # If there is a punctuation mark, and the last punctuation index is before MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation > MIN_CHUNK_SIZE_CHARS\n            and last_punctuation > MIN_CHUNK_SIZE_CHARS\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Remove any newline characters and strip any leading or trailing whitespace\n        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n\n        if len(chunk_text) > MIN_CHUNK_LENGTH_TO_EMBED:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text)\n\n        # Remove the tokens corresponding to the chunk text from the remaining tokens\n        tokens = tokens[len(tokenizer.encode(chunk_text, disallowed_special=())) :]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # Handle the remaining tokens\n    if tokens:\n        remaining_text = tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\n        if len(remaining_text) > MIN_CHUNK_LENGTH_TO_EMBED:\n            chunks.append(remaining_text)\n\n    return chunks", "\n\ndef create_document_chunks(\n    doc: Document, chunk_token_size: Optional[int] = 400, chunk_overlap: Optional[int] = 20\n) -> Tuple[List[DocumentChunk], str]:\n    \"\"\"\n    Create a list of document chunks from a document object and return the document id.\n\n    Args:\n        doc: The document object to create chunks from. It should have a text attribute and optionally an id and a metadata attribute.\n        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\n    Returns:\n        A tuple of (doc_chunks, doc_id), where doc_chunks is a list of document chunks, each of which is a DocumentChunk object with an id, a document_id, a text, and a metadata attribute,\n        and doc_id is the id of the document object, generated if not provided. The id of each chunk is generated from the document id and a sequential number, and the metadata is copied from the document object.\n    \"\"\"\n    # Check if the document text is empty or whitespace\n    if not doc.text or doc.text.isspace():\n        return [], doc.id or str(uuid.uuid4())\n\n    # Generate a document id if not provided\n    doc_id = doc.id or str(uuid.uuid4())\n\n    # initialize the text splitter\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_token_size,\n        chunk_overlap=chunk_overlap,  # number of tokens overlap between chunks\n        length_function=tiktoken_len,\n        separators=['\\n\\n', '.\\n', '\\n', '.', '?', '!', ' ', '']\n    )\n\n    # Split the document text into chunks\n    text_chunks = text_splitter(doc.text)\n\n    metadata = (\n        DocumentChunkMetadata(**doc.metadata.__dict__)\n        if doc.metadata is not None\n        else DocumentChunkMetadata()\n    )\n\n    metadata.document_id = doc_id\n\n    # Initialize an empty list of chunks for this document\n    doc_chunks = []\n\n    # Assign each chunk a sequential number and create a DocumentChunk object\n    for i, text_chunk in enumerate(text_chunks):\n        chunk_id = f\"{doc_id}_{i}\"\n        doc_chunk = DocumentChunk(\n            id=chunk_id,\n            text=text_chunk,\n            metadata=metadata,\n        )\n        # Append the chunk object to the list of chunks for this document\n        doc_chunks.append(doc_chunk)\n\n    # Return the list of chunks and the document id\n    return doc_chunks, doc_id", "\n\ndef get_document_chunks(\n    documents: List[Document], chunk_token_size: Optional[int]\n) -> Dict[str, List[DocumentChunk]]:\n    \"\"\"\n    Convert a list of documents into a dictionary from document id to list of document chunks.\n\n    Args:\n        documents: The list of documents to convert.\n        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\n    Returns:\n        A dictionary mapping each document id to a list of document chunks, each of which is a DocumentChunk object\n        with text, metadata, and embedding attributes.\n    \"\"\"\n    # Initialize an empty dictionary of lists of chunks\n    chunks: Dict[str, List[DocumentChunk]] = {}\n\n    # Initialize an empty list of all chunks\n    all_chunks: List[DocumentChunk] = []\n\n    # Loop over each document and create chunks\n    for doc in documents:\n        doc_chunks, doc_id = create_document_chunks(doc, chunk_token_size)\n\n        # Append the chunks for this document to the list of all chunks\n        all_chunks.extend(doc_chunks)\n\n        # Add the list of chunks for this document to the dictionary with the document id as the key\n        chunks[doc_id] = doc_chunks\n\n    # Check if there are no chunks\n    if not all_chunks:\n        return {}\n\n    # Get all the embeddings for the document chunks in batches, using get_embeddings\n    embeddings: List[List[float]] = []\n    for i in range(0, len(all_chunks), EMBEDDINGS_BATCH_SIZE):\n        # Get the text of the chunks in the current batch\n        batch_texts = [\n            chunk.text for chunk in all_chunks[i : i + EMBEDDINGS_BATCH_SIZE]\n        ]\n\n        # Get the embeddings for the batch texts\n        batch_embeddings = get_embeddings(batch_texts)\n\n        # Append the batch embeddings to the embeddings list\n        embeddings.extend(batch_embeddings)\n\n    # Update the document chunk objects with the embeddings\n    for i, chunk in enumerate(all_chunks):\n        # Assign the embedding from the embeddings list to the chunk object\n        chunk.embedding = embeddings[i]\n\n    return chunks", ""]}
{"filename": "services/extract_metadata.py", "chunked_list": ["from models.models import Source\nfrom services.openai import get_chat_completion\nimport json\nfrom typing import Dict\n\n\ndef extract_metadata_from_document(text: str) -> Dict[str, str]:\n    sources = Source.__members__.keys()\n    sources_string = \", \".join(sources)\n    # This prompt is just an example, change it to fit your use case\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"\n            Given a document from a user, try to extract the following metadata:\n            - source: string, one of {sources_string}\n            - url: string or don't specify\n            - created_at: string or don't specify\n            - author: string or don't specify\n\n            Respond with a JSON containing the extracted metadata in key value pairs. If you don't find a metadata field, don't specify it.\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": text},\n    ]\n\n    completion = get_chat_completion(messages, \"gpt-4\")\n\n    print(f\"completion: {completion}\")\n\n    try:\n        metadata = json.loads(completion)\n    except:\n        metadata = {}\n\n    return metadata", ""]}
{"filename": "services/openai.py", "chunked_list": ["from typing import List\nimport openai\n\n\nfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\n\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\ndef get_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"\n    Embed texts using OpenAI's ada model.\n\n    Args:\n        texts: The list of texts to embed.\n\n    Returns:\n        A list of embeddings, each of which is a list of floats.\n\n    Raises:\n        Exception: If the OpenAI API call fails.\n    \"\"\"\n    # Call the OpenAI API to get the embeddings\n    response = openai.Embedding.create(input=texts, model=\"text-embedding-ada-002\")\n\n    # Extract the embedding data from the response\n    data = response[\"data\"]  # type: ignore\n\n    # Return the embeddings as a list of lists of floats\n    return [result[\"embedding\"] for result in data]", "def get_embeddings(texts: List[str]) -> List[List[float]]:\n    \"\"\"\n    Embed texts using OpenAI's ada model.\n\n    Args:\n        texts: The list of texts to embed.\n\n    Returns:\n        A list of embeddings, each of which is a list of floats.\n\n    Raises:\n        Exception: If the OpenAI API call fails.\n    \"\"\"\n    # Call the OpenAI API to get the embeddings\n    response = openai.Embedding.create(input=texts, model=\"text-embedding-ada-002\")\n\n    # Extract the embedding data from the response\n    data = response[\"data\"]  # type: ignore\n\n    # Return the embeddings as a list of lists of floats\n    return [result[\"embedding\"] for result in data]", "\n\n@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\ndef get_chat_completion(\n    messages,\n    model=\"gpt-3.5-turbo\",  # use \"gpt-4\" for better results\n):\n    \"\"\"\n    Generate a chat completion using OpenAI's chat completion API.\n\n    Args:\n        messages: The list of messages in the chat history.\n        model: The name of the model to use for the completion. Default is gpt-3.5-turbo, which is a fast, cheap and versatile model. Use gpt-4 for higher quality but slower results.\n\n    Returns:\n        A string containing the chat completion.\n\n    Raises:\n        Exception: If the OpenAI API call fails.\n    \"\"\"\n    # call the OpenAI chat completion API with the given messages\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n    )\n\n    choices = response[\"choices\"]  # type: ignore\n    completion = choices[0].message.content.strip()\n    print(f\"Completion: {completion}\")\n    return completion", ""]}
{"filename": "services/pii_detection.py", "chunked_list": ["from services.openai import get_chat_completion\n\n\ndef screen_text_for_pii(text: str) -> bool:\n    # This prompt is just an example, change it to fit your use case\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"\n            You can only respond with the word \"True\" or \"False\", where your answer indicates whether the text in the user's message contains PII.\n            Do not explain your answer, and do not use punctuation.\n            Your task is to identify whether the text extracted from your company files\n            contains sensitive PII information that should not be shared with the broader company. Here are some things to look out for:\n            - An email address that identifies a specific person in either the local-part or the domain\n            - The postal address of a private residence (must include at least a street name)\n            - The postal address of a public place (must include either a street name or business name)\n            - Notes about hiring decisions with mentioned names of candidates. The user will send a document for you to analyze.\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": text},\n    ]\n\n    completion = get_chat_completion(\n        messages,\n    )\n\n    if completion.startswith(\"True\"):\n        return True\n\n    return False", ""]}
{"filename": "services/date.py", "chunked_list": ["import arrow\n\n\ndef to_unix_timestamp(date_str: str) -> int:\n    \"\"\"\n    Convert a date string to a unix timestamp (seconds since epoch).\n\n    Args:\n        date_str: The date string to convert.\n\n    Returns:\n        The unix timestamp corresponding to the date string.\n\n    If the date string cannot be parsed as a valid date format, returns the current unix timestamp and prints a warning.\n    \"\"\"\n    # Try to parse the date string using arrow, which supports many common date formats\n    try:\n        date_obj = arrow.get(date_str)\n        print()\n        return int(date_obj.timestamp())\n    except arrow.parser.ParserError:\n        # If the parsing fails, return the current unix timestamp and print a warning\n        print(f\"Invalid date format: {date_str}\")\n        return int(arrow.now().timestamp())", ""]}
{"filename": "services/file.py", "chunked_list": ["import os\nfrom io import BufferedReader\nfrom typing import Optional\nfrom fastapi import UploadFile\nimport mimetypes\nfrom PyPDF2 import PdfReader\nimport docx2txt\nimport csv\nimport pptx\n", "import pptx\n\nfrom models.models import Document, DocumentMetadata\n\n\nasync def get_document_from_file(file: UploadFile) -> Document:\n    extracted_text = await extract_text_from_form_file(file)\n    print(f\"extracted_text:\")\n    # get metadata\n    metadata = DocumentMetadata()", "    # get metadata\n    metadata = DocumentMetadata()\n    doc = Document(text=extracted_text, metadata=metadata)\n\n    return doc\n\n\ndef extract_text_from_filepath(filepath: str, mimetype: Optional[str] = None) -> str:\n    \"\"\"Return the text content of a file given its filepath.\"\"\"\n\n    if mimetype is None:\n        # Get the mimetype of the file based on its extension\n        mimetype, _ = mimetypes.guess_type(filepath)\n\n    if not mimetype:\n        if filepath.endswith(\".md\"):\n            mimetype = \"text/markdown\"\n        else:\n            raise Exception(\"Unsupported file type\")\n\n    # Open the file in binary mode\n    file = open(filepath, \"rb\")\n    extracted_text = extract_text_from_file(file, mimetype)\n\n    return extracted_text", "\n\ndef extract_text_from_file(file: BufferedReader, mimetype: str) -> str:\n    if mimetype == \"application/pdf\":\n        # Extract text from pdf using PyPDF2\n        reader = PdfReader(file)\n        extracted_text = \"\"\n        for page in reader.pages:\n            extracted_text += page.extract_text()\n    elif mimetype == \"text/plain\" or mimetype == \"text/markdown\":\n        # Read text from plain text file\n        extracted_text = file.read().decode(\"utf-8\")\n    elif (\n        mimetype\n        == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n    ):\n        # Extract text from docx using docx2txt\n        extracted_text = docx2txt.process(file)\n    elif mimetype == \"text/csv\":\n        # Extract text from csv using csv module\n        extracted_text = \"\"\n        decoded_buffer = (line.decode(\"utf-8\") for line in file)\n        reader = csv.reader(decoded_buffer)\n        for row in reader:\n            extracted_text += \" \".join(row) + \"\\n\"\n    elif (\n        mimetype\n        == \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n    ):\n        # Extract text from pptx using python-pptx\n        extracted_text = \"\"\n        presentation = pptx.Presentation(file)\n        for slide in presentation.slides:\n            for shape in slide.shapes:\n                if shape.has_text_frame:\n                    for paragraph in shape.text_frame.paragraphs:\n                        for run in paragraph.runs:\n                            extracted_text += run.text + \" \"\n                    extracted_text += \"\\n\"\n    else:\n        # Unsupported file type\n        file.close()\n        raise ValueError(\"Unsupported file type: {}\".format(mimetype))\n\n    file.close()\n    return extracted_text", "\n\n# Extract text from a file based on its mimetype\nasync def extract_text_from_form_file(file: UploadFile):\n    \"\"\"Return the text content of a file.\"\"\"\n    # get the file body from the upload file object\n    mimetype = file.content_type\n    print(f\"mimetype: {mimetype}\")\n    print(f\"file.file: {file.file}\")\n    print(\"file: \", file)", "    print(f\"file.file: {file.file}\")\n    print(\"file: \", file)\n\n    file_stream = await file.read()\n\n    temp_file_path = \"/tmp/temp_file\"\n\n    # write the file to a temporary locatoin\n    with open(temp_file_path, \"wb\") as f:\n        f.write(file_stream)", "    with open(temp_file_path, \"wb\") as f:\n        f.write(file_stream)\n\n    try:\n        extracted_text = extract_text_from_filepath(temp_file_path, mimetype)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        os.remove(temp_file_path)\n        raise e\n", "\n    # remove file from temp location\n    os.remove(temp_file_path)\n\n    return extracted_text\n"]}
{"filename": "examples/authentication-methods/no-auth/main.py", "chunked_list": ["# This is a version of the main.py file found in ../../../server/main.py without authentication.\n# Copy and paste this into the main file at ../../../server/main.py if you choose to use no authentication for your retrieval plugin.\n\nimport os\nimport uvicorn\nfrom fastapi import FastAPI, File, HTTPException, Depends, Body, UploadFile\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.staticfiles import StaticFiles\n\nfrom models.api import (", "\nfrom models.api import (\n    DeleteRequest,\n    DeleteResponse,\n    QueryRequest,\n    QueryResponse,\n    UpsertRequest,\n    UpsertResponse,\n)\nfrom datastore.factory import get_datastore", ")\nfrom datastore.factory import get_datastore\nfrom services.file import get_document_from_file\n\n\napp = FastAPI()\napp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n\n# Create a sub-application, in order to access just the query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\nsub_app = FastAPI(", "# Create a sub-application, in order to access just the query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\nsub_app = FastAPI(\n    title=\"Retrieval Plugin API\",\n    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n    version=\"1.0.0\",\n    servers=[{\"url\": \"https://your-app-url.com\"}],\n)\napp.mount(\"/sub\", sub_app)\n\n", "\n\n@app.post(\n    \"/upsert-file\",\n    response_model=UpsertResponse,\n)\nasync def upsert_file(\n    file: UploadFile = File(...),\n):\n    document = await get_document_from_file(file)", "):\n    document = await get_document_from_file(file)\n\n    try:\n        ids = await datastore.upsert([document])\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=f\"str({e})\")\n", "\n\n@app.post(\n    \"/upsert\",\n    response_model=UpsertResponse,\n)\nasync def upsert(\n    request: UpsertRequest = Body(...),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\n\n@app.post(", "\n@app.post(\n    \"/query\",\n    response_model=QueryResponse,\n)\nasync def query_main(\n    request: QueryRequest = Body(...),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\n", "\n\n@sub_app.post(\n    \"/query\",\n    response_model=QueryResponse,\n    description='Accepts an array of search query objects, each with a natural language query string (\"query\") and an optional metadata filter (\"filter\"). Filters are not necessary in most cases, but can sometimes help refine search results based on criteria such as document source or time period. Send multiple queries to compare information from different sources or break down complex questions into sub-questions. If you receive a ResponseTooLargeError, try splitting up the queries into multiple calls to this endpoint.',\n)\nasync def query(\n    request: QueryRequest = Body(...),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    request: QueryRequest = Body(...),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.delete(\n    \"/delete\",\n    response_model=DeleteResponse,\n)\nasync def delete(\n    request: DeleteRequest = Body(...),\n):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )", "):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )\n    try:\n        success = await datastore.delete(\n            ids=request.ids,\n            filter=request.filter,\n            delete_all=request.delete_all,\n        )\n        return DeleteResponse(success=success)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.on_event(\"startup\")\nasync def startup():\n    global datastore\n    datastore = await get_datastore()\n\n\ndef start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)", "def start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
{"filename": "examples/memory/main.py", "chunked_list": ["# This is a version of the main.py file found in ../../server/main.py that also gives ChatGPT access to the upsert endpoint\n# (allowing it to save information from the chat back to the vector) database.\n# Copy and paste this into the main file at ../../server/main.py if you choose to give the model access to the upsert endpoint\n# and want to access the openapi.json when you run the app locally at http://0.0.0.0:8000/sub/openapi.json.\nimport os\nimport uvicorn\nfrom fastapi import FastAPI, File, HTTPException, Depends, Body, UploadFile\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.staticfiles import StaticFiles\n", "from fastapi.staticfiles import StaticFiles\n\nfrom models.api import (\n    DeleteRequest,\n    DeleteResponse,\n    QueryRequest,\n    QueryResponse,\n    UpsertRequest,\n    UpsertResponse,\n)", "    UpsertResponse,\n)\nfrom datastore.factory import get_datastore\nfrom services.file import get_document_from_file\n\n\napp = FastAPI()\napp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n\n# Create a sub-application, in order to access just the upsert and query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally", "\n# Create a sub-application, in order to access just the upsert and query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\nsub_app = FastAPI(\n    title=\"Retrieval Plugin API\",\n    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n    version=\"1.0.0\",\n    servers=[{\"url\": \"https://your-app-url.com\"}],\n)\napp.mount(\"/sub\", sub_app)\n", "app.mount(\"/sub\", sub_app)\n\nbearer_scheme = HTTPBearer()\nBEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")\nassert BEARER_TOKEN is not None\n\n\ndef validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):\n    if credentials.scheme != \"Bearer\" or credentials.credentials != BEARER_TOKEN:\n        raise HTTPException(status_code=401, detail=\"Invalid or missing token\")\n    return credentials", "\n\n@app.post(\n    \"/upsert-file\",\n    response_model=UpsertResponse,\n)\nasync def upsert_file(\n    file: UploadFile = File(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    document = await get_document_from_file(file)\n\n    try:\n        ids = await datastore.upsert([document])\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=f\"str({e})\")", "\n\n@app.post(\n    \"/upsert\",\n    response_model=UpsertResponse,\n)\nasync def upsert_main(\n    request: UpsertRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\n", "\n\n@sub_app.post(\n    \"/upsert\",\n    response_model=UpsertResponse,\n    description=\"Save information from chat conversations as documents, only if the user asks you to. Accepts an array of documents, each document has a text field with the conversation text and possible questions that could lead to the answer, and metadata including the source (chat) and created_at timestamp. Confirm with the user before saving information, and ask if they want to add details / context.\",\n)\nasync def upsert(\n    request: UpsertRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),", "    request: UpsertRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        ids = await datastore.upsert(request.documents)\n        return UpsertResponse(ids=ids)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n", "\n\n@app.post(\n    \"/query\",\n    response_model=QueryResponse,\n)\nasync def query_main(\n    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@sub_app.post(\n    \"/query\",\n    response_model=QueryResponse,\n    description='Accepts an array of search query objects, each with a natural language query string (\"query\") and an optional metadata filter (\"filter\"). Filters are not necessary in most cases, but can sometimes help refine search results based on criteria such as document source or time period. Send multiple queries to compare information from different sources or break down complex questions into sub-questions. If you receive a ResponseTooLargeError, try splitting up the queries into multiple calls to this endpoint.',\n)\nasync def query(\n    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),", "    request: QueryRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    try:\n        results = await datastore.query(\n            request.queries,\n        )\n        return QueryResponse(results=results)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.delete(\n    \"/delete\",\n    response_model=DeleteResponse,\n)\nasync def delete(\n    request: DeleteRequest = Body(...),\n    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )", "    token: HTTPAuthorizationCredentials = Depends(validate_token),\n):\n    if not (request.ids or request.filter or request.delete_all):\n        raise HTTPException(\n            status_code=400,\n            detail=\"One of ids, filter, or delete_all is required\",\n        )\n    try:\n        success = await datastore.delete(\n            ids=request.ids,\n            filter=request.filter,\n            delete_all=request.delete_all,\n        )\n        return DeleteResponse(success=success)\n    except Exception as e:\n        print(\"Error:\", e)\n        raise HTTPException(status_code=500, detail=\"Internal Service Error\")", "\n\n@app.on_event(\"startup\")\nasync def startup():\n    global datastore\n    datastore = await get_datastore()\n\n\ndef start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)", "def start():\n    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
