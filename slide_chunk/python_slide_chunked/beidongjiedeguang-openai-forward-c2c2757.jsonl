{"filename": "openai_forward/__main__.py", "chunked_list": ["import os\n\nimport fire\nimport uvicorn\n\n\nclass Cli:\n    @staticmethod\n    def run(port=8000, workers=1, log_chat=None):\n        \"\"\"\n        Runs the application using the Uvicorn server.\n\n        Args:\n            port (int): The port number on which to run the server. Default is 8000.\n            workers (int): The number of worker processes to run. Default is 1.\n            log_chat (str): whether to log llm chat. Default is None.\n\n        Returns:\n            None\n        \"\"\"\n\n        if log_chat:\n            os.environ[\"LOG_CHAT\"] = log_chat\n\n        ssl_keyfile = os.environ.get(\"ssl_keyfile\", None) or None\n        ssl_certfile = os.environ.get(\"ssl_certfile\", None) or None\n        uvicorn.run(\n            app=\"openai_forward.app:app\",\n            host=\"0.0.0.0\",\n            port=port,\n            workers=workers,\n            app_dir=\"..\",\n            ssl_keyfile=ssl_keyfile,\n            ssl_certfile=ssl_certfile,\n        )\n\n    @staticmethod\n    def convert(log_folder: str = None, target_path: str = None):\n        \"\"\"\n        Converts log files in a folder to a JSONL file.\n\n        Args:\n            log_folder (str, optional): The path to the folder containing the log files. Default is None.\n            target_path (str, optional): The path to the target JSONL file. Default is None.\n\n        Returns:\n            None\n        \"\"\"\n        from openai_forward.forwarding.settings import OPENAI_ROUTE_PREFIX\n        from openai_forward.helper import convert_folder_to_jsonl\n\n        print(60 * '-')\n        if log_folder is None:\n            if target_path is not None:\n                raise ValueError(\"target_path must be None when log_folder is None\")\n            _prefix_list = [i.replace(\"/\", \"_\") for i in OPENAI_ROUTE_PREFIX]\n            for _prefix in _prefix_list:\n                log_folder = f\"./Log/chat/{_prefix}\"\n                target_path = f\"./Log/chat{_prefix}.json\"\n                print(f\"Convert {log_folder}/*.log to {target_path}\")\n                convert_folder_to_jsonl(log_folder, target_path)\n                print(60 * '-')\n        else:\n            print(f\"Convert {log_folder}/*.log to {target_path}\")\n            convert_folder_to_jsonl(log_folder, target_path)\n            print(60 * '-')", "\n\ndef main():\n    fire.Fire(Cli)\n\n\nif __name__ == \"__main__\":\n    main()\n", ""]}
{"filename": "openai_forward/app.py", "chunked_list": ["from fastapi import FastAPI, Request, status\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\n\nfrom .forwarding import get_fwd_anything_objs, get_fwd_openai_style_objs\nfrom .forwarding.settings import (\n    RATE_LIMIT_STRATEGY,\n    dynamic_rate_limit,\n    get_limiter_key,\n)", "    get_limiter_key,\n)\n\nlimiter = Limiter(key_func=get_limiter_key, strategy=RATE_LIMIT_STRATEGY)\n\napp = FastAPI(title=\"openai_forward\", version=\"0.5\")\n\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n", "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n\n@app.get(\n    \"/healthz\",\n    summary=\"Perform a Health Check\",\n    response_description=\"Return HTTP Status Code 200 (OK)\",\n    status_code=status.HTTP_200_OK,\n)\n@limiter.limit(dynamic_rate_limit)\ndef healthz(request: Request):\n    print(request.scope.get(\"client\"))\n    return \"OK\"", ")\n@limiter.limit(dynamic_rate_limit)\ndef healthz(request: Request):\n    print(request.scope.get(\"client\"))\n    return \"OK\"\n\n\nadd_route = lambda obj: app.add_route(\n    obj.ROUTE_PREFIX + \"{api_path:path}\",\n    limiter.limit(dynamic_rate_limit)(obj.reverse_proxy),", "    obj.ROUTE_PREFIX + \"{api_path:path}\",\n    limiter.limit(dynamic_rate_limit)(obj.reverse_proxy),\n    methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\", \"HEAD\", \"PATCH\", \"TRACE\"],\n)\n\n[add_route(obj) for obj in get_fwd_openai_style_objs()]\n[add_route(obj) for obj in get_fwd_anything_objs()]\n"]}
{"filename": "openai_forward/config.py", "chunked_list": ["import functools\nimport logging\nimport os\nimport sys\nimport time\n\nfrom loguru import logger\nfrom rich import print\nfrom rich.panel import Panel\nfrom rich.table import Table", "from rich.panel import Panel\nfrom rich.table import Table\n\n\ndef print_startup_info(base_url, route_prefix, api_key, fwd_key, log_chat, style):\n    \"\"\"\n    Prints the startup information of the application.\n    \"\"\"\n    try:\n        from dotenv import load_dotenv\n\n        load_dotenv(\".env\")\n    except Exception:\n        ...\n    route_prefix = route_prefix or \"/\"\n    if not isinstance(api_key, str):\n        api_key = True if len(api_key) else False\n    if not isinstance(fwd_key, str):\n        fwd_key = True if len(fwd_key) else False\n    table = Table(title=\"\", box=None, width=50)\n\n    matrcs = {\n        \"base url\": {\n            'value': base_url,\n        },\n        \"route prefix\": {\n            'value': route_prefix,\n        },\n        \"api keys\": {\n            'value': str(api_key),\n        },\n        \"forward keys\": {\n            'value': str(fwd_key),\n            'style': \"#62E883\" if fwd_key or not api_key else \"red\",\n        },\n        \"Log chat\": {\n            'value': str(log_chat),\n        },\n    }\n    table.add_column(\"\", justify='left', width=10)\n    table.add_column(\"\", justify='left')\n    for key, value in matrcs.items():\n        table.add_row(key, value['value'], style=value.get('style', style))\n\n    print(Panel(table, title=\"\ud83e\udd17 openai-forward is ready to serve! \", expand=False))", "\n\ndef print_rate_limit_info(rate_limit: dict, strategy: str, **kwargs):\n    \"\"\"\n    Print rate limit information.\n\n    Args:\n        rate_limit (dict): A dictionary containing route rate limit.\n        strategy (str): The strategy used for rate limiting.\n        **kwargs: Other limits info.\n\n    Returns:\n        None\n    \"\"\"\n    table = Table(title=\"\", box=None, width=50)\n    table.add_column(\"\")\n    table.add_column(\"\", justify='left')\n    table.add_row(\"strategy\", strategy, style='#7CD9FF')\n    for key, value in rate_limit.items():\n        table.add_row(key, str(value), style='#C5FF95')\n    for key, value in kwargs.items():\n        table.add_row(key, str(value), style='#C5FF95')\n    print(Panel(table, title=\"\u23f1\ufe0f Rate Limit configuration\", expand=False))", "\n\nclass InterceptHandler(logging.Handler):\n    def emit(self, record):\n        # Get corresponding Loguru level if it exists\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Find caller from where originated the logged message\n        frame, depth = logging.currentframe(), 6\n        while frame.f_code.co_filename == logging.__file__:\n            frame = frame.f_back\n            depth += 1\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )", "\n\ndef setting_log(\n    save_file=False,\n    openai_route_prefix=None,\n    log_name=\"openai_forward\",\n    multi_process=True,\n):\n    \"\"\"\n    Configures the logging settings for the application.\n    \"\"\"\n    # TODO \u4fee\u590d\u65f6\u533a\u914d\u7f6e\n    if os.environ.get(\"TZ\") == \"Asia/Shanghai\":\n        os.environ[\"TZ\"] = \"UTC-8\"\n        if hasattr(time, \"tzset\"):\n            time.tzset()\n\n    logging.root.handlers = [InterceptHandler()]\n    for name in logging.root.manager.loggerDict.keys():\n        logging.getLogger(name).handlers = []\n        logging.getLogger(name).propagate = True\n\n    config_handlers = [\n        {\"sink\": sys.stdout, \"level\": \"DEBUG\"},\n    ]\n\n    def filter_func(_prefix, _postfix, record):\n        chat_key = f\"{_prefix}{_postfix}\"\n        return chat_key in record[\"extra\"]\n\n    for prefix in openai_route_prefix or []:\n        _prefix = prefix.replace('/', '_')\n\n        config_handlers.extend(\n            [\n                {\n                    \"sink\": f\"./Log/chat/{_prefix}/chat.log\",\n                    \"enqueue\": multi_process,\n                    \"rotation\": \"50 MB\",\n                    \"filter\": functools.partial(filter_func, _prefix, \"_chat\"),\n                    \"format\": \"{message}\",\n                },\n                {\n                    \"sink\": f\"./Log/whisper/{_prefix}/whisper.log\",\n                    \"enqueue\": multi_process,\n                    \"rotation\": \"30 MB\",\n                    \"filter\": functools.partial(filter_func, _prefix, \"_whisper\"),\n                    \"format\": \"{message}\",\n                },\n            ]\n        )\n\n    if save_file:\n        config_handlers += [\n            {\n                \"sink\": f\"./Log/{log_name}.log\",\n                \"enqueue\": multi_process,\n                \"rotation\": \"100 MB\",\n                \"level\": \"INFO\",\n            }\n        ]\n\n    logger_config = {\"handlers\": config_handlers}\n    logger.configure(**logger_config)", ""]}
{"filename": "openai_forward/__init__.py", "chunked_list": ["__version__ = \"0.5.0\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=False)\n"]}
{"filename": "openai_forward/helper.py", "chunked_list": ["import ast\nimport asyncio\nimport inspect\nimport os\nimport time\nfrom functools import wraps\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\nimport orjson", "\nimport orjson\nfrom fastapi import Request\nfrom rich import print\n\n\ndef get_client_ip(request: Request):\n    if \"x-forwarded-for\" in request.headers:\n        return request.headers[\"x-forwarded-for\"]\n    elif not request.client or not request.client.host:\n        return \"127.0.0.1\"\n    return request.client.host", "\n\ndef relp(rel_path: Union[str, Path], parents=0, return_str=True, strict=False):\n    currentframe = inspect.currentframe()\n    f = currentframe.f_back\n    for _ in range(parents):\n        f = f.f_back\n    current_path = Path(f.f_code.co_filename).parent\n    pathlib_path = current_path / rel_path\n    pathlib_path = pathlib_path.resolve(strict=strict)\n    if return_str:\n        return str(pathlib_path)\n    else:\n        return pathlib_path", "\n\ndef ls(_dir, *patterns, concat='extend', recursive=False):\n    from glob import glob\n\n    path_list = []\n    for pattern in patterns:\n        if concat == 'extend':\n            path_list.extend(glob(os.path.join(_dir, pattern), recursive=recursive))\n        else:\n            path_list.append(glob(os.path.join(_dir, pattern), recursive=recursive))\n    return path_list", "\n\ndef retry(max_retries=3, delay=1, backoff=2, exceptions=(Exception,)):\n    \"\"\"\n    Retry decorator.\n\n    Parameters:\n    - max_retries: Maximum number of retries.\n    - delay: Initial delay in seconds.\n    - backoff: Multiplier for delay after each retry.\n    - exceptions: Exceptions to catch and retry on, as a tuple.\n\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            retries = 0\n            current_delay = delay\n            while retries < max_retries:\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    retries += 1\n                    if retries >= max_retries:\n                        raise\n                    time.sleep(current_delay)\n                    current_delay *= backoff\n\n        return wrapper\n\n    return decorator", "\n\ndef async_retry(max_retries=3, delay=1, backoff=2, exceptions=(Exception,)):\n    \"\"\"\n    Retry decorator for asynchronous functions.\n\n    Parameters:\n    - max_retries: Maximum number of retries.\n    - delay: Initial delay in seconds.\n    - backoff: Multiplier for delay after each retry.\n    - exceptions: Exceptions to catch and retry on, as a tuple.\n\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            retries = 0\n            current_delay = delay\n            while retries < max_retries:\n                try:\n                    return await func(*args, **kwargs)\n                except exceptions as e:\n                    retries += 1\n                    if retries >= max_retries:\n                        raise\n                    await asyncio.sleep(current_delay)\n                    current_delay *= backoff\n\n        return wrapper\n\n    return decorator", "\n\ndef json_load(filepath: str, rel=False, mode=\"rb\"):\n    abs_path = relp(filepath, parents=1) if rel else filepath\n    with open(abs_path, mode=mode) as f:\n        return orjson.loads(f.read())\n\n\ndef json_dump(\n    data: Union[List, Dict], filepath: str, rel=False, indent_2=False, mode=\"wb\"\n):\n    orjson_option = 0\n    if indent_2:\n        orjson_option = orjson.OPT_INDENT_2\n    abs_path = relp(filepath, parents=1) if rel else filepath\n    with open(abs_path, mode=mode) as f:\n        f.write(orjson.dumps(data, option=orjson_option))", "def json_dump(\n    data: Union[List, Dict], filepath: str, rel=False, indent_2=False, mode=\"wb\"\n):\n    orjson_option = 0\n    if indent_2:\n        orjson_option = orjson.OPT_INDENT_2\n    abs_path = relp(filepath, parents=1) if rel else filepath\n    with open(abs_path, mode=mode) as f:\n        f.write(orjson.dumps(data, option=orjson_option))\n", "\n\ndef toml_load(filepath: str, rel=False):\n    import toml\n\n    abs_path = relp(filepath, parents=1) if rel else filepath\n    return toml.load(abs_path)\n\n\ndef str2list(s: str, sep):\n    if s:\n        return [i.strip() for i in s.split(sep) if i.strip()]\n    else:\n        return []", "\ndef str2list(s: str, sep):\n    if s:\n        return [i.strip() for i in s.split(sep) if i.strip()]\n    else:\n        return []\n\n\ndef env2list(env_name: str, sep=\",\"):\n    return str2list(os.environ.get(env_name, \"\").strip(), sep=sep)", "def env2list(env_name: str, sep=\",\"):\n    return str2list(os.environ.get(env_name, \"\").strip(), sep=sep)\n\n\ndef env2dict(env_name: str) -> Dict:\n    import json\n\n    env_str = os.environ.get(env_name, \"\").strip()\n    if not env_str:\n        return {}\n    return json.loads(env_str)", "\n\ndef format_route_prefix(route_prefix: str):\n    if route_prefix:\n        if route_prefix.endswith(\"/\"):\n            route_prefix = route_prefix[:-1]\n        if not route_prefix.startswith(\"/\"):\n            route_prefix = \"/\" + route_prefix\n    return route_prefix\n", "\n\ndef get_matches(messages: List[Dict], assistants: List[Dict]):\n    msg_len, ass_len = len(messages), len(assistants)\n    if msg_len != ass_len:\n        print(f\"Length mismatch between message({msg_len}) and assistant({ass_len}) \")\n\n    cvt = lambda msg, ass: {\n        \"datetime\": msg.get('datetime'),\n        \"forwarded-for\": msg.get(\"forwarded-for\"),\n        \"model\": msg.get(\"model\"),\n        \"messages\": msg.get(\"messages\"),\n        \"assistant\": ass.get(\"assistant\"),\n    }\n\n    msg_uid_dict = {m.pop(\"uid\"): m for m in messages}\n    ass_uid_dict = {a.pop(\"uid\"): a for a in assistants}\n    matches = [\n        cvt(msg_uid_dict[uid], ass_uid_dict[uid])\n        for uid in msg_uid_dict\n        if uid in ass_uid_dict\n    ]\n\n    ref_len = max(msg_len, ass_len)\n    if len(matches) != ref_len:\n        print(f\"There are {ref_len - len(matches)} mismatched items\")\n    return matches", "\n\ndef parse_log_to_list(log_path: str):\n    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n        messages, assistant = [], []\n        for line in f.readlines():\n            content: dict = ast.literal_eval(line)\n            if content.get(\"messages\"):\n                messages.append(content)\n            else:\n                assistant.append(content)\n    return messages, assistant", "\n\ndef convert_chatlog_to_jsonl(log_path: str, target_path: str):\n    \"\"\"Convert single chatlog to jsonl\"\"\"\n    message_list, assistant_list = parse_log_to_list(log_path)\n    content_list = get_matches(messages=message_list, assistants=assistant_list)\n    json_dump(content_list, target_path, indent_2=True)\n\n\ndef get_log_files_from_folder(log_path: str):\n    return ls(log_path, \"*.log\")", "\ndef get_log_files_from_folder(log_path: str):\n    return ls(log_path, \"*.log\")\n\n\ndef convert_folder_to_jsonl(folder_path: str, target_path: str):\n    \"\"\"Convert chatlog folder to jsonl\"\"\"\n    log_files = get_log_files_from_folder(folder_path)\n    messages = []\n    assistants = []\n    for log_path in log_files:\n        msg, ass = parse_log_to_list(log_path)\n\n        msg_len, ass_len = len(msg), len(ass)\n        if msg_len != ass_len:\n            print(\n                f\"{log_path}: Length mismatch between message({msg_len}) and assistant({ass_len}) \"\n            )\n        messages.extend(msg)\n        assistants.extend(ass)\n    content_list = get_matches(messages=messages, assistants=assistants)\n    json_dump(content_list, target_path, indent_2=True)\n    print(\"Converted successfully\")\n    print(f\"File saved to {target_path}\")", ""]}
{"filename": "openai_forward/forwarding/extra.py", "chunked_list": ["from .base import ForwardingBase\n\n\nclass AnyForwarding(ForwardingBase):\n    def __init__(self, base_url: str, route_prefix: str, proxy=None):\n        import httpx\n\n        self.BASE_URL = base_url\n        self.ROUTE_PREFIX = route_prefix\n        self.client = httpx.AsyncClient(\n            base_url=self.BASE_URL, proxies=proxy, http1=True, http2=False\n        )", "\n\ndef get_fwd_anything_objs():\n    \"\"\"\n    Generate extra forwarding objects.\n\n    Returns:\n        list: A list of AnyForwarding objects.\n    \"\"\"\n    from .settings import EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX, PROXY\n\n    extra_fwd_objs = []\n    for base_url, route_prefix in zip(EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX):\n        extra_fwd_objs.append(AnyForwarding(base_url, route_prefix, PROXY))\n    return extra_fwd_objs", ""]}
{"filename": "openai_forward/forwarding/base.py", "chunked_list": ["import asyncio\nimport time\nimport traceback\nimport uuid\nfrom itertools import cycle\nfrom typing import Any, AsyncGenerator, List\n\nimport anyio\nimport httpx\nfrom fastapi import HTTPException, Request, status", "import httpx\nfrom fastapi import HTTPException, Request, status\nfrom fastapi.responses import StreamingResponse\nfrom loguru import logger\n\nfrom ..content import ChatSaver, WhisperSaver\nfrom ..helper import async_retry\nfrom .settings import *\n\n\nclass ForwardingBase:\n    BASE_URL = None\n    ROUTE_PREFIX = None\n    client: httpx.AsyncClient = None\n\n    if IP_BLACKLIST or IP_WHITELIST:\n        validate_host = True\n    else:\n        validate_host = False\n\n    timeout = TIMEOUT\n\n    @staticmethod\n    def validate_request_host(ip):\n        \"\"\"\n        Validates the request host IP address against the IP whitelist and blacklist.\n\n        Args:\n            ip (str): The IP address to be validated.\n\n        Raises:\n            HTTPException: If the IP address is not in the whitelist or if it is in the blacklist.\n        \"\"\"\n        if IP_WHITELIST and ip not in IP_WHITELIST:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=f\"Forbidden, ip={ip} not in whitelist!\",\n            )\n        if IP_BLACKLIST and ip in IP_BLACKLIST:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=f\"Forbidden, ip={ip} in blacklist!\",\n            )\n\n    @staticmethod\n    async def aiter_bytes(r: httpx.Response) -> AsyncGenerator[bytes, Any]:\n        async for chunk in r.aiter_bytes():\n            yield chunk\n        await r.aclose()\n\n    @async_retry(\n        max_retries=3,\n        delay=0.5,\n        backoff=2,\n        exceptions=(HTTPException, anyio.EndOfStream),\n    )\n    async def try_send(self, client_config: dict, request: Request):\n        \"\"\"\n        Try to send the request.\n\n        Args:\n            client_config (dict): The configuration for the client.\n            request (Request): The request to be sent.\n\n        Returns:\n            Response: The response from the client.\n\n        Raises:\n            HTTPException: If there is a connection error or any other exception occurs.\n        \"\"\"\n        try:\n            req = self.client.build_request(\n                method=request.method,\n                url=client_config['url'],\n                headers=client_config[\"headers\"],\n                content=request.stream(),\n                timeout=self.timeout,\n            )\n            return await self.client.send(req, stream=True)\n\n        except (httpx.ConnectError, httpx.ConnectTimeout) as e:\n            error_info = (\n                f\"{type(e)}: {e} | \"\n                f\"Please check if host={request.client.host} can access [{self.BASE_URL}] successfully?\"\n            )\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_504_GATEWAY_TIMEOUT, detail=error_info\n            )\n\n        except anyio.EndOfStream:\n            error_info = \"EndOfStream Error: trying to read from a stream that has been closed from the other end.\"\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n            )\n\n        except Exception as e:\n            error_info = f\"{type(e)}: {e}\"\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n            )\n\n    def prepare_client(self, request: Request):\n        \"\"\"\n        Prepares the client configuration based on the given request.\n\n        Args:\n            request (Request): The request object containing the necessary information.\n\n        Returns:\n            dict: The client configuration dictionary with the necessary parameters set.\n                  The dictionary has the following keys:\n                  - 'auth': The authorization header value.\n                  - 'headers': The dictionary of headers.\n                  - 'url': The URL object.\n                  - 'url_path': The URL path.\n\n        Raises:\n            AssertionError: If the `BASE_URL` or `ROUTE_PREFIX` is not set.\n        \"\"\"\n        assert self.BASE_URL is not None\n        assert self.ROUTE_PREFIX is not None\n        if self.validate_host:\n            ip = request.headers.get(\"x-forwarded-for\") or \"\"\n            self.validate_request_host(ip)\n\n        _url_path = request.url.path\n        prefix_index = 0 if self.ROUTE_PREFIX == '/' else len(self.ROUTE_PREFIX)\n\n        url_path = _url_path[prefix_index:]\n        url = httpx.URL(path=url_path, query=request.url.query.encode(\"utf-8\"))\n        headers = dict(request.headers)\n        auth = headers.pop(\"authorization\", \"\")\n        content_type = headers.pop(\"content-type\", \"application/json\")\n        auth_headers_dict = {\"Content-Type\": content_type, \"Authorization\": auth}\n        client_config = {\n            'auth': auth,\n            'headers': auth_headers_dict,\n            'url': url,\n            'url_path': url_path,\n        }\n\n        return client_config\n\n    async def reverse_proxy(self, request: Request):\n        \"\"\"\n        Reverse proxies the given request.\n\n        Args:\n            request (Request): The request to be reverse proxied.\n\n        Returns:\n            StreamingResponse: The response from the reverse proxied server, as a streaming response.\n        \"\"\"\n        assert self.client is not None\n\n        client_config = self.prepare_client(request)\n\n        r = await self.try_send(client_config, request)\n\n        return StreamingResponse(\n            self.aiter_bytes(r),\n            status_code=r.status_code,\n            media_type=r.headers.get(\"content-type\"),\n        )", "\n\nclass ForwardingBase:\n    BASE_URL = None\n    ROUTE_PREFIX = None\n    client: httpx.AsyncClient = None\n\n    if IP_BLACKLIST or IP_WHITELIST:\n        validate_host = True\n    else:\n        validate_host = False\n\n    timeout = TIMEOUT\n\n    @staticmethod\n    def validate_request_host(ip):\n        \"\"\"\n        Validates the request host IP address against the IP whitelist and blacklist.\n\n        Args:\n            ip (str): The IP address to be validated.\n\n        Raises:\n            HTTPException: If the IP address is not in the whitelist or if it is in the blacklist.\n        \"\"\"\n        if IP_WHITELIST and ip not in IP_WHITELIST:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=f\"Forbidden, ip={ip} not in whitelist!\",\n            )\n        if IP_BLACKLIST and ip in IP_BLACKLIST:\n            raise HTTPException(\n                status_code=status.HTTP_403_FORBIDDEN,\n                detail=f\"Forbidden, ip={ip} in blacklist!\",\n            )\n\n    @staticmethod\n    async def aiter_bytes(r: httpx.Response) -> AsyncGenerator[bytes, Any]:\n        async for chunk in r.aiter_bytes():\n            yield chunk\n        await r.aclose()\n\n    @async_retry(\n        max_retries=3,\n        delay=0.5,\n        backoff=2,\n        exceptions=(HTTPException, anyio.EndOfStream),\n    )\n    async def try_send(self, client_config: dict, request: Request):\n        \"\"\"\n        Try to send the request.\n\n        Args:\n            client_config (dict): The configuration for the client.\n            request (Request): The request to be sent.\n\n        Returns:\n            Response: The response from the client.\n\n        Raises:\n            HTTPException: If there is a connection error or any other exception occurs.\n        \"\"\"\n        try:\n            req = self.client.build_request(\n                method=request.method,\n                url=client_config['url'],\n                headers=client_config[\"headers\"],\n                content=request.stream(),\n                timeout=self.timeout,\n            )\n            return await self.client.send(req, stream=True)\n\n        except (httpx.ConnectError, httpx.ConnectTimeout) as e:\n            error_info = (\n                f\"{type(e)}: {e} | \"\n                f\"Please check if host={request.client.host} can access [{self.BASE_URL}] successfully?\"\n            )\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_504_GATEWAY_TIMEOUT, detail=error_info\n            )\n\n        except anyio.EndOfStream:\n            error_info = \"EndOfStream Error: trying to read from a stream that has been closed from the other end.\"\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n            )\n\n        except Exception as e:\n            error_info = f\"{type(e)}: {e}\"\n            traceback_info = traceback.format_exc()\n            logger.error(f\"{error_info} traceback={traceback_info}\")\n            raise HTTPException(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_info\n            )\n\n    def prepare_client(self, request: Request):\n        \"\"\"\n        Prepares the client configuration based on the given request.\n\n        Args:\n            request (Request): The request object containing the necessary information.\n\n        Returns:\n            dict: The client configuration dictionary with the necessary parameters set.\n                  The dictionary has the following keys:\n                  - 'auth': The authorization header value.\n                  - 'headers': The dictionary of headers.\n                  - 'url': The URL object.\n                  - 'url_path': The URL path.\n\n        Raises:\n            AssertionError: If the `BASE_URL` or `ROUTE_PREFIX` is not set.\n        \"\"\"\n        assert self.BASE_URL is not None\n        assert self.ROUTE_PREFIX is not None\n        if self.validate_host:\n            ip = request.headers.get(\"x-forwarded-for\") or \"\"\n            self.validate_request_host(ip)\n\n        _url_path = request.url.path\n        prefix_index = 0 if self.ROUTE_PREFIX == '/' else len(self.ROUTE_PREFIX)\n\n        url_path = _url_path[prefix_index:]\n        url = httpx.URL(path=url_path, query=request.url.query.encode(\"utf-8\"))\n        headers = dict(request.headers)\n        auth = headers.pop(\"authorization\", \"\")\n        content_type = headers.pop(\"content-type\", \"application/json\")\n        auth_headers_dict = {\"Content-Type\": content_type, \"Authorization\": auth}\n        client_config = {\n            'auth': auth,\n            'headers': auth_headers_dict,\n            'url': url,\n            'url_path': url_path,\n        }\n\n        return client_config\n\n    async def reverse_proxy(self, request: Request):\n        \"\"\"\n        Reverse proxies the given request.\n\n        Args:\n            request (Request): The request to be reverse proxied.\n\n        Returns:\n            StreamingResponse: The response from the reverse proxied server, as a streaming response.\n        \"\"\"\n        assert self.client is not None\n\n        client_config = self.prepare_client(request)\n\n        r = await self.try_send(client_config, request)\n\n        return StreamingResponse(\n            self.aiter_bytes(r),\n            status_code=r.status_code,\n            media_type=r.headers.get(\"content-type\"),\n        )", "\n\nclass OpenaiBase(ForwardingBase):\n    _cycle_api_key = cycle(OPENAI_API_KEY)\n    _no_auth_mode = OPENAI_API_KEY != [] and FWD_KEY == set()\n\n    chatsaver: ChatSaver = None\n    whispersaver: WhisperSaver = None\n\n    def _add_result_log(\n        self, byte_list: List[bytes], uid: str, route_path: str, request_method: str\n    ):\n        \"\"\"\n        Adds a result log for the given byte list, uid, route path, and request method.\n\n        Args:\n            byte_list (List[bytes]): The list of bytes to be processed.\n            uid (str): The unique identifier.\n            route_path (str): The route path.\n            request_method (str): The request method.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            if LOG_CHAT and request_method == \"POST\":\n                if route_path == \"/v1/chat/completions\":\n                    target_info = self.chatsaver.parse_iter_bytes(byte_list)\n                    self.chatsaver.log_chat(\n                        {target_info[\"role\"]: target_info[\"content\"], \"uid\": uid}\n                    )\n\n                elif route_path.startswith(\"/v1/audio/\"):\n                    self.whispersaver.add_log(b\"\".join([_ for _ in byte_list]))\n\n                else:\n                    ...\n        except Exception as e:\n            logger.warning(f\"log chat (not) error:\\n{traceback.format_exc()}\")\n\n    async def _add_payload_log(self, request: Request, url_path: str):\n        \"\"\"\n        Adds a payload log for the given request.\n\n        Args:\n            request (Request): The request object.\n            url_path (str): The URL path of the request.\n\n        Returns:\n            str: The unique identifier (UID) of the payload log, which is used to match the chat result log.\n\n        Raises:\n            Suppress all errors.\n\n        Notes:\n            - If `LOG_CHAT` is True and the request method is \"POST\", the chat payload will be logged.\n            - If the `url_path` is \"/v1/chat/completions\", the chat payload will be parsed and logged.\n            - If the `url_path` starts with \"/v1/audio/\", a new UID will be generated.\n        \"\"\"\n        uid = None\n        if LOG_CHAT and request.method == \"POST\":\n            try:\n                if url_path == \"/v1/chat/completions\":\n                    chat_info = await self.chatsaver.parse_payload(request)\n                    uid = chat_info.get(\"uid\")\n                    if chat_info:\n                        self.chatsaver.log_chat(chat_info)\n\n                elif url_path.startswith(\"/v1/audio/\"):\n                    uid = uuid.uuid4().__str__()\n\n                else:\n                    ...\n\n            except Exception as e:\n                logger.warning(\n                    f\"log chat error:\\nhost:{request.client.host} method:{request.method}: {traceback.format_exc()}\"\n                )\n        return uid\n\n    async def openai_aiter_bytes(\n        self, r: httpx.Response, request: Request, route_path: str, uid: str\n    ):\n        \"\"\"\n        Asynchronously iterates over the bytes of the response and yields each chunk.\n\n        Args:\n            r (httpx.Response): The HTTP response object.\n            request (Request): The original request object.\n            route_path (str): The route path.\n            uid (str): The unique identifier.\n\n        Returns:\n            A generator that yields each chunk of bytes from the response.\n        \"\"\"\n        byte_list = []\n        start_time = time.perf_counter()\n        idx = 0\n        async for chunk in r.aiter_bytes():\n            idx += 1\n            byte_list.append(chunk)\n            if TOKEN_INTERVAL > 0:\n                current_time = time.perf_counter()\n                delta = current_time - start_time\n                delay = TOKEN_INTERVAL - delta\n                if delay > 0:\n                    await asyncio.sleep(delay)\n                start_time = time.perf_counter()\n            yield chunk\n\n        await r.aclose()\n\n        if uid:\n            if r.is_success:\n                self._add_result_log(byte_list, uid, route_path, request.method)\n            else:\n                response_info = b\"\".join([_ for _ in byte_list])\n                logger.warning(f'uid: {uid}\\n' f'{response_info}')\n\n    async def reverse_proxy(self, request: Request):\n        \"\"\"\n        Reverse proxies the given requests.\n\n        Args:\n            request (Request): The incoming request object.\n\n        Returns:\n            StreamingResponse: The response from the reverse proxied server, as a streaming response.\n        \"\"\"\n        client_config = self.prepare_client(request)\n        url_path = client_config[\"url_path\"]\n\n        def set_apikey_from_preset():\n            nonlocal client_config\n            auth_prefix = \"Bearer \"\n            auth = client_config[\"auth\"]\n            if self._no_auth_mode or auth and auth[len(auth_prefix) :] in FWD_KEY:\n                auth = auth_prefix + next(self._cycle_api_key)\n                client_config[\"headers\"][\"Authorization\"] = auth\n\n        set_apikey_from_preset()\n\n        uid = await self._add_payload_log(request, url_path)\n\n        r = await self.try_send(client_config, request)\n\n        return StreamingResponse(\n            self.openai_aiter_bytes(r, request, url_path, uid),\n            status_code=r.status_code,\n            media_type=r.headers.get(\"content-type\"),\n        )", ""]}
{"filename": "openai_forward/forwarding/settings.py", "chunked_list": ["import itertools\nimport os\n\nimport limits\nfrom fastapi import Request\n\nfrom ..config import print_rate_limit_info, print_startup_info, setting_log\nfrom ..helper import env2dict, env2list, format_route_prefix\n\nTIMEOUT = 600", "\nTIMEOUT = 600\n\nENV_VAR_SEP = \",\"\nOPENAI_BASE_URL = env2list(\"OPENAI_BASE_URL\", sep=ENV_VAR_SEP) or [\n    \"https://api.openai.com\"\n]\n\nOPENAI_ROUTE_PREFIX = [\n    format_route_prefix(i) for i in env2list(\"OPENAI_ROUTE_PREFIX\", sep=ENV_VAR_SEP)", "OPENAI_ROUTE_PREFIX = [\n    format_route_prefix(i) for i in env2list(\"OPENAI_ROUTE_PREFIX\", sep=ENV_VAR_SEP)\n] or ['/']\n\nEXTRA_BASE_URL = env2list(\"EXTRA_BASE_URL\", sep=ENV_VAR_SEP)\nEXTRA_ROUTE_PREFIX = [\n    format_route_prefix(i) for i in env2list(\"EXTRA_ROUTE_PREFIX\", sep=ENV_VAR_SEP)\n]\n\nLOG_CHAT = os.environ.get(\"LOG_CHAT\", \"False\").strip().lower() == \"true\"\nif LOG_CHAT:\n    setting_log(openai_route_prefix=OPENAI_ROUTE_PREFIX)", "\nLOG_CHAT = os.environ.get(\"LOG_CHAT\", \"False\").strip().lower() == \"true\"\nif LOG_CHAT:\n    setting_log(openai_route_prefix=OPENAI_ROUTE_PREFIX)\n\nIP_WHITELIST = env2list(\"IP_WHITELIST\", sep=ENV_VAR_SEP)\nIP_BLACKLIST = env2list(\"IP_BLACKLIST\", sep=ENV_VAR_SEP)\n\nOPENAI_API_KEY = env2list(\"OPENAI_API_KEY\", sep=ENV_VAR_SEP)\nFWD_KEY = env2list(\"FORWARD_KEY\", sep=ENV_VAR_SEP)", "OPENAI_API_KEY = env2list(\"OPENAI_API_KEY\", sep=ENV_VAR_SEP)\nFWD_KEY = env2list(\"FORWARD_KEY\", sep=ENV_VAR_SEP)\n\nPROXY = os.environ.get(\"PROXY\", \"\").strip()\nPROXY = PROXY if PROXY else None\n\nGLOBAL_RATE_LIMIT = os.environ.get(\"GLOBAL_RATE_LIMIT\", \"fixed-window\").strip() or None\nRATE_LIMIT_STRATEGY = os.environ.get(\"RATE_LIMIT_STRATEGY\", \"\").strip() or None\nroute_rate_limit_conf = env2dict('ROUTE_RATE_LIMIT')\n", "route_rate_limit_conf = env2dict('ROUTE_RATE_LIMIT')\n\n\ndef get_limiter_key(request: Request):\n    limiter_prefix = f\"{request.scope.get('root_path')}{request.scope.get('path')}\"\n    key = f\"{limiter_prefix}\"  # -{get_client_ip(request)}\"\n    return key\n\n\ndef dynamic_rate_limit(key: str):\n    for route in route_rate_limit_conf:\n        if key.startswith(route):\n            return route_rate_limit_conf[route]\n    return GLOBAL_RATE_LIMIT", "\ndef dynamic_rate_limit(key: str):\n    for route in route_rate_limit_conf:\n        if key.startswith(route):\n            return route_rate_limit_conf[route]\n    return GLOBAL_RATE_LIMIT\n\n\nTOKEN_RATE_LIMIT = os.environ.get(\"TOKEN_RATE_LIMIT\", \"\").strip()\nif TOKEN_RATE_LIMIT:\n    rate_limit_item = limits.parse(TOKEN_RATE_LIMIT)\n    TOKEN_INTERVAL = (\n        rate_limit_item.multiples * rate_limit_item.GRANULARITY.seconds\n    ) / rate_limit_item.amount\nelse:\n    TOKEN_INTERVAL = 0", "TOKEN_RATE_LIMIT = os.environ.get(\"TOKEN_RATE_LIMIT\", \"\").strip()\nif TOKEN_RATE_LIMIT:\n    rate_limit_item = limits.parse(TOKEN_RATE_LIMIT)\n    TOKEN_INTERVAL = (\n        rate_limit_item.multiples * rate_limit_item.GRANULARITY.seconds\n    ) / rate_limit_item.amount\nelse:\n    TOKEN_INTERVAL = 0\n\nstyles = itertools.cycle(", "\nstyles = itertools.cycle(\n    [\"#7CD9FF\", \"#BDADFF\", \"#9EFFE3\", \"#f1b8e4\", \"#F5A88E\", \"#BBCA89\"]\n)\nfor base_url, route_prefix in zip(OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX):\n    print_startup_info(\n        base_url, route_prefix, OPENAI_API_KEY, FWD_KEY, LOG_CHAT, style=next(styles)\n    )\nfor base_url, route_prefix in zip(EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX):\n    print_startup_info(base_url, route_prefix, \"\\\\\", \"\\\\\", LOG_CHAT, style=next(styles))", "for base_url, route_prefix in zip(EXTRA_BASE_URL, EXTRA_ROUTE_PREFIX):\n    print_startup_info(base_url, route_prefix, \"\\\\\", \"\\\\\", LOG_CHAT, style=next(styles))\n\nprint_rate_limit_info(\n    route_rate_limit_conf,\n    strategy=RATE_LIMIT_STRATEGY,\n    global_rate_limit=GLOBAL_RATE_LIMIT if GLOBAL_RATE_LIMIT else 'inf',\n    token_rate_limit=TOKEN_RATE_LIMIT if TOKEN_RATE_LIMIT else 'inf',\n    token_interval_time=f\"{TOKEN_INTERVAL:.4f}s\",\n)", "    token_interval_time=f\"{TOKEN_INTERVAL:.4f}s\",\n)\n"]}
{"filename": "openai_forward/forwarding/openai.py", "chunked_list": ["import time\n\nfrom .base import ChatSaver, OpenaiBase, WhisperSaver\nfrom .settings import LOG_CHAT\n\n\nclass OpenaiForwarding(OpenaiBase):\n    def __init__(self, base_url: str, route_prefix: str, proxy=None):\n        import httpx\n\n        self.BASE_URL = base_url\n        self.ROUTE_PREFIX = route_prefix\n        if LOG_CHAT:\n            self.chatsaver = ChatSaver(route_prefix)\n            self.whispersaver = WhisperSaver(route_prefix)\n        self.client = httpx.AsyncClient(\n            base_url=self.BASE_URL, proxies=proxy, http1=True, http2=False\n        )\n        self.token_counts = 0\n        self.token_limit_dict = {'time': time.time(), 'count': 0}", "\n\ndef get_fwd_openai_style_objs():\n    \"\"\"\n    Generate OPENAI route style forwarding objects.\n\n    Returns:\n        fwd_objs (list): A list of OpenaiForwarding objects.\n    \"\"\"\n    from .settings import OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX, PROXY\n\n    fwd_objs = []\n    for base_url, route_prefix in zip(OPENAI_BASE_URL, OPENAI_ROUTE_PREFIX):\n        fwd_objs.append(OpenaiForwarding(base_url, route_prefix, PROXY))\n    return fwd_objs", ""]}
{"filename": "openai_forward/forwarding/__init__.py", "chunked_list": ["from .extra import AnyForwarding, get_fwd_anything_objs\nfrom .openai import OpenaiForwarding, get_fwd_openai_style_objs\n"]}
{"filename": "openai_forward/content/decode.py", "chunked_list": ["from httpx._decoders import LineDecoder, TextChunker, TextDecoder\n\n\ndef iter_text(iter_tytes: list):\n    decoder = TextDecoder(\"utf-8\")\n    chunker = TextChunker()\n    for byte_content in iter_tytes:\n        text_content = decoder.decode(byte_content)\n        for chunk in chunker.decode(text_content):\n            yield chunk\n    text_content = decoder.flush()\n    for chunk in chunker.decode(text_content):\n        yield chunk\n    for chunk in chunker.flush():\n        yield chunk", "\n\ndef parse_to_lines(iter_bytes: list) -> list:\n    decoder = LineDecoder()\n    lines = []\n    for text in iter_text(iter_bytes):\n        lines.extend(decoder.decode(text))\n    lines.extend(decoder.flush())\n    return lines\n", ""]}
{"filename": "openai_forward/content/whisper.py", "chunked_list": ["from loguru import logger\n\n\nclass WhisperSaver:\n    def __init__(self, route_prefix: str):\n        _prefix = route_prefix.replace('/', '_')\n        self.logger = logger.bind(**{f\"{_prefix}_whisper\": True})\n\n    def add_log(self, bytes_: bytes):\n        text_content = bytes_.decode(\"utf-8\")\n        self.logger.debug(text_content)", ""]}
{"filename": "openai_forward/content/__init__.py", "chunked_list": ["from .chat import ChatSaver\nfrom .whisper import WhisperSaver\n"]}
{"filename": "openai_forward/content/chat.py", "chunked_list": ["import time\nimport uuid\nfrom typing import List\n\nimport orjson\nfrom fastapi import Request\nfrom loguru import logger\nfrom orjson import JSONDecodeError\n\nfrom ..helper import get_client_ip", "\nfrom ..helper import get_client_ip\nfrom .decode import parse_to_lines\n\n\nclass ChatSaver:\n    def __init__(self, route_prefix: str):\n        _prefix = route_prefix.replace('/', '_')\n        kwargs = {_prefix + \"_chat\": True}\n        self.logger = logger.bind(**kwargs)\n\n    @staticmethod\n    async def parse_payload(request: Request):\n        uid = uuid.uuid4().__str__()\n        payload = await request.json()\n        msgs = payload[\"messages\"]\n        model = payload[\"model\"]\n        content = {\n            \"messages\": [{msg[\"role\"]: msg[\"content\"]} for msg in msgs],\n            \"model\": model,\n            \"forwarded-for\": get_client_ip(request) or \"\",\n            \"uid\": uid,\n            \"datetime\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n        }\n        return content\n\n    def parse_iter_bytes(self, byte_list: List[bytes]):\n        \"\"\"\n        Parses a list of bytes and returns a dictionary.\n\n        Args:\n            byte_list (List[bytes]): A list of bytes to parse.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing information about the target. The dictionary has the following keys:\n                - \"created\" (str)\n                - \"id\" (str)\n                - \"model\" (str)\n                - \"role\" (str)\n                - \"content\" (str)\n        \"\"\"\n        txt_lines = parse_to_lines(byte_list)\n\n        start_line = txt_lines[0]\n        target_info = dict()\n        start_token = \"data: \"\n        start_token_len = len(start_token)\n        if start_line.startswith(start_token):\n            stream = True\n            start_line = orjson.loads(start_line[start_token_len:])\n            msg = start_line[\"choices\"][0][\"delta\"]\n        else:\n            stream = False\n            start_line = orjson.loads(\"\".join(txt_lines))\n            msg = start_line[\"choices\"][0][\"message\"]\n\n        target_info[\"created\"] = start_line[\"created\"]\n        target_info[\"id\"] = start_line[\"id\"]\n        target_info[\"model\"] = start_line[\"model\"]\n        target_info[\"role\"] = msg[\"role\"]\n        target_info[\"content\"] = msg.get(\"content\", \"\")\n\n        if not stream:\n            return target_info\n\n        # loop for stream\n        for line in txt_lines[1:]:\n            if line.startswith(start_token):\n                target_info[\"content\"] += self._parse_one_line(line[start_token_len:])\n        return target_info\n\n    @staticmethod\n    def _parse_one_line(line: str):\n        try:\n            line_dict = orjson.loads(line)\n            return line_dict[\"choices\"][0][\"delta\"][\"content\"]\n        except JSONDecodeError:\n            return \"\"\n        except KeyError:\n            return \"\"\n\n    def log_chat(self, chat_info: dict):\n        self.logger.debug(f\"{chat_info}\")", ""]}
{"filename": "Examples/embedding.py", "chunked_list": ["import openai\nfrom sparrow import yaml_load\n\nconfig = yaml_load(\"config.yaml\")\nopenai.api_base = config[\"api_base\"]\nopenai.api_key = config[\"api_key\"]\nresponse = openai.Embedding.create(\n    input=\"Your text string goes here\", model=\"text-embedding-ada-002\"\n)\nembeddings = response['data'][0]['embedding']", ")\nembeddings = response['data'][0]['embedding']\nprint(embeddings)\n"]}
{"filename": "Examples/whisper.py", "chunked_list": ["# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\nimport openai\nfrom sparrow import relp, yaml_load\n\nconfig = yaml_load(\"config.yaml\")\nopenai.api_base = config[\"api_base\"]\nopenai.api_key = config[\"api_key\"]\naudio_file = open(relp(\"../.github/data/whisper.m4a\"), \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nprint(transcript)", "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nprint(transcript)\n"]}
{"filename": "Examples/chat.py", "chunked_list": ["import time\n\nimport openai\nfrom rich import print\nfrom sparrow import yaml_load\n\nconfig = yaml_load(\"config.yaml\", rel_path=True)\nprint(f\"{config=}\")\nopenai.api_base = config[\"api_base\"]\nopenai.api_key = config[\"api_key\"]", "openai.api_base = config[\"api_base\"]\nopenai.api_key = config[\"api_key\"]\n\nstream = True\nuser_content = \"\"\"\n\u7528c\u5b9e\u73b0\u76ee\u524d\u5df2\u77e5\u6700\u5feb\u5e73\u65b9\u6839\u7b97\u6cd5\n\"\"\"\n\nresp = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",", "resp = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    # model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n    stream=stream,\n)\n\nif stream:\n    chunk_message = next(resp)['choices'][0]['delta']\n    print(f\"{chunk_message['role']}: \")\n    for chunk in resp:\n        chunk_message = chunk['choices'][0]['delta']\n        content = chunk_message.get(\"content\", \"\")\n        print(content, end=\"\")\n    print()\nelse:\n    print(resp.choices)", "\nif stream:\n    chunk_message = next(resp)['choices'][0]['delta']\n    print(f\"{chunk_message['role']}: \")\n    for chunk in resp:\n        chunk_message = chunk['choices'][0]['delta']\n        content = chunk_message.get(\"content\", \"\")\n        print(content, end=\"\")\n    print()\nelse:\n    print(resp.choices)", "\n\"\"\"\ngpt-4:\n\n\u4ee5\u4e0b\u662f\u7528C\u8bed\u8a00\u5b9e\u73b0\u7684\u6700\u5feb\u5df2\u77e5\u7684\u4e00\u79cd\u5e73\u65b9\u6839\u7b97\u6cd5\uff0c\u4e5f\u53eb\u505a \"Fast Inverse Square Root\"\u3002\u8fd9\u79cd\u7b97\u6cd5\u9996\u6b21\u51fa\u73b0\u5728\u96f7\u795e\u4e4b\u95243\u7684\u6e90\u4ee3\u7801\u4e2d\uff0c\u88ab\u5927\u91cf\u7684\u73b0\u4ee33D\u56fe\u5f62\u8ba1\u7b97\u6240\u4f7f\u7528\u3002\n\n```c\n#include <stdint.h>\n\nfloat Q_rsqrt(float number){", "\nfloat Q_rsqrt(float number){\n    long i;\n    float x2, y;\n    const float threehalfs = 1.5F;\n\n    x2 = number * 0.5F;\n    y  = number;\n    i  = * ( long * ) &y;\n    i  = 0x5f3759df - ( i >> 1 );", "    i  = * ( long * ) &y;\n    i  = 0x5f3759df - ( i >> 1 );\n    y  = * ( float * ) &i;\n    y  = y * ( threehalfs - ( x2 * y * y ) );\n\n    return y;\n}\n```\n\u8fd9\u79cd\u7b97\u6cd5\u7684\u7cbe\u786e\u5ea6\u5e76\u4e0d\u662f\u5f88\u9ad8\uff0c\u4f46\u5b83\u7684\u901f\u5ea6\u5feb\u5230\u8db3\u4ee5\u505a\u5b9e\u65f6\u56fe\u5f62\u8ba1\u7b97\u3002\u4e0a\u8ff0\u4ee3\u7801\u7684\u4e3b\u8981\u601d\u60f3\u662f\u901a\u8fc7\u5bf9IEEE 754\u6d6e\u70b9\u6570\u8868\u793a\u6cd5\u7684\u7406\u89e3\u548c\u5229\u7528\uff0c\u501f\u52a9\u6574\u6570\u548c\u6d6e\u70b9\u6570\u7684\u4e8c\u8fdb\u5236\u8868\u793a\u5728\u7565\u6709\u4e0d\u540c\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u903c\u8fd1\u6c42\u89e3\u5e73\u65b9\u6839\u5012\u6570\u7684\u65b9\u6cd5\u3002\n", "\u8fd9\u79cd\u7b97\u6cd5\u7684\u7cbe\u786e\u5ea6\u5e76\u4e0d\u662f\u5f88\u9ad8\uff0c\u4f46\u5b83\u7684\u901f\u5ea6\u5feb\u5230\u8db3\u4ee5\u505a\u5b9e\u65f6\u56fe\u5f62\u8ba1\u7b97\u3002\u4e0a\u8ff0\u4ee3\u7801\u7684\u4e3b\u8981\u601d\u60f3\u662f\u901a\u8fc7\u5bf9IEEE 754\u6d6e\u70b9\u6570\u8868\u793a\u6cd5\u7684\u7406\u89e3\u548c\u5229\u7528\uff0c\u501f\u52a9\u6574\u6570\u548c\u6d6e\u70b9\u6570\u7684\u4e8c\u8fdb\u5236\u8868\u793a\u5728\u7565\u6709\u4e0d\u540c\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u903c\u8fd1\u6c42\u89e3\u5e73\u65b9\u6839\u5012\u6570\u7684\u65b9\u6cd5\u3002\n\n\u6ce8\u610f\uff0c\u8fd9\u4e2a\u51fd\u6570\u5b9e\u9645\u4e0a\u6c42\u89e3\u7684\u662f\u5e73\u65b9\u6839\u7684\u5012\u6570\uff0c\u4e5f\u5c31\u662f1/sqrt(x)\uff0c\u5982\u679c\u9700\u8981\u5f97\u5230sqrt(x)\u7684\u7ed3\u679c\uff0c\u53ea\u9700\u8981\u5c06\u51fd\u6570\u8fd4\u56de\u503c\u53d6\u5012\u6570\u5373\u53ef\u3002\u8fd9\u662f\u56e0\u4e3a\u57283D\u56fe\u5f62\u8ba1\u7b97\u4e2d\uff0c\u5f80\u5f80\u66f4\u9891\u7e41\u5730\u9700\u8981\u6c42\u89e3\u5e73\u65b9\u6839\u5012\u6570\uff0c\u800c\u76f4\u63a5\u6c42\u89e3\u5e73\u65b9\u6839\u53cd\u800c\u8f83\u4e3a\u7f55\u89c1\u3002\n\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n\ngpt-3.5-turbo:\n\n\u76ee\u524d\u5df2\u7ecf\u53d1\u73b0\u7684\u6700\u5feb\u5e73\u65b9\u6839\u7b97\u6cd5\u662f\u725b\u987f\u8fed\u4ee3\u6cd5\uff0c\u53ef\u4ee5\u7528C\u8bed\u8a00\u5b9e\u73b0\u5982\u4e0b\uff1a", "\n\u76ee\u524d\u5df2\u7ecf\u53d1\u73b0\u7684\u6700\u5feb\u5e73\u65b9\u6839\u7b97\u6cd5\u662f\u725b\u987f\u8fed\u4ee3\u6cd5\uff0c\u53ef\u4ee5\u7528C\u8bed\u8a00\u5b9e\u73b0\u5982\u4e0b\uff1a\n\n```c\n#include <stdio.h>\n\ndouble sqrt_newton(double x) {\n    if (x == 0) {\n        return 0;\n    }", "        return 0;\n    }\n    \n    double guess = x / 2;  // \u521d\u59cb\u731c\u6d4b\u503c\u4e3ax\u7684\u4e00\u534a\n    \n    while (1) {\n        double new_guess = (guess + x / guess) / 2;  // \u6839\u636e\u725b\u987f\u8fed\u4ee3\u6cd5\u8ba1\u7b97\u65b0\u7684\u731c\u6d4b\u503c\n        if (new_guess == guess) {  // \u5982\u679c\u65b0\u7684\u731c\u6d4b\u503c\u4e0e\u4e0a\u4e00\u6b21\u7684\u731c\u6d4b\u503c\u76f8\u540c\uff0c\u8fed\u4ee3\u7ed3\u675f\n            break;\n        }", "            break;\n        }\n        guess = new_guess;\n    }\n    \n    return guess;\n}\n\nint main() {\n    double x = 16;  // \u4ee516\u4e3a\u4f8b\u8fdb\u884c\u6d4b\u8bd5", "int main() {\n    double x = 16;  // \u4ee516\u4e3a\u4f8b\u8fdb\u884c\u6d4b\u8bd5\n    double result = sqrt_newton(x);\n    printf(\"The square root of %lf is %lf\\n\", x, result);\n    \n    return 0;\n}\n```\n\n\u8be5\u7a0b\u5e8f\u4f7f\u7528\u725b\u987f\u8fed\u4ee3\u6cd5\u6765\u8ba1\u7b97\u5e73\u65b9\u6839\uff0c\u521d\u59cb\u731c\u6d4b\u503c\u4e3a\u5f85\u5f00\u65b9\u6570\u7684\u4e00\u534a\u3002\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u8ba1\u7b97\u65b0\u7684\u731c\u6d4b\u503c\uff0c\u76f4\u5230\u65b0\u7684\u731c\u6d4b\u503c\u4e0e\u4e0a\u4e00\u6b21\u7684\u731c\u6d4b\u503c\u76f8\u540c\uff0c\u8fed\u4ee3\u7ed3\u675f\u3002\u6700\u540e\u8f93\u51fa\u8ba1\u7b97\u5f97\u5230\u7684\u5e73\u65b9\u6839\u7ed3\u679c\u3002", "\n\u8be5\u7a0b\u5e8f\u4f7f\u7528\u725b\u987f\u8fed\u4ee3\u6cd5\u6765\u8ba1\u7b97\u5e73\u65b9\u6839\uff0c\u521d\u59cb\u731c\u6d4b\u503c\u4e3a\u5f85\u5f00\u65b9\u6570\u7684\u4e00\u534a\u3002\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u8ba1\u7b97\u65b0\u7684\u731c\u6d4b\u503c\uff0c\u76f4\u5230\u65b0\u7684\u731c\u6d4b\u503c\u4e0e\u4e0a\u4e00\u6b21\u7684\u731c\u6d4b\u503c\u76f8\u540c\uff0c\u8fed\u4ee3\u7ed3\u675f\u3002\u6700\u540e\u8f93\u51fa\u8ba1\u7b97\u5f97\u5230\u7684\u5e73\u65b9\u6839\u7ed3\u679c\u3002\n\n\n\"\"\"\n"]}
{"filename": "scripts/keep_render_alive.py", "chunked_list": ["import time\nfrom urllib.parse import urljoin\n\nimport httpx\nimport schedule\n\n\ndef job(url: str = \"https://render.openai-forward.com\"):\n    health_url = urljoin(url, \"/healthz\")\n    try:\n        r = httpx.get(health_url, timeout=5)\n        result = r.json()\n        print(result)\n        assert result == \"OK\"\n    except Exception as e:\n        print(e)", "\n\nif __name__ == \"__main__\":\n    job()\n    schedule.every(10).minutes.do(job)\n    while True:\n        schedule.run_pending()\n        time.sleep(60)\n", ""]}
{"filename": "tests/test_http.py", "chunked_list": ["import subprocess\nimport time\n\nimport httpx\nfrom sparrow.multiprocess import kill\nfrom utils import rm\n\n\nclass TestRun:\n    @classmethod\n    def setup_class(cls):\n        kill(8000)\n        base_url = \"https://api.openai.com\"\n        subprocess.Popen([\"nohup\", \"openai-forward\", \"run\", \"--base_url\", base_url])\n        time.sleep(3)\n\n    @classmethod\n    def teardown_class(cls):\n        kill(8000)\n        rm(\"nohup.out\")\n\n    def test_get_doc(self):\n        resp = httpx.get(\"http://localhost:8000/healthz\")\n        assert resp.is_success\n\n    def test_get_chat_completions(self):\n        resp = httpx.get(\"http://localhost:8000/v1/chat/completions\")\n        assert resp.status_code == 401", "class TestRun:\n    @classmethod\n    def setup_class(cls):\n        kill(8000)\n        base_url = \"https://api.openai.com\"\n        subprocess.Popen([\"nohup\", \"openai-forward\", \"run\", \"--base_url\", base_url])\n        time.sleep(3)\n\n    @classmethod\n    def teardown_class(cls):\n        kill(8000)\n        rm(\"nohup.out\")\n\n    def test_get_doc(self):\n        resp = httpx.get(\"http://localhost:8000/healthz\")\n        assert resp.is_success\n\n    def test_get_chat_completions(self):\n        resp = httpx.get(\"http://localhost:8000/v1/chat/completions\")\n        assert resp.status_code == 401", ""]}
{"filename": "tests/utils.py", "chunked_list": ["import os\nimport shutil\n\nfrom sparrow import ls\n\n\ndef rm(*file_pattern: str, rel=False):\n    \"\"\"Remove files or directories.\n    Example:\n    --------\n        >>> rm(\"*.jpg\", \"*.png\")\n        >>> rm(\"*.jpg\", \"*.png\", rel=True)\n    \"\"\"\n    path_list = ls(\".\", *file_pattern, relp=rel, concat=\"extend\")\n    for file in path_list:\n        if os.path.isfile(file):\n            print(\"remove \", file)\n            os.remove(file)\n            # os.system(\"rm -f \" + file)\n        elif os.path.isdir(file):\n            shutil.rmtree(file, ignore_errors=True)\n            print(\"rm tree \", file)", ""]}
{"filename": "tests/test_api.py", "chunked_list": ["from itertools import cycle\n\nimport pytest\nfrom fastapi import HTTPException\n\nfrom openai_forward.forwarding.openai import OpenaiForwarding\n\n\n@pytest.fixture(scope=\"module\")\ndef openai() -> OpenaiForwarding:\n    return OpenaiForwarding(\"https://api.openai-forward.com\", \"/\")", "@pytest.fixture(scope=\"module\")\ndef openai() -> OpenaiForwarding:\n    return OpenaiForwarding(\"https://api.openai-forward.com\", \"/\")\n\n\nclass TestOpenai:\n    @staticmethod\n    def teardown_method():\n        OpenaiForwarding.IP_BLACKLIST = []\n        OpenaiForwarding.IP_WHITELIST = []\n        OpenaiForwarding._default_api_key_list = []\n\n    def test_env(self, openai: OpenaiForwarding):\n        from openai_forward.forwarding.settings import (\n            LOG_CHAT,\n            OPENAI_BASE_URL,\n            OPENAI_ROUTE_PREFIX,\n        )\n\n        assert LOG_CHAT is False\n        assert OPENAI_BASE_URL == [\"https://api.openai.com\"]\n        assert OPENAI_ROUTE_PREFIX == [\"/\"]\n\n    def test_api_keys(self, openai: OpenaiForwarding):\n        assert openai._default_api_key_list == []\n        openai._default_api_key_list = [\"a\", \"b\"]\n        openai._cycle_api_key = cycle(openai._default_api_key_list)\n        assert next(openai._cycle_api_key) == \"a\"\n        assert next(openai._cycle_api_key) == \"b\"\n        assert next(openai._cycle_api_key) == \"a\"\n        assert next(openai._cycle_api_key) == \"b\"\n        assert next(openai._cycle_api_key) == \"a\"\n\n    def test_validate_ip(self, openai: OpenaiForwarding):\n        from openai_forward.forwarding.settings import IP_BLACKLIST, IP_WHITELIST\n\n        ip1 = \"1.1.1.1\"\n        ip2 = \"2.2.2.2\"\n        IP_WHITELIST.append(ip1)\n        with pytest.raises(HTTPException):\n            openai.validate_request_host(ip2)\n        IP_WHITELIST.clear()\n        IP_BLACKLIST.append(ip1)\n        with pytest.raises(HTTPException):\n            openai.validate_request_host(ip1)", ""]}
{"filename": "tests/test_env.py", "chunked_list": ["import importlib\nimport os\nimport time\n\nimport pytest\nfrom dotenv import load_dotenv\n\nimport openai_forward\n\n\nclass TestEnv:\n    with open(\".env\", \"r\", encoding=\"utf-8\") as f:\n        defualt_env = f.read()\n\n    @classmethod\n    def setup_class(cls):\n        env = \"\"\"\\\nLOG_CHAT=true\nOPENAI_BASE_URL=https://api.openai.com\nOPENAI_API_KEY=key1,key2\nOPENAI_ROUTE_PREFIX=\nFORWARD_KEY=ps1,ps2,ps3\nIP_WHITELIST=\nIP_BLACKLIST=\n\"\"\"\n        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n            f.write(env)\n            time.sleep(0.1)\n\n        load_dotenv(override=True)\n        importlib.reload(openai_forward.forwarding.openai)\n        importlib.reload(openai_forward.forwarding.settings)\n        cls.aibase = openai_forward.forwarding.openai.OpenaiForwarding(\n            'https://api.openai.com', '/'\n        )\n\n    @classmethod\n    def teardown_class(cls):\n        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n            f.write(cls.defualt_env)\n\n    def test_env1(self):\n        from openai_forward.forwarding.settings import FWD_KEY, OPENAI_API_KEY\n\n        assert OPENAI_API_KEY == [\"key1\", \"key2\"]\n        assert FWD_KEY == [\"ps1\", \"ps2\", \"ps3\"]\n        assert self.aibase._no_auth_mode is False", "\n\nclass TestEnv:\n    with open(\".env\", \"r\", encoding=\"utf-8\") as f:\n        defualt_env = f.read()\n\n    @classmethod\n    def setup_class(cls):\n        env = \"\"\"\\\nLOG_CHAT=true\nOPENAI_BASE_URL=https://api.openai.com\nOPENAI_API_KEY=key1,key2\nOPENAI_ROUTE_PREFIX=\nFORWARD_KEY=ps1,ps2,ps3\nIP_WHITELIST=\nIP_BLACKLIST=\n\"\"\"\n        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n            f.write(env)\n            time.sleep(0.1)\n\n        load_dotenv(override=True)\n        importlib.reload(openai_forward.forwarding.openai)\n        importlib.reload(openai_forward.forwarding.settings)\n        cls.aibase = openai_forward.forwarding.openai.OpenaiForwarding(\n            'https://api.openai.com', '/'\n        )\n\n    @classmethod\n    def teardown_class(cls):\n        with open(\".env\", \"w\", encoding=\"utf-8\") as f:\n            f.write(cls.defualt_env)\n\n    def test_env1(self):\n        from openai_forward.forwarding.settings import FWD_KEY, OPENAI_API_KEY\n\n        assert OPENAI_API_KEY == [\"key1\", \"key2\"]\n        assert FWD_KEY == [\"ps1\", \"ps2\", \"ps3\"]\n        assert self.aibase._no_auth_mode is False", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n"]}
