{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\n\nsetup(\n    name='uss',\n    version='0.0.4',\n    description='Universal source separation (USS) with weakly labelled data.',\n    author='Qiuqiang Kong',\n    author_email='qiuqiangkong@gmail.com',\n    license='Apache2.0',", "    author_email='qiuqiangkong@gmail.com',\n    license='Apache2.0',\n    packages=find_packages(),\n    url=\"https://github.com/bytedance/uss\",\n    include_package_data=True,\n    install_requires=[\n        \"torch>=2.0.0\",\n        \"lightning>=2.0.0\",\n        \"panns_inference>=0.1.0\",\n        \"transformers\",", "        \"panns_inference>=0.1.0\",\n        \"transformers\",\n        \"h5py\",\n        \"librosa>=0.10.0.post2\",\n        \"pandas\",\n        \"tensorboard\",\n        \"einops\",\n    ],\n    python_requires='>=3.8',\n    entry_points={", "    python_requires='>=3.8',\n    entry_points={\n        'console_scripts': ['uss=uss.uss_inference:main'],\n    },\n)\n"]}
{"filename": "panns/utilities.py", "chunked_list": ["import os\nimport logging\nimport h5py\nimport soundfile\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats \nimport datetime\nimport pickle", "import datetime\nimport pickle\n\n\ndef create_folder(fd):\n    if not os.path.exists(fd):\n        os.makedirs(fd)\n        \n        \ndef get_filename(path):\n    path = os.path.realpath(path)\n    na_ext = path.split('/')[-1]\n    na = os.path.splitext(na_ext)[0]\n    return na", "        \ndef get_filename(path):\n    path = os.path.realpath(path)\n    na_ext = path.split('/')[-1]\n    na = os.path.splitext(na_ext)[0]\n    return na\n\n\ndef get_sub_filepaths(folder):\n    paths = []\n    for root, dirs, files in os.walk(folder):\n        for name in files:\n            path = os.path.join(root, name)\n            paths.append(path)\n    return paths", "def get_sub_filepaths(folder):\n    paths = []\n    for root, dirs, files in os.walk(folder):\n        for name in files:\n            path = os.path.join(root, name)\n            paths.append(path)\n    return paths\n    \n    \ndef create_logging(log_dir, filemode):\n    create_folder(log_dir)\n    i1 = 0\n\n    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n        i1 += 1\n        \n    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n        datefmt='%a, %d %b %Y %H:%M:%S',\n        filename=log_path,\n        filemode=filemode)\n\n    # Print to console\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n    console.setFormatter(formatter)\n    logging.getLogger('').addHandler(console)\n    \n    return logging", "    \ndef create_logging(log_dir, filemode):\n    create_folder(log_dir)\n    i1 = 0\n\n    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n        i1 += 1\n        \n    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n        datefmt='%a, %d %b %Y %H:%M:%S',\n        filename=log_path,\n        filemode=filemode)\n\n    # Print to console\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n    console.setFormatter(formatter)\n    logging.getLogger('').addHandler(console)\n    \n    return logging", "\n\ndef read_metadata(csv_path, classes_num, id_to_ix):\n    \"\"\"Read metadata of AudioSet from a csv file.\n    Args:\n      csv_path: str\n    Returns:\n      meta_dict: {'audio_name': (audios_num,), 'target': (audios_num, classes_num)}\n    \"\"\"\n\n    with open(csv_path, 'r') as fr:\n        lines = fr.readlines()\n        lines = lines[3:]   # Remove heads\n\n    audios_num = len(lines)\n    targets = np.zeros((audios_num, classes_num), dtype=bool)\n    audio_names = []\n \n    for n, line in enumerate(lines):\n        items = line.split(', ')\n        \"\"\"items: ['--4gqARaEJE', '0.000', '10.000', '\"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\\n']\"\"\"\n\n        audio_name = 'Y{}.wav'.format(items[0])   # Audios are started with an extra 'Y' when downloading\n        label_ids = items[3].split('\"')[1].split(',')\n\n        audio_names.append(audio_name)\n\n        # Target\n        for id in label_ids:\n            ix = id_to_ix[id]\n            targets[n, ix] = 1\n    \n    meta_dict = {'audio_name': np.array(audio_names), 'target': targets}\n    return meta_dict", "\n\ndef float32_to_int16(x):\n    assert np.max(np.abs(x)) <= 1.2\n    x = np.clip(x, -1, 1)\n    return (x * 32767.).astype(np.int16)\n\ndef int16_to_float32(x):\n    return (x / 32767.).astype(np.float32)\n    ", "    \n\ndef pad_or_truncate(x, audio_length):\n    \"\"\"Pad all audio to specific length.\"\"\"\n    if len(x) <= audio_length:\n        return np.concatenate((x, np.zeros(audio_length - len(x))), axis=0)\n    else:\n        return x[0 : audio_length]\n\n", "\n"]}
{"filename": "panns/config.py", "chunked_list": ["import numpy as np\nimport csv\n\nsample_rate = 32000\nclip_samples = sample_rate * 10     # Audio clips are 10-second\n\n# Load label\nwith open('metadata/class_labels_indices.csv', 'r') as f:\n    reader = csv.reader(f, delimiter=',')\n    lines = list(reader)", "\nlabels = []\nids = []    # Each label has a unique id such as \"/m/068hy\"\nfor i1 in range(1, len(lines)):\n    id = lines[i1][1]\n    label = lines[i1][2]\n    ids.append(id)\n    labels.append(label)\n\nclasses_num = len(labels)", "\nclasses_num = len(labels)\n\nlb_to_ix = {label : i for i, label in enumerate(labels)}\nix_to_lb = {i : label for i, label in enumerate(labels)}\n\nid_to_ix = {id : i for i, id in enumerate(ids)}\nix_to_id = {i : id for i, id in enumerate(ids)}\n\nfull_samples_per_class = np.array([", "\nfull_samples_per_class = np.array([\n        937432,  16344,   7822,  10271,   2043,  14420,    733,   1511,\n         1258,    424,   1751,    704,    369,    590,   1063,   1375,\n         5026,    743,    853,   1648,    714,   1497,   1251,   2139,\n         1093,    133,    224,  39469,   6423,    407,   1559,   4546,\n         6826,   7464,   2468,    549,   4063,    334,    587,    238,\n         1766,    691,    114,   2153,    236,    209,    421,    740,\n          269,    959,    137,   4192,    485,   1515,    655,    274,\n           69,    157,   1128,    807,   1022,    346,     98,    680,", "          269,    959,    137,   4192,    485,   1515,    655,    274,\n           69,    157,   1128,    807,   1022,    346,     98,    680,\n          890,    352,   4169,   2061,   1753,   9883,   1339,    708,\n        37857,  18504,  12864,   2475,   2182,    757,   3624,    677,\n         1683,   3583,    444,   1780,   2364,    409,   4060,   3097,\n         3143,    502,    723,    600,    230,    852,   1498,   1865,\n         1879,   2429,   5498,   5430,   2139,   1761,   1051,    831,\n         2401,   2258,   1672,   1711,    987,    646,    794,  25061,\n         5792,   4256,     96,   8126,   2740,    752,    513,    554,\n          106,    254,   1592,    556,    331,    615,   2841,    737,", "         5792,   4256,     96,   8126,   2740,    752,    513,    554,\n          106,    254,   1592,    556,    331,    615,   2841,    737,\n          265,   1349,    358,   1731,   1115,    295,   1070,    972,\n          174, 937780, 112337,  42509,  49200,  11415,   6092,  13851,\n         2665,   1678,  13344,   2329,   1415,   2244,   1099,   5024,\n         9872,  10948,   4409,   2732,   1211,   1289,   4807,   5136,\n         1867,  16134,  14519,   3086,  19261,   6499,   4273,   2790,\n         8820,   1228,   1575,   4420,   3685,   2019,    664,    324,\n          513,    411,    436,   2997,   5162,   3806,   1389,    899,\n         8088,   7004,   1105,   3633,   2621,   9753,   1082,  26854,", "          513,    411,    436,   2997,   5162,   3806,   1389,    899,\n         8088,   7004,   1105,   3633,   2621,   9753,   1082,  26854,\n         3415,   4991,   2129,   5546,   4489,   2850,   1977,   1908,\n         1719,   1106,   1049,    152,    136,    802,    488,    592,\n         2081,   2712,   1665,   1128,    250,    544,    789,   2715,\n         8063,   7056,   2267,   8034,   6092,   3815,   1833,   3277,\n         8813,   2111,   4662,   2678,   2954,   5227,   1472,   2591,\n         3714,   1974,   1795,   4680,   3751,   6585,   2109,  36617,\n         6083,  16264,  17351,   3449,   5034,   3931,   2599,   4134,\n         3892,   2334,   2211,   4516,   2766,   2862,   3422,   1788,", "         6083,  16264,  17351,   3449,   5034,   3931,   2599,   4134,\n         3892,   2334,   2211,   4516,   2766,   2862,   3422,   1788,\n         2544,   2403,   2892,   4042,   3460,   1516,   1972,   1563,\n         1579,   2776,   1647,   4535,   3921,   1261,   6074,   2922,\n         3068,   1948,   4407,    712,   1294,   1019,   1572,   3764,\n         5218,    975,   1539,   6376,   1606,   6091,   1138,   1169,\n         7925,   3136,   1108,   2677,   2680,   1383,   3144,   2653,\n         1986,   1800,   1308,   1344, 122231,  12977,   2552,   2678,\n         7824,    768,   8587,  39503,   3474,    661,    430,    193,\n         1405,   1442,   3588,   6280,  10515,    785,    710,    305,", "         7824,    768,   8587,  39503,   3474,    661,    430,    193,\n         1405,   1442,   3588,   6280,  10515,    785,    710,    305,\n          206,   4990,   5329,   3398,   1771,   3022,   6907,   1523,\n         8588,  12203,    666,   2113,   7916,    434,   1636,   5185,\n         1062,    664,    952,   3490,   2811,   2749,   2848,  15555,\n          363,    117,   1494,   1647,   5886,   4021,    633,   1013,\n         5951,  11343,   2324,    243,    372,    943,    734,    242,\n         3161,    122,    127,    201,   1654,    768,    134,   1467,\n          642,   1148,   2156,   1368,   1176,    302,   1909,     61,\n          223,   1812,    287,    422,    311,    228,    748,    230,", "          642,   1148,   2156,   1368,   1176,    302,   1909,     61,\n          223,   1812,    287,    422,    311,    228,    748,    230,\n         1876,    539,   1814,    737,    689,   1140,    591,    943,\n          353,    289,    198,    490,   7938,   1841,    850,    457,\n        814,    146,    551,    728,   1627,    620,    648,   1621,\n         2731,    535,     88,   1736,    736,    328,    293,   3170,\n          344,    384,   7640,    433,    215,    715,    626,    128,\n         3059,   1833,   2069,   3732,   1640,   1508,    836,    567,\n         2837,   1151,   2068,    695,   1494,   3173,    364,     88,\n          188,    740,    677,    273,   1533,    821,   1091,    293,", "         2837,   1151,   2068,    695,   1494,   3173,    364,     88,\n          188,    740,    677,    273,   1533,    821,   1091,    293,\n          647,    318,   1202,    328,    532,   2847,    526,    721,\n          370,    258,    956,   1269,   1641,    339,   1322,   4485,\n          286,   1874,    277,    757,   1393,   1330,    380,    146,\n          377,    394,    318,    339,   1477,   1886,    101,   1435,\n          284,   1425,    686,    621,    221,    117,     87,   1340,\n          201,   1243,   1222,    651,   1899,    421,    712,   1016,\n         1279,    124,    351,    258,   7043,    368,    666,    162,\n         7664,    137,  70159,  26179,   6321,  32236,  33320,    771,", "         1279,    124,    351,    258,   7043,    368,    666,    162,\n         7664,    137,  70159,  26179,   6321,  32236,  33320,    771,\n         1169,    269,   1103,    444,    364,   2710,    121,    751,\n         1609,    855,   1141,   2287,   1940,   3943,    289])"]}
{"filename": "panns/dataset.py", "chunked_list": ["import numpy as np\nimport argparse\nimport csv\nimport os\nimport glob\nimport datetime\nimport time\nimport logging\nimport h5py\nimport librosa", "import h5py\nimport librosa\n\nfrom panns.utilities import (create_folder, get_filename, create_logging, \n    float32_to_int16, pad_or_truncate, read_metadata)\nfrom panns import config\n\n\ndef pack_waveforms_to_hdf5(args):\n    \"\"\"Pack waveform and target of several audio clips to a single hdf5 file. \n    This can speed up loading and training.\n    \"\"\"\n\n    # Arguments & parameters\n    audios_dir = args.audios_dir\n    csv_path = args.csv_path\n    waveforms_hdf5_path = args.waveforms_hdf5_path\n    mini_data = args.mini_data\n\n    clip_samples = config.clip_samples\n    classes_num = config.classes_num\n    sample_rate = config.sample_rate\n    id_to_ix = config.id_to_ix\n\n    # Paths\n    if mini_data:\n        prefix = 'mini_'\n        waveforms_hdf5_path += '.mini'\n    else:\n        prefix = ''\n\n    create_folder(os.path.dirname(waveforms_hdf5_path))\n\n    logs_dir = '_logs/pack_waveforms_to_hdf5/{}{}'.format(prefix, get_filename(csv_path))\n    create_folder(logs_dir)\n    create_logging(logs_dir, filemode='w')\n    logging.info('Write logs to {}'.format(logs_dir))\n    \n    # Read csv file\n    meta_dict = read_metadata(csv_path, classes_num, id_to_ix)\n\n    if mini_data:\n        mini_num = 10\n        for key in meta_dict.keys():\n            meta_dict[key] = meta_dict[key][0 : mini_num]\n\n    audios_num = len(meta_dict['audio_name'])\n\n    # Pack waveform to hdf5\n    total_time = time.time()\n\n    with h5py.File(waveforms_hdf5_path, 'w') as hf:\n        hf.create_dataset('audio_name', shape=((audios_num,)), dtype='S20')\n        hf.create_dataset('waveform', shape=((audios_num, clip_samples)), dtype=np.int16)\n        hf.create_dataset('target', shape=((audios_num, classes_num)), dtype=bool)\n        hf.attrs.create('sample_rate', data=sample_rate, dtype=np.int32)\n\n        # Pack waveform & target of several audio clips to a single hdf5 file\n        for n in range(audios_num):\n            audio_path = os.path.join(audios_dir, meta_dict['audio_name'][n])\n\n            if os.path.isfile(audio_path):\n                logging.info('{} {}'.format(n, audio_path))\n                (audio, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n                audio = pad_or_truncate(audio, clip_samples)\n\n                hf['audio_name'][n] = meta_dict['audio_name'][n].encode()\n                hf['waveform'][n] = float32_to_int16(audio)\n                hf['target'][n] = meta_dict['target'][n]\n            else:\n                logging.info('{} File does not exist! {}'.format(n, audio_path))\n\n    logging.info('Write to {}'.format(waveforms_hdf5_path))\n    logging.info('Pack hdf5 time: {:.3f}'.format(time.time() - total_time))", "def pack_waveforms_to_hdf5(args):\n    \"\"\"Pack waveform and target of several audio clips to a single hdf5 file. \n    This can speed up loading and training.\n    \"\"\"\n\n    # Arguments & parameters\n    audios_dir = args.audios_dir\n    csv_path = args.csv_path\n    waveforms_hdf5_path = args.waveforms_hdf5_path\n    mini_data = args.mini_data\n\n    clip_samples = config.clip_samples\n    classes_num = config.classes_num\n    sample_rate = config.sample_rate\n    id_to_ix = config.id_to_ix\n\n    # Paths\n    if mini_data:\n        prefix = 'mini_'\n        waveforms_hdf5_path += '.mini'\n    else:\n        prefix = ''\n\n    create_folder(os.path.dirname(waveforms_hdf5_path))\n\n    logs_dir = '_logs/pack_waveforms_to_hdf5/{}{}'.format(prefix, get_filename(csv_path))\n    create_folder(logs_dir)\n    create_logging(logs_dir, filemode='w')\n    logging.info('Write logs to {}'.format(logs_dir))\n    \n    # Read csv file\n    meta_dict = read_metadata(csv_path, classes_num, id_to_ix)\n\n    if mini_data:\n        mini_num = 10\n        for key in meta_dict.keys():\n            meta_dict[key] = meta_dict[key][0 : mini_num]\n\n    audios_num = len(meta_dict['audio_name'])\n\n    # Pack waveform to hdf5\n    total_time = time.time()\n\n    with h5py.File(waveforms_hdf5_path, 'w') as hf:\n        hf.create_dataset('audio_name', shape=((audios_num,)), dtype='S20')\n        hf.create_dataset('waveform', shape=((audios_num, clip_samples)), dtype=np.int16)\n        hf.create_dataset('target', shape=((audios_num, classes_num)), dtype=bool)\n        hf.attrs.create('sample_rate', data=sample_rate, dtype=np.int32)\n\n        # Pack waveform & target of several audio clips to a single hdf5 file\n        for n in range(audios_num):\n            audio_path = os.path.join(audios_dir, meta_dict['audio_name'][n])\n\n            if os.path.isfile(audio_path):\n                logging.info('{} {}'.format(n, audio_path))\n                (audio, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n                audio = pad_or_truncate(audio, clip_samples)\n\n                hf['audio_name'][n] = meta_dict['audio_name'][n].encode()\n                hf['waveform'][n] = float32_to_int16(audio)\n                hf['target'][n] = meta_dict['target'][n]\n            else:\n                logging.info('{} File does not exist! {}'.format(n, audio_path))\n\n    logging.info('Write to {}'.format(waveforms_hdf5_path))\n    logging.info('Pack hdf5 time: {:.3f}'.format(time.time() - total_time))", "          \n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='mode')\n\n    parser_pack_wavs = subparsers.add_parser('pack_waveforms_to_hdf5')\n    parser_pack_wavs.add_argument('--csv_path', type=str, required=True, help='Path of csv file containing audio info to be downloaded.')\n    parser_pack_wavs.add_argument('--audios_dir', type=str, required=True, help='Directory to save out downloaded audio.')\n    parser_pack_wavs.add_argument('--waveforms_hdf5_path', type=str, required=True, help='Path to save out packed hdf5.')\n    parser_pack_wavs.add_argument('--mini_data', action='store_true', default=False, help='Set true to only download 10 audios for debugging.')\n\n    args = parser.parse_args()\n    \n    if args.mode == 'pack_waveforms_to_hdf5':\n        pack_waveforms_to_hdf5(args)\n\n    else:\n        raise Exception('Incorrect arguments!')", ""]}
{"filename": "panns/create_indexes.py", "chunked_list": ["import numpy as np\nimport argparse\nimport csv\nimport os\nimport glob\nimport datetime\nimport time\nimport logging\nimport h5py\nimport librosa", "import h5py\nimport librosa\n\nfrom uss.config import CLASSES_NUM\n\n\ndef get_sub_filepaths(folder):\n    paths = []\n    for root, dirs, files in os.walk(folder):\n        for name in files:\n            path = os.path.join(root, name)\n            paths.append(path)\n    return paths", "\n\ndef create_indexes(args):\n    r\"\"\"Create indexes a for dataloader to read for training. When users have \n    a new task and their own data, they need to create similar indexes. The \n    indexes contain meta information of \"where to find the data for training\".\n    \"\"\"\n\n    # Arguments & parameters\n    waveforms_hdf5_path = args.waveforms_hdf5_path\n    indexes_hdf5_path = args.indexes_hdf5_path\n\n    # Paths\n    os.makedirs(os.path.dirname(indexes_hdf5_path), exist_ok=True)\n\n    with h5py.File(waveforms_hdf5_path, 'r') as hr:\n        with h5py.File(indexes_hdf5_path, 'w') as hw:\n            audios_num = len(hr['audio_name'])\n            hw.create_dataset('audio_name', data=hr['audio_name'][:], dtype='S20')\n            hw.create_dataset('target', data=hr['target'][:], dtype=bool)\n            hw.create_dataset('hdf5_path', data=[waveforms_hdf5_path.encode()] * audios_num, dtype='S200')\n            hw.create_dataset('index_in_hdf5', data=np.arange(audios_num), dtype=np.int32)\n\n    print('Write to {}'.format(indexes_hdf5_path))", "          \n\ndef combine_full_indexes(args):\n    r\"\"\"Combine all balanced and unbalanced indexes hdf5s to a single hdf5. This \n    combined indexes hdf5 is used for training with full data (~20k balanced \n    audio clips + ~1.9m unbalanced audio clips).\n    \"\"\"\n\n    # Arguments & parameters\n    indexes_hdf5s_dir = args.indexes_hdf5s_dir\n    full_indexes_hdf5_path = args.full_indexes_hdf5_path\n\n    classes_num = CLASSES_NUM\n\n    # Paths\n    paths = get_sub_filepaths(indexes_hdf5s_dir)\n    paths = [path for path in paths if (\n        'train' in path and 'full_train' not in path and 'mini' not in path)]\n\n    print('Total {} hdf5 to combine.'.format(len(paths)))\n\n    with h5py.File(full_indexes_hdf5_path, 'w') as full_hf:\n        full_hf.create_dataset(\n            name='audio_name', \n            shape=(0,), \n            maxshape=(None,), \n            dtype='S20')\n        \n        full_hf.create_dataset(\n            name='target', \n            shape=(0, classes_num), \n            maxshape=(None, classes_num), \n            dtype=bool)\n\n        full_hf.create_dataset(\n            name='hdf5_path', \n            shape=(0,), \n            maxshape=(None,), \n            dtype='S200')\n\n        full_hf.create_dataset(\n            name='index_in_hdf5', \n            shape=(0,), \n            maxshape=(None,), \n            dtype=np.int32)\n\n        for path in paths:\n            with h5py.File(path, 'r') as part_hf:\n                print(path)\n                n = len(full_hf['audio_name'][:])\n                new_n = n + len(part_hf['audio_name'][:])\n\n                full_hf['audio_name'].resize((new_n,))\n                full_hf['audio_name'][n : new_n] = part_hf['audio_name'][:]\n\n                full_hf['target'].resize((new_n, classes_num))\n                full_hf['target'][n : new_n] = part_hf['target'][:]\n\n                full_hf['hdf5_path'].resize((new_n,))\n                full_hf['hdf5_path'][n : new_n] = part_hf['hdf5_path'][:]\n\n                full_hf['index_in_hdf5'].resize((new_n,))\n                full_hf['index_in_hdf5'][n : new_n] = part_hf['index_in_hdf5'][:]\n                \n    print('Write combined full hdf5 to {}'.format(full_indexes_hdf5_path))", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='mode')\n\n    parser_create_indexes = subparsers.add_parser('create_indexes')\n    parser_create_indexes.add_argument('--waveforms_hdf5_path', type=str, required=True, help='Path of packed waveforms hdf5.')\n    parser_create_indexes.add_argument('--indexes_hdf5_path', type=str, required=True, help='Path to write out indexes hdf5.')\n\n    parser_combine_full_indexes = subparsers.add_parser('combine_full_indexes')\n    parser_combine_full_indexes.add_argument('--indexes_hdf5s_dir', type=str, required=True, help='Directory containing indexes hdf5s to be combined.')\n    parser_combine_full_indexes.add_argument('--full_indexes_hdf5_path', type=str, required=True, help='Path to write out full indexes hdf5 file.')\n\n    args = parser.parse_args()\n    \n    if args.mode == 'create_indexes':\n        create_indexes(args)\n\n    elif args.mode == 'combine_full_indexes':\n        combine_full_indexes(args)\n\n    else:\n        raise Exception('Incorrect arguments!')", ""]}
{"filename": "uss/parse_ontology.py", "chunked_list": ["import json\nfrom typing import List\n\nfrom uss.config import ID_TO_IX, IDS, ROOT_CLASS_ID_DICT\n\n\nclass Node:\n    def __init__(self, data, level):\n        r\"\"\"Sound class Node.\n\n        Args:\n            data: dict, e.g., {\n                \"id\": \"/m/0dgw9r\",\n                \"name\": \"Human sounds\",\n                \"description\": ...,\n                child_ids: [...],\n                ...}\n        \"\"\"\n\n        self.class_id = data[\"id\"]\n        self.data = data\n        self.children = []\n        self.level = level\n\n    @staticmethod\n    def search(node, class_id: str):  # -> Union[Node, None]:\n        r\"\"\"Search the node with class_id in the ontology tree.\"\"\"\n\n        if node.class_id == class_id:\n            return node\n\n        else:\n            for child in node.children:\n                result = Node.search(node=child, class_id=class_id)\n                if result:\n                    return result\n\n        return None\n\n    @staticmethod\n    def search_parent(node, class_id: str):  # -> Union[Node, None]:\n        r\"\"\"Search the parent of a node with class_id.\"\"\"\n\n        if class_id in node.data[\"child_ids\"]:\n            return node\n\n        else:\n            for child in node.children:\n                result = Node.search_parent(node=child, class_id=class_id)\n                if result:\n                    return result\n\n        return None\n\n    @staticmethod\n    def traverse(node):  # -> List[Node]:\n        r\"\"\"Traver all children of a Node including itself.\"\"\"\n\n        nodes = [node]\n\n        for child in node.children:\n            nodes.extend(Node.traverse(node=child))\n\n        return nodes", "\n\ndef get_ontology_tree(ontology_path: str) -> Node:\n    r\"\"\"Parse and build the AudioSet ontology tree.\"\"\"\n\n    with open(ontology_path) as f:\n        data_list = json.load(f)    # len: 632\n\n    root_class_ids = list(ROOT_CLASS_ID_DICT.keys())\n\n    data = {\n        \"id\": \"root\",\n        \"name\": \"root\",\n        \"child_ids\": root_class_ids,\n    }\n\n    root = Node(data=data, level=0)\n\n    for data in data_list:\n        # E.g., data: {\"id\": \"/m/0dgw9r\", \"name\": \"Human sounds\",\n        # \"description\": ..., child_ids: [...]}\n\n        father = Node.search_parent(node=root, class_id=data[\"id\"])\n\n        child = Node(data=data, level=father.level + 1)\n\n        father.children.append(child)\n\n    return root", "\n\ndef get_subclass_indexes(root: Node, id: str) -> List[int]:\n    r\"\"\"Get class indexes of all children of a node with id=id.\n\n    Args:\n        root: Node\n        id: str\n\n    Returns:\n        index_list: list of int\n    \"\"\"\n\n    node = root.search(id=id)\n    nodes, layers = root.traverse(node)\n\n    id_list = []\n    for node in nodes:\n        id_list.append(node.data[\"id\"])\n\n    index_list = []\n    for id in id_list:\n        if id in IDS:\n            index_list.append(ID_TO_IX[id])\n\n    return index_list", "\n\nif __name__ == \"__main__\":\n\n    get_ontology_tree(ontology_path=\"./metadata/ontology.json\")\n"]}
{"filename": "uss/losses.py", "chunked_list": ["from typing import Callable, Dict\n\nimport torch\nimport torch.nn as nn\nfrom torchlibrosa.stft import STFT\n\nfrom uss.models.base import Base\n\n\ndef l1(output: torch. Tensor, target: torch.Tensor) -> torch.float:\n    r\"\"\"L1 distance between the output and target.\"\"\"\n\n    return torch.mean(torch.abs(output - target))", "\ndef l1(output: torch. Tensor, target: torch.Tensor) -> torch.float:\n    r\"\"\"L1 distance between the output and target.\"\"\"\n\n    return torch.mean(torch.abs(output - target))\n\n\ndef l1_wav(output_dict: Dict, target_dict: Dict) -> torch.float:\n    r\"\"\"L1 distance between the output waveform and target waveform.\n\n    Args:\n        output_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n        target_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n\n    Returns:\n        loss: torch.float\n    \"\"\"\n\n    return l1(output_dict[\"segment\"], target_dict[\"segment\"])", "\n\nclass L1_Wav_L1_Sp(nn.Module, Base):\n    def __init__(self) -> None:\n        r\"\"\"Waveform domain L1 and spectrogram domain L1 losses.\"\"\"\n\n        super(L1_Wav_L1_Sp, self).__init__()\n\n        self.window_size = 2048\n        hop_size = 320\n        center = True\n        pad_mode = \"reflect\"\n        window = \"hann\"\n\n        self.stft = STFT(\n            n_fft=self.window_size,\n            hop_length=hop_size,\n            win_length=self.window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True,\n        )\n\n    def __call__(\n            self, output_dict, target_dict) -> torch.float:\n        r\"\"\"L1 loss in the time-domain and in the spectrogram.\n\n        Args:\n            output_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n            target_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n\n        Returns:\n            loss: torch.float\n        \"\"\"\n\n        # L1 loss in the time-domain\n        wav_loss = l1_wav(output_dict, target_dict)\n\n        # L1 loss on the spectrogram\n        sp_loss = l1(\n            self.wav_to_spectrogram(output_dict[\"segment\"], eps=1e-8),\n            self.wav_to_spectrogram(target_dict[\"segment\"], eps=1e-8),\n        )\n\n        # Total loss\n        return wav_loss + sp_loss", "\n\ndef get_loss_function(loss_type: str) -> Callable:\n    r\"\"\"Get loss function.\"\"\"\n\n    if loss_type == \"l1_wav\":\n        return l1_wav\n\n    elif loss_type == \"l1_wav_l1_sp\":\n        return L1_Wav_L1_Sp()\n\n    else:\n        raise NotImplementedError(\"Error!\")", ""]}
{"filename": "uss/inference.py", "chunked_list": ["import argparse\nimport os\nimport pickle\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport librosa\nimport lightning.pytorch as pl", "import librosa\nimport lightning.pytorch as pl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport soundfile\nimport torch\nimport torch.nn as nn\n\nfrom uss.config import (ID_TO_IX, IX_TO_LB, LB_TO_IX, csv_paths_dict,\n                        panns_paths_dict)", "from uss.config import (ID_TO_IX, IX_TO_LB, LB_TO_IX, csv_paths_dict,\n                        panns_paths_dict)\nfrom uss.models.pl_modules import LitSeparation, get_model_class\nfrom uss.models.query_nets import initialize_query_net\nfrom uss.parse_ontology import Node, get_ontology_tree\nfrom uss.utils import (get_audioset632_id_to_lb, get_path,\n                       load_pretrained_panns, parse_yaml, remove_silence,\n                       repeat_to_length)\n\n\ndef separate(args) -> None:\n    r\"\"\"Do separation for active sound classes.\"\"\"\n\n    # Arguments & parameters\n    audio_path = args.audio_path\n    levels = args.levels\n    class_ids = args.class_ids\n    queries_dir = args.queries_dir\n    query_emb_path = args.query_emb_path\n    config_yaml = args.config_yaml\n    checkpoint_path = args.checkpoint_path\n    output_dir = args.output_dir\n\n    non_sil_threshold = 1e-6\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ontology_path = get_path(csv_paths_dict[\"ontology.csv\"])\n\n    configs = parse_yaml(config_yaml)\n    sample_rate = configs[\"data\"][\"sample_rate\"]\n    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n    segment_samples = int(sample_rate * segment_seconds)\n\n    print(\"Using {}.\".format(device))\n\n    # Create directory\n    if not output_dir:\n        output_dir = os.path.join(\n            \"separated_results\",\n            Path(audio_path).stem)\n\n    # Load pretrained universal source separation model\n    print(\"Loading model ...\")\n\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n    pl_model = load_ss_model(\n        configs=configs,\n        checkpoint_path=checkpoint_path,\n    ).to(device)\n\n    # Load audio\n    audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)\n\n    # Load pretrained audio tagging model\n    at_model_type = \"Cnn14\"\n\n    at_model = load_pretrained_panns(\n        model_type=at_model_type,\n        checkpoint_path=get_path(panns_paths_dict[at_model_type]),\n        freeze=True,\n    ).to(device)\n\n    flag_sum = sum([\n        len(levels) > 0,\n        len(class_ids) > 0,\n        len(queries_dir) > 0,\n        len(query_emb_path) > 0,\n    ])\n\n    assert flag_sum in [0, 1], \"Please use only `levels` or `class_ids` or \\\n        `queries_dir` or `query_emb_path` arguments.\"\n\n    if flag_sum == 0:\n        levels = [1, 2, 3]\n\n    print(\"Separating ...\")\n\n    # Separate by hierarchy\n    if len(levels) > 0:\n        separate_by_hierarchy(\n            audio=audio,\n            sample_rate=sample_rate,\n            segment_samples=segment_samples,\n            at_model=at_model,\n            pl_model=pl_model,\n            device=device,\n            levels=levels,\n            ontology_path=ontology_path,\n            non_sil_threshold=non_sil_threshold,\n            output_dir=output_dir\n        )\n\n    # Separate by class IDs\n    elif len(class_ids) > 0:\n        separate_by_class_ids(\n            audio=audio,\n            sample_rate=sample_rate,\n            segment_samples=segment_samples,\n            at_model=at_model,\n            pl_model=pl_model,\n            device=device,\n            class_ids=class_ids,\n            output_dir=output_dir\n        )\n\n    # Calculate query embedding and do separation\n    elif len(queries_dir) > 0:\n\n        print(\"Calculate query condition ...\")\n        query_time = time.time()\n\n        query_condition = calculate_query_emb(\n            queries_dir=queries_dir,\n            pl_model=pl_model,\n            sample_rate=sample_rate,\n            remove_sil=True,\n            segment_samples=segment_samples,\n        )\n\n        print(\"Time: {:.3f} s\".format(time.time() - query_time))\n\n        pickle_path = os.path.join(\n            \"./query_conditions\",\n            \"config={}\".format(\n                Path(config_yaml).stem),\n            \"{}.pkl\".format(\n                Path(queries_dir).stem))\n\n        os.makedirs(os.path.dirname(pickle_path), exist_ok=True)\n\n        pickle.dump(query_condition, open(pickle_path, 'wb'))\n        print(\"Write query condition to {}\".format(pickle_path))\n\n        output_path = os.path.join(\n            output_dir, \"query={}.wav\".format(\n                Path(queries_dir).stem))\n\n        separate_by_query_condition(\n            audio=audio,\n            segment_samples=segment_samples,\n            sample_rate=sample_rate,\n            query_condition=query_condition,\n            pl_model=pl_model,\n            output_path=output_path,\n        )\n\n    # Load pre-calculated query embedding and do separation\n    elif Path(query_emb_path).is_file():\n\n        query_condition = pickle.load(open(query_emb_path, 'rb'))\n\n        output_path = os.path.join(\n            output_dir, \"query={}.wav\".format(\n                Path('111').stem))\n\n        separate_by_query_condition(\n            audio=audio,\n            segment_samples=segment_samples,\n            sample_rate=sample_rate,\n            query_condition=query_condition,\n            pl_model=pl_model,\n            output_path=output_path,\n        )", "\n\ndef separate(args) -> None:\n    r\"\"\"Do separation for active sound classes.\"\"\"\n\n    # Arguments & parameters\n    audio_path = args.audio_path\n    levels = args.levels\n    class_ids = args.class_ids\n    queries_dir = args.queries_dir\n    query_emb_path = args.query_emb_path\n    config_yaml = args.config_yaml\n    checkpoint_path = args.checkpoint_path\n    output_dir = args.output_dir\n\n    non_sil_threshold = 1e-6\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ontology_path = get_path(csv_paths_dict[\"ontology.csv\"])\n\n    configs = parse_yaml(config_yaml)\n    sample_rate = configs[\"data\"][\"sample_rate\"]\n    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n    segment_samples = int(sample_rate * segment_seconds)\n\n    print(\"Using {}.\".format(device))\n\n    # Create directory\n    if not output_dir:\n        output_dir = os.path.join(\n            \"separated_results\",\n            Path(audio_path).stem)\n\n    # Load pretrained universal source separation model\n    print(\"Loading model ...\")\n\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n    pl_model = load_ss_model(\n        configs=configs,\n        checkpoint_path=checkpoint_path,\n    ).to(device)\n\n    # Load audio\n    audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)\n\n    # Load pretrained audio tagging model\n    at_model_type = \"Cnn14\"\n\n    at_model = load_pretrained_panns(\n        model_type=at_model_type,\n        checkpoint_path=get_path(panns_paths_dict[at_model_type]),\n        freeze=True,\n    ).to(device)\n\n    flag_sum = sum([\n        len(levels) > 0,\n        len(class_ids) > 0,\n        len(queries_dir) > 0,\n        len(query_emb_path) > 0,\n    ])\n\n    assert flag_sum in [0, 1], \"Please use only `levels` or `class_ids` or \\\n        `queries_dir` or `query_emb_path` arguments.\"\n\n    if flag_sum == 0:\n        levels = [1, 2, 3]\n\n    print(\"Separating ...\")\n\n    # Separate by hierarchy\n    if len(levels) > 0:\n        separate_by_hierarchy(\n            audio=audio,\n            sample_rate=sample_rate,\n            segment_samples=segment_samples,\n            at_model=at_model,\n            pl_model=pl_model,\n            device=device,\n            levels=levels,\n            ontology_path=ontology_path,\n            non_sil_threshold=non_sil_threshold,\n            output_dir=output_dir\n        )\n\n    # Separate by class IDs\n    elif len(class_ids) > 0:\n        separate_by_class_ids(\n            audio=audio,\n            sample_rate=sample_rate,\n            segment_samples=segment_samples,\n            at_model=at_model,\n            pl_model=pl_model,\n            device=device,\n            class_ids=class_ids,\n            output_dir=output_dir\n        )\n\n    # Calculate query embedding and do separation\n    elif len(queries_dir) > 0:\n\n        print(\"Calculate query condition ...\")\n        query_time = time.time()\n\n        query_condition = calculate_query_emb(\n            queries_dir=queries_dir,\n            pl_model=pl_model,\n            sample_rate=sample_rate,\n            remove_sil=True,\n            segment_samples=segment_samples,\n        )\n\n        print(\"Time: {:.3f} s\".format(time.time() - query_time))\n\n        pickle_path = os.path.join(\n            \"./query_conditions\",\n            \"config={}\".format(\n                Path(config_yaml).stem),\n            \"{}.pkl\".format(\n                Path(queries_dir).stem))\n\n        os.makedirs(os.path.dirname(pickle_path), exist_ok=True)\n\n        pickle.dump(query_condition, open(pickle_path, 'wb'))\n        print(\"Write query condition to {}\".format(pickle_path))\n\n        output_path = os.path.join(\n            output_dir, \"query={}.wav\".format(\n                Path(queries_dir).stem))\n\n        separate_by_query_condition(\n            audio=audio,\n            segment_samples=segment_samples,\n            sample_rate=sample_rate,\n            query_condition=query_condition,\n            pl_model=pl_model,\n            output_path=output_path,\n        )\n\n    # Load pre-calculated query embedding and do separation\n    elif Path(query_emb_path).is_file():\n\n        query_condition = pickle.load(open(query_emb_path, 'rb'))\n\n        output_path = os.path.join(\n            output_dir, \"query={}.wav\".format(\n                Path('111').stem))\n\n        separate_by_query_condition(\n            audio=audio,\n            segment_samples=segment_samples,\n            sample_rate=sample_rate,\n            query_condition=query_condition,\n            pl_model=pl_model,\n            output_path=output_path,\n        )", "\n\ndef load_ss_model(\n    configs: Dict,\n    checkpoint_path: str,\n) -> nn.Module:\n    r\"\"\"Load trained universal source separation model.\n\n    Args:\n        configs (Dict)\n        checkpoint_path (str): path of the checkpoint to load\n        device (str): e.g., \"cpu\" | \"cuda\"\n\n    Returns:\n        pl_model: pl.LightningModule\n    \"\"\"\n\n    # Initialize query net\n    query_net = initialize_query_net(\n        configs=configs,\n    )\n\n    ss_model_type = configs[\"ss_model\"][\"model_type\"]\n    input_channels = configs[\"ss_model\"][\"input_channels\"]\n    output_channels = configs[\"ss_model\"][\"output_channels\"]\n    condition_size = configs[\"query_net\"][\"outputs_num\"]\n\n    # Initialize separation model\n    SsModel = get_model_class(model_type=ss_model_type)\n\n    ss_model = SsModel(\n        input_channels=input_channels,\n        output_channels=output_channels,\n        condition_size=condition_size,\n    )\n\n    # Load PyTorch Lightning model\n    pl_model = LitSeparation.load_from_checkpoint(\n        checkpoint_path=checkpoint_path,\n        strict=False,\n        ss_model=ss_model,\n        anchor_segment_detector=None,\n        anchor_segment_mixer=None,\n        query_net=query_net,\n        loss_function=None,\n        optimizer_type=None,\n        learning_rate=None,\n        lr_lambda_func=None,\n        map_location=\"cpu\",\n    )\n\n    return pl_model", "\n\ndef separate_by_hierarchy(\n    audio: np.ndarray,\n    sample_rate: int,\n    segment_samples: int,\n    at_model: nn.Module,\n    pl_model: pl.LightningModule,\n    device: str,\n    levels: List[int],\n    ontology_path: str,\n    non_sil_threshold: float,\n    output_dir: str,\n) -> None:\n    r\"\"\"Separate by hierarchy.\"\"\"\n\n    audioset632_id_to_lb = get_audioset632_id_to_lb(\n        ontology_path=ontology_path)\n\n    at_probs = calculate_segment_at_probs(\n        audio=audio,\n        segment_samples=segment_samples,\n        at_model=at_model,\n        device=device,\n    )\n    # at_probs: (segments_num, condition_dim)\n\n    # Parse and build AudioSet ontology tree\n    root = get_ontology_tree(ontology_path=ontology_path)\n\n    nodes = Node.traverse(root)\n\n    for level in levels:\n\n        print(\"------ Level {} ------\".format(level))\n\n        nodes_level_n = get_nodes_with_level_n(nodes=nodes, level=level)\n\n        hierarchy_at_probs = []\n\n        for node in nodes_level_n:\n\n            class_id = node.class_id\n\n            subclass_indexes = get_children_indexes(node=node)\n            # E.g., [0, 1, ..., 71]\n\n            if len(subclass_indexes) == 0:\n                continue\n\n            sep_audio = separate_by_query_conditions(\n                audio=audio,\n                segment_samples=segment_samples,\n                at_probs=at_probs,\n                subclass_indexes=subclass_indexes,\n                pl_model=pl_model,\n                device=device,\n            )\n            # sep_audio: (audio_samples,)\n\n            # Write out separated audio\n            label = audioset632_id_to_lb[class_id]\n\n            output_name = \"{}.wav\".format(label)\n\n            # if label in LB_TO_IX.keys():\n            #     output_name = \"classid={}_{}.wav\".format(LB_TO_IX[label], label)\n            # else:\n            #     output_name = \"classid=unknown_{}.wav\".format(label)\n\n            output_path = os.path.join(\n                output_dir,\n                \"level={}\".format(level),\n                output_name,\n            )\n\n            if np.max(sep_audio) > non_sil_threshold:\n                write_audio(\n                    audio=sep_audio,\n                    output_path=output_path,\n                    sample_rate=sample_rate,\n                )\n\n            hierarchy_at_prob = np.max(at_probs[:, subclass_indexes], axis=-1)\n            hierarchy_at_probs.append(hierarchy_at_prob)\n\n        hierarchy_at_probs = np.stack(hierarchy_at_probs, axis=-1)\n        plt.matshow(\n            hierarchy_at_probs.T,\n            origin=\"lower\",\n            aspect=\"auto\",\n            cmap=\"jet\")\n        plt.savefig(\"_zz_{}.pdf\".format(level))", "\n\ndef separate_by_class_ids(\n    audio: np.ndarray,\n    sample_rate: int,\n    segment_samples: int,\n    at_model: nn.Module,\n    pl_model: pl.LightningModule,\n    device: str,\n    class_ids: List[int],\n    output_dir: str,\n) -> None:\n    r\"\"\"Separate by class IDs.\"\"\"\n\n    at_probs = calculate_segment_at_probs(\n        audio=audio,\n        segment_samples=segment_samples,\n        at_model=at_model,\n        device=device,\n    )\n    # at_probs: (segments_num, condition_dim)\n\n    sep_audio = separate_by_query_conditions(\n        audio=audio,\n        segment_samples=segment_samples,\n        at_probs=at_probs,\n        subclass_indexes=class_ids,\n        pl_model=pl_model,\n        device=device,\n    )\n    # sep_audio: (audio_samples,)\n\n    # Write out separated audio\n    output_name = \";\".join(\n        [\"{}_{}\".format(class_id, IX_TO_LB[class_id]) for class_id in class_ids])\n    output_name += \".wav\"\n\n    output_path = os.path.join(\n        output_dir,\n        output_name,\n    )\n\n    write_audio(\n        audio=sep_audio,\n        output_path=output_path,\n        sample_rate=sample_rate,\n    )", "\n\ndef calculate_query_emb(\n    queries_dir: str,\n    pl_model: pl.LightningModule,\n    sample_rate: int,\n    remove_sil: bool,\n    segment_samples: int,\n    batch_size=8,\n) -> np.ndarray:\n    r\"\"\"Calculate the query embddings of audio files in a directory.\"\"\"\n\n    audio_names = sorted(os.listdir(queries_dir))\n\n    avg_query_conditions = []\n\n    # Average query conditions of all audios\n    for audio_index, audio_name in enumerate(audio_names):\n\n        print(\"{} / {}, {}\".format(audio_index, len(audio_names), audio_name))\n\n        audio_path = os.path.join(queries_dir, audio_name)\n\n        audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)\n\n        # Remove silence\n        if remove_sil:\n            audio = remove_silence(audio=audio, sample_rate=sample_rate)\n\n        audio_samples = audio.shape[0]\n\n        segments_num = int(np.ceil(audio_samples / segment_samples))\n\n        segments = []\n\n        # Get all segments\n        for segment_index in range(segments_num):\n\n            begin_sample = segment_index * segment_samples\n            end_sample = begin_sample + segment_samples\n\n            segment = audio[begin_sample: end_sample]\n            segment = repeat_to_length(\n                audio=segment, segment_samples=segment_samples)\n            segments.append(segment)\n\n        if len(segments) == 0:\n            continue\n\n        segments = np.stack(segments, axis=0)\n\n        # Calcualte query conditions in mini-batch\n        pointer = 0\n        query_conditions = []\n\n        while pointer < len(segments):\n\n            batch_segments = segments[pointer: pointer + batch_size]\n\n            query_condition = _do_query_in_minibatch(\n                batch_segments=batch_segments,\n                query_net=pl_model.query_net,\n            )\n\n            query_conditions.extend(query_condition)\n            pointer += batch_size\n\n        avg_query_condition = np.mean(query_conditions, axis=0)\n        avg_query_conditions.append(avg_query_condition)\n\n    # Average query conditions of all audio files\n    avg_query_condition = np.mean(avg_query_conditions, axis=0)\n\n    return avg_query_condition", "\n\ndef calculate_segment_at_probs(\n    audio: np.ndarray,\n    segment_samples: int,\n    at_model: nn.Module,\n    device: str,\n) -> np.ndarray:\n    r\"\"\"Split audio into short segments. Calcualte the audio tagging\n    predictions of all segments.\n\n    Args:\n        audio (np.ndarray): (audio_samples,)\n        segment_samples (int): short segment duration\n        at_model (nn.Module): pretrained audio tagging model\n        device (str): \"cpu\" | \"cuda\"\n\n    Returns:\n        at_probs (np.ndarray): audio tagging probabilities on all segments,\n            (segments_num, classes_num)\n    \"\"\"\n\n    audio_samples = audio.shape[-1]\n    pointer = 0\n    at_probs = []\n\n    while pointer < audio_samples:\n\n        segment = librosa.util.fix_length(\n            data=audio[pointer: pointer + segment_samples],\n            size=segment_samples,\n            axis=0,\n        )\n\n        segments = torch.Tensor(segment).unsqueeze(dim=0).to(device)\n        # segments: (batch_size=1, segment_samples)\n\n        with torch.no_grad():\n            at_model.eval()\n            at_prob = at_model(input=segments)[\"clipwise_output\"]\n\n        at_prob = at_prob.squeeze(dim=0).data.cpu().numpy()\n        # at_prob: (classes_num,)\n\n        at_probs.append(at_prob)\n\n        pointer += segment_samples\n\n    at_probs = np.stack(at_probs, axis=0)\n    # at_probs: (segments_num, condition_dim)\n\n    return at_probs", "\n\ndef get_nodes_with_level_n(nodes: List[Node], level: int) -> List[Node]:\n    r\"\"\"Return nodes with level=N.\"\"\"\n\n    nodes_level_n = []\n\n    for node in nodes:\n        if node.level == level:\n            nodes_level_n.append(node)\n\n    return nodes_level_n", "\n\ndef get_children_indexes(node: Node) -> List[int]:\n    r\"\"\"Get class indexes of all children of a node.\"\"\"\n\n    nodes_level_n_children = Node.traverse(node=node)\n\n    subclass_indexes = [ID_TO_IX[node.class_id]\n                        for node in nodes_level_n_children if node.class_id in ID_TO_IX]\n\n    return subclass_indexes", "\n\ndef separate_by_query_conditions(\n    audio: np.ndarray,\n    segment_samples: int,\n    at_probs: np.ndarray,\n    subclass_indexes: List[int],\n    pl_model: pl.LightningModule,\n    device: str,\n) -> np.ndarray:\n    r\"\"\"Do separation for active segments depending on the subclass_indexes.\n\n    Args:\n        audio (np.ndarray): audio clip\n        segment_samples (int): segment samples\n        at_probs (np.ndarray): predicted audio tagging probability on segments,\n            (segments_num, classes_num)\n        subclass_indexes (List[int]): all values in subclass_indexes are\n            remained to build the condition\n        pl_model (pl.LightningModule): trained universal source separation model\n        device (str), e.g., \"cpu\" | \"cuda\"\n\n    Returns:\n        sep_audio (np.ndarray): separated audio\n    \"\"\"\n\n    audio_samples = audio.shape[-1]\n    at_threshold = 0.2\n    batch_size = 8\n\n    segments_num = int(np.ceil(audio_samples / segment_samples))\n\n    active_segment_indexes = []\n    active_segments = []\n\n    # Collect active segments\n    for segment_index in range(segments_num):\n\n        max_prob = np.max(at_probs[segment_index, subclass_indexes])\n\n        if max_prob >= at_threshold:\n            # Only do separation for active segments\n\n            begin_sample = segment_index * segment_samples\n            end_sample = begin_sample + segment_samples\n\n            segment = librosa.util.fix_length(\n                data=audio[begin_sample: end_sample],\n                size=segment_samples,\n                axis=0,\n            )\n\n            active_segments.append(segment)\n            active_segment_indexes.append(segment_index)\n\n    if len(active_segments) > 0:\n        active_segments = np.stack(active_segments, axis=0)\n        active_segment_indexes = np.stack(active_segment_indexes, axis=0)\n\n    # Do separation in mini-batch\n    pointer = 0\n    active_sep_segments = []\n\n    while pointer < len(active_segments):\n\n        batch_segments = active_segments[pointer: pointer + batch_size]\n\n        batch_sep_segments = _do_sep_by_id_in_minibatch(\n            batch_segments=batch_segments,\n            subclass_indexes=subclass_indexes,\n            pl_model=pl_model,\n        )\n        active_sep_segments.extend(batch_sep_segments)\n        pointer += batch_size\n\n    # Get separated segments\n    sep_segments = np.zeros((segments_num, segment_samples))\n\n    for i in range(len(active_segment_indexes)):\n        sep_segments[active_segment_indexes[i]] = active_sep_segments[i]\n\n    sep_audio = sep_segments.flatten()[0: audio_samples]\n\n    return sep_audio", "\n\ndef separate_by_query_condition(\n    audio: np.ndarray,\n    segment_samples: int,\n    sample_rate: int,\n    query_condition: np.ndarray,\n    pl_model: pl.LightningModule,\n    output_path: str,\n    batch_size: int = 8,\n) -> np.ndarray:\n    r\"\"\"Do separation for active segments depending on the subclass_indexes.\n\n    Args:\n        audio (np.ndarray): audio clip\n        segment_samples (int): segment samples\n        at_probs (np.ndarray): predicted audio tagging probability on segments,\n            (segments_num, classes_num)\n        subclass_indexes (List[int]): all values in subclass_indexes are\n            remained to build the condition\n        pl_model (pl.LightningModule): trained universal source separation model\n        device (str), e.g., \"cpu\" | \"cuda\"\n\n    Returns:\n        sep_audio (np.ndarray): separated audio\n    \"\"\"\n\n    audio_samples = audio.shape[-1]\n\n    segments_num = int(np.ceil(audio_samples / segment_samples))\n\n    segments = []\n\n    # Collect active segments\n    for segment_index in range(segments_num):\n\n        begin_sample = segment_index * segment_samples\n        end_sample = begin_sample + segment_samples\n\n        segment = librosa.util.fix_length(\n            data=audio[begin_sample: end_sample],\n            size=segment_samples,\n            axis=0,\n        )\n\n        segments.append(segment)\n\n    segments = np.stack(segments, axis=0)\n\n    # Do separation in mini-batch\n    pointer = 0\n    sep_segments = []\n\n    while pointer < len(segments):\n\n        batch_segments = segments[pointer: pointer + batch_size]\n\n        batch_sep_segments = _do_sep_by_query_in_minibatch(\n            batch_segments=batch_segments,\n            query_condition=query_condition,\n            ss_model=pl_model.ss_model,\n        )\n        sep_segments.extend(batch_sep_segments)\n        pointer += batch_size\n\n    sep_segments = np.concatenate(sep_segments, axis=0)\n\n    sep_audio = sep_segments.flatten()[0: audio_samples]\n\n    if output_path:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        soundfile.write(\n            file=output_path,\n            data=sep_audio,\n            samplerate=sample_rate)\n        print(\"Write out separated file to {}\".format(output_path))\n\n    return sep_audio", "\n\ndef _do_sep_by_id_in_minibatch(\n    batch_segments: np.ndarray,\n    subclass_indexes: List[int],\n    pl_model: pl.LightningModule,\n) -> np.ndarray:\n    r\"\"\"Separate by class IDs in mini-batch.\n\n    Args:\n        batch_segments (np.ndarray): (batch_size, segment_samples)\n        subclass_indexes (List[int]): a list of subclasses\n        pl_model (pl.LightningModule): universal separation model\n\n    Returns:\n        batch_sep_segments (np.ndarray): separated mini-batch segments\n    \"\"\"\n\n    device = pl_model.device\n\n    batch_segments = torch.Tensor(batch_segments).to(device)\n    # shape: (batch_size, segment_samples)\n\n    with torch.no_grad():\n        pl_model.query_net.eval()\n\n        bottleneck = pl_model.query_net.forward_base(source=batch_segments)\n        # bottleneck: (batch_size, bottleneck_dim)\n\n        masked_bottleneck = torch.zeros_like(bottleneck)\n        masked_bottleneck[:,\n                          subclass_indexes] = bottleneck[:,\n                                                         subclass_indexes]\n\n        condition = pl_model.query_net.forward_adaptor(masked_bottleneck)\n        # condition: (batch_size, condition_dim)\n\n    input_dict = {\n        \"mixture\": torch.Tensor(batch_segments.unsqueeze(1)),\n        \"condition\": torch.Tensor(condition),\n    }\n\n    with torch.no_grad():\n        pl_model.ss_model.eval()\n        output_dict = pl_model.ss_model(input_dict=input_dict)\n\n    batch_sep_segments = output_dict[\"waveform\"].squeeze(\n        dim=1).data.cpu().numpy()\n    # (batch_size, segment_samples)\n\n    return batch_sep_segments", "\n\ndef _do_sep_by_query_in_minibatch(\n    batch_segments: np.ndarray,\n    query_condition: np.ndarray,\n    ss_model: nn.Module,\n) -> np.ndarray:\n    r\"\"\"Separate by query condition in mini-batch.\n\n    Args:\n        batch_segments (np.ndarray): (batch_size, segment_samples)\n        query_condition (np.ndarray): (batch_size, embedding_dim)\n        pl_model (pl.LightningModule): universal separation model\n\n    Returns:\n        batch_sep_segments (np.ndarray): separated mini-batch segments\n    \"\"\"\n\n    device = next(ss_model.parameters()).device\n\n    batch_segments = torch.Tensor(batch_segments).to(device).unsqueeze(dim=1)\n    # shape: (batch_size, 1, segment_samples)\n\n    query_condition = torch.Tensor(query_condition).to(device).unsqueeze(dim=0)\n\n    input_dict = {\n        \"mixture\": batch_segments,\n        \"condition\": query_condition,\n    }\n\n    with torch.no_grad():\n        ss_model.eval()\n        output_dict = ss_model(input_dict=input_dict)\n\n    batch_sep_segments = output_dict[\"waveform\"].squeeze(\n        dim=1).data.cpu().numpy()\n    # (batch_size, segment_samples)\n\n    return batch_sep_segments", "\n\ndef _do_query_in_minibatch(\n    batch_segments: np.ndarray,\n    query_net: nn.Module,\n) -> np.ndarray:\n    r\"\"\"Separate by mini-batch.\n\n    Args:\n        batch_segments (np.ndarray): (batch_size, segment_samples)\n        pl_model (pl.LightningModule): universal separation model\n\n    Returns:\n        batch_condition (np.ndarray): mini-batch conditions.\n    \"\"\"\n\n    device = next(query_net.parameters()).device\n\n    batch_segments = torch.Tensor(batch_segments).to(device)\n    # shape: (batch_size, segment_samples)\n\n    with torch.no_grad():\n        query_net.eval()\n\n        batch_condition = query_net.forward(source=batch_segments)[\"output\"]\n        # condition: (batch_size, condition_dim)\n\n    batch_condition = batch_condition.data.cpu().numpy()\n\n    return batch_condition", "\n\ndef write_audio(\n    audio: np.ndarray,\n    output_path: str,\n    sample_rate: int,\n):\n    r\"\"\"Write audio to disk.\n\n    Args:\n        audio (np.ndarray): audio to write out\n        output_path (str): path to write out the audio\n        sample_rate (int)\n\n    Returns:\n        None\n    \"\"\"\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    soundfile.write(file=output_path, data=audio, samplerate=sample_rate)\n    print(\"Write out to {}\".format(output_path))", "\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--audio_path\", type=str, required=True)\n    parser.add_argument(\"--levels\", nargs=\"*\", type=int, default=[])\n    parser.add_argument(\"--class_ids\", nargs=\"*\", type=int, default=[])\n    parser.add_argument(\"--queries_dir\", type=str, default=\"\")\n    parser.add_argument(\"--query_emb_path\", type=str, default=\"\")\n    parser.add_argument(\"--config_yaml\", type=str, default=\"\")\n    parser.add_argument(\"--checkpoint_path\", type=str, default=\"\")\n    parser.add_argument(\"--output_dir\", type=str)\n\n    args = parser.parse_args()\n\n    separate(args)", ""]}
{"filename": "uss/train.py", "chunked_list": ["import argparse\nimport logging\nimport os\nimport pathlib\nfrom typing import List\n\nimport lightning.pytorch as pl\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n", "from torch.utils.tensorboard import SummaryWriter\n\nfrom uss.callbacks.base import CheckpointEveryNSteps\nfrom uss.callbacks.evaluate import EvaluateCallback\nfrom uss.config import CLIP_SECONDS, FRAMES_PER_SECOND, panns_paths_dict\nfrom uss.data.anchor_segment_detectors import AnchorSegmentDetector\nfrom uss.data.anchor_segment_mixers import AnchorSegmentMixer\nfrom uss.data.datamodules import DataModule\nfrom uss.data.datasets import Dataset\nfrom uss.data.samplers import BalancedSampler", "from uss.data.datasets import Dataset\nfrom uss.data.samplers import BalancedSampler\nfrom uss.losses import get_loss_function\nfrom uss.models.pl_modules import LitSeparation, get_model_class\nfrom uss.models.query_nets import initialize_query_net\nfrom uss.optimizers.lr_schedulers import get_lr_lambda\nfrom uss.utils import (create_logging, get_path, load_pretrained_panns,\n                       parse_yaml)\n\n\ndef train(args) -> None:\n    r\"\"\"Train, evaluate, and save checkpoints.\n\n    Args:\n        workspace (str): directory of workspace\n        config_yaml (str): config yaml path\n\n    Returns:\n        None\n    \"\"\"\n\n    # Arguments & parameters\n    workspace = args.workspace\n    config_yaml = args.config_yaml\n    filename = args.filename\n\n    # GPUs number\n    devices_num = torch.cuda.device_count()\n\n    # Read config file\n    configs = parse_yaml(config_yaml)\n\n    # Configurations of pretrained sound event detection model from PANNs\n    sed_model_type = configs[\"sound_event_detection\"][\"model_type\"]\n\n    # Configuration of data to train the universal source separation system\n    clip_seconds = CLIP_SECONDS\n    frames_per_second = FRAMES_PER_SECOND\n    sample_rate = configs[\"data\"][\"sample_rate\"]\n    classes_num = configs[\"data\"][\"classes_num\"]\n    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n    anchor_segment_detect_mode = configs[\"data\"][\"anchor_segment_detect_mode\"]\n    mix_num = configs[\"data\"][\"mix_num\"]\n    match_energy = configs[\"data\"][\"augmentation\"][\"match_energy\"]\n\n    # Configuration of the universal source separation model\n    ss_model_type = configs[\"ss_model\"][\"model_type\"]\n    input_channels = configs[\"ss_model\"][\"input_channels\"]\n    output_channels = configs[\"ss_model\"][\"output_channels\"]\n    condition_size = configs[\"query_net\"][\"outputs_num\"]\n\n    # Configuration of the trainer\n    num_workers = configs[\"train\"][\"num_workers\"]\n    loss_type = configs[\"train\"][\"loss_type\"]\n    optimizer_type = configs[\"train\"][\"optimizer\"][\"optimizer_type\"]\n    learning_rate = float(configs[\"train\"][\"optimizer\"][\"learning_rate\"])\n    lr_lambda_type = configs[\"train\"][\"optimizer\"][\"lr_lambda_type\"]\n    warm_up_steps = configs[\"train\"][\"optimizer\"][\"warm_up_steps\"]\n    reduce_lr_steps = configs[\"train\"][\"optimizer\"][\"reduce_lr_steps\"]\n    save_step_frequency = configs[\"train\"][\"save_step_frequency\"]\n    evaluate_step_frequency = configs[\"train\"][\"evaluate_step_frequency\"]\n    resume_checkpoint_path = configs[\"train\"][\"resume_checkpoint_path\"]\n    if resume_checkpoint_path == \"\":\n        resume_checkpoint_path = None\n\n    # Configuration of the evaluation\n    balanced_train_eval_dir = os.path.join(\n        workspace, configs[\"evaluate\"][\"balanced_train_eval_dir\"])\n    test_eval_dir = os.path.join(\n        workspace, configs[\"evaluate\"][\"test_eval_dir\"])\n    max_eval_per_class = configs[\"evaluate\"][\"max_eval_per_class\"]\n\n    # Get directories and paths\n    checkpoints_dir, logs_dir, tf_logs_dir, statistics_path = get_dirs(\n        workspace, filename, config_yaml, devices_num,\n    )\n\n    # Create a PyTorch Lightning datamodule\n    datamodule = get_datamodule(\n        workspace=workspace,\n        config_yaml=config_yaml,\n        num_workers=num_workers,\n        devices_num=devices_num,\n    )\n\n    sed_model = load_pretrained_panns(\n        model_type=sed_model_type,\n        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),\n        freeze=True,\n    )\n\n    # Initialize query net\n    query_net = initialize_query_net(\n        configs=configs,\n    )\n\n    # Initialize separation model\n    SsModel = get_model_class(model_type=ss_model_type)\n\n    ss_model = SsModel(\n        input_channels=input_channels,\n        output_channels=output_channels,\n        condition_size=condition_size,\n    )\n\n    # Loss function\n    loss_function = get_loss_function(loss_type=loss_type)\n\n    # Anchor segment detector\n    anchor_segment_detector = AnchorSegmentDetector(\n        sed_model=sed_model,\n        clip_seconds=clip_seconds,\n        segment_seconds=segment_seconds,\n        frames_per_second=frames_per_second,\n        sample_rate=sample_rate,\n        detect_mode=anchor_segment_detect_mode,\n    )\n\n    # Anchor segment mixer\n    anchor_segment_mixer = AnchorSegmentMixer(\n        mix_num=mix_num,\n        match_energy=match_energy,\n    )\n\n    # Learning rate scaler\n    lr_lambda_func = get_lr_lambda(\n        lr_lambda_type=lr_lambda_type,\n        warm_up_steps=warm_up_steps,\n        reduce_lr_steps=reduce_lr_steps,\n    )\n\n    # PyTorch Lightning model\n    pl_model = LitSeparation(\n        ss_model=ss_model,\n        anchor_segment_detector=anchor_segment_detector,\n        anchor_segment_mixer=anchor_segment_mixer,\n        query_net=query_net,\n        loss_function=loss_function,\n        optimizer_type=optimizer_type,\n        learning_rate=learning_rate,\n        lr_lambda_func=lr_lambda_func,\n    )\n\n    # Checkpoint\n    checkpoint_every_n_steps = CheckpointEveryNSteps(\n        checkpoints_dir=checkpoints_dir,\n        save_step_frequency=save_step_frequency,\n    )\n\n    # Summary writer\n    summary_writer = SummaryWriter(log_dir=tf_logs_dir)\n\n    # Evaluation callback\n    evaluate_callback = EvaluateCallback(\n        pl_model=pl_model,\n        balanced_train_eval_dir=balanced_train_eval_dir,\n        test_eval_dir=test_eval_dir,\n        classes_num=classes_num,\n        max_eval_per_class=max_eval_per_class,\n        evaluate_step_frequency=evaluate_step_frequency,\n        summary_writer=summary_writer,\n        statistics_path=statistics_path,\n    )\n\n    # All callbacks\n    callbacks = [checkpoint_every_n_steps, evaluate_callback]\n\n    trainer = pl.Trainer(\n        accelerator=\"auto\",\n        devices=\"auto\",\n        num_nodes=1,\n        precision=\"32-true\",\n        logger=None,\n        callbacks=callbacks,\n        fast_dev_run=False,\n        max_epochs=-1,\n        use_distributed_sampler=False,\n        sync_batchnorm=True,\n        num_sanity_val_steps=2,\n        enable_checkpointing=False,\n        enable_progress_bar=True,\n        enable_model_summary=True,\n        strategy=\"ddp_find_unused_parameters_true\",\n    )\n\n    # Fit, evaluate, and save checkpoints\n    trainer.fit(\n        model=pl_model,\n        train_dataloaders=None,\n        val_dataloaders=None,\n        datamodule=datamodule,\n        ckpt_path=resume_checkpoint_path,\n    )", "\n\ndef train(args) -> None:\n    r\"\"\"Train, evaluate, and save checkpoints.\n\n    Args:\n        workspace (str): directory of workspace\n        config_yaml (str): config yaml path\n\n    Returns:\n        None\n    \"\"\"\n\n    # Arguments & parameters\n    workspace = args.workspace\n    config_yaml = args.config_yaml\n    filename = args.filename\n\n    # GPUs number\n    devices_num = torch.cuda.device_count()\n\n    # Read config file\n    configs = parse_yaml(config_yaml)\n\n    # Configurations of pretrained sound event detection model from PANNs\n    sed_model_type = configs[\"sound_event_detection\"][\"model_type\"]\n\n    # Configuration of data to train the universal source separation system\n    clip_seconds = CLIP_SECONDS\n    frames_per_second = FRAMES_PER_SECOND\n    sample_rate = configs[\"data\"][\"sample_rate\"]\n    classes_num = configs[\"data\"][\"classes_num\"]\n    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n    anchor_segment_detect_mode = configs[\"data\"][\"anchor_segment_detect_mode\"]\n    mix_num = configs[\"data\"][\"mix_num\"]\n    match_energy = configs[\"data\"][\"augmentation\"][\"match_energy\"]\n\n    # Configuration of the universal source separation model\n    ss_model_type = configs[\"ss_model\"][\"model_type\"]\n    input_channels = configs[\"ss_model\"][\"input_channels\"]\n    output_channels = configs[\"ss_model\"][\"output_channels\"]\n    condition_size = configs[\"query_net\"][\"outputs_num\"]\n\n    # Configuration of the trainer\n    num_workers = configs[\"train\"][\"num_workers\"]\n    loss_type = configs[\"train\"][\"loss_type\"]\n    optimizer_type = configs[\"train\"][\"optimizer\"][\"optimizer_type\"]\n    learning_rate = float(configs[\"train\"][\"optimizer\"][\"learning_rate\"])\n    lr_lambda_type = configs[\"train\"][\"optimizer\"][\"lr_lambda_type\"]\n    warm_up_steps = configs[\"train\"][\"optimizer\"][\"warm_up_steps\"]\n    reduce_lr_steps = configs[\"train\"][\"optimizer\"][\"reduce_lr_steps\"]\n    save_step_frequency = configs[\"train\"][\"save_step_frequency\"]\n    evaluate_step_frequency = configs[\"train\"][\"evaluate_step_frequency\"]\n    resume_checkpoint_path = configs[\"train\"][\"resume_checkpoint_path\"]\n    if resume_checkpoint_path == \"\":\n        resume_checkpoint_path = None\n\n    # Configuration of the evaluation\n    balanced_train_eval_dir = os.path.join(\n        workspace, configs[\"evaluate\"][\"balanced_train_eval_dir\"])\n    test_eval_dir = os.path.join(\n        workspace, configs[\"evaluate\"][\"test_eval_dir\"])\n    max_eval_per_class = configs[\"evaluate\"][\"max_eval_per_class\"]\n\n    # Get directories and paths\n    checkpoints_dir, logs_dir, tf_logs_dir, statistics_path = get_dirs(\n        workspace, filename, config_yaml, devices_num,\n    )\n\n    # Create a PyTorch Lightning datamodule\n    datamodule = get_datamodule(\n        workspace=workspace,\n        config_yaml=config_yaml,\n        num_workers=num_workers,\n        devices_num=devices_num,\n    )\n\n    sed_model = load_pretrained_panns(\n        model_type=sed_model_type,\n        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),\n        freeze=True,\n    )\n\n    # Initialize query net\n    query_net = initialize_query_net(\n        configs=configs,\n    )\n\n    # Initialize separation model\n    SsModel = get_model_class(model_type=ss_model_type)\n\n    ss_model = SsModel(\n        input_channels=input_channels,\n        output_channels=output_channels,\n        condition_size=condition_size,\n    )\n\n    # Loss function\n    loss_function = get_loss_function(loss_type=loss_type)\n\n    # Anchor segment detector\n    anchor_segment_detector = AnchorSegmentDetector(\n        sed_model=sed_model,\n        clip_seconds=clip_seconds,\n        segment_seconds=segment_seconds,\n        frames_per_second=frames_per_second,\n        sample_rate=sample_rate,\n        detect_mode=anchor_segment_detect_mode,\n    )\n\n    # Anchor segment mixer\n    anchor_segment_mixer = AnchorSegmentMixer(\n        mix_num=mix_num,\n        match_energy=match_energy,\n    )\n\n    # Learning rate scaler\n    lr_lambda_func = get_lr_lambda(\n        lr_lambda_type=lr_lambda_type,\n        warm_up_steps=warm_up_steps,\n        reduce_lr_steps=reduce_lr_steps,\n    )\n\n    # PyTorch Lightning model\n    pl_model = LitSeparation(\n        ss_model=ss_model,\n        anchor_segment_detector=anchor_segment_detector,\n        anchor_segment_mixer=anchor_segment_mixer,\n        query_net=query_net,\n        loss_function=loss_function,\n        optimizer_type=optimizer_type,\n        learning_rate=learning_rate,\n        lr_lambda_func=lr_lambda_func,\n    )\n\n    # Checkpoint\n    checkpoint_every_n_steps = CheckpointEveryNSteps(\n        checkpoints_dir=checkpoints_dir,\n        save_step_frequency=save_step_frequency,\n    )\n\n    # Summary writer\n    summary_writer = SummaryWriter(log_dir=tf_logs_dir)\n\n    # Evaluation callback\n    evaluate_callback = EvaluateCallback(\n        pl_model=pl_model,\n        balanced_train_eval_dir=balanced_train_eval_dir,\n        test_eval_dir=test_eval_dir,\n        classes_num=classes_num,\n        max_eval_per_class=max_eval_per_class,\n        evaluate_step_frequency=evaluate_step_frequency,\n        summary_writer=summary_writer,\n        statistics_path=statistics_path,\n    )\n\n    # All callbacks\n    callbacks = [checkpoint_every_n_steps, evaluate_callback]\n\n    trainer = pl.Trainer(\n        accelerator=\"auto\",\n        devices=\"auto\",\n        num_nodes=1,\n        precision=\"32-true\",\n        logger=None,\n        callbacks=callbacks,\n        fast_dev_run=False,\n        max_epochs=-1,\n        use_distributed_sampler=False,\n        sync_batchnorm=True,\n        num_sanity_val_steps=2,\n        enable_checkpointing=False,\n        enable_progress_bar=True,\n        enable_model_summary=True,\n        strategy=\"ddp_find_unused_parameters_true\",\n    )\n\n    # Fit, evaluate, and save checkpoints\n    trainer.fit(\n        model=pl_model,\n        train_dataloaders=None,\n        val_dataloaders=None,\n        datamodule=datamodule,\n        ckpt_path=resume_checkpoint_path,\n    )", "\n\ndef get_dirs(\n    workspace: str,\n    filename: str,\n    config_yaml: str,\n    devices_num: int\n) -> List[str]:\n    r\"\"\"Get directories and paths.\n\n    Args:\n        workspace (str): directory of workspace\n        filename (str): filename of current .py file.\n        config_yaml (str): config yaml path\n        devices_num (int): 0 for cpu and 8 for training with 8 GPUs\n\n    Returns:\n        checkpoints_dir (str): directory to save checkpoints\n        logs_dir (str), directory to save logs\n        tf_logs_dir (str), directory to save TensorBoard logs\n        statistics_path (str), directory to save statistics\n    \"\"\"\n\n    yaml_name = pathlib.Path(config_yaml).stem\n\n    # Directory to save checkpoints\n    checkpoints_dir = os.path.join(\n        workspace,\n        \"checkpoints\",\n        filename,\n        \"{},devices={}\".format(yaml_name, devices_num),\n    )\n    os.makedirs(checkpoints_dir, exist_ok=True)\n\n    # Directory to save logs\n    logs_dir = os.path.join(\n        workspace,\n        \"logs\",\n        filename,\n        \"{},devices={}\".format(yaml_name, devices_num),\n    )\n    os.makedirs(logs_dir, exist_ok=True)\n\n    # Directory to save TensorBoard logs\n    create_logging(logs_dir, filemode=\"w\")\n    logging.info(args)\n\n    tf_logs_dir = os.path.join(\n        workspace,\n        \"tf_logs\",\n        filename,\n        \"{},devices={}\".format(yaml_name, devices_num),\n    )\n\n    # Directory to save statistics\n    statistics_path = os.path.join(\n        workspace,\n        \"statistics\",\n        filename,\n        \"{},devices={}\".format(yaml_name, devices_num),\n        \"statistics.pkl\",\n    )\n    os.makedirs(os.path.dirname(statistics_path), exist_ok=True)\n\n    return checkpoints_dir, logs_dir, tf_logs_dir, statistics_path", "\n\ndef get_datamodule(\n    workspace: str,\n    config_yaml: str,\n    num_workers: int,\n    devices_num: int,\n) -> DataModule:\n    r\"\"\"Create a PyTorch Lightning datamodule for yielding mini-batches of data.\n\n    Args:\n        workspace (str): directory of workspace\n        config_yaml (str): config yaml path\n        num_workers (int): e.g., 16 for using multiple cpu cores for preparing\n            data in parallel\n        devices_num (int): the number of GPUs to run\n\n    Returns:\n        datamodule: DataModule\n\n    Examples::\n\n        >>> data_module.setup()\n        >>> for batch_data_dict in datamodule:\n        >>>     print(batch_data_dict.keys())\n        >>>     break\n    \"\"\"\n\n    # Read configs\n    configs = parse_yaml(config_yaml)\n    indexes_hdf5_path = os.path.join(\n        workspace, configs[\"data\"][\"indexes_dict\"])\n    batch_size = configs[\"train\"][\"batch_size_per_device\"] * devices_num\n    steps_per_epoch = configs[\"train\"][\"steps_per_epoch\"]\n\n    # dataset\n    train_dataset = Dataset(\n        steps_per_epoch=steps_per_epoch,\n    )\n\n    # sampler\n    train_sampler = BalancedSampler(\n        indexes_hdf5_path=indexes_hdf5_path,\n        batch_size=batch_size,\n        steps_per_epoch=steps_per_epoch,\n    )\n\n    # data module\n    data_module = DataModule(\n        train_sampler=train_sampler,\n        train_dataset=train_dataset,\n        num_workers=num_workers,\n    )\n\n    return data_module", "\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--workspace\", type=str, required=True, help=\"Directory of workspace.\"\n    )\n    parser.add_argument(\n        \"--config_yaml\",\n        type=str,\n        required=True,\n        help=\"Path of config file for training.\",\n    )\n\n    args = parser.parse_args()\n    args.filename = pathlib.Path(__file__).stem\n\n    train(args)", ""]}
{"filename": "uss/config.py", "chunked_list": ["from pathlib import Path\n\nimport pandas as pd\n\nfrom uss.utils import get_path\n\ncsv_paths_dict = {\n    \"class_labels_indices.csv\": {\n        \"path\": Path(Path.home(), \".cache/uss/metadata/class_labels_indices.csv\"),\n        \"remote_path\": \"https://sandbox.zenodo.org/record/1186898/files/class_labels_indices.csv?download=1\",", "        \"path\": Path(Path.home(), \".cache/uss/metadata/class_labels_indices.csv\"),\n        \"remote_path\": \"https://sandbox.zenodo.org/record/1186898/files/class_labels_indices.csv?download=1\",\n        \"size\": 14675,\n    },\n    \"ontology.csv\": {\n        \"path\": Path(Path.home(), \".cache/uss/metadata/ontology.json\"),\n        \"remote_path\": \"https://sandbox.zenodo.org/record/1186898/files/ontology.json?download=1\",\n        \"size\": 342780,\n    },\n}", "    },\n}\n\npanns_paths_dict = {\n    \"Cnn14\": {\n        \"path\": Path(Path.home(), \".cache/panns/Cnn14_mAP=0.431.pth\"),\n        \"remote_path\": \"https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\",\n        \"size\": 327428481,\n    },\n    \"Cnn14_DecisionLevelMax\": {", "    },\n    \"Cnn14_DecisionLevelMax\": {\n        \"path\": Path(Path.home(), \".cache/panns/Cnn14_DecisionLevelMax_mAP=0.385.pth\"),\n        \"remote_path\": \"https://zenodo.org/record/3987831/files/Cnn14_DecisionLevelMax_mAP%3D0.385.pth?download=1\",\n        \"size\": 327428481,\n    },\n}\n\n\nSAMPLE_RATE = 32000", "\nSAMPLE_RATE = 32000\nCLIP_SECONDS = 10.\nCLIP_SAMPLES = int(SAMPLE_RATE * CLIP_SECONDS)\nFRAMES_PER_SECOND = 100\n\n# Parse metadata\nmeta_csv_path = get_path(meta=csv_paths_dict[\"class_labels_indices.csv\"])\n\ndf = pd.read_csv(meta_csv_path, sep=',')", "\ndf = pd.read_csv(meta_csv_path, sep=',')\n\nIDS = df[\"mid\"].tolist()\nLABELS = df[\"display_name\"].tolist()\n\nCLASSES_NUM = len(LABELS)\n\nLB_TO_IX = {label: i for i, label in enumerate(LABELS)}\nIX_TO_LB = {i: label for i, label in enumerate(LABELS)}", "LB_TO_IX = {label: i for i, label in enumerate(LABELS)}\nIX_TO_LB = {i: label for i, label in enumerate(LABELS)}\n\nID_TO_IX = {id: i for i, id in enumerate(IDS)}\nIX_TO_ID = {i: id for i, id in enumerate(IDS)}\n\nROOT_CLASS_ID_DICT = {\n    \"/m/0dgw9r\": \"Human sounds\",\n    \"/m/0jbk\": \"Animal\",\n    \"/m/04rlf\": \"Music\",", "    \"/m/0jbk\": \"Animal\",\n    \"/m/04rlf\": \"Music\",\n    \"/m/059j3w\": \"Natural sounds\",\n    \"/t/dd00041\": \"Sounds of things\",\n    \"/t/dd00098\": \"Source-ambiguous sounds\",\n    \"/t/dd00123\": \"Channel, environment and background\",\n}\n"]}
{"filename": "uss/evaluate.py", "chunked_list": ["import logging\nimport os\nimport pathlib\nimport re\nfrom typing import Dict, List\n\nimport librosa\nimport lightning.pytorch as pl\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\n\nfrom uss.config import IX_TO_LB\nfrom uss.inference import load_ss_model\nfrom uss.utils import (calculate_sdr, create_logging, get_mean_sdr_from_dict,\n                       parse_yaml)\n\n\nclass AudioSetEvaluator:\n    def __init__(\n        self,\n        audios_dir: str,\n        classes_num: int,\n        max_eval_per_class=None,\n    ) -> None:\n        r\"\"\"AudioSet evaluator.\n\n        Args:\n            audios_dir (str): directory of evaluation segments\n            classes_num (int): the number of sound classes\n            max_eval_per_class (int), the number of samples to evaluate for each sound class\n\n        Returns:\n            None\n        \"\"\"\n\n        self.audios_dir = audios_dir\n        self.classes_num = classes_num\n        self.max_eval_per_class = max_eval_per_class\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        pl_model: pl.LightningModule\n    ) -> Dict:\n        r\"\"\"Evalute.\"\"\"\n\n        sdrs_dict = {class_id: [] for class_id in range(self.classes_num)}\n        sdris_dict = {class_id: [] for class_id in range(self.classes_num)}\n\n        for class_id in range(self.classes_num):\n\n            sub_dir = os.path.join(\n                self.audios_dir,\n                \"class_id={}\".format(class_id))\n\n            audio_names = self._get_audio_names(audios_dir=sub_dir)\n\n            for audio_index, audio_name in enumerate(audio_names):\n\n                if audio_index == self.max_eval_per_class:\n                    break\n\n                source_path = os.path.join(\n                    sub_dir, \"{},source.wav\".format(audio_name))\n                mixture_path = os.path.join(\n                    sub_dir, \"{},mixture.wav\".format(audio_name))\n\n                source, fs = librosa.load(source_path, sr=None, mono=True)\n                mixture, fs = librosa.load(mixture_path, sr=None, mono=True)\n\n                sdr_no_sep = calculate_sdr(ref=source, est=mixture)\n\n                device = pl_model.device\n\n                conditions = pl_model.query_net(\n                    source=torch.Tensor(source)[None, :].to(device),\n                )[\"output\"]\n                # conditions: (batch_size=1, condition_dim)\n\n                input_dict = {\n                    \"mixture\": torch.Tensor(mixture)[None, None, :].to(device),\n                    \"condition\": conditions,\n                }\n\n                pl_model.eval()\n                sep_segment = pl_model.ss_model(input_dict)[\"waveform\"]\n                # sep_segment: (batch_size=1, channels_num=1, segment_samples)\n\n                sep_segment = sep_segment.squeeze(\n                    dim=(0, 1)).data.cpu().numpy()\n                # sep_segment: (segment_samples,)\n\n                sdr = calculate_sdr(ref=source, est=sep_segment)\n                sdri = sdr - sdr_no_sep\n\n                sdrs_dict[class_id].append(sdr)\n                sdris_dict[class_id].append(sdri)\n\n            logging.info(\n                \"Class ID: {} / {}, SDR: {:.3f}, SDRi: {:.3f}\".format(\n                    class_id, self.classes_num, np.mean(\n                        sdrs_dict[class_id]), np.mean(\n                        sdris_dict[class_id])))\n\n        stats_dict = {\n            \"sdrs_dict\": sdrs_dict,\n            \"sdris_dict\": sdris_dict,\n        }\n\n        return stats_dict\n\n    def _get_audio_names(self, audios_dir: str) -> List[str]:\n        r\"\"\"Get evaluation audio names.\"\"\"\n\n        audio_names = sorted(os.listdir(audios_dir))\n\n        audio_names = [\n            re.search(\n                \"(.*),(mixture|source).wav\",\n                audio_name).group(1) for audio_name in audio_names]\n\n        audio_names = sorted(list(set(audio_names)))\n\n        return audio_names\n\n    @staticmethod\n    def get_median_metrics(stats_dict, metric_type):\n        class_ids = stats_dict[metric_type].keys()\n        median_stats_dict = {\n            class_id: np.nanmedian(\n                stats_dict[metric_type][class_id]) for class_id in class_ids}\n        return median_stats_dict", "\nclass AudioSetEvaluator:\n    def __init__(\n        self,\n        audios_dir: str,\n        classes_num: int,\n        max_eval_per_class=None,\n    ) -> None:\n        r\"\"\"AudioSet evaluator.\n\n        Args:\n            audios_dir (str): directory of evaluation segments\n            classes_num (int): the number of sound classes\n            max_eval_per_class (int), the number of samples to evaluate for each sound class\n\n        Returns:\n            None\n        \"\"\"\n\n        self.audios_dir = audios_dir\n        self.classes_num = classes_num\n        self.max_eval_per_class = max_eval_per_class\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        pl_model: pl.LightningModule\n    ) -> Dict:\n        r\"\"\"Evalute.\"\"\"\n\n        sdrs_dict = {class_id: [] for class_id in range(self.classes_num)}\n        sdris_dict = {class_id: [] for class_id in range(self.classes_num)}\n\n        for class_id in range(self.classes_num):\n\n            sub_dir = os.path.join(\n                self.audios_dir,\n                \"class_id={}\".format(class_id))\n\n            audio_names = self._get_audio_names(audios_dir=sub_dir)\n\n            for audio_index, audio_name in enumerate(audio_names):\n\n                if audio_index == self.max_eval_per_class:\n                    break\n\n                source_path = os.path.join(\n                    sub_dir, \"{},source.wav\".format(audio_name))\n                mixture_path = os.path.join(\n                    sub_dir, \"{},mixture.wav\".format(audio_name))\n\n                source, fs = librosa.load(source_path, sr=None, mono=True)\n                mixture, fs = librosa.load(mixture_path, sr=None, mono=True)\n\n                sdr_no_sep = calculate_sdr(ref=source, est=mixture)\n\n                device = pl_model.device\n\n                conditions = pl_model.query_net(\n                    source=torch.Tensor(source)[None, :].to(device),\n                )[\"output\"]\n                # conditions: (batch_size=1, condition_dim)\n\n                input_dict = {\n                    \"mixture\": torch.Tensor(mixture)[None, None, :].to(device),\n                    \"condition\": conditions,\n                }\n\n                pl_model.eval()\n                sep_segment = pl_model.ss_model(input_dict)[\"waveform\"]\n                # sep_segment: (batch_size=1, channels_num=1, segment_samples)\n\n                sep_segment = sep_segment.squeeze(\n                    dim=(0, 1)).data.cpu().numpy()\n                # sep_segment: (segment_samples,)\n\n                sdr = calculate_sdr(ref=source, est=sep_segment)\n                sdri = sdr - sdr_no_sep\n\n                sdrs_dict[class_id].append(sdr)\n                sdris_dict[class_id].append(sdri)\n\n            logging.info(\n                \"Class ID: {} / {}, SDR: {:.3f}, SDRi: {:.3f}\".format(\n                    class_id, self.classes_num, np.mean(\n                        sdrs_dict[class_id]), np.mean(\n                        sdris_dict[class_id])))\n\n        stats_dict = {\n            \"sdrs_dict\": sdrs_dict,\n            \"sdris_dict\": sdris_dict,\n        }\n\n        return stats_dict\n\n    def _get_audio_names(self, audios_dir: str) -> List[str]:\n        r\"\"\"Get evaluation audio names.\"\"\"\n\n        audio_names = sorted(os.listdir(audios_dir))\n\n        audio_names = [\n            re.search(\n                \"(.*),(mixture|source).wav\",\n                audio_name).group(1) for audio_name in audio_names]\n\n        audio_names = sorted(list(set(audio_names)))\n\n        return audio_names\n\n    @staticmethod\n    def get_median_metrics(stats_dict, metric_type):\n        class_ids = stats_dict[metric_type].keys()\n        median_stats_dict = {\n            class_id: np.nanmedian(\n                stats_dict[metric_type][class_id]) for class_id in class_ids}\n        return median_stats_dict", "\n\ndef test_evaluate(config_yaml: str, workspace: str):\n    r\"\"\"Evaluate using pretrained checkpoint.\n\n    Args:\n        config_yaml (str), path of the config yaml file\n        workspace (str), directory of workspace\n\n    Returns:\n        None\n    \"\"\"\n\n    device = \"cuda\"\n\n    configs = parse_yaml(config_yaml)\n\n    classes_num = configs[\"data\"][\"classes_num\"]\n\n    audios_dir = os.path.join(\n        workspace, \"evaluation/audioset/2s_segments_test\")\n\n    create_logging(\"_tmp_log\", filemode=\"w\")\n\n    # Evlauator\n    evaluator = AudioSetEvaluator(\n        audios_dir=audios_dir,\n        classes_num=classes_num,\n        max_eval_per_class=10\n    )\n\n    steps = [1]\n\n    for step in steps:\n\n        # Checkpoint path\n        checkpoint_path = os.path.join(\n            workspace, \"checkpoints/train/config={},devices=1/step={}.ckpt\".format(\n                pathlib.Path(config_yaml).stem, step))\n\n        # Load model\n        pl_model = load_ss_model(\n            configs=configs,\n            checkpoint_path=checkpoint_path\n        ).to(device)\n\n        # Evaluate statistics\n        stats_dict = evaluator(pl_model=pl_model)\n\n        median_sdris = {}\n\n        for class_id in range(classes_num):\n\n            median_sdris[class_id] = np.nanmedian(\n                stats_dict[\"sdris_dict\"][class_id])\n\n            print(\n                \"{} {}: {:.3f}\".format(\n                    class_id,\n                    IX_TO_LB[class_id],\n                    median_sdris[class_id]))\n\n        mean_sdri = get_mean_sdr_from_dict(median_sdris)\n        # final_sdri = np.nanmean([mean_sdris[class_id]\n        # for class_id in range(classes_num)])\n        print(\"--------\")\n        print(\"Average SDRi: {:.3f}\".format(mean_sdri))", "\n\nif __name__ == \"__main__\":\n\n    test_evaluate(\n        config_yaml=\"./scripts/train/ss_model=resunet30,querynet=at_soft,gpus=1.yaml\",\n        workspace=\"./workspaces/uss\",\n    )\n", ""]}
{"filename": "uss/uss_inference.py", "chunked_list": ["import argparse\nfrom pathlib import Path\n\nfrom uss.inference import separate\nfrom uss.utils import get_path\n\nmodel_paths_dict = {\n    \"at_soft\": {\n        \"config_yaml\": {\n            \"path\": Path(Path.home(), \".cache/uss/scripts/ss_model=resunet30,querynet=at_soft,data=full.yaml\"),", "        \"config_yaml\": {\n            \"path\": Path(Path.home(), \".cache/uss/scripts/ss_model=resunet30,querynet=at_soft,data=full.yaml\"),\n            \"remote_path\": \"https://huggingface.co/RSNuts/Universal_Source_Separation/resolve/main/uss_material/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull.yaml?download=1\",\n            \"size\": 1558,\n        },\n        \"checkpoint\": {\n            \"path\": Path(Path.home(), \".cache/uss/checkpoints/ss_model=resunet30,querynet=at_soft,data=full,devices=8,step=1000000.ckpt\"),\n            \"remote_path\": \"https://huggingface.co/RSNuts/Universal_Source_Separation/resolve/main/uss_material/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull%2Cdevices%3D8%2Cstep%3D1000000.ckpt\",\n            \"size\": 1121024828,\n        },", "            \"size\": 1121024828,\n        },\n    }\n}\n\n\ndef main():\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--audio_path\", type=str, required=True)\n    parser.add_argument(\n        \"-c\",\n        \"--condition_type\",\n        type=str,\n        default=\"at_soft\",\n        choices=[\n            \"at_soft\",\n            \"embedding\"])\n    parser.add_argument(\"--levels\", nargs=\"*\", type=int, default=[])\n    parser.add_argument(\"--class_ids\", nargs=\"*\", type=int, default=[])\n    parser.add_argument(\"--queries_dir\", type=str, default=\"\")\n    parser.add_argument(\"--query_emb_path\", type=str, default=\"\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"\")\n\n    args = parser.parse_args()\n\n    condition_type = args.condition_type\n\n    # Use default pretrained models\n    if condition_type == \"at_soft\":\n        args.config_yaml = get_path(\n            meta=model_paths_dict[condition_type][\"config_yaml\"])\n        args.checkpoint_path = get_path(\n            meta=model_paths_dict[condition_type][\"checkpoint\"])\n\n    elif condition_type == \"embedding\":\n        pass\n\n    else:\n        raise NotImplementedError\n\n    separate(args)", ""]}
{"filename": "uss/utils.py", "chunked_list": ["import datetime\nimport json\nimport logging\nimport os\nimport pickle\nfrom pathlib import Path\nfrom typing import Dict\n\nimport librosa\nimport numpy as np", "import librosa\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport yaml\nfrom panns_inference.models import Cnn14, Cnn14_DecisionLevelMax\n\n\ndef create_logging(log_dir, filemode):\n    os.makedirs(log_dir, exist_ok=True)\n    i1 = 0\n\n    while os.path.isfile(os.path.join(log_dir, \"{:04d}.log\".format(i1))):\n        i1 += 1\n\n    log_path = os.path.join(log_dir, \"{:04d}.log\".format(i1))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",\n        datefmt=\"%a, %d %b %Y %H:%M:%S\",\n        filename=log_path,\n        filemode=filemode,\n    )\n\n    # Print to console\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\"%(name)-12s: %(levelname)-8s %(message)s\")\n    console.setFormatter(formatter)\n    logging.getLogger(\"\").addHandler(console)\n\n    return logging", "def create_logging(log_dir, filemode):\n    os.makedirs(log_dir, exist_ok=True)\n    i1 = 0\n\n    while os.path.isfile(os.path.join(log_dir, \"{:04d}.log\".format(i1))):\n        i1 += 1\n\n    log_path = os.path.join(log_dir, \"{:04d}.log\".format(i1))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",\n        datefmt=\"%a, %d %b %Y %H:%M:%S\",\n        filename=log_path,\n        filemode=filemode,\n    )\n\n    # Print to console\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\"%(name)-12s: %(levelname)-8s %(message)s\")\n    console.setFormatter(formatter)\n    logging.getLogger(\"\").addHandler(console)\n\n    return logging", "\n\ndef float32_to_int16(x: float) -> int:\n    x = np.clip(x, a_min=-1, a_max=1)\n    return (x * 32767.0).astype(np.int16)\n\n\ndef int16_to_float32(x: int) -> float:\n    return (x / 32767.0).astype(np.float32)\n", "\n\ndef parse_yaml(config_yaml: str) -> Dict:\n    r\"\"\"Parse yaml file.\n\n    Args:\n        config_yaml (str): config yaml path\n\n    Returns:\n        yaml_dict (Dict): parsed yaml file\n    \"\"\"\n\n    with open(config_yaml, \"r\") as fr:\n        return yaml.load(fr, Loader=yaml.FullLoader)", "\n\ndef get_audioset632_id_to_lb(ontology_path: str) -> Dict:\n    r\"\"\"Get AudioSet 632 classes ID to label mapping.\"\"\"\n\n    audioset632_id_to_lb = {}\n\n    with open(ontology_path) as f:\n        data_list = json.load(f)\n\n    for e in data_list:\n        audioset632_id_to_lb[e[\"id\"]] = e[\"name\"]\n\n    return audioset632_id_to_lb", "\n\ndef load_pretrained_panns(\n    model_type: str,\n    checkpoint_path: str,\n    freeze: bool\n) -> nn.Module:\n    r\"\"\"Load pretrained pretrained audio neural networks (PANNs).\n\n    Args:\n        model_type: str, e.g., \"Cnn14\"\n        checkpoint_path, str, e.g., \"Cnn14_mAP=0.431.pth\"\n        freeze: bool\n\n    Returns:\n        model: nn.Module\n    \"\"\"\n\n    if model_type == \"Cnn14\":\n        Model = Cnn14\n\n    elif model_type == \"Cnn14_DecisionLevelMax\":\n        Model = Cnn14_DecisionLevelMax\n\n    else:\n        raise NotImplementedError\n\n    model = Model(sample_rate=32000, window_size=1024, hop_size=320,\n                  mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n\n    if checkpoint_path:\n        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n\n    if freeze:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    return model", "\n\ndef energy(x):\n    return torch.mean(x ** 2)\n\n\ndef magnitude_to_db(x):\n    eps = 1e-10\n    return 20. * np.log10(max(x, eps))\n", "\n\ndef db_to_magnitude(x):\n    return 10. ** (x / 20)\n\n\ndef ids_to_hots(ids, classes_num, device):\n    hots = torch.zeros(classes_num).to(device)\n    for id in ids:\n        hots[id] = 1\n    return hots", "\n\ndef calculate_sdr(\n    ref: np.ndarray,\n    est: np.ndarray,\n    eps=1e-10\n) -> float:\n    r\"\"\"Calculate SDR between reference and estimation.\n\n    Args:\n        ref (np.ndarray), reference signal\n        est (np.ndarray), estimated signal\n    \"\"\"\n\n    noise = est - ref\n\n    numerator = np.clip(a=np.mean(ref ** 2), a_min=eps, a_max=None)\n\n    denominator = np.clip(a=np.mean(noise ** 2), a_min=eps, a_max=None)\n\n    sdr = 10. * np.log10(numerator / denominator)\n\n    return sdr", "\n\nclass StatisticsContainer(object):\n    def __init__(self, statistics_path):\n        self.statistics_path = statistics_path\n\n        self.backup_statistics_path = \"{}_{}.pkl\".format(\n            os.path.splitext(self.statistics_path)[0],\n            datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n        )\n\n        self.statistics_dict = {\"balanced_train\": [], \"test\": []}\n\n    def append(self, steps, statistics, split, flush=True):\n        statistics[\"steps\"] = steps\n        self.statistics_dict[split].append(statistics)\n\n        if flush:\n            self.flush()\n\n    def flush(self):\n        pickle.dump(self.statistics_dict, open(self.statistics_path, \"wb\"))\n        pickle.dump(\n            self.statistics_dict, open(\n                self.backup_statistics_path, \"wb\"))\n        logging.info(\"    Dump statistics to {}\".format(self.statistics_path))\n        logging.info(\n            \"    Dump statistics to {}\".format(\n                self.backup_statistics_path))", "\n\ndef get_mean_sdr_from_dict(sdris_dict):\n    mean_sdr = np.nanmean(list(sdris_dict.values()))\n    return mean_sdr\n\n\ndef remove_silence(audio: np.ndarray, sample_rate: int) -> np.ndarray:\n    r\"\"\"Remove silent frames.\"\"\"\n    window_size = int(sample_rate * 0.1)\n    threshold = 0.02\n\n    frames = librosa.util.frame(\n        x=audio,\n        frame_length=window_size,\n        hop_length=window_size).T\n    # shape: (frames_num, window_size)\n\n    new_frames = get_active_frames(frames, threshold)\n    # shape: (new_frames_num, window_size)\n\n    new_audio = new_frames.flatten()\n    # shape: (new_audio_samples,)\n\n    return new_audio", "\n\ndef get_active_frames(frames: np.ndarray, threshold: float) -> np.ndarray:\n    r\"\"\"Get active frames.\"\"\"\n\n    energy = np.max(np.abs(frames), axis=-1)\n    # shape: (frames_num,)\n\n    active_indexes = np.where(energy > threshold)[0]\n    # shape: (new_frames_num,)\n\n    new_frames = frames[active_indexes]\n    # shape: (new_frames_num,)\n\n    return new_frames", "\n\ndef repeat_to_length(audio: np.ndarray, segment_samples: int) -> np.ndarray:\n    r\"\"\"Repeat audio to length.\"\"\"\n\n    repeats_num = (segment_samples // audio.shape[-1]) + 1\n    audio = np.tile(audio, repeats_num)[0: segment_samples]\n\n    return audio\n", "\n\ndef get_path(meta, re_download=False):\n\n    path = meta[\"path\"]\n    remote_path = meta[\"remote_path\"]\n    size = meta[\"size\"]\n\n    if not Path(path).is_file() or Path(\n            path).stat().st_size != size or re_download:\n\n        Path(path).parents[0].mkdir(parents=True, exist_ok=True)\n        os.system(\"wget -O {} {}\".format(path, remote_path))\n        print(\"Download to {}\".format(path))\n\n    return path", ""]}
{"filename": "uss/data/samplers.py", "chunked_list": ["import logging\nimport time\nfrom typing import Dict, List\n\nimport h5py\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass Base:\n    def __init__(self,\n                 indexes_hdf5_path: str,\n                 batch_size: int,\n                 steps_per_epoch: int,\n                 random_seed: int,\n                 ):\n        r\"\"\"Base class of train samplers.\n\n        Args:\n            indexes_hdf5_path (str)\n            batch_size (int)\n            steps_per_epoch (int)\n            random_seed (int)\n\n        Returns:\n            None\n        \"\"\"\n\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(random_seed)\n        self.steps_per_epoch = steps_per_epoch\n\n        # Load targets of training data\n        load_time = time.time()\n\n        with h5py.File(indexes_hdf5_path, 'r') as hf:\n            self.audio_names = [audio_name.decode()\n                                for audio_name in hf['audio_name'][:]]\n            self.hdf5_paths = [hdf5_path.decode()\n                               for hdf5_path in hf['hdf5_path'][:]]\n            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n            self.targets = hf['target'][:].astype(np.float32)\n            # self.targets: (audios_num, classes_num)\n\n        self.audios_num, self.classes_num = self.targets.shape\n\n        logging.info('Training number: {}'.format(self.audios_num))\n        logging.info(\n            'Load target time: {:.3f} s'.format(\n                time.time() - load_time))\n\n        # Number of training samples of different sound classes\n        self.samples_num_per_class = np.sum(self.targets, axis=0)\n\n        logging.info('samples_num_per_class: {}'.format(\n            self.samples_num_per_class.astype(np.int32)))", "\n\nclass Base:\n    def __init__(self,\n                 indexes_hdf5_path: str,\n                 batch_size: int,\n                 steps_per_epoch: int,\n                 random_seed: int,\n                 ):\n        r\"\"\"Base class of train samplers.\n\n        Args:\n            indexes_hdf5_path (str)\n            batch_size (int)\n            steps_per_epoch (int)\n            random_seed (int)\n\n        Returns:\n            None\n        \"\"\"\n\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(random_seed)\n        self.steps_per_epoch = steps_per_epoch\n\n        # Load targets of training data\n        load_time = time.time()\n\n        with h5py.File(indexes_hdf5_path, 'r') as hf:\n            self.audio_names = [audio_name.decode()\n                                for audio_name in hf['audio_name'][:]]\n            self.hdf5_paths = [hdf5_path.decode()\n                               for hdf5_path in hf['hdf5_path'][:]]\n            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n            self.targets = hf['target'][:].astype(np.float32)\n            # self.targets: (audios_num, classes_num)\n\n        self.audios_num, self.classes_num = self.targets.shape\n\n        logging.info('Training number: {}'.format(self.audios_num))\n        logging.info(\n            'Load target time: {:.3f} s'.format(\n                time.time() - load_time))\n\n        # Number of training samples of different sound classes\n        self.samples_num_per_class = np.sum(self.targets, axis=0)\n\n        logging.info('samples_num_per_class: {}'.format(\n            self.samples_num_per_class.astype(np.int32)))", "\n\nclass BalancedSampler(Base, Sampler):\n    def __init__(self,\n                 indexes_hdf5_path: str,\n                 batch_size: int,\n                 steps_per_epoch: int,\n                 random_seed: int = 1234,\n                 ) -> None:\n        r\"\"\"Balanced sampler. Generate mini-batches meta for training. Data are\n        evenly sampled from different sound classes.\n\n        Args:\n            indexes_hdf5_path (str)\n            batch_size (int)\n            steps_per_epoch (int)\n            random_seed (int)\n\n        Returns:\n            None\n        \"\"\"\n\n        super(BalancedSampler, self).__init__(\n            indexes_hdf5_path=indexes_hdf5_path,\n            batch_size=batch_size,\n            steps_per_epoch=steps_per_epoch,\n            random_seed=random_seed,\n        )\n\n        # Training indexes of all sound classes. E.g.:\n        # [[0, 11, 12, ...], [3, 4, 15, 16, ...], [7, 8, ...], ...]\n        self.indexes_per_class = []\n\n        for k in range(self.classes_num):\n            self.indexes_per_class.append(\n                np.where(self.targets[:, k] == 1)[0])\n\n        # Shuffle indexes\n        for k in range(self.classes_num):\n            self.random_state.shuffle(self.indexes_per_class[k])\n\n        self.queue = []  # Contains sound class IDs\n\n        self.pointers_of_classes = [0] * self.classes_num\n\n    def expand_queue(self, queue) -> List:\n        r\"\"\"Append more class IDs to the queue.\n\n        Args:\n            queue: List, e.g., [431, 73]\n\n        Returns:\n            queue: List, e.g., [431, 73, 2, 54, 379, ...]\n        \"\"\"\n\n        classes_set = np.arange(self.classes_num).tolist()\n        self.random_state.shuffle(classes_set)\n        queue.extend(classes_set)\n        return queue\n\n    def __iter__(self) -> List[Dict]:\n        r\"\"\"Yield mini-batch meta.\n\n        Args:\n            None\n\n        Returns:\n            batch_meta: e.g.: [\n                {\"audio_name\": \"YfWBzCRl6LUs.wav\",\n                 \"hdf5_path\": \"xx/balanced_train.h5\",\n                 \"index_in_hdf5\": 15734,\n                 \"target\": [0, 1, 0, 0, ...]},\n            ...]\n        \"\"\"\n\n        batch_size = self.batch_size\n\n        while True:\n\n            batch_meta = []\n\n            while len(batch_meta) < batch_size:\n\n                if len(self.queue) == 0:\n                    self.queue = self.expand_queue(self.queue)\n\n                # Pop a class ID and get the audio index\n                class_id = self.queue.pop(0)\n                pointer = self.pointers_of_classes[class_id]\n                self.pointers_of_classes[class_id] += 1\n                index = self.indexes_per_class[class_id][pointer]\n\n                # When finish one epoch of a sound class, then shuffle its\n                # indexes and reset pointer.\n                if self.pointers_of_classes[class_id] >= self.samples_num_per_class[class_id]:\n                    self.pointers_of_classes[class_id] = 0\n                    self.random_state.shuffle(self.indexes_per_class[class_id])\n\n                batch_meta.append({\n                    'hdf5_path': self.hdf5_paths[index],\n                    'index_in_hdf5': self.indexes_in_hdf5[index],\n                    'class_id': class_id})\n\n            yield batch_meta\n\n    def __len__(self) -> int:\n        return self.steps_per_epoch", "\n\nclass DistributedSamplerWrapper:\n    def __init__(self, sampler: object) -> None:\n        r\"\"\"Distributed wrapper of sampler.\n\n        Args:\n            sampler (Sampler object)\n\n        Returns:\n            None\n        \"\"\"\n\n        self.sampler = sampler\n\n    def __iter__(self) -> List:\n        r\"\"\"Yield a part of mini-batch meta on each device.\n\n        Args:\n            None\n\n        Returns:\n            list_meta (List), a part of mini-batch meta.\n        \"\"\"\n\n        if dist.is_initialized():\n            num_replicas = dist.get_world_size()\n            rank = dist.get_rank()\n\n        else:\n            num_replicas = 1\n            rank = 0\n\n        for list_meta in self.sampler:\n            yield list_meta[rank:: num_replicas]\n\n    def __len__(self) -> int:\n        return len(self.sampler)", ""]}
{"filename": "uss/data/datamodules.py", "chunked_list": ["from typing import Dict, List, Optional\n\nimport lightning.pytorch as pl\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom uss.data.samplers import DistributedSamplerWrapper\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        train_sampler: object,\n        train_dataset: object,\n        num_workers: int,\n    ) -> None:\n        r\"\"\"PyTorch Lightning Data module. A wrapper of the DataLoader. Can be\n        used to yield mini-batches of train, validation, and test data.\n\n        Args:\n            train_sampler (Sampler object)\n            train_dataset (Dataset object)\n            num_workers: int\n\n        Returns:\n            None\n\n        Examples::\n            >>> data_module.setup()\n            >>> for batch_data_dict in datamodule.train_dataloader():\n            >>>     print(batch_data_dict.keys())\n            >>>     break\n        \"\"\"\n\n        super().__init__()\n        self._train_sampler = train_sampler\n        self._train_dataset = train_dataset\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        r\"\"\"called on every GPU.\"\"\"\n\n        self.train_dataset = self._train_dataset\n\n        # The sampler yields a part of mini-batch meta on each device\n        self.train_sampler = DistributedSamplerWrapper(self._train_sampler)\n\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\n        r\"\"\"Get train loader.\"\"\"\n\n        if self.num_workers > 0:\n            persistent_workers = True\n        else:\n            persistent_workers = False\n\n        train_loader = DataLoader(\n            dataset=self.train_dataset,\n            batch_sampler=self.train_sampler,\n            collate_fn=self.collate_fn,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=persistent_workers,\n        )\n\n        return train_loader\n\n    def val_dataloader(self):\n        r\"\"\"We use `uss.callbacks.evaluate` to evaluate on the train / test\n        dataset\"\"\"\n        pass", "\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        train_sampler: object,\n        train_dataset: object,\n        num_workers: int,\n    ) -> None:\n        r\"\"\"PyTorch Lightning Data module. A wrapper of the DataLoader. Can be\n        used to yield mini-batches of train, validation, and test data.\n\n        Args:\n            train_sampler (Sampler object)\n            train_dataset (Dataset object)\n            num_workers: int\n\n        Returns:\n            None\n\n        Examples::\n            >>> data_module.setup()\n            >>> for batch_data_dict in datamodule.train_dataloader():\n            >>>     print(batch_data_dict.keys())\n            >>>     break\n        \"\"\"\n\n        super().__init__()\n        self._train_sampler = train_sampler\n        self._train_dataset = train_dataset\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n\n    def setup(self, stage: Optional[str] = None) -> None:\n        r\"\"\"called on every GPU.\"\"\"\n\n        self.train_dataset = self._train_dataset\n\n        # The sampler yields a part of mini-batch meta on each device\n        self.train_sampler = DistributedSamplerWrapper(self._train_sampler)\n\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\n        r\"\"\"Get train loader.\"\"\"\n\n        if self.num_workers > 0:\n            persistent_workers = True\n        else:\n            persistent_workers = False\n\n        train_loader = DataLoader(\n            dataset=self.train_dataset,\n            batch_sampler=self.train_sampler,\n            collate_fn=self.collate_fn,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=persistent_workers,\n        )\n\n        return train_loader\n\n    def val_dataloader(self):\n        r\"\"\"We use `uss.callbacks.evaluate` to evaluate on the train / test\n        dataset\"\"\"\n        pass", "\n\ndef collate_fn(list_data_dict: List[Dict]) -> Dict:\n    r\"\"\"Collate a mini-batch of data.\n\n    Args:\n        list_data_dict (List[Dict]): e.g., [\n            {\"hdf5_path\": \"xx/balanced_train.h5\",\n             \"index_in_hdf5\": 11072,\n             ...},\n        ...]\n\n    Returns:\n        data_dict (Dict): e.g., {\n            \"hdf5_path\": [\"xx/balanced_train.h5\", \"xx/balanced_train.h5\", ...]\n            \"index_in_hdf5\": [11072, 17251, ...],\n        }\n    \"\"\"\n\n    data_dict = {}\n\n    for key in list_data_dict[0].keys():\n\n        data_dict[key] = np.array([data_dict[key]\n                                  for data_dict in list_data_dict])\n\n        if str(data_dict[key].dtype) in [\"float32\"]:\n            data_dict[key] = torch.Tensor(data_dict[key])\n\n    return data_dict", ""]}
{"filename": "uss/data/datasets.py", "chunked_list": ["from typing import Dict\n\nimport h5py\nimport numpy as np\n\nfrom uss.utils import int16_to_float32\n\n\nclass Dataset:\n    def __init__(self, steps_per_epoch) -> None:\n        r\"\"\"This class takes the meta as input, and return the waveform, target\n        and other information of the audio clip. This class is used by\n        DataLoader.\n\n        Args:\n            steps_per_epoch (int): the number of steps in an epoch\n        \"\"\"\n        self.steps_per_epoch = steps_per_epoch\n\n    def __getitem__(self, meta) -> Dict:\n        \"\"\"Load waveform, target and other information of an audio clip.\n\n        Args:\n            meta (Dict): {\n                \"hdf5_path\": str,\n                \"index_in_hdf5\": int,\n                \"class_id\": int,\n            }\n\n        Returns:\n            data_dict (Dict): {\n                \"hdf5_path\": str,\n                \"index_in_hdf5\": int,\n                \"audio_name\": str,\n                \"waveform\": (clip_samples,),\n                \"target\": (classes_num,),\n                \"class_id\": int,\n            }\n        \"\"\"\n\n        hdf5_path = meta[\"hdf5_path\"]\n        index_in_hdf5 = meta[\"index_in_hdf5\"]\n        class_id = meta[\"class_id\"]\n\n        with h5py.File(hdf5_path, 'r') as hf:\n\n            audio_name = hf[\"audio_name\"][index_in_hdf5].decode()\n\n            waveform = int16_to_float32(hf[\"waveform\"][index_in_hdf5])\n            waveform = waveform\n            # shape: (clip_samples,)\n\n            target = hf[\"target\"][index_in_hdf5].astype(np.float32)\n            # shape: (classes_num,)\n\n        data_dict = {\n            \"hdf5_path\": hdf5_path,\n            \"index_in_hdf5\": index_in_hdf5,\n            \"audio_name\": audio_name,\n            \"waveform\": waveform,\n            \"target\": target,\n            \"class_id\": class_id,\n        }\n\n        return data_dict\n\n    def __len__(self) -> int:\n        return self.steps_per_epoch", "class Dataset:\n    def __init__(self, steps_per_epoch) -> None:\n        r\"\"\"This class takes the meta as input, and return the waveform, target\n        and other information of the audio clip. This class is used by\n        DataLoader.\n\n        Args:\n            steps_per_epoch (int): the number of steps in an epoch\n        \"\"\"\n        self.steps_per_epoch = steps_per_epoch\n\n    def __getitem__(self, meta) -> Dict:\n        \"\"\"Load waveform, target and other information of an audio clip.\n\n        Args:\n            meta (Dict): {\n                \"hdf5_path\": str,\n                \"index_in_hdf5\": int,\n                \"class_id\": int,\n            }\n\n        Returns:\n            data_dict (Dict): {\n                \"hdf5_path\": str,\n                \"index_in_hdf5\": int,\n                \"audio_name\": str,\n                \"waveform\": (clip_samples,),\n                \"target\": (classes_num,),\n                \"class_id\": int,\n            }\n        \"\"\"\n\n        hdf5_path = meta[\"hdf5_path\"]\n        index_in_hdf5 = meta[\"index_in_hdf5\"]\n        class_id = meta[\"class_id\"]\n\n        with h5py.File(hdf5_path, 'r') as hf:\n\n            audio_name = hf[\"audio_name\"][index_in_hdf5].decode()\n\n            waveform = int16_to_float32(hf[\"waveform\"][index_in_hdf5])\n            waveform = waveform\n            # shape: (clip_samples,)\n\n            target = hf[\"target\"][index_in_hdf5].astype(np.float32)\n            # shape: (classes_num,)\n\n        data_dict = {\n            \"hdf5_path\": hdf5_path,\n            \"index_in_hdf5\": index_in_hdf5,\n            \"audio_name\": audio_name,\n            \"waveform\": waveform,\n            \"target\": target,\n            \"class_id\": class_id,\n        }\n\n        return data_dict\n\n    def __len__(self) -> int:\n        return self.steps_per_epoch", ""]}
{"filename": "uss/data/anchor_segment_mixers.py", "chunked_list": ["from typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n\nclass AnchorSegmentMixer(nn.Module):\n    def __init__(\n        self,\n        mix_num: int,\n        match_energy: bool,\n    ) -> None:\n        r\"\"\"Anchor segment mixer. Used to mix multiple sources into a mixture.\n\n        Args:\n            mix_num (int): the number of sources to mix\n            match_energy (bool): set to `True` to rescale segments to have the\n                same energy before mixing\n\n        Returns:\n            None\n        \"\"\"\n\n        super(AnchorSegmentMixer, self).__init__()\n\n        self.mix_num = mix_num\n        self.match_energy = match_energy\n\n    def __call__(self, waveforms: torch.Tensor) -> Dict:\n        r\"\"\"Mix multiple sources to mixture.\n\n        Args:\n            waveforms (torch.Tensor): (batch_size, segment_samples)\n\n        Returns:\n            mixtures (torch.Tensor): (batch_size, segment_samples)\n            targets (torch.Tensor): (batch_size, segment_samples)\n        \"\"\"\n\n        batch_size = waveforms.shape[0]\n\n        targets = []\n        mixtures = []\n\n        for n in range(0, batch_size):\n\n            segment = waveforms[n].clone()\n            mixture = segment.clone()\n\n            for i in range(1, self.mix_num):\n\n                next_segment = waveforms[(n + i) % batch_size].clone()\n\n                if self.match_energy:\n                    # Rescale the energy of the next_segment to match the energy of\n                    # the segment\n                    next_segment = rescale_to_match_energy(\n                        segment, next_segment)\n\n                mixture += next_segment\n\n            targets.append(segment)\n            mixtures.append(mixture)\n\n        targets = torch.stack(targets, dim=0)\n        mixtures = torch.stack(mixtures, dim=0)\n\n        return mixtures, targets", "\n\ndef rescale_to_match_energy(\n    segment1: torch.Tensor,\n    segment2: torch.Tensor,\n) -> torch.Tensor:\n    r\"\"\"Rescale segment2 to match the energy of segment1.\n\n    Args:\n        segment1 (torch.Tensor), signal\n        segment2 (torch.Tensor), signal\n    \"\"\"\n\n    ratio = get_energy_ratio(segment1, segment2)\n    recaled_segment2 = segment2 * ratio\n    return recaled_segment2", "\n\ndef get_energy(x: torch.Tensor) -> torch.float:\n    r\"\"\"Calculate the energy of a signal.\"\"\"\n\n    return torch.mean(x ** 2)\n\n\ndef get_energy_ratio(\n    segment1: torch.Tensor,\n    segment2: torch.Tensor,\n    eps=1e-10\n) -> float:\n    r\"\"\"Calculate ratio = sqrt(E(s1) / E(s2)).\"\"\"\n\n    energy1 = get_energy(segment1)\n    energy2 = get_energy(segment2)\n    ratio = (energy1 / max(energy2, eps)) ** 0.5\n    ratio = torch.clamp(ratio, 0.02, 50)\n\n    return ratio", "def get_energy_ratio(\n    segment1: torch.Tensor,\n    segment2: torch.Tensor,\n    eps=1e-10\n) -> float:\n    r\"\"\"Calculate ratio = sqrt(E(s1) / E(s2)).\"\"\"\n\n    energy1 = get_energy(segment1)\n    energy2 = get_energy(segment2)\n    ratio = (energy1 / max(energy2, eps)) ** 0.5\n    ratio = torch.clamp(ratio, 0.02, 50)\n\n    return ratio", ""]}
{"filename": "uss/data/anchor_segment_detectors.py", "chunked_list": ["import random\nfrom typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AnchorSegmentDetector(nn.Module):\n    def __init__(self,\n                 sed_model: nn.Module,\n                 clip_seconds: float,\n                 segment_seconds: float,\n                 frames_per_second: int,\n                 sample_rate: float,\n                 detect_mode: str,\n                 ) -> None:\n        r\"\"\"The anchor segment detector is used to detect 2-second anchor\n        segments from 10-second weakly labelled audio clips during training.\n\n        Args:\n            sed_model (nn.Module): pretrained sound event detection model\n            clip_seconds (float): audio clip duration, e.g., 10.\n            segment_seconds (float): anchor segment duration, e.g., 2.\n            frames_per_second (int):, e.g., 100\n            sample_rate (int)\n            detect_mode (str), \"max_area\" | \"random\"\n\n        Returns:\n            None\n        \"\"\"\n        super(AnchorSegmentDetector, self).__init__()\n\n        assert detect_mode in [\"max_area\", \"random\"]\n\n        self.sed_model = sed_model\n        self.clip_frames = int(clip_seconds * frames_per_second)\n        self.segment_frames = int(segment_seconds * frames_per_second + 1)\n        self.hop_samples = sample_rate // frames_per_second\n        self.sample_rate = sample_rate\n        self.detect_mode = detect_mode\n\n        # Used to the area under the probability curve of anchor segments\n        self.anchor_segment_scorer = AnchorSegmentScorer(\n            segment_frames=self.segment_frames,\n        )\n\n    def __call__(\n        self,\n        waveforms: torch.Tensor,\n        class_ids: List,\n        debug: bool = False,\n    ) -> Dict:\n        r\"\"\"Input a batch of 10-second audio clips. Mine 2-second anchor\n        segments.\n\n        Args:\n            waveforms (torch.Tensor): (batch_size, clip_samples)\n            class_ids (List): (batch_size,)\n            debug (bool)\n\n        Returns:\n            segments_dict (Dict): e.g., {\n                \"waveform\": (batch_size, segment_samples),\n                \"class_id\": (batch_size,),\n                \"bgn_sample\": (batch_size,),\n                \"end_sample\": (batch_size,),\n            }\n        \"\"\"\n\n        batch_size, _ = waveforms.shape\n\n        # Sound event detection\n        with torch.no_grad():\n            self.sed_model.eval()\n\n            framewise_output = self.sed_model(\n                input=waveforms,\n            )['framewise_output']\n            # (batch_size, clip_frames, classes_num)\n\n        segments = []\n        bgn_samples = []\n        end_samples = []\n\n        # Detect the anchor segments of a mini-batch of audio clips one by one\n        for n in range(batch_size):\n\n            # Class ID of the current 10-second clip.\n            class_id = class_ids[n]\n            # There can be multiple tags in the 10-second clip. We only detect\n            # the anchor segment of #class_id\n\n            prob_array = framewise_output[n, :, class_id]\n            # shape: (segment_frames,)\n\n            if self.detect_mode == \"max_area\":\n\n                # The area of the probability curve in anchor segments\n                anchor_segment_scores = self.anchor_segment_scorer(\n                    prob_array=prob_array,\n                )\n\n                anchor_index = torch.argmax(anchor_segment_scores)\n\n                if debug:\n                    _debug_plot_anchor_segment(\n                        waveform=waveforms[0],\n                        anchor_segment_scores=anchor_segment_scores,\n                        class_id=class_id,\n                    )\n\n            elif self.detect_mode == \"random\":\n                anchor_index = random.randint(0, self.segment_frames - 1)\n                anchor_index = torch.tensor(anchor_index).to(waveforms.device)\n\n            else:\n                raise NotImplementedError\n\n            # Get begin and end samples of an anchor segment.\n            bgn_sample, end_sample = self.get_segment_bgn_end_samples(\n                anchor_index=anchor_index,\n                clip_frames=self.clip_frames,\n            )\n\n            segment = waveforms[n, bgn_sample: end_sample]\n\n            segments.append(segment)\n            bgn_samples.append(bgn_sample)\n            end_samples.append(end_sample)\n\n        segments = torch.stack(segments, dim=0)\n        # (batch_size, segment_samples)\n\n        bgn_samples = torch.stack(bgn_samples, dim=0)\n        end_samples = torch.stack(end_samples, dim=0)\n\n        segments_dict = {\n            'waveform': segments,\n            'class_id': class_ids,\n            'bgn_sample': bgn_samples,\n            'end_sample': end_samples,\n        }\n\n        return segments_dict\n\n    def get_segment_bgn_end_samples(\n        self,\n        anchor_index: int,\n        clip_frames: int\n    ) -> Tuple[int, int]:\n        r\"\"\"Get begin and end samples of an anchor segment.\n\n        Args:\n            anchor_index (torch.int): e.g., 155\n\n        Returns:\n            bgn_sample (torch.int), e.g., 17600\n            end_sample (torch.int): e.g., 81600\n        \"\"\"\n\n        anchor_index = torch.clamp(\n            input=anchor_index,\n            min=self.segment_frames // 2,\n            max=clip_frames - self.segment_frames // 2,\n        )\n\n        bgn_frame = anchor_index - self.segment_frames // 2\n        end_frame = anchor_index + self.segment_frames // 2\n\n        bgn_sample = bgn_frame * self.hop_samples\n        end_sample = end_frame * self.hop_samples\n\n        return bgn_sample, end_sample", "class AnchorSegmentDetector(nn.Module):\n    def __init__(self,\n                 sed_model: nn.Module,\n                 clip_seconds: float,\n                 segment_seconds: float,\n                 frames_per_second: int,\n                 sample_rate: float,\n                 detect_mode: str,\n                 ) -> None:\n        r\"\"\"The anchor segment detector is used to detect 2-second anchor\n        segments from 10-second weakly labelled audio clips during training.\n\n        Args:\n            sed_model (nn.Module): pretrained sound event detection model\n            clip_seconds (float): audio clip duration, e.g., 10.\n            segment_seconds (float): anchor segment duration, e.g., 2.\n            frames_per_second (int):, e.g., 100\n            sample_rate (int)\n            detect_mode (str), \"max_area\" | \"random\"\n\n        Returns:\n            None\n        \"\"\"\n        super(AnchorSegmentDetector, self).__init__()\n\n        assert detect_mode in [\"max_area\", \"random\"]\n\n        self.sed_model = sed_model\n        self.clip_frames = int(clip_seconds * frames_per_second)\n        self.segment_frames = int(segment_seconds * frames_per_second + 1)\n        self.hop_samples = sample_rate // frames_per_second\n        self.sample_rate = sample_rate\n        self.detect_mode = detect_mode\n\n        # Used to the area under the probability curve of anchor segments\n        self.anchor_segment_scorer = AnchorSegmentScorer(\n            segment_frames=self.segment_frames,\n        )\n\n    def __call__(\n        self,\n        waveforms: torch.Tensor,\n        class_ids: List,\n        debug: bool = False,\n    ) -> Dict:\n        r\"\"\"Input a batch of 10-second audio clips. Mine 2-second anchor\n        segments.\n\n        Args:\n            waveforms (torch.Tensor): (batch_size, clip_samples)\n            class_ids (List): (batch_size,)\n            debug (bool)\n\n        Returns:\n            segments_dict (Dict): e.g., {\n                \"waveform\": (batch_size, segment_samples),\n                \"class_id\": (batch_size,),\n                \"bgn_sample\": (batch_size,),\n                \"end_sample\": (batch_size,),\n            }\n        \"\"\"\n\n        batch_size, _ = waveforms.shape\n\n        # Sound event detection\n        with torch.no_grad():\n            self.sed_model.eval()\n\n            framewise_output = self.sed_model(\n                input=waveforms,\n            )['framewise_output']\n            # (batch_size, clip_frames, classes_num)\n\n        segments = []\n        bgn_samples = []\n        end_samples = []\n\n        # Detect the anchor segments of a mini-batch of audio clips one by one\n        for n in range(batch_size):\n\n            # Class ID of the current 10-second clip.\n            class_id = class_ids[n]\n            # There can be multiple tags in the 10-second clip. We only detect\n            # the anchor segment of #class_id\n\n            prob_array = framewise_output[n, :, class_id]\n            # shape: (segment_frames,)\n\n            if self.detect_mode == \"max_area\":\n\n                # The area of the probability curve in anchor segments\n                anchor_segment_scores = self.anchor_segment_scorer(\n                    prob_array=prob_array,\n                )\n\n                anchor_index = torch.argmax(anchor_segment_scores)\n\n                if debug:\n                    _debug_plot_anchor_segment(\n                        waveform=waveforms[0],\n                        anchor_segment_scores=anchor_segment_scores,\n                        class_id=class_id,\n                    )\n\n            elif self.detect_mode == \"random\":\n                anchor_index = random.randint(0, self.segment_frames - 1)\n                anchor_index = torch.tensor(anchor_index).to(waveforms.device)\n\n            else:\n                raise NotImplementedError\n\n            # Get begin and end samples of an anchor segment.\n            bgn_sample, end_sample = self.get_segment_bgn_end_samples(\n                anchor_index=anchor_index,\n                clip_frames=self.clip_frames,\n            )\n\n            segment = waveforms[n, bgn_sample: end_sample]\n\n            segments.append(segment)\n            bgn_samples.append(bgn_sample)\n            end_samples.append(end_sample)\n\n        segments = torch.stack(segments, dim=0)\n        # (batch_size, segment_samples)\n\n        bgn_samples = torch.stack(bgn_samples, dim=0)\n        end_samples = torch.stack(end_samples, dim=0)\n\n        segments_dict = {\n            'waveform': segments,\n            'class_id': class_ids,\n            'bgn_sample': bgn_samples,\n            'end_sample': end_samples,\n        }\n\n        return segments_dict\n\n    def get_segment_bgn_end_samples(\n        self,\n        anchor_index: int,\n        clip_frames: int\n    ) -> Tuple[int, int]:\n        r\"\"\"Get begin and end samples of an anchor segment.\n\n        Args:\n            anchor_index (torch.int): e.g., 155\n\n        Returns:\n            bgn_sample (torch.int), e.g., 17600\n            end_sample (torch.int): e.g., 81600\n        \"\"\"\n\n        anchor_index = torch.clamp(\n            input=anchor_index,\n            min=self.segment_frames // 2,\n            max=clip_frames - self.segment_frames // 2,\n        )\n\n        bgn_frame = anchor_index - self.segment_frames // 2\n        end_frame = anchor_index + self.segment_frames // 2\n\n        bgn_sample = bgn_frame * self.hop_samples\n        end_sample = end_frame * self.hop_samples\n\n        return bgn_sample, end_sample", "\n\ndef _debug_plot_anchor_segment(\n    waveform: torch.Tensor,\n    anchor_segment_scores: torch.Tensor,\n    class_id: int,\n) -> None:\n    r\"\"\"For debug only. Plot anchor segment prediction.\"\"\"\n\n    import os\n\n    import matplotlib.pyplot as plt\n    import soundfile\n\n    from uss.config import IX_TO_LB\n    sample_rate = 32000\n\n    n = 0\n    audio_path = os.path.join(\"_debug_anchor_segment{}.wav\".format(n))\n    fig_path = os.path.join(\"_debug_anchor_segment{}.pdf\".format(n))\n\n    soundfile.write(\n        file=audio_path,\n        data=waveform.data.cpu().numpy(),\n        samplerate=sample_rate,\n    )\n    print(\"Write out to {}\".format(audio_path))\n\n    plt.figure()\n    plt.plot(anchor_segment_scores.data.cpu().numpy())\n    plt.title(IX_TO_LB[class_id])\n    plt.ylim(0, 1)\n    plt.savefig(fig_path)\n    print(\"Write out to {}\".format(fig_path))\n\n    os._exit(0)", "\n\nclass AnchorSegmentScorer(nn.Module):\n    def __init__(self,\n                 segment_frames: int,\n                 ) -> None:\n        r\"\"\"Calculate the area under the probabiltiy curve of an anchor segment.\n\n        Args:\n            segment_frames (int)\n\n        Returns:\n            None\n        \"\"\"\n\n        super(AnchorSegmentScorer, self).__init__()\n\n        self.segment_frames = segment_frames\n\n        filter_len = self.segment_frames\n        self.register_buffer(\n            'smooth_filter', torch.ones(\n                1, 1, filter_len) / filter_len)\n\n    def __call__(self, prob_array: torch.Tensor):\n        r\"\"\"Calculate the area under the probabiltiy curve of an anchor segment.\n\n        Args:\n            prob_array (torch.Tensor): (clip_frames,), sound event\n                detection probability of a specific sound class.\n\n        Returns:\n            output: (clip_frames,), smoothed probability, equivalent to the\n                area of probability of anchor segments.\n        \"\"\"\n\n        x = F.pad(\n            input=prob_array[None, None, :],\n            pad=(self.segment_frames // 2, self.segment_frames // 2),\n            mode='replicate'\n        )\n        # shape: (1, 1, clip_frames)\n\n        output = torch.conv1d(\n            input=x,\n            weight=self.smooth_filter,\n            padding=0,\n        )\n        # shape: (1, 1, clip_frames)\n\n        output = output.squeeze(dim=(0, 1))\n        # (clip_frames,)\n\n        return output", ""]}
{"filename": "uss/models/query_nets.py", "chunked_list": ["from typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom uss.config import panns_paths_dict\nfrom uss.models.base import init_layer\nfrom uss.utils import get_path, load_pretrained_panns\n", "from uss.utils import get_path, load_pretrained_panns\n\n\ndef initialize_query_net(configs):\n    r\"\"\"Initialize query net.\n\n    Args:\n        configs (Dict)\n\n    Returns:\n        model (nn.Module)\n    \"\"\"\n\n    model_type = configs[\"query_net\"][\"model_type\"]\n    bottleneck_type = configs[\"query_net\"][\"bottleneck_type\"]\n    # base_checkpoint_path = configs[\"query_net\"][\"base_checkpoint_path\"]\n    base_checkpoint_type = configs[\"query_net\"][\"base_checkpoint_type\"]\n    freeze_base = configs[\"query_net\"][\"freeze_base\"]\n    outputs_num = configs[\"query_net\"][\"outputs_num\"]\n\n    base_checkpoint_path = get_path(panns_paths_dict[base_checkpoint_type])\n\n    if model_type == \"Cnn14_Wrapper\":\n\n        model = Cnn14_Wrapper(\n            bottleneck_type=bottleneck_type,\n            base_checkpoint_path=base_checkpoint_path,\n            freeze_base=freeze_base,\n        )\n\n    elif model_type == \"AdaptiveCnn14_Wrapper\":\n\n        model = AdaptiveCnn14_Wrapper(\n            bottleneck_type=bottleneck_type,\n            base_checkpoint_path=base_checkpoint_path,\n            freeze_base=freeze_base,\n            freeze_adaptor=configs[\"query_net\"][\"freeze_adaptor\"],\n            outputs_num=outputs_num,\n        )\n\n    elif model_type == \"YourOwn_QueryNet\":\n        model = YourOwn_QueryNet(outputs_num=outputs_num)\n\n    else:\n        raise NotImplementedError\n\n    return model", "\n\ndef get_panns_bottleneck_type(bottleneck_type: str) -> str:\n    r\"\"\"Get PANNs bottleneck name.\n\n    Args:\n        bottleneck_type (str)\n\n    Returns:\n        panns_bottleneck_type (str)\n    \"\"\"\n\n    if bottleneck_type == \"at_soft\":\n        panns_bottleneck_type = \"clipwise_output\"\n\n    else:\n        panns_bottleneck_type = bottleneck_type\n\n    return panns_bottleneck_type", "\n\nclass Cnn14_Wrapper(nn.Module):\n    def __init__(self,\n                 bottleneck_type: str,\n                 base_checkpoint_path: str,\n                 freeze_base: bool,\n                 ) -> None:\n        r\"\"\"Query Net based on Cnn14 of PANNs. There are no extra learnable\n        parameters.\n\n        Args:\n            bottleneck_type (str), \"at_soft\" | \"embedding\"\n            base_checkpoint_path (str), Cnn14 checkpoint path\n            freeze_base (bool), whether to freeze the parameters of the Cnn14\n        \"\"\"\n\n        super(Cnn14_Wrapper, self).__init__()\n\n        self.panns_bottleneck_type = get_panns_bottleneck_type(bottleneck_type)\n\n        self.base = load_pretrained_panns(\n            model_type=\"Cnn14\",\n            checkpoint_path=base_checkpoint_path,\n            freeze=freeze_base,\n        )\n\n        self.freeze_base = freeze_base\n\n    def forward_base(self, source: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Forward a source into a the base part of the query net.\n\n        Args:\n            source (torch.Tensor), (batch_size, audio_samples)\n\n        Returns:\n            bottleneck (torch.Tensor), (bottleneck_dim,)\n        \"\"\"\n\n        if self.freeze_base:\n            self.base.eval()\n            with torch.no_grad():\n                base_output_dict = self.base(source)\n        else:\n            self.base.train()\n            base_output_dict = self.base(source)\n\n        bottleneck = base_output_dict[self.panns_bottleneck_type]\n\n        return bottleneck\n\n    def forward_adaptor(self, bottleneck: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Forward a bottleneck into a the adaptor part of the query net.\n\n        Args:\n            bottleneck (torch.Tensor), (bottleneck_dim,)\n\n        Returns:\n            output (torch.Tensor), (output_dim,)\n        \"\"\"\n\n        output = bottleneck\n\n        return output\n\n    def forward(self, source: torch.Tensor) -> Dict:\n        r\"\"\"Forward a source into a query net.\n\n        Args:\n            source (torch.Tensor), (batch_size, audio_samples)\n\n        Returns:\n            output_dict (Dict), {\n                \"bottleneck\": (bottleneck_dim,)\n                \"output\": (output_dim,)\n            }\n        \"\"\"\n\n        bottleneck = self.forward_base(source=source)\n\n        output = self.forward_adaptor(bottleneck=bottleneck)\n\n        output_dict = {\n            \"bottleneck\": bottleneck,\n            \"output\": output,\n        }\n\n        return output_dict", "\n\nclass AdaptiveCnn14_Wrapper(nn.Module):\n    def __init__(self,\n                 bottleneck_type: str,\n                 base_checkpoint_path: str,\n                 freeze_base: bool,\n                 freeze_adaptor: bool,\n                 outputs_num: int,\n                 ) -> None:\n        r\"\"\"Query Net based on Cnn14 of PANNs. There are no extra learnable\n        parameters.\n\n        Args:\n            bottleneck_type (str), \"at_soft\" | \"embedding\"\n            base_checkpoint_path (str), Cnn14 checkpoint path\n            freeze_base (bool), whether to freeze the parameters of the Cnn14\n            freeze_adaptor (bool), whether to freeze the parameters of the\n                adaptor\n            outputs_num (int), output dimension\n        \"\"\"\n\n        super(AdaptiveCnn14_Wrapper, self).__init__()\n\n        self.freeze_base = freeze_base\n\n        self.panns_bottleneck_type = get_panns_bottleneck_type(bottleneck_type)\n\n        self.base = load_pretrained_panns(\n            model_type=\"Cnn14\",\n            checkpoint_path=base_checkpoint_path,\n            freeze=freeze_base,\n        )\n\n        bottleneck_units = self._get_bottleneck_units(\n            self.panns_bottleneck_type)\n\n        self.fc1 = nn.Linear(bottleneck_units, 2048)\n        self.fc2 = nn.Linear(2048, outputs_num)\n\n        if freeze_adaptor:\n            for param in self.fc1.parameters():\n                param.requires_grad = False\n\n            for param in self.fc2.parameters():\n                param.requires_grad = False\n\n        self.init_weights()\n\n    def _get_bottleneck_units(self, panns_bottleneck_type) -> int:\n\n        if panns_bottleneck_type == \"embedding\":\n            bottleneck_hid_units = self.base.fc_audioset.in_features\n\n        elif panns_bottleneck_type == \"clipwise_output\":\n            bottleneck_hid_units = self.base.fc_audioset.out_features\n\n        else:\n            raise NotImplementedError\n\n        return bottleneck_hid_units\n\n    def init_weights(self):\n        r\"\"\"Initialize weights.\"\"\"\n        init_layer(self.fc1)\n        init_layer(self.fc2)\n\n    def forward_base(self, source: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Forward a source into a the base part of the query net.\n\n        Args:\n            source (torch.Tensor), (batch_size, audio_samples)\n\n        Returns:\n            bottleneck (torch.Tensor), (bottleneck_dim,)\n        \"\"\"\n\n        if self.freeze_base:\n            self.base.eval()\n            with torch.no_grad():\n                base_output_dict = self.base(source)\n        else:\n            self.base.train()\n            base_output_dict = self.base(source)\n\n        bottleneck = base_output_dict[self.panns_bottleneck_type]\n\n        return bottleneck\n\n    def forward_adaptor(self, bottleneck: torch.Tensor) -> torch.Tensor:\n        r\"\"\"Forward a bottleneck into a the adaptor part of the query net.\n\n        Args:\n            bottleneck (torch.Tensor), (bottleneck_dim,)\n\n        Returns:\n            output (torch.Tensor), (output_dim,)\n        \"\"\"\n\n        x = F.leaky_relu(self.fc1(bottleneck), negative_slope=0.01)\n        x = F.dropout(x, p=0.5, training=self.training, inplace=True)\n\n        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n        output = F.dropout(x, p=0.5, training=self.training, inplace=True)\n\n        return output\n\n    def forward(self, source: torch.Tensor) -> Dict:\n        r\"\"\"Forward a source into a query net.\n\n        Args:\n            source (torch.Tensor), (batch_size, audio_samples)\n\n        Returns:\n            output_dict (Dict), {\n                \"bottleneck\": (bottleneck_dim,)\n                \"output\": (output_dim,)\n            }\n        \"\"\"\n\n        bottleneck = self.forward_base(source=source)\n\n        output = self.forward_adaptor(bottleneck=bottleneck)\n\n        output_dict = {\n            \"bottleneck\": bottleneck,\n            \"output\": output,\n        }\n\n        return output_dict", "\n\nclass YourOwn_QueryNet(nn.Module):\n    def __init__(self, outputs_num: int) -> None:\n        r\"\"\"User defined query net.\"\"\"\n\n        super(YourOwn_QueryNet, self).__init__()\n\n        self.fc1 = nn.Linear(1, outputs_num)\n\n    def forward(self, source: torch.Tensor) -> Dict:\n        r\"\"\"Forward a source into a query net.\n\n        Args:\n            source (torch.Tensor), (batch_size, audio_samples)\n\n        Returns:\n            output_dict (Dict), {\n                \"bottleneck\": (bottleneck_dim,)\n                \"output\": (output_dim,)\n            }\n        \"\"\"\n\n        x = torch.mean(source, dim=-1, keepdim=True)\n        bottleneck = self.fc1(x)\n\n        output_dict = {\n            \"bottleneck\": bottleneck,\n            \"output\": bottleneck,\n        }\n\n        return output_dict", ""]}
{"filename": "uss/models/base.py", "chunked_list": ["import math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchlibrosa.stft import magphase\n\n\ndef init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.0)", "\ndef init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.xavier_uniform_(layer.weight)\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.0)\n\n\ndef init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.0)\n    bn.weight.data.fill_(1.0)", "\n\ndef init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.0)\n    bn.weight.data.fill_(1.0)\n\n\ndef init_embedding(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.uniform_(layer.weight, -1., 1.)\n\n    if hasattr(layer, 'bias'):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)", "def init_embedding(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.uniform_(layer.weight, -1., 1.)\n\n    if hasattr(layer, 'bias'):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_gru(rnn):\n    \"\"\"Initialize a GRU layer. \"\"\"\n\n    def _concat_init(tensor, init_funcs):\n        (length, fan_out) = tensor.shape\n        fan_in = length // len(init_funcs)\n\n        for (i, init_func) in enumerate(init_funcs):\n            init_func(tensor[i * fan_in: (i + 1) * fan_in, :])\n\n    def _inner_uniform(tensor):\n        fan_in = nn.init._calculate_correct_fan(tensor, \"fan_in\")\n        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n\n    for i in range(rnn.num_layers):\n        _concat_init(\n            getattr(rnn, \"weight_ih_l{}\".format(i)),\n            [_inner_uniform, _inner_uniform, _inner_uniform],\n        )\n        torch.nn.init.constant_(getattr(rnn, \"bias_ih_l{}\".format(i)), 0)\n\n        _concat_init(\n            getattr(rnn, \"weight_hh_l{}\".format(i)),\n            [_inner_uniform, _inner_uniform, nn.init.orthogonal_],\n        )\n        torch.nn.init.constant_(getattr(rnn, \"bias_hh_l{}\".format(i)), 0)", "\ndef init_gru(rnn):\n    \"\"\"Initialize a GRU layer. \"\"\"\n\n    def _concat_init(tensor, init_funcs):\n        (length, fan_out) = tensor.shape\n        fan_in = length // len(init_funcs)\n\n        for (i, init_func) in enumerate(init_funcs):\n            init_func(tensor[i * fan_in: (i + 1) * fan_in, :])\n\n    def _inner_uniform(tensor):\n        fan_in = nn.init._calculate_correct_fan(tensor, \"fan_in\")\n        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n\n    for i in range(rnn.num_layers):\n        _concat_init(\n            getattr(rnn, \"weight_ih_l{}\".format(i)),\n            [_inner_uniform, _inner_uniform, _inner_uniform],\n        )\n        torch.nn.init.constant_(getattr(rnn, \"bias_ih_l{}\".format(i)), 0)\n\n        _concat_init(\n            getattr(rnn, \"weight_hh_l{}\".format(i)),\n            [_inner_uniform, _inner_uniform, nn.init.orthogonal_],\n        )\n        torch.nn.init.constant_(getattr(rnn, \"bias_hh_l{}\".format(i)), 0)", "\n\ndef act(x, activation):\n    if activation == \"relu\":\n        return F.relu_(x)\n\n    elif activation == \"leaky_relu\":\n        return F.leaky_relu_(x, negative_slope=0.01)\n\n    elif activation == \"swish\":\n        return x * torch.sigmoid(x)\n\n    else:\n        raise Exception(\"Incorrect activation!\")", "\n\nclass Base:\n    def __init__(self):\n        pass\n\n    def spectrogram(self, input, eps=0.):\n        (real, imag) = self.stft(input)\n        return torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n\n    def spectrogram_phase(self, input, eps=0.):\n        (real, imag) = self.stft(input)\n        mag = torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n        cos = real / mag\n        sin = imag / mag\n        return mag, cos, sin\n\n    def wav_to_spectrogram_phase(self, input, eps=1e-10):\n        \"\"\"Waveform to spectrogram.\n\n        Args:\n          input: (batch_size, segment_samples, channels_num)\n\n        Outputs:\n          output: (batch_size, channels_num, time_steps, freq_bins)\n        \"\"\"\n        sp_list = []\n        cos_list = []\n        sin_list = []\n        channels_num = input.shape[1]\n        for channel in range(channels_num):\n            mag, cos, sin = self.spectrogram_phase(\n                input[:, channel, :], eps=eps)\n            sp_list.append(mag)\n            cos_list.append(cos)\n            sin_list.append(sin)\n\n        sps = torch.cat(sp_list, dim=1)\n        coss = torch.cat(cos_list, dim=1)\n        sins = torch.cat(sin_list, dim=1)\n        return sps, coss, sins\n\n    def wav_to_spectrogram(self, input, eps=0.):\n        \"\"\"Waveform to spectrogram.\n\n        Args:\n          input: (batch_size, segment_samples, channels_num)\n\n        Outputs:\n          output: (batch_size, channels_num, time_steps, freq_bins)\n        \"\"\"\n        sp_list = []\n        channels_num = input.shape[1]\n        for channel in range(channels_num):\n            sp_list.append(self.spectrogram(input[:, channel, :], eps=eps))\n\n        output = torch.cat(sp_list, dim=1)\n        return output\n\n    def spectrogram_to_wav(self, input, spectrogram, length=None):\n        \"\"\"Spectrogram to waveform.\n\n        Args:\n          input: (batch_size, segment_samples, channels_num)\n          spectrogram: (batch_size, channels_num, time_steps, freq_bins)\n\n        Outputs:\n          output: (batch_size, segment_samples, channels_num)\n        \"\"\"\n        channels_num = input.shape[1]\n        wav_list = []\n        for channel in range(channels_num):\n            (real, imag) = self.stft(input[:, channel, :])\n            (_, cos, sin) = magphase(real, imag)\n            wav_list.append(self.istft(spectrogram[:,\n                                                   channel: channel + 1,\n                                                   :,\n                                                   :] * cos,\n                                       spectrogram[:,\n                                                   channel: channel + 1,\n                                                   :,\n                                                   :] * sin,\n                                       length))\n\n        output = torch.stack(wav_list, dim=1)\n        return output", ""]}
{"filename": "uss/models/film.py", "chunked_list": ["from typing import Dict, List\n\nimport torch\nimport torch.nn as nn\n\nfrom uss.models.base import init_layer\n\n\ndef get_film_meta(module: nn.Module) -> Dict:\n    r\"\"\"Get FiLM meta dict of a module.\n\n    Args:\n        module (nn.Module), the module to extract meta dict\n\n    Returns:\n        film_meta (Dict), FiLM meta dict\n    \"\"\"\n\n    film_meta = {}\n\n    if hasattr(module, 'has_film'):\\\n\n        if module.has_film:\n            film_meta['beta1'] = module.bn1.num_features\n            film_meta['beta2'] = module.bn2.num_features\n        else:\n            film_meta['beta1'] = 0\n            film_meta['beta2'] = 0\n\n    # Pre-order traversal of modules\n    for child_name, child_module in module.named_children():\n\n        child_meta = get_film_meta(child_module)\n\n        if len(child_meta) > 0:\n            film_meta[child_name] = child_meta\n\n    return film_meta", "def get_film_meta(module: nn.Module) -> Dict:\n    r\"\"\"Get FiLM meta dict of a module.\n\n    Args:\n        module (nn.Module), the module to extract meta dict\n\n    Returns:\n        film_meta (Dict), FiLM meta dict\n    \"\"\"\n\n    film_meta = {}\n\n    if hasattr(module, 'has_film'):\\\n\n        if module.has_film:\n            film_meta['beta1'] = module.bn1.num_features\n            film_meta['beta2'] = module.bn2.num_features\n        else:\n            film_meta['beta1'] = 0\n            film_meta['beta2'] = 0\n\n    # Pre-order traversal of modules\n    for child_name, child_module in module.named_children():\n\n        child_meta = get_film_meta(child_module)\n\n        if len(child_meta) > 0:\n            film_meta[child_name] = child_meta\n\n    return film_meta", "\n\nclass FiLM(nn.Module):\n    def __init__(\n        self,\n        film_meta: Dict,\n        condition_size: int,\n    ) -> None:\n        r\"\"\"Create FiLM modules from film meta dict.\n\n        Args:\n            film_meta (Dict), e.g.,\n                {'encoder_block1': {'conv_block1': {'beta1': 32, 'beta2': 32}},\n                 ...}\n            condition_size: int\n\n        Returns:\n            None\n        \"\"\"\n\n        super(FiLM, self).__init__()\n\n        self.condition_size = condition_size\n\n        self.modules, _ = self._create_film_modules(\n            film_meta=film_meta,\n            prefix_names=[],\n        )\n\n    def _create_film_modules(\n        self,\n        film_meta: Dict,\n        prefix_names: List[str],\n    ):\n        r\"\"\"Create FiLM modules.\n\n        Args:\n            film_meta (Dict), e.g.,\n                {\"encoder_block1\": {\"conv_block1\": {\"beta1\": 32, \"beta2\": 32}},\n                 ...}\n            prefix_names (str), only used to get correct module name, e.g.,\n                [\"encoder_block1\", \"conv_block1\"]\n        \"\"\"\n\n        modules = {}\n\n        # Pre-order traversal of modules\n        for module_name, value in film_meta.items():\n\n            if isinstance(value, dict):\n\n                prefix_names.append(module_name)\n\n                modules[module_name], _ = self._create_film_modules(\n                    film_meta=value,\n                    prefix_names=prefix_names,\n                )\n\n            elif isinstance(value, int):\n\n                prefix_names.append(module_name)\n                unique_module_name = '->'.join(prefix_names)\n\n                modules[module_name] = self._add_film_layer_to_module(\n                    num_features=value,\n                    unique_module_name=unique_module_name,\n                )\n\n            prefix_names.pop()\n\n        return modules, prefix_names\n\n    def _add_film_layer_to_module(\n        self,\n        num_features: int,\n        unique_module_name: str,\n    ) -> nn.Module:\n        r\"\"\"Add a FiLM layer.\"\"\"\n\n        layer = nn.Linear(self.condition_size, num_features)\n        init_layer(layer)\n        self.add_module(name=unique_module_name, module=layer)\n\n        return layer\n\n    def _calculate_film_data(self, conditions, modules):\n\n        film_data = {}\n\n        # Pre-order traversal of modules\n        for module_name, module in modules.items():\n\n            if isinstance(module, dict):\n                film_data[module_name] = self._calculate_film_data(\n                    conditions, module)\n\n            elif isinstance(module, nn.Module):\n                film_data[module_name] = module(conditions)[:, :, None, None]\n\n        return film_data\n\n    def forward(self, conditions: torch.Tensor) -> Dict:\n        r\"\"\"Forward conditions to all FiLM layers to get FiLM data.\n\n        Args:\n            conditions (torch.Tensor): query net outputs,\n                (batch_size, condition_dim)\n\n        Returns:\n            film_dict (Dict): e.g., {\n                \"encoder_block1\": {\n                    \"conv_block1\": {\n                        \"beta1\": (16, 32, 1, 1),\n                        \"beta2\": (16, 32, 1, 1),\n                    },\n                    ...,\n                },\n                ...,\n            }\n        \"\"\"\n\n        film_dict = self._calculate_film_data(\n            conditions=conditions,\n            modules=self.modules,\n        )\n\n        return film_dict", ""]}
{"filename": "uss/models/resunet.py", "chunked_list": ["from typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom torchlibrosa.stft import ISTFT, STFT, magphase\n\nfrom uss.models.base import Base, init_bn, init_layer", "\nfrom uss.models.base import Base, init_bn, init_layer\nfrom uss.models.film import FiLM, get_film_meta\n\n\nclass ConvBlockRes(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Tuple,\n        momentum: float,\n        has_film: bool,\n    ) -> None:\n        r\"\"\"Residual convolutional block.\"\"\"\n\n        super(ConvBlockRes, self).__init__()\n\n        padding = [kernel_size[0] // 2, kernel_size[1] // 2]\n\n        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=(1, 1),\n            dilation=(1, 1),\n            padding=padding,\n            bias=False,\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=(1, 1),\n            dilation=(1, 1),\n            padding=padding,\n            bias=False,\n        )\n\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=(1, 1),\n                stride=(1, 1),\n                padding=(0, 0),\n            )\n            self.is_shortcut = True\n        else:\n            self.is_shortcut = False\n\n        self.has_film = has_film\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        r\"\"\"Initialize weights.\"\"\"\n\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n\n        if self.is_shortcut:\n            init_layer(self.shortcut)\n\n    def forward(self,\n                input_tensor: torch.Tensor,\n                film_dict: Dict\n                ) -> torch.Tensor:\n        r\"\"\"Forward input feature maps to the encoder block.\n\n        Args:\n            input_tensor (torch.Tensor), (B, C, T, F)\n            film_dict (Dict)\n\n        Returns:\n            output (torch.Tensor), (B, C, T, F)\n        \"\"\"\n\n        b1 = film_dict['beta1']\n        b2 = film_dict['beta2']\n\n        x = self.conv1(\n            F.leaky_relu_(\n                self.bn1(input_tensor) + b1,\n                negative_slope=0.01))\n        x = self.conv2(F.leaky_relu_(self.bn2(x) + b2, negative_slope=0.01))\n\n        if self.is_shortcut:\n            output = self.shortcut(input_tensor) + x\n        else:\n            output = input_tensor + x\n\n        return output", "\n\nclass EncoderBlockRes1B(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Tuple,\n        downsample: Tuple,\n        momentum: float,\n        has_film: bool,\n    ) -> None:\n        r\"\"\"Encoder block.\"\"\"\n\n        super(EncoderBlockRes1B, self).__init__()\n\n        self.conv_block1 = ConvBlockRes(\n            in_channels, out_channels, kernel_size, momentum, has_film,\n        )\n        self.downsample = downsample\n\n    def forward(self,\n                input_tensor: torch.Tensor,\n                film_dict: Dict\n                ) -> torch.Tensor:\n        r\"\"\"Forward input feature maps to the encoder block.\n\n        Args:\n            input_tensor (torch.Tensor), (B, C_in, T, F)\n            film_dict (Dict)\n\n        Returns:\n            encoder (torch.Tensor): (B, C_out, T, F)\n            encoder_pool (torch.Tensor): (B, C_out, T / downsample, F / downsample)\n        \"\"\"\n\n        encoder = self.conv_block1(input_tensor, film_dict['conv_block1'])\n\n        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n\n        return encoder_pool, encoder", "\n\nclass DecoderBlockRes1B(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Tuple,\n        upsample: Tuple,\n        momentum: float,\n        has_film: bool,\n    ):\n        r\"\"\"Decoder block.\"\"\"\n\n        super(DecoderBlockRes1B, self).__init__()\n\n        self.kernel_size = kernel_size\n        self.stride = upsample\n\n        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n        self.bn2 = nn.BatchNorm2d(in_channels, momentum=momentum)\n        # Do not delate the dummy self.bn2. FiLM need self.bn2 to parse the\n        # FiLM meta correctly.\n\n        self.conv1 = torch.nn.ConvTranspose2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=self.stride,\n            stride=self.stride,\n            padding=(0, 0),\n            bias=False,\n            dilation=(1, 1),\n        )\n\n        self.conv_block2 = ConvBlockRes(\n            in_channels=out_channels * 2,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            momentum=momentum,\n            has_film=has_film,\n        )\n\n        self.has_film = has_film\n\n        self.init_weights()\n\n    def init_weights(self):\n        r\"\"\"Initialize weights.\"\"\"\n        init_bn(self.bn1)\n        init_layer(self.conv1)\n\n    def forward(\n        self,\n        input_tensor: torch.Tensor,\n        concat_tensor: torch.Tensor,\n        film_dict: Dict,\n    ) -> torch.Tensor:\n        r\"\"\"Forward input feature maps to the decoder block.\n\n        Args:\n            input_tensor (torch.Tensor), (B, C_in, T, F)\n            film_dict (Dict)\n\n        Returns:\n            output (torch.Tensor): (B, C_out, T * upsample, F * upsample)\n        \"\"\"\n\n        b1 = film_dict['beta1']\n\n        x = self.conv1(F.leaky_relu_(self.bn1(input_tensor) + b1))\n        # (B, C_out, T * upsample, F * upsample)\n\n        x = torch.cat((x, concat_tensor), dim=1)\n        # (B, C_out * 2, T * upsample, F * upsample)\n\n        output = self.conv_block2(x, film_dict['conv_block2'])\n        # output: (B, C_out, T * upsample, F * upsample)\n\n        return output", "\n\nclass ResUNet30_Base(nn.Module, Base):\n    def __init__(self,\n                 input_channels: int,\n                 output_channels: int,\n                 ) -> None:\n        r\"\"\"Base separation model.\n\n        Args:\n            input_channels (int), audio channels, e.g., 1 | 2\n            output_channels (int), audio channels, e.g., 1 | 2\n        \"\"\"\n\n        super(ResUNet30_Base, self).__init__()\n\n        window_size = 2048\n        hop_size = 320\n        center = True\n        pad_mode = \"reflect\"\n        window = \"hann\"\n        momentum = 0.01\n\n        self.output_channels = output_channels\n\n        self.K = 3  # mag, cos, sin\n\n        # This number equals 2^{#encoder_blcoks}\n        self.time_downsample_ratio = 2 ** 5\n\n        self.stft = STFT(\n            n_fft=window_size,\n            hop_length=hop_size,\n            win_length=window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True,\n        )\n\n        self.istft = ISTFT(\n            n_fft=window_size,\n            hop_length=hop_size,\n            win_length=window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True,\n        )\n\n        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n\n        self.pre_conv = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=32,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=(0, 0),\n            bias=True,\n        )\n\n        self.encoder_block1 = EncoderBlockRes1B(\n            in_channels=32,\n            out_channels=32,\n            kernel_size=(3, 3),\n            downsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.encoder_block2 = EncoderBlockRes1B(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=(3, 3),\n            downsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.encoder_block3 = EncoderBlockRes1B(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=(3, 3),\n            downsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.encoder_block4 = EncoderBlockRes1B(\n            in_channels=128,\n            out_channels=256,\n            kernel_size=(3, 3),\n            downsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.encoder_block5 = EncoderBlockRes1B(\n            in_channels=256,\n            out_channels=384,\n            kernel_size=(3, 3),\n            downsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.encoder_block6 = EncoderBlockRes1B(\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            downsample=(1, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.conv_block7a = EncoderBlockRes1B(\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            downsample=(1, 1),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block1 = DecoderBlockRes1B(\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            upsample=(1, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block2 = DecoderBlockRes1B(\n            in_channels=384,\n            out_channels=384,\n            kernel_size=(3, 3),\n            upsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block3 = DecoderBlockRes1B(\n            in_channels=384,\n            out_channels=256,\n            kernel_size=(3, 3),\n            upsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block4 = DecoderBlockRes1B(\n            in_channels=256,\n            out_channels=128,\n            kernel_size=(3, 3),\n            upsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block5 = DecoderBlockRes1B(\n            in_channels=128,\n            out_channels=64,\n            kernel_size=(3, 3),\n            upsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n        self.decoder_block6 = DecoderBlockRes1B(\n            in_channels=64,\n            out_channels=32,\n            kernel_size=(3, 3),\n            upsample=(2, 2),\n            momentum=momentum,\n            has_film=True,\n        )\n\n        self.after_conv = nn.Conv2d(\n            in_channels=32,\n            out_channels=output_channels * self.K,\n            kernel_size=(1, 1),\n            stride=(1, 1),\n            padding=(0, 0),\n            bias=True,\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        r\"\"\"Initialize weights.\"\"\"\n        init_bn(self.bn0)\n        init_layer(self.pre_conv)\n        init_layer(self.after_conv)\n\n    def feature_maps_to_wav(\n        self,\n        input_tensor: torch.Tensor,\n        sp: torch.Tensor,\n        sin_in: torch.Tensor,\n        cos_in: torch.Tensor,\n        audio_length: int,\n    ) -> torch.Tensor:\n        r\"\"\"Convert feature maps to waveform.\n\n        Args:\n            input_tensor: (B, input_channels, T, F)\n            sp: (B, output_channels, T, F)\n            sin_in: (B, output_channels, T, F)\n            cos_in: (B, output_channels, T, F)\n\n            (There is input_channels == output_channels for the source separation task.)\n\n        Outputs:\n            waveform: (B, output_channels, audio_samples)\n        \"\"\"\n\n        x = rearrange(\n            input_tensor,\n            'b (c k) t f -> b c k t f',\n            c=self.output_channels)\n\n        mask_mag = torch.sigmoid(x[:, :, 0, :, :])\n        mask_real = torch.tanh(x[:, :, 1, :, :])\n        mask_imag = torch.tanh(x[:, :, 2, :, :])\n\n        _, mask_cos, mask_sin = magphase(mask_real, mask_imag)\n        # mask_cos, mask_sin: (B, output_channels, T, F)\n\n        # Y = |Y|cos\u2220Y + j|Y|sin\u2220Y\n        #   = |Y|cos(\u2220X + \u2220M) + j|Y|sin(\u2220X + \u2220M)\n        #   = |Y|(cos\u2220X cos\u2220M - sin\u2220X sin\u2220M) + j|Y|(sin\u2220X cos\u2220M + cos\u2220X sin\u2220M)\n        out_cos = (\n            cos_in * mask_cos - sin_in * mask_sin\n        )\n        out_sin = (\n            sin_in * mask_cos + cos_in * mask_sin\n        )\n        # out_cos: (B, output_channels, T, F)\n        # out_sin: (B, output_channels, T, F)\n\n        # Calculate |Y|.\n        out_mag = F.relu_(sp * mask_mag)\n        # out_mag: (B, output_channels, T, F)\n\n        # Calculate Y_{real} and Y_{imag} for ISTFT.\n        out_real = out_mag * out_cos\n        out_imag = out_mag * out_sin\n        # out_real, out_imag: (B, output_channels, T, F)\n\n        # Reshape to (N, 1, T, F) for ISTFT\n        out_real = rearrange(out_real, 'b c t f -> (b c) t f').unsqueeze(1)\n        out_imag = rearrange(out_imag, 'b c t f -> (b c) t f').unsqueeze(1)\n\n        # ISTFT\n        x = self.istft(out_real, out_imag, audio_length)\n        # (B * output_channels, audio_samples)\n\n        # Reshape to (B, output_channels, audio_samples)\n        waveform = rearrange(x, '(b c) t -> b c t', c=self.output_channels)\n\n        return waveform\n\n    def forward(self, mixtures, film_dict):\n        r\"\"\"Forward mixtures and conditions to separate target sources.\n\n        Args:\n            input (torch.Tensor): (batch_size, output_channels, segment_samples)\n\n        Outputs:\n            output_dict: {\n                \"waveform\": (batch_size, output_channels, segment_samples),\n            }\n        \"\"\"\n\n        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(mixtures)\n        x = mag\n\n        # Batch normalization\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)   # shape: (B, input_channels, T, F)\n\n        # Pad spectrogram to be evenly divided by downsample ratio\n        origin_len = x.shape[2]\n        pad_len = (int(np.ceil(x.shape[2] /\n                               self.time_downsample_ratio)) *\n                   self.time_downsample_ratio -\n                   origin_len)\n        x = F.pad(x, pad=(0, 0, 0, pad_len))\n        # x: (B, input_channels, T, F)\n\n        # Let frequency bins be evenly divided by 2, e.g., 513 -> 512\n        x = x[..., 0: x.shape[-1] - 1]  # (B, input_channels, T, F)\n\n        # UNet\n        x = self.pre_conv(x)\n\n        x1_pool, x1 = self.encoder_block1(\n            x, film_dict['encoder_block1'])  # x1_pool: (B, 32, T / 2, F / 2)\n        x2_pool, x2 = self.encoder_block2(\n            x1_pool, film_dict['encoder_block2'])  # x2_pool: (B, 64, T / 4, F / 4)\n        x3_pool, x3 = self.encoder_block3(\n            x2_pool, film_dict['encoder_block3'])  # x3_pool: (B, 128, T / 8, F / 8)\n        # x4_pool: (B, 256, T / 16, F / 16)\n        x4_pool, x4 = self.encoder_block4(x3_pool, film_dict['encoder_block4'])\n        # x5_pool: (B, 384, T / 32, F / 32)\n        x5_pool, x5 = self.encoder_block5(x4_pool, film_dict['encoder_block5'])\n        # x6_pool: (B, 384, T / 32, F / 64)\n        x6_pool, x6 = self.encoder_block6(x5_pool, film_dict['encoder_block6'])\n        x_center, _ = self.conv_block7a(\n            x6_pool, film_dict['conv_block7a'])  # (B, 384, T / 32, F / 64)\n        # (B, 384, T / 32, F / 32)\n        x7 = self.decoder_block1(x_center, x6, film_dict['decoder_block1'])\n        # (B, 384, T / 16, F / 16)\n        x8 = self.decoder_block2(x7, x5, film_dict['decoder_block2'])\n        x9 = self.decoder_block3(\n            x8, x4, film_dict['decoder_block3'])  # (B, 256, T / 8, F / 8)\n        x10 = self.decoder_block4(\n            x9, x3, film_dict['decoder_block4'])  # (B, 128, T / 4, F / 4)\n        x11 = self.decoder_block5(\n            x10, x2, film_dict['decoder_block5'])  # (B, 64, T / 2, F / 2)\n        x12 = self.decoder_block6(\n            x11, x1, film_dict['decoder_block6'])  # (B, 32, T, F)\n\n        x = self.after_conv(x12)\n\n        # Recover shape\n        x = F.pad(x, pad=(0, 1))\n        x = x[:, :, 0:origin_len, :]\n\n        audio_length = mixtures.shape[2]\n\n        # Convert feature maps to waveform\n        separated_audio = self.feature_maps_to_wav(\n            input_tensor=x,\n            sp=mag,\n            sin_in=sin_in,\n            cos_in=cos_in,\n            audio_length=audio_length,\n        )\n        # shape:\uff08B, output_channels, segment_samples)\n\n        output_dict = {'waveform': separated_audio}\n\n        return output_dict", "\n\nclass ResUNet30(nn.Module):\n    def __init__(self,\n                 input_channels: int,\n                 output_channels: int,\n                 condition_size: int,\n                 ) -> None:\n        r\"\"\"Universal separation model.\n\n        Args:\n            input_channels (int), audio channels, e.g., 1 | 2\n            output_channels (int), audio channels, e.g., 1 | 2\n            condition_size (int), FiLM condition size, e.g., 527 | 2048\n        \"\"\"\n\n        super(ResUNet30, self).__init__()\n\n        self.base = ResUNet30_Base(\n            input_channels=input_channels,\n            output_channels=output_channels,\n        )\n\n        self.film_meta = get_film_meta(\n            module=self.base,\n        )\n\n        self.film = FiLM(\n            film_meta=self.film_meta,\n            condition_size=condition_size\n        )\n\n    def forward(self, input_dict: Dict) -> Dict:\n        r\"\"\"Forward mixtures and conditions to separate target sources.\n\n        Args:\n            input_dict (Dict): {\n                \"mixture\": (batch_size, audio_channels, audio_samples),\n                \"condition\": (batch_size, condition_dim),\n            }\n\n        Returns:\n            output_dict (Dict): {\n                \"waveform\": (batch_size, audio_channels, audio_samples)\n            }\n        \"\"\"\n\n        mixtures = input_dict['mixture']\n        conditions = input_dict['condition']\n\n        film_dict = self.film(\n            conditions=conditions,\n        )\n\n        output_dict = self.base(\n            mixtures=mixtures,\n            film_dict=film_dict,\n        )\n\n        return output_dict", ""]}
{"filename": "uss/models/pl_modules.py", "chunked_list": ["from typing import Callable, Dict\n\nimport lightning.pytorch as pl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\nclass LitSeparation(pl.LightningModule):\n    def __init__(\n        self,\n        ss_model: nn.Module,\n        anchor_segment_detector: nn.Module,\n        anchor_segment_mixer: nn.Module,\n        query_net: nn.Module,\n        loss_function: Callable,\n        optimizer_type: str,\n        learning_rate: float,\n        lr_lambda_func: Callable,\n    ) -> None:\n        r\"\"\"Pytorch Lightning wrapper of PyTorch model, including forward,\n        optimization of model, etc.\n\n        Args:\n            ss_model (nn.Module): universal source separation module\n            anchor_segment_detector (nn.Module): used to detect anchor segments\n                from audio clips\n            anchor_segment_mixer (nn.Module): used to mix segments into mixtures\n            query_net (nn.Module): used to extract conditions for separation\n            loss_function (Callable): loss function to train the separation model\n            optimizer_type (str): e.g., \"AdamW\"\n            learning_rate (float)\n            lr_lambda_func (Callable), learning rate scaler\n        \"\"\"\n\n        super().__init__()\n        self.ss_model = ss_model\n        self.anchor_segment_detector = anchor_segment_detector\n        self.anchor_segment_mixer = anchor_segment_mixer\n        self.query_net = query_net\n        self.loss_function = loss_function\n        self.optimizer_type = optimizer_type\n        self.learning_rate = learning_rate\n        self.lr_lambda_func = lr_lambda_func\n\n    def training_step(\n        self,\n        batch_data_dict: Dict,\n        batch_idx: int\n    ) -> torch.float:\n        r\"\"\"Forward a mini-batch data to model, calculate loss function, and\n        train for one step. A mini-batch data is evenly distributed on multiple\n        devices (if there are) for parallel training.\n\n        Args:\n            batch_data_dict (Dict): e.g. {\n                'hdf5_path': (batch_size,),\n                'index_in_hdf5': (batch_size,),\n                'audio_name': (batch_size,),\n                'waveform': (batch_size,),\n                'target': (batch_size,),\n                'class_id': (batch_size,),\n            }\n            batch_idx: int\n\n        Returns:\n            loss (torch.float): loss function of this mini-batch\n        \"\"\"\n\n        # Mine anchor segments from audio clips\n        segments_dict = self.anchor_segment_detector(\n            waveforms=batch_data_dict['waveform'],\n            class_ids=batch_data_dict['class_id'],\n        )\n        # segments_dict: {\n        #     \"waveform\": (batch_size, segment_samples),\n        #     \"class_id\": (batch_size,),\n        #     \"bgn_sample\": (batch_size,),\n        #     \"end_sample\": (batch_size,),\n        # }\n\n        # Mix segments into mixtures and execute energy augmentation\n        mixtures, segments = self.anchor_segment_mixer(\n            waveforms=segments_dict['waveform'],\n        )\n        # mixtures: (batch_size, segment_samples)\n        # segments: (batch_size, segment_samples)\n\n        # Use query net to calculate conditional embedding\n        conditions = self.query_net(\n            source=segments,\n        )['output']\n        # conditions: (batch_size, condition_dim)\n\n        input_dict = {\n            'mixture': mixtures[:, None, :],\n            'condition': conditions,\n        }\n\n        target_dict = {\n            'segment': segments[:, None, :],\n        }\n\n        # Do separation using mixtures and conditions as input\n        self.ss_model.train()\n        sep_segment = self.ss_model(input_dict)['waveform']\n        # sep_segment: (batch_size, 1, segment_samples)\n\n        output_dict = {\n            'segment': sep_segment,\n        }\n\n        # Calculate loss\n        loss = self.loss_function(output_dict, target_dict)\n\n        return loss\n\n    def configure_optimizers(self) -> Dict:\n        r\"\"\"Configure optimizer.\n        \"\"\"\n\n        if self.optimizer_type == \"AdamW\":\n            optimizer = optim.AdamW(\n                params=self.ss_model.parameters(),\n                lr=self.learning_rate,\n                betas=(0.9, 0.999),\n                eps=1e-08,\n                weight_decay=0.0,\n                amsgrad=True,\n            )\n        else:\n            raise NotImplementedError\n\n        scheduler = LambdaLR(optimizer, self.lr_lambda_func)\n\n        output_dict = {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1,\n            }\n        }\n\n        return output_dict", "\nclass LitSeparation(pl.LightningModule):\n    def __init__(\n        self,\n        ss_model: nn.Module,\n        anchor_segment_detector: nn.Module,\n        anchor_segment_mixer: nn.Module,\n        query_net: nn.Module,\n        loss_function: Callable,\n        optimizer_type: str,\n        learning_rate: float,\n        lr_lambda_func: Callable,\n    ) -> None:\n        r\"\"\"Pytorch Lightning wrapper of PyTorch model, including forward,\n        optimization of model, etc.\n\n        Args:\n            ss_model (nn.Module): universal source separation module\n            anchor_segment_detector (nn.Module): used to detect anchor segments\n                from audio clips\n            anchor_segment_mixer (nn.Module): used to mix segments into mixtures\n            query_net (nn.Module): used to extract conditions for separation\n            loss_function (Callable): loss function to train the separation model\n            optimizer_type (str): e.g., \"AdamW\"\n            learning_rate (float)\n            lr_lambda_func (Callable), learning rate scaler\n        \"\"\"\n\n        super().__init__()\n        self.ss_model = ss_model\n        self.anchor_segment_detector = anchor_segment_detector\n        self.anchor_segment_mixer = anchor_segment_mixer\n        self.query_net = query_net\n        self.loss_function = loss_function\n        self.optimizer_type = optimizer_type\n        self.learning_rate = learning_rate\n        self.lr_lambda_func = lr_lambda_func\n\n    def training_step(\n        self,\n        batch_data_dict: Dict,\n        batch_idx: int\n    ) -> torch.float:\n        r\"\"\"Forward a mini-batch data to model, calculate loss function, and\n        train for one step. A mini-batch data is evenly distributed on multiple\n        devices (if there are) for parallel training.\n\n        Args:\n            batch_data_dict (Dict): e.g. {\n                'hdf5_path': (batch_size,),\n                'index_in_hdf5': (batch_size,),\n                'audio_name': (batch_size,),\n                'waveform': (batch_size,),\n                'target': (batch_size,),\n                'class_id': (batch_size,),\n            }\n            batch_idx: int\n\n        Returns:\n            loss (torch.float): loss function of this mini-batch\n        \"\"\"\n\n        # Mine anchor segments from audio clips\n        segments_dict = self.anchor_segment_detector(\n            waveforms=batch_data_dict['waveform'],\n            class_ids=batch_data_dict['class_id'],\n        )\n        # segments_dict: {\n        #     \"waveform\": (batch_size, segment_samples),\n        #     \"class_id\": (batch_size,),\n        #     \"bgn_sample\": (batch_size,),\n        #     \"end_sample\": (batch_size,),\n        # }\n\n        # Mix segments into mixtures and execute energy augmentation\n        mixtures, segments = self.anchor_segment_mixer(\n            waveforms=segments_dict['waveform'],\n        )\n        # mixtures: (batch_size, segment_samples)\n        # segments: (batch_size, segment_samples)\n\n        # Use query net to calculate conditional embedding\n        conditions = self.query_net(\n            source=segments,\n        )['output']\n        # conditions: (batch_size, condition_dim)\n\n        input_dict = {\n            'mixture': mixtures[:, None, :],\n            'condition': conditions,\n        }\n\n        target_dict = {\n            'segment': segments[:, None, :],\n        }\n\n        # Do separation using mixtures and conditions as input\n        self.ss_model.train()\n        sep_segment = self.ss_model(input_dict)['waveform']\n        # sep_segment: (batch_size, 1, segment_samples)\n\n        output_dict = {\n            'segment': sep_segment,\n        }\n\n        # Calculate loss\n        loss = self.loss_function(output_dict, target_dict)\n\n        return loss\n\n    def configure_optimizers(self) -> Dict:\n        r\"\"\"Configure optimizer.\n        \"\"\"\n\n        if self.optimizer_type == \"AdamW\":\n            optimizer = optim.AdamW(\n                params=self.ss_model.parameters(),\n                lr=self.learning_rate,\n                betas=(0.9, 0.999),\n                eps=1e-08,\n                weight_decay=0.0,\n                amsgrad=True,\n            )\n        else:\n            raise NotImplementedError\n\n        scheduler = LambdaLR(optimizer, self.lr_lambda_func)\n\n        output_dict = {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1,\n            }\n        }\n\n        return output_dict", "\n\ndef get_model_class(model_type: str) -> nn.Module:\n    r\"\"\"Get separation module by model_type.\"\"\"\n\n    if model_type == 'ResUNet30':\n        from uss.models.resunet import ResUNet30\n        return ResUNet30\n\n    else:\n        raise NotImplementedError", ""]}
{"filename": "uss/callbacks/base.py", "chunked_list": ["import os\n\nimport lightning.pytorch as pl\nfrom lightning.pytorch.utilities import rank_zero_only\n\n\nclass CheckpointEveryNSteps(pl.Callback):\n    def __init__(\n        self,\n        checkpoints_dir,\n        save_step_frequency,\n    ) -> None:\n        r\"\"\"Save a checkpoint every N steps.\n\n        Args:\n            checkpoints_dir (str): directory to save checkpoints\n            save_step_frequency (int): save checkpoint every N step\n        \"\"\"\n\n        self.checkpoints_dir = checkpoints_dir\n        self.save_step_frequency = save_step_frequency\n\n    @rank_zero_only\n    def on_train_batch_end(self, *args, **kwargs) -> None:\n        r\"\"\"Save a checkpoint every N steps.\"\"\"\n\n        trainer = args[0]\n        global_step = trainer.global_step\n\n        if global_step == 1 or global_step % self.save_step_frequency == 0:\n\n            ckpt_path = os.path.join(\n                self.checkpoints_dir,\n                \"steps={}.ckpt\".format(global_step))\n            trainer.save_checkpoint(ckpt_path)\n            print(\"Save checkpoint to {}\".format(ckpt_path))", ""]}
{"filename": "uss/callbacks/evaluate.py", "chunked_list": ["import logging\n\nimport lightning.pytorch as pl\nfrom lightning.pytorch.utilities import rank_zero_only\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom uss.evaluate import AudioSetEvaluator\nfrom uss.utils import StatisticsContainer, get_mean_sdr_from_dict\n\n\nclass EvaluateCallback(pl.Callback):\n    def __init__(\n        self,\n        pl_model: pl.LightningModule,\n        balanced_train_eval_dir: str,\n        test_eval_dir: str,\n        classes_num: int,\n        max_eval_per_class: int,\n        evaluate_step_frequency: int,\n        summary_writer: SummaryWriter,\n        statistics_path: str,\n    ) -> None:\n        \"\"\"Evaluate on AudioSet separation.\n\n        Args:\n            pl_model (pl.LightningModule): universal source separation module\n            balanced_train_eval_dir (str): directory of balanced train set for evaluation\n            test_eval_dir (str): directory of test set for evaluation\n            classes_num (int): sound classes number\n            max_eval_per_class (int): the number of samples to evaluate for each sound class\n            evaluate_step_frequency (int): evaluate every N steps\n            summary_writer (SummaryWriter): used to write TensorBoard logs\n            statistics_path (str): path to write statistics\n\n        Returns:\n            None\n        \"\"\"\n\n        # Evaluators\n        self.balanced_train_evaluator = AudioSetEvaluator(\n            audios_dir=balanced_train_eval_dir,\n            classes_num=classes_num,\n            max_eval_per_class=max_eval_per_class,\n        )\n\n        self.test_evaluator = AudioSetEvaluator(\n            audios_dir=test_eval_dir,\n            classes_num=classes_num,\n            max_eval_per_class=max_eval_per_class,\n        )\n\n        self.pl_model = pl_model\n\n        self.evaluate_step_frequency = evaluate_step_frequency\n\n        self.summary_writer = summary_writer\n\n        # Statistics container\n        self.statistics_container = StatisticsContainer(statistics_path)\n\n    @rank_zero_only\n    def on_train_batch_end(self, *args, **kwargs):\n        r\"\"\"Evaluate every #evaluate_step_frequency steps.\"\"\"\n\n        trainer = args[0]\n        epoch = trainer.current_epoch\n        global_step = trainer.global_step\n\n        if global_step == 1 or global_step % self.evaluate_step_frequency == 0:\n\n            for split, evaluator in zip([\"balanced_train\", \"test\"], [\n                                        self.balanced_train_evaluator, self.test_evaluator]):\n\n                logging.info(\"------ {} ------\".format(split))\n\n                stats_dict = evaluator(pl_model=self.pl_model)\n\n                median_sdris_dict = AudioSetEvaluator.get_median_metrics(\n                    stats_dict=stats_dict,\n                    metric_type=\"sdris_dict\",\n                )\n\n                median_sdri = get_mean_sdr_from_dict(median_sdris_dict)\n                logging.info(\"Average SDRi: {:.3f}\".format(median_sdri))\n\n                self.summary_writer.add_scalar(\n                    \"SDRi/{}\".format(split),\n                    global_step=global_step,\n                    scalar_value=median_sdri)\n\n                logging.info(\n                    \"    Flush tensorboard logs to {}\".format(\n                        self.summary_writer.log_dir))\n\n                self.statistics_container.append(\n                    steps=global_step,\n                    statistics={\"sdri_dict\": median_sdris_dict},\n                    split=split,\n                    flush=True,\n                )", "\n\nclass EvaluateCallback(pl.Callback):\n    def __init__(\n        self,\n        pl_model: pl.LightningModule,\n        balanced_train_eval_dir: str,\n        test_eval_dir: str,\n        classes_num: int,\n        max_eval_per_class: int,\n        evaluate_step_frequency: int,\n        summary_writer: SummaryWriter,\n        statistics_path: str,\n    ) -> None:\n        \"\"\"Evaluate on AudioSet separation.\n\n        Args:\n            pl_model (pl.LightningModule): universal source separation module\n            balanced_train_eval_dir (str): directory of balanced train set for evaluation\n            test_eval_dir (str): directory of test set for evaluation\n            classes_num (int): sound classes number\n            max_eval_per_class (int): the number of samples to evaluate for each sound class\n            evaluate_step_frequency (int): evaluate every N steps\n            summary_writer (SummaryWriter): used to write TensorBoard logs\n            statistics_path (str): path to write statistics\n\n        Returns:\n            None\n        \"\"\"\n\n        # Evaluators\n        self.balanced_train_evaluator = AudioSetEvaluator(\n            audios_dir=balanced_train_eval_dir,\n            classes_num=classes_num,\n            max_eval_per_class=max_eval_per_class,\n        )\n\n        self.test_evaluator = AudioSetEvaluator(\n            audios_dir=test_eval_dir,\n            classes_num=classes_num,\n            max_eval_per_class=max_eval_per_class,\n        )\n\n        self.pl_model = pl_model\n\n        self.evaluate_step_frequency = evaluate_step_frequency\n\n        self.summary_writer = summary_writer\n\n        # Statistics container\n        self.statistics_container = StatisticsContainer(statistics_path)\n\n    @rank_zero_only\n    def on_train_batch_end(self, *args, **kwargs):\n        r\"\"\"Evaluate every #evaluate_step_frequency steps.\"\"\"\n\n        trainer = args[0]\n        epoch = trainer.current_epoch\n        global_step = trainer.global_step\n\n        if global_step == 1 or global_step % self.evaluate_step_frequency == 0:\n\n            for split, evaluator in zip([\"balanced_train\", \"test\"], [\n                                        self.balanced_train_evaluator, self.test_evaluator]):\n\n                logging.info(\"------ {} ------\".format(split))\n\n                stats_dict = evaluator(pl_model=self.pl_model)\n\n                median_sdris_dict = AudioSetEvaluator.get_median_metrics(\n                    stats_dict=stats_dict,\n                    metric_type=\"sdris_dict\",\n                )\n\n                median_sdri = get_mean_sdr_from_dict(median_sdris_dict)\n                logging.info(\"Average SDRi: {:.3f}\".format(median_sdri))\n\n                self.summary_writer.add_scalar(\n                    \"SDRi/{}\".format(split),\n                    global_step=global_step,\n                    scalar_value=median_sdri)\n\n                logging.info(\n                    \"    Flush tensorboard logs to {}\".format(\n                        self.summary_writer.log_dir))\n\n                self.statistics_container.append(\n                    steps=global_step,\n                    statistics={\"sdri_dict\": median_sdris_dict},\n                    split=split,\n                    flush=True,\n                )", ""]}
{"filename": "uss/dataset_creation/create_audioset_evaluation_meta.py", "chunked_list": ["import argparse\nimport os\nimport multiprocessing\nimport pathlib\n\nimport numpy as np\nimport soundfile\nfrom torch.utils.data import DataLoader\n\nfrom uss.config import (CLASSES_NUM, CLIP_SECONDS, FRAMES_PER_SECOND,", "\nfrom uss.config import (CLASSES_NUM, CLIP_SECONDS, FRAMES_PER_SECOND,\n                        SAMPLE_RATE, panns_paths_dict)\nfrom uss.data.anchor_segment_detectors import AnchorSegmentDetector\nfrom uss.data.anchor_segment_mixers import AnchorSegmentMixer\nfrom uss.data.datamodules import collate_fn\nfrom uss.data.datasets import Dataset\nfrom uss.data.samplers import BalancedSampler\nfrom uss.utils import get_path, load_pretrained_panns\n", "from uss.utils import get_path, load_pretrained_panns\n\n\ndef create_evaluation_meta(args):\n    r\"\"\"Create csv containing information of anchor segments for creating\n    mixtures. For each sound class k, we select M anchor segments that will be\n    randomly mixed with anchor segments that do not contain sound class k. In\n    total, there are classes_num x M mixtures to separate. Anchor segments are\n    short segments (such as 2 s) detected by a pretrained sound event detection\n    system on 10-second audio clips from AudioSet. All time stamps of anchor\n    segments are written into a csv file. E.g.,\n\n    .. code-block:: csv\n        index_in_hdf5   audio_name  bgn_sample  end_sample  class_id    mix_rank\n        4768    YC0j69NCIKfw.wav    140480  204480  347 0\n        15640   Yip4ZCCgoVXc.wav    81920   145920  496 1\n        10614   YTRxF5y6hFbE.wav    130240  194240  270 0\n        9969    YRN1ho4G-W0o.wav    256000  320000  305 1\n        ...\n\n    When creating mixtures, for example:\n        mixture_0 = YC0j69NCIKfw.wav + Yip4ZCCgoVXc.wav\n        mixture_1 = YTRxF5y6hFbE.wav + YRN1ho4G-W0o.wav\n        ...\n\n    Args:\n        workspace: str, path\n        split: str, 'balanced_train' | 'test'\n        gpus: int\n        config_yaml: str, path of config file\n    \"\"\"\n\n    # arguments & parameters\n    workspace = args.workspace\n    split = args.split\n    output_audios_dir = args.output_audios_dir\n    output_meta_csv_path = args.output_meta_csv_path\n    device = args.device\n\n    sample_rate = SAMPLE_RATE\n    frames_per_second = FRAMES_PER_SECOND\n    clip_seconds = CLIP_SECONDS\n    classes_num = CLASSES_NUM\n\n    eval_segments_per_class = 100\n    segment_seconds = 2.\n    anchor_segment_detect_mode = \"max_area\"\n    match_energy = True\n    mix_num = 2\n\n    batch_size = 32\n    steps_per_epoch = 10000\n    num_workers = min(16, multiprocessing.cpu_count())\n    sed_model_type = \"Cnn14_DecisionLevelMax\"\n\n    if split == 'balanced_train':\n        indexes_hdf5_path = os.path.join(\n            workspace, \"hdf5s/indexes/balanced_train.h5\")\n\n    elif split == 'test':\n        indexes_hdf5_path = os.path.join(workspace, \"hdf5s/indexes/eval.h5\")\n    # E.g., indexes_hdf5 looks like: {\n    #     'audio_name': (audios_num,),\n    #     'hdf5_path': (audios_num,),\n    #     'index_in_hdf5': (audios_num,),\n    #     'target': (audios_num, classes_num)\n    # }\n\n    sed_model = load_pretrained_panns(\n        model_type=sed_model_type,\n        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),\n        freeze=True,\n    ).to(device)\n\n    # dataset\n    dataset = Dataset(\n        steps_per_epoch=steps_per_epoch,\n    )\n\n    # sampler\n    sampler = BalancedSampler(\n        indexes_hdf5_path=indexes_hdf5_path,\n        batch_size=batch_size,\n        steps_per_epoch=steps_per_epoch,\n    )\n\n    dataloader = DataLoader(\n        dataset=dataset,\n        batch_sampler=sampler,\n        collate_fn=collate_fn,\n        num_workers=num_workers,\n        pin_memory=True,\n        persistent_workers=False,\n    )\n\n    anchor_segment_detector = AnchorSegmentDetector(\n        sed_model=sed_model,\n        clip_seconds=clip_seconds,\n        segment_seconds=segment_seconds,\n        frames_per_second=frames_per_second,\n        sample_rate=sample_rate,\n        detect_mode=anchor_segment_detect_mode,\n    ).to(device)\n\n    anchor_segment_mixer = AnchorSegmentMixer(\n        mix_num=mix_num,\n        match_energy=match_energy,\n    ).to(device)\n\n    count_dict = {class_id: 0 for class_id in range(classes_num)}\n\n    meta_dict = {\n        'audio_name': [],\n    }\n\n    for i in range(mix_num):\n        meta_dict['source{}_name'.format(i + 1)] = []\n        meta_dict['source{}_class_id'.format(i + 1)] = []\n        meta_dict['source{}_onset'.format(i + 1)] = []\n\n    for class_id in range(classes_num):\n        sub_dir = os.path.join(\n            output_audios_dir,\n            \"class_id={}\".format(class_id))\n        os.makedirs(sub_dir, exist_ok=True)\n\n    for batch_index, batch_data_dict in enumerate(dataloader):\n\n        batch_data_dict['waveform'] = batch_data_dict['waveform'].to(device)\n        # (batch_size, clip_samples)\n\n        segments_dict = anchor_segment_detector(\n            waveforms=batch_data_dict['waveform'],\n            class_ids=batch_data_dict['class_id'],\n        )\n\n        mixtures, segments = anchor_segment_mixer(\n            waveforms=segments_dict['waveform'],\n        )\n\n        mixtures = mixtures.data.cpu().numpy()\n        segments = segments.data.cpu().numpy()\n\n        source_names = batch_data_dict['audio_name']\n        class_ids = segments_dict['class_id']\n        bgn_samples = segments_dict['bgn_sample'].data.cpu().numpy()\n\n        for n in range(batch_size):\n\n            class_id = class_ids[n]\n\n            if count_dict[class_id] < eval_segments_per_class:\n\n                mixture_name = \"class_id={},index={:03d},mixture.wav\".format(\n                    class_id, count_dict[class_id])\n                source_name = \"class_id={},index={:03d},source.wav\".format(\n                    class_id, count_dict[class_id])\n\n                mixture_path = os.path.join(\n                    output_audios_dir,\n                    \"class_id={}\".format(class_id),\n                    mixture_name)\n                source_path = os.path.join(\n                    output_audios_dir,\n                    \"class_id={}\".format(class_id),\n                    source_name)\n\n                soundfile.write(\n                    file=mixture_path,\n                    data=mixtures[n],\n                    samplerate=sample_rate)\n                soundfile.write(\n                    file=source_path,\n                    data=segments[n],\n                    samplerate=sample_rate)\n\n                print(\"Write out to {}\".format(mixture_path))\n                print(\"Write out to {}\".format(source_path))\n\n                # Write mixing information into a csv file.\n                meta_dict['audio_name'].append(mixture_name)\n\n                for i in range(mix_num):\n                    meta_dict['source{}_name'.format(\n                        i + 1)].append(source_names[(n + i) % batch_size])\n                    meta_dict['source{}_onset'.format(\n                        i + 1)].append(bgn_samples[(n + i) % batch_size] / sample_rate)\n                    meta_dict['source{}_class_id'.format(\n                        i + 1)].append(class_ids[(n + i) % batch_size])\n\n                ###\n                meta_dict['audio_name'].append(source_name)\n                meta_dict['source1_name'].append(source_names[n])\n                meta_dict['source1_onset'].append(bgn_samples[n] / sample_rate)\n                meta_dict['source1_class_id'].append(class_ids[n])\n\n                for i in range(1, mix_num):\n                    meta_dict['source{}_name'.format(i + 1)].append(\"\")\n                    meta_dict['source{}_onset'.format(i + 1)].append(\"\")\n                    meta_dict['source{}_class_id'.format(i + 1)].append(\"\")\n\n                count_dict[class_id] += 1\n\n        finished_n = np.sum([count_dict[class_id]\n                            for class_id in range(classes_num)])\n        print('Finished: {} / {}'.format(finished_n,\n              eval_segments_per_class * classes_num))\n\n        if all_classes_finished(count_dict, eval_segments_per_class):\n            break\n\n    write_meta_dict_to_csv(meta_dict, output_meta_csv_path)\n    print(\"Write csv to {}\".format(output_meta_csv_path))", "\n\ndef all_classes_finished(count_dict, segments_per_class):\n    r\"\"\"Check if all sound classes have #segments_per_class segments in\n    count_dict.\n\n    Args:\n        count_dict: dict, e.g., {\n            0: 12,\n            1: 4,\n            ...,\n            526: 33,\n        }\n        segments_per_class: int\n\n    Returns:\n        bool\n    \"\"\"\n\n    for class_id in count_dict.keys():\n        if count_dict[class_id] < segments_per_class:\n            return False\n\n    return True", "\n\ndef write_meta_dict_to_csv(meta_dict, output_meta_csv_path):\n    r\"\"\"Write meta dict into a csv file.\n\n    Args:\n        meta_dict: dict, e.g., {\n            'index_in_hdf5': (segments_num,),\n            'audio_name': (segments_num,),\n            'class_id': (segments_num,),\n        }\n        output_csv_path: str\n    \"\"\"\n\n    keys = list(meta_dict.keys())\n\n    items_num = len(meta_dict[keys[0]])\n\n    os.makedirs(os.path.dirname(output_meta_csv_path), exist_ok=True)\n\n    with open(output_meta_csv_path, 'w') as fw:\n\n        fw.write(','.join(keys) + \"\\n\")\n\n        for n in range(items_num):\n\n            fw.write(\",\".join([str(meta_dict[key][n]) for key in keys]) + \"\\n\")\n\n    print('Write out to {}'.format(output_meta_csv_path))", "\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(description=\"\")\n    subparsers = parser.add_subparsers(dest=\"mode\")\n\n    parser_train = subparsers.add_parser(\"create_evaluation_meta\")\n    parser_train.add_argument(\"--workspace\", type=str, required=True)\n    parser_train.add_argument(\n        \"--split\",\n        type=str,\n        required=True,\n        choices=[\n            'balanced_train',\n            'test'])\n    parser_train.add_argument(\"--output_audios_dir\", type=str, required=True)\n    parser_train.add_argument(\n        \"--output_meta_csv_path\",\n        type=str,\n        required=True)\n    parser_train.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\")\n\n    args = parser.parse_args()\n    args.filename = pathlib.Path(__file__).stem\n\n    if args.mode == \"create_evaluation_meta\":\n        create_evaluation_meta(args)\n\n    else:\n        raise Exception(\"Error argument!\")", ""]}
{"filename": "uss/optimizers/lr_schedulers.py", "chunked_list": ["from functools import partial\nfrom typing import Callable\n\n\ndef linear_warm_up(\n    step: int,\n    warm_up_steps: int,\n    reduce_lr_steps: int\n) -> float:\n    r\"\"\"Get linear warm up scheduler for LambdaLR.\n\n    Args:\n        step (int): global step\n        warm_up_steps (int): steps for warm up\n        reduce_lr_steps (int): reduce learning rate by a factor of 0.9 #reduce_lr_steps step\n\n    .. code-block: python\n        >>> lr_lambda = partial(linear_warm_up, warm_up_steps=1000, reduce_lr_steps=10000)\n        >>> from torch.optim.lr_scheduler import LambdaLR\n        >>> LambdaLR(optimizer, lr_lambda)\n\n    Returns:\n        lr_scale (float): learning rate scaler\n    \"\"\"\n\n    if step <= warm_up_steps:\n        lr_scale = step / warm_up_steps\n    else:\n        lr_scale = 0.9 ** (step // reduce_lr_steps)\n\n    return lr_scale", "\n\ndef constant_warm_up(\n    step: int,\n    warm_up_steps: int,\n    reduce_lr_steps: int\n) -> float:\n    r\"\"\"Get constant warm up scheduler for LambdaLR.\n\n    Args:\n        step (int): global step\n        warm_up_steps (int): steps for warm up\n        reduce_lr_steps (int): reduce learning rate by a factor of 0.9 #reduce_lr_steps step\n\n    .. code-block: python\n        >>> lr_lambda = partial(constant_warm_up, warm_up_steps=1000, reduce_lr_steps=10000)\n        >>> from torch.optim.lr_scheduler import LambdaLR\n        >>> LambdaLR(optimizer, lr_lambda)\n\n    Returns:\n        lr_scale (float): learning rate scaler\n    \"\"\"\n\n    if 0 <= step < warm_up_steps:\n        lr_scale = 0.001\n\n    elif warm_up_steps <= step < 2 * warm_up_steps:\n        lr_scale = 0.01\n\n    elif 2 * warm_up_steps <= step < 3 * warm_up_steps:\n        lr_scale = 0.1\n\n    else:\n        lr_scale = 1\n\n    return lr_scale", "\n\ndef get_lr_lambda(\n    lr_lambda_type: str,\n    **kwargs\n) -> Callable:\n    r\"\"\"Get learning scheduler.\n\n    Args:\n        lr_lambda_type (str), e.g., \"constant_warm_up\" | \"linear_warm_up\"\n\n    Returns:\n        lr_lambda_func (Callable)\n    \"\"\"\n    if lr_lambda_type == \"constant_warm_up\":\n\n        lr_lambda_func = partial(\n            constant_warm_up,\n            warm_up_steps=kwargs[\"warm_up_steps\"],\n            reduce_lr_steps=kwargs[\"reduce_lr_steps\"],\n        )\n\n    elif lr_lambda_type == \"linear_warm_up\":\n\n        lr_lambda_func = partial(\n            linear_warm_up,\n            warm_up_steps=kwargs[\"warm_up_steps\"],\n            reduce_lr_steps=kwargs[\"reduce_lr_steps\"],\n        )\n\n    else:\n        raise NotImplementedError\n\n    return lr_lambda_func", ""]}
