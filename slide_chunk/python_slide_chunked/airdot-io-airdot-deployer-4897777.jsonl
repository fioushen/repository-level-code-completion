{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nVERSION = \"0.2.0b0\"\nREQUIRES_PYTHON = \">=3.7.0\"\nREQUIRED = [\n    \"black==22.6\",\n    \"pytest==7.1\",", "    \"black==22.6\",\n    \"pytest==7.1\",\n    \"google-api-python-client==2.78\",\n    \"google-cloud-core==2.3\",\n    \"google-cloud-storage==2.7\",\n    \"google-auth\",\n    \"boto==2.49\",\n    \"botocore==1.29\",\n    \"boto3==1.26\",\n    \"docker==6.1\",", "    \"boto3==1.26\",\n    \"docker==6.1\",\n    \"redis==4.5\",\n    \"seldon-core==1.16\",\n    \"pydantic==1.10.8\",\n    \"kubernetes\",\n    \"tabulate\"\n]\n\nDEV_REQUIRED = [", "\nDEV_REQUIRED = [\n    \"black==22.6.0\",\n    \"pytest==7.1.2\",\n    \"google-api-python-client==2.78.0\",\n    \"google-cloud-core==2.3.2\",\n    \"google-cloud-storage==2.7.0\",\n    \"google-auth\",\n    \"zstd==1.5.2.6\",\n    \"boto==2.49.0\",", "    \"zstd==1.5.2.6\",\n    \"boto==2.49.0\",\n    \"botocore==1.29.127\",\n    \"boto3==1.26\",\n    \"docker==6.1.2\",\n    \"redis==4.5.5\",\n    \"seldon-core==1.16\",\n    \"pydantic==1.10.8\",\n    \"kubernetes==26.1.0\"\n]", "    \"kubernetes==26.1.0\"\n]\n\nsetup(\n    name=\"airdot\",\n    url=\"https://github.com/airdot-io/airdot-Deploy/\",\n    author=\"airdot-io\",\n    author_email=\"abhhinav035991@gmail.com\",\n    packages=find_packages(),\n    version=VERSION,", "    packages=find_packages(),\n    version=VERSION,\n    description=\"A code base for deploying python functions\",\n    long_description=long_description,\n    python_requires=REQUIRES_PYTHON,\n    install_requires=REQUIRED,\n)\n"]}
{"filename": "test_gsutil.py", "chunked_list": ["from google.cloud import storage\nfrom google.oauth2 import service_account\nfrom googleapiclient import discovery\nfrom google import auth\nfrom datetime import datetime, timedelta\nimport json\nimport pickle\n\n\nfile_ = open(\"/Users/abhinav.singh-mbp/Downloads/token.json\", mode=\"rb\").read()", "\nfile_ = open(\"/Users/abhinav.singh-mbp/Downloads/token.json\", mode=\"rb\").read()\n\nprivate_json = json.loads(file_)\n\nfrom pprint import pprint\n\n\ndef get_gcs_bucket(config):\n    storage_client = storage.Client()\n    return storage_client.bucket(\"gs://model-ml-deployer\")", "def get_gcs_bucket(config):\n    storage_client = storage.Client()\n    return storage_client.bucket(\"gs://model-ml-deployer\")\n\n\ndef generate_upload_signed_url_v4(bucket_name, blob_name):\n    \"\"\"Generates a v4 signed URL for uploading a blob using HTTP PUT.\n\n    Note that this method requires a service account key file. You can not use\n    this if you are using Application Default Credentials from Google Compute\n    Engine or from the Google Cloud SDK.\n    \"\"\"\n    # bucket_name = 'your-bucket-name'\n    # blob_name = 'your-object-name'\n\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(blob_name)\n\n    url = blob.generate_signed_url(\n        version=\"v4\",\n        # This URL is valid for 15 minutes\n        expiration=datetime.timedelta(minutes=5),\n        # Allow PUT requests using this URL.\n        method=\"PUT\",\n        content_type=\"application/octet-stream\",\n    )\n\n    return url", "\n\n# credentials, project = auth.default(\n\n# )\n# credentials.refresh(auth.transport.requests.Request())\n\n# credentials = service_account.Credentials.from_service_account_info(private_json)\n\n# #credentials.refresh(auth.transport.requests.Request())", "\n# #credentials.refresh(auth.transport.requests.Request())\n\n\n# expiration_timedelta = timedelta(minutes=3)\n\n# storage_client = storage.Client(credentials=credentials)\n# bucket = storage_client.bucket('model-ml-deployer')\n# blob = bucket.blob(\"test-folder2/test_model.pkl\")\n# test_dict = {'test_key':'test_value'}", "# blob = bucket.blob(\"test-folder2/test_model.pkl\")\n# test_dict = {'test_key':'test_value'}\n# file = pickle.dumps(test_dict)\n# blob.upload_from_string(file)\n\n# signed_url = blob.generate_signed_url(\n#     expiration=expiration_timedelta,\n#     service_account_email=credentials.service_account_email,\n#     access_token=credentials.token,\n# )", "#     access_token=credentials.token,\n# )\n\n# print('signed url', signed_url)\n\n\n# pickle.dump()\n\n\nstorage_client = storage.Client()", "\nstorage_client = storage.Client()\nbucket = storage_client.bucket(\"model-ml-deployer\")\nblob = bucket.blob(\"116997773269197393190/lm.pkl\")\nlm = pickle.loads(blob.download_as_string())\n"]}
{"filename": "airdot/version.py", "chunked_list": ["VERSION=\"0.5.0b0\""]}
{"filename": "airdot/deployer.py", "chunked_list": ["import json\nimport requests\nimport docker\nimport shutil\nimport logging\nfrom time import sleep\nfrom typing_extensions import Literal\nfrom typing import Optional\nfrom docker.errors import APIError\nfrom typing import cast, Union, Callable, Any, Dict, List, Optional", "from docker.errors import APIError\nfrom typing import cast, Union, Callable, Any, Dict, List, Optional\nfrom airdot.helpers.version_helpers import get_python_default_version\nfrom airdot.helpers.pkg_helpers import get_environment_pkgs, get_pip_list\nfrom airdot.helpers.runtime_helper import get_function_properties\nfrom airdot.helpers.template_helpers import make_soruce_file, make_soruce_file_seldon\nfrom airdot.helpers.general_helpers import get_name, get_difference\nfrom airdot.helpers.data_object_helpers import (\n    make_and_upload_data_files,\n    upload_runtime_object,", "    make_and_upload_data_files,\n    upload_runtime_object,\n)\nfrom airdot.collection.collections import authentication\nfrom tabulate import tabulate\nfrom airdot.helpers.authentication import (\n    user_login,\n    verify_user,\n    get_function_status,\n)", "    get_function_status,\n)\nfrom airdot import URL, VERIFY\nfrom airdot.helpers.docker_helper import docker_helper\nfrom airdot.helpers.redis_helper import redis_helper\nfrom airdot.helpers.network_helper import find_available_port\nfrom airdot.helpers.seldon_helper import seldon_helpers\nfrom airdot.helpers.content_helper import content_helper\nfrom airdot.helpers.s2i_helper import s2i_python_helper\nfrom airdot.contants.runtime_images import seldon_images", "from airdot.helpers.s2i_helper import s2i_python_helper\nfrom airdot.contants.runtime_images import seldon_images\nfrom airdot.helpers.kubernetes_helper import k8s\n\n\nauth_ = authentication()\n# python custom imports\nlog_fmt = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\nlogging.basicConfig(level=logging.INFO, format=log_fmt)\n", "logging.basicConfig(level=logging.INFO, format=log_fmt)\n\n\nclass Deployer:\n    def __init__(\n        self,\n        minio_endpoint: str = \"http://127.0.0.1:9000\",\n        redis_endpoint: str = \"localhost:6379\",\n        deployment_configuration: dict = {\n            \"deployment_type\": \"test\",\n            \"bucket_type\": \"minio\",\n        },\n    ) -> None:\n        \"\"\"\n        Deployer class provides interface for user to create and deploy their ML Models\n\n        Args:\n            minio_endpoint (str, optional): Local minio endpoint. Defaults to \"http://127.0.0.1:9000\".\n            redis_endpoint (str, optional): Local redis endpoint. Defaults to \"localhost:6379\".\n            deployment_configuration (dict, optional): _description_. Defaults to { \"deployment_type\": \"test\", \"bucket_type\": \"minio\", }.\n        \"\"\"\n\n        self.minio_endpoint = minio_endpoint\n        self.redis_endpoint = redis_endpoint\n        self.deployment_type = deployment_configuration[\"deployment_type\"]\n        self.deployment_configuration = deployment_configuration\n        self.docker_client = docker_helper()\n        self.redis_helper_obj = redis_helper(\n            host=self.redis_endpoint.split(\":\")[0],\n            port=self.redis_endpoint.split(\":\")[1],\n        )\n        if self.deployment_type == \"test\":\n            self.minio_network = \"minio-network\"\n\n    def _perform_user_login(self):\n        login_uri = user_login(auth_=auth_)\n        if login_uri is None:\n            print(\"login failed please try again\")\n            return False\n        try_auth = 50\n        while not (self._is_user_authenticated()) and try_auth > 0:\n            sleep(1)\n            try_auth -= 1\n            continue\n        if self._is_user_authenticated(True):\n            self.user_login = True\n            return self.user_login\n        self.user_login = False\n        return self.user_login\n\n    def _is_user_authenticated(self, print_status=False):\n        if auth_.refresh_token is not None and verify_user(auth_=auth_):\n            if print_status:\n                print(\"User authenticated.\")\n            return True\n        return False\n\n    def build_deployment(\n        self,\n        func: Callable,\n        name: Optional[str] = None,\n        python_version: Optional[str] = \"3.8\",\n        python_packages: Optional[List[str]] = None,\n        system_packages: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Build the source code from user specified function, this is done by tracking call trace\n        of the function.\n\n        Args:\n            func (Callable): primary function which predicts, this can be model object itself.\n            name (Optional[str], optional): service name. Defaults to None.\n            python_version (Optional[str], optional): python version to be used for runtime. Defaults to \"3.8\".\n            python_packages (Optional[List[str]], optional): List of python pkgs \n                if not provided uses func to get user pakgs. Defaults to None.\n            system_packages (Optional[List[str]], optional): Not yet implemented. Defaults to None.\n\n        Raises:\n            Exception: if func is not callable\n\n        Returns:\n            dict: {\n                \"source_file\": source code,\n                \"value_files\": {},\n                \"name\": name of the function,\n                \"data_files\": object datafiles like datframes or model object files,\n                \"module\": service name,\n                \"arg_names\": args for primary function to call service,\n                \"arg_types\": types but only available if types defined in function definition,\n                \"requirements_txt\": list of python packages to be used in order to run the callable,\n                \"python_version\": python version,\n                \"system_packages\": Not Implemented\n                \"dockerRun\": Not implemented\n                \"func_props\": properties of function,\n        }\n        \"\"\"\n        data_files = None\n        dir_id = None\n        bucket_type = self.deployment_configuration[\"bucket_type\"]\n\n        if callable(func):\n            python_version = get_python_default_version(python_version)\n            env_python_packages = get_environment_pkgs(\n                python_packages, func.__globals__\n            )\n            if python_packages is not None:\n                env_python_packages = env_python_packages + python_packages\n            func_props = get_function_properties(func, env_python_packages)\n            name = get_name(name)\n            data_files = make_and_upload_data_files(\n                bucket_id=func_props.name.replace(\"_\", \"-\"),\n                open_id=dir_id,\n                py_state=func_props,\n                endpoint=self.minio_endpoint,\n                bucket_type=bucket_type,\n            )  # uploading of data objects.\n        elif (\n            hasattr(func, \"__module__\")\n            and \"sklearn\" in func.__module__\n            and hasattr(func, \"predict\")\n        ):\n            pass\n        else:\n            raise Exception(\"Passed object is not callable\")\n\n        if self.deployment_type == \"test\":\n            source_file = make_soruce_file(\n                dir=dir_id,\n                pyProps=func_props,\n                source_file_name=name,\n            )\n\n        if self.deployment_type == \"seldon\":\n            source_file = make_soruce_file_seldon(\n                dir=dir_id,\n                pyProps=func_props,\n                source_file_name=name,\n                bucket_type=bucket_type,\n            )\n\n        elif self.deployment_type == \"kserve\":\n            pass\n\n        return {\n            \"source_file\": source_file.as_dict(),\n            \"value_files\": {},\n            \"name\": func_props.name,\n            \"data_files\": data_files,\n            \"module\": name,\n            \"arg_names\": func_props.arg_names,\n            \"arg_types\": func_props.arg_types,\n            \"requirements_txt\": \"\\n\".join(env_python_packages),\n            \"python_version\": python_version,\n            \"system_packages\": None,  # need to see this\n            \"dockerRun\": None,  # need to build this\n            \"func_props\": func_props,\n        }\n\n    def _list_to_json(self, cld_function_string):\n        return dict(\n            line.strip().split(\":\", 1)\n            for line in cld_function_string.split(\"\\n\")\n            if \":\" in line\n        )\n\n    def _build_url(self, json_string, deploy_dict):\n        if json_string is None:\n            logging.error(\"failed to deploy. please try again\")\n        json_value = self._list_to_json(json_string)\n        url = json_value[\"url\"]\n        print(\"Generating Curl request\")\n        data_dict = {}\n        if len(deploy_dict[\"arg_names\"]) > 0:\n            for arg_name in deploy_dict[\"arg_names\"]:\n                data_dict[arg_name] = \"<value-for-argument>\"\n        curl = f\"curl -XPOST {url} -d '{json.dumps(data_dict)}' -H 'Content-Type: application/json' \"\n        return curl\n\n    def _run__test_function(self, port, image):\n        try:\n            self.container = self.docker_client.run_container(\n                image,\n                detach=True,\n                ports={f\"{8080}/tcp\": port},\n                network=self.minio_network,\n            )\n            return True\n        except Exception as e:\n            logging.error(f\"{e}\")\n            return False\n\n    def restart(self, function_id):\n        \"\"\"\n        To restart service. Currently only implemented for local deployment\n\n        Args:\n            function_id (str): name of service\n        \"\"\"\n        container_id = self.docker_client.get_container_id(function_id)\n        self.docker_client.restart_container(container_id=container_id)\n\n    def stop(self, image_name):\n        \"\"\"\n        To stop the service. Currently only implemented for local deployment\n\n        Args:\n            image_name (str):  name of service\n        \"\"\"\n        try:\n            container_id = self.docker_client.get_container_id(image_name=image_name)\n            container_status = self.docker_client.kill_container(\n                container_id=container_id\n            )\n            if container_status:\n                self.docker_client.delete_container(container_id=container_id)\n                print(\"deployment killed successfully\")\n        except APIError as e:\n            print(f\"{e}\")\n\n    def update_redis(self, function_curl, object_refresh=False):\n        self.redis_helper_obj.set_user_function(\n            self.deploy_dict[\"name\"],\n            self.deploy_dict,\n            function_curl_req=function_curl,\n            object_refresh=object_refresh,\n        )\n\n    def run(\n        self,\n        func: Callable,\n        name: Optional[str] = None,\n        python_version: Optional[str] = None,\n        python_packages: Optional[List[str]] = None,\n        system_packages: Optional[List[str]] = None,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            func (Callable): primary function which predicts, this can be model object itself.\n            name (Optional[str], optional): service name. Defaults to None.\n            python_version (Optional[str], optional): python version to be used for runtime. Defaults to \"3.8\".\n            python_packages (Optional[List[str]], optional): List of python pkgs \n                if not provided uses func to get user pakgs. Defaults to None.\n            system_packages (Optional[List[str]], optional): Not yet implemented. Defaults to None.\n\n        Raises:\n            TypeError: raise if empty seldon uri supplied with seldon deployment.\n        \"\"\"\n\n        print(\"deployment started\")\n        self.deploy_dict = self.build_deployment(\n            func=func,\n            name=name,\n            python_packages=python_packages,\n            python_version=python_version,\n            system_packages=system_packages,\n        )\n\n        # changes for seldon deployment.\n        if self.deployment_type == \"test\":\n            print(\n                \"switching to test deployment no deployment configuration is provided.\"\n            )\n            port = find_available_port(8000)\n            content_helper_obj = content_helper(\n                deploy_dict=self.deploy_dict,\n                deployment_type=self.deployment_type,\n                seldon_configuration=None,\n            )\n            deployment_path = content_helper_obj.write_contents()\n            image = self.docker_client.build_image(\n                path=deployment_path, name=self.deploy_dict[\"name\"]\n            )\n            print(f\"deploying on port: {port}\")\n            function_status = self._run__test_function(port=port, image=image)\n            if function_status:\n                url = f\"http://127.0.0.1:{port}\"\n                function_curl = self.build_function_url(url=url)\n                print(\"deployment ready, access using the curl command below\")\n                print(function_curl)\n                self.update_redis(function_curl)\n\n        elif self.deployment_type == \"seldon\":\n            if self.deployment_configuration['image_uri'] is None:\n                raise TypeError('cannot provide empty image_uri for seldon deployment')\n\n            # building seldon deployment dictionary\n            seldon_helpers_obj = seldon_helpers(\n                deployment_configuration=self.deployment_configuration\n            )\n            seldon_configuration = seldon_helpers_obj.create_seldon_configuration(\n                deploy_dict=self.deploy_dict, image_uri = self.deployment_configuration['image_uri']\n            )\n            # building contents\n            content_helper_obj = content_helper(\n                deploy_dict=self.deploy_dict,\n                deployment_type=self.deployment_type,\n                seldon_configuration=seldon_configuration,\n            )\n            deployment_path = content_helper_obj.write_contents()\n            #raise TypeError\n            # building s2i image\n            base_image = seldon_images[self.deploy_dict[\"python_version\"]]\n            builder_image = self.deployment_configuration['image_uri']\n            s2i_python_helper_obj = s2i_python_helper(\n                base_image=base_image, builder_image=builder_image\n            )\n            #s2i_python_helper_obj.build_and_push_image(source_path=deployment_path)\n            s2i_python_helper_obj.build_and_push_image(source_path=deployment_path)\n            # k8s application\n            namespace = seldon_configuration['metadata']['namespace']\n            k8s_obj = k8s()\n            if k8s_obj.create_namespace(namespace=namespace):\n                _ = k8s_obj.apply_kubernetes_resources(\n                    resource_paths=deployment_path + \"/seldon_model.json\"\n                )\n\n        else:\n            print(\"failed to run function. Please try again.\")\n\n    def update_objects(self, object, function_id):\n        \"\"\"_summary_\n\n        Args:\n            object (list, tuple): Either list of tuple or just a tuple. tuple will contain two values\n                object name and object\n            function_id (str): service name\n\n        Returns:\n            bool: True if objects successfully updated else False. Currently only Implemented for local deployment.\n        \"\"\"\n        data_files: Dict[str, str] = {}\n        if (\n            isinstance(object, list)\n            and len(object) > 0\n            and isinstance(object[0], tuple)\n        ):\n            for item in object:\n                nName = item[0]\n                nVal = item[1]\n                data_files[f\"{nName}.pkl\"] = upload_runtime_object(\n                    function_id.replace(\"_\", \"-\"),\n                    None,\n                    nVal,\n                    nName,\n                    endpoint=self.minio_endpoint,\n                )\n            json_dict = {\n                \"data_files\": data_files,\n                \"function_id\": function_id,\n                \"auth_session_token\": auth_.refresh_token,\n            }\n            status = self.restart(function_id)\n            if status is not None:\n                return status\n        elif isinstance(object, tuple):\n            nName = object[0]\n            nVal = object[1]\n            data_files[f\"{nName}.pkl\"] = upload_runtime_object(\n                function_id.replace(\"_\", \"-\"),\n                None,\n                nVal,\n                nName,\n                endpoint=self.minio_endpoint,\n            )\n            json_dict = {\n                \"data_files\": data_files,\n                \"name\": function_id,\n                \"auth_session_token\": auth_.refresh_token,\n            }\n            status = self.restart(function_id)\n            if status is not None:\n                return status\n        else:\n            print(\"Please pass object tuple or list of tuples\")\n\n    def _check_function_status(self, deploy_dict):\n        payload = {\n            \"auth_session_token\": deploy_dict[\"auth_session_token\"],\n            \"name\": deploy_dict[\"name\"],\n        }\n\n        def function_status(payload):\n            status = get_function_status(payload=payload)\n            if status is not None and json.loads(status)[\"status\"] == \"DONE\":\n                return True\n            else:\n                return False\n\n        try_check_function_status = 10\n        function_status_flag = function_status(payload=payload)\n        while not (function_status_flag) and try_check_function_status > 0:\n            function_status_flag = function_status(payload=payload)\n            try_check_function_status -= 1\n            sleep(50)\n            continue\n        function_status_flag = function_status(payload=payload)\n        if function_status_flag:\n            return json.loads(get_function_status(payload=payload))\n        else:\n            return None\n\n    def build_function_url(self, url):\n        if url is None:\n            print(\"failed to generate url\")\n            exit(1)\n        url = url\n        data_dict = {}\n        if len(self.deploy_dict[\"arg_names\"]) > 0:\n            for arg_name in self.deploy_dict[\"arg_names\"]:\n                data_dict[arg_name] = \"<value-for-argument>\"\n        curl = f\"curl -XPOST {url} -H 'Content-Type: application/json' -d '{json.dumps(data_dict)}'  \"\n        return curl\n\n    def generate_arg_list(self, function_request):\n        arg_list = []\n        if len(function_request[\"metadata\"][\"arg_names\"]) > 0:\n            for arg_name in function_request[\"metadata\"][\"arg_names\"]:\n                arg_list.append(f\"{arg_name}\")\n            arg_list = arg_list if len(arg_list) > 0 else [\"None args defined\"]\n            return arg_list\n\n    def data_objects_list(self, function_request):\n        data_obj_keys = list(function_request[\"data_files\"].keys())\n        data_objects_list = []\n        for key in data_obj_keys:\n            update_keys = list(function_request[\"data_files\"][key].keys())\n            for update_key in update_keys:\n                data_objects_list.append(\n                    f\"name {update_key} update time {key.replace('$', ' ')}\"\n                )\n        data_objects_list = (\n            data_objects_list\n            if len(data_objects_list) > 0\n            else [\"None data objects found\"]\n        )\n        return data_objects_list\n\n    def list_deployments(self):\n        \"\"\"\n        List all deployments. Currently only Implemented for local deployment.\n        \"\"\"\n        user_functions = self.redis_helper_obj.get_keys(\"*\")\n        if user_functions is not None:\n            keys = [\"deployment-name\", \"args\", \"data-objects\"]\n            function_names = [value.decode() for value in user_functions]\n            for function_name in function_names:\n                table = []\n                json_value = json.loads(self.redis_helper_obj.get_key(function_name))\n                curl_request = json_value[function_name][\"curl\"]\n                table.append(\n                    [\n                        function_name,\n                        \"\\n\".join(self.generate_arg_list(json_value[function_name])),\n                        \"\\n\".join(self.data_objects_list(json_value[function_name])),\n                    ]\n                )\n                print(f\"curl-request {curl_request}\")\n                print(tabulate(table, keys, \"grid\"))", ""]}
{"filename": "airdot/__init__.py", "chunked_list": ["__version__ = \"0.0.1\"\n__authur__ = \"abhinav.singh\"\nURL = \"https://deploy.creatoronly.in/\"\nVERIFY = True\n# URL = \"https://127.0.0.1:8000/\"\n# VERIFY = False\n\n\nfrom airdot.deployer import Deployer\n", "from airdot.deployer import Deployer\n\n__all__ = [\"Deployer\"]\n"]}
{"filename": "airdot/cli.py", "chunked_list": [""]}
{"filename": "airdot/contants/__init__.py", "chunked_list": [""]}
{"filename": "airdot/contants/runtime_images.py", "chunked_list": ["# python runtime images\n\nseldon_images = {\n    \"3.7\": \"seldonio/seldon-core-s2i-python37-ubi8:1.17.0-dev\",\n    \"3.8\": \"seldonio/seldon-core-s2i-python38-ubi8:1.17.0-dev\",\n}\n"]}
{"filename": "airdot/collection/__init__.py", "chunked_list": [""]}
{"filename": "airdot/collection/collections.py", "chunked_list": ["# python imports\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\n\n\nclass namespace:\n    def __init__(self):\n        self.functions: Dict[str, str] = {}\n        self.vars: Dict[str, Any] = {}\n        self.imports: Dict[str, str] = {}\n        self.froms: Dict[str, str] = {\"*\": \"typing\"}\n        self.all_modules: List[str] = []\n        self.custom_init_code: List[str] = []", "\n\nclass python_function_prop:\n    exclude_from_dict: List[str] = [\"errors\"]\n\n    def __init__(self):\n        self.source: Optional[str] = None\n        self.name: Optional[str] = None\n        self.arg_names: Optional[List[str]] = None\n        self.arg_types: Optional[Dict[str, str]] = None\n        self.namespace_vars_desc: Optional[Dict[str, str]] = None\n        self.namespace_functions: Optional[Dict[str, str]] = None\n        self.namespace_imports: Optional[Dict[str, str]] = None\n        self.namespace_froms: Optional[Dict[str, str]] = None\n        self.namespace_modules: Optional[List[str]] = None\n        self.errors: Optional[List[str]] = None\n        self.namespace_vars: Optional[Dict[str, Any]] = None\n        self.custom_init_code: Optional[List[str]] = None", "\n\nclass authentication:\n    def __init__(self) -> None:\n        self.refresh_token: Optional[str] = None\n        self.token_time: Optional[datetime] = datetime(2000, 1, 1)\n\n\nclass source_file_props:\n    def __init__(\n        self, user_contents: str, seldon_contents: str = None, name: str = \"source.py\"\n    ):\n        self.seldon_name = \"seldon_wrapper.py\"\n        self.user_name = name\n        self.seldon_contents = seldon_contents\n        self.user_contents = user_contents\n\n    def as_dict(self):\n        return {\n            \"user_name\": self.user_name,\n            \"seldon_name\": self.seldon_name,\n            \"seldon_contents\": self.seldon_contents,\n            \"user_contents\": self.user_contents,\n        }", "class source_file_props:\n    def __init__(\n        self, user_contents: str, seldon_contents: str = None, name: str = \"source.py\"\n    ):\n        self.seldon_name = \"seldon_wrapper.py\"\n        self.user_name = name\n        self.seldon_contents = seldon_contents\n        self.user_contents = user_contents\n\n    def as_dict(self):\n        return {\n            \"user_name\": self.user_name,\n            \"seldon_name\": self.seldon_name,\n            \"seldon_contents\": self.seldon_contents,\n            \"user_contents\": self.user_contents,\n        }", ""]}
{"filename": "airdot/helpers/docker_helper.py", "chunked_list": ["import os\nimport string\nimport random\nimport docker\nfrom airdot.helpers.template_helpers import get_docker_template\n\nDEFAULT_PKG_LIST = [\"Flask\", \"gunicorn\", \"boto3\"]\n\n\nclass docker_helper:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def run_container(\n        self,\n        image_name,\n        command=None,\n        detach=True,\n        remove=True,\n        ports=None,\n        network=None,\n    ):\n        try:\n            container = self.client.containers.run(\n                image_name,\n                command,\n                detach=detach,\n                remove=remove,\n                ports=ports,\n                network=network,\n            )\n            return container\n        except docker.errors.ImageNotFound:\n            print(f\"Error: Image '{image_name}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error starting container: {e}\")\n\n    def get_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            return container.id\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error getting container ID: {e}\")\n\n    def get_container_id(self, image_name):\n        container_id = None\n        containers = self.client.containers.list(all=True)\n        for container in containers:\n            if len(container.image.tags) > 0 and container.image.tags[0].split(\":\")[0] == image_name:\n                container_id = container.id\n        return container_id\n\n    def kill_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.kill()\n            return True\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error killing container: {e}\")\n            return False\n\n    def delete_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.remove()\n            return True\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error deleting container: {e}\")\n            return False\n\n    def restart_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.restart()\n            print(f\"Container '{container_id}' restarted successfully\")\n        except docker.errors.NotFound:\n            print(f\"Container '{container_id}' not found\")\n\n    def create_docker_runtime(self, deploy_dict):\n        dir = self.write_user_file(deploy_dict[\"source_file\"])\n        custum_req = self.get_custom_requirements(deploy_dict[\"requirements_txt\"])\n        success_flag = self.create_custom_docker_file(custum_req, dir)\n        if success_flag:\n            image, _ = self.client.images.build(\n                path=f\"/tmp/{dir}/\", tag=f\"{deploy_dict['name']}\"\n            )\n            return image, dir\n        return None, None\n    \n    def build_image(self, path, name):\n        image, _ = self.client.images.build(\n                path=path, tag=f\"{name}\"\n            )\n        return image\n\n    def get_available_images(self):\n        try:\n            images = self.client.images.list()\n            return [image.tags[0] for image in images if image.tags]\n        except docker.errors.APIError as e:\n            print(f\"Failed to retrieve available images: {e}\")\n            return None\n\n    def pull_image(self, image_name):\n        try:\n            self.client.images.pull(image_name)\n            print(f\"Image '{image_name}' pulled successfully.\")\n        except docker.errors.APIError as e:\n            print(f\"Failed to pull image: {e}\")\n\n    def create_custom_docker_file(self, custum_req, dir):\n        try:\n            docker_template = get_docker_template(custum_req)\n            with open(f\"/tmp/{dir}/Dockerfile\", \"w\") as py_file:\n                py_file.write(\"\\n\".join(docker_template) + \"\\n\")\n            return True\n        except:\n            return False\n\n    def get_custom_requirements(self, requirements_txt):\n        pkg_list = requirements_txt.split(\"\\n\")\n        return \" \".join(pkg_list + DEFAULT_PKG_LIST)\n\n    def write_user_file(self, source_file):\n        try:\n            dir = self.id_generator()\n            os.mkdir(f\"/tmp/{dir}\")\n            with open(f\"/tmp/{dir}/app.py\", \"w\") as py_file:\n                py_file.write(\"\\n\".join(source_file[\"contents\"].split(\"\\n\")) + \"\\n\")\n            return dir\n        except:\n            return None\n\n    def id_generator(self, size=4, chars=string.ascii_lowercase + string.digits):\n        return \"\".join(random.choice(chars) for _ in range(size))", "\nclass docker_helper:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def run_container(\n        self,\n        image_name,\n        command=None,\n        detach=True,\n        remove=True,\n        ports=None,\n        network=None,\n    ):\n        try:\n            container = self.client.containers.run(\n                image_name,\n                command,\n                detach=detach,\n                remove=remove,\n                ports=ports,\n                network=network,\n            )\n            return container\n        except docker.errors.ImageNotFound:\n            print(f\"Error: Image '{image_name}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error starting container: {e}\")\n\n    def get_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            return container.id\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error getting container ID: {e}\")\n\n    def get_container_id(self, image_name):\n        container_id = None\n        containers = self.client.containers.list(all=True)\n        for container in containers:\n            if len(container.image.tags) > 0 and container.image.tags[0].split(\":\")[0] == image_name:\n                container_id = container.id\n        return container_id\n\n    def kill_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.kill()\n            return True\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error killing container: {e}\")\n            return False\n\n    def delete_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.remove()\n            return True\n        except docker.errors.NotFound:\n            print(f\"Error: Container '{container_id}' not found\")\n        except docker.errors.APIError as e:\n            print(f\"Error deleting container: {e}\")\n            return False\n\n    def restart_container(self, container_id):\n        try:\n            container = self.client.containers.get(container_id)\n            container.restart()\n            print(f\"Container '{container_id}' restarted successfully\")\n        except docker.errors.NotFound:\n            print(f\"Container '{container_id}' not found\")\n\n    def create_docker_runtime(self, deploy_dict):\n        dir = self.write_user_file(deploy_dict[\"source_file\"])\n        custum_req = self.get_custom_requirements(deploy_dict[\"requirements_txt\"])\n        success_flag = self.create_custom_docker_file(custum_req, dir)\n        if success_flag:\n            image, _ = self.client.images.build(\n                path=f\"/tmp/{dir}/\", tag=f\"{deploy_dict['name']}\"\n            )\n            return image, dir\n        return None, None\n    \n    def build_image(self, path, name):\n        image, _ = self.client.images.build(\n                path=path, tag=f\"{name}\"\n            )\n        return image\n\n    def get_available_images(self):\n        try:\n            images = self.client.images.list()\n            return [image.tags[0] for image in images if image.tags]\n        except docker.errors.APIError as e:\n            print(f\"Failed to retrieve available images: {e}\")\n            return None\n\n    def pull_image(self, image_name):\n        try:\n            self.client.images.pull(image_name)\n            print(f\"Image '{image_name}' pulled successfully.\")\n        except docker.errors.APIError as e:\n            print(f\"Failed to pull image: {e}\")\n\n    def create_custom_docker_file(self, custum_req, dir):\n        try:\n            docker_template = get_docker_template(custum_req)\n            with open(f\"/tmp/{dir}/Dockerfile\", \"w\") as py_file:\n                py_file.write(\"\\n\".join(docker_template) + \"\\n\")\n            return True\n        except:\n            return False\n\n    def get_custom_requirements(self, requirements_txt):\n        pkg_list = requirements_txt.split(\"\\n\")\n        return \" \".join(pkg_list + DEFAULT_PKG_LIST)\n\n    def write_user_file(self, source_file):\n        try:\n            dir = self.id_generator()\n            os.mkdir(f\"/tmp/{dir}\")\n            with open(f\"/tmp/{dir}/app.py\", \"w\") as py_file:\n                py_file.write(\"\\n\".join(source_file[\"contents\"].split(\"\\n\")) + \"\\n\")\n            return dir\n        except:\n            return None\n\n    def id_generator(self, size=4, chars=string.ascii_lowercase + string.digits):\n        return \"\".join(random.choice(chars) for _ in range(size))", ""]}
{"filename": "airdot/helpers/data_object_helpers.py", "chunked_list": ["import sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parents[2]))\n\nimport hashlib\nimport pickle\nfrom google.cloud import storage\nfrom typing import Dict, Tuple, Any, cast\nimport pprint", "from typing import Dict, Tuple, Any, cast\nimport pprint\nimport yaml\n\nfrom google.cloud import storage\n#from google.oauth2 import service_account\nfrom googleapiclient import discovery\nfrom datetime import timedelta\n\nSCHEMA_VERSION = 1", "\nSCHEMA_VERSION = 1\nMAX_DESCRIPTION_SIZE = 5000\nNULL_BYTE = b\"\\x00\"\n\nfrom airdot.helpers.minio_helper import minio_helper\n\ndef serialize_zstd(obj) -> Tuple[bytes, str, int]:\n    pkl_data = pickle.dumps(obj)\n    content_hash = f\"sha1:{hashlib.sha1(pkl_data).hexdigest()}\"\n    obj_size = len(pkl_data)\n    return (pkl_data, content_hash, obj_size)", "\n\ndef is_binary_file(content: bytes) -> bool:\n    return NULL_BYTE in content\n\n\ndef decode_string(b: bytes) -> str:\n    for encoding in (\"ascii\", \"utf8\", \"latin1\"):\n        try:\n            return b.decode(encoding)\n        except UnicodeDecodeError:\n            pass\n    return b.decode(\"ascii\", \"ignore\")", "\n\ndef describe_object(\n    obj: Any, max_depth: int, remaining_characters=MAX_DESCRIPTION_SIZE\n) -> Dict[str, Any]:\n    objT = type(obj)\n    if objT is dict and max_depth > 0:\n        ret = {}\n        for k, v in obj.items():\n            ret[k] = describe_object(v, max_depth - 1, max(0, remaining_characters))\n            remaining_characters -= len(str(ret[k]))\n        return ret\n    elif objT is bytes:\n        if is_binary_file(obj):\n            obj = \"Unknown binary file\"\n        else:\n            obj = decode_string(obj)\n            objT = type(obj)\n    description = (\n        obj[:remaining_characters].strip()\n        if type(obj) is str\n        else pprint.pformat(obj, depth=1, width=100, compact=True)[\n            :remaining_characters\n        ].strip()\n    )\n    return {\n        \"module\": objT.__module__,\n        \"class\": objT.__name__,\n        \"description\": description,\n    }", "\n\ndef repr_str(dumper: yaml.Dumper, data: str):\n    if \"\\n\" in data:\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\n\n\ndef to_file_stub_dict(content_hash: str, obj_desc: Dict[str, Any]) -> Dict[str, Any]:\n    return {\n        \"_\": \"MBFileStub\",\n        \"content_hash\": content_hash,\n        \"metadata\": obj_desc,\n        \"schemaVersion\": SCHEMA_VERSION,\n    }", "def to_file_stub_dict(content_hash: str, obj_desc: Dict[str, Any]) -> Dict[str, Any]:\n    return {\n        \"_\": \"MBFileStub\",\n        \"content_hash\": content_hash,\n        \"metadata\": obj_desc,\n        \"schemaVersion\": SCHEMA_VERSION,\n    }\n\n\ndef to_yaml(content_hash: str, fileSize: int, obj_desc: Dict[str, Any]) -> str:\n    metadata: Dict[str, Any] = {\"file_size\": fileSize, \"object\": obj_desc}\n\n    obj = to_file_stub_dict(content_hash, metadata)\n    yaml.add_representer(str, repr_str)\n    return yaml.dump(obj, width=1000)", "\ndef to_yaml(content_hash: str, fileSize: int, obj_desc: Dict[str, Any]) -> str:\n    metadata: Dict[str, Any] = {\"file_size\": fileSize, \"object\": obj_desc}\n\n    obj = to_file_stub_dict(content_hash, metadata)\n    yaml.add_representer(str, repr_str)\n    return yaml.dump(obj, width=1000)\n\n\ndef put_secure_data(\n    bucket_id, open_id, data: bytes, desc: str, endpoint: str, bucket_type\n):\n    try:\n        if bucket_type == 'minio':\n            minio_helper_obj = minio_helper(endpoint=endpoint)\n            minio_helper_obj.create_bucket(bucket_name=bucket_id)\n            minio_helper_obj.put_object(bucket=bucket_id, key=f\"{desc}.pkl\", data=data)\n            return True\n        # TODO add gcs and aws support\n    except Exception as e:\n        print(f\"failed to upload data object. Please try again {e}\")\n        return False", "\ndef put_secure_data(\n    bucket_id, open_id, data: bytes, desc: str, endpoint: str, bucket_type\n):\n    try:\n        if bucket_type == 'minio':\n            minio_helper_obj = minio_helper(endpoint=endpoint)\n            minio_helper_obj.create_bucket(bucket_name=bucket_id)\n            minio_helper_obj.put_object(bucket=bucket_id, key=f\"{desc}.pkl\", data=data)\n            return True\n        # TODO add gcs and aws support\n    except Exception as e:\n        print(f\"failed to upload data object. Please try again {e}\")\n        return False", "\n\ndef upload_runtime_object(\n    bucket_id, open_id, obj, desc: str, endpoint: str, bucket_type: str\n):\n    (data, content_hash, obj_size) = serialize_zstd(obj)\n    response = put_secure_data(bucket_id, open_id, data, desc, endpoint, bucket_type)\n    if response:\n        yamlObj = to_yaml(content_hash, obj_size, describe_object(obj, 1))\n        return yamlObj  # need to think a way to save complete yamlObj\n    else:\n        return \"None\"", "\n\n# uploading\ndef make_and_upload_data_files(\n    bucket_id, open_id, py_state, endpoint, bucket_type=None\n):\n    dataFiles: Dict[str, str] = {}\n    if py_state.namespace_vars and py_state.namespace_vars_desc:\n        for nName, nVal in py_state.namespace_vars.items():\n            dataFiles[f\"{nName}.pkl\"] = upload_runtime_object(\n                bucket_id, open_id, nVal, nName, endpoint, bucket_type=bucket_type\n            )\n    return dataFiles", ""]}
{"filename": "airdot/helpers/kubernetes_helper.py", "chunked_list": ["import subprocess\nimport yaml\nfrom kubernetes import config, client\n\n\nclass k8s:\n    def __init__(self):  # Optional: specify the context to use\n        self.context = self.get_current_cluster_config()\n\n    def get_current_cluster_config(self):\n        try:\n            current_context = config.list_kube_config_contexts()[1][\"context\"]\n            return current_context\n        except Exception as e:\n            print(f\"Failed to retrieve cluster configuration: {e}\")\n            return None\n\n    def apply_kubernetes_resources(self, resource_paths):\n        try:\n            subprocess.check_call(\n                    [\"kubectl\", \"apply\", \"-f\", resource_paths]\n                )\n            print(\"Resources applied successfully.\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to apply resources: {e}\")\n\n    def create_namespace(self, namespace):\n        try:\n            subprocess.check_call(\n                [\"kubectl\", \"create\", \"namespace\", namespace]\n            )\n            print(f\"Namespace '{namespace}' created successfully\")\n            return True\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to create namespace: {e}\")\n            return False\n\n    def delete_namespace(self, namespace):\n        try:\n            subprocess.check_call(\n                [\"kubectl\", \"delete\", \"namespace\", namespace, \"--context\", self.context]\n            )\n            print(f\"Namespace '{namespace}' deleted successfully.\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to delete namespace: {e}\")", ""]}
{"filename": "airdot/helpers/redis_helper.py", "chunked_list": ["import redis\nimport json\nfrom airdot.helpers.general_helpers import get_datetime\n\n\nclass redis_helper:\n    def __init__(self, host, port, db=None):\n        self.host = host\n        self.port = port\n        self.db = db\n        self.redis = redis.Redis(host=self.host, port=self.port, db=self.db)\n\n    def set_key(self, key, value):\n        self.redis.set(key, value)\n\n    def get_key(self, key):\n        return self.redis.get(key)\n\n    def delete_key(self, key):\n        self.redis.delete(key)\n\n    def get_keys(self, pattern):\n        return self.redis.keys(pattern)\n\n    def increment_key(self, key):\n        self.redis.incr(key)\n\n    def decrement_key(self, key):\n        self.redis.decr(key)\n\n    def set_user_function(\n        self, id, deploy_dict, function_curl_req, object_refresh=False\n    ):\n        user_function = self.get_key(id)\n        if user_function is not None:  # for a old deployment\n            user_function = json.loads(user_function)\n            user_function[deploy_dict[\"name\"]][\"curl\"] = function_curl_req\n            user_function[deploy_dict[\"name\"]][\"version\"] = (\n                user_function[deploy_dict[\"name\"]][\"version\"] + 1\n                if not (object_refresh)\n                else user_function[deploy_dict[\"name\"]][\"version\"]\n            )\n            user_function[deploy_dict[\"name\"]][\"data_files\"][get_datetime()] = (\n                \"\" if deploy_dict[\"data_files\"] is None else deploy_dict[\"data_files\"]\n            )\n            user_function[deploy_dict[\"name\"]][\"metadata\"] = {\n                \"python_version\": deploy_dict[\"python_version\"]\n                if not (object_refresh)\n                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"python_version\"],\n                \"arg_types\": deploy_dict[\"arg_types\"]\n                if not (object_refresh)\n                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"arg_types\"],\n                \"arg_names\": deploy_dict[\"arg_names\"]\n                if not (object_refresh)\n                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"arg_names\"],\n            }\n        else:\n            user_function = dict()\n            user_function[deploy_dict[\"name\"]] = {\n                \"curl\": function_curl_req,\n                \"version\": 1,\n                \"data_files\": {\n                    get_datetime(): \"\"\n                    if deploy_dict[\"data_files\"] is None\n                    else deploy_dict[\"data_files\"]\n                },\n                \"metadata\": {\n                    \"python_version\": deploy_dict[\"python_version\"],\n                    \"arg_types\": deploy_dict[\"arg_types\"],\n                    \"arg_names\": deploy_dict[\"arg_names\"],\n                },\n            }\n        try:\n            status = self.set_key(\n                id,\n                json.dumps(user_function),\n            )\n            return status\n        except Exception as e:\n            return None", ""]}
{"filename": "airdot/helpers/template_helpers.py", "chunked_list": ["from typing import List\n\nfrom airdot.collection.collections import python_function_prop, source_file_props\nfrom airdot.helpers.general_helpers import add_space\n\nbucket_type_import = {\n    \"gcs\": \"from google.cloud import storage\",\n    \"s3\": \"import boto3\",\n    \"azure\": \"import boto3\",\n    \"minio\": \"import boto3\",", "    \"azure\": \"import boto3\",\n    \"minio\": \"import boto3\",\n}\n\nobject_type_import = {}\n\n\ndef make_soruce_file(\n    dir: str, pyProps: python_function_prop, source_file_name: str = \"source\"\n):\n\n    source_parts: List[str] = [\n        \"import sys\",\n        \"from flask import escape, jsonify, Flask, request\",\n        \"import pickle\",\n        \"import boto3\",\n        \"app = Flask('ml-deployer')\",\n    ]\n    if pyProps.namespace_froms:\n        for iAs, iModule in pyProps.namespace_froms.items():\n            source_parts.append(f\"from {iModule} import {iAs}\")\n    if pyProps.namespace_imports:\n        for iAs, iModule in pyProps.namespace_imports.items():\n            if iModule == iAs:\n                source_parts.append(f\"import {iModule}\")\n            else:\n                source_parts.append(f\"import {iModule} as {iAs}\")\n    add_space(source_parts)\n    source_parts.append(\n        \"client = boto3.resource('s3', endpoint_url='http://airdot-minio-1:9000', aws_access_key_id='minioadmin',aws_secret_access_key='miniopassword')\"\n    )\n    source_parts.append(f\"bucket = client.Bucket('{pyProps.name.replace('_','-')}')\")\n    if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n        for nName, _ in pyProps.namespace_vars.items():\n            source_parts.append(\n                f\"{nName} = pickle.loads(bucket.Object('{nName}.pkl').get()['Body'].read())\"\n            )\n    if pyProps.custom_init_code:\n        source_parts.append(\"\\n\" + \"\\n\\n\".join(pyProps.custom_init_code))\n    add_space(source_parts)\n    if pyProps.namespace_functions:\n        for _, fSource in pyProps.namespace_functions.items():\n            source_parts.append(fSource)\n            add_space(source_parts)\n    add_space(source_parts)\n\n    if pyProps.source:\n        source_parts.append(\"# main function\")\n        source_parts.append(pyProps.source)\n\n    # add calling method\n    add_space(source_parts)\n    source_parts.append(\"@app.route('/', methods=['POST'])\")\n    source_parts.append(f\"def main_{pyProps.name}():\")\n    source_parts.append(\"\\tdata = request.get_json()\")\n    source_parts.append(\"\\tif data is None:\")\n    source_parts.append(f\"\\t\\treturn jsonify(str({pyProps.name}()))\")\n    source_parts.append(\"\\telse:\")\n    source_parts.append(f\"\\t\\treturn jsonify(str({pyProps.name}(**data)))\")\n    return source_file_props(\n        name=f\"{source_file_name}.py\", user_contents=\"\\n\".join(source_parts)\n    )", "\n\ndef build_source_template(\n    dir: str,\n    pyProps: python_function_prop,\n    source_file_name: str = \"source\",\n    bucket_type=\"gcs\",\n    bucket_name=\"seldon-test\",\n):\n    source_parts: List[str] = [\n        \"import sys\",\n        \"from flask import escape, jsonify, Flask, request\",\n        \"import pickle\",\n        \"import logging\",\n        \"from io import BytesIO, StringIO\",\n        bucket_type_import[bucket_type],\n    ]\n\n    # adding custom imports\n    if pyProps.namespace_froms:\n        for iAs, iModule in pyProps.namespace_froms.items():\n            source_parts.append(f\"from {iModule} import {iAs}\")\n    if pyProps.namespace_imports:\n        for iAs, iModule in pyProps.namespace_imports.items():\n            if iModule == iAs:\n                source_parts.append(f\"import {iModule}\")\n            else:\n                source_parts.append(f\"import {iModule} as {iAs}\")\n\n    for _ in range(4):\n        add_space(source_parts)\n    # adding bucket imports\n    if bucket_type is \"gcs\":\n        source_parts.append(\"storage_client = storage.Client()\")\n        source_parts.append(\n            f\"bucket = storage_client.bucket('{pyProps.name.replace('_','-')}')\"\n        )\n        if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n            for nName, _ in pyProps.namespace_vars.items():\n                source_parts.append(f\"{nName}_blob = bucket.get_blob({nName}.pkl')\")\n                source_parts.append(f\"self.{nName} = BytesIO(blob.download_as_bytes())\")\n\n    elif bucket_type is \"minio\":\n        source_parts.append(\n            \"client = boto3.resource('s3', endpoint_url='http://airdot-minio-1:9000', aws_access_key_id='minioadmin',aws_secret_access_key='miniopassword')\"\n        )\n        source_parts.append(\n            f\"bucket = client.Bucket('{pyProps.name.replace('_','-')}')\"\n        )\n        if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n            for nName, _ in pyProps.namespace_vars.items():\n                source_parts.append(\n                    f\"{nName} = pickle.loads(bucket.Object('{nName}.pkl').get()['Body'].read())\"\n                )\n    if pyProps.custom_init_code:\n        source_parts.append(\"\\n\" + \"\\n\\n\".join(pyProps.custom_init_code))\n    add_space(source_parts)\n    if pyProps.namespace_functions:\n        for _, fSource in pyProps.namespace_functions.items():\n            source_parts.append(fSource)\n            add_space(source_parts)\n    add_space(source_parts)\n\n    if pyProps.source:\n        source_parts.append(\"# main function\")\n        source_parts.append(pyProps.source)\n    return source_parts", "\n\ndef make_soruce_file_seldon(\n    dir: str,\n    pyProps: python_function_prop,\n    source_file_name: str = \"source\",\n    bucket_type=\"gcs\",\n    bucket_name=\"seldon-test\",\n):\n\n    source_parts: List[str] = [\n        \"import logging\",\n        f\"from {pyProps.name}_source import {pyProps.name}\",\n    ]\n\n    add_space(source_parts)\n\n    source_parts.append(f\"class {pyProps.name}_class(object):\")\n    source_parts.append(f\"\\tdef __init__(self):\")\n    source_parts.append(f\"\\t\\t logging.info('service created ready to serve')\")\n\n    add_space(source_parts)\n\n    # add calling method\n    add_space(source_parts)\n    source_parts.append(f\"\\tdef predict(self, data):\")\n    source_parts.append(f\"\\t\\treturn {pyProps.name}(**data)\")\n\n    user_source = build_source_template(\n        dir, pyProps, source_file_name, bucket_type, bucket_name\n    )\n\n    return source_file_props(\n        name=f\"{pyProps.name}_source.py\",\n        seldon_contents=\"\\n\".join(source_parts),\n        user_contents=\"\\n\".join(user_source),\n    )", "\n\ndef get_docker_template(req_string, source_name):\n    dockerBuildParts: List[str] = [\n        \"FROM python:3.8-slim\",\n        \"ENV APP_HOME /app\",\n        \"WORKDIR $APP_HOME\",\n        \"COPY . ./\",\n        f\"RUN pip install {req_string}\",\n        f\"CMD exec gunicorn --bind :8080 --workers 1 --threads 8 {source_name}:app\",\n    ]\n    return dockerBuildParts", ""]}
{"filename": "airdot/helpers/runtime_helper.py", "chunked_list": ["import inspect\nfrom typing import Callable, Any, List, Dict\nimport tempfile, os\nimport ast\nimport re\n\nfrom airdot.collection.collections import namespace, python_function_prop\n\n\ndef get_function_properties(func, imported_modules):\n    props = python_function_prop()\n    if not callable(func):\n        raise Exception(\"Object is not a callable function\")\n    else:\n        props.name = func.__name__\n        props.source = get_function_source_code(func)\n        props.arg_names = get_func_args_name(func)\n        props.arg_types = annotation_to_type_str(func.__annotations__)\n        ns_collection = namespace()\n        ns_collection = get_function_dep(func, ns_collection, imported_modules)\n        props.namespace_functions = ns_collection.functions\n        props.namespace_vars = ns_collection.vars\n        props.namespace_vars_desc = get_string_values(ns_collection.vars)\n        props.namespace_imports = ns_collection.imports\n        props.namespace_froms = ns_collection.froms\n        props.namespace_modules = list(set(ns_collection.all_modules))\n        props.custom_init_code = ns_collection.custom_init_code\n    return props", "\ndef get_function_properties(func, imported_modules):\n    props = python_function_prop()\n    if not callable(func):\n        raise Exception(\"Object is not a callable function\")\n    else:\n        props.name = func.__name__\n        props.source = get_function_source_code(func)\n        props.arg_names = get_func_args_name(func)\n        props.arg_types = annotation_to_type_str(func.__annotations__)\n        ns_collection = namespace()\n        ns_collection = get_function_dep(func, ns_collection, imported_modules)\n        props.namespace_functions = ns_collection.functions\n        props.namespace_vars = ns_collection.vars\n        props.namespace_vars_desc = get_string_values(ns_collection.vars)\n        props.namespace_imports = ns_collection.imports\n        props.namespace_froms = ns_collection.froms\n        props.namespace_modules = list(set(ns_collection.all_modules))\n        props.custom_init_code = ns_collection.custom_init_code\n    return props", "\n\ndef get_string_values(args: Dict[str, Any]):\n    new_dict: Dict[str, str] = {}\n    for k, v in args.items():\n        str_val = re.sub(r\"\\s+\", \" \", str(v))\n        if type(v) is bytes:\n            str_val = \"Binary data\"\n        elif len(str_val) > 200:\n            str_val = str_val[0:200] + \"...\"\n        new_dict[k] = str_val\n    return new_dict", "\n\ndef unindent(source: str) -> str:\n    leading_whitespaces = len(source) - len(source.lstrip())\n    if leading_whitespaces == 0:\n        return source\n    new_lines = [line[leading_whitespaces:] for line in source.split(\"\\n\")]\n    return \"\\n\".join(new_lines)\n\n\ndef get_function_source_code(func: Callable = None):\n    if not callable(func):\n        return None\n    return unindent(inspect.getsource(func))", "\n\ndef get_function_source_code(func: Callable = None):\n    if not callable(func):\n        return None\n    return unindent(inspect.getsource(func))\n\n\ndef get_func_args_name(func: Callable = None):\n    arg_spec = inspect.getfullargspec(func)\n    if arg_spec.varargs:\n        return [\"...\"]\n    if arg_spec.args:\n        return arg_spec.args\n    noArgs: List[str] = []\n    return noArgs", "def get_func_args_name(func: Callable = None):\n    arg_spec = inspect.getfullargspec(func)\n    if arg_spec.varargs:\n        return [\"...\"]\n    if arg_spec.args:\n        return arg_spec.args\n    noArgs: List[str] = []\n    return noArgs\n\n\ndef annotation_to_type_str(annotations: Dict[str, Any]):\n    anno_strs: Dict[str, str] = {}\n    for name, t_class in annotations.items():\n        try:\n            if t_class == Any:\n                anno_strs[name] = \"Any\"\n            else:\n                anno_strs[name] = t_class.__name__\n        except:\n            pass\n    return anno_strs", "\n\ndef annotation_to_type_str(annotations: Dict[str, Any]):\n    anno_strs: Dict[str, str] = {}\n    for name, t_class in annotations.items():\n        try:\n            if t_class == Any:\n                anno_strs[name] = \"Any\"\n            else:\n                anno_strs[name] = t_class.__name__\n        except:\n            pass\n    return anno_strs", "\n\ndef has_state(obj: Any) -> bool:\n    try:\n        return len(obj.__dict__) > 0\n    except:\n        return False\n\n\ndef collect_byte_obj(\n    maybe_func_var: Any, maybe_func_var_name: str, collection: namespace\n):\n    tmp_file_path = os.path.join(tempfile.gettempdir(), \"btyd.pkl\")\n    maybe_func_var.save_model(tmp_file_path)\n    with open(tmp_file_path, \"rb\") as f:\n        collection.vars[maybe_func_var_name + \"_state\"] = f.read()\n    collection.custom_init_code.append(\n        f\"\"\"\n    with open('data/{maybe_func_var_name}_state.tmp', 'wb') as fo:\n    with open('data/{maybe_func_var_name}_state.pkl', 'rb') as fi:\n    fo.write(pickle.load(fi))\n    {maybe_func_var_name} = {maybe_func_var.__class__.__name__}()\n    {maybe_func_var_name}.load_model('data/{maybe_func_var_name}_state.tmp')\n    \"\"\".strip()\n    )\n    collection.froms[maybe_func_var.__class__.__name__] = maybe_func_var.__module__\n    collection.imports[\"pickle\"] = \"pickle\"\n    collection.imports[\"btyd\"] = \"btyd\"", "\ndef collect_byte_obj(\n    maybe_func_var: Any, maybe_func_var_name: str, collection: namespace\n):\n    tmp_file_path = os.path.join(tempfile.gettempdir(), \"btyd.pkl\")\n    maybe_func_var.save_model(tmp_file_path)\n    with open(tmp_file_path, \"rb\") as f:\n        collection.vars[maybe_func_var_name + \"_state\"] = f.read()\n    collection.custom_init_code.append(\n        f\"\"\"\n    with open('data/{maybe_func_var_name}_state.tmp', 'wb') as fo:\n    with open('data/{maybe_func_var_name}_state.pkl', 'rb') as fi:\n    fo.write(pickle.load(fi))\n    {maybe_func_var_name} = {maybe_func_var.__class__.__name__}()\n    {maybe_func_var_name}.load_model('data/{maybe_func_var_name}_state.tmp')\n    \"\"\".strip()\n    )\n    collection.froms[maybe_func_var.__class__.__name__] = maybe_func_var.__module__\n    collection.imports[\"pickle\"] = \"pickle\"\n    collection.imports[\"btyd\"] = \"btyd\"", "\n\ndef is_imported_module(imported_modules, module_name):\n    for item in imported_modules:\n        pkg_name, _ = item.split(\"==\")\n        if pkg_name == module_name:\n            return True\n    return False\n\n\ndef get_function_dep(func: Callable[..., Any], collection: namespace, imported_modules):\n    if not callable(func):\n        return collection\n    collection = get_function_args(func, collection)\n    globalsDict = func.__globals__  # type: ignore\n    allNames = func.__code__.co_names + func.__code__.co_freevars\n    for maybe_func_var_name in allNames:\n        if maybe_func_var_name in globalsDict:\n            maybe_func_var = globalsDict[maybe_func_var_name]\n            if \"__module__\" in dir(maybe_func_var):\n                if maybe_func_var.__module__ == \"__main__\":\n                    arg_names = list(maybe_func_var.__code__.co_varnames or [])\n                    funcSig = f\"{maybe_func_var.__name__}({', '.join(arg_names)})\"\n                    if funcSig not in collection.functions:\n                        collection.functions[funcSig] = inspect.getsource(\n                            maybe_func_var\n                        )\n                        get_function_dep(maybe_func_var, collection, imported_modules)\n                else:\n                    if inspect.isclass(maybe_func_var):\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = maybe_func_var.__module__  #\n                        collection.all_modules.append(maybe_func_var.__module__)\n                    elif callable(maybe_func_var) and not has_state(maybe_func_var):\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = maybe_func_var.__module__  #\n                        collection.all_modules.append(maybe_func_var.__module__)\n                    elif \"btyd.fitters\" in f\"{maybe_func_var.__module__}\":\n                        collection = collect_byte_obj(\n                            maybe_func_var, maybe_func_var_name, collection\n                        )\n                    elif isinstance(maybe_func_var, object):\n                        collection.froms[\n                            maybe_func_var.__class__.__name__\n                        ] = maybe_func_var.__module__\n                        collection.all_modules.append(maybe_func_var.__module__)\n                        collection.vars[maybe_func_var_name] = maybe_func_var\n                    else:\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = f\"NYI: {maybe_func_var.__module__}\"\n            elif str(maybe_func_var).startswith(\"<module\"):\n                collection.imports[maybe_func_var_name] = maybe_func_var.__name__\n                collection.all_modules.append(maybe_func_var.__name__)\n            elif inspect.isclass(maybe_func_var):\n                collection.froms[maybe_func_var_name] = maybe_func_var.__module__  #\n                collection.all_modules.append(maybe_func_var.__module__)\n            else:\n                collection.vars[maybe_func_var_name] = maybe_func_var\n    return collection", "\n\ndef get_function_dep(func: Callable[..., Any], collection: namespace, imported_modules):\n    if not callable(func):\n        return collection\n    collection = get_function_args(func, collection)\n    globalsDict = func.__globals__  # type: ignore\n    allNames = func.__code__.co_names + func.__code__.co_freevars\n    for maybe_func_var_name in allNames:\n        if maybe_func_var_name in globalsDict:\n            maybe_func_var = globalsDict[maybe_func_var_name]\n            if \"__module__\" in dir(maybe_func_var):\n                if maybe_func_var.__module__ == \"__main__\":\n                    arg_names = list(maybe_func_var.__code__.co_varnames or [])\n                    funcSig = f\"{maybe_func_var.__name__}({', '.join(arg_names)})\"\n                    if funcSig not in collection.functions:\n                        collection.functions[funcSig] = inspect.getsource(\n                            maybe_func_var\n                        )\n                        get_function_dep(maybe_func_var, collection, imported_modules)\n                else:\n                    if inspect.isclass(maybe_func_var):\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = maybe_func_var.__module__  #\n                        collection.all_modules.append(maybe_func_var.__module__)\n                    elif callable(maybe_func_var) and not has_state(maybe_func_var):\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = maybe_func_var.__module__  #\n                        collection.all_modules.append(maybe_func_var.__module__)\n                    elif \"btyd.fitters\" in f\"{maybe_func_var.__module__}\":\n                        collection = collect_byte_obj(\n                            maybe_func_var, maybe_func_var_name, collection\n                        )\n                    elif isinstance(maybe_func_var, object):\n                        collection.froms[\n                            maybe_func_var.__class__.__name__\n                        ] = maybe_func_var.__module__\n                        collection.all_modules.append(maybe_func_var.__module__)\n                        collection.vars[maybe_func_var_name] = maybe_func_var\n                    else:\n                        collection.froms[\n                            maybe_func_var_name\n                        ] = f\"NYI: {maybe_func_var.__module__}\"\n            elif str(maybe_func_var).startswith(\"<module\"):\n                collection.imports[maybe_func_var_name] = maybe_func_var.__name__\n                collection.all_modules.append(maybe_func_var.__name__)\n            elif inspect.isclass(maybe_func_var):\n                collection.froms[maybe_func_var_name] = maybe_func_var.__module__  #\n                collection.all_modules.append(maybe_func_var.__module__)\n            else:\n                collection.vars[maybe_func_var_name] = maybe_func_var\n    return collection", "\n\ndef is_valid_package(pkg_str: str):\n    return len(pkg_str.split(\".\")) > 0\n\n\ndef collect_mod_name(func, modName: str, collection: namespace):\n    if modName in func.__globals__:\n        gMod = func.__globals__[modName]\n        if hasattr(gMod, \"__module__\"):\n            collection.froms[modName] = gMod.__module__\n        else:\n            collection.imports[modName] = gMod.__name__\n            collection.all_modules.append(func.__globals__[modName].__name__)\n    return collection", "\n\ndef parse_ast_name_to_id(astName: Any):\n    if hasattr(astName, \"attr\"):\n        return astName.value.id\n    else:\n        return astName.id\n\n\ndef get_function_args(func: Callable[..., Any], collection: namespace):\n    try:\n        sigAst = ast.parse(inspect.getsource(func)).body[0]  # type: ignore\n        for a in sigAst.args.args:  # type: ignore\n            if a.annotation is None:  # type: ignore\n                continue\n            collection = collect_mod_name(func, parse_ast_name_to_id(a.annotation), collection=collection)  # type: ignore\n        if sigAst.returns is not None:  # type: ignore\n            collection = collect_mod_name(func, parse_ast_name_to_id(sigAst.returns), collection=collection)  # type: ignore\n        return collection\n    except Exception as err:\n        strErr = f\"{err}\"\n        if (\n            strErr != \"could not get source code\"\n        ):  # triggers when deploying pure sklearn model\n            print(f\"Warning: failed parsing type annotations: {err}\")", "\ndef get_function_args(func: Callable[..., Any], collection: namespace):\n    try:\n        sigAst = ast.parse(inspect.getsource(func)).body[0]  # type: ignore\n        for a in sigAst.args.args:  # type: ignore\n            if a.annotation is None:  # type: ignore\n                continue\n            collection = collect_mod_name(func, parse_ast_name_to_id(a.annotation), collection=collection)  # type: ignore\n        if sigAst.returns is not None:  # type: ignore\n            collection = collect_mod_name(func, parse_ast_name_to_id(sigAst.returns), collection=collection)  # type: ignore\n        return collection\n    except Exception as err:\n        strErr = f\"{err}\"\n        if (\n            strErr != \"could not get source code\"\n        ):  # triggers when deploying pure sklearn model\n            print(f\"Warning: failed parsing type annotations: {err}\")", ""]}
{"filename": "airdot/helpers/seldon_helper.py", "chunked_list": ["from airdot.data_models.deployment import Deployment\n\n\nclass seldon_helpers(object):\n    def __init__(self, deployment_configuration) -> None:\n        if deployment_configuration is None:\n            raise TypeError(\n                \"failed to build seldon deployment no deployment configuration is provided\"\n            )\n\n        self.seldon_configuration = Deployment(**deployment_configuration).dict()['seldon_configuration']\n\n    def create_seldon_configuration(self, deploy_dict, image_uri):\n        if self.seldon_configuration['apiVersion'] == \"None\":\n            self.seldon_configuration['apiVersion'] = \"machinelearning.seldon.io/v1\"\n            self.seldon_configuration['metadata']['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n            self.seldon_configuration['metadata']['namespace'] = f\"{deploy_dict['name'].replace('_','-')}\"\n            self.seldon_configuration['spec']['predictors'][0]['componentSpecs'][0]['spec']['containers'][0]['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n            self.seldon_configuration['spec']['predictors'][0]['componentSpecs'][0]['spec']['containers'][0]['image'] = f\"{image_uri}\"\n            self.seldon_configuration['spec']['predictors'][0]['graph']['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n            self.seldon_configuration['spec']['predictors'][0]['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n            self.seldon_configuration['spec']['predictors'][0]['replicas'] = 1\n        return self.seldon_configuration", ""]}
{"filename": "airdot/helpers/network_helper.py", "chunked_list": ["import socket\n\n\ndef find_available_port(start_port, max_port=9000):\n    for port in range(start_port, max_port + 1):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            try:\n                sock.bind((\"localhost\", port))\n                return port\n            except OSError:\n                pass\n    return None", ""]}
{"filename": "airdot/helpers/__init__.py", "chunked_list": [""]}
{"filename": "airdot/helpers/minio_helper.py", "chunked_list": ["import boto3\nfrom botocore.exceptions import ClientError\n\n\nclass minio_helper:\n    def __init__(self, endpoint, access_key=None, secret_key=None, secure=True):\n        self.client = boto3.resource(\n            \"s3\",\n            endpoint_url=\"http://127.0.0.1:9000\",\n            aws_access_key_id=\"minioadmin\",\n            aws_secret_access_key=\"miniopassword\",\n        )\n\n    def bucket_exists(self, bucket_name):\n        if self.client.Bucket(bucket_name) in self.client.buckets.all():\n            # print(f'Bucket {bucket_name} exists.')\n            return True\n        else:\n            # print(f'Bucket {bucket_name} does not exist.')\n            return False\n\n    def create_bucket(self, bucket_name):\n        try:\n            if not (self.bucket_exists(bucket_name=bucket_name)):\n                bucket = self.client.create_bucket(Bucket=bucket_name)\n                # print(f'Bucket {bucket_name} created successfully.')\n            else:\n                pass\n                # print('bucket already exists')\n        except Exception as e:\n            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n\n    def delete_bucket(self, bucket_name):\n        try:\n            self.client.Bucket(bucket_name).delete()\n            # print(f'Bucket {bucket_name} deleted successfully.')\n        except Exception as e:\n            print(f\"Error deleting bucket {bucket_name}: {str(e)}\")\n\n    def put_object(self, bucket, key, data):\n        try:\n            self.client.Object(bucket, key).put(Body=data)\n            print(f\"{key} uploaded successfully and available at {bucket}/{key}\")\n        except Exception as e:\n            print(f\"Error uploading object {key}: {str(e)}\")\n\n    def get_object(self, bucket, key):\n        try:\n            response = self.client.get_object(Bucket=bucket, Key=key)\n            return response[\"Body\"].read()\n        except ClientError as e:\n            # print(f\"Error getting object '{key}' from MinIO: {e}\")\n            return None", ""]}
{"filename": "airdot/helpers/version_helpers.py", "chunked_list": ["import os, sys\nfrom typing import Optional\n\nsupported_python_versions = [\"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n\n\ndef get_python_default_version(python_version: Optional[str] = None):\n    if python_version is None:\n        # add getting environment python version here\n        python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n        return verify_version(python_version)\n    else:\n        return verify_version(python_version)", "\n\ndef verify_version(python_version):\n    if python_version.split(\".\")[-1][0] in supported_python_versions:\n        return python_version\n    else:\n        raise Exception(\n            f\"Unsupported python version passed {python_version}. current supported version {supported_python_versions}\"\n        )\n", ""]}
{"filename": "airdot/helpers/authentication.py", "chunked_list": ["from airdot.collection.collections import authentication\nimport requests\nfrom datetime import datetime\nfrom airdot import URL, VERIFY\n\n\ndef get_authentication_token(auth_: authentication):\n    r = requests.get(f\"{URL}get_auth_token\", verify=VERIFY)\n\n    if r.status_code == 200:\n        auth_.refresh_token = r.content.decode()\n        auth_.token_time = datetime.now()", "\n\ndef verify_user(auth_: authentication):\n    json = {\"auth_session_token\": auth_.refresh_token}\n    r = requests.post(f\"{URL}check_authentication\", verify=VERIFY, json=json)\n    if r.status_code == 200:\n        if r.content.decode() == \"false\":\n            return False\n        return True\n", "\n\ndef user_login(auth_: authentication):\n    get_authentication_token(auth_=auth_)\n    json = {\"auth_session_token\": auth_.refresh_token}\n    r = requests.post(f\"{URL}login\", verify=VERIFY, json=json)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None", "\n\ndef get_user_function(auth_: authentication):\n    json = {\"auth_session_token\": auth_.refresh_token}\n    r = requests.post(f\"{URL}get_my_functions\", verify=VERIFY, json=json)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None\n", "\n\ndef get_function_status(payload: dict):\n    r = requests.post(f\"{URL}check_function_status\", verify=VERIFY, json=payload)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None\n\n\ndef get_gcs_signed_token(auth_: authentication):\n    json = {\"auth_session_token\": auth_.refresh_token}\n\n    r = requests.post(f\"{URL}get_gcs_token\", verify=VERIFY, json=json)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None", "\n\ndef get_gcs_signed_token(auth_: authentication):\n    json = {\"auth_session_token\": auth_.refresh_token}\n\n    r = requests.post(f\"{URL}get_gcs_token\", verify=VERIFY, json=json)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None", "\n\ndef push_refreshed_objects(json_dict):\n    r = requests.post(f\"{URL}update_objects\", verify=VERIFY, json=json_dict)\n    if r.status_code == 200:\n        return r.content.decode()\n    else:\n        return None\n", ""]}
{"filename": "airdot/helpers/general_helpers.py", "chunked_list": ["# python module\nfrom typing import List\nfrom datetime import datetime\n\n\ndef add_space(strList: List[str]):\n    if len(strList) > 0 and strList[-1] != \"\":\n        strList.append(\"\")\n\n\ndef get_name(name: str):\n    if name is None:\n        return \"source\"\n    return name", "\n\ndef get_name(name: str):\n    if name is None:\n        return \"source\"\n    return name\n\n\ndef in_notebook() -> bool:\n    # From: https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n    # Tested in Jupyter, Hex, DeepNote and Colab\n    try:\n        import IPython\n\n        return (\n            hasattr(IPython.get_ipython(), \"config\")\n            and len(IPython.get_ipython().config) > 0\n        )\n    except (NameError, ModuleNotFoundError):\n        return False", "def in_notebook() -> bool:\n    # From: https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n    # Tested in Jupyter, Hex, DeepNote and Colab\n    try:\n        import IPython\n\n        return (\n            hasattr(IPython.get_ipython(), \"config\")\n            and len(IPython.get_ipython().config) > 0\n        )\n    except (NameError, ModuleNotFoundError):\n        return False", "\n\ndef get_difference(time):\n    return (time - datetime.now()).seconds\n\n\ndef get_datetime():\n    return datetime.now().strftime(\"%Y-%m-%d$%H:%M\")\n", ""]}
{"filename": "airdot/helpers/pkg_helpers.py", "chunked_list": ["import os, sys\nfrom typing import Optional, List, Dict\nimport pkg_resources\nimport types\nfrom importlib.metadata import version\nimport json\nimport requests\nimport warnings\n\n", "\n\npkg_mapping = {\"sklearn\": \"scikit-learn\"}\n\nbase_pkg = []\n\n\ndef get_environment_pkgs(\n    python_packages: Optional[List[str]] = None, func_globals=None\n):\n    if python_packages is None:\n        return verify_packages(\n            get_pip_list().split(\"\\n\"), func_globals\n        )  # get_pip_list().split(\"\\n\")\n    elif isinstance(python_packages, list):\n        return python_packages + base_pkg", "\n\n# only explicit install ?\ndef get_pip_list() -> List[Dict[str, str]]:\n    return os.popen(\"python -m pip freeze | grep == \").read().strip()\n\n\ndef imports(func_globals):\n    module_list = []\n    for name, val in func_globals.items():\n        if isinstance(val, types.ModuleType):\n            module_list.append(val.__name__)\n    return module_list", "\n\ndef get_locally_installed_packages(encoding=None):\n    packages = []\n    ignore = [\"tests\", \"_tests\", \"egg\", \"EGG\", \"info\"]\n    for path in sys.path:\n        for root, dirs, files in os.walk(path):\n            for item in files:\n                if \"top_level\" in item:\n                    item = os.path.join(root, item)\n                    with open(item, \"r\", encoding=encoding) as f:\n                        package = root.split(os.sep)[-1].split(\"-\")\n                        try:\n                            package_import = f.read().strip().split(\"\\n\")\n                        except:  # NOQA\n                            # TODO: What errors do we intend to suppress here?\n                            continue\n                        for i_item in package_import:\n                            if (i_item not in ignore) and (package[0] not in ignore):\n                                version = None\n                                if len(package) > 1:\n                                    version = (\n                                        package[1]\n                                        .replace(\".dist\", \"\")\n                                        .replace(\".egg\", \"\")\n                                    )\n                                packages.append(f\"{package[0]}=={version}\")\n    return list(set(packages))", "\n\ndef get_root_pkgs(pkg_list):\n    root_pkg = []\n    for pkg in pkg_list:\n        if len(pkg.split(\".\")) > 1 and pkg.split(\".\")[0] in pkg_mapping:\n            root_pkg.append(pkg_mapping[pkg.split(\".\")[0]])\n        else:\n            root_pkg.append(pkg)\n    return root_pkg", "\n\ndef verify_packages(pkg_list, func_globals):\n    # get only used modules\n    used_pkg_list = []\n    used_imports = get_root_pkgs(imports(func_globals))\n    for item in pkg_list:\n        pkg_name, pkg_version = item.split(\"==\")\n        if pkg_name in used_imports:\n            used_pkg_list.append(f\"{pkg_name}=={pkg_version}\")\n    pypi_url = \"https://pypi.org/pypi/{}/{}/json\"\n    final_pkg_list = []\n    for item in used_pkg_list:\n        pkg_name, pkg_version = item.split(\"==\")\n        status_code = requests.get(pypi_url.format(pkg_name, pkg_version)).status_code\n        if status_code == 200:\n            final_pkg_list.append(item)\n        else:\n            warnings.warn(f\"Not a valid Pypi package {pkg_name}. ignoring the package\")\n    final_pkg_list = final_pkg_list + base_pkg\n    return final_pkg_list", ""]}
{"filename": "airdot/helpers/content_helper.py", "chunked_list": ["import os\nimport json\nimport yaml\nimport random\nimport string\nfrom airdot.helpers.template_helpers import get_docker_template\nfrom airdot.helpers.s2i_helper import get_s2i_environment\n\nDEFAULT_PKG_LIST = [\"Flask\", \"gunicorn\", \"boto3\"]\n", "DEFAULT_PKG_LIST = [\"Flask\", \"gunicorn\", \"boto3\"]\n\n\nclass content_helper:\n    def __init__(\n        self, deploy_dict=None, deployment_type=None, seldon_configuration=None\n    ) -> str:\n        self.deploy_dict = deploy_dict\n        self.deployment_type = deployment_type\n        self.seldon_configuration = seldon_configuration\n\n    def write_contents(self):\n        try:\n            deployment_path = self.create_tmp_directory()\n            if self.deployment_type == \"test\":\n                # write python file\n                user_contents = self.deploy_dict[\"source_file\"][\"user_contents\"]\n                file_name = self.deploy_dict[\"source_file\"][\"user_name\"]\n                py_path = os.path.join(deployment_path, file_name)\n                self.write_python_file(\n                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n                )\n                # write docker file\n                requirements_file_content = self.get_custom_requirements(\n                    self.deploy_dict[\"requirements_txt\"], DEFAULT_PKG_LIST\n                )\n                docker_template = get_docker_template(requirements_file_content, self.deploy_dict[\"source_file\"][\"user_name\"].split('.')[0])\n                dockerfile_path = os.path.join(deployment_path, \"Dockerfile\")\n                self.write_custom_file(dockerfile_path, \"\\n\".join(docker_template))\n                return deployment_path\n\n            elif self.deployment_type == \"seldon\":\n                # write user python file\n                user_contents = self.deploy_dict[\"source_file\"][\"user_contents\"]\n                file_name = self.deploy_dict[\"source_file\"][\"user_name\"]\n                py_path = os.path.join(deployment_path, file_name)\n                self.write_python_file(\n                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n                )\n\n                # write seldon contents file\n                user_contents = self.deploy_dict[\"source_file\"][\"seldon_contents\"]\n                file_name = self.deploy_dict[\"source_file\"][\"seldon_name\"]\n                py_path = os.path.join(deployment_path, file_name)\n                self.write_python_file(\n                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n                )\n\n                # .s2i/env file\n                os.mkdir(os.path.join(deployment_path, \".s2i\"))\n                s2i_path = os.path.join(deployment_path, \".s2i/environment\")\n                self.write_custom_file(\n                    s2i_path, get_s2i_environment(self.deploy_dict[\"name\"])\n                )\n                # write kubect yaml file\n                yaml_path = os.path.join(deployment_path, \"seldon_model.json\")\n                self.write_json_file(yaml_path, self.seldon_configuration)\n                return deployment_path\n\n            elif self.deployment_type == \"kserve\":\n\n                pass\n        except Exception as e:\n            print(f\"{e}\")\n\n    def id_generator(self, size=4, chars=string.ascii_lowercase + string.digits):\n        return \"\".join(random.choice(chars) for _ in range(size))\n\n    def get_custom_requirements(self, requirements_txt, default_path: str = \"\"):\n        pkg_list = requirements_txt.split(\"\\n\")\n        return \" \".join(pkg_list + default_path)\n\n    def create_tmp_directory(self):\n        try:\n            dir = self.id_generator(size=9)\n            path = f\"/tmp/{dir}\"\n            os.mkdir(path)\n            return path\n        except Exception as e:\n            print(f\"failed to create temporary directory {e}\")\n            return None\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Write content to a file.\n\n        :param file_path: The path of the file to be written.\n        :param content: The content to write to the file.\n        \"\"\"\n        directory = os.path.dirname(file_path)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        with open(file_path, \"w\") as file:\n            file.write(content)\n\n    def write_python_file(self, file_path, content):\n        \"\"\"\n        Write content to a Python (.py) file.\n\n        :param file_path: The path of the Python file to be written.\n        :param content: The content to write to the file.\n        \"\"\"\n        if not file_path.endswith(\".py\"):\n            file_path += \".py\"\n\n        self.write_file(file_path, content)\n\n    def write_text_file(self, file_path, content):\n        \"\"\"\n        Write content to a text (.txt) file.\n\n        :param file_path: The path of the text file to be written.\n        :param content: The content to write to the file.\n        \"\"\"\n        if not file_path.endswith(\".txt\"):\n            file_path += \".txt\"\n\n        self.write_file(file_path, content)\n\n    def write_json_file(self, file_path, data):\n        \"\"\"\n        Write JSON data to a JSON (.json) file.\n\n        :param file_path: The path of the JSON file to be written.\n        :param data: The JSON data to write to the file.\n        \"\"\"\n        if not file_path.endswith(\".json\"):\n            file_path += \".json\"\n\n        json_content = json.dumps(data, indent=4)\n        self.write_file(file_path, json_content)\n\n    def write_yaml_file(self, file_path, data):\n        \"\"\"\n        Write YAML data to a YAML (.yaml) file.\n\n        :param file_path: The path of the YAML file to be written.\n        :param data: The YAML data to write to the file.\n        \"\"\"\n        if not file_path.endswith(\".yaml\"):\n            file_path += \".yaml\"\n\n        yaml_content = yaml.dump(data, sort_keys=False)\n        self.write_file(file_path, yaml_content)\n\n    def write_custom_file(self, file_path, content):\n        \"\"\"\n        Write content to a custom file with no extension.\n\n        :param file_path: The path of the custom file to be written.\n        :param content: The content to write to the file.\n        \"\"\"\n        self.write_file(file_path, content)", ""]}
{"filename": "airdot/helpers/gcs_helper.py", "chunked_list": ["from google.cloud import storage\nimport pandas as pd\nfrom io import BytesIO, StringIO\n\n\nclass gcs_utils:\n    \"\"\"\n    Gcs helpers utility module. This module enables to perfrom\n    operations on gcs [uploading, downloading, exists check, connecting to gcs bucket]\n    \"\"\"\n\n    def _init_(self, gcs_uri) -> None:\n        \"\"\"\n        Default constructor for gcs_utils modules\n\n        Args:\n            gcs_uri (str): gcs uri string for the blob\n        \"\"\"\n        self.gcs_uri = gcs_uri\n        self.bucket_name, self.blob_name = self.split_gcs_path(self.gcs_uri)\n        self.bucket = self.connect_bucket()\n        self.storage_client = storage.Client()\n\n    def split_gcs_path(self, path):\n        \"\"\"\n        Splits the gcs path into bucket name and blob uri string.\n\n        Args:\n            path (str): gcs uri string for the blob\n\n        Returns:\n            str : it returns two values for bucket name and blob name\n        \"\"\"\n        bucket_name = path.split(\"//\")[1].split(\"/\")[0]\n        blob_name = \"/\".join(path.split(\"//\")[1].split(\"/\")[1:])\n        return bucket_name, blob_name\n\n    def connect_bucket(self):\n        \"\"\"\n        Connects to gcs bucket\n\n        Returns:\n            gcs bucket: gcs bucket connection\n        \"\"\"\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(self.bucket_name)\n        return bucket\n\n    def download_as_file_bytes(self, blob_name):\n        \"\"\"\n        Downloads gcs files as bytes\n\n        Args:\n            gcs_uri (str): gcs uri for the blob to download\n\n        Returns:\n            bytes: data of the blob at gcs uri\n        \"\"\"\n        blob = self.bucket.get_blob(blob_name)\n        data = blob.download_as_bytes()\n        return BytesIO(data)\n\n    def check_gcs_blob_existence(self, gcs_uri):\n        \"\"\"\n        checks if the blob exists at gcs uri\n\n        Args:\n            gcs_uri (str): gcs uri for the blob\n\n        Returns:\n            bool: True if Exists otherwise false\n        \"\"\"\n        _, blob_name = self.split_gcs_path(gcs_uri)\n        return storage.Blob(bucket=self.bucket, name=blob_name).exists(\n            self.storage_client\n        )\n\n    def file_name_filter(self, name, blob_name):\n        \"\"\"\n        checks if blob name starts with name\n\n        Args:\n            name (str):name string that needs to be checked\n            blob_name (str): string that needs to be present in name\n\n        Returns:\n            bool: True if blob name starts with name otherwise False\n        \"\"\"\n        return name.startswith(blob_name)\n\n    def get_file_list(self):\n        \"\"\"\n        returns list of files inside a bucket\n\n        Returns:\n            list: returns list of files name that starts with blob names in bucket.\n        \"\"\"\n        blobs = self.storage_client.list_blobs(self.bucket_name)\n        files = [\n            blob.name\n            for blob in blobs\n            if self.file_name_filter(blob.name, self.blob_name)\n        ]\n        return sorted(files)\n\n    def download_file_as_string(self, blob_name):\n        \"\"\"\n        Downloads gcs files as string\n\n        Args:\n            gcs_uri (str): gcs uri for the blob to download\n\n        Returns:\n            str: data of the blob at gcs uri\n        \"\"\"\n        blob = self.bucket.get_blob(blob_name)\n        file = blob.download_as_string()\n        return StringIO(file)\n\n    def put_objects(self, data, gcs_uri):\n        \"\"\"\n        Uploads file to desired gcs_uri\n\n        Args:\n            data (object): data that needs to be uploaded.\n            gcs_uri (str): gcs uri at which DataFrame needs to be dumped.\n        \"\"\"\n        bucket_name, blob_name = self.split_gcs_path(gcs_uri)\n        storage_client = storage.Client()\n        bucket = storage_client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        blob.upload_from_string(data)\n        return True", ""]}
{"filename": "airdot/helpers/s2i_helper.py", "chunked_list": ["import subprocess\n\n\ndef get_s2i_environment(name):\n    contents = [\n        f\"MODEL_NAME={name}\",\n        \"API_TYPE=REST\",\n        \"SERVICE_TYPE=MODEL\",\n        \"PERSISTENCE=0\",\n    ]\n    return \"\\n\".join(contents)", "\n\nclass s2i_python_helper:\n    def __init__(self, base_image, builder_image):\n        self.base_image = base_image\n        self.builder_image = builder_image\n\n    def build_image(self, source_path):\n        \"\"\"\n        Build a container image using S2I.\n\n        :param source_path: The path to the source code directory.\n        :param image_name: The name of the container image to be built.\n        \"\"\"\n        command = [\n            \"s2i\",\n            \"build\",\n            source_path,\n            self.base_image,\n            self.builder_image,\n        ]\n\n        subprocess.run(command, check=True)\n\n    def build_and_push_image(\n        self, source_path, registry_url=None, username=None, password=None\n    ):\n        \"\"\"\n        Build a container image using S2I and push it to a container registry.\n\n        :param source_path: The path to the source code directory.\n        :param image_name: The name of the container image to be built.\n        :param registry_url: The URL of the container registry to push the image to.\n        :param username: Optional username for the container registry.\n        :param password: Optional password for the container registry.\n        \"\"\"\n        self.build_image(source_path)\n\n        if username and password:\n            docker_login_command = [\n                \"docker\",\n                \"login\",\n                registry_url,\n                \"--username\",\n                username,\n                \"--password\",\n                password,\n            ]\n            subprocess.run(docker_login_command, check=True)\n\n        # docker_tag_command = [\n        #     \"docker\",\n        #     \"tag\",\n        #     f\"{self.builder_image}\",\n        # ]\n        # subprocess.run(docker_tag_command, check=True)\n\n        docker_push_command = [\"docker\", \"push\", f\"{self.builder_image}\"]\n        subprocess.run(docker_push_command, check=True)", ""]}
{"filename": "airdot/data_models/seldon_model.py", "chunked_list": ["from pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom typing import Optional\n\n\nclass SeldonMetadata(BaseModel):\n    labels: Optional[dict] = {\"app\": \"seldon\"}\n    name: Optional[str] = \"seldon_app\"\n\n\nclass SeldonAnnotations(BaseModel):\n    project_name: Optional[str] = \"seldon_model\"\n    deployment_version: Optional[str] = \"0\"", "\n\nclass SeldonAnnotations(BaseModel):\n    project_name: Optional[str] = \"seldon_model\"\n    deployment_version: Optional[str] = \"0\"\n\n\nclass SeldonContainer(BaseModel):\n    name: str\n    imagePullPolicy: Optional[str] = \"IfNotPresent\"\n    resources: Optional[dict] = {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}}", "\n\nclass SeldonPodSpecs(BaseModel):\n    nodeSepector: Optional[dict]\n    containers: Optional[SeldonContainer] = {\n        \"name\": \"seldon_container\",\n        \"image\": \"None\",\n        \"imagePullPolicy\": \"IfNotPresent\",\n        \"resources\": {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}},\n    }", "\n\nclass SeldonComponentSpecs(BaseModel):\n    specs: SeldonPodSpecs\n\n\nclass SeldonSpecs(BaseModel):\n    annotations: Optional[SeldonAnnotations]\n    name: Optional[str] = \"seldon_model\"\n    predictors: Optional[SeldonComponentSpecs]", "\n\nclass SeldonGraph(BaseModel):\n    name: Optional[str] = {\"name\": \"seldon_model\"}\n\n\nclass SeldonConfiguration(BaseModel):\n    apiVersion: Optional[\n        Literal[\"machinelearning.seldon.io/v1\", \"machinelearning.seldon.io/v1alpha2\"]\n    ] = \"machinelearning.seldon.io/v1\"\n    kind: Optional[str] = \"SeldonDeployment\"\n    metadata: Optional[SeldonMetadata] = {\"name\": \"seldon_model\"}\n    specs: Optional[SeldonSpecs] = {\n        \"specs\":{\n            \"name\": \"None\",\n            \"predictors\": {\n                \"componentSpecs\": {\n                    \"specs\": {\n                        \"container\": {\n                            \"name\": \"None\",\n                            \"image\": \"None\",\n                            \"imagePullPolicy\": \"ifNotPresent\",\n                            \"resources\": {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}},\n                        }\n                    }\n                }\n            },\n        }\n    }\n    graph: Optional[SeldonGraph] = {\"name\": \"None\"}\n    name: Optional[str] = \"default\"\n    replicas: Optional[int] = 1", ""]}
{"filename": "airdot/data_models/kserve_model.py", "chunked_list": ["from pydantic import BaseModel\nfrom typing_extensions import Literal\n\n\nclass KServe(BaseModel):\n    pass\n"]}
{"filename": "airdot/data_models/__init__.py", "chunked_list": [""]}
{"filename": "airdot/data_models/deployment.py", "chunked_list": ["from pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom typing import Optional\nfrom airdot.data_models.seldon_model import SeldonConfiguration\nfrom airdot.data_models.kserve_model import KServe\n\n\nclass Deployment(BaseModel):\n    deployment_type: Literal[\"seldon\", \"kserve\", \"local\"]\n    bucket_type: Optional[str] = None,\n    image_uri: Optional[str] = None\n    seldon_configuration: Optional[SeldonConfiguration] = {\n                \"apiVersion\": \"None\",\n                \"kind\": \"SeldonDeployment\",\n                \"metadata\": {\n                    \"name\": \"None\",\n                    \"namespace\":\"None\"\n                },\n                \"spec\": {\n                    \"name\": \"seldon-test\",\n                    \"predictors\": [{\n                    \"componentSpecs\":[{\n                        \"spec\": {\n                        \"containers\": [{\n                            \"name\": \"None\",\n                            \"image\": \"None\",\n                            \"imagePullPolicy\": \"Always\",\n                            \"resources\": {\n                            \"requests\": {\n                                \"cpu\": \"1\",\n                                \"memory\": \"1M\"\n                            }\n                            }\n                        }]\n                        }\n                    }],\n                    \"graph\":{\n                        \"children\":[],\n                        \"name\":\"None\",\n                        \"endpoint\": {\n                            \"type\":\"REST\"\n                        },\n                        \"type\": \"MODEL\"\n                    },\n                    \"name\":\"seldon-test\",\n                    \"replicas\":1\n                    }]\n                }\n            }\n    kserve_configuration: Optional[KServe]", ""]}
{"filename": "tests/deployer_test.py", "chunked_list": ["from airdot import Deployer\n\nimport pandas as pd\ndeployer = Deployer() \n\n\n# declare a function\ndf2 = pd.DataFrame(data=[[10,20],[10,40]], columns=['1', '2'])\n\n# def func_one(value):", "\n# def func_one(value):\n#     return value\n\n# def func_two(value):\n#     return func_one(value)\n\n\ndef get_value_data(cl_idx='1'):\n    return df2[cl_idx].values.tolist()", "def get_value_data(cl_idx='1'):\n    return df2[cl_idx].values.tolist()\n\ndeployer.run(get_value_data) # to deploy local\n\n# # \n\n# deployer.list_deployments() # to list all deployments\n\n# df2 = pd.DataFrame(data=[[3,4],[7,8]], columns=['1', '2'])", "\n# df2 = pd.DataFrame(data=[[3,4],[7,8]], columns=['1', '2'])\n# deployer.update_objects(('df2',df2), 'get_value_data')\n\n# #deployer.stop('get_value_data') # to stop container"]}
{"filename": "tests/__init__.py", "chunked_list": ["from tests.test_files.example_1 import func_4\n\n__all__ = [\"func_4\"]\n"]}
{"filename": "tests/test_files/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_files/example_1.py", "chunked_list": ["import sys\n\n\ndef func_1(val):\n    print(val)\n\n\ndef func_2(val):\n    func_1(val)\n", "\n\ndef func_3(val):\n    func_2(val)\n\n\ndef func_4(val=4):\n    return func_3(val)\n\n\nif __name__ == \"__main__\":\n    func_4()", "\n\nif __name__ == \"__main__\":\n    func_4()\n"]}
