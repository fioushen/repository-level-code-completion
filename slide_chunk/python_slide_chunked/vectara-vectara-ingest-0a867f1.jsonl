{"filename": "ingest.py", "chunked_list": ["import logging\nimport json\nimport requests\nimport time\nfrom omegaconf import OmegaConf, DictConfig\nimport toml     # type: ignore\nimport sys\nimport os\nfrom typing import Any\n", "from typing import Any\n\nimport importlib\nfrom core.crawler import Crawler\nfrom authlib.integrations.requests_client import OAuth2Session\n\ndef instantiate_crawler(base_class, folder_name: str, class_name: str, *args, **kwargs) -> Any:   # type: ignore\n    sys.path.insert(0, os.path.abspath(folder_name))\n\n    crawler_name = class_name.split('Crawler')[0]\n    module_name = f\"{folder_name}.{crawler_name.lower()}_crawler\"  # Construct the full module path\n    module = importlib.import_module(module_name)\n    \n    class_ = getattr(module, class_name)\n\n    # Ensure the class is a subclass of the base class\n    if not issubclass(class_, base_class):\n        raise TypeError(f\"{class_name} is not a subclass of {base_class.__name__}\")\n\n    # Instantiate the class and return the instance\n    return class_(*args, **kwargs)", "\ndef get_jwt_token(auth_url: str, auth_id: str, auth_secret: str, customer_id: str) -> Any:\n    \"\"\"Connect to the server and get a JWT token.\"\"\"\n    token_endpoint = f'{auth_url}/oauth2/token'\n    session = OAuth2Session(auth_id, auth_secret, scope=\"\")\n    token = session.fetch_token(token_endpoint, grant_type=\"client_credentials\")\n    return token[\"access_token\"]\n\ndef reset_corpus(endpoint: str, customer_id: str, corpus_id: int, auth_url: str, auth_id: str, auth_secret: str) -> None:\n    \"\"\"\n    Reset the corpus by deleting all documents and metadata.\n\n    Args:\n        endpoint (str): Endpoint for the Vectara API.\n        customer_id (str): ID of the Vectara customer.\n        appclient_id (str): ID of the Vectara app client.\n        appclient_secret (str): Secret key for the Vectara app client.\n        corpus_id (int): ID of the Vectara corpus to index to.\n    \"\"\"\n    url = f\"https://{endpoint}/v1/reset-corpus\"\n    payload = json.dumps({\n        \"customerId\": customer_id,\n        \"corpusId\": corpus_id\n    })\n    token = get_jwt_token(auth_url, auth_id, auth_secret, customer_id)\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n        'customer-id': str(customer_id),\n        'Authorization': f'Bearer {token}'\n    }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    if response.status_code == 200:\n        logging.info(f\"Reset corpus {corpus_id}\")\n    else:\n        logging.error(f\"Error resetting corpus: {response.status_code} {response.text}\")", "def reset_corpus(endpoint: str, customer_id: str, corpus_id: int, auth_url: str, auth_id: str, auth_secret: str) -> None:\n    \"\"\"\n    Reset the corpus by deleting all documents and metadata.\n\n    Args:\n        endpoint (str): Endpoint for the Vectara API.\n        customer_id (str): ID of the Vectara customer.\n        appclient_id (str): ID of the Vectara app client.\n        appclient_secret (str): Secret key for the Vectara app client.\n        corpus_id (int): ID of the Vectara corpus to index to.\n    \"\"\"\n    url = f\"https://{endpoint}/v1/reset-corpus\"\n    payload = json.dumps({\n        \"customerId\": customer_id,\n        \"corpusId\": corpus_id\n    })\n    token = get_jwt_token(auth_url, auth_id, auth_secret, customer_id)\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n        'customer-id': str(customer_id),\n        'Authorization': f'Bearer {token}'\n    }\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n    if response.status_code == 200:\n        logging.info(f\"Reset corpus {corpus_id}\")\n    else:\n        logging.error(f\"Error resetting corpus: {response.status_code} {response.text}\")", "                      \n\ndef main() -> None:\n    \"\"\"\n    Main function that runs the web crawler based on environment variables.\n    \n    Reads the necessary environment variables and sets up the web crawler\n    accordingly. Starts the crawl loop and logs the progress and errors.\n    \"\"\"\n\n    if len(sys.argv) != 3:\n        logging.info(\"Usage: python ingest.py <config_file> <secrets-profile>\")\n        return\n    config_name = sys.argv[1]\n    profile_name = sys.argv[2]\n\n    # process arguments \n    cfg: DictConfig = DictConfig(OmegaConf.load(config_name))\n    \n    # add .env params, by profile\n    volume = '/home/vectara/env'\n    with open(f\"{volume}/secrets.toml\", 'r') as f:\n        env_dict = toml.load(f)\n    if profile_name not in env_dict:\n        logging.info(f'Profile \"{profile_name}\" not found in secrets.toml')\n        return\n    env_dict = env_dict[profile_name]\n\n    for k,v in env_dict.items():\n        if k=='HUBSPOT_API_KEY':\n            OmegaConf.update(cfg, f'hubspot_crawler.{k.lower()}', v)\n            continue\n        if k=='NOTION_API_KEY':\n            OmegaConf.update(cfg, f'notion_crawler.{k.lower()}', v)\n            continue\n        if k=='DISCOURSE_API_KEY':\n            OmegaConf.update(cfg, f'discourse_crawler.{k.lower()}', v)\n            continue\n        if k=='FMP_API_KEY':\n            OmegaConf.update(cfg, f'fmp_crawler.{k.lower()}', v)\n            continue\n        if k=='JIRA_PASSWORD':\n            OmegaConf.update(cfg, f'jira_crawler.{k.lower()}', v)\n            continue\n        if k=='GITHUB_TOKEN':\n            OmegaConf.update(cfg, f'github_crawler.{k.lower()}', v)\n            continue\n        if k.startswith('aws_'):\n            OmegaConf.update(cfg, f's3_crawler.{k.lower()}', v)\n            continue\n\n        # default (otherwise) - add to vectara config\n        OmegaConf.update(cfg['vectara'], k, v)\n\n    endpoint = 'api.vectara.io'\n    customer_id = cfg.vectara.customer_id\n    corpus_id = cfg.vectara.corpus_id\n    api_key = cfg.vectara.api_key\n    crawler_type = cfg.crawling.crawler_type\n\n    # instantiate the crawler\n    crawler = instantiate_crawler(Crawler, 'crawlers', f'{crawler_type.capitalize()}Crawler', cfg, endpoint, customer_id, corpus_id, api_key)\n\n    # When debugging a crawler, it is sometimes useful to reset the corpus (remove all documents)\n    # To do that you would have to set this to True and also include <auth_url> and <auth_id> in the secrets.toml file\n    # NOTE: use with caution; this will delete all documents in the corpus and is irreversible\n    reset_corpus_flag = False\n    if reset_corpus_flag:\n        logging.info(\"Resetting corpus\")\n        reset_corpus(endpoint, customer_id, corpus_id, cfg.vectara.auth_url, cfg.vectara.auth_id, cfg.vectara.auth_secret)\n        time.sleep(5)   # wait 5 seconds to allow reset_corpus enough time to complete on the backend\n    logging.info(f\"Starting crawl of type {crawler_type}...\")\n    crawler.crawl()\n    logging.info(f\"Finished crawl of type {crawler_type}...\")", "\nif __name__ == '__main__':\n    logging.basicConfig(format=\"%(asctime)s %(levelname)-8s %(message)s\", level=logging.INFO)\n    main()"]}
{"filename": "core/indexer.py", "chunked_list": ["import logging\nimport json\nimport os\nfrom typing import Tuple, Dict, Any, List, Optional\n\nfrom slugify import slugify         \nimport time\nfrom core.utils import create_session_with_retries\n\nfrom goose3 import Goose", "\nfrom goose3 import Goose\nfrom goose3.text import StopWordsArabic, StopWordsKorean, StopWordsChinese\n\nimport justext\nfrom bs4 import BeautifulSoup\n\nfrom omegaconf import OmegaConf\nfrom nbconvert import HTMLExporter      # type: ignore\nimport nbformat", "from nbconvert import HTMLExporter      # type: ignore\nimport nbformat\nimport markdown\nimport docutils.core\nfrom core.utils import html_to_text, detect_language, get_file_size_in_MB\n\nfrom playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError\n\nfrom unstructured.partition.auto import partition\nimport unstructured as us", "from unstructured.partition.auto import partition\nimport unstructured as us\n\n\n\nlanguage_stopwords_Goose = {\n    'en': None,  # English stopwords are the default\n    'ar': StopWordsArabic,  # Use Arabic stopwords\n    'zh-cn': StopWordsChinese,  # Use Chinese stopwords\n    'zh-tw': StopWordsChinese,  # Use Chinese stopwords", "    'zh-cn': StopWordsChinese,  # Use Chinese stopwords\n    'zh-tw': StopWordsChinese,  # Use Chinese stopwords\n    'ko': StopWordsKorean  # Use Korean stopwords\n        }\n\nlanguage_stopwords_JusText = {\n    'en': None,  # English stopwords are the default\n    'ar': 'Arabic',  # Use Arabic stopwords\n    'ko': 'Korean' , # Use Korean stopwords\n    'ur': 'Urdu' ,  # Use urdu stopwords", "    'ko': 'Korean' , # Use Korean stopwords\n    'ur': 'Urdu' ,  # Use urdu stopwords\n    'hi': 'Hindi' ,  # Use Hindi stopwords \n    'fa': 'Persian',  # Use Persian stopwords\n    'ja': 'Japanese',  # Use Japanese stopwords\n    'bg': 'Bulgarian',  # Use Bulgarian stopwords\n    'sv': 'Swedish',  # Use Swedish stopwords\n    'lv': 'Latvian',  # Use Latvian stopwords\n    'sr': 'Serbian',  # Use Serbian stopwords\n    'sq': 'Albanian',  # Use Albanian stopwords", "    'sr': 'Serbian',  # Use Serbian stopwords\n    'sq': 'Albanian',  # Use Albanian stopwords\n    'es': 'Spanish',  # Use Spanish stopwords\n    'ka': 'Gerogian',  # Use Georgian stopwords\n    'de': 'German',  # Use German stopwords\n    'el': 'Greek',  # Use Greek stopwords\n    'ga': 'Irish',  # Use Irish stopwords\n    'vi': 'Vietnamese',  # Use Vietnamese stopwords\n    'hu': 'Hungarian',  # Use Hungarian stopwords\n    'pt': 'Portuguese',  # Use Portuguese stopwords", "    'hu': 'Hungarian',  # Use Hungarian stopwords\n    'pt': 'Portuguese',  # Use Portuguese stopwords\n    'pl': 'Polish',  # Use Polish stopwords\n    'it': 'Italian',  # Use Italian stopwords\n    'la': 'Latin',  # Use Latin stopwords\n    'tr': 'Turkish',  # Use Turkish stopwords\n    'id': 'Indonesian',  # Use Indonesian stopwords\n    'hr': 'Croatian',  # Use Croatian stopwords\n    'be': 'Belarusian',  # Use Belarusian stopwords\n    'ru': 'Russian',  # Use Russian stopwords", "    'be': 'Belarusian',  # Use Belarusian stopwords\n    'ru': 'Russian',  # Use Russian stopwords\n    'et': 'Estonian',  # Use Estonian stopwords\n    'uk': 'Ukranian',  # Use Ukranian stopwords\n    'ro': 'Romanian',  # Use Romanian stowords\n    'cs': 'Czech',  # Use Czech stopwords\n    'ml': 'Malyalam',  # Use Malyalam stopwords\n    'sk': 'Slovak',  # Use Slovak stopwords\n    'fi': 'Finnish',  # Use Finnish stopwords\n    'da': 'Danish',  # Use Danish stopwords", "    'fi': 'Finnish',  # Use Finnish stopwords\n    'da': 'Danish',  # Use Danish stopwords\n    'ms': 'Malay',  # Use Malay stopwords\n    'ca': 'Catalan',  # Use Catalan stopwords\n    'eo': 'Esperanto',  # Use Esperanto stopwords\n    'nb': 'Norwegian_Bokmal',  # Use Norwegian_Bokmal stopwords\n    'nn': 'Norwegian_Nynorsk'  # Use Norwegian_Nynorsk stopwords\n    # Add more languages and their stopwords keywords here\n}\n", "}\n\n\n\ndef get_content_with_justext(html_content: str, detected_language: str) -> Tuple[str, str]:\n    if detected_language == 'en':\n        paragraphs = justext.justext(html_content, justext.get_stoplist(\"English\")) \n    else:\n        stopwords_keyword = language_stopwords_JusText.get(detected_language, 'English')\n    \n    # Extract paragraphs using the selected stoplist\n        paragraphs = justext.justext(html_content, justext.get_stoplist(stopwords_keyword))\n    text = '\\n'.join([p.text for p in paragraphs if not p.is_boilerplate])\n    soup = BeautifulSoup(html_content, 'html.parser')\n    stitle = soup.find('title')\n    if stitle:\n        title = stitle.text\n    else:\n        title = 'No title'\n    return text, title", "\ndef get_content_with_goose3(html_content: str, url: str, detected_language: str) -> Tuple[str, str]:\n    if detected_language in language_stopwords_Goose:\n        stopwords_class = language_stopwords_Goose.get(detected_language, None)\n        \n        if stopwords_class is not None:\n            g = Goose({'stopwords_class': stopwords_class})\n        else:\n            g = Goose()  # Use the default stopwords for languages that don't have a configured StopWords class\n\n        article = g.extract(url=url, raw_html=html_content)\n        title = article.title\n        text = article.cleaned_text\n        return text, title\n    else:\n        title = \"\"\n        text = \"\"\n        logging.info(f\"{detected_language} is not supported by Goose\")\n        return text, title", "\n\ndef get_content_and_title(html_content: str, url: str, detected_language: str) -> Tuple[str, str]:\n    text1, title1 = get_content_with_goose3(html_content, url, detected_language)\n    text2, title2 = get_content_with_justext(html_content, detected_language)\n    \n    # both Goose and Justext do extraction without boilerplate; return the one that produces the longest text, trying to optimize for content\n    if len(text1)>len(text2):\n        return text1, title1\n    else:\n        return text2, title2", "\nclass Indexer(object):\n    \"\"\"\n    Vectara API class.\n    Args:\n        endpoint (str): Endpoint for the Vectara API.\n        customer_id (str): ID of the Vectara customer.\n        corpus_id (int): ID of the Vectara corpus to index to.\n        api_key (str): API key for the Vectara API.\n    \"\"\"\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str, reindex: bool = True) -> None:\n        self.cfg = cfg\n        self.endpoint = endpoint\n        self.customer_id = customer_id\n        self.corpus_id = corpus_id\n        self.api_key = api_key\n        self.reindex = reindex\n        self.timeout = 30\n        self.detected_language: Optional[str] = None\n\n        # setup requests session and mount adapter to retry requests\n        self.session = create_session_with_retries()\n\n        # create playwright browser so we can reuse it across all Indexer operations\n        self.p = sync_playwright().start()\n        self.browser = self.p.firefox.launch(headless=True)\n\n    def fetch_content_with_timeout(self, url: str, timeout: int = 30) -> Tuple[str, str] :\n        '''\n        Fetch content from a URL with a timeout.\n        Args:\n            url (str): URL to fetch.\n            timeout (int, optional): Timeout in seconds. Defaults to 30.\n        Returns:\n            str: Content from the URL.\n        '''\n        page = context = None\n        try:\n            context = self.browser.new_context()\n            page = context.new_page()\n            page.route(\"**/*\", lambda route: route.abort()  # do not load images as they are unnecessary for our purpose\n                if route.request.resource_type == \"image\" \n                else route.continue_() \n            ) \n            page.goto(url, timeout=timeout*1000)\n            content = page.content()\n            out_url = page.url\n        except PlaywrightTimeoutError:\n            logging.info(f\"Page loading timed out for {url}\")\n            return '', ''\n        except Exception as e:\n            logging.info(f\"Page loading failed for {url} with exception '{e}'\")\n            content = ''\n            out_url = ''\n            if not self.browser.is_connected():\n                self.browser = self.p.firefox.launch(headless=True)\n        finally:\n            if context:\n                context.close()\n            if page:\n                page.close()\n            \n        return out_url, content\n\n    # delete document; returns True if successful, False otherwise\n    def delete_doc(self, doc_id: str) -> bool:\n        \"\"\"\n        Delete a document from the Vectara corpus.\n\n        Args:\n            url (str): URL of the page to delete.\n            doc_id (str): ID of the document to delete.\n\n        Returns:\n            bool: True if the delete was successful, False otherwise.\n        \"\"\"\n        body = {'customer_id': self.customer_id, 'corpus_id': self.corpus_id, 'document_id': doc_id}\n        post_headers = { 'x-api-key': self.api_key, 'customer-id': str(self.customer_id) }\n        response = self.session.post(\n            f\"https://{self.endpoint}/v1/delete-doc\", data=json.dumps(body),\n            verify=True, headers=post_headers)\n        \n        if response.status_code != 200:\n            logging.error(f\"Delete request failed for doc_id = {doc_id} with status code {response.status_code}, reason {response.reason}, text {response.text}\")\n            return False\n        return True\n    \n    def index_file(self, filename: str, uri: str, metadata: Dict[str, Any]) -> bool:\n        \"\"\"\n        Index a file on local file system by uploading it to the Vectara corpus.\n        Args:\n            filename (str): Name of the PDF file to create.\n            uri (str): URI for where the document originated. In some cases the local file name is not the same, and we want to include this in the index.\n            metadata (dict): Metadata for the document.\n        Returns:\n            bool: True if the upload was successful, False otherwise.\n        \"\"\"\n        if os.path.exists(filename) == False:\n            logging.error(f\"File {filename} does not exist\")\n            return False\n\n        # if file size is more than 50MB, then we extract the text locally and send over with standard API\n        if filename.endswith(\".pdf\") and get_file_size_in_MB(filename) > 50:\n            elements = partition(filename)\n            parts = [str(t) for t in elements if type(t)!=us.documents.elements.Title]\n            titles = [str(x) for x in elements if type(x)==us.documents.elements.Title and len(str(x))>20]\n            title = titles[0] if len(titles)>0 else 'unknown'\n            succeeded = self.index_segments(doc_id=slugify(filename), parts=parts, metadatas=[{}]*len(parts), \n                                            doc_metadata=metadata, title=title)\n            logging.info(f\"For file {filename}, indexing text only since file size is larger than 50MB\")\n            return succeeded\n\n        post_headers = { \n            'x-api-key': self.api_key,\n            'customer-id': str(self.customer_id),\n        }\n\n        files: Any = {\n            \"file\": (uri, open(filename, 'rb')),\n            \"doc_metadata\": (None, json.dumps(metadata)),\n        }  \n        response = self.session.post(\n            f\"https://{self.endpoint}/upload?c={self.customer_id}&o={self.corpus_id}&d=True\",\n            files=files, verify=True, headers=post_headers)\n\n        if response.status_code == 409:\n            if self.reindex:\n                doc_id = response.json()['details'].split('document id')[1].split(\"'\")[1]\n                self.delete_doc(doc_id)\n                response = self.session.post(\n                    f\"https://{self.endpoint}/upload?c={self.customer_id}&o={self.corpus_id}\",\n                    files=files, verify=True, headers=post_headers)\n                if response.status_code == 200:\n                    logging.info(f\"REST upload for {uri} successful (reindex)\")\n                    return True\n                else:\n                    logging.info(f\"REST upload for {uri} failed with code = {response.status_code}, text = {response.text}\")\n                    return True\n            return False\n        elif response.status_code != 200:\n            logging.error(f\"REST upload for {uri} failed with code {response.status_code}, text = {response.text}\")\n            return False\n\n        logging.info(f\"REST upload for {uri} succeesful\")\n        return True\n\n    def index_url(self, url: str, metadata: Dict[str, Any]) -> bool:\n        \"\"\"\n        Index a url by rendering it with scrapy-playwright, extracting paragraphs, then uploading to the Vectara corpus.\n        Args:\n            url (str): URL for where the document originated. \n            metadata (dict): Metadata for the document.\n        Returns:\n            bool: True if the upload was successful, False otherwise.\n        \"\"\"\n        st = time.time()\n        try:\n            headers = {\n                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\"\n            }\n            response = self.session.get(url, headers=headers, timeout=self.timeout)\n            if response.status_code != 200:\n                logging.info(f\"Page {url} is unavailable ({response.status_code})\")\n                return False\n            else:\n                content_type = str(response.headers[\"Content-Type\"])\n        except Exception as e:\n            logging.info(f\"Failed to crawl {url} to get content_type, skipping...\")\n            return False\n        \n        # read page content: everything is translated into various segments (variable \"elements\") so that we can use index_segment()\n        # If PDF then use partition from  \"unstructured.io\" to extract the content\n        if content_type == 'application/pdf' or url.endswith(\".pdf\"):\n            try:\n                response = self.session.get(url, timeout=self.timeout)\n                response.raise_for_status()\n                fname = 'tmp.pdf'\n                with open(fname, 'wb') as f:\n                    f.write(response.content)\n                elements = partition(fname)\n                parts = [str(t) for t in elements if type(t)!=us.documents.elements.Title]\n                titles = [str(x) for x in elements if type(x)==us.documents.elements.Title and len(str(x))>20]\n                extracted_title = titles[0] if len(titles)>0 else 'unknown'\n            except Exception as e:\n                logging.info(f\"Failed to crawl {url} to get PDF content with error {e}, skipping...\")\n                return False\n\n        # If MD, RST of IPYNB file, then we don't need playwright - can just download content directly and convert to text\n        elif url.endswith(\".md\") or url.endswith(\".rst\") or url.lower().endswith(\".ipynb\"):\n            response = self.session.get(url, timeout=self.timeout)\n            response.raise_for_status()\n            dl_content = response.content.decode('utf-8')\n            if url.endswith('rst'):\n                html_content = docutils.core.publish_string(dl_content, writer_name='html')\n            elif url.endswith('md'):\n                html_content = markdown.markdown(dl_content)\n            elif url.lower().endswith('ipynb'):\n                nb = nbformat.reads(dl_content, nbformat.NO_CONVERT)    # type: ignore\n                exporter = HTMLExporter()\n                html_content, _ = exporter.from_notebook_node(nb)\n            extracted_title = url.split('/')[-1]      # no title in these files, so using file name\n            text = html_to_text(html_content)\n            parts = [text]\n\n        # for everything else, use PlayWright as we may want it to render JS on the page before reading the content\n        else:\n            try:\n                actual_url, html_content = self.fetch_content_with_timeout(url)\n                if html_content is None or len(html_content)<3:\n                    return False\n                if self.detected_language is None:\n                    soup = BeautifulSoup(html_content, 'html.parser')\n                    body_text = soup.body.get_text()\n                    self.detected_language = detect_language(body_text)\n                    logging.info(f\"The detected language is {self.detected_language}\")\n                url = actual_url\n                text, extracted_title = get_content_and_title(html_content, url, self.detected_language)\n                parts = [text]\n                logging.info(f\"retrieving content took {time.time()-st:.2f} seconds\")\n            except Exception as e:\n                import traceback\n                logging.info(f\"Failed to crawl {url}, skipping due to error {e}, traceback={traceback.format_exc()}\")\n                return False\n\n        succeeded = self.index_segments(doc_id=slugify(url), parts=parts, metadatas=[{}]*len(parts), \n                                        doc_metadata=metadata, title=extracted_title)\n        if succeeded:\n            return True\n        else:\n            return False\n\n    def index_segments(self, doc_id: str, parts: List[str], metadatas: List[Dict[str, Any]], doc_metadata: Dict[str, Any] = {}, title: str = \"\") -> bool:\n        \"\"\"\n        Index a document (by uploading it to the Vectara corpus) from the set of segments (parts) that make up the document.\n        \"\"\"\n        document = {}\n        document[\"documentId\"] = doc_id\n        if title:\n            document[\"title\"] = title\n        document[\"section\"] = [{\"text\": part, \"metadataJson\": json.dumps(md)} for part,md in zip(parts,metadatas)]  # type: ignore\n        if doc_metadata:\n            document[\"metadataJson\"] = json.dumps(doc_metadata)\n        return self.index_document(document)\n    \n    def index_document(self, document: Dict[str, Any]) -> bool:\n        \"\"\"\n        Index a document (by uploading it to the Vectara corpus) from the document dictionary\n        \"\"\"\n        api_endpoint = f\"https://{self.endpoint}/v1/index\"\n\n        request = {\n            'customer_id': self.customer_id,\n            'corpus_id': self.corpus_id,\n            'document': document,\n        }\n\n        post_headers = { \n            'x-api-key': self.api_key,\n            'customer-id': str(self.customer_id),\n        }\n        try:\n            data = json.dumps(request)\n        except Exception as e:\n            logging.info(f\"Can't serialize request {request}, skipping\")   \n            return False\n\n        try:\n            response = self.session.post(api_endpoint, data=data, verify=True, headers=post_headers)\n        except Exception as e:\n            logging.info(f\"Exception {e} while indexing document {document['documentId']}\")\n            return False\n\n        if response.status_code != 200:\n            logging.error(\"REST upload failed with code %d, reason %s, text %s\",\n                          response.status_code,\n                          response.reason,\n                          response.text)\n            return False\n\n        result = response.json()\n        if \"status\" in result and result[\"status\"] and \\\n           (\"ALREADY_EXISTS\" in result[\"status\"][\"code\"] or \\\n            (\"CONFLICT\" in result[\"status\"][\"code\"] and \"Indexing doesn't support updating documents\" in result[\"status\"][\"statusDetail\"])):\n            if self.reindex:\n                logging.info(f\"Document {document['documentId']} already exists, re-indexing\")\n                self.delete_doc(document['documentId'])\n                response = self.session.post(api_endpoint, data=json.dumps(request), verify=True, headers=post_headers)\n                return True\n            else:\n                logging.info(f\"Document {document['documentId']} already exists, skipping\")\n                return False\n        if \"status\" in result and result[\"status\"] and \"OK\" in result[\"status\"][\"code\"]:\n            return True\n        \n        logging.info(f\"Indexing document {document['documentId']} failed, response = {result}\")\n        return False", "\n"]}
{"filename": "core/crawler.py", "chunked_list": ["from omegaconf import OmegaConf, DictConfig\nfrom slugify import slugify\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nimport logging\nfrom typing import Set, Optional, List, Any\nfrom core.indexer import Indexer\nfrom core.pdf_convert import PDFConverter\nfrom core.utils import binary_extensions, doc_extensions", "from core.pdf_convert import PDFConverter\nfrom core.utils import binary_extensions, doc_extensions\n\ndef recursive_crawl(url: str, depth: int, url_regex: List[Any], visited: Optional[Set[str]]=None, session: Optional[requests.Session]=None) -> Set[str]:\n    if depth <= 0:\n        return set() if visited is None else set(visited)\n\n    if visited is None:\n        visited = set()\n    if session is None:\n        session = requests.Session()\n\n    # For binary files - we don't extract links from them, nor are they included in the crawled URLs list\n    # for document files (like PPT, DOCX, etc) we don't extract links from the, but they ARE included in the crawled URLs list\n    url_without_fragment = url.split(\"#\")[0]\n    if any([url_without_fragment.endswith(ext) for ext in binary_extensions]):\n        return visited\n    visited.add(url)\n    if any([url_without_fragment.endswith(ext) for ext in doc_extensions]):\n        return visited\n\n    try:\n        response = session.get(url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        # Find all anchor tags and their href attributes\n        new_urls = [urljoin(url, link[\"href\"]) for link in soup.find_all(\"a\") if \"href\" in link.attrs]\n        new_urls = [u for u in new_urls if u not in visited and u.startswith('http') and any([r.match(u) for r in url_regex])]\n        new_urls = list(set(new_urls))\n        visited.update(new_urls)\n        for new_url in new_urls:\n            visited = recursive_crawl(new_url, depth-1, url_regex, visited, session)\n    except Exception as e:\n        logging.info(f\"Error {e} in recursive_crawl for {url}\")\n        pass\n\n    return set(visited)", "\n\nclass Crawler(object):\n    \"\"\"\n    Base class for a crawler that indexes documents to a Vectara corpus.\n\n    Args:\n        endpoint (str): Endpoint for the Vectara API.\n        customer_id (str): ID of the Vectara customer.\n        token (str): Bearer JWT token\n        corpus_id (int): ID of the Vectara corpus to index to.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: OmegaConf,\n        endpoint: str,\n        customer_id: str,\n        corpus_id: int,\n        api_key: str,\n    ) -> None:\n        self.cfg: DictConfig = DictConfig(cfg)\n        reindex = self.cfg.vectara.get(\"reindex\", False)\n        self.indexer = Indexer(cfg, endpoint, customer_id, corpus_id, api_key, reindex)\n\n    def url_to_file(self, url: str, title: str) -> str:\n        \"\"\"\n        Crawl a single webpage and create a PDF file to reflect its rendered content.\n\n        Args:\n            url (str): URL of the page to crawl.\n            title (str): Title to use in case HTML does not have its own title.\n\n        Returns:\n            str: Name of the PDF file created.\n        \"\"\"\n        # first verify the URL is valid\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n            \"Accept-Encoding\": \"gzip, deflate\",\n            \"Connection\": \"keep-alive\",\n            \"Upgrade-Insecure-Requests\": \"1\",\n            \"Sec-Fetch-Dest\": \"document\",\n            \"Sec-Fetch-Mode\": \"navigate\",\n            \"Sec-Fetch-Site\": \"none\",\n            \"Sec-Fetch-User\": \"?1\",\n            \"Cache-Control\": \"max-age=0\",\n        }\n\n        response = requests.get(url, headers=headers)\n        if response.status_code != 200:\n            if response.status_code == 404:\n                raise Exception(f\"Error 404 - URL not found: {url}\")\n            elif response.status_code == 401:\n                raise Exception(f\"Error 403 - Unauthorized: {url}\")\n            elif response.status_code == 403:\n                raise Exception(f\"Error 403 - Access forbidden: {url}\")\n            elif response.status_code == 405:\n                raise Exception(f\"Error 405 - Method not allowed: {url}\")\n            else:\n                raise Exception(\n                    f\"Invalid URL: {url} (status code={response.status_code}, reason={response.reason})\"\n                )\n\n        if title is None:\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            title = soup.title.string\n\n        # convert to local file (PDF)\n        filename = slugify(url) + \".pdf\"\n        if not PDFConverter(use_pdfkit=False).from_url(url, filename, title=title):\n            raise Exception(f\"Failed to convert {url} to PDF\")\n\n        return filename\n\n    def crawl(self) -> None:\n        raise Exception(\"Not implemented\")", ""]}
{"filename": "core/__init__.py", "chunked_list": ["\n"]}
{"filename": "core/utils.py", "chunked_list": ["from bs4 import BeautifulSoup\nimport requests\nfrom urllib.parse import urlparse, urlunparse, ParseResult\nimport re\nfrom langdetect import detect\nfrom typing import List, Set\nimport os\n\nimg_extensions = [\"gif\", \"jpeg\", \"jpg\", \"mp3\", \"mp4\", \"png\", \"svg\", \"bmp\", \"eps\", \"ico\"]\ndoc_extensions = [\"doc\", \"docx\", \"ppt\", \"pptx\", \"xls\", \"xlsx\", \"pdf\", \"ps\"]", "img_extensions = [\"gif\", \"jpeg\", \"jpg\", \"mp3\", \"mp4\", \"png\", \"svg\", \"bmp\", \"eps\", \"ico\"]\ndoc_extensions = [\"doc\", \"docx\", \"ppt\", \"pptx\", \"xls\", \"xlsx\", \"pdf\", \"ps\"]\narchive_extensions = [\"zip\", \"gz\", \"tar\", \"bz2\", \"7z\", \"rar\"]\nbinary_extensions = archive_extensions + img_extensions + doc_extensions\n\ndef html_to_text(html: str) -> str:\n    soup = BeautifulSoup(html, features='html.parser')\n    return soup.get_text()\n\ndef create_session_with_retries(retries: int = 3) -> requests.Session:\n    session = requests.Session()\n    adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session", "\ndef create_session_with_retries(retries: int = 3) -> requests.Session:\n    session = requests.Session()\n    adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session\n\ndef remove_anchor(url: str) -> str:\n    parsed = urlparse(url)\n    url_without_anchor = urlunparse(parsed._replace(fragment=\"\"))\n    return url_without_anchor", "def remove_anchor(url: str) -> str:\n    parsed = urlparse(url)\n    url_without_anchor = urlunparse(parsed._replace(fragment=\"\"))\n    return url_without_anchor\n\n\ndef normalize_url(url: str) -> str:\n    \"\"\"Normalize a URL by removing 'www', and query parameters.\"\"\"    \n    # Prepend with 'http://' if URL has no scheme\n    if '://' not in url:\n        url = 'http://' + url\n    p = urlparse(url)\n    \n    # Remove 'www.'\n    netloc = p.netloc.replace('www.', '')\n    \n    # Remove query parameters\n    path = p.path.split('?', 1)[0]\n\n    # Reconstruct URL with scheme, without 'www', and query parameters\n    return ParseResult(p.scheme, netloc, path, '', '', '').geturl()", "\ndef clean_urls(urls: Set[str]) -> List[str]:\n    return list(set(normalize_url(url) for url in urls))\n\ndef clean_email_text(text: str) -> str:\n    \"\"\"\n    Clean the text email by removing any unnecessary characters and indentation.\n    This function can be extended to clean emails in other ways.\n    \"\"\"    \n    cleaned_text = text.strip()\n    cleaned_text = re.sub(r\"[<>]+\", \"\", cleaned_text, flags=re.MULTILINE)\n    return cleaned_text", "\ndef detect_language(text: str) -> str:\n    try:\n        lang = detect(text)\n        return str(lang)\n    except Exception as e:\n        print(f\"Language detection failed with error: {e}\")\n        return \"en\"  # Default to English in case of errors\n\ndef get_file_size_in_MB(file_path: str) -> float:\n    file_size_bytes = os.path.getsize(file_path)\n    file_size_MB = file_size_bytes / (1024 * 1024)    \n    return file_size_MB", "\ndef get_file_size_in_MB(file_path: str) -> float:\n    file_size_bytes = os.path.getsize(file_path)\n    file_size_MB = file_size_bytes / (1024 * 1024)    \n    return file_size_MB\n"]}
{"filename": "core/pdf_convert.py", "chunked_list": ["import logging\nimport subprocess\nimport pdfkit\n\nclass PDFConverter:\n    \"\"\"\n    Helper class for converting web pages to PDF.\n    \"\"\"\n    def __init__(self, use_pdfkit: bool = False):\n        self.use_pdfkit = use_pdfkit\n\n    def from_url(self, url: str, filename: str, title: str = \"No Title\") -> bool:\n        \"\"\"\n        Convert a webpage to PDF and save it to a file.\n\n        Args:\n            url (str): The URL of the webpage to convert.\n            filename (str): The name of the file to save the PDF to.\n\n        Returns:\n            name of file\n        \"\"\"\n        try:\n            if self.use_pdfkit:\n                pdfkit.from_url(\n                    url=url,\n                    output_path=filename,\n                    verbose=False,\n                    options={'load-error-handling': 'ignore'}\n                )\n            else:\n                cmd = [\"wkhtmltopdf\", \"--quiet\", \"--load-error-handling\", \"ignore\", '--title', title, url, filename]\n                try:\n                    subprocess.call(cmd, timeout=120)\n                except subprocess.TimeoutExpired:\n                    logging.warning(f\"Timeout converting {url} to PDF\")\n                    return False\n\n            return True\n\n        except Exception as e:\n            logging.error(f\"Error {e} converting {url} to PDF\")\n            return False", "        "]}
{"filename": "crawlers/s3_crawler.py", "chunked_list": ["import logging\nimport pathlib\nfrom slugify import slugify\nimport boto3\nimport os\nfrom typing import List, Tuple\n\nfrom core.crawler import Crawler\n\ndef list_files_in_s3_bucket(bucket_name: str, prefix: str) -> List[str]:\n    \"\"\"\n    List all files in an S3 bucket.\n\n    args:\n        bucket_name: name of the S3 bucket\n        prefix: the \"folder\" on S3 to list files from\n    \"\"\"\n    s3 = boto3.client('s3')\n    result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n    files = []\n\n    for content in result.get('Contents', []):\n        files.append(content['Key'])\n\n    while result.get('IsTruncated', False):\n        continuation_key = result.get('NextContinuationToken')\n        result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=continuation_key)\n\n        for content in result.get('Contents', []):\n            files.append(content['Key'])\n\n    return files", "\ndef list_files_in_s3_bucket(bucket_name: str, prefix: str) -> List[str]:\n    \"\"\"\n    List all files in an S3 bucket.\n\n    args:\n        bucket_name: name of the S3 bucket\n        prefix: the \"folder\" on S3 to list files from\n    \"\"\"\n    s3 = boto3.client('s3')\n    result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n    files = []\n\n    for content in result.get('Contents', []):\n        files.append(content['Key'])\n\n    while result.get('IsTruncated', False):\n        continuation_key = result.get('NextContinuationToken')\n        result = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, ContinuationToken=continuation_key)\n\n        for content in result.get('Contents', []):\n            files.append(content['Key'])\n\n    return files", "\ndef split_s3_uri(s3_uri: str) -> Tuple[str, str]:\n    \"\"\"\n    Split an S3 URI into bucket and object key.\n    \"\"\"\n    bucket_and_object = s3_uri[len(\"s3://\"):].split(\"/\", 1)\n    bucket = bucket_and_object[0]\n    object_key = bucket_and_object[1] if len(bucket_and_object) > 1 else \"\"\n    return bucket, object_key\n\nclass S3Crawler(Crawler):\n    \"\"\"\n    Crawler for S3 files.\n    \"\"\"\n    def crawl(self) -> None:\n        folder = self.cfg.s3_crawler.s3_path\n        extensions = self.cfg.s3_crawler.extensions\n\n        os.environ['AWS_ACCESS_KEY_ID'] = self.cfg.s3_crawler.aws_access_key_id\n        os.environ['AWS_SECRET_ACCESS_KEY'] = self.cfg.s3_crawler.aws_secret_access_key\n\n        bucket, key = split_s3_uri(folder)\n        s3_files = list_files_in_s3_bucket(bucket, key)\n\n        # process all files\n        s3 = boto3.client('s3')\n        for s3_file in s3_files:\n            file_extension = pathlib.Path(s3_file).suffix\n            if file_extension in extensions or \"*\" in extensions:\n                extension = s3_file.split('.')[-1]\n                local_fname = slugify(s3_file.replace(extension, ''), separator='_') + '.' + extension\n                s3.download_file(bucket, s3_file, local_fname)\n                url = f's3://{bucket}/{s3_file}'\n                metadata = {\n                    'source': 's3',\n                    'title': s3_file,\n                    'url': url\n                }\n                self.indexer.index_file(filename=local_fname, uri=url, metadata=metadata)", "\nclass S3Crawler(Crawler):\n    \"\"\"\n    Crawler for S3 files.\n    \"\"\"\n    def crawl(self) -> None:\n        folder = self.cfg.s3_crawler.s3_path\n        extensions = self.cfg.s3_crawler.extensions\n\n        os.environ['AWS_ACCESS_KEY_ID'] = self.cfg.s3_crawler.aws_access_key_id\n        os.environ['AWS_SECRET_ACCESS_KEY'] = self.cfg.s3_crawler.aws_secret_access_key\n\n        bucket, key = split_s3_uri(folder)\n        s3_files = list_files_in_s3_bucket(bucket, key)\n\n        # process all files\n        s3 = boto3.client('s3')\n        for s3_file in s3_files:\n            file_extension = pathlib.Path(s3_file).suffix\n            if file_extension in extensions or \"*\" in extensions:\n                extension = s3_file.split('.')[-1]\n                local_fname = slugify(s3_file.replace(extension, ''), separator='_') + '.' + extension\n                s3.download_file(bucket, s3_file, local_fname)\n                url = f's3://{bucket}/{s3_file}'\n                metadata = {\n                    'source': 's3',\n                    'title': s3_file,\n                    'url': url\n                }\n                self.indexer.index_file(filename=local_fname, uri=url, metadata=metadata)", ""]}
{"filename": "crawlers/notion_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nfrom omegaconf import OmegaConf\nfrom notion_client import Client\nfrom typing import Any, List, Dict\n\ndef get_text_from_block(block: Any) -> str:\n    \"\"\"\n    Recursively extract all text from a block.\n    \"\"\"\n    if block[\"type\"] == \"paragraph\":\n        text = \" \".join([text[\"plain_text\"] for text in block[\"paragraph\"][\"rich_text\"]])\n    else:\n        text = \"\"\n    if \"children\" in block:\n        for child_block in block[\"children\"]:\n            text += \"\\n\" + get_text_from_block(child_block)\n    return text", "\n\ndef list_all_pages(notion: Any) -> List[Dict[str, Any]]:\n    \"\"\"\n    List all pages in a Notion workspace.\n    \"\"\"\n    pages = []\n    has_more = True\n    start_cursor = None\n    while has_more:\n        list_pages_response = notion.search(filter={\"property\": \"object\", \"value\": \"page\"}, start_cursor=start_cursor)\n        for page in list_pages_response[\"results\"]:\n            pages.append(page)\n        has_more = list_pages_response[\"has_more\"]\n        start_cursor = list_pages_response[\"next_cursor\"]\n    \n    return pages", "\n\nclass NotionCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.notion_api_key = self.cfg.notion_crawler.notion_api_key\n\n    def crawl(self) -> None:\n        notion = Client(auth=self.notion_api_key)\n\n        pages = list_all_pages(notion)\n\n        logging.info(f\"Found {len(pages)} pages in Notion.\")\n        for page in pages:\n            page_id = page[\"id\"]\n            title_obj = page.get('properties', {}).get('title', {}).get('title', [])\n            if len(title_obj)>0:\n                title = title_obj[0][\"plain_text\"]\n            else:\n                title = None\n\n            # Extract all text blocks from the page\n            try:\n                blocks = notion.blocks.children.list(page_id).get(\"results\")        # type: ignore\n            except Exception as e:\n                logging.error(f\"Failed to get blocks for page {page['url']}: {e}\")\n                continue\n            segments = []\n            metadatas = []\n            for block in blocks:\n                text = get_text_from_block(block)\n                if len(text)>2:\n                    segments.append(text)\n                    metadatas.append({'block_id': block['id'], 'block_type': block['type']})\n            doc_id = page['url']\n            if len(segments)>0:\n                logging.info(f\"Indexing {len(segments)} segments in page {doc_id}\")\n                succeeded = self.indexer.index_segments(doc_id, segments, metadatas, doc_metadata={'source': 'notion', 'title': title, 'url': page['url']})\n                if succeeded:\n                    logging.info(f\"Indexed notion page {doc_id}\")\n                else:\n                    logging.info(f\"Indexing failed for notion page {doc_id}\")\n            else:\n                logging.info(f\"No text found in notion page {doc_id}\")", "            "]}
{"filename": "crawlers/mediawiki_crawler.py", "chunked_list": ["import logging\nimport json\nimport urllib.parse\nimport time\nfrom datetime import datetime, timedelta  \nfrom mwviews.api import PageviewsClient\n\nfrom core.crawler import Crawler\nfrom core.utils import create_session_with_retries\n\nclass MediawikiCrawler(Crawler):\n\n    def crawl(self) -> None:\n        api_url = self.cfg.mediawiki_crawler.api_url\n        project = self.cfg.mediawiki_crawler.project\n        n_pages = self.cfg.mediawiki_crawler.n_pages\n        session = create_session_with_retries()\n        if n_pages > 1000:\n            n_pages = 1000\n            logging.info(f\"n_pages is too large, setting to 1000\")\n        metrics_date = datetime.now() - timedelta(days=7)\n\n        # Get most viewed pages\n        p = PageviewsClient(user_agent=\"crawler@example.com\")\n        year, month, day = metrics_date.strftime(\"%Y-%m-%d\").split(\"-\")\n        titles = [v['article'] for v in p.top_articles(project, limit=n_pages, year=year, month=month, day=day)]\n        logging.info(f\"indexing {len(titles)} top titles from {project}\")\n\n        # Process the pages in batches\n        for title in titles:\n            time.sleep(1)\n            params = {'action': 'query', 'prop': 'info|revisions', 'titles': title, 'inprop': 'url', 'rvprop': 'timestamp', 'format': 'json'}\n            response = session.get(api_url, params=params).json()\n            page_id = list(response['query']['pages'].keys())[0]\n            if int(page_id) <= 0:\n                continue\n            page_url = response['query']['pages'][page_id]['fullurl']\n            last_revision = response['query']['pages'][page_id]['revisions'][0]\n            last_editor = last_revision.get('user', 'unknown')\n            last_edited_at = last_revision['timestamp']\n\n            params = {'action': 'query', 'prop': 'extracts', 'titles': title, 'format': 'json', 'explaintext': 1}\n            response = session.get(api_url, params=params).json()\n            page_id = list(response[\"query\"][\"pages\"].keys())[0]\n            page_content = response[\"query\"][\"pages\"][page_id][\"extract\"]\n\n            if page_content is None or len(page_content) < 3:\n                continue                    # skip pages without content\n\n            logging.info(f\"Indexing page with {title}: url={page_url}\")\n\n            # Index the page into Vectara\n            document = {\n                \"documentId\": title,\n                \"title\": title,\n                \"description\": \"\",\n                \"metadataJson\": json.dumps({\n                    \"url\": f\"https://en.wikipedia.org/wiki/{urllib.parse.quote(title)}\",\n                    \"last_edit_time\": last_edited_at,\n                    \"last_edit_user\": last_editor,\n                    \"source\": \"mediawiki\",\n                }),\n                \"section\": [\n                    {\n                        \"text\": page_content,\n                    }\n                ]\n            }\n            succeeded = self.indexer.index_document(document)\n            if not succeeded:\n                logging.info(f\"Failed to index page {page_id}: url={page_url}, title={title}\")", "from core.utils import create_session_with_retries\n\nclass MediawikiCrawler(Crawler):\n\n    def crawl(self) -> None:\n        api_url = self.cfg.mediawiki_crawler.api_url\n        project = self.cfg.mediawiki_crawler.project\n        n_pages = self.cfg.mediawiki_crawler.n_pages\n        session = create_session_with_retries()\n        if n_pages > 1000:\n            n_pages = 1000\n            logging.info(f\"n_pages is too large, setting to 1000\")\n        metrics_date = datetime.now() - timedelta(days=7)\n\n        # Get most viewed pages\n        p = PageviewsClient(user_agent=\"crawler@example.com\")\n        year, month, day = metrics_date.strftime(\"%Y-%m-%d\").split(\"-\")\n        titles = [v['article'] for v in p.top_articles(project, limit=n_pages, year=year, month=month, day=day)]\n        logging.info(f\"indexing {len(titles)} top titles from {project}\")\n\n        # Process the pages in batches\n        for title in titles:\n            time.sleep(1)\n            params = {'action': 'query', 'prop': 'info|revisions', 'titles': title, 'inprop': 'url', 'rvprop': 'timestamp', 'format': 'json'}\n            response = session.get(api_url, params=params).json()\n            page_id = list(response['query']['pages'].keys())[0]\n            if int(page_id) <= 0:\n                continue\n            page_url = response['query']['pages'][page_id]['fullurl']\n            last_revision = response['query']['pages'][page_id]['revisions'][0]\n            last_editor = last_revision.get('user', 'unknown')\n            last_edited_at = last_revision['timestamp']\n\n            params = {'action': 'query', 'prop': 'extracts', 'titles': title, 'format': 'json', 'explaintext': 1}\n            response = session.get(api_url, params=params).json()\n            page_id = list(response[\"query\"][\"pages\"].keys())[0]\n            page_content = response[\"query\"][\"pages\"][page_id][\"extract\"]\n\n            if page_content is None or len(page_content) < 3:\n                continue                    # skip pages without content\n\n            logging.info(f\"Indexing page with {title}: url={page_url}\")\n\n            # Index the page into Vectara\n            document = {\n                \"documentId\": title,\n                \"title\": title,\n                \"description\": \"\",\n                \"metadataJson\": json.dumps({\n                    \"url\": f\"https://en.wikipedia.org/wiki/{urllib.parse.quote(title)}\",\n                    \"last_edit_time\": last_edited_at,\n                    \"last_edit_user\": last_editor,\n                    \"source\": \"mediawiki\",\n                }),\n                \"section\": [\n                    {\n                        \"text\": page_content,\n                    }\n                ]\n            }\n            succeeded = self.indexer.index_document(document)\n            if not succeeded:\n                logging.info(f\"Failed to index page {page_id}: url={page_url}, title={title}\")", ""]}
{"filename": "crawlers/jira_crawler.py", "chunked_list": ["import logging\nimport requests\nimport json\nfrom core.crawler import Crawler\nfrom core.utils import create_session_with_retries\n\nclass JiraCrawler(Crawler):\n\n    def crawl(self) -> None:\n        self.jira_headers = { \"Accept\": \"application/json\" }\n        self.jira_auth = (self.cfg.jira_crawler.jira_username, self.cfg.jira_crawler.jira_password)\n        session = create_session_with_retries()\n\n        issue_count = 0\n        startAt = 0\n        res_cnt = 100\n        while True:\n            jira_query_url = f\"{self.cfg.jira_crawler.jira_base_url}/rest/api/3/search?jql={self.cfg.jira_crawler.jira_jql}&fields=*all&maxResults={res_cnt}&startAt={startAt}\"\n\n            jira_response = session.get(jira_query_url, headers=self.jira_headers, auth=self.jira_auth)\n            jira_response.raise_for_status()\n            jira_data = jira_response.json()\n\n            actual_cnt = len(jira_data[\"issues\"])\n            if actual_cnt > 0:\n                for issue in jira_data[\"issues\"]:\n                    # Collect as much metadata as possible\n                    metadata = {}\n                    metadata[\"project\"] = issue[\"fields\"][\"project\"][\"name\"]\n                    metadata[\"issueType\"] = issue[\"fields\"][\"issuetype\"][\"name\"]\n                    metadata[\"status\"] = issue[\"fields\"][\"status\"][\"name\"]\n                    metadata[\"priority\"] = issue[\"fields\"][\"priority\"][\"name\"]\n                    metadata[\"reporter\"] = issue[\"fields\"][\"reporter\"][\"displayName\"]\n                    metadata[\"assignee\"] = issue[\"fields\"][\"assignee\"][\"displayName\"] if issue[\"fields\"][\"assignee\"] else None\n                    metadata[\"created\"] = issue[\"fields\"][\"created\"]\n                    metadata[\"updated\"] = issue[\"fields\"][\"updated\"]\n                    metadata[\"resolved\"] = issue[\"fields\"][\"resolutiondate\"] if \"resolutiondate\" in issue[\"fields\"] else None\n                    metadata[\"labels\"] = issue[\"fields\"][\"labels\"]\n                    metadata[\"source\"] = \"jira\"\n                    metadata[\"url\"] = f\"{self.cfg.jira_crawler.jira_base_url}/browse/{issue['key']}\"\n\n                    # Create a Vectara document with the metadata and the issue fields\n                    title = issue[\"fields\"][\"summary\"]\n                    document = {\n                        \"documentId\": issue[\"key\"],\n                        \"title\": title,\n                        \"metadataJson\": json.dumps(metadata),\n                        \"section\": []\n                    }\n                    comments_data = issue[\"fields\"][\"comment\"][\"comments\"]\n                    comments = []\n                    for comment in comments_data:\n                        author = comment[\"author\"][\"displayName\"]\n                        try:\n                            comment_body = comment[\"body\"][\"content\"][0][\"content\"][0][\"text\"]\n                            comments.append(f'{author}: {comment_body}')\n                        except Exception as e:\n                            continue\n\n                    try:\n                        description = issue[\"fields\"][\"description\"][\"content\"][0][\"content\"][0][\"text\"]\n                    except Exception as e:\n                        description = str(issue['key'])\n\n                    document[\"section\"] = [\n                        {\n                            \"title\": \"Comments\",\n                            \"text\": \"\\n\\n\".join(comments)\n                        },\n                        {\n                            \"title\": \"Description\",\n                            \"text\": description\n                        },\n                        {\n                            \"title\": \"Status\",\n                            \"text\": f'Issue {title} is {issue[\"fields\"][\"status\"][\"name\"]}'\n                        }\n                    ]\n\n                    succeeded = self.indexer.index_document(document)\n                    if succeeded:\n                        logging.info(f\"Indexed issue {document['documentId']}\")\n                        issue_count += 1\n                    else:\n                        logging.info(f\"Error indexing issue {document['documentId']}\")\n                startAt = startAt + actual_cnt\n            else:\n                break\n\n        logging.info(f\"Finished indexing all issues (total={issue_count})\")", ""]}
{"filename": "crawlers/database_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nimport sqlalchemy\nimport pandas as pd\nimport unicodedata\n\nclass DatabaseCrawler(Crawler):\n\n    def crawl(self) -> None: \n        db_url = self.cfg.database_crawler.db_url\n        db_table = self.cfg.database_crawler.db_table\n        text_columns = list(self.cfg.database_crawler.text_columns)\n        metadata_columns = list(self.cfg.database_crawler.metadata_columns)\n        select_condition = self.cfg.database_crawler.get(\"select_condition\", None)\n        doc_id_columns = list(self.cfg.database_crawler.get(\"doc_id_columns\", None))\n\n        all_columns = text_columns + metadata_columns\n        if select_condition:\n            query = f'SELECT {\",\".join(all_columns)} FROM {db_table} WHERE {select_condition}'\n        else:\n            query = f'SELECT {\",\".join(all_columns)} FROM {db_table}'\n\n        conn = sqlalchemy.create_engine(db_url).connect()\n        df = pd.read_sql_query(sqlalchemy.text(query), conn)\n        \n        logging.info(f\"indexing {len(df)} rows from the database using query: '{query}'\")\n\n        def index_df(doc_id: str, title: str, df: pd.DataFrame) -> None:\n            parts = []\n            metadatas = []\n            for _, row in df.iterrows():\n                text = ' - '.join(str(x) for x in row[text_columns].tolist() if x)\n                parts.append(unicodedata.normalize('NFD', text))\n                metadatas.append({column: row[column] for column in metadata_columns})\n            logging.info(f\"Indexing df for '{doc_id}' with ({len(df)}) rows\")\n            self.indexer.index_segments(doc_id, parts, metadatas, title=title, doc_metadata = {'source': 'database'})\n\n        if doc_id_columns:\n            grouped = df.groupby(doc_id_columns)\n            for name, group in grouped:\n                gr_str = name if type(name)==str else ' - '.join(str(x) for x in name)\n                index_df(doc_id=gr_str, title=gr_str, df=group)\n        else:\n            rows_per_chunk = self.cfg.database_crawler.get(\"rows_per_chunk\", 500)\n            for inx in range(0, df.shape[0], rows_per_chunk):\n                sub_df = df[inx: inx+rows_per_chunk]\n                name = f'rows {inx}-{inx+rows_per_chunk-1}'\n                index_df(doc_id=name, title=name, df=sub_df)", "        "]}
{"filename": "crawlers/folder_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nimport os\nimport pathlib\nimport time\n\nclass FolderCrawler(Crawler):\n\n    def crawl(self) -> None:\n        folder = \"/home/vectara/data\"\n        extensions = self.cfg.folder_crawler.extensions\n\n        # Walk the directory and upload files with the specified extension to Vectara\n        logging.info(f\"indexing files in {self.cfg.folder_crawler.path} with extensions {extensions}\")\n        source = self.cfg.folder_crawler.source\n        for root, _, files in os.walk(folder):\n            for file in files:\n                file_extension = pathlib.Path(file).suffix\n                if file_extension in extensions or \"*\" in extensions:\n                    file_path = os.path.join(root, file)\n                    file_name = os.path.relpath(file_path, folder)\n                    file_metadata = {\n                        'created_at': time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(os.path.getctime(file_path))),\n                        'modified_at': time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(os.path.getmtime(file_path))),\n                        'file_size': os.path.getsize(file_path),\n                        'source': source,\n                        'title': file_name,\n#                        'url': file_name\n                    }\n                    self.indexer.index_file(filename=file_path, uri=file_name, metadata=file_metadata)", ""]}
{"filename": "crawlers/__init__.py", "chunked_list": ["\n"]}
{"filename": "crawlers/docs_crawler.py", "chunked_list": ["from core.crawler import Crawler\nfrom bs4 import BeautifulSoup\nimport logging\nfrom urllib.parse import urljoin, urlparse\nimport re\nfrom collections import deque\nfrom ratelimiter import RateLimiter\nfrom core.utils import create_session_with_retries, binary_extensions\nfrom typing import Tuple, Set\n\nclass DocsCrawler(Crawler):\n\n    def concat_url_and_href(self, url: str, href: str) -> str:\n        if href.startswith('http'):\n            return href\n        else:\n            if 'index.html?' in href:\n                href = href.replace('index.html?', '/')     # special fix for Spark docs\n            joined = urljoin(url, href)\n            return joined\n\n    def get_url_content(self, url: str) -> Tuple[str, BeautifulSoup]:\n        headers = {\n            'User-Agent': 'Mozilla/5.0',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n        }\n        with self.rate_limiter:\n            response = self.session.get(url, headers=headers)\n        if response.status_code != 200:\n            logging.info(f\"Failed to crawl {url}, response code is {response.status_code}\")\n            return None, None\n\n        # check for refresh redirect        \n        soup = BeautifulSoup(response.content, 'html.parser')\n        meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})\n        if meta_refresh:\n            href = meta_refresh['content'].split('url=')[-1]            # type: ignore\n            url = self.concat_url_and_href(url, href)\n            response = self.session.get(url, headers=headers)\n            if response.status_code != 200:\n                logging.info(f\"Failed to crawl redirect {url}, response code is {response.status_code}\")\n                return None, None\n\n        page_content = BeautifulSoup(response.content, 'lxml')\n        return url, page_content\n\n    def collect_urls(self, base_url: str) -> None:\n        new_urls = deque([base_url])\n\n        # Crawl each URL in the queue\n        while len(new_urls):\n            n_urls = len(self.crawled_urls)\n            if n_urls>0 and n_urls%100==0:\n                logging.info(f\"Currently have {n_urls} crawled urls identified\")\n            \n            # pop the left-most URL from new_urls\n            url = new_urls.popleft()\n\n            try:\n                url, page_content = self.get_url_content(url)\n                self.crawled_urls.add(url)\n\n                # Find all the new URLs in the page's content and add them into the queue\n                if page_content:\n                    for link in page_content.find_all('a'):\n                        href = link.get('href')\n                        if href is None:\n                            continue\n                        abs_url = self.concat_url_and_href(url, href)\n                        if ((any([r.match(abs_url) for r in self.pos_regex])) and                           # match any of the positive regexes\n                            (not any([r.match(abs_url) for r in self.neg_regex])) and                       # don't match any of the negative regexes\n                            (abs_url.startswith(\"http\")) and                                                # starts with http/https\n                            (abs_url not in self.ignored_urls) and                                          # not previously ignored    \n                            (len(urlparse(abs_url).fragment)==0) and                                        # does not have fragment\n                            (any([abs_url.endswith(ext) for ext in self.extensions_to_ignore])==False)):    # not any of the specified extensions to ignore\n                                # add URL if needed\n                                if abs_url not in self.crawled_urls and abs_url not in new_urls:\n                                    new_urls.append(abs_url)\n                        else:\n                            self.ignored_urls.add(abs_url)\n\n            except Exception as e:\n                import traceback\n                logging.info(f\"Error crawling {url}: {e}, traceback={traceback.format_exc()}\")\n                continue\n\n    def crawl(self) -> None:\n        self.crawled_urls: Set[str] = set()\n        self.ignored_urls: Set[str] = set()\n        self.extensions_to_ignore = list(set(self.cfg.docs_crawler.extensions_to_ignore + binary_extensions))\n        self.pos_regex = [re.compile(r) for r in self.cfg.docs_crawler.pos_regex] if self.cfg.docs_crawler.pos_regex else []\n        self.neg_regex = [re.compile(r) for r in self.cfg.docs_crawler.neg_regex] if self.cfg.docs_crawler.neg_regex else []\n\n        self.session = create_session_with_retries()\n        self.rate_limiter = RateLimiter(max_calls=2, period=1)\n\n        for base_url in self.cfg.docs_crawler.base_urls:\n            self.collect_urls(base_url)\n\n        logging.info(f\"Found {len(self.crawled_urls)} urls in {self.cfg.docs_crawler.base_urls}\")\n        source = self.cfg.docs_crawler.docs_system\n        for url in self.crawled_urls:\n            self.indexer.index_url(url, metadata={'url': url, 'source': source})\n            logging.info(f\"{source.capitalize()} Crawler: finished indexing {url}\")", "from typing import Tuple, Set\n\nclass DocsCrawler(Crawler):\n\n    def concat_url_and_href(self, url: str, href: str) -> str:\n        if href.startswith('http'):\n            return href\n        else:\n            if 'index.html?' in href:\n                href = href.replace('index.html?', '/')     # special fix for Spark docs\n            joined = urljoin(url, href)\n            return joined\n\n    def get_url_content(self, url: str) -> Tuple[str, BeautifulSoup]:\n        headers = {\n            'User-Agent': 'Mozilla/5.0',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n        }\n        with self.rate_limiter:\n            response = self.session.get(url, headers=headers)\n        if response.status_code != 200:\n            logging.info(f\"Failed to crawl {url}, response code is {response.status_code}\")\n            return None, None\n\n        # check for refresh redirect        \n        soup = BeautifulSoup(response.content, 'html.parser')\n        meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})\n        if meta_refresh:\n            href = meta_refresh['content'].split('url=')[-1]            # type: ignore\n            url = self.concat_url_and_href(url, href)\n            response = self.session.get(url, headers=headers)\n            if response.status_code != 200:\n                logging.info(f\"Failed to crawl redirect {url}, response code is {response.status_code}\")\n                return None, None\n\n        page_content = BeautifulSoup(response.content, 'lxml')\n        return url, page_content\n\n    def collect_urls(self, base_url: str) -> None:\n        new_urls = deque([base_url])\n\n        # Crawl each URL in the queue\n        while len(new_urls):\n            n_urls = len(self.crawled_urls)\n            if n_urls>0 and n_urls%100==0:\n                logging.info(f\"Currently have {n_urls} crawled urls identified\")\n            \n            # pop the left-most URL from new_urls\n            url = new_urls.popleft()\n\n            try:\n                url, page_content = self.get_url_content(url)\n                self.crawled_urls.add(url)\n\n                # Find all the new URLs in the page's content and add them into the queue\n                if page_content:\n                    for link in page_content.find_all('a'):\n                        href = link.get('href')\n                        if href is None:\n                            continue\n                        abs_url = self.concat_url_and_href(url, href)\n                        if ((any([r.match(abs_url) for r in self.pos_regex])) and                           # match any of the positive regexes\n                            (not any([r.match(abs_url) for r in self.neg_regex])) and                       # don't match any of the negative regexes\n                            (abs_url.startswith(\"http\")) and                                                # starts with http/https\n                            (abs_url not in self.ignored_urls) and                                          # not previously ignored    \n                            (len(urlparse(abs_url).fragment)==0) and                                        # does not have fragment\n                            (any([abs_url.endswith(ext) for ext in self.extensions_to_ignore])==False)):    # not any of the specified extensions to ignore\n                                # add URL if needed\n                                if abs_url not in self.crawled_urls and abs_url not in new_urls:\n                                    new_urls.append(abs_url)\n                        else:\n                            self.ignored_urls.add(abs_url)\n\n            except Exception as e:\n                import traceback\n                logging.info(f\"Error crawling {url}: {e}, traceback={traceback.format_exc()}\")\n                continue\n\n    def crawl(self) -> None:\n        self.crawled_urls: Set[str] = set()\n        self.ignored_urls: Set[str] = set()\n        self.extensions_to_ignore = list(set(self.cfg.docs_crawler.extensions_to_ignore + binary_extensions))\n        self.pos_regex = [re.compile(r) for r in self.cfg.docs_crawler.pos_regex] if self.cfg.docs_crawler.pos_regex else []\n        self.neg_regex = [re.compile(r) for r in self.cfg.docs_crawler.neg_regex] if self.cfg.docs_crawler.neg_regex else []\n\n        self.session = create_session_with_retries()\n        self.rate_limiter = RateLimiter(max_calls=2, period=1)\n\n        for base_url in self.cfg.docs_crawler.base_urls:\n            self.collect_urls(base_url)\n\n        logging.info(f\"Found {len(self.crawled_urls)} urls in {self.cfg.docs_crawler.base_urls}\")\n        source = self.cfg.docs_crawler.docs_system\n        for url in self.crawled_urls:\n            self.indexer.index_url(url, metadata={'url': url, 'source': source})\n            logging.info(f\"{source.capitalize()} Crawler: finished indexing {url}\")", ""]}
{"filename": "crawlers/hackernews_crawler.py", "chunked_list": ["\nimport requests\nimport logging\nfrom core.crawler import Crawler\nimport os\nfrom slugify import slugify\nfrom core.utils import html_to_text, create_session_with_retries\nfrom typing import List\n\ndef get_comments(kids: List[str], entrypoint: str) -> List[str]:\n    comments = []\n    for kid in kids:\n        try:\n            response = requests.get(entrypoint + 'item/{}.json'.format(kid))\n            comment = response.json()\n        except Exception as e:\n            logging.info(f\"Error retrieving comment {kid}, e={e}\")\n            comment = None\n        if comment is not None and comment.get('type', '') == 'comment':\n            comments.append(html_to_text(comment.get('text', '')))\n            sub_kids = comment.get('kids', [])\n            if len(sub_kids)>0:\n                comments += get_comments(sub_kids, entrypoint)\n    return comments", "\ndef get_comments(kids: List[str], entrypoint: str) -> List[str]:\n    comments = []\n    for kid in kids:\n        try:\n            response = requests.get(entrypoint + 'item/{}.json'.format(kid))\n            comment = response.json()\n        except Exception as e:\n            logging.info(f\"Error retrieving comment {kid}, e={e}\")\n            comment = None\n        if comment is not None and comment.get('type', '') == 'comment':\n            comments.append(html_to_text(comment.get('text', '')))\n            sub_kids = comment.get('kids', [])\n            if len(sub_kids)>0:\n                comments += get_comments(sub_kids, entrypoint)\n    return comments", "\nclass HackernewsCrawler(Crawler):\n\n    def crawl(self) -> None:\n        N_ARTICLES = self.cfg.hackernews_crawler.max_articles\n\n        # URL for the Hacker News API\n        entrypoint = 'https://hacker-news.firebaseio.com/v0/'\n\n        # Retrieve the IDs of the top N_ARTICLES stories\n        session = create_session_with_retries()\n        resp1= session.get(entrypoint + 'topstories.json')\n        resp2 = session.get(entrypoint + 'newstories.json')\n        resp3 = session.get(entrypoint + 'beststories.json')\n\n        top_ids = list(set(list(resp1.json()) + list(resp2.json()) + list(resp3.json())))[:N_ARTICLES]\n        num_ids = len(top_ids)\n        logging.info(f\"Crawling {num_ids} stories\")\n\n        # Retrieve the details of each story\n        for n_id, id in enumerate(top_ids):\n            if n_id % 20 == 0:\n                logging.info(f\"Crawled {n_id} stories so far\")\n            try:\n                response = session.get(entrypoint + 'item/{}.json'.format(id))\n                story = response.json()\n                url = story.get('url', None)\n                if url is None:\n                    continue\n                title = html_to_text(story.get('title', ''))\n                text = story.get('text', None)\n                if text:\n                    fname = slugify(url) + \".html\"\n                    with open(fname, 'w') as f:\n                        f.write(text)\n                    self.indexer.index_file(fname, uri=url, metadata={'title': title})\n                    os.remove(fname)\n                else:\n                    metadata = {'source': 'hackernews', 'title': title}\n                    self.indexer.index_url(url, metadata=metadata)\n            except Exception as e:\n                import traceback\n                logging.error(f\"Error crawling story {url}, error={e}, traceback={traceback.format_exc()}\")", ""]}
{"filename": "crawlers/arxiv_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nimport arxiv\nfrom core.utils import create_session_with_retries\n\ndef validate_category(category: str) -> bool:\n    valid_categories = [\n        \"cs\", \"econ\", \"q-fin\",\"stat\",\n        \"math\", \"math-ph\", \"q-bio\", \"stat-mech\",\n        \"physics\", \"astro-ph\", \"cond-mat\", \"gr-qc\", \"hep-ex\", \"hep-lat\", \"hep-ph\", \n        \"hep-th\", \"nucl-ex\", \"nucl-th\", \"physics-ao-ph\", \"physics-ao-pl\", \"physics-ao-po\",\n        \"physics-ao-ps\", \"physics-app-ph\",\n        \"quant-ph\"\n    ]\n    return category in valid_categories", "\nclass ArxivCrawler(Crawler):\n\n    def get_citations(self, arxiv_id: str) -> int:\n        \"\"\"\n        Retrieves the number of citations for a given paper from Semantic Scholar API based on its arXiv ID.\n\n        Parameters:\n        arxiv_id (str): The arXiv ID of the paper.\n\n        Returns:\n        int: Number of citations if the paper exists and the request was successful, otherwise -1.\n        If an exception occurs during the request, it also returns -1 and logs the exception.\n        \"\"\"\n        base_url = \"https://api.semanticscholar.org/v1/paper/arXiv:\"\n        arxiv_id = arxiv_id.split('v')[0]   # remove any 'v1' or 'v2', etc from the ending, if it exists\n\n        try:\n            response = self.session.get(base_url + arxiv_id)\n            if response.status_code != 200:\n                return -1\n            paper_info = response.json()\n        \n            paper_id = paper_info.get('paperId')\n            base_url = \"https://api.semanticscholar.org/v1/paper/\"\n            response = self.session.get(base_url + paper_id)\n\n            if response.status_code == 200:\n                paper_info = response.json()\n                n_citations = len(paper_info.get(\"citations\"))\n                return n_citations\n            else:\n                return -1\n\n        except Exception as e:\n            logging.info(f\"Error parsing response from arxiv API: {e}, response={response.text}\")\n            return -1\n\n\n    def crawl(self) -> None:\n        n_papers = self.cfg.arxiv_crawler.n_papers\n        query_terms = self.cfg.arxiv_crawler.query_terms\n        year = self.cfg.arxiv_crawler.start_year\n        category = self.cfg.arxiv_crawler.arxiv_category\n        if not validate_category(category):\n            logging.info(f\"Invalid arxiv category: {category}, please check the config file\")\n            exit(1)\n\n        # setup requests session and mount adapter to retry requests\n        self.session = create_session_with_retries()\n\n        # define query for arxiv: search for the query in the \"computer science\" (cs.*) category\n        query = f\"cat:{category}.* AND \" + ' AND '.join([f'all:{q}' for q in query_terms])\n        if self.cfg.arxiv_crawler.sort_by == 'citations':\n            # for sort by n_citations We pull 100x papers so that we can get citations and enough highly cited paper.\n            search = arxiv.Search(\n                query = query,\n                max_results = n_papers*100,\n                sort_by = arxiv.SortCriterion.Relevance,\n                sort_order = arxiv.SortOrder.Descending\n            )\n        else:\n            search = arxiv.Search(\n                query = query,\n                max_results = n_papers,\n                sort_by = arxiv.SortCriterion.submittedDate,\n                sort_order = arxiv.SortOrder.Descending\n            )\n\n        # filter by publication year\n        papers = []\n        try:\n            for result in search.results():\n                date = result.published.date()\n                if date.year < year:\n                    continue\n                id = result.entry_id.split('/')[-1]\n                papers.append({\n                    'id': result.entry_id,\n                    'citation_count': self.get_citations(id),\n                    'url': result.pdf_url,\n                    'title': result.title,\n                    'authors': result.authors,\n                    'abstract': result.summary,\n                    'published': str(date)\n                })\n        except Exception as e:\n            logging.info(f\"Exception {e}, we have {len(papers)} papers already, so will continue with indexing\")\n\n        if len(papers) == 0:\n            logging.info(f\"Found 0 papers for query: {query}, ignore crawl\")\n            return\n\n        # sort by citation count and get top n papers\n        if self.cfg.arxiv_crawler.sort_by == 'citations':\n            sorted_papers = sorted(papers, key=lambda x: x['citation_count'], reverse=True) \n            top_n = sorted_papers[:n_papers]\n        else:\n            top_n = papers\n\n        # Index top papers selected in Vectara\n        for paper in top_n:\n            url = paper['url'] + \".pdf\"\n            metadata = {'source': 'arxiv', 'title': paper['title'], 'abstract': paper['abstract'], 'url': paper['url'],\n                        'published': str(paper['published']), 'citations': str(paper['citation_count'])}\n            self.indexer.index_url(url, metadata=metadata)", ""]}
{"filename": "crawlers/website_crawler.py", "chunked_list": ["import logging\nimport os\nfrom usp.tree import sitemap_tree_for_homepage\nfrom ratelimiter import RateLimiter\nfrom core.crawler import Crawler, recursive_crawl\nfrom core.utils import clean_urls, archive_extensions, img_extensions\nimport re\nfrom typing import List, Set\n# disable USP annoying logging\nlogging.getLogger(\"usp.fetch_parse\").setLevel(logging.ERROR)", "# disable USP annoying logging\nlogging.getLogger(\"usp.fetch_parse\").setLevel(logging.ERROR)\nlogging.getLogger(\"usp.helpers\").setLevel(logging.ERROR)\n\n\n\nclass WebsiteCrawler(Crawler):\n    def crawl(self) -> None:\n        base_urls = self.cfg.website_crawler.urls\n        crawled_urls = set()\n\n        if \"url_regex\" in self.cfg.website_crawler:\n            url_regex = [re.compile(r) for r in self.cfg.website_crawler.url_regex]\n            logging.info(\n                f\"Filtering URLs by these regular expressions: {self.cfg.website_crawler.url_regex}\"\n            )\n        else:\n            url_regex = []\n\n        for homepage in base_urls:\n            if self.cfg.website_crawler.pages_source == \"sitemap\":\n                tree = sitemap_tree_for_homepage(homepage)\n                urls = [page.url for page in tree.all_pages()]\n            elif self.cfg.website_crawler.pages_source == \"crawl\":\n                urls_set = recursive_crawl(homepage, self.cfg.website_crawler.max_depth, url_regex=url_regex)\n                urls = clean_urls(urls_set)\n            else:\n                logging.info(f\"Unknown pages_source: {self.cfg.website_crawler.pages_source}\")\n                return\n\n            # remove URLS that are out of our regex regime or are archives or images\n            if url_regex:\n                urls = [u for u in urls if any([r.match(u) for r in url_regex])]\n            urls = [u for u in urls if not any([u.endswith(ext) for ext in archive_extensions + img_extensions])]\n            urls = list(set(urls))\n\n            logging.info(f\"Finished crawling using {homepage}, found {len(urls)} URLs to index\")\n            file_types = list(set([u[-10:].split('.')[-1] for u in urls if '.' in u[-10:]]))\n            logging.info(f\"File types = {file_types}\")\n\n            delay = max(self.cfg.website_crawler.get(\"delay\", 0.1), 0.1)    # seconds between requests\n            rate_limiter = RateLimiter(\n                max_calls=1, period=delay                                   # at most 1 call every `delay` seconds\n            )\n\n            extraction = self.cfg.website_crawler.extraction\n            for inx, url in enumerate(urls):\n                if url in crawled_urls:\n                    logging.info(\n                        f\"Skipping {url} since it was already crawled in this round\"\n                    )\n                    continue\n\n                if inx % 100 == 0:\n                    logging.info(f\"Crawling URL number {inx} out of {len(urls)}\")\n\n                metadata = {\"source\": \"website\", \"url\": url}\n                if extraction == \"pdf\":\n                    try:\n                        with rate_limiter:\n                            filename = self.url_to_file(url, title=\"\")\n                    except Exception as e:\n                        logging.error(f\"Error while processing {url}: {e}\")\n                        continue\n                    try:\n                        succeeded = self.indexer.index_file(filename, uri=url, metadata=metadata)\n                        if not succeeded:\n                            logging.info(f\"Indexing failed for {url}\")\n                        else:\n                            if os.path.exists(filename):\n                                os.remove(filename)\n                            crawled_urls.add(url)\n                            logging.info(f\"Indexing {url} was successfully\")\n                    except Exception as e:\n                        import traceback\n                        logging.error(\n                            f\"Error while indexing {url}: {e}, traceback={traceback.format_exc()}\"\n                        )\n                else:  # use index_url which uses PlayWright\n                    logging.info(f\"Crawling and indexing {url}\")\n                    try:\n                        with rate_limiter:\n                            succeeded = self.indexer.index_url(url, metadata=metadata)\n                        if not succeeded:\n                            logging.info(f\"Indexing failed for {url}\")\n                        else:\n                            crawled_urls.add(url)\n                            logging.info(f\"Indexing {url} was successfully\")\n                    except Exception as e:\n                        import traceback\n                        logging.error(\n                            f\"Error while indexing {url}: {e}, traceback={traceback.format_exc()}\"\n                        )", ""]}
{"filename": "crawlers/github_crawler.py", "chunked_list": ["import json\nfrom core.crawler import Crawler\nfrom omegaconf import OmegaConf\nimport requests\nfrom attrdict import AttrDict\nimport logging\nimport base64\n\nfrom ratelimiter import RateLimiter\nfrom core.utils import create_session_with_retries", "from ratelimiter import RateLimiter\nfrom core.utils import create_session_with_retries\n\nfrom typing import List, Any\n\nclass Github(object):\n    def __init__(self, repo: str, owner: str, token: str) -> None:\n        self.repo = repo\n        self.owner = owner\n        self.token = token\n        self.session = create_session_with_retries()\n\n\n    def get_issues(self, state: str) -> List[Any]:\n        # state can be \"open\", \"closed\", or \"all\"\n        api_url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/issues?state={state}\"\n        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Accept\": \"application/vnd.github+json\"}\n        response = self.session.get(api_url, headers=headers)\n        if response.status_code == 200:\n            return list(response.json())\n        else:\n            logging.info(f\"Error retrieving issues: {response.status_code}, {response.text}\")\n            return []\n\n    def get_comments(self, issue_number: str) -> List[Any]:\n        api_url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/issues/{issue_number}/comments\"\n        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Accept\": \"application/vnd.github+json\"}\n        response = self.session.get(api_url, headers=headers)\n        if response.status_code == 200:\n            return list(response.json())\n        else:\n            logging.info(f\"Error retrieving comments: {response.status_code}, {response.text}\")\n            return []", "\n\nclass GithubCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.github_token = self.cfg.github_crawler.get(\"github_token\", None)\n        self.owner = self.cfg.github_crawler.owner\n        self.repos = self.cfg.github_crawler.repos\n        self.crawl_code = self.cfg.github_crawler.crawl_code\n        self.rate_limiter = RateLimiter(max_calls=1, period=1)\n        self.session = create_session_with_retries()\n        adapter = requests.adapters.HTTPAdapter(max_retries=3)\n        self.session.mount('http://', adapter)\n        self.session.mount('https://', adapter)\n\n    def crawl_code_folder(self, base_url: str, path: str = \"\") -> None:\n        headers = { \"Accept\": \"application/vnd.github+json\"}\n        if self.github_token:\n            headers[\"Authorization\"] = f\"token {self.github_token}\"\n        with self.rate_limiter:\n            response = self.session.get( f\"{base_url}/contents/{path}\", headers=headers)\n        if response.status_code != 200:\n            logging.info(f\"Error fetching {base_url}/contents/{path}: {response.text}\")\n            return\n\n        for item in response.json():\n            if item[\"type\"] == \"file\":\n                fname = item[\"path\"]\n                url = item[\"html_url\"]\n                if url.lower().endswith(\".md\") or url.lower().endswith(\".mdx\"):     # Only index markdown files from the code, not the code itself\n                    try:\n                        file_response = self.session.get(item[\"url\"], headers={\"Authorization\": f\"token {self.github_token}\"})\n                        file_content = base64.b64decode(file_response.json()[\"content\"]).decode(\"utf-8\")\n                    except Exception as e:\n                        logging.info(f\"Failed to retrieve content for {fname} with url {url}: {e}\")\n                        continue\n\n                    metadata = {'file': fname, 'source': 'github', 'url': url}\n                    code_doc = {\n                        'documentId': f'github-{item[\"path\"]}',\n                        'title': item[\"name\"],\n                        'description': f'Markdown of {fname}',\n                        'metadataJson': json.dumps(metadata),\n                        'section': [{\n                            'title': 'markdown',\n                            'text': file_content,\n                        }]\n                    }\n                    logging.info(f\"Indexing codebase markdown: {item['path']}\")\n                    self.indexer.index_document(code_doc)\n            elif item[\"type\"] == \"dir\":\n                self.crawl_code_folder(base_url, path=item[\"path\"])\n\n    def crawl_repo(self, repo: str, owner: str, token: str) -> None:\n\n        g = Github(repo, owner, token)\n        issues = g.get_issues(\"all\")\n\n        for d_issue in issues:\n            # Extract issue metadata\n            issue = AttrDict(d_issue)\n            issue_id = f'github-issue-{issue.id}'\n            title = issue.title\n            description = issue.body\n            created_at = str(issue.created_at)\n            updated_at = str(issue.updated_at)\n            labels = [label.name for label in issue.labels]\n            author = issue.user.login\n            metadata = {'issue_number': issue.number, 'labels': labels, 'source': 'github', 'url': issue.html_url, 'state': issue.state}\n\n            issue_doc = {\n                'documentId': f'github-issue-{issue_id}',\n                'title': title,\n                'description': description,\n                'metadataJson': json.dumps(metadata),\n                'section': [{\n                    'title': 'issue',\n                    'text': description,\n                    'metadataJson': json.dumps({\n                        'author': author,\n                        'created_at': created_at,\n                        'updated_at': updated_at\n                    })\n                }]\n            }\n            logging.info(f\"Indexing issue: {issue.id}\")\n            self.indexer.index_document(issue_doc)\n\n            # Extract and index comments\n            comments = g.get_comments(issue.number)\n            if len(comments)>0:\n                logging.info(f\"Indexing {len(comments)} comments for issue {issue.number}\")\n            else:\n                logging.info(f\"No comments for issue {issue.number}\")\n            \n            for d_comment in comments:\n                comment = AttrDict(d_comment)\n                comment_id = comment.id\n                comment_text = comment.body\n                comment_author = comment.user.login\n                comment_created_at = str(comment.created_at)\n                metadata = {'comment_id': comment.id, 'url': comment.html_url, 'source': 'github'}\n\n                comment_doc = {\n                    'documentId': f'github-comment-{comment_id}',\n                    'title': title,\n                    'description': comment_text,\n                    'metadataJson': json.dumps(metadata),\n                    'section': [{\n                        'title': 'comment',\n                        'text': comment_text,\n                        'metadataJson': json.dumps({\n                            'author': comment_author,\n                            'created_at': comment_created_at,\n                            'updated_at': updated_at\n                        })\n                    }]\n                }\n                try:\n                    self.indexer.index_document(comment_doc)\n                except Exception as e:\n                    logging.info(f\"Error {e} indexing comment document {comment_doc}\")\n                    continue\n\n        if self.crawl_code:\n            base_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n            self.crawl_code_folder(base_url)\n\n\n    def crawl(self) -> None:\n        for repo in self.repos:\n            logging.info(f\"Crawling repo {repo}\")\n            self.crawl_repo(repo, self.owner, self.github_token)", "\n"]}
{"filename": "crawlers/pmc_crawler.py", "chunked_list": ["import logging\nimport os\nfrom Bio import Entrez\nimport json\nfrom bs4 import BeautifulSoup\nfrom ratelimiter import RateLimiter\nimport xmltodict\nfrom datetime import datetime, timedelta\nfrom typing import Set, List, Dict, Any\nfrom core.utils import html_to_text, create_session_with_retries", "from typing import Set, List, Dict, Any\nfrom core.utils import html_to_text, create_session_with_retries\nfrom core.crawler import Crawler\nfrom omegaconf import OmegaConf\n\ndef get_top_n_papers(topic: str, n: int, email: str) -> Any:\n    \"\"\"\n    Get the top n papers for a given topic from PMC\n    \"\"\"\n    Entrez.email = email\n    search_results = Entrez.read(\n        Entrez.esearch(\n            db=\"pmc\",\n            term=topic,\n            retmax=n,\n            usehistory=\"y\",\n        )\n    )\n    id_list = search_results[\"IdList\"]    \n    return id_list", "\nclass PmcCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.site_urls: Set[str] = set()\n        self.crawled_pmc_ids: Set[str] = set()\n        self.session = create_session_with_retries()\n\n    def index_papers_by_topic(self, topic: str, n_papers: int) -> None:\n        \"\"\"\n        Index the top n papers for a given topic\n        \"\"\"\n        email = \"crawler@vectara.com\"\n        papers = list(set(get_top_n_papers(topic, n_papers, email)))\n        logging.info(f\"Found {len(papers)} papers for topic {topic}, now indexing...\")\n\n        # index the papers\n        rate_limiter = RateLimiter(max_calls=3, period=1)\n        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n        for i, pmc_id in enumerate(papers):\n            if i%100 == 0:\n                logging.info(f\"Indexed {i} papers so far for topic {topic}\")\n            if pmc_id in self.crawled_pmc_ids:\n                continue\n\n            params = {\"db\": \"pmc\", \"id\": pmc_id, \"retmode\": \"xml\", \"tool\": \"python_script\", \"email\": email}\n            try:\n                with rate_limiter:\n                    response = self.session.get(base_url, params=params)\n            except Exception as e:\n                logging.info(f\"Failed to download paper {pmc_id} due to error {e}, skipping\")\n                continue\n            if response.status_code != 200:\n                logging.info(f\"Failed to download paper {pmc_id}, skipping\")\n                continue\n\n            soup = BeautifulSoup(response.text, \"xml\")\n\n            # Extract the title\n            title_element = soup.find(\"article-title\")\n            if title_element:\n                title = title_element.get_text(strip=True)\n            else:\n                title = \"Title not found\"\n\n    \n            # Extract the publication date\n            pub_date_soup = soup.find(\"pub-date\")\n            if pub_date_soup is not None:\n                year = pub_date_soup.find(\"year\")\n                if year is None:\n                    year_text = '1970'\n                else:\n                    year_text = str(year)\n                month = pub_date_soup.find(\"month\")\n                if month is None:\n                    month_text = '1'\n                else:\n                    month_text = str(month)\n                day = pub_date_soup.find(\"day\")\n                if day is None:\n                    day_text = '1'\n                else:\n                    day_text = str(day)\n\n                try:\n                    pub_date = f\"{year_text}-{month_text}-{day_text}\"\n                except Exception as e:\n                    pub_date = 'unknown'\n            else:\n                pub_date = \"Publication date not found\"\n            \n            self.crawled_pmc_ids.add(pmc_id)\n            logging.info(f\"Indexing paper {pmc_id} with publication date {pub_date} and title '{title}'\")\n\n            # Index the page into Vectara\n            document = {\n                \"documentId\": pmc_id,\n                \"title\": title,\n                \"description\": \"\",\n                \"metadataJson\": json.dumps({\n                    \"url\": f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/\",\n                    \"publicationDate\": pub_date,\n                    \"source\": \"pmc\",\n                }),\n                \"section\": []\n            }\n            for paragraph in soup.findall('body p'):\n                document['section'].append({\n                    \"text\": paragraph.text,\n                })\n\n            succeeded = self.indexer.index_document(document)\n            if not succeeded:\n                logging.info(f\"Failed to index document {pmc_id}\")\n\n    def _get_xml_dict(self) -> Any:\n        days_back = 1\n        max_days = 30\n        while (days_back <= max_days):\n            xml_date = (datetime.now() - timedelta(days = days_back)).strftime(\"%Y-%m-%d\")\n            url = f'https://medlineplus.gov/xml/mplus_topics_{xml_date}.xml'\n            response = self.session.get(url)\n            if response.status_code == 200:\n                break\n            days_back += 1\n        if days_back == max_days:\n            logging.info(f\"Could not find medline plus topics after checkint last {max_days} days\")\n            return {}\n\n        logging.info(f\"Using MedlinePlus topics from {xml_date}\")        \n        url = f'https://medlineplus.gov/xml/mplus_topics_{xml_date}.xml'\n        response = self.session.get(url)\n        response.raise_for_status()\n        xml_dict = xmltodict.parse(response.text)\n        return xml_dict\n\n    def index_medline_plus(self, topics: List[str]) -> None:\n        xml_dict = self._get_xml_dict()\n        logging.info(f\"Indexing {xml_dict['health-topics']['@total']} health topics from MedlinePlus\")    \n        rate_limiter = RateLimiter(max_calls=3, period=1)\n\n        for ht in xml_dict['health-topics']['health-topic']:\n            title = ht['@title']\n            all_names = [title.lower()]\n            if 'also-called' in ht:\n                synonyms = ht['also-called']\n                if type(synonyms)==list:\n                    all_names += [x.lower() for x in synonyms]\n                else:\n                    all_names += [synonyms.lower()]\n            if not any([t.lower() in all_names for t in topics]):\n                logging.info(f\"Skipping {title} because it is not in our list of topics to crawl\")\n                continue\n\n            medline_id = ht['@id']\n            topic_url = ht['@url']\n            date_created = ht['@date-created']\n            summary = html_to_text(ht['full-summary'])\n            meta_desc = ht['@meta-desc']\n            document = {\n                \"documentId\": f'medline-plus-{medline_id}',\n                \"title\": title,\n                \"description\": f'medline information for {title}',\n                \"metadataJson\": json.dumps({\n                    \"url\": topic_url,\n                    \"publicationDate\": date_created,\n                    \"source\": \"pmc\",\n                }),\n                \"section\": [\n                    {\n                        'text': meta_desc\n                    },\n                    {\n                        'text': summary\n                    }\n                ]\n            }\n            logging.info(f\"Indexing data about {title}\")\n            succeeded = self.indexer.index_document(document)\n            if not succeeded:\n                logging.info(f\"Failed to index document with title {title}\")\n                continue\n            for site in ht['site']:\n                site_title = site['@title']\n                site_url = site['@url']\n                if site_url in self.site_urls:\n                    continue\n                else:\n                    self.site_urls.add(site_url)\n                with rate_limiter:\n                    succeeded = self.indexer.index_url(site_url, metadata={'url': site_url, 'source': 'medline_plus', 'title': site_title})\n\n    def crawl(self) -> None:\n        folder = 'papers'\n        os.makedirs(folder, exist_ok=True)\n\n        topics = self.cfg.pmc_crawler.topics\n        n_papers = self.cfg.pmc_crawler.n_papers\n\n        self.index_medline_plus(topics)\n        for topic in topics:\n            self.index_papers_by_topic(topic, n_papers)", ""]}
{"filename": "crawlers/hubspot_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nfrom omegaconf import OmegaConf\nimport requests\nfrom core.utils import clean_email_text\nfrom slugify import slugify\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nimport datetime\n", "import datetime\n\nfrom typing import Any, Dict, List, Tuple\n\n# Initialize Presidio Analyzer and Anonymizer\nanalyzer = AnalyzerEngine()\nanonymizer = AnonymizerEngine()\n\n\nclass HubspotCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.hubspot_api_key = self.cfg.hubspot_crawler.hubspot_api_key\n\n\n    def mask_pii(self, text: str) -> str:\n        # Analyze and anonymize PII data\n        results = analyzer.analyze(text=text,\n                    entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\", \"EMAIL_ADDRESS\", \"IBAN_CODE\", \"PERSON\", \"US_BANK_NUMBER\", \"US_PASSPORT\", \"US_SSN\", \"LOCATION\"],\n                    language='en')    \n        anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n        return str(anonymized_text.text)\n\n\n    def crawl(self) -> None:\n        logging.info(\"Starting HubSpot Crawler.\")\n        \n        # API endpoint for fetching contacts\n        api_endpoint_contacts = \"https://api.hubapi.com/crm/v3/objects/contacts\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        query_params_contacts = {\n            \"limit\": 100\n        }\n\n        after_contact = 1  # This is to use for pagination of contacts. The loop breaks when after_contact is None\n        email_count = 0\n\n        while after_contact:\n            if after_contact:\n                query_params_contacts[\"after\"] = after_contact\n\n            response_contacts = requests.get(api_endpoint_contacts, headers=headers, params=query_params_contacts)\n\n            if response_contacts.status_code == 200:\n                contacts_data = response_contacts.json()\n                contacts = contacts_data[\"results\"]\n\n                if not contacts:\n                    break\n\n                for contact in contacts:\n                    contact_id = contact[\"id\"]\n                    engagements, engagements_per_contact = self.get_contact_engagements(contact_id)\n                    logging.info(f\"NUMBER OF ENGAGEMENTS: {engagements_per_contact} FOR CONTACT ID: {contact_id}\")\n                    \n            \n                    for engagement in engagements:\n                        engagement_type = engagement[\"engagement\"].get(\"type\", \"UNKNOWN\")\n                        if engagement_type == \"EMAIL\" and \"text\" in engagement[\"metadata\"] and \"subject\" in engagement[\"metadata\"]:\n                            email_subject = engagement[\"metadata\"][\"subject\"]\n                            email_text = engagement[\"metadata\"][\"text\"]\n                            email_url = self.get_email_url(contact_id, engagement[\"engagement\"][\"id\"])\n                        else:\n                            continue\n                        \n                        # Skip indexing if email text is empty or None\n                        if email_text is None or email_text.strip() == \"\":\n                            logging.info(f\"Email '{email_subject}' has no text. Skipping indexing.\")\n                            continue\n                        \n                        masked_email_text = self.mask_pii(email_text)\n                        cleaned_email_text = clean_email_text(masked_email_text)\n                        \n                        metadata = {\n                            \"source\": engagement['engagement']['source'],\n                            \"createdAt\": datetime.datetime.utcfromtimestamp(int(engagement['engagement']['createdAt'])/1000).strftime(\"%Y-%m-%d\"),\n                        }\n                        \n                        \n                        # Generate a unique doc_id for indexing\n                        doc_id = str(contact_id) + \"_\" + str(engagement['engagement']['id'])\n                        logging.info(f\"Indexing email with doc_id '{doc_id}' and subject '{email_subject}'\")\n                        succeeded = self.indexer.index_segments(\n                            doc_id=doc_id,\n                            parts=[cleaned_email_text],\n                            metadatas=[metadata],\n                            doc_metadata={'source': 'hubspot', 'title': email_subject, 'url': email_url}\n                        )\n\n                        if succeeded:\n                            logging.info(f\"Email with doc_id '{doc_id}' and subject '{email_subject}' indexed successfully.\")\n                            email_count += 1\n                        else:\n                            logging.error(f\"Failed to index email '{email_subject}'.\")\n\n                    \n                            \n                paging_info = contacts_data.get(\"paging\", {})\n                after_contact = paging_info.get(\"next\", {}).get(\"after\")\n\n                logging.info(f\"Crawled and indexed {email_count} emails successfully\")\n\n            else:\n                logging.error(f\"Error: {response_contacts.status_code} - {response_contacts.text}\")\n\n\n    def get_contact_engagements(self, contact_id: str) -> Tuple[List[Dict[str, Any]], int]:\n        api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        all_engagements = []\n\n        while True:\n            response_engagements = requests.get(api_endpoint_engagements, headers=headers)\n\n            if response_engagements.status_code == 200:\n                engagements_data = response_engagements.json()\n                engagements = engagements_data.get(\"results\", [])\n                all_engagements.extend(engagements)\n\n                # Check if there are more engagements to fetch\n                if engagements_data.get(\"hasMore\"):\n                    offset = engagements_data.get(\"offset\")\n                    api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged?offset={offset}\"\n                else:\n                    break\n            else:\n                logging.error(f\"Error: {response_engagements.status_code} - {response_engagements.text}\")\n                break\n\n        return all_engagements, len(all_engagements)\n\n\n    def get_email_url(self, contact_id: str, engagement_id: str) -> str:\n        email_url = f\"https://app.hubspot.com/contacts/{self.cfg.hubspot_crawler.hubspot_customer_id}/contact/{contact_id}/?engagement={engagement_id}\"\n        return email_url", "\nclass HubspotCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.hubspot_api_key = self.cfg.hubspot_crawler.hubspot_api_key\n\n\n    def mask_pii(self, text: str) -> str:\n        # Analyze and anonymize PII data\n        results = analyzer.analyze(text=text,\n                    entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\", \"EMAIL_ADDRESS\", \"IBAN_CODE\", \"PERSON\", \"US_BANK_NUMBER\", \"US_PASSPORT\", \"US_SSN\", \"LOCATION\"],\n                    language='en')    \n        anonymized_text = anonymizer.anonymize(text=text, analyzer_results=results)\n        return str(anonymized_text.text)\n\n\n    def crawl(self) -> None:\n        logging.info(\"Starting HubSpot Crawler.\")\n        \n        # API endpoint for fetching contacts\n        api_endpoint_contacts = \"https://api.hubapi.com/crm/v3/objects/contacts\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        query_params_contacts = {\n            \"limit\": 100\n        }\n\n        after_contact = 1  # This is to use for pagination of contacts. The loop breaks when after_contact is None\n        email_count = 0\n\n        while after_contact:\n            if after_contact:\n                query_params_contacts[\"after\"] = after_contact\n\n            response_contacts = requests.get(api_endpoint_contacts, headers=headers, params=query_params_contacts)\n\n            if response_contacts.status_code == 200:\n                contacts_data = response_contacts.json()\n                contacts = contacts_data[\"results\"]\n\n                if not contacts:\n                    break\n\n                for contact in contacts:\n                    contact_id = contact[\"id\"]\n                    engagements, engagements_per_contact = self.get_contact_engagements(contact_id)\n                    logging.info(f\"NUMBER OF ENGAGEMENTS: {engagements_per_contact} FOR CONTACT ID: {contact_id}\")\n                    \n            \n                    for engagement in engagements:\n                        engagement_type = engagement[\"engagement\"].get(\"type\", \"UNKNOWN\")\n                        if engagement_type == \"EMAIL\" and \"text\" in engagement[\"metadata\"] and \"subject\" in engagement[\"metadata\"]:\n                            email_subject = engagement[\"metadata\"][\"subject\"]\n                            email_text = engagement[\"metadata\"][\"text\"]\n                            email_url = self.get_email_url(contact_id, engagement[\"engagement\"][\"id\"])\n                        else:\n                            continue\n                        \n                        # Skip indexing if email text is empty or None\n                        if email_text is None or email_text.strip() == \"\":\n                            logging.info(f\"Email '{email_subject}' has no text. Skipping indexing.\")\n                            continue\n                        \n                        masked_email_text = self.mask_pii(email_text)\n                        cleaned_email_text = clean_email_text(masked_email_text)\n                        \n                        metadata = {\n                            \"source\": engagement['engagement']['source'],\n                            \"createdAt\": datetime.datetime.utcfromtimestamp(int(engagement['engagement']['createdAt'])/1000).strftime(\"%Y-%m-%d\"),\n                        }\n                        \n                        \n                        # Generate a unique doc_id for indexing\n                        doc_id = str(contact_id) + \"_\" + str(engagement['engagement']['id'])\n                        logging.info(f\"Indexing email with doc_id '{doc_id}' and subject '{email_subject}'\")\n                        succeeded = self.indexer.index_segments(\n                            doc_id=doc_id,\n                            parts=[cleaned_email_text],\n                            metadatas=[metadata],\n                            doc_metadata={'source': 'hubspot', 'title': email_subject, 'url': email_url}\n                        )\n\n                        if succeeded:\n                            logging.info(f\"Email with doc_id '{doc_id}' and subject '{email_subject}' indexed successfully.\")\n                            email_count += 1\n                        else:\n                            logging.error(f\"Failed to index email '{email_subject}'.\")\n\n                    \n                            \n                paging_info = contacts_data.get(\"paging\", {})\n                after_contact = paging_info.get(\"next\", {}).get(\"after\")\n\n                logging.info(f\"Crawled and indexed {email_count} emails successfully\")\n\n            else:\n                logging.error(f\"Error: {response_contacts.status_code} - {response_contacts.text}\")\n\n\n    def get_contact_engagements(self, contact_id: str) -> Tuple[List[Dict[str, Any]], int]:\n        api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.hubspot_api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        all_engagements = []\n\n        while True:\n            response_engagements = requests.get(api_endpoint_engagements, headers=headers)\n\n            if response_engagements.status_code == 200:\n                engagements_data = response_engagements.json()\n                engagements = engagements_data.get(\"results\", [])\n                all_engagements.extend(engagements)\n\n                # Check if there are more engagements to fetch\n                if engagements_data.get(\"hasMore\"):\n                    offset = engagements_data.get(\"offset\")\n                    api_endpoint_engagements = f\"https://api.hubapi.com/engagements/v1/engagements/associated/contact/{contact_id}/paged?offset={offset}\"\n                else:\n                    break\n            else:\n                logging.error(f\"Error: {response_engagements.status_code} - {response_engagements.text}\")\n                break\n\n        return all_engagements, len(all_engagements)\n\n\n    def get_email_url(self, contact_id: str, engagement_id: str) -> str:\n        email_url = f\"https://app.hubspot.com/contacts/{self.cfg.hubspot_crawler.hubspot_customer_id}/contact/{contact_id}/?engagement={engagement_id}\"\n        return email_url", ""]}
{"filename": "crawlers/rss_crawler.py", "chunked_list": ["import logging\nimport time\nfrom core.crawler import Crawler\nimport feedparser\nfrom datetime import datetime, timedelta\nfrom time import mktime\n\nclass RssCrawler(Crawler):\n\n    def crawl(self) -> None:\n        \"\"\"\n        Crawl RSS feeds and upload to Vectara.\n        \"\"\"\n        rss_pages = self.cfg.rss_crawler.rss_pages\n        source = self.cfg.rss_crawler.source\n        if type(rss_pages) == str:\n            rss_pages = [rss_pages]\n        delay_in_secs = self.cfg.rss_crawler.delay\n        today = datetime.now().replace(microsecond=0)\n        days_ago = today - timedelta(days=self.cfg.rss_crawler.days_past)\n\n        # collect all URLs from the RSS feeds\n        urls = []\n        for rss_page in rss_pages:\n            feed = feedparser.parse(rss_page)\n            for entry in feed.entries:\n                if \"published_parsed\" not in entry:\n                    urls.append([entry.link, entry.title, None])\n                    continue\n                entry_date = datetime.fromtimestamp(mktime(entry.published_parsed))\n                if entry_date >= days_ago and entry_date <= today:\n                    urls.append([entry.link, entry.title, entry_date])\n\n        logging.info(f\"Found {len(urls)} URLs to index from the last {self.cfg.rss_crawler.days_past} days ({source})\")\n\n        crawled_urls = set()        # sometimes same url (with diff title) may appear twice, so we keep track of crawled pages to avoid duplication.\n        for url,title,pub_date in urls:\n            if url in crawled_urls:\n                logging.info(f\"Skipping {url} since it was already crawled in this round\")\n                continue\n            \n            # index document into Vectara\n            try:\n                if pub_date:\n                    pub_date_int = int(str(pub_date.timestamp()).split('.')[0])\n                else:\n                    pub_date_int = 0        # unknown published date\n                    pub_date = 'unknown'\n                crawl_date_int = int(str(today.timestamp()).split('.')[0])\n                metadata = {\n                    'source': source, 'url': url, 'title': title, \n                    'pub_date': str(pub_date), 'pub_date_int': pub_date_int,\n                    'crawl_date': str(today),\n                    'crawl_date_int': crawl_date_int\n                }\n                succeeded = self.indexer.index_url(url, metadata=metadata)\n                if succeeded:\n                    logging.info(f\"Successfully indexed {url}\")\n                    crawled_urls.add(url)\n                else:\n                    logging.info(f\"Indexing failed for {url}\")\n            except Exception as e:\n                logging.error(f\"Error while indexing {url}: {e}\")\n            time.sleep(delay_in_secs)\n\n        return", "\n"]}
{"filename": "crawlers/fmp_crawler.py", "chunked_list": ["import logging\nimport json\n\nfrom typing import Dict, Any\nfrom omegaconf import OmegaConf, DictConfig\n\nfrom core.crawler import Crawler\nfrom core.utils import create_session_with_retries\n\n# Crawler for financial information using the financialmodelingprep.com service", "\n# Crawler for financial information using the financialmodelingprep.com service\n# To use this crawler you have to have an fmp API_key in your secrets.toml profile\nclass FmpCrawler(Crawler):\n    \n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        cfg_dict: DictConfig = DictConfig(cfg)\n        self.tickers = cfg_dict.fmp_crawler.tickers\n        self.start_year = int(cfg_dict.fmp_crawler.start_year)\n        self.end_year = int(cfg_dict.fmp_crawler.end_year)\n        self.api_key = cfg_dict.fmp_crawler.fmp_api_key\n        self.session = create_session_with_retries()\n\n    def index_doc(self, document: Dict[str, Any]) -> bool:\n        try:\n            succeeded = self.indexer.index_document(document)\n            if succeeded:\n                logging.info(f\"Indexed {document['documentId']}\")\n            else:\n                logging.info(f\"Error indexing issue {document['documentId']}\")\n            return succeeded\n        except Exception as e:\n            logging.info(f\"Error during indexing of {document['documentId']}: {e}\")\n            return False\n\n    def crawl(self) -> None:\n        base_url = 'https://financialmodelingprep.com'\n        for ticker in self.tickers:\n            # get profile\n            url = f'{base_url}/api/v3/profile/{ticker}?apikey={self.api_key}'\n            try:\n                response = self.session.get(url)\n            except Exception as e:\n                logging.info(f\"Error getting transcript for {ticker}: {e}\")\n                continue\n            if response.status_code == 200:\n                data = response.json()\n                company_name = data[0]['companyName']\n                logging.info(f\"Processing {company_name}\")\n            else:\n                logging.info(f\"Can't get company profile for {ticker} - skipping\")\n                continue\n\n            # index 10-K for ticker in date range\n            url = f'{base_url}/api/v3/sec_filings/{ticker}?type=10-K&page=0&apikey={self.api_key}'\n            filings = self.session.get(url).json()\n            for year in range(self.start_year, self.end_year+1):\n                url = f'{base_url}/api/v4/financial-reports-json?symbol={ticker}&year={year}&period=FY&apikey={self.api_key}'\n                try:\n                    response = self.session.get(url)\n                except Exception as e:\n                    logging.info(f\"Error getting transcript for {ticker}: {e}\")\n                    continue\n                if response.status_code == 200:\n                    data = response.json()\n                    doc_title = f\"10-K for {company_name} from {year}\"\n                    rel_filings = [f for f in filings if f['acceptedDate'][:4] == str(year)]\n                    url = rel_filings[0]['finalLink'] if len(rel_filings)>0 else None\n                    metadata = {'source': 'finsearch', 'title': doc_title, 'ticker': ticker, 'company name': company_name, 'year': year, 'type': '10-K', 'url': url}\n                    document: Dict[str, Any] = {\n                        \"documentId\": f\"10-K-{company_name}-{year}\",\n                        \"title\": doc_title,\n                        \"metadataJson\": json.dumps(metadata),\n                        \"section\": []\n                    }\n                    for key in data.keys():\n                        if type(data[key])==str:\n                            continue\n                        for item_dict in data[key]:\n                            for title, values in item_dict.items():\n                                values = [v for v in values if v and type(v)==str and len(v)>=10]\n                                if len(values)>0 and len(' '.join(values))>100:\n                                    document['section'].append({'title': f'{key} - {title}', 'text': '\\n'.join(values)})\n                    self.index_doc(document)\n\n            # Index earnings call transcript\n            logging.info(f\"Getting transcripts\")\n            for year in range(self.start_year, self.end_year+1):\n                for quarter in range(1, 5):\n                    url = f'{base_url}/api/v3/earning_call_transcript/{ticker}?quarter={quarter}&year={year}&apikey={self.api_key}'\n                    try:\n                        response = self.session.get(url)\n                    except Exception as e:\n                        logging.info(f\"Error getting transcript for {company_name} quarter {quarter} of {year}: {e}\")\n                        continue\n                    if response.status_code == 200:\n                        for transcript in response.json():\n                            title = f\"Earnings call transcript for {company_name}, quarter {quarter} of {year}\"\n                            metadata = {'source': 'finsearch', 'title': title, 'ticker': ticker, 'company name': company_name, 'year': year, 'quarter': quarter, 'type': 'transcript'}\n                            document = {\n                                \"documentId\": f\"transcript-{company_name}-{year}-{quarter}\",\n                                \"title\": title,\n                                \"metadataJson\": json.dumps(metadata),\n                                \"section\": [\n                                    {\n                                        'text': transcript['content']\n                                    }\n                                ]\n                            }\n                            self.index_doc(document)", ""]}
{"filename": "crawlers/discourse_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nfrom omegaconf import OmegaConf\nimport json\nfrom html.parser import HTMLParser\nfrom io import StringIO\nfrom core.utils import create_session_with_retries\nfrom typing import List, Dict, Any\n\nclass MLStripper(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.text = StringIO()\n    def get_data(self) -> str:\n        return self.text.getvalue()", "\nclass MLStripper(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.reset()\n        self.strict = False\n        self.convert_charrefs= True\n        self.text = StringIO()\n    def get_data(self) -> str:\n        return self.text.getvalue()", "\ndef strip_html(text: str) -> str:\n    \"\"\"\n    Strip HTML tags from text\n    \"\"\"\n    s = MLStripper()\n    s.feed(text)\n    return s.get_data()\n\nclass DiscourseCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.discourse_base_url = self.cfg.discourse_crawler.base_url\n        self.discourse_api_key = self.cfg.discourse_crawler.discourse_api_key\n        self.session = create_session_with_retries()\n\n    # function to fetch the topics from the Discourse API\n    def index_topics(self) -> List[Dict[str, Any]]:\n        url = self.discourse_base_url + '/latest.json'\n        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com', 'page': '0'}\n        response = self.session.get(url, params=params)\n        if response.status_code != 200:\n            raise Exception(f'Failed to fetch topics from Discourse, exception = {response.status_code}, {response.text}')\n\n        # index all topics\n        topics = list(json.loads(response.text)['topic_list']['topics'])\n        for topic in topics:\n            topic_id = topic['id']\n            logging.info(f\"Indexing topic {topic_id}\")\n            url = self.discourse_base_url + '/t/' + str(topic_id)\n            document = {\n                'documentId': 'topic-' + str(topic_id),\n                'title': topic['title'],\n                'metadataJson': json.dumps({\n                    'created_at': topic['created_at'],\n                    'views': topic['views'],\n                    'like_count': topic['like_count'],\n                    'last_poster': topic['last_poster_username'],\n                    'source': 'discourse',\n                    'url': url\n                }),\n                'section': [\n                    {\n                        'text': topic['fancy_title'],\n                    }\n                ]\n            }\n            self.indexer.index_document(document)\n        return topics\n\n    # function to fetch the posts for a topic from the Discourse API\n    def index_posts(self, topic: Dict[str, Any]) -> List[Any]:\n        topic_id = topic[\"id\"]\n        url_json = self.discourse_base_url + '/t/' + str(topic_id) + '.json'\n        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com'}\n        response = self.session.get(url_json, params=params)\n        if response.status_code != 200:\n            raise Exception('Failed to fetch posts for topic ' + str(topic_id) + ' from Discourse')\n\n        # parse the response JSON\n        posts = list(json.loads(response.text)['post_stream']['posts'])\n        for post in posts:\n            post_id = post['id']\n            logging.info(f\"Indexing post {post_id}\")\n            document = {\n                'documentId': 'post-' + str(post_id),\n                'title': topic['title'],\n                'metadataJson': json.dumps({\n                    'created_at': post['created_at'],\n                    'updated_at': post['updated_at'],\n                    'poster': post['username'],\n                    'poster_name': post['name'],\n                    'source': 'discourse',\n                    'url': self.discourse_base_url + '/p/' + str(post_id)\n                }),\n                'section': [\n                    {\n                        'text': strip_html(post['cooked'])\n                    }\n                ]\n            }\n            self.indexer.index_document(document)\n        return posts\n\n    def crawl(self) -> None:\n        topics = self.index_topics()\n        logging.info(f\"Indexed {len(topics)} topics from Discourse\")\n        for topic in topics:\n            posts = self.index_posts(topic)\n            logging.info(f\"Indexed {len(posts)} posts for topic {topic['id']} from Discourse\")", "\nclass DiscourseCrawler(Crawler):\n\n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.discourse_base_url = self.cfg.discourse_crawler.base_url\n        self.discourse_api_key = self.cfg.discourse_crawler.discourse_api_key\n        self.session = create_session_with_retries()\n\n    # function to fetch the topics from the Discourse API\n    def index_topics(self) -> List[Dict[str, Any]]:\n        url = self.discourse_base_url + '/latest.json'\n        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com', 'page': '0'}\n        response = self.session.get(url, params=params)\n        if response.status_code != 200:\n            raise Exception(f'Failed to fetch topics from Discourse, exception = {response.status_code}, {response.text}')\n\n        # index all topics\n        topics = list(json.loads(response.text)['topic_list']['topics'])\n        for topic in topics:\n            topic_id = topic['id']\n            logging.info(f\"Indexing topic {topic_id}\")\n            url = self.discourse_base_url + '/t/' + str(topic_id)\n            document = {\n                'documentId': 'topic-' + str(topic_id),\n                'title': topic['title'],\n                'metadataJson': json.dumps({\n                    'created_at': topic['created_at'],\n                    'views': topic['views'],\n                    'like_count': topic['like_count'],\n                    'last_poster': topic['last_poster_username'],\n                    'source': 'discourse',\n                    'url': url\n                }),\n                'section': [\n                    {\n                        'text': topic['fancy_title'],\n                    }\n                ]\n            }\n            self.indexer.index_document(document)\n        return topics\n\n    # function to fetch the posts for a topic from the Discourse API\n    def index_posts(self, topic: Dict[str, Any]) -> List[Any]:\n        topic_id = topic[\"id\"]\n        url_json = self.discourse_base_url + '/t/' + str(topic_id) + '.json'\n        params = { 'api_key': self.discourse_api_key, 'api_username': 'ofer@vectara.com'}\n        response = self.session.get(url_json, params=params)\n        if response.status_code != 200:\n            raise Exception('Failed to fetch posts for topic ' + str(topic_id) + ' from Discourse')\n\n        # parse the response JSON\n        posts = list(json.loads(response.text)['post_stream']['posts'])\n        for post in posts:\n            post_id = post['id']\n            logging.info(f\"Indexing post {post_id}\")\n            document = {\n                'documentId': 'post-' + str(post_id),\n                'title': topic['title'],\n                'metadataJson': json.dumps({\n                    'created_at': post['created_at'],\n                    'updated_at': post['updated_at'],\n                    'poster': post['username'],\n                    'poster_name': post['name'],\n                    'source': 'discourse',\n                    'url': self.discourse_base_url + '/p/' + str(post_id)\n                }),\n                'section': [\n                    {\n                        'text': strip_html(post['cooked'])\n                    }\n                ]\n            }\n            self.indexer.index_document(document)\n        return posts\n\n    def crawl(self) -> None:\n        topics = self.index_topics()\n        logging.info(f\"Indexed {len(topics)} topics from Discourse\")\n        for topic in topics:\n            posts = self.index_posts(topic)\n            logging.info(f\"Indexed {len(posts)} posts for topic {topic['id']} from Discourse\")", ""]}
{"filename": "crawlers/edgar_crawler.py", "chunked_list": ["import logging\nfrom omegaconf import OmegaConf\nimport time\nfrom bs4 import BeautifulSoup \nimport pandas as pd\nimport datetime\nfrom ratelimiter import RateLimiter\n\nfrom core.crawler import Crawler\nfrom core.utils import create_session_with_retries", "from core.crawler import Crawler\nfrom core.utils import create_session_with_retries\n\nfrom typing import Dict, List\n\n\n# build mapping of ticker to cik\ndf = pd.read_csv('https://www.sec.gov/include/ticker.txt', sep='\\t', names=['ticker', 'cik'], dtype=str)\nticker_dict = dict(zip(df.ticker.map(lambda x: str(x).upper()), df.cik))\n    \ndef get_headers() -> Dict[str, str]:\n    \"\"\"\n    Get a set of headers to use for HTTP requests.\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\" \n    }\n    return headers", "ticker_dict = dict(zip(df.ticker.map(lambda x: str(x).upper()), df.cik))\n    \ndef get_headers() -> Dict[str, str]:\n    \"\"\"\n    Get a set of headers to use for HTTP requests.\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\" \n    }\n    return headers", "\ndef get_filings(cik: str, start_date_str: str, end_date_str: str, filing_type: str = \"10-K\") -> List[Dict[str, str]]:\n    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n    params = {\n        \"action\": \"getcompany\", \"CIK\": cik, \"type\": filing_type, \"dateb\": \"\", \"owner\": \"exclude\", \n        \"start\": \"\", \"output\": \"atom\", \"count\": \"100\"\n    }\n    start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d')\n    \n    filings: List[Dict[str, str]] = []\n    current_start = 0\n    rate_limiter = RateLimiter(max_calls=1, period=1)\n    \n    session = create_session_with_retries()\n\n    while True:\n        params[\"start\"] = str(current_start)\n\n        with rate_limiter:\n            response = session.get(base_url, params=params, headers=get_headers())\n        if response.status_code != 200:\n            logging.warning(f\"Error: status code {response.status_code} for {cik}\")\n            return filings\n        soup = BeautifulSoup(response.content, 'lxml-xml')\n        entries = soup.find_all(\"entry\")\n\n        if len(entries) == 0:\n            break\n        \n        for entry in entries:\n            filing_date_str = entry.find(\"filing-date\").text\n            filing_date = datetime.datetime.strptime(filing_date_str, '%Y-%m-%d')\n\n            if start_date <= filing_date <= end_date:\n                try:\n                    url = entry.link[\"href\"]\n                    with rate_limiter:\n                        soup = BeautifulSoup(session.get(url, headers=get_headers()).content, \"html.parser\")\n                    l = soup.select_one('td:-soup-contains(\"10-K\") + td a')\n                    html_url = \"https://www.sec.gov\" + str(l[\"href\"])\n                    l = soup.select_one('td:-soup-contains(\"Complete submission text file\") + td a')\n                    submission_url = \"https://www.sec.gov\" + str(l[\"href\"])\n                    filings.append({\"date\": filing_date_str, \"submission_url\": submission_url, \"html_url\": html_url})\n                except Exception as e:\n                    pass\n            elif filing_date < start_date:\n                logging.info(f\"Error: filing date {filing_date_str} is before start date {start_date}\")\n                return filings\n        \n        current_start += len(entries)\n\n    return filings", "\nclass EdgarCrawler(Crawler):\n    \n    def __init__(self, cfg: OmegaConf, endpoint: str, customer_id: str, corpus_id: int, api_key: str) -> None:\n        super().__init__(cfg, endpoint, customer_id, corpus_id, api_key)\n        self.tickers = self.cfg.edgar_crawler.tickers\n        self.start_date = self.cfg.edgar_crawler.start_date\n        self.end_date = self.cfg.edgar_crawler.end_date\n\n    def crawl(self) -> None:\n        rate_limiter = RateLimiter(max_calls=1, period=1)\n        for ticker in self.tickers:\n            logging.info(f\"downloading 10-Ks for {ticker}\")\n            \n            cik = ticker_dict[ticker]\n            filings = get_filings(cik, self.start_date, self.end_date, '10-K')\n\n            # no more filings in search universe\n            if len(filings) == 0:\n                logging.info(f\"For {ticker}, no filings found in search universe\")\n                continue\n            for filing in filings:\n                url = filing['html_url']\n                title = ticker + '-' + filing['date'] + '-' + filing['html_url'].split(\"/\")[-1].split(\".\")[0]\n                logging.info(f\"indexing document {url}\")\n                metadata = {'source': 'edgar', 'url': url, 'title': title}\n                with rate_limiter:\n                    succeeded = self.indexer.index_url(url, metadata=metadata)\n                if not succeeded:\n                    logging.info(f\"Indexing failed for url {url}\")\n                time.sleep(1)", "\n\n"]}
{"filename": "crawlers/csv_crawler.py", "chunked_list": ["import logging\nfrom core.crawler import Crawler\nimport pandas as pd\nimport unicodedata\n\nclass CsvCrawler(Crawler):\n\n    def crawl(self) -> None:\n        text_columns = list(self.cfg.csv_crawler.text_columns)\n        metadata_columns = list(self.cfg.csv_crawler.metadata_columns)\n        csv_path = self.cfg.csv_crawler.csv_path\n        csv_file = '/home/vectara/data/file.csv'\n        doc_id_columns = list(self.cfg.csv_crawler.get(\"doc_id_columns\", None))\n\n        all_columns = text_columns + metadata_columns\n        df = pd.read_csv(csv_file, usecols=all_columns)\n        \n        logging.info(f\"indexing {len(df)} rows from the CSV file {csv_path}\")\n\n        def index_df(doc_id: str, title: str, df: pd.DataFrame) -> None:\n            parts = []\n            metadatas = []\n            for _, row in df.iterrows():\n                text = ' - '.join(str(x) for x in row[text_columns].tolist() if x)\n                parts.append(unicodedata.normalize('NFD', text))\n                metadatas.append({column: row[column] for column in metadata_columns})\n            logging.info(f\"Indexing df for '{doc_id}' with ({len(df)}) rows\")\n            self.indexer.index_segments(doc_id, parts, metadatas, title=title, doc_metadata = {'source': 'csv'})\n\n        if doc_id_columns:\n            grouped = df.groupby(doc_id_columns)\n            for name, group in grouped:\n                gr_str = name if type(name)==str else ' - '.join(str(x) for x in name)\n                index_df(doc_id=gr_str, title=gr_str, df=group)\n        else:\n            rows_per_chunk = self.cfg.csv_crawler.get(\"rows_per_chunk\", 500)\n            for inx in range(0, df.shape[0], rows_per_chunk):\n                sub_df = df[inx: inx+rows_per_chunk]\n                name = f'rows {inx}-{inx+rows_per_chunk-1}'\n                index_df(doc_id=name, title=name, df=sub_df)", "        "]}
